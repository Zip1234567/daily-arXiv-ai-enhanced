<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 116]
- [eess.IV](#eess.IV) [Total: 23]
- [cs.GR](#cs.GR) [Total: 3]
- [physics.geo-ph](#physics.geo-ph) [Total: 4]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [cs.LG](#cs.LG) [Total: 8]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [cs.CL](#cs.CL) [Total: 5]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [physics.optics](#physics.optics) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Dual Branch VideoMamba with Gated Class Token Fusion for Violence Detection](https://arxiv.org/abs/2506.03162)
*Damith Chamalke Senadeera,Xiaoyun Yang,Dimitrios Kollias,Gregory Slabaugh*

Main category: cs.CV

TL;DR: 提出了一种结合双分支设计和状态空间模型（SSM）的高效架构Dual Branch VideoMamba with GCTF，用于暴力检测，并在新基准上实现了最优性能。


<details>
  <summary>Details</summary>
Motivation: 监控摄像头的快速普及增加了对自动化暴力检测的需求，但现有方法（如CNN和Transformer）在长时依赖和计算效率方面表现不足。

Method: 采用双分支设计，分别捕捉空间和时间特征，并通过门控机制进行连续融合，结合SSM作为主干网络。

Result: 在新合并的暴力检测数据集上实现了最优性能，平衡了准确性和计算效率。

Conclusion: SSM在可扩展的实时监控暴力检测中具有潜力。

Abstract: The rapid proliferation of surveillance cameras has increased the demand for
automated violence detection. While CNNs and Transformers have shown success in
extracting spatio-temporal features, they struggle with long-term dependencies
and computational efficiency. We propose Dual Branch VideoMamba with Gated
Class Token Fusion (GCTF), an efficient architecture combining a dual-branch
design and a state-space model (SSM) backbone where one branch captures spatial
features, while the other focuses on temporal dynamics, with continuous fusion
via a gating mechanism. We also present a new benchmark by merging RWF-2000,
RLVS, and VioPeru datasets in video violence detection, ensuring strict
separation between training and testing sets. Our model achieves
state-of-the-art performance on this benchmark offering an optimal balance
between accuracy and computational efficiency, demonstrating the promise of
SSMs for scalable, real-time surveillance violence detection.

</details>


### [2] [Farm-LightSeek: An Edge-centric Multimodal Agricultural IoT Data Analytics Framework with Lightweight LLMs](https://arxiv.org/abs/2506.03168)
*Dawen Jiang,Zhishu Shen,Qiushi Zheng,Tiehua Zhang,Wei Xiang,Jiong Jin*

Main category: cs.CV

TL;DR: Farm-LightSeek是一个基于边缘计算和大型语言模型（LLM）的农业物联网数据分析框架，旨在解决智能农业中的多模态数据融合、动态环境适应和实时决策问题。


<details>
  <summary>Details</summary>
Motivation: 全球人口增长和气候变化对传统农业物联网系统提出了挑战，智能农业依赖专家知识且难以实时处理多模态数据，LLM为解决这些问题提供了潜力。

Method: 提出Farm-LightSeek框架，通过边缘计算和LLM集成，实时采集农田多源数据（图像、天气、地理信息），进行跨模态推理和疾病检测，并实现低延迟决策。

Result: 实验表明，Farm-LightSeek在边缘计算资源有限的情况下，仍能可靠完成关键任务。

Conclusion: Farm-LightSeek推动了实时智能农业解决方案的发展，展示了农业物联网与LLM深度融合的潜力。

Abstract: Amid the challenges posed by global population growth and climate change,
traditional agricultural Internet of Things (IoT) systems is currently
undergoing a significant digital transformation to facilitate efficient big
data processing. While smart agriculture utilizes artificial intelligence (AI)
technologies to enable precise control, it still encounters significant
challenges, including excessive reliance on agricultural expert knowledge,
difficulties in fusing multimodal data, poor adaptability to dynamic
environments, and bottlenecks in real-time decision-making at the edge. Large
language models (LLMs), with their exceptional capabilities in knowledge
acquisition and semantic understanding, provide a promising solution to address
these challenges. To this end, we propose Farm-LightSeek, an edge-centric
multimodal agricultural IoT data analytics framework that integrates LLMs with
edge computing. This framework collects real-time farmland multi-source data
(images, weather, geographic information) via sensors, performs cross-modal
reasoning and disease detection at edge nodes, conducts low-latency management
decisions, and enables cloud collaboration for model updates. The main
innovations of Farm-LightSeek include: (1) an agricultural
"perception-decision-action" closed-loop architecture; (2) cross-modal adaptive
monitoring; and (3)a lightweight LLM deployment strategy balancing performance
and efficiency. Experiments conducted on two real-world datasets demonstrate
that Farm-LightSeek consistently achieves reliable performance in
mission-critical tasks, even under the limitations of edge computing resources.
This work advances intelligent real-time agricultural solutions and highlights
the potential for deeper integration of agricultural IoT with LLMs.

</details>


### [3] [Improvement of human health lifespan with hybrid group pose estimation methods](https://arxiv.org/abs/2506.03169)
*Arindam Chaudhuri*

Main category: cs.CV

TL;DR: 提出了一种基于混合集成方法的群体姿态估计技术，旨在通过结合改进的群体姿态估计和实时姿态估计方法，提升多人姿态检测的准确性和实时性。


<details>
  <summary>Details</summary>
Motivation: 人类姿态估计在健康监测等领域有广泛应用，但现有方法在遮挡和密集回归准确性上存在不足。

Method: 结合改进的群体姿态估计和实时姿态估计方法，通过姿态变换提取特征，训练混合集成模型，并在公开数据集上进行评估。

Result: 该方法在实时姿态估计中表现最优，提高了对遮挡的鲁棒性和密集回归准确性。

Conclusion: 该方法在实时应用中具有潜力，能够改善人类健康监测的效果。

Abstract: Human beings rely heavily on estimation of poses in order to access their
body movements. Human pose estimation methods take advantage of computer vision
advances in order to track human body movements in real life applications. This
comes from videos which are recorded through available devices. These
para-digms provide potential to make human movement measurement more accessible
to users. The consumers of pose estimation movements believe that human poses
content tend to supplement available videos. This has increased pose estimation
software usage to estimate human poses. In order to address this problem, we
develop hybrid-ensemble-based group pose estimation method to improve human
health. This proposed hybrid-ensemble-based group pose estimation method aims
to detect multi-person poses using modified group pose estimation and modified
real time pose estimation. This ensemble allows fusion of performance of stated
methods in real time. The input poses from images are fed into individual
meth-ods. The pose transformation method helps to identify relevant features
for en-semble to perform training effectively. After this, customized
pre-trained hybrid ensemble is trained on public benchmarked datasets which is
being evaluated through test datasets. The effectiveness and viability of
proposed method is estab-lished based on comparative analysis of group pose
estimation methods and ex-periments conducted on benchmarked datasets. It
provides best optimized results in real-time pose estimation. It makes pose
estimation method more robust to oc-clusion and improves dense regression
accuracy. These results have affirmed po-tential application of this method in
several real-time situations with improvement in human health life span

</details>


### [4] [PALADIN : Robust Neural Fingerprinting for Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.03170)
*Murthy L,Subarna Tripathi*

Main category: cs.CV

TL;DR: 论文提出了一种基于循环纠错码的神经指纹技术，用于提高文本到图像扩散模型的归属准确性。


<details>
  <summary>Details</summary>
Motivation: 开源文本到图像生成模型可能被恶意滥用，现有神经指纹技术无法达到100%的归属准确性，限制了实际部署。

Method: 利用编码理论中的循环纠错码概念，设计了一种新的神经指纹方法。

Result: 该方法显著提高了归属准确性，接近完美水平。

Conclusion: 提出的方法为解决神经指纹技术的归属准确性提供了可行方案。

Abstract: The risk of misusing text-to-image generative models for malicious uses,
especially due to the open-source development of such models, has become a
serious concern. As a risk mitigation strategy, attributing generative models
with neural fingerprinting is emerging as a popular technique. There has been a
plethora of recent work that aim for addressing neural fingerprinting. A
trade-off between the attribution accuracy and generation quality of such
models has been studied extensively. None of the existing methods yet achieved
$100\%$ attribution accuracy. However, any model with less than \emph{perfect}
accuracy is practically non-deployable. In this work, we propose an accurate
method to incorporate neural fingerprinting for text-to-image diffusion models
leveraging the concepts of cyclic error correcting codes from the literature of
coding theory.

</details>


### [5] [EdgeVidSum: Real-Time Personalized Video Summarization at the Edge](https://arxiv.org/abs/2506.03171)
*Ghulam Mujtaba,Eun-Seok Ryu*

Main category: cs.CV

TL;DR: EdgeVidSum是一种轻量级方法，直接在边缘设备上生成长视频的个性化快进摘要，通过本地数据处理保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 解决传统视频摘要方法计算复杂度高、无法在资源受限设备上实时运行的问题，同时满足个性化需求和隐私保护。

Method: 使用基于缩略图的容器技术和高效神经网络架构，通过轻量级2D CNN模型从缩略图中识别用户偏好内容并生成时间戳，实现快速摘要。

Result: 能够在资源受限设备（如Jetson Nano）上实时生成长视频的个性化摘要，如电影、体育赛事和电视节目。

Conclusion: EdgeVidSum在计算效率、个性化和隐私保护方面解决了现代视频消费环境中的关键挑战。

Abstract: EdgeVidSum is a lightweight method that generates personalized, fast-forward
summaries of long-form videos directly on edge devices. The proposed approach
enables real-time video summarization while safeguarding user privacy through
local data processing using innovative thumbnail-based techniques and efficient
neural architectures. Unlike conventional methods that process entire videos
frame by frame, the proposed method uses thumbnail containers to significantly
reduce computational complexity without sacrificing semantic relevance. The
framework employs a hierarchical analysis approach, where a lightweight 2D CNN
model identifies user-preferred content from thumbnails and generates
timestamps to create fast-forward summaries. Our interactive demo highlights
the system's ability to create tailored video summaries for long-form videos,
such as movies, sports events, and TV shows, based on individual user
preferences. The entire computation occurs seamlessly on resource-constrained
devices like Jetson Nano, demonstrating how EdgeVidSum addresses the critical
challenges of computational efficiency, personalization, and privacy in modern
video consumption environments.

</details>


### [6] [FOLIAGE: Towards Physical Intelligence World Models Via Unbounded Surface Evolution](https://arxiv.org/abs/2506.03173)
*Xiaoyi Liu,Hao Tang*

Main category: cs.CV

TL;DR: FOLIAGE提出了一种基于物理的多模态世界模型，用于无界表面增长，通过统一编码器和物理感知预测器生成模态无关的生长嵌入（MAGE），并在SURF-GARDEN平台上验证其性能。


<details>
  <summary>Details</summary>
Motivation: 物理智能对下一代世界模型至关重要，需要从多感官观察中预测和塑造世界。

Method: FOLIAGE采用统一上下文编码器和物理感知预测器，结合Accretive Graph Network（AGN）和多种增强技术（如Geometry-Correspondence Fusion）生成MAGE。

Result: FOLIAGE在SURF-BENCH评估中表现优于专用基线，并在动态环境中保持鲁棒性。

Conclusion: FOLIAGE为物理智能提供了一种新的多模态世界模型路径。

Abstract: Physical intelligence -- anticipating and shaping the world from partial,
multisensory observations -- is critical for next-generation world models. We
propose FOLIAGE, a physics-informed multimodal world model for unbounded
accretive surface growth. In its Action-Perception loop, a unified context
encoder maps images, mesh connectivity, and point clouds to a shared latent
state. A physics-aware predictor, conditioned on physical control actions,
advances this latent state in time to align with the target latent of the
surface, yielding a Modality-Agnostic Growth Embedding (MAGE) that interfaces
with critic heads for downstream objectives. FOLIAGE's Accretive Graph Network
(AGN) captures dynamic connectivity through Age Positional Encoding and
Energy-Gated Message-Passing. Geometry-Correspondence Fusion and Cross-Patch
Masking enhance MAGE's expressiveness, while Hierarchical Pooling balances
global context with local dynamics. We create SURF-GARDEN, a world model
learning platform comprising a Counterfactual Physics Simulator, a Multimodal
Correspondence Extractor, and Evolution Tracing, which generates 7,200 diverse
surface-growth sequences. SURF-BENCH, our physical-intelligence evaluation
suite, evaluates six core tasks -- topology recognition, inverse material
estimation, growth-stage classification, latent roll-out, cross-modal
retrieval, and dense correspondence -- and four stress tests -- sensor dropout,
zero-shot modality transfer, long-horizon prediction, and physics ablation --
to probe resilience. FOLIAGE outperforms specialized baselines while remaining
robust across dynamic environments, establishing a new world-model based,
multimodal pathway to physical intelligence.

</details>


### [7] [Multimodal Foundation Model for Cross-Modal Retrieval and Activity Recognition Tasks](https://arxiv.org/abs/2506.03174)
*Koki Matsuishi,Kosuke Ukita,Tsuyoshi Okita*

Main category: cs.CV

TL;DR: 论文提出AURA-MFM，一种多模态基础模型，整合第三人称视频、动作捕捉、IMU和文本数据，以更全面地分析人类活动，并在实验中获得优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基础模型主要依赖第一人称视频和文本数据，无法全面分析全身活动，因此需要一种能整合更多模态的模型。

Method: 提出AURA-MFM，整合第三人称视频、动作捕捉、IMU和文本四种模态，并采用基于Transformer的IMU编码器提升性能。

Result: 在检索和活动识别任务中表现优异，零样本分类的F1分数为0.6226，准确率为0.7320，显著优于现有方法。

Conclusion: AURA-MFM通过多模态整合显著提升了人类活动分析的全面性和准确性，为未来研究提供了新方向。

Abstract: In recent years, the widespread adoption of wearable devices has highlighted
the growing importance of behavior analysis using IMU. While applications span
diverse fields such as healthcare and robotics, recent studies have
increasingly focused on multimodal analysis, in addition to unimodal analysis.
Several studies have proposed multimodal foundation models that incorporate
first-person video and text data; however, these models still fall short in
providing a detailed analysis of full-body human activity. To address this
limitation, we propose Activity Understanding and Representations Alignment -
Multimodal Foundation Model (AURA-MFM), a foundational model integrating four
modalities: third-person video, motion capture, IMU, and text. By incorporating
third-person video and motion capture data, the model enables a detailed and
multidimensional understanding of human activity, which first-person
perspectives alone fail to capture. Additionally, a Transformer-based IMU
encoder is employed to enhance the model's overall performance. Experimental
evaluations on retrieval and activity recognition tasks demonstrate that our
model surpasses existing methods. Notably, in the zero-shot classification for
action recognition, our method achieved significantly higher performance, with
an F1-score of 0.6226 and an accuracy of 0.7320, whereas the existing method
recorded an F1-score of 0.0747 and an accuracy of 0.1961.

</details>


### [8] [Vid-SME: Membership Inference Attacks against Large Video Understanding Models](https://arxiv.org/abs/2506.03179)
*Qi Li,Runpeng Yu,Xinchao Wang*

Main category: cs.CV

TL;DR: Vid-SME是一种针对视频数据的成员推理方法，用于检测视频理解大语言模型（VULLMs）中是否使用了特定视频作为训练数据。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在视频理解中的应用日益广泛，但其训练数据可能包含敏感视频内容，引发隐私问题。现有成员推理方法（MIAs）在视频领域效果不佳。

Method: 提出Vid-SME方法，利用模型输出的置信度和Sharma-Mittal熵（SME）差异，结合自适应参数化，计算视频帧的成员分数。

Result: 实验表明，Vid-SME在多种自训练和开源VULLMs中表现优异。

Conclusion: Vid-SME是首个针对视频数据的成员推理方法，有效解决了视频隐私问题。

Abstract: Multimodal large language models (MLLMs) demonstrate remarkable capabilities
in handling complex multimodal tasks and are increasingly adopted in video
understanding applications. However, their rapid advancement raises serious
data privacy concerns, particularly given the potential inclusion of sensitive
video content, such as personal recordings and surveillance footage, in their
training datasets. Determining improperly used videos during training remains a
critical and unresolved challenge. Despite considerable progress on membership
inference attacks (MIAs) for text and image data in MLLMs, existing methods
fail to generalize effectively to the video domain. These methods suffer from
poor scalability as more frames are sampled and generally achieve negligible
true positive rates at low false positive rates (TPR@Low FPR), mainly due to
their failure to capture the inherent temporal variations of video frames and
to account for model behavior differences as the number of frames varies. To
address these challenges, we introduce Vid-SME, the first membership inference
method tailored for video data used in video understanding LLMs (VULLMs).
Vid-SME leverages the confidence of model output and integrates adaptive
parameterization to compute Sharma-Mittal entropy (SME) for video inputs. By
leveraging the SME difference between natural and temporally-reversed video
frames, Vid-SME derives robust membership scores to determine whether a given
video is part of the model's training set. Experiments on various self-trained
and open-sourced VULLMs demonstrate the strong effectiveness of Vid-SME.

</details>


### [9] [TerraIncognita: A Dynamic Benchmark for Species Discovery Using Frontier Models](https://arxiv.org/abs/2506.03182)
*Shivani Chiranjeevi,Hossein Zaremehrjerdi,Zi K. Deng,Talukder Z. Jubery,Ari Grele,Arti Singh,Asheesh K Singh,Soumik Sarkar,Nirav Merchant,Harold F. Greeney,Baskar Ganapathysubramanian,Chinmay Hegde*

Main category: cs.CV

TL;DR: TerraIncognita是一个动态基准，用于评估多模态模型在识别未知昆虫物种方面的能力，结合已知和稀有物种图像，模拟真实发现场景。


<details>
  <summary>Details</summary>
Motivation: 解决昆虫物种发现速度慢、依赖专家知识的问题，推动及时保护行动。

Method: 构建包含已知和稀有昆虫物种图像的基准数据集，评估模型在分类、检测OOD样本和生成解释方面的能力。

Result: 模型在粗分类（如目级）表现优异（F1>90%），但在细分类（如种级）表现极差（F1<2%）。

Conclusion: TerraIncognita为AI方法提供了一个持续演进的纵向基准平台，支持未来扩展。

Abstract: The rapid global loss of biodiversity, particularly among insects, represents
an urgent ecological crisis. Current methods for insect species discovery are
manual, slow, and severely constrained by taxonomic expertise, hindering timely
conservation actions. We introduce TerraIncognita, a dynamic benchmark designed
to evaluate state-of-the-art multimodal models for the challenging problem of
identifying unknown, potentially undescribed insect species from image data.
Our benchmark dataset combines a mix of expertly annotated images of insect
species likely known to frontier AI models, and images of rare and poorly known
species, for which few/no publicly available images exist. These images were
collected from underexplored biodiversity hotspots, realistically mimicking
open-world discovery scenarios faced by ecologists. The benchmark assesses
models' proficiency in hierarchical taxonomic classification, their capability
to detect and abstain from out-of-distribution (OOD) samples representing novel
species, and their ability to generate explanations aligned with expert
taxonomic knowledge. Notably, top-performing models achieve over 90\% F1 at the
Order level on known species, but drop below 2\% at the Species level,
highlighting the sharp difficulty gradient from coarse to fine taxonomic
prediction (Order $\rightarrow$ Family $\rightarrow$ Genus $\rightarrow$
Species). TerraIncognita will be updated regularly, and by committing to
quarterly dataset expansions (of both known and novel species), will provide an
evolving platform for longitudinal benchmarking of frontier AI methods. All
TerraIncognita data, results, and future updates are available
\href{https://baskargroup.github.io/TerraIncognita/}{here}.

</details>


### [10] [Impact of Tuning Parameters in Deep Convolutional Neural Network Using a Crack Image Dataset](https://arxiv.org/abs/2506.03184)
*Mahe Zabin,Ho-Jin Choi,Md. Monirul Islam,Jia Uddin*

Main category: cs.CV

TL;DR: 研究了深度卷积神经网络（DCNN）中不同调参对性能的影响，发现使用maxpooling、adam优化器和tanh激活函数时性能最佳。


<details>
  <summary>Details</summary>
Motivation: 探讨DCNN中调参对分类性能的影响，以优化模型表现。

Method: 使用包含2个卷积层、2个池化层、1个dropout层和1个密集层的DCNN，通过实验评估池化、激活函数和优化器的调参效果。

Result: 实验结果表明，maxpooling、adam优化器和tanh激活函数的组合性能最佳。

Conclusion: 调参对DCNN性能有显著影响，maxpooling、adam和tanh是优化选择。

Abstract: The performance of a classifier depends on the tuning of its parame ters. In
this paper, we have experimented the impact of various tuning parameters on the
performance of a deep convolutional neural network (DCNN). In the ex perimental
evaluation, we have considered a DCNN classifier that consists of 2
convolutional layers (CL), 2 pooling layers (PL), 1 dropout, and a dense layer.
To observe the impact of pooling, activation function, and optimizer tuning pa
rameters, we utilized a crack image dataset having two classes: negative and
pos itive. The experimental results demonstrate that with the maxpooling, the
DCNN demonstrates its better performance for adam optimizer and tanh activation
func tion.

</details>


### [11] [Continual Learning in Vision-Language Models via Aligned Model Merging](https://arxiv.org/abs/2506.03189)
*Ghada Sokar,Gintare Karolina Dziugaite,Anurag Arnab,Ahmet Iscen,Pablo Samuel Castro,Cordelia Schmid*

Main category: cs.CV

TL;DR: 提出了一种基于模型合并的持续学习方法，通过合并新任务参数与旧任务参数，平衡稳定性和可塑性，减少遗忘并提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法通过顺序微调倾向于可塑性而非稳定性，导致对近期任务的偏见和遗忘问题。

Method: 提出模型合并方法，通过合并新旧任务参数并学习对齐权重，避免干扰。

Result: 在大型视觉语言模型上验证，有效减少遗忘，增强任务顺序和相似性的鲁棒性，并提高泛化能力。

Conclusion: 模型合并方法在持续学习中实现了稳定性和可塑性的更好平衡，具有实际应用潜力。

Abstract: Continual learning is conventionally tackled through sequential fine-tuning,
a process that, while enabling adaptation, inherently favors plasticity over
the stability needed to retain prior knowledge. While existing approaches
attempt to mitigate catastrophic forgetting, a bias towards recent tasks
persists as they build upon this sequential nature. In this work we present a
new perspective based on model merging to maintain stability while still
retaining plasticity. Rather than just sequentially updating the model weights,
we propose merging newly trained task parameters with previously learned ones,
promoting a better balance. To maximize the effectiveness of the merging
process, we propose a simple mechanism that promotes learning aligned weights
with previous ones, thereby avoiding interference when merging. We evaluate
this approach on large Vision-Language Models (VLMs), and demonstrate its
effectiveness in reducing forgetting, increasing robustness to various task
orders and similarities, and improving generalization.

</details>


### [12] [MINT: Memory-Infused Prompt Tuning at Test-time for CLIP](https://arxiv.org/abs/2506.03190)
*Jiaming Yi,Ruirui Pan,Jishen Yang,Xiulong Yang*

Main category: cs.CV

TL;DR: 提出了一种名为MINT的新框架，通过记忆提示库动态调整视觉语言预训练模型，以提升其在测试时数据分布变化下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有测试时适应方法未能充分利用模型内部知识，特别是在动态适应复杂和分层的视觉语义信息方面。

Method: MINT引入记忆提示库（MPB），存储可学习的键值提示对，通过分层视觉特征检索相关提示对，动态组装关联提示并注入图像编码器。

Result: MINT实现了无需源数据或重新训练的快速、精确视觉语言模型适应。

Conclusion: MINT通过记忆提示库有效提升了模型在测试时的适应能力。

Abstract: Improving the generalization ability of Vision-Language Pre-trained Models
(VLMs) under test-time data distribution shifts remains a critical challenge.
The existing Test-Time Adaptation (TTA) methods fall short in fully leveraging
the model's internal knowledge, particularly in dynamically adapting to complex
and hierarchical visual semantic information. In this paper, we propose
Memory-Infused Prompt Tuning (MINT), a novel framework to address this issue.
Inspired by human associative memory theory, MINT introduces a Memory Prompt
Bank (MPB), which stores learnable key-value prompt pairs that work as a memory
of previously seen samples. During the test time, relevant prompt pairs in the
MPB are retrieved by the hierarchical visual features of test images to
dynamically assemble Associative Prompts. The associative prompts are then
injected into the image encoder for fine-grained, customized visual contextual
guidance. MINT also utilizes learnable text prompts. MINT thus enables rapid,
precise VLM adaptation at test time by leveraging this MPB-acquired memory,
without source data or retraining. The code is available at
https://github.com/Jamieyi2004/MINT.

</details>


### [13] [Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward](https://arxiv.org/abs/2506.03191)
*Muhammad Islam,Tao Huang,Euijoon Ahn,Usman Naseem*

Main category: cs.CV

TL;DR: 本文综述了多模态生成人工智能（GenAI）和自回归大语言模型（LLMs）在人体运动理解与生成中的应用，探讨了新兴方法、架构及其在提升运动合成逼真度和多样性方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索如何通过文本描述指导生成复杂、类人的运动序列，并分析不同生成方法的优缺点。

Method: 研究分析了自回归模型、扩散模型、生成对抗网络（GANs）、变分自编码器（VAEs）和基于Transformer的模型，重点关注文本条件运动生成的最新进展。

Result: LLMs的引入增强了语义对齐，提高了运动输出的连贯性和上下文相关性。

Conclusion: 文本到运动的GenAI和LLM架构在医疗、人形机器人、游戏、动画和辅助技术等领域具有变革潜力，但仍面临生成高效逼真运动的挑战。

Abstract: This paper presents an in-depth survey on the use of multimodal Generative
Artificial Intelligence (GenAI) and autoregressive Large Language Models (LLMs)
for human motion understanding and generation, offering insights into emerging
methods, architectures, and their potential to advance realistic and versatile
motion synthesis. Focusing exclusively on text and motion modalities, this
research investigates how textual descriptions can guide the generation of
complex, human-like motion sequences. The paper explores various generative
approaches, including autoregressive models, diffusion models, Generative
Adversarial Networks (GANs), Variational Autoencoders (VAEs), and
transformer-based models, by analyzing their strengths and limitations in terms
of motion quality, computational efficiency, and adaptability. It highlights
recent advances in text-conditioned motion generation, where textual inputs are
used to control and refine motion outputs with greater precision. The
integration of LLMs further enhances these models by enabling semantic
alignment between instructions and motion, improving coherence and contextual
relevance. This systematic survey underscores the transformative potential of
text-to-motion GenAI and LLM architectures in applications such as healthcare,
humanoids, gaming, animation, and assistive technologies, while addressing
ongoing challenges in generating efficient and realistic human motion.

</details>


### [14] [Human Fall Detection using Transfer Learning-based 3D CNN](https://arxiv.org/abs/2506.03193)
*Ekram Alam,Abu Sufian,Paramartha Dutta,Marco Leo*

Main category: cs.CV

TL;DR: 本文提出了一种基于预训练3D CNN的视觉跌倒检测系统，利用时空特征分类跌倒与日常活动，减少了训练时间。


<details>
  <summary>Details</summary>
Motivation: 老年人跌倒是一个严重的健康问题，随着老年人口增加，需要自动化的跌倒检测系统。

Method: 使用预训练的3D CNN提取时空特征，仅训练SVM分类器，采用分层五折交叉验证划分数据集。

Result: 在GMDCSA和CAUCAFall数据集上进行了实验，代码已开源。

Conclusion: 该方法有效减少了训练时间，并展示了在跌倒检测中的潜力。

Abstract: Unintentional or accidental falls are one of the significant health issues in
senior persons. The population of senior persons is increasing steadily. So,
there is a need for an automated fall detection monitoring system. This paper
introduces a vision-based fall detection system using a pre-trained 3D CNN.
Unlike 2D CNN, 3D CNN extracts not only spatial but also temporal features. The
proposed model leverages the original learned weights of a 3D CNN model
pre-trained on the Sports1M dataset to extract the spatio-temporal features.
Only the SVM classifier was trained, which saves the time required to train the
3D CNN. Stratified shuffle five split cross-validation has been used to split
the dataset into training and testing data. Extracted features from the
proposed 3D CNN model were fed to an SVM classifier to classify the activity as
fall or ADL. Two datasets, GMDCSA and CAUCAFall, were utilized to conduct the
experiment. The source code for this work can be accessed via the following
link: https://github.com/ekramalam/HFD_3DCNN.

</details>


### [15] [HueManity: Probing Fine-Grained Visual Perception in MLLMs](https://arxiv.org/abs/2506.03194)
*Rynaa Grover,Jayant Sravan Tamarapalli,Sahiti Yerramilli,Nilay Pande*

Main category: cs.CV

TL;DR: HueManity是一个评估多模态大语言模型（MLLMs）视觉感知能力的基准测试，结果显示MLLMs在精确模式识别任务上表现显著低于人类和传统计算机视觉模型。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在高层次视觉推理上表现优异，但在细微感知任务上表现有限，需要评估和改进其视觉能力。

Method: 使用包含83,850张Ishihara风格点阵图像的HueManity数据集，测试MLLMs对嵌入字符的精确识别能力。

Result: MLLMs在数字“简单”任务上最高准确率为33.6%，字母数字“困难”任务上仅3%，远低于人类（100%和95.6%）和ResNet50模型（96.5%和94.5%）。

Conclusion: 当前MLLMs在视觉感知能力上存在显著不足，需改进架构和训练范式。HueManity数据集和代码已开源以促进研究。

Abstract: Multimodal Large Language Models (MLLMs) excel at high-level visual
reasoning, but their performance on nuanced perceptual tasks remains
surprisingly limited. We present HueManity, a benchmark designed to assess
visual perception in MLLMs. The dataset comprises 83,850 images featuring
two-character alphanumeric strings embedded in Ishihara test style dot
patterns, challenging models on precise pattern recognition. Our evaluation of
nine state-of-the-art MLLMs on HueManity demonstrates a significant performance
deficit compared to human and traditional computer vision baselines. The
best-performing MLLM achieved a 33.6% accuracy on the numeric `easy' task and a
striking 3% on the alphanumeric `hard' task. In contrast, human participants
achieved near-perfect scores (100% and 95.6%), and a fine-tuned ResNet50 model
reached accuracies of 96.5% and 94.5%. These results highlight a critical gap
in the visual capabilities of current MLLMs. Our analysis further explores
potential architectural and training-paradigm factors contributing to this
perceptual gap in MLLMs. We open-source HueManity dataset and code to foster
further research in improving perceptual robustness of MLLMs.

</details>


### [16] [Unlabeled Data Improves Fine-Grained Image Zero-shot Classification with Multimodal LLMs](https://arxiv.org/abs/2506.03195)
*Yunqi Hong,Sohyun An,Andrew Bai,Neil Y. C. Lin,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: AutoSEP是一种自监督提示学习框架，旨在提升多模态大语言模型（MLLMs）在细粒度图像分类中的性能，无需监督数据。


<details>
  <summary>Details</summary>
Motivation: 细粒度图像分类需要关注细微视觉差异，而现有MLLMs缺乏明确指导，容易忽略这些细节。

Method: 提出AutoSEP框架，通过无标签数据迭代优化描述提示，引导MLLMs识别关键判别特征。

Result: 在多个细粒度分类数据集上表现优异，平均比零样本分类提升13%，比最佳基线提升5%。

Conclusion: AutoSEP有效提升了MLLMs在细粒度分类中的性能，且无需训练或微调。

Abstract: Despite Multimodal Large Language Models (MLLMs) showing promising results on
general zero-shot image classification tasks, fine-grained image classification
remains challenging. It demands precise attention to subtle visual details to
distinguish between visually similar subcategories--details that MLLMs may
easily overlook without explicit guidance. To address this, we introduce
AutoSEP, an iterative self-supervised prompt learning framework designed to
enhance MLLM fine-grained classification capabilities in a fully unsupervised
manner. Our core idea is to leverage unlabeled data to learn a description
prompt that guides MLLMs in identifying crucial discriminative features within
an image, and boosts classification accuracy. We developed an automatic
self-enhancing prompt learning framework called AutoSEP to iteratively improve
the description prompt using unlabeled data, based on instance-level
classification scoring function. AutoSEP only requires black-box access to
MLLMs, eliminating the need for any training or fine-tuning. We evaluate our
approach on multiple fine-grained classification datasets. It consistently
outperforms other unsupervised baselines, demonstrating the effectiveness of
our self-supervised optimization framework. Notably, AutoSEP on average
improves 13 percent over standard zero-shot classification and 5 percent over
the best-performing baselines. Code is available at:
https://github.com/yq-hong/AutoSEP

</details>


### [17] [Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing](https://arxiv.org/abs/2506.03197)
*Baode Wang,Biao Wu,Weizhen Li,Meng Fang,Yanjie Liang,Zuming Huang,Haozhe Wang,Jun Huang,Ling Chen,Wei Chu,Yuan Qi*

Main category: cs.CV

TL;DR: 论文提出了一种名为layoutRL的端到端强化学习框架，通过优化复合奖励（包括编辑距离、段落计数准确性和阅读顺序保持）来训练模型，使其对文档布局具有显式感知。基于新发布的数据集Infinity-Doc-55K，论文实现了Infinity-Parser，并在多项任务上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统多阶段文档解析流程存在错误传播和适应性问题，亟需一种更高效、适应性强的解决方案。

Method: 采用强化学习框架layoutRL，结合复合奖励机制，利用新数据集Infinity-Doc-55K训练模型Infinity-Parser。

Result: Infinity-Parser在OCR、表格和公式提取、阅读顺序检测等任务中，准确性和结构保真度均达到最新水平。

Conclusion: 论文提出的方法显著提升了文档解析性能，并公开代码和数据集以推动文档理解领域的进步。

Abstract: Automated parsing of scanned documents into richly structured,
machine-readable formats remains a critical bottleneck in Document AI, as
traditional multi-stage pipelines suffer from error propagation and limited
adaptability to diverse layouts. We introduce layoutRL, an end-to-end
reinforcement learning framework that trains models to be explicitly
layout-aware by optimizing a composite reward of normalized edit distance,
paragraph count accuracy, and reading order preservation. Leveraging our newly
released dataset, Infinity-Doc-55K, which combines 55K high-fidelity synthetic
scanned document parsing data with expert-filtered real-world documents, we
instantiate layoutRL in a vision-language-model-based parser called
Infinity-Parser. Evaluated on English and Chinese benchmarks for OCR, table and
formula extraction, and reading order detection, Infinity-Parser achieves new
state-of-the-art performance in both accuracy and structural fidelity,
outpacing specialist pipelines and general-purpose vision-language models. We
will publicly release our code and dataset to accelerate progress in robust
document understanding.

</details>


### [18] [FLEX: A Large-Scale Multi-Modal Multi-Action Dataset for Fitness Action Quality Assessment](https://arxiv.org/abs/2506.03198)
*Hao Yin,Lijun Gu,Paritosh Parmar,Lin Xu,Tianxiao Guo,Weiwei Fu,Yang Zhang,Tianyou Zheng*

Main category: cs.CV

TL;DR: 论文提出了FLEX数据集，首个多模态、多动作的大规模数据集，结合表面肌电信号（sEMG）用于动作质量评估（AQA），旨在解决现有AQA方法和数据集在健身动作评估中的不足。


<details>
  <summary>Details</summary>
Motivation: 随着健身趋势的兴起，健身动作的风险不容忽视。现有AQA方法和数据集局限于单视角竞技体育场景和RGB模态，缺乏对健身动作的专业评估和指导。

Method: 构建FLEX数据集，包含20种负重动作，由38名不同技能水平的受试者完成，采集多视角RGB视频、3D姿态、sEMG和生理信息。结合知识图谱，通过惩罚函数形式标注动作关键步骤、错误类型和反馈。

Result: 实验表明，多模态数据、多视角数据和细粒度标注显著提升了模型性能。

Conclusion: FLEX推动了AQA方法和数据集向多模态和多动作场景发展，促进了人工智能在健身领域的应用。

Abstract: With the increasing awareness of health and the growing desire for aesthetic
physique, fitness has become a prevailing trend. However, the potential risks
associated with fitness training, especially with weight-loaded fitness
actions, cannot be overlooked. Action Quality Assessment (AQA), a technology
that quantifies the quality of human action and provides feedback, holds the
potential to assist fitness enthusiasts of varying skill levels in achieving
better training outcomes. Nevertheless, current AQA methodologies and datasets
are limited to single-view competitive sports scenarios and RGB modality and
lack professional assessment and guidance of fitness actions. To address this
gap, we propose the FLEX dataset, the first multi-modal, multi-action,
large-scale dataset that incorporates surface electromyography (sEMG) signals
into AQA. FLEX utilizes high-precision MoCap to collect 20 different
weight-loaded actions performed by 38 subjects across 3 different skill levels
for 10 repetitions each, containing 5 different views of the RGB video, 3D
pose, sEMG, and physiological information. Additionally, FLEX incorporates
knowledge graphs into AQA, constructing annotation rules in the form of penalty
functions that map weight-loaded actions, action keysteps, error types, and
feedback. We conducted various baseline methodologies on FLEX, demonstrating
that multimodal data, multiview data, and fine-grained annotations
significantly enhance model performance. FLEX not only advances AQA
methodologies and datasets towards multi-modal and multi-action scenarios but
also fosters the integration of artificial intelligence within the fitness
domain. Dataset and code are available at
https://haoyin116.github.io/FLEX_Dataset.

</details>


### [19] [Channel-adaptive Cross-modal Generative Semantic Communication for Point Cloud Transmission](https://arxiv.org/abs/2506.03211)
*Wanting Yang,Zehui Xiong,Qianqian Yang,Ping Zhang,Merouane Debbah,Rahim Tafazolli*

Main category: cs.CV

TL;DR: GenSeC-PC是一种新型的跨模态生成语义通信方法，用于高效点云传输，融合图像和点云语义，支持实时通信和完全模拟传输。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶和扩展现实的快速发展，高效点云传输需求日益增长。

Method: 采用跨模态语义编码器融合图像和点云，基于PointDif的解码器，并设计通道自适应联合语义-信道编码架构。

Result: 相比PointDif，GenSeC-PC具有更高压缩效率和重建性能，支持实时通信和鲁棒传输。

Conclusion: GenSeC-PC通过生成先验和跨模态设计，在低信噪比和带宽限制下表现优异，为点云传输提供了高效解决方案。

Abstract: With the rapid development of autonomous driving and extended reality,
efficient transmission of point clouds (PCs) has become increasingly important.
In this context, we propose a novel channel-adaptive cross-modal generative
semantic communication (SemCom) for PC transmission, called GenSeC-PC.
GenSeC-PC employs a semantic encoder that fuses images and point clouds, where
images serve as non-transmitted side information. Meanwhile, the decoder is
built upon the backbone of PointDif. Such a cross-modal design not only ensures
high compression efficiency but also delivers superior reconstruction
performance compared to PointDif. Moreover, to ensure robust transmission and
reduce system complexity, we design a streamlined and asymmetric
channel-adaptive joint semantic-channel coding architecture, where only the
encoder needs the feedback of average signal-to-noise ratio (SNR) and available
bandwidth. In addition, rectified denoising diffusion implicit models is
employed to accelerate the decoding process to the millisecond level, enabling
real-time PC communication. Unlike existing methods, GenSeC-PC leverages
generative priors to ensure reliable reconstruction even from noisy or
incomplete source PCs. More importantly, it supports fully analog transmission,
improving compression efficiency by eliminating the need for error-free side
information transmission common in prior SemCom approaches. Simulation results
confirm the effectiveness of cross-modal semantic extraction and dual-metric
guided fine-tuning, highlighting the framework's robustness across diverse
conditions, including low SNR, bandwidth limitations, varying numbers of 2D
images, and previously unseen objects.

</details>


### [20] [ConMamba: Contrastive Vision Mamba for Plant Disease Detection](https://arxiv.org/abs/2506.03213)
*Abdullah Al Mamun,Miaohua Zhang,David Ahmedt-Aristizabal,Zeeshan Hayder,Mohammad Awrangjeb*

Main category: cs.CV

TL;DR: ConMamba是一种新型自监督学习框架，专为植物病害检测设计，通过双向状态空间模型和动态对比损失优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖大量标注数据且计算成本高，自监督学习虽能利用未标注数据，但现有方法在长距离依赖和特征对齐上表现不佳。

Method: 提出ConMamba框架，结合Vision Mamba Encoder（双向状态空间模型）和动态权重调整的双层对比损失。

Result: 在三个基准数据集上显著优于现有方法。

Conclusion: ConMamba为植物病害检测提供了高效且鲁棒的解决方案。

Abstract: Plant Disease Detection (PDD) is a key aspect of precision agriculture.
However, existing deep learning methods often rely on extensively annotated
datasets, which are time-consuming and costly to generate. Self-supervised
Learning (SSL) offers a promising alternative by exploiting the abundance of
unlabeled data. However, most existing SSL approaches suffer from high
computational costs due to convolutional neural networks or transformer-based
architectures. Additionally, they struggle to capture long-range dependencies
in visual representation and rely on static loss functions that fail to align
local and global features effectively. To address these challenges, we propose
ConMamba, a novel SSL framework specially designed for PDD. ConMamba integrates
the Vision Mamba Encoder (VME), which employs a bidirectional State Space Model
(SSM) to capture long-range dependencies efficiently. Furthermore, we introduce
a dual-level contrastive loss with dynamic weight adjustment to optimize
local-global feature alignment. Experimental results on three benchmark
datasets demonstrate that ConMamba significantly outperforms state-of-the-art
methods across multiple evaluation metrics. This provides an efficient and
robust solution for PDD.

</details>


### [21] [OpenCarbon: A Contrastive Learning-based Cross-Modality Neural Approach for High-Resolution Carbon Emission Prediction Using Open Data](https://arxiv.org/abs/2506.03224)
*Jinwei Zeng,Yu Liu,Guozhen Zhang,Jingtao Ding,Yuming Lin,Jian Yuan,Yong Li*

Main category: cs.CV

TL;DR: OpenCarbon模型结合卫星图像和POI数据，通过跨模态信息提取和邻域聚合模块，高效预测高分辨率城市碳排放，性能提升26.6%。


<details>
  <summary>Details</summary>
Motivation: 传统碳排放估算方法数据收集成本高，开放数据和先进学习技术为解决这一问题提供了可能。

Method: 结合卫星图像和POI数据，设计跨模态信息提取与融合模块及邻域聚合模块。

Result: 模型性能显著提升26.6%，并能捕捉城市功能与碳排放的内在关系。

Conclusion: OpenCarbon为高效碳排放治理和针对性减排规划提供了潜力。

Abstract: Accurately estimating high-resolution carbon emissions is crucial for
effective emission governance and mitigation planning. While conventional
methods for precise carbon accounting are hindered by substantial data
collection efforts, the rise of open data and advanced learning techniques
offers a promising solution. Once an open data-based prediction model is
developed and trained, it can easily infer emissions for new areas based on
available open data. To address this, we incorporate two modalities of open
data, satellite images and point-of-interest (POI) data, to predict
high-resolution urban carbon emissions, with satellite images providing
macroscopic and static and POI data offering fine-grained and relatively
dynamic functionality information. However, estimating high-resolution carbon
emissions presents two significant challenges: the intertwined and implicit
effects of various functionalities on carbon emissions, and the complex spatial
contiguity correlations that give rise to the agglomeration effect. Our model,
OpenCarbon, features two major designs that target the challenges: a
cross-modality information extraction and fusion module to extract
complementary functionality information from two modules and model their
interactions, and a neighborhood-informed aggregation module to capture the
spatial contiguity correlations. Extensive experiments demonstrate our model's
superiority, with a significant performance gain of 26.6\% on R2. Further
generalizability tests and case studies also show OpenCarbon's capacity to
capture the intrinsic relation between urban functionalities and carbon
emissions, validating its potential to empower efficient carbon governance and
targeted carbon mitigation planning. Codes and data are available:
https://github.com/JinweiZzz/OpenCarbon.

</details>


### [22] [Pre-trained Vision-Language Models Assisted Noisy Partial Label Learning](https://arxiv.org/abs/2506.03229)
*Qian-Wei Wang,Yuqiu Xie,Letian Zhang,Zimo Liu,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 本文提出了一种协作一致性正则化（Co-Reg）方法，用于从预训练视觉语言模型（VLMs）生成的噪声部分标签中学习，解决了实例依赖性噪声问题。


<details>
  <summary>Details</summary>
Motivation: 预训练VLMs（如CLIP、LLaVa和GPT-4V）可以替代耗时的手动标注，但其生成的噪声标签具有实例依赖性，增加了学习难度。

Method: 通过同时训练两个神经网络，利用“协作伪标签”机制净化标签，并在标签空间和特征表示空间施加一致性正则化约束。

Result: 实验验证了该方法的有效性，并展示了弱监督学习技术与预训练模型知识蒸馏结合的广阔前景。

Conclusion: 提出的Co-Reg方法能有效处理预训练模型生成的噪声标签，为“免手动标注”训练提供了新思路。

Abstract: In the context of noisy partial label learning (NPLL), each training sample
is associated with a set of candidate labels annotated by multiple noisy
annotators. With the emergence of high-performance pre-trained vision-language
models (VLMs) such as CLIP, LLaVa and GPT-4V, the direction of using these
models to replace time-consuming manual annotation workflows and achieve
"manual-annotation-free" training for downstream tasks has become a highly
promising research avenue. This paper focuses on learning from noisy partial
labels annotated by pre-trained VLMs and proposes an innovative collaborative
consistency regularization (Co-Reg) method. Unlike the symmetric noise
primarily addressed in traditional noisy label learning, the noise generated by
pre-trained models is instance-dependent, embodying the underlying patterns of
the pre-trained models themselves, which significantly increases the learning
difficulty for the model. To address this, we simultaneously train two neural
networks that implement collaborative purification of training labels through a
"Co-Pseudo-Labeling" mechanism, while enforcing consistency regularization
constraints in both the label space and feature representation space. Our
method can also leverage few-shot manually annotated valid labels to further
enhance its performances. Comparative experiments with different denoising and
disambiguation algorithms, annotation manners, and pre-trained model
application schemes fully validate the effectiveness of the proposed method,
while revealing the broad prospects of integrating weakly-supervised learning
techniques into the knowledge distillation process of pre-trained models.

</details>


### [23] [Chipmunk: Training-Free Acceleration of Diffusion Transformers with Dynamic Column-Sparse Deltas](https://arxiv.org/abs/2506.03275)
*Austin Silveria,Soham V. Govande,Daniel Y. Fu*

Main category: cs.CV

TL;DR: 论文提出Chipmunk方法，通过动态稀疏性减少DiT推理时的计算冗余，显著提升速度且不影响生成质量。


<details>
  <summary>Details</summary>
Motivation: DiT在高质量图像和视频生成中表现出色，但推理计算成本高。研究发现DiT潜在噪声向量变化缓慢，表明计算存在冗余。

Method: Chipmunk利用动态稀疏性，仅重新计算变化最快的中间激活值，其余缓存。通过列稀疏核和计算重叠优化系统挑战。

Result: Chipmunk在HunyuanVideo和FLUX.1-dev上分别实现2.16x和1.41x加速，叠加全步缓存后速度提升更显著。

Conclusion: Chipmunk有效减少DiT推理冗余，显著提升速度且保持生成质量，适用于多种模型。

Abstract: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in
high-quality image and video generation but incur substantial compute cost at
inference. A common observation is that DiT latent noise vectors change slowly
across inference steps, which suggests that the DiT compute may be redundant
across steps. In this paper, we aim to speed up inference by reducing this
redundancy, without additional training. We first study how activations change
between steps in two state-of-the-art open-source DiTs. We find that just 5-25%
of the values in attention and MLP explain 70-90% of the change in activations
across steps. This finding motivates our approach, Chipmunk, which uses dynamic
sparsity at inference time to recompute only the fastest-changing intermediate
activations, while caching the rest. Dynamic sparsity introduces two systems
challenges: (1) sparse attention and MLP operations tend to underutilize GPU
tensor cores; and (2) computing dynamic sparsity patterns at runtime and
caching activations both introduce overhead. To address these challenges,
Chipmunk first uses a voxel-based reordering of input tokens to introduce
column-wise sparsity. We implement column-sparse kernels utilizing efficient
sparse gathers from global to shared GPU memory, achieving a 9.3x speedup at
93% sparsity compared to highly-optimized dense baselines. Second, Chipmunk
overlaps the computation of sparsity patterns and cache updates with other
parts of the computation (e.g., second layer of the MLP) to hide the extra
latency. Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on
FLUX.1-dev without compromising generation quality. Furthermore, we show that
Chipmunk can be stacked on top of full step caching, achieving a 3.72x speedup
on HunyuanVideo, a 2.67x speedup on WAN2.1, and a 2.25x speedup on FLUX.1-dev
with minimal quality impact.

</details>


### [24] [Learning Optical Flow Field via Neural Ordinary Differential Equation](https://arxiv.org/abs/2506.03290)
*Leyla Mirvakhabova,Hong Cai,Jisoo Jeong,Hanno Ackermann,Farhad Zanjani,Fatih Porikli*

Main category: cs.CV

TL;DR: 提出了一种基于神经ODE的新方法，用于动态调整光流估计中的计算步骤，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有光流估计方法使用固定步骤的迭代优化，可能因未针对输入数据调整而导致性能不佳。

Method: 采用神经ODE模型预测光流导数，动态调整计算步骤，通过特定架构和超参数实现与现有方法相同的更新。

Result: 在光流基准测试中，模型显著优于基线和其他现有模型，且仅需单次优化步骤。

Conclusion: 神经ODE方法在光流估计中具有动态调整的优势，性能显著提升。

Abstract: Recent works on optical flow estimation use neural networks to predict the
flow field that maps positions of one image to positions of the other. These
networks consist of a feature extractor, a correlation volume, and finally
several refinement steps. These refinement steps mimic the iterative
refinements performed by classical optimization algorithms and are usually
implemented by neural layers (e.g., GRU) which are recurrently executed for a
fixed and pre-determined number of steps. However, relying on a fixed number of
steps may result in suboptimal performance because it is not tailored to the
input data. In this paper, we introduce a novel approach for predicting the
derivative of the flow using a continuous model, namely neural ordinary
differential equations (ODE). One key advantage of this approach is its
capacity to model an equilibrium process, dynamically adjusting the number of
compute steps based on the data at hand. By following a particular neural
architecture, ODE solver, and associated hyperparameters, our proposed model
can replicate the exact same updates as recurrent cells used in existing works,
offering greater generality. Through extensive experimental analysis on optical
flow benchmarks, we demonstrate that our approach achieves an impressive
improvement over baseline and existing models, all while requiring only a
single refinement step.

</details>


### [25] [YOND: Practical Blind Raw Image Denoising Free from Camera-Specific Data Dependency](https://arxiv.org/abs/2506.03645)
*Hansen Feng,Lizhi Wang,Yiqi Huang,Tong Li,Lin Zhu,Hua Huang*

Main category: cs.CV

TL;DR: 论文提出了一种名为YOND的盲原始图像去噪方法，通过三个关键模块（CNE、EM-VST和SNR-Net）实现跨相机的鲁棒去噪。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的方法依赖相机特定数据，导致在未知相机数据上性能下降，需要一种更通用的去噪方法。

Method: YOND包含三个模块：CNE用于噪声参数估计，EM-VST消除相机依赖性，SNR-Net实现可控去噪。

Result: 实验表明YOND在未知相机数据上表现优异，具有实用性和灵活性。

Conclusion: YOND通过合成数据训练，实现了对未知相机数据的鲁棒去噪，解决了现有方法的局限性。

Abstract: The rapid advancement of photography has created a growing demand for a
practical blind raw image denoising method. Recently, learning-based methods
have become mainstream due to their excellent performance. However, most
existing learning-based methods suffer from camera-specific data dependency,
resulting in performance drops when applied to data from unknown cameras. To
address this challenge, we introduce a novel blind raw image denoising method
named YOND, which represents You Only Need a Denoiser. Trained solely on
synthetic data, YOND can generalize robustly to noisy raw images captured by
diverse unknown cameras. Specifically, we propose three key modules to
guarantee the practicality of YOND: coarse-to-fine noise estimation (CNE),
expectation-matched variance-stabilizing transform (EM-VST), and SNR-guided
denoiser (SNR-Net). Firstly, we propose CNE to identify the camera noise
characteristic, refining the estimated noise parameters based on the coarse
denoised image. Secondly, we propose EM-VST to eliminate camera-specific data
dependency, correcting the bias expectation of VST according to the noisy
image. Finally, we propose SNR-Net to offer controllable raw image denoising,
supporting adaptive adjustments and manual fine-tuning. Extensive experiments
on unknown cameras, along with flexible solutions for challenging cases,
demonstrate the superior practicality of our method. The source code will be
publicly available at the
\href{https://fenghansen.github.io/publication/YOND}{project homepage}.

</details>


### [26] [SportMamba: Adaptive Non-Linear Multi-Object Tracking with State Space Models for Team Sports](https://arxiv.org/abs/2506.03335)
*Dheeraj Khanna,Jerrin Bright,Yuhao Chen,John S. Zelek*

Main category: cs.CV

TL;DR: SportMamba是一种针对动态团队运动的多目标跟踪技术，通过引入mamba-attention机制和高度自适应空间关联度量，解决了快速运动和遮挡带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 团队运动中的多目标跟踪因快速运动、遮挡和非线性运动模式而极具挑战性，现有方法依赖检测和外观跟踪，效果不佳。

Method: 提出SportMamba，结合mamba-attention机制建模非线性运动，高度自适应空间关联度量减少遮挡导致的ID切换，并扩展检测搜索空间。

Result: 在SportsMOT数据集上表现优异，并在VIP-HTD冰球数据集上展示了零样本迁移能力。

Conclusion: SportMamba在复杂团队运动场景中实现了先进的跟踪性能，并具备良好的泛化能力。

Abstract: Multi-object tracking (MOT) in team sports is particularly challenging due to
the fast-paced motion and frequent occlusions resulting in motion blur and
identity switches, respectively. Predicting player positions in such scenarios
is particularly difficult due to the observed highly non-linear motion
patterns. Current methods are heavily reliant on object detection and
appearance-based tracking, which struggle to perform in complex team sports
scenarios, where appearance cues are ambiguous and motion patterns do not
necessarily follow a linear pattern. To address these challenges, we introduce
SportMamba, an adaptive hybrid MOT technique specifically designed for tracking
in dynamic team sports. The technical contribution of SportMamba is twofold.
First, we introduce a mamba-attention mechanism that models non-linear motion
by implicitly focusing on relevant embedding dependencies. Second, we propose a
height-adaptive spatial association metric to reduce ID switches caused by
partial occlusions by accounting for scale variations due to depth changes.
Additionally, we extend the detection search space with adaptive buffers to
improve associations in fast-motion scenarios. Our proposed technique,
SportMamba, demonstrates state-of-the-art performance on various metrics in the
SportsMOT dataset, which is characterized by complex motion and severe
occlusion. Furthermore, we demonstrate its generalization capability through
zero-shot transfer to VIP-HTD, an ice hockey dataset.

</details>


### [27] [Seeing the Arrow of Time in Large Multimodal Models](https://arxiv.org/abs/2506.03340)
*Zihui Xue,Mi Luo,Kristen Grauman*

Main category: cs.CV

TL;DR: 论文提出ArrowRL，一种基于强化学习的训练策略，通过反向奖励机制提升大型多模态模型（LMMs）对时间方向性的感知能力，并在新基准AoTBench上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代大型多模态模型（LMMs）在视频理解中难以感知和利用时间方向性（Arrow of Time, AoT），阻碍了更深层次的时间理解。

Method: 提出ArrowRL，一种基于强化学习的训练策略，通过反向奖励机制鼓励模型在正向和反向视频帧中产生不同的解释。

Result: ArrowRL显著提升了时间感知能力，在AoTBench上表现优异，并在标准视频问答（VQA）基准上实现了显著性能提升（峰值准确率分别提高20%和10%）。

Conclusion: ArrowRL的有效性验证了在LMMs中专门理解时间方向性的重要性。

Abstract: The Arrow of Time (AoT)-time's irreversible flow shaping physical events-is
fundamental to video comprehension, yet remains a significant challenge for
modern large multimodal models (LMMs). Current LMMs struggle to perceive and
utilize temporal directionality in video when responding to language queries,
obstructing deeper temporal understanding. We tackle this deficiency by first
providing a critical analysis of existing benchmarks and models. We then
introduce ArrowRL, a reinforcement learning (RL)-based training strategy with
an innovative reverse reward that instills AoT awareness by encouraging
divergent video interpretations between forward and reversed visual frames. For
rigorous evaluation, we additionally develop AoTBench, a new multi-faceted
benchmark probing temporally challenging questions. Experiments show ArrowRL
greatly advances temporal perception: it not only achieves substantial
improvements on our challenging AoTBench but also demonstrably boosts
performance on standard video question answering (VQA) benchmarks (with peak
accuracy gains reaching over 20% and 10% respectively). This validates
ArrowRL's effectiveness and highlights the critical need for dedicated AoT
understanding in LMMs.

</details>


### [28] [Semiconductor SEM Image Defect Classification Using Supervised and Semi-Supervised Learning with Vision Transformers](https://arxiv.org/abs/2506.03345)
*Chien-Fu,Huang,Katherine Sieg,Leonid Karlinksy,Nash Flores,Rebekah Sheraw,Xin Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于视觉变换器（ViT）的自动缺陷分类方法，用于半导体晶圆缺陷的SEM图像分析，实现了高精度分类（>90%）且仅需少量样本。


<details>
  <summary>Details</summary>
Motivation: 半导体工艺中缺陷控制对提高产量、降低成本和预防故障至关重要，但传统人工分类方法效率低且易受主观影响。

Method: 采用ViT神经网络，结合DinoV2的迁移学习和半监督学习，对7400多张SEM图像中的11种缺陷类型进行分类。

Result: 在每类缺陷仅需少于15张图像的情况下，分类准确率超过90%。

Conclusion: 该方法为快速、灵活的晶圆缺陷分类提供了平台无关的解决方案。

Abstract: Controlling defects in semiconductor processes is important for maintaining
yield, improving production cost, and preventing time-dependent critical
component failures. Electron beam-based imaging has been used as a tool to
survey wafers in the line and inspect for defects. However, manual
classification of images for these nano-scale defects is limited by time, labor
constraints, and human biases. In recent years, deep learning computer vision
algorithms have shown to be effective solutions for image-based inspection
applications in industry. This work proposes application of vision transformer
(ViT) neural networks for automatic defect classification (ADC) of scanning
electron microscope (SEM) images of wafer defects. We evaluated our proposed
methods on 300mm wafer semiconductor defect data from our fab in IBM Albany. We
studied 11 defect types from over 7400 total images and investigated the
potential of transfer learning of DinoV2 and semi-supervised learning for
improved classification accuracy and efficient computation. We were able to
achieve classification accuracies of over 90% with less than 15 images per
defect class. Our work demonstrates the potential to apply the proposed
framework for a platform agnostic in-house classification tool with faster
turnaround time and flexibility.

</details>


### [29] [Toward Reliable VLM: A Fine-Grained Benchmark and Framework for Exposure, Bias, and Inference in Korean Street Views](https://arxiv.org/abs/2506.03371)
*Xiaonan Wang,Bo Shao,Hansaem Kim*

Main category: cs.CV

TL;DR: 该论文介绍了KoreaGEO Bench，首个针对韩国街景的细粒度多模态地理定位基准数据集，填补了现有基准在细粒度、多模态和隐私意识评估方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前的地理定位基准存在粗粒度、语言偏见和缺乏多模态及隐私意识评估的问题，亟需改进。

Method: 构建了包含1,080张高分辨率图像的韩国街景数据集，涵盖四种城市集群和九种地点类型，并引入多上下文注释和两种韩语标题风格。采用三路径评估协议测试十种主流视觉语言模型。

Result: 结果显示，不同输入模态下模型的定位精度存在差异，并揭示了模型对核心城市的结构性预测偏见。

Conclusion: KoreaGEO Bench为细粒度、多模态地理定位提供了新基准，揭示了模型在隐私和地理定位中的行为特征。

Abstract: Recent advances in vision-language models (VLMs) have enabled accurate
image-based geolocation, raising serious concerns about location privacy risks
in everyday social media posts. However, current benchmarks remain
coarse-grained, linguistically biased, and lack multimodal and privacy-aware
evaluations. To address these gaps, we present KoreaGEO Bench, the first
fine-grained, multimodal geolocation benchmark for Korean street views. Our
dataset comprises 1,080 high-resolution images sampled across four urban
clusters and nine place types, enriched with multi-contextual annotations and
two styles of Korean captions simulating real-world privacy exposure. We
introduce a three-path evaluation protocol to assess ten mainstream VLMs under
varying input modalities and analyze their accuracy, spatial bias, and
reasoning behavior. Results reveal modality-driven shifts in localization
precision and highlight structural prediction biases toward core cities.

</details>


### [30] [A Foundation Model for Spatial Proteomics](https://arxiv.org/abs/2506.03373)
*Muhammad Shaban,Yuzhou Chang,Huaying Qiu,Yao Yu Yeo,Andrew H. Song,Guillaume Jaume,Yuchen Wang,Luca L. Weishaupt,Tong Ding,Anurag Vaidya,Abdallah Lamane,Daniel Shao,Mohammed Zidane,Yunhao Bai,Paige McCallum,Shuli Luo,Wenrui Wu,Yang Wang,Precious Cramer,Chi Ngai Chan,Pierre Stephan,Johanna Schaffenrath,Jia Le Lee,Hendrik A. Michel,Caiwei Tian,Cristina Almagro-Perez,Sophia J. Wagner,Sharifa Sahai,Ming Y. Lu,Richard J. Chen,Andrew Zhang,Mark Edward M. Gonzales,Ahmad Makky,Jia-Ying Joey Lee,Hao Cheng,Nourhan El Ahmar,Sayed Matar,Maximilian Haist,Darci Phillips,Yuqi Tan,Garry P. Nolan,W. Richard Burack,Jacob D. Estes,Jonathan T. C. Liu,Toni K Choueiri,Neeraj Agarwal,Marc Barry,Scott J. Rodig,Long Phi Le,Georg Gerber,Christian M. Schürch,Fabian J. Theis,Youn H Kim,Joe Yeong,Sabina Signoretti,Brooke E. Howitt,Lit-Hsin Loo,Qin Ma,Sizun Jiang,Faisal Mahmood*

Main category: cs.CV

TL;DR: KRONOS是一个为空间蛋白质组学设计的基础模型，通过自监督学习训练，支持多种下游任务，如细胞表型分析、区域分类和患者分层。


<details>
  <summary>Details</summary>
Motivation: 空间蛋白质组学在基础模型中的应用尚未充分探索，KRONOS旨在填补这一空白。

Method: KRONOS采用自监督学习，训练于4700万图像块，涵盖175种蛋白质标记、16种组织类型和8种荧光成像平台，并针对多通道成像进行了架构优化。

Result: 在11个独立队列中，KRONOS在细胞表型分析、治疗反应预测等任务中表现优异，且数据效率高。

Conclusion: KRONOS为空间蛋白质组学提供了灵活且可扩展的工具，支持跨机构比较和图像反向搜索。

Abstract: Foundation models have begun to transform image analysis by acting as
pretrained generalist backbones that can be adapted to many tasks even when
post-training data are limited, yet their impact on spatial proteomics, imaging
that maps proteins at single-cell resolution, remains limited. Here, we
introduce KRONOS, a foundation model built for spatial proteomics. KRONOS was
trained in a self-supervised manner on over 47 million image patches covering
175 protein markers, 16 tissue types, and 8 fluorescence-based imaging
platforms. We introduce key architectural adaptations to address the
high-dimensional, multi-channel, and heterogeneous nature of multiplex imaging.
We demonstrate that KRONOS learns biologically meaningful representations
across multiple scales, ranging from cellular and microenvironment to tissue
levels, enabling it to address diverse downstream tasks, including cell
phenotyping, region classification, and patient stratification. Evaluated
across 11 independent cohorts, KRONOS achieves state-of-the-art performance
across cell phenotyping, treatment response prediction, and retrieval tasks,
and is highly data-efficient. KRONOS also introduces the paradigm of
segmentation-free patch-level processing for efficient and scalable spatial
proteomics analysis, allowing cross-institutional comparisons, and as an image
reverse search engine for spatial patterns. Together, these results position
KRONOS as a flexible and scalable tool for spatial proteomics. The model is
publicly accessible at https://github.com/mahmoodlab/KRONOS.

</details>


### [31] [Cross-Modal Urban Sensing: Evaluating Sound-Vision Alignment Across Street-Level and Aerial Imagery](https://arxiv.org/abs/2506.03388)
*Pengyu Chen,Xiao Huang,Teng Fei,Sicheng Wang*

Main category: cs.CV

TL;DR: 研究探讨了城市声音与视觉场景的对应关系，通过多模态方法比较了不同视觉表示策略在捕捉声音语义上的效果。


<details>
  <summary>Details</summary>
Motivation: 环境声景蕴含丰富的生态和社会信息，但其在大规模地理分析中的潜力尚未充分挖掘。

Method: 采用多模态方法，结合地理标记的声音记录、街景和遥感图像，使用AST、CLIP、RemoteCLIP等模型提取嵌入和分类特征，评估跨模态相似性。

Result: 街景嵌入与声音对齐更强，而遥感分割在生态分类（BGA框架）中更有效。

Conclusion: 嵌入模型提供更好的语义对齐，分割方法则更易解释视觉结构与声学生态的联系，为多模态城市感知提供了新视角。

Abstract: Environmental soundscapes convey substantial ecological and social
information regarding urban environments; however, their potential remains
largely untapped in large-scale geographic analysis. In this study, we
investigate the extent to which urban sounds correspond with visual scenes by
comparing various visual representation strategies in capturing acoustic
semantics. We employ a multimodal approach that integrates geo-referenced sound
recordings with both street-level and remote sensing imagery across three major
global cities: London, New York, and Tokyo. Utilizing the AST model for audio,
along with CLIP and RemoteCLIP for imagery, as well as CLIPSeg and Seg-Earth OV
for semantic segmentation, we extract embeddings and class-level features to
evaluate cross-modal similarity. The results indicate that street view
embeddings demonstrate stronger alignment with environmental sounds compared to
segmentation outputs, whereas remote sensing segmentation is more effective in
interpreting ecological categories through a Biophony--Geophony--Anthrophony
(BGA) framework. These findings imply that embedding-based models offer
superior semantic alignment, while segmentation-based methods provide
interpretable links between visual structure and acoustic ecology. This work
advances the burgeoning field of multimodal urban sensing by offering novel
perspectives for incorporating sound into geospatial analysis.

</details>


### [32] [Temporal Vegetation Index-Based Unsupervised Crop Stress Detection via Eigenvector-Guided Contrastive Learning](https://arxiv.org/abs/2506.03394)
*Shafqaat Ahmad*

Main category: cs.CV

TL;DR: EigenCL是一种基于时间NDRE动态和生物特征的无监督对比学习框架，用于早期作物胁迫检测，无需标记数据，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 早期检测作物胁迫对精准农业至关重要，但传统方法依赖标记数据或仅能检测可见症状，限制了可扩展性。

Method: 利用Sentinel-2 NDRE图像构建时间序列，通过RBF相似性矩阵和特征分解提取主特征向量，用于对比学习嵌入。

Result: EigenCL在聚类指标（Silhouette: 0.748, DBI: 0.35）和早期检测（提前12天）上表现优异，分类准确率达95%。

Conclusion: EigenCL提供了一种无需标记、可扩展的早期胁迫检测方法，适用于数据稀缺的农业环境。

Abstract: Early detection of crop stress is vital for minimizing yield loss and
enabling timely intervention in precision agriculture. Traditional approaches
using NDRE often detect stress only after visible symptoms appear or require
labeled datasets, limiting scalability. This study introduces EigenCL, a novel
unsupervised contrastive learning framework guided by temporal NDRE dynamics
and biologically grounded eigen decomposition. Using over 10,000 Sentinel-2
NDRE image patches from drought-affected Iowa cornfields, we constructed
five-point NDRE time series per patch and derived an RBF similarity matrix. The
principal eigenvector explaining 76% of the variance and strongly correlated (r
= 0.95) with raw NDRE values was used to define stress-aware similarity for
contrastive embedding learning. Unlike existing methods that rely on visual
augmentations, EigenCL pulls embeddings together based on biologically similar
stress trajectories and pushes apart divergent ones. The learned embeddings
formed physiologically meaningful clusters, achieving superior clustering
metrics (Silhouette: 0.748, DBI: 0.35) and enabling 76% early stress detection
up to 12 days before conventional NDRE thresholds. Downstream classification
yielded 95% k-NN and 91% logistic regression accuracy. Validation on an
independent 2023 Nebraska dataset confirmed generalizability without
retraining. EigenCL offers a label-free, scalable approach for early stress
detection that aligns with underlying plant physiology and is suitable for
real-world deployment in data-scarce agricultural environments.

</details>


### [33] [ViT-Split: Unleashing the Power of Vision Foundation Models via Efficient Splitting Heads](https://arxiv.org/abs/2506.03433)
*Yifan Li,Xin Li,Tianqin Li,Wenbin He,Yu Kong,Liu Ren*

Main category: cs.CV

TL;DR: ViT-Split是一种新的视觉基础模型（VFM）适配方法，通过分离VFM的提取器和适配器组件，减少训练时间和参数调优，同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有VFM适配方法存在梯度反向传播过早和参数调优复杂的问题，且未能充分利用VFM的先验知识。

Method: 提出ViT-Split，将VFM分为提取器和适配器两部分，引入任务头和先验头，冻结VFM并减少调优参数。

Result: 在多个任务（如分割、检测等）上验证了ViT-Split的高效性和性能优势，训练时间减少4倍。

Conclusion: ViT-Split通过优化VFM适配机制，显著提升了效率和性能，适用于多种视觉任务。

Abstract: Vision foundation models (VFMs) have demonstrated remarkable performance
across a wide range of downstream tasks. While several VFM adapters have shown
promising results by leveraging the prior knowledge of VFMs, we identify two
inefficiencies in these approaches. First, the interaction between
convolutional neural network (CNN) and VFM backbone triggers early layer
gradient backpropagation. Second, existing methods require tuning all
components, adding complexity. Besides, these adapters alter VFM features,
underutilizing the prior knowledge. To tackle these challenges, we propose a
new approach called ViT-Split, based on a key observation: the layers of
several VFMs, like DINOv2, can be divided into two distinct components: an
extractor for learning low-level features and an adapter for learning
task-specific features. Leveraging this insight, we eliminate the CNN branch
and introduce two heads, task head and prior head, to the frozen VFM. The task
head is designed to learn task-specific features, mitigating the early gradient
propagation issue. The prior head is used to leverage the multi-scale prior
features from the frozen VFM, reducing tuning parameters and overfitting.
Extensive experiments on various tasks (e.g., segmentation, detection, depth
estimation, and visual question answering) validate the effectiveness and
efficiency of ViT-Split. Specifically, ViT-Split reduces training time up to
$4\times$ while achieving comparable or even better results on ADE20K, compared
to other VFM adapters.

</details>


### [34] [Geometric Visual Fusion Graph Neural Networks for Multi-Person Human-Object Interaction Recognition in Videos](https://arxiv.org/abs/2506.03440)
*Tanqiu Qiao,Ruochen Li,Frederick W. B. Li,Yoshiki Kubotani,Shigeo Morishima,Hubert P. H. Shum*

Main category: cs.CV

TL;DR: 论文提出了一种名为GeoVis-GNN的几何视觉融合图神经网络，通过双注意力特征融合和相互依赖的实体图学习，从实体特定表示逐步构建高级交互理解。同时，引入了MPHOI-120数据集以支持动态多人交互研究。


<details>
  <summary>Details</summary>
Motivation: 视频中的人-物交互（HOI）识别需要理解随时间演变的视觉模式和几何关系。视觉和几何特征各有优势，但如何在不损害其独特性的情况下有效融合这些多模态特征仍具挑战性。

Method: 提出GeoVis-GNN，采用双注意力特征融合和相互依赖的实体图学习，从实体特定表示逐步构建交互理解。并引入MPHOI-120数据集。

Result: 在多种HOI场景（如双人交互、单人活动、双手操作及复杂并发部分交互）中，方法表现出色，达到最先进性能。

Conclusion: GeoVis-GNN通过实体特定表示和多模态融合，有效提升了HOI识别性能，MPHOI-120数据集为复杂交互研究提供了支持。

Abstract: Human-Object Interaction (HOI) recognition in videos requires understanding
both visual patterns and geometric relationships as they evolve over time.
Visual and geometric features offer complementary strengths. Visual features
capture appearance context, while geometric features provide structural
patterns. Effectively fusing these multimodal features without compromising
their unique characteristics remains challenging. We observe that establishing
robust, entity-specific representations before modeling interactions helps
preserve the strengths of each modality. Therefore, we hypothesize that a
bottom-up approach is crucial for effective multimodal fusion. Following this
insight, we propose the Geometric Visual Fusion Graph Neural Network
(GeoVis-GNN), which uses dual-attention feature fusion combined with
interdependent entity graph learning. It progressively builds from
entity-specific representations toward high-level interaction understanding. To
advance HOI recognition to real-world scenarios, we introduce the Concurrent
Partial Interaction Dataset (MPHOI-120). It captures dynamic multi-person
interactions involving concurrent actions and partial engagement. This dataset
helps address challenges like complex human-object dynamics and mutual
occlusions. Extensive experiments demonstrate the effectiveness of our method
across various HOI scenarios. These scenarios include two-person interactions,
single-person activities, bimanual manipulations, and complex concurrent
partial interactions. Our method achieves state-of-the-art performance.

</details>


### [35] [RefEdit: A Benchmark and Method for Improving Instruction-based Image Editing Model on Referring Expressions](https://arxiv.org/abs/2506.03448)
*Bimsara Pathiraja,Maitreya Patel,Shivam Singh,Yezhou Yang,Chitta Baral*

Main category: cs.CV

TL;DR: 论文提出RefEdit模型，用于解决复杂场景中多实体编辑问题，通过合成数据训练，性能优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法在编辑复杂场景中的多实体时表现不佳，需量化并解决这一问题。

Method: 引入RefEdit-Bench基准，开发RefEdit模型，基于合成数据训练。

Result: RefEdit在少量数据训练下优于基线模型，并在多个基准测试中达到最优性能。

Conclusion: RefEdit在复杂场景编辑任务中表现优异，数据与模型已开源。

Abstract: Despite recent advances in inversion and instruction-based image editing,
existing approaches primarily excel at editing single, prominent objects but
significantly struggle when applied to complex scenes containing multiple
entities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous
real-world benchmark rooted in RefCOCO, where even baselines trained on
millions of samples perform poorly. To overcome this limitation, we introduce
RefEdit -- an instruction-based editing model trained on our scalable synthetic
data generation pipeline. Our RefEdit, trained on only 20,000 editing triplets,
outperforms the Flux/SD3 model-based baselines trained on millions of data.
Extensive evaluations across various benchmarks demonstrate that our model not
only excels in referring expression tasks but also enhances performance on
traditional benchmarks, achieving state-of-the-art results comparable to
closed-source methods. We release data \& checkpoint for reproducibility.

</details>


### [36] [The effects of using created synthetic images in computer vision training](https://arxiv.org/abs/2506.03449)
*John W. Smutny*

Main category: cs.CV

TL;DR: 论文探讨了使用渲染引擎（如Unreal Engine 4）生成合成图像以补充深度学习计算机视觉模型的数据集，验证了合成图像的有效性，并提出了衡量合成图像质量的方法。


<details>
  <summary>Details</summary>
Motivation: 解决真实图像数据稀缺、昂贵或存在潜在安全风险的问题，提供一种低成本、可扩展的替代方案。

Method: 通过UE生成合成图像，测试其在两种不同规模的CV模型（VGG16和MobileNetV3-small）和两种分类任务（猫狗分类和焊接缺陷检测）中的表现，并使用预训练模型作为审计工具衡量合成图像质量。

Result: 在训练集中添加超过60%的合成图像可将测试与训练准确率差距缩小至1-2%，且仅需使用10%的真实图像即可达到与传统方法（50-70%真实图像）相当的效果。

Conclusion: 合成图像为数据稀缺项目提供了有效的解决方案，并展示了其在实际应用中的潜力。

Abstract: This paper investigates how rendering engines, like Unreal Engine 4 (UE), can
be used to create synthetic images to supplement datasets for deep computer
vision (CV) models in image abundant and image limited use cases. Using
rendered synthetic images from UE can provide developers and businesses with a
method of accessing nearly unlimited, reproducible, agile, and cheap training
sets for their customers and applications without the threat of poisoned images
from the internet or the cost of collecting them. The validity of these
generated images are examined by testing the change in model test accuracy in
two different sized CV models across two binary classification cases (Cat vs
Dog and Weld Defect Detection). In addition, this paper provides an
implementation of how to measure the quality of synthetic images by using
pre-trained CV models as auditors. Results imply that for large (VGG16) and
small (MobileNetV3-small) parameter deep CV models, adding >60% additional
synthetic images to a real image dataset during model training can narrow the
test-training accuracy gap to ~1-2% without a conclusive effect on test
accuracy compared to using real world images alone. Likewise, adding <10%
additional real training images to synthetic only training sets decreased the
classification error rate in half, then decreasing further when adding more
real training images. For these cases tested, using synthetic images from
rendering engines allow researchers to only use 10% of their real images during
training, compared to the traditional 50-70%. This research serves as an
example of how to create synthetic images, guidelines on how to use the images,
potential restrictions and possible performance improvements for data-scarce
projects.

</details>


### [37] [RoNFA: Robust Neural Field-based Approach for Few-Shot Image Classification with Noisy Labels](https://arxiv.org/abs/2506.03461)
*Nan Xiang,Lifeng Xing,Dequan Jin*

Main category: cs.CV

TL;DR: 提出了一种鲁棒的神经场方法（RoNFA），用于带有噪声标签的小样本图像分类，显著提升了模型对标签噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 小样本学习中标签样本稀缺，标签错误会显著降低分类准确性，因此提升模型在标签错误情况下的鲁棒性至关重要。

Method: RoNFA由两个神经场组成，分别用于特征和类别表示，通过软聚类生成类别代表性神经元，并动态调整感受野范围以确保预测准确性。

Result: 在真实世界小样本数据集上，RoNFA显著优于现有方法，甚至在噪声标签下的准确性超过干净标签训练的先进方法。

Conclusion: RoNFA具备出色的小样本学习能力和对标签噪声的强鲁棒性，为噪声标签下的学习提供了有效解决方案。

Abstract: In few-shot learning (FSL), the labeled samples are scarce. Thus, label
errors can significantly reduce classification accuracy. Since label errors are
inevitable in realistic learning tasks, improving the robustness of the model
in the presence of label errors is critical. This paper proposes a new robust
neural field-based image approach (RoNFA) for few-shot image classification
with noisy labels. RoNFA consists of two neural fields for feature and category
representation. They correspond to the feature space and category set. Each
neuron in the field for category representation (FCR) has a receptive field
(RF) on the field for feature representation (FFR) centered at the
representative neuron for its category generated by soft clustering. In the
prediction stage, the range of these receptive fields adapts according to the
neuronal activation in FCR to ensure prediction accuracy. These learning
strategies provide the proposed model with excellent few-shot learning
capability and strong robustness against label noises. The experimental results
on real-world FSL datasets with three different types of label noise
demonstrate that the proposed method significantly outperforms state-of-the-art
FSL methods. Its accuracy obtained in the presence of noisy labels even
surpasses the results obtained by state-of-the-art FSL methods trained on clean
support sets, indicating its strong robustness against noisy labels.

</details>


### [38] [MamFusion: Multi-Mamba with Temporal Fusion for Partially Relevant Video Retrieval](https://arxiv.org/abs/2506.03473)
*Xinru Ying,Jiaqi Mo,Jingyang Lin,Canghong Jin,Fangfang Wang,Lina Wei*

Main category: cs.CV

TL;DR: 论文提出了一种名为MamFusion的多Mamba模块框架，用于解决部分相关视频检索（PRVR）任务中的长序列视频内容理解问题，通过时间融合技术提升检索效果。


<details>
  <summary>Details</summary>
Motivation: 解决PRVR任务中长序列视频内容的信息冗余问题，提升检索准确性。

Method: 利用Mamba模块的长时状态空间建模能力和线性可扩展性，设计了多Mamba模块的时间融合框架（MamFusion），并引入Temporal T-to-V Fusion和Temporal V-to-T Fusion明确建模文本查询与视频片段的时间关系。

Result: 在大规模数据集上的实验表明，MamFusion在检索效果上达到了最先进的性能。

Conclusion: MamFusion框架通过时间融合技术有效提升了PRVR任务的检索性能，具有显著的实际应用价值。

Abstract: Partially Relevant Video Retrieval (PRVR) is a challenging task in the domain
of multimedia retrieval. It is designed to identify and retrieve untrimmed
videos that are partially relevant to the provided query. In this work, we
investigate long-sequence video content understanding to address information
redundancy issues. Leveraging the outstanding long-term state space modeling
capability and linear scalability of the Mamba module, we introduce a
multi-Mamba module with temporal fusion framework (MamFusion) tailored for PRVR
task. This framework effectively captures the state-relatedness in long-term
video content and seamlessly integrates it into text-video relevance
understanding, thereby enhancing the retrieval process. Specifically, we
introduce Temporal T-to-V Fusion and Temporal V-to-T Fusion to explicitly model
temporal relationships between text queries and video moments, improving
contextual awareness and retrieval accuracy. Extensive experiments conducted on
large-scale datasets demonstrate that MamFusion achieves state-of-the-art
performance in retrieval effectiveness. Code is available at the link:
https://github.com/Vision-Multimodal-Lab-HZCU/MamFusion.

</details>


### [39] [Heterogeneous Skeleton-Based Action Representation Learning](https://arxiv.org/abs/2506.03481)
*Hongsong Wang,Xiaoyan Ma,Jidong Kuang,Jie Gui*

Main category: cs.CV

TL;DR: 论文提出了一种处理异构骨架数据的框架，包括异构骨架处理和统一表示学习两部分，通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于骨架数据的异构性（关节维度和拓扑结构不同），现有方法仅针对同构骨架设计，无法有效处理异构数据。

Method: 框架包括异构骨架处理（通过辅助网络将二维骨架转为三维，并构建统一骨架）和统一表示学习（使用共享主干网络处理异构骨架）。

Result: 在NTU-60、NTU-120和PKU-MMD II数据集上的实验证明了方法的有效性。

Conclusion: 该方法适用于不同人形结构的机器人动作识别。

Abstract: Skeleton-based human action recognition has received widespread attention in
recent years due to its diverse range of application scenarios. Due to the
different sources of human skeletons, skeleton data naturally exhibit
heterogeneity. The previous works, however, overlook the heterogeneity of human
skeletons and solely construct models tailored for homogeneous skeletons. This
work addresses the challenge of heterogeneous skeleton-based action
representation learning, specifically focusing on processing skeleton data that
varies in joint dimensions and topological structures. The proposed framework
comprises two primary components: heterogeneous skeleton processing and unified
representation learning. The former first converts two-dimensional skeleton
data into three-dimensional skeleton via an auxiliary network, and then
constructs a prompted unified skeleton using skeleton-specific prompts. We also
design an additional modality named semantic motion encoding to harness the
semantic information within skeletons. The latter module learns a unified
action representation using a shared backbone network that processes different
heterogeneous skeletons. Extensive experiments on the NTU-60, NTU-120, and
PKU-MMD II datasets demonstrate the effectiveness of our method in various
tasks of action understanding. Our approach can be applied to action
recognition in robots with different humanoid structures.

</details>


### [40] [CHIME: Conditional Hallucination and Integrated Multi-scale Enhancement for Time Series Diffusion Model](https://arxiv.org/abs/2506.03502)
*Yuxuan Chen,Haipeng Xie*

Main category: cs.CV

TL;DR: CHIME是一个用于时间序列扩散模型的条件幻觉和多尺度增强框架，解决了多尺度特征对齐和生成能力的问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在时间序列任务中面临多尺度特征对齐和跨实体、长时间尺度生成能力的挑战。

Method: 采用多尺度分解和自适应集成，结合特征幻觉模块，实现时间序列特征的对齐和转移。

Result: 在公开数据集上，CHIME实现了最先进的性能，并在少样本场景中表现出优秀的生成泛化能力。

Conclusion: CHIME通过多尺度增强和条件幻觉模块，显著提升了时间序列扩散模型的性能。

Abstract: The denoising diffusion probabilistic model has become a mainstream
generative model, achieving significant success in various computer vision
tasks. Recently, there has been initial exploration of applying diffusion
models to time series tasks. However, existing studies still face challenges in
multi-scale feature alignment and generative capabilities across different
entities and long-time scales. In this paper, we propose CHIME, a conditional
hallucination and integrated multi-scale enhancement framework for time series
diffusion models. By employing multi-scale decomposition and adaptive
integration, CHIME captures the decomposed features of time series, achieving
in-domain distribution alignment between generated and original samples. In
addition, we introduce a feature hallucination module in the conditional
denoising process, enabling the transfer of temporal features through the
training of category-independent transformation layers. Experimental results on
publicly available real-world datasets demonstrate that CHIME achieves
state-of-the-art performance and exhibits excellent generative generalization
capabilities in few-shot scenarios.

</details>


### [41] [EDCFlow: Exploring Temporally Dense Difference Maps for Event-based Optical Flow Estimation](https://arxiv.org/abs/2506.03512)
*Daikun Liu,Lei Cheng,Teng Wang,changyin Sun*

Main category: cs.CV

TL;DR: EDCFlow提出了一种轻量级事件光流网络，通过结合时间密集特征差异和成本体积，实现高分辨率光流估计。


<details>
  <summary>Details</summary>
Motivation: 现有基于事件的光流估计方法存在计算冗余和分辨率扩展受限的问题，需要更高效且可扩展的解决方案。

Method: 设计了一个基于注意力的多尺度时间特征差异层，高效捕捉高分辨率运动模式，并自适应融合高低分辨率运动特征。

Result: EDCFlow在较低复杂度下表现优于现有方法，具有更好的泛化能力。

Conclusion: EDCFlow可作为RAFT类方法的即插即用模块，提升光流细节估计效果。

Abstract: Recent learning-based methods for event-based optical flow estimation utilize
cost volumes for pixel matching but suffer from redundant computations and
limited scalability to higher resolutions for flow refinement. In this work, we
take advantage of the complementarity between temporally dense feature
differences of adjacent event frames and cost volume and present a lightweight
event-based optical flow network (EDCFlow) to achieve high-quality flow
estimation at a higher resolution. Specifically, an attention-based multi-scale
temporal feature difference layer is developed to capture diverse motion
patterns at high resolution in a computation-efficient manner. An adaptive
fusion of high-resolution difference motion features and low-resolution
correlation motion features is performed to enhance motion representation and
model generalization. Notably, EDCFlow can serve as a plug-and-play refinement
module for RAFT-like event-based methods to enhance flow details. Extensive
experiments demonstrate that EDCFlow achieves better performance with lower
complexity compared to existing methods, offering superior generalization.

</details>


### [42] [DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models](https://arxiv.org/abs/2506.03517)
*Ziyi Wu,Anil Kag,Ivan Skorokhodov,Willi Menapace,Ashkan Mirzaei,Igor Gilitschenski,Sergey Tulyakov,Aliaksandr Siarohin*

Main category: cs.CV

TL;DR: DenseDPO通过改进视频对生成方式、引入分段标注和利用自动标注技术，解决了传统DPO在视频生成中的运动偏见问题，显著提升了运动生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统DPO方法在视频生成中存在运动偏见和标注效率低的问题，DenseDPO旨在通过改进数据生成和标注方式解决这些问题。

Method: 1. 通过去噪生成对齐的视频对，减少运动偏见；2. 引入分段标注，提高标注密度和精度；3. 利用VLMs实现自动标注。

Result: DenseDPO仅需三分之一标注数据，即可显著提升运动生成质量，同时在文本对齐、视觉质量和时间一致性上与DPO相当。自动标注的性能接近人工标注。

Conclusion: DenseDPO通过改进数据生成和标注方式，有效解决了DPO的局限性，为视频生成提供了更高效和准确的优化方法。

Abstract: Direct Preference Optimization (DPO) has recently been applied as a
post-training technique for text-to-video diffusion models. To obtain training
data, annotators are asked to provide preferences between two videos generated
from independent noise. However, this approach prohibits fine-grained
comparisons, and we point out that it biases the annotators towards low-motion
clips as they often contain fewer visual artifacts. In this work, we introduce
DenseDPO, a method that addresses these shortcomings by making three
contributions. First, we create each video pair for DPO by denoising corrupted
copies of a ground truth video. This results in aligned pairs with similar
motion structures while differing in local details, effectively neutralizing
the motion bias. Second, we leverage the resulting temporal alignment to label
preferences on short segments rather than entire clips, yielding a denser and
more precise learning signal. With only one-third of the labeled data, DenseDPO
greatly improves motion generation over vanilla DPO, while matching it in text
alignment, visual quality, and temporal consistency. Finally, we show that
DenseDPO unlocks automatic preference annotation using off-the-shelf Vision
Language Models (VLMs): GPT accurately predicts segment-level preferences
similar to task-specifically fine-tuned video reward models, and DenseDPO
trained on these labels achieves performance close to using human labels.

</details>


### [43] [Target Semantics Clustering via Text Representations for Robust Universal Domain Adaptation](https://arxiv.org/abs/2506.03521)
*Weinan He,Zilei Wang,Yixin Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉语言模型的通用域适应方法（TASC），通过在语义明确的离散文本表示空间中搜索语义中心，简化并增强了域对齐算法。


<details>
  <summary>Details</summary>
Motivation: 解决通用域适应中因域偏移和未知类别偏移导致的复杂、不鲁棒的语义中心对齐问题。

Method: 提出TASC方法，分两阶段：1）通过贪婪搜索在文本表示空间中找到目标语义中心；2）固定搜索结果，通过梯度下降优化编码器。同时提出UniMS评分函数检测开放集样本。

Result: 在四个基准测试中，方法表现出高效性和鲁棒性，达到最先进性能。

Conclusion: 基于文本表示空间的语义中心搜索方法简化了域对齐，提升了通用域适应的性能。

Abstract: Universal Domain Adaptation (UniDA) focuses on transferring source domain
knowledge to the target domain under both domain shift and unknown category
shift. Its main challenge lies in identifying common class samples and aligning
them. Current methods typically obtain target domain semantics centers from an
unconstrained continuous image representation space. Due to domain shift and
the unknown number of clusters, these centers often result in complex and less
robust alignment algorithm. In this paper, based on vision-language models, we
search for semantic centers in a semantically meaningful and discrete text
representation space. The constrained space ensures almost no domain bias and
appropriate semantic granularity for these centers, enabling a simple and
robust adaptation algorithm. Specifically, we propose TArget Semantics
Clustering (TASC) via Text Representations, which leverages information
maximization as a unified objective and involves two stages. First, with the
frozen encoders, a greedy search-based framework is used to search for an
optimal set of text embeddings to represent target semantics. Second, with the
search results fixed, encoders are refined based on gradient descent,
simultaneously achieving robust domain alignment and private class clustering.
Additionally, we propose Universal Maximum Similarity (UniMS), a scoring
function tailored for detecting open-set samples in UniDA. Experimentally, we
evaluate the universality of UniDA algorithms under four category shift
scenarios. Extensive experiments on four benchmarks demonstrate the
effectiveness and robustness of our method, which has achieved state-of-the-art
performance.

</details>


### [44] [Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning](https://arxiv.org/abs/2506.03525)
*Daeun Lee,Jaehong Yoon,Jaemin Cho,Mohit Bansal*

Main category: cs.CV

TL;DR: Video-SKoT通过自动构建技能感知的CoT监督，提升领域自适应视频推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法难以适应不同视频内容中的领域特定技能（如事件检测、空间关系理解等）。

Method: 1. 构建基于技能的CoT标注；2. 引入技能特定专家学习框架。

Result: 在三个视频理解基准测试中，Video-SKoT表现优于基线方法。

Conclusion: Video-SKoT通过技能感知的CoT监督和专家学习，有效提升视频推理能力。

Abstract: Recent advances in Chain-of-Thought (CoT) reasoning have improved complex
video understanding, but existing methods often struggle to adapt to
domain-specific skills (e.g., event detection, spatial relation understanding,
emotion understanding) over various video content. To address this, we propose
Video-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs
and leverages skill-aware CoT supervisions for domain-adaptive video reasoning.
First, we construct skill-based CoT annotations: we extract domain-relevant
reasoning skills from training questions, cluster them into a shared skill
taxonomy, and create detailed multi-step CoT rationale tailored to each
video-question pair for training. Second, we introduce a skill-specific expert
learning framework. Each expert module specializes in a subset of reasoning
skills and is trained with lightweight adapters using the collected CoT
supervision. We demonstrate the effectiveness of the proposed approach on three
video understanding benchmarks, where Video-SKoT consistently outperforms
strong baselines. We also provide in-depth analyses on comparing different CoT
annotation pipelines and learned skills over multiple video domains.

</details>


### [45] [Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian Splatting](https://arxiv.org/abs/2506.03538)
*Chengqi Li,Zhihao Shi,Yangdi Lu,Wenbo He,Xiangyu Xu*

Main category: cs.CV

TL;DR: 提出了一种名为Asymmetric Dual 3DGS的新框架，通过并行训练两个3D高斯泼溅模型并施加一致性约束，减少视觉伪影，同时引入发散掩码策略和动态EMA代理以提高效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决野外图像3D重建中因光照不一致和瞬态干扰导致的低质量训练数据问题，现有方法难以生成稳定且一致的重建结果。

Method: 并行训练两个3DGS模型，施加一致性约束；引入发散掩码策略（多线索自适应掩码和自监督软掩码）防止模型陷入相似错误模式；使用动态EMA代理提高效率。

Result: 在真实世界数据集上表现优于现有方法，同时保持高效性。

Conclusion: Asymmetric Dual 3DGS通过不对称训练和动态EMA代理，显著提升了3D重建的稳定性和效率。

Abstract: 3D reconstruction from in-the-wild images remains a challenging task due to
inconsistent lighting conditions and transient distractors. Existing methods
typically rely on heuristic strategies to handle the low-quality training data,
which often struggle to produce stable and consistent reconstructions,
frequently resulting in visual artifacts. In this work, we propose Asymmetric
Dual 3DGS, a novel framework that leverages the stochastic nature of these
artifacts: they tend to vary across different training runs due to minor
randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS)
models in parallel, enforcing a consistency constraint that encourages
convergence on reliable scene geometry while suppressing inconsistent
artifacts. To prevent the two models from collapsing into similar failure modes
due to confirmation bias, we introduce a divergent masking strategy that
applies two complementary masks: a multi-cue adaptive mask and a
self-supervised soft mask, which leads to an asymmetric training process of the
two models, reducing shared error modes. In addition, to improve the efficiency
of model training, we introduce a lightweight variant called Dynamic EMA Proxy,
which replaces one of the two models with a dynamically updated Exponential
Moving Average (EMA) proxy, and employs an alternating masking strategy to
preserve divergence. Extensive experiments on challenging real-world datasets
demonstrate that our method consistently outperforms existing approaches while
achieving high efficiency. Codes and trained models will be released.

</details>


### [46] [WIFE-Fusion:Wavelet-aware Intra-inter Frequency Enhancement for Multi-model Image Fusion](https://arxiv.org/abs/2506.03555)
*Tianpei Zhang,Jufeng Zhao,Yiming Zhu,Guangmang Cui*

Main category: cs.CV

TL;DR: 提出了一种基于频域交互的多模态图像融合框架WIFE-Fusion，通过频域自注意力机制和频域间交互提升融合效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视频域特征探索和交互关系，限制了多模态图像融合的性能。

Method: 采用Intra-Frequency Self-Attention（IFSA）和Inter-Frequency Interaction（IFI）机制，提取和增强频域特征。

Result: 在五个数据集和三种多模态融合任务中表现优于现有方法。

Conclusion: WIFE-Fusion通过频域交互显著提升了多模态图像融合的效果。

Abstract: Multimodal image fusion effectively aggregates information from diverse
modalities, with fused images playing a crucial role in vision systems.
However, existing methods often neglect frequency-domain feature exploration
and interactive relationships. In this paper, we propose wavelet-aware
Intra-inter Frequency Enhancement Fusion (WIFE-Fusion), a multimodal image
fusion framework based on frequency-domain components interactions. Its core
innovations include: Intra-Frequency Self-Attention (IFSA) that leverages
inherent cross-modal correlations and complementarity through interactive
self-attention mechanisms to extract enriched frequency-domain features, and
Inter-Frequency Interaction (IFI) that enhances enriched features and filters
latent features via combinatorial interactions between heterogeneous
frequency-domain components across modalities. These processes achieve precise
source feature extraction and unified modeling of feature
extraction-aggregation. Extensive experiments on five datasets across three
multimodal fusion tasks demonstrate WIFE-Fusion's superiority over current
specialized and unified fusion methods. Our code is available at
https://github.com/Lmmh058/WIFE-Fusion.

</details>


### [47] [DiagNet: Detecting Objects using Diagonal Constraints on Adjacency Matrix of Graph Neural Network](https://arxiv.org/abs/2506.03571)
*Chong Hyun Lee,Kibae Lee*

Main category: cs.CV

TL;DR: DaigNet提出了一种基于图卷积网络（GCN）邻接矩阵对角约束的目标检测新方法，通过硬约束和软约束算法及两种损失函数，无需设计锚框，实验表明其性能优于多个YOLO版本。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测方法依赖锚框设计，DaigNet旨在通过GCN邻接矩阵的对角约束简化检测流程并提升性能。

Method: 提出两种对角化算法（硬约束和软约束）及两种损失函数（对角约束和互补约束），结合YOLO检测头实现目标检测。

Result: 在Pascal VOC上mAP50比YOLOv1高7.5%，在MS COCO上mAP分别比YOLOv3u、YOLOv5u和YOLOv8高5.1%、3.7%和2.9%。

Conclusion: DaigNet通过GCN对角约束简化了目标检测流程，并在多个数据集上显著优于YOLO系列模型。

Abstract: We propose DaigNet, a new approach to object detection with which we can
detect an object bounding box using diagonal constraints on adjacency matrix of
a graph convolutional network (GCN). We propose two diagonalization algorithms
based on hard and soft constraints on adjacency matrix and two loss functions
using diagonal constraint and complementary constraint. The DaigNet eliminates
the need for designing a set of anchor boxes commonly used. To prove
feasibility of our novel detector, we adopt detection head in YOLO models.
Experiments show that the DiagNet achieves 7.5% higher mAP50 on Pascal VOC than
YOLOv1. The DiagNet also shows 5.1% higher mAP on MS COCO than YOLOv3u, 3.7%
higher mAP than YOLOv5u, and 2.9% higher mAP than YOLOv8.

</details>


### [48] [ViTSGMM: A Robust Semi-Supervised Image Recognition Network Using Sparse Labels](https://arxiv.org/abs/2506.03582)
*Rui Yann,Xianglei Xing*

Main category: cs.CV

TL;DR: ViTSGMM是一种高效的半监督学习图像识别网络，通过优化特征表示与目标类别间的互信息，构建分层混合密度分类决策机制，在极少量标注数据下实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖复杂训练技术和架构，且在标注数据极少时泛化能力不足，ViTSGMM旨在解决这些问题。

Method: 构建分层混合密度分类决策机制，优化互信息并压缩冗余信息，保留关键判别成分。

Result: 在STL-10和CIFAR-10/100数据集上，使用极少标注样本即达到SOTA性能，并发现并修复了STL-10数据集的数据泄漏问题。

Conclusion: ViTSGMM高效且可靠，为半监督学习提供了新思路。

Abstract: We present ViTSGMM, an image recognition network that leverages
semi-supervised learning in a highly efficient manner. Existing works often
rely on complex training techniques and architectures, while their
generalization ability when dealing with extremely limited labeled data remains
to be improved. To address these limitations, we construct a hierarchical
mixture density classification decision mechanism by optimizing mutual
information between feature representations and target classes, compressing
redundant information while retaining crucial discriminative components.
Experimental results demonstrate that our method achieves state-of-the-art
performance on STL-10 and CIFAR-10/100 datasets when using negligible labeled
samples. Notably, this paper also reveals a long-overlooked data leakage issue
in the STL-10 dataset for semi-supervised learning tasks and removes duplicates
to ensure the reliability of experimental results. Code available at
https://github.com/Shu1L0n9/ViTSGMM.

</details>


### [49] [A Large-Scale Referring Remote Sensing Image Segmentation Dataset and Benchmark](https://arxiv.org/abs/2506.03583)
*Zhigang Yang,Huiguang Yao,Linmao Tian,Xuezhi Zhao,Qiang Li,Qi Wang*

Main category: cs.CV

TL;DR: 论文介绍了NWPU-Refer数据集和MRSNet模型，解决了现有RRSIS数据集的局限性，并在实验中验证了MRSNet的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有RRSIS数据集在分辨率、场景多样性和类别覆盖上存在不足，限制了模型的泛化和实际应用。

Method: 提出NWPU-Refer数据集和MRSNet模型，后者包含IFIM和HFIM模块，用于多尺度特征交互。

Result: MRSNet在NWPU-Refer数据集上实现了最先进的性能。

Conclusion: NWPU-Refer和MRSNet为RRSIS领域的发展提供了重要支持。

Abstract: Referring Remote Sensing Image Segmentation is a complex and challenging task
that integrates the paradigms of computer vision and natural language
processing. Existing datasets for RRSIS suffer from critical limitations in
resolution, scene diversity, and category coverage, which hinders the
generalization and real-world applicability of refer segmentation models. To
facilitate the development of this field, we introduce NWPU-Refer, the largest
and most diverse RRSIS dataset to date, comprising 15,003 high-resolution
images (1024-2048px) spanning 30+ countries with 49,745 annotated targets
supporting single-object, multi-object, and non-object segmentation scenarios.
Additionally, we propose the Multi-scale Referring Segmentation Network
(MRSNet), a novel framework tailored for the unique demands of RRSIS. MRSNet
introduces two key innovations: (1) an Intra-scale Feature Interaction Module
(IFIM) that captures fine-grained details within each encoder stage, and (2) a
Hierarchical Feature Interaction Module (HFIM) to enable seamless cross-scale
feature fusion, preserving spatial integrity while enhancing discriminative
power. Extensive experiments conducte on the proposed NWPU-Refer dataset
demonstrate that MRSNet achieves state-of-the-art performance across multiple
evaluation metrics, validating its effectiveness. The dataset and code are
publicly available at https://github.com/CVer-Yang/NWPU-Refer.

</details>


### [50] [BiMa: Towards Biases Mitigation for Text-Video Retrieval via Scene Element Guidance](https://arxiv.org/abs/2506.03589)
*Huy Le,Nhat Chung,Tung Kieu,Anh Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: BiMa框架通过视觉和文本去偏方法提升文本-视频检索性能，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决文本-视频检索系统中因数据集视觉-语言偏差导致的关键细节忽略问题。

Method: 生成场景元素增强视频嵌入以去视觉偏差，解耦文本特征为内容和偏差部分以去文本偏差。

Result: 在五个主要TVR基准测试中表现优异，并在分布外检索任务中验证了去偏能力。

Conclusion: BiMa通过视觉和文本去偏有效提升了检索性能，并展示了强大的偏差缓解能力。

Abstract: Text-video retrieval (TVR) systems often suffer from visual-linguistic biases
present in datasets, which cause pre-trained vision-language models to overlook
key details. To address this, we propose BiMa, a novel framework designed to
mitigate biases in both visual and textual representations. Our approach begins
by generating scene elements that characterize each video by identifying
relevant entities/objects and activities. For visual debiasing, we integrate
these scene elements into the video embeddings, enhancing them to emphasize
fine-grained and salient details. For textual debiasing, we introduce a
mechanism to disentangle text features into content and bias components,
enabling the model to focus on meaningful content while separately handling
biased information. Extensive experiments and ablation studies across five
major TVR benchmarks (i.e., MSR-VTT, MSVD, LSMDC, ActivityNet, and DiDeMo)
demonstrate the competitive performance of BiMa. Additionally, the model's bias
mitigation capability is consistently validated by its strong results on
out-of-distribution retrieval tasks.

</details>


### [51] [Resolving Task Objective Conflicts in Unified Multimodal Understanding and Generation via Task-Aware Mixture-of-Experts](https://arxiv.org/abs/2506.03591)
*Jiaxing Zhang,Xinyi Zeng,Hao Tang*

Main category: cs.CV

TL;DR: 论文提出了一种名为UTAMoE的新方法，通过解耦自回归模型的内部组件来解决多模态大语言模型中任务目标冲突的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在理解和生成任务之间存在任务目标冲突，导致性能不佳，现有方法无法从根本上解决这一问题。

Method: 设计了UTAMoE框架，通过任务感知的混合专家层解耦自回归模块，并引入两阶段训练策略以增强任务区分度。

Result: 实验表明，UTAMoE有效缓解了任务目标冲突，在多项任务中达到了最先进的性能。

Conclusion: UTAMoE通过解耦和任务感知优化，显著提升了多模态大语言模型的性能。

Abstract: Unified multimodal large language models (MLLMs) based on end-to-end
autoregressive (AR) transformers effectively integrate both understanding and
generation tasks within a single framework. However, intrinsic Task Objective
Conflicts between high-level semantic abstraction in understanding and
fine-grained detail preservation in generation pose significant challenges,
often leading to suboptimal trade-offs and task interference. Existing
solutions, such as decoupling shared visual encoders, fall short of
fundamentally resolving these conflicts due to inherent AR architecture. In
this paper, we propose a novel approach that decouples internal components of
AR to resolve task objective conflicts. Specifically, we design UTAMoE, a
Unified Task-Aware Mixture-of-Experts (MoE) framework that decouples internal
AR modules via a Task-Aware MoE Layer to create task-specific optimization
subpaths. To enhance task differentiation while maintaining overall
coordination, we introduce a novel Two-Stage Training Strategy. Extensive
experiments on multimodal benchmarks demonstrate that UTAMoE mitigates task
objective conflicts, achieving state-of-the-art performance across various
tasks. Visualizations and ablation studies further validate the effectiveness
of our approach.

</details>


### [52] [ControlThinker: Unveiling Latent Semantics for Controllable Image Generation through Visual Reasoning](https://arxiv.org/abs/2506.03596)
*Feng Han,Yang Jiao,Shaoxiang Chen,Junhao Xu,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: ControlThinker提出了一种新框架，通过视觉推理能力丰富文本提示的语义，从而改善图像生成的质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在稀疏语义文本提示与目标图像之间的语义鸿沟问题，减少对低层次控制信号的依赖。

Method: 采用“理解-生成”范式，利用MLLM挖掘控制图像的潜在语义，并通过奖励模型选择最优推理轨迹。

Result: 实验表明，ControlThinker有效缩小了文本提示与目标图像的语义差距，提升了视觉质量和语义一致性。

Conclusion: ControlThinker通过语义增强和推理优化，显著提升了可控图像生成的性能。

Abstract: The field of controllable image generation has seen significant advancements,
with various architectures improving generation layout consistency with control
signals. However, contemporary methods still face challenges in bridging the
semantic gap between input text prompts with sparse semantics and the target
images, often over-relying on low-level control signals to infer regional
details. To address this challenge, we propose ControlThinker, a novel
framework that employs a "comprehend-then-generate" paradigm. Firstly, by
incentivizing the visual reasoning capability of a MLLM, latent semantics from
control images are mined to enrich text prompts. This enriched semantic
understanding then seamlessly aids in image generation without the need for
additional complex modifications. To further tackle the uncertainty arising
from the ambiguity of control images, we encourage broader exploration of
reasoning trajectories and select the optimal one using a metric-based output
reward model (ORM). Extensive experimental results demonstrate that
ControlThinker effectively mitigates the semantic gap between raw text prompts
and target images, resulting in improved visual quality and semantic
consistency across a wide range of benchmarks. The code and models are
available at https://github.com/Maplebb/ControlThinker.

</details>


### [53] [Generating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision](https://arxiv.org/abs/2506.03605)
*Tomoya Yoshida,Shuhei Kurita,Taichi Nishimura,Shinsuke Mori*

Main category: cs.CV

TL;DR: 提出一个框架，利用大规模视频数据集提取多样化操作轨迹，并基于视觉和点云语言模型生成轨迹。


<details>
  <summary>Details</summary>
Motivation: 开发交互式机器人需要大量多样化的操作演示数据，但收集这些数据规模庞大且困难。

Method: 利用Exo-Ego4D数据集提取操作轨迹，结合文本描述，开发基于视觉和点云的轨迹生成模型。

Result: 在HOT3D数据集上验证了模型能成功生成有效的物体轨迹。

Conclusion: 为从第一视角生成6自由度操作轨迹的任务提供了数据集和基线模型。

Abstract: Learning to use tools or objects in common scenes, particularly handling them
in various ways as instructed, is a key challenge for developing interactive
robots. Training models to generate such manipulation trajectories requires a
large and diverse collection of detailed manipulation demonstrations for
various objects, which is nearly unfeasible to gather at scale. In this paper,
we propose a framework that leverages large-scale ego- and exo-centric video
datasets -- constructed globally with substantial effort -- of Exo-Ego4D to
extract diverse manipulation trajectories at scale. From these extracted
trajectories with the associated textual action description, we develop
trajectory generation models based on visual and point cloud-based language
models. In the recently proposed egocentric vision-based in-a-quality
trajectory dataset of HOT3D, we confirmed that our models successfully generate
valid object trajectories, establishing a training dataset and baseline models
for the novel task of generating 6DoF manipulation trajectories from action
descriptions in egocentric vision.

</details>


### [54] [Analyzing Transformer Models and Knowledge Distillation Approaches for Image Captioning on Edge AI](https://arxiv.org/abs/2506.03607)
*Wing Man Casca Kwok,Yip Chiu Tung,Kunal Bhagchandani*

Main category: cs.CV

TL;DR: 研究探讨了在边缘计算设备上部署基于Transformer的图像描述模型，通过知识蒸馏技术优化模型，实现在资源受限设备上的高效推理。


<details>
  <summary>Details</summary>
Motivation: 边缘设备计算资源有限，但需要实时AI决策能力，传统深度学习模型难以满足需求。

Method: 评估资源高效的Transformer模型，并应用知识蒸馏技术。

Result: 在资源受限设备上加速推理，同时保持模型性能。

Conclusion: Transformer模型结合知识蒸馏技术，适合边缘计算场景，提升机器感知能力。

Abstract: Edge computing decentralizes processing power to network edge, enabling
real-time AI-driven decision-making in IoT applications. In industrial
automation such as robotics and rugged edge AI, real-time perception and
intelligence are critical for autonomous operations. Deploying
transformer-based image captioning models at the edge can enhance machine
perception, improve scene understanding for autonomous robots, and aid in
industrial inspection.
  However, these edge or IoT devices are often constrained in computational
resources for physical agility, yet they have strict response time
requirements. Traditional deep learning models can be too large and
computationally demanding for these devices. In this research, we present
findings of transformer-based models for image captioning that operate
effectively on edge devices. By evaluating resource-effective transformer
models and applying knowledge distillation techniques, we demonstrate inference
can be accelerated on resource-constrained devices while maintaining model
performance using these techniques.

</details>


### [55] [PDSE: A Multiple Lesion Detector for CT Images using PANet and Deformable Squeeze-and-Excitation Block](https://arxiv.org/abs/2506.03608)
*Di Fan,Heng Yu,Zhiyuan Xu*

Main category: cs.CV

TL;DR: 提出了一种名为PDSE的单阶段病灶检测框架，通过改进Retinanet实现了更高的准确性和效率，特别是在多模态CT图像中。


<details>
  <summary>Details</summary>
Motivation: CT扫描中病灶的检测因病灶类型、大小和位置的多样性而具有挑战性，需要更高效的检测方法。

Method: 通过引入低层特征图增强路径聚合流，并利用自适应Squeeze-and-Excitation块和通道特征图注意力机制提升模型表现。

Result: 在公共DeepLesion基准测试中，mAP超过0.20，显著提升了小尺寸和多尺度目标的检测效果。

Conclusion: PDSE框架在多模态CT图像病灶检测中达到了新的最优性能。

Abstract: Detecting lesions in Computed Tomography (CT) scans is a challenging task in
medical image processing due to the diverse types, sizes, and locations of
lesions. Recently, various one-stage and two-stage framework networks have been
developed to focus on lesion localization. We introduce a one-stage lesion
detection framework, PDSE, by redesigning Retinanet to achieve higher accuracy
and efficiency for detecting lesions in multimodal CT images. Specifically, we
enhance the path aggregation flow by incorporating a low-level feature map.
Additionally, to improve model representation, we utilize the adaptive
Squeeze-and-Excitation (SE) block and integrate channel feature map attention.
This approach has resulted in achieving new state-of-the-art performance. Our
method significantly improves the detection of small and multiscaled objects.
When evaluated against other advanced algorithms on the public DeepLesion
benchmark, our algorithm achieved an mAP of over 0.20.

</details>


### [56] [VLMs Can Aggregate Scattered Training Patches](https://arxiv.org/abs/2506.03614)
*Zhanhui Zhou,Lingjie Chen,Chao Yang,Chaochao Lu*

Main category: cs.CV

TL;DR: 论文提出视觉语言模型（VLMs）中的视觉拼接能力可能导致有害内容绕过数据审核，通过将危险图像分割为看似无害的小块分散训练，模型可能在推理时重新组合并生成有害响应。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示VLMs中视觉拼接能力的潜在风险，即通过分散训练数据绕过内容审核，导致模型在推理时生成有害输出。

Method: 方法包括在三个数据集上验证VLMs的视觉拼接能力，通过将图像分割为小块并标记唯一ID进行微调，模拟对抗性数据中毒场景。

Result: 结果显示VLMs能够通过视觉拼接从分散的补丁中重建完整图像或文本描述，验证了有害内容绕过审核的可能性。

Conclusion: 结论指出视觉拼接能力对VLM安全性构成严重威胁，需开发新的防御机制应对此类攻击。

Abstract: One way to mitigate risks in vision-language models (VLMs) is to remove
dangerous samples in their training data. However, such data moderation can be
easily bypassed when harmful images are split into small, benign-looking
patches, scattered across many training samples. VLMs may then learn to piece
these fragments together during training and generate harmful responses at
inference, either from full images or text references. For instance, if trained
on image patches from a bloody scene paired with the descriptions "safe," VLMs
may later describe, the full image or a text reference to the scene, as "safe."
We define the core ability of VLMs enabling this attack as $\textit{visual
stitching}$ -- the ability to integrate visual information spread across
multiple training samples that share the same textual descriptions. In our
work, we first demonstrate visual stitching abilities in common open-source
VLMs on three datasets where each image is labeled with a unique synthetic ID:
we split each $(\texttt{image}, \texttt{ID})$ pair into $\{(\texttt{patch},
\texttt{ID})\}$ pairs at different granularity for finetuning, and we find that
tuned models can verbalize the correct IDs from full images or text reference.
Building on this, we simulate the adversarial data poisoning scenario mentioned
above by using patches from dangerous images and replacing IDs with text
descriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can
evade moderation in patches and later be reconstructed through visual
stitching, posing serious VLM safety risks. Code is available at
https://github.com/ZHZisZZ/visual-stitching.

</details>


### [57] [Isharah: A Large-Scale Multi-Scene Dataset for Continuous Sign Language Recognition](https://arxiv.org/abs/2506.03615)
*Sarah Alyami,Hamzah Luqman,Sadam Al-Azani,Maad Alowaifeer,Yazeed Alharbi,Yaser Alonaizan*

Main category: cs.CV

TL;DR: Isharah是一个大规模多场景连续手语识别（CSLR）数据集，首次在非受控环境中通过智能手机摄像头收集，包含30,000个视频片段，适用于开发鲁棒的CSLR和手语翻译（SLT）系统。


<details>
  <summary>Details</summary>
Motivation: 现有CSLR数据集主要在受控环境中收集，限制了其在现实场景中的适用性，Isharah旨在填补这一空白。

Method: 通过18名聋人和专业手语者使用智能手机摄像头在非受控环境中收集数据，提供丰富的语言注释。

Result: 数据集包含30,000个视频片段，支持多种手语理解基准测试，如独立手语者和未见句子的CSLR。

Conclusion: Isharah为开发鲁棒的CSLR和SLT系统提供了重要资源，填补了现有数据集的不足。

Abstract: Current benchmarks for sign language recognition (SLR) focus mainly on
isolated SLR, while there are limited datasets for continuous SLR (CSLR), which
recognizes sequences of signs in a video. Additionally, existing CSLR datasets
are collected in controlled settings, which restricts their effectiveness in
building robust real-world CSLR systems. To address these limitations, we
present Isharah, a large multi-scene dataset for CSLR. It is the first dataset
of its type and size that has been collected in an unconstrained environment
using signers' smartphone cameras. This setup resulted in high variations of
recording settings, camera distances, angles, and resolutions. This variation
helps with developing sign language understanding models capable of handling
the variability and complexity of real-world scenarios. The dataset consists of
30,000 video clips performed by 18 deaf and professional signers. Additionally,
the dataset is linguistically rich as it provides a gloss-level annotation for
all dataset's videos, making it useful for developing CSLR and sign language
translation (SLT) systems. This paper also introduces multiple sign language
understanding benchmarks, including signer-independent and unseen-sentence
CSLR, along with gloss-based and gloss-free SLT. The Isharah dataset is
available on https://snalyami.github.io/Isharah_CSLR/.

</details>


### [58] [Negative-Guided Subject Fidelity Optimization for Zero-Shot Subject-Driven Generation](https://arxiv.org/abs/2506.03621)
*Chaehun Shin,Jooyoung Choi,Johan Barthelemy,Jungbeom Lee,Sungroh Yoon*

Main category: cs.CV

TL;DR: SFO是一种新的对比学习框架，通过引入合成负样本和优化扩散时间步，显著提升零样本主题驱动生成的主题保真度和文本对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖正样本和扩散损失，缺乏对负样本的利用，限制了主题保真度的提升。

Method: 提出SFO框架，引入Condition-Degradation Negative Sampling（CDNS）自动生成负样本，并通过对比学习优化模型；同时重加权扩散时间步以聚焦关键步骤。

Result: 实验表明，SFO在主题保真度和文本对齐上显著优于基线方法。

Conclusion: SFO通过对比学习和时间步优化，有效提升了零样本主题驱动生成的性能。

Abstract: We present Subject Fidelity Optimization (SFO), a novel comparative learning
framework for zero-shot subject-driven generation that enhances subject
fidelity. Beyond supervised fine-tuning methods that rely only on positive
targets and use the diffusion loss as in the pre-training stage, SFO introduces
synthetic negative targets and explicitly guides the model to favor positives
over negatives through pairwise comparison. For negative targets, we propose
Condition-Degradation Negative Sampling (CDNS), which automatically generates
distinctive and informative negatives by intentionally degrading visual and
textual cues without expensive human annotations. Moreover, we reweight the
diffusion timesteps to focus finetuning on intermediate steps where subject
details emerge. Extensive experiments demonstrate that SFO with CDNS
significantly outperforms baselines in terms of both subject fidelity and text
alignment on a subject-driven generation benchmark. Project page:
https://subjectfidelityoptimization.github.io/

</details>


### [59] [FingerVeinSyn-5M: A Million-Scale Dataset and Benchmark for Finger Vein Recognition](https://arxiv.org/abs/2506.03635)
*Yinfan Wang,Jie Gui,Baosheng Yu,Qi Li,Zhenan Sun,Juho Kannala,Guoying Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种合成手指静脉图像生成器FVeinSyn，并创建了大规模数据集FingerVeinSyn-5M，显著提升了深度学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有手指静脉数据集规模小，限制了深度学习方法的进展，需要更大规模且多样化的数据集。

Method: 使用FVeinSyn生成合成手指静脉图像，构建包含500万样本的FingerVeinSyn-5M数据集，涵盖多种变化。

Result: 预训练模型在多个基准测试中平均性能提升53.91%。

Conclusion: FingerVeinSyn-5M为手指静脉识别领域提供了重要资源，显著推动了深度学习应用。

Abstract: A major challenge in finger vein recognition is the lack of large-scale
public datasets. Existing datasets contain few identities and limited samples
per finger, restricting the advancement of deep learning-based methods. To
address this, we introduce FVeinSyn, a synthetic generator capable of producing
diverse finger vein patterns with rich intra-class variations. Using FVeinSyn,
we created FingerVeinSyn-5M -- the largest available finger vein dataset --
containing 5 million samples from 50,000 unique fingers, each with 100
variations including shift, rotation, scale, roll, varying exposure levels,
skin scattering blur, optical blur, and motion blur. FingerVeinSyn-5M is also
the first to offer fully annotated finger vein images, supporting deep learning
applications in this field. Models pretrained on FingerVeinSyn-5M and
fine-tuned with minimal real data achieve an average 53.91\% performance gain
across multiple benchmarks. The dataset is publicly available at:
https://github.com/EvanWang98/FingerVeinSyn-5M.

</details>


### [60] [Spatial Understanding from Videos: Structured Prompts Meet Simulation Data](https://arxiv.org/abs/2506.03642)
*Haoyu Zhang,Meng Liu,Zaijing Li,Haokun Wen,Weili Guan,Yaowei Wang,Liqiang Nie*

Main category: cs.CV

TL;DR: 提出了一种增强预训练视觉语言模型3D空间推理能力的统一框架，结合结构化提示策略和自动化构建的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在空间不确定性和数据稀缺问题，限制了预训练视觉语言模型的3D空间推理能力。

Method: 结合SpatialMind（结构化提示策略）和ScanForgeQA（自动化构建的问答数据集），无需修改模型架构。

Result: 在多个基准测试中验证了提示和微调策略的独立和联合有效性。

Conclusion: 框架为视觉空间理解的未来研究提供了启发。

Abstract: Visual-spatial understanding, the ability to infer object relationships and
layouts from visual input, is fundamental to downstream tasks such as robotic
navigation and embodied interaction. However, existing methods face spatial
uncertainty and data scarcity, limiting the 3D spatial reasoning capability of
pre-trained vision-language models (VLMs). To address these challenges, we
present a unified framework for enhancing 3D spatial reasoning in pre-trained
VLMs without modifying their architecture. This framework combines SpatialMind,
a structured prompting strategy that decomposes complex scenes and questions
into interpretable reasoning steps, with ScanForgeQA, a scalable
question-answering dataset built from diverse 3D simulation scenes through an
automated construction process designed for fine-tuning. Extensive experiments
across multiple benchmarks demonstrate the individual and combined
effectiveness of our prompting and fine-tuning strategies, and yield insights
that may inspire future research on visual-spatial understanding.

</details>


### [61] [Images are Worth Variable Length of Representations](https://arxiv.org/abs/2506.03643)
*Lingjun Mao,Rodolfo Corona,Xin Liang,Wenhao Yan,Zineng Tang*

Main category: cs.CV

TL;DR: DOVE是一种动态视觉编码器，根据图像信息量动态生成可变数量的视觉标记，显著减少平均标记数同时保持高质量重建，并在多项任务中优于固定长度编码方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉编码器将图像映射为固定长度的标记序列，忽略了不同图像信息量的差异，导致效率低下。

Method: 提出DOVE动态视觉编码器，生成可变数量的视觉标记以重建图像，并扩展为查询条件化标记化以聚焦查询相关区域。

Result: DOVE显著减少标记数且保持高质量重建，在多项任务中优于固定长度编码方法。

Conclusion: DOVE通过动态标记化和查询条件化，实现了更高效和有针对性的语义提取。

Abstract: Most existing vision encoders map images into a fixed-length sequence of
tokens, overlooking the fact that different images contain varying amounts of
information. For example, a visually complex image (e.g., a cluttered room)
inherently carries more information and thus deserves more tokens than a simple
image (e.g., a blank wall). To address this inefficiency, we propose DOVE, a
dynamic vision encoder that produces a variable number of visual tokens (i.e.,
continuous representation vectors) to reconstruct each image. Our results show
that DOVE significantly reduces the average number of tokens while maintaining
high reconstruction quality. In several linear probing and downstream
multimodal tasks, it outperforms existing autoencoder-based tokenization
methods when using far fewer tokens, capturing more expressive semantic
features compared to fixed-length encoding. We further extend DOVE with
query-conditioned tokenization. By guiding the model to focus on query-relevant
regions, it achieves more efficient and targeted semantic extraction. Our code
and checkpoints are available at https://dove-encoder.github.io/dove-encoder.

</details>


### [62] [EmoArt: A Multidimensional Dataset for Emotion-Aware Artistic Generation](https://arxiv.org/abs/2506.03652)
*Cheng Zhang,Hongxia xie,Bin Wen,Songhan Zuo,Ruoxuan Zhang,Wen-huang Cheng*

Main category: cs.CV

TL;DR: 论文介绍了EmoArt数据集，用于解决文本到图像生成中情感表达不足的问题，并评估了现有模型的情感对齐能力。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型在情感表达和抽象艺术生成方面存在不足，主要由于缺乏细粒度的情感数据集。

Method: 提出了EmoArt数据集，包含132,664幅艺术作品，涵盖56种绘画风格，每幅作品有多维情感标注。

Result: 通过EmoArt数据集，系统地评估了流行文本到图像扩散模型的情感对齐能力。

Conclusion: EmoArt数据集为情感驱动的图像合成提供了重要数据和基准，推动了情感计算、多模态学习和计算艺术的发展。

Abstract: With the rapid advancement of diffusion models, text-to-image generation has
achieved significant progress in image resolution, detail fidelity, and
semantic alignment, particularly with models like Stable Diffusion 3.5, Stable
Diffusion XL, and FLUX 1. However, generating emotionally expressive and
abstract artistic images remains a major challenge, largely due to the lack of
large-scale, fine-grained emotional datasets. To address this gap, we present
the EmoArt Dataset -- one of the most comprehensive emotion-annotated art
datasets to date. It contains 132,664 artworks across 56 painting styles (e.g.,
Impressionism, Expressionism, Abstract Art), offering rich stylistic and
cultural diversity. Each image includes structured annotations: objective scene
descriptions, five key visual attributes (brushwork, composition, color, line,
light), binary arousal-valence labels, twelve emotion categories, and potential
art therapy effects. Using EmoArt, we systematically evaluate popular
text-to-image diffusion models for their ability to generate emotionally
aligned images from text. Our work provides essential data and benchmarks for
emotion-driven image synthesis and aims to advance fields such as affective
computing, multimodal learning, and computational art, enabling applications in
art therapy and creative design. The dataset and more details can be accessed
via our project website.

</details>


### [63] [MambaNeXt-YOLO: A Hybrid State Space Model for Real-time Object Detection](https://arxiv.org/abs/2506.03654)
*Xiaochun Lei,Siqi Wu,Weilin Wu,Zetao Jiang*

Main category: cs.CV

TL;DR: 论文提出了一种名为MambaNeXt-YOLO的新型实时目标检测框架，通过结合CNN与Mamba模型，实现了高效的长距离依赖建模，并在边缘设备上表现出色。


<details>
  <summary>Details</summary>
Motivation: 实时目标检测在计算资源有限的情况下具有挑战性，Transformer的高复杂度限制了其实用性，因此需要一种更高效的替代方案。

Method: 提出了MambaNeXt Block（结合CNN与Mamba）、MAFPN（多尺度特征融合）和边缘优化设计。

Result: 在PASCAL VOC数据集上达到66.6% mAP和31.9 FPS，支持边缘设备部署。

Conclusion: MambaNeXt-YOLO在准确性和效率之间取得了平衡，适用于实时和边缘计算场景。

Abstract: Real-time object detection is a fundamental but challenging task in computer
vision, particularly when computational resources are limited. Although
YOLO-series models have set strong benchmarks by balancing speed and accuracy,
the increasing need for richer global context modeling has led to the use of
Transformer-based architectures. Nevertheless, Transformers have high
computational complexity because of their self-attention mechanism, which
limits their practicality for real-time and edge deployments. To overcome these
challenges, recent developments in linear state space models, such as Mamba,
provide a promising alternative by enabling efficient sequence modeling with
linear complexity. Building on this insight, we propose MambaNeXt-YOLO, a novel
object detection framework that balances accuracy and efficiency through three
key contributions: (1) MambaNeXt Block: a hybrid design that integrates CNNs
with Mamba to effectively capture both local features and long-range
dependencies; (2) Multi-branch Asymmetric Fusion Pyramid Network (MAFPN): an
enhanced feature pyramid architecture that improves multi-scale object
detection across various object sizes; and (3) Edge-focused Efficiency: our
method achieved 66.6\% mAP at 31.9 FPS on the PASCAL VOC dataset without any
pre-training and supports deployment on edge devices such as the NVIDIA Jetson
Xavier NX and Orin NX.

</details>


### [64] [INP-Former++: Advancing Universal Anomaly Detection via Intrinsic Normal Prototypes and Residual Learning](https://arxiv.org/abs/2506.03660)
*Wei Luo,Haiming Yao,Yunkang Cao,Qiyu Chen,Ang Gao,Weiming Shen,Weihang Zhang,Wenyong Yu*

Main category: cs.CV

TL;DR: INP-Former是一种新颖的异常检测方法，通过从测试图像中提取固有正常原型（INPs）来避免对外部训练集的依赖，显著提升了检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法依赖训练集中的正常参考图像，但由于外观和位置的变化，这些参考图像与测试图像的对齐困难，限制了检测精度。

Method: 提出INP-Former，通过INP提取器从测试图像中提取固有正常原型（INPs），并使用INP一致性损失和软挖掘损失优化模型。

Result: INP-Former在单类、多类和少样本异常检测任务中达到最先进性能，并展示了一定的零样本能力。改进版INP-Former++进一步提升了性能。

Conclusion: INP-Former及其改进版为异常检测提供了通用且高效的解决方案，尤其在复杂场景下表现优异。

Abstract: Anomaly detection (AD) is essential for industrial inspection and medical
diagnosis, yet existing methods typically rely on ``comparing'' test images to
normal references from a training set. However, variations in appearance and
positioning often complicate the alignment of these references with the test
image, limiting detection accuracy. We observe that most anomalies manifest as
local variations, meaning that even within anomalous images, valuable normal
information remains. We argue that this information is useful and may be more
aligned with the anomalies since both the anomalies and the normal information
originate from the same image. Therefore, rather than relying on external
normality from the training set, we propose INP-Former, a novel method that
extracts Intrinsic Normal Prototypes (INPs) directly from the test image.
Specifically, we introduce the INP Extractor, which linearly combines normal
tokens to represent INPs. We further propose an INP Coherence Loss to ensure
INPs can faithfully represent normality for the testing image. These INPs then
guide the INP-guided Decoder to reconstruct only normal tokens, with
reconstruction errors serving as anomaly scores. Additionally, we propose a
Soft Mining Loss to prioritize hard-to-optimize samples during training.
INP-Former achieves state-of-the-art performance in single-class, multi-class,
and few-shot AD tasks across MVTec-AD, VisA, and Real-IAD, positioning it as a
versatile and universal solution for AD. Remarkably, INP-Former also
demonstrates some zero-shot AD capability. Furthermore, we propose a soft
version of the INP Coherence Loss and enhance INP-Former by incorporating
residual learning, leading to the development of INP-Former++. The proposed
method significantly improves detection performance across single-class,
multi-class, semi-supervised, few-shot, and zero-shot settings.

</details>


### [65] [Zero-Shot Temporal Interaction Localization for Egocentric Videos](https://arxiv.org/abs/2506.03662)
*Erhang Zhang,Junyi Ma,Yin-Dong Zheng,Yixuan Zhou,Hesheng Wang*

Main category: cs.CV

TL;DR: EgoLoc是一种零样本时间交互定位方法，通过自适应采样策略和2D/3D观察，高效定位人类-物体交互的抓取动作时间点。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖标注数据导致领域偏差和低效，而现有零样本方法因粗粒度估计和开环流程性能受限。

Method: EgoLoc结合自适应采样策略和2D/3D观察，生成高质量初始猜测，并通过闭环反馈优化结果。

Result: 在公开数据集和新基准上，EgoLoc优于现有方法。

Conclusion: EgoLoc为第一视角视频中的交互定位提供了高效准确的解决方案，代码将开源。

Abstract: Locating human-object interaction (HOI) actions within video serves as the
foundation for multiple downstream tasks, such as human behavior analysis and
human-robot skill transfer. Current temporal action localization methods
typically rely on annotated action and object categories of interactions for
optimization, which leads to domain bias and low deployment efficiency.
Although some recent works have achieved zero-shot temporal action localization
(ZS-TAL) with large vision-language models (VLMs), their coarse-grained
estimations and open-loop pipelines hinder further performance improvements for
temporal interaction localization (TIL). To address these issues, we propose a
novel zero-shot TIL approach dubbed EgoLoc to locate the timings of grasp
actions for human-object interaction in egocentric videos. EgoLoc introduces a
self-adaptive sampling strategy to generate reasonable visual prompts for VLM
reasoning. By absorbing both 2D and 3D observations, it directly samples
high-quality initial guesses around the possible contact/separation timestamps
of HOI according to 3D hand velocities, leading to high inference accuracy and
efficiency. In addition, EgoLoc generates closed-loop feedback from visual and
dynamic cues to further refine the localization results. Comprehensive
experiments on the publicly available dataset and our newly proposed benchmark
demonstrate that EgoLoc achieves better temporal interaction localization for
egocentric videos compared to state-of-the-art baselines. We will release our
code and relevant data as open-source at https://github.com/IRMVLab/EgoLoc.

</details>


### [66] [Intersectional Bias in Pre-Trained Image Recognition Models](https://arxiv.org/abs/2506.03664)
*Valerie Krug,Sebastian Stober*

Main category: cs.CV

TL;DR: 研究发现ImageNet分类器在面部图像中表现出年龄、种族和性别的偏见，尤其是年龄区分明显，种族和性别偏见较弱。


<details>
  <summary>Details</summary>
Motivation: 探讨深度学习模型中预训练模型可能存在的偏见问题，特别是在面部图像中对年龄、种族和性别的敏感变量。

Method: 使用线性分类器探针和地形图可视化激活，分析ImageNet分类器的表示。

Result: ImageNet分类器对年龄区分明显，对种族和性别的区分较弱，尤其是中年群体。

Conclusion: 预训练模型在面部图像中存在偏见，需进一步研究和改进以减少偏见。

Abstract: Deep Learning models have achieved remarkable success. Training them is often
accelerated by building on top of pre-trained models which poses the risk of
perpetuating encoded biases. Here, we investigate biases in the representations
of commonly used ImageNet classifiers for facial images while considering
intersections of sensitive variables age, race and gender. To assess the
biases, we use linear classifier probes and visualize activations as
topographic maps. We find that representations in ImageNet classifiers
particularly allow differentiation between ages. Less strongly pronounced, the
models appear to associate certain ethnicities and distinguish genders in
middle-aged groups.

</details>


### [67] [Accelerating SfM-based Pose Estimation with Dominating Set](https://arxiv.org/abs/2506.03667)
*Joji Joseph,Bharadwaj Amrutur,Shalabh Bhatnagar*

Main category: cs.CV

TL;DR: 提出了一种基于图论支配集的预处理技术，显著加速了SfM位姿估计，适用于AR、VR和机器人等实时应用。


<details>
  <summary>Details</summary>
Motivation: 解决SfM位姿估计在实时应用中的速度瓶颈问题。

Method: 利用图论中的支配集概念预处理SfM模型，减少参考图像和点云规模。

Result: 处理速度提升1.5-14.48倍，参考图像和点云规模分别减少17-23倍和2.27-4倍。

Conclusion: 该方法在速度和精度之间取得了平衡，为实时3D位姿估计提供了高效解决方案。

Abstract: This paper introduces a preprocessing technique to speed up
Structure-from-Motion (SfM) based pose estimation, which is critical for
real-time applications like augmented reality (AR), virtual reality (VR), and
robotics. Our method leverages the concept of a dominating set from graph
theory to preprocess SfM models, significantly enhancing the speed of the pose
estimation process without losing significant accuracy. Using the OnePose
dataset, we evaluated our method across various SfM-based pose estimation
techniques. The results demonstrate substantial improvements in processing
speed, ranging from 1.5 to 14.48 times, and a reduction in reference images and
point cloud size by factors of 17-23 and 2.27-4, respectively. This work offers
a promising solution for efficient and accurate 3D pose estimation, balancing
speed and accuracy in real-time applications.

</details>


### [68] [BiXFormer: A Robust Framework for Maximizing Modality Effectiveness in Multi-Modal Semantic Segmentation](https://arxiv.org/abs/2506.03675)
*Jialei Chen,Xu Zheng,Danda Pani Paudel,Luc Van Gool,Hiroshi Murase,Daisuke Deguchi*

Main category: cs.CV

TL;DR: BiXFormer通过统一模态匹配（UMM）和跨模态对齐（CMA）优化多模态语义分割，提升模态效果并处理缺失模态。


<details>
  <summary>Details</summary>
Motivation: 现有方法融合多模态特征或知识蒸馏，限制了各模态在不同情境下的优势发挥。

Method: BiXFormer将多模态输入分为RGB和非RGB（X），分别处理，并提出UMM（含MAM和CM）和CMA。

Result: 实验显示，mIoU提升+2.75%和+22.74%。

Conclusion: BiXFormer有效提升多模态语义分割性能，尤其在模态缺失情况下表现优异。

Abstract: Utilizing multi-modal data enhances scene understanding by providing
complementary semantic and geometric information. Existing methods fuse
features or distill knowledge from multiple modalities into a unified
representation, improving robustness but restricting each modality's ability to
fully leverage its strengths in different situations. We reformulate
multi-modal semantic segmentation as a mask-level classification task and
propose BiXFormer, which integrates Unified Modality Matching (UMM) and Cross
Modality Alignment (CMA) to maximize modality effectiveness and handle missing
modalities. Specifically, BiXFormer first categorizes multi-modal inputs into
RGB and X, where X represents any non-RGB modalities, e.g., depth, allowing
separate processing for each. This design leverages the well-established
pretraining for RGB, while addressing the relative lack of attention to X
modalities. Then, we propose UMM, which includes Modality Agnostic Matching
(MAM) and Complementary Matching (CM). MAM assigns labels to features from all
modalities without considering modality differences, leveraging each modality's
strengths. CM then reassigns unmatched labels to remaining unassigned features
within their respective modalities, ensuring that each available modality
contributes to the final prediction and mitigating the impact of missing
modalities. Moreover, to further facilitate UMM, we introduce CMA, which
enhances the weaker queries assigned in CM by aligning them with optimally
matched queries from MAM. Experiments on both synthetic and real-world
multi-modal benchmarks demonstrate the effectiveness of our method, achieving
significant improvements in mIoU of +2.75% and +22.74% over the prior arts.

</details>


### [69] [How PARTs assemble into wholes: Learning the relative composition of images](https://arxiv.org/abs/2506.03682)
*Melika Ayoughi,Samira Abnar,Chen Huang,Chris Sandino,Sayeri Lala,Eeshan Gunesh Dhekane,Dan Busbridge,Shuangfei Zhai,Vimal Thilak,Josh Susskind,Pascal Mettes,Paul Groth,Hanlin Goh*

Main category: cs.CV

TL;DR: PART是一种自监督学习方法，通过连续相对变换学习图像部分的相对组成，优于基于网格的方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于网格的方法无法捕捉真实世界物体组成的连续性和流动性，因此提出PART来解决这一问题。

Method: 利用连续相对变换建模图像部分之间的关系，实现非网格结构的相对定位。

Result: 在对象检测和时间序列预测等任务中优于MAE和DropPos等网格方法，同时在全局分类任务中表现竞争性。

Conclusion: PART为跨数据类型的通用自监督预训练提供了新方向，具有广泛的应用潜力。

Abstract: The composition of objects and their parts, along with object-object
positional relationships, provides a rich source of information for
representation learning. Hence, spatial-aware pretext tasks have been actively
explored in self-supervised learning. Existing works commonly start from a grid
structure, where the goal of the pretext task involves predicting the absolute
position index of patches within a fixed grid. However, grid-based approaches
fall short of capturing the fluid and continuous nature of real-world object
compositions. We introduce PART, a self-supervised learning approach that
leverages continuous relative transformations between off-grid patches to
overcome these limitations. By modeling how parts relate to each other in a
continuous space, PART learns the relative composition of images-an off-grid
structural relative positioning process that generalizes beyond occlusions and
deformations. In tasks requiring precise spatial understanding such as object
detection and time series prediction, PART outperforms strong grid-based
methods like MAE and DropPos, while also maintaining competitive performance on
global classification tasks with minimal hyperparameter tuning. By breaking
free from grid constraints, PART opens up an exciting new trajectory for
universal self-supervised pretraining across diverse datatypes-from natural
images to EEG signals-with promising potential in video, medical imaging, and
audio.

</details>


### [70] [PRJ: Perception-Retrieval-Judgement for Generated Images](https://arxiv.org/abs/2506.03683)
*Qiang Fu,Zonglei Jing,Zonghao Ying,Xiaoqian Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为PRJ的认知启发框架，通过感知-检索-判断三阶段设计，改进AI生成视觉内容的安全性检测，提升检测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展带来了创意能力，但也引发了AI生成视觉内容的安全性问题，现有系统缺乏上下文理解和动态毒性评估能力。

Method: 提出PRJ框架，分为感知（图像转描述语言）、检索（获取外部知识）、判断（基于规则评估毒性）三阶段，并引入动态评分机制。

Result: 实验表明PRJ在检测准确性和鲁棒性上优于现有安全检测系统，支持结构化类别级毒性解释。

Conclusion: PRJ框架有效解决了现有系统在上下文理解和动态毒性评估上的不足，为AI生成内容的安全检测提供了新方法。

Abstract: The rapid progress of generative AI has enabled remarkable creative
capabilities, yet it also raises urgent concerns regarding the safety of
AI-generated visual content in real-world applications such as content
moderation, platform governance, and digital media regulation. This includes
unsafe material such as sexually explicit images, violent scenes, hate symbols,
propaganda, and unauthorized imitations of copyrighted artworks. Existing image
safety systems often rely on rigid category filters and produce binary outputs,
lacking the capacity to interpret context or reason about nuanced,
adversarially induced forms of harm. In addition, standard evaluation metrics
(e.g., attack success rate) fail to capture the semantic severity and dynamic
progression of toxicity. To address these limitations, we propose
Perception-Retrieval-Judgement (PRJ), a cognitively inspired framework that
models toxicity detection as a structured reasoning process. PRJ follows a
three-stage design: it first transforms an image into descriptive language
(perception), then retrieves external knowledge related to harm categories and
traits (retrieval), and finally evaluates toxicity based on legal or normative
rules (judgement). This language-centric structure enables the system to detect
both explicit and implicit harms with improved interpretability and categorical
granularity. In addition, we introduce a dynamic scoring mechanism based on a
contextual toxicity risk matrix to quantify harmfulness across different
semantic dimensions. Experiments show that PRJ surpasses existing safety
checkers in detection accuracy and robustness while uniquely supporting
structured category-level toxicity interpretation.

</details>


### [71] [DSSAU-Net:U-Shaped Hybrid Network for Pubic Symphysis and Fetal Head Segmentation](https://arxiv.org/abs/2506.03684)
*Zunhui Xia,Hongxing Li,Libin Lan*

Main category: cs.CV

TL;DR: 提出了一种名为DSSAU-Net的稀疏自注意力网络架构，用于胎儿头部和耻骨联合的精确分割，以提高分娩过程中的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 传统分娩诊断方法主观且不准确，超声辅助诊断通过客观参数（AoP和HSD）提供更可靠的评估，但需要精确分割胎儿头部和耻骨联合。

Method: 设计了DSSAU-Net，采用对称U形编码器-解码器架构，结合双稀疏选择注意力块（DSSA）和多尺度特征融合，以减少计算复杂度并提取关键特征。

Result: 在MICCAI IUGC 2024竞赛中，DSSAU-Net在分类和分割任务中排名第四，验证了其有效性。

Conclusion: DSSAU-Net在计算效率和性能上表现优异，为超声辅助分娩诊断提供了可靠工具。

Abstract: In the childbirth process, traditional methods involve invasive vaginal
examinations, but research has shown that these methods are both subjective and
inaccurate. Ultrasound-assisted diagnosis offers an objective yet effective way
to assess fetal head position via two key parameters: Angle of Progression
(AoP) and Head-Symphysis Distance (HSD), calculated by segmenting the fetal
head (FH) and pubic symphysis (PS), which aids clinicians in ensuring a smooth
delivery process. Therefore, accurate segmentation of FH and PS is crucial. In
this work, we propose a sparse self-attention network architecture with good
performance and high computational efficiency, named DSSAU-Net, for the
segmentation of FH and PS. Specifically, we stack varying numbers of Dual
Sparse Selection Attention (DSSA) blocks at each stage to form a symmetric
U-shaped encoder-decoder network architecture. For a given query, DSSA is
designed to explicitly perform one sparse token selection at both the region
and pixel levels, respectively, which is beneficial for further reducing
computational complexity while extracting the most relevant features. To
compensate for the information loss during the upsampling process, skip
connections with convolutions are designed. Additionally, multiscale feature
fusion is employed to enrich the model's global and local information. The
performance of DSSAU-Net has been validated using the Intrapartum Ultrasound
Grand Challenge (IUGC) 2024 \textit{test set} provided by the organizer in the
MICCAI IUGC 2024
competition\footnote{\href{https://codalab.lisn.upsaclay.fr/competitions/18413\#learn\_the\_details}{https://codalab.lisn.upsaclay.fr/competitions/18413\#learn\_the\_details}},
where we win the fourth place on the tasks of classification and segmentation,
demonstrating its effectiveness. The codes will be available at
https://github.com/XiaZunhui/DSSAU-Net.

</details>


### [72] [Advancements in Artificial Intelligence Applications for Cardiovascular Disease Research](https://arxiv.org/abs/2506.03698)
*Yuanlin Mo,Haishan Huang,Bocheng Liang,Weibo Ma*

Main category: cs.CV

TL;DR: AI在心血管医学中的应用通过深度学习技术提升了诊断效率和准确性，但仍需解决数据验证问题以确保临床可靠性。


<details>
  <summary>Details</summary>
Motivation: 探索AI在心血管医学中的潜力，尤其是通过深度学习技术提升诊断效率和准确性。

Method: 使用卷积神经网络和生成对抗网络等深度学习架构，自动化分析医学影像和生理信号。

Result: AI在诊断准确性和工作流程效率上超越人类能力，但仍存在输入数据验证不足的问题。

Conclusion: AI在心血管医学中具有变革性潜力，但需建立稳健的验证协议，未来方向包括多模态数据整合和自适应算法。

Abstract: Recent advancements in artificial intelligence (AI) have revolutionized
cardiovascular medicine, particularly through integration with computed
tomography (CT), magnetic resonance imaging (MRI), electrocardiography (ECG)
and ultrasound (US). Deep learning architectures, including convolutional
neural networks and generative adversarial networks, enable automated analysis
of medical imaging and physiological signals, surpassing human capabilities in
diagnostic accuracy and workflow efficiency. However, critical challenges
persist, including the inability to validate input data accuracy, which may
propagate diagnostic errors. This review highlights AI's transformative
potential in precision diagnostics while underscoring the need for robust
validation protocols to ensure clinical reliability. Future directions
emphasize hybrid models integrating multimodal data and adaptive algorithms to
refine personalized cardiovascular care.

</details>


### [73] [OV-COAST: Cost Aggregation with Optimal Transport for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2506.03706)
*Aditya Gandhamal,Aniruddh Sikdar,Suresh Sundaram*

Main category: cs.CV

TL;DR: OV-COAST提出了一种基于最优传输理论的开集词汇语义分割方法，通过两阶段优化策略显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 提升开集词汇语义分割（OVSS）的域外泛化能力，利用最优传输理论对齐视觉-语言特征。

Method: 采用两阶段优化：第一阶段通过Sinkhorn距离解决最优传输问题，第二阶段利用对齐方案指导CAT-Seg模型训练。

Result: 在MESS基准测试中，OV-COAST显著优于现有方法，CAT-Seg提升1.72%，SAN-B提升4.9% mIoU。

Conclusion: OV-COAST通过最优传输理论有效提升了开集词汇语义分割的性能，具有实际应用潜力。

Abstract: Open-vocabulary semantic segmentation (OVSS) entails assigning semantic
labels to each pixel in an image using textual descriptions, typically
leveraging world models such as CLIP. To enhance out-of-domain generalization,
we propose Cost Aggregation with Optimal Transport (OV-COAST) for
open-vocabulary semantic segmentation. To align visual-language features within
the framework of optimal transport theory, we employ cost volume to construct a
cost matrix, which quantifies the distance between two distributions. Our
approach adopts a two-stage optimization strategy: in the first stage, the
optimal transport problem is solved using cost volume via Sinkhorn distance to
obtain an alignment solution; in the second stage, this solution is used to
guide the training of the CAT-Seg model. We evaluate state-of-the-art OVSS
models on the MESS benchmark, where our approach notably improves the
performance of the cost-aggregation model CAT-Seg with ViT-B backbone,
achieving superior results, surpassing CAT-Seg by 1.72 % and SAN-B by 4.9 %
mIoU. The code is available at
https://github.com/adityagandhamal/OV-COAST/}{https://github.com/adityagandhamal/OV-COAST/ .

</details>


### [74] [AetherVision-Bench: An Open-Vocabulary RGB-Infrared Benchmark for Multi-Angle Segmentation across Aerial and Ground Perspectives](https://arxiv.org/abs/2506.03709)
*Aniruddh Sikdar,Aditya Gandhamal,Suresh Sundaram*

Main category: cs.CV

TL;DR: 论文提出了AetherVision-Bench基准，用于评估多视角分割性能，并分析了零样本迁移模型的关键影响因素。


<details>
  <summary>Details</summary>
Motivation: 解决开放词汇语义分割（OVSS）在跨域泛化中的挑战，提升其在现实应用中的实用性。

Method: 提出AetherVision-Bench基准，评估多视角（空中和地面）分割性能，并分析零样本迁移模型的关键因素。

Result: 评估了现有OVSS模型的性能，并识别了影响零样本迁移模型的关键因素。

Conclusion: 该研究为未来研究提供了基准和见解，推动了开放词汇语义分割的进一步发展。

Abstract: Open-vocabulary semantic segmentation (OVSS) involves assigning labels to
each pixel in an image based on textual descriptions, leveraging world models
like CLIP. However, they encounter significant challenges in cross-domain
generalization, hindering their practical efficacy in real-world applications.
Embodied AI systems are transforming autonomous navigation for ground vehicles
and drones by enhancing their perception abilities, and in this study, we
present AetherVision-Bench, a benchmark for multi-angle segmentation across
aerial, and ground perspectives, which facilitates an extensive evaluation of
performance across different viewing angles and sensor modalities. We assess
state-of-the-art OVSS models on the proposed benchmark and investigate the key
factors that impact the performance of zero-shot transfer models. Our work
pioneers the creation of a robustness benchmark, offering valuable insights and
establishing a foundation for future research.

</details>


### [75] [OSGNet @ Ego4D Episodic Memory Challenge 2025](https://arxiv.org/abs/2506.03710)
*Yisen Feng,Haoyu Zhang,Qiaohui Chu,Meng Liu,Weili Guan,Yaowei Wang,Liqiang Nie*

Main category: cs.CV

TL;DR: 本文介绍了在CVPR 2025 Ego4D Episodic Memory Challenge中三个自我中心视频定位任务的冠军解决方案，采用早期融合模型提升定位精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有统一视频定位方法因依赖后期融合策略而导致结果不佳的问题。

Method: 采用基于早期融合的视频定位模型处理三个任务。

Result: 在Natural Language Queries、Goal Step和Moment Queries三个赛道中获得第一名。

Conclusion: 早期融合模型显著提升了视频定位的准确性，证明了其有效性。

Abstract: In this report, we present our champion solutions for the three egocentric
video localization tracks of the Ego4D Episodic Memory Challenge at CVPR 2025.
All tracks require precise localization of the interval within an untrimmed
egocentric video. Previous unified video localization approaches often rely on
late fusion strategies, which tend to yield suboptimal results. To address
this, we adopt an early fusion-based video localization model to tackle all
three tasks, aiming to enhance localization accuracy. Ultimately, our method
achieved first place in the Natural Language Queries, Goal Step, and Moment
Queries tracks, demonstrating its effectiveness. Our code can be found at
https://github.com/Yisen-Feng/OSGNet.

</details>


### [76] [PlückeRF: A Line-based 3D Representation for Few-view Reconstruction](https://arxiv.org/abs/2506.03713)
*Sam Bahrami,Dylan Campbell*

Main category: cs.CV

TL;DR: 本文提出了一种改进的多视角3D重建方法，通过PlückeRF表示更有效地利用多视角信息，提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有的单视角和少视角3D重建方法在利用多视角信息方面仍有改进空间。

Method: 提出了一种基于PlückeRF表示的模型，通过结构化特征增强的线条连接3D表示与输入视图的像素光线，优先共享邻近3D位置和像素光线的信息。

Result: 实验表明，该方法在重建质量上优于等效的三平面表示和最先进的feedforward重建方法。

Conclusion: PlückeRF表示在多视角3D重建中表现优越，为未来研究提供了新思路。

Abstract: Feed-forward 3D reconstruction methods aim to predict the 3D structure of a
scene directly from input images, providing a faster alternative to per-scene
optimization approaches. Significant progress has been made in single-view and
few-view reconstruction using learned priors that infer object shape and
appearance, even for unobserved regions. However, there is substantial
potential to enhance these methods by better leveraging information from
multiple views when available. To address this, we propose a few-view
reconstruction model that more effectively harnesses multi-view information.
Our approach introduces a simple mechanism that connects the 3D representation
with pixel rays from the input views, allowing for preferential sharing of
information between nearby 3D locations and between 3D locations and nearby
pixel rays. We achieve this by defining the 3D representation as a set of
structured, feature-augmented lines; the Pl\"uckeRF representation. Using this
representation, we demonstrate improvements in reconstruction quality over the
equivalent triplane representation and state-of-the-art feedforward
reconstruction methods.

</details>


### [77] [FSHNet: Fully Sparse Hybrid Network for 3D Object Detection](https://arxiv.org/abs/2506.03714)
*Shuai Liu,Mingyue Cui,Boyang Li,Quanmin Liang,Tinghe Hong,Kai Huang,Yunxiao Shan,Kai Huang*

Main category: cs.CV

TL;DR: FSHNet通过SlotFormer块和动态稀疏标签分配策略，解决了稀疏3D检测器中长距离特征提取和中心特征缺失的问题，提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏3D检测器仅从非空体素提取特征，导致长距离交互能力弱和中心特征缺失，影响特征提取和网络优化。

Method: 提出FSHNet，引入SlotFormer块增强长距离特征提取，采用动态稀疏标签分配策略优化网络，并设计稀疏上采样模块保留细节。

Result: 在Waymo、nuScenes和Argoverse2基准测试中验证了FSHNet的有效性。

Conclusion: FSHNet通过改进特征提取和优化策略，显著提升了稀疏3D检测器的性能。

Abstract: Fully sparse 3D detectors have recently gained significant attention due to
their efficiency in long-range detection. However, sparse 3D detectors extract
features only from non-empty voxels, which impairs long-range interactions and
causes the center feature missing. The former weakens the feature extraction
capability, while the latter hinders network optimization. To address these
challenges, we introduce the Fully Sparse Hybrid Network (FSHNet). FSHNet
incorporates a proposed SlotFormer block to enhance the long-range feature
extraction capability of existing sparse encoders. The SlotFormer divides
sparse voxels using a slot partition approach, which, compared to traditional
window partition, provides a larger receptive field. Additionally, we propose a
dynamic sparse label assignment strategy to deeply optimize the network by
providing more high-quality positive samples. To further enhance performance,
we introduce a sparse upsampling module to refine downsampled voxels,
preserving fine-grained details crucial for detecting small objects. Extensive
experiments on the Waymo, nuScenes, and Argoverse2 benchmarks demonstrate the
effectiveness of FSHNet. The code is available at
https://github.com/Say2L/FSHNet.

</details>


### [78] [ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices](https://arxiv.org/abs/2506.03737)
*Hao Yu,Tangyu Jiang,Shuning Jia,Shannan Yan,Shunning Liu,Haolong Qian,Guanghao Li,Shuting Dong,Huaisong Zhang,Chun Yuan*

Main category: cs.CV

TL;DR: 论文提出ComRoPE，通过可训练的交换角度矩阵改进RoPE，提升位置编码的灵活性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统位置编码方法缺乏鲁棒性和灵活性，RoPE虽改进但仍受限于手动定义的旋转矩阵。

Method: 提出ComRoPE，基于可训练的交换角度矩阵，满足RoPE方程，确保位置偏移的一致性。

Result: 在ImageNet-1K数据集上，训练分辨率和高分辨率下分别提升1.6%和2.9%。

Conclusion: ComRoPE不仅提升性能，还泛化现有RoPE方法，为未来位置编码研究提供新思路。

Abstract: The Transformer architecture has revolutionized various regions since it was
proposed, and its effectiveness largely depends on the ability to encode
positional information. Traditional position encoding methods exhibit
significant limitations due to lack of robustness and flexibility of position.
Therefore, Rotary Positional Encoding (RoPE) was proposed to alleviate these
issues, which integrates positional information by rotating the embeddings in
the attention mechanism. However, RoPE requires manually defined rotation
matrices with limited transformation space, constraining the model's capacity.
In this work, we propose ComRoPE, which generalizes RoPE by defining it in
terms of trainable commuting angle matrices. Specifically, we demonstrate that
pairwise commutativity of these matrices is essential for RoPE to achieve
scalability and positional robustness. We formally define the RoPE Equation,
which is an essential condition that ensures consistent performance with
position offsets. Based on the theoretical analysis, we present two types of
trainable commuting angle matrices as sufficient solutions to the RoPE
equation, which significantly improve performance, surpassing the current
state-of-the-art method by 1.6% at training resolution and 2.9% at higher
resolution on the ImageNet-1K dataset. Furthermore, our framework shows
versatility in generalizing to existing RoPE formulations and offering new
insights for future positional encoding research. To ensure reproducibility,
the source code and instructions are available at
https://github.com/Longin-Yu/ComRoPE

</details>


### [79] [SAAT: Synergistic Alternating Aggregation Transformer for Image Super-Resolution](https://arxiv.org/abs/2506.03740)
*Jianfeng Wu,Nannan Xu*

Main category: cs.CV

TL;DR: 论文提出了一种新型模型SAAT，通过结合通道和空间注意力机制，提升了单图像超分辨率任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的超分辨率方法在计算自注意力时忽略了跨通道信息和中间过程的空间结构信息，且未充分探索通道与空间注意力的协同关系。

Method: 提出SAAT模型，引入CWSAG和SWSAG模块，分别结合通道注意力与窗口注意力、空间注意力与窗口注意力，以增强特征融合和结构特征提取。

Result: SAAT在超分辨率任务中表现优异，性能与现有最优方法相当。

Conclusion: SAAT通过协同注意力机制有效提升了超分辨率任务的性能，证明了其潜力。

Abstract: Single image super-resolution is a well-known downstream task which aims to
restore low-resolution images into high-resolution images. At present, models
based on Transformers have shone brightly in the field of super-resolution due
to their ability to capture long-term dependencies in information. However,
current methods typically compute self-attention in nonoverlapping windows to
save computational costs, and the standard self-attention computation only
focuses on its results, thereby neglecting the useful information across
channels and the rich spatial structural information generated in the
intermediate process. Channel attention and spatial attention have,
respectively, brought significant improvements to various downstream visual
tasks in terms of extracting feature dependency and spatial structure
relationships, but the synergistic relationship between channel and spatial
attention has not been fully explored yet.To address these issues, we propose a
novel model. Synergistic Alternating Aggregation Transformer (SAAT), which can
better utilize the potential information of features. In SAAT, we introduce the
Efficient Channel & Window Synergistic Attention Group (CWSAG) and the Spatial
& Window Synergistic Attention Group (SWSAG). On the one hand, CWSAG combines
efficient channel attention with shifted window attention, enhancing non-local
feature fusion, and producing more visually appealing results. On the other
hand, SWSAG leverages spatial attention to capture rich structured feature
information, thereby enabling SAAT to more effectively extract structural
features.Extensive experimental results and ablation studies demonstrate the
effectiveness of SAAT in the field of super-resolution. SAAT achieves
performance comparable to that of the state-of-the-art (SOTA) under the same
quantity of parameters.

</details>


### [80] [HUMOF: Human Motion Forecasting in Interactive Social Scenes](https://arxiv.org/abs/2506.03753)
*Caiyi Sun,Yujing Sun,Xiao Han,Zemin Yang,Jiawei Liu,Xinge Zhu,Siu Ming Yiu,Yuexin Ma*

Main category: cs.CV

TL;DR: 提出了一种用于复杂交互场景中人体运动预测的分层交互特征表示方法，结合空间和频率视角的推理模块，显著提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 复杂场景中的人体行为预测因交互信息丰富而具有挑战性，现有方法难以应对。

Method: 设计了分层交互特征表示和高低层次特征结合的粗到细推理模块。

Result: 在四个公共数据集上实现了最先进的性能。

Conclusion: 该方法有效提升了复杂场景中人体运动预测的准确性。

Abstract: Complex scenes present significant challenges for predicting human behaviour
due to the abundance of interaction information, such as human-human and
humanenvironment interactions. These factors complicate the analysis and
understanding of human behaviour, thereby increasing the uncertainty in
forecasting human motions. Existing motion prediction methods thus struggle in
these complex scenarios. In this paper, we propose an effective method for
human motion forecasting in interactive scenes. To achieve a comprehensive
representation of interactions, we design a hierarchical interaction feature
representation so that high-level features capture the overall context of the
interactions, while low-level features focus on fine-grained details. Besides,
we propose a coarse-to-fine interaction reasoning module that leverages both
spatial and frequency perspectives to efficiently utilize hierarchical
features, thereby enhancing the accuracy of motion predictions. Our method
achieves state-of-the-art performance across four public datasets. Code will be
released when this paper is published.

</details>


### [81] [CoLa: Chinese Character Decomposition with Compositional Latent Components](https://arxiv.org/abs/2506.03798)
*Fan Shi,Haiyang Yu,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 论文提出了一种名为CoLa的深度潜在变量模型，通过学习汉字的组合潜在成分，无需依赖人工定义的分解方案，实现了零样本汉字识别。


<details>
  <summary>Details</summary>
Motivation: 人类能够通过分解和重组汉字的组成部分来识别未见过的汉字，这体现了组合性和学习能力。现有方法依赖人工定义的分解方案，忽略了学习能力，限制了泛化能力。

Method: 提出了一种深度潜在变量模型（CoLa），学习汉字的组合潜在成分，并在潜在空间中进行识别和匹配。

Result: CoLa在零样本汉字识别任务中优于现有方法，且学习到的成分能够以可解释的方式反映汉字结构。

Conclusion: CoLa不仅提升了零样本汉字识别的性能，还展示了跨数据集的泛化能力，为汉字识别提供了新的思路。

Abstract: Humans can decompose Chinese characters into compositional components and
recombine them to recognize unseen characters. This reflects two cognitive
principles: Compositionality, the idea that complex concepts are built on
simpler parts; and Learning-to-learn, the ability to learn strategies for
decomposing and recombining components to form new concepts. These principles
provide inductive biases that support efficient generalization. They are
critical to Chinese character recognition (CCR) in solving the zero-shot
problem, which results from the common long-tail distribution of Chinese
character datasets. Existing methods have made substantial progress in modeling
compositionality via predefined radical or stroke decomposition. However, they
often ignore the learning-to-learn capability, limiting their ability to
generalize beyond human-defined schemes. Inspired by these principles, we
propose a deep latent variable model that learns Compositional Latent
components of Chinese characters (CoLa) without relying on human-defined
decomposition schemes. Recognition and matching can be performed by comparing
compositional latent components in the latent space, enabling zero-shot
character recognition. The experiments illustrate that CoLa outperforms
previous methods in both character the radical zero-shot CCR. Visualization
indicates that the learned components can reflect the structure of characters
in an interpretable way. Moreover, despite being trained on historical
documents, CoLa can analyze components of oracle bone characters, highlighting
its cross-dataset generalization ability.

</details>


### [82] [ConText: Driving In-context Learning for Text Removal and Segmentation](https://arxiv.org/abs/2506.03799)
*Fei Zhang,Pei Zhang,Baosong Yang,Fei Huang,Yanfeng Wang,Ya Zhang*

Main category: cs.CV

TL;DR: 论文首次将视觉上下文学习（V-ICL）应用于光学字符识别任务，提出任务链式组合器和上下文感知聚合方法，解决了单步推理和视觉异质性问题，最终提出的ConText模型在多个基准测试中达到新最优。


<details>
  <summary>Details</summary>
Motivation: 现有V-ICL方法采用直接提示的单步推理方式，限制了模型性能。视觉异质性也增加了任务难度，需改进方法以提升推理能力。

Method: 提出任务链式组合器（image-removal-segmentation）和上下文感知聚合，增强推理中间步骤；采用自提示策略解决视觉异质性问题。

Result: ConText模型在领域内和领域外基准测试中均达到最优性能。

Conclusion: 通过改进提示设计和解决视觉异质性，ConText模型显著提升了光学字符识别任务的性能。

Abstract: This paper presents the first study on adapting the visual in-context
learning (V-ICL) paradigm to optical character recognition tasks, specifically
focusing on text removal and segmentation. Most existing V-ICL generalists
employ a reasoning-as-reconstruction approach: they turn to using a
straightforward image-label compositor as the prompt and query input, and then
masking the query label to generate the desired output. This direct prompt
confines the model to a challenging single-step reasoning process. To address
this, we propose a task-chaining compositor in the form of
image-removal-segmentation, providing an enhanced prompt that elicits reasoning
with enriched intermediates. Additionally, we introduce context-aware
aggregation, integrating the chained prompt pattern into the latent query
representation, thereby strengthening the model's in-context reasoning. We also
consider the issue of visual heterogeneity, which complicates the selection of
homogeneous demonstrations in text recognition. Accordingly, this is
effectively addressed through a simple self-prompting strategy, preventing the
model's in-context learnability from devolving into specialist-like,
context-free inference. Collectively, these insights culminate in our ConText
model, which achieves new state-of-the-art across both in- and out-of-domain
benchmarks. The code is available at https://github.com/Ferenas/ConText.

</details>


### [83] [Animal Pose Labeling Using General-Purpose Point Trackers](https://arxiv.org/abs/2506.03868)
*Zhuoyang Pan,Boxiao Pan,Guandao Yang,Adam W. Harley,Leonidas Guibas*

Main category: cs.CV

TL;DR: 提出了一种基于测试时优化的动物姿态标注方法，通过微调预训练的点跟踪器，在稀疏标注帧上实现高效自动标注。


<details>
  <summary>Details</summary>
Motivation: 现有方法因数据集不全面而不可靠，而收集全面数据集又因动物形态多样性而困难。

Method: 在稀疏标注帧上微调轻量级外观嵌入的预训练点跟踪器，用于自动标注其余帧。

Result: 方法在合理标注成本下达到最优性能。

Conclusion: 该流程为动物行为的自动量化提供了有价值的工具。

Abstract: Automatically estimating animal poses from videos is important for studying
animal behaviors. Existing methods do not perform reliably since they are
trained on datasets that are not comprehensive enough to capture all necessary
animal behaviors. However, it is very challenging to collect such datasets due
to the large variations in animal morphology. In this paper, we propose an
animal pose labeling pipeline that follows a different strategy, i.e. test time
optimization. Given a video, we fine-tune a lightweight appearance embedding
inside a pre-trained general-purpose point tracker on a sparse set of annotated
frames. These annotations can be obtained from human labelers or off-the-shelf
pose detectors. The fine-tuned model is then applied to the rest of the frames
for automatic labeling. Our method achieves state-of-the-art performance at a
reasonable annotation cost. We believe our pipeline offers a valuable tool for
the automatic quantification of animal behavior. Visit our project webpage at
https://zhuoyang-pan.github.io/animal-labeling.

</details>


### [84] [JointSplat: Probabilistic Joint Flow-Depth Optimization for Sparse-View Gaussian Splatting](https://arxiv.org/abs/2506.03872)
*Yang Xiao,Guoan Xu,Qiang Wu,Wenjing Jia*

Main category: cs.CV

TL;DR: JointSplat提出了一种结合光流和深度的统一框架，通过概率优化机制解决稀疏视角3D重建中的问题，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 稀疏视角3D重建在低纹理或重复区域存在定位错误和伪影问题，而光流-深度联合估计则因缺乏真实光流监督而受噪声和不一致性影响。

Method: 提出JointSplat框架，通过概率优化机制在像素级别融合光流和深度信息，并引入多视角深度一致性损失。

Result: 在RealEstate10K和ACID数据集上，JointSplat表现优于现有方法。

Conclusion: JointSplat通过概率联合优化光流和深度，实现了高保真稀疏视角3D重建。

Abstract: Reconstructing 3D scenes from sparse viewpoints is a long-standing challenge
with wide applications. Recent advances in feed-forward 3D Gaussian sparse-view
reconstruction methods provide an efficient solution for real-time novel view
synthesis by leveraging geometric priors learned from large-scale multi-view
datasets and computing 3D Gaussian centers via back-projection. Despite
offering strong geometric cues, both feed-forward multi-view depth estimation
and flow-depth joint estimation face key limitations: the former suffers from
mislocation and artifact issues in low-texture or repetitive regions, while the
latter is prone to local noise and global inconsistency due to unreliable
matches when ground-truth flow supervision is unavailable. To overcome this, we
propose JointSplat, a unified framework that leverages the complementarity
between optical flow and depth via a novel probabilistic optimization
mechanism. Specifically, this pixel-level mechanism scales the information
fusion between depth and flow based on the matching probability of optical flow
during training. Building upon the above mechanism, we further propose a novel
multi-view depth-consistency loss to leverage the reliability of supervision
while suppressing misleading gradients in uncertain areas. Evaluated on
RealEstate10K and ACID, JointSplat consistently outperforms state-of-the-art
(SOTA) methods, demonstrating the effectiveness and robustness of our proposed
probabilistic joint flow-depth optimization approach for high-fidelity
sparse-view 3D reconstruction.

</details>


### [85] [Video, How Do Your Tokens Merge?](https://arxiv.org/abs/2506.03885)
*Sam Pollard,Michael Wray*

Main category: cs.CV

TL;DR: 论文探讨了无需重新训练的视频令牌合并方法，在保持精度的同时显著提升计算速度。


<details>
  <summary>Details</summary>
Motivation: 视频Transformer模型因输入时空尺度问题需要大量计算资源，现有方法多针对图像模型，视频令牌合并尚未在复杂时序数据集上验证。

Method: 研究了四种视频Transformer模型在三个数据集上的训练免费令牌合并方法，涵盖粗粒度和细粒度动作识别。

Result: 视频令牌合并实现了约2.5倍加速，精度损失平均仅-0.55%（ViViT模型）。

Conclusion: 视频令牌合并是一种高效且无需重新训练的方法，适用于多种视频Transformer模型。

Abstract: Video transformer models require huge amounts of compute resources due to the
spatio-temporal scaling of the input. Tackling this, recent methods have
proposed to drop or merge tokens for image models, whether randomly or via
learned methods. Merging tokens has many benefits: it can be plugged into any
vision transformer, does not require model re-training, and it propagates
information that would otherwise be dropped through the model. Before now,
video token merging has not been evaluated on temporally complex datasets for
video understanding. In this work, we explore training-free token merging for
video to provide comprehensive experiments and find best practices across four
video transformers on three datasets that exhibit coarse and fine-grained
action recognition. Our results showcase the benefits of video token merging
with a speedup of around $2.5$X while maintaining accuracy (avg. $-0.55\%$ for
ViViT). Code available at
https://github.com/sjpollard/video-how-do-your-tokens-merge.

</details>


### [86] [Joint Video Enhancement with Deblurring, Super-Resolution, and Frame Interpolation Network](https://arxiv.org/abs/2506.03892)
*Giyong Choi,HyunWook Park*

Main category: cs.CV

TL;DR: 提出了一种联合视频增强方法DSFN，通过同时解决多个视频退化因素，直接从低分辨率、低帧率和模糊视频生成高质量视频。


<details>
  <summary>Details</summary>
Motivation: 视频质量通常受多种因素共同影响，而传统的顺序增强方法效率低下且效果不佳。

Method: DSFN网络包含联合去模糊和超分辨率模块（JDSR）以及基于三帧的帧插值模块（TFBFI），共同处理多种退化问题。

Result: 实验表明，DSFN在公共数据集上优于其他顺序方法，且网络规模更小、处理速度更快。

Conclusion: DSFN通过联合处理多种退化因素，显著提升了视频增强的效果和效率。

Abstract: Video quality is often severely degraded by multiple factors rather than a
single factor. These low-quality videos can be restored to high-quality videos
by sequentially performing appropriate video enhancement techniques. However,
the sequential approach was inefficient and sub-optimal because most video
enhancement approaches were designed without taking into account that multiple
factors together degrade video quality. In this paper, we propose a new joint
video enhancement method that mitigates multiple degradation factors
simultaneously by resolving an integrated enhancement problem. Our proposed
network, named DSFN, directly produces a high-resolution, high-frame-rate, and
clear video from a low-resolution, low-frame-rate, and blurry video. In the
DSFN, low-resolution and blurry input frames are enhanced by a joint deblurring
and super-resolution (JDSR) module. Meanwhile, intermediate frames between
input adjacent frames are interpolated by a triple-frame-based frame
interpolation (TFBFI) module. The proper combination of the proposed modules of
DSFN can achieve superior performance on the joint video enhancement task.
Experimental results show that the proposed method outperforms other sequential
state-of-the-art techniques on public datasets with a smaller network size and
faster processing time.

</details>


### [87] [Learning from Noise: Enhancing DNNs for Event-Based Vision through Controlled Noise Injection](https://arxiv.org/abs/2506.03918)
*Marcin Kowalczyk,Kamil Jeziorek,Tomasz Kryjak*

Main category: cs.CV

TL;DR: 论文提出了一种新颖的噪声注入训练方法，旨在提升神经网络对事件数据噪声的鲁棒性，优于传统的事件过滤技术。


<details>
  <summary>Details</summary>
Motivation: 事件数据常受噪声影响，传统过滤方法可能丢失有效信息，因此需要一种更有效的方法提升模型鲁棒性。

Method: 通过向训练数据中注入可控噪声，使模型学习噪声鲁棒性表示。

Result: 实验表明，该方法在多种噪声强度下表现稳定，分类准确率最高，优于传统过滤方法。

Conclusion: 噪声注入训练是事件数据分类系统中传统过滤方法的可行替代方案。

Abstract: Event-based sensors offer significant advantages over traditional frame-based
cameras, especially in scenarios involving rapid motion or challenging lighting
conditions. However, event data frequently suffers from considerable noise,
negatively impacting the performance and robustness of deep learning models.
Traditionally, this problem has been addressed by applying filtering algorithms
to the event stream, but this may also remove some of relevant data. In this
paper, we propose a novel noise-injection training methodology designed to
enhance the neural networks robustness against varying levels of event noise.
Our approach introduces controlled noise directly into the training data,
enabling models to learn noise-resilient representations. We have conducted
extensive evaluations of the proposed method using multiple benchmark datasets
(N-Caltech101, N-Cars, and Mini N-ImageNet) and various network architectures,
including Convolutional Neural Networks, Vision Transformers, Spiking Neural
Networks, and Graph Convolutional Networks. Experimental results show that our
noise-injection training strategy achieves stable performance over a range of
noise intensities, consistently outperforms event-filtering techniques, and
achieves the highest average classification accuracy, making it a viable
alternative to traditional event-data filtering methods in an object
classification system. Code: https://github.com/vision-agh/DVS_Filtering

</details>


### [88] [Multiple Stochastic Prompt Tuning for Practical Cross-Domain Few Shot Learning](https://arxiv.org/abs/2506.03926)
*Debarshi Brahma,Soma Biswas*

Main category: cs.CV

TL;DR: 提出了一种实用的跨域少样本学习（pCDFSL）任务，利用预训练模型CLIP处理极端领域偏移下的未见类别分类，并提出MIST框架，通过多随机提示调优解决领域和语义偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有CDFSL框架依赖人工设计的训练和测试模式，难以适应真实场景。pCDFSL任务旨在解决这一问题，使其更具挑战性和实用性。

Method: 提出MIST框架，为每个类别学习多个随机提示，捕捉输入数据的多峰分布，并将提示权重建模为可学习的高斯分布，以减少过拟合。

Result: 在四个CDFSL基准测试中，MIST框架表现优于现有方法，验证了其有效性。

Conclusion: MIST框架通过多随机提示和高斯分布建模，显著提升了跨域少样本学习的性能，适用于真实场景。

Abstract: In this work, we propose a practical cross-domain few-shot learning (pCDFSL)
task, where a large-scale pre-trained model like CLIP can be easily deployed on
a target dataset. The goal is to simultaneously classify all unseen classes
under extreme domain shifts, by utilizing only a few labeled samples per class.
The pCDFSL paradigm is source-free and moves beyond artificially created
episodic training and testing regimes followed by existing CDFSL frameworks,
making it more challenging and relevant to real-world applications. Towards
that goal, we propose a novel framework, termed MIST (MultIple STochastic
Prompt tuning), where multiple stochastic prompts are utilized to handle
significant domain and semantic shifts. Specifically, multiple prompts are
learnt for each class, effectively capturing multiple peaks in the input data.
Furthermore, instead of representing the weights of the multiple prompts as
point-estimates, we model them as learnable Gaussian distributions with two
different strategies, encouraging an efficient exploration of the prompt
parameter space, which mitigate overfitting due to the few labeled training
samples. Extensive experiments and comparison with the state-of-the-art methods
on four CDFSL benchmarks adapted to this setting, show the effectiveness of the
proposed framework.

</details>


### [89] [Vision Remember: Alleviating Visual Forgetting in Efficient MLLM with Vision Feature Resample](https://arxiv.org/abs/2506.03928)
*Ze Feng,Jiang-Jiang Liu,Sen Yang,Lingyu Xiao,Xiaofan Li,Wankou Yang,Jingdong Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为Vision Remember的方法，通过在LLM解码层之间插入模块，重新记忆视觉特征，以解决视觉信息压缩导致的细节丢失问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉投影器压缩视觉标记会丢失细节信息，尤其是在需要细粒度空间关系的任务中。

Method: 提出Vision Remember模块，保留多级视觉特征并通过局部注意力重新采样，增强细节和空间关系。

Result: 在多个视觉理解基准测试中表现优异，结合高效视觉投影器时性能提升且不牺牲效率。

Conclusion: Vision Remember方法有效解决了视觉信息压缩问题，LLaVA-VR模型在参数较少的情况下优于其他大型MLLM。

Abstract: In this work, we study the Efficient Multimodal Large Language Model.
Redundant vision tokens consume a significant amount of computational memory
and resources. Therefore, many previous works compress them in the Vision
Projector to reduce the number of vision tokens. However, simply compressing in
the Vision Projector can lead to the loss of visual information, especially for
tasks that rely on fine-grained spatial relationships, such as OCR and Chart \&
Table Understanding. To address this problem, we propose Vision Remember, which
is inserted between the LLM decoder layers to allow vision tokens to
re-memorize vision features. Specifically, we retain multi-level vision
features and resample them with the vision tokens that have interacted with the
text token. During the resampling process, each vision token only attends to a
local region in vision features, which is referred to as saliency-enhancing
local attention. Saliency-enhancing local attention not only improves
computational efficiency but also captures more fine-grained contextual
information and spatial relationships within the region. Comprehensive
experiments on multiple visual understanding benchmarks validate the
effectiveness of our method when combined with various Efficient Vision
Projectors, showing performance gains without sacrificing efficiency. Based on
Vision Remember, LLaVA-VR with only 2B parameters is also superior to previous
representative MLLMs such as Tokenpacker-HD-7B and DeepSeek-VL-7B.

</details>


### [90] [DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision Language Models](https://arxiv.org/abs/2506.03933)
*Jia Fu,Yongtao Wu,Yihang Chen,Kunyu Peng,Xiao Zhang,Volkan Cevher,Sepideh Pashami,Anders Holst*

Main category: cs.CV

TL;DR: DiffCAP是一种基于扩散的净化策略，能有效中和视觉语言模型（VLMs）中的对抗性扰动，显著提升模型在对抗环境中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管VLMs在多模态理解中表现出色，但其对扰动的敏感性威胁了实际应用的可靠性。DiffCAP旨在解决这一问题。

Method: DiffCAP通过逐步注入高斯噪声中和对抗性扰动，并使用预训练的扩散模型去噪，恢复干净的输入表示。

Result: 实验表明，DiffCAP在多种数据集和任务中显著优于现有防御技术，同时减少了超参数调整和去噪时间。

Conclusion: DiffCAP为在对抗环境中安全部署VLMs提供了稳健且实用的解决方案。

Abstract: Vision Language Models (VLMs) have shown remarkable capabilities in
multimodal understanding, yet their susceptibility to perturbations poses a
significant threat to their reliability in real-world applications. Despite
often being imperceptible to humans, these perturbations can drastically alter
model outputs, leading to erroneous interpretations and decisions. This paper
introduces DiffCAP, a novel diffusion-based purification strategy that can
effectively neutralize adversarial corruptions in VLMs. We observe that adding
minimal noise to an adversarially corrupted image significantly alters its
latent embedding with respect to VLMs. Building on this insight, DiffCAP
cumulatively injects random Gaussian noise into adversarially perturbed input
data. This process continues until the embeddings of two consecutive noisy
images reach a predefined similarity threshold, indicating a potential approach
to neutralize the adversarial effect. Subsequently, a pretrained diffusion
model is employed to denoise the stabilized image, recovering a clean
representation suitable for the VLMs to produce an output. Through extensive
experiments across six datasets with three VLMs under varying attack strengths
in three task scenarios, we show that DiffCAP consistently outperforms existing
defense techniques by a substantial margin. Notably, DiffCAP significantly
reduces both hyperparameter tuning complexity and the required diffusion time,
thereby accelerating the denoising process. Equipped with strong theoretical
and empirical support, DiffCAP provides a robust and practical solution for
securely deploying VLMs in adversarial environments.

</details>


### [91] [Average Calibration Losses for Reliable Uncertainty in Medical Image Segmentation](https://arxiv.org/abs/2506.03942)
*Theodore Barfoot,Luis C. Garcia-Peraza-Herrera,Samet Akcay,Ben Glocker,Tom Vercauteren*

Main category: cs.CV

TL;DR: 论文提出了一种可微分的mL1-ACE损失函数，用于改善医学图像分割中的校准误差，实验显示其显著降低了ACE和MCE，同时保持了较高的DSC。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在医学图像分割中过度自信的问题，以提高预测的可信度和临床实用性。

Method: 提出硬分箱和软分箱的mL1-ACE损失函数，作为辅助损失函数，并在四个数据集上验证其效果。

Result: 软分箱mL1-ACE显著改善了校准性能，但可能影响分割性能；硬分箱mL1-ACE保持了分割性能，但校准改进较弱。

Conclusion: 该方法提升了分割预测的可信度，有助于深度学习在临床中的安全应用。

Abstract: Deep neural networks for medical image segmentation are often overconfident,
compromising both reliability and clinical utility. In this work, we propose
differentiable formulations of marginal L1 Average Calibration Error (mL1-ACE)
as an auxiliary loss that can be computed on a per-image basis. We compare both
hard- and soft-binning approaches to directly improve pixel-wise calibration.
Our experiments on four datasets (ACDC, AMOS, KiTS, BraTS) demonstrate that
incorporating mL1-ACE significantly reduces calibration errors, particularly
Average Calibration Error (ACE) and Maximum Calibration Error (MCE), while
largely maintaining high Dice Similarity Coefficients (DSCs). We find that the
soft-binned variant yields the greatest improvements in calibration, over the
Dice plus cross-entropy loss baseline, but often compromises segmentation
performance, with hard-binned mL1-ACE maintaining segmentation performance,
albeit with weaker calibration improvement. To gain further insight into
calibration performance and its variability across an imaging dataset, we
introduce dataset reliability histograms, an aggregation of per-image
reliability diagrams. The resulting analysis highlights improved alignment
between predicted confidences and true accuracies. Overall, our approach not
only enhances the trustworthiness of segmentation predictions but also shows
potential for safer integration of deep learning methods into clinical
workflows. We share our code here:
https://github.com/cai4cai/Average-Calibration-Losses

</details>


### [92] [MS-YOLO: A Multi-Scale Model for Accurate and Efficient Blood Cell Detection](https://arxiv.org/abs/2506.03972)
*Guohua Wu,Shengqi Chen,Pengchao Deng,Wenting Yu*

Main category: cs.CV

TL;DR: MS-YOLO是一种基于YOLOv11的多尺度血细胞检测模型，通过三种关键架构创新提升了检测性能，尤其在重叠细胞和多尺度目标检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统手动显微镜方法效率低且准确性不足，现有自动化检测方法成本高且精度不理想，深度学习在该领域仍面临重叠细胞和多尺度目标检测的挑战。

Method: 提出MS-YOLO模型，包含多尺度扩张残差模块（MS-DRM）、动态跨路径特征增强模块（DCFEM）和轻量自适应权重下采样模块（LADS）。

Result: 在CBC基准测试中，MS-YOLO的mAP@50达到97.4%，优于现有模型，并在WBCDD数据集上验证了其泛化能力。

Conclusion: MS-YOLO具有轻量级架构和实时推理效率，满足临床部署需求，为标准化血液病理评估提供了可靠技术支持。

Abstract: Complete blood cell detection holds significant value in clinical
diagnostics. Conventional manual microscopy methods suffer from time
inefficiency and diagnostic inaccuracies. Existing automated detection
approaches remain constrained by high deployment costs and suboptimal accuracy.
While deep learning has introduced powerful paradigms to this field, persistent
challenges in detecting overlapping cells and multi-scale objects hinder
practical deployment. This study proposes the multi-scale YOLO (MS-YOLO), a
blood cell detection model based on the YOLOv11 framework, incorporating three
key architectural innovations to enhance detection performance. Specifically,
the multi-scale dilated residual module (MS-DRM) replaces the original C3K2
modules to improve multi-scale discriminability; the dynamic cross-path feature
enhancement module (DCFEM) enables the fusion of hierarchical features from the
backbone with aggregated features from the neck to enhance feature
representations; and the light adaptive-weight downsampling module (LADS)
improves feature downsampling through adaptive spatial weighting while reducing
computational complexity. Experimental results on the CBC benchmark demonstrate
that MS-YOLO achieves precise detection of overlapping cells and multi-scale
objects, particularly small targets such as platelets, achieving an mAP@50 of
97.4% that outperforms existing models. Further validation on the supplementary
WBCDD dataset confirms its robust generalization capability. Additionally, with
a lightweight architecture and real-time inference efficiency, MS-YOLO meets
clinical deployment requirements, providing reliable technical support for
standardized blood pathology assessment.

</details>


### [93] [RAID: A Dataset for Testing the Adversarial Robustness of AI-Generated Image Detectors](https://arxiv.org/abs/2506.03988)
*Hicham Eddoubi,Jonas Ricker,Federico Cocchi,Lorenzo Baraldi,Angelo Sotgiu,Maura Pintor,Marcella Cornia,Lorenzo Baraldi,Asja Fischer,Rita Cucchiara,Battista Biggio*

Main category: cs.CV

TL;DR: 论文提出了一种评估AI生成图像检测器鲁棒性的方法，并发布了一个包含72k对抗样本的数据集RAID，实验表明当前检测器易受对抗样本欺骗。


<details>
  <summary>Details</summary>
Motivation: AI生成图像质量高，人类难以区分，检测其真实性成为迫切需求，但现有方法在对抗鲁棒性上缺乏评估。

Method: 提出RAID数据集，通过攻击七种先进检测器和四种文本生成图像模型，生成多样且高迁移性的对抗样本。

Result: 实验显示对抗样本能高效欺骗未见过的新检测器，表明当前检测器鲁棒性不足。

Conclusion: 强调开发更鲁棒的检测方法的必要性，并公开了数据集和评估代码。

Abstract: AI-generated images have reached a quality level at which humans are
incapable of reliably distinguishing them from real images. To counteract the
inherent risk of fraud and disinformation, the detection of AI-generated images
is a pressing challenge and an active research topic. While many of the
presented methods claim to achieve high detection accuracy, they are usually
evaluated under idealized conditions. In particular, the adversarial robustness
is often neglected, potentially due to a lack of awareness or the substantial
effort required to conduct a comprehensive robustness analysis. In this work,
we tackle this problem by providing a simpler means to assess the robustness of
AI-generated image detectors. We present RAID (Robust evaluation of
AI-generated image Detectors), a dataset of 72k diverse and highly transferable
adversarial examples. The dataset is created by running attacks against an
ensemble of seven state-of-the-art detectors and images generated by four
different text-to-image models. Extensive experiments show that our methodology
generates adversarial images that transfer with a high success rate to unseen
detectors, which can be used to quickly provide an approximate yet still
reliable estimate of a detector's adversarial robustnessOur findings indicate
that current state-of-the-art AI-generated image detectors can be easily
deceived by adversarial examples, highlighting the critical need for the
development of more robust methods. We release our dataset at
https://huggingface.co/datasets/aimagelab/RAID and evaluation code at
https://github.com/pralab/RAID.

</details>


### [94] [Vocabulary-free few-shot learning for Vision-Language Models](https://arxiv.org/abs/2506.04005)
*Maxime Zanella,Clément Fuchs,Ismail Ben Ayed,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: 论文提出了一种无需词汇表的少样本学习方法（SiM），通过相似性映射分类目标实例，无需依赖预定义的类别名称。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义的类别名称，限制了其适用性，尤其是在类别名称难以获取的场景。

Method: 提出Similarity Mapping（SiM），基于通用提示（文本或视觉）的相似性分数分类目标实例。

Result: SiM表现优异，计算高效（学习映射通常不到一秒），且具有可解释性。

Conclusion: SiM为无需词汇表的少样本学习提供了重要基线，未来研究可在此基础上展开。

Abstract: Recent advances in few-shot adaptation for Vision-Language Models (VLMs) have
greatly expanded their ability to generalize across tasks using only a few
labeled examples. However, existing approaches primarily build upon the strong
zero-shot priors of these models by leveraging carefully designed,
task-specific prompts. This dependence on predefined class names can restrict
their applicability, especially in scenarios where exact class names are
unavailable or difficult to specify. To address this limitation, we introduce
vocabulary-free few-shot learning for VLMs, a setting where target class
instances - that is, images - are available but their corresponding names are
not. We propose Similarity Mapping (SiM), a simple yet effective baseline that
classifies target instances solely based on similarity scores with a set of
generic prompts (textual or visual), eliminating the need for carefully
handcrafted prompts. Although conceptually straightforward, SiM demonstrates
strong performance, operates with high computational efficiency (learning the
mapping typically takes less than one second), and provides interpretability by
linking target classes to generic prompts. We believe that our approach could
serve as an important baseline for future research in vocabulary-free few-shot
learning. Code is available at
https://github.com/MaxZanella/vocabulary-free-FSL.

</details>


### [95] [Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.04034)
*Qing Jiang,Xingyu Chen,Zhaoyang Zeng,Junzhi Yu,Lei Zhang*

Main category: cs.CV

TL;DR: Rex-Thinker将对象引用任务转化为显式的CoT推理任务，通过结构化推理提高模型的解释性和可靠性，并在实验中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有对象引用模型缺乏解释性和对无匹配对象的拒绝能力，Rex-Thinker旨在通过显式推理解决这些问题。

Method: 将对象引用任务分解为候选对象识别和逐步推理评估，构建HumanRef-CoT数据集支持训练，采用两阶段训练（监督微调和GRPO强化学习）。

Result: Rex-Thinker在精度和解释性上优于基线方法，能更好地拒绝无匹配输出，并表现出良好的泛化能力。

Conclusion: 通过结构化推理和两阶段训练，Rex-Thinker显著提升了对象引用任务的性能和可靠性。

Abstract: Object referring aims to detect all objects in an image that match a given
natural language description. We argue that a robust object referring model
should be grounded, meaning its predictions should be both explainable and
faithful to the visual content. Specifically, it should satisfy two key
properties: 1) Verifiable, by producing interpretable reasoning that justifies
its predictions and clearly links them to visual evidence; and 2) Trustworthy,
by learning to abstain when no object in the image satisfies the given
expression. However, most methods treat referring as a direct bounding box
prediction task, offering limited interpretability and struggling to reject
expressions with no matching object. In this work, we propose Rex-Thinker, a
model that formulates object referring as an explicit CoT reasoning task. Given
a referring expression, we first identify all candidate object instances
corresponding to the referred object category. Rex-Thinker then performs
step-by-step reasoning over each candidate to assess whether it matches the
given expression, before making a final prediction. To support this paradigm,
we construct a large-scale CoT-style referring dataset named HumanRef-CoT by
prompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a
structured planning, action, and summarization format, enabling the model to
learn decomposed, interpretable reasoning over object candidates. We then train
Rex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach
the model how to perform structured reasoning, followed by GRPO-based RL
learning to improve accuracy and generalization. Experiments show that our
approach outperforms standard baselines in both precision and interpretability
on in-domain evaluation, while also demonstrating improved ability to reject
hallucinated outputs and strong generalization in out-of-domain settings.

</details>


### [96] [Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization](https://arxiv.org/abs/2506.04039)
*Jiulong Wu,Zhengliang Shi,Shuaiqiang Wang,Jizhou Huang,Dawei Yin,Lingyong Yan,Min Cao,Min Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为EMPO的新方法，通过实体中心的多模态偏好优化，解决了大型视觉语言模型（LVLM）中的幻觉问题，显著降低了幻觉率。


<details>
  <summary>Details</summary>
Motivation: 现有偏好对齐方法忽视了图像-文本模态对齐，导致模型过度依赖LLM并产生幻觉，EMPO旨在解决这一问题。

Method: 利用开源指令数据集自动构建高质量多模态偏好数据，涵盖图像、指令和响应三个方面，实现模态对齐。

Result: 在两个人类偏好数据集和五个多模态幻觉基准测试中，EMPO显著降低幻觉率，如Object-HalBench上减少85.9%，MM-HalBench上减少49.8%。

Conclusion: EMPO通过增强模态对齐，有效减少了LVLM的幻觉问题，提升了模型的可信度。

Abstract: Large Visual Language Models (LVLMs) have demonstrated impressive
capabilities across multiple tasks. However, their trustworthiness is often
challenged by hallucinations, which can be attributed to the modality
misalignment and the inherent hallucinations of their underlying Large Language
Models (LLMs) backbone. Existing preference alignment methods focus on aligning
model responses with human preferences while neglecting image-text modality
alignment, resulting in over-reliance on LLMs and hallucinations. In this
paper, we propose Entity-centric Multimodal Preference Optimization (EMPO),
which achieves enhanced modality alignment than existing human preference
alignment methods. Besides, to overcome the scarcity of high-quality multimodal
preference data, we utilize open-source instruction datasets to automatically
construct high-quality preference data across three aspects: image,
instruction, and response. Experiments on two human preference datasets and
five multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,
e.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on
MM-HalBench.

</details>


### [97] [EV-Flying: an Event-based Dataset for In-The-Wild Recognition of Flying Objects](https://arxiv.org/abs/2506.04048)
*Gabriele Magrini,Federico Becattini,Giovanni Colombo,Pietro Pala*

Main category: cs.CV

TL;DR: 论文探讨了基于事件视觉的飞行物体检测与识别方法，提出EV-Flying数据集和点云处理方法，解决了传统RGB方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统RGB方法在检测飞行物体时面临尺度变化、运动模糊和高速运动等挑战，尤其是对小飞行物体如昆虫和无人机。事件相机因其高时间分辨率和低延迟特性，更适合此类任务。

Method: 提出EV-Flying数据集，包含手动标注的鸟类、昆虫和无人机数据。采用基于点云的轻量级架构（受PointNet启发）处理异步事件流。

Result: 研究验证了点云事件表示在飞行物体分类中的有效性，为实际场景中的高效识别提供了新方法。

Conclusion: EV-Flying数据集和点云处理方法为飞行物体识别提供了更高效可靠的解决方案，推动了事件视觉在相关领域的应用。

Abstract: Monitoring aerial objects is crucial for security, wildlife conservation, and
environmental studies. Traditional RGB-based approaches struggle with
challenges such as scale variations, motion blur, and high-speed object
movements, especially for small flying entities like insects and drones. In
this work, we explore the potential of event-based vision for detecting and
recognizing flying objects, in particular animals that may not follow short and
long-term predictable patters. Event cameras offer high temporal resolution,
low latency, and robustness to motion blur, making them well-suited for this
task. We introduce EV-Flying, an event-based dataset of flying objects,
comprising manually annotated birds, insects and drones with spatio-temporal
bounding boxes and track identities. To effectively process the asynchronous
event streams, we employ a point-based approach leveraging lightweight
architectures inspired by PointNet. Our study investigates the classification
of flying objects using point cloud-based event representations. The proposed
dataset and methodology pave the way for more efficient and reliable aerial
object recognition in real-world scenarios.

</details>


### [98] [Video Deblurring with Deconvolution and Aggregation Networks](https://arxiv.org/abs/2506.04054)
*Giyong Choi,HyunWook Park*

Main category: cs.CV

TL;DR: 提出了一种用于视频去模糊的解卷积和聚合网络（DAN），通过三个子网络有效利用相邻帧信息，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频去模糊算法未能充分利用相邻帧信息，导致性能不佳。

Method: DAN包含预处理网络（PPN）、基于对齐的解卷积网络（ABDN）和帧聚合网络（FAN），分别负责预处理、解卷积和聚合。

Result: 实验表明，DAN在公共数据集上的定量和定性评估均优于现有方法。

Conclusion: DAN通过合理结合三个子网络，有效利用相邻帧信息，实现了优越的视频去模糊性能。

Abstract: In contrast to single-image deblurring, video deblurring has the advantage
that neighbor frames can be utilized to deblur a target frame. However,
existing video deblurring algorithms often fail to properly employ the neighbor
frames, resulting in sub-optimal performance. In this paper, we propose a
deconvolution and aggregation network (DAN) for video deblurring that utilizes
the information of neighbor frames well. In DAN, both deconvolution and
aggregation strategies are achieved through three sub-networks: the
preprocessing network (PPN) and the alignment-based deconvolution network
(ABDN) for the deconvolution scheme; the frame aggregation network (FAN) for
the aggregation scheme. In the deconvolution part, blurry inputs are first
preprocessed by the PPN with non-local operations. Then, the output frames from
the PPN are deblurred by the ABDN based on the frame alignment. In the FAN,
these deblurred frames from the deconvolution part are combined into a latent
frame according to reliability maps which infer pixel-wise sharpness. The
proper combination of three sub-networks can achieve favorable performance on
video deblurring by using the neighbor frames suitably. In experiments, the
proposed DAN was demonstrated to be superior to existing state-of-the-art
methods through both quantitative and qualitative evaluations on the public
datasets.

</details>


### [99] [Point Cloud Quality Assessment Using the Perceptual Clustering Weighted Graph (PCW-Graph) and Attention Fusion Network](https://arxiv.org/abs/2506.04081)
*Abdelouahed Laazoufi,Mohammed El Hassouni,Hocine Cherifi*

Main category: cs.CV

TL;DR: 无参考点云质量评估（NR-PCQA）在缺乏参考模型的实际应用中至关重要。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，3D内容的质量评估常因缺乏参考模型而变得困难，因此需要无参考的评估方法。

Method: 未明确提及具体方法，但聚焦于无参考点云质量评估（NR-PCQA）。

Result: 未明确提及具体结果，但强调了NR-PCQA的重要性。

Conclusion: NR-PCQA在无参考模型的场景下对3D内容质量评估具有重要意义。

Abstract: No-Reference Point Cloud Quality Assessment (NR-PCQA) is critical for
evaluating 3D content in real-world applications where reference models are
unavailable.

</details>


### [100] [GlobalBuildingAtlas: An Open Global and Complete Dataset of Building Polygons, Heights and LoD1 3D Models](https://arxiv.org/abs/2506.04106)
*Xiao Xiang Zhu,Sining Chen,Fahong Zhang,Yilei Shi,Yuanyuan Wang*

Main category: cs.CV

TL;DR: GlobalBuildingAtlas是一个公开数据集，提供全球建筑的多边形、高度和LoD1 3D模型，覆盖全面且质量高。


<details>
  <summary>Details</summary>
Motivation: 填补全球范围内高质量、一致的建筑数据空白，支持高分辨率的地理空间分析。

Method: 利用机器学习从卫星数据中提取建筑多边形和高度，并通过质量融合策略优化数据。

Result: 数据集包含27.5亿建筑，多边形数据领先现有数据库10亿；高度数据分辨率达3x3米；LoD1模型覆盖26.8亿建筑，高度完整度97%。

Conclusion: GlobalBuildingAtlas为全球建筑现状提供新视角，支持联合国可持续发展目标监测。

Abstract: We introduce GlobalBuildingAtlas, a publicly available dataset providing
global and complete coverage of building polygons, heights and Level of Detail
1 (LoD1) 3D building models. This is the first open dataset to offer high
quality, consistent, and complete building data in 2D and 3D form at the
individual building level on a global scale. Towards this dataset, we developed
machine learning-based pipelines to derive building polygons and heights
(called GBA.Height) from global PlanetScope satellite data, respectively. Also
a quality-based fusion strategy was employed to generate higher-quality
polygons (called GBA.Polygon) based on existing open building polygons,
including our own derived one. With more than 2.75 billion buildings worldwide,
GBA.Polygon surpasses the most comprehensive database to date by more than 1
billion buildings. GBA.Height offers the most detailed and accurate global 3D
building height maps to date, achieving a spatial resolution of 3x3 meters-30
times finer than previous global products (90 m), enabling a high-resolution
and reliable analysis of building volumes at both local and global scales.
Finally, we generated a global LoD1 building model (called GBA.LoD1) from the
resulting GBA.Polygon and GBA.Height. GBA.LoD1 represents the first complete
global LoD1 building models, including 2.68 billion building instances with
predicted heights, i.e., with a height completeness of more than 97%, achieving
RMSEs ranging from 1.5 m to 8.9 m across different continents. With its height
accuracy, comprehensive global coverage and rich spatial details,
GlobalBuildingAltas offers novel insights on the status quo of global
buildings, which unlocks unprecedented geospatial analysis possibilities, as
showcased by a better illustration of where people live and a more
comprehensive monitoring of the progress on the 11th Sustainable Development
Goal of the United Nations.

</details>


### [101] [Multi-view Surface Reconstruction Using Normal and Reflectance Cues](https://arxiv.org/abs/2506.04115)
*Robin Bruneau,Baptiste Brument,Yvain Quéau,Jean Mélou,François Bernard Lauze,Jean-Denis Durou,Lilian Calvet*

Main category: cs.CV

TL;DR: 提出了一种结合多视角法线和反射率图的高保真3D表面重建框架，在复杂反射材料和稀疏视角下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂反射材料和稀疏视角下高保真3D表面重建的挑战。

Method: 采用像素级联合重参数化反射率和表面法线，将其表示为模拟变化光照下的辐射向量，并集成到标准重建流程中。

Result: 在MVPS基准数据集上达到最先进性能，尤其在细节重建和复杂可见性条件下表现突出。

Conclusion: 该方法在细节重建和鲁棒性上优于现有技术，代码和数据已开源。

Abstract: Achieving high-fidelity 3D surface reconstruction while preserving fine
details remains challenging, especially in the presence of materials with
complex reflectance properties and without a dense-view setup. In this paper,
we introduce a versatile framework that incorporates multi-view normal and
optionally reflectance maps into radiance-based surface reconstruction. Our
approach employs a pixel-wise joint re-parametrization of reflectance and
surface normals, representing them as a vector of radiances under simulated,
varying illumination. This formulation enables seamless incorporation into
standard surface reconstruction pipelines, such as traditional multi-view
stereo (MVS) frameworks or modern neural volume rendering (NVR) ones. Combined
with the latter, our approach achieves state-of-the-art performance on
multi-view photometric stereo (MVPS) benchmark datasets, including DiLiGenT-MV,
LUCES-MV and Skoltech3D. In particular, our method excels in reconstructing
fine-grained details and handling challenging visibility conditions. The
present paper is an extended version of the earlier conference paper by Brument
et al. (in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), 2024), featuring an accelerated and more robust
algorithm as well as a broader empirical evaluation. The code and data relative
to this article is available at https://github.com/RobinBruneau/RNb-NeuS2.

</details>


### [102] [Contour Errors: An Ego-Centric Metric for Reliable 3D Multi-Object Tracking](https://arxiv.org/abs/2506.04122)
*Sharang Kaul,Mario Berk,Thiemo Gerbich,Abhinav Valada*

Main category: cs.CV

TL;DR: 论文提出了一种新的度量标准Contour Errors（CEs），用于在3D场景中提升多目标追踪的匹配可靠性，相比传统2D指标（如IoU和CPD），显著减少了功能失效。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用（如自动驾驶）中，可靠的匹配对感知系统的准确性至关重要。传统2D指标在复杂3D场景中表现不佳，需要一种更功能相关的度量标准。

Method: 提出Contour Errors（CEs），通过比较自车坐标系中的边界框，从功能角度评估匹配。

Result: 在nuScenes数据集上的实验表明，CEs在追踪检测方法中显著优于2D IoU和CPD，减少了功能失效（FPs/FNs）80%（近距离）和60%（远距离）。

Conclusion: Contour Errors是一种更有效的3D追踪匹配度量标准，显著提升了感知系统的可靠性和安全性。

Abstract: Finding reliable matches is essential in multi-object tracking to ensure the
accuracy and reliability of perception systems in safety-critical applications
such as autonomous vehicles. Effective matching mitigates perception errors,
enhancing object identification and tracking for improved performance and
safety. However, traditional metrics such as Intersection over Union (IoU) and
Center Point Distances (CPDs), which are effective in 2D image planes, often
fail to find critical matches in complex 3D scenes. To address this limitation,
we introduce Contour Errors (CEs), an ego or object-centric metric for
identifying matches of interest in tracking scenarios from a functional
perspective. By comparing bounding boxes in the ego vehicle's frame, contour
errors provide a more functionally relevant assessment of object matches.
Extensive experiments on the nuScenes dataset demonstrate that contour errors
improve the reliability of matches over the state-of-the-art 2D IoU and CPD
metrics in tracking-by-detection methods. In 3D car tracking, our results show
that Contour Errors reduce functional failures (FPs/FNs) by 80% at close ranges
and 60% at far ranges compared to IoU in the evaluation stage.

</details>


### [103] [UniCUE: Unified Recognition and Generation Framework for Chinese Cued Speech Video-to-Speech Generation](https://arxiv.org/abs/2506.04134)
*Jinting Wang,Shan Yang,Li Liu*

Main category: cs.CV

TL;DR: UniCUE框架通过直接生成语音避免了中间文本的依赖，显著提升了CSV2S任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统CSV2S任务中因依赖中间文本导致的错误传播和时序不对齐问题。

Method: 提出UniCUE框架，包含细粒度语义对齐池、VisioPhonetic适配器和姿态感知视觉处理器。

Result: 在中文CS数据集上，Word Error Rate降低78.3%，唇语同步性提升32%。

Conclusion: UniCUE为CSV2S任务提供了高效统一的解决方案，显著优于传统方法。

Abstract: Cued Speech (CS) enhances lipreading through hand coding, providing precise
speech perception support for the hearing-impaired. CS Video-to-Speech
generation (CSV2S) task aims to convert the CS visual expressions (CS videos)
of hearing-impaired individuals into comprehensible speech signals. Direct
generation of speech from CS video (called single CSV2S) yields poor
performance due to insufficient CS data. Current research mostly focuses on CS
Recognition (CSR), which convert video content into linguistic text. Based on
this, one straightforward way of CSV2S is to combine CSR with a Text-to-Speech
system. This combined architecture relies on text as an intermediate medium for
stepwise cross-modal alignment, which may lead to error propagation and
temporal misalignment between speech and video dynamics. To address these
challenges, we propose a novel approach that directly generates speech from CS
videos without relying on intermediate text. Building upon this, we propose
UniCUE, the first unified framework for CSV2S, whose core innovation lies in
the integration of the CSR task that provides fine-grained visual-semantic
information to facilitate speech generation from CS videos. More precisely, (1)
a novel fine-grained semantic alignment pool to ensure precise mapping between
visual features and speech contents; (2) a VisioPhonetic adapter to bridge
cross-task representations, ensuring seamless compatibility between two
distinct tasks (i.e., CSV2S and CSR); (3) a pose-aware visual processor is
introduced to enhance fine-grained spatiotemporal correlations between lip and
hand movements in CS video. Experiments on our new established Chinese CS
dataset (14 cuers1: 8 hearing-impaired and 6 normal-hearing) show that our
UniCUE significantly reduces Word Error Rate by 78.3% and improves lip-speech
synchronization by 32% compared to the single CSV2S.

</details>


### [104] [MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos](https://arxiv.org/abs/2506.04141)
*Kejian Zhu,Zhuoran Jin,Hongbang Yuan,Jiachun Li,Shangqing Tu,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CV

TL;DR: MMR-V是一个新的视频多模态深度推理基准，要求模型进行长距离多帧推理和隐藏信息分析，现有模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准主要关注理解任务，缺乏对多帧证据定位和多模态推理能力的评估。

Method: 提出MMR-V基准，包含长距离多帧推理、超越感知的问题、人工标注任务和干扰项设计。

Result: 当前模型在多模态推理上表现较差（最佳模型准确率52.5%），推理增强策略效果有限。

Conclusion: MMR-V旨在推动多模态推理能力的研究，揭示当前模型与策略的不足。

Abstract: The sequential structure of videos poses a challenge to the ability of
multimodal large language models (MLLMs) to locate multi-frame evidence and
conduct multimodal reasoning. However, existing video benchmarks mainly focus
on understanding tasks, which only require models to match frames mentioned in
the question (hereafter referred to as "question frame") and perceive a few
adjacent frames. To address this gap, we propose MMR-V: A Benchmark for
Multimodal Deep Reasoning in Videos. The benchmark is characterized by the
following features. (1) Long-range, multi-frame reasoning: Models are required
to infer and analyze evidence frames that may be far from the question frame.
(2) Beyond perception: Questions cannot be answered through direct perception
alone but require reasoning over hidden information. (3) Reliability: All tasks
are manually annotated, referencing extensive real-world user understanding to
align with common perceptions. (4) Confusability: Carefully designed distractor
annotation strategies to reduce model shortcuts. MMR-V consists of 317 videos
and 1,257 tasks. Our experiments reveal that current models still struggle with
multi-modal reasoning; even the best-performing model, o4-mini, achieves only
52.5% accuracy. Additionally, current reasoning enhancement strategies
(Chain-of-Thought and scaling test-time compute) bring limited gains. Further
analysis indicates that the CoT demanded for multi-modal reasoning differs from
it in textual reasoning, which partly explains the limited performance gains.
We hope that MMR-V can inspire further research into enhancing multi-modal
reasoning capabilities.

</details>


### [105] [Person Re-Identification System at Semantic Level based on Pedestrian Attributes Ontology](https://arxiv.org/abs/2506.04143)
*Ngoc Q. Ly,Hieu N. M. Cao,Thi T. Nguyen*

Main category: cs.CV

TL;DR: 提出了一种统一的行人重识别系统，包含三个模块（PAO、Local MDCNN、IDS），通过语义信息解决属性不平衡问题，并在Market1501数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决行人重识别中的大规模数据集、不平衡数据、视角和细粒度属性等挑战，尤其是语义级局部特征和属性不平衡问题。

Method: 提出三个模块：Pedestrian Attribute Ontology（PAO）、Local Multi-task DCNN（Local MDCNN）和Imbalance Data Solver（IDS），利用属性内组关联和语义信息预过滤候选。

Result: 在Market1501数据集上表现优于现有方法。

Conclusion: 该系统通过模块间的相互支持有效解决了属性不平衡问题，提升了行人重识别的性能。

Abstract: Person Re-Identification (Re-ID) is a very important task in video
surveillance systems such as tracking people, finding people in public places,
or analysing customer behavior in supermarkets. Although there have been many
works to solve this problem, there are still remaining challenges such as
large-scale datasets, imbalanced data, viewpoint, fine grained data
(attributes), the Local Features are not employed at semantic level in online
stage of Re-ID task, furthermore, the imbalanced data problem of attributes are
not taken into consideration. This paper has proposed a Unified Re-ID system
consisted of three main modules such as Pedestrian Attribute Ontology (PAO),
Local Multi-task DCNN (Local MDCNN), Imbalance Data Solver (IDS). The new main
point of our Re-ID system is the power of mutual support of PAO, Local MDCNN
and IDS to exploit the inner-group correlations of attributes and pre-filter
the mismatch candidates from Gallery set based on semantic information as
Fashion Attributes and Facial Attributes, to solve the imbalanced data of
attributes without adjusting network architecture and data augmentation. We
experimented on the well-known Market1501 dataset. The experimental results
have shown the effectiveness of our Re-ID system and it could achieve the
higher performance on Market1501 dataset in comparison to some state-of-the-art
Re-ID methods.

</details>


### [106] [Image Editing As Programs with Diffusion Models](https://arxiv.org/abs/2506.04158)
*Yujia Hu,Songhua Liu,Zhenxiong Tan,Xingyi Yang,Xinchao Wang*

Main category: cs.CV

TL;DR: IEAP框架通过将复杂编辑指令分解为原子操作，利用Diffusion Transformer架构和VLM代理，显著提升了指令驱动的图像编辑能力。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在指令驱动的图像编辑中，尤其是涉及结构不一致的布局变化时的困难。

Method: 提出IEAP框架，将编辑指令分解为原子操作，每个操作由轻量级适配器实现，并通过VLM代理编程。

Result: 在标准基准测试中，IEAP显著优于现有方法，尤其在复杂多步指令下表现优异。

Conclusion: IEAP通过模块化和序列化编辑操作，实现了对广泛编辑任务的鲁棒性，提升了编辑的准确性和语义保真度。

Abstract: While diffusion models have achieved remarkable success in text-to-image
generation, they encounter significant challenges with instruction-driven image
editing. Our research highlights a key challenge: these models particularly
struggle with structurally inconsistent edits that involve substantial layout
changes. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a
unified image editing framework built upon the Diffusion Transformer (DiT)
architecture. At its core, IEAP approaches instructional editing through a
reductionist lens, decomposing complex editing instructions into sequences of
atomic operations. Each operation is implemented via a lightweight adapter
sharing the same DiT backbone and is specialized for a specific type of edit.
Programmed by a vision-language model (VLM)-based agent, these operations
collaboratively support arbitrary and structurally inconsistent
transformations. By modularizing and sequencing edits in this way, IEAP
generalizes robustly across a wide range of editing tasks, from simple
adjustments to substantial structural changes. Extensive experiments
demonstrate that IEAP significantly outperforms state-of-the-art methods on
standard benchmarks across various editing scenarios. In these evaluations, our
framework delivers superior accuracy and semantic fidelity, particularly for
complex, multi-step instructions. Codes are available at
https://github.com/YujiaHu1109/IEAP.

</details>


### [107] [FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D Gaussian Splatting](https://arxiv.org/abs/2506.04174)
*Hengyu Liu,Yuehao Wang,Chenxin Li,Ruisi Cai,Kevin Wang,Wuyang Li,Pavlo Molchanov,Peihao Wang,Zhangyang Wang*

Main category: cs.CV

TL;DR: 提出了一种弹性推理方法，通过选择和变换高斯子集，无需微调即可适应不同设备的特定内存需求。


<details>
  <summary>Details</summary>
Motivation: 3D高斯喷绘（3DGS）在3D场景表示和新视角合成中应用广泛，但对GPU内存需求较高，限制了其在资源受限设备上的使用。现有方法需微调且缺乏适应性。

Method: 引入小型可学习模块控制高斯选择，并结合变换模块调整所选高斯，以补充缩减模型的性能。

Result: 在ZipNeRF、MipNeRF和Tanks&Temples场景上的实验验证了方法的有效性。

Conclusion: 该方法无需额外微调即可显著提升渲染性能，适用于不同内存需求的设备。

Abstract: 3D Gaussian splatting (3DGS) has enabled various applications in 3D scene
representation and novel view synthesis due to its efficient rendering
capabilities. However, 3DGS demands relatively significant GPU memory, limiting
its use on devices with restricted computational resources. Previous approaches
have focused on pruning less important Gaussians, effectively compressing 3DGS
but often requiring a fine-tuning stage and lacking adaptability for the
specific memory needs of different devices. In this work, we present an elastic
inference method for 3DGS. Given an input for the desired model size, our
method selects and transforms a subset of Gaussians, achieving substantial
rendering performance without additional fine-tuning. We introduce a tiny
learnable module that controls Gaussian selection based on the input
percentage, along with a transformation module that adjusts the selected
Gaussians to complement the performance of the reduced model. Comprehensive
experiments on ZipNeRF, MipNeRF and Tanks\&Temples scenes demonstrate the
effectiveness of our approach. Code is available at https://flexgs.github.io.

</details>


### [108] [Language-Image Alignment with Fixed Text Encoders](https://arxiv.org/abs/2506.04209)
*Jingfeng Yang,Ziyang Wu,Yue Zhao,Yi Ma*

Main category: cs.CV

TL;DR: LIFT框架通过固定预训练的大型语言模型（LLM）作为文本编码器，仅训练图像编码器，实现了高效的语言-图像对齐，并在组合理解和长文本场景中优于CLIP。


<details>
  <summary>Details</summary>
Motivation: 质疑是否需要昂贵的联合训练（如CLIP）来实现语言-图像对齐，探索固定LLM文本编码器是否能有效指导视觉表示学习。

Method: 提出LIFT框架，固定LLM文本编码器，仅训练图像编码器，简化训练流程。

Result: LIFT在组合理解和长文本任务中表现优于CLIP，同时显著提升计算效率。

Conclusion: LIFT为探索LLM文本嵌入指导视觉学习提供了新思路，并提出了语言对齐视觉表示学习的替代设计。

Abstract: Currently, the most dominant approach to establishing language-image
alignment is to pre-train text and image encoders jointly through contrastive
learning, such as CLIP and its variants. In this work, we question whether such
a costly joint training is necessary. In particular, we investigate if a
pre-trained fixed large language model (LLM) offers a good enough text encoder
to guide visual representation learning. That is, we propose to learn
Language-Image alignment with a Fixed Text encoder (LIFT) from an LLM by
training only the image encoder. Somewhat surprisingly, through comprehensive
benchmarking and ablation studies, we find that this much simplified framework
LIFT is highly effective and it outperforms CLIP in most scenarios that involve
compositional understanding and long captions, while achieving considerable
gains in computational efficiency. Our work takes a first step towards
systematically exploring how text embeddings from LLMs can guide visual
learning and suggests an alternative design choice for learning
language-aligned visual representations.

</details>


### [109] [Diffusion Domain Teacher: Diffusion Guided Domain Adaptive Object Detector](https://arxiv.org/abs/2506.04211)
*Boyong He,Yuxiang Ji,Zhuoyue Tan,Liaoni Wu*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散模型的跨域目标检测方法（DDT），通过冻结权重的扩散模型生成伪标签，显著提升了跨域检测性能。


<details>
  <summary>Details</summary>
Motivation: 目标检测器在训练数据（源域）和真实数据（目标域）之间存在较大领域差距时性能下降，扩散模型具有提取多领域特征的潜力。

Method: 在源域上训练冻结权重的扩散模型作为教师模型，生成目标域的伪标签，指导学生模型在目标域上的监督学习。

Result: 在6个跨域检测数据集上平均mAP提升21.2%，超越当前最优方法5.7%。

Conclusion: DDT方法简单有效，适用于复杂模型，具有广泛的领域适应能力。

Abstract: Object detectors often suffer a decrease in performance due to the large
domain gap between the training data (source domain) and real-world data
(target domain). Diffusion-based generative models have shown remarkable
abilities in generating high-quality and diverse images, suggesting their
potential for extracting valuable feature from various domains. To effectively
leverage the cross-domain feature representation of diffusion models, in this
paper, we train a detector with frozen-weight diffusion model on the source
domain, then employ it as a teacher model to generate pseudo labels on the
unlabeled target domain, which are used to guide the supervised learning of the
student model on the target domain. We refer to this approach as Diffusion
Domain Teacher (DDT). By employing this straightforward yet potent framework,
we significantly improve cross-domain object detection performance without
compromising the inference speed. Our method achieves an average mAP
improvement of 21.2% compared to the baseline on 6 datasets from three common
cross-domain detection benchmarks (Cross-Camera, Syn2Real, Real2Artistic},
surpassing the current state-of-the-art (SOTA) methods by an average of 5.7%
mAP. Furthermore, extensive experiments demonstrate that our method
consistently brings improvements even in more powerful and complex models,
highlighting broadly applicable and effective domain adaptation capability of
our DDT. The code is available at
https://github.com/heboyong/Diffusion-Domain-Teacher.

</details>


### [110] [FullDiT2: Efficient In-Context Conditioning for Video Diffusion Transformers](https://arxiv.org/abs/2506.04213)
*Xuanhua He,Quande Liu,Zixuan Ye,Wecai Ye,Qiulin Wang,Xintao Wang,Qifeng Chen,Pengfei Wan,Di Zhang,Kun Gai*

Main category: cs.CV

TL;DR: 论文提出FullDiT2，通过动态令牌选择和选择性上下文缓存机制，显著提升了视频扩散变换器的计算效率，实现了2-3倍的速度提升，同时保持或提高生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于全注意力的上下文条件视频生成方法（如FullDiT）存在二次计算开销问题，限制了实际应用。

Method: FullDiT2通过动态令牌选择减少冗余令牌，并通过选择性上下文缓存优化条件令牌与视频潜在表示的交互。

Result: 在六种任务中，FullDiT2显著降低了计算量，平均每扩散步骤时间减少2-3倍，且生成质量未下降或有所提升。

Conclusion: FullDiT2为视频生成和编辑任务提供了一种高效且通用的上下文条件框架。

Abstract: Fine-grained and efficient controllability on video diffusion transformers
has raised increasing desires for the applicability. Recently, In-context
Conditioning emerged as a powerful paradigm for unified conditional video
generation, which enables diverse controls by concatenating varying context
conditioning signals with noisy video latents into a long unified token
sequence and jointly processing them via full-attention, e.g., FullDiT. Despite
their effectiveness, these methods face quadratic computation overhead as task
complexity increases, hindering practical deployment. In this paper, we study
the efficiency bottleneck neglected in original in-context conditioning video
generation framework. We begin with systematic analysis to identify two key
sources of the computation inefficiencies: the inherent redundancy within
context condition tokens and the computational redundancy in context-latent
interactions throughout the diffusion process. Based on these insights, we
propose FullDiT2, an efficient in-context conditioning framework for general
controllability in both video generation and editing tasks, which innovates
from two key perspectives. Firstly, to address the token redundancy, FullDiT2
leverages a dynamic token selection mechanism to adaptively identify important
context tokens, reducing the sequence length for unified full-attention.
Additionally, a selective context caching mechanism is devised to minimize
redundant interactions between condition tokens and video latents. Extensive
experiments on six diverse conditional video editing and generation tasks
demonstrate that FullDiT2 achieves significant computation reduction and 2-3
times speedup in averaged time cost per diffusion step, with minimal
degradation or even higher performance in video generation quality. The project
page is at \href{https://fulldit2.github.io/}{https://fulldit2.github.io/}.

</details>


### [111] [Sounding that Object: Interactive Object-Aware Image to Audio Generation](https://arxiv.org/abs/2506.04214)
*Tingle Li,Baihe Huang,Xiaobin Zhuang,Dongya Jia,Jiawei Chen,Yuping Wang,Zhuo Chen,Gopala Anumanchipalli,Yuxuan Wang*

Main category: cs.CV

TL;DR: 提出了一种交互式对象感知音频生成模型，通过多模态注意力将声音生成与用户选择的视觉对象关联。


<details>
  <summary>Details</summary>
Motivation: 解决复杂视听场景中多对象和多声源情况下的准确声音生成问题。

Method: 结合对象中心学习和条件潜在扩散模型，通过多模态注意力关联图像区域与声音，测试时利用图像分割实现交互式声音生成。

Result: 定量和定性评估显示模型优于基线，实现了对象与声音的更好对齐。

Conclusion: 模型通过理论验证和实验证明了其在对象级声音生成中的有效性。

Abstract: Generating accurate sounds for complex audio-visual scenes is challenging,
especially in the presence of multiple objects and sound sources. In this
paper, we propose an {\em interactive object-aware audio generation} model that
grounds sound generation in user-selected visual objects within images. Our
method integrates object-centric learning into a conditional latent diffusion
model, which learns to associate image regions with their corresponding sounds
through multi-modal attention. At test time, our model employs image
segmentation to allow users to interactively generate sounds at the {\em
object} level. We theoretically validate that our attention mechanism
functionally approximates test-time segmentation masks, ensuring the generated
audio aligns with selected objects. Quantitative and qualitative evaluations
show that our model outperforms baselines, achieving better alignment between
objects and their associated sounds. Project page:
https://tinglok.netlify.app/files/avobject/

</details>


### [112] [UNIC: Unified In-Context Video Editing](https://arxiv.org/abs/2506.04216)
*Zixuan Ye,Xuanhua He,Quande Liu,Qiulin Wang,Xintao Wang,Pengfei Wan,Di Zhang,Kun Gai,Qifeng Chen,Wenhan Luo*

Main category: cs.CV

TL;DR: UNIC是一个统一的视频编辑框架，通过将不同编辑任务的输入表示为三种令牌，并利用DiT的注意力操作实现任务统一，避免了任务特定设计。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖任务特定架构或定制化设计，限制了编辑条件的多样性和任务的统一性。

Method: 将输入表示为源视频令牌、噪声视频潜在和多模态条件令牌，通过任务感知RoPE和条件偏置解决任务混淆问题。

Result: 在包含六个代表性任务的统一视频编辑基准上表现优异，并展现出任务组合能力。

Conclusion: UNIC框架简单有效，统一了多样化视频编辑任务，支持灵活的任务组合。

Abstract: Recent advances in text-to-video generation have sparked interest in
generative video editing tasks. Previous methods often rely on task-specific
architectures (e.g., additional adapter modules) or dedicated customizations
(e.g., DDIM inversion), which limit the integration of versatile editing
conditions and the unification of various editing tasks. In this paper, we
introduce UNified In-Context Video Editing (UNIC), a simple yet effective
framework that unifies diverse video editing tasks within a single model in an
in-context manner. To achieve this unification, we represent the inputs of
various video editing tasks as three types of tokens: the source video tokens,
the noisy video latent, and the multi-modal conditioning tokens that vary
according to the specific editing task. Based on this formulation, our key
insight is to integrate these three types into a single consecutive token
sequence and jointly model them using the native attention operations of DiT,
thereby eliminating the need for task-specific adapter designs. Nevertheless,
direct task unification under this framework is challenging, leading to severe
token collisions and task confusion due to the varying video lengths and
diverse condition modalities across tasks. To address these, we introduce
task-aware RoPE to facilitate consistent temporal positional encoding, and
condition bias that enables the model to clearly differentiate different
editing tasks. This allows our approach to adaptively perform different video
editing tasks by referring the source video and varying condition tokens "in
context", and support flexible task composition. To validate our method, we
construct a unified video editing benchmark containing six representative video
editing tasks. Results demonstrate that our unified approach achieves superior
performance on each task and exhibits emergent task composition abilities.

</details>


### [113] [Struct2D: A Perception-Guided Framework for Spatial Reasoning in Large Multimodal Models](https://arxiv.org/abs/2506.04220)
*Fangrui Zhu,Hanhui Wang,Yiming Xie,Jing Gu,Tianye Ding,Jianwei Yang,Huaizu Jiang*

Main category: cs.CV

TL;DR: 论文提出Struct2D框架，通过结构化2D输入（如鸟瞰图和物体标记）增强大型多模态模型（LMMs）的空间推理能力，无需显式3D输入。实验表明LMMs在零样本任务中表现优异，并构建了200K QA对的数据集Struct2D-Set，微调后模型在多个任务中表现竞争力。


<details>
  <summary>Details</summary>
Motivation: 探索LMMs是否仅通过结构化2D输入（而非显式3D输入）就能进行3D空间推理，以简化模型架构并提升智能交互能力。

Method: 提出Struct2D框架，结合鸟瞰图、物体标记和元数据，生成结构化2D输入；构建Struct2D-Set数据集，用于微调开源LMM（Qwen2.5VL）。

Result: 实验证明LMMs在零样本任务中表现优异；微调后的模型在3D问答、密集描述和物体定位等任务中达到竞争力性能。

Conclusion: 结构化2D输入能有效连接感知与语言推理，无需显式3D输入，为未来研究提供了代码和数据集支持。

Abstract: Unlocking spatial reasoning in Large Multimodal Models (LMMs) is crucial for
enabling intelligent interaction with 3D environments. While prior efforts
often rely on explicit 3D inputs or specialized model architectures, we ask:
can LMMs reason about 3D space using only structured 2D representations derived
from perception? We introduce Struct2D, a perception-guided prompting framework
that combines bird's-eye-view (BEV) images with object marks and object-centric
metadata, optionally incorporating egocentric keyframes when needed. Using
Struct2D, we conduct an in-depth zero-shot analysis of closed-source LMMs
(e.g., GPT-o3) and find that they exhibit surprisingly strong spatial reasoning
abilities when provided with structured 2D inputs, effectively handling tasks
such as relative direction estimation and route planning. Building on these
insights, we construct Struct2D-Set, a large-scale instruction tuning dataset
with 200K fine-grained QA pairs across eight spatial reasoning categories,
generated automatically from 3D indoor scenes. We fine-tune an open-source LMM
(Qwen2.5VL) on Struct2D-Set, achieving competitive performance on multiple
benchmarks, including 3D question answering, dense captioning, and object
grounding. Our approach demonstrates that structured 2D inputs can effectively
bridge perception and language reasoning in LMMs-without requiring explicit 3D
representations as input. We will release both our code and dataset to support
future research.

</details>


### [114] [Seeing in the Dark: Benchmarking Egocentric 3D Vision with the Oxford Day-and-Night Dataset](https://arxiv.org/abs/2506.04224)
*Zirui Wang,Wenjing Bian,Xinghui Li,Yifu Tao,Jianeng Wang,Maurice Fallon,Victor Adrian Prisacariu*

Main category: cs.CV

TL;DR: Oxford Day-and-Night是一个大规模的自中心数据集，用于挑战性光照条件下的新视角合成和视觉重定位，填补了现有数据集的不足。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏真实3D几何、广泛光照变化和完整6DoF运动的组合，该数据集旨在解决这些问题。

Method: 利用Meta ARIA眼镜捕获自中心视频，并通过多会话SLAM估计相机姿态、重建3D点云，并对齐不同光照条件下的序列。

Result: 数据集覆盖30公里轨迹和40,000平方米区域，支持新视角合成和重定位两个核心基准测试。

Conclusion: 该数据集为自中心3D视觉研究提供了丰富资源，适用于多样化和真实环境的模型评估。

Abstract: We introduce Oxford Day-and-Night, a large-scale, egocentric dataset for
novel view synthesis (NVS) and visual relocalisation under challenging lighting
conditions. Existing datasets often lack crucial combinations of features such
as ground-truth 3D geometry, wide-ranging lighting variation, and full 6DoF
motion. Oxford Day-and-Night addresses these gaps by leveraging Meta ARIA
glasses to capture egocentric video and applying multi-session SLAM to estimate
camera poses, reconstruct 3D point clouds, and align sequences captured under
varying lighting conditions, including both day and night. The dataset spans
over 30 $\mathrm{km}$ of recorded trajectories and covers an area of 40,000
$\mathrm{m}^2$, offering a rich foundation for egocentric 3D vision research.
It supports two core benchmarks, NVS and relocalisation, providing a unique
platform for evaluating models in realistic and diverse environments.

</details>


### [115] [Voyager: Long-Range and World-Consistent Video Diffusion for Explorable 3D Scene Generation](https://arxiv.org/abs/2506.04225)
*Tianyu Huang,Wangguandong Zheng,Tengfei Wang,Yuhao Liu,Zhenwei Wang,Junta Wu,Jie Jiang,Hui Li,Rynson W. H. Lau,Wangmeng Zuo,Chunchao Guo*

Main category: cs.CV

TL;DR: Voyager是一种新颖的视频扩散框架，通过单张图像和用户定义的相机路径生成世界一致的3D点云序列，避免了传统3D重建流程的需求。


<details>
  <summary>Details</summary>
Motivation: 现实应用（如视频游戏和虚拟现实）需要能够生成用户可沿自定义相机轨迹探索的3D场景，但现有方法在生成长范围、3D一致的场景方面仍面临挑战。

Method: Voyager整合了三个关键组件：1）世界一致视频扩散，2）长范围世界探索，3）可扩展数据引擎，实现了端到端的场景生成与重建。

Result: 该方法在视觉质量和几何精度上优于现有方法，具有广泛的应用潜力。

Conclusion: Voyager通过创新的框架设计，解决了3D场景生成中的一致性和长范围探索问题，为相关领域提供了实用工具。

Abstract: Real-world applications like video gaming and virtual reality often demand
the ability to model 3D scenes that users can explore along custom camera
trajectories. While significant progress has been made in generating 3D objects
from text or images, creating long-range, 3D-consistent, explorable 3D scenes
remains a complex and challenging problem. In this work, we present Voyager, a
novel video diffusion framework that generates world-consistent 3D point-cloud
sequences from a single image with user-defined camera path. Unlike existing
approaches, Voyager achieves end-to-end scene generation and reconstruction
with inherent consistency across frames, eliminating the need for 3D
reconstruction pipelines (e.g., structure-from-motion or multi-view stereo).
Our method integrates three key components: 1) World-Consistent Video
Diffusion: A unified architecture that jointly generates aligned RGB and depth
video sequences, conditioned on existing world observation to ensure global
coherence 2) Long-Range World Exploration: An efficient world cache with point
culling and an auto-regressive inference with smooth video sampling for
iterative scene extension with context-aware consistency, and 3) Scalable Data
Engine: A video reconstruction pipeline that automates camera pose estimation
and metric depth prediction for arbitrary videos, enabling large-scale, diverse
training data curation without manual 3D annotations. Collectively, these
designs result in a clear improvement over existing methods in visual quality
and geometric accuracy, with versatile applications.

</details>


### [116] [LayerFlow: A Unified Model for Layer-aware Video Generation](https://arxiv.org/abs/2506.04228)
*Sihui Ji,Hao Luo,Xi Chen,Yuanpeng Tu,Yiyang Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: LayerFlow是一种统一的分层感知视频生成解决方案，支持透明前景、干净背景和混合场景的生成，并能分解混合视频或生成背景与前景的互换。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏高质量的分层训练视频数据，且无法统一支持多种分层视频生成任务。

Method: 基于文本到视频扩散变换器，将不同层的视频组织为子片段，利用层嵌入区分每个片段及其对应的分层提示。采用多阶段训练策略，先使用低质量视频数据训练，再通过运动LoRA适配静态帧，最后结合高质量分层图像和复制粘贴的视频数据训练内容LoRA。

Result: LayerFlow能够生成平滑的分层视频，支持多种分层视频生成任务。

Conclusion: LayerFlow通过统一框架和多阶段训练策略，解决了分层视频生成的多样性和数据质量问题。

Abstract: We present LayerFlow, a unified solution for layer-aware video generation.
Given per-layer prompts, LayerFlow generates videos for the transparent
foreground, clean background, and blended scene. It also supports versatile
variants like decomposing a blended video or generating the background for the
given foreground and vice versa. Starting from a text-to-video diffusion
transformer, we organize the videos for different layers as sub-clips, and
leverage layer embeddings to distinguish each clip and the corresponding
layer-wise prompts. In this way, we seamlessly support the aforementioned
variants in one unified framework. For the lack of high-quality layer-wise
training videos, we design a multi-stage training strategy to accommodate
static images with high-quality layer annotations. Specifically, we first train
the model with low-quality video data. Then, we tune a motion LoRA to make the
model compatible with static frames. Afterward, we train the content LoRA on
the mixture of image data with high-quality layered images along with
copy-pasted video data. During inference, we remove the motion LoRA thus
generating smooth videos with desired layers.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [117] [Adaptive and Robust Image Processing on CubeSats](https://arxiv.org/abs/2506.03152)
*Robert Bayer,Julian Priest,Daniel Kjellberg,Jeppe Lindhard,Nikolaj Sørenesen,Nicolaj Valsted,Ívar Óli,Pınar Tözün*

Main category: eess.IV

TL;DR: 论文提出了DIPP和DISH两个系统，分别用于解决CubeSat上图像处理管道的灵活性和资源受限问题。DIPP提供模块化配置，DISH是专用语言和运行时系统。实验证明DIPP高效且稳健，DISH比通用语言更节省内存。


<details>
  <summary>Details</summary>
Motivation: CubeSat资源受限且部署后难以调整，需要灵活的图像处理管道和调度系统。

Method: DIPP是模块化框架，支持动态调整；DISH是专用语言和运行时系统，优化资源使用。

Result: DIPP减少网络需求且稳健；DISH比Lua更省内存且表达力相当。

Conclusion: DIPP和DISH有效解决了CubeSat上图像处理管道的灵活性和资源问题。

Abstract: CubeSats offer a low-cost platform for space research, particularly for Earth
observation. However, their resource-constrained nature and being in space,
challenge the flexibility and complexity of the deployed image processing
pipelines and their orchestration. This paper introduces two novel systems,
DIPP and DISH, to address these challenges. DIPP is a modular and configurable
image processing pipeline framework that allows for adaptability to changing
mission goals even after deployment, while preserving robustness. DISH is a
domain-specific language (DSL) and runtime system designed to schedule complex
imaging workloads on low-power and memory-constrained processors.
  Our experiments demonstrate that DIPP's decomposition of the processing
pipelines adds negligible overhead, while significantly reducing the network
requirements of updating pipelines and being robust against erroneous module
uploads. Furthermore, we compare DISH to Lua, a general purpose scripting
language, and demonstrate its comparable expressiveness and lower memory
requirement.

</details>


### [118] [Super-temporal-resolution Photoacoustic Imaging with Dynamic Reconstruction through Implicit Neural Representation in Sparse-view](https://arxiv.org/abs/2506.03175)
*Youshen Xiao,Yiling Shi,Ruixi Sun,Hongjiang Wei,Fei Gao,Yuyao Zhang*

Main category: eess.IV

TL;DR: 提出了一种基于隐式神经表示（INR）的动态光声图像重建方法，解决了稀疏传感器数据下的图像伪影问题，并提升了时间分辨率。


<details>
  <summary>Details</summary>
Motivation: 传统光声图像重建方法在稀疏数据下产生严重伪影，且未考虑动态成像中的帧间关系，同时高功率激光技术的低重复率限制了时间分辨率。

Method: 利用INR将动态光声图像表示为隐式函数，仅需时空坐标作为输入，通过神经网络学习权重，无需外部训练数据或先验图像。结合低秩和稀疏性正则化，优化重建效果。

Result: 在两种稀疏条件下，该方法优于传统重建方法，有效抑制伪影并确保图像质量。

Conclusion: INR方法为稀疏数据下的动态光声成像提供了高质量重建方案，具有实际应用潜力。

Abstract: Dynamic Photoacoustic Computed Tomography (PACT) is an important imaging
technique for monitoring physiological processes, capable of providing
high-contrast images of optical absorption at much greater depths than
traditional optical imaging methods. However, practical instrumentation and
geometric constraints limit the number of acoustic sensors available around the
imaging target, leading to sparsity in sensor data. Traditional photoacoustic
(PA) image reconstruction methods, when directly applied to sparse PA data,
produce severe artifacts. Additionally, these traditional methods do not
consider the inter-frame relationships in dynamic imaging. Temporal resolution
is crucial for dynamic photoacoustic imaging, which is fundamentally limited by
the low repetition rate (e.g., 20 Hz) and high cost of high-power laser
technology. Recently, Implicit Neural Representation (INR) has emerged as a
powerful deep learning tool for solving inverse problems with sparse data, by
characterizing signal properties as continuous functions of their coordinates
in an unsupervised manner. In this work, we propose an INR-based method to
improve dynamic photoacoustic image reconstruction from sparse-views and
enhance temporal resolution, using only spatiotemporal coordinates as input.
Specifically, the proposed INR represents dynamic photoacoustic images as
implicit functions and encodes them into a neural network. The weights of the
network are learned solely from the acquired sparse sensor data, without the
need for external training datasets or prior images. Benefiting from the strong
implicit continuity regularization provided by INR, as well as explicit
regularization for low-rank and sparsity, our proposed method outperforms
traditional reconstruction methods under two different sparsity conditions,
effectively suppressing artifacts and ensuring image quality.

</details>


### [119] [Deep Learning-Based Breast Cancer Detection in Mammography: A Multi-Center Validation Study in Thai Population](https://arxiv.org/abs/2506.03177)
*Isarun Chamveha,Supphanut Chaiyungyuen,Sasinun Worakriangkrai,Nattawadee Prasawang,Warasinee Chaisangmongkon,Pornpim Korpraphong,Voraparee Suvannarerg,Shanigarn Thiravit,Chalermdej Kannawat,Kewalin Rungsinaporn,Suwara Issaragrisil,Payia Chadbunchachai,Pattiya Gatechumpol,Chawiporn Muktabhant,Patarachai Sereerat*

Main category: eess.IV

TL;DR: 提出了一种基于改进EfficientNetV2架构的深度学习系统，用于乳腺X光检查中的乳腺癌检测，并在多个数据集上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种高效且准确的乳腺癌检测系统，以辅助临床医生提高筛查效率和准确性。

Method: 使用改进的EfficientNetV2架构，结合增强的注意力机制，在泰国某医疗中心的乳腺X光数据上进行训练，并在三个不同数据集上验证。

Result: 模型在癌症检测上表现优异（AUROCs分别为0.89、0.96和0.94），并在病灶定位和临床验证中显示出与放射科医生的高度一致性。

Conclusion: 该系统在乳腺癌筛查中具有潜在临床应用价值，能够显著提升工作流程效率和诊断准确性。

Abstract: This study presents a deep learning system for breast cancer detection in
mammography, developed using a modified EfficientNetV2 architecture with
enhanced attention mechanisms. The model was trained on mammograms from a major
Thai medical center and validated on three distinct datasets: an in-domain test
set (9,421 cases), a biopsy-confirmed set (883 cases), and an out-of-domain
generalizability set (761 cases) collected from two different hospitals. For
cancer detection, the model achieved AUROCs of 0.89, 0.96, and 0.94 on the
respective datasets. The system's lesion localization capability, evaluated
using metrics including Lesion Localization Fraction (LLF) and Non-Lesion
Localization Fraction (NLF), demonstrated robust performance in identifying
suspicious regions. Clinical validation through concordance tests showed strong
agreement with radiologists: 83.5% classification and 84.0% localization
concordance for biopsy-confirmed cases, and 78.1% classification and 79.6%
localization concordance for out-of-domain cases. Expert radiologists'
acceptance rate also averaged 96.7% for biopsy-confirmed cases, and 89.3% for
out-of-domain cases. The system achieved a System Usability Scale score of
74.17 for source hospital, and 69.20 for validation hospitals, indicating good
clinical acceptance. These results demonstrate the model's effectiveness in
assisting mammogram interpretation, with the potential to enhance breast cancer
screening workflows in clinical practice.

</details>


### [120] [LLaMA-XR: A Novel Framework for Radiology Report Generation using LLaMA and QLoRA Fine Tuning](https://arxiv.org/abs/2506.03178)
*Md. Zihad Bin Jahangir,Muhammad Ashad Kabir,Sumaiya Akter,Israt Jahan,Minh Chau*

Main category: eess.IV

TL;DR: LLaMA-XR是一种结合LLaMA 3.1和DenseNet-121图像嵌入的新型框架，通过QLoRA微调提升放射报告生成的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 自动化放射报告生成可减轻放射科医生负担并提高诊断准确性，但现有模型在保持准确性和上下文相关性方面存在挑战。

Method: LLaMA-XR整合LLaMA 3.1与DenseNet-121图像嵌入，采用QLoRA微调优化参数利用和内存开销。

Result: 在IU X-ray数据集上，LLaMA-XR的ROUGE-L得分为0.433，METEOR得分为0.336，优于现有方法。

Conclusion: LLaMA-XR展示了在自动化放射报告生成中的高效性和临床实用性，具有潜在广泛应用前景。

Abstract: Automated radiology report generation holds significant potential to reduce
radiologists' workload and enhance diagnostic accuracy. However, generating
precise and clinically meaningful reports from chest radiographs remains
challenging due to the complexity of medical language and the need for
contextual understanding. Existing models often struggle with maintaining both
accuracy and contextual relevance. In this paper, we present LLaMA-XR, a novel
framework that integrates LLaMA 3.1 with DenseNet-121-based image embeddings
and Quantized Low-Rank Adaptation (QLoRA) fine-tuning. LLaMA-XR achieves
improved coherence and clinical accuracy while maintaining computational
efficiency. This efficiency is driven by an optimization strategy that enhances
parameter utilization and reduces memory overhead, enabling faster report
generation with lower computational resource demands. Extensive experiments
conducted on the IU X-ray benchmark dataset demonstrate that LLaMA-XR
outperforms a range of state-of-the-art methods. Our model achieves a ROUGE-L
score of 0.433 and a METEOR score of 0.336, establishing new performance
benchmarks in the domain. These results underscore LLaMA-XR's potential as an
effective and efficient AI system for automated radiology reporting, offering
enhanced clinical utility and reliability.

</details>


### [121] [Dc-EEMF: Pushing depth-of-field limit of photoacoustic microscopy via decision-level constrained learning](https://arxiv.org/abs/2506.03181)
*Wangting Zhou,Jiangshan He,Tong Cai,Lin Wang,Zhen Yuan,Xunbin Wei,Xueli Chen*

Main category: eess.IV

TL;DR: 提出了一种决策级约束的端到端多焦点图像融合方法（Dc-EEMF），用于突破光学分辨率光声显微镜（OR-PAM）的景深限制。该方法结合了轻量级Siamese网络和抗伪影的通道空间频率特征融合规则，通过精心设计的U-Net感知损失函数实现端到端训练，无需后处理。实验表明该方法在保持横向分辨率的同时，显著提升了PAM图像的融合效果。


<details>
  <summary>Details</summary>
Motivation: 传统OR-PAM因高斯光束的窄景深限制了深度方向的细节解析，无法满足生物医学研究中无标记测量的需求。

Method: 提出Dc-EEMF方法，基于轻量级Siamese网络，结合通道空间频率特征融合规则和U-Net感知损失函数，实现端到端训练。

Result: 实验和数值分析表明，Dc-EEMF在保持横向分辨率的同时，显著提升了PAM图像的融合效果。

Conclusion: Dc-EEMF方法有望成为临床前和临床研究中扩展景深的实用工具。

Abstract: Photoacoustic microscopy holds the potential to measure biomarkers'
structural and functional status without labels, which significantly aids in
comprehending pathophysiological conditions in biomedical research. However,
conventional optical-resolution photoacoustic microscopy (OR-PAM) is hindered
by a limited depth-of-field (DoF) due to the narrow depth range focused on a
Gaussian beam. Consequently, it fails to resolve sufficient details in the
depth direction. Herein, we propose a decision-level constrained end-to-end
multi-focus image fusion (Dc-EEMF) to push DoF limit of PAM. The DC-EEMF method
is a lightweight siamese network that incorporates an artifact-resistant
channel-wise spatial frequency as its feature fusion rule. The meticulously
crafted U-Net-based perceptual loss function for decision-level focus
properties in end-to-end fusion seamlessly integrates the complementary
advantages of spatial domain and transform domain methods within Dc-EEMF. This
approach can be trained end-to-end without necessitating post-processing
procedures. Experimental results and numerical analyses collectively
demonstrate our method's robust performance, achieving an impressive fusion
result for PAM images without a substantial sacrifice in lateral resolution.
The utilization of Dc-EEMF-powered PAM has the potential to serve as a
practical tool in preclinical and clinical studies requiring extended DoF for
various applications.

</details>


### [122] [Edge Computing for Physics-Driven AI in Computational MRI: A Feasibility Study](https://arxiv.org/abs/2506.03183)
*Yaşar Utku Alçalar,Yu Cao,Mehmet Akçakaya*

Main category: eess.IV

TL;DR: 提出了一种针对FPGA边缘计算优化的PD-AI MRI重建方法，通过8位复数数据量化和消除冗余FFT/IFFT操作，提高了计算效率，同时保持了重建质量。


<details>
  <summary>Details</summary>
Motivation: 高分辨率MRI扫描产生大量数据，导致传输、存储和实时处理挑战，尤其是在功能MRI中。边缘计算结合FPGA为解决这些问题提供了可能，但需优化PD-AI模型以适应硬件效率。

Method: 采用8位复数数据量化并消除冗余FFT/IFFT操作，优化PD-AI模型以适应FPGA边缘计算设备。

Result: 该方法在计算效率上有所提升，重建质量与传统PD-AI方法相当，且优于标准临床方法。

Conclusion: 该研究为资源受限设备上的高分辨率MRI重建提供了可行方案，展示了其在实际应用中的潜力。

Abstract: Physics-driven artificial intelligence (PD-AI) reconstruction methods have
emerged as the state-of-the-art for accelerating MRI scans, enabling higher
spatial and temporal resolutions. However, the high resolution of these scans
generates massive data volumes, leading to challenges in transmission, storage,
and real-time processing. This is particularly pronounced in functional MRI,
where hundreds of volumetric acquisitions further exacerbate these demands.
Edge computing with FPGAs presents a promising solution for enabling PD-AI
reconstruction near the MRI sensors, reducing data transfer and storage
bottlenecks. However, this requires optimization of PD-AI models for hardware
efficiency through quantization and bypassing traditional FFT-based approaches,
which can be a limitation due to their computational demands. In this work, we
propose a novel PD-AI computational MRI approach optimized for FPGA-based edge
computing devices, leveraging 8-bit complex data quantization and eliminating
redundant FFT/IFFT operations. Our results show that this strategy improves
computational efficiency while maintaining reconstruction quality comparable to
conventional PD-AI methods, and outperforms standard clinical methods. Our
approach presents an opportunity for high-resolution MRI reconstruction on
resource-constrained devices, highlighting its potential for real-world
deployment.

</details>


### [123] [DLiPath: A Benchmark for the Comprehensive Assessment of Donor Liver Based on Histopathological Image Dataset](https://arxiv.org/abs/2506.03185)
*Liangrui Pan,Xingchen Li,Zhongyi Chen,Ling Chu,Shaoliang Peng*

Main category: eess.IV

TL;DR: DLiPath是首个基于组织病理学图像数据集的供体肝脏评估基准，旨在解决供体肝脏活检评估中的快速准确性问题。


<details>
  <summary>Details</summary>
Motivation: 供体肝脏活检评估对移植结果至关重要，但传统方法存在观察者间和观察者内变异性高的问题。

Method: 收集并公开了304名患者的636张全切片图像，标注关键病理特征，并基于此数据集评估了九种多实例学习模型。

Result: 实验表明，多种MIL模型在DLiPath数据集上对供体肝脏评估指标实现了高准确率。

Conclusion: DLiPath为未来自动化智能供体肝脏评估研究提供了明确方向。

Abstract: Pathologists comprehensive evaluation of donor liver biopsies provides
crucial information for accepting or discarding potential grafts. However,
rapidly and accurately obtaining these assessments intraoperatively poses a
significant challenge for pathologists. Features in donor liver biopsies, such
as portal tract fibrosis, total steatosis, macrovesicular steatosis, and
hepatocellular ballooning are correlated with transplant outcomes, yet
quantifying these indicators suffers from substantial inter- and intra-observer
variability. To address this, we introduce DLiPath, the first benchmark for
comprehensive donor liver assessment based on a histopathology image dataset.
We collected and publicly released 636 whole slide images from 304 donor liver
patients at the Department of Pathology, the Third Xiangya Hospital, with
expert annotations for key pathological features (including cholestasis, portal
tract fibrosis, portal inflammation, total steatosis, macrovesicular steatosis,
and hepatocellular ballooning). We selected nine state-of-the-art
multiple-instance learning (MIL) models based on the DLiPath dataset as
baselines for extensive comparative analysis. The experimental results
demonstrate that several MIL models achieve high accuracy across donor liver
assessment indicators on DLiPath, charting a clear course for future automated
and intelligent donor liver assessment research. Data and code are available at
https://github.com/panliangrui/ACM_MM_2025.

</details>


### [124] [Lightweight Convolutional Neural Networks for Retinal Disease Classification](https://arxiv.org/abs/2506.03186)
*Duaa Kareem Qasim,Sabah Abdulazeez Jebur,Lafta Raheem Ali,Abdul Jalil M. Khalaf,Abir Jaafar Hussain*

Main category: eess.IV

TL;DR: 论文提出使用轻量级CNN架构MobileNet和NASNetMobile对糖尿病视网膜病变（DR）和黄斑裂孔（MH）进行分类，MobileNetV2表现最佳，准确率达90.8%。


<details>
  <summary>Details</summary>
Motivation: DR和MH严重影响视力，早期检测至关重要。AI辅助诊断可帮助实现早期干预。

Method: 使用MobileNet和NASNetMobile模型，基于RFMiD数据集（3,200张眼底图像），通过预处理、迁移学习和数据增强提升性能。

Result: MobileNetV2准确率90.8%，优于NASNetMobile的89.5%。

Conclusion: CNN在视网膜疾病分类中表现优异，为AI辅助眼科诊断提供了基础。

Abstract: Retinal diseases such as Diabetic Retinopathy (DR) and Macular Hole (MH)
significantly impact vision and affect millions worldwide. Early detection is
crucial, as DR, a complication of diabetes, damages retinal blood vessels,
potentially leading to blindness, while MH disrupts central vision, affecting
tasks like reading and facial recognition. This paper employed two lightweight
and efficient Convolution Neural Network architectures, MobileNet and
NASNetMobile, for the classification of Normal, DR, and MH retinal images. The
models were trained on the RFMiD dataset, consisting of 3,200 fundus images,
after undergoing preprocessing steps such as resizing, normalization, and
augmentation. To address data scarcity, this study leveraged transfer learning
and data augmentation techniques, enhancing model generalization and
performance. The experimental results demonstrate that MobileNetV2 achieved the
highest accuracy of 90.8%, outperforming NASNetMobile, which achieved 89.5%
accuracy. These findings highlight the effectiveness of CNNs in retinal disease
classification, providing a foundation for AI-assisted ophthalmic diagnosis and
early intervention.

</details>


### [125] [Multi-Analyte, Swab-based Automated Wound Monitor with AI](https://arxiv.org/abs/2506.03188)
*Madhu Babu Sikha,Lalith Appari,Gurudatt Nanjanagudu Ganesh,Amay Bandodkar,Imon Banerjee*

Main category: eess.IV

TL;DR: 开发了一种低成本、多分析物的3D打印检测试纸和iOS应用，用于早期识别糖尿病足溃疡（DFU）的愈合情况，通过计算机视觉技术自动分析伤口严重程度。


<details>
  <summary>Details</summary>
Motivation: 糖尿病足溃疡（DFU）是一种慢性伤口，早期识别非愈合性DFU可显著降低治疗成本和截肢风险。

Method: 使用3D打印的多分析物试纸和iOS应用，通过计算机视觉技术比较伤口暴露前后的图像密度变化，自动评估伤口严重程度。

Result: 开发的系统能够实时监测伤口状况，跟踪愈合进展，并提供可操作的见解。

Conclusion: 集成的传感器和iOS应用为医疗专业人员提供了一种高效的工具，用于早期检测和监测DFU。

Abstract: Diabetic foot ulcers (DFUs), a class of chronic wounds, affect ~750,000
individuals every year in the US alone and identifying non-healing DFUs that
develop to chronic wounds early can drastically reduce treatment costs and
minimize risks of amputation. There is therefore a pressing need for diagnostic
tools that can detect non-healing DFUs early. We develop a low cost,
multi-analyte 3D printed assays seamlessly integrated on swabs that can
identify non-healing DFUs and a Wound Sensor iOS App - an innovative mobile
application developed for the controlled acquisition and automated analysis of
wound sensor data. By comparing both the original base image (before exposure
to the wound) and the wound-exposed image, we developed automated computer
vision techniques to compare density changes between the two assay images,
which allow us to automatically determine the severity of the wound. The iOS
app ensures accurate data collection and presents actionable insights, despite
challenges such as variations in camera configurations and ambient conditions.
The proposed integrated sensor and iOS app will allow healthcare professionals
to monitor wound conditions real-time, track healing progress, and assess
critical parameters related to wound care.

</details>


### [126] [Encoding of Demographic and Anatomical Information in Chest X-Ray-based Severe Left Ventricular Hypertrophy Classifiers](https://arxiv.org/abs/2506.03192)
*Basudha Pal,Rama Chellappa,Muhammad Umair*

Main category: eess.IV

TL;DR: 提出了一种基于胸片直接预测严重左心室肥厚的分类框架，无需依赖解剖测量或人口统计输入，性能优异且支持模型解释。


<details>
  <summary>Details</summary>
Motivation: 超声心动图和MRI成本高且不易获取，限制了其临床应用，因此需要一种更便捷的替代方法。

Method: 采用直接分类框架，利用Mutual Information Neural Estimation量化特征表达性。

Result: 模型在AUROC和AUPRC上表现优异，揭示了具有临床意义的特征编码。

Conclusion: 该方法为左心室肥厚的诊断提供了一种高效且透明的替代方案。

Abstract: While echocardiography and MRI are clinical standards for evaluating cardiac
structure, their use is limited by cost and accessibility.We introduce a direct
classification framework that predicts severe left ventricular hypertrophy from
chest X-rays, without relying on anatomical measurements or demographic inputs.
Our approach achieves high AUROC and AUPRC, and employs Mutual Information
Neural Estimation to quantify feature expressivity. This reveals clinically
meaningful attribute encoding and supports transparent model interpretation.

</details>


### [127] [A combined Machine Learning and Finite Element Modelling tool for the surgical planning of craniosynostosis correction](https://arxiv.org/abs/2506.03202)
*Itxasne Antúnez Sáenz,Ane Alberdi Aramendi,David Dunaway,Juling Ong,Lara Deliège,Amparo Sáenz,Anita Ahmadi Birjandi,Noor UI Owase Jeelani,Silvia Schievano,Alessandro Borghi*

Main category: eess.IV

TL;DR: 该研究开发了一种基于机器学习的实时预测工具，用于预测矢状缝早闭症手术结果，无需CT扫描，减少辐射暴露。


<details>
  <summary>Details</summary>
Motivation: 目前手术结果难以预测，依赖医生经验和患者年龄，且传统有限元建模方法复杂且耗时。

Method: 利用3D照片创建个性化合成头骨，结合人群平均数据，使用机器学习替代模型预测手术结果。

Result: 多输出支持向量回归模型R2为0.95，MSE和MAE低于0.13，效果显著。

Conclusion: 该工具不仅能模拟手术场景，未来还可优化参数以实现最佳颅骨指数。

Abstract: Craniosynostosis is a medical condition that affects the growth of babies'
heads, caused by an early fusion of cranial sutures. In recent decades,
surgical treatments for craniosynostosis have significantly improved, leading
to reduced invasiveness, faster recovery, and less blood loss. At Great Ormond
Street Hospital (GOSH), the main surgical treatment for patients diagnosed with
sagittal craniosynostosis (SC) is spring assisted cranioplasty (SAC). This
procedure involves a 15x15 mm2 osteotomy, where two springs are inserted to
induce distraction. Despite the numerous advantages of this surgical technique
for patients, the outcome remains unpredictable due to the lack of efficient
preoperative planning tools. The surgeon's experience and the baby's age are
currently relied upon to determine the osteotomy location and spring selection.
Previous tools for predicting the surgical outcome of SC relied on finite
element modeling (FEM), which involved computed tomography (CT) imaging and
required engineering expertise and lengthy calculations. The main goal of this
research is to develop a real-time prediction tool for the surgical outcome of
patients, eliminating the need for CT scans to minimise radiation exposure
during preoperative planning. The proposed methodology involves creating
personalised synthetic skulls based on three-dimensional (3D) photographs,
incorporating population average values of suture location, skull thickness,
and soft tissue properties. A machine learning (ML) surrogate model is employed
to achieve the desired surgical outcome. The resulting multi-output support
vector regressor model achieves a R2 metric of 0.95 and MSE and MAE below 0.13.
Furthermore, in the future, this model could not only simulate various surgical
scenarios but also provide optimal parameters for achieving a maximum cranial
index (CI).

</details>


### [128] [A Survey of Deep Learning Video Super-Resolution](https://arxiv.org/abs/2506.03216)
*Arbind Agrahari Baniya,Tsz-Kwan Lee,Peter Eklund,Sunil Aryal*

Main category: eess.IV

TL;DR: 本文对基于深度学习的视频超分辨率（VSR）模型进行了全面综述，分析了各组件及其应用意义，并提出了多级分类法以指导未来研究。


<details>
  <summary>Details</summary>
Motivation: 由于VSR在多个领域具有潜在影响，但现有方法的使用和决策缺乏充分解释，因此需要对VSR研究中的深度学习方法和组件进行系统分析。

Method: 通过综述现有VSR模型，分析其关键组件和技术，并建立多级分类法。

Result: 识别了VSR领域的趋势、需求和挑战，并提出了分类法以指导未来研究。

Conclusion: 本文为VSR研究提供了系统化的综述和分类框架，有助于推动该领域的成熟和实际应用。

Abstract: Video super-resolution (VSR) is a prominent research topic in low-level
computer vision, where deep learning technologies have played a significant
role. The rapid progress in deep learning and its applications in VSR has led
to a proliferation of tools and techniques in the literature. However, the
usage of these methods is often not adequately explained, and decisions are
primarily driven by quantitative improvements. Given the significance of VSR's
potential influence across multiple domains, it is imperative to conduct a
comprehensive analysis of the elements and deep learning methodologies employed
in VSR research. This methodical analysis will facilitate the informed
development of models tailored to specific application needs. In this paper, we
present an overarching overview of deep learning-based video super-resolution
models, investigating each component and discussing its implications.
Furthermore, we provide a synopsis of key components and technologies employed
by state-of-the-art and earlier VSR models. By elucidating the underlying
methodologies and categorising them systematically, we identified trends,
requirements, and challenges in the domain. As a first-of-its-kind survey of
deep learning-based VSR models, this work also establishes a multi-level
taxonomy to guide current and future VSR research, enhancing the maturation and
interpretation of VSR practices for various practical applications.

</details>


### [129] [petBrain: A New Pipeline for Amyloid, Tau Tangles and Neurodegeneration Quantification Using PET and MRI](https://arxiv.org/abs/2506.03217)
*Pierrick Coupé,Boris Mansencal,Floréal Morandat,Sergio Morell-Ortega,Nicolas Villain,Jose V. Manjón,Vincent Planche*

Main category: eess.IV

TL;DR: petBrain是一个基于深度学习的新型端到端处理流程，用于标准化阿尔茨海默病（AD）生物标志物分析，提供快速可靠的量化结果。


<details>
  <summary>Details</summary>
Motivation: 现有流程在处理时间、示踪剂类型多样性和多模态整合方面存在局限性，需要一种更高效的解决方案。

Method: petBrain结合深度学习分割、标准化生物标志物量化（Centiloid、CenTauR、HAVAs）和A/T2/N生物标志物的同步估计，实现为无需本地计算基础设施的Web平台。

Result: petBrain的量化结果与现有流程相当，与ADNI数据库数据高度一致，且与CSF/血浆生物标志物、临床状态和认知表现吻合良好。

Conclusion: petBrain是一个强大且开放的平台，有助于标准化AD生物标志物分析，推动临床研究应用。

Abstract: INTRODUCTION: Quantification of amyloid plaques (A), neurofibrillary tangles
(T2), and neurodegeneration (N) using PET and MRI is critical for Alzheimer's
disease (AD) diagnosis and prognosis. Existing pipelines face limitations
regarding processing time, variability in tracer types, and challenges in
multimodal integration.
  METHODS: We developed petBrain, a novel end-to-end processing pipeline for
amyloid-PET, tau-PET, and structural MRI. It leverages deep learning-based
segmentation, standardized biomarker quantification (Centiloid, CenTauR,
HAVAs), and simultaneous estimation of A, T2, and N biomarkers. The pipeline is
implemented as a web-based platform, requiring no local computational
infrastructure or specialized software knowledge.
  RESULTS: petBrain provides reliable and rapid biomarker quantification, with
results comparable to existing pipelines for A and T2. It shows strong
concordance with data processed in ADNI databases. The staging and
quantification of A/T2/N by petBrain demonstrated good agreement with
CSF/plasma biomarkers, clinical status, and cognitive performance.
  DISCUSSION: petBrain represents a powerful and openly accessible platform for
standardized AD biomarker analysis, facilitating applications in clinical
research.

</details>


### [130] [Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric Approach](https://arxiv.org/abs/2506.03238)
*Ziheng Zhao,Lisong Dai,Ya Zhang,Yanfeng Wang,Weidi Xie*

Main category: eess.IV

TL;DR: 论文提出了一种自动化CT图像解释方法，包括分类系统、数据集、模型和基准测试，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决临床放射学中多平面和全身CT图像异常定位和描述的自动化挑战。

Method: 提出分类系统、构建数据集、开发OminiAbnorm-CT模型，并建立临床基准测试。

Result: OminiAbnorm-CT在所有任务和指标上显著优于现有方法。

Conclusion: 该方法在自动化CT图像解释方面具有重要临床价值。

Abstract: Automated interpretation of CT images-particularly localizing and describing
abnormal findings across multi-plane and whole-body scans-remains a significant
challenge in clinical radiology. This work aims to address this challenge
through four key contributions: (i) On taxonomy, we collaborate with senior
radiologists to propose a comprehensive hierarchical classification system,
with 404 representative abnormal findings across all body regions; (ii) On
data, we contribute a dataset containing over 14.5K CT images from multiple
planes and all human body regions, and meticulously provide grounding
annotations for over 19K abnormalities, each linked to the detailed description
and cast into the taxonomy; (iii) On model development, we propose
OminiAbnorm-CT, which can automatically ground and describe abnormal findings
on multi-plane and whole-body CT images based on text queries, while also
allowing flexible interaction through visual prompts; (iv) On benchmarks, we
establish three representative evaluation tasks based on real clinical
scenarios. Through extensive experiments, we show that OminiAbnorm-CT can
significantly outperform existing methods on all the tasks and metrics.

</details>


### [131] [Hybrid Ensemble of Segmentation-Assisted Classification and GBDT for Skin Cancer Detection with Engineered Metadata and Synthetic Lesions from ISIC 2024 Non-Dermoscopic 3D-TBP Images](https://arxiv.org/abs/2506.03420)
*Muhammad Zubair Hasan,Fahmida Yasmin Rifat*

Main category: eess.IV

TL;DR: 提出了一种结合机器学习和深度学习的混合方法，用于皮肤癌分类，采用SLICE-3D数据集，通过特征融合和增强技术取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌早期检测对患者预后至关重要，但现有方法在非皮肤镜条件下表现不足，需要更鲁棒的分类方法。

Method: 结合视觉变换器（EVA02）和卷积ViT混合模型（EdgeNeXtSAC），采用分割辅助分类管道，并通过GBDT融合预测结果。使用合成数据和重标记策略解决数据不平衡。

Result: 在部分AUC（pAUC）指标上达到0.1755，优于其他配置。

Conclusion: 混合AI系统在皮肤癌分类中表现出潜力，尤其适用于远程医疗和资源有限环境。

Abstract: Skin cancer is among the most prevalent and life-threatening diseases
worldwide, with early detection being critical to patient outcomes. This work
presents a hybrid machine and deep learning-based approach for classifying
malignant and benign skin lesions using the SLICE-3D dataset from ISIC 2024,
which comprises 401,059 cropped lesion images extracted from 3D Total Body
Photography (TBP), emulating non-dermoscopic, smartphone-like conditions. Our
method combines vision transformers (EVA02) and our designed convolutional ViT
hybrid (EdgeNeXtSAC) to extract robust features, employing a
segmentation-assisted classification pipeline to enhance lesion localization.
Predictions from these models are fused with a gradient-boosted decision tree
(GBDT) ensemble enriched by engineered features and patient-specific relational
metrics. To address class imbalance and improve generalization, we augment
malignant cases with Stable Diffusion-generated synthetic lesions and apply a
diagnosis-informed relabeling strategy to harmonize external datasets into a
3-class format. Using partial AUC (pAUC) above 80 percent true positive rate
(TPR) as the evaluation metric, our approach achieves a pAUC of 0.1755 -- the
highest among all configurations. These results underscore the potential of
hybrid, interpretable AI systems for skin cancer triage in telemedicine and
resource-constrained settings.

</details>


### [132] [Frame-Level Real-Time Assessment of Stroke Rehabilitation Exercises from Video-Level Labeled Data: Task-Specific vs. Foundation Models](https://arxiv.org/abs/2506.03752)
*Gonçalo Mesquita,Ana Rita Cóias,Artur Dubrawski,Alexandre Bernardino*

Main category: eess.IV

TL;DR: 提出了一种利用视频级标注训练帧级分类器的框架，用于中风康复中的实时运动评估，减少标注成本。


<details>
  <summary>Details</summary>
Motivation: 解决中风康复中实时运动分析系统需要昂贵帧级标注的问题。

Method: 使用梯度技术和伪标签选择方法生成帧级伪标签，结合预训练模型（Action Transformer、SkateFormer、MOMENT）提升泛化能力。

Result: 在SERE数据集上，MOMENT视频级评估AUC达73%，Action Transformer帧级评估AUC达72%，优于基线。

Conclusion: 该方法通过预训练模型提升泛化能力，减少标注需求，适用于新患者定制。

Abstract: The growing demands of stroke rehabilitation have increased the need for
solutions to support autonomous exercising. Virtual coaches can provide
real-time exercise feedback from video data, helping patients improve motor
function and keep engagement. However, training real-time motion analysis
systems demands frame-level annotations, which are time-consuming and costly to
obtain. In this work, we present a framework that learns to classify individual
frames from video-level annotations for real-time assessment of compensatory
motions in rehabilitation exercises. We use a gradient-based technique and a
pseudo-label selection method to create frame-level pseudo-labels for training
a frame-level classifier. We leverage pre-trained task-specific models - Action
Transformer, SkateFormer - and a foundation model - MOMENT - for pseudo-label
generation, aiming to improve generalization to new patients. To validate the
approach, we use the \textit{SERE} dataset with 18 post-stroke patients
performing five rehabilitation exercises annotated on compensatory motions.
MOMENT achieves better video-level assessment results (AUC = $73\%$),
outperforming the baseline LSTM (AUC = $58\%$). The Action Transformer, with
the Integrated Gradient technique, leads to better outcomes (AUC = $72\%$) for
frame-level assessment, outperforming the baseline trained with ground truth
frame-level labeling (AUC = $69\%$). We show that our proposed approach with
pre-trained models enhances model generalization ability and facilitates the
customization to new patients, reducing the demands of data labeling.

</details>


### [133] [Identifying Alzheimer's Disease Prediction Strategies of Convolutional Neural Network Classifiers using R2* Maps and Spectral Clustering](https://arxiv.org/abs/2506.03890)
*Christian Tinauer,Maximilian Sackl,Stefan Ropele,Christian Langkammer*

Main category: eess.IV

TL;DR: 该研究通过LRP和谱聚类分析深度学习模型在R2*图像上分类阿尔茨海默病的决策策略，发现预处理和训练选择对模型决策有显著影响，谱聚类可揭示分类策略差异。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在阿尔茨海默病分类中表现优异但缺乏可解释性，且可能存在决策偏差，因此需要进一步分析其决策策略。

Method: 使用3D卷积神经网络训练R2*图像，通过LRP生成热图，并应用谱聚类和t-SNE可视化分析决策模式。

Result: 谱聚类揭示了明显的决策模式，其中基于相关性的模型在AD和NC病例间分离最清晰，t-SNE验证了热图分组与受试者组的一致性。

Conclusion: 预处理和训练选择对模型决策影响显著，谱聚类为识别分类策略差异提供了结构化方法，强调了医学AI中可解释性的重要性。

Abstract: Deep learning models have shown strong performance in classifying Alzheimer's
disease (AD) from R2* maps, but their decision-making remains opaque, raising
concerns about interpretability. Previous studies suggest biases in model
decisions, necessitating further analysis. This study uses Layer-wise Relevance
Propagation (LRP) and spectral clustering to explore classifier decision
strategies across preprocessing and training configurations using R2* maps. We
trained a 3D convolutional neural network on R2* maps, generating relevance
heatmaps via LRP and applied spectral clustering to identify dominant patterns.
t-Stochastic Neighbor Embedding (t-SNE) visualization was used to assess
clustering structure. Spectral clustering revealed distinct decision patterns,
with the relevance-guided model showing the clearest separation between AD and
normal control (NC) cases. The t-SNE visualization confirmed that this model
aligned heatmap groupings with the underlying subject groups. Our findings
highlight the significant impact of preprocessing and training choices on deep
learning models trained on R2* maps, even with similar performance metrics.
Spectral clustering offers a structured method to identify classification
strategy differences, emphasizing the importance of explainability in medical
AI.

</details>


### [134] [Conformal coronary calcification volume estimation with conditional coverage via histogram clustering](https://arxiv.org/abs/2506.04030)
*Olivier Jaubert,Salman Mohammadi,Keith A. Goatman,Shadia S. Mikhael,Conor Bradley,Rebecca Hughes,Richard Good,John H. Hipwell,Sonia Dahdouh*

Main category: eess.IV

TL;DR: 提出了一种基于聚类的条件共形预测框架，用于从训练好的分割网络中提供校准覆盖的分数区间，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 通过CT扫描偶然检测和量化冠状动脉钙化可能带来早期干预，但过度报告可能对患者和医疗系统产生负面影响，因此需要谨慎自动报告钙化分数。

Method: 提出了一种基于聚类的条件共形预测框架，无需重新训练即可为3D UNet模型（确定性、MCDropout和深度集成）校准预测区间。

Result: 该方法与传统共形预测相比，达到了相似的覆盖范围，但在分流指标上表现更好。

Conclusion: 有意义的钙化分数预测区间可以帮助根据风险类别预测的置信度对患者进行分流。

Abstract: Incidental detection and quantification of coronary calcium in CT scans could
lead to the early introduction of lifesaving clinical interventions. However,
over-reporting could negatively affect patient wellbeing and unnecessarily
burden the medical system. Therefore, careful considerations should be taken
when automatically reporting coronary calcium scores. A cluster-based
conditional conformal prediction framework is proposed to provide score
intervals with calibrated coverage from trained segmentation networks without
retraining. The proposed method was tuned and used to calibrate predictive
intervals for 3D UNet models (deterministic, MCDropout and deep ensemble)
reaching similar coverage with better triage metrics compared to conventional
conformal prediction. Meaningful predictive intervals of calcium scores could
help triage patients according to the confidence of their risk category
prediction.

</details>


### [135] [Towards generating more interpretable counterfactuals via concept vectors: a preliminary study on chest X-rays](https://arxiv.org/abs/2506.04058)
*Bulat Maksudov,Kathleen Curran,Alessandra Mileo*

Main category: eess.IV

TL;DR: 通过将临床概念映射到生成模型的潜在空间，识别概念激活向量（CAVs），提供可解释的医学影像分析。


<details>
  <summary>Details</summary>
Motivation: 确保医学影像模型与临床知识对齐并具有可解释性。

Method: 使用简单的重建自编码器，将用户定义的概念与图像级特征关联，无需显式标签训练。

Result: 提取的概念在不同数据集中稳定，能生成突出临床特征的视觉解释，但对小病理的重建效果有限。

Conclusion: 该方法为基于临床概念的可解释分析提供了路径，尽管性能未超越基线。

Abstract: An essential step in deploying medical imaging models is ensuring alignment
with clinical knowledge and interpretability. We focus on mapping clinical
concepts into the latent space of generative models to identify Concept
Activation Vectors (CAVs). Using a simple reconstruction autoencoder, we link
user-defined concepts to image-level features without explicit label training.
The extracted concepts are stable across datasets, enabling visual explanations
that highlight clinically relevant features. By traversing latent space along
concept directions, we produce counterfactuals that exaggerate or reduce
specific clinical features. Preliminary results on chest X-rays show promise
for large pathologies like cardiomegaly, while smaller pathologies remain
challenging due to reconstruction limits. Although not outperforming baselines,
this approach offers a path toward interpretable, concept-based explanations
aligned with clinical knowledge.

</details>


### [136] [A Diffusion-Driven Temporal Super-Resolution and Spatial Consistency Enhancement Framework for 4D MRI imaging](https://arxiv.org/abs/2506.04116)
*Xuanru Zhou,Jiarun Liu,Shoujun Yu,Hao Yang,Cheng Li,Tao Tan,Shanshan Wang*

Main category: eess.IV

TL;DR: TSSC-Net是一个新型框架，通过扩散模型和Mamba模块解决4D MRI中快速运动导致的时空分辨率问题，提升动态MRI的质量。


<details>
  <summary>Details</summary>
Motivation: 4D MRI在快速大范围运动中时空分辨率难以兼顾，传统方法在变形大时易产生伪影和空间不一致。

Method: 提出TSSC-Net，结合扩散模型生成中间帧，并引入三向Mamba模块解决跨切片对齐问题。

Result: 在ACDC心脏MRI和动态4D膝关节数据集上验证，TSSC-Net实现了6倍时间超分辨率，保持结构保真和空间一致性。

Conclusion: TSSC-Net有效提升动态MRI在快速运动中的时空分辨率，具有临床应用潜力。

Abstract: In medical imaging, 4D MRI enables dynamic 3D visualization, yet the
trade-off between spatial and temporal resolution requires prolonged scan time
that can compromise temporal fidelity--especially during rapid, large-amplitude
motion. Traditional approaches typically rely on registration-based
interpolation to generate intermediate frames. However, these methods struggle
with large deformations, resulting in misregistration, artifacts, and
diminished spatial consistency. To address these challenges, we propose
TSSC-Net, a novel framework that generates intermediate frames while preserving
spatial consistency. To improve temporal fidelity under fast motion, our
diffusion-based temporal super-resolution network generates intermediate frames
using the start and end frames as key references, achieving 6x temporal
super-resolution in a single inference step. Additionally, we introduce a novel
tri-directional Mamba-based module that leverages long-range contextual
information to effectively resolve spatial inconsistencies arising from
cross-slice misalignment, thereby enhancing volumetric coherence and correcting
cross-slice errors. Extensive experiments were performed on the public ACDC
cardiac MRI dataset and a real-world dynamic 4D knee joint dataset. The results
demonstrate that TSSC-Net can generate high-resolution dynamic MRI from
fast-motion data while preserving structural fidelity and spatial consistency.

</details>


### [137] [A Comprehensive Study on Medical Image Segmentation using Deep Neural Networks](https://arxiv.org/abs/2506.04121)
*Loan Dao,Ngoc Quoc Ly*

Main category: eess.IV

TL;DR: 论文综述了基于深度神经网络的医学图像分割（MIS）的研究进展，重点关注DIKIW框架下的性能评估和可解释人工智能（XAI）的重要性，并探讨了MIS在疾病诊断中的潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提升MIS在疾病早期诊断中的效能，尤其是癌症患者的生存率，同时解决DNN的“黑盒”问题以满足透明性和伦理需求。

Method: 通过DIKIW框架评估现有MIS解决方案，并引入XAI技术以增强模型的可解释性。

Result: 研究总结了MIS在DIKIW各层级的最新技术，并指出XAI和早期预测是从“智能”到“智慧”的关键步骤。

Conclusion: 论文强调了MIS在医疗领域的潜力，提出了提升DNN效率的潜在解决方案，并呼吁进一步研究XAI和早期诊断技术。

Abstract: Over the past decade, Medical Image Segmentation (MIS) using Deep Neural
Networks (DNNs) has achieved significant performance improvements and holds
great promise for future developments. This paper presents a comprehensive
study on MIS based on DNNs. Intelligent Vision Systems are often evaluated
based on their output levels, such as Data, Information, Knowledge,
Intelligence, and Wisdom (DIKIW),and the state-of-the-art solutions in MIS at
these levels are the focus of research. Additionally, Explainable Artificial
Intelligence (XAI) has become an important research direction, as it aims to
uncover the "black box" nature of previous DNN architectures to meet the
requirements of transparency and ethics. The study emphasizes the importance of
MIS in disease diagnosis and early detection, particularly for increasing the
survival rate of cancer patients through timely diagnosis. XAI and early
prediction are considered two important steps in the journey from
"intelligence" to "wisdom." Additionally, the paper addresses existing
challenges and proposes potential solutions to enhance the efficiency of
implementing DNN-based MIS.

</details>


### [138] [Recent Advances in Medical Image Classification](https://arxiv.org/abs/2506.04129)
*Loan Dao,Ngoc Quoc Ly*

Main category: eess.IV

TL;DR: 论文综述了医学图像分类领域的最新进展，重点关注基础、特定和应用三个层面的解决方案，并探讨了深度学习和视觉语言模型的应用。


<details>
  <summary>Details</summary>
Motivation: 医学图像分类对诊断和治疗至关重要，人工智能的进步为其提供了显著支持。

Method: 论文分析了传统方法（如卷积神经网络和视觉变换器）以及前沿的视觉语言模型，并探讨了可解释人工智能在增强和解释预测结果中的作用。

Result: 这些方法解决了标记数据有限的问题，并通过可解释人工智能提升了预测结果的可靠性和透明度。

Conclusion: 论文总结了当前技术的进展，并强调了可解释人工智能在医学图像分类中的重要性。

Abstract: Medical image classification is crucial for diagnosis and treatment,
benefiting significantly from advancements in artificial intelligence. The
paper reviews recent progress in the field, focusing on three levels of
solutions: basic, specific, and applied. It highlights advances in traditional
methods using deep learning models like Convolutional Neural Networks and
Vision Transformers, as well as state-of-the-art approaches with Vision
Language Models. These models tackle the issue of limited labeled data, and
enhance and explain predictive results through Explainable Artificial
Intelligence.

</details>


### [139] [Synthetic multi-inversion time magnetic resonance images for visualization of subcortical structures](https://arxiv.org/abs/2506.04173)
*Savannah P. Hays,Lianrui Zuo,Anqi Feng,Yihao Liu,Blake E. Dewey,Jiachen Zhuo,Ellen M. Mowry,Scott D. Newsome Jerry L. Prince,Aaron Carass*

Main category: eess.IV

TL;DR: SyMTIC是一种深度学习方法，通过常规MRI图像生成高质量的多反转时间（multi-TI）图像，提升皮层下灰质的可视化效果。


<details>
  <summary>Details</summary>
Motivation: 多反转时间T1加权MRI在临床中很少使用，但其对皮层下结构的可视化至关重要。SyMTIC旨在通过常规MRI图像生成多TI图像，解决这一需求。

Method: 结合深度神经网络和成像物理，从常规T1-w、T2-w和FLAIR图像估计T1和质子密度图，进而生成任意反转时间的多TI图像。

Result: SyMTIC生成的图像质量接近实际采集的多TI数据，显著提升了皮层下结构的可视化和丘脑核团的分割效果。

Conclusion: SyMTIC为临床提供了一种实用的解决方案，能够从常规MRI图像中生成高质量的多TI图像，适用于不同数据集。

Abstract: Purpose: Visualization of subcortical gray matter is essential in
neuroscience and clinical practice, particularly for disease understanding and
surgical planning.While multi-inversion time (multi-TI) T$_1$-weighted
(T$_1$-w) magnetic resonance (MR) imaging improves visualization, it is rarely
acquired in clinical settings. Approach: We present SyMTIC (Synthetic Multi-TI
Contrasts), a deep learning method that generates synthetic multi-TI images
using routinely acquired T$_1$-w, T$_2$-weighted (T$_2$-w), and FLAIR images.
Our approach combines image translation via deep neural networks with imaging
physics to estimate longitudinal relaxation time (T$_1$) and proton density
(PD) maps. These maps are then used to compute multi-TI images with arbitrary
inversion times. Results: SyMTIC was trained using paired MPRAGE and FGATIR
images along with T$_2$-w and FLAIR images. It accurately synthesized multi-TI
images from standard clinical inputs, achieving image quality comparable to
that from explicitly acquired multi-TI data.The synthetic images, especially
for TI values between 400-800 ms, enhanced visualization of subcortical
structures and improved segmentation of thalamic nuclei. Conclusion: SyMTIC
enables robust generation of high-quality multi-TI images from routine MR
contrasts. It generalizes well to varied clinical datasets, including those
with missing FLAIR images or unknown parameters, offering a practical solution
for improving brain MR image visualization and analysis.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [140] [Multi-Spectral Gaussian Splatting with Neural Color Representation](https://arxiv.org/abs/2506.03407)
*Lukas Meyer,Josef Grün,Maximilian Weiherer,Bernhard Egger,Marc Stamminger,Linus Franke*

Main category: cs.GR

TL;DR: MS-Splatting是一种多光谱3D高斯泼溅框架，能够从不同光谱域的独立相机图像生成多视角一致的新视图，无需跨模态相机标定。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效利用光谱和空间相关性，且需要复杂的跨模态标定。MS-Splatting旨在解决这些问题，支持多种光谱（如热红外和近红外）的统一建模。

Method: 采用神经颜色表示，将多光谱信息编码为紧凑的每泼溅特征嵌入，并通过浅层MLP解码为光谱颜色值，实现所有波段的联合学习。

Result: 实验表明，该方法提升了多光谱渲染质量，并在单光谱渲染质量上优于现有技术。

Conclusion: MS-Splatting是一种简单有效的多光谱渲染方法，适用于农业等领域的植被指数渲染。

Abstract: We present MS-Splatting -- a multi-spectral 3D Gaussian Splatting (3DGS)
framework that is able to generate multi-view consistent novel views from
images of multiple, independent cameras with different spectral domains. In
contrast to previous approaches, our method does not require cross-modal camera
calibration and is versatile enough to model a variety of different spectra,
including thermal and near-infra red, without any algorithmic changes.
  Unlike existing 3DGS-based frameworks that treat each modality separately (by
optimizing per-channel spherical harmonics) and therefore fail to exploit the
underlying spectral and spatial correlations, our method leverages a novel
neural color representation that encodes multi-spectral information into a
learned, compact, per-splat feature embedding. A shallow multi-layer perceptron
(MLP) then decodes this embedding to obtain spectral color values, enabling
joint learning of all bands within a unified representation.
  Our experiments show that this simple yet effective strategy is able to
improve multi-spectral rendering quality, while also leading to improved
per-spectra rendering quality over state-of-the-art methods. We demonstrate the
effectiveness of this new technique in agricultural applications to render
vegetation indices, such as normalized difference vegetation index (NDVI).

</details>


### [141] [Facial Appearance Capture at Home with Patch-Level Reflectance Prior](https://arxiv.org/abs/2506.03478)
*Yuxuan Han,Junfeng Lyu,Kuan Sheng,Minghao Que,Qixuan Zhang,Lan Xu,Feng Xu*

Main category: cs.GR

TL;DR: 本文提出了一种基于智能手机和闪光灯的低成本面部反射捕捉方法，通过扩散先验和补丁级后采样技术，显著提升了重建质量，接近工作室录制水平。


<details>
  <summary>Details</summary>
Motivation: 现有智能手机视频捕捉的面部反射重建质量远低于工作室录制，本文旨在填补这一差距，提供日常可用的高质量解决方案。

Method: 利用扩散先验学习Light Stage扫描数据分布，并通过补丁级训练提升泛化能力；提出补丁级后采样技术生成无缝高分辨率反射图。

Result: 实验表明，该方法大幅缩小了低成本与工作室录制之间的质量差距，使日常用户能够轻松数字化自身面部。

Conclusion: 该方法为日常用户提供了一种高质量的面部反射捕捉方案，代码已开源。

Abstract: Existing facial appearance capture methods can reconstruct plausible facial
reflectance from smartphone-recorded videos. However, the reconstruction
quality is still far behind the ones based on studio recordings. This paper
fills the gap by developing a novel daily-used solution with a co-located
smartphone and flashlight video capture setting in a dim room. To enhance the
quality, our key observation is to solve facial reflectance maps within the
data distribution of studio-scanned ones. Specifically, we first learn a
diffusion prior over the Light Stage scans and then steer it to produce the
reflectance map that best matches the captured images. We propose to train the
diffusion prior at the patch level to improve generalization ability and
training stability, as current Light Stage datasets are in ultra-high
resolution but limited in data size. Tailored to this prior, we propose a
patch-level posterior sampling technique to sample seamless full-resolution
reflectance maps from this patch-level diffusion model. Experiments demonstrate
our method closes the quality gap between low-cost and studio recordings by a
large margin, opening the door for everyday users to clone themselves to the
digital world. Our code will be released at https://github.com/yxuhan/DoRA.

</details>


### [142] [SplArt: Articulation Estimation and Part-Level Reconstruction with 3D Gaussian Splatting](https://arxiv.org/abs/2506.03594)
*Shengjie Lin,Jiading Fang,Muhammad Zubair Irshad,Vitor Campagnolo Guizilini,Rares Andrei Ambrus,Greg Shakhnarovich,Matthew R. Walter*

Main category: cs.GR

TL;DR: SplArt是一个自监督、类别无关的框架，利用3D高斯泼溅技术重建铰接物体并推断运动学，实现实时逼真渲染。


<details>
  <summary>Details</summary>
Motivation: 现有方法在可扩展性、鲁棒性和渲染质量方面存在不足，需要3D监督或昂贵标注。

Method: 通过为每个高斯添加可微移动参数，采用多阶段优化策略处理重建、部件分割和运动估计。

Result: 在基准测试和实际应用中表现出色，无需3D标注或类别先验。

Conclusion: SplArt在铰接物体重建和渲染方面具有先进性能和实用性。

Abstract: Reconstructing articulated objects prevalent in daily environments is crucial
for applications in augmented/virtual reality and robotics. However, existing
methods face scalability limitations (requiring 3D supervision or costly
annotations), robustness issues (being susceptible to local optima), and
rendering shortcomings (lacking speed or photorealism). We introduce SplArt, a
self-supervised, category-agnostic framework that leverages 3D Gaussian
Splatting (3DGS) to reconstruct articulated objects and infer kinematics from
two sets of posed RGB images captured at different articulation states,
enabling real-time photorealistic rendering for novel viewpoints and
articulations. SplArt augments 3DGS with a differentiable mobility parameter
per Gaussian, achieving refined part segmentation. A multi-stage optimization
strategy is employed to progressively handle reconstruction, part segmentation,
and articulation estimation, significantly enhancing robustness and accuracy.
SplArt exploits geometric self-supervision, effectively addressing challenging
scenarios without requiring 3D annotations or category-specific priors.
Evaluations on established and newly proposed benchmarks, along with
applications to real-world scenarios using a handheld RGB camera, demonstrate
SplArt's state-of-the-art performance and real-world practicality. Code is
publicly available at https://github.com/ripl/splart.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [143] [Low-EFFourth: A computational framework for generating and studying multilevel model ensembles in low-dimensional systems](https://arxiv.org/abs/2506.03313)
*Francisco de Melo Viríssimo*

Main category: physics.geo-ph

TL;DR: LEF4是一个基于MATLAB的计算框架，用于生成和研究连续动力系统中的多级模型集合，适用于气候建模及其他领域。


<details>
  <summary>Details</summary>
Motivation: 解决气候建模中的问题，并扩展到流行病学、经济学和工程学等领域。

Method: 提供高效灵活的方法，研究初始条件、模型参数、数值方法和模型公式带来的不确定性。

Result: LEF4框架适用于多学科，代码修改少，功能强大。

Conclusion: LEF4是一个多功能工具，为多学科研究提供了高效的计算支持。

Abstract: This paper introduces Low-EFFourth (LEF4), a MATLAB-based computational
framework designed for generating and studying multilevel model ensembles in
continuous dynamical systems. Initially developed to address questions in
climate modelling, LEF4 can also be used in other disciplines such as
epidemiology, economics, and engineering, with minimal modifications to the
code. The framework provides an efficient and flexible approach for
investigating uncertainties arising from initial conditions, model parameters,
numerical methods, and model formulation. This preprint serves as a formal
reference for the LEF4 codebase and provides a concise technical and conceptual
overview of its purpose, structure, applications and development pipeline.

</details>


### [144] [Model for the Thermodynamics of Iron at High Pressures Near Melting](https://arxiv.org/abs/2506.03386)
*Rann Shikler,Noya Dimanstein Firman,Yinon Ashkenazy*

Main category: physics.geo-ph

TL;DR: 基于间隙理论（ITCM）的Fe相图与熔化线模型，扩展至地球内核边界条件，解释了熔化线矛盾数据并提出新的液相相变。


<details>
  <summary>Details</summary>
Motivation: 为铁核行星模型提供约束，解决Fe熔化线数据矛盾及内核成核悖论。

Method: 采用间隙理论（ITCM）描述金属固液相变，扩展模型以覆盖宽压力-温度范围，拟合熔化数据并外推至内核边界条件。

Result: 模型解释了熔化线矛盾数据，提出FCC与HCP液相间的新相变，为高压近熔化相提供新解释。

Conclusion: ITCM模型成功应用于Fe相图，解决了熔化线争议，并提出内核成核悖论的潜在解决方案。

Abstract: The Fe pressure-temperature phase diagram and its melting line have a wide
range of applications, including providing constraints for iron-core planetary
models. We propose an equation of state (EOS) model based on the interstitial
theory of simple condensed matter (ITCM), as suggested by A.V. Granato. When
applied to Fe, this model enables the extrapolation of measured melting lines
to the conditions of the Earth's inner core boundary (ICB). The ITCM describes
the solid-liquid phase transition in metals as resulting from a strong
structural perturbation due to a high concentration of interstitial-like
defects. The strong nonlinearity of their self-interaction causes the
stabilization of this interstitial-rich phase. The original model is expanded
to describe melting over a wide range of pressures and temperatures rather than
focusing on a specific isobaric transition. Using this model, we fit the
measured melting data, extrapolate it to cover ICB conditions, and develop a
multiphase equation of state that encompasses this regime. The model is used to
explain contradictory data regarding the location of the melting line,
resulting from a novel phase transition between two separate liquid phases,
specifically between FCC-based and HCP-based liquids. This additional liquid
phase offers a new interpretation of the previously suggested near-melting
high-pressure phase and may also provide a solution to the inner core
nucleation paradox.

</details>


### [145] [Geo-Disasters: geocoding climate-related events in the international disaster database EM-DAT](https://arxiv.org/abs/2506.03797)
*Khalil Teber,Mélanie Weynants,Fabian Gans,Miguel D. Mahecha*

Main category: physics.geo-ph

TL;DR: 对1990-2023年EM-DAT数据库中9,217起气候相关灾害进行地理编码，提供可复现的更新框架，以提高灾害影响研究的准确性。


<details>
  <summary>Details</summary>
Motivation: 气候灾害可能演变为人道主义灾难，但现有数据库EM-DAT缺乏精确地理信息，限制了其应用。

Method: 开发地理编码方法，即使仅提供区域名称也能保持准确性，并引入质量标志评估可靠性。

Result: 增强后的EM-DAT支持与其他地理编码数据整合，提升气候灾害影响和适应缺陷评估的准确性。

Conclusion: 通过地理编码和开放框架，填补了EM-DAT的地理信息空白，为灾害研究提供了更可靠的数据支持。

Abstract: Climate hazards can escalate into humanitarian disasters. Understanding their
trajectories -- considering hazard intensity, human exposure, and societal
vulnerability -- is essential for effective anticipatory action. The
International Disaster Database (EM-DAT) is the only freely available global
resource of humanitarian disaster records. However, it lacks exact geospatial
information, limiting its use for climate hazard impact research. Here, we
provide geocoding of 9,217 climate-related disasters reported by EM-DAT from
1990 to 2023, along with an open, reproducible framework for updating. Our
method remains accurate even when only region names are available and includes
quality flags to assess reliability. The augmented EM-DAT enables integration
with other geocoded data, supporting more accurate assessment of climate
disaster impacts and adaptation deficits.

</details>


### [146] [Determination of Effect of the Movement of a Finite, Dip-slip Fault in Viscoelastic Half-space of Fractional Burger Rheology](https://arxiv.org/abs/2506.03977)
*Pabita Mahato,Seema Sarkar*

Main category: physics.geo-ph

TL;DR: 论文研究了地震活跃区域的应力与应变积累/释放，通过数学建模分析了倾斜断层在黏弹性半空间中的行为，发现断层倾角和蠕变速度对位移、应力和应变有显著影响。


<details>
  <summary>Details</summary>
Motivation: 探究地震活跃区域在非地震期的变形及应力应变积累/释放机制，以理解断层运动对地震的影响。

Method: 采用分数阶Burgers流变学模型，结合拉普拉斯变换、修正格林函数技术和对应原理，推导位移、应力和应变的解析解，并用MATLAB进行图形分析。

Result: 断层倾角和蠕变速度的变化对位移、应力和应变有显著影响，而分数阶导数阶数的影响较小。

Conclusion: 研究结果为理解地下变形及其对断层运动和地震的影响提供了新视角。

Abstract: The seismically active regions often correlate with fault lines, and the
movement of these faults plays a crucial role in defining how stress is stored
or released in these areas. To investigate the deformation and
accumulation/release of stress and strain in seismically active regions during
the aseismic period, a mathematical model has been developed by considering a
finite, creeping dip-slip fault inclined in the viscoelastic half-space of a
fractional Burger rheology. Laplace transformation for fractional derivatives,
Modified Green's function technique, correspondence principle and finally, the
inverse Laplace transformation have been used to derive analytical solutions
for displacement, stress and strain components. The graphical representations
were depicted using MATLAB to understand the effect on displacement, stresses
and strains due to changes in inclinations and creep velocities of the fault,
as well as orders of the fractional derivative. Our investigation indicates
that a change in creep velocity and inclination of the fault has a significant
effect, while a change in the order of fractional derivative has a moderate
effect on displacement, stress, and strain components. Analysis of these
results can provide insights into subsurface deformation and its impact on
fault movement, which can lead to earthquakes.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [147] [3D Holographic Flow Cytometry Measurements of Microalgae: Strategies for Angle Recovery in Complex Rotation Patterns](https://arxiv.org/abs/2506.03738)
*Francesca Borrelli,Giusy Giugliano,Emilie Houliez,Jaromir Behal,Daniele Pirone,Leonilde Roselli,Angela Sardo,Valerio Zupo,Maria Costantini,Lisa Miccio,Pasquale Memmolo,Vittorio Bianco,Pietro Ferraro*

Main category: physics.bio-ph

TL;DR: 本文提出了一种基于全息显微镜的工作流程，用于获取微藻的3D信息，解决了传统显微镜在物种识别和形态评估中的局限性。


<details>
  <summary>Details</summary>
Motivation: 由于环境变化威胁海洋生态系统的生物多样性和功能，微藻作为关键生态组成部分需要更先进的监测技术。

Method: 使用全息显微镜在流式细胞模式下工作，根据样本的旋转模式定制方法以获取其滚动角度，实现3D折射率映射和形态重建。

Result: 实验证明该方法能够对透明和低散射微藻进行3D折射率映射，对高吸收或强散射的微藻也能实现3D形态重建。

Conclusion: 该技术为非侵入性获取微藻3D信息提供了突破，为水生生态系统研究开辟了新途径。

Abstract: Marine ecosystems are in the spotlight, because environmental changes are
threatening biodiversity and ecological functions. In this context, microalgae
play key ecological roles both in planktonic and benthic ecosystems.
Consequently, they are considered indispensable targets for global monitoring
programs. However, due to a high spatial and temporal variability and to
difficulties of species identification (still relying on microscopy
observations), the assessment of roles played by these components of marine
ecosystems is demanding. In addition, technologies for a 3D assessment of their
complex morphology are scarcely available. Here, we present a comprehensive
workflow for retrieving 3D information on microalgae with diverse geometries
through holographic microscopy operating in flow-cytometry mode. Depending on
the rotation patterns of samples, a tailored approach is used to retrieve their
rolling angles. We demonstrate the feasibility of measuring 3D data of various
microalgae, contingent to the intrinsic optical properties of cells.
Specifically, we show that for quasi-transparent and low-scattering
microorganisms, the retrieved angles permit to achieve quantitative 3D
tomographic Refractive Index (RI) mapping, providing a full characterization of
the alga in terms of its inner structure and the outer shape. Moreover, even in
the most challenging scenarios, where microalgae exhibit high light absorption
or strong scattering, quantitative 3D shape reconstructions of diatoms and
dinoflagellates can be at least achieved. Finally, we compare our direct 3D
measurements with 2D inferences of 3D properties, obtained using a commercially
available microscopy system. The ability to non-invasively obtain 3D
information on microalgae marks a fundamental advancement in the field,
unlocking a wealth of novel biological insights for characterizing aquatic
ecosystems.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [148] [How Far Are We from Predicting Missing Modalities with Foundation Models?](https://arxiv.org/abs/2506.03530)
*Guanzhou Ke,Yi Xie,Xiaoli Wang,Guoqing Chao,Bo Wang,Shengfeng He*

Main category: cs.MM

TL;DR: 本文探讨了多模态基础模型在缺失模态预测中的潜力，提出了一个动态代理框架和自我优化机制，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在细粒度语义提取和生成模态验证方面表现不足，导致预测结果不理想。

Method: 提出动态代理框架和自我优化机制，动态制定模态感知策略并迭代验证生成模态质量。

Result: 实验显示，该方法在缺失图像预测中FID降低至少14%，缺失文本预测中MER降低至少10%。

Conclusion: 所提框架有效解决了现有模型的局限性，显著提升了缺失模态预测的准确性和鲁棒性。

Abstract: Multimodal foundation models have demonstrated impressive capabilities across
diverse tasks. However, their potential as plug-and-play solutions for missing
modality prediction remains underexplored. To investigate this, we categorize
existing approaches into three representative paradigms, encompassing a total
of 42 model variants, and conduct a comprehensive evaluation in terms of
prediction accuracy and adaptability to downstream tasks. Our analysis reveals
that current foundation models often fall short in two critical aspects: (i)
fine-grained semantic extraction from the available modalities, and (ii) robust
validation of generated modalities. These limitations lead to suboptimal and,
at times, misaligned predictions. To address these challenges, we propose an
agentic framework tailored for missing modality prediction. This framework
dynamically formulates modality-aware mining strategies based on the input
context, facilitating the extraction of richer and more discriminative semantic
features. In addition, we introduce a \textit{self-refinement mechanism}, which
iteratively verifies and enhances the quality of generated modalities through
internal feedback. Experimental results show that our method reduces FID for
missing image prediction by at least 14% and MER for missing text prediction by
at least 10% compared to baselines.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [149] [Urban Visibility Hotspots: Quantifying Building Vertex Visibility from Connected Vehicle Trajectories using Spatial Indexing](https://arxiv.org/abs/2506.03365)
*Artur Grigorev,Adriana-Simona Mihaita*

Main category: eess.SY

TL;DR: 提出了一种基于车辆轨迹数据的户外广告和街道家具位置优化方法，通过动态视野建模和空间索引高效计算视觉曝光。


<details>
  <summary>Details</summary>
Motivation: 传统选址方法依赖静态数据或主观评估，缺乏客观性。研究旨在通过数据驱动方法量化位置视觉曝光。

Method: 利用车辆轨迹数据和OpenStreetMap建筑顶点，构建动态视野模型和BallTree空间索引，高效计算视觉曝光。

Result: 发现视觉曝光高度集中，存在‘视觉热点’，且曝光量符合对数正态分布。

Conclusion: 该方法为户外广告和街道家具的精准选址提供了客观、高效的解决方案。

Abstract: Effective placement of Out-of-Home advertising and street furniture requires
accurate identification of locations offering maximum visual exposure to target
audiences, particularly vehicular traffic. Traditional site selection methods
often rely on static traffic counts or subjective assessments. This research
introduces a data-driven methodology to objectively quantify location
visibility by analyzing large-scale connected vehicle trajectory data (sourced
from Compass IoT) within urban environments. We model the dynamic driver
field-of-view using a forward-projected visibility area for each vehicle
position derived from interpolated trajectories. By integrating this with
building vertex locations extracted from OpenStreetMap, we quantify the
cumulative visual exposure, or ``visibility count'', for thousands of potential
points of interest near roadways. The analysis reveals that visibility is
highly concentrated, identifying specific ``visual hotspots'' that receive
disproportionately high exposure compared to average locations. The core
technical contribution involves the construction of a BallTree spatial index
over building vertices. This enables highly efficient (O(logN) complexity)
radius queries to determine which vertices fall within the viewing circles of
millions of trajectory points across numerous trips, significantly
outperforming brute-force geometric checks. Analysis reveals two key findings:
1) Visibility is highly concentrated, identifying distinct 'visual hotspots'
receiving disproportionately high exposure compared to average locations. 2)
The aggregated visibility counts across vertices conform to a Log-Normal
distribution.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [150] [The QTF-Backbone: Proposal for a Nationwide Optical Fibre Backbone in Germany for Quantum Technology and Time and Frequency Metrology](https://arxiv.org/abs/2506.03998)
*Klaus Blaum,Peter Kaufmann,Jochen Kronjäger,Stefan Kück,Tara Cubel Liebisch,Dieter Meschede,Susanne Naegele-Jackson,Stephan Schiller,Harald Schnatz*

Main category: physics.ins-det

TL;DR: QTF-Backbone提案：德国专用光纤基础设施，用于量子信息和时间频率信号分发，支持全国性生态系统。


<details>
  <summary>Details</summary>
Motivation: 量子信息和时间频率信号在长距离光纤网络中的分发具有变革潜力，但目前仅限于少数机构。

Method: 通过四阶段部署，利用暗光纤和专用硬件，构建QTF-Backbone基础设施。

Result: 支持全国性量子与时间频率生态系统，推动德国和欧洲在前沿领域的领先地位。

Conclusion: QTF-Backbone将促进创新向实际部署的过渡，成为国家及欧洲的枢纽。

Abstract: The recent breakthroughs in the distribution of quantum information and
high-precision time and frequency (T&F) signals over long-haul optical fibre
networks have transformative potential for physically secure communications,
resilience of Global Navigation Satellite Systems (GNSS) and fundamental
physics. However, so far these capabilities remain confined to isolated
testbeds, with quantum and T&F signals accessible, for example in Germany, to
only a few institutions.
  We propose the QTF-Backbone: a dedicated national fibre-optic infrastructure
in Germany for the networked distribution of quantum and T&F signals using dark
fibres and specialized hardware. The QTF-Backbone is planned as a four-phase
deployment over ten years to ensure scalable, sustainable access for research
institutions and industry. The concept builds on successful demonstrations of
high-TRL time and frequency distribution across Europe, including PTB-MPQ links
in Germany, REFIMEVE in France, and the Italian LIFT network. The QTF-Backbone
will enable transformative R&D, support a nationwide QTF ecosystem, and ensure
the transition from innovation to deployment. As a national and European hub,
it will position Germany and Europe at the forefront of quantum networking, as
well as time and frequency transfer.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [151] [Solving Inverse Problems via Diffusion-Based Priors: An Approximation-Free Ensemble Sampling Approach](https://arxiv.org/abs/2506.03979)
*Haoxuan Chen,Yinuo Ren,Martin Renqiang Min,Lexing Ying,Zachary Izzo*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型（DMs）的集成算法，用于贝叶斯逆问题（BIPs）的后验采样，避免了启发式近似，并通过理论和实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的后验采样方法依赖启发式近似，限制了其生成能力。本文旨在利用扩散模型的生成能力，避免近似，提高后验采样的准确性。

Method: 提出了一种集成算法，结合扩散模型和顺序蒙特卡罗（SMC）方法，通过分析扩散过程中先验的演化，推导出修正的偏微分方程（PDE），并利用加权粒子方法模拟。

Result: 理论证明后验分布误差受预训练得分函数训练误差和粒子数限制；实验表明在成像逆问题中，该方法比现有方法重建更准确。

Conclusion: 本文方法避免了启发式近似，通过修正PDE和加权粒子方法实现了更准确的后验采样，理论和实验均验证了其有效性。

Abstract: Diffusion models (DMs) have proven to be effective in modeling
high-dimensional distributions, leading to their widespread adoption for
representing complex priors in Bayesian inverse problems (BIPs). However,
current DM-based posterior sampling methods proposed for solving common BIPs
rely on heuristic approximations to the generative process. To exploit the
generative capability of DMs and avoid the usage of such approximations, we
propose an ensemble-based algorithm that performs posterior sampling without
the use of heuristic approximations. Our algorithm is motivated by existing
works that combine DM-based methods with the sequential Monte Carlo (SMC)
method. By examining how the prior evolves through the diffusion process
encoded by the pre-trained score function, we derive a modified partial
differential equation (PDE) governing the evolution of the corresponding
posterior distribution. This PDE includes a modified diffusion term and a
reweighting term, which can be simulated via stochastic weighted particle
methods. Theoretically, we prove that the error between the true posterior
distribution can be bounded in terms of the training error of the pre-trained
score function and the number of particles in the ensemble. Empirically, we
validate our algorithm on several inverse problems in imaging to show that our
method gives more accurate reconstructions compared to existing DM-based
methods.

</details>


### [152] [DUAL: Dynamic Uncertainty-Aware Learning](https://arxiv.org/abs/2506.03158)
*Jiahao Qin,Bei Peng,Feng Liu,Guangliang Cheng,Lu Zong*

Main category: cs.LG

TL;DR: 论文提出了一种名为DUAL的动态不确定性感知学习框架，用于处理单模态和多模态场景中的特征不确定性，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在多模态场景中面临特征不确定性的挑战，影响性能和可靠性。

Method: DUAL框架包含动态特征不确定性建模、自适应分布感知调制和不确定性感知跨模态关系学习。

Result: 在多个任务中表现优异，如CIFAR-10准确率提升7.1%，CMU-MOSEI情感分析准确率提升4.1%。

Conclusion: DUAL框架有效解决了特征不确定性问题，并在多个领域验证了其优越性。

Abstract: Deep learning models frequently encounter feature uncertainty in diverse
learning scenarios, significantly impacting their performance and reliability.
This challenge is particularly complex in multi-modal scenarios, where models
must integrate information from different sources with inherent uncertainties.
We propose Dynamic Uncertainty-Aware Learning (DUAL), a unified framework that
effectively handles feature uncertainty in both single-modal and multi-modal
scenarios. DUAL introduces three key innovations: Dynamic Feature Uncertainty
Modeling, which continuously refines uncertainty estimates through joint
consideration of feature characteristics and learning dynamics; Adaptive
Distribution-Aware Modulation, which maintains balanced feature distributions
through dynamic sample influence adjustment; and Uncertainty-aware Cross-Modal
Relationship Learning, which explicitly models uncertainties in cross-modal
interactions. Through extensive experiments, we demonstrate DUAL's
effectiveness across multiple domains: in computer vision tasks, it achieves
substantial improvements of 7.1% accuracy on CIFAR-10, 6.5% accuracy on
CIFAR-100, and 2.3% accuracy on Tiny-ImageNet; in multi-modal learning, it
demonstrates consistent gains of 4.1% accuracy on CMU-MOSEI and 2.8% accuracy
on CMU-MOSI for sentiment analysis, while achieving 1.4% accuracy improvements
on MISR. The code will be available on GitHub soon.

</details>


### [153] [Robustness in Both Domains: CLIP Needs a Robust Text Encoder](https://arxiv.org/abs/2506.03355)
*Elias Abad Rocamora,Christian Schlarmann,Naman Deep Singh,Yongtao Wu,Matthias Hein,Volkan Cevher*

Main category: cs.LG

TL;DR: LEAF是一种高效的对抗性微调方法，用于提升CLIP文本编码器的鲁棒性，同时保持视觉性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注CLIP图像编码器的鲁棒性，而文本编码器的鲁棒性尚未探索，本文填补了这一空白。

Method: 提出LEAF方法，对CLIP文本编码器进行对抗性微调，适用于大规模模型。

Result: 显著提升文本域的零样本对抗准确性，改善对抗噪声下的生成质量和多模态检索任务中的召回率。

Conclusion: 鲁棒的文本编码器能通过直接优化更好地重建输入文本，提升整体模型的鲁棒性。

Abstract: Adversarial input attacks can cause a significant shift of CLIP embeddings.
This can affect the downstream robustness of models incorporating CLIP in the
pipeline, such as text-to-image generative models or large vision language
models. While some efforts have been done towards making the CLIP image
encoders robust, the robustness of text encoders remains unexplored. In this
work, we cover this gap in the literature. We propose LEAF: an efficient
adversarial finetuning method for the text domain, with the ability to scale to
large CLIP models. Our models significantly improve the zero-shot adversarial
accuracy in the text domain, while maintaining the vision performance provided
by robust image encoders. When combined with text-to-image diffusion models, we
can improve the generation quality under adversarial noise. When employing our
robust CLIP encoders in multimodal retrieval tasks, we improve the recall under
adversarial noise over standard CLIP models. Finally, we show that robust text
encoders facilitate better reconstruction of input text from its embedding via
direct optimization.

</details>


### [154] [Rethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspective](https://arxiv.org/abs/2506.03951)
*Aojun Lu,Hangjie Yuan,Tao Feng,Yanan Sun*

Main category: cs.LG

TL;DR: 论文提出Dual-Arch框架，通过双网络设计解决持续学习中的稳定性与可塑性冲突，实验证明其性能优越且参数更紧凑。


<details>
  <summary>Details</summary>
Motivation: 持续学习需要平衡稳定性与可塑性，现有方法多关注参数层面，忽视了网络架构的影响。

Method: 提出Dual-Arch框架，利用两个独立网络分别专注于可塑性和稳定性，架构轻量且互补。

Result: 实验表明Dual-Arch提升了现有方法的性能，参数减少了87%。

Conclusion: Dual-Arch通过架构层面的创新，有效解决了持续学习中的稳定性与可塑性冲突。

Abstract: The quest for Continual Learning (CL) seeks to empower neural networks with
the ability to learn and adapt incrementally. Central to this pursuit is
addressing the stability-plasticity dilemma, which involves striking a balance
between two conflicting objectives: preserving previously learned knowledge and
acquiring new knowledge. While numerous CL methods aim to achieve this
trade-off, they often overlook the impact of network architecture on stability
and plasticity, restricting the trade-off to the parameter level. In this
paper, we delve into the conflict between stability and plasticity at the
architectural level. We reveal that under an equal parameter constraint, deeper
networks exhibit better plasticity, while wider networks are characterized by
superior stability. To address this architectural-level dilemma, we introduce a
novel framework denoted Dual-Arch, which serves as a plug-in component for CL.
This framework leverages the complementary strengths of two distinct and
independent networks: one dedicated to plasticity and the other to stability.
Each network is designed with a specialized and lightweight architecture,
tailored to its respective objective. Extensive experiments demonstrate that
Dual-Arch enhances the performance of existing CL methods while being up to 87%
more compact in terms of parameters.

</details>


### [155] [Adapt before Continual Learning](https://arxiv.org/abs/2506.03956)
*Aojun Lu,Tao Feng,Hangjie Yuan,Chunhui Ding,Yanan Sun*

Main category: cs.LG

TL;DR: 论文提出了一种名为ACL的新框架，通过在核心持续学习过程之前对预训练模型（PTM）进行适应性调整，以平衡稳定性和可塑性。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中预训练模型在冻结主干时限制可塑性，而全模型微调又容易导致灾难性遗忘的问题。

Method: 提出ACL框架，在每学习新任务前通过适应性阶段调整PTM主干，结合现有持续学习方法（如提示调优）。

Result: 实验表明ACL显著提升了持续学习的性能，平衡了稳定性和可塑性。

Conclusion: ACL为基于PTM的持续学习提供了一种通用解决方案。

Abstract: Continual Learning (CL) seeks to enable neural networks to incrementally
acquire new knowledge (plasticity) while retaining existing knowledge
(stability). While pre-trained models (PTMs) have become pivotal in CL,
prevailing approaches freeze the PTM backbone to preserve stability, limiting
their plasticity, particularly when encountering significant domain gaps in
incremental tasks. Conversely, sequentially finetuning the entire PTM risks
catastrophic forgetting of generalizable knowledge, exposing a critical
stability-plasticity trade-off. To address this challenge, we propose Adapting
PTMs before the core CL process (ACL), a novel framework that refines the PTM
backbone through a plug-and-play adaptation phase before learning each new task
with existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by
aligning embeddings with their original class prototypes while distancing them
from others, theoretically and empirically shown to balance stability and
plasticity. Extensive experiments demonstrate that ACL significantly improves
CL performance across benchmarks and integrated methods, offering a versatile
solution for PTM-based CL.

</details>


### [156] [Optimal Transport-based Domain Alignment as a Preprocessing Step for Federated Learning](https://arxiv.org/abs/2506.04071)
*Luiz Manella Pereira,M. Hadi Amini*

Main category: cs.LG

TL;DR: 论文提出了一种基于最优传输的预处理算法，用于解决联邦学习中的数据集不平衡问题，通过最小化边缘设备间的数据分布差异提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据不平衡问题会导致全局模型聚合性能下降，影响本地模型更新和决策准确性。

Method: 利用Wasserstein重心计算通道平均值，生成目标RGB空间，通过投影数据集最小化全局分布差异。

Result: 在CIFAR-10数据集上验证了该方法能在更少的通信轮次中实现更高的泛化能力。

Conclusion: 该方法有效解决了联邦学习中的数据不平衡问题，提升了模型性能和效率。

Abstract: Federated learning (FL) is a subfield of machine learning that avoids sharing
local data with a central server, which can enhance privacy and scalability.
The inability to consolidate data leads to a unique problem called dataset
imbalance, where agents in a network do not have equal representation of the
labels one is trying to learn to predict. In FL, fusing locally-trained models
with unbalanced datasets may deteriorate the performance of global model
aggregation, and reduce the quality of updated local models and the accuracy of
the distributed agents' decisions. In this work, we introduce an Optimal
Transport-based preprocessing algorithm that aligns the datasets by minimizing
the distributional discrepancy of data along the edge devices. We accomplish
this by leveraging Wasserstein barycenters when computing channel-wise
averages. These barycenters are collected in a trusted central server where
they collectively generate a target RGB space. By projecting our dataset
towards this target space, we minimize the distributional discrepancy on a
global level, which facilitates the learning process due to a minimization of
variance across the samples. We demonstrate the capabilities of the proposed
approach over the CIFAR-10 dataset, where we show its capability of reaching
higher degrees of generalization in fewer communication rounds.

</details>


### [157] [Multimodal Tabular Reasoning with Privileged Structured Information](https://arxiv.org/abs/2506.04088)
*Jun-Peng Jiang,Yu Xia,Hai-Long Sun,Shiyin Lu,Qing-Guo Chen,Weihua Luo,Kaifu Zhang,De-Chuan Zhan,Han-Jia Ye*

Main category: cs.LG

TL;DR: 论文提出了一种名为Turbo的多模态表格推理框架，利用训练时的结构化信息增强多模态大语言模型（MLLMs），在少量数据下实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中表格常以图像形式存在，缺乏高质量文本表示，而现有方法依赖结构化表格。本文旨在解决从表格图像进行推理的挑战。

Method: 提出Turbo框架，结合结构感知的推理轨迹生成器（基于DeepSeek-R1），生成高质量跨模态数据，并通过反复生成和选择推理路径提升模型能力。

Result: 在仅9k数据下，Turbo在多个数据集上达到最优性能（比之前SOTA提升7.2%）。

Conclusion: Turbo通过跨模态信息桥接和推理路径优化，显著提升了多模态表格推理能力。

Abstract: Tabular reasoning involves multi-step information extraction and logical
inference over tabular data. While recent advances have leveraged large
language models (LLMs) for reasoning over structured tables, such high-quality
textual representations are often unavailable in real-world settings, where
tables typically appear as images. In this paper, we tackle the task of tabular
reasoning from table images, leveraging privileged structured information
available during training to enhance multimodal large language models (MLLMs).
The key challenges lie in the complexity of accurately aligning structured
information with visual representations, and in effectively transferring
structured reasoning skills to MLLMs despite the input modality gap. To address
these, we introduce TabUlar Reasoning with Bridged infOrmation ({\sc Turbo}), a
new framework for multimodal tabular reasoning with privileged structured
tables. {\sc Turbo} benefits from a structure-aware reasoning trace generator
based on DeepSeek-R1, contributing to high-quality modality-bridged data. On
this basis, {\sc Turbo} repeatedly generates and selects the advantageous
reasoning paths, further enhancing the model's tabular reasoning ability.
Experimental results demonstrate that, with limited ($9$k) data, {\sc Turbo}
achieves state-of-the-art performance ($+7.2\%$ vs. previous SOTA) across
multiple datasets.

</details>


### [158] [Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning](https://arxiv.org/abs/2506.04207)
*Shuang Chen,Yue Guo,Zhaochen Su,Yafu Li,Yulun Wu,Jiacheng Chen,Jiayu Chen,Weijie Wang,Xiaoye Qu,Yu Cheng*

Main category: cs.LG

TL;DR: 论文提出ReVisual-R1，通过分阶段训练和解决多模态RL问题，在7B MLLMs中达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 激发多模态大语言模型（MLLMs）的复杂推理能力，现有方法直接应用强化学习（RL）效果不佳。

Method: 分阶段训练：1）冷启动初始化；2）解决多模态RL的梯度停滞问题；3）后续纯文本RL训练。

Result: ReVisual-R1在多个挑战性基准测试中表现优异。

Conclusion: 分阶段训练和解决多模态RL问题可显著提升MLLMs的推理能力。

Abstract: Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex
textual tasks, many works attempt to incentivize similar capabilities in
Multimodal Large Language Models (MLLMs) by directly applying reinforcement
learning (RL). However, they still struggle to activate complex reasoning. In
this paper, rather than examining multimodal RL in isolation, we delve into
current training pipelines and identify three crucial phenomena: 1) Effective
cold start initialization is critical for enhancing MLLM reasoning.
Intriguingly, we find that initializing with carefully selected text data alone
can lead to performance surpassing many recent multimodal reasoning models,
even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers
from gradient stagnation, which degrades training stability and performance. 3)
Subsequent text-only RL training, following the multimodal RL phase, further
enhances multimodal reasoning. This staged training approach effectively
balances perceptual grounding and cognitive reasoning development. By
incorporating the above insights and addressing multimodal RL issues, we
introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B
MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,
LogicVista, DynaMath, and challenging AIME2024 and AIME2025.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [159] [Dreaming up scale invariance via inverse renormalization group](https://arxiv.org/abs/2506.04016)
*Adam Rançon,Ulysse Rançon,Tomislav Ivek,Ivan Balog*

Main category: cond-mat.stat-mech

TL;DR: 小规模神经网络能够逆向重正化群（RG）粗粒化过程，生成二维Ising模型的微观构型，无需依赖微观输入即可重建尺度不变分布。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络如何通过概率方法逆向RG粗粒化，生成临界构型，并验证其是否能捕捉尺度不变性和RG变换的非平凡特征值。

Method: 使用仅含三个可训练参数的神经网络生成临界构型，并通过重正化群分析验证其性能。

Result: 简单神经网络能成功生成临界构型，捕捉尺度不变性和RG特征值，增加网络复杂度无显著提升。

Conclusion: 简单局部规则足以编码临界现象的普适性，为高效生成统计物理系综模型提供新思路。

Abstract: We explore how minimal neural networks can invert the renormalization group
(RG) coarse-graining procedure in the two-dimensional Ising model, effectively
"dreaming up" microscopic configurations from coarse-grained states. This
task-formally impossible at the level of configurations-can be approached
probabilistically, allowing machine learning models to reconstruct
scale-invariant distributions without relying on microscopic input. We
demonstrate that even neural networks with as few as three trainable parameters
can learn to generate critical configurations, reproducing the scaling behavior
of observables such as magnetic susceptibility, heat capacity, and Binder
ratios. A real-space renormalization group analysis of the generated
configurations confirms that the models capture not only scale invariance but
also reproduce nontrivial eigenvalues of the RG transformation. Surprisingly,
we find that increasing network complexity by introducing multiple layers
offers no significant benefit. These findings suggest that simple local rules,
akin to those generating fractal structures, are sufficient to encode the
universality of critical phenomena, opening the door to efficient generative
models of statistical ensembles in physics.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [160] [mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation](https://arxiv.org/abs/2505.24073)
*Chan-Wei Hu,Yueqi Wang,Shuo Xing,Chia-Ju Chen,Zhengzhong Tu*

Main category: cs.AI

TL;DR: 论文探讨了如何通过检索增强生成（RAG）提升大型视觉语言模型（LVLMs）的性能，分析了检索、重排序和生成阶段的策略，并提出了一种统一的自主框架。


<details>
  <summary>Details</summary>
Motivation: LVLMs在动态现实应用中受限于静态训练数据、幻觉问题和无法验证最新外部证据，RAG提供了一种解决方案。

Method: 系统分析了多模态RAG流程，包括检索阶段的模态配置和策略、重排序阶段的去偏和相关性提升策略，以及生成阶段的候选整合方法。

Result: 提出的自主框架显著提升了性能，平均性能提升5%，无需微调。

Conclusion: RAG为LVLMs提供了有效的性能提升途径，尤其在动态应用中表现突出。

Abstract: Large Vision-Language Models (LVLMs) have made remarkable strides in
multimodal tasks such as visual question answering, visual grounding, and
complex reasoning. However, they remain limited by static training data,
susceptibility to hallucinations, and inability to verify claims against
up-to-date, external evidence, compromising their performance in dynamic
real-world applications. Retrieval-Augmented Generation (RAG) offers a
practical solution to mitigate these challenges by allowing the LVLMs to access
large-scale knowledge databases via retrieval mechanisms, thereby grounding
model outputs in factual, contextually relevant information. Here in this
paper, we conduct the first systematic dissection of the multimodal RAG
pipeline for LVLMs, explicitly investigating (1) the retrieval phase: on the
modality configurations and retrieval strategies, (2) the re-ranking stage: on
strategies to mitigate positional biases and improve the relevance of retrieved
evidence, and (3) the generation phase: we further investigate how to best
integrate retrieved candidates into the final generation process. Finally, we
extend to explore a unified agentic framework that integrates re-ranking and
generation through self-reflection, enabling LVLMs to select relevant evidence
and suppress irrelevant context dynamically. Our full-stack exploration of RAG
for LVLMs yields substantial insights, resulting in an average performance
boost of 5% without any fine-tuning.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [161] [A Resonant Beginning for the Solar System Terrestrial Planets](https://arxiv.org/abs/2506.04164)
*Shuo Huang,Chris Ormel,Simon Portegies Zwart,Eiichiro Kokubo,Tian Yi*

Main category: astro-ph.EP

TL;DR: 论文探讨了类地行星（包括忒伊亚）早期形成的共振链是否能在巨行星不稳定性后演化为现今的构型。通过N体模拟，发现巨行星不稳定性会破坏类地行星共振链，并在20-50%的模拟系统中触发月球形成的巨大撞击。


<details>
  <summary>Details</summary>
Motivation: 研究类地行星早期共振链的形成及其在巨行星不稳定性后的演化，解释火星与金星当前周期比的自然起源。

Method: 使用N体模拟，分析巨行星不稳定性对类地行星共振链的影响，并评估其与现今构型的匹配程度。

Result: 模拟显示巨行星不稳定性会破坏共振链，触发月球形成的巨大撞击，且模拟行星的偏心率和倾角与现今值匹配。火星与金星的周期比3.05是共振链的遗迹。

Conclusion: 类地行星的当前构型可能是早期共振链在巨行星不稳定性后的遗迹，火星与金星的周期比3.05支持这一假说。

Abstract: In the past two decades, transit surveys have revealed a class of planets
with thick atmospheres -- sub-Neptunes -- that must have completed their
accretion in protoplanet disks. When planets form in the gaseous disk, the
gravitational interaction with the disk gas drives their migration and results
in the trapping of neighboring planets in mean motion resonances, though these
resonances can later be broken when the damping effects of disk gas or
planetesimals wane. It is widely accepted that the outer Solar System gas giant
planets originally formed in a resonant chain, which was later disrupted by
dynamical instabilities. Here, we explore whether the early formation of the
terrestrial planets in a resonance chain (including Theia) can evolve to the
present configuration. Using N-body simulations, we demonstrate that the giant
planet instability would also have destabilized the terrestrial resonance
chain, triggering moon-forming giant impacts in 20--50\% of our simulated
systems, dependent on the initial resonance architecture. After the
instability, the eccentricity and inclination of the simulated planets match
their present-day values. Under the proposed scenario, the current period ratio
of 3.05 between Mars and Venus -- devoid of any special significance in
traditional late formation models -- naturally arises as a relic of the former
resonance chain.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [162] [Knowledge Graphs for Digitized Manuscripts in Jagiellonian Digital Library Application](https://arxiv.org/abs/2506.03180)
*Jan Ignatowicz,Krzysztof Kutt,Grzegorz J. Nalepa*

Main category: cs.DL

TL;DR: 论文探讨了结合计算机视觉、人工智能和语义网技术来丰富数字化文化遗产元数据并构建知识图谱的方法。


<details>
  <summary>Details</summary>
Motivation: 数字化文化遗产的元数据通常不完整且缺乏标准化，限制了其可搜索性和潜在关联。

Method: 采用计算机视觉、人工智能和语义网技术的综合方法，以丰富元数据并构建知识图谱。

Result: 该方法有望提升数字化文化遗产的元数据质量和关联性。

Conclusion: 通过技术整合，可以解决数字化文化遗产元数据的挑战，增强其可用性和连接性。

Abstract: Digitizing cultural heritage collections has become crucial for preservation
of historical artifacts and enhancing their availability to the wider public.
Galleries, libraries, archives and museums (GLAM institutions) are actively
digitizing their holdings and creates extensive digital collections. Those
collections are often enriched with metadata describing items but not exactly
their contents. The Jagiellonian Digital Library, standing as a good example of
such an effort, offers datasets accessible through protocols like OAI-PMH.
Despite these improvements, metadata completeness and standardization continue
to pose substantial obstacles, limiting the searchability and potential
connections between collections. To deal with these challenges, we explore an
integrated methodology of computer vision (CV), artificial intelligence (AI),
and semantic web technologies to enrich metadata and construct knowledge graphs
for digitized manuscripts and incunabula.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [163] [Analytical Reconstruction of Periodically Deformed Objects in Time-resolved CT](https://arxiv.org/abs/2506.03792)
*Qianwei Qu,Christian M. Schlepütz,Marco Stampanoni*

Main category: physics.med-ph

TL;DR: 论文提出了两种分析重建流程，用于解决基于门控的时间周期CT重建中辐射剂量利用率低的问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 基于门控的重建方法仅利用部分投影数据，忽略了不同集合间的相关性，导致辐射剂量利用效率低。

Method: 提出了两种分析重建流程，通过利用所有投影数据并考虑集合间相关性，优化重建效果。

Result: 新方法显著减少了重建图像中的随机噪声，同时保留了物体的锐利特征，或在相同质量下降低了辐射剂量。

Conclusion: 新方法在时间周期CT重建中提高了辐射剂量利用效率，具有实际应用潜力。

Abstract: Time-resolved CT is an advanced measurement technique that has been widely
used to observe dynamic objects, including periodically varying structures such
as hearts, lungs, or hearing structures. To reconstruct these objects from CT
projections, a common approach is to divide the projections into several
collections based on their motion phases and perform reconstruction within each
collection, assuming they originate from a static object. This describes the
gating-based method, which is the standard approach for time-periodic
reconstruction. However, the gating-based reconstruction algorithm only
utilizes a limited subset of projections within each collection and ignores the
correlation between different collections, leading to inefficient use of the
radiation dose. To address this issue, we propose two analytical reconstruction
pipelines in this paper, and validate them with experimental data captured
using tomographic synchrotron microscopy. We demonstrate that our approaches
significantly reduce random noise in the reconstructed images without blurring
the sharp features of the observed objects. Equivalently, our methods can
achieve the same reconstruction quality as gating-based methods but with a
lower radiation dose. Our code is available at github.com/PeriodRecon.

</details>


### [164] [Personalized MR-Informed Diffusion Models for 3D PET Image Reconstruction](https://arxiv.org/abs/2506.03804)
*George Webber,Alexander Hammers,Andrew P. King,Andrew J. Reader*

Main category: physics.med-ph

TL;DR: 提出了一种基于图像配准生成个性化伪PET图像的方法，结合扩散模型提升低计数PET数据的重建精度。


<details>
  <summary>Details</summary>
Motivation: 解决传统PET图像重建中因噪声和超参数限制导致的病灶检测能力不足问题，同时避免依赖大规模训练数据或生成式深度学习。

Method: 通过多受试者PET-MR扫描数据集，利用图像配准技术生成受试者特异性伪PET图像，并用于预训练个性化扩散模型。

Result: 实验表明，该方法在低计数数据下提高了重建精度，同时保留了PET独特的图像特征，避免了过度依赖MR解剖信息。

Conclusion: 该方法为医学影像任务提供了一种无需大规模数据或复杂生成模型的合成数据生成与利用途径。

Abstract: Recent work has shown improved lesion detectability and flexibility to
reconstruction hyperparameters (e.g. scanner geometry or dose level) when PET
images are reconstructed by leveraging pre-trained diffusion models. Such
methods train a diffusion model (without sinogram data) on high-quality, but
still noisy, PET images. In this work, we propose a simple method for
generating subject-specific PET images from a dataset of multi-subject PET-MR
scans, synthesizing "pseudo-PET" images by transforming between different
patients' anatomy using image registration. The images we synthesize retain
information from the subject's MR scan, leading to higher resolution and the
retention of anatomical features compared to the original set of PET images.
With simulated and real [$^{18}$F]FDG datasets, we show that pre-training a
personalized diffusion model with subject-specific "pseudo-PET" images improves
reconstruction accuracy with low-count data. In particular, the method shows
promise in combining information from a guidance MR scan without overly
imposing anatomical features, demonstrating an improved trade-off between
reconstructing PET-unique image features versus features present in both PET
and MR. We believe this approach for generating and utilizing synthetic data
has further applications to medical imaging tasks, particularly because
patient-specific PET images can be generated without resorting to generative
deep learning or large training datasets.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [165] [Trajectory Prediction Meets Large Language Models: A Survey](https://arxiv.org/abs/2506.03408)
*Yi Xu,Ruining Yang,Yitian Zhang,Yizhou Wang,Jianglin Lu,Mingyuan Zhang,Lili Su,Yun Fu*

Main category: cs.CL

TL;DR: 综述探讨了大型语言模型（LLMs）在轨迹预测中的应用，总结了五个研究方向及其挑战。


<details>
  <summary>Details</summary>
Motivation: 利用LLMs的语义和推理能力改进自主系统的轨迹预测。

Method: 分类为五种方法：语言建模范式、预训练模型直接预测、语言引导场景理解、语言驱动数据生成、语言推理与可解释性。

Result: 提供了代表性方法和设计选择的分析，并指出开放挑战。

Conclusion: 综述为自然语言处理与轨迹预测的融合提供了统一视角。

Abstract: Recent advances in large language models (LLMs) have sparked growing interest
in integrating language-driven techniques into trajectory prediction. By
leveraging their semantic and reasoning capabilities, LLMs are reshaping how
autonomous systems perceive, model, and predict trajectories. This survey
provides a comprehensive overview of this emerging field, categorizing recent
work into five directions: (1) Trajectory prediction via language modeling
paradigms, (2) Direct trajectory prediction with pretrained language models,
(3) Language-guided scene understanding for trajectory prediction, (4)
Language-driven data generation for trajectory prediction, (5) Language-based
reasoning and interpretability for trajectory prediction. For each, we analyze
representative methods, highlight core design choices, and identify open
challenges. This survey bridges natural language processing and trajectory
prediction, offering a unified perspective on how language can enrich
trajectory prediction.

</details>


### [166] [ROSA: Addressing text understanding challenges in photographs via ROtated SAmpling](https://arxiv.org/abs/2506.03665)
*Hernán Maina,Guido Ivetta,Mateo Lione Stuto,Julian Martin Eisenschlos,Jorge Sánchez,Luciana Benotti*

Main category: cs.CL

TL;DR: ROSA解码策略提升视觉问答系统在文本方向错误图像中的性能，优于贪婪解码11.7个百分点。


<details>
  <summary>Details</summary>
Motivation: 视觉障碍人士依赖VQA系统解读周围文本，但现有模型难以处理他们拍摄的文本方向错误的图像。

Method: 通过访谈识别常见文本方向问题，提出ROSA解码策略以改善性能。

Result: ROSA在最佳模型中比贪婪解码表现高出11.7个百分点。

Conclusion: ROSA有效解决了VQA系统在文本方向错误图像中的性能问题。

Abstract: Visually impaired people could benefit from Visual Question Answering (VQA)
systems to interpret text in their surroundings. However, current models often
struggle with recognizing text in the photos taken by this population. Through
in-depth interviews with visually impaired individuals, we identified common
framing conventions that frequently result in misaligned text. Existing VQA
benchmarks primarily feature well-oriented text captured by sighted users,
under-representing these challenges. To address this gap, we introduce ROtated
SAmpling (ROSA), a decoding strategy that enhances VQA performance in text-rich
images with incorrectly oriented text. ROSA outperforms Greedy decoding by 11.7
absolute points in the best-performing model.

</details>


### [167] [Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages](https://arxiv.org/abs/2506.03884)
*Utkarsh Pathak,Chandra Sai Krishna Gunda,Anusha Prakash,Keshav Agarwal,Hema A. Murthy*

Main category: cs.CL

TL;DR: 论文提出了一种零样本合成方法，通过共享音素表示和调整文本解析规则，为资源稀缺的印度语言生成可理解和自然的语音。


<details>
  <summary>Details</summary>
Motivation: 印度有1369种语言，其中许多缺乏数字资源，传统TTS系统需要高质量数据和准确转录，难以覆盖所有语言。

Method: 通过共享音素表示和调整文本解析规则，匹配目标语言的音位规则，减少合成器开销并实现快速适应。

Result: 成功为梵语、马拉地语、卡纳达语、迈蒂利语和库鲁克语生成了可理解和自然的语音。

Conclusion: 该方法有效扩展了语音技术对资源稀缺语言的覆盖，具有广泛应用潜力。

Abstract: Text-to-speech (TTS) systems typically require high-quality studio data and
accurate transcriptions for training. India has 1369 languages, with 22
official using 13 scripts. Training a TTS system for all these languages, most
of which have no digital resources, seems a Herculean task. Our work focuses on
zero-shot synthesis, particularly for languages whose scripts and phonotactics
come from different families. The novelty of our work is in the augmentation of
a shared phone representation and modifying the text parsing rules to match the
phonotactics of the target language, thus reducing the synthesiser overhead and
enabling rapid adaptation. Intelligible and natural speech was generated for
Sanskrit, Maharashtrian and Canara Konkani, Maithili and Kurukh by leveraging
linguistic connections across languages with suitable synthesisers. Evaluations
confirm the effectiveness of this approach, highlighting its potential to
expand speech technology access for under-represented languages.

</details>


### [168] [DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding](https://arxiv.org/abs/2506.03990)
*Hongzhi Zhang,Jingyuan Zhang,Xingguang Ji,Qi Wang,Fuzheng Zhang*

Main category: cs.CL

TL;DR: DynTok是一种动态视频令牌压缩策略，通过自适应分组和合并视觉令牌，显著减少计算开销，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频建模方法（如LLava）因处理大量视觉令牌而计算开销巨大，尤其是长视频。

Method: DynTok动态分组并合并视觉令牌，在高信息密度区域保留内容，低密度区域压缩。

Result: 令牌数量减少至44.4%，性能相当；在Video-MME和MLVU上分别达到65.3%和72.5%。

Conclusion: DynTok揭示了视频令牌表示的冗余性，为高效视频建模技术设计提供了思路。

Abstract: Typical video modeling methods, such as LLava, represent videos as sequences
of visual tokens, which are then processed by the LLM backbone for effective
video understanding. However, this approach leads to a massive number of visual
tokens, especially for long videos. A practical solution is to first extract
relevant visual information from the large visual context before feeding it
into the LLM backbone, thereby reducing computational overhead. In this work,
we introduce DynTok, a novel \textbf{Dyn}amic video \textbf{Tok}en compression
strategy. DynTok adaptively splits visual tokens into groups and merges them
within each group, achieving high compression in regions with low information
density while preserving essential content. Our method reduces the number of
tokens to 44.4% of the original size while maintaining comparable performance.
It further benefits from increasing the number of video frames and achieves
65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective
compression method, we expose the redundancy in video token representations and
offer insights for designing more efficient video modeling techniques.

</details>


### [169] [Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era](https://arxiv.org/abs/2506.03994)
*Dan Oneata,Desmond Elliott,Stella Frank*

Main category: cs.CL

TL;DR: 论文研究了大规模模型如何表示具体物体概念的语义特征，通过探测任务测试模型对物体属性的理解能力。


<details>
  <summary>Details</summary>
Motivation: 探讨人类学习与概念表征基于感知运动经验，而现有大规模模型是否也能有效表示类似语义特征。

Method: 使用探测任务评估图像编码器和语言模型对McRae和Binder数据集中属性的预测能力。

Result: 多模态图像编码器略优于纯语言模型，纯图像编码器在非视觉属性上也表现良好。

Conclusion: 结果揭示了单模态学习的潜力及多模态的互补性。

Abstract: Human learning and conceptual representation is grounded in sensorimotor
experience, in contrast to state-of-the-art foundation models. In this paper,
we investigate how well such large-scale models, trained on vast quantities of
data, represent the semantic feature norms of concrete object concepts, e.g. a
ROSE is red, smells sweet, and is a flower. More specifically, we use probing
tasks to test which properties of objects these models are aware of. We
evaluate image encoders trained on image data alone, as well as
multimodally-trained image encoders and language-only models, on predicting an
extended denser version of the classic McRae norms and the newer Binder dataset
of attribute ratings. We find that multimodal image encoders slightly
outperform language-only approaches, and that image-only encoders perform
comparably to the language models, even on non-visual attributes that are
classified as "encyclopedic" or "function". These results offer new insights
into what can be learned from pure unimodal learning, and the complementarity
of the modalities.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [170] [SNIFR : Boosting Fine-Grained Child Harmful Content Detection Through Audio-Visual Alignment with Cascaded Cross-Transformer](https://arxiv.org/abs/2506.03378)
*Orchid Chetia Phukan,Mohd Mujtaba Akhtar,Girish,Swarup Ranjan Behera,Abu Osama Siddiqui,Sarthak Jain,Priyabrata Mallick,Jaya Sai Kiran Patibandla,Pailla Balakrishna Reddy,Arun Balaji Buduru,Rajesh Sharma*

Main category: eess.AS

TL;DR: 论文提出SNIFR框架，结合音频与视觉特征，用于儿童有害内容检测，性能优于单模态和基线融合方法。


<details>
  <summary>Details</summary>
Motivation: 随着视频平台儿童观众增加，需精准检测有害内容（如暴力或露骨场景），而现有研究多关注视觉特征，音频特征未充分探索。

Method: 提出SNIFR框架，使用Transformer编码器进行模态内交互，级联跨模态Transformer进行模态间对齐。

Result: 方法在性能上优于单模态和基线融合方法，达到新最优水平。

Conclusion: 结合音频与视觉特征的SNIFR框架能有效提升儿童有害内容检测的精确性。

Abstract: As video-sharing platforms have grown over the past decade, child viewership
has surged, increasing the need for precise detection of harmful content like
violence or explicit scenes. Malicious users exploit moderation systems by
embedding unsafe content in minimal frames to evade detection. While prior
research has focused on visual cues and advanced such fine-grained detection,
audio features remain underexplored. In this study, we embed audio cues with
visual for fine-grained child harmful content detection and introduce SNIFR, a
novel framework for effective alignment. SNIFR employs a transformer encoder
for intra-modality interaction, followed by a cascaded cross-transformer for
inter-modality alignment. Our approach achieves superior performance over
unimodal and baseline fusion methods, setting a new state-of-the-art.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [171] [Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot Data](https://arxiv.org/abs/2506.04120)
*Ben Moran,Mauro Comi,Steven Bohez,Tom Erez,Zhibin Li,Leonard Hasenclever*

Main category: cs.RO

TL;DR: 论文提出了一种新颖的“真实到仿真”框架，通过混合场景表示和端到端优化，解决了从真实机器人运动中创建精确物理模拟的挑战。


<details>
  <summary>Details</summary>
Motivation: 真实机器人数据存在遮挡、噪声相机位姿和动态场景元素等问题，阻碍了创建几何精确且逼真的数字孪生体。

Method: 采用混合场景表示（3D高斯点云与显式对象网格结合），结合可微分渲染和物理引擎MuJoCo，进行端到端优化。

Result: 实现了高保真对象网格重建、逼真新视图生成和无标注机器人位姿校准。

Conclusion: 该方法在仿真和真实场景中均表现出色，为更实用和鲁棒的“真实到仿真”流程提供了可能。

Abstract: Creating accurate, physical simulations directly from real-world robot motion
holds great value for safe, scalable, and affordable robot learning, yet
remains exceptionally challenging. Real robot data suffers from occlusions,
noisy camera poses, dynamic scene elements, which hinder the creation of
geometrically accurate and photorealistic digital twins of unseen objects. We
introduce a novel real-to-sim framework tackling all these challenges at once.
Our key insight is a hybrid scene representation merging the photorealistic
rendering of 3D Gaussian Splatting with explicit object meshes suitable for
physics simulation within a single representation. We propose an end-to-end
optimization pipeline that leverages differentiable rendering and
differentiable physics within MuJoCo to jointly refine all scene components -
from object geometry and appearance to robot poses and physical parameters -
directly from raw and imprecise robot trajectories. This unified optimization
allows us to simultaneously achieve high-fidelity object mesh reconstruction,
generate photorealistic novel views, and perform annotation-free robot pose
calibration. We demonstrate the effectiveness of our approach both in
simulation and on challenging real-world sequences using an ALOHA 2 bi-manual
manipulator, enabling more practical and robust real-to-simulation pipelines.

</details>


### [172] [Pseudo-Simulation for Autonomous Driving](https://arxiv.org/abs/2506.04218)
*Wei Cao,Marcel Hallgarten,Tianyu Li,Daniel Dauner,Xunjiang Gu,Caojun Wang,Yakov Miron,Marco Aiello,Hongyang Li,Igor Gilitschenski,Boris Ivanovic,Marco Pavone,Andreas Geiger,Kashyap Chitta*

Main category: cs.RO

TL;DR: 提出了一种名为伪模拟的新范式，结合真实数据和合成观测，解决了自动驾驶车辆评估中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶车辆评估方法存在安全性、可重复性、真实性和计算成本等问题，需要一种更高效的解决方案。

Method: 使用3D高斯散射生成合成观测，结合真实数据集，并通过基于邻近性的加权方案评估潜在未来状态。

Result: 伪模拟与闭环模拟的相关性（R^2=0.8）优于现有开环方法（R^2=0.7）。

Conclusion: 伪模拟是一种高效且准确的评估方法，适用于自动驾驶车辆的测试和基准比较。

Abstract: Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical
limitations. Real-world evaluation is often challenging due to safety concerns
and a lack of reproducibility, whereas closed-loop simulation can face
insufficient realism or high computational costs. Open-loop evaluation, while
being efficient and data-driven, relies on metrics that generally overlook
compounding errors. In this paper, we propose pseudo-simulation, a novel
paradigm that addresses these limitations. Pseudo-simulation operates on real
datasets, similar to open-loop evaluation, but augments them with synthetic
observations generated prior to evaluation using 3D Gaussian Splatting. Our key
idea is to approximate potential future states the AV might encounter by
generating a diverse set of observations that vary in position, heading, and
speed. Our method then assigns a higher importance to synthetic observations
that best match the AV's likely behavior using a novel proximity-based
weighting scheme. This enables evaluating error recovery and the mitigation of
causal confusion, as in closed-loop benchmarks, without requiring sequential
interactive simulation. We show that pseudo-simulation is better correlated
with closed-loop simulations (R^2=0.8) than the best existing open-loop
approach (R^2=0.7). We also establish a public leaderboard for the community to
benchmark new methodologies with pseudo-simulation. Our code is available at
https://github.com/autonomousvision/navsim.

</details>


### [173] [Object-centric 3D Motion Field for Robot Learning from Human Videos](https://arxiv.org/abs/2506.04227)
*Zhao-Heng Yin,Sherry Yang,Pieter Abbeel*

Main category: cs.RO

TL;DR: 提出了一种基于对象中心3D运动场的动作表示方法，用于从人类视频中学习机器人控制策略，显著提升了3D运动估计精度和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有动作表示方法（如视频帧、像素流等）存在建模复杂或信息丢失的问题，需要一种更有效的表示方法。

Method: 提出了一种对象中心3D运动场表示，并设计了去噪3D运动场估计器和密集预测架构，以支持跨实体迁移和背景泛化。

Result: 实验表明，3D运动估计误差降低50%以上，任务平均成功率达55%，优于现有方法（<10%）。

Conclusion: 对象中心3D运动场是一种有效的动作表示方法，显著提升了机器人从人类视频中学习控制策略的性能。

Abstract: Learning robot control policies from human videos is a promising direction
for scaling up robot learning. However, how to extract action knowledge (or
action representations) from videos for policy learning remains a key
challenge. Existing action representations such as video frames, pixelflow, and
pointcloud flow have inherent limitations such as modeling complexity or loss
of information. In this paper, we propose to use object-centric 3D motion field
to represent actions for robot learning from human videos, and present a novel
framework for extracting this representation from videos for zero-shot control.
We introduce two novel components in its implementation. First, a novel
training pipeline for training a ''denoising'' 3D motion field estimator to
extract fine object 3D motions from human videos with noisy depth robustly.
Second, a dense object-centric 3D motion field prediction architecture that
favors both cross-embodiment transfer and policy generalization to background.
We evaluate the system in real world setups. Experiments show that our method
reduces 3D motion estimation error by over 50% compared to the latest method,
achieve 55% average success rate in diverse tasks where prior approaches
fail~($\lesssim 10$\%), and can even acquire fine-grained manipulation skills
like insertion.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [174] [Structural Vibration Monitoring with Diffractive Optical Processors](https://arxiv.org/abs/2506.03317)
*Yuntian Wang,Zafer Yilmaz,Yuhang Li,Edward Liu,Eric Ahlberg,Farid Ghahari,Ertugrul Taciroglu,Aydogan Ozcan*

Main category: physics.optics

TL;DR: 提出了一种基于衍射振动监测系统的低成本、低功耗、可扩展的3D结构振动监测方案，通过联合优化的衍射层和浅层神经网络实现高效数据采集与处理。


<details>
  <summary>Details</summary>
Motivation: 当前结构健康监测（SHM）方案在成本、功耗、可扩展性和数据处理复杂性方面存在局限，需要一种更高效的解决方案。

Method: 结合空间优化的被动衍射层和浅层神经网络，将3D结构位移编码为调制光信号，通过少量探测器实时解码。

Result: 系统在毫米波照明下通过实验验证，精度比传统光学或单独训练模块提高一个数量级。

Conclusion: 该框架为结构的高通量3D监测奠定了基础，并在灾害韧性、航空航天诊断和自主导航等领域具有潜在应用。

Abstract: Structural Health Monitoring (SHM) is vital for maintaining the safety and
longevity of civil infrastructure, yet current solutions remain constrained by
cost, power consumption, scalability, and the complexity of data processing.
Here, we present a diffractive vibration monitoring system, integrating a
jointly optimized diffractive layer with a shallow neural network-based backend
to remotely extract 3D structural vibration spectra, offering a low-power,
cost-effective and scalable solution. This architecture eliminates the need for
dense sensor arrays or extensive data acquisition; instead, it uses a
spatially-optimized passive diffractive layer that encodes 3D structural
displacements into modulated light, captured by a minimal number of detectors
and decoded in real-time by shallow and low-power neural networks to
reconstruct the 3D displacement spectra of structures. The diffractive system's
efficacy was demonstrated both numerically and experimentally using
millimeter-wave illumination on a laboratory-scale building model with a
programmable shake table. Our system achieves more than an order-of-magnitude
improvement in accuracy over conventional optics or separately trained modules,
establishing a foundation for high-throughput 3D monitoring of structures.
Beyond SHM, the 3D vibration monitoring capabilities of this cost-effective and
data-efficient framework establish a new computational sensing modality with
potential applications in disaster resilience, aerospace diagnostics, and
autonomous navigation, where energy efficiency, low latency, and
high-throughput are critical.

</details>
