<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 220]
- [eess.IV](#eess.IV) [Total: 21]
- [cs.GR](#cs.GR) [Total: 5]
- [physics.geo-ph](#physics.geo-ph) [Total: 2]
- [cs.AI](#cs.AI) [Total: 5]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.RO](#cs.RO) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Med-GRIM: Enhanced Zero-Shot Medical VQA using prompt-embedded Multimodal Graph RAG](https://arxiv.org/abs/2508.06496)
*Rakesh Raj Madavan,Akshat Kaimal,Hashim Faisal,Chandrakala S*

Main category: cs.CV

TL;DR: BIND和Med-GRIM通过密集编码和图形检索提升医疗VQA任务的精确性，减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有VQA模型在复杂领域（如医疗）中缺乏精确性，需要高效解决方案。

Method: BIND改进联合嵌入空间，Med-GRIM结合图形检索和提示工程，使用小型语言模型。

Result: Med-GRIM在低计算成本下实现高性能，并发布DermaGraph数据集支持研究。

Conclusion: 该方法为医疗VQA提供了高效、精确的解决方案，并推动了零样本多模态研究。

Abstract: An ensemble of trained multimodal encoders and vision-language models (VLMs)
has become a standard approach for visual question answering (VQA) tasks.
However, such models often fail to produce responses with the detailed
precision necessary for complex, domain-specific applications such as medical
VQA. Our representation model, BIND: BLIVA Integrated with Dense Encoding,
extends prior multimodal work by refining the joint embedding space through
dense, query-token-based encodings inspired by contrastive pretraining
techniques. This refined encoder powers Med-GRIM, a model designed for medical
VQA tasks that leverages graph-based retrieval and prompt engineering to
integrate domain-specific knowledge. Rather than relying on compute-heavy
fine-tuning of vision and language models on specific datasets, Med-GRIM
applies a low-compute, modular workflow with small language models (SLMs) for
efficiency. Med-GRIM employs prompt-based retrieval to dynamically inject
relevant knowledge, ensuring both accuracy and robustness in its responses. By
assigning distinct roles to each agent within the VQA system, Med-GRIM achieves
large language model performance at a fraction of the computational cost.
Additionally, to support scalable research in zero-shot multimodal medical
applications, we introduce DermaGraph, a novel Graph-RAG dataset comprising
diverse dermatological conditions. This dataset facilitates both multimodal and
unimodal querying. The code and dataset are available at:
https://github.com/Rakesh-123-cryp/Med-GRIM.git

</details>


### [2] [DiTalker: A Unified DiT-based Framework for High-Quality and Speaking Styles Controllable Portrait Animation](https://arxiv.org/abs/2508.06511)
*He Feng,Yongjia Ma,Donglin Di,Lei Fan,Tonghua Su,Xiangqian Wu*

Main category: cs.CV

TL;DR: DiTalker是一个基于DiT的统一框架，用于控制说话风格的人像动画，通过分离音频和风格特征，优化唇同步和细节保留。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注唇同步或静态情感转换，忽略了动态风格（如头部运动），且双U-Net架构计算开销大。

Method: 设计了风格-情感编码模块和音频-风格融合模块，通过并行交叉注意力层分离音频与风格特征，并采用优化约束提升唇同步和细节保留。

Result: 实验表明DiTalker在唇同步和说话风格可控性方面表现优越。

Conclusion: DiTalker通过统一框架解决了动态风格和计算效率问题，提升了人像动画的质量和可控性。

Abstract: Portrait animation aims to synthesize talking videos from a static reference
face, conditioned on audio and style frame cues (e.g., emotion and head poses),
while ensuring precise lip synchronization and faithful reproduction of
speaking styles. Existing diffusion-based portrait animation methods primarily
focus on lip synchronization or static emotion transformation, often
overlooking dynamic styles such as head movements. Moreover, most of these
methods rely on a dual U-Net architecture, which preserves identity consistency
but incurs additional computational overhead. To this end, we propose DiTalker,
a unified DiT-based framework for speaking style-controllable portrait
animation. We design a Style-Emotion Encoding Module that employs two separate
branches: a style branch extracting identity-specific style information (e.g.,
head poses and movements), and an emotion branch extracting identity-agnostic
emotion features. We further introduce an Audio-Style Fusion Module that
decouples audio and speaking styles via two parallel cross-attention layers,
using these features to guide the animation process. To enhance the quality of
results, we adopt and modify two optimization constraints: one to improve lip
synchronization and the other to preserve fine-grained identity and background
details. Extensive experiments demonstrate the superiority of DiTalker in terms
of lip synchronization and speaking style controllability. Project Page:
https://thenameishope.github.io/DiTalker/

</details>


### [3] [BigTokDetect: A Clinically-Informed Vision-Language Model Framework for Detecting Pro-Bigorexia Videos on TikTok](https://arxiv.org/abs/2508.06515)
*Minh Duc Chu,Kshitij Pawar,Zihao He,Roxanna Sharifi,Ross Sonnenblick,Magdalayna Curry,Laura D'Adamo,Lindsay Young,Stuart B Murray,Kristina Lerman*

Main category: cs.CV

TL;DR: 论文提出BigTokDetect框架，用于检测TikTok上促进肌肉畸形的有害内容，通过多模态融合提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上促进肌肉畸形的有害内容（如pro-bigorexia）伪装成健身内容，传统文本检测方法难以识别，需多模态解决方案。

Method: 开发BigTokDetect框架，使用专家标注的多模态数据集BigTok（2200+视频），结合视觉语言模型进行多模态分类。

Result: 模型在主要类别分类上达到82.9%准确率，子类别检测达69%，多模态融合比纯文本方法性能提升5-10%。

Conclusion: 研究为心理健康领域的有害内容检测提供了新基准和可扩展的解决方案。

Abstract: Social media platforms increasingly struggle to detect harmful content that
promotes muscle dysmorphic behaviors, particularly pro-bigorexia content that
disproportionately affects adolescent males. Unlike traditional eating disorder
detection focused on the "thin ideal," pro-bigorexia material masquerades as
legitimate fitness content through complex multimodal combinations of visual
displays, coded language, and motivational messaging that evade text-based
detection systems. We address this challenge by developing BigTokDetect, a
clinically-informed detection framework for identifying pro-bigorexia content
on TikTok. We introduce BigTok, the first expert-annotated multimodal dataset
of over 2,200 TikTok videos labeled by clinical psychologists and psychiatrists
across five primary categories spanning body image, nutrition, exercise,
supplements, and masculinity. Through a comprehensive evaluation of
state-of-the-art vision language models, we achieve 0.829% accuracy on primary
category classification and 0.690% on subcategory detection via domain-specific
finetuning. Our ablation studies demonstrate that multimodal fusion improves
performance by 5-10% over text-only approaches, with video features providing
the most discriminative signals. These findings establish new benchmarks for
multimodal harmful content detection and provide both the computational tools
and methodological framework needed for scalable content moderation in
specialized mental health domains.

</details>


### [4] [Frequency Prior Guided Matching: A Data Augmentation Approach for Generalizable Semi-Supervised Polyp Segmentation](https://arxiv.org/abs/2508.06517)
*Haoran Xi,Chen Liu,Xiaolin Li*

Main category: cs.CV

TL;DR: FPGM是一种基于频率先验的半监督学习方法，通过利用息肉边缘的频率特征，显著提升了跨域分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决息肉分割中标注数据不足和跨域性能下降的问题。

Method: 两阶段方法：学习域不变频率先验，并对未标注图像进行频谱扰动。

Result: 在六个数据集上表现最优，零样本泛化能力提升10%以上。

Conclusion: FPGM为临床部署提供了高效的息肉分割解决方案。

Abstract: Automated polyp segmentation is essential for early diagnosis of colorectal
cancer, yet developing robust models remains challenging due to limited
annotated data and significant performance degradation under domain shift.
Although semi-supervised learning (SSL) reduces annotation requirements,
existing methods rely on generic augmentations that ignore polyp-specific
structural properties, resulting in poor generalization to new imaging centers
and devices. To address this, we introduce Frequency Prior Guided Matching
(FPGM), a novel augmentation framework built on a key discovery: polyp edges
exhibit a remarkably consistent frequency signature across diverse datasets.
FPGM leverages this intrinsic regularity in a two-stage process. It first
learns a domain-invariant frequency prior from the edge regions of labeled
polyps. Then, it performs principled spectral perturbations on unlabeled
images, aligning their amplitude spectra with this learned prior while
preserving phase information to maintain structural integrity. This targeted
alignment normalizes domain-specific textural variations, thereby compelling
the model to learn the underlying, generalizable anatomical structure.
Validated on six public datasets, FPGM establishes a new state-of-the-art
against ten competing methods. It demonstrates exceptional zero-shot
generalization capabilities, achieving over 10% absolute gain in Dice score in
data-scarce scenarios. By significantly enhancing cross-domain robustness, FPGM
presents a powerful solution for clinically deployable polyp segmentation under
limited supervision.

</details>


### [5] [Large Language Models Facilitate Vision Reflection in Image Classification](https://arxiv.org/abs/2508.06525)
*Guoyuan An,JaeYoon Kim,SungEui Yoon*

Main category: cs.CV

TL;DR: 本文揭示了大型多模态模型（LMMs）中视觉反射的可解释性新发现，包括通过提示验证提升识别精度、视觉-语言连接器的工作原理，以及无需训练的连接器对细粒度任务的改进。


<details>
  <summary>Details</summary>
Motivation: 探索LMMs在视觉任务中的表现及其内部机制，以提升模型的可解释性和性能。

Method: 通过提示验证、分析视觉-语言连接器行为，以及测试训练无关的连接器。

Result: 发现视觉反射能提升识别精度，连接器将视觉特征映射为文本概念，且少量文本标记可替代大量视觉标记。

Conclusion: 视觉反射为LMMs提供了鲁棒且可解释的视觉识别策略。

Abstract: This paper presents several novel findings on the explainability of vision
reflection in large multimodal models (LMMs). First, we show that prompting an
LMM to verify the prediction of a specialized vision model can improve
recognition accuracy, even on benchmarks like ImageNet, despite prior evidence
that LMMs typically underperform dedicated vision encoders. Second, we analyze
the internal behavior of vision reflection and find that the vision-language
connector maps visual features into explicit textual concepts, allowing the
language model to reason about prediction plausibility using commonsense
knowledge. We further observe that replacing a large number of vision tokens
with only a few text tokens still enables LLaVA to generate similar answers,
suggesting that LMMs may rely primarily on a compact set of distilled textual
representations rather than raw vision features. Third, we show that a
training-free connector can enhance LMM performance in fine-grained recognition
tasks, without extensive feature-alignment training. Together, these findings
offer new insights into the explainability of vision-language models and
suggest that vision reflection is a promising strategy for achieving robust and
interpretable visual recognition.

</details>


### [6] [A Framework Combining 3D CNN and Transformer for Video-Based Behavior Recognition](https://arxiv.org/abs/2508.06528)
*Xiuliang Zhang,Tadiwa Elisha Nyamasvisva,Chuntao Liu*

Main category: cs.CV

TL;DR: 提出了一种结合3D CNN和Transformer的混合框架，用于视频行为识别，解决了传统方法在长程依赖和高计算成本上的问题。


<details>
  <summary>Details</summary>
Motivation: 视频行为识别在公共安全、智能监控和人机交互中至关重要，但传统3D CNN难以建模长程依赖，而Transformer计算成本高。

Method: 混合框架中，3D CNN提取低层次时空特征，Transformer捕获长程时间依赖，并通过融合机制整合两者。

Result: 在基准数据集上，该模型优于传统3D CNN和独立Transformer，识别精度更高且复杂度可控。

Conclusion: 混合框架为视频行为识别提供了高效且可扩展的解决方案。

Abstract: Video-based behavior recognition is essential in fields such as public
safety, intelligent surveillance, and human-computer interaction. Traditional
3D Convolutional Neural Network (3D CNN) effectively capture local
spatiotemporal features but struggle with modeling long-range dependencies.
Conversely, Transformers excel at learning global contextual information but
face challenges with high computational costs. To address these limitations, we
propose a hybrid framework combining 3D CNN and Transformer architectures. The
3D CNN module extracts low-level spatiotemporal features, while the Transformer
module captures long-range temporal dependencies, with a fusion mechanism
integrating both representations. Evaluated on benchmark datasets, the proposed
model outperforms traditional 3D CNN and standalone Transformers, achieving
higher recognition accuracy with manageable complexity. Ablation studies
further validate the complementary strengths of the two modules. This hybrid
framework offers an effective and scalable solution for video-based behavior
recognition.

</details>


### [7] [VOccl3D: A Video Benchmark Dataset for 3D Human Pose and Shape Estimation under real Occlusions](https://arxiv.org/abs/2508.06757)
*Yash Garg,Saketh Bachu,Arindam Dutta,Rohit Lal,Sarosij Bose,Calvin-Khang Ta,M. Salman Asif,Amit Roy-Chowdhury*

Main category: cs.CV

TL;DR: 论文提出了一种新的视频基准数据集VOccl3D，用于解决现有3D人体姿态和形状估计方法在复杂遮挡场景下的不足，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体姿态和形状估计方法在复杂遮挡场景下表现不佳，且现有数据集缺乏真实遮挡情况。

Method: 使用计算机图形渲染技术构建VOccl3D数据集，包含多样化的真实遮挡场景，并微调HPS方法CLIFF和BEDLAM-CLIFF。

Result: 微调后的方法在多个公共数据集及VOccl3D测试集上表现显著提升，同时提升了遮挡下的人体检测性能。

Conclusion: VOccl3D数据集为未来研究提供了更真实的遮挡场景基准，推动了遮挡下HPS估计的发展。

Abstract: Human pose and shape (HPS) estimation methods have been extensively studied,
with many demonstrating high zero-shot performance on in-the-wild images and
videos. However, these methods often struggle in challenging scenarios
involving complex human poses or significant occlusions. Although some studies
address 3D human pose estimation under occlusion, they typically evaluate
performance on datasets that lack realistic or substantial occlusions, e.g.,
most existing datasets introduce occlusions with random patches over the human
or clipart-style overlays, which may not reflect real-world challenges. To
bridge this gap in realistic occlusion datasets, we introduce a novel benchmark
dataset, VOccl3D, a Video-based human Occlusion dataset with 3D body pose and
shape annotations. Inspired by works such as AGORA and BEDLAM, we constructed
this dataset using advanced computer graphics rendering techniques,
incorporating diverse real-world occlusion scenarios, clothing textures, and
human motions. Additionally, we fine-tuned recent HPS methods, CLIFF and
BEDLAM-CLIFF, on our dataset, demonstrating significant qualitative and
quantitative improvements across multiple public datasets, as well as on the
test split of our dataset, while comparing its performance with other
state-of-the-art methods. Furthermore, we leveraged our dataset to enhance
human detection performance under occlusion by fine-tuning an existing object
detector, YOLO11, thus leading to a robust end-to-end HPS estimation system
under occlusions. Overall, this dataset serves as a valuable resource for
future research aimed at benchmarking methods designed to handle occlusions,
offering a more realistic alternative to existing occlusion datasets. See the
Project page for code and dataset:https://yashgarg98.github.io/VOccl3D-dataset/

</details>


### [8] [RMT-PPAD: Real-time Multi-task Learning for Panoptic Perception in Autonomous Driving](https://arxiv.org/abs/2508.06529)
*Jiayuan Wang,Q. M. Jonathan Wu,Katsuya Suto,Ning Zhang*

Main category: cs.CV

TL;DR: RMT-PPAD是一种基于Transformer的实时多任务模型，用于自动驾驶感知，联合执行目标检测、可行驶区域分割和车道线分割，通过轻量级模块和自适应解码器提升性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需要高精度和实时性的全景驾驶感知，现有方法在多任务联合处理中存在负迁移和手动设计问题。

Method: 提出轻量级门控适配器模块自适应融合共享和任务特定特征，设计自适应分割解码器自动学习多尺度特征权重，解决训练与测试标签不一致问题。

Result: 在BDD100K数据集上，目标检测mAP50达84.9%，可行驶区域分割mIoU达92.6%，车道线分割IoU达56.8%，推理速度32.6 FPS。

Conclusion: RMT-PPAD在性能和实时性上均达到先进水平，实际场景中表现稳定，代码和模型已开源。

Abstract: Autonomous driving systems rely on panoptic driving perception that requires
both precision and real-time performance. In this work, we propose RMT-PPAD, a
real-time, transformer-based multi-task model that jointly performs object
detection, drivable area segmentation, and lane line segmentation. We introduce
a lightweight module, a gate control with an adapter to adaptively fuse shared
and task-specific features, effectively alleviating negative transfer between
tasks. Additionally, we design an adaptive segmentation decoder to learn the
weights over multi-scale features automatically during the training stage. This
avoids the manual design of task-specific structures for different segmentation
tasks. We also identify and resolve the inconsistency between training and
testing labels in lane line segmentation. This allows fairer evaluation.
Experiments on the BDD100K dataset demonstrate that RMT-PPAD achieves
state-of-the-art results with mAP50 of 84.9% and Recall of 95.4% for object
detection, mIoU of 92.6% for drivable area segmentation, and IoU of 56.8% and
accuracy of 84.7% for lane line segmentation. The inference speed reaches 32.6
FPS. Moreover, we introduce real-world scenarios to evaluate RMT-PPAD
performance in practice. The results show that RMT-PPAD consistently delivers
stable performance. The source codes and pre-trained models are released at
https://github.com/JiayuanWang-JW/RMT-PPAD.

</details>


### [9] [Statistical Confidence Rescoring for Robust 3D Scene Graph Generation from Multi-View Images](https://arxiv.org/abs/2508.06546)
*Qi Xun Yeo,Yanyan Li,Gim Hee Lee*

Main category: cs.CV

TL;DR: 论文提出了一种仅使用多视角RGB图像进行3D语义场景图估计的方法，通过语义掩码和邻域信息增强特征，减少噪声，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在缺乏3D真实标注的情况下，探索仅用多视角RGB图像进行3D语义场景图估计，需解决伪点几何噪声和背景噪声问题。

Method: 设计语义掩码引导特征聚合，引入邻域节点信息增强鲁棒性，并利用统计先验优化节点和边预测。

Result: 实验表明，该方法在仅使用多视角图像输入时优于现有方法。

Conclusion: 通过语义和邻域信息增强特征，能有效提升3D场景图估计的准确性。

Abstract: Modern 3D semantic scene graph estimation methods utilize ground truth 3D
annotations to accurately predict target objects, predicates, and
relationships. In the absence of given 3D ground truth representations, we
explore leveraging only multi-view RGB images to tackle this task. To attain
robust features for accurate scene graph estimation, we must overcome the noisy
reconstructed pseudo point-based geometry from predicted depth maps and reduce
the amount of background noise present in multi-view image features. The key is
to enrich node and edge features with accurate semantic and spatial information
and through neighboring relations. We obtain semantic masks to guide feature
aggregation to filter background features and design a novel method to
incorporate neighboring node information to aid robustness of our scene graph
estimates. Furthermore, we leverage on explicit statistical priors calculated
from the training summary statistics to refine node and edge predictions based
on their one-hop neighborhood. Our experiments show that our method outperforms
current methods purely using multi-view images as the initial input. Our
project page is available at https://qixun1.github.io/projects/SCRSSG.

</details>


### [10] [DiffUS: Differentiable Ultrasound Rendering from Volumetric Imaging](https://arxiv.org/abs/2508.06768)
*Noe Bertramo,Gabriel Duguey,Vivek Gopalakrishnan*

Main category: cs.CV

TL;DR: DiffUS是一个基于物理的可微分超声渲染器，能够从MRI数据生成逼真的B超图像，用于手术导航。


<details>
  <summary>Details</summary>
Motivation: 解决术中超声图像因噪声、伪影和对齐问题难以与术前MRI/CT匹配的挑战。

Method: 通过机器学习将MRI转换为声阻抗体积，利用射线追踪模拟超声传播，并通过稀疏线性系统捕捉多次反射，最终生成包含噪声和伪影的B超图像。

Result: 在ReMIND数据集上验证了DiffUS能够从脑部MRI生成解剖学准确的超声图像。

Conclusion: DiffUS为手术导航提供了一种有效的工具，支持基于梯度的优化应用。

Abstract: Intraoperative ultrasound imaging provides real-time guidance during numerous
surgical procedures, but its interpretation is complicated by noise, artifacts,
and poor alignment with high-resolution preoperative MRI/CT scans. To bridge
the gap between reoperative planning and intraoperative guidance, we present
DiffUS, a physics-based, differentiable ultrasound renderer that synthesizes
realistic B-mode images from volumetric imaging. DiffUS first converts MRI 3D
scans into acoustic impedance volumes using a machine learning approach. Next,
we simulate ultrasound beam propagation using ray tracing with coupled
reflection-transmission equations. DiffUS formulates wave propagation as a
sparse linear system that captures multiple internal reflections. Finally, we
reconstruct B-mode images via depth-resolved echo extraction across fan-shaped
acquisition geometry, incorporating realistic artifacts including speckle noise
and depth-dependent degradation. DiffUS is entirely implemented as
differentiable tensor operations in PyTorch, enabling gradient-based
optimization for downstream applications such as slice-to-volume registration
and volumetric reconstruction. Evaluation on the ReMIND dataset demonstrates
DiffUS's ability to generate anatomically accurate ultrasound images from brain
MRI data.

</details>


### [11] [What Makes "Good" Distractors for Object Hallucination Evaluation in Large Vision-Language Models?](https://arxiv.org/abs/2508.06530)
*Ming-Kun Xie,Jia-Hao Xiao,Gang Niu,Lei Feng,Zhiqiang Kou,Min-Ling Zhang,Masashi Sugiyama*

Main category: cs.CV

TL;DR: 论文提出HOPE基准，通过生成误导性干扰项更严格评估大型视觉语言模型（LVLM）的幻觉问题，显著优于现有POPE基准。


<details>
  <summary>Details</summary>
Motivation: 现有POPE基准在评估LVLM的幻觉问题时效果逐渐减弱，因其忽略图像特定信息且仅限制于负面对象类别。

Method: 提出HOPE基准，利用CLIP选择高预测似然的负面对象作为干扰项，并通过真实对象与虚假描述配对构建误导性干扰项。

Result: 实验显示HOPE使多种先进LVLM的精度下降9%至23%，显著优于POPE。

Conclusion: HOPE能更严格评估LVLM的幻觉问题，为未来研究提供更有效的基准。

Abstract: Large Vision-Language Models (LVLMs), empowered by the success of Large
Language Models (LLMs), have achieved impressive performance across domains.
Despite the great advances in LVLMs, they still suffer from the unavailable
object hallucination issue, which tends to generate objects inconsistent with
the image content. The most commonly used Polling-based Object Probing
Evaluation (POPE) benchmark evaluates this issue by sampling negative
categories according to category-level statistics, \textit{e.g.}, category
frequencies and co-occurrence. However, with the continuous advancement of
LVLMs, the POPE benchmark has shown diminishing effectiveness in assessing
object hallucination, as it employs a simplistic sampling strategy that
overlooks image-specific information and restricts distractors to negative
object categories only. In this paper, we introduce the Hallucination
searching-based Object Probing Evaluation (HOPE) benchmark, aiming to generate
the most misleading distractors (\textit{i.e.}, non-existent objects or
incorrect image descriptions) that can trigger hallucination in LVLMs, which
serves as a means to more rigorously assess their immunity to hallucination. To
explore the image-specific information, the content-aware hallucination
searching leverages Contrastive Language-Image Pre-Training (CLIP) to
approximate the predictive behavior of LVLMs by selecting negative objects with
the highest predicted likelihood as distractors. To expand the scope of
hallucination assessment, the description-based hallucination searching
constructs highly misleading distractors by pairing true objects with false
descriptions. Experimental results show that HOPE leads to a precision drop of
at least 9\% and up to 23\% across various state-of-the-art LVLMs,
significantly outperforming POPE in exposing hallucination vulnerabilities. The
code is available at https://github.com/xiemk/HOPE.

</details>


### [12] [SLRTP2025 Sign Language Production Challenge: Methodology, Results, and Future Work](https://arxiv.org/abs/2508.06951)
*Harry Walsh,Ed Fish,Ozge Mercanoglu Sincan,Mohamed Ilyes Lakhal,Richard Bowden,Neil Fox,Bencie Woll,Kepeng Wu,Zecheng Li,Weichao Zhao,Haodong Wang,Wengang Zhou,Houqiang Li,Shengeng Tang,Jiayi He,Xu Wang,Ruobei Zhang,Yaxiong Wang,Lechao Cheng,Meryem Tasyurek,Tugce Kiziltepe,Hacer Yalim Keles*

Main category: cs.CV

TL;DR: 该论文介绍了首届手语生成挑战赛（SLP Challenge），旨在通过标准化评估指标比较不同系统在文本到姿势（T2P）翻译任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标准化的评估指标，手语生成（SLP）领域难以进行系统间的有效比较。

Method: 挑战赛使用RWTH-PHOENIX-Weather-2014T数据集和自定义隐藏测试集，评估T2P翻译方法。

Result: 33名参与者提交了231个解决方案，最佳团队BLEU-1得分为31.40，DTW-MJE为0.0574。

Conclusion: 挑战赛为SLP领域提供了标准化评估网络和基线，促进未来研究的比较。

Abstract: Sign Language Production (SLP) is the task of generating sign language video
from spoken language inputs. The field has seen a range of innovations over the
last few years, with the introduction of deep learning-based approaches
providing significant improvements in the realism and naturalness of generated
outputs. However, the lack of standardized evaluation metrics for SLP
approaches hampers meaningful comparisons across different systems. To address
this, we introduce the first Sign Language Production Challenge, held as part
of the third SLRTP Workshop at CVPR 2025. The competition's aims are to
evaluate architectures that translate from spoken language sentences to a
sequence of skeleton poses, known as Text-to-Pose (T2P) translation, over a
range of metrics. For our evaluation data, we use the
RWTH-PHOENIX-Weather-2014T dataset, a German Sign Language - Deutsche
Gebardensprache (DGS) weather broadcast dataset. In addition, we curate a
custom hidden test set from a similar domain of discourse. This paper presents
the challenge design and the winning methodologies. The challenge attracted 33
participants who submitted 231 solutions, with the top-performing team
achieving BLEU-1 scores of 31.40 and DTW-MJE of 0.0574. The winning approach
utilized a retrieval-based framework and a pre-trained language model. As part
of the workshop, we release a standardized evaluation network, including
high-quality skeleton extraction-based keypoints establishing a consistent
baseline for the SLP field, which will enable future researchers to compare
their work against a broader range of methods.

</details>


### [13] [Benchmarking Deep Learning-Based Object Detection Models on Feature Deficient Astrophotography Imagery Dataset](https://arxiv.org/abs/2508.06537)
*Shantanusinh Parmar*

Main category: cs.CV

TL;DR: 论文分析了智能手机天文摄影数据集MobilTelesco，评估了目标检测模型在稀疏特征条件下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测数据集（如ImageNet、COCO）主要关注日常物体，缺乏非商业领域（如天文摄影）中的信号稀疏性。

Method: 使用MobilTelesco数据集对多个目标检测模型进行基准测试。

Result: 研究揭示了在特征不足条件下目标检测模型面临的挑战。

Conclusion: MobilTelesco数据集填补了稀疏信号领域的空白，为模型在非传统场景中的性能评估提供了新工具。

Abstract: Object detection models are typically trained on datasets like ImageNet,
COCO, and PASCAL VOC, which focus on everyday objects. However, these lack
signal sparsity found in non-commercial domains. MobilTelesco, a
smartphone-based astrophotography dataset, addresses this by providing sparse
night-sky images. We benchmark several detection models on it, highlighting
challenges under feature-deficient conditions.

</details>


### [14] [OpenHAIV: A Framework Towards Practical Open-World Learning](https://arxiv.org/abs/2508.07270)
*Xiang Xiang,Qinhao Zhou,Zhuo Xu,Jing Ma,Jiaxin Dai,Yifan Liang,Hanlin Li*

Main category: cs.CV

TL;DR: OpenHAIV框架整合了OOD检测、新类发现和增量持续微调，解决了开放世界场景中模型知识更新的问题。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测和增量学习方法在开放世界场景中存在局限性，无法实现自主知识更新。

Method: 提出OpenHAIV框架，统一OOD检测、新类发现和增量持续微调。

Result: 框架支持模型在开放世界环境中自主获取和更新知识。

Conclusion: OpenHAIV为开放世界识别提供了一种有效的解决方案。

Abstract: Substantial progress has been made in various techniques for open-world
recognition. Out-of-distribution (OOD) detection methods can effectively
distinguish between known and unknown classes in the data, while incremental
learning enables continuous model knowledge updates. However, in open-world
scenarios, these approaches still face limitations. Relying solely on OOD
detection does not facilitate knowledge updates in the model, and incremental
fine-tuning typically requires supervised conditions, which significantly
deviate from open-world settings. To address these challenges, this paper
proposes OpenHAIV, a novel framework that integrates OOD detection, new class
discovery, and incremental continual fine-tuning into a unified pipeline. This
framework allows models to autonomously acquire and update knowledge in
open-world environments. The proposed framework is available at
https://haiv-lab.github.io/openhaiv .

</details>


### [15] [Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View](https://arxiv.org/abs/2508.06968)
*Ulas Gunes,Matias Turkulainen,Juho Kannala,Esa Rahtu*

Main category: cs.CV

TL;DR: 首次评估了基于鱼眼镜头的3D高斯溅射方法Fisheye-GS和3DGUT在视场超过180度的真实图像上的表现，分析了极端失真下的性能，并提出了一种基于深度的初始化策略。


<details>
  <summary>Details</summary>
Motivation: 研究鱼眼镜头在极端失真条件下的3D重建性能，解决传统SfM初始化在强失真场景中的局限性。

Method: 在室内外场景中使用200度鱼眼相机，评估不同视场（200、160、120度）下的性能，并提出基于UniK3D预测的深度初始化策略。

Result: Fisheye-GS在160度视场表现最佳，3DGUT在200度视场下保持稳定；UniK3D策略在稀疏图像输入下实现与SfM相当的3D重建质量。

Conclusion: 鱼眼3DGS方法在广角3D重建中具有实用潜力，尤其适用于稀疏且失真严重的图像输入。

Abstract: We present the first evaluation of fisheye-based 3D Gaussian Splatting
methods, Fisheye-GS and 3DGUT, on real images with fields of view exceeding 180
degree. Our study covers both indoor and outdoor scenes captured with 200
degree fisheye cameras and analyzes how each method handles extreme distortion
in real world settings. We evaluate performance under varying fields of view
(200 degree, 160 degree, and 120 degree) to study the tradeoff between
peripheral distortion and spatial coverage. Fisheye-GS benefits from field of
view (FoV) reduction, particularly at 160 degree, while 3DGUT remains stable
across all settings and maintains high perceptual quality at the full 200
degree view. To address the limitations of SfM-based initialization, which
often fails under strong distortion, we also propose a depth-based strategy
using UniK3D predictions from only 2-3 fisheye images per scene. Although
UniK3D is not trained on real fisheye data, it produces dense point clouds that
enable reconstruction quality on par with SfM, even in difficult scenes with
fog, glare, or sky. Our results highlight the practical viability of
fisheye-based 3DGS methods for wide-angle 3D reconstruction from sparse and
distortion-heavy image inputs.

</details>


### [16] [MILD: Multi-Layer Diffusion Strategy for Complex and Precise Multi-IP Aware Human Erasing](https://arxiv.org/abs/2508.06543)
*Jinghan Yu,Zhiyuan Ma,Yue Ma,Kaiqi Liu,Yuhan Wang,Jianjun Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为MILD的新方法，用于解决复杂多实例场景下的人类擦除任务，通过多层扩散和空间解耦策略显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂多实例场景（如人类遮挡、物体纠缠和背景干扰）中表现不佳，主要由于数据集限制和缺乏空间解耦能力。

Method: 提出Multi-Layer Diffusion (MILD)策略，将生成过程分解为语义分离的路径，并引入Human Morphology Guidance和Spatially-Modulated Attention。

Result: MILD在挑战性的人类擦除基准测试中优于现有最先进方法。

Conclusion: MILD通过多层扩散和空间解耦策略，有效解决了复杂场景下的人类擦除问题。

Abstract: Recent years have witnessed the success of diffusion models in
image-customized tasks. Prior works have achieved notable progress on
human-oriented erasing using explicit mask guidance and semantic-aware
inpainting. However, they struggle under complex multi-IP scenarios involving
human-human occlusions, human-object entanglements, and background
interferences. These challenges are mainly due to: 1) Dataset limitations, as
existing datasets rarely cover dense occlusions, camouflaged backgrounds, and
diverse interactions; 2) Lack of spatial decoupling, where foreground instances
cannot be effectively disentangled, limiting clean background restoration. In
this work, we introduce a high-quality multi-IP human erasing dataset with
diverse pose variations and complex backgrounds. We then propose Multi-Layer
Diffusion (MILD), a novel strategy that decomposes generation into semantically
separated pathways for each instance and the background. To enhance
human-centric understanding, we introduce Human Morphology Guidance,
integrating pose, parsing, and spatial relations. We further present
Spatially-Modulated Attention to better guide attention flow. Extensive
experiments show that MILD outperforms state-of-the-art methods on challenging
human erasing benchmarks.

</details>


### [17] [Novel View Synthesis with Gaussian Splatting: Impact on Photogrammetry Model Accuracy and Resolution](https://arxiv.org/abs/2508.07483)
*Pranav Chougule*

Main category: cs.CV

TL;DR: 本文比较了摄影测量与高斯泼溅技术在3D模型重建和视图合成中的表现，通过多种指标评估性能，并展示了高斯泼溅在生成高质量新视图方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在比较摄影测量和高斯泼溅技术在3D重建和视图合成中的效果，探索高斯泼溅在提升摄影测量模型质量方面的潜力。

Method: 使用真实场景图像数据集，分别通过摄影测量和高斯泼溅技术构建3D模型，并通过SSIM、PSNR、LPIPS和分辨率等指标进行评估。开发了改进的高斯泼溅工具，支持从Blender生成的新相机姿态渲染图像。

Result: 结果表明，高斯泼溅能够生成高质量的新视图，并可能提升基于摄影测量的3D重建效果。比较分析揭示了两者的优缺点。

Conclusion: 高斯泼溅在生成新视图方面表现出色，对摄影测量改进有潜力，为XR、摄影测量和自动驾驶模拟提供了有价值的信息。

Abstract: In this paper, I present a comprehensive study comparing Photogrammetry and
Gaussian Splatting techniques for 3D model reconstruction and view synthesis. I
created a dataset of images from a real-world scene and constructed 3D models
using both methods. To evaluate the performance, I compared the models using
structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), learned
perceptual image patch similarity (LPIPS), and lp/mm resolution based on the
USAF resolution chart. A significant contribution of this work is the
development of a modified Gaussian Splatting repository, which I forked and
enhanced to enable rendering images from novel camera poses generated in the
Blender environment. This innovation allows for the synthesis of high-quality
novel views, showcasing the flexibility and potential of Gaussian Splatting. My
investigation extends to an augmented dataset that includes both original
ground images and novel views synthesized via Gaussian Splatting. This
augmented dataset was employed to generate a new photogrammetry model, which
was then compared against the original photogrammetry model created using only
the original images. The results demonstrate the efficacy of using Gaussian
Splatting to generate novel high-quality views and its potential to improve
photogrammetry-based 3D reconstructions. The comparative analysis highlights
the strengths and limitations of both approaches, providing valuable
information for applications in extended reality (XR), photogrammetry, and
autonomous vehicle simulations. Code is available at
https://github.com/pranavc2255/gaussian-splatting-novel-view-render.git.

</details>


### [18] [HiMat: DiT-based Ultra-High Resolution SVBRDF Generation](https://arxiv.org/abs/2508.07011)
*Zixiong Wang,Jian Yang,Yiwei Hu,Milos Hasan,Beibei Wang*

Main category: cs.CV

TL;DR: HiMat是一个高效生成4K分辨率SVBRDF的扩散框架，通过轻量级CrossStitch模块保持多图一致性。


<details>
  <summary>Details</summary>
Motivation: 利用扩散变换器（DiT）生成高分辨率SVBRDF，但需解决多图对齐和一致性问题。

Method: 引入CrossStitch模块，通过局部操作捕获图间依赖，保持DiT主干不变。

Result: 生成具有强结构一致性和高频细节的4K SVBRDF，适用于文本提示生成。

Conclusion: HiMat高效且通用，可扩展至其他任务如本征分解。

Abstract: Creating highly detailed SVBRDFs is essential for 3D content creation. The
rise of high-resolution text-to-image generative models, based on diffusion
transformers (DiT), suggests an opportunity to finetune them for this task.
However, retargeting the models to produce multiple aligned SVBRDF maps instead
of just RGB images, while achieving high efficiency and ensuring consistency
across different maps, remains a challenge. In this paper, we introduce HiMat:
a memory- and computation-efficient diffusion-based framework capable of
generating native 4K-resolution SVBRDFs. A key challenge we address is
maintaining consistency across different maps in a lightweight manner, without
relying on training new VAEs or significantly altering the DiT backbone (which
would damage its prior capabilities). To tackle this, we introduce the
CrossStitch module, a lightweight convolutional module that captures inter-map
dependencies through localized operations. Its weights are initialized such
that the DiT backbone operation is unchanged before finetuning starts. HiMat
enables generation with strong structural coherence and high-frequency details.
Results with a large set of text prompts demonstrate the effectiveness of our
approach for 4K SVBRDF generation. Further experiments suggest generalization
to tasks such as intrinsic decomposition.

</details>


### [19] [Slice or the Whole Pie? Utility Control for AI Models](https://arxiv.org/abs/2508.06551)
*Ye Tao*

Main category: cs.CV

TL;DR: NNObfuscator是一种新型实用控制机制，允许AI模型根据预设条件动态调整性能，避免为不同用户需求训练多个模型。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络训练资源密集、适应多样化需求时效率低下的问题。

Method: 提出NNObfuscator机制，使单一模型能实时调整性能，支持分层访问控制。

Result: 实验证明NNObfuscator提高了模型的适应性和资源利用率，减少了计算开销。

Conclusion: NNObfuscator为AI部署提供了高效、可持续的解决方案，支持多样化需求。

Abstract: Training deep neural networks (DNNs) has become an increasingly
resource-intensive task, requiring large volumes of labeled data, substantial
computational power, and considerable fine-tuning efforts to achieve optimal
performance across diverse use cases. Although pre-trained models offer a
useful starting point, adapting them to meet specific user needs often demands
extensive customization, and infrastructure overhead. This challenge grows when
a single model must support diverse appli-cations with differing requirements
for performance. Traditional solutions often involve training multiple model
versions to meet varying requirements, which can be inefficient and difficult
to maintain. In order to overcome this challenge, we propose NNObfuscator, a
novel utility control mechanism that enables AI models to dynamically modify
their performance according to predefined conditions. It is different from
traditional methods that need separate models for each user. Instead,
NNObfuscator allows a single model to be adapted in real time, giving you
controlled access to multiple levels of performance. This mechanism enables
model owners set up tiered access, ensuring that free-tier users receive a
baseline level of performance while premium users benefit from enhanced
capabilities. The approach improves resource allocation, reduces unnecessary
computation, and supports sustainable business models in AI deployment. To
validate our approach, we conducted experiments on multiple tasks, including
image classification, semantic segmentation, and text to image generation,
using well-established models such as ResNet, DeepLab, VGG16, FCN and Stable
Diffusion. Experimental results show that NNObfuscator successfully makes model
more adaptable, so that a single trained model can handle a broad range of
tasks without requiring a lot of changes.

</details>


### [20] [Matrix-3D: Omnidirectional Explorable 3D World Generation](https://arxiv.org/abs/2508.08086)
*Zhongqi Yang,Wenhang Ge,Yuqi Li,Jiaqi Chen,Haoyuan Li,Mengyin An,Fei Kang,Hua Xue,Baixin Xu,Yuyang Yin,Eric Li,Yang Liu,Yikai Wang,Hao-Xiang Guo,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-3D是一个利用全景表示和视频扩散模型生成可探索3D世界的框架，结合了条件视频生成和全景3D重建，实现了高质量和几何一致的场景生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成场景时范围有限，Matrix-3D旨在通过全景表示和两种3D重建方法解决这一问题。

Method: 训练轨迹引导的全景视频扩散模型，结合前馈式大范围重建和优化式重建方法，并引入Matrix-Pano数据集支持训练。

Result: 实验表明，Matrix-3D在全景视频生成和3D世界生成中达到最先进性能。

Conclusion: Matrix-3D框架通过全景表示和高效重建方法，显著提升了3D世界生成的覆盖范围和质量。

Abstract: Explorable 3D world generation from a single image or text prompt forms a
cornerstone of spatial intelligence. Recent works utilize video model to
achieve wide-scope and generalizable 3D world generation. However, existing
approaches often suffer from a limited scope in the generated scenes. In this
work, we propose Matrix-3D, a framework that utilize panoramic representation
for wide-coverage omnidirectional explorable 3D world generation that combines
conditional video generation and panoramic 3D reconstruction. We first train a
trajectory-guided panoramic video diffusion model that employs scene mesh
renders as condition, to enable high-quality and geometrically consistent scene
video generation. To lift the panorama scene video to 3D world, we propose two
separate methods: (1) a feed-forward large panorama reconstruction model for
rapid 3D scene reconstruction and (2) an optimization-based pipeline for
accurate and detailed 3D scene reconstruction. To facilitate effective
training, we also introduce the Matrix-Pano dataset, the first large-scale
synthetic collection comprising 116K high-quality static panoramic video
sequences with depth and trajectory annotations. Extensive experiments
demonstrate that our proposed framework achieves state-of-the-art performance
in panoramic video generation and 3D world generation. See more in
https://matrix-3d.github.io.

</details>


### [21] [Age-Diverse Deepfake Dataset: Bridging the Age Gap in Deepfake Detection](https://arxiv.org/abs/2508.06552)
*Unisha Joshi*

Main category: cs.CV

TL;DR: 本文提出了一种解决深度伪造检测中年龄偏见的年龄多样化数据集，通过结合现有数据集和合成数据填补年龄分布空白，提升了模型的公平性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度伪造检测中的数据集存在年龄偏见问题，影响了模型的公平性和准确性，本文旨在解决这一问题。

Method: 构建了一个年龄多样化的深度伪造数据集，结合Celeb-DF、FaceForensics++和UTKFace数据集，并通过合成数据填补年龄分布空白。使用XceptionNet、EfficientNet和LipForensics三种模型评估数据集的公平性和泛化能力。

Result: 实验结果表明，基于年龄多样化数据集训练的模型在公平性、准确性和泛化能力上均有显著提升。

Conclusion: 本研究提供了一个可复现的、关注公平性的深度伪造数据集和模型流程，为未来公平性研究奠定了基础。

Abstract: The challenges associated with deepfake detection are increasing
significantly with the latest advancements in technology and the growing
popularity of deepfake videos and images. Despite the presence of numerous
detection models, demographic bias in the deepfake dataset remains largely
unaddressed. This paper focuses on the mitigation of age-specific bias in the
deepfake dataset by introducing an age-diverse deepfake dataset that will
improve fairness across age groups. The dataset is constructed through a
modular pipeline incorporating the existing deepfake datasets Celeb-DF,
FaceForensics++, and UTKFace datasets, and the creation of synthetic data to
fill the age distribution gaps. The effectiveness and generalizability of this
dataset are evaluated using three deepfake detection models: XceptionNet,
EfficientNet, and LipForensics. Evaluation metrics, including AUC, pAUC, and
EER, revealed that models trained on the age-diverse dataset demonstrated
fairer performance across age groups, improved overall accuracy, and higher
generalization across datasets. This study contributes a reproducible,
fairness-aware deepfake dataset and model pipeline that can serve as a
foundation for future research in fairer deepfake detection. The complete
dataset and implementation code are available at
https://github.com/unishajoshi/age-diverse-deepfake-detection.

</details>


### [22] [Static and Plugged: Make Embodied Evaluation Simple](https://arxiv.org/abs/2508.06553)
*Jiahao Xiao,Jianbo Zhang,BoWen Yan,Shengyu Guo,Tongrui Ye,Kaiwei Zhang,Zicheng Zhang,Xiaohong Liu,Zhengxue Cheng,Lei Fan,Chuyi Li,Guangtao Zhai*

Main category: cs.CV

TL;DR: StaticEmbodiedBench是一个用于评估具身智能的静态场景基准，覆盖42个场景和8个核心维度，支持高效评估，并发布了200个样本以促进研究。


<details>
  <summary>Details</summary>
Motivation: 当前具身智能评估依赖交互式模拟或真实环境，成本高且难以扩展。

Method: 提出StaticEmbodiedBench，基于静态场景表示的统一评估方法。

Result: 评估了19个VLMs和11个VLAs，建立了首个静态排行榜。

Conclusion: StaticEmbodiedBench为具身智能提供了高效、可扩展的评估工具。

Abstract: Embodied intelligence is advancing rapidly, driving the need for efficient
evaluation. Current benchmarks typically rely on interactive simulated
environments or real-world setups, which are costly, fragmented, and hard to
scale. To address this, we introduce StaticEmbodiedBench, a plug-and-play
benchmark that enables unified evaluation using static scene representations.
Covering 42 diverse scenarios and 8 core dimensions, it supports scalable and
comprehensive assessment through a simple interface. Furthermore, we evaluate
19 Vision-Language Models (VLMs) and 11 Vision-Language-Action models (VLAs),
establishing the first unified static leaderboard for Embodied intelligence.
Moreover, we release a subset of 200 samples from our benchmark to accelerate
the development of embodied intelligence.

</details>


### [23] [StyleTailor: Towards Personalized Fashion Styling via Hierarchical Negative Feedback](https://arxiv.org/abs/2508.06555)
*Hongbo Ma,Fei Shen,Hongbin Xu,Xiaoce Wang,Gang Xu,Jinkai Zheng,Liangqiong Qu,Ming Li*

Main category: cs.CV

TL;DR: StyleTailor是一个协作代理框架，整合了服装设计、购物推荐、虚拟试穿和系统评估，通过多级负面反馈实现个性化时尚推荐。


<details>
  <summary>Details</summary>
Motivation: 个性化时尚搭配的解决方案尚未充分探索，而智能代理的进步为提升购物体验提供了潜力。

Method: StyleTailor采用迭代视觉优化范式，通过多级负面反馈驱动，包含Designer和Consultant两个核心代理，利用分层视觉语言模型反馈逐步优化。

Result: 实验表明，StyleTailor在个性化设计和推荐方面表现优异，超越了无负面反馈的基线方法，并设立了新的智能时尚系统基准。

Conclusion: StyleTailor通过闭环反馈机制提升了推荐质量，为智能时尚系统提供了新的解决方案。

Abstract: The advancement of intelligent agents has revolutionized problem-solving
across diverse domains, yet solutions for personalized fashion styling remain
underexplored, which holds immense promise for promoting shopping experiences.
In this work, we present StyleTailor, the first collaborative agent framework
that seamlessly unifies personalized apparel design, shopping recommendation,
virtual try-on, and systematic evaluation into a cohesive workflow. To this
end, StyleTailor pioneers an iterative visual refinement paradigm driven by
multi-level negative feedback, enabling adaptive and precise user alignment.
Specifically, our framework features two core agents, i.e., Designer for
personalized garment selection and Consultant for virtual try-on, whose outputs
are progressively refined via hierarchical vision-language model feedback
spanning individual items, complete outfits, and try-on efficacy.
Counterexamples are aggregated into negative prompts, forming a closed-loop
mechanism that enhances recommendation quality.To assess the performance, we
introduce a comprehensive evaluation suite encompassing style consistency,
visual quality, face similarity, and artistic appraisal. Extensive experiments
demonstrate StyleTailor's superior performance in delivering personalized
designs and recommendations, outperforming strong baselines without negative
feedback and establishing a new benchmark for intelligent fashion systems.

</details>


### [24] [From Label Error Detection to Correction: A Modular Framework and Benchmark for Object Detection Datasets](https://arxiv.org/abs/2508.06556)
*Sarina Penquitt,Jonathan Klees,Rinor Cakaj,Daniel Kondermann,Matthias Rottmann,Lars Schmarje*

Main category: cs.CV

TL;DR: 论文提出了一种半自动化的标签错误修正框架REC$\checkmark$D，通过结合现有检测器和众包微任务，显著提升了标签质量。


<details>
  <summary>Details</summary>
Motivation: 对象检测数据集中的标签错误（如缺失标签、分类错误或定位不准确）会影响训练和评估结果，但目前缺乏系统性的大规模修正方法。

Method: REC$\checkmark$D框架结合现有错误检测器和众包微任务，通过多标注者独立验证候选边界框，聚合响应以估计模糊性并提升标签质量。

Result: 在KITTI数据集的“行人”类中，修正后的标注显示原始标注中至少24%存在缺失或不准确。

Conclusion: REC$\checkmark$D框架有效提升了标签质量，但现有方法仍会遗漏大量错误，需进一步研究。

Abstract: Object detection has advanced rapidly in recent years, driven by increasingly
large and diverse datasets. However, label errors, defined as missing labels,
incorrect classification or inaccurate localization, often compromise the
quality of these datasets. This can have a significant impact on the outcomes
of training and benchmark evaluations. Although several methods now exist for
detecting label errors in object detection datasets, they are typically
validated only on synthetic benchmarks or limited manual inspection. How to
correct such errors systemically and at scale therefore remains an open
problem. We introduce a semi-automated framework for label-error correction
called REC$\checkmark$D (Rechecked). Building on existing detectors, the
framework pairs their error proposals with lightweight, crowd-sourced
microtasks. These tasks enable multiple annotators to independently verify each
candidate bounding box, and their responses are aggregated to estimate
ambiguity and improve label quality. To demonstrate the effectiveness of
REC$\checkmark$D, we apply it to the class pedestrian in the KITTI dataset. Our
crowdsourced review yields high-quality corrected annotations, which indicate a
rate of at least 24% of missing and inaccurate annotations in original
annotations. This validated set will be released as a new real-world benchmark
for label error detection and correction. We show that current label error
detection methods, when combined with our correction framework, can recover
hundreds of errors in the time it would take a human to annotate bounding boxes
from scratch. However, even the best methods still miss up to 66% of the true
errors and with low quality labels introduce more errors than they find. This
highlights the urgent need for further research, now enabled by our released
benchmark.

</details>


### [25] [On the effectiveness of multimodal privileged knowledge distillation in two vision transformer based diagnostic applications](https://arxiv.org/abs/2508.06558)
*Simon Baur,Alexandra Benova,Emilio Dolgener Cantú,Jackie Ma*

Main category: cs.CV

TL;DR: 提出了一种多模态特权知识蒸馏（MMPKD）方法，利用训练时额外的模态指导单模态视觉模型，提升注意力图的零样本能力，但效果不跨域。


<details>
  <summary>Details</summary>
Motivation: 临床实践中部署深度学习模型常需多模态数据，但推理时并非所有模态都可用。

Method: 使用文本和表格元数据的教师模型（MIMIC-CXR和CBIS-DDSM）向视觉Transformer学生模型蒸馏知识。

Result: MMPKD提升了注意力图在输入图像中定位ROI的零样本能力，但效果不跨域。

Conclusion: MMPKD在单模态任务中有效，但跨域泛化能力有限。

Abstract: Deploying deep learning models in clinical practice often requires leveraging
multiple data modalities, such as images, text, and structured data, to achieve
robust and trustworthy decisions. However, not all modalities are always
available at inference time. In this work, we propose multimodal privileged
knowledge distillation (MMPKD), a training strategy that utilizes additional
modalities available solely during training to guide a unimodal vision model.
Specifically, we used a text-based teacher model for chest radiographs
(MIMIC-CXR) and a tabular metadata-based teacher model for mammography
(CBIS-DDSM) to distill knowledge into a vision transformer student model. We
show that MMPKD can improve the resulting attention maps' zero-shot
capabilities of localizing ROI in input images, while this effect does not
generalize across domains, as contrarily suggested by prior research.

</details>


### [26] [Grounding Emotion Recognition with Visual Prototypes: VEGA -- Revisiting CLIP in MERC](https://arxiv.org/abs/2508.06564)
*Guanyu Hu,Dimitrios Kollias,Xinyu Yang*

Main category: cs.CV

TL;DR: 提出了一种基于CLIP的视觉情感引导锚定机制（VEGA），通过引入类级视觉语义提升多模态情感识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型缺乏心理学先验指导多模态对齐，VEGA通过视觉锚定机制解决这一问题。

Method: 利用CLIP的图像编码器构建情感特定视觉锚点，结合随机锚点采样和自蒸馏双分支架构。

Result: 在IEMOCAP和MELD数据集上达到最优性能。

Conclusion: VEGA通过心理学对齐的视觉锚点提升了多模态情感识别的效果。

Abstract: Multimodal Emotion Recognition in Conversations remains a challenging task
due to the complex interplay of textual, acoustic and visual signals. While
recent models have improved performance via advanced fusion strategies, they
often lack psychologically meaningful priors to guide multimodal alignment. In
this paper, we revisit the use of CLIP and propose a novel Visual Emotion
Guided Anchoring (VEGA) mechanism that introduces class-level visual semantics
into the fusion and classification process. Distinct from prior work that
primarily utilizes CLIP's textual encoder, our approach leverages its image
encoder to construct emotion-specific visual anchors based on facial exemplars.
These anchors guide unimodal and multimodal features toward a perceptually
grounded and psychologically aligned representation space, drawing inspiration
from cognitive theories (prototypical emotion categories and multisensory
integration). A stochastic anchor sampling strategy further enhances robustness
by balancing semantic stability and intra-class diversity. Integrated into a
dual-branch architecture with self-distillation, our VEGA-augmented model
achieves sota performance on IEMOCAP and MELD. Code is available at:
https://github.com/dkollias/VEGA.

</details>


### [27] [Bridging Brain Connectomes and Clinical Reports for Early Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.06565)
*Jing Zhang,Xiaowei Yu,Minheng Chen,Lu Zhang,Tong Chen,Yan Zhuang,Chao Cao,Yanjun Lyu,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.CV

TL;DR: 提出一种新框架，将脑连接组与临床报告在共享跨模态潜在空间中对齐，提升表征学习，应用于轻度认知障碍研究。


<details>
  <summary>Details</summary>
Motivation: 整合脑成像数据与临床报告以提升诊断效果，解决如何有效链接客观成像数据与主观文本报告的挑战。

Method: 将脑子网络作为成像数据的标记，与临床报告中的词标记对齐，构建共享跨模态潜在空间。

Result: 在ADNI数据集上实现最佳预测性能，并识别出有临床意义的连接组-文本对。

Conclusion: 该方法为阿尔茨海默病早期机制提供新见解，支持开发临床有用的多模态生物标志物。

Abstract: Integrating brain imaging data with clinical reports offers a valuable
opportunity to leverage complementary multimodal information for more effective
and timely diagnosis in practical clinical settings. This approach has gained
significant attention in brain disorder research, yet a key challenge remains:
how to effectively link objective imaging data with subjective text-based
reports, such as doctors' notes. In this work, we propose a novel framework
that aligns brain connectomes with clinical reports in a shared cross-modal
latent space at both the subject and connectome levels, thereby enhancing
representation learning. The key innovation of our approach is that we treat
brain subnetworks as tokens of imaging data, rather than raw image patches, to
align with word tokens in clinical reports. This enables a more efficient
identification of system-level associations between neuroimaging findings and
clinical observations, which is critical since brain disorders often manifest
as network-level abnormalities rather than isolated regional alterations. We
applied our method to mild cognitive impairment (MCI) using the ADNI dataset.
Our approach not only achieves state-of-the-art predictive performance but also
identifies clinically meaningful connectome-text pairs, offering new insights
into the early mechanisms of Alzheimer's disease and supporting the development
of clinically useful multimodal biomarkers.

</details>


### [28] [Surformer v1: Transformer-Based Surface Classification Using Tactile and Vision Features](https://arxiv.org/abs/2508.06566)
*Manish Kansana,Elias Hossain,Shahram Rahimi,Noorbakhsh Amiri Golilarz*

Main category: cs.CV

TL;DR: Surformer v1是一种基于Transformer的架构，结合触觉和视觉输入进行表面材料分类，在准确性和计算效率之间取得了良好平衡。


<details>
  <summary>Details</summary>
Motivation: 表面材料识别是机器人感知和物理交互的关键，尤其是在结合触觉和视觉输入时。

Method: 提出Surformer v1，使用结构化触觉特征和PCA降维的视觉嵌入，结合模态特定编码器和跨模态注意力层。

Result: Surformer v1在触觉分类中表现最佳，多模态融合后达到99.4%准确率，推理时间为0.77毫秒。

Conclusion: Surformer v1在准确性、效率和计算成本之间提供了理想平衡，适合实时应用。

Abstract: Surface material recognition is a key component in robotic perception and
physical interaction, particularly when leveraging both tactile and visual
sensory inputs. In this work, we propose Surformer v1, a transformer-based
architecture designed for surface classification using structured tactile
features and PCA-reduced visual embeddings extracted via ResNet-50. The model
integrates modality-specific encoders with cross-modal attention layers,
enabling rich interactions between vision and touch. Currently,
state-of-the-art deep learning models for vision tasks have achieved remarkable
performance. With this in mind, our first set of experiments focused
exclusively on tactile-only surface classification. Using feature engineering,
we trained and evaluated multiple machine learning models, assessing their
accuracy and inference time. We then implemented an encoder-only Transformer
model tailored for tactile features. This model not only achieved the highest
accuracy but also demonstrated significantly faster inference time compared to
other evaluated models, highlighting its potential for real-time applications.
To extend this investigation, we introduced a multimodal fusion setup by
combining vision and tactile inputs. We trained both Surformer v1 (using
structured features) and Multimodal CNN (using raw images) to examine the
impact of feature-based versus image-based multimodal learning on
classification accuracy and computational efficiency. The results showed that
Surformer v1 achieved 99.4% accuracy with an inference time of 0.77 ms, while
the Multimodal CNN achieved slightly higher accuracy but required significantly
more inference time. These findings suggest Surformer v1 offers a compelling
balance between accuracy, efficiency, and computational cost for surface
material recognition.

</details>


### [29] [ImpliHateVid: A Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos](https://arxiv.org/abs/2508.06570)
*Mohammad Zia Ur Rehman,Anukriti Bhatnagar,Omkar Kabde,Shubhi Bansal,Nagendra Kumar*

Main category: cs.CV

TL;DR: 该论文提出了一个新颖的视频数据集ImpliHateVid，用于检测视频中的隐性仇恨言论，并提出了一种两阶段对比学习框架，结合多模态特征提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在文本和图像中的仇恨言论检测，而视频领域的隐性仇恨言论检测尚未充分探索。

Method: 提出两阶段对比学习框架：第一阶段训练音频、文本和图像的模态特定编码器；第二阶段训练跨模态编码器以优化多模态表示。结合情感、情绪和字幕特征增强检测。

Result: 在ImpliHateVid和HateMM数据集上验证了方法的有效性，证明了多模态对比学习在视频仇恨内容检测中的优势。

Conclusion: 论文填补了视频隐性仇恨言论检测的空白，提出的数据集和方法为未来研究提供了重要基础。

Abstract: The existing research has primarily focused on text and image-based hate
speech detection, video-based approaches remain underexplored. In this work, we
introduce a novel dataset, ImpliHateVid, specifically curated for implicit hate
speech detection in videos. ImpliHateVid consists of 2,009 videos comprising
509 implicit hate videos, 500 explicit hate videos, and 1,000 non-hate videos,
making it one of the first large-scale video datasets dedicated to implicit
hate detection. We also propose a novel two-stage contrastive learning
framework for hate speech detection in videos. In the first stage, we train
modality-specific encoders for audio, text, and image using contrastive loss by
concatenating features from the three encoders. In the second stage, we train
cross-encoders using contrastive learning to refine multimodal representations.
Additionally, we incorporate sentiment, emotion, and caption-based features to
enhance implicit hate detection. We evaluate our method on two datasets,
ImpliHateVid for implicit hate speech detection and another dataset for general
hate speech detection in videos, HateMM dataset, demonstrating the
effectiveness of the proposed multimodal contrastive learning for hateful
content detection in videos and the significance of our dataset.

</details>


### [30] [ContextGuard-LVLM: Enhancing News Veracity through Fine-grained Cross-modal Contextual Consistency Verification](https://arxiv.org/abs/2508.06623)
*Sihan Ma,Qiming Wu,Ruotong Jiang,Frank Burns*

Main category: cs.CV

TL;DR: 论文提出ContextGuard-LVLM框架，通过多阶段上下文推理机制增强视觉-语言大模型，解决细粒度跨模态上下文一致性（FCCC）问题，显著优于现有零样本基线。


<details>
  <summary>Details</summary>
Motivation: 数字新闻媒体的普及需要验证内容真实性，尤其是视觉与文本信息的细粒度一致性，传统方法难以满足。

Method: 基于视觉-语言大模型（LVLMs），结合多阶段上下文推理机制，并通过强化或对抗学习增强模型能力。

Result: 在多个数据集上表现优于现有零样本基线（如InstructBLIP和LLaVA 1.5），尤其在复杂逻辑推理和上下文理解方面。

Conclusion: ContextGuard-LVLM在检测上下文一致性方面高效且鲁棒，与人类专家判断一致。

Abstract: The proliferation of digital news media necessitates robust methods for
verifying content veracity, particularly regarding the consistency between
visual and textual information. Traditional approaches often fall short in
addressing the fine-grained cross-modal contextual consistency (FCCC) problem,
which encompasses deeper alignment of visual narrative, emotional tone, and
background information with text, beyond mere entity matching. To address this,
we propose ContextGuard-LVLM, a novel framework built upon advanced
Vision-Language Large Models (LVLMs) and integrating a multi-stage contextual
reasoning mechanism. Our model is uniquely enhanced through reinforced or
adversarial learning paradigms, enabling it to detect subtle contextual
misalignments that evade zero-shot baselines. We extend and augment three
established datasets (TamperedNews-Ent, News400-Ent, MMG-Ent) with new
fine-grained contextual annotations, including "contextual sentiment," "visual
narrative theme," and "scene-event logical coherence," and introduce a
comprehensive CTXT (Contextual Coherence) entity type. Extensive experiments
demonstrate that ContextGuard-LVLM consistently outperforms state-of-the-art
zero-shot LVLM baselines (InstructBLIP and LLaVA 1.5) across nearly all
fine-grained consistency tasks, showing significant improvements in complex
logical reasoning and nuanced contextual understanding. Furthermore, our model
exhibits superior robustness to subtle perturbations and a higher agreement
rate with human expert judgments on challenging samples, affirming its efficacy
in discerning sophisticated forms of context detachment.

</details>


### [31] [VL-MedGuide: A Visual-Linguistic Large Model for Intelligent and Explainable Skin Disease Auxiliary Diagnosis](https://arxiv.org/abs/2508.06624)
*Kexin Yu,Zihan Xu,Jialei Xie,Carter Adams*

Main category: cs.CV

TL;DR: VL-MedGuide利用视觉-语言大模型的多模态能力，通过两阶段框架实现皮肤病的智能诊断和解释性分析，性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 解决皮肤病诊断中视觉特征复杂多样及现有模型缺乏解释性的问题。

Method: 采用多模态概念感知模块和可解释疾病推理模块，结合提示工程和链式思维提示。

Result: 在Derm7pt数据集上，疾病诊断和概念检测性能均达到最优（BACC 83.55%和76.10%）。

Conclusion: VL-MedGuide为皮肤病诊断提供了高性能且可解释的解决方案，具有临床实用性。

Abstract: Accurate diagnosis of skin diseases remains a significant challenge due to
the complex and diverse visual features present in dermatoscopic images, often
compounded by a lack of interpretability in existing purely visual diagnostic
models. To address these limitations, this study introduces VL-MedGuide
(Visual-Linguistic Medical Guide), a novel framework leveraging the powerful
multi-modal understanding and reasoning capabilities of Visual-Language Large
Models (LVLMs) for intelligent and inherently interpretable auxiliary diagnosis
of skin conditions. VL-MedGuide operates in two interconnected stages: a
Multi-modal Concept Perception Module, which identifies and linguistically
describes dermatologically relevant visual features through sophisticated
prompt engineering, and an Explainable Disease Reasoning Module, which
integrates these concepts with raw visual information via Chain-of-Thought
prompting to provide precise disease diagnoses alongside transparent
rationales. Comprehensive experiments on the Derm7pt dataset demonstrate that
VL-MedGuide achieves state-of-the-art performance in both disease diagnosis
(83.55% BACC, 80.12% F1) and concept detection (76.10% BACC, 67.45% F1),
surpassing existing baselines. Furthermore, human evaluations confirm the high
clarity, completeness, and trustworthiness of its generated explanations,
bridging the gap between AI performance and clinical utility by offering
actionable, explainable insights for dermatological practice.

</details>


### [32] [CycleDiff: Cycle Diffusion Models for Unpaired Image-to-image Translation](https://arxiv.org/abs/2508.06625)
*Shilong Zou,Yuhang Huang,Renjiao Yi,Chenyang Zhu,Kai Xu*

Main category: cs.CV

TL;DR: 提出一种基于扩散模型的跨域图像翻译方法，通过联合学习框架对齐扩散和翻译过程，提升全局最优性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在跨域图像翻译中因扩散与翻译过程未对齐而导致的局部最优问题。

Method: 提出联合学习框架，用扩散模型提取图像成分表示干净信号，并结合时间依赖的翻译网络进行端到端学习。

Result: 在RGB↔RGB及多种跨模态翻译任务中表现优于现有方法。

Conclusion: 联合学习框架有效提升全局最优性，生成结果在保真度和结构一致性上更优。

Abstract: We introduce a diffusion-based cross-domain image translator in the absence
of paired training data. Unlike GAN-based methods, our approach integrates
diffusion models to learn the image translation process, allowing for more
coverable modeling of the data distribution and performance improvement of the
cross-domain translation. However, incorporating the translation process within
the diffusion process is still challenging since the two processes are not
aligned exactly, i.e., the diffusion process is applied to the noisy signal
while the translation process is conducted on the clean signal. As a result,
recent diffusion-based studies employ separate training or shallow integration
to learn the two processes, yet this may cause the local minimal of the
translation optimization, constraining the effectiveness of diffusion models.
To address the problem, we propose a novel joint learning framework that aligns
the diffusion and the translation process, thereby improving the global
optimality. Specifically, we propose to extract the image components with
diffusion models to represent the clean signal and employ the translation
process with the image components, enabling an end-to-end joint learning
manner. On the other hand, we introduce a time-dependent translation network to
learn the complex translation mapping, resulting in effective translation
learning and significant performance improvement. Benefiting from the design of
joint learning, our method enables global optimization of both processes,
enhancing the optimality and achieving improved fidelity and structural
consistency. We have conducted extensive experiments on RGB$\leftrightarrow$RGB
and diverse cross-modality translation tasks including
RGB$\leftrightarrow$Edge, RGB$\leftrightarrow$Semantics and
RGB$\leftrightarrow$Depth, showcasing better generative performances than the
state of the arts.

</details>


### [33] [CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition](https://arxiv.org/abs/2508.06632)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Tiancheng Zhao,Gaolei Li,Changting Lin,Yike Guo,Meng Han*

Main category: cs.CV

TL;DR: 提出了一种基于动态系数分解的神经渲染框架，用于改进复杂视点依赖外观的建模，特别是在处理镜面反射和高光时表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂镜面反射和高光时，常因光照与材质属性的纠缠导致模糊反射，或依赖基于物理的逆渲染时优化不稳定。

Method: 通过动态系数分解，将复杂外观分解为共享的静态神经基（编码材质属性）和由视点与光照条件生成的动态系数，再通过动态辐射积分器合成最终辐射。

Result: 在多个挑战性基准测试中，该方法能生成比现有技术更清晰、更真实的镜面高光。

Conclusion: 这种分解范式为神经场景表示中复杂外观建模提供了灵活有效的方向。

Abstract: Neural Radiance Fields (NeRF) have shown impressive performance in novel view
synthesis, but challenges remain in rendering scenes with complex specular
reflections and highlights. Existing approaches may produce blurry reflections
due to entanglement between lighting and material properties, or encounter
optimization instability when relying on physically-based inverse rendering. In
this work, we present a neural rendering framework based on dynamic coefficient
decomposition, aiming to improve the modeling of view-dependent appearance. Our
approach decomposes complex appearance into a shared, static neural basis that
encodes intrinsic material properties, and a set of dynamic coefficients
generated by a Coefficient Network conditioned on view and illumination. A
Dynamic Radiance Integrator then combines these components to synthesize the
final radiance. Experimental results on several challenging benchmarks suggest
that our method can produce sharper and more realistic specular highlights
compared to existing techniques. We hope that this decomposition paradigm can
provide a flexible and effective direction for modeling complex appearance in
neural scene representations.

</details>


### [34] [Rethinking Key-frame-based Micro-expression Recognition: A Robust and Accurate Framework Against Key-frame Errors](https://arxiv.org/abs/2508.06640)
*Zheyuan Zhang,Weihao Tang,Hong Chen*

Main category: cs.CV

TL;DR: CausalNet 是一种新颖的框架，旨在在关键帧索引错误的情况下实现稳健的微表情识别（MER），同时保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖准确的关键帧索引，但实际中获取准确索引困难且存在误差，限制了实际应用。

Method: CausalNet 通过输入完整微表情序列增强鲁棒性，并提出 CMPLM 和 CAB 模块减少冗余信息并学习肌肉运动的因果关系。

Result: 实验表明，CausalNet 在不同噪声水平下表现稳健，并在标准 MER 基准上超越现有方法。

Conclusion: CausalNet 解决了关键帧索引误差问题，为 MER 的实际应用提供了可行方案。

Abstract: Micro-expression recognition (MER) is a highly challenging task in affective
computing. With the reduced-sized micro-expression (ME) input that contains key
information based on key-frame indexes, key-frame-based methods have
significantly improved the performance of MER. However, most of these methods
focus on improving the performance with relatively accurate key-frame indexes,
while ignoring the difficulty of obtaining accurate key-frame indexes and the
objective existence of key-frame index errors, which impedes them from moving
towards practical applications. In this paper, we propose CausalNet, a novel
framework to achieve robust MER facing key-frame index errors while maintaining
accurate recognition. To enhance robustness, CausalNet takes the representation
of the entire ME sequence as the input. To address the information redundancy
brought by the complete ME range input and maintain accurate recognition,
first, the Causal Motion Position Learning Module (CMPLM) is proposed to help
the model locate the muscle movement areas related to Action Units (AUs),
thereby reducing the attention to other redundant areas. Second, the Causal
Attention Block (CAB) is proposed to deeply learn the causal relationships
between the muscle contraction and relaxation movements in MEs. Empirical
experiments have demonstrated that on popular ME benchmarks, the CausalNet has
achieved robust MER under different levels of key-frame index noise. Meanwhile,
it has surpassed state-of-the-art (SOTA) methods on several standard MER
benchmarks when using the provided annotated key-frames. Code is available at
https://github.com/tony19980810/CausalNet.

</details>


### [35] [Towards Robust Red-Green Watermarking for Autoregressive Image Generators](https://arxiv.org/abs/2508.06656)
*Denis Lukovnikov,Andreas Müller,Erwin Quiring,Asja Fischer*

Main category: cs.CV

TL;DR: 论文探讨了在自回归图像模型中使用生成水印的方法，提出了两种基于视觉令牌聚类的新水印方案，提高了对抗扰动和再生攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索自回归图像模型中生成水印的应用，填补现有研究空白，并解决水印在图像扰动下可检测性下降的问题。

Method: 方法包括两种基于视觉令牌聚类的水印方案：一种是无训练的聚类查找表方法，另一种是微调VAE编码器直接预测令牌聚类。

Result: 实验表明，聚类级水印提高了对抗扰动和再生攻击的鲁棒性，同时保持图像质量，且聚类分类进一步提升了水印可检测性。

Conclusion: 结论是提出的方法在鲁棒性和运行效率上优于基线方法，适用于自回归图像模型的水印检测与属性识别。

Abstract: In-generation watermarking for detecting and attributing generated content
has recently been explored for latent diffusion models (LDMs), demonstrating
high robustness. However, the use of in-generation watermarks in autoregressive
(AR) image models has not been explored yet. AR models generate images by
autoregressively predicting a sequence of visual tokens that are then decoded
into pixels using a vector-quantized decoder. Inspired by red-green watermarks
for large language models, we examine token-level watermarking schemes that
bias the next-token prediction based on prior tokens. We find that a direct
transfer of these schemes works in principle, but the detectability of the
watermarks decreases considerably under common image perturbations. As a
remedy, we propose two novel watermarking methods that rely on visual token
clustering to assign similar tokens to the same set. Firstly, we investigate a
training-free approach that relies on a cluster lookup table, and secondly, we
finetune VAE encoders to predict token clusters directly from perturbed images.
Overall, our experiments show that cluster-level watermarks improve robustness
against perturbations and regeneration attacks while preserving image quality.
Cluster classification further boosts watermark detectability, outperforming a
set of baselines. Moreover, our methods offer fast verification runtime,
comparable to lightweight post-hoc watermarking methods.

</details>


### [36] [Learning More by Seeing Less: Line Drawing Pretraining for Efficient, Transferable, and Human-Aligned Vision](https://arxiv.org/abs/2508.06696)
*Tianqin Li,George Liu,Tai Sing Lee*

Main category: cs.CV

TL;DR: 论文提出使用线描作为结构优先的预训练方式，以生成更紧凑和可泛化的视觉表示。实验表明，线描预训练模型在分类、检测和分割任务中表现更优，且具有更低的内在维度和更好的知识蒸馏效果。


<details>
  <summary>Details</summary>
Motivation: 现代视觉识别系统依赖冗余的视觉输入，而人类却能轻松理解稀疏的线描。论文旨在探索结构优先的视觉学习是否能提升模型的效率和泛化能力。

Method: 提出使用线描作为预训练模态，并通过实验验证其在分类、检测和分割任务中的效果。同时，提出无监督方法“学习绘制”以扩展线描预训练的应用。

Result: 线描预训练模型表现出更强的形状偏好、更集中的注意力和更高的数据效率。其表示具有更低的内在维度，且在知识蒸馏中表现更优。

Conclusion: 结构优先的视觉学习能提升模型的效率、泛化能力和人类对齐的归纳偏差，为构建更鲁棒和适应性强的视觉系统提供了简单而有效的策略。

Abstract: Despite remarkable progress in computer vision, modern recognition systems
remain limited by their dependence on rich, redundant visual inputs. In
contrast, humans can effortlessly understand sparse, minimal representations
like line drawings - suggesting that structure, rather than appearance,
underlies efficient visual understanding. In this work, we propose using line
drawings as a structure-first pretraining modality to induce more compact and
generalizable visual representations. We show that models pretrained on line
drawings develop stronger shape bias, more focused attention, and greater data
efficiency across classification, detection, and segmentation tasks. Notably,
these models also exhibit lower intrinsic dimensionality, requiring
significantly fewer principal components to capture representational variance -
echoing the similar observation in low dimensional efficient representation in
the brain. Beyond performance improvements, line drawing pretraining produces
more compressible representations, enabling better distillation into
lightweight student models. Students distilled from line-pretrained teachers
consistently outperform those trained from color-supervised teachers,
highlighting the benefits of structurally compact knowledge. Finally, we
demonstrate that the pretraining with line-drawing can also be extended to
unsupervised setting via our proposed method "learning to draw". Together, our
results support the view that structure-first visual learning fosters
efficiency, generalization, and human-aligned inductive biases - offering a
simple yet powerful strategy for building more robust and adaptable vision
systems.

</details>


### [37] [MMFformer: Multimodal Fusion Transformer Network for Depression Detection](https://arxiv.org/abs/2508.06701)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Hamdi Altaheri,Lobna Nassar,Fakhri Karray*

Main category: cs.CV

TL;DR: MMFformer是一种多模态抑郁症检测网络，通过社交媒体信息提取时空特征，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 抑郁症早期诊断困难，基于社交媒体内容的研究成为重要方向，但多模态数据的融合和时空信息提取是挑战。

Method: 使用带有残差连接的Transformer网络提取视频空间特征，Transformer编码器设计音频时间动态，通过后期和中间融合策略融合特征。

Result: 在两个大规模抑郁症检测数据集上，F1-Score分别提升13.92%和7.74%。

Conclusion: MMFformer在多模态抑郁症检测中表现出色，代码已公开。

Abstract: Depression is a serious mental health illness that significantly affects an
individual's well-being and quality of life, making early detection crucial for
adequate care and treatment. Detecting depression is often difficult, as it is
based primarily on subjective evaluations during clinical interviews. Hence,
the early diagnosis of depression, thanks to the content of social networks,
has become a prominent research area. The extensive and diverse nature of
user-generated information poses a significant challenge, limiting the accurate
extraction of relevant temporal information and the effective fusion of data
across multiple modalities. This paper introduces MMFformer, a multimodal
depression detection network designed to retrieve depressive spatio-temporal
high-level patterns from multimodal social media information. The transformer
network with residual connections captures spatial features from videos, and a
transformer encoder is exploited to design important temporal dynamics in
audio. Moreover, the fusion architecture fused the extracted features through
late and intermediate fusion strategies to find out the most relevant
intermodal correlations among them. Finally, the proposed network is assessed
on two large-scale depression detection datasets, and the results clearly
reveal that it surpasses existing state-of-the-art approaches, improving the
F1-Score by 13.92% for D-Vlog dataset and 7.74% for LMVD dataset. The code is
made available publicly at
https://github.com/rezwanh001/Large-Scale-Multimodal-Depression-Detection.

</details>


### [38] [Fourier Optics and Deep Learning Methods for Fast 3D Reconstruction in Digital Holography](https://arxiv.org/abs/2508.06703)
*Justin London*

Main category: cs.CV

TL;DR: 提出了一种高效快速的计算机生成全息图（CGH）合成框架，基于点云和MRI数据，通过非凸傅里叶光学优化算法生成相位全息图和复杂全息图，并通过2D中值滤波提升性能。


<details>
  <summary>Details</summary>
Motivation: 计算机生成全息图（CGH）是一种有前景的方法，但需要高效且快速的合成框架来优化波形调制。

Method: 利用初始点云和MRI数据重建体积对象，通过交替投影、SGD和拟牛顿方法生成相位全息图和复杂全息图，并使用2D中值滤波去除噪声。

Result: 通过MSE、RMSE和PSNR评估算法性能，并对比HoloNet深度学习方法，显示优化后的性能指标有所提升。

Conclusion: 提出的框架在CGH合成中表现出高效性和性能优势，2D中值滤波有效减少了噪声和伪影。

Abstract: Computer-generated holography (CGH) is a promising method that modulates
user-defined waveforms with digital holograms. An efficient and fast pipeline
framework is proposed to synthesize CGH using initial point cloud and MRI data.
This input data is reconstructed into volumetric objects that are then input
into non-convex Fourier optics optimization algorithms for phase-only hologram
(POH) and complex-hologram (CH) generation using alternating projection, SGD,
and quasi-Netwton methods. Comparison of reconstruction performance of these
algorithms as measured by MSE, RMSE, and PSNR is analyzed as well as to HoloNet
deep learning CGH. Performance metrics are shown to be improved by using 2D
median filtering to remove artifacts and speckled noise during optimization.

</details>


### [39] [Restage4D: Reanimating Deformable 3D Reconstruction from a Single Video](https://arxiv.org/abs/2508.06715)
*Jixuan He,Chieh Hubert Lin,Lu Qi,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 论文提出Restage4D方法，利用真实视频的运动先验生成物理一致的4D内容，通过视频重放训练策略和几何一致性损失提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型难以捕捉物理真实性和运动动态性，而真实视频能提供物理基础的几何和运动线索。

Method: 提出Restage4D，结合视频重放训练策略、遮挡感知刚性损失和去遮挡回溯机制。

Result: 在DAVIS和PointOdyssey数据集上验证，显示几何一致性、运动质量和3D跟踪性能的提升。

Conclusion: Restage4D能保持可变形结构并纠正生成模型的错误，展示了视频先验在4D重演任务中的潜力。

Abstract: Creating deformable 3D content has gained increasing attention with the rise
of text-to-image and image-to-video generative models. While these models
provide rich semantic priors for appearance, they struggle to capture the
physical realism and motion dynamics needed for authentic 4D scene synthesis.
In contrast, real-world videos can provide physically grounded geometry and
articulation cues that are difficult to hallucinate. One question is raised:
\textit{Can we generate physically consistent 4D content by leveraging the
motion priors of the real-world video}? In this work, we explore the task of
reanimating deformable 3D scenes from a single video, using the original
sequence as a supervisory signal to correct artifacts from synthetic motion. We
introduce \textbf{Restage4D}, a geometry-preserving pipeline for
video-conditioned 4D restaging. Our approach uses a video-rewinding training
strategy to temporally bridge a real base video and a synthetic driving video
via a shared motion representation. We further incorporate an occlusion-aware
rigidity loss and a disocclusion backtracing mechanism to improve structural
and geometry consistency under challenging motion. We validate Restage4D on
DAVIS and PointOdyssey, demonstrating improved geometry consistency, motion
quality, and 3D tracking performance. Our method not only preserves deformable
structure under novel motion, but also automatically corrects errors introduced
by generative models, revealing the potential of video prior in 4D restaging
task. Source code and trained models will be released.

</details>


### [40] [FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI](https://arxiv.org/abs/2508.06756)
*Somayeh Farahani,Marjaneh Hejazi,Antonio Di Ieva,Sidong Liu*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的非侵入性方法（FoundBioNet），用于从多参数MRI预测胶质瘤IDH突变状态，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统侵入性组织采样方法无法捕捉肿瘤的空间异质性，且现有深度学习模型因标注数据稀缺而性能受限。

Method: 采用SWIN-UNETR架构，结合肿瘤感知特征编码（TAFE）和跨模态差异（CMD）模块，从多参数MRI中提取特征并预测IDH突变。

Result: 在多个独立测试集上AUC达65.41%-90.58%，显著优于基线方法（p <= 0.05）。

Conclusion: FoundBioNet通过大规模预训练和任务微调，实现了可泛化的胶质瘤表征，提升了诊断准确性和可解释性。

Abstract: Accurate, noninvasive detection of isocitrate dehydrogenase (IDH) mutation is
essential for effective glioma management. Traditional methods rely on invasive
tissue sampling, which may fail to capture a tumor's spatial heterogeneity.
While deep learning models have shown promise in molecular profiling, their
performance is often limited by scarce annotated data. In contrast, foundation
deep learning models offer a more generalizable approach for glioma imaging
biomarkers. We propose a Foundation-based Biomarker Network (FoundBioNet) that
utilizes a SWIN-UNETR-based architecture to noninvasively predict IDH mutation
status from multi-parametric MRI. Two key modules are incorporated: Tumor-Aware
Feature Encoding (TAFE) for extracting multi-scale, tumor-focused features, and
Cross-Modality Differential (CMD) for highlighting subtle T2-FLAIR mismatch
signals associated with IDH mutation. The model was trained and validated on a
diverse, multi-center cohort of 1705 glioma patients from six public datasets.
Our model achieved AUCs of 90.58%, 88.08%, 65.41%, and 80.31% on independent
test sets from EGD, TCGA, Ivy GAP, RHUH, and UPenn, consistently outperforming
baseline approaches (p <= 0.05). Ablation studies confirmed that both the TAFE
and CMD modules are essential for improving predictive accuracy. By integrating
large-scale pretraining and task-specific fine-tuning, FoundBioNet enables
generalizable glioma characterization. This approach enhances diagnostic
accuracy and interpretability, with the potential to enable more personalized
patient care.

</details>


### [41] [SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding](https://arxiv.org/abs/2508.06763)
*Zihao Sheng,Zilin Huang,Yen-Jung Chen,Yansong Qu,Yuhao Luo,Yue Leng,Sikai Chen*

Main category: cs.CV

TL;DR: SafePLUG 是一个新型多模态大语言模型框架，旨在通过像素级理解和时间定位提升交通场景的细粒度分析能力。


<details>
  <summary>Details</summary>
Motivation: 现有 MLLMs 在交通事故理解中主要关注粗粒度图像或视频级理解，难以处理细粒度视觉细节或局部场景组件，限制了其在复杂事故场景中的应用。

Method: 提出 SafePLUG 框架，支持任意形状的视觉提示进行区域感知问答和基于语言指令的像素级分割，同时识别时间锚定事件。

Result: 实验表明，SafePLUG 在区域问答、像素级分割、时间事件定位和事故理解等任务中表现优异。

Conclusion: SafePLUG 为复杂交通场景的细粒度理解奠定了基础，有望提升智能交通系统的驾驶安全和情境感知能力。

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress
across a range of vision-language tasks and demonstrate strong potential for
traffic accident understanding. However, existing MLLMs in this domain
primarily focus on coarse-grained image-level or video-level comprehension and
often struggle to handle fine-grained visual details or localized scene
components, limiting their applicability in complex accident scenarios. To
address these limitations, we propose SafePLUG, a novel framework that empowers
MLLMs with both Pixel-Level Understanding and temporal Grounding for
comprehensive traffic accident analysis. SafePLUG supports both
arbitrary-shaped visual prompts for region-aware question answering and
pixel-level segmentation based on language instructions, while also enabling
the recognition of temporally anchored events in traffic accident scenarios. To
advance the development of MLLMs for traffic accident understanding, we curate
a new dataset containing multimodal question-answer pairs centered on diverse
accident scenarios, with detailed pixel-level annotations and temporal event
boundaries. Experimental results show that SafePLUG achieves strong performance
on multiple tasks, including region-based question answering, pixel-level
segmentation, temporal event localization, and accident event understanding.
These capabilities lay a foundation for fine-grained understanding of complex
traffic scenes, with the potential to improve driving safety and enhance
situational awareness in smart transportation systems. The code, dataset, and
model checkpoints will be made publicly available at:
https://zihaosheng.github.io/SafePLUG

</details>


### [42] [Edge Detection for Organ Boundaries via Top Down Refinement and SubPixel Upsampling](https://arxiv.org/abs/2508.06805)
*Aarav Mehta,Priya Deshmukh,Vikram Singh,Siddharth Malhotra,Krishnan Menon Iyer,Tanvi Iyer*

Main category: cs.CV

TL;DR: 提出一种针对医学图像的精确边缘检测方法，通过反向细化架构提升边界定位精度，显著改善医学图像任务。


<details>
  <summary>Details</summary>
Motivation: 医学图像中器官边界的精确定位对分割、配准和手术规划至关重要，现有卷积网络在定位精度上不足。

Method: 采用自上而下的反向细化架构，融合高层语义特征与低层细节，支持2D和体积图像，并处理各向异性体积。

Result: 在CT和MRI数据集上表现优于基线方法，边界定位精度显著提升，下游任务（如分割和配准）效果改善。

Conclusion: 该方法生成的精确边缘对医学图像任务具有临床价值，显著提升了边界定位和下游任务性能。

Abstract: Accurate localization of organ boundaries is critical in medical imaging for
segmentation, registration, surgical planning, and radiotherapy. While deep
convolutional networks (ConvNets) have advanced general-purpose edge detection
to near-human performance on natural images, their outputs often lack precise
localization, a limitation that is particularly harmful in medical applications
where millimeter-level accuracy is required. Building on a systematic analysis
of ConvNet edge outputs, we propose a medically focused crisp edge detector
that adapts a novel top-down backward refinement architecture to medical images
(2D and volumetric). Our method progressively upsamples and fuses high-level
semantic features with fine-grained low-level cues through a backward
refinement pathway, producing high-resolution, well-localized organ boundaries.
We further extend the design to handle anisotropic volumes by combining 2D
slice-wise refinement with light 3D context aggregation to retain computational
efficiency. Evaluations on several CT and MRI organ datasets demonstrate
substantially improved boundary localization under strict criteria (boundary
F-measure, Hausdorff distance) compared to baseline ConvNet detectors and
contemporary medical edge/contour methods. Importantly, integrating our crisp
edge maps into downstream pipelines yields consistent gains in organ
segmentation (higher Dice scores, lower boundary errors), more accurate image
registration, and improved delineation of lesions near organ interfaces. The
proposed approach produces clinically valuable, crisp organ edges that
materially enhance common medical-imaging tasks.

</details>


### [43] [DualResolution Residual Architecture with Artifact Suppression for Melanocytic Lesion Segmentation](https://arxiv.org/abs/2508.06816)
*Vikram Singh,Kabir Malhotra,Rohan Desai,Ananya Shankaracharya,Priyadarshini Chatterjee,Krishnan Menon Iyer*

Main category: cs.CV

TL;DR: 提出了一种新型双分辨率架构（Our method），用于精确分割皮肤镜图像中的黑色素瘤，结合全分辨率流和多尺度上下文流，并通过边界感知残差连接和通道注意力模块优化性能。


<details>
  <summary>Details</summary>
Motivation: 皮肤镜图像中黑色素瘤的精确分割对自动化癌症筛查和临床决策至关重要，但现有方法难以处理纹理、颜色变化和常见成像伪影。

Method: 采用ResNet启发的双分辨率架构，结合全分辨率流和多尺度池化流，通过边界感知残差连接和通道注意力模块增强性能，并引入轻量级伪影抑制块和多任务训练目标。

Result: 在公共皮肤镜基准测试中，该方法显著提升了边界贴合度和临床相关分割指标，优于标准编码器-解码器基线。

Conclusion: 该方法为自动化黑色素瘤评估系统提供了实用的解决方案，无需复杂的后处理或预训练协议。

Abstract: Accurate segmentation of melanocytic tumors in dermoscopic images is a
critical step for automated skin cancer screening and clinical decision
support. Unlike natural scene segmentation, lesion delineation must reconcile
subtle texture and color variations, frequent artifacts (hairs, rulers,
bubbles), and a strong need for precise boundary localization to support
downstream diagnosis. In this paper we introduce Our method, a novel ResNet
inspired dual resolution architecture specifically designed for melanocytic
tumor segmentation. Our method maintains a full resolution stream that
preserves fine grained boundary information while a complementary pooled stream
aggregates multi scale contextual cues for robust lesion recognition. The
streams are tightly coupled by boundary aware residual connections that inject
high frequency edge information into deep feature maps, and by a channel
attention module that adapts color and texture sensitivity to dermoscopic
appearance. To further address common imaging artifacts and the limited size of
clinical datasets, we propose a lightweight artifact suppression block and a
multi task training objective that combines a Dice Tversky segmentation loss
with an explicit boundary loss and a contrastive regularizer for feature
stability. The combined design yields pixel accurate masks without requiring
heavy post processing or complex pre training protocols. Extensive experiments
on public dermoscopic benchmarks demonstrate that Our method significantly
improves boundary adherence and clinically relevant segmentation metrics
compared to standard encoder decoder baselines, making it a practical building
block for automated melanoma assessment systems.

</details>


### [44] [VesselRW: Weakly Supervised Subcutaneous Vessel Segmentation via Learned Random Walk Propagation](https://arxiv.org/abs/2508.06819)
*Ayaan Nooruddin Siddiqui,Mahnoor Zaidi,Ayesha Nazneen Shahbaz,Priyadarshini Chatterjee,Krishnan Menon Iyer*

Main category: cs.CV

TL;DR: 提出了一种弱监督训练框架，用于皮下血管分割，利用稀疏标注生成密集监督，结合不确定性加权损失和拓扑感知正则化，显著减少标注负担并提升分割效果。


<details>
  <summary>Details</summary>
Motivation: 皮下血管分割面临标注稀缺、成本高以及图像对比度低、噪声多的问题，需要一种高效且低成本的方法。

Method: 通过可微随机游走标签传播模型将稀疏标注扩展为密集概率监督，结合图像驱动的血管特征和连续性先验，同时学习标签传播器和CNN分割器，并引入拓扑感知正则化。

Result: 在临床数据集上优于稀疏标签训练和传统伪标签方法，生成更完整的血管图并提供更可靠的校准不确定性。

Conclusion: 该方法显著降低了标注负担，同时保持了临床相关的血管拓扑结构，具有较高的实用价值。

Abstract: Accurate segmentation of subcutaneous vessels from clinical images is
hampered by scarce, expensive ground truth and by low contrast, noisy
appearance of vessels across patients and modalities. We present a novel weakly
supervised training framework tailored for subcutaneous vessel segmentation
that leverages inexpensive sparse annotations (e.g., centerline traces, dot
markers, or short scribbles). Sparse labels are expanded into dense,
probabilistic supervision via a differentiable random walk label propagation
model whose transition weights incorporate image driven vesselness cues and
tubular continuity priors. The propagation yields per-pixel hitting
probabilities together with calibrated uncertainty estimates; these are
incorporated into an uncertainty weighted loss to avoid over fitting to
ambiguous regions. Crucially, the label-propagator is learned jointly with a
CNN based segmentation predictor, enabling the system to discover vessel edges
and continuity constraints without explicit edge supervision. We further
introduce a topology aware regularizer that encourages centerline connectivity
and penalizes spurious branches, improving clinical usability. In experiments
on clinical subcutaneous imaging datasets, our method consistently outperforms
naive training on sparse labels and conventional dense pseudo-labeling,
producing more complete vascular maps and better calibrated uncertainty for
downstream decision making. The approach substantially reduces annotation
burden while preserving clinically relevant vessel topology.

</details>


### [45] [Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification](https://arxiv.org/abs/2508.06831)
*Taha Mustapha Nehdi,Nairouz Mrabah,Atif Belal,Marco Pedersoli,Eric Granger*

Main category: cs.CV

TL;DR: SAGE-reID是一种高效、无需源数据的多源域自适应方法，通过低秩适配器和轻量级门控网络实现跨域知识迁移，显著减少计算成本和内存消耗。


<details>
  <summary>Details</summary>
Motivation: 解决多源域自适应（MSDA）方法中训练参数和计算成本高的问题，同时保持高性能。

Method: 使用源无关的低秩适配器（LoRA）和轻量级门控网络动态融合适配器，实现高效知识迁移。

Result: 在Market-1501、DukeMTMC-reID和MSMT17等基准测试中表现优于现有方法，且计算效率高。

Conclusion: SAGE-reID是一种高效、轻量级的MSDA方法，适用于行人重识别任务。

Abstract: Adapting person re-identification (reID) models to new target environments
remains a challenging problem that is typically addressed using unsupervised
domain adaptation (UDA) methods. Recent works show that when labeled data
originates from several distinct sources (e.g., datasets and cameras),
considering each source separately and applying multi-source domain adaptation
(MSDA) typically yields higher accuracy and robustness compared to blending the
sources and performing conventional UDA. However, state-of-the-art MSDA methods
learn domain-specific backbone models or require access to source domain data
during adaptation, resulting in significant growth in training parameters and
computational cost. In this paper, a Source-free Adaptive Gated Experts
(SAGE-reID) method is introduced for person reID. Our SAGE-reID is a
cost-effective, source-free MSDA method that first trains individual
source-specific low-rank adapters (LoRA) through source-free UDA. Next, a
lightweight gating network is introduced and trained to dynamically assign
optimal merging weights for fusion of LoRA experts, enabling effective
cross-domain knowledge transfer. While the number of backbone parameters
remains constant across source domains, LoRA experts scale linearly but remain
negligible in size (<= 2% of the backbone), reducing both the memory
consumption and risk of overfitting. Extensive experiments conducted on three
challenging benchmarks: Market-1501, DukeMTMC-reID, and MSMT17 indicate that
SAGE-reID outperforms state-of-the-art methods while being computationally
efficient.

</details>


### [46] [Hybrid Machine Learning Framework for Predicting Geometric Deviations from 3D Surface Metrology](https://arxiv.org/abs/2508.06845)
*Hamidreza Samadi,Md Manjurul Ahsan,Shivakumar Raman*

Main category: cs.CV

TL;DR: 该研究提出了一种结合高分辨率3D扫描和混合机器学习框架的方法，用于预测制造组件的几何偏差，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 现代制造中保持复杂几何形状的尺寸精度仍然具有挑战性，需要更准确的预测方法。

Method: 使用高分辨率3D扫描仪采集多角度表面数据，通过精确对齐、降噪和合并技术生成3D模型，并结合卷积神经网络和梯度提升决策树进行预测建模。

Result: 预测精度达到0.012毫米（95%置信水平），比传统方法提升了73%，并揭示了制造参数与几何偏差之间的隐藏关联。

Conclusion: 该方法为自动化质量控制、预测性维护和设计优化提供了潜力，数据集也为未来研究奠定了基础。

Abstract: This study addresses the challenge of accurately forecasting geometric
deviations in manufactured components using advanced 3D surface analysis.
Despite progress in modern manufacturing, maintaining dimensional precision
remains difficult, particularly for complex geometries. We present a
methodology that employs a high-resolution 3D scanner to acquire multi-angle
surface data from 237 components produced across different batches. The data
were processed through precise alignment, noise reduction, and merging
techniques to generate accurate 3D representations. A hybrid machine learning
framework was developed, combining convolutional neural networks for feature
extraction with gradient-boosted decision trees for predictive modeling. The
proposed system achieved a prediction accuracy of 0.012 mm at a 95% confidence
level, representing a 73% improvement over conventional statistical process
control methods. In addition to improved accuracy, the model revealed hidden
correlations between manufacturing parameters and geometric deviations. This
approach offers significant potential for automated quality control, predictive
maintenance, and design optimization in precision manufacturing, and the
resulting dataset provides a strong foundation for future predictive modeling
research.

</details>


### [47] [AGIC: Attention-Guided Image Captioning to Improve Caption Relevance](https://arxiv.org/abs/2508.06853)
*L. D. M. S. Sai Teja,Ashok Urlana,Pruthwik Mishra*

Main category: cs.CV

TL;DR: AGIC通过注意力机制增强显著视觉区域，结合混合解码策略，在Flickr8k和Flickr30k数据集上表现优异，推理速度更快。


<details>
  <summary>Details</summary>
Motivation: 尽管图像描述生成取得进展，但生成准确且描述性强的标题仍具挑战性。

Method: 提出AGIC，通过特征空间增强显著视觉区域指导标题生成，并引入混合解码策略平衡流畅性与多样性。

Result: 在Flickr8k和Flickr30k数据集上，AGIC匹配或超越多个先进模型，推理速度更快，且在多指标下表现优异。

Conclusion: AGIC为图像描述生成提供了可扩展且可解释的解决方案。

Abstract: Despite significant progress in image captioning, generating accurate and
descriptive captions remains a long-standing challenge. In this study, we
propose Attention-Guided Image Captioning (AGIC), which amplifies salient
visual regions directly in the feature space to guide caption generation. We
further introduce a hybrid decoding strategy that combines deterministic and
probabilistic sampling to balance fluency and diversity. To evaluate AGIC, we
conduct extensive experiments on the Flickr8k and Flickr30k datasets. The
results show that AGIC matches or surpasses several state-of-the-art models
while achieving faster inference. Moreover, AGIC demonstrates strong
performance across multiple evaluation metrics, offering a scalable and
interpretable solution for image captioning.

</details>


### [48] [A Joint Sparse Self-Representation Learning Method for Multiview Clustering](https://arxiv.org/abs/2508.06857)
*Mengxue Jia,Zhihua Allen-Zhao,You Zhao,Sanyang Liu*

Main category: cs.CV

TL;DR: 提出了一种新的联合稀疏自表示学习模型用于多视图聚类，通过引入基数约束提取局部信息，并开发了全局收敛的交替二次惩罚方法。


<details>
  <summary>Details</summary>
Motivation: 多视图聚类需要利用一致和互补信息，现有方法通常使用图拉普拉斯正则化，而本文提出基数约束以提取更可靠的局部和全局结构信息。

Method: 引入基数（ℓ0范数）约束替代图拉普拉斯正则化，提取视图特定局部信息；低秩约束用于揭示全局一致结构；开发交替二次惩罚（AQP）方法解决非凸非光滑模型的收敛问题。

Result: 在六个标准数据集上验证了模型和AQP方法的优越性，优于八种先进算法。

Conclusion: 基数约束和AQP方法在多视图聚类中表现优异，提供了更可靠的局部和全局结构信息。

Abstract: Multiview clustering (MC) aims to group samples using consistent and
complementary information across various views. The subspace clustering, as a
fundamental technique of MC, has attracted significant attention. In this
paper, we propose a novel joint sparse self-representation learning model for
MC, where a featured difference is the extraction of view-specific local
information by introducing cardinality (i.e., $\ell_0$-norm) constraints
instead of Graph-Laplacian regularization. Specifically, under each view,
cardinality constraints directly restrict the samples used in the
self-representation stage to extract reliable local and global structure
information, while the low-rank constraint aids in revealing a global coherent
structure in the consensus affinity matrix during merging. The attendant
challenge is that Augmented Lagrange Method (ALM)-based alternating
minimization algorithms cannot guarantee convergence when applied directly to
our nonconvex, nonsmooth model, thus resulting in poor generalization ability.
To address it, we develop an alternating quadratic penalty (AQP) method with
global convergence, where two subproblems are iteratively solved by closed-form
solutions. Empirical results on six standard datasets demonstrate the
superiority of our model and AQP method, compared to eight state-of-the-art
algorithms.

</details>


### [49] [VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding](https://arxiv.org/abs/2508.06869)
*Jianxiang He,Shaoguang Wang,Weiyu Guo,Meisheng Hong,Jungang Li,Yijie Xu,Ziyang Chen,Hui Xiong*

Main category: cs.CV

TL;DR: VSI是一种多模态关键帧搜索方法，通过整合字幕、时间戳和场景边界，显著提升了长视频理解任务中的关键帧定位和视频问答准确性。


<details>
  <summary>Details</summary>
Motivation: 长视频理解因数据规模巨大而具有挑战性，现有方法因文本查询与视觉内容的多模态对齐不足而效果受限。

Method: 提出VSI方法，通过双流搜索机制（视频搜索流和字幕匹配流）整合视觉和文本信息，提升关键帧搜索精度。

Result: 在LongVideoBench上，VSI的关键帧定位准确率达40.00%，视频问答任务准确率达68.48%，显著优于基线方法。

Conclusion: VSI在长视频理解任务中表现出鲁棒性和泛化能力，达到SOTA性能。

Abstract: Long video understanding presents a significant challenge to multimodal large
language models (MLLMs) primarily due to the immense data scale. A critical and
widely adopted strategy for making this task computationally tractable is
keyframe retrieval, which seeks to identify a sparse set of video frames that
are most salient to a given textual query. However, the efficacy of this
approach is hindered by weak multimodal alignment between textual queries and
visual content and fails to capture the complex temporal semantic information
required for precise reasoning. To address this, we propose Visual-Subtitle
Integeration(VSI), a multimodal keyframe search method that integrates
subtitles, timestamps, and scene boundaries into a unified multimodal search
process. The proposed method captures the visual information of video frames as
well as the complementary textual information through a dual-stream search
mechanism by Video Search Stream as well as Subtitle Match Stream,
respectively, and improves the keyframe search accuracy through the interaction
of the two search streams. Experimental results show that VSI achieve 40.00%
key frame localization accuracy on the text-relevant subset of LongVideoBench
and 68.48% accuracy on downstream long Video-QA tasks, surpassing competitive
baselines by 20.35% and 15.79%, respectively. Furthermore, on the
LongVideoBench, VSI achieved state-of-the-art(SOTA) in medium-to-long video-QA
tasks, demonstrating the robustness and generalizability of the proposed
multimodal search strategy.

</details>


### [50] [NS-FPN: Improving Infrared Small Target Detection and Segmentation from Noise Suppression Perspective](https://arxiv.org/abs/2508.06878)
*Maoxun Yuan,Duanni Meng,Ziteng Xi,Tianyi Zhao,Shiji Zhao,Yimian Dai,Xingxing Wei*

Main category: cs.CV

TL;DR: 本文提出了一种新型噪声抑制特征金字塔网络（NS-FPN），通过低频引导特征净化（LFP）模块和螺旋感知特征采样（SFS）模块，从噪声抑制角度提升红外小目标检测与分割（IRSTDS）的性能。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测与分割任务因目标暗淡、形状模糊及背景干扰严重而极具挑战性。现有CNN方法虽能增强特征表示，但未能有效抑制噪声，导致误报率增加。

Method: 提出NS-FPN，包含LFP模块（通过净化高频成分抑制噪声）和SFS模块（通过螺旋采样融合目标相关特征），结构轻量且易于集成到现有框架。

Result: 在公开数据集上，NS-FPN显著降低误报率，性能优于现有方法。

Conclusion: 从频域角度出发的噪声抑制方法有效提升了IRSTDS任务的性能，NS-FPN具有轻量化和易集成的优势。

Abstract: Infrared small target detection and segmentation (IRSTDS) is a critical yet
challenging task in defense and civilian applications, owing to the dim,
shapeless appearance of targets and severe background clutter. Recent CNN-based
methods have achieved promising target perception results, but they only focus
on enhancing feature representation to offset the impact of noise, which
results in the increased false alarms problem. In this paper, through analyzing
the problem from the frequency domain, we pioneer in improving performance from
noise suppression perspective and propose a novel noise-suppression feature
pyramid network (NS-FPN), which integrates a low-frequency guided feature
purification (LFP) module and a spiral-aware feature sampling (SFS) module into
the original FPN structure. The LFP module suppresses the noise features by
purifying high-frequency components to achieve feature enhancement devoid of
noise interference, while the SFS module further adopts spiral sampling to fuse
target-relevant features in feature fusion process. Our NS-FPN is designed to
be lightweight yet effective and can be easily plugged into existing IRSTDS
frameworks. Extensive experiments on the public IRSTDS datasets demonstrate
that our method significantly reduces false alarms and achieves superior
performance on IRSTDS tasks.

</details>


### [51] [BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models](https://arxiv.org/abs/2508.06895)
*Jianting Tang,Yubo Wang,Haoyu Cao,Linli Xu*

Main category: cs.CV

TL;DR: 论文提出BASIC方法，通过直接监督视觉嵌入优化多模态大语言模型的视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法仅将视觉嵌入作为上下文线索，缺乏直接视觉监督，限制了视觉嵌入的精细对齐。

Method: BASIC利用LLM浅层中精炼的视觉嵌入作为监督，从嵌入方向和语义匹配两个角度优化初始视觉嵌入。

Result: BASIC显著提升了MLLMs在多个基准测试中的性能。

Conclusion: 直接视觉监督有效提升了视觉嵌入的对齐质量。

Abstract: Mainstream Multimodal Large Language Models (MLLMs) achieve visual
understanding by using a vision projector to bridge well-pretrained vision
encoders and large language models (LLMs). The inherent gap between visual and
textual modalities makes the embeddings from the vision projector critical for
visual comprehension. However, current alignment approaches treat visual
embeddings as contextual cues and merely apply auto-regressive supervision to
textual outputs, neglecting the necessity of introducing equivalent direct
visual supervision, which hinders the potential finer alignment of visual
embeddings. In this paper, based on our analysis of the refinement process of
visual embeddings in the LLM's shallow layers, we propose BASIC, a method that
utilizes refined visual embeddings within the LLM as supervision to directly
guide the projector in generating initial visual embeddings. Specifically, the
guidance is conducted from two perspectives: (i) optimizing embedding
directions by reducing angles between initial and supervisory embeddings in
semantic space; (ii) improving semantic matching by minimizing disparities
between the logit distributions of both visual embeddings. Without additional
supervisory models or artificial annotations, BASIC significantly improves the
performance of MLLMs across a wide range of benchmarks, demonstrating the
effectiveness of our introduced direct visual supervision.

</details>


### [52] [Advancements in Chinese font generation since deep learning era: A survey](https://arxiv.org/abs/2508.06900)
*Weiran Chen,Guiqian Zhu,Ying Li,Yi Ji,Chunping Liu*

Main category: cs.CV

TL;DR: 本文综述了基于深度学习的汉字字体生成方法，包括研究背景、文献选择方法、相关基础知识、现有方法分类（多参考样本和少参考样本生成）及其优缺点，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 汉字字体生成在设计和排版领域具有重要意义，但如何提高生成质量仍是一个难题。本文旨在通过综述现有方法，为该领域的研究者提供有价值的参考。

Method: 首先介绍研究背景和文献选择方法，然后回顾深度学习架构、字体表示格式、数据集和评估指标。接着将现有方法分为多参考样本和少参考样本生成两类，并详细分析其优缺点。

Result: 综述了多种汉字字体生成方法，总结了各类方法的优势和局限性，为研究者提供了全面的技术参考。

Conclusion: 本文总结了汉字字体生成领域的挑战和未来方向，期望为该领域的研究者提供启发。

Abstract: Chinese font generation aims to create a new Chinese font library based on
some reference samples. It is a topic of great concern to many font designers
and typographers. Over the past years, with the rapid development of deep
learning algorithms, various new techniques have achieved flourishing and
thriving progress. Nevertheless, how to improve the overall quality of
generated Chinese character images remains a tough issue. In this paper, we
conduct a holistic survey of the recent Chinese font generation approaches
based on deep learning. To be specific, we first illustrate the research
background of the task. Then, we outline our literature selection and analysis
methodology, and review a series of related fundamentals, including classical
deep learning architectures, font representation formats, public datasets, and
frequently-used evaluation metrics. After that, relying on the number of
reference samples required to generate a new font, we categorize the existing
methods into two major groups: many-shot font generation and few-shot font
generation methods. Within each category, representative approaches are
summarized, and their strengths and limitations are also discussed in detail.
Finally, we conclude our paper with the challenges and future directions, with
the expectation to provide some valuable illuminations for the researchers in
this field.

</details>


### [53] [eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos](https://arxiv.org/abs/2508.06902)
*Xuecheng Wu,Dingkang Yang,Danlei Huang,Xinyi Yin,Yifan Wang,Jia Zhang,Jiayu Nie,Liangyu Fu,Yang Liu,Junxiao Xue,Hadi Amirpour,Wei Zhou*

Main category: cs.CV

TL;DR: 论文介绍了eMotions数据集和AV-CANet网络，用于短视频情感分析，解决了多模态复杂性和情感表达不一致的挑战。


<details>
  <summary>Details</summary>
Motivation: 短视频（SVs）的多模态复杂性为情感分析带来新挑战，现有数据不足且情感表达不一致。

Method: 提出eMotions数据集（27,996个视频）和多阶段标注流程；设计AV-CANet网络，结合视频Transformer和局部-全局融合模块，使用EP-CE Loss优化。

Result: 在三个eMotions数据集和四个公共VEA数据集上验证了AV-CANet的有效性。

Conclusion: AV-CANet为短视频情感分析提供了有效解决方案，数据集和代码将开源。

Abstract: Short-form videos (SVs) have become a vital part of our online routine for
acquiring and sharing information. Their multimodal complexity poses new
challenges for video analysis, highlighting the need for video emotion analysis
(VEA) within the community. Given the limited availability of SVs emotion data,
we introduce eMotions, a large-scale dataset consisting of 27,996 videos with
full-scale annotations. To ensure quality and reduce subjective bias, we
emphasize better personnel allocation and propose a multi-stage annotation
procedure. Additionally, we provide the category-balanced and test-oriented
variants through targeted sampling to meet diverse needs. While there have been
significant studies on videos with clear emotional cues (e.g., facial
expressions), analyzing emotions in SVs remains a challenging task. The
challenge arises from the broader content diversity, which introduces more
distinct semantic gaps and complicates the representations learning of
emotion-related features. Furthermore, the prevalence of audio-visual
co-expressions in SVs leads to the local biases and collective information gaps
caused by the inconsistencies in emotional expressions. To tackle this, we
propose AV-CANet, an end-to-end audio-visual fusion network that leverages
video transformer to capture semantically relevant representations. We further
introduce the Local-Global Fusion Module designed to progressively capture the
correlations of audio-visual features. Besides, EP-CE Loss is constructed to
globally steer optimizations with tripolar penalties. Extensive experiments
across three eMotions-related datasets and four public VEA datasets demonstrate
the effectiveness of our proposed AV-CANet, while providing broad insights for
future research. Moreover, we conduct ablation studies to examine the critical
components of our method. Dataset and code will be made available at Github.

</details>


### [54] [A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation](https://arxiv.org/abs/2508.06904)
*Chao Yin,Jide Li,Xiaoqiang Li*

Main category: cs.CV

TL;DR: 提出了一种无需训练的实例感知提示框架（IAPF），通过多模态大语言模型和区域约束点提示生成精细实例掩码，显著提升了伪装目标分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有训练免费方法因语义级提示导致粗糙分割的问题，特别是在多离散伪装实例场景中。

Method: IAPF框架包括文本提示生成器、实例掩码生成器和自一致性投票，结合MLLM和Grounding DINO生成精细实例掩码。

Result: 在标准COS基准测试中，IAPF显著优于现有训练免费方法。

Conclusion: IAPF为伪装目标分割提供了一种高效且无需训练的解决方案，适用于复杂场景。

Abstract: Camouflaged Object Segmentation (COS) remains highly challenging due to the
intrinsic visual similarity between target objects and their surroundings.
While training-based COS methods achieve good performance, their performance
degrades rapidly with increased annotation sparsity. To circumvent this
limitation, recent studies have explored training-free COS methods, leveraging
the Segment Anything Model (SAM) by automatically generating visual prompts
from a single task-generic prompt (\textit{e.g.}, "\textit{camouflaged
animal}") uniformly applied across all test images. However, these methods
typically produce only semantic-level visual prompts, causing SAM to output
coarse semantic masks and thus failing to handle scenarios with multiple
discrete camouflaged instances effectively. To address this critical
limitation, we propose a simple yet powerful \textbf{I}nstance-\textbf{A}ware
\textbf{P}rompting \textbf{F}ramework (IAPF), the first training-free COS
pipeline that explicitly converts a task-generic prompt into fine-grained
instance masks. Specifically, the IAPF comprises three steps: (1) Text Prompt
Generator, utilizing task-generic queries to prompt a Multimodal Large Language
Model (MLLM) for generating image-specific foreground and background tags; (2)
\textbf{Instance Mask Generator}, leveraging Grounding DINO to produce precise
instance-level bounding box prompts, alongside the proposed Single-Foreground
Multi-Background Prompting strategy to sample region-constrained point prompts
within each box, enabling SAM to yield a candidate instance mask; (3)
Self-consistency Instance Mask Voting, which selects the final COS prediction
by identifying the candidate mask most consistent across multiple candidate
instance masks. Extensive evaluations on standard COS benchmarks demonstrate
that the proposed IAPF significantly surpasses existing state-of-the-art
training-free COS methods.

</details>


### [55] [MultiRef: Controllable Image Generation with Multiple Visual References](https://arxiv.org/abs/2508.06905)
*Ruoxi Chen,Dongping Chen,Siyuan Wu,Sinan Wang,Shiyun Lang,Petr Sushko,Gaoyang Jiang,Yao Wan,Ranjay Krishna*

Main category: cs.CV

TL;DR: 论文提出MultiRef-bench评估框架，用于多视觉参考图像生成任务，发现现有模型在多参考条件下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成框架主要依赖单源输入，无法满足设计师从多视觉参考中获取灵感的需求。

Method: 引入MultiRef-bench评估框架和RefBlend数据引擎，构建包含38k高质量图像的MultiRef数据集。

Result: 实验表明，即使最先进的模型（如OmniGen）在多参考条件下表现有限，合成样本和真实样本分别仅达66.6%和79.0%。

Conclusion: 研究为开发更灵活、类人的创意工具提供了方向，数据集已公开。

Abstract: Visual designers naturally draw inspiration from multiple visual references,
combining diverse elements and aesthetic principles to create artwork. However,
current image generative frameworks predominantly rely on single-source inputs
-- either text prompts or individual reference images. In this paper, we focus
on the task of controllable image generation using multiple visual references.
We introduce MultiRef-bench, a rigorous evaluation framework comprising 990
synthetic and 1,000 real-world samples that require incorporating visual
content from multiple reference images. The synthetic samples are synthetically
generated through our data engine RefBlend, with 10 reference types and 33
reference combinations. Based on RefBlend, we further construct a dataset
MultiRef containing 38k high-quality images to facilitate further research. Our
experiments across three interleaved image-text models (i.e., OmniGen, ACE, and
Show-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that
even state-of-the-art systems struggle with multi-reference conditioning, with
the best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in
real-world cases on average compared to the golden answer. These findings
provide valuable directions for developing more flexible and human-like
creative tools that can effectively integrate multiple sources of visual
inspiration. The dataset is publicly available at: https://multiref.github.io/.

</details>


### [56] [MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification](https://arxiv.org/abs/2508.06908)
*Jinhao Li,Zijian Chen,Lirong Deng,Changbo Wang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 论文提出MMReID-Bench，首个多任务多模态行人重识别基准，利用多模态大语言模型（MLLMs）解决传统模型在多模态数据中泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统行人重识别模型在多模态数据（如RGB、热成像、红外、草图等）中泛化能力不足，而现有方法未能充分利用MLLMs的推理和跨模态理解能力。

Method: 引入MMReID-Bench基准，包含20,710个多模态查询和库图像，覆盖10种行人重识别任务，验证MLLMs的有效性和多样性。

Result: 实验证明MLLMs在行人重识别中表现优异，但在处理热成像和红外数据时存在局限性。

Conclusion: MMReID-Bench有望推动开发更鲁棒、通用的多模态基础模型用于行人重识别。

Abstract: Person re-identification (ReID) aims to retrieve the images of an interested
person in the gallery images, with wide applications in medical rehabilitation,
abnormal behavior detection, and public security. However, traditional person
ReID models suffer from uni-modal capability, leading to poor generalization
ability in multi-modal data, such as RGB, thermal, infrared, sketch images,
textual descriptions, etc. Recently, the emergence of multi-modal large
language models (MLLMs) shows a promising avenue for addressing this problem.
Despite this potential, existing methods merely regard MLLMs as feature
extractors or caption generators, which do not fully unleash their reasoning,
instruction-following, and cross-modal understanding capabilities. To bridge
this gap, we introduce MMReID-Bench, the first multi-task multi-modal benchmark
specifically designed for person ReID. The MMReID-Bench includes 20,710
multi-modal queries and gallery images covering 10 different person ReID tasks.
Comprehensive experiments demonstrate the remarkable capabilities of MLLMs in
delivering effective and versatile person ReID. Nevertheless, they also have
limitations in handling a few modalities, particularly thermal and infrared
data. We hope MMReID-Bench can facilitate the community to develop more robust
and generalizable multimodal foundation models for person ReID.

</details>


### [57] [Talk2Image: A Multi-Agent System for Multi-Turn Image Generation and Editing](https://arxiv.org/abs/2508.06916)
*Shichao Ma,Yunhe Guo,Jiahao Su,Qihe Huang,Zhengyang Zhou,Yang Wang*

Main category: cs.CV

TL;DR: Talk2Image是一个多代理系统，用于多轮对话场景中的交互式图像生成和编辑，解决了单代理系统的意图漂移和不连贯编辑问题。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成任务主要关注单轮场景，难以处理迭代式、多轮创意任务，而现有的对话系统又存在意图漂移和不连贯编辑的问题。

Method: Talk2Image通过三个关键组件实现：从对话历史中解析意图、任务分解与协作执行、基于多视角评估机制的反馈驱动优化。

Result: 实验表明，Talk2Image在可控性、连贯性和用户满意度方面优于现有基线。

Conclusion: Talk2Image能够逐步对齐用户意图并实现一致的图像编辑，为多轮交互式图像生成和编辑提供了有效解决方案。

Abstract: Text-to-image generation tasks have driven remarkable advances in diverse
media applications, yet most focus on single-turn scenarios and struggle with
iterative, multi-turn creative tasks. Recent dialogue-based systems attempt to
bridge this gap, but their single-agent, sequential paradigm often causes
intention drift and incoherent edits. To address these limitations, we present
Talk2Image, a novel multi-agent system for interactive image generation and
editing in multi-turn dialogue scenarios. Our approach integrates three key
components: intention parsing from dialogue history, task decomposition and
collaborative execution across specialized agents, and feedback-driven
refinement based on a multi-view evaluation mechanism. Talk2Image enables
step-by-step alignment with user intention and consistent image editing.
Experiments demonstrate that Talk2Image outperforms existing baselines in
controllability, coherence, and user satisfaction across iterative image
generation and editing tasks.

</details>


### [58] [AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning](https://arxiv.org/abs/2508.06924)
*Shihao Yuan,Yahui Liu,Yang Yue,Jingyuan Zhang,Wangmeng Zuo,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CV

TL;DR: AR-GRPO利用在线强化学习优化自回归图像生成模型，通过多维度奖励函数提升生成图像的质量和人类偏好。


<details>
  <summary>Details</summary>
Motivation: 受强化学习在大型语言模型中的成功启发，探索其在自回归图像生成模型中的应用潜力。

Method: 采用Group Relative Policy Optimization (GRPO)算法，设计多维度奖励函数评估图像质量。

Result: 在类别和文本条件图像生成任务中，图像质量和人类偏好显著提升。

Conclusion: RL优化为自回归图像生成提供了新方向，支持可控高质量图像合成。

Abstract: Inspired by the success of reinforcement learning (RL) in refining large
language models (LLMs), we propose AR-GRPO, an approach to integrate online RL
training into autoregressive (AR) image generation models. We adapt the Group
Relative Policy Optimization (GRPO) algorithm to refine the vanilla
autoregressive models' outputs by carefully designed reward functions that
evaluate generated images across multiple quality dimensions, including
perceptual quality, realism, and semantic fidelity. We conduct comprehensive
experiments on both class-conditional (i.e., class-to-image) and
text-conditional (i.e., text-to-image) image generation tasks, demonstrating
that our RL-enhanced framework significantly improves both the image quality
and human preference of generated images compared to the standard AR baselines.
Our results show consistent improvements across various evaluation metrics,
establishing the viability of RL-based optimization for AR image generation and
opening new avenues for controllable and high-quality image synthesis. The
source codes and models are available at:
https://github.com/Kwai-Klear/AR-GRPO.

</details>


### [59] [CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing](https://arxiv.org/abs/2508.06937)
*Weiyan Xie,Han Gao,Didan Deng,Kaican Li,April Hua Liu,Yongxiang Huang,Nevin L. Zhang*

Main category: cs.CV

TL;DR: CannyEdit是一种无需训练的图像编辑框架，通过选择性Canny控制和双提示引导，解决了现有方法在文本遵循、上下文保真和编辑无缝性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在区域图像编辑中难以平衡文本遵循、上下文保真和编辑的无缝性。

Method: 采用选择性Canny控制（保留未编辑区域的细节）和双提示引导（结合局部和全局提示）。

Result: 在真实图像编辑任务中，CannyEdit在文本遵循和上下文保真上提升了2.93%至10.49%，且用户难以识别编辑痕迹。

Conclusion: CannyEdit通过创新方法显著提升了图像编辑的质量和自然度。

Abstract: Recent advances in text-to-image (T2I) models have enabled training-free
regional image editing by leveraging the generative priors of foundation
models. However, existing methods struggle to balance text adherence in edited
regions, context fidelity in unedited areas, and seamless integration of edits.
We introduce CannyEdit, a novel training-free framework that addresses these
challenges through two key innovations: (1) Selective Canny Control, which
masks the structural guidance of Canny ControlNet in user-specified editable
regions while strictly preserving details of the source images in unedited
areas via inversion-phase ControlNet information retention. This enables
precise, text-driven edits without compromising contextual integrity. (2)
Dual-Prompt Guidance, which combines local prompts for object-specific edits
with a global target prompt to maintain coherent scene interactions. On
real-world image editing tasks (addition, replacement, removal), CannyEdit
outperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent
improvement in the balance of text adherence and context fidelity. In terms of
editing seamlessness, user studies reveal only 49.2 percent of general users
and 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited
when paired with real images without edits, versus 76.08 to 89.09 percent for
competitor methods.

</details>


### [60] [Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification](https://arxiv.org/abs/2508.06959)
*Qin Xu,Lili Zhu,Xiaoxia Cheng,Bo Jiang*

Main category: cs.CV

TL;DR: 提出了一种名为SCOPE的新方法，通过动态增强空间域中的细节和语义特征，解决了频率域方法在细粒度视觉分类中的局限性。


<details>
  <summary>Details</summary>
Motivation: 频率域方法基于固定基函数，缺乏对图像内容的适应性和动态调整能力，无法满足不同图像的判别需求。

Method: SCOPE包含两个模块：Subtle Detail Extractor (SDE)动态增强浅层细节，Salient Semantic Refiner (SSR)学习高层语义特征，两者级联结合局部细节与全局语义。

Result: 在四个流行的细粒度图像分类基准测试中取得了新的最优性能。

Conclusion: SCOPE通过动态增强多尺度特征，突破了频率域方法的限制，显著提升了细粒度分类的灵活性。

Abstract: The crux of resolving fine-grained visual classification (FGVC) lies in
capturing discriminative and class-specific cues that correspond to subtle
visual characteristics. Recently, frequency decomposition/transform based
approaches have attracted considerable interests since its appearing
discriminative cue mining ability. However, the frequency-domain methods are
based on fixed basis functions, lacking adaptability to image content and
unable to dynamically adjust feature extraction according to the discriminative
requirements of different images. To address this, we propose a novel method
for FGVC, named Subtle-Cue Oriented Perception Engine (SCOPE), which adaptively
enhances the representational capability of low-level details and high-level
semantics in the spatial domain, breaking through the limitations of fixed
scales in the frequency domain and improving the flexibility of multi-scale
fusion. The core of SCOPE lies in two modules: the Subtle Detail Extractor
(SDE), which dynamically enhances subtle details such as edges and textures
from shallow features, and the Salient Semantic Refiner (SSR), which learns
semantically coherent and structure-aware refinement features from the
high-level features guided by the enhanced shallow features. The SDE and SSR
are cascaded stage-by-stage to progressively combine local details with global
semantics. Extensive experiments demonstrate that our method achieves new
state-of-the-art on four popular fine-grained image classification benchmarks.

</details>


### [61] [Adversarial Video Promotion Against Text-to-Video Retrieval](https://arxiv.org/abs/2508.06964)
*Qiwei Tian,Chenhao Lin,Zhengyu Zhao,Qian Li,Shuai Liu,Chao Shen*

Main category: cs.CV

TL;DR: 该论文提出了首个针对文本到视频检索（T2VR）的攻击方法ViPro，旨在通过对抗性攻击提升视频排名，并提出了模态细化（MoRe）方法以增强黑盒迁移性。实验表明ViPro在多种场景下显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法主要关注降低视频排名，而提升视频排名的攻击方法尚未充分研究。这种攻击可能带来更大的经济利益和传播（错误）信息的影响。

Method: 提出Video Promotion攻击（ViPro）和Modal Refinement（MoRe）方法，通过对抗性攻击和模态间精细交互提升视频排名。

Result: ViPro在白盒、灰盒和黑盒设置下分别平均超过基线方法30%、10%和4%。

Conclusion: 论文揭示了T2VR的潜在漏洞，分析了攻击的上下界，并提供了防御思路。代码将公开。

Abstract: Thanks to the development of cross-modal models, text-to-video retrieval
(T2VR) is advancing rapidly, but its robustness remains largely unexamined.
Existing attacks against T2VR are designed to push videos away from queries,
i.e., suppressing the ranks of videos, while the attacks that pull videos
towards selected queries, i.e., promoting the ranks of videos, remain largely
unexplored. These attacks can be more impactful as attackers may gain more
views/clicks for financial benefits and widespread (mis)information. To this
end, we pioneer the first attack against T2VR to promote videos adversarially,
dubbed the Video Promotion attack (ViPro). We further propose Modal Refinement
(MoRe) to capture the finer-grained, intricate interaction between visual and
textual modalities to enhance black-box transferability. Comprehensive
experiments cover 2 existing baselines, 3 leading T2VR models, 3 prevailing
datasets with over 10k videos, evaluated under 3 scenarios. All experiments are
conducted in a multi-target setting to reflect realistic scenarios where
attackers seek to promote the video regarding multiple queries simultaneously.
We also evaluated our attacks for defences and imperceptibility. Overall, ViPro
surpasses other baselines by over $30/10/4\%$ for white/grey/black-box settings
on average. Our work highlights an overlooked vulnerability, provides a
qualitative analysis on the upper/lower bound of our attacks, and offers
insights into potential counterplays. Code will be publicly available at
https://github.com/michaeltian108/ViPro.

</details>


### [62] [WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering](https://arxiv.org/abs/2508.06982)
*Yixin Zhu,Zuoliang Zhu,Miloš Hašan,Jian Yang,Jin Xie,Beibei Wang*

Main category: cs.CV

TL;DR: WeatherDiffusion是一个基于扩散模型的框架，用于自动驾驶场景中的正向和逆向渲染，支持天气和光照的编辑，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶场景中复杂天气和光照条件下的渲染任务具有挑战性，现有方法难以控制和缺乏鲁棒性。

Method: 提出WeatherDiffusion框架，结合文本引导的预测本征图和本征图感知注意力（MAA）技术，实现高质量逆向渲染。

Result: 在多个基准测试中优于现有方法，并在下游任务（如目标检测和图像分割）中表现出显著优势。

Conclusion: WeatherDiffusion为自动驾驶场景中的渲染任务提供了高效且可控的解决方案，具有实际应用价值。

Abstract: Forward and inverse rendering have emerged as key techniques for enabling
understanding and reconstruction in the context of autonomous driving (AD).
However, complex weather and illumination pose great challenges to this task.
The emergence of large diffusion models has shown promise in achieving
reasonable results through learning from 2D priors, but these models are
difficult to control and lack robustness. In this paper, we introduce
WeatherDiffusion, a diffusion-based framework for forward and inverse rendering
on AD scenes with various weather and lighting conditions. Our method enables
authentic estimation of material properties, scene geometry, and lighting, and
further supports controllable weather and illumination editing through the use
of predicted intrinsic maps guided by text descriptions. We observe that
different intrinsic maps should correspond to different regions of the original
image. Based on this observation, we propose Intrinsic map-aware attention
(MAA) to enable high-quality inverse rendering. Additionally, we introduce a
synthetic dataset (\ie WeatherSynthetic) and a real-world dataset (\ie
WeatherReal) for forward and inverse rendering on AD scenes with diverse
weather and lighting. Extensive experiments show that our WeatherDiffusion
outperforms state-of-the-art methods on several benchmarks. Moreover, our
method demonstrates significant value in downstream tasks for AD, enhancing the
robustness of object detection and image segmentation in challenging weather
scenarios.

</details>


### [63] [TADoc: Robust Time-Aware Document Image Dewarping](https://arxiv.org/abs/2508.06988)
*Fangmin Zhao,Weichao Zeng,Zhenhang Li,Dongbao Yang,Yu Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种动态建模文档图像去扭曲任务的方法，并设计了一个轻量级框架TADoc，同时引入新评价指标DLS。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂文档结构和高变形时效果不佳，作者认为去扭曲是一个渐进过程而非一步变换。

Method: 将任务重新建模为动态过程，设计TADoc框架，并提出DLS评价指标。

Result: 实验表明TADoc在多种文档类型和变形程度下表现优越。

Conclusion: 动态建模和TADoc框架显著提升了文档去扭曲的效果和鲁棒性。

Abstract: Flattening curved, wrinkled, and rotated document images captured by portable
photographing devices, termed document image dewarping, has become an
increasingly important task with the rise of digital economy and online
working. Although many methods have been proposed recently, they often struggle
to achieve satisfactory results when confronted with intricate document
structures and higher degrees of deformation in real-world scenarios. Our main
insight is that, unlike other document restoration tasks (e.g., deblurring),
dewarping in real physical scenes is a progressive motion rather than a
one-step transformation. Based on this, we have undertaken two key initiatives.
Firstly, we reformulate this task, modeling it for the first time as a dynamic
process that encompasses a series of intermediate states. Secondly, we design a
lightweight framework called TADoc (Time-Aware Document Dewarping Network) to
address the geometric distortion of document images. In addition, due to the
inadequacy of OCR metrics for document images containing sparse text, the
comprehensiveness of evaluation is insufficient. To address this shortcoming,
we propose a new metric -- DLS (Document Layout Similarity) -- to evaluate the
effectiveness of document dewarping in downstream tasks. Extensive experiments
and in-depth evaluations have been conducted and the results indicate that our
model possesses strong robustness, achieving superiority on several benchmarks
with different document types and degrees of distortion.

</details>


### [64] [OctreeNCA: Single-Pass 184 MP Segmentation on Consumer Hardware](https://arxiv.org/abs/2508.06993)
*Nick Lemke,John Kalkhof,Niklas Babendererde,Anirban Mukhopadhyay*

Main category: cs.CV

TL;DR: OctreeNCA提出了一种基于八叉树数据结构的轻量级神经细胞自动机（NCA），用于高效分割高分辨率医学图像和视频，显著降低VRAM消耗并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 医学图像和视频分割需要处理大尺寸输入，传统方法如UNet或Vision Transformers因VRAM消耗高而受限，导致全局一致性和推理速度下降。NCA虽轻量但缺乏全局知识。

Method: 通过八叉树数据结构扩展NCA的邻域定义，实现全局知识的高效传递，并开发CUDA实现的NCA推理函数以优化VRAM和速度。

Result: OctreeNCA在高分辨率图像和视频分割中占用VRAM比UNet少90%，可一次性处理184兆像素病理切片或1分钟手术视频。

Conclusion: OctreeNCA是一种高效、轻量的解决方案，适用于大尺寸医学图像和视频分割，显著优于传统方法。

Abstract: Medical applications demand segmentation of large inputs, like prostate MRIs,
pathology slices, or videos of surgery. These inputs should ideally be inferred
at once to provide the model with proper spatial or temporal context. When
segmenting large inputs, the VRAM consumption of the GPU becomes the
bottleneck. Architectures like UNets or Vision Transformers scale very poorly
in VRAM consumption, resulting in patch- or frame-wise approaches that
compromise global consistency and inference speed. The lightweight Neural
Cellular Automaton (NCA) is a bio-inspired model that is by construction
size-invariant. However, due to its local-only communication rules, it lacks
global knowledge. We propose OctreeNCA by generalizing the neighborhood
definition using an octree data structure. Our generalized neighborhood
definition enables the efficient traversal of global knowledge. Since deep
learning frameworks are mainly developed for large multi-layer networks, their
implementation does not fully leverage the advantages of NCAs. We implement an
NCA inference function in CUDA that further reduces VRAM demands and increases
inference speed. Our OctreeNCA segments high-resolution images and videos
quickly while occupying 90% less VRAM than a UNet during evaluation. This
allows us to segment 184 Megapixel pathology slices or 1-minute surgical videos
at once.

</details>


### [65] [S2-UniSeg: Fast Universal Agglomerative Pooling for Scalable Segment Anything without Supervision](https://arxiv.org/abs/2508.06995)
*Huihui Xu,Jin Ye,Hongqiu Wang,Changkai Ji,Jiashi Lin,Ming Hu,Ziyan Huang,Ying Chen,Chenglong Ma,Tianbin Li,Lihao Liu,Junjun He,Lei Zhu*

Main category: cs.CV

TL;DR: 提出了一种快速伪掩码生成算法UniAP，并基于此设计了自监督通用分割模型S2-UniSeg，通过连续预训练显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有自监督图像分割模型多阶段预训练耗时且优化不连续的问题。

Method: 使用UniAP快速生成多粒度伪掩码，结合QuerySD任务和师生框架进行连续预训练。

Result: 在多个基准测试中显著超越现有SOTA模型，如COCO上AP+6.9。

Conclusion: S2-UniSeg通过高效伪掩码生成和连续优化，实现了自监督分割的显著性能提升。

Abstract: Recent self-supervised image segmentation models have achieved promising
performance on semantic segmentation and class-agnostic instance segmentation.
However, their pretraining schedule is multi-stage, requiring a time-consuming
pseudo-masks generation process between each training epoch. This
time-consuming offline process not only makes it difficult to scale with
training dataset size, but also leads to sub-optimal solutions due to its
discontinuous optimization routine. To solve these, we first present a novel
pseudo-mask algorithm, Fast Universal Agglomerative Pooling (UniAP). Each layer
of UniAP can identify groups of similar nodes in parallel, allowing to generate
both semantic-level and instance-level and multi-granular pseudo-masks within
ens of milliseconds for one image. Based on the fast UniAP, we propose the
Scalable Self-Supervised Universal Segmentation (S2-UniSeg), which employs a
student and a momentum teacher for continuous pretraining. A novel
segmentation-oriented pretext task, Query-wise Self-Distillation (QuerySD), is
proposed to pretrain S2-UniSeg to learn the local-to-global correspondences.
Under the same setting, S2-UniSeg outperforms the SOTA UnSAM model, achieving
notable improvements of AP+6.9 on COCO, AR+11.1 on UVO, PixelAcc+4.5 on
COCOStuff-27, RQ+8.0 on Cityscapes. After scaling up to a larger 2M-image
subset of SA-1B, S2-UniSeg further achieves performance gains on all four
benchmarks. Our code and pretrained models are available at
https://github.com/bio-mlhui/S2-UniSeg

</details>


### [66] [TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders](https://arxiv.org/abs/2508.07020)
*Tanjim Bin Faruk,Abdul Matin,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.CV

TL;DR: TerraMAE是一种专为高光谱图像设计的自监督编码框架，通过自适应通道分组和增强重建损失函数，显著提升了空间-光谱信息的表示能力，并在多个地理空间任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 高光谱卫星图像具有数百个连续光谱波段，但现有自监督方法难以有效利用其复杂的空间-光谱相关性。

Method: 提出TerraMAE框架，采用自适应通道分组策略和增强重建损失函数，优化空间-光谱嵌入学习。

Result: TerraMAE在高保真图像重建中表现出色，并在作物识别、土地覆盖分类和土壤纹理预测等任务中验证了其有效性。

Conclusion: TerraMAE为高光谱图像分析提供了高效的自监督学习解决方案，具有广泛的应用潜力。

Abstract: Hyperspectral satellite imagery offers sub-30 m views of Earth in hundreds of
contiguous spectral bands, enabling fine-grained mapping of soils, crops, and
land cover. While self-supervised Masked Autoencoders excel on RGB and low-band
multispectral data, they struggle to exploit the intricate spatial-spectral
correlations in 200+ band hyperspectral images. We introduce TerraMAE, a novel
HSI encoding framework specifically designed to learn highly representative
spatial-spectral embeddings for diverse geospatial analyses. TerraMAE features
an adaptive channel grouping strategy, based on statistical reflectance
properties to capture spectral similarities, and an enhanced reconstruction
loss function that incorporates spatial and spectral quality metrics. We
demonstrate TerraMAE's effectiveness through superior spatial-spectral
information preservation in high-fidelity image reconstruction. Furthermore, we
validate its practical utility and the quality of its learned representations
through strong performance on three key downstream geospatial tasks: crop
identification, land cover classification, and soil texture prediction.

</details>


### [67] [DocRefine: An Intelligent Framework for Scientific Document Understanding and Content Optimization based on Multimodal Large Model Agents](https://arxiv.org/abs/2508.07021)
*Kun Qian,Wenjie Li,Tianyu Sun,Wenhong Wang,Wenhan Luo*

Main category: cs.CV

TL;DR: DocRefine是一个创新的框架，用于科学PDF文档的智能理解、内容优化和自动摘要，通过多代理系统实现高精度和视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 科学文献的快速增长需要高效工具，传统方法和现有大模型在处理复杂布局和多模态内容时存在不足。

Method: DocRefine采用多代理系统，包括布局分析、内容理解、指令分解、内容优化、摘要生成和一致性验证。

Result: 在DocEditBench数据集上，DocRefine在语义一致性、布局保真度和指令遵循率上均优于现有方法。

Conclusion: DocRefine在自动化科学文档处理中表现出色，显著提升了语义完整性和视觉一致性。

Abstract: The exponential growth of scientific literature in PDF format necessitates
advanced tools for efficient and accurate document understanding,
summarization, and content optimization. Traditional methods fall short in
handling complex layouts and multimodal content, while direct application of
Large Language Models (LLMs) and Vision-Language Large Models (LVLMs) lacks
precision and control for intricate editing tasks. This paper introduces
DocRefine, an innovative framework designed for intelligent understanding,
content refinement, and automated summarization of scientific PDF documents,
driven by natural language instructions. DocRefine leverages the power of
advanced LVLMs (e.g., GPT-4o) by orchestrating a sophisticated multi-agent
system comprising six specialized and collaborative agents: Layout & Structure
Analysis, Multimodal Content Understanding, Instruction Decomposition, Content
Refinement, Summarization & Generation, and Fidelity & Consistency
Verification. This closed-loop feedback architecture ensures high semantic
accuracy and visual fidelity. Evaluated on the comprehensive DocEditBench
dataset, DocRefine consistently outperforms state-of-the-art baselines across
various tasks, achieving overall scores of 86.7% for Semantic Consistency Score
(SCS), 93.9% for Layout Fidelity Index (LFI), and 85.0% for Instruction
Adherence Rate (IAR). These results demonstrate DocRefine's superior capability
in handling complex multimodal document editing, preserving semantic integrity,
and maintaining visual consistency, marking a significant advancement in
automated scientific document processing.

</details>


### [68] [MV-CoRe: Multimodal Visual-Conceptual Reasoning for Complex Visual Question Answering](https://arxiv.org/abs/2508.07023)
*Jingwei Peng,Jiehao Chen,Mateo Alejandro Rojas,Meilin Zhang*

Main category: cs.CV

TL;DR: MV-CoRe模型通过深度融合多模态视觉和语言信息，显著提升了复杂视觉问答任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型依赖高层全局特征，难以应对复杂视觉问答任务的多模态推理和外部知识整合需求。

Method: MV-CoRe结合预训练视觉和语言模型的全局嵌入与细粒度语义视觉特征（如物体检测和场景图表示），并通过多模态融合Transformer进行深度整合。

Result: 在GQA、A-OKVQA和OKVQA等基准测试中，MV-CoRe表现优于现有模型，GQA准确率达77.5%。

Conclusion: MV-CoRe通过多模态深度融合，显著提升了复杂视觉问答任务的性能，验证了其在深度视觉和概念理解方面的优势。

Abstract: Complex Visual Question Answering (Complex VQA) tasks, which demand
sophisticated multi-modal reasoning and external knowledge integration, present
significant challenges for existing large vision-language models (LVLMs) often
limited by their reliance on high-level global features. To address this, we
propose MV-CoRe (Multimodal Visual-Conceptual Reasoning), a novel model
designed to enhance Complex VQA performance through the deep fusion of diverse
visual and linguistic information. MV-CoRe meticulously integrates global
embeddings from pre-trained Vision Large Models (VLMs) and Language Large
Models (LLMs) with fine-grained semantic-aware visual features, including
object detection characteristics and scene graph representations. An innovative
Multimodal Fusion Transformer then processes and deeply integrates these
diverse feature sets, enabling rich cross-modal attention and facilitating
complex reasoning. We evaluate MV-CoRe on challenging Complex VQA benchmarks,
including GQA, A-OKVQA, and OKVQA, after training on VQAv2. Our experimental
results demonstrate that MV-CoRe consistently outperforms established LVLM
baselines, achieving an overall accuracy of 77.5% on GQA. Ablation studies
confirm the critical contribution of both object and scene graph features, and
human evaluations further validate MV-CoRe's superior factual correctness and
reasoning depth, underscoring its robust capabilities for deep visual and
conceptual understanding.

</details>


### [69] [Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation](https://arxiv.org/abs/2508.07028)
*Juntong Fan,Shuyi Fan,Debesh Jha,Changsheng Fang,Tieyong Zeng,Hengyong Yu,Dayang Wang*

Main category: cs.CV

TL;DR: FOCUS-Med提出了一种结合空间和结构图的双图卷积网络（Dual-GCN）模块，用于内窥镜图像中息肉的分割，通过注意力机制和全局上下文增强分割效果，并在公开基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像中息肉分割对早期结直肠癌检测至关重要，但低对比度、高光反射和模糊边界等问题使其具有挑战性。

Method: 提出FOCUS-Med，结合Dual-GCN模块捕捉空间和拓扑结构依赖，利用自注意力增强全局上下文，并引入加权快速归一化融合策略进行多尺度聚合。

Result: 在公开基准测试中，FOCUS-Med在五项关键指标上达到最先进性能。

Conclusion: FOCUS-Med展示了AI辅助结肠镜检查的临床潜力，其方法在息肉分割任务中表现出色。

Abstract: Accurate endoscopic image segmentation on the polyps is critical for early
colorectal cancer detection. However, this task remains challenging due to low
contrast with surrounding mucosa, specular highlights, and indistinct
boundaries. To address these challenges, we propose FOCUS-Med, which stands for
Fusion of spatial and structural graph with attentional context-aware polyp
segmentation in endoscopic medical imaging. FOCUS-Med integrates a Dual Graph
Convolutional Network (Dual-GCN) module to capture contextual spatial and
topological structural dependencies. This graph-based representation enables
the model to better distinguish polyps from background tissues by leveraging
topological cues and spatial connectivity, which are often obscured in raw
image intensities. It enhances the model's ability to preserve boundaries and
delineate complex shapes typical of polyps. In addition, a location-fused
stand-alone self-attention is employed to strengthen global context
integration. To bridge the semantic gap between encoder-decoder layers, we
incorporate a trainable weighted fast normalized fusion strategy for efficient
multi-scale aggregation. Notably, we are the first to introduce the use of a
Large Language Model (LLM) to provide detailed qualitative evaluations of
segmentation quality. Extensive experiments on public benchmarks demonstrate
that FOCUS-Med achieves state-of-the-art performance across five key metrics,
underscoring its effectiveness and clinical potential for AI-assisted
colonoscopy.

</details>


### [70] [TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree](https://arxiv.org/abs/2508.07083)
*Yueyu Hu,Ran Gong,Tingyu Fan,Yao Wang*

Main category: cs.CV

TL;DR: 提出了一种名为TeSO的新型3D表示方法，结合了八叉树结构和纹理贴图，提升了渲染质量并降低了比特率。


<details>
  <summary>Details</summary>
Motivation: 现有3D表示方法（如点云、网格和3D高斯）在渲染质量、表面定义和压缩性方面存在局限性，需要一种更高效的解决方案。

Method: 基于点云构建TeSO，将3D场景表示为八叉树组织的带纹理的surfel，并通过压缩方案高效编码几何和纹理。

Result: TeSO在较低比特率下实现了比现有基线方法更高的渲染质量。

Conclusion: TeSO是一种高效且高质量的3D表示方法，适用于3D流媒体应用。

Abstract: 3D visual content streaming is a key technology for emerging 3D telepresence
and AR/VR applications. One fundamental element underlying the technology is a
versatile 3D representation that is capable of producing high-quality renders
and can be efficiently compressed at the same time. Existing 3D representations
like point clouds, meshes and 3D Gaussians each have limitations in terms of
rendering quality, surface definition, and compressibility. In this paper, we
present the Textured Surfel Octree (TeSO), a novel 3D representation that is
built from point clouds but addresses the aforementioned limitations. It
represents a 3D scene as cube-bounded surfels organized on an octree, where
each surfel is further associated with a texture patch. By approximating a
smooth surface with a large surfel at a coarser level of the octree, it reduces
the number of primitives required to represent the 3D scene, and yet retains
the high-frequency texture details through the texture map attached to each
surfel. We further propose a compression scheme to encode the geometry and
texture efficiently, leveraging the octree structure. The proposed textured
surfel octree combined with the compression scheme achieves higher rendering
quality at lower bit-rates compared to multiple point cloud and 3D
Gaussian-based baselines.

</details>


### [71] [ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting](https://arxiv.org/abs/2508.07089)
*Sandro Papais,Letian Wang,Brian Cheong,Steven L. Waslander*

Main category: cs.CV

TL;DR: ForeSight是一种用于自动驾驶车辆3D感知的新型联合检测与预测框架，通过多任务流式学习和双向学习，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法将检测与预测作为独立任务，无法充分利用时间线索，限制了性能。

Method: 采用多任务流式学习和双向学习，结合检测与预测的查询记忆和信息传播，无需显式目标关联。

Result: 在nuScenes数据集上，EPA达到54.9%，优于之前方法9.3%，同时在mAP和minADE上表现最佳。

Conclusion: ForeSight通过联合检测与预测，实现了高效且性能优越的3D感知。

Abstract: We introduce ForeSight, a novel joint detection and forecasting framework for
vision-based 3D perception in autonomous vehicles. Traditional approaches treat
detection and forecasting as separate sequential tasks, limiting their ability
to leverage temporal cues. ForeSight addresses this limitation with a
multi-task streaming and bidirectional learning approach, allowing detection
and forecasting to share query memory and propagate information seamlessly. The
forecast-aware detection transformer enhances spatial reasoning by integrating
trajectory predictions from a multiple hypothesis forecast memory queue, while
the streaming forecast transformer improves temporal consistency using past
forecasts and refined detections. Unlike tracking-based methods, ForeSight
eliminates the need for explicit object association, reducing error propagation
with a tracking-free model that efficiently scales across multi-frame
sequences. Experiments on the nuScenes dataset show that ForeSight achieves
state-of-the-art performance, achieving an EPA of 54.9%, surpassing previous
methods by 9.3%, while also attaining the best mAP and minADE among multi-view
detection and forecasting models.

</details>


### [72] [Communication-Efficient Multi-Agent 3D Detection via Hybrid Collaboration](https://arxiv.org/abs/2508.07092)
*Yue Hu,Juntong Peng,Yunqiao Yang,Siheng Chen*

Main category: cs.CV

TL;DR: 论文提出了一种名为HyComm的协作3D检测系统，通过自适应整合两种通信消息（感知输出和原始观测）来优化检测性能与通信带宽的权衡。HyComm在多种场景下表现优异，显著降低了通信量。


<details>
  <summary>Details</summary>
Motivation: 解决协作3D检测中检测性能与通信带宽之间的瓶颈问题。

Method: 提出HyComm系统，自适应整合紧凑的感知输出和丰富的原始观测，并优先处理关键数据。

Result: 在DAIR-V2X和OPV2V数据集上，HyComm显著优于现有方法，通信量降低超过2006倍，同时保持更高的检测精度。

Conclusion: HyComm通过灵活的消息压缩和标准化数据格式，实现了高效的协作3D检测，适用于不同代理配置。

Abstract: Collaborative 3D detection can substantially boost detection performance by
allowing agents to exchange complementary information. It inherently results in
a fundamental trade-off between detection performance and communication
bandwidth. To tackle this bottleneck issue, we propose a novel hybrid
collaboration that adaptively integrates two types of communication messages:
perceptual outputs, which are compact, and raw observations, which offer richer
information. This approach focuses on two key aspects: i) integrating
complementary information from two message types and ii) prioritizing the most
critical data within each type. By adaptively selecting the most critical set
of messages, it ensures optimal perceptual information and adaptability,
effectively meeting the demands of diverse communication scenarios.Building on
this hybrid collaboration, we present \texttt{HyComm}, a
communication-efficient LiDAR-based collaborative 3D detection system.
\texttt{HyComm} boasts two main benefits: i) it facilitates adaptable
compression rates for messages, addressing various communication requirements,
and ii) it uses standardized data formats for messages. This ensures they are
independent of specific detection models, fostering adaptability across
different agent configurations. To evaluate HyComm, we conduct experiments on
both real-world and simulation datasets: DAIR-V2X and OPV2V. HyComm
consistently outperforms previous methods and achieves a superior
performance-bandwidth trade-off regardless of whether agents use the same or
varied detection models. It achieves a lower communication volume of more than
2,006$\times$ and still outperforms Where2comm on DAIR-V2X in terms of AP50.
The related code will be released.

</details>


### [73] [AugLift: Boosting Generalization in Lifting-based 3D Human Pose Estimation](https://arxiv.org/abs/2508.07112)
*Nikolai Warner,Wenjin Zhang,Irfan Essa,Apaar Sadhwani*

Main category: cs.CV

TL;DR: AugLift通过增强2D关键点输入（加入置信度分数和深度估计），显著提升了3D人体姿态估计的泛化性能，无需额外数据或传感器。


<details>
  <summary>Details</summary>
Motivation: 现有基于提升的方法在跨数据集和真实场景中泛化能力不足，AugLift旨在通过简单改进提升泛化性能。

Method: 在标准2D关键点坐标基础上，稀疏地加入置信度分数和深度估计，利用预训练模型计算这些信号。

Result: 在四个数据集上，AugLift平均提升跨数据集性能10.1%，同时提升同分布性能4.0%。

Conclusion: AugLift是一种模块化改进方法，能显著提升任何基于提升的姿态估计模型的泛化能力。

Abstract: Lifting-based methods for 3D Human Pose Estimation (HPE), which predict 3D
poses from detected 2D keypoints, often generalize poorly to new datasets and
real-world settings. To address this, we propose \emph{AugLift}, a simple yet
effective reformulation of the standard lifting pipeline that significantly
improves generalization performance without requiring additional data
collection or sensors. AugLift sparsely enriches the standard input -- the 2D
keypoint coordinates $(x, y)$ -- by augmenting it with a keypoint detection
confidence score $c$ and a corresponding depth estimate $d$. These additional
signals are computed from the image using off-the-shelf, pre-trained models
(e.g., for monocular depth estimation), thereby inheriting their strong
generalization capabilities. Importantly, AugLift serves as a modular add-on
and can be readily integrated into existing lifting architectures.
  Our extensive experiments across four datasets demonstrate that AugLift
boosts cross-dataset performance on unseen datasets by an average of $10.1\%$,
while also improving in-distribution performance by $4.0\%$. These gains are
consistent across various lifting architectures, highlighting the robustness of
our method. Our analysis suggests that these sparse, keypoint-aligned cues
provide robust frame-level context, offering a practical way to significantly
improve the generalization of any lifting-based pose estimation model. Code
will be made publicly available.

</details>


### [74] [Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays](https://arxiv.org/abs/2508.07128)
*Gregory Schuit,Denis Parra,Cecilia Besa*

Main category: cs.CV

TL;DR: 研究评估了GAN和扩散模型在生成胸部X光片时的效果，发现扩散模型整体更真实，但GAN在某些条件下更准确。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像数据稀缺问题，尤其是低发病率异常，提升AI诊断工具的性能。

Method: 使用MIMIC-CXR数据集和生成图像进行读者研究，由三位放射科医生评估真实与合成图像的区分及特征一致性。

Result: 扩散模型生成图像更真实，GAN在特定条件下（如无ECS）更准确。

Conclusion: GAN和扩散模型各有优势，需进一步优化以可靠增强AI诊断系统的训练数据。

Abstract: Generative image models have achieved remarkable progress in both natural and
medical imaging. In the medical context, these techniques offer a potential
solution to data scarcity-especially for low-prevalence anomalies that impair
the performance of AI-driven diagnostic and segmentation tools. However,
questions remain regarding the fidelity and clinical utility of synthetic
images, since poor generation quality can undermine model generalizability and
trust. In this study, we evaluate the effectiveness of state-of-the-art
generative models-Generative Adversarial Networks (GANs) and Diffusion Models
(DMs)-for synthesizing chest X-rays conditioned on four abnormalities:
Atelectasis (AT), Lung Opacity (LO), Pleural Effusion (PE), and Enlarged
Cardiac Silhouette (ECS). Using a benchmark composed of real images from the
MIMIC-CXR dataset and synthetic images from both GANs and DMs, we conducted a
reader study with three radiologists of varied experience. Participants were
asked to distinguish real from synthetic images and assess the consistency
between visual features and the target abnormality. Our results show that while
DMs generate more visually realistic images overall, GANs can report better
accuracy for specific conditions, such as absence of ECS. We further identify
visual cues radiologists use to detect synthetic images, offering insights into
the perceptual gaps in current models. These findings underscore the
complementary strengths of GANs and DMs and point to the need for further
refinement to ensure generative models can reliably augment training datasets
for AI diagnostic systems.

</details>


### [75] [CMAMRNet: A Contextual Mask-Aware Network Enhancing Mural Restoration Through Comprehensive Mask Guidance](https://arxiv.org/abs/2508.07140)
*Yingtie Lei,Fanghai Yi,Yihang Dong,Weihuang Liu,Xiaofeng Zhang,Zimeng Li,Chi-Man Pun,Xuhang Chen*

Main category: cs.CV

TL;DR: CMAMRNet提出了一种针对壁画数字修复的新方法，通过多尺度特征提取和掩码感知技术，解决了现有方法在修复质量和艺术真实性上的不足。


<details>
  <summary>Details</summary>
Motivation: 壁画作为文化遗产，面临环境和人为因素的持续破坏，现有修复方法在保持艺术真实性和修复质量上存在不足。

Method: 提出了CMAMRNet网络，包含掩码感知上下采样器（MAUDS）和共特征聚合器（CFA），通过多尺度特征提取和掩码引导提升修复效果。

Result: 实验表明，CMAMRNet在基准数据集上优于现有方法，能有效保持壁画的结构完整性和艺术细节。

Conclusion: CMAMRNet通过创新的掩码感知和多尺度特征提取技术，显著提升了壁画数字修复的质量和艺术真实性。

Abstract: Murals, as invaluable cultural artifacts, face continuous deterioration from
environmental factors and human activities. Digital restoration of murals faces
unique challenges due to their complex degradation patterns and the critical
need to preserve artistic authenticity. Existing learning-based methods
struggle with maintaining consistent mask guidance throughout their networks,
leading to insufficient focus on damaged regions and compromised restoration
quality. We propose CMAMRNet, a Contextual Mask-Aware Mural Restoration Network
that addresses these limitations through comprehensive mask guidance and
multi-scale feature extraction. Our framework introduces two key components:
(1) the Mask-Aware Up/Down-Sampler (MAUDS), which ensures consistent mask
sensitivity across resolution scales through dedicated channel-wise feature
selection and mask-guided feature fusion; and (2) the Co-Feature Aggregator
(CFA), operating at both the highest and lowest resolutions to extract
complementary features for capturing fine textures and global structures in
degraded regions. Experimental results on benchmark datasets demonstrate that
CMAMRNet outperforms state-of-the-art methods, effectively preserving both
structural integrity and artistic details in restored murals. The code is
available
at~\href{https://github.com/CXH-Research/CMAMRNet}{https://github.com/CXH-Research/CMAMRNet}.

</details>


### [76] [Dynamic Pattern Alignment Learning for Pretraining Lightweight Human-Centric Vision Models](https://arxiv.org/abs/2508.07144)
*Xuanhan Wang,Huimin Deng,Ke Liu,Jun Wang,Lianli Gao,Jingkuan Song*

Main category: cs.CV

TL;DR: 提出了一种名为DPAL的蒸馏预训练框架，通过动态模式对齐学习，使轻量级HVM从大型HVM中学习典型视觉模式，实现强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 大型HVM依赖大规模预训练数据和复杂架构，限制了实际应用。DPAL旨在通过蒸馏方法提升轻量级HVM的泛化能力。

Method: 设计了动态模式解码器（D-PaDe）和三层次对齐目标，分别从全局、局部和实例关系层面对齐轻量级与大型HVM。

Result: 在15个数据集上验证了DPAL的有效性，轻量级模型（5M参数）性能接近大型HVM（84M/307M），并显著优于其他蒸馏方法。

Conclusion: DPAL通过动态模式对齐学习，成功提升了轻量级HVM的泛化能力，适用于多种人本视觉任务。

Abstract: Human-centric vision models (HVMs) have achieved remarkable generalization
due to large-scale pretraining on massive person images. However, their
dependence on large neural architectures and the restricted accessibility of
pretraining data significantly limits their practicality in real-world
applications. To address this limitation, we propose Dynamic Pattern Alignment
Learning (DPAL), a novel distillation-based pretraining framework that
efficiently trains lightweight HVMs to acquire strong generalization from large
HVMs. In particular, human-centric visual perception are highly dependent on
three typical visual patterns, including global identity pattern, local shape
pattern and multi-person interaction pattern. To achieve generalizable
lightweight HVMs, we firstly design a dynamic pattern decoder (D-PaDe), acting
as a dynamic Mixture of Expert (MoE) model. It incorporates three specialized
experts dedicated to adaptively extract typical visual patterns, conditioned on
both input image and pattern queries. And then, we present three levels of
alignment objectives, which aims to minimize generalization gap between
lightweight HVMs and large HVMs at global image level, local pixel level, and
instance relation level. With these two deliberate designs, the DPAL
effectively guides lightweight model to learn all typical human visual patterns
from large HVMs, which can generalize to various human-centric vision tasks.
Extensive experiments conducted on 15 challenging datasets demonstrate the
effectiveness of the DPAL. Remarkably, when employing PATH-B as the teacher,
DPAL-ViT/Ti (5M parameters) achieves surprising generalizability similar to
existing large HVMs such as PATH-B (84M) and Sapiens-L (307M), and outperforms
previous distillation-based pretraining methods including Proteus-ViT/Ti (5M)
and TinyMiM-ViT/Ti (5M) by a large margin.

</details>


### [77] [Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2508.07146)
*Yu Liu,Zhijie Liu,Xiao Ren,You-Fu Li,He Kong*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的行人轨迹预测框架，结合短期和长期运动意图，提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型缺乏对行人意图的显式语义建模，可能导致行为误解和预测精度下降。

Method: 使用残差极坐标表示短期意图，学习型端点预测器建模长期意图，并结合自适应引导和残差噪声预测器优化扩散过程。

Result: 在ETH、UCY和SDD基准测试中表现出色，优于现有方法。

Conclusion: 结合意图建模的扩散框架显著提升了行人轨迹预测的准确性和鲁棒性。

Abstract: Predicting pedestrian motion trajectories is critical for the path planning
and motion control of autonomous vehicles. Recent diffusion-based models have
shown promising results in capturing the inherent stochasticity of pedestrian
behavior for trajectory prediction. However, the absence of explicit semantic
modelling of pedestrian intent in many diffusion-based methods may result in
misinterpreted behaviors and reduced prediction accuracy. To address the above
challenges, we propose a diffusion-based pedestrian trajectory prediction
framework that incorporates both short-term and long-term motion intentions.
Short-term intent is modelled using a residual polar representation, which
decouples direction and magnitude to capture fine-grained local motion
patterns. Long-term intent is estimated through a learnable, token-based
endpoint predictor that generates multiple candidate goals with associated
probabilities, enabling multimodal and context-aware intention modelling.
Furthermore, we enhance the diffusion process by incorporating adaptive
guidance and a residual noise predictor that dynamically refines denoising
accuracy. The proposed framework is evaluated on the widely used ETH, UCY, and
SDD benchmarks, demonstrating competitive results against state-of-the-art
methods.

</details>


### [78] [SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.07149)
*Ruolin Yang,Da Li,Honggang Zhang,Yi-Zhe Song*

Main category: cs.CV

TL;DR: 提出了一种名为SketchAnimator的模型，通过三个阶段为静态草图添加动态效果，使其能够模仿参考视频的动作。


<details>
  <summary>Details</summary>
Motivation: 草图动画通常需要专业技能且耗时，业余用户难以完成。本文旨在简化这一过程。

Method: 分为三个阶段：外观学习、动作学习和视频先验蒸馏，利用LoRA和SDS技术整合草图与参考视频的动态信息。

Result: 模型生成的草图视频保留了原始外观并模仿了参考视频的动作，优于其他方法。

Conclusion: SketchAnimator在一次性动作定制任务中表现优异，为草图动画提供了高效解决方案。

Abstract: Sketching is a uniquely human tool for expressing ideas and creativity. The
animation of sketches infuses life into these static drawings, opening a new
dimension for designers. Animating sketches is a time-consuming process that
demands professional skills and extensive experience, often proving daunting
for amateurs. In this paper, we propose a novel sketch animation model
SketchAnimator, which enables adding creative motion to a given sketch, like "a
jumping car''. Namely, given an input sketch and a reference video, we divide
the sketch animation into three stages: Appearance Learning, Motion Learning
and Video Prior Distillation. In stages 1 and 2, we utilize LoRA to integrate
sketch appearance information and motion dynamics from the reference video into
the pre-trained T2V model. In the third stage, we utilize Score Distillation
Sampling (SDS) to update the parameters of the Bezier curves in each sketch
frame according to the acquired motion information. Consequently, our model
produces a sketch video that not only retains the original appearance of the
sketch but also mirrors the dynamic movements of the reference video. We
compare our method with alternative approaches and demonstrate that it
generates the desired sketch video under the challenge of one-shot motion
customization.

</details>


### [79] [CoopDiff: Anticipating 3D Human-object Interactions via Contact-consistent Decoupled Diffusion](https://arxiv.org/abs/2508.07162)
*Xiaotong Lin,Tianming Liang,Jian-Fang Hu,Kun-Yu Lin,Yulei Kang,Chunwei Tian,Jianhuang Lai,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种名为CoopDiff的解耦扩散框架，通过分离建模人体和物体的运动，利用接触点作为共享锚点，提升了3D人-物交互预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常忽略人体和物体运动的本质差异，统一建模导致预测效果不佳。本文旨在通过解耦建模和接触点一致性约束，提升预测的连贯性和准确性。

Method: 采用双分支结构：人体动力学分支预测结构化运动，物体动力学分支处理刚体运动。通过共享接触点和一致性约束桥接两分支，并引入人驱交互模块增强一致性。

Result: 在BEHAVE和Human-object Interaction数据集上，CoopDiff优于现有方法。

Conclusion: 解耦建模和接触点一致性约束显著提升了3D人-物交互预测的性能。

Abstract: 3D human-object interaction (HOI) anticipation aims to predict the future
motion of humans and their manipulated objects, conditioned on the historical
context. Generally, the articulated humans and rigid objects exhibit different
motion patterns, due to their distinct intrinsic physical properties. However,
this distinction is ignored by most of the existing works, which intend to
capture the dynamics of both humans and objects within a single prediction
model. In this work, we propose a novel contact-consistent decoupled diffusion
framework CoopDiff, which employs two distinct branches to decouple human and
object motion modeling, with the human-object contact points as shared anchors
to bridge the motion generation across branches. The human dynamics branch is
aimed to predict highly structured human motion, while the object dynamics
branch focuses on the object motion with rigid translations and rotations.
These two branches are bridged by a series of shared contact points with
consistency constraint for coherent human-object motion prediction. To further
enhance human-object consistency and prediction reliability, we propose a
human-driven interaction module to guide object motion modeling. Extensive
experiments on the BEHAVE and Human-object Interaction datasets demonstrate
that our CoopDiff outperforms state-of-the-art methods.

</details>


### [80] [Lightweight Multi-Scale Feature Extraction with Fully Connected LMF Layer for Salient Object Detection](https://arxiv.org/abs/2508.07170)
*Yunpeng Shi,Lei Chen,Xiaolu Shen,Yanju Guo*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级多尺度特征提取层（LMF层），并基于此构建了LMFNet网络，显著减少了参数量，同时在显著目标检测任务中保持了竞争力。


<details>
  <summary>Details</summary>
Motivation: 解决轻量级网络中多尺度特征提取的效率与性能之间的权衡问题。

Method: 采用深度可分离扩张卷积构建LMF层，并集成多个LMF层形成LMFNet网络。

Result: 在五个基准数据集上达到或接近最优性能，仅需0.81M参数，优于传统和轻量级模型。

Conclusion: LMFNet不仅解决了轻量级网络中的多尺度学习问题，还展示了在图像处理任务中的广泛应用潜力。

Abstract: In the domain of computer vision, multi-scale feature extraction is vital for
tasks such as salient object detection. However, achieving this capability in
lightweight networks remains challenging due to the trade-off between
efficiency and performance. This paper proposes a novel lightweight multi-scale
feature extraction layer, termed the LMF layer, which employs depthwise
separable dilated convolutions in a fully connected structure. By integrating
multiple LMF layers, we develop LMFNet, a lightweight network tailored for
salient object detection. Our approach significantly reduces the number of
parameters while maintaining competitive performance. Here, we show that LMFNet
achieves state-of-the-art or comparable results on five benchmark datasets with
only 0.81M parameters, outperforming several traditional and lightweight models
in terms of both efficiency and accuracy. Our work not only addresses the
challenge of multi-scale learning in lightweight networks but also demonstrates
the potential for broader applications in image processing tasks. The related
code files are available at https://github.com/Shi-Yun-peng/LMFNet

</details>


### [81] [EventRR: Event Referential Reasoning for Referring Video Object Segmentation](https://arxiv.org/abs/2508.07171)
*Huihui Xu,Jiashi Lin,Haoyu Chen,Junjun He,Lei Zhu*

Main category: cs.CV

TL;DR: 论文提出EventRR框架，通过解耦视频对象分割为对象总结和引用推理两部分，利用事件图结构提升视频引用表达的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视视频引用表达中的语义结构，而视频引用表达比图像引用表达更复杂，包含事件属性和时间关系，传统方法难以处理。

Method: EventRR框架分为对象总结和引用推理两部分：总结阶段通过瓶颈令牌汇总帧信息；推理阶段构建引用事件图（REG），并通过时间概念-角色推理（TCRR）逐步计算引用分数。

Result: 在四个基准数据集上的实验表明，EventRR在定量和定性上均优于现有方法。

Conclusion: EventRR通过结构化的事件图推理，显著提升了视频引用对象分割的性能。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment out the object in
a video referred by an expression. Current RVOS methods view referring
expressions as unstructured sequences, neglecting their crucial semantic
structure essential for referent reasoning. Besides, in contrast to
image-referring expressions whose semantics focus only on object attributes and
object-object relations, video-referring expressions also encompass event
attributes and event-event temporal relations. This complexity challenges
traditional structured reasoning image approaches. In this paper, we propose
the Event Referential Reasoning (EventRR) framework. EventRR decouples RVOS
into object summarization part and referent reasoning part. The summarization
phase begins by summarizing each frame into a set of bottleneck tokens, which
are then efficiently aggregated in the video-level summarization step to
exchange the global cross-modal temporal context. For reasoning part, EventRR
extracts semantic eventful structure of a video-referring expression into
highly expressive Referential Event Graph (REG), which is a single-rooted
directed acyclic graph. Guided by topological traversal of REG, we propose
Temporal Concept-Role Reasoning (TCRR) to accumulate the referring score of
each temporal query from REG leaf nodes to root node. Each reasoning step can
be interpreted as a question-answer pair derived from the concept-role
relations in REG. Extensive experiments across four widely recognized benchmark
datasets, show that EventRR quantitatively and qualitatively outperforms
state-of-the-art RVOS methods. Code is available at
https://github.com/bio-mlhui/EventRR

</details>


### [82] [Similarity Matters: A Novel Depth-guided Network for Image Restoration and A New Dataset](https://arxiv.org/abs/2508.07211)
*Junyi He,Liuling Chen,Hongyang Zhou,Zhang xiaoxing,Xiaobin Zhu,Shengxiang Yu,Jingyan Qin,Xu-Cheng Yin*

Main category: cs.CV

TL;DR: 论文提出了一种深度引导网络（DGN）用于图像修复，结合了一个新的大规模高分辨率数据集，通过双分支交互提升修复质量。


<details>
  <summary>Details</summary>
Motivation: 现有图像修复方法常忽略深度信息，导致相似性匹配不佳、注意力分散或背景过度增强。

Method: DGN包含深度估计分支和图像修复分支，利用窗口自注意力捕获对象内相似性，稀疏非局部注意力捕获对象间相似性。

Result: 实验表明，DGN在多个标准基准上达到最优性能，并能泛化到未见过的植物图像。

Conclusion: 深度引导的图像修复方法有效且鲁棒，新数据集为训练和评估提供了丰富资源。

Abstract: Image restoration has seen substantial progress in recent years. However,
existing methods often neglect depth information, which hurts similarity
matching, results in attention distractions in shallow depth-of-field (DoF)
scenarios, and excessive enhancement of background content in deep DoF
settings. To overcome these limitations, we propose a novel Depth-Guided
Network (DGN) for image restoration, together with a novel large-scale
high-resolution dataset. Specifically, the network consists of two interactive
branches: a depth estimation branch that provides structural guidance, and an
image restoration branch that performs the core restoration task. In addition,
the image restoration branch exploits intra-object similarity through
progressive window-based self-attention and captures inter-object similarity
via sparse non-local attention. Through joint training, depth features
contribute to improved restoration quality, while the enhanced visual features
from the restoration branch in turn help refine depth estimation. Notably, we
also introduce a new dataset for training and evaluation, consisting of 9,205
high-resolution images from 403 plant species, with diverse depth and texture
variations. Extensive experiments show that our method achieves
state-of-the-art performance on several standard benchmarks and generalizes
well to unseen plant images, demonstrating its effectiveness and robustness.

</details>


### [83] [Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling](https://arxiv.org/abs/2508.07214)
*Hongyang Zhou,Xiaobin Zhu,Liuling Chen,Junyi He,Jingyan Qin,Xu-Cheng Yin,Zhang xiaoxing*

Main category: cs.CV

TL;DR: 提出了一种基于修正流的无监督真实世界超分辨率方法，通过RFDM和FGDM模块合成具有真实退化特性的LR-HR图像对，显著提升了现有SR方法在真实场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 真实世界超分辨率面临复杂且未知的退化分布问题，现有方法因域差距难以泛化到真实数据。

Method: 提出Rectified Flow Degradation Module (RFDM)和Fourier Prior Guided Degradation Module (FGDM)，通过连续可逆的退化轨迹建模和傅里叶相位信息指导，合成真实退化的LR图像。

Result: 在真实世界数据集上的实验表明，该方法显著提升了现有SR方法的性能。

Conclusion: 通过RFDM和FGDM模块合成的LR-HR图像对有效解决了真实世界超分辨率的退化建模问题，提升了SR方法的实用性。

Abstract: Unsupervised real-world super-resolution (SR) faces critical challenges due
to the complex, unknown degradation distributions in practical scenarios.
Existing methods struggle to generalize from synthetic low-resolution (LR) and
high-resolution (HR) image pairs to real-world data due to a significant domain
gap. In this paper, we propose an unsupervised real-world SR method based on
rectified flow to effectively capture and model real-world degradation,
synthesizing LR-HR training pairs with realistic degradation. Specifically,
given unpaired LR and HR images, we propose a novel Rectified Flow Degradation
Module (RFDM) that introduces degradation-transformed LR (DT-LR) images as
intermediaries. By modeling the degradation trajectory in a continuous and
invertible manner, RFDM better captures real-world degradation and enhances the
realism of generated LR images. Additionally, we propose a Fourier Prior Guided
Degradation Module (FGDM) that leverages structural information embedded in
Fourier phase components to ensure more precise modeling of real-world
degradation. Finally, the LR images are processed by both FGDM and RFDM,
producing final synthetic LR images with real-world degradation. The synthetic
LR images are paired with the given HR images to train the off-the-shelf SR
networks. Extensive experiments on real-world datasets demonstrate that our
method significantly enhances the performance of existing SR approaches in
real-world scenarios.

</details>


### [84] [Bridging Semantic Logic Gaps: A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization](https://arxiv.org/abs/2508.07216)
*Songlin Li,Zhiqing Guo,Yuanman Li,Zeyu Li,Yunfeng Diao,Gaobo Yang,Liejun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种认知启发的多模态边界保持网络（CMB-Net），通过结合大语言模型（LLMs）和视觉信息，提升图像篡改定位（IML）的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有IML模型主要依赖视觉线索，忽略了内容特征的语义逻辑关系，而真实图像的内容语义通常符合人类认知规律。篡改技术破坏了内容特征的内部关系，为IML提供了语义线索。

Method: CMB-Net利用LLMs分析篡改区域并生成基于提示的文本信息，弥补视觉信息中语义关系的不足。提出图像-文本中心模糊模块（ITCAM）量化文本与图像特征的模糊性，为文本特征分配权重。图像-文本交互模块（ITIM）通过相关矩阵对齐视觉和文本特征。基于可逆神经网络，提出恢复边缘解码器（RED）保留篡改区域的边界信息。

Result: 大量实验表明，CMB-Net优于大多数现有IML模型。

Conclusion: CMB-Net通过多模态信息融合和边界保持机制，显著提升了图像篡改定位的准确性。

Abstract: The existing image manipulation localization (IML) models mainly relies on
visual cues, but ignores the semantic logical relationships between content
features. In fact, the content semantics conveyed by real images often conform
to human cognitive laws. However, image manipulation technology usually
destroys the internal relationship between content features, thus leaving
semantic clues for IML. In this paper, we propose a cognition-inspired
multimodal boundary-preserving network (CMB-Net). Specifically, CMB-Net
utilizes large language models (LLMs) to analyze manipulated regions within
images and generate prompt-based textual information to compensate for the lack
of semantic relationships in the visual information. Considering that the
erroneous texts induced by hallucination from LLMs will damage the accuracy of
IML, we propose an image-text central ambiguity module (ITCAM). It assigns
weights to the text features by quantifying the ambiguity between text and
image features, thereby ensuring the beneficial impact of textual information.
We also propose an image-text interaction module (ITIM) that aligns visual and
text features using a correlation matrix for fine-grained interaction. Finally,
inspired by invertible neural networks, we propose a restoration edge decoder
(RED) that mutually generates input and output features to preserve boundary
information in manipulated regions without loss. Extensive experiments show
that CMB-Net outperforms most existing IML models.

</details>


### [85] [Generic Calibration: Pose Ambiguity/Linear Solution and Parametric-hybrid Pipeline](https://arxiv.org/abs/2508.07217)
*Yuqi Han,Qi Cai,Yuanxin Wu*

Main category: cs.CV

TL;DR: 论文提出了一种混合标定方法，结合通用和参数化模型，解决了通用标定中的姿态模糊问题，并提高了标定精度。


<details>
  <summary>Details</summary>
Motivation: 离线相机标定中，参数化模型依赖用户经验，通用标定方法复杂且无法提供传统内参，且存在姿态模糊问题。

Method: 提出线性求解器和非线性优化解决姿态模糊，并引入全局优化的混合标定方法，结合通用和参数化模型。

Result: 仿真和实际实验表明，混合方法在各种镜头类型和噪声污染下表现优异。

Conclusion: 混合标定方法为复杂场景下的相机标定提供了可靠且精确的解决方案。

Abstract: Offline camera calibration techniques typically employ parametric or generic
camera models. Selecting parametric models relies heavily on user experience,
and an inappropriate camera model can significantly affect calibration
accuracy. Meanwhile, generic calibration methods involve complex procedures and
cannot provide traditional intrinsic parameters. This paper reveals a pose
ambiguity in the pose solutions of generic calibration methods that
irreversibly impacts subsequent pose estimation. A linear solver and a
nonlinear optimization are proposed to address this ambiguity issue. Then a
global optimization hybrid calibration method is introduced to integrate
generic and parametric models together, which improves extrinsic parameter
accuracy of generic calibration and mitigates overfitting and numerical
instability in parametric calibration. Simulation and real-world experimental
results demonstrate that the generic-parametric hybrid calibration method
consistently excels across various lens types and noise contamination,
hopefully serving as a reliable and accurate solution for camera calibration in
complex scenarios.

</details>


### [86] [Landmark Guided Visual Feature Extractor for Visual Speech Recognition with Limited Resource](https://arxiv.org/abs/2508.07233)
*Lei Yang,Junshan Jin,Mingyuan Zhang,Yi He,Bofan Chen,Shilin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于面部标志物的视觉特征提取方法，通过多图卷积网络和多级唇部动态融合框架，提升了小数据量下的视觉语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语音识别易受光照、皮肤纹理等视觉干扰影响，传统数据驱动方法需要大量数据和计算资源。本文旨在减少用户特定特征的影响，并在有限数据下提升性能。

Method: 使用面部标志物作为辅助信息，设计时空多图卷积网络提取特征，并引入多级唇部动态融合框架结合原始视频帧的视觉特征。

Result: 实验表明，该方法在有限数据下表现良好，并提高了对未见说话者的识别准确率。

Conclusion: 该方法有效减少了视觉干扰的影响，提升了小数据量下的视觉语音识别性能。

Abstract: Visual speech recognition is a technique to identify spoken content in silent
speech videos, which has raised significant attention in recent years.
Advancements in data-driven deep learning methods have significantly improved
both the speed and accuracy of recognition. However, these deep learning
methods can be effected by visual disturbances, such as lightning conditions,
skin texture and other user-specific features. Data-driven approaches could
reduce the performance degradation caused by these visual disturbances using
models pretrained on large-scale datasets. But these methods often require
large amounts of training data and computational resources, making them costly.
To reduce the influence of user-specific features and enhance performance with
limited data, this paper proposed a landmark guided visual feature extractor.
Facial landmarks are used as auxiliary information to aid in training the
visual feature extractor. A spatio-temporal multi-graph convolutional network
is designed to fully exploit the spatial locations and spatio-temporal features
of facial landmarks. Additionally, a multi-level lip dynamic fusion framework
is introduced to combine the spatio-temporal features of the landmarks with the
visual features extracted from the raw video frames. Experimental results show
that this approach performs well with limited data and also improves the
model's accuracy on unseen speakers.

</details>


### [87] [ASM-UNet: Adaptive Scan Mamba Integrating Group Commonalities and Individual Variations for Fine-Grained Segmentation](https://arxiv.org/abs/2508.07237)
*Bo Wang,Mengyuan Xu,Yue Yan,Yuqun Yang,Kechen Shu,Wei Ping,Xu Tang,Wei Jiang,Zheng You*

Main category: cs.CV

TL;DR: ASM-UNet是一种基于Mamba的架构，通过动态调整扫描顺序解决细粒度分割问题，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有粗粒度分割方法在细粒度分割任务中表现不佳，且Mamba模型依赖固定扫描顺序，难以适应个体差异。

Method: 提出ASM-UNet，引入自适应扫描分数动态调整扫描顺序，结合群体共性和个体差异。

Result: 在ACDC、Synapse和BTMS数据集上，ASM-UNet在粗粒度和细粒度分割任务中均表现优异。

Conclusion: ASM-UNet通过动态扫描顺序解决了细粒度分割的挑战，具有临床实用价值。

Abstract: Precise lesion resection depends on accurately identifying fine-grained
anatomical structures. While many coarse-grained segmentation (CGS) methods
have been successful in large-scale segmentation (e.g., organs), they fall
short in clinical scenarios requiring fine-grained segmentation (FGS), which
remains challenging due to frequent individual variations in small-scale
anatomical structures. Although recent Mamba-based models have advanced medical
image segmentation, they often rely on fixed manually-defined scanning orders,
which limit their adaptability to individual variations in FGS. To address
this, we propose ASM-UNet, a novel Mamba-based architecture for FGS. It
introduces adaptive scan scores to dynamically guide the scanning order,
generated by combining group-level commonalities and individual-level
variations. Experiments on two public datasets (ACDC and Synapse) and a newly
proposed challenging biliary tract FGS dataset, namely BTMS, demonstrate that
ASM-UNet achieves superior performance in both CGS and FGS tasks. Our code and
dataset are available at https://github.com/YqunYang/ASM-UNet.

</details>


### [88] [Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers](https://arxiv.org/abs/2508.07246)
*Xin Ma,Yaohui Wang,Genyun Jia,Xinyuan Chen,Tien-Tsin Wong,Cunjian Chen*

Main category: cs.CV

TL;DR: MiraMo框架通过高效线性注意力、运动残差学习和DCT噪声优化，提升图像动画的效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决图像动画中外观一致性和运动平滑性的挑战，同时降低计算复杂度。

Method: 采用文本到视频架构、运动残差学习和DCT噪声优化策略。

Result: 实验证明MiraMo在生成一致、平滑且可控的动画方面优于现有方法。

Conclusion: MiraMo在图像动画任务中表现出高效性和多功能性。

Abstract: Image animation has seen significant progress, driven by the powerful
generative capabilities of diffusion models. However, maintaining appearance
consistency with static input images and mitigating abrupt motion transitions
in generated animations remain persistent challenges. While text-to-video (T2V)
generation has demonstrated impressive performance with diffusion transformer
models, the image animation field still largely relies on U-Net-based diffusion
models, which lag behind the latest T2V approaches. Moreover, the quadratic
complexity of vanilla self-attention mechanisms in Transformers imposes heavy
computational demands, making image animation particularly resource-intensive.
To address these issues, we propose MiraMo, a framework designed to enhance
efficiency, appearance consistency, and motion smoothness in image animation.
Specifically, MiraMo introduces three key elements: (1) A foundational
text-to-video architecture replacing vanilla self-attention with efficient
linear attention to reduce computational overhead while preserving generation
quality; (2) A novel motion residual learning paradigm that focuses on modeling
motion dynamics rather than directly predicting frames, improving temporal
consistency; and (3) A DCT-based noise refinement strategy during inference to
suppress sudden motion artifacts, complemented by a dynamics control module to
balance motion smoothness and expressiveness. Extensive experiments against
state-of-the-art methods validate the superiority of MiraMo in generating
consistent, smooth, and controllable animations with accelerated inference
speed. Additionally, we demonstrate the versatility of MiraMo through
applications in motion transfer and video editing tasks.

</details>


### [89] [SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking](https://arxiv.org/abs/2508.07250)
*Fengchao Xiong,Zhenxing Wu,Sen Jia,Yuntao Qian*

Main category: cs.CV

TL;DR: 该论文提出了一种新的高光谱视频跟踪方法，通过结合空间和光谱交互，利用Transformer和集合论的包含-排除原理，提升了跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注模板与搜索区域的空间交互，忽略了光谱交互，导致性能不佳。论文旨在从架构和训练层面解决这一问题。

Method: 在架构层面，使用Transformer建立模板与搜索区域的波段间长程空间关系，并通过集合论建模光谱交互；在训练层面，引入光谱损失以增强材料分布对齐。

Result: 实验表明，该方法实现了最先进的跟踪性能。

Conclusion: 论文提出的方法有效整合了空间和光谱信息，显著提升了高光谱视频跟踪的鲁棒性和准确性。

Abstract: Hyperspectral videos (HSVs), with their inherent spatial-spectral-temporal
structure, offer distinct advantages in challenging tracking scenarios such as
cluttered backgrounds and small objects. However, existing methods primarily
focus on spatial interactions between the template and search regions, often
overlooking spectral interactions, leading to suboptimal performance. To
address this issue, this paper investigates spectral interactions from both the
architectural and training perspectives. At the architectural level, we first
establish band-wise long-range spatial relationships between the template and
search regions using Transformers. We then model spectral interactions using
the inclusion-exclusion principle from set theory, treating them as the union
of spatial interactions across all bands. This enables the effective
integration of both shared and band-specific spatial cues. At the training
level, we introduce a spectral loss to enforce material distribution alignment
between the template and predicted regions, enhancing robustness to shape
deformation and appearance variations. Extensive experiments demonstrate that
our tracker achieves state-of-the-art tracking performance. The source code,
trained models and results will be publicly available via
https://github.com/bearshng/suit to support reproducibility.

</details>


### [90] [Understanding Dynamic Scenes in Ego Centric 4D Point Clouds](https://arxiv.org/abs/2508.07251)
*Junsheng Huang,Shengyu Hao,Bocheng Hu,Gaoang Wang*

Main category: cs.CV

TL;DR: EgoDynamic4D是一个新的QA基准，用于动态4D场景理解，包含RGB-D视频、相机姿态、实例掩码和4D边界框，支持细粒度时空推理。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏统一的4D标注和任务驱动的评估协议，无法支持细粒度的时空推理。

Method: 提出EgoDynamic4D数据集和端到端时空推理框架，结合动态和静态场景信息，使用实例感知特征编码和时间-相机编码。

Result: 实验表明，该方法在EgoDynamic4D上优于基线，验证了多模态时间建模的有效性。

Conclusion: EgoDynamic4D和提出的框架为动态场景理解提供了新工具，支持细粒度的时空推理任务。

Abstract: Understanding dynamic 4D scenes from an egocentric perspective-modeling
changes in 3D spatial structure over time-is crucial for human-machine
interaction, autonomous navigation, and embodied intelligence. While existing
egocentric datasets contain dynamic scenes, they lack unified 4D annotations
and task-driven evaluation protocols for fine-grained spatio-temporal
reasoning, especially on motion of objects and human, together with their
interactions. To address this gap, we introduce EgoDynamic4D, a novel QA
benchmark on highly dynamic scenes, comprising RGB-D video, camera poses,
globally unique instance masks, and 4D bounding boxes. We construct 927K QA
pairs accompanied by explicit Chain-of-Thought (CoT), enabling verifiable,
step-by-step spatio-temporal reasoning. We design 12 dynamic QA tasks covering
agent motion, human-object interaction, trajectory prediction, relation
understanding, and temporal-causal reasoning, with fine-grained,
multidimensional metrics. To tackle these tasks, we propose an end-to-end
spatio-temporal reasoning framework that unifies dynamic and static scene
information, using instance-aware feature encoding, time and camera encoding,
and spatially adaptive down-sampling to compress large 4D scenes into token
sequences manageable by LLMs. Experiments on EgoDynamic4D show that our method
consistently outperforms baselines, validating the effectiveness of multimodal
temporal modeling for egocentric dynamic scene understanding.

</details>


### [91] [Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM](https://arxiv.org/abs/2508.07260)
*Sihan Yang,Huitong Ji,Shaolin Lu,Jiayi Chen,Binxiao Xu,Ming Lu,Yuanxing Zhang,Wenhui Dong,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为Small-Large Collaboration (SLC)的协作框架，通过小型和大型视觉语言模型的协作实现个性化，同时避免高训练成本和推理能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（VLMs）虽然具备强大的多模态理解能力，但训练成本高且难以直接个性化；小型VLMs易于个性化但推理能力不足。为解决这一问题，提出了SLC框架。

Method: SLC框架中，小型VLM负责生成个性化信息，大型VLM整合这些信息以提供准确响应。采用测试时反射策略防止小型VLM的幻觉问题。

Result: 实验证明SLC框架在多种基准测试和大型VLMs中有效，且训练高效。

Conclusion: SLC是首个支持开源和闭源大型VLMs的训练高效框架，为个性化应用提供了更广泛的现实可能性。

Abstract: Personalizing Vision-Language Models (VLMs) to transform them into daily
assistants has emerged as a trending research direction. However, leading
companies like OpenAI continue to increase model size and develop complex
designs such as the chain of thought (CoT). While large VLMs are proficient in
complex multi-modal understanding, their high training costs and limited access
via paid APIs restrict direct personalization. Conversely, small VLMs are
easily personalized and freely available, but they lack sufficient reasoning
capabilities. Inspired by this, we propose a novel collaborative framework
named Small-Large Collaboration (SLC) for large VLM personalization, where the
small VLM is responsible for generating personalized information, while the
large model integrates this personalized information to deliver accurate
responses. To effectively incorporate personalized information, we develop a
test-time reflection strategy, preventing the potential hallucination of the
small VLM. Since SLC only needs to train a meta personalized small VLM for the
large VLMs, the overall process is training-efficient. To the best of our
knowledge, this is the first training-efficient framework that supports both
open-source and closed-source large VLMs, enabling broader real-world
personalized applications. We conduct thorough experiments across various
benchmarks and large VLMs to demonstrate the effectiveness of the proposed SLC
framework. The code will be released at https://github.com/Hhankyangg/SLC.

</details>


### [92] [Representation Understanding via Activation Maximization](https://arxiv.org/abs/2508.07281)
*Hongbo Zhu,Angelo Cangelosi*

Main category: cs.CV

TL;DR: 提出了一种适用于CNN和ViT的统一特征可视化框架，扩展了中间层的特征可视化，并探讨了激活最大化在生成对抗样本中的应用。


<details>
  <summary>Details</summary>
Motivation: 理解DNN的内部特征表示是模型可解释性的关键步骤，现有研究主要关注CNN的输出层神经元，缺乏对中间层和ViT的探索。

Method: 采用激活最大化方法，扩展特征可视化至中间层，并研究其在生成对抗样本中的应用。

Result: 实验证明该方法在CNN和ViT中均有效，揭示了模型的潜在脆弱性和决策边界。

Conclusion: 该框架具有通用性和解释价值，为DNN的特征理解和安全性提供了新视角。

Abstract: Understanding internal feature representations of deep neural networks (DNNs)
is a fundamental step toward model interpretability. Inspired by neuroscience
methods that probe biological neurons using visual stimuli, recent deep
learning studies have employed Activation Maximization (AM) to synthesize
inputs that elicit strong responses from artificial neurons. In this work, we
propose a unified feature visualization framework applicable to both
Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). Unlike
prior efforts that predominantly focus on the last output-layer neurons in
CNNs, we extend feature visualization to intermediate layers as well, offering
deeper insights into the hierarchical structure of learned feature
representations. Furthermore, we investigate how activation maximization can be
leveraged to generate adversarial examples, revealing potential vulnerabilities
and decision boundaries of DNNs. Our experiments demonstrate the effectiveness
of our approach in both traditional CNNs and modern ViT, highlighting its
generalizability and interpretive value.

</details>


### [93] [SynMatch: Rethinking Consistency in Medical Image Segmentation with Sparse Annotations](https://arxiv.org/abs/2508.07298)
*Zhiqiang Shen,Peng Cao,Xiaoli Liu,Jinzhu Yang,Osmar R. Zaiane*

Main category: cs.CV

TL;DR: SynMatch通过合成图像匹配伪标签，解决了医学图像分割中标签稀缺问题，无需额外训练参数，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中标签稀缺问题严重，现有伪监督方法因伪标签与图像不一致而性能受限。

Method: SynMatch从生成伪标签的分割模型中提取纹理和形状特征，合成与伪标签高度一致的图像。

Result: 在多种标注限制下（SSL、WSL、BSL），SynMatch表现优异，尤其在BSL设置中，性能显著优于现有方法。

Conclusion: SynMatch通过合成图像匹配伪标签，有效解决了标签稀缺问题，为医学图像分割提供了新思路。

Abstract: Label scarcity remains a major challenge in deep learning-based medical image
segmentation. Recent studies use strong-weak pseudo supervision to leverage
unlabeled data. However, performance is often hindered by inconsistencies
between pseudo labels and their corresponding unlabeled images. In this work,
we propose \textbf{SynMatch}, a novel framework that sidesteps the need for
improving pseudo labels by synthesizing images to match them instead.
Specifically, SynMatch synthesizes images using texture and shape features
extracted from the same segmentation model that generates the corresponding
pseudo labels for unlabeled images. This design enables the generation of
highly consistent synthesized-image-pseudo-label pairs without requiring any
training parameters for image synthesis. We extensively evaluate SynMatch
across diverse medical image segmentation tasks under semi-supervised learning
(SSL), weakly-supervised learning (WSL), and barely-supervised learning (BSL)
settings with increasingly limited annotations. The results demonstrate that
SynMatch achieves superior performance, especially in the most challenging BSL
setting. For example, it outperforms the recent strong-weak pseudo
supervision-based method by 29.71\% and 10.05\% on the polyp segmentation task
with 5\% and 10\% scribble annotations, respectively. The code will be released
at https://github.com/Senyh/SynMatch.

</details>


### [94] [BEVANet: Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation](https://arxiv.org/abs/2508.07300)
*Ping-Mao Huang,I-Tien Chao,Ping-Chia Huang,Jia-Wei Liao,Yung-Yu Chuang*

Main category: cs.CV

TL;DR: 论文提出了一种名为BEVANet的双边高效视觉注意力网络，通过大核注意力机制（LKA）和动态调整感受野的综合核选择机制（CKS），实现了实时语义分割的高性能。


<details>
  <summary>Details</summary>
Motivation: 解决实时语义分割中高效架构设计和大感受野捕获的双重挑战，同时优化细节轮廓。

Method: 采用稀疏分解大分离核注意力（SDLSKA）和深度大核金字塔池化模块（DLKPPM），结合边界引导自适应融合（BGAF）模块，实现分支频繁通信和边界增强。

Result: BEVANet在未预训练和ImageNet预训练下分别达到79.3%和81.0%的mIoU，实时速度为33 FPS。

Conclusion: BEVANet通过创新的注意力机制和动态调整策略，在实时语义分割任务中实现了最先进的性能。

Abstract: Real-time semantic segmentation presents the dual challenge of designing
efficient architectures that capture large receptive fields for semantic
understanding while also refining detailed contours. Vision transformers model
long-range dependencies effectively but incur high computational cost. To
address these challenges, we introduce the Large Kernel Attention (LKA)
mechanism. Our proposed Bilateral Efficient Visual Attention Network (BEVANet)
expands the receptive field to capture contextual information and extracts
visual and structural features using Sparse Decomposed Large Separable Kernel
Attentions (SDLSKA). The Comprehensive Kernel Selection (CKS) mechanism
dynamically adapts the receptive field to further enhance performance.
Furthermore, the Deep Large Kernel Pyramid Pooling Module (DLKPPM) enriches
contextual features by synergistically combining dilated convolutions and large
kernel attention. The bilateral architecture facilitates frequent branch
communication, and the Boundary Guided Adaptive Fusion (BGAF) module enhances
boundary delineation by integrating spatial and semantic features under
boundary guidance. BEVANet achieves real-time segmentation at 33 FPS, yielding
79.3% mIoU without pretraining and 81.0% mIoU on Cityscapes after ImageNet
pretraining, demonstrating state-of-the-art performance. The code and model is
available at https://github.com/maomao0819/BEVANet.

</details>


### [95] [DragonFruitQualityNet: A Lightweight Convolutional Neural Network for Real-Time Dragon Fruit Quality Inspection on Mobile Devices](https://arxiv.org/abs/2508.07306)
*Md Zahurul Haquea,Yeahyea Sarker,Muhammed Farhan Sadique Mahi,Syed Jubayer Jaman,Md Robiul Islam*

Main category: cs.CV

TL;DR: 该研究提出了一种轻量级CNN模型DragonFruitQualityNet，用于火龙果的实时质量检测，准确率达93.98%，并开发了移动应用以支持实际应用。


<details>
  <summary>Details</summary>
Motivation: 火龙果需求增长，但缺乏高效的采前和采后质量检测方法，影响农业生产效率和减少损失。

Method: 研究使用13,789张火龙果图像数据集，训练轻量级CNN模型，并嵌入移动应用实现实时检测。

Result: 模型准确率达93.98%，优于现有方法，并成功应用于移动设备。

Conclusion: 该研究为火龙果质量控制提供了高效、可扩展的AI解决方案，支持数字农业和可持续种植。

Abstract: Dragon fruit, renowned for its nutritional benefits and economic value, has
experienced rising global demand due to its affordability and local
availability. As dragon fruit cultivation expands, efficient pre- and
post-harvest quality inspection has become essential for improving agricultural
productivity and minimizing post-harvest losses. This study presents
DragonFruitQualityNet, a lightweight Convolutional Neural Network (CNN)
optimized for real-time quality assessment of dragon fruits on mobile devices.
We curated a diverse dataset of 13,789 images, integrating self-collected
samples with public datasets (dataset from Mendeley Data), and classified them
into four categories: fresh, immature, mature, and defective fruits to ensure
robust model training. The proposed model achieves an impressive 93.98%
accuracy, outperforming existing methods in fruit quality classification. To
facilitate practical adoption, we embedded the model into an intuitive mobile
application, enabling farmers and agricultural stakeholders to conduct
on-device, real-time quality inspections. This research provides an accurate,
efficient, and scalable AI-driven solution for dragon fruit quality control,
supporting digital agriculture and empowering smallholder farmers with
accessible technology. By bridging the gap between research and real-world
application, our work advances post-harvest management and promotes sustainable
farming practices.

</details>


### [96] [MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark](https://arxiv.org/abs/2508.07307)
*Haiyang Guo,Fei Zhu,Hongbo Zhao,Fanhu Zeng,Wenzhuo Liu,Shijie Ma,Da-Han Wang,Xu-Yao Zhang*

Main category: cs.CV

TL;DR: 论文介绍了MCITlib，一个用于多模态大语言模型持续指令调优的代码库，旨在解决多模态持续学习中的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法在单模态任务中表现良好，但多模态大语言模型的兴起需要解决跨模态交互和协调的挑战。

Method: 开发了MCITlib代码库，实现了8种代表性算法，并在2个基准上进行了系统评估。

Result: MCITlib为多模态持续学习研究提供了工具，并将持续更新以反映领域进展。

Conclusion: MCITlib是一个支持多模态持续学习的开源工具，有助于推动该领域的研究。

Abstract: Continual learning aims to equip AI systems with the ability to continuously
acquire and adapt to new knowledge without forgetting previously learned
information, similar to human learning. While traditional continual learning
methods focusing on unimodal tasks have achieved notable success, the emergence
of Multimodal Large Language Models has brought increasing attention to
Multimodal Continual Learning tasks involving multiple modalities, such as
vision and language. In this setting, models are expected to not only mitigate
catastrophic forgetting but also handle the challenges posed by cross-modal
interactions and coordination. To facilitate research in this direction, we
introduce MCITlib, a comprehensive and constantly evolving code library for
continual instruction tuning of Multimodal Large Language Models. In MCITlib,
we have currently implemented 8 representative algorithms for Multimodal
Continual Instruction Tuning and systematically evaluated them on 2 carefully
selected benchmarks. MCITlib will be continuously updated to reflect advances
in the Multimodal Continual Learning field. The codebase is released at
https://github.com/Ghy0501/MCITlib.

</details>


### [97] [MobileViCLIP: An Efficient Video-Text Model for Mobile Devices](https://arxiv.org/abs/2508.07312)
*Min Yang,Zihan Jia,Zhilin Dai,Sheng Guo,Limin Wang*

Main category: cs.CV

TL;DR: MobileViCLIP是一种高效的视频文本模型，通过引入时间结构重参数化技术，在移动设备上实现快速推理和零样本分类与检索。


<details>
  <summary>Details</summary>
Motivation: 现有视频预训练模型主要基于高延迟的ViT架构，缺乏针对移动设备的高效架构。

Method: 将时间结构重参数化引入高效的图像文本模型，并在大规模高质量视频文本数据集上训练。

Result: MobileViCLIP-Small在移动设备上的推理速度显著提升，零样本检索性能接近或优于现有模型。

Conclusion: MobileViCLIP填补了移动设备高效视频文本模型的空白，具有实际应用潜力。

Abstract: Efficient lightweight neural networks are with increasing attention due to
their faster reasoning speed and easier deployment on mobile devices. However,
existing video pre-trained models still focus on the common ViT architecture
with high latency, and few works attempt to build efficient architecture on
mobile devices. This paper bridges this gap by introducing temporal structural
reparameterization into an efficient image-text model and training it on a
large-scale high-quality video-text dataset, resulting in an efficient
video-text model that can run on mobile devices with strong zero-shot
classification and retrieval capabilities, termed as MobileViCLIP. In
particular, in terms of inference speed on mobile devices, our
MobileViCLIP-Small is 55.4x times faster than InternVideo2-L14 and 6.7x faster
than InternVideo2-S14. In terms of zero-shot retrieval performance, our
MobileViCLIP-Small obtains similar performance as InternVideo2-L14 and obtains
6.9\% better than InternVideo2-S14 on MSR-VTT. The code is available at
https://github.com/MCG-NJU/MobileViCLIP.

</details>


### [98] [DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding](https://arxiv.org/abs/2508.07313)
*Junyu Xiong,Yonghui Wang,Weichao Zhao,Chenyu Liu,Bing Yin,Wengang Zhou,Houqiang Li*

Main category: cs.CV

TL;DR: DocR1是一种基于强化学习的多模态大语言模型，通过EviGRPO框架实现多页文档理解，采用证据感知奖励机制和两阶段标注流程，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 多页文档理解对多模态大语言模型提出了挑战，需要细粒度视觉理解和跨页多跳推理。现有强化学习方法在此领域尚未充分探索。

Method: 提出EviGRPO框架，结合证据感知奖励机制，采用两阶段标注流程和课程学习策略，构建EviBench和ArxivFullQA数据集。

Result: DocR1在多页任务中达到最先进性能，同时在单页任务中保持强劲表现。

Conclusion: EviGRPO框架和DocR1模型为多页文档理解提供了高效解决方案，展示了强化学习在此领域的潜力。

Abstract: Understanding multi-page documents poses a significant challenge for
multimodal large language models (MLLMs), as it requires fine-grained visual
comprehension and multi-hop reasoning across pages. While prior work has
explored reinforcement learning (RL) for enhancing advanced reasoning in MLLMs,
its application to multi-page document understanding remains underexplored. In
this paper, we introduce DocR1, an MLLM trained with a novel RL framework,
Evidence Page-Guided GRPO (EviGRPO). EviGRPO incorporates an evidence-aware
reward mechanism that promotes a coarse-to-fine reasoning strategy, guiding the
model to first retrieve relevant pages before generating answers. This training
paradigm enables us to build high-quality models with limited supervision. To
support this, we design a two-stage annotation pipeline and a curriculum
learning strategy, based on which we construct two datasets: EviBench, a
high-quality training set with 4.8k examples, and ArxivFullQA, an evaluation
benchmark with 8.6k QA pairs based on scientific papers. Extensive experiments
across a wide range of benchmarks demonstrate that DocR1 achieves
state-of-the-art performance on multi-page tasks, while consistently
maintaining strong results on single-page benchmarks.

</details>


### [99] [RORPCap: Retrieval-based Objects and Relations Prompt for Image Captioning](https://arxiv.org/abs/2508.07318)
*Jinjing Gu,Tianbao Qin,Yuanyuan Pu,Zhengpeng Zhao*

Main category: cs.CV

TL;DR: RORPCap提出了一种基于检索的对象和关系提示方法，用于图像描述生成，解决了传统方法中的冗余检测、GCN构建困难和训练成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统图像描述生成方法依赖对象检测器或GCN，存在冗余信息、构建困难和训练成本高的问题，需要更高效的解决方案。

Method: RORPCap通过检索提取对象和关系词，结合预定义提示模板和Mamba映射网络，生成文本增强的特征嵌入，最终通过GPT-2生成描述。

Result: 在MS-COCO数据集上，RORPCap仅需2.6小时训练，CIDEr得分120.5%，SPICE得分22.0%，性能与检测器和GCN模型相当。

Conclusion: RORPCap在训练时间和性能上均表现优异，可作为图像描述生成的替代方案。

Abstract: Image captioning aims to generate natural language descriptions for input
images in an open-form manner. To accurately generate descriptions related to
the image, a critical step in image captioning is to identify objects and
understand their relations within the image. Modern approaches typically
capitalize on object detectors or combine detectors with Graph Convolutional
Network (GCN). However, these models suffer from redundant detection
information, difficulty in GCN construction, and high training costs. To
address these issues, a Retrieval-based Objects and Relations Prompt for Image
Captioning (RORPCap) is proposed, inspired by the fact that image-text
retrieval can provide rich semantic information for input images. RORPCap
employs an Objects and relations Extraction Model to extract object and
relation words from the image. These words are then incorporate into predefined
prompt templates and encoded as prompt embeddings. Next, a Mamba-based mapping
network is designed to quickly map image embeddings extracted by CLIP to
visual-text embeddings. Finally, the resulting prompt embeddings and
visual-text embeddings are concatenated to form textual-enriched feature
embeddings, which are fed into a GPT-2 model for caption generation. Extensive
experiments conducted on the widely used MS-COCO dataset show that the RORPCap
requires only 2.6 hours under cross-entropy loss training, achieving 120.5%
CIDEr score and 22.0% SPICE score on the "Karpathy" test split. RORPCap
achieves comparable performance metrics to detector-based and GCN-based models
with the shortest training time and demonstrates its potential as an
alternative for image captioning.

</details>


### [100] [Planner-Refiner: Dynamic Space-Time Refinement for Vision-Language Alignment in Videos](https://arxiv.org/abs/2508.07330)
*Tuyen Tran,Thao Minh Le,Quang-Hung Le,Truyen Tran*

Main category: cs.CV

TL;DR: Planner-Refiner框架通过语言引导的迭代细化视觉元素时空表示，解决视频-语言对齐中的语义鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 视频-语言对齐需处理语言的复杂性、动态交互实体及其动作链，以及语言与视觉间的语义鸿沟。

Method: Planner模块分解复杂语言提示为短句链，Refiner模块通过名词-动词对引导视觉令牌的自注意力，实现高效单步细化。

Result: 在Referring Video Object Segmentation和Temporal Grounding任务中表现优异，尤其在复杂提示下优于现有方法。

Conclusion: Planner-Refiner框架有效缩小语义鸿沟，为复杂语言提示的视频-语言对齐提供了潜力。

Abstract: Vision-language alignment in video must address the complexity of language,
evolving interacting entities, their action chains, and semantic gaps between
language and vision. This work introduces Planner-Refiner, a framework to
overcome these challenges. Planner-Refiner bridges the semantic gap by
iteratively refining visual elements' space-time representation, guided by
language until semantic gaps are minimal. A Planner module schedules language
guidance by decomposing complex linguistic prompts into short sentence chains.
The Refiner processes each short sentence, a noun-phrase and verb-phrase pair,
to direct visual tokens' self-attention across space then time, achieving
efficient single-step refinement. A recurrent system chains these steps,
maintaining refined visual token representations. The final representation
feeds into task-specific heads for alignment generation. We demonstrate
Planner-Refiner's effectiveness on two video-language alignment tasks:
Referring Video Object Segmentation and Temporal Grounding with varying
language complexity. We further introduce a new MeViS-X benchmark to assess
models' capability with long queries. Superior performance versus
state-of-the-art methods on these benchmarks shows the approach's potential,
especially for complex prompts.

</details>


### [101] [CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation](https://arxiv.org/abs/2508.07341)
*Fangtai Wu,Mushui Liu,Weijie He,Wanggui He,Hao Jiang,Zhao Wang,Yunlong Yu*

Main category: cs.CV

TL;DR: CoAR是一种新颖的框架，通过冻结预训练参数，仅调整极少量参数（<0.05%）实现定制化图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有定制化生成方法依赖全微调或适配器，成本高且易过拟合或灾难性遗忘。

Method: 采用Layerwise Multimodal Context Learning策略学习特定主题表示，并引入正则化防止过拟合和语言漂移。

Result: CoAR在主题和风格个性化任务中表现优异，计算和内存效率显著提升。

Conclusion: CoAR在保持预训练模型性能的同时，实现了高效、低成本的定制化生成。

Abstract: The unified autoregressive (AR) model excels at multimodal understanding and
generation, but its potential for customized image generation remains
underexplored. Existing customized generation methods rely on full fine-tuning
or adapters, making them costly and prone to overfitting or catastrophic
forgetting. In this paper, we propose \textbf{CoAR}, a novel framework for
injecting subject concepts into the unified AR models while keeping all
pre-trained parameters completely frozen. CoAR learns effective, specific
subject representations with only a minimal number of parameters using a
Layerwise Multimodal Context Learning strategy. To address overfitting and
language drift, we further introduce regularization that preserves the
pre-trained distribution and anchors context tokens to improve subject fidelity
and re-contextualization. Additionally, CoAR supports training-free subject
customization in a user-provided style. Experiments demonstrate that CoAR
achieves superior performance on both subject-driven personalization and style
personalization, while delivering significant gains in computational and memory
efficiency. Notably, CoAR tunes less than \textbf{0.05\%} of the parameters
while achieving competitive performance compared to recent Proxy-Tuning. Code:
https://github.com/KZF-kzf/CoAR

</details>


### [102] [SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal](https://arxiv.org/abs/2508.07346)
*Tingyu Yang,Jue Gong,Jinpei Guo,Wenbo Li,Yong Guo,Yulun Zhang*

Main category: cs.CV

TL;DR: SODiff是一种新颖的语义导向一步扩散模型，用于去除JPEG伪影，通过语义对齐的图像提示提取器和质量因子感知时间预测器，显著提升了恢复效果。


<details>
  <summary>Details</summary>
Motivation: JPEG高压缩比会引入严重的视觉伪影，现有深度学习方法难以恢复复杂纹理细节，导致输出过度平滑。

Method: 提出SODiff模型，结合语义对齐的图像提示提取器（SAIPE）和质量因子感知时间预测器，优化扩散过程。

Result: 实验表明SODiff在视觉质量和定量指标上优于现有方法。

Conclusion: SODiff通过语义导向和自适应时间预测，有效提升了JPEG伪影去除的性能。

Abstract: JPEG, as a widely used image compression standard, often introduces severe
visual artifacts when achieving high compression ratios. Although existing deep
learning-based restoration methods have made considerable progress, they often
struggle to recover complex texture details, resulting in over-smoothed
outputs. To overcome these limitations, we propose SODiff, a novel and
efficient semantic-oriented one-step diffusion model for JPEG artifacts
removal. Our core idea is that effective restoration hinges on providing
semantic-oriented guidance to the pre-trained diffusion model, thereby fully
leveraging its powerful generative prior. To this end, SODiff incorporates a
semantic-aligned image prompt extractor (SAIPE). SAIPE extracts rich features
from low-quality (LQ) images and projects them into an embedding space
semantically aligned with that of the text encoder. Simultaneously, it
preserves crucial information for faithful reconstruction. Furthermore, we
propose a quality factor-aware time predictor that implicitly learns the
compression quality factor (QF) of the LQ image and adaptively selects the
optimal denoising start timestep for the diffusion process. Extensive
experimental results show that our SODiff outperforms recent leading methods in
both visual quality and quantitative metrics. Code is available at:
https://github.com/frakenation/SODiff

</details>


### [103] [GS4Buildings: Prior-Guided Gaussian Splatting for 3D Building Reconstruction](https://arxiv.org/abs/2508.07355)
*Qilin Zhang,Olaf Wysocki,Boris Jutzi*

Main category: cs.CV

TL;DR: GS4Buildings利用语义3D建筑模型改进2D高斯泼溅（2DGS），提升大规模城市场景中的建筑重建完整性和几何精度。


<details>
  <summary>Details</summary>
Motivation: 2DGS在大规模和复杂城市场景中性能下降，导致建筑重建不完整。

Method: GS4Buildings直接从低细节语义3D建筑模型初始化高斯，并利用平面几何生成先验深度和法线图，优化表面一致性和结构准确性。

Result: 实验显示，GS4Buildings将重建完整性提升20.5%，几何精度提升32.8%，并减少71.8%的高斯基元。

Conclusion: 语义建筑模型集成有助于推动基于GS的重建技术在城市应用中的实际应用。

Abstract: Recent advances in Gaussian Splatting (GS) have demonstrated its
effectiveness in photo-realistic rendering and 3D reconstruction. Among these,
2D Gaussian Splatting (2DGS) is particularly suitable for surface
reconstruction due to its flattened Gaussian representation and integrated
normal regularization. However, its performance often degrades in large-scale
and complex urban scenes with frequent occlusions, leading to incomplete
building reconstructions. We propose GS4Buildings, a novel prior-guided
Gaussian Splatting method leveraging the ubiquity of semantic 3D building
models for robust and scalable building surface reconstruction. Instead of
relying on traditional Structure-from-Motion (SfM) pipelines, GS4Buildings
initializes Gaussians directly from low-level Level of Detail (LoD)2 semantic
3D building models. Moreover, we generate prior depth and normal maps from the
planar building geometry and incorporate them into the optimization process,
providing strong geometric guidance for surface consistency and structural
accuracy. We also introduce an optional building-focused mode that limits
reconstruction to building regions, achieving a 71.8% reduction in Gaussian
primitives and enabling a more efficient and compact representation.
Experiments on urban datasets demonstrate that GS4Buildings improves
reconstruction completeness by 20.5% and geometric accuracy by 32.8%. These
results highlight the potential of semantic building model integration to
advance GS-based reconstruction toward real-world urban applications such as
smart cities and digital twins. Our project is available:
https://github.com/zqlin0521/GS4Buildings.

</details>


### [104] [Training and Inference within 1 Second -- Tackle Cross-Sensor Degradation of Real-World Pansharpening with Efficient Residual Feature Tailoring](https://arxiv.org/abs/2508.07369)
*Tianyu Xin,Jin-Liang Xiao,Zeyu Xia,Shan Yin,Liang-Jian Deng*

Main category: cs.CV

TL;DR: 该论文提出了一种解决深度学习全色锐化模型跨传感器退化问题的新方法，通过模块分解和特征裁剪器实现高效训练和推理，显著提升泛化能力和降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型在跨传感器数据上泛化能力差，传统方法如重新训练或零样本方法耗时且需要额外数据。

Method: 通过模块分解揭示关键接口，引入特征裁剪器，采用物理感知的无监督损失进行高效训练，并以分块方式提升效率。

Result: 实验表明，该方法在跨传感器情况下性能显著提升，训练和推理速度极快（如512x512x8图像仅需0.2秒）。

Conclusion: 该方法在质量和效率上均达到最先进水平，适用于实际应用中的跨传感器全色锐化任务。

Abstract: Deep learning methods for pansharpening have advanced rapidly, yet models
pretrained on data from a specific sensor often generalize poorly to data from
other sensors. Existing methods to tackle such cross-sensor degradation include
retraining model or zero-shot methods, but they are highly time-consuming or
even need extra training data. To address these challenges, our method first
performs modular decomposition on deep learning-based pansharpening models,
revealing a general yet critical interface where high-dimensional fused
features begin mapping to the channel space of the final image. % may need
revisement A Feature Tailor is then integrated at this interface to address
cross-sensor degradation at the feature level, and is trained efficiently with
physics-aware unsupervised losses. Moreover, our method operates in a
patch-wise manner, training on partial patches and performing parallel
inference on all patches to boost efficiency. Our method offers two key
advantages: (1) $\textit{Improved Generalization Ability}$: it significantly
enhance performance in cross-sensor cases. (2) $\textit{Low Generalization
Cost}$: it achieves sub-second training and inference, requiring only partial
test inputs and no external data, whereas prior methods often take minutes or
even hours. Experiments on the real-world data from multiple datasets
demonstrate that our method achieves state-of-the-art quality and efficiency in
tackling cross-sensor degradation. For example, training and inference of
$512\times512\times8$ image within $\textit{0.2 seconds}$ and
$4000\times4000\times8$ image within $\textit{3 seconds}$ at the fastest
setting on a commonly used RTX 3090 GPU, which is over 100 times faster than
zero-shot methods.

</details>


### [105] [DIP-GS: Deep Image Prior For Gaussian Splatting Sparse View Recovery](https://arxiv.org/abs/2508.07372)
*Rajaei Khatib,Raja Giryes*

Main category: cs.CV

TL;DR: DIP-GS结合深度图像先验（DIP）改进了3D高斯泼溅（3DGS），解决了稀疏视图重建问题，无需预训练模型。


<details>
  <summary>Details</summary>
Motivation: 3DGS在多视图下表现优异，但在稀疏视图重建中表现不佳，因此需要改进。

Method: 提出DIP-GS，利用DIP先验以粗到细的方式优化3DGS，适用于稀疏视图场景。

Result: DIP-GS在稀疏视图重建任务中取得SOTA竞争性结果。

Conclusion: DIP-GS是一种高效且无需预训练模型的稀疏视图重建方法。

Abstract: 3D Gaussian Splatting (3DGS) is a leading 3D scene reconstruction method,
obtaining high-quality reconstruction with real-time rendering runtime
performance. The main idea behind 3DGS is to represent the scene as a
collection of 3D gaussians, while learning their parameters to fit the given
views of the scene. While achieving superior performance in the presence of
many views, 3DGS struggles with sparse view reconstruction, where the input
views are sparse and do not fully cover the scene and have low overlaps. In
this paper, we propose DIP-GS, a Deep Image Prior (DIP) 3DGS representation. By
using the DIP prior, which utilizes internal structure and patterns, with
coarse-to-fine manner, DIP-based 3DGS can operate in scenarios where vanilla
3DGS fails, such as sparse view recovery. Note that our approach does not use
any pre-trained models such as generative models and depth estimation, but
rather relies only on the input frames. Among such methods, DIP-GS obtains
state-of-the-art (SOTA) competitive results on various sparse-view
reconstruction tasks, demonstrating its capabilities.

</details>


### [106] [LET-US: Long Event-Text Understanding of Scenes](https://arxiv.org/abs/2508.07401)
*Rui Chen,Xingyu Chen,Shaoan Wang,Shihan Kong,Junzhi Yu*

Main category: cs.CV

TL;DR: LET-US是一个用于长事件流-文本理解的框架，通过自适应压缩机制减少输入事件量，同时保留关键视觉细节，提升了跨模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在处理事件流时效果不佳或仅能处理短序列，LET-US旨在解决这一问题，实现长事件流与文本的高效对齐和理解。

Method: 采用两阶段优化范式，结合文本引导的跨模态查询、分层聚类和相似性计算，减少事件特征维度，并使用大规模事件-文本对齐数据集训练模型。

Result: 实验表明，LET-US在长事件流的描述准确性和语义理解上优于现有MLLMs。

Conclusion: LET-US为长事件流与文本的跨模态理解开辟了新方向，相关数据和模型将公开。

Abstract: Event cameras output event streams as sparse, asynchronous data with
microsecond-level temporal resolution, enabling visual perception with low
latency and a high dynamic range. While existing Multimodal Large Language
Models (MLLMs) have achieved significant success in understanding and analyzing
RGB video content, they either fail to interpret event streams effectively or
remain constrained to very short sequences. In this paper, we introduce LET-US,
a framework for long event-stream--text comprehension that employs an adaptive
compression mechanism to reduce the volume of input events while preserving
critical visual details. LET-US thus establishes a new frontier in cross-modal
inferential understanding over extended event sequences. To bridge the
substantial modality gap between event streams and textual representations, we
adopt a two-stage optimization paradigm that progressively equips our model
with the capacity to interpret event-based scenes. To handle the voluminous
temporal information inherent in long event streams, we leverage text-guided
cross-modal queries for feature reduction, augmented by hierarchical clustering
and similarity computation to distill the most representative event features.
Moreover, we curate and construct a large-scale event-text aligned dataset to
train our model, achieving tighter alignment of event features within the LLM
embedding space. We also develop a comprehensive benchmark covering a diverse
set of tasks -- reasoning, captioning, classification, temporal localization
and moment retrieval. Experimental results demonstrate that LET-US outperforms
prior state-of-the-art MLLMs in both descriptive accuracy and semantic
comprehension on long-duration event streams. All datasets, codes, and models
will be publicly available.

</details>


### [107] [ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack](https://arxiv.org/abs/2508.07402)
*Rongxuan Peng,Shunquan Tan,Chenqi Kong,Anwei Luo,Alex C. Kot,Jiwu Huang*

Main category: cs.CV

TL;DR: 论文提出ForensicsSAM框架，通过注入伪造和对抗专家增强模型对抗攻击的鲁棒性，同时提升图像伪造检测和定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调（PEFT）方法在对抗攻击下表现脆弱，需提升其鲁棒性。

Method: 1. 注入伪造专家增强图像编码器；2. 设计轻量级对抗检测器；3. 注入对抗专家修正特征偏移。

Result: ForensicsSAM在对抗攻击下表现优异，同时在伪造检测和定位任务中达到SOTA性能。

Conclusion: ForensicsSAM通过多专家注入和自适应激活机制，显著提升了对抗鲁棒性和任务性能。

Abstract: Parameter-efficient fine-tuning (PEFT) has emerged as a popular strategy for
adapting large vision foundation models, such as the Segment Anything Model
(SAM) and LLaVA, to downstream tasks like image forgery detection and
localization (IFDL). However, existing PEFT-based approaches overlook their
vulnerability to adversarial attacks. In this paper, we show that highly
transferable adversarial images can be crafted solely via the upstream model,
without accessing the downstream model or training data, significantly
degrading the IFDL performance. To address this, we propose ForensicsSAM, a
unified IFDL framework with built-in adversarial robustness. Our design is
guided by three key ideas: (1) To compensate for the lack of forgery-relevant
knowledge in the frozen image encoder, we inject forgery experts into each
transformer block to enhance its ability to capture forgery artifacts. These
forgery experts are always activated and shared across any input images. (2) To
detect adversarial images, we design an light-weight adversary detector that
learns to capture structured, task-specific artifact in RGB domain, enabling
reliable discrimination across various attack methods. (3) To resist
adversarial attacks, we inject adversary experts into the global attention
layers and MLP modules to progressively correct feature shifts induced by
adversarial noise. These adversary experts are adaptively activated by the
adversary detector, thereby avoiding unnecessary interference with clean
images. Extensive experiments across multiple benchmarks demonstrate that
ForensicsSAM achieves superior resistance to various adversarial attack
methods, while also delivering state-of-the-art performance in image-level
forgery detection and pixel-level forgery localization. The resource is
available at https://github.com/siriusPRX/ForensicsSAM.

</details>


### [108] [CharacterShot: Controllable and Consistent 4D Character Animation](https://arxiv.org/abs/2508.07409)
*Junyao Gao,Jiaxing Li,Wenran Liu,Yanhong Zeng,Fei Shen,Kai Chen,Yanan Sun,Cairong Zhao*

Main category: cs.CV

TL;DR: CharacterShot是一个可控且一致的4D角色动画框架，通过单张参考图像和2D姿势序列生成动态3D角色动画。


<details>
  <summary>Details</summary>
Motivation: 旨在让个体设计师能够轻松创建动态3D角色动画，无需复杂工具或专业知识。

Method: 基于DiT的图像到视频模型预训练2D动画模型，引入双注意力模块和相机先验提升到3D，并通过4D高斯溅射优化生成连续稳定的4D表示。

Result: 在新建的CharacterBench基准测试中表现优于现有方法，并构建了大规模数据集Character4D。

Conclusion: CharacterShot为4D角色动画提供了高效且高质量的解决方案，代码和数据集将公开。

Abstract: In this paper, we propose \textbf{CharacterShot}, a controllable and
consistent 4D character animation framework that enables any individual
designer to create dynamic 3D characters (i.e., 4D character animation) from a
single reference character image and a 2D pose sequence. We begin by
pretraining a powerful 2D character animation model based on a cutting-edge
DiT-based image-to-video model, which allows for any 2D pose sequnce as
controllable signal. We then lift the animation model from 2D to 3D through
introducing dual-attention module together with camera prior to generate
multi-view videos with spatial-temporal and spatial-view consistency. Finally,
we employ a novel neighbor-constrained 4D gaussian splatting optimization on
these multi-view videos, resulting in continuous and stable 4D character
representations. Moreover, to improve character-centric performance, we
construct a large-scale dataset Character4D, containing 13,115 unique
characters with diverse appearances and motions, rendered from multiple
viewpoints. Extensive experiments on our newly constructed benchmark,
CharacterBench, demonstrate that our approach outperforms current
state-of-the-art methods. Code, models, and datasets will be publicly available
at https://github.com/Jeoyal/CharacterShot.

</details>


### [109] [CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization](https://arxiv.org/abs/2508.07413)
*Youqi Wang,Shunquan Tan,Rongxuan Peng,Bin Li,Jiwu Huang*

Main category: cs.CV

TL;DR: CLUE利用Stable Diffusion 3和Segment Anything Model，通过噪声注入和参数高效调整，定位高保真伪造区域，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着图像编辑工具和生成式AI的普及，数字媒体的真实性受到威胁，需要更高效的伪造定位工具。

Method: CLUE结合SD3的Rectified Flow机制和LoRA调整，注入噪声放大伪造痕迹，并利用SAM的语义上下文增强定位精度。

Result: CLUE在泛化性能和鲁棒性上显著优于现有方法，对抗常见后处理攻击和社交网络攻击表现优异。

Conclusion: CLUE为伪造定位提供了高效且鲁棒的解决方案，代码已开源。

Abstract: The increasing accessibility of image editing tools and generative AI has led
to a proliferation of visually convincing forgeries, compromising the
authenticity of digital media. In this paper, in addition to leveraging
distortions from conventional forgeries, we repurpose the mechanism of a
state-of-the-art (SOTA) text-to-image synthesis model by exploiting its
internal generative process, turning it into a high-fidelity forgery
localization tool. To this end, we propose CLUE (Capture Latent Uncovered
Evidence), a framework that employs Low- Rank Adaptation (LoRA) to
parameter-efficiently reconfigure Stable Diffusion 3 (SD3) as a forensic
feature extractor. Our approach begins with the strategic use of SD3's
Rectified Flow (RF) mechanism to inject noise at varying intensities into the
latent representation, thereby steering the LoRAtuned denoising process to
amplify subtle statistical inconsistencies indicative of a forgery. To
complement the latent analysis with high-level semantic context and precise
spatial details, our method incorporates contextual features from the image
encoder of the Segment Anything Model (SAM), which is parameter-efficiently
adapted to better trace the boundaries of forged regions. Extensive evaluations
demonstrate CLUE's SOTA generalization performance, significantly outperforming
prior methods. Furthermore, CLUE shows superior robustness against common
post-processing attacks and Online Social Networks (OSNs). Code is publicly
available at https://github.com/SZAISEC/CLUE.

</details>


### [110] [Freeze and Reveal: Exposing Modality Bias in Vision-Language Models](https://arxiv.org/abs/2508.07432)
*Vivek Hruday Kavuri,Vysishtya Karanam,Venkata Jahnavi Venkamsetty,Kriti Madumadukala,Lakshmipathi Balaji Darur,Ponnurangam Kumaraguru*

Main category: cs.CV

TL;DR: 论文研究了视觉语言模型中的性别偏见，提出两种去偏方法（CDA和DAUDoS），并通过实验发现视觉和文本编码器的偏见来源不同。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在训练数据中继承了性别偏见，需要探究偏见的来源并开发高效的去偏方法。

Method: 使用反事实数据增强（CDA）和基于新指标Degree of Stereotypicality的DAUDoS方法进行去偏。

Result: CDA减少性别差距6%，DAUDoS减少3%且仅需三分之一数据；两种方法均提升性别识别准确率3%。

Conclusion: 视觉编码器（CLIP）和文本编码器（PaliGemma2）的偏见来源不同，研究为未来多模态系统的针对性去偏提供了方向。

Abstract: Vision Language Models achieve impressive multi-modal performance but often
inherit gender biases from their training data. This bias might be coming from
both the vision and text modalities. In this work, we dissect the contributions
of vision and text backbones to these biases by applying targeted debiasing
using Counterfactual Data Augmentation and Task Vector methods. Inspired by
data-efficient approaches in hate-speech classification, we introduce a novel
metric, Degree of Stereotypicality and a corresponding debiasing method, Data
Augmentation Using Degree of Stereotypicality - DAUDoS, to reduce bias with
minimal computational cost. We curate a gender annotated dataset and evaluate
all methods on VisoGender benchmark to quantify improvements and identify
dominant source of bias. Our results show that CDA reduces the gender gap by 6%
and DAUDoS by 3% but using only one-third of the data. Both methods also
improve the model's ability to correctly identify gender in images by 3%, with
DAUDoS achieving this improvement using only almost one-third of training data.
From our experiment's, we observed that CLIP's vision encoder is more biased
whereas PaliGemma2's text encoder is more biased. By identifying whether bias
stems more from vision or text encoders, our work enables more targeted and
effective bias mitigation strategies in future multi-modal systems.

</details>


### [111] [Levarging Learning Bias for Noisy Anomaly Detection](https://arxiv.org/abs/2508.07441)
*Yuxin Zhang,Yunkang Cao,Yuqi Cheng,Yihan Sun,Weiming Shen*

Main category: cs.CV

TL;DR: 本文提出了一种两阶段框架，利用模型的学习偏置来解决完全无监督图像异常检测（FUIAD）中训练数据可能包含未标记异常的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设训练数据无异常，但现实数据可能被污染，导致模型将异常误认为正常，降低检测性能。

Method: 通过两阶段框架：第一阶段利用学习偏置（正常样本的统计优势和特征空间差异）过滤出纯净数据集；第二阶段在纯净数据集上训练最终检测器。

Result: 在Real-IAD基准测试中表现出优异的异常检测和定位性能，且对噪声具有鲁棒性。

Conclusion: 该框架模型无关，适用于多种无监督骨干网络，为现实场景中不完美的训练数据提供了实用解决方案。

Abstract: This paper addresses the challenge of fully unsupervised image anomaly
detection (FUIAD), where training data may contain unlabeled anomalies.
Conventional methods assume anomaly-free training data, but real-world
contamination leads models to absorb anomalies as normal, degrading detection
performance. To mitigate this, we propose a two-stage framework that
systematically exploits inherent learning bias in models. The learning bias
stems from: (1) the statistical dominance of normal samples, driving models to
prioritize learning stable normal patterns over sparse anomalies, and (2)
feature-space divergence, where normal data exhibit high intra-class
consistency while anomalies display high diversity, leading to unstable model
responses. Leveraging the learning bias, stage 1 partitions the training set
into subsets, trains sub-models, and aggregates cross-model anomaly scores to
filter a purified dataset. Stage 2 trains the final detector on this dataset.
Experiments on the Real-IAD benchmark demonstrate superior anomaly detection
and localization performance under different noise conditions. Ablation studies
further validate the framework's contamination resilience, emphasizing the
critical role of learning bias exploitation. The model-agnostic design ensures
compatibility with diverse unsupervised backbones, offering a practical
solution for real-world scenarios with imperfect training data. Code is
available at https://github.com/hustzhangyuxin/LLBNAD.

</details>


### [112] [Health Care Waste Classification Using Deep Learning Aligned with Nepal's Bin Color Guidelines](https://arxiv.org/abs/2508.07450)
*Suman Kunwar,Prabesh Rai*

Main category: cs.CV

TL;DR: 研究评估了多种医疗废物分类模型，发现YOLOv5-s在准确率上表现最佳（95.06%），但推理速度略逊于YOLOv8-n。EfficientNet-B0准确率次之（93.22%），但推理时间最长。最终部署了YOLOv5-s模型，并建议进一步优化数据和本地化。


<details>
  <summary>Details</summary>
Motivation: 尼泊尔医疗设施增加导致医疗废物管理挑战加剧，不当分类和处理可能引发污染和疾病传播。

Method: 使用Stratified K-fold技术（5折）评估ResNeXt-50、EfficientNet-B0、MobileNetV3-S、YOLOv8-n和YOLOv5-s模型，并通过重复ANOVA验证统计显著性。

Result: YOLOv5-s准确率最高（95.06%），EfficientNet-B0次之（93.22%），但推理时间最长。YOLOv8-n推理速度最快。

Conclusion: YOLOv5-s被部署为尼泊尔医疗废物分类的解决方案，建议未来优化数据和本地化。

Abstract: The increasing number of Health Care facilities in Nepal has also added up
the challenges on managing health care waste (HCW). Improper segregation and
disposal of HCW leads to the contamination, spreading of infectious diseases
and puts a risk of waste handlers. This study benchmarks the state of the art
waste classification models: ResNeXt-50, EfficientNet-B0, MobileNetV3-S,
YOLOv8-n and YOLOv5-s using Stratified K-fold techniques where we use 5 folds
on combined HCW data, and found that the YOLOv5-s achieved higher of 95.06%
accuracy but fell short few milliseconds in inference speed with YOLOv8-n
model. The EfficientNet-B0 showed promising results of 93.22% accuracy but took
the highest inference time. A repetitive ANOVA was performed to see statistical
significance and the best performing model (YOLOv5-s) was deployed to the web
with mapped bin color using Nepal's HCW management standards for public usage.
Further work on the data was suggested along with localized context.

</details>


### [113] [AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning](https://arxiv.org/abs/2508.07470)
*Siminfar Samakoush Galougah,Rishie Raj,Sanjoy Chowdhury,Sayan Nag,Ramani Duraiswami*

Main category: cs.CV

TL;DR: AURA是一个新的音频-视觉基准测试，旨在评估跨模态推理能力，避免单模态捷径，并引入AuraScore度量推理真实性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅关注答案准确性，忽略了推理过程，无法区分真实理解与错误推理或幻觉导致的正确答案。

Method: AURA包含六个认知领域的问题，设计为无法通过单模态回答，并提出AuraScore度量推理的连贯性和逻辑性。

Result: 评估显示，尽管模型在某些任务上准确率高达92%，但推理一致性得分低于45%，表明模型常通过错误逻辑得出正确答案。

Conclusion: AURA揭示了现有模型的推理缺陷，为更稳健的多模态评估铺平了道路。

Abstract: Current audio-visual (AV) benchmarks focus on final answer accuracy,
overlooking the underlying reasoning process. This makes it difficult to
distinguish genuine comprehension from correct answers derived through flawed
reasoning or hallucinations. To address this, we introduce AURA (Audio-visual
Understanding and Reasoning Assessment), a benchmark for evaluating the
cross-modal reasoning capabilities of Audio-Visual Large Language Models
(AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions across
six challenging cognitive domains, such as causality, timbre and pitch, tempo
and AV synchronization, unanswerability, implicit distractions, and skill
profiling, explicitly designed to be unanswerable from a single modality. This
forces models to construct a valid logical path grounded in both audio and
video, setting AURA apart from AV datasets that allow uni-modal shortcuts. To
assess reasoning traces, we propose a novel metric, AuraScore, which addresses
the lack of robust tools for evaluating reasoning fidelity. It decomposes
reasoning into two aspects: (i) Factual Consistency - whether reasoning is
grounded in perceptual evidence, and (ii) Core Inference - the logical validity
of each reasoning step. Evaluations of SOTA models on AURA reveal a critical
reasoning gap: although models achieve high accuracy (up to 92% on some tasks),
their Factual Consistency and Core Inference scores fall below 45%. This
discrepancy highlights that models often arrive at correct answers through
flawed logic, underscoring the need for our benchmark and paving the way for
more robust multimodal evaluation.

</details>


### [114] [VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding](https://arxiv.org/abs/2508.07493)
*Jian Chen,Ming Li,Jihyung Kil,Chenguang Wang,Tong Yu,Ryan Rossi,Tianyi Zhou,Changyou Chen,Ruiyi Zhang*

Main category: cs.CV

TL;DR: VisR-Bench是一个多语言基准测试，用于长文档中的问题驱动多模态检索，覆盖16种语言和3种问题类型，评估了多种检索模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注英语文档检索或单页图像的多语言问答，无法满足多语言长文档的多模态检索需求。

Method: 构建包含35K QA对和1.2K文档的VisR-Bench基准，涵盖16种语言和3种问题类型，评估文本、多模态编码器和MLLM模型的性能。

Result: MLLM模型显著优于其他模型，但在结构化表格和低资源语言上仍存在挑战。

Conclusion: VisR-Bench填补了多语言长文档多模态检索的空白，揭示了当前模型的局限性，为未来研究提供了方向。

Abstract: Most organizational data in this world are stored as documents, and visual
retrieval plays a crucial role in unlocking the collective intelligence from
all these documents. However, existing benchmarks focus on English-only
document retrieval or only consider multilingual question-answering on a
single-page image. To bridge this gap, we introduce VisR-Bench, a multilingual
benchmark designed for question-driven multimodal retrieval in long documents.
Our benchmark comprises over 35K high-quality QA pairs across 1.2K documents,
enabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans
sixteen languages with three question types (figures, text, and tables),
offering diverse linguistic and question coverage. Unlike prior datasets, we
include queries without explicit answers, preventing models from relying on
superficial keyword matching. We evaluate various retrieval models, including
text-based methods, multimodal encoders, and MLLMs, providing insights into
their strengths and limitations. Our results show that while MLLMs
significantly outperform text-based and multimodal encoder models, they still
struggle with structured tables and low-resource languages, highlighting key
challenges in multilingual visual retrieval.

</details>


### [115] [FormCoach: Lift Smarter, Not Harder](https://arxiv.org/abs/2508.07501)
*Xiaoye Zuo,Nikos Athanasiou,Ginger Delmas,Yiming Huang,Xingyu Fu,Lingjie Liu*

Main category: cs.CV

TL;DR: FormCoach是一个基于视觉语言模型（VLMs）的AI健身教练，通过摄像头实时检测和纠正用户的动作错误，并发布了一个包含1700个专家标注视频对的数据集和评估工具。


<details>
  <summary>Details</summary>
Motivation: 为居家健身爱好者提供专家级的实时反馈，解决专业指导难以获取的问题。

Method: 利用视觉语言模型（VLMs）开发了一个实时交互式AI系统，并通过1700个专家标注的视频对进行模型评估。

Result: 基准测试显示，当前AI模型与人类教练水平仍有显著差距，但为AI驱动的健身教练研究提供了新方向。

Conclusion: FormCoach通过将动作纠正视为人与机器的协作过程，为具身AI开辟了新领域。

Abstract: Good form is the difference between strength and strain, yet for the
fast-growing community of at-home fitness enthusiasts, expert feedback is often
out of reach. FormCoach transforms a simple camera into an always-on,
interactive AI training partner, capable of spotting subtle form errors and
delivering tailored corrections in real time, leveraging vision-language models
(VLMs). We showcase this capability through a web interface and benchmark
state-of-the-art VLMs on a dataset of 1,700 expert-annotated user-reference
video pairs spanning 22 strength and mobility exercises. To accelerate research
in AI-driven coaching, we release both the dataset and an automated,
rubric-based evaluation pipeline, enabling standardized comparison across
models. Our benchmarks reveal substantial gaps compared to human-level
coaching, underscoring both the challenges and opportunities in integrating
nuanced, context-aware movement analysis into interactive AI systems. By
framing form correction as a collaborative and creative process between humans
and machines, FormCoach opens a new frontier in embodied AI.

</details>


### [116] [From Field to Drone: Domain Drift Tolerant Automated Multi-Species and Damage Plant Semantic Segmentation for Herbicide Trials](https://arxiv.org/abs/2508.07514)
*Artzai Picon,Itziar Eguskiza,Daniel Mugica,Javier Romero,Carlos Javier Jimenez,Eric White,Gabriel Do-Lago-Junqueira,Christian Klukas,Ramon Navarra-Mestre*

Main category: cs.CV

TL;DR: 提出了一种结合自监督视觉模型和植物分类学层次推理的改进分割模型，显著提升了除草剂研究中物种和损害识别的自动化效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统手动视觉评估耗时、费力且主观，自动化识别因视觉差异细微而具挑战性，但能显著提升效率和一致性。

Method: 结合自监督视觉模型与植物分类学层次推理，利用多国多年数据集训练，并通过跨设备测试评估模型鲁棒性。

Result: 物种识别（F1-score: 0.52到0.85）和损害分类（F1-score: 0.28到0.44）显著提升，且在无人机图像中表现稳健。

Conclusion: 模型具备强鲁棒性和实际应用价值，已部署于BASF的表型分析流程中，支持大规模自动化作物和杂草监测。

Abstract: Field trials are vital in herbicide research and development to assess
effects on crops and weeds under varied conditions. Traditionally, evaluations
rely on manual visual assessments, which are time-consuming, labor-intensive,
and subjective. Automating species and damage identification is challenging due
to subtle visual differences, but it can greatly enhance efficiency and
consistency.
  We present an improved segmentation model combining a general-purpose
self-supervised visual model with hierarchical inference based on botanical
taxonomy. Trained on a multi-year dataset (2018-2020) from Germany and Spain
using digital and mobile cameras, the model was tested on digital camera data
(year 2023) and drone imagery from the United States, Germany, and Spain (year
2024) to evaluate robustness under domain shift. This cross-device evaluation
marks a key step in assessing generalization across platforms of the model.
  Our model significantly improved species identification (F1-score: 0.52 to
0.85, R-squared: 0.75 to 0.98) and damage classification (F1-score: 0.28 to
0.44, R-squared: 0.71 to 0.87) over prior methods. Under domain shift (drone
images), it maintained strong performance with moderate degradation (species:
F1-score 0.60, R-squared 0.80; damage: F1-score 0.41, R-squared 0.62), where
earlier models failed.
  These results confirm the model's robustness and real-world applicability. It
is now deployed in BASF's phenotyping pipeline, enabling large-scale, automated
crop and weed monitoring across diverse geographies.

</details>


### [117] [Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing](https://arxiv.org/abs/2508.07519)
*Joonghyuk Shin,Alchan Hwang,Yujin Kim,Daneul Kim,Jaesik Park*

Main category: cs.CV

TL;DR: 本文分析了多模态扩散变换器（MM-DiT）的注意力机制，提出了一种基于提示的图像编辑方法，适用于从全局到局部的编辑。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖单向跨注意力机制，而MM-DiT引入了双向信息流，这对现有编辑技术提出了挑战。

Method: 通过分解注意力矩阵为四个块，分析其特性，并提出一种基于提示的编辑方法。

Result: 该方法支持多种MM-DiT变体的图像编辑，包括少步模型。

Conclusion: 研究填补了U-Net方法与新兴架构之间的差距，提供了对MM-DiT行为模式的深入理解。

Abstract: Transformer-based diffusion models have recently superseded traditional U-Net
architectures, with multimodal diffusion transformers (MM-DiT) emerging as the
dominant approach in state-of-the-art models like Stable Diffusion 3 and
Flux.1. Previous approaches have relied on unidirectional cross-attention
mechanisms, with information flowing from text embeddings to image latents. In
contrast, MMDiT introduces a unified attention mechanism that concatenates
input projections from both modalities and performs a single full attention
operation, allowing bidirectional information flow between text and image
branches. This architectural shift presents significant challenges for existing
editing techniques. In this paper, we systematically analyze MM-DiT's attention
mechanism by decomposing attention matrices into four distinct blocks,
revealing their inherent characteristics. Through these analyses, we propose a
robust, prompt-based image editing method for MM-DiT that supports global to
local edits across various MM-DiT variants, including few-step models. We
believe our findings bridge the gap between existing U-Net-based methods and
emerging architectures, offering deeper insights into MMDiT's behavioral
patterns.

</details>


### [118] [Enhancing Reliability of Medical Image Diagnosis through Top-rank Learning with Rejection Module](https://arxiv.org/abs/2508.07528)
*Xiaotong Ji,Ryoma Bise,Seiichi Uchida*

Main category: cs.CV

TL;DR: 提出了一种结合拒绝模块的top-rank学习方法，用于处理医学图像中的噪声标签和类别模糊实例，提高诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 医学图像诊断中，噪声标签和类别模糊实例会严重影响top-rank学习的效果，导致误诊。

Method: 通过集成一个与top-rank损失协同优化的拒绝模块，识别并减少异常值的影响。

Result: 在医学数据集上的实验验证了该方法能有效检测和缓解异常值，提升诊断可靠性。

Conclusion: 该方法显著提高了医学图像诊断的准确性和鲁棒性。

Abstract: In medical image processing, accurate diagnosis is of paramount importance.
Leveraging machine learning techniques, particularly top-rank learning, shows
significant promise by focusing on the most crucial instances. However,
challenges arise from noisy labels and class-ambiguous instances, which can
severely hinder the top-rank objective, as they may be erroneously placed among
the top-ranked instances. To address these, we propose a novel approach that
enhances toprank learning by integrating a rejection module. Cooptimized with
the top-rank loss, this module identifies and mitigates the impact of outliers
that hinder training effectiveness. The rejection module functions as an
additional branch, assessing instances based on a rejection function that
measures their deviation from the norm. Through experimental validation on a
medical dataset, our methodology demonstrates its efficacy in detecting and
mitigating outliers, improving the reliability and accuracy of medical image
diagnoses.

</details>


### [119] [Enhanced Generative Structure Prior for Chinese Text Image Super-resolution](https://arxiv.org/abs/2508.07537)
*Xiaoming Li,Wangmeng Zuo,Chen Change Loy*

Main category: cs.CV

TL;DR: 本文提出了一种针对中文文本图像超分辨率（SR）的高质量框架，通过结构先验和StyleGAN模型结合，准确恢复低分辨率字符的笔画。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注英文文本，对更复杂的中文脚本关注不足。本文旨在解决中文文本图像SR的挑战，特别是字符结构的精确恢复。

Method: 提出了一种结构先验，结合StyleGAN模型，利用代码本机制限制生成空间，确保字符结构的完整性。代码本表示字符结构，StyleGAN控制字符样式。

Result: 实验表明，该方法能准确恢复低分辨率中文字符的清晰笔画，即使在布局不规则的真实场景中也表现良好。

Conclusion: 该框架通过结构先验和StyleGAN的协同作用，为中文文本图像SR提供了有效的解决方案。

Abstract: Faithful text image super-resolution (SR) is challenging because each
character has a unique structure and usually exhibits diverse font styles and
layouts. While existing methods primarily focus on English text, less attention
has been paid to more complex scripts like Chinese. In this paper, we introduce
a high-quality text image SR framework designed to restore the precise strokes
of low-resolution (LR) Chinese characters. Unlike methods that rely on
character recognition priors to regularize the SR task, we propose a novel
structure prior that offers structure-level guidance to enhance visual quality.
Our framework incorporates this structure prior within a StyleGAN model,
leveraging its generative capabilities for restoration. To maintain the
integrity of character structures while accommodating various font styles and
layouts, we implement a codebook-based mechanism that restricts the generative
space of StyleGAN. Each code in the codebook represents the structure of a
specific character, while the vector $w$ in StyleGAN controls the character's
style, including typeface, orientation, and location. Through the collaborative
interaction between the codebook and style, we generate a high-resolution
structure prior that aligns with LR characters both spatially and structurally.
Experiments demonstrate that this structure prior provides robust,
character-specific guidance, enabling the accurate restoration of clear strokes
in degraded characters, even for real-world LR Chinese text with irregular
layouts. Our code and pre-trained models will be available at
https://github.com/csxmli2016/MARCONetPlusPlus

</details>


### [120] [A DICOM Image De-identification Algorithm in the MIDI-B Challenge](https://arxiv.org/abs/2508.07538)
*Hongzhu Jiang,Sihan Xie,Zhiyu Wan*

Main category: cs.CV

TL;DR: 论文探讨了医学图像去标识化的重要性，介绍了MIDI-B挑战赛及其方法，展示了算法的优异表现，并分析了当前方法的局限性和未来改进方向。


<details>
  <summary>Details</summary>
Motivation: 医学图像共享需遵守隐私法规（如HIPAA、DICOM PS3.15），去标识化是保护患者隐私的关键。

Method: 采用像素掩码、日期偏移、哈希、文本识别与替换等方法，严格遵循标准处理数据。

Result: 算法在MIDI-B挑战赛中正确执行99.92%的操作，排名第二。

Conclusion: 当前方法表现优异，但仍需改进，未来研究应关注进一步提升去标识化效果。

Abstract: Image de-identification is essential for the public sharing of medical
images, particularly in the widely used Digital Imaging and Communications in
Medicine (DICOM) format as required by various regulations and standards,
including Health Insurance Portability and Accountability Act (HIPAA) privacy
rules, the DICOM PS3.15 standard, and best practices recommended by the Cancer
Imaging Archive (TCIA). The Medical Image De-Identification Benchmark (MIDI-B)
Challenge at the 27th International Conference on Medical Image Computing and
Computer Assisted Intervention (MICCAI 2024) was organized to evaluate
rule-based DICOM image de-identification algorithms with a large dataset of
clinical DICOM images. In this report, we explore the critical challenges of
de-identifying DICOM images, emphasize the importance of removing personally
identifiable information (PII) to protect patient privacy while ensuring the
continued utility of medical data for research, diagnostics, and treatment, and
provide a comprehensive overview of the standards and regulations that govern
this process. Additionally, we detail the de-identification methods we applied
- such as pixel masking, date shifting, date hashing, text recognition, text
replacement, and text removal - to process datasets during the test phase in
strict compliance with these standards. According to the final leaderboard of
the MIDI-B challenge, the latest version of our solution algorithm correctly
executed 99.92% of the required actions and ranked 2nd out of 10 teams that
completed the challenge (from a total of 22 registered teams). Finally, we
conducted a thorough analysis of the resulting statistics and discussed the
limitations of current approaches and potential avenues for future improvement.

</details>


### [121] [Domain Generalization of Pathological Image Segmentation by Patch-Level and WSI-Level Contrastive Learning](https://arxiv.org/abs/2508.07539)
*Yuki Shigeyasu,Shota Harada,Akihiko Yoshizawa,Kazuhiro Terada,Naoki Nakazima,Mariyo Kurata,Hiroyuki Abe,Tetsuo Ushiku,Ryoma Bise*

Main category: cs.CV

TL;DR: 提出一种针对病理图像中域偏移的方法，利用WSI级特征聚类和对比学习减少特征差异。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖多医院数据，但数据收集困难。本文关注WSI内部的域偏移（如患者特征和组织厚度）。

Method: 通过聚类非肿瘤区域的WSI级特征作为域，并采用两阶段对比学习（WSI级和patch级）减少域间特征差异。

Result: 有效减少了WSI内部域偏移带来的特征差异。

Conclusion: 该方法为病理图像域泛化提供了新思路，尤其适用于数据收集受限的场景。

Abstract: In this paper, we address domain shifts in pathological images by focusing on
shifts within whole slide images~(WSIs), such as patient characteristics and
tissue thickness, rather than shifts between hospitals. Traditional approaches
rely on multi-hospital data, but data collection challenges often make this
impractical. Therefore, the proposed domain generalization method captures and
leverages intra-hospital domain shifts by clustering WSI-level features from
non-tumor regions and treating these clusters as domains. To mitigate domain
shift, we apply contrastive learning to reduce feature gaps between WSI pairs
from different clusters. The proposed method introduces a two-stage contrastive
learning approach WSI-level and patch-level contrastive learning to minimize
these gaps effectively.

</details>


### [122] [CoT-Pose: Chain-of-Thought Reasoning for 3D Pose Generation from Abstract Prompts](https://arxiv.org/abs/2508.07540)
*Junuk Cha,Jihyeon Kim*

Main category: cs.CV

TL;DR: 提出了一种结合链式思维推理的3D人体姿态生成框架CoT-Pose，解决了现有模型依赖低层次描述的问题，通过高层次抽象语言生成准确姿态。


<details>
  <summary>Details</summary>
Motivation: 现有文本到姿态生成模型依赖低层次详细描述，而人类更倾向于使用高层次抽象语言，导致实际应用中的不匹配。

Method: 引入链式思维推理框架，将抽象提示转化为准确3D姿态，并提出数据合成管道生成训练所需的三元组（抽象提示、详细提示、3D姿态）。

Result: 实验表明，CoT-Pose能从抽象文本输入生成语义对齐的合理姿态。

Conclusion: 强调了高层次理解在姿态生成中的重要性，为推理增强方法开辟了新方向。

Abstract: Recent advances in multi-modal large language models (MLLMs) and
chain-of-thought (CoT) reasoning have led to significant progress in image and
text generation tasks. However, the field of 3D human pose generation still
faces critical limitations. Most existing text-to-pose models rely heavily on
detailed (low-level) prompts that explicitly describe joint configurations. In
contrast, humans tend to communicate actions and intentions using abstract
(high-level) language. This mismatch results in a practical challenge for
deploying pose generation systems in real-world scenarios. To bridge this gap,
we introduce a novel framework that incorporates CoT reasoning into the pose
generation process, enabling the interpretation of abstract prompts into
accurate 3D human poses. We further propose a data synthesis pipeline that
automatically generates triplets of abstract prompts, detailed prompts, and
corresponding 3D poses for training process. Experimental results demonstrate
that our reasoning-enhanced model, CoT-Pose, can effectively generate plausible
and semantically aligned poses from abstract textual inputs. This work
highlights the importance of high-level understanding in pose generation and
opens new directions for reasoning-enhanced approach for human pose generation.

</details>


### [123] [Commentary Generation for Soccer Highlights](https://arxiv.org/abs/2508.07543)
*Chidaksh Ravuru*

Main category: cs.CV

TL;DR: 本文扩展了MatchVoice模型，用于生成足球集锦的实时解说，通过实验验证了其性能，并探讨了训练配置和零样本表现的影响。


<details>
  <summary>Details</summary>
Motivation: 现有系统在视频内容与解说之间的细粒度对齐方面存在不足，MatchVoice通过粗粒度和细粒度对齐技术改善了这一问题。

Method: 扩展MatchVoice模型，使用GOAL数据集进行实验，评估不同训练配置和硬件限制的影响，并探索窗口大小对零样本性能的作用。

Result: 实验表明MatchVoice具有较好的泛化能力，但需要整合更广泛的视频-语言技术以进一步提升性能。

Conclusion: MatchVoice在足球集锦解说生成中表现良好，但未来需结合更多领域技术以优化效果。

Abstract: Automated soccer commentary generation has evolved from template-based
systems to advanced neural architectures, aiming to produce real-time
descriptions of sports events. While frameworks like SoccerNet-Caption laid
foundational work, their inability to achieve fine-grained alignment between
video content and commentary remains a significant challenge. Recent efforts
such as MatchTime, with its MatchVoice model, address this issue through coarse
and fine-grained alignment techniques, achieving improved temporal
synchronization. In this paper, we extend MatchVoice to commentary generation
for soccer highlights using the GOAL dataset, which emphasizes short clips over
entire games. We conduct extensive experiments to reproduce the original
MatchTime results and evaluate our setup, highlighting the impact of different
training configurations and hardware limitations. Furthermore, we explore the
effect of varying window sizes on zero-shot performance. While MatchVoice
exhibits promising generalization capabilities, our findings suggest the need
for integrating techniques from broader video-language domains to further
enhance performance. Our code is available at
https://github.com/chidaksh/SoccerCommentary.

</details>


### [124] [Adaptive Pseudo Label Selection for Individual Unlabeled Data by Positive and Unlabeled Learning](https://arxiv.org/abs/2508.07548)
*Takehiro Yamane,Itaru Tsuge,Susumu Saito,Ryoma Bise*

Main category: cs.CV

TL;DR: 提出一种基于PU学习的伪标签方法，用于医学图像分割，通过个体图像学习选择有效伪标签。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中伪标签选择的问题，特别是在背景区域多样化的场景下。

Method: 采用PU学习（仅使用正样本和未标记数据）来区分前景和背景区域，并选择有效的伪标签。

Result: 实验结果表明该方法在医学图像分割中具有有效性。

Conclusion: 提出的PU学习方法能够有效选择伪标签，适用于多样化的背景区域。

Abstract: This paper proposes a novel pseudo-labeling method for medical image
segmentation that can perform learning on ``individual images'' to select
effective pseudo-labels. We introduce Positive and Unlabeled Learning (PU
learning), which uses only positive and unlabeled data for binary
classification problems, to obtain the appropriate metric for discriminating
foreground and background regions on each unlabeled image. Our PU learning
makes us easy to select pseudo-labels for various background regions. The
experimental results show the effectiveness of our method.

</details>


### [125] [Decoupled Functional Evaluation of Autonomous Driving Models via Feature Map Quality Scoring](https://arxiv.org/abs/2508.07552)
*Ludan Zhang,Sihan Wang,Yuqi Dai,Shuofei Qiao,Lei He*

Main category: cs.CV

TL;DR: 本文提出了一种基于特征图收敛分数（FMCS）的独立评估方法，通过双粒度动态加权评分系统（DG-DWSS）和CLIP-FMQE-Net网络，实现对功能模块生成特征图质量的实时评估，并在NuScenes数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 端到端模型在自动驾驶感知和规划中成为主流，但缺乏对中间功能模块的显式监督信号，导致机制不透明且可解释性有限，传统方法难以独立评估和训练这些模块。

Method: 基于特征图-真值表示相似性评估框架，提出FMCS评估方法，构建DG-DWSS评分系统，并开发CLIP-FMQE-Net网络，结合特征-真值编码器和质量评分预测头，实现实时质量分析。

Result: 在NuScenes数据集上的实验表明，该方法提高了3D目标检测性能，NDS提升了3.89%。

Conclusion: 该方法有效提升了特征表示质量和整体模型性能，验证了其可行性和实用性。

Abstract: End-to-end models are emerging as the mainstream in autonomous driving
perception and planning. However, the lack of explicit supervision signals for
intermediate functional modules leads to opaque operational mechanisms and
limited interpretability, making it challenging for traditional methods to
independently evaluate and train these modules. Pioneering in the issue, this
study builds upon the feature map-truth representation similarity-based
evaluation framework and proposes an independent evaluation method based on
Feature Map Convergence Score (FMCS). A Dual-Granularity Dynamic Weighted
Scoring System (DG-DWSS) is constructed, formulating a unified quantitative
metric - Feature Map Quality Score - to enable comprehensive evaluation of the
quality of feature maps generated by functional modules. A CLIP-based Feature
Map Quality Evaluation Network (CLIP-FMQE-Net) is further developed, combining
feature-truth encoders and quality score prediction heads to enable real-time
quality analysis of feature maps generated by functional modules. Experimental
results on the NuScenes dataset demonstrate that integrating our evaluation
module into the training improves 3D object detection performance, achieving a
3.89 percent gain in NDS. These results verify the effectiveness of our method
in enhancing feature representation quality and overall model performance.

</details>


### [126] [Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and Spatially Consistent Content Creation](https://arxiv.org/abs/2508.07557)
*Minghao Yin,Yukang Cao,Songyou Peng,Kai Han*

Main category: cs.CV

TL;DR: Splat4D是一个从单目视频生成高质量4D内容的新框架，通过多视角渲染、不一致性识别、视频扩散模型和非对称U-Net优化，实现了时空一致性和高保真度。


<details>
  <summary>Details</summary>
Motivation: 解决从单目视频生成4D内容时面临的时空一致性、细节保留和用户指导有效性等挑战。

Method: 利用多视角渲染、不一致性识别、视频扩散模型和非对称U-Net进行优化。

Result: 在公开基准测试中表现优异，支持多种应用如文本/图像条件4D生成和内容编辑。

Conclusion: Splat4D在4D内容生成中表现出高效性和多功能性，适用于多种场景。

Abstract: Generating high-quality 4D content from monocular videos for applications
such as digital humans and AR/VR poses challenges in ensuring temporal and
spatial consistency, preserving intricate details, and incorporating user
guidance effectively. To overcome these challenges, we introduce Splat4D, a
novel framework enabling high-fidelity 4D content generation from a monocular
video. Splat4D achieves superior performance while maintaining faithful
spatial-temporal coherence by leveraging multi-view rendering, inconsistency
identification, a video diffusion model, and an asymmetric U-Net for
refinement. Through extensive evaluations on public benchmarks, Splat4D
consistently demonstrates state-of-the-art performance across various metrics,
underscoring the efficacy of our approach. Additionally, the versatility of
Splat4D is validated in various applications such as text/image conditioned 4D
generation, 4D human generation, and text-guided content editing, producing
coherent outcomes following user instructions.

</details>


### [127] [Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language Models](https://arxiv.org/abs/2508.07570)
*Khanh-Binh Nguyen,Phuoc-Nguyen Bui,Hyunseung Choo,Duc Thanh Nguyen*

Main category: cs.CV

TL;DR: ACE框架通过动态缓存和自适应决策边界，提升了视觉语言模型在分布偏移下的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在分布偏移下性能下降，现有缓存方法因不可靠的置信度指标和刚性决策边界而受限。

Method: 提出ACE框架，动态存储高置信度或低熵图像嵌入，并使用动态阈值和指数移动平均优化缓存。

Result: 在15个基准数据集上，ACE表现优于现有方法，具有更强的鲁棒性和泛化能力。

Conclusion: ACE通过自适应缓存和决策边界，显著提升了视觉语言模型在分布偏移下的性能。

Abstract: Vision-language models (VLMs) exhibit remarkable zero-shot generalization but
suffer performance degradation under distribution shifts in downstream tasks,
particularly in the absence of labeled data. Test-Time Adaptation (TTA)
addresses this challenge by enabling online optimization of VLMs during
inference, eliminating the need for annotated data. Cache-based TTA methods
exploit historical knowledge by maintaining a dynamic memory cache of
low-entropy or high-confidence samples, promoting efficient adaptation to
out-of-distribution data. Nevertheless, these methods face two critical
challenges: (1) unreliable confidence metrics under significant distribution
shifts, resulting in error accumulation within the cache and degraded
adaptation performance; and (2) rigid decision boundaries that fail to
accommodate substantial distributional variations, leading to suboptimal
predictions. To overcome these limitations, we introduce the Adaptive Cache
Enhancement (ACE) framework, which constructs a robust cache by selectively
storing high-confidence or low-entropy image embeddings per class, guided by
dynamic, class-specific thresholds initialized from zero-shot statistics and
iteratively refined using an exponential moving average and
exploration-augmented updates. This approach enables adaptive, class-wise
decision boundaries, ensuring robust and accurate predictions across diverse
visual distributions. Extensive experiments on 15 diverse benchmark datasets
demonstrate that ACE achieves state-of-the-art performance, delivering superior
robustness and generalization compared to existing TTA methods in challenging
out-of-distribution scenarios.

</details>


### [128] [Exploiting Layer Normalization Fine-tuning in Visual Transformer Foundation Models for Classification](https://arxiv.org/abs/2508.07577)
*Zhaorui Tan,Tan Pan,Kaizhu Huang,Weimiao Yu,Kai Yao,Chen Jiang,Qiufeng Wang,Anh Nguyen,Xin Guo,Yuan Cheng,Xi Yang*

Main category: cs.CV

TL;DR: 本文研究了LayerNorm在ViTs中的微调动态，提出了一种基于Fine-tuning Shift Ratio（FSR）的重新缩放机制，以优化目标域适应性。


<details>
  <summary>Details</summary>
Motivation: 探索LayerNorm在数据稀缺和域转移下的微调行为，填补研究空白。

Method: 提出FSR量化目标域代表性，设计标量λ和循环框架优化LayerNorm微调。

Result: 实验验证了框架的有效性，OOD任务FSR较低且λ较高，病理数据表现类似ID。

Conclusion: 揭示了LayerNorm在迁移学习中的动态特性，提供了实用的微调策略。

Abstract: LayerNorm is pivotal in Vision Transformers (ViTs), yet its fine-tuning
dynamics under data scarcity and domain shifts remain underexplored. This paper
shows that shifts in LayerNorm parameters after fine-tuning (LayerNorm shifts)
are indicative of the transitions between source and target domains; its
efficacy is contingent upon the degree to which the target training samples
accurately represent the target domain, as quantified by our proposed
Fine-tuning Shift Ratio ($FSR$). Building on this, we propose a simple yet
effective rescaling mechanism using a scalar $\lambda$ that is negatively
correlated to $FSR$ to align learned LayerNorm shifts with those ideal shifts
achieved under fully representative data, combined with a cyclic framework that
further enhances the LayerNorm fine-tuning. Extensive experiments across
natural and pathological images, in both in-distribution (ID) and
out-of-distribution (OOD) settings, and various target training sample regimes
validate our framework. Notably, OOD tasks tend to yield lower $FSR$ and higher
$\lambda$ in comparison to ID cases, especially with scarce data, indicating
under-represented target training samples. Moreover, ViTFs fine-tuned on
pathological data behave more like ID settings, favoring conservative LayerNorm
updates. Our findings illuminate the underexplored dynamics of LayerNorm in
transfer learning and provide practical strategies for LayerNorm fine-tuning.

</details>


### [129] [GAPNet: A Lightweight Framework for Image and Video Salient Object Detection via Granularity-Aware Paradigm](https://arxiv.org/abs/2508.07585)
*Yu-Huan Wu,Wei Liu,Zi-Xuan Zhu,Zizhou Wang,Yong Liu,Liangli Zhen*

Main category: cs.CV

TL;DR: GAPNet是一种轻量级网络，基于粒度感知范式，用于图像和视频显著目标检测（SOD），通过多尺度解码器输出和高效特征融合模块，实现了高性能且低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有SOD模型依赖重型主干网络，计算成本高，限制了在边缘设备等实际场景中的应用。

Method: 采用粒度感知连接、粒度金字塔卷积（GPC）和跨尺度注意力（CSA）模块，结合自注意力模块优化特征利用和语义解释。

Result: 在轻量级图像和视频SOD模型中达到了新的最先进性能。

Conclusion: GAPNet通过高效的特征融合和适当的监督策略，显著提升了轻量级SOD模型的性能。

Abstract: Recent salient object detection (SOD) models predominantly rely on
heavyweight backbones, incurring substantial computational cost and hindering
their practical application in various real-world settings, particularly on
edge devices. This paper presents GAPNet, a lightweight network built on the
granularity-aware paradigm for both image and video SOD. We assign saliency
maps of different granularities to supervise the multi-scale decoder
side-outputs: coarse object locations for high-level outputs and fine-grained
object boundaries for low-level outputs. Specifically, our decoder is built
with granularity-aware connections which fuse high-level features of low
granularity and low-level features of high granularity, respectively. To
support these connections, we design granular pyramid convolution (GPC) and
cross-scale attention (CSA) modules for efficient fusion of low-scale and
high-scale features, respectively. On top of the encoder, a self-attention
module is built to learn global information, enabling accurate object
localization with negligible computational cost. Unlike traditional U-Net-based
approaches, our proposed method optimizes feature utilization and semantic
interpretation while applying appropriate supervision at each processing stage.
Extensive experiments show that the proposed method achieves a new
state-of-the-art performance among lightweight image and video SOD models. Code
is available at https://github.com/yuhuan-wu/GAPNet.

</details>


### [130] [Voice Pathology Detection Using Phonation](https://arxiv.org/abs/2508.07587)
*Sri Raksha Siva,Nived Suthahar,Prakash Boominathan,Uma Ranjan*

Main category: cs.CV

TL;DR: 提出了一种基于机器学习的非侵入性方法，利用声学特征和RNN分类语音病理，提高了诊断的准确性和可及性。


<details>
  <summary>Details</summary>
Motivation: 语音障碍严重影响生活质量，传统诊断方法侵入性强且主观，需要非侵入、自动化的解决方案。

Method: 使用MFCCs、chroma特征和Mel频谱图分析语音数据，结合LSTM和注意力机制的RNN进行分类，并通过数据增强提高泛化能力。

Result: 框架提供了一种非侵入性、自动化的语音病理诊断工具，支持AI驱动的医疗保健。

Conclusion: 该方法为早期语音病理检测提供了高效、准确的解决方案，改善了患者预后。

Abstract: Voice disorders significantly affect communication and quality of life,
requiring an early and accurate diagnosis. Traditional methods like
laryngoscopy are invasive, subjective, and often inaccessible. This research
proposes a noninvasive, machine learning-based framework for detecting voice
pathologies using phonation data.
  Phonation data from the Saarbr\"ucken Voice Database are analyzed using
acoustic features such as Mel Frequency Cepstral Coefficients (MFCCs), chroma
features, and Mel spectrograms. Recurrent Neural Networks (RNNs), including
LSTM and attention mechanisms, classify samples into normal and pathological
categories. Data augmentation techniques, including pitch shifting and Gaussian
noise addition, enhance model generalizability, while preprocessing ensures
signal quality. Scale-based features, such as H\"older and Hurst exponents,
further capture signal irregularities and long-term dependencies.
  The proposed framework offers a noninvasive, automated diagnostic tool for
early detection of voice pathologies, supporting AI-driven healthcare, and
improving patient outcomes.

</details>


### [131] [From Prediction to Explanation: Multimodal, Explainable, and Interactive Deepfake Detection Framework for Non-Expert Users](https://arxiv.org/abs/2508.07596)
*Shahroz Tariq,Simon S. Woo,Priyanka Singh,Irena Irmalasari,Saakshi Gupta,Dev Gupta*

Main category: cs.CV

TL;DR: DF-P2E是一个多模态框架，通过视觉、语义和叙事层解释，使深度伪造检测更具可解释性和可访问性。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术对数字完整性构成严重威胁，现有检测系统缺乏透明度和可解释性，限制了其在实际决策中的应用。

Method: DF-P2E包含三个模块：基于Grad-CAM的深度伪造分类器、视觉描述模块和叙事优化模块，结合LLM生成用户敏感的上下文解释。

Result: 在DF40数据集上，系统实现了竞争性的检测性能，并提供与Grad-CAM激活一致的高质量解释。

Conclusion: DF-P2E通过统一预测和解释，为可信赖和透明的AI系统在对抗性媒体环境中提供了可扩展的解决方案。

Abstract: The proliferation of deepfake technologies poses urgent challenges and
serious risks to digital integrity, particularly within critical sectors such
as forensics, journalism, and the legal system. While existing detection
systems have made significant progress in classification accuracy, they
typically function as black-box models, offering limited transparency and
minimal support for human reasoning. This lack of interpretability hinders
their usability in real-world decision-making contexts, especially for
non-expert users. In this paper, we present DF-P2E (Deepfake: Prediction to
Explanation), a novel multimodal framework that integrates visual, semantic,
and narrative layers of explanation to make deepfake detection interpretable
and accessible. The framework consists of three modular components: (1) a
deepfake classifier with Grad-CAM-based saliency visualisation, (2) a visual
captioning module that generates natural language summaries of manipulated
regions, and (3) a narrative refinement module that uses a fine-tuned Large
Language Model (LLM) to produce context-aware, user-sensitive explanations. We
instantiate and evaluate the framework on the DF40 benchmark, the most diverse
deepfake dataset to date. Experiments demonstrate that our system achieves
competitive detection performance while providing high-quality explanations
aligned with Grad-CAM activations. By unifying prediction and explanation in a
coherent, human-aligned pipeline, this work offers a scalable approach to
interpretable deepfake detection, advancing the broader vision of trustworthy
and transparent AI systems in adversarial media environments.

</details>


### [132] [ShoulderShot: Generating Over-the-Shoulder Dialogue Videos](https://arxiv.org/abs/2508.07597)
*Yuang Zhang,Junqi Cheng,Haoyu Zhao,Jiaxi Gu,Fangyuan Zou,Zenghui Lu,Peng Shu*

Main category: cs.CV

TL;DR: ShoulderShot框架通过双镜头生成和循环视频技术，解决了对话视频生成中的角色一致性和空间连续性挑战，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 对话视频在影视作品中至关重要，但现有研究对其生成技术探索不足，尤其是在角色一致性、空间连续性和长对话生成方面。

Method: 结合双镜头生成与循环视频技术，实现角色一致性和空间连续性，支持长对话生成。

Result: 在镜头反转布局、空间连续性和对话长度灵活性方面优于现有方法。

Conclusion: ShoulderShot为实用对话视频生成提供了新可能性。

Abstract: Over-the-shoulder dialogue videos are essential in films, short dramas, and
advertisements, providing visual variety and enhancing viewers' emotional
connection. Despite their importance, such dialogue scenes remain largely
underexplored in video generation research. The main challenges include
maintaining character consistency across different shots, creating a sense of
spatial continuity, and generating long, multi-turn dialogues within limited
computational budgets. Here, we present ShoulderShot, a framework that combines
dual-shot generation with looping video, enabling extended dialogues while
preserving character consistency. Our results demonstrate capabilities that
surpass existing methods in terms of shot-reverse-shot layout, spatial
continuity, and flexibility in dialogue length, thereby opening up new
possibilities for practical dialogue video generation. Videos and comparisons
are available at https://shouldershot.github.io.

</details>


### [133] [LaVieID: Local Autoregressive Diffusion Transformers for Identity-Preserving Video Creation](https://arxiv.org/abs/2508.07603)
*Wenhui Song,Hanhui Li,Jiehui Huang,Panwen Hu,Yuhao Cheng,Long Chen,Yiqiang Yan,Xiaodan Liang*

Main category: cs.CV

TL;DR: LaVieID是一种新的局部自回归视频扩散框架，旨在解决身份保持的文本到视频任务，通过空间和时间视角减少身份信息损失。


<details>
  <summary>Details</summary>
Motivation: 解决扩散变换器（DiTs）在全局生成过程中身份信息丢失的问题。

Method: 引入局部路由器明确表示潜在状态，并通过时间自回归模块增强帧间身份一致性。

Result: LaVieID能生成高保真个性化视频，并达到最先进性能。

Conclusion: LaVieID通过局部和时间的优化，显著提升了身份保持视频生成的效果。

Abstract: In this paper, we present LaVieID, a novel \underline{l}ocal
\underline{a}utoregressive \underline{vi}d\underline{e}o diffusion framework
designed to tackle the challenging \underline{id}entity-preserving
text-to-video task. The key idea of LaVieID is to mitigate the loss of identity
information inherent in the stochastic global generation process of diffusion
transformers (DiTs) from both spatial and temporal perspectives. Specifically,
unlike the global and unstructured modeling of facial latent states in existing
DiTs, LaVieID introduces a local router to explicitly represent latent states
by weighted combinations of fine-grained local facial structures. This
alleviates undesirable feature interference and encourages DiTs to capture
distinctive facial characteristics. Furthermore, a temporal autoregressive
module is integrated into LaVieID to refine denoised latent tokens before video
decoding. This module divides latent tokens temporally into chunks, exploiting
their long-range temporal dependencies to predict biases for rectifying tokens,
thereby significantly enhancing inter-frame identity consistency. Consequently,
LaVieID can generate high-fidelity personalized videos and achieve
state-of-the-art performance. Our code and models are available at
https://github.com/ssugarwh/LaVieID.

</details>


### [134] [X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning](https://arxiv.org/abs/2508.07607)
*Jian Ma,Xujie Zhu,Zihao Pan,Qirong Peng,Xu Guo,Chen Chen,Haonan Lu*

Main category: cs.CV

TL;DR: 论文提出了X2Edit数据集和任务感知的MoE-LoRA训练方法，解决了开源图像编辑数据集的不足，并提升了与社区生成模型的兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有开源图像编辑数据集质量不高，且缺乏与主流生成模型的兼容模块。

Method: 1. 构建X2Edit数据集，覆盖14种编辑任务；2. 设计任务感知的MoE-LoRA训练方法；3. 引入对比学习优化性能。

Result: 模型编辑性能优异，数据集显著优于现有开源数据集。

Conclusion: X2Edit为图像编辑任务提供了高质量数据和高效训练方法。

Abstract: Existing open-source datasets for arbitrary-instruction image editing remain
suboptimal, while a plug-and-play editing module compatible with
community-prevalent generative models is notably absent. In this paper, we
first introduce the X2Edit Dataset, a comprehensive dataset covering 14 diverse
editing tasks, including subject-driven generation. We utilize the
industry-leading unified image generation models and expert models to construct
the data. Meanwhile, we design reasonable editing instructions with the VLM and
implement various scoring mechanisms to filter the data. As a result, we
construct 3.7 million high-quality data with balanced categories. Second, to
better integrate seamlessly with community image generation models, we design
task-aware MoE-LoRA training based on FLUX.1, with only 8\% of the parameters
of the full model. To further improve the final performance, we utilize the
internal representations of the diffusion model and define positive/negative
samples based on image editing types to introduce contrastive learning.
Extensive experiments demonstrate that the model's editing performance is
competitive among many excellent models. Additionally, the constructed dataset
exhibits substantial advantages over existing open-source datasets. The
open-source code, checkpoints, and datasets for X2Edit can be found at the
following link: https://github.com/OPPO-Mente-Lab/X2Edit.

</details>


### [135] [An Iterative Reconstruction Method for Dental Cone-Beam Computed Tomography with a Truncated Field of View](https://arxiv.org/abs/2508.07618)
*Hyoung Suk Park,Kiwan Jeon*

Main category: cs.CV

TL;DR: 提出一种两阶段方法，通过隐式神经表示（INR）生成先验图像并校正投影数据，以减少牙科CBCT中的截断伪影。


<details>
  <summary>Details</summary>
Motivation: 小型探测器导致视野截断，影响迭代重建图像质量。

Method: 第一阶段用INR生成先验图像并校正投影数据；第二阶段用校正数据进行常规迭代重建。

Result: 数值结果表明该方法有效抑制截断伪影，提升图像质量。

Conclusion: 两阶段方法显著改善牙科CBCT的截断问题。

Abstract: In dental cone-beam computed tomography (CBCT), compact and cost-effective
system designs often use small detectors, resulting in a truncated field of
view (FOV) that does not fully encompass the patient's head. In iterative
reconstruction approaches, the discrepancy between the actual projection and
the forward projection within the truncated FOV accumulates over iterations,
leading to significant degradation in the reconstructed image quality. In this
study, we propose a two-stage approach to mitigate truncation artifacts in
dental CBCT. In the first stage, we employ Implicit Neural Representation
(INR), leveraging its superior representation power, to generate a prior image
over an extended region so that its forward projection fully covers the
patient's head. To reduce computational and memory burdens, INR reconstruction
is performed with a coarse voxel size. The forward projection of this prior
image is then used to estimate the discrepancy due to truncated FOV in the
measured projection data. In the second stage, the discrepancy-corrected
projection data is utilized in a conventional iterative reconstruction process
within the truncated region. Our numerical results demonstrate that the
proposed two-grid approach effectively suppresses truncation artifacts, leading
to improved CBCT image quality.

</details>


### [136] [SOFA: Deep Learning Framework for Simulating and Optimizing Atrial Fibrillation Ablation](https://arxiv.org/abs/2508.07621)
*Yunsung Chung,Chanho Lim,Ghassan Bidaoui,Christian Massad,Nassir Marrouche,Jihun Hamm*

Main category: cs.CV

TL;DR: SOFA是一个深度学习框架，用于模拟和优化心房颤动消融手术，预测复发风险并优化手术参数，以减少复发。


<details>
  <summary>Details</summary>
Motivation: 由于患者特异性组织和手术因素的复杂交互，评估和改进消融手术效果具有挑战性。SOFA旨在通过模拟手术效果和优化参数来解决这一问题。

Method: SOFA通过生成消融后的疤痕图像模拟手术效果，并结合多模态、多视图生成器处理2.5D心房表示，预测复发风险并优化手术参数。

Result: SOFA能准确合成消融后图像，优化方案使模型预测的复发风险降低22.18%。

Conclusion: SOFA是首个整合手术效果模拟、复发预测和参数优化的框架，为个性化心房颤动消融提供了新工具。

Abstract: Atrial fibrillation (AF) is a prevalent cardiac arrhythmia often treated with
catheter ablation procedures, but procedural outcomes are highly variable.
Evaluating and improving ablation efficacy is challenging due to the complex
interaction between patient-specific tissue and procedural factors. This paper
asks two questions: Can AF recurrence be predicted by simulating the effects of
procedural parameters? How should we ablate to reduce AF recurrence? We propose
SOFA (Simulating and Optimizing Atrial Fibrillation Ablation), a novel
deep-learning framework that addresses these questions. SOFA first simulates
the outcome of an ablation strategy by generating a post-ablation image
depicting scar formation, conditioned on a patient's pre-ablation LGE-MRI and
the specific procedural parameters used (e.g., ablation locations, duration,
temperature, power, and force). During this simulation, it predicts AF
recurrence risk. Critically, SOFA then introduces an optimization scheme that
refines these procedural parameters to minimize the predicted risk. Our method
leverages a multi-modal, multi-view generator that processes 2.5D
representations of the atrium. Quantitative evaluations show that SOFA
accurately synthesizes post-ablation images and that our optimization scheme
leads to a 22.18\% reduction in the model-predicted recurrence risk. To the
best of our knowledge, SOFA is the first framework to integrate the simulation
of procedural effects, recurrence prediction, and parameter optimization,
offering a novel tool for personalizing AF ablation.

</details>


### [137] [Enhancing Egocentric Object Detection in Static Environments using Graph-based Spatial Anomaly Detection and Correction](https://arxiv.org/abs/2508.07624)
*Vishakha Lall,Yisi Liu*

Main category: cs.CV

TL;DR: 提出一种基于图神经网络的后期处理流程，利用空间关系修正物体检测异常，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有物体检测模型未能充分利用静态环境中的空间布局先验，导致预测不一致或错误。

Method: 使用图神经网络（GNN）建模物体间的空间关系，修正异常检测结果。

Result: 实验显示该方法显著提升检测性能，mAP@50最高提升4%。

Conclusion: 利用环境空间结构可显著提升物体检测系统的可靠性。

Abstract: In many real-world applications involving static environments, the spatial
layout of objects remains consistent across instances. However,
state-of-the-art object detection models often fail to leverage this spatial
prior, resulting in inconsistent predictions, missed detections, or
misclassifications, particularly in cluttered or occluded scenes. In this work,
we propose a graph-based post-processing pipeline that explicitly models the
spatial relationships between objects to correct detection anomalies in
egocentric frames. Using a graph neural network (GNN) trained on manually
annotated data, our model identifies invalid object class labels and predicts
corrected class labels based on their neighbourhood context. We evaluate our
approach both as a standalone anomaly detection and correction framework and as
a post-processing module for standard object detectors such as YOLOv7 and
RT-DETR. Experiments demonstrate that incorporating this spatial reasoning
significantly improves detection performance, with mAP@50 gains of up to 4%.
This method highlights the potential of leveraging the environment's spatial
structure to improve reliability in object detection systems.

</details>


### [138] [A Trustworthy Method for Multimodal Emotion Recognition](https://arxiv.org/abs/2508.07625)
*Junxiao Xue,Xiaozhen Liu,Jie Wang,Xuecheng Wu,Bin Wu*

Main category: cs.CV

TL;DR: 提出了一种基于不确定性估计的可信情绪识别方法（TER），通过结合多模态数据的置信度值输出可信预测，并引入新的评估标准来衡量预测的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有情绪识别方法通常依赖复杂深度模型，虽有效但缺乏对噪声和异常数据的鲁棒性，因此需要一种能提供可靠预测的方法。

Method: TER利用不确定性估计计算预测的置信度值，结合多模态结果输出可信预测，并引入可信精度和可信召回等新评估标准。

Result: TER在Music-video数据集上达到82.40%的准确率，在IEMOCAP和Music-video上的可信F1分数分别为0.7511和0.9035，表现优于其他方法。

Conclusion: TER通过引入置信度模块和新的评估标准，显著提升了情绪识别的可靠性和鲁棒性，适用于噪声和异常数据场景。

Abstract: Existing emotion recognition methods mainly focus on enhancing performance by
employing complex deep models, typically resulting in significantly higher
model complexity. Although effective, it is also crucial to ensure the
reliability of the final decision, especially for noisy, corrupted and
out-of-distribution data. To this end, we propose a novel emotion recognition
method called trusted emotion recognition (TER), which utilizes uncertainty
estimation to calculate the confidence value of predictions. TER combines the
results from multiple modalities based on their confidence values to output the
trusted predictions. We also provide a new evaluation criterion to assess the
reliability of predictions. Specifically, we incorporate trusted precision and
trusted recall to determine the trusted threshold and formulate the trusted
Acc. and trusted F1 score to evaluate the model's trusted performance. The
proposed framework combines the confidence module that accordingly endows the
model with reliability and robustness against possible noise or corruption. The
extensive experimental results validate the effectiveness of our proposed
model. The TER achieves state-of-the-art performance on the Music-video,
achieving 82.40% Acc. In terms of trusted performance, TER outperforms other
methods on the IEMOCAP and Music-video, achieving trusted F1 scores of 0.7511
and 0.9035, respectively.

</details>


### [139] [AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning](https://arxiv.org/abs/2508.07626)
*Dejie Yang,Zijing Zhao,Yang Liu*

Main category: cs.CV

TL;DR: 论文提出AR-VRM方法，通过显式模仿人类动作（基于手部关键点）解决视觉机器人操作（VRM）中的数据不足问题，并在CALVIN基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法因依赖与机器人任务不匹配的网页数据或隐式训练（如像素级预测未来帧），在数据不足时泛化能力有限。

Method: 提出基于关键点的视觉语言模型（VLM）预训练方案，从人类动作视频中学习动作知识，并通过类比推理（AR）将人类手部关键点映射到机器人组件。

Result: 在CALVIN基准测试和真实实验中表现领先，尤其在少样本场景中显著优于现有方法。

Conclusion: 显式模仿人类动作能有效提升VRM在数据稀缺时的性能。

Abstract: Visual Robot Manipulation (VRM) aims to enable a robot to follow natural
language instructions based on robot states and visual observations, and
therefore requires costly multi-modal data. To compensate for the deficiency of
robot data, existing approaches have employed vision-language pretraining with
large-scale data. However, they either utilize web data that differs from
robotic tasks, or train the model in an implicit way (e.g., predicting future
frames at the pixel level), thus showing limited generalization ability under
insufficient robot data. In this paper, we propose to learn from large-scale
human action video datasets in an explicit way (i.e., imitating human actions
from hand keypoints), introducing Visual Robot Manipulation with Analogical
Reasoning (AR-VRM). To acquire action knowledge explicitly from human action
videos, we propose a keypoint Vision-Language Model (VLM) pretraining scheme,
enabling the VLM to learn human action knowledge and directly predict human
hand keypoints. During fine-tuning on robot data, to facilitate the robotic arm
in imitating the action patterns of human motions, we first retrieve human
action videos that perform similar manipulation tasks and have similar
historical observations , and then learn the Analogical Reasoning (AR) map
between human hand keypoints and robot components. Taking advantage of focusing
on action keypoints instead of irrelevant visual cues, our method achieves
leading performance on the CALVIN benchmark {and real-world experiments}. In
few-shot scenarios, our AR-VRM outperforms previous methods by large margins ,
underscoring the effectiveness of explicitly imitating human actions under data
scarcity.

</details>


### [140] [LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering](https://arxiv.org/abs/2508.07647)
*Xiaohang Zhan,Dingming Liu*

Main category: cs.CV

TL;DR: 提出了一种无需训练的图像生成算法，通过体积渲染原理在潜在空间中控制物体遮挡关系，显著提升了遮挡精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖提示或布局控制遮挡，缺乏精确性，无法显式处理遮挡关系。

Method: 利用预训练的图像扩散模型，结合体积渲染原理和物体透射率估计，在潜在空间中“渲染”场景。

Result: 实验表明，该方法在遮挡精度上显著优于现有方法，并能通过调整物体透明度等参数实现多种效果。

Conclusion: 该方法无需重新训练模型，基于物理原理实现了精确的遮挡控制，扩展了图像生成的功能。

Abstract: We propose a novel training-free image generation algorithm that precisely
controls the occlusion relationships between objects in an image. Existing
image generation methods typically rely on prompts to influence occlusion,
which often lack precision. While layout-to-image methods provide control over
object locations, they fail to address occlusion relationships explicitly.
Given a pre-trained image diffusion model, our method leverages volume
rendering principles to "render" the scene in latent space, guided by occlusion
relationships and the estimated transmittance of objects. This approach does
not require retraining or fine-tuning the image diffusion model, yet it enables
accurate occlusion control due to its physics-grounded foundation. In extensive
experiments, our method significantly outperforms existing approaches in terms
of occlusion accuracy. Furthermore, we demonstrate that by adjusting the
opacities of objects or concepts during rendering, our method can achieve a
variety of effects, such as altering the transparency of objects, the density
of mass (e.g., forests), the concentration of particles (e.g., rain, fog), the
intensity of light, and the strength of lens effects, etc.

</details>


### [141] [Collaborative Learning of Scattering and Deep Features for SAR Target Recognition with Noisy Labels](https://arxiv.org/abs/2508.07656)
*Yimin Fu,Zhunga Liu,Dongxiu Guo,Longfei Wang*

Main category: cs.CV

TL;DR: 提出了一种结合散射和深度特征的协作学习方法（CLSDF），用于处理合成孔径雷达（SAR）自动目标识别（ATR）中的噪声标签问题。


<details>
  <summary>Details</summary>
Motivation: 由于SAR数据标注需要专业知识，噪声标签不可避免，导致ATR性能下降。现有方法主要针对图像数据，难以直接应用于SAR数据。

Method: 设计了多模型特征融合框架，将散射特征和深度特征结合；利用高斯混合模型（GMM）划分干净和噪声标签样本；通过半监督学习和联合分布对齐策略提升性能。

Result: 在MSTAR数据集上的实验表明，该方法在不同噪声条件下均能达到最优性能。

Conclusion: CLSDF方法有效解决了SAR ATR中的噪声标签问题，提升了识别性能。

Abstract: The acquisition of high-quality labeled synthetic aperture radar (SAR) data
is challenging due to the demanding requirement for expert knowledge.
Consequently, the presence of unreliable noisy labels is unavoidable, which
results in performance degradation of SAR automatic target recognition (ATR).
Existing research on learning with noisy labels mainly focuses on image data.
However, the non-intuitive visual characteristics of SAR data are insufficient
to achieve noise-robust learning. To address this problem, we propose
collaborative learning of scattering and deep features (CLSDF) for SAR ATR with
noisy labels. Specifically, a multi-model feature fusion framework is designed
to integrate scattering and deep features. The attributed scattering centers
(ASCs) are treated as dynamic graph structure data, and the extracted physical
characteristics effectively enrich the representation of deep image features.
Then, the samples with clean and noisy labels are divided by modeling the loss
distribution with multiple class-wise Gaussian Mixture Models (GMMs).
Afterward, the semi-supervised learning of two divergent branches is conducted
based on the data divided by each other. Moreover, a joint distribution
alignment strategy is introduced to enhance the reliability of co-guessed
labels. Extensive experiments have been done on the Moving and Stationary
Target Acquisition and Recognition (MSTAR) dataset, and the results show that
the proposed method can achieve state-of-the-art performance under different
operating conditions with various label noises.

</details>


### [142] [Undress to Redress: A Training-Free Framework for Virtual Try-On](https://arxiv.org/abs/2508.07680)
*Zhiying Li,Junhao Wu,Yeying Jin,Daiheng Gao,Yun Ji,Kaichuan Kong,Lei Yu,Hao Xu,Kai Chen,Bruce Gu,Nana Wang,Zhaoxin Fan*

Main category: cs.CV

TL;DR: UR-VTON提出了一种无需训练的新框架，通过“脱衣-穿衣”机制解决长袖转短袖虚拟试衣中的皮肤恢复问题，结合动态指导和结构细化器提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试衣方法在长袖转短袖场景中因皮肤恢复不准确导致效果不佳，UR-VTON旨在解决这一问题。

Method: 采用“脱衣-穿衣”两步机制，结合动态分类器自由指导和结构细化器优化细节。

Result: 实验表明UR-VTON在细节保留和图像质量上优于现有方法。

Conclusion: UR-VTON为长袖转短袖虚拟试衣提供了高效解决方案，并提出了新基准LS-TON。

Abstract: Virtual try-on (VTON) is a crucial task for enhancing user experience in
online shopping by generating realistic garment previews on personal photos.
Although existing methods have achieved impressive results, they struggle with
long-sleeve-to-short-sleeve conversions-a common and practical scenario-often
producing unrealistic outputs when exposed skin is underrepresented in the
original image. We argue that this challenge arises from the ''majority''
completion rule in current VTON models, which leads to inaccurate skin
restoration in such cases. To address this, we propose UR-VTON (Undress-Redress
Virtual Try-ON), a novel, training-free framework that can be seamlessly
integrated with any existing VTON method. UR-VTON introduces an
''undress-to-redress'' mechanism: it first reveals the user's torso by
virtually ''undressing,'' then applies the target short-sleeve garment,
effectively decomposing the conversion into two more manageable steps.
Additionally, we incorporate Dynamic Classifier-Free Guidance scheduling to
balance diversity and image quality during DDPM sampling, and employ Structural
Refiner to enhance detail fidelity using high-frequency cues. Finally, we
present LS-TON, a new benchmark for long-sleeve-to-short-sleeve try-on.
Extensive experiments demonstrate that UR-VTON outperforms state-of-the-art
methods in both detail preservation and image quality. Code will be released
upon acceptance.

</details>


### [143] [TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding](https://arxiv.org/abs/2508.07683)
*Chaohong Guo,Xun Mo,Yongwei Nie,Xuemiao Xu,Chao Xu,Fei Yu,Chengjiang Long*

Main category: cs.CV

TL;DR: TAR-TVG框架通过引入时间戳锚点和三阶段训练策略，提升视频时间定位的准确性和推理过程的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法未能显式约束推理过程，影响时间预测质量。

Method: 提出TAR-TVG框架，引入时间戳锚点作为中间验证点，并采用三阶段训练策略（GRPO、SFT、GRPO）。

Result: 实验表明模型达到最优性能，生成可解释且逐步优化的时间估计。

Conclusion: TAR-TVG通过显式监督和渐进式推理，显著提升视频时间定位的效果。

Abstract: Temporal Video Grounding (TVG) aims to precisely localize video segments
corresponding to natural language queries, which is a critical capability for
long-form video understanding. Although existing reinforcement learning
approaches encourage models to generate reasoning chains before predictions,
they fail to explicitly constrain the reasoning process to ensure the quality
of the final temporal predictions. To address this limitation, we propose
Timestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG),
a novel framework that introduces timestamp anchors within the reasoning
process to enforce explicit supervision to the thought content. These anchors
serve as intermediate verification points. More importantly, we require each
reasoning step to produce increasingly accurate temporal estimations, thereby
ensuring that the reasoning process contributes meaningfully to the final
prediction. To address the challenge of low-probability anchor generation in
models (e.g., Qwen2.5-VL-3B), we develop an efficient self-distillation
training strategy: (1) initial GRPO training to collect 30K high-quality
reasoning traces containing multiple timestamp anchors, (2) supervised
fine-tuning (SFT) on distilled data, and (3) final GRPO optimization on the
SFT-enhanced model. This three-stage training strategy enables robust anchor
generation while maintaining reasoning quality. Experiments show that our model
achieves state-of-the-art performance while producing interpretable, verifiable
reasoning chains with progressively refined temporal estimations.

</details>


### [144] [Make Your MoVe: Make Your 3D Contents by Adapting Multi-View Diffusion Models to External Editing](https://arxiv.org/abs/2508.07700)
*Weitao Wang,Haoran Xu,Jun Meng,Haoqian Wang*

Main category: cs.CV

TL;DR: 提出了一种无需调优的即插即用方案，用于在3D生成中保持几何一致性，提升编辑后的多视图一致性和网格质量。


<details>
  <summary>Details</summary>
Motivation: 随着3D生成技术的发展，用户对个性化内容的需求增加，但现有编辑工具多为2D领域，直接应用于3D生成会导致信息丢失和质量下降。

Method: 提出了一种几何保持模块，通过原始输入法线潜在空间指导编辑后的多视图生成，并引入注入切换器控制监督程度。

Result: 实验表明，该方法显著提升了编辑后3D资产的多视图一致性和网格质量。

Conclusion: 该方法为3D内容编辑提供了一种高效且无需调优的解决方案。

Abstract: As 3D generation techniques continue to flourish, the demand for generating
personalized content is rapidly rising. Users increasingly seek to apply
various editing methods to polish generated 3D content, aiming to enhance its
color, style, and lighting without compromising the underlying geometry.
However, most existing editing tools focus on the 2D domain, and directly
feeding their results into 3D generation methods (like multi-view diffusion
models) will introduce information loss, degrading the quality of the final 3D
assets. In this paper, we propose a tuning-free, plug-and-play scheme that
aligns edited assets with their original geometry in a single inference run.
Central to our approach is a geometry preservation module that guides the
edited multi-view generation with original input normal latents. Besides, an
injection switcher is proposed to deliberately control the supervision extent
of the original normals, ensuring the alignment between the edited color and
normal views. Extensive experiments show that our method consistently improves
both the multi-view consistency and mesh quality of edited 3D assets, across
multiple combinations of multi-view diffusion models and editing methods.

</details>


### [145] [Multi-view Normal and Distance Guidance Gaussian Splatting for Surface Reconstruction](https://arxiv.org/abs/2508.07701)
*Bo Jia,Yanan Guo,Ying Chang,Benkui Zhang,Ying Xie,Kangning Du,Lin Cao*

Main category: cs.CV

TL;DR: 论文提出了一种多视角距离和法线引导的高斯泼溅方法，解决了3D高斯泼溅在多视角场景中的几何偏差问题，提升了重建精度。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅在单视角投影平面内几何表现合理，但在切换视角时可能出现偏差，需解决多视角场景中的距离和全局匹配问题。

Method: 设计了多视角距离重投影正则化模块和法线增强模块，通过约束邻近深度图和对齐3D法线实现几何深度统一和高精度重建。

Result: 实验结果表明，该方法在定量和定性评估中均优于基线，显著提升了3D高斯泼溅的表面重建能力。

Conclusion: 提出的方法有效解决了多视角场景中的几何偏差问题，实现了高精度的表面重建。

Abstract: 3D Gaussian Splatting (3DGS) achieves remarkable results in the field of
surface reconstruction. However, when Gaussian normal vectors are aligned
within the single-view projection plane, while the geometry appears reasonable
in the current view, biases may emerge upon switching to nearby views. To
address the distance and global matching challenges in multi-view scenes, we
design multi-view normal and distance-guided Gaussian splatting. This method
achieves geometric depth unification and high-accuracy reconstruction by
constraining nearby depth maps and aligning 3D normals. Specifically, for the
reconstruction of small indoor and outdoor scenes, we propose a multi-view
distance reprojection regularization module that achieves multi-view Gaussian
alignment by computing the distance loss between two nearby views and the same
Gaussian surface. Additionally, we develop a multi-view normal enhancement
module, which ensures consistency across views by matching the normals of pixel
points in nearby views and calculating the loss. Extensive experimental results
demonstrate that our method outperforms the baseline in both quantitative and
qualitative evaluations, significantly enhancing the surface reconstruction
capability of 3DGS.

</details>


### [146] [DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models](https://arxiv.org/abs/2508.07714)
*Licheng Zhang,Bach Le,Naveed Akhtar,Tuan Ngo*

Main category: cs.CV

TL;DR: 提出了一种结合深度学习与多模态推理的半自动化流程，用于构建多类别门检测数据集，显著降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集稀缺，而精确检测和分类门类型对建筑合规检查和室内场景理解至关重要。

Method: 使用深度学习目标检测模型统一检测门，再通过大型语言模型（LLM）分类，最后人工验证确保质量。

Result: 显著减少了标注成本，生成了适用于神经模型基准测试的数据集。

Conclusion: 展示了深度学习与多模态推理结合在复杂现实领域高效构建数据集的潜力。

Abstract: Accurate detection and classification of diverse door types in floor plans
drawings is critical for multiple applications, such as building compliance
checking, and indoor scene understanding. Despite their importance, publicly
available datasets specifically designed for fine-grained multi-class door
detection remain scarce. In this work, we present a semi-automated pipeline
that leverages a state-of-the-art object detector and a large language model
(LLM) to construct a multi-class door detection dataset with minimal manual
effort. Doors are first detected as a unified category using a deep object
detection model. Next, an LLM classifies each detected instance based on its
visual and contextual features. Finally, a human-in-the-loop stage ensures
high-quality labels and bounding boxes. Our method significantly reduces
annotation cost while producing a dataset suitable for benchmarking neural
models in floor plan analysis. This work demonstrates the potential of
combining deep learning and multimodal reasoning for efficient dataset
construction in complex real-world domains.

</details>


### [147] [A Registration-Based Star-Shape Segmentation Model and Fast Algorithms](https://arxiv.org/abs/2508.07721)
*Daoping Zhang,Xue-Cheng Tai,Lok Ming Lui*

Main category: cs.CV

TL;DR: 提出了一种基于配准框架的星形分割模型，结合水平集表示和约束，支持单/多中心的全/部分星形分割，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 图像分割在提取目标时面临遮挡、模糊或噪声的挑战，星形先验信息被用于解决这一问题。

Method: 结合水平集表示与配准框架，对变形水平集函数施加约束，支持全/部分星形分割，并通过交替方向乘子法求解。

Result: 在合成和真实图像上的实验表明，该方法能有效实现精确的星形分割。

Conclusion: 所提模型在星形分割任务中表现出色，支持多种分割需求，并通过实验验证了其性能。

Abstract: Image segmentation plays a crucial role in extracting objects of interest and
identifying their boundaries within an image. However, accurate segmentation
becomes challenging when dealing with occlusions, obscurities, or noise in
corrupted images. To tackle this challenge, prior information is often
utilized, with recent attention on star-shape priors. In this paper, we propose
a star-shape segmentation model based on the registration framework. By
combining the level set representation with the registration framework and
imposing constraints on the deformed level set function, our model enables both
full and partial star-shape segmentation, accommodating single or multiple
centers. Additionally, our approach allows for the enforcement of identified
boundaries to pass through specified landmark locations. We tackle the proposed
models using the alternating direction method of multipliers. Through numerical
experiments conducted on synthetic and real images, we demonstrate the efficacy
of our approach in achieving accurate star-shape segmentation.

</details>


### [148] [Enhancing Small-Scale Dataset Expansion with Triplet-Connection-based Sample Re-Weighting](https://arxiv.org/abs/2508.07723)
*Ting Xiang,Changjian Chen,Zhuo Tang,Qifeng Zhang,Fei Lyu,Li Yang,Jiapeng Zhang,Kenli Li*

Main category: cs.CV

TL;DR: 论文提出TriReWeight方法，通过三重连接样本重加权提升生成数据增强效果，理论和实验均验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决生成数据增强中因不可控生成过程和自然语言模糊性导致的噪声图像问题。

Method: 基于理论分析，开发TriReWeight方法，通过三重连接样本重加权优化生成数据增强。

Result: 在六组自然图像数据集和三组医学数据集上平均分别提升7.9%和3.4%，且兼容多种生成方法。

Conclusion: TriReWeight方法理论最优且实验有效，显著提升生成数据增强性能。

Abstract: The performance of computer vision models in certain real-world applications,
such as medical diagnosis, is often limited by the scarcity of available
images. Expanding datasets using pre-trained generative models is an effective
solution. However, due to the uncontrollable generation process and the
ambiguity of natural language, noisy images may be generated. Re-weighting is
an effective way to address this issue by assigning low weights to such noisy
images. We first theoretically analyze three types of supervision for the
generated images. Based on the theoretical analysis, we develop TriReWeight, a
triplet-connection-based sample re-weighting method to enhance generative data
augmentation. Theoretically, TriReWeight can be integrated with any generative
data augmentation methods and never downgrade their performance. Moreover, its
generalization approaches the optimal in the order $O(\sqrt{d\ln (n)/n})$. Our
experiments validate the correctness of the theoretical analysis and
demonstrate that our method outperforms the existing SOTA methods by $7.9\%$ on
average over six natural image datasets and by $3.4\%$ on average over three
medical datasets. We also experimentally validate that our method can enhance
the performance of different generative data augmentation methods.

</details>


### [149] [Grouped Speculative Decoding for Autoregressive Image Generation](https://arxiv.org/abs/2508.07747)
*Junhyuk So,Juncheol Shin,Hyunho Kook,Eunhyeok Park*

Main category: cs.CV

TL;DR: 提出了一种名为GSD的无训练加速方法，显著提升了自回归图像模型的推理速度。


<details>
  <summary>Details</summary>
Motivation: 自回归图像模型推理速度慢，限制了其实际应用。

Method: 通过动态分组推测解码（GSD），评估视觉有效令牌簇而非单一目标令牌。

Result: GSD平均加速3.7倍，且保持图像质量。

Conclusion: GSD是一种高效的无训练加速方法，适用于自回归图像模型。

Abstract: Recently, autoregressive (AR) image models have demonstrated remarkable
generative capabilities, positioning themselves as a compelling alternative to
diffusion models. However, their sequential nature leads to long inference
times, limiting their practical scalability. In this work, we introduce Grouped
Speculative Decoding (GSD), a novel, training-free acceleration method for AR
image models. While recent studies have explored Speculative Decoding (SD) as a
means to speed up AR image generation, existing approaches either provide only
modest acceleration or require additional training. Our in-depth analysis
reveals a fundamental difference between language and image tokens: image
tokens exhibit inherent redundancy and diversity, meaning multiple tokens can
convey valid semantics. However, traditional SD methods are designed to accept
only a single most-likely token, which fails to leverage this difference,
leading to excessive false-negative rejections. To address this, we propose a
new SD strategy that evaluates clusters of visually valid tokens rather than
relying on a single target token. Additionally, we observe that static
clustering based on embedding distance is ineffective, which motivates our
dynamic GSD approach. Extensive experiments show that GSD accelerates AR image
models by an average of 3.7x while preserving image quality-all without
requiring any additional training. The source code is available at
https://github.com/junhyukso/GSD

</details>


### [150] [Comparison Reveals Commonality: Customized Image Generation through Contrastive Inversion](https://arxiv.org/abs/2508.07755)
*Minseo Kim,Minchan Kwon,Dongyeun Lee,Yunho Jeon,Junmo Kim*

Main category: cs.CV

TL;DR: 提出了一种名为Contrastive Inversion的新方法，通过对比输入图像提取共同概念，无需额外信息，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖额外指导（如文本提示或空间掩码）提取共同概念，可能导致辅助特征分离不彻底，影响生成质量。

Method: 通过对比学习训练目标标记和图像辅助文本标记，提取目标真实语义；采用解耦交叉注意力微调提高概念保真度。

Result: 实验表明，该方法在概念表示和编辑方面均表现优异，优于现有技术。

Conclusion: Contrastive Inversion方法在无需额外信息的情况下，实现了高质量的概念提取和生成。

Abstract: The recent demand for customized image generation raises a need for
techniques that effectively extract the common concept from small sets of
images. Existing methods typically rely on additional guidance, such as text
prompts or spatial masks, to capture the common target concept. Unfortunately,
relying on manually provided guidance can lead to incomplete separation of
auxiliary features, which degrades generation quality.In this paper, we propose
Contrastive Inversion, a novel approach that identifies the common concept by
comparing the input images without relying on additional information. We train
the target token along with the image-wise auxiliary text tokens via
contrastive learning, which extracts the well-disentangled true semantics of
the target. Then we apply disentangled cross-attention fine-tuning to improve
concept fidelity without overfitting. Experimental results and analysis
demonstrate that our method achieves a balanced, high-level performance in both
concept representation and editing, outperforming existing techniques.

</details>


### [151] [Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild](https://arxiv.org/abs/2508.07759)
*Haoran Wang,Zekun Li,Jian Zhang,Lei Qi,Yinghuan Shi*

Main category: cs.CV

TL;DR: CAV-SAM利用伪视频表示参考-目标图像对的对应关系，通过SAM2的iVOS能力轻量级适应下游任务，性能提升5%以上。


<details>
  <summary>Details</summary>
Motivation: 现有参考分割方法依赖元学习，数据与计算成本高，CAV-SAM旨在轻量级适应SAM2。

Method: 将参考-目标图像对表示为伪视频，利用DBST模块构建语义转换序列，TTGA模块进行几何对齐。

Result: 在多个数据集上性能提升超过5%。

Conclusion: CAV-SAM为轻量级适应视觉模型提供了新方向。

Abstract: Large vision models like the Segment Anything Model (SAM) exhibit significant
limitations when applied to downstream tasks in the wild. Consequently,
reference segmentation, which leverages reference images and their
corresponding masks to impart novel knowledge to the model, emerges as a
promising new direction for adapting vision models. However, existing reference
segmentation approaches predominantly rely on meta-learning, which still
necessitates an extensive meta-training process and brings massive data and
computational cost. In this study, we propose a novel approach by representing
the inherent correspondence between reference-target image pairs as a pseudo
video. This perspective allows the latest version of SAM, known as SAM2, which
is equipped with interactive video object segmentation (iVOS) capabilities, to
be adapted to downstream tasks in a lightweight manner. We term this approach
Correspondence As Video for SAM (CAV-SAM). CAV-SAM comprises two key modules:
the Diffusion-Based Semantic Transition (DBST) module employs a diffusion model
to construct a semantic transformation sequence, while the Test-Time Geometric
Alignment (TTGA) module aligns the geometric changes within this sequence
through test-time fine-tuning. We evaluated CAVSAM on widely-used datasets,
achieving segmentation performance improvements exceeding 5% over SOTA methods.
Implementation is provided in the supplementary materials.

</details>


### [152] [UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models](https://arxiv.org/abs/2508.07766)
*Jinke Li,Jiarui Yu,Chenxing Wei,Hande Dong,Qiang Lin,Liangjing Yang,Zhicai Wang,Yanbin Hao*

Main category: cs.CV

TL;DR: 论文提出UniSVG数据集，用于多模态大语言模型（MLLM）训练和评估，以解决SVG理解和生成（U&G）的挑战。


<details>
  <summary>Details</summary>
Motivation: SVG在计算机视觉和艺术设计中广泛应用，但AI驱动的SVG理解和生成仍面临高精度和多模态处理的挑战。

Method: 提出UniSVG数据集（525k数据项），支持文本提示和图像到SVG的生成，以及SVG的理解任务。

Result: 使用UniSVG训练的开源MLLM在SVG U&G任务上表现优于GPT-4V等闭源模型。

Conclusion: UniSVG数据集为SVG U&G任务提供了统一解决方案，并展示了MLLM在该领域的潜力。

Abstract: Unlike bitmap images, scalable vector graphics (SVG) maintain quality when
scaled, frequently employed in computer vision and artistic design in the
representation of SVG code. In this era of proliferating AI-powered systems,
enabling AI to understand and generate SVG has become increasingly urgent.
However, AI-driven SVG understanding and generation (U&G) remain significant
challenges. SVG code, equivalent to a set of curves and lines controlled by
floating-point parameters, demands high precision in SVG U&G. Besides, SVG
generation operates under diverse conditional constraints, including textual
prompts and visual references, which requires powerful multi-modal processing
for condition-to-SVG transformation. Recently, the rapid growth of Multi-modal
Large Language Models (MLLMs) have demonstrated capabilities to process
multi-modal inputs and generate complex vector controlling parameters,
suggesting the potential to address SVG U&G tasks within a unified model. To
unlock MLLM's capabilities in the SVG area, we propose an SVG-centric dataset
called UniSVG, comprising 525k data items, tailored for MLLM training and
evaluation. To our best knowledge, it is the first comprehensive dataset
designed for unified SVG generation (from textual prompts and images) and SVG
understanding (color, category, usage, etc.). As expected, learning on the
proposed dataset boosts open-source MLLMs' performance on various SVG U&G
tasks, surpassing SOTA close-source MLLMs like GPT-4V. We release dataset,
benchmark, weights, codes and experiment details on
https://ryanlijinke.github.io/.

</details>


### [153] [Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation](https://arxiv.org/abs/2508.07769)
*Xiaoyan Liu,Kangrui Li,Jiaxin Liu*

Main category: cs.CV

TL;DR: Dream4D框架通过结合可控视频生成和神经4D重建，解决了4D内容合成的时空一致性问题，提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂场景动态和大规模环境中难以保持视角一致性和物理合理性。

Method: 采用两阶段架构：先通过少样本学习预测相机轨迹，再通过姿态条件扩散过程生成几何一致的多视角序列，最终转换为持久4D表示。

Result: 框架首次结合视频扩散模型的时间先验和重建模型的几何感知，显著提升了4D生成质量（如mPSNR、mSSIM）。

Conclusion: Dream4D在4D内容合成中表现出色，优于现有方法。

Abstract: The synthesis of spatiotemporally coherent 4D content presents fundamental
challenges in computer vision, requiring simultaneous modeling of high-fidelity
spatial representations and physically plausible temporal dynamics. Current
approaches often struggle to maintain view consistency while handling complex
scene dynamics, particularly in large-scale environments with multiple
interacting elements. This work introduces Dream4D, a novel framework that
bridges this gap through a synergy of controllable video generation and neural
4D reconstruction. Our approach seamlessly combines a two-stage architecture:
it first predicts optimal camera trajectories from a single image using
few-shot learning, then generates geometrically consistent multi-view sequences
via a specialized pose-conditioned diffusion process, which are finally
converted into a persistent 4D representation. This framework is the first to
leverage both rich temporal priors from video diffusion models and geometric
awareness of the reconstruction models, which significantly facilitates 4D
generation and shows higher quality (e.g., mPSNR, mSSIM) over existing methods.

</details>


### [154] [Prototype-Guided Curriculum Learning for Zero-Shot Learning](https://arxiv.org/abs/2508.07771)
*Lei Wang,Shiming Chen,Guo-Sen Xie,Ziming Hong,Chaojian Yu,Qinmu Peng,Xinge You*

Main category: cs.CV

TL;DR: 论文提出了一种原型引导的课程学习框架（CLZSL），通过原型引导课程学习模块（PCL）和原型更新模块（PUP）解决零样本学习中的实例级和类级语义不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 在零样本学习中，基于嵌入的方法通过学习视觉-语义映射实现知识迁移，但手动定义的语义原型可能引入噪声监督，影响知识迁移效果。

Method: CLZSL框架包含PCL模块（通过课程学习减少实例级不匹配）和PUP模块（动态更新类级语义原型以减少类级不精确）。

Result: 在AWA2、SUN和CUB数据集上的实验验证了方法的有效性。

Conclusion: CLZSL通过原型引导的课程学习和动态原型更新，显著提升了零样本学习的性能。

Abstract: In Zero-Shot Learning (ZSL), embedding-based methods enable knowledge
transfer from seen to unseen classes by learning a visual-semantic mapping from
seen-class images to class-level semantic prototypes (e.g., attributes).
However, these semantic prototypes are manually defined and may introduce noisy
supervision for two main reasons: (i) instance-level mismatch: variations in
perspective, occlusion, and annotation bias will cause discrepancies between
individual sample and the class-level semantic prototypes; and (ii) class-level
imprecision: the manually defined semantic prototypes may not accurately
reflect the true semantics of the class. Consequently, the visual-semantic
mapping will be misled, reducing the effectiveness of knowledge transfer to
unseen classes. In this work, we propose a prototype-guided curriculum learning
framework (dubbed as CLZSL), which mitigates instance-level mismatches through
a Prototype-Guided Curriculum Learning (PCL) module and addresses class-level
imprecision via a Prototype Update (PUP) module. Specifically, the PCL module
prioritizes samples with high cosine similarity between their visual mappings
and the class-level semantic prototypes, and progressively advances to
less-aligned samples, thereby reducing the interference of instance-level
mismatches to achieve accurate visual-semantic mapping. Besides, the PUP module
dynamically updates the class-level semantic prototypes by leveraging the
visual mappings learned from instances, thereby reducing class-level
imprecision and further improving the visual-semantic mapping. Experiments were
conducted on standard benchmark datasets-AWA2, SUN, and CUB-to verify the
effectiveness of our method.

</details>


### [155] [Forecasting Continuous Non-Conservative Dynamical Systems in SO(3)](https://arxiv.org/abs/2508.07775)
*Lennart Bastian,Mohammad Rashed,Nassir Navab,Tolga Birdal*

Main category: cs.CV

TL;DR: 该论文提出了一种基于神经控制微分方程和SO(3) Savitzky-Golay路径的方法，用于在3D旋转流形上建模噪声姿态估计的轨迹，解决了SO(3)外推中的物理和几何挑战。


<details>
  <summary>Details</summary>
Motivation: SO(3)外推在计算机视觉中是一个基础任务，但面临未知物理量、非保守动力学和稀疏噪声观测等挑战。现有方法依赖能量守恒或恒定速度假设，限制了其在实际场景中的应用。

Method: 利用神经控制微分方程结合SO(3) Savitzky-Golay路径，建模噪声姿态估计的轨迹，无需依赖能量或动量守恒假设。

Result: 该方法在仿真和实际场景中表现出鲁棒的外推能力，并能适应未知物理参数。

Conclusion: 提出的方法在复杂非惯性系统中具有广泛应用潜力，且易于集成到现有流程中。

Abstract: Modeling the rotation of moving objects is a fundamental task in computer
vision, yet $SO(3)$ extrapolation still presents numerous challenges: (1)
unknown quantities such as the moment of inertia complicate dynamics, (2) the
presence of external forces and torques can lead to non-conservative
kinematics, and (3) estimating evolving state trajectories under sparse, noisy
observations requires robustness. We propose modeling trajectories of noisy
pose estimates on the manifold of 3D rotations in a physically and
geometrically meaningful way by leveraging Neural Controlled Differential
Equations guided with $SO(3)$ Savitzky-Golay paths. Existing extrapolation
methods often rely on energy conservation or constant velocity assumptions,
limiting their applicability in real-world scenarios involving non-conservative
forces. In contrast, our approach is agnostic to energy and momentum
conservation while being robust to input noise, making it applicable to
complex, non-inertial systems. Our approach is easily integrated as a module in
existing pipelines and generalizes well to trajectories with unknown physical
parameters. By learning to approximate object dynamics from noisy states during
training, our model attains robust extrapolation capabilities in simulation and
various real-world settings. Code is available at
https://github.com/bastianlb/forecasting-rotational-dynamics

</details>


### [156] [GaitSnippet: Gait Recognition Beyond Unordered Sets and Ordered Sequences](https://arxiv.org/abs/2508.07782)
*Saihui Hou,Chenye Wang,Wenpeng Lang,Zhengxiang Lan,Yongzhen Huang*

Main category: cs.CV

TL;DR: 论文提出了一种新的步态识别方法，将步态视为个性化动作的集合，通过随机选择连续片段（称为“片段”）来捕捉多尺度时间上下文，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有步态识别方法（基于无序集或有序序列）在短范围和长范围时间依赖性捕捉方面存在不足，需要更全面的时间上下文学习方法。

Method: 提出“片段”概念，将步态表示为个性化动作的集合，通过随机选择连续片段（片段采样）和建模（片段建模）来学习多尺度时间上下文。

Result: 在四个常用步态数据集上验证了方法的有效性，例如在Gait3D上达到77.5%的Rank-1准确率，在GREW上达到81.7%。

Conclusion: 片段方法为步态识别提供了新视角，展示了多尺度时间上下文学习的潜力。

Abstract: Recent advancements in gait recognition have significantly enhanced
performance by treating silhouettes as either an unordered set or an ordered
sequence. However, both set-based and sequence-based approaches exhibit notable
limitations. Specifically, set-based methods tend to overlook short-range
temporal context for individual frames, while sequence-based methods struggle
to capture long-range temporal dependencies effectively. To address these
challenges, we draw inspiration from human identification and propose a new
perspective that conceptualizes human gait as a composition of individualized
actions. Each action is represented by a series of frames, randomly selected
from a continuous segment of the sequence, which we term a snippet.
Fundamentally, the collection of snippets for a given sequence enables the
incorporation of multi-scale temporal context, facilitating more comprehensive
gait feature learning. Moreover, we introduce a non-trivial solution for
snippet-based gait recognition, focusing on Snippet Sampling and Snippet
Modeling as key components. Extensive experiments on four widely-used gait
datasets validate the effectiveness of our proposed approach and, more
importantly, highlight the potential of gait snippets. For instance, our method
achieves the rank-1 accuracy of 77.5% on Gait3D and 81.7% on GREW using a 2D
convolution-based backbone.

</details>


### [157] [Boosting Active Defense Persistence: A Two-Stage Defense Framework Combining Interruption and Poisoning Against Deepfake](https://arxiv.org/abs/2508.07795)
*Hongrui Zheng,Yuezun Li,Liejun Wang,Yunfeng Diao,Zhiqing Guo*

Main category: cs.CV

TL;DR: 论文提出了一种两阶段防御框架（TSDF），通过双功能对抗扰动来持久防御深度伪造技术，防止攻击者通过重新训练模型绕过防御。


<details>
  <summary>Details</summary>
Motivation: 现有主动防御策略缺乏持久性，攻击者可通过重新训练模型绕过防御，限制了实际应用。

Method: 提出TSDF框架，利用强度分离机制生成双功能对抗扰动，既能直接扭曲伪造结果，又能破坏攻击者重新训练所需的数据准备过程。

Result: 实验表明，TSDF具有强大的双重防御能力，显著提高了防御的持久性。

Conclusion: TSDF通过双功能对抗扰动实现了持久防御，解决了传统防御策略在对抗重新训练时的失效问题。

Abstract: Active defense strategies have been developed to counter the threat of
deepfake technology. However, a primary challenge is their lack of persistence,
as their effectiveness is often short-lived. Attackers can bypass these
defenses by simply collecting protected samples and retraining their models.
This means that static defenses inevitably fail when attackers retrain their
models, which severely limits practical use. We argue that an effective defense
not only distorts forged content but also blocks the model's ability to adapt,
which occurs when attackers retrain their models on protected images. To
achieve this, we propose an innovative Two-Stage Defense Framework (TSDF).
Benefiting from the intensity separation mechanism designed in this paper, the
framework uses dual-function adversarial perturbations to perform two roles.
First, it can directly distort the forged results. Second, it acts as a
poisoning vehicle that disrupts the data preparation process essential for an
attacker's retraining pipeline. By poisoning the data source, TSDF aims to
prevent the attacker's model from adapting to the defensive perturbations, thus
ensuring the defense remains effective long-term. Comprehensive experiments
show that the performance of traditional interruption methods degrades sharply
when it is subjected to adversarial retraining. However, our framework shows a
strong dual defense capability, which can improve the persistence of active
defense. Our code will be available at https://github.com/vpsg-research/TSDF.

</details>


### [158] [Power Battery Detection](https://arxiv.org/abs/2508.07797)
*Xiaoqi Zhao,Peiqian Cao,Lihe Zhang,Zonglei Feng,Hanqi Liu,Jiaming Zuo,Youwei Pang,Weisi Lin,Georges El Fakhri,Huchuan Lu,Xiaofeng Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种新的任务——动力电池检测（PBD），旨在从工业X射线图像中定位阴极和阳极板的密集端点，以进行质量检查。作者发布了首个大规模基准数据集PBD5K，并提出了一种智能标注流程和模型MDCNeXt，结合多维结构线索和状态空间模块，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 动力电池内部结构缺陷可能引发严重安全问题，传统人工检测效率低且易出错，现有视觉算法难以处理密集排列、低对比度等问题。因此，需要一种高效、自动化的解决方案。

Method: 作者提出MDCNeXt模型，结合点、线、计数等多维结构信息，并引入两个状态空间模块：提示过滤模块和密度感知重排序模块。此外，还设计了距离自适应掩码生成策略。

Result: PBD5K数据集包含5,000张X射线图像，涵盖9种电池类型和8种视觉干扰。MDCNeXt模型在抑制视觉干扰和区分密集排列的极板方面表现出色。

Conclusion: 该研究为动力电池检测提供了首个大规模基准和高效模型，推动了这一领域的发展，并为工业应用提供了实用工具。

Abstract: Power batteries are essential components in electric vehicles, where internal
structural defects can pose serious safety risks. We conduct a comprehensive
study on a new task, power battery detection (PBD), which aims to localize the
dense endpoints of cathode and anode plates from industrial X-ray images for
quality inspection. Manual inspection is inefficient and error-prone, while
traditional vision algorithms struggle with densely packed plates, low
contrast, scale variation, and imaging artifacts. To address this issue and
drive more attention into this meaningful task, we present PBD5K, the first
large-scale benchmark for this task, consisting of 5,000 X-ray images from nine
battery types with fine-grained annotations and eight types of real-world
visual interference. To support scalable and consistent labeling, we develop an
intelligent annotation pipeline that combines image filtering, model-assisted
pre-labeling, cross-verification, and layered quality evaluation. We formulate
PBD as a point-level segmentation problem and propose MDCNeXt, a model designed
to extract and integrate multi-dimensional structure clues including point,
line, and count information from the plate itself. To improve discrimination
between plates and suppress visual interference, MDCNeXt incorporates two state
space modules. The first is a prompt-filtered module that learns contrastive
relationships guided by task-specific prompts. The second is a density-aware
reordering module that refines segmentation in regions with high plate density.
In addition, we propose a distance-adaptive mask generation strategy to provide
robust supervision under varying spatial distributions of anode and cathode
positions. The source code and datasets will be publicly available at
\href{https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD}{PBD5K}.

</details>


### [159] [MambaTrans: Multimodal Fusion Image Translation via Large Language Model Priors for Downstream Visual Tasks](https://arxiv.org/abs/2508.07803)
*Yushen Xu,Xiaosong Li,Zhenyu Kuang,Xiaoqi Cheng,Haishu Tan,Huafeng Li*

Main category: cs.CV

TL;DR: MambaTrans是一种新型多模态融合图像模态翻译器，旨在解决可见图像与多模态融合图像之间的像素分布差异问题，提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有下游预训练模型通常在可见图像上训练，而多模态融合图像与可见图像的像素分布差异显著，导致下游任务性能下降。本文探索如何将多模态融合图像适配到基于可见图像训练的模型中。

Method: 提出MambaTrans，利用多模态大语言模型的描述和语义分割模型的掩码作为输入，结合Multi-Model State Space Block（掩码-图像-文本交叉注意力和3D选择性扫描模块）增强视觉能力，最小化检测损失并捕获长程依赖关系。

Result: 在公共数据集上的实验表明，MambaTrans有效提升了多模态图像在下游任务中的性能。

Conclusion: MambaTrans通过模态翻译器成功解决了多模态融合图像与可见图像之间的适配问题，无需调整预训练模型参数即可实现性能提升。

Abstract: The goal of multimodal image fusion is to integrate complementary information
from infrared and visible images, generating multimodal fused images for
downstream tasks. Existing downstream pre-training models are typically trained
on visible images. However, the significant pixel distribution differences
between visible and multimodal fusion images can degrade downstream task
performance, sometimes even below that of using only visible images. This paper
explores adapting multimodal fused images with significant modality differences
to object detection and semantic segmentation models trained on visible images.
To address this, we propose MambaTrans, a novel multimodal fusion image
modality translator. MambaTrans uses descriptions from a multimodal large
language model and masks from semantic segmentation models as input. Its core
component, the Multi-Model State Space Block, combines mask-image-text
cross-attention and a 3D-Selective Scan Module, enhancing pure visual
capabilities. By leveraging object detection prior knowledge, MambaTrans
minimizes detection loss during training and captures long-term dependencies
among text, masks, and images. This enables favorable results in pre-trained
models without adjusting their parameters. Experiments on public datasets show
that MambaTrans effectively improves multimodal image performance in downstream
tasks.

</details>


### [160] [Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning](https://arxiv.org/abs/2508.07804)
*Bao Li,Xiaomei Zhang,Miao Xu,Zhaoxin Fan,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: Pose-RFT提出了一种基于强化学习的框架，用于从多模态输入生成3D人体姿势，通过混合动作强化学习优化语言预测和姿势生成。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督学习的多模态大语言模型在3D姿势生成任务中难以建模模糊性和任务对齐，Pose-RFT旨在解决这一问题。

Method: 提出Pose-RFT框架，结合HyGRPO算法进行混合动作强化学习，优化离散和连续动作，并引入任务特定奖励函数。

Result: 实验表明Pose-RFT在多个基准测试中显著优于现有方法，验证了混合动作强化学习的有效性。

Conclusion: Pose-RFT通过强化学习优化多模态3D姿势生成，为任务对齐和模糊性建模提供了有效解决方案。

Abstract: Generating 3D human poses from multimodal inputs such as images or text
requires models to capture both rich spatial and semantic correspondences.
While pose-specific multimodal large language models (MLLMs) have shown promise
in this task, they are typically trained with supervised objectives such as
SMPL parameter regression or token-level prediction, which struggle to model
the inherent ambiguity and achieve task-specific alignment required for
accurate 3D pose generation. To address these limitations, we propose Pose-RFT,
a reinforcement fine-tuning framework tailored for 3D human pose generation in
MLLMs. We formulate the task as a hybrid action reinforcement learning problem
that jointly optimizes discrete language prediction and continuous pose
generation. To this end, we introduce HyGRPO, a hybrid reinforcement learning
algorithm that performs group-wise reward normalization over sampled responses
to guide joint optimization of discrete and continuous actions. Pose-RFT
further incorporates task-specific reward functions to guide optimization
towards spatial alignment in image-to-pose generation and semantic consistency
in text-to-pose generation. Extensive experiments on multiple pose generation
benchmarks demonstrate that Pose-RFT significantly improves performance over
existing pose-specific MLLMs, validating the effectiveness of hybrid action
reinforcement fine-tuning for 3D pose generation.

</details>


### [161] [DiTVR: Zero-Shot Diffusion Transformer for Video Restoration](https://arxiv.org/abs/2508.07811)
*Sicheng Gao,Nancy Mehta,Zongwei Wu,Radu Timofte*

Main category: cs.CV

TL;DR: DiTVR是一种零样本视频恢复框架，结合扩散变换器和轨迹感知注意力机制，通过光流轨迹对齐令牌，提升时间一致性。


<details>
  <summary>Details</summary>
Motivation: 传统回归方法生成不真实细节且需大量配对数据，而生成扩散模型难以保证时间一致性。DiTVR旨在解决这些问题。

Method: 结合扩散变换器与轨迹感知注意力，使用光流轨迹对齐令牌，并通过小波引导的流一致采样器注入数据一致性。

Result: DiTVR在视频恢复基准测试中达到零样本最优，时间一致性和细节保留表现优异。

Conclusion: DiTVR通过创新注意力机制和采样器设计，显著提升了视频恢复的质量和效率。

Abstract: Video restoration aims to reconstruct high quality video sequences from low
quality inputs, addressing tasks such as super resolution, denoising, and
deblurring. Traditional regression based methods often produce unrealistic
details and require extensive paired datasets, while recent generative
diffusion models face challenges in ensuring temporal consistency. We introduce
DiTVR, a zero shot video restoration framework that couples a diffusion
transformer with trajectory aware attention and a wavelet guided, flow
consistent sampler. Unlike prior 3D convolutional or frame wise diffusion
approaches, our attention mechanism aligns tokens along optical flow
trajectories, with particular emphasis on vital layers that exhibit the highest
sensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically
selects relevant tokens based on motion correspondences across frames. The flow
guided sampler injects data consistency only into low-frequency bands,
preserving high frequency priors while accelerating convergence. DiTVR
establishes a new zero shot state of the art on video restoration benchmarks,
demonstrating superior temporal consistency and detail preservation while
remaining robust to flow noise and occlusions.

</details>


### [162] [Semi-supervised Multiscale Matching for SAR-Optical Image](https://arxiv.org/abs/2508.07812)
*Jingze Gai,Changchun Li*

Main category: cs.CV

TL;DR: 提出了一种半监督的SAR-光学图像匹配方法（S2M2-SAR），利用少量标记数据和大量未标记数据，通过伪标签和跨模态特征增强模块提升匹配性能。


<details>
  <summary>Details</summary>
Motivation: 现有SAR-光学图像匹配方法依赖像素级匹配标注，耗时且复杂，难以获取足够标记数据。

Method: 设计半监督匹配流程，结合深度和浅层匹配结果生成伪标签，并引入无监督的跨模态特征增强模块。

Result: S2M2-SAR在基准数据集上表现优于现有半监督方法，甚至媲美全监督SOTA方法。

Conclusion: S2M2-SAR高效且实用，为SAR-光学图像匹配提供了新思路。

Abstract: Driven by the complementary nature of optical and synthetic aperture radar
(SAR) images, SAR-optical image matching has garnered significant interest.
Most existing SAR-optical image matching methods aim to capture effective
matching features by employing the supervision of pixel-level matched
correspondences within SAR-optical image pairs, which, however, suffers from
time-consuming and complex manual annotation, making it difficult to collect
sufficient labeled SAR-optical image pairs. To handle this, we design a
semi-supervised SAR-optical image matching pipeline that leverages both scarce
labeled and abundant unlabeled image pairs and propose a semi-supervised
multiscale matching for SAR-optical image matching (S2M2-SAR). Specifically, we
pseudo-label those unlabeled SAR-optical image pairs with pseudo ground-truth
similarity heatmaps by combining both deep and shallow level matching results,
and train the matching model by employing labeled and pseudo-labeled similarity
heatmaps. In addition, we introduce a cross-modal feature enhancement module
trained using a cross-modality mutual independence loss, which requires no
ground-truth labels. This unsupervised objective promotes the separation of
modality-shared and modality-specific features by encouraging statistical
independence between them, enabling effective feature disentanglement across
optical and SAR modalities. To evaluate the effectiveness of S2M2-SAR, we
compare it with existing competitors on benchmark datasets. Experimental
results demonstrate that S2M2-SAR not only surpasses existing semi-supervised
methods but also achieves performance competitive with fully supervised SOTA
methods, demonstrating its efficiency and practical potential.

</details>


### [163] [Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models](https://arxiv.org/abs/2508.07818)
*Chenyue Song,Chen Hui,Haiqi Zhu,Feng Jiang,Yachun Mi,Wei Zhang,Shaohui Liu*

Main category: cs.CV

TL;DR: RSFIQA是一种细粒度无参考图像质量评估模型，通过动态分割语义区域并结合多模态大语言模型感知局部质量退化，利用区域感知语义注意力机制提升全局质量预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有NR-IQA方法要么关注全局表征而忽视语义显著区域，要么对区域特征采用均匀加权而弱化局部质量变化的敏感性。

Method: 利用SAM动态分割图像为语义区域，结合MLLM提取描述性内容并感知多维失真，引入RSA机制聚合局部细粒度表征生成全局注意力图。

Result: 在多个基准数据集上表现出竞争性的质量预测性能，验证了方法的鲁棒性和有效性。

Conclusion: RSFIQA通过区域级失真信息感知多维质量差异，为无参考图像质量评估提供了新思路。

Abstract: No-reference image quality assessment (NR-IQA) aims to simulate the process
of perceiving image quality aligned with subjective human perception. However,
existing NR-IQA methods either focus on global representations that leads to
limited insights into the semantically salient regions or employ a uniform
weighting for region features that weakens the sensitivity to local quality
variations. In this paper, we propose a fine-grained image quality assessment
model, named RSFIQA, which integrates region-level distortion information to
perceive multi-dimensional quality discrepancies. To enhance regional quality
awareness, we first utilize the Segment Anything Model (SAM) to dynamically
partition the input image into non-overlapping semantic regions. For each
region, we teach a powerful Multi-modal Large Language Model (MLLM) to extract
descriptive content and perceive multi-dimensional distortions, enabling a
comprehensive understanding of both local semantics and quality degradations.
To effectively leverage this information, we introduce Region-Aware Semantic
Attention (RSA) mechanism, which generates a global attention map by
aggregating fine-grained representations from local regions. In addition,
RSFIQA is backbone-agnostic and can be seamlessly integrated into various deep
neural network architectures. Extensive experiments demonstrate the robustness
and effectiveness of the proposed method, which achieves competitive quality
prediction performance across multiple benchmark datasets.

</details>


### [164] [Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP](https://arxiv.org/abs/2508.07819)
*Ke Ma,Jun Long,Hongxiao Fei,Liujie Hua,Yueyi Luo*

Main category: cs.CV

TL;DR: 提出了一种架构协同设计框架，通过卷积低秩适配器和动态融合网关改进视觉语言模型在零样本异常检测中的表现。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉语言模型在零样本异常检测中存在适应性差距，缺乏局部归纳偏置和灵活的特征融合方式。

Method: 结合卷积低秩适配器注入局部归纳偏置，并引入动态融合网关自适应调制文本提示。

Result: 在工业和医学基准测试中表现出更高的准确性和鲁棒性。

Conclusion: 协同设计框架显著提升了基础模型在密集感知任务中的适应性。

Abstract: Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap
when applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of
local inductive biases for dense prediction and their reliance on inflexible
feature fusion paradigms. We address these limitations through an Architectural
Co-Design framework that jointly refines feature representation and cross-modal
fusion. Our method integrates a parameter-efficient Convolutional Low-Rank
Adaptation (Conv-LoRA) adapter to inject local inductive biases for
fine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that
leverages visual context to adaptively modulate text prompts, enabling a
powerful bidirectional fusion. Extensive experiments on diverse industrial and
medical benchmarks demonstrate superior accuracy and robustness, validating
that this synergistic co-design is critical for robustly adapting foundation
models to dense perception tasks.

</details>


### [165] [MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization](https://arxiv.org/abs/2508.07833)
*Animesh Jain,Alexandros Stergiou*

Main category: cs.CV

TL;DR: MIMIC框架通过视觉概念合成可视化VLMs内部表示，提升透明度和信任。


<details>
  <summary>Details</summary>
Motivation: VLMs的多模态输入编码复杂且难以解释，限制了透明度和信任。

Method: MIMIC结合VLM反演和特征对齐目标，采用三重正则化处理空间对齐、图像平滑和语义真实性。

Result: 通过定量和定性评估，MIMIC在自由文本输出中成功反演视觉概念，并展示了视觉质量和语义指标。

Conclusion: MIMIC是首个针对VLM视觉概念解释的反演方法，具有创新性。

Abstract: Vision Language Models (VLMs) encode multimodal inputs over large, complex,
and difficult-to-interpret architectures, which limit transparency and trust.
We propose a Multimodal Inversion for Model Interpretation and
Conceptualization (MIMIC) framework to visualize the internal representations
of VLMs by synthesizing visual concepts corresponding to internal encodings.
MIMIC uses a joint VLM-based inversion and a feature alignment objective to
account for VLM's autoregressive processing. It additionally includes a triplet
of regularizers for spatial alignment, natural image smoothness, and semantic
realism. We quantitatively and qualitatively evaluate MIMIC by inverting visual
concepts over a range of varying-length free-form VLM output texts. Reported
results include both standard visual quality metrics as well as semantic
text-based metrics. To the best of our knowledge, this is the first model
inversion approach addressing visual interpretations of VLM concepts.

</details>


### [166] [Effortless Vision-Language Model Specialization in Histopathology without Annotation](https://arxiv.org/abs/2508.07835)
*Jingna Qiu,Nishanth Jain,Jonas Ammeling,Marc Aubreville,Katharina Breininger*

Main category: cs.CV

TL;DR: 本文研究了通过无标注的领域相关图像-文本对继续预训练视觉语言模型（VLMs），以提升其在组织病理学任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管通用视觉语言模型（如CONCH和QuiltNet）在组织病理学中表现出色，但其通用设计可能在特定任务中表现不佳。现有监督微调方法需要手动标注样本，限制了其应用。

Method: 通过从现有数据库中提取领域和任务相关的图像-文本对，进行无标注的继续预训练。

Result: 实验表明，这种方法显著提升了零样本和小样本性能，且在大规模训练时能与小样本方法媲美。

Conclusion: 无标注继续预训练是一种高效、任务无关的方法，为VLMs在组织病理学中的适应提供了新途径。

Abstract: Recent advances in Vision-Language Models (VLMs) in histopathology, such as
CONCH and QuiltNet, have demonstrated impressive zero-shot classification
capabilities across various tasks. However, their general-purpose design may
lead to suboptimal performance in specific downstream applications. While
supervised fine-tuning methods address this issue, they require manually
labeled samples for adaptation. This paper investigates annotation-free
adaptation of VLMs through continued pretraining on domain- and task-relevant
image-caption pairs extracted from existing databases. Our experiments on two
VLMs, CONCH and QuiltNet, across three downstream tasks reveal that these pairs
substantially enhance both zero-shot and few-shot performance. Notably, with
larger training sizes, continued pretraining matches the performance of
few-shot methods while eliminating manual labeling. Its effectiveness,
task-agnostic design, and annotation-free workflow make it a promising pathway
for adapting VLMs to new histopathology tasks. Code is available at
https://github.com/DeepMicroscopy/Annotation-free-VLM-specialization.

</details>


### [167] [CBDES MoE: Hierarchically Decoupled Mixture-of-Experts for Functional Modules in Autonomous Driving](https://arxiv.org/abs/2508.07838)
*Qi Xiang,Kunsong Shi,Zhigui Lin,Lei He*

Main category: cs.CV

TL;DR: 提出了一种名为CBDES MoE的分层解耦混合专家架构，用于解决多模态BEV感知系统的输入适应性、建模能力和泛化性能问题，并在nuScenes数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于多传感器特征融合的BEV感知系统存在输入适应性有限、建模能力不足和泛化性能不佳的问题。

Method: 提出CBDES MoE架构，结合异构专家网络和轻量级自注意力路由器（SAR），实现动态专家路径选择和稀疏高效推理。

Result: 在nuScenes数据集上，CBDES MoE在3D目标检测任务中表现优于单专家模型，mAP提升1.6点，NDS提升4.1点。

Conclusion: CBDES MoE通过模块化混合专家框架，显著提升了BEV感知系统的性能，具有实际应用优势。

Abstract: Bird's Eye View (BEV) perception systems based on multi-sensor feature fusion
have become a fundamental cornerstone for end-to-end autonomous driving.
However, existing multi-modal BEV methods commonly suffer from limited input
adaptability, constrained modeling capacity, and suboptimal generalization. To
address these challenges, we propose a hierarchically decoupled
Mixture-of-Experts architecture at the functional module level, termed
Computing Brain DEvelopment System Mixture-of-Experts (CBDES MoE). CBDES MoE
integrates multiple structurally heterogeneous expert networks with a
lightweight Self-Attention Router (SAR) gating mechanism, enabling dynamic
expert path selection and sparse, input-aware efficient inference. To the best
of our knowledge, this is the first modular Mixture-of-Experts framework
constructed at the functional module granularity within the autonomous driving
domain. Extensive evaluations on the real-world nuScenes dataset demonstrate
that CBDES MoE consistently outperforms fixed single-expert baselines in 3D
object detection. Compared to the strongest single-expert model, CBDES MoE
achieves a 1.6-point increase in mAP and a 4.1-point improvement in NDS,
demonstrating the effectiveness and practical advantages of the proposed
approach.

</details>


### [168] [Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images](https://arxiv.org/abs/2508.07847)
*Shunya Nagashima,Komei Sugiura*

Main category: cs.CV

TL;DR: 论文提出了一种基于多深度状态空间模型的Deep SWM方法，用于处理太阳图像和长时空依赖性，并通过新的预训练策略和公开基准FlareBench验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 准确可靠的太阳耀斑预测对减少关键基础设施的潜在破坏至关重要，但现有方法在表征学习和长时空依赖性建模方面存在不足。

Method: 提出Deep SWM，结合多深度状态空间模型和稀疏掩码自编码器，采用两阶段掩码策略保留关键区域并压缩空间信息。

Result: Deep SWM在性能和可靠性上优于基线方法及人类专家。

Conclusion: Deep SWM为太阳耀斑预测提供了高效可靠的解决方案，并通过公开基准验证了其优越性。

Abstract: Accurate, reliable solar flare prediction is crucial for mitigating potential
disruptions to critical infrastructure, while predicting solar flares remains a
significant challenge. Existing methods based on heuristic physical features
often lack representation learning from solar images. On the other hand,
end-to-end learning approaches struggle to model long-range temporal
dependencies in solar images. In this study, we propose Deep Space Weather
Model (Deep SWM), which is based on multiple deep state space models for
handling both ten-channel solar images and long-range spatio-temporal
dependencies. Deep SWM also features a sparse masked autoencoder, a novel
pretraining strategy that employs a two-phase masking approach to preserve
crucial regions such as sunspots while compressing spatial information.
Furthermore, we built FlareBench, a new public benchmark for solar flare
prediction covering a full 11-year solar activity cycle, to validate our
method. Our method outperformed baseline methods and even human expert
performance on standard metrics in terms of performance and reliability. The
project page can be found at https://keio-smilab25.github.io/DeepSWM.

</details>


### [169] [Morphological Analysis of Semiconductor Microstructures using Skeleton Graphs](https://arxiv.org/abs/2508.07850)
*Noriko Nitta,Rei Miyata,Naoto Oishi*

Main category: cs.CV

TL;DR: 论文通过电子显微镜图像提取Ge表面微结构的拓扑特征，使用图卷积网络嵌入，并通过PCA分析嵌入结果，发现辐照角度对Ge表面形态的影响比辐照通量更大。


<details>
  <summary>Details</summary>
Motivation: 研究离子束辐照对Ge表面微结构形态的影响，特别是辐照角度和通量的差异。

Method: 处理电子显微镜图像提取拓扑特征，使用图卷积网络嵌入，并通过PCA和Davies-Bouldin指数分析聚类分离性。

Result: 辐照角度对Ge表面形态的影响比辐照通量更显著。

Conclusion: 辐照角度是影响Ge表面微结构形态的关键因素。

Abstract: In this paper, electron microscopy images of microstructures formed on Ge
surfaces by ion beam irradiation were processed to extract topological features
as skeleton graphs, which were then embedded using a graph convolutional
network. The resulting embeddings were analyzed using principal component
analysis, and cluster separability in the resulting PCA space was evaluated
using the Davies-Bouldin index. The results indicate that variations in
irradiation angle have a more significant impact on the morphological
properties of Ge surfaces than variations in irradiation fluence.

</details>


### [170] [Tracking Any Point Methods for Markerless 3D Tissue Tracking in Endoscopic Stereo Images](https://arxiv.org/abs/2508.07851)
*Konrad Reuter,Suresh Guttikonda,Sarah Latus,Lennart Maack,Christian Betz,Tobias Maurer,Alexander Schlaefer*

Main category: cs.CV

TL;DR: 提出了一种基于2D TAP网络的无标记3D组织追踪方法，结合两个CoTracker模型，实现了在动态手术场景中的高精度追踪。


<details>
  <summary>Details</summary>
Motivation: 微创手术中动态组织运动和视野受限带来挑战，准确的组织追踪可提升手术安全性并支持机器人辅助。

Method: 结合两个CoTracker模型（时间追踪和立体匹配），从立体内窥镜图像中估计3D运动。

Result: 在鸡组织模型上实现了1.1 mm的欧氏距离误差（速度为10 mm/s），验证了方法的可靠性。

Conclusion: 基于TAP的模型在复杂手术场景中具有高精度无标记3D追踪潜力。

Abstract: Minimally invasive surgery presents challenges such as dynamic tissue motion
and a limited field of view. Accurate tissue tracking has the potential to
support surgical guidance, improve safety by helping avoid damage to sensitive
structures, and enable context-aware robotic assistance during complex
procedures. In this work, we propose a novel method for markerless 3D tissue
tracking by leveraging 2D Tracking Any Point (TAP) networks. Our method
combines two CoTracker models, one for temporal tracking and one for stereo
matching, to estimate 3D motion from stereo endoscopic images. We evaluate the
system using a clinical laparoscopic setup and a robotic arm simulating tissue
motion, with experiments conducted on a synthetic 3D-printed phantom and a
chicken tissue phantom. Tracking on the chicken tissue phantom yielded more
reliable results, with Euclidean distance errors as low as 1.1 mm at a velocity
of 10 mm/s. These findings highlight the potential of TAP-based models for
accurate, markerless 3D tracking in challenging surgical scenarios.

</details>


### [171] [Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model](https://arxiv.org/abs/2508.07863)
*Bin Cao,Sipeng Zheng,Ye Wang,Lujie Xia,Qianshan Wei,Qin Jin,Jing Liu,Zongqing Lu*

Main category: cs.CV

TL;DR: 论文提出Being-M0.5，一种实时可控的视觉-语言-运动模型，解决了现有模型在可控性方面的五大限制，并在多个运动生成任务中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-运动模型（VLMMs）在可控性方面存在显著限制，阻碍了其实际应用。论文旨在解决这些限制，提升模型的实用性和性能。

Method: 基于HuMo100M数据集（包含500万自收集运动序列和1亿多任务指令实例），提出了一种新颖的部分感知残差量化技术，用于运动标记化，实现对身体部分的精细控制。

Result: 实验验证表明，Being-M0.5在多样运动基准测试中表现优异，同时具备实时能力。

Conclusion: HuMo100M和Being-M0.5为运动生成技术的实际应用提供了重要进展，推动了该领域的未来发展。

Abstract: Human motion generation has emerged as a critical technology with
transformative potential for real-world applications. However, existing
vision-language-motion models (VLMMs) face significant limitations that hinder
their practical deployment. We identify controllability as a main bottleneck,
manifesting in five key aspects: inadequate response to diverse human commands,
limited pose initialization capabilities, poor performance on long-term
sequences, insufficient handling of unseen scenarios, and lack of fine-grained
control over individual body parts. To overcome these limitations, we present
Being-M0.5, the first real-time, controllable VLMM that achieves
state-of-the-art performance across multiple motion generation tasks. Our
approach is built upon HuMo100M, the largest and most comprehensive human
motion dataset to date, comprising over 5 million self-collected motion
sequences, 100 million multi-task instructional instances, and detailed
part-level annotations that address a critical gap in existing datasets. We
introduce a novel part-aware residual quantization technique for motion
tokenization that enables precise, granular control over individual body parts
during generation. Extensive experimental validation demonstrates Being-M0.5's
superior performance across diverse motion benchmarks, while comprehensive
efficiency analysis confirms its real-time capabilities. Our contributions
include design insights and detailed computational analysis to guide future
development of practical motion generators. We believe that HuMo100M and
Being-M0.5 represent significant advances that will accelerate the adoption of
motion generation technologies in real-world applications. The project page is
available at https://beingbeyond.github.io/Being-M0.5.

</details>


### [172] [CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning](https://arxiv.org/abs/2508.07871)
*Yanshu Li,Jianjiang Yang,Zhennan Shen,Ligong Han,Haoyan Xu,Ruixiang Tang*

Main category: cs.CV

TL;DR: 论文提出了一种针对多模态上下文学习（ICL）的上下文自适应令牌剪枝方法（CATP），以减少图像令牌冗余并提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有图像令牌剪枝方法多针对单图像任务，忽视了多模态ICL中更大的冗余和效率需求，导致性能不稳定。

Method: CATP采用两阶段渐进式剪枝，充分考虑了输入序列中的跨模态交互。

Result: 在去除77.8%的图像令牌后，CATP在四个LVLM和八个基准测试中平均性能提升0.6%，推理延迟平均降低10.78%。

Conclusion: CATP提升了多模态ICL的实用价值，为未来图像-文本交错场景的研究奠定了基础。

Abstract: Modern large vision-language models (LVLMs) convert each input image into a
large set of tokens, far outnumbering the text tokens. Although this improves
visual perception, it introduces severe image token redundancy. Because image
tokens carry sparse information, many add little to reasoning, yet greatly
increase inference cost. The emerging image token pruning methods tackle this
issue by identifying the most important tokens and discarding the rest. These
methods can raise efficiency with only modest performance loss. However, most
of them only consider single-image tasks and overlook multimodal in-context
learning (ICL), where redundancy is greater and efficiency is more critical.
Redundant tokens weaken the advantage of multimodal ICL for rapid domain
adaptation and cause unstable performance. Applying existing pruning methods in
this setting leads to large accuracy drops, exposing a clear gap and the need
for new techniques. Thus, we propose Contextually Adaptive Token Pruning
(CATP), a training-free pruning method targeted at multimodal ICL. CATP
consists of two stages that perform progressive pruning to fully account for
the complex cross-modal interactions in the input sequence. After removing
77.8\% of the image tokens, CATP produces an average performance gain of 0.6\%
over the vanilla model on four LVLMs and eight benchmarks, exceeding all
baselines remarkably. Meanwhile, it effectively improves efficiency by
achieving an average reduction of 10.78\% in inference latency. CATP enhances
the practical value of multimodal ICL and lays the groundwork for future
progress in interleaved image-text scenarios.

</details>


### [173] [Selective Contrastive Learning for Weakly Supervised Affordance Grounding](https://arxiv.org/abs/2508.07877)
*WonJun Moon,Hyun Seok Seong,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 论文提出了一种选择性原型和像素对比目标的方法，通过多视角学习来改进弱监督功能部件定位（WSAG），解决了传统方法依赖分类而忽略功能相关性的问题。


<details>
  <summary>Details</summary>
Motivation: 人类能从第三人称演示中直观理解功能部件，而无需像素级标注。现有WSAG方法依赖共享分类器和蒸馏策略，但容易关注与功能无关的类特定模式。

Method: 结合选择性原型和像素对比目标，自适应学习部件和对象级别的功能相关线索，利用CLIP发现动作相关对象，并通过多视角交叉验证精确定位功能部件。

Result: 实验表明，该方法能有效将激活从无关区域转移到有意义的功能线索上。

Conclusion: 提出的方法在多视角学习中显著提升了功能部件定位的准确性。

Abstract: Facilitating an entity's interaction with objects requires accurately
identifying parts that afford specific actions. Weakly supervised affordance
grounding (WSAG) seeks to imitate human learning from third-person
demonstrations, where humans intuitively grasp functional parts without needing
pixel-level annotations. To achieve this, grounding is typically learned using
a shared classifier across images from different perspectives, along with
distillation strategies incorporating part discovery process. However, since
affordance-relevant parts are not always easily distinguishable, models
primarily rely on classification, often focusing on common class-specific
patterns that are unrelated to affordance. To address this limitation, we move
beyond isolated part-level learning by introducing selective prototypical and
pixel contrastive objectives that adaptively learn affordance-relevant cues at
both the part and object levels, depending on the granularity of the available
information. Initially, we find the action-associated objects in both
egocentric (object-focused) and exocentric (third-person example) images by
leveraging CLIP. Then, by cross-referencing the discovered objects of
complementary views, we excavate the precise part-level affordance clues in
each perspective. By consistently learning to distinguish affordance-relevant
regions from affordance-irrelevant background context, our approach effectively
shifts activation from irrelevant areas toward meaningful affordance cues.
Experimental results demonstrate the effectiveness of our method. Codes are
available at github.com/hynnsk/SelectiveCL.

</details>


### [174] [TAP: Parameter-efficient Task-Aware Prompting for Adverse Weather Removal](https://arxiv.org/abs/2508.07878)
*Hanting Wang,Shengpeng Ji,Shulei Wang,Hai Huang,Xiao Jin,Qifei Zhang,Tao Jin*

Main category: cs.CV

TL;DR: 提出了一种参数高效的All-in-One图像修复框架，通过任务感知增强提示处理多种恶劣天气退化问题，显著减少参数开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常为每种退化类型设计专用模块或参数，导致参数开销大且忽略任务间的相关性。

Method: 采用两阶段训练范式（预训练和提示调优），利用任务感知增强提示和低秩分解捕捉任务通用与特定特征。

Result: 实验表明，该方法仅用2.75M参数即可在不同修复任务中实现优异性能。

Conclusion: 提出的框架通过任务感知提示和低秩分解，高效解决了多任务图像修复问题。

Abstract: Image restoration under adverse weather conditions has been extensively
explored, leading to numerous high-performance methods. In particular, recent
advances in All-in-One approaches have shown impressive results by training on
multi-task image restoration datasets. However, most of these methods rely on
dedicated network modules or parameters for each specific degradation type,
resulting in a significant parameter overhead. Moreover, the relatedness across
different restoration tasks is often overlooked. In light of these issues, we
propose a parameter-efficient All-in-One image restoration framework that
leverages task-aware enhanced prompts to tackle various adverse weather
degradations.Specifically, we adopt a two-stage training paradigm consisting of
a pretraining phase and a prompt-tuning phase to mitigate parameter conflicts
across tasks. We first employ supervised learning to acquire general
restoration knowledge, and then adapt the model to handle specific degradation
via trainable soft prompts. Crucially, we enhance these task-specific prompts
in a task-aware manner. We apply low-rank decomposition to these prompts to
capture both task-general and task-specific characteristics, and impose
contrastive constraints to better align them with the actual inter-task
relatedness. These enhanced prompts not only improve the parameter efficiency
of the restoration model but also enable more accurate task modeling, as
evidenced by t-SNE analysis. Experimental results on different restoration
tasks demonstrate that the proposed method achieves superior performance with
only 2.75M parameters.

</details>


### [175] [NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction](https://arxiv.org/abs/2508.07897)
*Tianle Zeng,Junlei Hu,Gerardo Loza Galindo,Sharib Ali,Duygu Sarikaya,Pietro Valdastri,Dominic Jones*

Main category: cs.CV

TL;DR: 提出了一种动态高斯Splatting技术，解决手术图像数据集稀缺问题，生成高质量合成数据，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动方法需要大量高质量标注数据，限制了手术数据科学的应用。

Method: 使用动态高斯模型表示手术场景，结合动态训练调整策略和自动标注方法。

Result: 生成的照片级合成数据在PSNR上表现最佳（29.87），模型性能提升15%。

Conclusion: 该方法有效解决了数据稀缺问题，显著提升了手术自动化模型的性能。

Abstract: Computer vision-based technologies significantly enhance surgical automation
by advancing tool tracking, detection, and localization. However, Current
data-driven approaches are data-voracious, requiring large, high-quality
labeled image datasets, which limits their application in surgical data
science. Our Work introduces a novel dynamic Gaussian Splatting technique to
address the data scarcity in surgical image datasets. We propose a dynamic
Gaussian model to represent dynamic surgical scenes, enabling the rendering of
surgical instruments from unseen viewpoints and deformations with real tissue
backgrounds. We utilize a dynamic training adjustment strategy to address
challenges posed by poorly calibrated camera poses from real-world scenarios.
Additionally, we propose a method based on dynamic Gaussians for automatically
generating annotations for our synthetic data. For evaluation, we constructed a
new dataset featuring seven scenes with 14,000 frames of tool and camera motion
and tool jaw articulation, with a background of an ex-vivo porcine model. Using
this dataset, we synthetically replicate the scene deformation from the ground
truth data, allowing direct comparisons of synthetic image quality.
Experimental results illustrate that our method generates photo-realistic
labeled image datasets with the highest values in Peak-Signal-to-Noise Ratio
(29.87). We further evaluate the performance of medical-specific neural
networks trained on real and synthetic images using an unseen real-world image
dataset. Our results show that the performance of models trained on synthetic
images generated by the proposed method outperforms those trained with
state-of-the-art standard data augmentation by 10%, leading to an overall
improvement in model performances by nearly 15%.

</details>


### [176] [Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation](https://arxiv.org/abs/2508.07901)
*Bowen Xue,Qixin Yan,Wenjing Wang,Hao Liu,Chen Li*

Main category: cs.CV

TL;DR: 提出了一种轻量级、即插即用的视频生成框架Stand-In，用于身份保留，仅需少量参数和训练数据即可实现高质量视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖过多训练参数且与其他AIGC工具不兼容，需要一种更高效的身份保留方案。

Method: 在预训练视频生成模型中引入条件图像分支，通过受限自注意力和条件位置映射实现身份控制。

Result: 仅需约1%额外参数和2000对训练数据，视频质量和身份保留效果优于全参数训练方法。

Conclusion: Stand-In框架高效且多功能，适用于多种视频生成任务。

Abstract: Generating high-fidelity human videos that match user-specified identities is
important yet challenging in the field of generative AI. Existing methods often
rely on an excessive number of training parameters and lack compatibility with
other AIGC tools. In this paper, we propose Stand-In, a lightweight and
plug-and-play framework for identity preservation in video generation.
Specifically, we introduce a conditional image branch into the pre-trained
video generation model. Identity control is achieved through restricted
self-attentions with conditional position mapping, and can be learned quickly
with only 2000 pairs. Despite incorporating and training just $\sim$1\%
additional parameters, our framework achieves excellent results in video
quality and identity preservation, outperforming other full-parameter training
methods. Moreover, our framework can be seamlessly integrated for other tasks,
such as subject-driven video generation, pose-referenced video generation,
stylization, and face swapping.

</details>


### [177] [CTC Transcription Alignment of the Bullinger Letters: Automatic Improvement of Annotation Quality](https://arxiv.org/abs/2508.07904)
*Marco Peer,Anna Scius-Bertrand,Andreas Fischer*

Main category: cs.CV

TL;DR: 提出一种基于CTC对齐算法的自训练方法，用于解决历史手写文档中的标注错误（如连字符问题），在Bullinger信件数据集中提升了识别性能和对齐精度。


<details>
  <summary>Details</summary>
Motivation: 历史手写文档识别面临手写变异性、文档退化及标注不足的挑战，尤其是连字符问题。

Method: 采用CTC对齐算法，通过动态规划和模型输出概率匹配完整转录与文本行图像，并基于自训练策略迭代优化。

Result: CER提升1.1个百分点，对齐精度提高，且发现较弱模型对齐更准确。

Conclusion: 方法可迭代优化CER和对齐质量，并发布了100页手动校正数据及代码。

Abstract: Handwritten text recognition for historical documents remains challenging due
to handwriting variability, degraded sources, and limited layout-aware
annotations. In this work, we address annotation errors - particularly
hyphenation issues - in the Bullinger correspondence, a large 16th-century
letter collection. We introduce a self-training method based on a CTC alignment
algorithm that matches full transcriptions to text line images using dynamic
programming and model output probabilities trained with the CTC loss. Our
approach improves performance (e.g., by 1.1 percentage points CER with PyLaia)
and increases alignment accuracy. Interestingly, we find that weaker models
yield more accurate alignments, enabling an iterative training strategy. We
release a new manually corrected subset of 100 pages from the Bullinger
dataset, along with our code and benchmarks. Our approach can be applied
iteratively to further improve the CER as well as the alignment quality for
text recognition pipelines. Code and data are available via
https://github.com/andreas-fischer-unifr/nntp.

</details>


### [178] [Generative Video Matting](https://arxiv.org/abs/2508.07905)
*Yongtao Ge,Kangyang Xie,Guangkai Xu,Mingyu Liu,Li Ke,Longtao Huang,Hui Xue,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 论文提出了一种新的视频抠图方法，通过大规模预训练和合成数据生成，结合视频扩散模型的先验知识，显著提升了真实场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统视频抠图方法因缺乏高质量真实数据而泛化能力差，本文旨在通过合成数据和预训练模型解决这一问题。

Method: 提出大规模预训练策略，利用合成和伪标签分割数据集；开发可扩展的合成数据生成流程；设计基于视频扩散模型的新架构。

Result: 在三个基准数据集上表现出色，定性结果显示在多样真实场景中具有强泛化能力。

Conclusion: 通过合成数据和预训练模型，显著提升了视频抠图的性能和泛化能力。

Abstract: Video matting has traditionally been limited by the lack of high-quality
ground-truth data. Most existing video matting datasets provide only
human-annotated imperfect alpha and foreground annotations, which must be
composited to background images or videos during the training stage. Thus, the
generalization capability of previous methods in real-world scenarios is
typically poor. In this work, we propose to solve the problem from two
perspectives. First, we emphasize the importance of large-scale pre-training by
pursuing diverse synthetic and pseudo-labeled segmentation datasets. We also
develop a scalable synthetic data generation pipeline that can render diverse
human bodies and fine-grained hairs, yielding around 200 video clips with a
3-second duration for fine-tuning. Second, we introduce a novel video matting
approach that can effectively leverage the rich priors from pre-trained video
diffusion models. This architecture offers two key advantages. First, strong
priors play a critical role in bridging the domain gap between synthetic and
real-world scenes. Second, unlike most existing methods that process video
matting frame-by-frame and use an independent decoder to aggregate temporal
information, our model is inherently designed for video, ensuring strong
temporal consistency. We provide a comprehensive quantitative evaluation across
three benchmark datasets, demonstrating our approach's superior performance,
and present comprehensive qualitative results in diverse real-world scenes,
illustrating the strong generalization capability of our method. The code is
available at https://github.com/aim-uofa/GVM.

</details>


### [179] [Mem4D: Decoupling Static and Dynamic Memory for Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.07908)
*Xudong Cai,Shuo Wang,Peng Wang,Yongcai Wang,Zhaoxin Fan,Wanting Li,Tianbao Zhang,Jianrong Tao,Yeying Jin,Deying Li*

Main category: cs.CV

TL;DR: Mem4D提出了一种双内存架构，解决了动态场景重建中的内存需求困境，同时保持静态结构的长期稳定性和动态运动的高保真细节。


<details>
  <summary>Details</summary>
Motivation: 动态场景的单目视频密集几何重建存在内存需求困境，现有方法在静态结构的长期稳定性和动态运动的高保真细节之间难以平衡。

Method: Mem4D通过双内存架构（TDM和PSM）分别处理动态内容和静态结构，TDM捕获高频运动细节，PSM保留长期空间信息。

Result: 实验表明，Mem4D在保持高效的同时，实现了最先进的性能。

Conclusion: Mem4D有效解决了动态场景重建中的内存需求困境，同时兼顾静态和动态内容的高质量重建。

Abstract: Reconstructing dense geometry for dynamic scenes from a monocular video is a
critical yet challenging task. Recent memory-based methods enable efficient
online reconstruction, but they fundamentally suffer from a Memory Demand
Dilemma: The memory representation faces an inherent conflict between the
long-term stability required for static structures and the rapid, high-fidelity
detail retention needed for dynamic motion. This conflict forces existing
methods into a compromise, leading to either geometric drift in static
structures or blurred, inaccurate reconstructions of dynamic objects. To
address this dilemma, we propose Mem4D, a novel framework that decouples the
modeling of static geometry and dynamic motion. Guided by this insight, we
design a dual-memory architecture: 1) The Transient Dynamics Memory (TDM)
focuses on capturing high-frequency motion details from recent frames, enabling
accurate and fine-grained modeling of dynamic content; 2) The Persistent
Structure Memory (PSM) compresses and preserves long-term spatial information,
ensuring global consistency and drift-free reconstruction for static elements.
By alternating queries to these specialized memories, Mem4D simultaneously
maintains static geometry with global consistency and reconstructs dynamic
elements with high fidelity. Experiments on challenging benchmarks demonstrate
that our method achieves state-of-the-art or competitive performance while
maintaining high efficiency. Codes will be publicly available.

</details>


### [180] [RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering](https://arxiv.org/abs/2508.07918)
*Xing Zi,Jinghao Xiao,Yunxiao Shi,Xian Tao,Jun Li,Ali Braytee,Mukesh Prasad*

Main category: cs.CV

TL;DR: 论文介绍了RSVLM-QA数据集，一个用于遥感视觉问答（VQA）的大规模、内容丰富的数据集，旨在解决现有数据集在标注丰富性、问题多样性和推理能力评估上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有遥感VQA数据集在标注丰富性、问题多样性和推理能力评估上存在局限，需要更全面的数据集推动研究。

Method: 通过整合多个遥感分割和检测数据集，采用双轨标注生成流程：利用GPT-4.1生成详细标注和复杂VQA对，并开发专门流程生成计数QA对。

Result: RSVLM-QA包含13,820张图像和162,373个VQA对，标注丰富且问题多样，实验表明其能有效评估主流视觉语言模型在遥感领域的理解和推理能力。

Conclusion: RSVLM-QA将成为遥感VQA和视觉语言模型研究的重要资源，推动领域发展。

Abstract: Visual Question Answering (VQA) in remote sensing (RS) is pivotal for
interpreting Earth observation data. However, existing RS VQA datasets are
constrained by limitations in annotation richness, question diversity, and the
assessment of specific reasoning capabilities. This paper introduces RSVLM-QA
dataset, a new large-scale, content-rich VQA dataset for the RS domain.
RSVLM-QA is constructed by integrating data from several prominent RS
segmentation and detection datasets: WHU, LoveDA, INRIA, and iSAID. We employ
an innovative dual-track annotation generation pipeline. Firstly, we leverage
Large Language Models (LLMs), specifically GPT-4.1, with meticulously designed
prompts to automatically generate a suite of detailed annotations including
image captions, spatial relations, and semantic tags, alongside complex
caption-based VQA pairs. Secondly, to address the challenging task of object
counting in RS imagery, we have developed a specialized automated process that
extracts object counts directly from the original segmentation data; GPT-4.1
then formulates natural language answers from these counts, which are paired
with preset question templates to create counting QA pairs. RSVLM-QA comprises
13,820 images and 162,373 VQA pairs, featuring extensive annotations and
diverse question types. We provide a detailed statistical analysis of the
dataset and a comparison with existing RS VQA benchmarks, highlighting the
superior depth and breadth of RSVLM-QA's annotations. Furthermore, we conduct
benchmark experiments on Six mainstream Vision Language Models (VLMs),
demonstrating that RSVLM-QA effectively evaluates and challenges the
understanding and reasoning abilities of current VLMs in the RS domain. We
believe RSVLM-QA will serve as a pivotal resource for the RS VQA and VLM
research communities, poised to catalyze advancements in the field.

</details>


### [181] [Safeguarding Generative AI Applications in Preclinical Imaging through Hybrid Anomaly Detection](https://arxiv.org/abs/2508.07923)
*Jakub Binda,Valentina Paneta,Vasileios Eleftheriadis,Hongkyou Chung,Panagiotis Papadimitroulas,Neo Christopher Chung*

Main category: cs.CV

TL;DR: 论文提出了一种混合异常检测框架，用于增强生成式AI在核医学数据合成中的可靠性和安全性。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在核医学中潜力巨大，但需要确保其行为的可靠性和安全性，尤其是在高风险的生物医学影像领域。

Method: 开发并实施了一种混合异常检测框架，应用于Pose2Xray和DosimetrEYE两个系统，以提高模型可靠性。

Result: 异常检测框架显著提升了系统的可靠性，减少了人工监督需求，并支持实时质量控制。

Conclusion: 该方法增强了生成式AI在临床前环境中的工业可行性，提高了鲁棒性、可扩展性和法规合规性。

Abstract: Generative AI holds great potentials to automate and enhance data synthesis
in nuclear medicine. However, the high-stakes nature of biomedical imaging
necessitates robust mechanisms to detect and manage unexpected or erroneous
model behavior. We introduce development and implementation of a hybrid anomaly
detection framework to safeguard GenAI models in BIOEMTECH's eyes(TM) systems.
Two applications are demonstrated: Pose2Xray, which generates synthetic X-rays
from photographic mouse images, and DosimetrEYE, which estimates 3D radiation
dose maps from 2D SPECT/CT scans. In both cases, our outlier detection (OD)
enhances reliability, reduces manual oversight, and supports real-time quality
control. This approach strengthens the industrial viability of GenAI in
preclinical settings by increasing robustness, scalability, and regulatory
compliance.

</details>


### [182] [TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video Temporal Grounding](https://arxiv.org/abs/2508.07925)
*Jin-Seop Lee,SungJoon Lee,Jaehan Ahn,YunSeok Choi,Jee-Hyong Lee*

Main category: cs.CV

TL;DR: 论文提出了一种名为TAG的零样本视频时间定位方法，通过时间池化、时间相干性聚类和相似性调整，解决了现有方法的语义碎片化和相似性分布偏差问题，无需训练即可实现高性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本视频时间定位方法存在语义碎片化和相似性分布偏差问题，且依赖昂贵的LLMs推理。

Method: 提出TAG方法，结合时间池化、时间相干性聚类和相似性调整，无需训练即可捕捉视频时间上下文并修正相似性分布。

Result: 在Charades-STA和ActivityNet Captions基准数据集上达到最先进性能，且不依赖LLMs。

Conclusion: TAG是一种简单有效的零样本视频时间定位方法，解决了现有方法的局限性，性能优越。

Abstract: Video Temporal Grounding (VTG) aims to extract relevant video segments based
on a given natural language query. Recently, zero-shot VTG methods have gained
attention by leveraging pretrained vision-language models (VLMs) to localize
target moments without additional training. However, existing approaches suffer
from semantic fragmentation, where temporally continuous frames sharing the
same semantics are split across multiple segments. When segments are
fragmented, it becomes difficult to predict an accurate target moment that
aligns with the text query. Also, they rely on skewed similarity distributions
for localization, making it difficult to select the optimal segment.
Furthermore, they heavily depend on the use of LLMs which require expensive
inferences. To address these limitations, we propose a \textit{TAG}, a simple
yet effective Temporal-Aware approach for zero-shot video temporal Grounding,
which incorporates temporal pooling, temporal coherence clustering, and
similarity adjustment. Our proposed method effectively captures the temporal
context of videos and addresses distorted similarity distributions without
training. Our approach achieves state-of-the-art results on Charades-STA and
ActivityNet Captions benchmark datasets without rely on LLMs. Our code is
available at https://github.com/Nuetee/TAG

</details>


### [183] [VOIDFace: A Privacy-Preserving Multi-Network Face Recognition With Enhanced Security](https://arxiv.org/abs/2508.07960)
*Ajnas Muhammed,Iurri Medvedev,Nuno Gonçalves*

Main category: cs.CV

TL;DR: VOIDFace是一种新型面部识别框架，通过视觉秘密共享消除数据复制，提升数据控制，同时提出基于补丁的多训练网络，保护隐私并保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决面部识别系统中数据复制和隐私控制问题，提升数据安全和用户控制权。

Method: 使用视觉秘密共享存储训练数据，提出基于补丁的多训练网络。

Result: 在VGGFace2数据集上验证，VOIDFace实现了数据控制、安全和隐私保护，同时保持高性能。

Conclusion: VOIDFace为面部识别系统提供了隐私保护、数据控制和高效性能的解决方案。

Abstract: Advancement of machine learning techniques, combined with the availability of
large-scale datasets, has significantly improved the accuracy and efficiency of
facial recognition. Modern facial recognition systems are trained using large
face datasets collected from diverse individuals or public repositories.
However, for training, these datasets are often replicated and stored in
multiple workstations, resulting in data replication, which complicates
database management and oversight. Currently, once a user submits their face
for dataset preparation, they lose control over how their data is used, raising
significant privacy and ethical concerns. This paper introduces VOIDFace, a
novel framework for facial recognition systems that addresses two major issues.
First, it eliminates the need of data replication and improves data control to
securely store training face data by using visual secret sharing. Second, it
proposes a patch-based multi-training network that uses this novel training
data storage mechanism to develop a robust, privacy-preserving facial
recognition system. By integrating these advancements, VOIDFace aims to improve
the privacy, security, and efficiency of facial recognition training, while
ensuring greater control over sensitive personal face data. VOIDFace also
enables users to exercise their Right-To-Be-Forgotten property to control their
personal data. Experimental evaluations on the VGGFace2 dataset show that
VOIDFace provides Right-To-Be-Forgotten, improved data control, security, and
privacy while maintaining competitive facial recognition performance. Code is
available at: https://github.com/ajnasmuhammed89/VOIDFace

</details>


### [184] [TrackOR: Towards Personalized Intelligent Operating Rooms Through Robust Tracking](https://arxiv.org/abs/2508.07968)
*Tony Danjun Wang,Christian Heiliger,Nassir Navab,Lennart Bastian*

Main category: cs.CV

TL;DR: TrackOR框架通过3D几何特征实现手术室中长期多人跟踪与重识别，提升关联准确性11%，并支持离线轨迹恢复，为个性化智能系统铺路。


<details>
  <summary>Details</summary>
Motivation: 为手术团队提供智能支持以改善患者结果，需解决长期手术中人员位置一致性问题。

Method: 利用3D几何特征进行在线跟踪和离线轨迹恢复。

Result: 实现11%的关联准确性提升，支持分析就绪的轨迹生成。

Conclusion: 3D几何信息使持久身份跟踪可行，为手术室个性化智能系统提供基础。

Abstract: Providing intelligent support to surgical teams is a key frontier in
automated surgical scene understanding, with the long-term goal of improving
patient outcomes. Developing personalized intelligence for all staff members
requires maintaining a consistent state of who is located where for long
surgical procedures, which still poses numerous computational challenges. We
propose TrackOR, a framework for tackling long-term multi-person tracking and
re-identification in the operating room. TrackOR uses 3D geometric signatures
to achieve state-of-the-art online tracking performance (+11% Association
Accuracy over the strongest baseline), while also enabling an effective offline
recovery process to create analysis-ready trajectories. Our work shows that by
leveraging 3D geometric information, persistent identity tracking becomes
attainable, enabling a critical shift towards the more granular, staff-centric
analyses required for personalized intelligent systems in the operating room.
This new capability opens up various applications, including our proposed
temporal pathway imprints that translate raw tracking data into actionable
insights for improving team efficiency and safety and ultimately providing
personalized support.

</details>


### [185] [Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation](https://arxiv.org/abs/2508.07981)
*Fangyuan Mao,Aiming Hao,Jintao Chen,Dongxia Liu,Xiaokun Feng,Jiashu Zhu,Meiqi Wu,Chubin Chen,Jiahong Wu,Xiangxiang Chu*

Main category: cs.CV

TL;DR: Omni-Effects提出了一种统一的框架，通过LoRA-MoE和SAP技术实现多效果的空间可控生成，解决了现有方法在复合效果生成中的干扰和空间控制问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在视觉特效（VFX）生产中受限于单效果训练，无法实现多效果的空间可控复合生成。

Method: 提出了Omni-Effects框架，包含LoRA-MoE（专家LoRA组）和SAP（空间感知提示），并引入IIF模块隔离控制信号。

Result: 实验表明，Omni-Effects能够精确控制空间位置并生成多样效果，用户可指定效果类别和位置。

Conclusion: Omni-Effects为多效果VFX生成提供了高效且可控的解决方案。

Abstract: Visual effects (VFX) are essential visual enhancements fundamental to modern
cinematic production. Although video generation models offer cost-efficient
solutions for VFX production, current methods are constrained by per-effect
LoRA training, which limits generation to single effects. This fundamental
limitation impedes applications that require spatially controllable composite
effects, i.e., the concurrent generation of multiple effects at designated
locations. However, integrating diverse effects into a unified framework faces
major challenges: interference from effect variations and spatial
uncontrollability during multi-VFX joint training. To tackle these challenges,
we propose Omni-Effects, a first unified framework capable of generating
prompt-guided effects and spatially controllable composite effects. The core of
our framework comprises two key innovations: (1) LoRA-based Mixture of Experts
(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects
within a unified model while effectively mitigating cross-task interference.
(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the
text token, enabling precise spatial control. Furthermore, we introduce an
Independent-Information Flow (IIF) module integrated within the SAP, isolating
the control signals corresponding to individual effects to prevent any unwanted
blending. To facilitate this research, we construct a comprehensive VFX dataset
Omni-VFX via a novel data collection pipeline combining image editing and
First-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX
evaluation framework for validating model performance. Extensive experiments
demonstrate that Omni-Effects achieves precise spatial control and diverse
effect generation, enabling users to specify both the category and location of
desired effects.

</details>


### [186] [The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility](https://arxiv.org/abs/2508.07989)
*Xiantao Zhang*

Main category: cs.CV

TL;DR: 论文指出多模态大语言模型（MLLMs）在盲人和视障人士（BVI）辅助技术中存在‘电梯问题’，即无法感知电梯运行方向，揭示了‘隐式运动盲区’的深层限制。


<details>
  <summary>Details</summary>
Motivation: 提升MLLMs在动态环境中的可信度和实用性，尤其是为BVI社区提供更可靠的辅助技术。

Method: 通过分析现有模型的‘隐式运动盲区’，提出从语义识别转向物理感知的范式转变。

Result: 发现现有模型因帧采样范式而无法感知连续低信号运动，影响用户信任。

Conclusion: 呼吁开发以人为中心的新基准，强调安全性、可靠性和动态环境中的实际需求。

Abstract: Multimodal Large Language Models (MLLMs) hold immense promise as assistive
technologies for the blind and visually impaired (BVI) community. However, we
identify a critical failure mode that undermines their trustworthiness in
real-world applications. We introduce the Escalator Problem -- the inability of
state-of-the-art models to perceive an escalator's direction of travel -- as a
canonical example of a deeper limitation we term Implicit Motion Blindness.
This blindness stems from the dominant frame-sampling paradigm in video
understanding, which, by treating videos as discrete sequences of static
images, fundamentally struggles to perceive continuous, low-signal motion. As a
position paper, our contribution is not a new model but rather to: (I) formally
articulate this blind spot, (II) analyze its implications for user trust, and
(III) issue a call to action. We advocate for a paradigm shift from purely
semantic recognition towards robust physical perception and urge the
development of new, human-centered benchmarks that prioritize safety,
reliability, and the genuine needs of users in dynamic environments.

</details>


### [187] [Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models](https://arxiv.org/abs/2508.07996)
*Thinesh Thiyakesan Ponbagavathi,Chengzheng Yang,Alina Roitberg*

Main category: cs.CV

TL;DR: 论文提出了一种名为ProGraD的方法，通过可学习的群体提示和轻量级GroupContext Transformer，提升了Vision Foundation Models在群体活动检测中的表现，尤其在多群体复杂场景中效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有的Vision Foundation Models（如DinoV2）虽然在对象中心数据上表现优异，但在群体动态建模方面研究不足，直接替换CNN骨干网络效果有限，需要更结构化的群体感知推理。

Method: ProGraD方法包括：1）可学习的群体提示，引导VFM关注社交配置；2）轻量级两层GroupContext Transformer，推断参与者-群体关联和集体行为。

Result: 在Cafe和Social-CAD两个GAD基准测试中，ProGraD均超越现有方法，尤其是在多群体场景中，Group mAP@1.0和Group mAP@0.5分别提升6.5%和8.2%，仅需10M可训练参数。

Conclusion: ProGraD不仅提升了性能，还生成了可解释的注意力图，为参与者-群体推理提供了洞察。代码和模型将公开。

Abstract: Group Activity Detection (GAD) involves recognizing social groups and their
collective behaviors in videos. Vision Foundation Models (VFMs), like DinoV2,
offer excellent features, but are pretrained primarily on object-centric data
and remain underexplored for modeling group dynamics. While they are a
promising alternative to highly task-specific GAD architectures that require
full fine-tuning, our initial investigation reveals that simply swapping CNN
backbones used in these methods with VFMs brings little gain, underscoring the
need for structured, group-aware reasoning on top.
  We introduce Prompt-driven Group Activity Detection (ProGraD) -- a method
that bridges this gap through 1) learnable group prompts to guide the VFM
attention toward social configurations, and 2) a lightweight two-layer
GroupContext Transformer that infers actor-group associations and collective
behavior. We evaluate our approach on two recent GAD benchmarks: Cafe, which
features multiple concurrent social groups, and Social-CAD, which focuses on
single-group interactions. While we surpass state-of-the-art in both settings,
our method is especially effective in complex multi-group scenarios, where we
yield a gain of 6.5\% (Group mAP\@1.0) and 8.2\% (Group mAP\@0.5) using only
10M trainable parameters. Furthermore, our experiments reveal that ProGraD
produces interpretable attention maps, offering insights into actor-group
reasoning. Code and models will be released.

</details>


### [188] [Sample-aware RandAugment: Search-free Automatic Data Augmentation for Effective Image Recognition](https://arxiv.org/abs/2508.08004)
*Anqi Xiao,Weichen Yu,Hongyuan Yu*

Main category: cs.CV

TL;DR: 提出了一种名为Sample-aware RandAugment (SRA) 的自动数据增强方法，解决了现有方法搜索耗时或性能不足的问题，通过动态调整策略实现高效且高性能的数据增强。


<details>
  <summary>Details</summary>
Motivation: 主流自动数据增强方法存在搜索耗时或性能不足的问题，限制了实际应用。

Method: SRA采用启发式评分模块评估数据复杂度，动态调整增强策略，并结合非对称增强策略优化效果。

Result: 在ImageNet上达到78.31%的Top-1准确率，性能接近基于搜索的方法，且兼容现有增强流程。

Conclusion: SRA是一种简单、高效且通用的自动数据增强方法，适用于未来多种任务。

Abstract: Automatic data augmentation (AutoDA) plays an important role in enhancing the
generalization of neural networks. However, mainstream AutoDA methods often
encounter two challenges: either the search process is excessively
time-consuming, hindering practical application, or the performance is
suboptimal due to insufficient policy adaptation during training. To address
these issues, we propose Sample-aware RandAugment (SRA), an asymmetric,
search-free AutoDA method that dynamically adjusts augmentation policies while
maintaining straightforward implementation. SRA incorporates a heuristic
scoring module that evaluates the complexity of the original training data,
enabling the application of tailored augmentations for each sample.
Additionally, an asymmetric augmentation strategy is employed to maximize the
potential of this scoring module. In multiple experimental settings, SRA
narrows the performance gap between search-based and search-free AutoDA
methods, achieving a state-of-the-art Top-1 accuracy of 78.31\% on ImageNet
with ResNet-50. Notably, SRA demonstrates good compatibility with existing
augmentation pipelines and solid generalization across new tasks, without
requiring hyperparameter tuning. The pretrained models leveraging SRA also
enhance recognition in downstream object detection tasks. SRA represents a
promising step towards simpler, more effective, and practical AutoDA designs
applicable to a variety of future tasks. Our code is available at
\href{https://github.com/ainieli/Sample-awareRandAugment}{https://github.com/ainieli/Sample-awareRandAugment

</details>


### [189] [Mitigating Biases in Surgical Operating Rooms with Geometry](https://arxiv.org/abs/2508.08028)
*Tony Danjun Wang,Tobias Czempiel,Nassir Navab,Lennart Bastian*

Main category: cs.CV

TL;DR: 论文探讨了深度学习模型在手术室（OR）中因标准化服装导致的学习偏差问题，提出使用3D点云序列方法避免外观干扰，实验表明几何方法在真实临床环境中表现更优。


<details>
  <summary>Details</summary>
Motivation: 手术室中标准化服装（如手术服）掩盖了识别特征，导致深度学习模型依赖无关视觉线索（如鞋子、眼镜），影响任务准确性。

Method: 通过梯度显著性分析揭示CNN模型的偏差，提出使用3D点云序列编码人员特征，分离形状和运动模式与外观干扰。

Result: 实验显示，在真实临床环境中，RGB模型准确率下降12%，而几何方法表现更稳定，验证了几何特征的有效性。

Conclusion: 几何表示能捕捉更有意义的生物特征，为手术室中鲁棒的人员建模提供了新方向。

Abstract: Deep neural networks are prone to learning spurious correlations, exploiting
dataset-specific artifacts rather than meaningful features for prediction. In
surgical operating rooms (OR), these manifest through the standardization of
smocks and gowns that obscure robust identifying landmarks, introducing model
bias for tasks related to modeling OR personnel. Through gradient-based
saliency analysis on two public OR datasets, we reveal that CNN models succumb
to such shortcuts, fixating on incidental visual cues such as footwear beneath
surgical gowns, distinctive eyewear, or other role-specific identifiers.
Avoiding such biases is essential for the next generation of intelligent
assistance systems in the OR, which should accurately recognize personalized
workflow traits, such as surgical skill level or coordination with other staff
members. We address this problem by encoding personnel as 3D point cloud
sequences, disentangling identity-relevant shape and motion patterns from
appearance-based confounders. Our experiments demonstrate that while RGB and
geometric methods achieve comparable performance on datasets with apparent
simulation artifacts, RGB models suffer a 12% accuracy drop in realistic
clinical settings with decreased visual diversity due to standardizations. This
performance gap confirms that geometric representations capture more meaningful
biometric features, providing an avenue to developing robust methods of
modeling humans in the OR.

</details>


### [190] [TRIDE: A Text-assisted Radar-Image weather-aware fusion network for Depth Estimation](https://arxiv.org/abs/2508.08038)
*Huawei Sun,Zixu Wang,Hao Feng,Julius Ott,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

TL;DR: 本文提出了一种结合雷达和相机融合的深度估计方法TRIDE，通过引入天气感知模块和文本特征提取，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有雷达-相机融合算法未考虑天气影响，且视觉语言模型在深度估计中的应用尚未充分探索。

Method: 提出文本生成策略和特征融合技术，结合雷达点信息增强文本特征提取，并设计天气感知融合模块动态调整雷达权重。

Result: 在nuScenes数据集上，MAE和RMSE分别提升12.87%和9.08%。

Conclusion: TRIDE通过多模态融合和天气适应机制，显著提升了深度估计的准确性和鲁棒性。

Abstract: Depth estimation, essential for autonomous driving, seeks to interpret the 3D
environment surrounding vehicles. The development of radar sensors, known for
their cost-efficiency and robustness, has spurred interest in radar-camera
fusion-based solutions. However, existing algorithms fuse features from these
modalities without accounting for weather conditions, despite radars being
known to be more robust than cameras under adverse weather. Additionally, while
Vision-Language models have seen rapid advancement, utilizing language
descriptions alongside other modalities for depth estimation remains an open
challenge. This paper first introduces a text-generation strategy along with
feature extraction and fusion techniques that can assist monocular depth
estimation pipelines, leading to improved accuracy across different algorithms
on the KITTI dataset. Building on this, we propose TRIDE, a radar-camera fusion
algorithm that enhances text feature extraction by incorporating radar point
information. To address the impact of weather on sensor performance, we
introduce a weather-aware fusion block that adaptively adjusts radar weighting
based on current weather conditions. Our method, benchmarked on the nuScenes
dataset, demonstrates performance gains over the state-of-the-art, achieving a
12.87% improvement in MAE and a 9.08% improvement in RMSE. Code:
https://github.com/harborsarah/TRIDE

</details>


### [191] [S^2VG: 3D Stereoscopic and Spatial Video Generation via Denoising Frame Matrix](https://arxiv.org/abs/2508.08048)
*Peng Dai,Feitong Tan,Qiangeng Xu,Yihua Huang,David Futschik,Ruofei Du,Sean Fanello,Yinda Zhang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 提出了一种无需姿态估计和额外训练的3D视频生成方法，利用单目视频生成模型通过深度估计和帧矩阵修复框架生成沉浸式3D视频。


<details>
  <summary>Details</summary>
Motivation: 解决现有视频生成模型在3D立体和空间视频生成方面的不足，提供一种高效且无需额外训练的方法。

Method: 通过深度估计将单目视频映射到预定义视角，利用帧矩阵修复框架合成缺失内容，并采用双更新方案提升修复质量。

Result: 实验表明，该方法在多种生成模型（如Sora、Lumiere等）上显著优于现有方法。

Conclusion: 该方法为3D视频生成提供了一种高效且通用的解决方案，无需额外训练或姿态估计。

Abstract: While video generation models excel at producing high-quality monocular
videos, generating 3D stereoscopic and spatial videos for immersive
applications remains an underexplored challenge. We present a pose-free and
training-free method that leverages an off-the-shelf monocular video generation
model to produce immersive 3D videos. Our approach first warps the generated
monocular video into pre-defined camera viewpoints using estimated depth
information, then applies a novel \textit{frame matrix} inpainting framework.
This framework utilizes the original video generation model to synthesize
missing content across different viewpoints and timestamps, ensuring spatial
and temporal consistency without requiring additional model fine-tuning.
Moreover, we develop a \dualupdate~scheme that further improves the quality of
video inpainting by alleviating the negative effects propagated from
disoccluded areas in the latent space. The resulting multi-view videos are then
adapted into stereoscopic pairs or optimized into 4D Gaussians for spatial
video synthesis. We validate the efficacy of our proposed method by conducting
experiments on videos from various generative models, such as Sora, Lumiere,
WALT, and Zeroscope. The experiments demonstrate that our method has a
significant improvement over previous methods. Project page at:
https://daipengwa.github.io/S-2VG_ProjectPage/

</details>


### [192] [PrIINeR: Towards Prior-Informed Implicit Neural Representations for Accelerated MRI](https://arxiv.org/abs/2508.08058)
*Ziad Al-Haj Hemidi,Eytan Kats,Mattias P. Heinrich*

Main category: cs.CV

TL;DR: PrIINeR是一种基于INR的MRI重建方法，通过整合预训练深度学习模型的知识，解决了高加速因子下图像质量下降的问题。


<details>
  <summary>Details</summary>
Motivation: 加速MRI扫描时间会降低图像质量，现有INR方法在高加速因子下表现不佳。

Method: PrIINeR结合预训练模型的先验知识，通过双数据一致性和实例优化改进重建。

Result: 在NYU fastMRI数据集上，PrIINeR优于现有INR和学习方法，显著提升图像质量和去伪影能力。

Conclusion: PrIINeR为高质量加速MRI重建提供了可靠方案，结合了深度学习和INR技术。

Abstract: Accelerating Magnetic Resonance Imaging (MRI) reduces scan time but often
degrades image quality. While Implicit Neural Representations (INRs) show
promise for MRI reconstruction, they struggle at high acceleration factors due
to weak prior constraints, leading to structural loss and aliasing artefacts.
To address this, we propose PrIINeR, an INR-based MRI reconstruction method
that integrates prior knowledge from pre-trained deep learning models into the
INR framework. By combining population-level knowledge with instance-based
optimization and enforcing dual data consistency, PrIINeR aligns both with the
acquired k-space data and the prior-informed reconstruction. Evaluated on the
NYU fastMRI dataset, our method not only outperforms state-of-the-art INR-based
approaches but also improves upon several learning-based state-of-the-art
methods, significantly improving structural preservation and fidelity while
effectively removing aliasing artefacts.PrIINeR bridges deep learning and
INR-based techniques, offering a more reliable solution for high-quality,
accelerated MRI reconstruction. The code is publicly available on
https://github.com/multimodallearning/PrIINeR.

</details>


### [193] [Investigating the Design Space of Visual Grounding in Multimodal Large Language Model](https://arxiv.org/abs/2508.08066)
*Weitai Kang,Weiming Zhuang,Zhizhong Li,Yan Yan,Lingjuan Lyu*

Main category: cs.CV

TL;DR: 该论文系统研究了多模态大语言模型（MLLMs）在视觉定位（VG）任务中的设计选择，通过LLaVA-1.5模型验证了不同设计对性能的影响，并提出了优化方法，最终在RefCOCO/+/g数据集上提升了5.6%-7.0%的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在MLLMs的视觉定位任务中缺乏系统性的设计验证，导致性能提升受限。本文旨在填补这一空白，通过全面分析设计选择对VG性能的影响。

Method: 使用LLaVA-1.5模型，研究了两种关键设计：视觉定位范式的选择和接地数据的设计，并通过消融实验验证其效果。

Result: 研究发现优化后的设计显著提升了VG性能，在RefCOCO/+/g数据集上分别提升了5.6%、6.9%和7.0%。

Conclusion: 本文为MLLMs在视觉定位任务中的设计提供了系统性指导，显著提升了性能，并具有广泛的适用性。

Abstract: Fine-grained multimodal capability in Multimodal Large Language Models
(MLLMs) has emerged as a critical research direction, particularly for tackling
the visual grounding (VG) problem. Despite the strong performance achieved by
existing approaches, they often employ disparate design choices when
fine-tuning MLLMs for VG, lacking systematic verification to support these
designs. To bridge this gap, this paper presents a comprehensive study of
various design choices that impact the VG performance of MLLMs. We conduct our
analysis using LLaVA-1.5, which has been widely adopted in prior empirical
studies of MLLMs. While more recent models exist, we follow this convention to
ensure our findings remain broadly applicable and extendable to other
architectures. We cover two key aspects: (1) exploring different visual
grounding paradigms in MLLMs, identifying the most effective design, and
providing our insights; and (2) conducting ablation studies on the design of
grounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our
findings contribute to a stronger MLLM for VG, achieving improvements of +5.6%
/ +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.

</details>


### [194] [Information Bottleneck-based Causal Attention for Multi-label Medical Image Recognition](https://arxiv.org/abs/2508.08069)
*Xiaoxiao Cui,Yiran Li,Kai He,Shanzhi Jiang,Mengli Xue,Wentao Li,Junhong Leng,Zhi Liu,Lizhen Cui,Shuo Li*

Main category: cs.CV

TL;DR: 提出了一种基于信息瓶颈的因果注意力方法（IBCA），用于医学图像的多标签分类，通过高斯混合多标签空间注意力过滤无关信息，并通过对比增强干预减少虚假注意力。


<details>
  <summary>Details</summary>
Motivation: 当前方法在医学图像多标签分类中难以区分因果和虚假注意力，导致诊断准确性受限。

Method: 使用高斯混合多标签空间注意力捕捉类别特定模式，结合对比增强干预减少虚假注意力和噪声。

Result: 在Endo和MuReD数据集上，IBCA在多个指标上显著优于其他方法，如CR、OR和mAP。

Conclusion: IBCA能有效学习类别特定注意力，提升医学图像多标签分类的准确性和可解释性。

Abstract: Multi-label classification (MLC) of medical images aims to identify multiple
diseases and holds significant clinical potential. A critical step is to learn
class-specific features for accurate diagnosis and improved interpretability
effectively. However, current works focus primarily on causal attention to
learn class-specific features, yet they struggle to interpret the true cause
due to the inadvertent attention to class-irrelevant features. To address this
challenge, we propose a new structural causal model (SCM) that treats
class-specific attention as a mixture of causal, spurious, and noisy factors,
and a novel Information Bottleneck-based Causal Attention (IBCA) that is
capable of learning the discriminative class-specific attention for MLC of
medical images. Specifically, we propose learning Gaussian mixture multi-label
spatial attention to filter out class-irrelevant information and capture each
class-specific attention pattern. Then a contrastive enhancement-based causal
intervention is proposed to gradually mitigate the spurious attention and
reduce noise information by aligning multi-head attention with the Gaussian
mixture multi-label spatial. Quantitative and ablation results on Endo and
MuReD show that IBCA outperforms all methods. Compared to the second-best
results for each metric, IBCA achieves improvements of 6.35\% in CR, 7.72\% in
OR, and 5.02\% in mAP for MuReD, 1.47\% in CR, and 1.65\% in CF1, and 1.42\% in
mAP for Endo.

</details>


### [195] [ME-TST+: Micro-expression Analysis via Temporal State Transition with ROI Relationship Awareness](https://arxiv.org/abs/2508.08082)
*Zizheng Guo,Bochao Zou,Junbao Zhuo,Huimin Ma*

Main category: cs.CV

TL;DR: 论文提出ME-TST和ME-TST+两种基于状态空间模型的架构，通过时间状态转换机制改进微表情分析，实现视频级回归和多粒度ROI建模，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在微表情分析中存在窗口长度固定、分类硬性以及任务分离的问题，限制了实际应用效果。

Method: 提出ME-TST和ME-TST+架构，利用时间状态转换机制替代传统窗口分类，引入多粒度ROI建模和慢快Mamba框架，并设计特征和结果层面的协同策略。

Result: 实验表明，所提方法在性能上达到最先进水平。

Conclusion: 通过状态空间模型和协同策略，论文有效解决了微表情分析中的关键问题，显著提升了性能。

Abstract: Micro-expressions (MEs) are regarded as important indicators of an
individual's intrinsic emotions, preferences, and tendencies. ME analysis
requires spotting of ME intervals within long video sequences and recognition
of their corresponding emotional categories. Previous deep learning approaches
commonly employ sliding-window classification networks. However, the use of
fixed window lengths and hard classification presents notable limitations in
practice. Furthermore, these methods typically treat ME spotting and
recognition as two separate tasks, overlooking the essential relationship
between them. To address these challenges, this paper proposes two state space
model-based architectures, namely ME-TST and ME-TST+, which utilize temporal
state transition mechanisms to replace conventional window-level classification
with video-level regression. This enables a more precise characterization of
the temporal dynamics of MEs and supports the modeling of MEs with varying
durations. In ME-TST+, we further introduce multi-granularity ROI modeling and
the slowfast Mamba framework to alleviate information loss associated with
treating ME analysis as a time-series task. Additionally, we propose a synergy
strategy for spotting and recognition at both the feature and result levels,
leveraging their intrinsic connection to enhance overall analysis performance.
Extensive experiments demonstrate that the proposed methods achieve
state-of-the-art performance. The codes are available at
https://github.com/zizheng-guo/ME-TST.

</details>


### [196] [MDD-Net: Multimodal Depression Detection through Mutual Transformer](https://arxiv.org/abs/2508.08093)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Hamdi Altaheri,Lobna Nassar,Fakhri Karray*

Main category: cs.CV

TL;DR: 提出了一种多模态抑郁症检测网络（MDD-Net），利用社交媒体中的声学和视觉数据，通过互变压器高效提取和融合多模态特征，显著提升了抑郁症检测性能。


<details>
  <summary>Details</summary>
Motivation: 抑郁症严重影响个体身心健康，社交媒体数据的易获取性为心理健康研究提供了新途径。

Method: MDD-Net包含声学特征提取模块、视觉特征提取模块、互变压器模块和检测层，用于多模态特征融合与抑郁症检测。

Result: 在D-Vlog数据集上的实验表明，MDD-Net的F1-Score比现有技术高出17.37%。

Conclusion: MDD-Net在多模态抑郁症检测中表现出卓越性能，为心理健康研究提供了有效工具。

Abstract: Depression is a major mental health condition that severely impacts the
emotional and physical well-being of individuals. The simple nature of data
collection from social media platforms has attracted significant interest in
properly utilizing this information for mental health research. A Multimodal
Depression Detection Network (MDD-Net), utilizing acoustic and visual data
obtained from social media networks, is proposed in this work where mutual
transformers are exploited to efficiently extract and fuse multimodal features
for efficient depression detection. The MDD-Net consists of four core modules:
an acoustic feature extraction module for retrieving relevant acoustic
attributes, a visual feature extraction module for extracting significant
high-level patterns, a mutual transformer for computing the correlations among
the generated features and fusing these features from multiple modalities, and
a detection layer for detecting depression using the fused feature
representations. The extensive experiments are performed using the multimodal
D-Vlog dataset, and the findings reveal that the developed multimodal
depression detection network surpasses the state-of-the-art by up to 17.37% for
F1-Score, demonstrating the greater performance of the proposed system. The
source code is accessible at
https://github.com/rezwanh001/Multimodal-Depression-Detection.

</details>


### [197] [3D Plant Root Skeleton Detection and Extraction](https://arxiv.org/abs/2508.08094)
*Jiakai Lin,Jinchang Zhang,Ge Jin,Wenzhan Song,Tianming Liu,Guoyu Lu*

Main category: cs.CV

TL;DR: 提出了一种从少量图像中高效提取植物根系3D骨架的方法，验证了其有效性，并展示了在自动化育种机器人中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 植物根系结构复杂且缺乏纹理信息，传统2D研究难以满足需求，3D根系表型信息对研究遗传性状至关重要。

Method: 包括侧根检测与匹配、三角测量提取侧根骨架结构，以及侧根与主根整合。

Result: 提取的3D根系骨架与真实结构高度相似，验证了模型的有效性。

Conclusion: 该方法可提升育种效率，减少人工干预，推动现代农业智能化发展。

Abstract: Plant roots typically exhibit a highly complex and dense architecture,
incorporating numerous slender lateral roots and branches, which significantly
hinders the precise capture and modeling of the entire root system.
Additionally, roots often lack sufficient texture and color information, making
it difficult to identify and track root traits using visual methods. Previous
research on roots has been largely confined to 2D studies; however, exploring
the 3D architecture of roots is crucial in botany. Since roots grow in real 3D
space, 3D phenotypic information is more critical for studying genetic traits
and their impact on root development. We have introduced a 3D root skeleton
extraction method that efficiently derives the 3D architecture of plant roots
from a few images. This method includes the detection and matching of lateral
roots, triangulation to extract the skeletal structure of lateral roots, and
the integration of lateral and primary roots. We developed a highly complex
root dataset and tested our method on it. The extracted 3D root skeletons
showed considerable similarity to the ground truth, validating the
effectiveness of the model. This method can play a significant role in
automated breeding robots. Through precise 3D root structure analysis, breeding
robots can better identify plant phenotypic traits, especially root structure
and growth patterns, helping practitioners select seeds with superior root
systems. This automated approach not only improves breeding efficiency but also
reduces manual intervention, making the breeding process more intelligent and
efficient, thus advancing modern agriculture.

</details>


### [198] [TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning](https://arxiv.org/abs/2508.08098)
*Junzhe Xu,Yuyang Yin,Xi Chen*

Main category: cs.CV

TL;DR: TBAC-UniImage是一种新型多模态理解与生成统一模型，通过深度整合预训练扩散模型和多模态大语言模型（MLLM），解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型统一方法存在浅层连接或计算成本高的问题，需要一种更高效且深入的多模态统一方法。

Method: 利用MLLM多个中间层的表征作为扩散模型的生成条件，而非仅依赖最终隐藏状态，实现更深入的理解与生成统一。

Result: TBAC-UniImage实现了更深层次、更细粒度的理解与生成统一。

Conclusion: 该方法为多模态统一模型提供了新的范式，显著提升了性能。

Abstract: This paper introduces TBAC-UniImage, a novel unified model for multimodal
understanding and generation. We achieve this by deeply integrating a
pre-trained Diffusion Model, acting as a generative ladder, with a Multimodal
Large Language Model (MLLM). Previous diffusion-based unified models face two
primary limitations. One approach uses only the MLLM's final hidden state as
the generative condition. This creates a shallow connection, as the generator
is isolated from the rich, hierarchical representations within the MLLM's
intermediate layers. The other approach, pretraining a unified generative
architecture from scratch, is computationally expensive and prohibitive for
many researchers. To overcome these issues, our work explores a new paradigm.
Instead of relying on a single output, we use representations from multiple,
diverse layers of the MLLM as generative conditions for the diffusion model.
This method treats the pre-trained generator as a ladder, receiving guidance
from various depths of the MLLM's understanding process. Consequently,
TBAC-UniImage achieves a much deeper and more fine-grained unification of
understanding and generation.

</details>


### [199] [Hyperspectral Imaging](https://arxiv.org/abs/2508.08107)
*Danfeng Hong,Chenyu Li,Naoto Yokoya,Bing Zhang,Xiuping Jia,Antonio Plaza,Paolo Gamba,Jon Atli Benediktsson,Jocelyn Chanussot*

Main category: cs.CV

TL;DR: 该论文综述了高光谱成像（HSI）的原理、技术、应用及未来发展方向，强调了其在多领域中的潜力和挑战。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像作为一种非侵入性、无标记的分析技术，能够同时捕捉空间和光谱信息，为材料、化学和生物分析提供了强大工具。本文旨在全面介绍HSI技术及其应用。

Method: 论文从HSI的物理原理和传感器架构入手，详细介绍了数据采集、校准和校正的关键步骤，并总结了常见的数据结构和分析方法，包括降维、分类、光谱解混和深度学习等。

Result: HSI在地球观测、精准农业、生物医学、工业检测、文化遗产和安全等领域有广泛应用，能够揭示亚视觉特征，支持高级监测和决策。

Conclusion: 尽管HSI面临硬件限制、数据复杂性等挑战，但通过计算成像、跨模态融合和自监督学习等新兴技术，未来有望实现实时、嵌入式HSI系统，推动跨学科应用。

Abstract: Hyperspectral imaging (HSI) is an advanced sensing modality that
simultaneously captures spatial and spectral information, enabling
non-invasive, label-free analysis of material, chemical, and biological
properties. This Primer presents a comprehensive overview of HSI, from the
underlying physical principles and sensor architectures to key steps in data
acquisition, calibration, and correction. We summarize common data structures
and highlight classical and modern analysis methods, including dimensionality
reduction, classification, spectral unmixing, and AI-driven techniques such as
deep learning. Representative applications across Earth observation, precision
agriculture, biomedicine, industrial inspection, cultural heritage, and
security are also discussed, emphasizing HSI's ability to uncover sub-visual
features for advanced monitoring, diagnostics, and decision-making. Persistent
challenges, such as hardware trade-offs, acquisition variability, and the
complexity of high-dimensional data, are examined alongside emerging solutions,
including computational imaging, physics-informed modeling, cross-modal fusion,
and self-supervised learning. Best practices for dataset sharing,
reproducibility, and metadata documentation are further highlighted to support
transparency and reuse. Looking ahead, we explore future directions toward
scalable, real-time, and embedded HSI systems, driven by sensor
miniaturization, self-supervised learning, and foundation models. As HSI
evolves into a general-purpose, cross-disciplinary platform, it holds promise
for transformative applications in science, technology, and society.

</details>


### [200] [GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking](https://arxiv.org/abs/2508.08117)
*Xudong Han,Pengcheng Fang,Yueying Tian,Jianhui Yu,Xiaohao Cai,Daniel Roggen,Philip Birch*

Main category: cs.CV

TL;DR: GRASPTrack是一种新颖的深度感知多目标跟踪框架，通过结合单目深度估计和实例分割，解决了传统跟踪方法在遮挡和深度模糊问题上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统基于检测的跟踪方法（TBD）因缺乏几何感知能力，难以解决遮挡和深度模糊问题。

Method: GRASPTrack将单目深度估计和实例分割整合到TBD流程中，生成3D点云，并利用体素化的3D IoU进行空间关联，同时引入深度感知自适应噪声补偿和深度增强的观测中心动量。

Result: 在MOT17、MOT20和DanceTrack基准测试中表现出色，显著提升了复杂场景下的跟踪鲁棒性。

Conclusion: GRASPTrack通过3D几何推理和动态噪声补偿，有效提升了多目标跟踪在遮挡和复杂运动场景中的性能。

Abstract: Multi-object tracking (MOT) in monocular videos is fundamentally challenged
by occlusions and depth ambiguity, issues that conventional
tracking-by-detection (TBD) methods struggle to resolve owing to a lack of
geometric awareness. To address these limitations, we introduce GRASPTrack, a
novel depth-aware MOT framework that integrates monocular depth estimation and
instance segmentation into a standard TBD pipeline to generate high-fidelity 3D
point clouds from 2D detections, thereby enabling explicit 3D geometric
reasoning. These 3D point clouds are then voxelized to enable a precise and
robust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. To
further enhance tracking robustness, our approach incorporates Depth-aware
Adaptive Noise Compensation, which dynamically adjusts the Kalman filter
process noise based on occlusion severity for more reliable state estimation.
Additionally, we propose a Depth-enhanced Observation-Centric Momentum, which
extends the motion direction consistency from the image plane into 3D space to
improve motion-based association cues, particularly for objects with complex
trajectories. Extensive experiments on the MOT17, MOT20, and DanceTrack
benchmarks demonstrate that our method achieves competitive performance,
significantly improving tracking robustness in complex scenes with frequent
occlusions and intricate motion patterns.

</details>


### [201] [Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control](https://arxiv.org/abs/2508.08134)
*Zeqian Long,Mingzhe Zheng,Kunyu Feng,Xinhua Zhang,Hongyu Liu,Harry Yang,Linfeng Zhang,Qifeng Chen,Yue Ma*

Main category: cs.CV

TL;DR: 提出了一种无需训练和掩码的框架Follow-Your-Shape，通过轨迹差异图（TDM）和计划KV注入机制，实现精确可控的形状编辑，同时保留非目标内容。


<details>
  <summary>Details</summary>
Motivation: 现有流式图像编辑模型在大规模形状变换任务中表现不佳，容易误改非目标区域。

Method: 通过计算反转与去噪路径的轨迹差异图（TDM），定位可编辑区域，并采用计划KV注入机制实现稳定编辑。

Result: 实验表明，该方法在形状替换任务中具有更高的编辑能力和视觉保真度。

Conclusion: Follow-Your-Shape框架在形状编辑任务中表现出色，为复杂编辑提供了新思路。

Abstract: While recent flow-based image editing models demonstrate general-purpose
capabilities across diverse tasks, they often struggle to specialize in
challenging scenarios -- particularly those involving large-scale shape
transformations. When performing such structural edits, these methods either
fail to achieve the intended shape change or inadvertently alter non-target
regions, resulting in degraded background quality. We propose
Follow-Your-Shape, a training-free and mask-free framework that supports
precise and controllable editing of object shapes while strictly preserving
non-target content. Motivated by the divergence between inversion and editing
trajectories, we compute a Trajectory Divergence Map (TDM) by comparing
token-wise velocity differences between the inversion and denoising paths. The
TDM enables precise localization of editable regions and guides a Scheduled KV
Injection mechanism that ensures stable and faithful editing. To facilitate a
rigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120
new images and enriched prompt pairs specifically curated for shape-aware
editing. Experiments demonstrate that our method achieves superior editability
and visual fidelity, particularly in tasks requiring large-scale shape
replacement.

</details>


### [202] [FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting](https://arxiv.org/abs/2508.08136)
*Yitong Yang,Yinglin Wang,Changshuo Wang,Huajie Wang,Shuting He*

Main category: cs.CV

TL;DR: FantasyStyle提出了一种基于3DGS的风格迁移框架，通过扩散模型蒸馏解决多视角不一致和VGG特征依赖问题，实现了更高质量的风格化效果。


<details>
  <summary>Details</summary>
Motivation: 当前3DGS风格迁移方法存在多视角不一致导致风格冲突，以及VGG特征难以分离风格和内容的问题。

Method: 1. 多视角频率一致性：通过3D滤波器减少低频分量以增强一致性。2. 可控风格化蒸馏：利用负引导抑制内容泄漏，优化3D高斯分布。

Result: 实验表明，该方法在多种场景和风格下均优于现有技术，具有更高的风格化质量和视觉真实感。

Conclusion: FantasyStyle通过创新的多视角一致性和可控蒸馏技术，显著提升了3D风格迁移的效果。

Abstract: The success of 3DGS in generative and editing applications has sparked
growing interest in 3DGS-based style transfer. However, current methods still
face two major challenges: (1) multi-view inconsistency often leads to style
conflicts, resulting in appearance smoothing and distortion; and (2) heavy
reliance on VGG features, which struggle to disentangle style and content from
style images, often causing content leakage and excessive stylization. To
tackle these issues, we introduce \textbf{FantasyStyle}, a 3DGS-based style
transfer framework, and the first to rely entirely on diffusion model
distillation. It comprises two key components: (1) \textbf{Multi-View Frequency
Consistency}. We enhance cross-view consistency by applying a 3D filter to
multi-view noisy latent, selectively reducing low-frequency components to
mitigate stylized prior conflicts. (2) \textbf{Controllable Stylized
Distillation}. To suppress content leakage from style images, we introduce
negative guidance to exclude undesired content. In addition, we identify the
limitations of Score Distillation Sampling and Delta Denoising Score in 3D
style transfer and remove the reconstruction term accordingly. Building on
these insights, we propose a controllable stylized distillation that leverages
negative guidance to more effectively optimize the 3D Gaussians. Extensive
experiments demonstrate that our method consistently outperforms
state-of-the-art approaches, achieving higher stylization quality and visual
realism across various scenes and styles.

</details>


### [203] [Pindrop it! Audio and Visual Deepfake Countermeasures for Robust Detection and Fine Grained-Localization](https://arxiv.org/abs/2508.08141)
*Nicholas Klein,Hemlata Tak,James Fullwood,Krishna Regmi,Leonidas Spinoulas,Ganesh Sivaraman,Tianxiang Chen,Elie Khoury*

Main category: cs.CV

TL;DR: 论文提出了针对深度伪造视频分类和定位的解决方案，在ACM 1M Deepfakes Detection Challenge中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着视觉和音频生成技术的快速发展，检测合成内容的需求日益迫切，尤其是针对细粒度修改的挑战。

Method: 提交了深度伪造视频分类和定位的方法。

Result: 在时间定位任务中表现最佳，在分类任务的TestA分割中排名前四。

Conclusion: 论文的方法在检测深度伪造视频方面具有显著效果。

Abstract: The field of visual and audio generation is burgeoning with new
state-of-the-art methods. This rapid proliferation of new techniques
underscores the need for robust solutions for detecting synthetic content in
videos. In particular, when fine-grained alterations via localized
manipulations are performed in visual, audio, or both domains, these subtle
modifications add challenges to the detection algorithms. This paper presents
solutions for the problems of deepfake video classification and localization.
The methods were submitted to the ACM 1M Deepfakes Detection Challenge,
achieving the best performance in the temporal localization task and a top four
ranking in the classification task for the TestA split of the evaluation
dataset.

</details>


### [204] [Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning](https://arxiv.org/abs/2508.08165)
*Yan Wang,Da-Wei Zhou,Han-Jia Ye*

Main category: cs.CV

TL;DR: 论文提出了一种结合任务特定和通用适配器（TUNA）的方法，以解决类增量学习中模块选择错误和共享知识忽略的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在类增量学习中冻结预训练网络并使用轻量模块，但模块选择错误和任务特定模块忽视共享知识导致性能下降。

Method: 训练任务特定适配器捕捉关键特征，引入基于熵的选择机制；通过适配器融合构建通用适配器，结合两者预测。

Result: 在多个基准数据集上展示了最先进的性能。

Conclusion: TUNA方法有效结合了任务特定和通用知识，提升了类增量学习的性能。

Abstract: Class-Incremental Learning (CIL) requires a learning system to continually
learn new classes without forgetting. Existing pre-trained model-based CIL
methods often freeze the pre-trained network and adapt to incremental tasks
using additional lightweight modules such as adapters. However, incorrect
module selection during inference hurts performance, and task-specific modules
often overlook shared general knowledge, leading to errors on distinguishing
between similar classes across tasks. To address the aforementioned challenges,
we propose integrating Task-Specific and Universal Adapters (TUNA) in this
paper. Specifically, we train task-specific adapters to capture the most
crucial features relevant to their respective tasks and introduce an
entropy-based selection mechanism to choose the most suitable adapter.
Furthermore, we leverage an adapter fusion strategy to construct a universal
adapter, which encodes the most discriminative features shared across tasks. We
combine task-specific and universal adapter predictions to harness both
specialized and general knowledge during inference. Extensive experiments on
various benchmark datasets demonstrate the state-of-the-art performance of our
approach. Code is available at: https://github.com/LAMDA-CL/ICCV2025-TUNA

</details>


### [205] [ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction](https://arxiv.org/abs/2508.08170)
*Chaojun Ni,Guosheng Zhao,Xiaofeng Wang,Zheng Zhu,Wenkang Qin,Xinze Chen,Guanghong Jia,Guan Huang,Wenjun Mei*

Main category: cs.CV

TL;DR: ReconDreamer-RL框架通过结合视频扩散先验和动力学模型，缩小自动驾驶训练中的sim2real差距，并引入动态对抗代理和轨迹生成器以覆盖更多极端场景。


<details>
  <summary>Details</summary>
Motivation: 解决现有仿真环境与真实世界差异大、难以生成高质量传感器数据的问题。

Method: 结合视频扩散先验和动力学模型构建ReconSimulator，引入动态对抗代理（DAA）和轨迹生成器（CTG）。

Result: 实验表明，ReconDreamer-RL在端到端自动驾驶训练中优于模仿学习方法，碰撞率降低5倍。

Conclusion: ReconDreamer-RL有效缩小sim2real差距，提升自动驾驶训练效果。

Abstract: Reinforcement learning for training end-to-end autonomous driving models in
closed-loop simulations is gaining growing attention. However, most simulation
environments differ significantly from real-world conditions, creating a
substantial simulation-to-reality (sim2real) gap. To bridge this gap, some
approaches utilize scene reconstruction techniques to create photorealistic
environments as a simulator. While this improves realistic sensor simulation,
these methods are inherently constrained by the distribution of the training
data, making it difficult to render high-quality sensor data for novel
trajectories or corner case scenarios. Therefore, we propose ReconDreamer-RL, a
framework designed to integrate video diffusion priors into scene
reconstruction to aid reinforcement learning, thereby enhancing end-to-end
autonomous driving training. Specifically, in ReconDreamer-RL, we introduce
ReconSimulator, which combines the video diffusion prior for appearance
modeling and incorporates a kinematic model for physical modeling, thereby
reconstructing driving scenarios from real-world data. This narrows the
sim2real gap for closed-loop evaluation and reinforcement learning. To cover
more corner-case scenarios, we introduce the Dynamic Adversary Agent (DAA),
which adjusts the trajectories of surrounding vehicles relative to the ego
vehicle, autonomously generating corner-case traffic scenarios (e.g., cut-in).
Finally, the Cousin Trajectory Generator (CTG) is proposed to address the issue
of training data distribution, which is often biased toward simple
straight-line movements. Experiments show that ReconDreamer-RL improves
end-to-end autonomous driving training, outperforming imitation learning
methods with a 5x reduction in the Collision Ratio.

</details>


### [206] [CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data](https://arxiv.org/abs/2508.08173)
*Chongke Bi,Xin Gao,Jiangkang Deng,Guan*

Main category: cs.CV

TL;DR: CD-TVD结合对比学习和改进的扩散模型，通过少量高分辨率数据实现3D超分辨率，减少对大规模数据集的依赖。


<details>
  <summary>Details</summary>
Motivation: 大规模科学模拟生成高分辨率时变数据成本高，现有超分辨率方法依赖大量高分辨率训练数据，适用性受限。

Method: 结合对比学习和改进的扩散模型，预训练阶段学习降级模式，训练阶段仅需一个高分辨率时间步进行微调。

Result: 在流体和大气模拟数据集上验证，CD-TVD实现了准确且资源高效的3D超分辨率。

Conclusion: CD-TVD显著提升了大规模科学模拟的数据增强能力，代码已开源。

Abstract: Large-scale scientific simulations require significant resources to generate
high-resolution time-varying data (TVD). While super-resolution is an efficient
post-processing strategy to reduce costs, existing methods rely on a large
amount of HR training data, limiting their applicability to diverse simulation
scenarios. To address this constraint, we proposed CD-TVD, a novel framework
that combines contrastive learning and an improved diffusion-based
super-resolution model to achieve accurate 3D super-resolution from limited
time-step high-resolution data. During pre-training on historical simulation
data, the contrastive encoder and diffusion superresolution modules learn
degradation patterns and detailed features of high-resolution and
low-resolution samples. In the training phase, the improved diffusion model
with a local attention mechanism is fine-tuned using only one newly generated
high-resolution timestep, leveraging the degradation knowledge learned by the
encoder. This design minimizes the reliance on large-scale high-resolution
datasets while maintaining the capability to recover fine-grained details.
Experimental results on fluid and atmospheric simulation datasets confirm that
CD-TVD delivers accurate and resource-efficient 3D super-resolution, marking a
significant advancement in data augmentation for large-scale scientific
simulations. The code is available at
https://github.com/Xin-Gao-private/CD-TVD.

</details>


### [207] [MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision](https://arxiv.org/abs/2508.08177)
*Zhonghao Yan,Muxi Diao,Yuxuan Yang,Jiayuan Xu,Kaizhou Zhang,Ruoyan Jing,Lele Yang,Yanxi Liu,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: 论文提出了一种新的医学视觉-语言任务UMRG，并发布了数据集U-MRG-14K，同时提出了MedReasoner框架，通过强化学习优化推理和分割，取得了先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前医学图像中的区域定位方法依赖显式空间提示，无法处理临床实践中的隐式查询，需要一种结合临床推理和像素级定位的新方法。

Method: 提出了UMRG任务和U-MRG-14K数据集，并设计了MedReasoner框架，通过强化学习优化推理模块，同时使用冻结的分割专家生成掩码。

Result: MedReasoner在U-MRG-14K数据集上达到最先进性能，并能泛化到未见过的临床查询。

Conclusion: 强化学习在可解释的医学定位中具有显著潜力，MedReasoner框架为临床实践提供了更灵活的解决方案。

Abstract: Accurately grounding regions of interest (ROIs) is critical for diagnosis and
treatment planning in medical imaging. While multimodal large language models
(MLLMs) combine visual perception with natural language, current
medical-grounding pipelines still rely on supervised fine-tuning with explicit
spatial hints, making them ill-equipped to handle the implicit queries common
in clinical practice. This work makes three core contributions. We first define
Unified Medical Reasoning Grounding (UMRG), a novel vision-language task that
demands clinical reasoning and pixel-level grounding. Second, we release
U-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside
implicit clinical queries and reasoning traces, spanning 10 modalities, 15
super-categories, and 108 specific categories. Finally, we introduce
MedReasoner, a modular framework that distinctly separates reasoning from
segmentation: an MLLM reasoner is optimized with reinforcement learning, while
a frozen segmentation expert converts spatial prompts into masks, with
alignment achieved through format and accuracy rewards. MedReasoner achieves
state-of-the-art performance on U-MRG-14K and demonstrates strong
generalization to unseen clinical queries, underscoring the significant promise
of reinforcement learning for interpretable medical grounding.

</details>


### [208] [3D Human Mesh Estimation from Single View RGBD](https://arxiv.org/abs/2508.08178)
*Ozhan Suat,Bedirhan Uguz,Batuhan Karagoz,Muhammed Can Keles,Emre Akbas*

Main category: cs.CV

TL;DR: 提出了一种利用RGBD相机数据进行3D人体网格估计的方法M$^3$，通过虚拟投影和掩码自编码器解决数据稀缺问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: RGBD相机虽普及但深度数据利用不足，现有数据集小且多样性有限，需解决数据稀缺问题。

Method: 利用MoCap数据集生成虚拟单视角深度数据，训练掩码自编码器补全部分网格，推理时匹配深度数据生成完整网格。

Result: 在SURREAL、CAPE和BEHAVE数据集上表现优异，PVE分别为16.8 mm、22.0 mm和70.9 mm，优于现有方法。

Conclusion: M$^3$方法有效利用深度数据，显著提升3D人体网格估计精度，代码将开源。

Abstract: Despite significant progress in 3D human mesh estimation from RGB images;
RGBD cameras, offering additional depth data, remain underutilized. In this
paper, we present a method for accurate 3D human mesh estimation from a single
RGBD view, leveraging the affordability and widespread adoption of RGBD cameras
for real-world applications. A fully supervised approach for this problem,
requires a dataset with RGBD image and 3D mesh label pairs. However, collecting
such a dataset is costly and challenging, hence, existing datasets are small,
and limited in pose and shape diversity. To overcome this data scarcity, we
leverage existing Motion Capture (MoCap) datasets. We first obtain complete 3D
meshes from the body models found in MoCap datasets, and create partial,
single-view versions of them by projection to a virtual camera. This simulates
the depth data provided by an RGBD camera from a single viewpoint. Then, we
train a masked autoencoder to complete the partial, single-view mesh. During
inference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',
matches the depth values coming from the sensor to vertices of a template human
mesh, which creates a partial, single-view mesh. We effectively recover parts
of the 3D human body mesh model that are not visible, resulting in a full body
mesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREAL
and CAPE datasets, respectively; outperforming existing methods that use
full-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVE
dataset, outperforming a recently published RGB based method by 18.4 mm,
highlighting the usefulness of depth data. Code will be released.

</details>


### [209] [PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation](https://arxiv.org/abs/2508.08179)
*Sihan Zhao,Zixuan Wang,Tianyu Luan,Jia Jia,Wentao Zhu,Jiebo Luo,Junsong Yuan,Nan Xi*

Main category: cs.CV

TL;DR: 论文提出了一种物理标注方法和PP-Motion指标，用于评估人体运动的物理和感知保真度，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在评估运动保真度时存在人类感知与物理可行性之间的差距，且主观标签限制了数据驱动指标的开发。

Method: 通过计算运动与物理定律对齐所需的最小修改，生成细粒度物理标注，并设计PP-Motion指标，结合皮尔逊相关损失和人类感知损失。

Result: PP-Motion在物理对齐和人类感知保真度上均优于先前工作。

Conclusion: PP-Motion为运动保真度评估提供了更客观、全面的解决方案。

Abstract: Human motion generation has found widespread applications in AR/VR, film,
sports, and medical rehabilitation, offering a cost-effective alternative to
traditional motion capture systems. However, evaluating the fidelity of such
generated motions is a crucial, multifaceted task. Although previous approaches
have attempted at motion fidelity evaluation using human perception or physical
constraints, there remains an inherent gap between human-perceived fidelity and
physical feasibility. Moreover, the subjective and coarse binary labeling of
human perception further undermines the development of a robust data-driven
metric. We address these issues by introducing a physical labeling method. This
method evaluates motion fidelity by calculating the minimum modifications
needed for a motion to align with physical laws. With this approach, we are
able to produce fine-grained, continuous physical alignment annotations that
serve as objective ground truth. With these annotations, we propose PP-Motion,
a novel data-driven metric to evaluate both physical and perceptual fidelity of
human motion. To effectively capture underlying physical priors, we employ
Pearson's correlation loss for the training of our metric. Additionally, by
incorporating a human-based perceptual fidelity loss, our metric can capture
fidelity that simultaneously considers both human perception and physical
alignment. Experimental results demonstrate that our metric, PP-Motion, not
only aligns with physical laws but also aligns better with human perception of
motion fidelity than previous work.

</details>


### [210] [THAT: Token-wise High-frequency Augmentation Transformer for Hyperspectral Pansharpening](https://arxiv.org/abs/2508.08183)
*Hongkun Jin,Hongcheng Jiang,Zejun Zhang,Yuan Zhang,Jia Fu,Tingfeng Li,Kai Luo*

Main category: cs.CV

TL;DR: THAT框架通过改进高频特征表示和令牌选择，提升了高光谱图像融合的性能。


<details>
  <summary>Details</summary>
Motivation: Transformer在高光谱图像融合中表现受限，主要由于冗余令牌表示和多尺度特征建模不足。

Method: 提出THAT框架，包括PTSA（关键令牌选择性注意力）和MVFN（多级方差感知前馈网络）。

Result: 在标准基准测试中，THAT实现了最先进的性能和更高的重建质量。

Conclusion: THAT有效解决了Transformer在高光谱图像融合中的局限性，提升了高频细节保留和效率。

Abstract: Transformer-based methods have demonstrated strong potential in hyperspectral
pansharpening by modeling long-range dependencies. However, their effectiveness
is often limited by redundant token representations and a lack of multi-scale
feature modeling. Hyperspectral images exhibit intrinsic spectral priors (e.g.,
abundance sparsity) and spatial priors (e.g., non-local similarity), which are
critical for accurate reconstruction. From a spectral-spatial perspective,
Vision Transformers (ViTs) face two major limitations: they struggle to
preserve high-frequency components--such as material edges and texture
transitions--and suffer from attention dispersion across redundant tokens.
These issues stem from the global self-attention mechanism, which tends to
dilute high-frequency signals and overlook localized details. To address these
challenges, we propose the Token-wise High-frequency Augmentation Transformer
(THAT), a novel framework designed to enhance hyperspectral pansharpening
through improved high-frequency feature representation and token selection.
Specifically, THAT introduces: (1) Pivotal Token Selective Attention (PTSA) to
prioritize informative tokens and suppress redundancy; (2) a Multi-level
Variance-aware Feed-forward Network (MVFN) to enhance high-frequency detail
learning. Experiments on standard benchmarks show that THAT achieves
state-of-the-art performance with improved reconstruction quality and
efficiency. The source code is available at https://github.com/kailuo93/THAT.

</details>


### [211] [KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning](https://arxiv.org/abs/2508.08186)
*Md Meftahul Ferdaus,Mahdi Abdelguerfi,Elias Ioup,Steven Sloan,Kendall N. Niles,Ken Pathak*

Main category: cs.CV

TL;DR: KARMA是一种高效的语义分割框架，通过一维函数组合建模缺陷，参数少且适合实时检测。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习方法参数过多、不适合实时检测的问题。

Method: 使用TiKAN模块、优化的特征金字塔结构和静态-动态原型机制。

Result: 在基准测试中表现优异，参数减少97%，推理速度快。

Conclusion: KARMA为实时基础设施检测提供高效解决方案。

Abstract: Semantic segmentation of structural defects in civil infrastructure remains
challenging due to variable defect appearances, harsh imaging conditions, and
significant class imbalance. Current deep learning methods, despite their
effectiveness, typically require millions of parameters, rendering them
impractical for real-time inspection systems. We introduce KARMA
(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient
semantic segmentation framework that models complex defect patterns through
compositions of one-dimensional functions rather than conventional
convolutions. KARMA features three technical innovations: (1) a
parameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging
low-rank factorization for KAN-based feature transformation; (2) an optimized
feature pyramid structure with separable convolutions for multi-scale defect
analysis; and (3) a static-dynamic prototype mechanism that enhances feature
representation for imbalanced classes. Extensive experiments on benchmark
infrastructure inspection datasets demonstrate that KARMA achieves competitive
or superior mean IoU performance compared to state-of-the-art approaches, while
using significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).
Operating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for
real-time deployment, enabling practical automated infrastructure inspection
systems without compromising accuracy. The source code can be accessed at the
following URL: https://github.com/faeyelab/karma.

</details>


### [212] [Reinforcement Learning in Vision: A Survey](https://arxiv.org/abs/2508.08189)
*Weijia Wu,Chen Gao,Joya Chen,Kevin Qinghong Lin,Qingwei Meng,Yiming Zhang,Yuke Qiu,Hong Zhou,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 该论文综述了视觉强化学习（RL）的最新进展，包括问题形式化、策略优化演变、四大主题支柱（多模态大语言模型、视觉生成、统一模型框架、视觉-语言-动作模型）以及评估协议和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 为研究者和从业者提供视觉RL领域的全面概述，并指出未来研究方向。

Method: 通过分析200多篇代表性工作，将其分为四大主题支柱，并探讨算法设计、奖励工程和基准进展。

Result: 总结了视觉RL的关键趋势（如课程驱动训练、偏好对齐扩散）和评估协议（如集合级保真度、样本级偏好）。

Conclusion: 视觉RL领域发展迅速，但仍面临样本效率、泛化性和安全部署等挑战，未来研究需重点关注这些方向。

Abstract: Recent advances at the intersection of reinforcement learning (RL) and visual
intelligence have enabled agents that not only perceive complex visual scenes
but also reason, generate, and act within them. This survey offers a critical
and up-to-date synthesis of the field. We first formalize visual RL problems
and trace the evolution of policy-optimization strategies from RLHF to
verifiable reward paradigms, and from Proximal Policy Optimization to Group
Relative Policy Optimization. We then organize more than 200 representative
works into four thematic pillars: multi-modal large language models, visual
generation, unified model frameworks, and vision-language-action models. For
each pillar we examine algorithmic design, reward engineering, benchmark
progress, and we distill trends such as curriculum-driven training,
preference-aligned diffusion, and unified reward modeling. Finally, we review
evaluation protocols spanning set-level fidelity, sample-level preference, and
state-level stability, and we identify open challenges that include sample
efficiency, generalization, and safe deployment. Our goal is to provide
researchers and practitioners with a coherent map of the rapidly expanding
landscape of visual RL and to highlight promising directions for future
inquiry. Resources are available at:
https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.

</details>


### [213] [Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model](https://arxiv.org/abs/2508.08199)
*Peiqi He,Zhenhao Zhang,Yixiang Zhang,Xiongjun Zhao,Shaoliang Peng*

Main category: cs.CV

TL;DR: Spatial-ORMLLM是一种基于RGB模态的大型视觉语言模型，用于手术室3D空间推理，无需额外传感器或专家标注。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多模态数据且忽略3D能力，而手术室缺乏多传感器数据，2D数据无法捕捉复杂场景细节。

Method: 提出Spatial-ORMLLM，通过空间增强特征融合块将2D输入与3D知识结合，端到端框架实现3D推理。

Result: 在多个临床数据集上表现最优，泛化能力强。

Conclusion: Spatial-ORMLLM为手术室3D空间推理提供高效解决方案。

Abstract: Precise spatial modeling in the operating room (OR) is foundational to many
clinical tasks, supporting intraoperative awareness, hazard avoidance, and
surgical decision-making. While existing approaches leverage large-scale
multimodal datasets for latent-space alignment to implicitly learn spatial
relationships, they overlook the 3D capabilities of MLLMs. However, this
approach raises two issues: (1) Operating rooms typically lack multiple video
and audio sensors, making multimodal 3D data difficult to obtain; (2) Training
solely on readily available 2D data fails to capture fine-grained details in
complex scenes. To address this gap, we introduce Spatial-ORMLLM, the first
large vision-language model for 3D spatial reasoning in operating rooms using
only RGB modality to infer volumetric and semantic cues, enabling downstream
medical tasks with detailed and holistic spatial context. Spatial-ORMLLM
incorporates a Spatial-Enhanced Feature Fusion Block, which integrates 2D
modality inputs with rich 3D spatial knowledge extracted by the estimation
algorithm and then feeds the combined features into the visual tower. By
employing a unified end-to-end MLLM framework, it combines powerful spatial
features with textual features to deliver robust 3D scene reasoning without any
additional expert annotations or sensor inputs. Experiments on multiple
benchmark clinical datasets demonstrate that Spatial-ORMLLM achieves
state-of-the-art performance and generalizes robustly to previously unseen
surgical scenarios and downstream tasks.

</details>


### [214] [SAGOnline: Segment Any Gaussians Online](https://arxiv.org/abs/2508.08219)
*Wentao Sun,Quanyun Wu,Hanqing Xu,Kyle Gao,Zhengsen Xu,Yiping Chen,Dedong Zhang,Lingfei Ma,John S. Zelek,Jonathan Li*

Main category: cs.CV

TL;DR: SAGOnline是一个轻量级、零样本的实时3D分割框架，通过解耦策略和GPU加速算法，解决了3D高斯场景中的高效分割和多目标跟踪问题。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯场景分割方法存在计算成本高、空间推理能力有限以及无法同时跟踪多目标的问题。

Method: 结合视频基础模型（如SAM2）进行视图一致的2D掩码传播，并使用GPU加速的3D掩码生成和高斯级实例标注算法。

Result: 在NVOS和Spin-NeRF基准测试中表现优异（92.7%和95.2% mIoU），推理速度提升15-1500倍（27毫秒/帧）。

Conclusion: SAGOnline实现了实时渲染和3D场景理解，为AR/VR和机器人应用提供了实用解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit
3D scene representation, yet achieving efficient and consistent 3D segmentation
remains challenging. Current methods suffer from prohibitive computational
costs, limited 3D spatial reasoning, and an inability to track multiple objects
simultaneously. We present Segment Any Gaussians Online (SAGOnline), a
lightweight and zero-shot framework for real-time 3D segmentation in Gaussian
scenes that addresses these limitations through two key innovations: (1) a
decoupled strategy that integrates video foundation models (e.g., SAM2) for
view-consistent 2D mask propagation across synthesized views; and (2) a
GPU-accelerated 3D mask generation and Gaussian-level instance labeling
algorithm that assigns unique identifiers to 3D primitives, enabling lossless
multi-object tracking and segmentation across views. SAGOnline achieves
state-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU)
benchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times
in inference speed (27 ms/frame). Qualitative results demonstrate robust
multi-object segmentation and tracking in complex scenes. Our contributions
include: (i) a lightweight and zero-shot framework for 3D segmentation in
Gaussian scenes, (ii) explicit labeling of Gaussian primitives enabling
simultaneous segmentation and tracking, and (iii) the effective adaptation of
2D video foundation models to the 3D domain. This work allows real-time
rendering and 3D scene understanding, paving the way for practical AR/VR and
robotic applications.

</details>


### [215] [Learning User Preferences for Image Generation Model](https://arxiv.org/abs/2508.08220)
*Wenyi Mo,Ying Ba,Tianyu Zhang,Yalong Bai,Biye Li*

Main category: cs.CV

TL;DR: 提出了一种基于多模态大语言模型的方法，通过对比偏好损失和偏好标记学习个性化用户偏好，显著提升了偏好预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖通用偏好或静态用户画像，忽略了个人差异和动态多面的品味特征。

Method: 引入对比偏好损失和偏好标记，从历史交互中学习个性化偏好，并激活群体共享兴趣表示。

Result: 实验表明，该方法在偏好预测准确性上优于其他方法，能有效识别相似审美倾向的用户。

Conclusion: 该方法能更精准地生成符合个人品味的图像，为个性化推荐提供了有效工具。

Abstract: User preference prediction requires a comprehensive and accurate
understanding of individual tastes. This includes both surface-level
attributes, such as color and style, and deeper content-related aspects, such
as themes and composition. However, existing methods typically rely on general
human preferences or assume static user profiles, often neglecting individual
variability and the dynamic, multifaceted nature of personal taste. To address
these limitations, we propose an approach built upon Multimodal Large Language
Models, introducing contrastive preference loss and preference tokens to learn
personalized user preferences from historical interactions. The contrastive
preference loss is designed to effectively distinguish between user ''likes''
and ''dislikes'', while the learnable preference tokens capture shared interest
representations among existing users, enabling the model to activate
group-specific preferences and enhance consistency across similar users.
Extensive experiments demonstrate our model outperforms other methods in
preference prediction accuracy, effectively identifying users with similar
aesthetic inclinations and providing more precise guidance for generating
images that align with individual tastes. The project page is
\texttt{https://learn-user-pref.github.io/}.

</details>


### [216] [OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution](https://arxiv.org/abs/2508.08227)
*Zhiqiang Wu,Zhaomang Sun,Tong Zhou,Bingtao Fu,Ji Cong,Yitong Dong,Huaqi Zhang,Xuan Tang,Mingsong Chen,Xian Wei*

Main category: cs.CV

TL;DR: OMGSR是一种基于DDPM/FM的一步式Real-ISR框架，通过在中时间步注入低质量图像潜在分布，减少分布差距，并优化生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有一步式Real-ISR模型在初始时间步注入低质量图像潜在分布，与高斯噪声潜在分布存在差距，限制了生成先验的有效利用。

Method: 提出OMGSR框架，在中时间步注入低质量图像潜在分布，引入潜在分布细化损失和重叠分块LPIPS/GAN损失。

Result: OMGSR-S和OMGSR-F在512分辨率下表现优异，OMGSR-F在所有参考指标中占优，1k分辨率下细节生成效果出色。

Conclusion: OMGSR框架有效解决了潜在分布差距问题，显著提升了一步式Real-ISR的性能和生成质量。

Abstract: Denoising Diffusion Probabilistic Models (DDPM) and Flow Matching (FM)
generative models show promising potential for one-step Real-World Image
Super-Resolution (Real-ISR). Recent one-step Real-ISR models typically inject a
Low-Quality (LQ) image latent distribution at the initial timestep. However, a
fundamental gap exists between the LQ image latent distribution and the
Gaussian noisy latent distribution, limiting the effective utilization of
generative priors. We observe that the noisy latent distribution at DDPM/FM
mid-timesteps aligns more closely with the LQ image latent distribution. Based
on this insight, we present One Mid-timestep Guidance Real-ISR (OMGSR), a
universal framework applicable to DDPM/FM-based generative models. OMGSR
injects the LQ image latent distribution at a pre-computed mid-timestep,
incorporating the proposed Latent Distribution Refinement loss to alleviate the
latent distribution gap. We also design the Overlap-Chunked LPIPS/GAN loss to
eliminate checkerboard artifacts in image generation. Within this framework, we
instantiate OMGSR for DDPM/FM-based generative models with two variants:
OMGSR-S (SD-Turbo) and OMGSR-F (FLUX.1-dev). Experimental results demonstrate
that OMGSR-S/F achieves balanced/excellent performance across quantitative and
qualitative metrics at 512-resolution. Notably, OMGSR-F establishes
overwhelming dominance in all reference metrics. We further train a
1k-resolution OMGSR-F to match the default resolution of FLUX.1-dev, which
yields excellent results, especially in the details of the image generation. We
also generate 2k-resolution images by the 1k-resolution OMGSR-F using our
two-stage Tiled VAE & Diffusion.

</details>


### [217] [Cut2Next: Generating Next Shot via In-Context Tuning](https://arxiv.org/abs/2508.08244)
*Jingwen He,Hongbo Liu,Jiajun Li,Ziqi Huang,Yu Qiao,Wanli Ouyang,Ziwei Liu*

Main category: cs.CV

TL;DR: 论文提出Next Shot Generation (NSG)框架Cut2Next，通过Diffusion Transformer和分层多提示策略生成符合专业剪辑模式的高质量镜头。


<details>
  <summary>Details</summary>
Motivation: 当前多镜头生成方法忽视叙事编辑模式，导致输出缺乏叙事复杂性和电影完整性。

Method: 采用Diffusion Transformer (DiT)和分层多提示策略（关系提示和个体提示），结合Context-Aware Condition Injection (CACI)和Hierarchical Attention Mask (HAM)。

Result: Cut2Next在视觉一致性和文本保真度上表现优异，用户研究显示其更符合编辑模式和电影连续性。

Conclusion: Cut2Next能生成高质量、叙事表达丰富且电影连贯的后续镜头。

Abstract: Effective multi-shot generation demands purposeful, film-like transitions and
strict cinematic continuity. Current methods, however, often prioritize basic
visual consistency, neglecting crucial editing patterns (e.g., shot/reverse
shot, cutaways) that drive narrative flow for compelling storytelling. This
yields outputs that may be visually coherent but lack narrative sophistication
and true cinematic integrity. To bridge this, we introduce Next Shot Generation
(NSG): synthesizing a subsequent, high-quality shot that critically conforms to
professional editing patterns while upholding rigorous cinematic continuity.
Our framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs
in-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This
strategy uses Relational Prompts to define overall context and inter-shot
editing styles. Individual Prompts then specify per-shot content and
cinematographic attributes. Together, these guide Cut2Next to generate
cinematically appropriate next shots. Architectural innovations, Context-Aware
Condition Injection (CACI) and Hierarchical Attention Mask (HAM), further
integrate these diverse signals without introducing new parameters. We
construct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with
hierarchical prompts, and introduce CutBench for evaluation. Experiments show
Cut2Next excels in visual consistency and text fidelity. Crucially, user
studies reveal a strong preference for Cut2Next, particularly for its adherence
to intended editing patterns and overall cinematic continuity, validating its
ability to generate high-quality, narratively expressive, and cinematically
coherent subsequent shots.

</details>


### [218] [StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation](https://arxiv.org/abs/2508.08248)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: StableAvatar提出了一种端到端的视频扩散变换器，解决了现有音频驱动虚拟视频生成模型在长视频合成中音频同步和身份一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型依赖第三方音频提取器，导致潜在分布误差累积，影响长视频生成质量。

Method: 引入时间步感知音频适配器和音频原生引导机制，结合动态加权滑动窗口策略。

Result: 实验证明StableAvatar在质量和数量上均优于现有方法。

Conclusion: StableAvatar通过创新模块实现了高质量、无限长度的视频生成。

Abstract: Current diffusion models for audio-driven avatar video generation struggle to
synthesize long videos with natural audio synchronization and identity
consistency. This paper presents StableAvatar, the first end-to-end video
diffusion transformer that synthesizes infinite-length high-quality videos
without post-processing. Conditioned on a reference image and audio,
StableAvatar integrates tailored training and inference modules to enable
infinite-length video generation. We observe that the main reason preventing
existing models from generating long videos lies in their audio modeling. They
typically rely on third-party off-the-shelf extractors to obtain audio
embeddings, which are then directly injected into the diffusion model via
cross-attention. Since current diffusion backbones lack any audio-related
priors, this approach causes severe latent distribution error accumulation
across video clips, leading the latent distribution of subsequent segments to
drift away from the optimal distribution gradually. To address this,
StableAvatar introduces a novel Time-step-aware Audio Adapter that prevents
error accumulation via time-step-aware modulation. During inference, we propose
a novel Audio Native Guidance Mechanism to further enhance the audio
synchronization by leveraging the diffusion's own evolving joint audio-latent
prediction as a dynamic guidance signal. To enhance the smoothness of the
infinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy
that fuses latent over time. Experiments on benchmarks show the effectiveness
of StableAvatar both qualitatively and quantitatively.

</details>


### [219] [ReferSplat: Referring Segmentation in 3D Gaussian Splatting](https://arxiv.org/abs/2508.08252)
*Shuting He,Guangquan Jie,Changshuo Wang,Yun Zhou,Shuming Hu,Guanbin Li,Henghui Ding*

Main category: cs.CV

TL;DR: R3DGS任务旨在通过自然语言描述在3D高斯场景中分割目标物体，提出新数据集Ref-LERF和框架ReferSplat，解决3D多模态理解和空间关系建模的挑战。


<details>
  <summary>Details</summary>
Motivation: 推动具身AI发展，解决3D场景中基于语言描述的物体分割问题，尤其是遮挡或不可见物体的识别。

Method: 提出ReferSplat框架，显式建模3D高斯点与自然语言表达的空间关系。

Result: ReferSplat在R3DGS任务和3D开放词汇分割基准上达到最先进性能。

Conclusion: R3DGS任务和ReferSplat框架为3D多模态理解提供了新方向，数据集和代码已开源。

Abstract: We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task
that aims to segment target objects in a 3D Gaussian scene based on natural
language descriptions, which often contain spatial relationships or object
attributes. This task requires the model to identify newly described objects
that may be occluded or not directly visible in a novel view, posing a
significant challenge for 3D multi-modal understanding. Developing this
capability is crucial for advancing embodied AI. To support research in this
area, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that
3D multi-modal understanding and spatial relationship modeling are key
challenges for R3DGS. To address these challenges, we propose ReferSplat, a
framework that explicitly models 3D Gaussian points with natural language
expressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art
performance on both the newly proposed R3DGS task and 3D open-vocabulary
segmentation benchmarks. Dataset and code are available at
https://github.com/heshuting555/ReferSplat.

</details>


### [220] [Learning an Implicit Physics Model for Image-based Fluid Simulation](https://arxiv.org/abs/2508.08254)
*Emily Yue-Ting Jia,Jiageng Mao,Zhiyuan Gao,Yajie Zhao,Yue Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于物理约束的神经网络方法，从单张图像生成4D场景（包含3D几何和运动），显著提升了动画的物理合理性。


<details>
  <summary>Details</summary>
Motivation: 人类能从单张图像想象4D场景（包含运动和3D几何），但现有方法生成的动画常违背物理规律。本文旨在通过神经网络模拟这一能力，尤其针对自然流体图像。

Method: 提出了一种物理约束的神经网络，预测每个表面点的运动，并通过纳维-斯托克斯方程等物理原理指导损失函数。同时，从输入图像和深度估计预测3D高斯特征，并基于预测运动渲染动画。

Result: 实验表明，该方法能生成物理合理的动画，性能显著优于现有方法。

Conclusion: 该方法成功实现了从单张图像生成物理一致的4D场景，为相关领域提供了新思路。

Abstract: Humans possess an exceptional ability to imagine 4D scenes, encompassing both
motion and 3D geometry, from a single still image. This ability is rooted in
our accumulated observations of similar scenes and an intuitive understanding
of physics. In this paper, we aim to replicate this capacity in neural
networks, specifically focusing on natural fluid imagery. Existing methods for
this task typically employ simplistic 2D motion estimators to animate the
image, leading to motion predictions that often defy physical principles,
resulting in unrealistic animations. Our approach introduces a novel method for
generating 4D scenes with physics-consistent animation from a single image. We
propose the use of a physics-informed neural network that predicts motion for
each surface point, guided by a loss term derived from fundamental physical
principles, including the Navier-Stokes equations. To capture appearance, we
predict feature-based 3D Gaussians from the input image and its estimated
depth, which are then animated using the predicted motions and rendered from
any desired camera perspective. Experimental results highlight the
effectiveness of our method in producing physically plausible animations,
showcasing significant performance improvements over existing methods. Our
project page is https://physfluid.github.io/ .

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [221] [Sea-Undistort: A Dataset for Through-Water Image Restoration in High Resolution Airborne Bathymetric Mapping](https://arxiv.org/abs/2508.07760)
*Maximilian Kromer,Panagiotis Agrafiotis,Begüm Demir*

Main category: eess.IV

TL;DR: 论文提出Sea-Undistort数据集，用于解决浅水区图像测深中的光学失真问题，并通过增强的扩散模型提升测深精度。


<details>
  <summary>Details</summary>
Motivation: 浅水区图像测深因水面动态、水体特性和太阳光照导致的光学失真（如波浪、散射和太阳耀斑）而具有挑战性。

Method: 使用Blender渲染1200对512x512图像（含失真和无失真视图），并利用这些数据训练和评估图像恢复方法，包括一种增强的轻量级扩散模型。

Result: 增强的扩散模型在真实航空数据中表现更优，能生成更完整的海底数字表面模型（DSM），减少测深误差，抑制耀斑和散射，并恢复精细海底细节。

Conclusion: Sea-Undistort数据集和增强的扩散模型为浅水区图像测深提供了有效的解决方案，相关资源已公开。

Abstract: Accurate image-based bathymetric mapping in shallow waters remains
challenging due to the complex optical distortions such as wave induced
patterns, scattering and sunglint, introduced by the dynamic water surface, the
water column properties, and solar illumination. In this work, we introduce
Sea-Undistort, a comprehensive synthetic dataset of 1200 paired 512x512
through-water scenes rendered in Blender. Each pair comprises a distortion-free
and a distorted view, featuring realistic water effects such as sun glint,
waves, and scattering over diverse seabeds. Accompanied by per-image metadata
such as camera parameters, sun position, and average depth, Sea-Undistort
enables supervised training that is otherwise infeasible in real environments.
We use Sea-Undistort to benchmark two state-of-the-art image restoration
methods alongside an enhanced lightweight diffusion-based framework with an
early-fusion sun-glint mask. When applied to real aerial data, the enhanced
diffusion model delivers more complete Digital Surface Models (DSMs) of the
seabed, especially in deeper areas, reduces bathymetric errors, suppresses
glint and scattering, and crisply restores fine seabed details. Dataset,
weights, and code are publicly available at
https://www.magicbathy.eu/Sea-Undistort.html.

</details>


### [222] [PCA-Guided Autoencoding for Structured Dimensionality Reduction in Active Infrared Thermography](https://arxiv.org/abs/2508.07773)
*Mohammed Salah,Numan Saeed,Davor Svetinovic,Stefano Sfarra,Mohammed Omar,Yusra Abdulrahman*

Main category: eess.IV

TL;DR: 本文提出了一种基于主成分分析（PCA）引导的自编码框架，用于结构化降维，以改进红外热成像数据的缺陷表征效果。


<details>
  <summary>Details</summary>
Motivation: 当前红外热成像数据的非线性自编码器降维方法缺乏结构化的潜在空间，限制了缺陷表征的效果。

Method: 提出PCA引导的自编码框架，引入PCA蒸馏损失函数，强制潜在空间与结构化PCA组件对齐。

Result: 实验结果表明，该方法在PVC、CFRP和PLA样本上，在对比度、信噪比和神经网络指标上优于现有方法。

Conclusion: PCA引导的自编码框架有效提升了红外热成像数据的结构化降维和缺陷表征能力。

Abstract: Active Infrared thermography (AIRT) is a widely adopted non-destructive
testing (NDT) technique for detecting subsurface anomalies in industrial
components. Due to the high dimensionality of AIRT data, current approaches
employ non-linear autoencoders (AEs) for dimensionality reduction. However, the
latent space learned by AIRT AEs lacks structure, limiting their effectiveness
in downstream defect characterization tasks. To address this limitation, this
paper proposes a principal component analysis guided (PCA-guided) autoencoding
framework for structured dimensionality reduction to capture intricate,
non-linear features in thermographic signals while enforcing a structured
latent space. A novel loss function, PCA distillation loss, is introduced to
guide AIRT AEs to align the latent representation with structured PCA
components while capturing the intricate, non-linear patterns in thermographic
signals. To evaluate the utility of the learned, structured latent space, we
propose a neural network-based evaluation metric that assesses its suitability
for defect characterization. Experimental results show that the proposed
PCA-guided AE outperforms state-of-the-art dimensionality reduction methods on
PVC, CFRP, and PLA samples in terms of contrast, signal-to-noise ratio (SNR),
and neural network-based metrics.

</details>


### [223] [Deep Learning-Based Desikan-Killiany Parcellation of the Brain Using Diffusion MRI](https://arxiv.org/abs/2508.07815)
*Yousef Sadegheih,Dorit Merhof*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的框架，直接在扩散MRI数据上实现脑部分割，无需依赖解剖MRI，提高了准确性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖解剖MRI进行分割和跨模态配准，可能引入误差并限制技术通用性。

Method: 采用分层两阶段分割网络：第一阶段粗分割为大脑区域，第二阶段细化分割为子区域。通过消融研究确定最佳扩散参数组合。

Result: 在多个数据集上表现优于现有方法，Dice相似系数更高，且对不同分辨率和采集协议具有鲁棒性。

Conclusion: 该方法为扩散MRI脑部分割提供了精确、可靠且无需配准的解决方案，适用于研究和临床应用。

Abstract: Accurate brain parcellation in diffusion MRI (dMRI) space is essential for
advanced neuroimaging analyses. However, most existing approaches rely on
anatomical MRI for segmentation and inter-modality registration, a process that
can introduce errors and limit the versatility of the technique. In this study,
we present a novel deep learning-based framework for direct parcellation based
on the Desikan-Killiany (DK) atlas using only diffusion MRI data. Our method
utilizes a hierarchical, two-stage segmentation network: the first stage
performs coarse parcellation into broad brain regions, and the second stage
refines the segmentation to delineate more detailed subregions within each
coarse category. We conduct an extensive ablation study to evaluate various
diffusion-derived parameter maps, identifying an optimal combination of
fractional anisotropy, trace, sphericity, and maximum eigenvalue that enhances
parellation accuracy. When evaluated on the Human Connectome Project and
Consortium for Neuropsychiatric Phenomics datasets, our approach achieves
superior Dice Similarity Coefficients compared to existing state-of-the-art
models. Additionally, our method demonstrates robust generalization across
different image resolutions and acquisition protocols, producing more
homogeneous parcellations as measured by the relative standard deviation within
regions. This work represents a significant advancement in dMRI-based brain
segmentation, providing a precise, reliable, and registration-free solution
that is critical for improved structural connectivity and microstructural
analyses in both research and clinical applications. The implementation of our
method is publicly available on github.com/xmindflow/DKParcellationdMRI.

</details>


### [224] [MIND: A Noise-Adaptive Denoising Framework for Medical Images Integrating Multi-Scale Transformer](https://arxiv.org/abs/2508.07817)
*Tao Tang,Chengxu Yang*

Main category: eess.IV

TL;DR: 提出了一种结合多尺度卷积和Transformer架构的医学图像自适应去噪模型（MI-ND），通过噪声感知驱动特征融合，显著提升了图像质量和下游诊断任务性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像质量直接影响临床判断准确性，但常受非均匀噪声干扰，影响结构识别和病变检测。

Method: 集成多尺度卷积和Transformer架构，引入噪声水平估计器（NLE）和噪声自适应注意力模块（NAAB），实现噪声感知驱动的特征融合。

Result: 在PSNR、SSIM等指标上显著优于对比方法，提升了下游诊断任务的F1分数和ROC-AUC。

Conclusion: 模型在结构恢复、诊断敏感性和跨模态鲁棒性方面表现突出，为医学图像增强和AI辅助诊疗提供了有效解决方案。

Abstract: The core role of medical images in disease diagnosis makes their quality
directly affect the accuracy of clinical judgment. However, due to factors such
as low-dose scanning, equipment limitations and imaging artifacts, medical
images are often accompanied by non-uniform noise interference, which seriously
affects structure recognition and lesion detection. This paper proposes a
medical image adaptive denoising model (MI-ND) that integrates multi-scale
convolutional and Transformer architecture, introduces a noise level estimator
(NLE) and a noise adaptive attention module (NAAB), and realizes
channel-spatial attention regulation and cross-modal feature fusion driven by
noise perception. Systematic testing is carried out on multimodal public
datasets. Experiments show that this method significantly outperforms the
comparative methods in image quality indicators such as PSNR, SSIM, and LPIPS,
and improves the F1 score and ROC-AUC in downstream diagnostic tasks, showing
strong prac-tical value and promotional potential. The model has outstanding
benefits in structural recovery, diagnostic sensitivity, and cross-modal
robustness, and provides an effective solution for medical image enhancement
and AI-assisted diagnosis and treatment.

</details>


### [225] [Learned Regularization for Microwave Tomography](https://arxiv.org/abs/2508.08114)
*Bowen Tong,Hao Chen,Shaorui Guo,Dong Liu*

Main category: eess.IV

TL;DR: 提出了一种基于扩散模型的物理信息混合框架（SSD-Reg），用于微波断层扫描中的逆问题求解，无需配对数据即可恢复复杂结构。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法难以恢复精细结构，深度学习需要大量配对数据且泛化能力有限。

Method: 结合扩散模型作为学习正则化，嵌入变分框架中，提出单步扩散正则化（SSD-Reg）。

Result: SSD-Reg在保持物理一致性和结构分布的同时，提高了重建的准确性、稳定性和鲁棒性。

Conclusion: SSD-Reg为功能图像重建中的不适定问题提供了灵活有效的解决方案。

Abstract: Microwave Tomography (MWT) aims to reconstruct the dielectric properties of
tissues from measured scattered electromagnetic fields. This inverse problem is
highly nonlinear and ill-posed, posing significant challenges for conventional
optimization-based methods, which, despite being grounded in physical models,
often fail to recover fine structural details. Recent deep learning strategies,
including end-to-end and post-processing networks, have improved reconstruction
quality but typically require large paired training datasets and may struggle
to generalize. To overcome these limitations, we propose a physics-informed
hybrid framework that integrates diffusion models as learned regularization
within a data-consistency-driven variational scheme. Specifically, we introduce
Single-Step Diffusion Regularization (SSD-Reg), a novel approach that embeds
diffusion priors into the iterative reconstruction process, enabling the
recovery of complex anatomical structures without the need for paired data.
SSD-Reg maintains fidelity to both the governing physics and learned structural
distributions, improving accuracy, stability, and robustness. Extensive
experiments demonstrate that SSD-Reg, implemented as a Plug-and-Play (PnP)
module, provides a flexible and effective solution for tackling the
ill-posedness inherent in functional image reconstruction.

</details>


### [226] [Mamba-FCS: Joint Spatio- Frequency Feature Fusion, Change-Guided Attention, and SeK Loss for Enhanced Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2508.08232)
*Buddhi Wijenayake,Athulya Ratnayake,Praveen Sumanasekara,Roshan Godaliyadda,Parakrama Ekanayake,Vijitha Herath,Nichula Wasalathilaka*

Main category: eess.IV

TL;DR: Mamba-FCS框架结合视觉状态空间模型，通过联合时空频率融合块和变化引导注意力模块，优化了语义变化检测任务，在多个数据集上达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决语义变化检测中全局上下文建模与计算效率的平衡问题，同时应对类别不平衡的挑战。

Method: 提出Mamba-FCS框架，包含视觉状态空间模型、联合时空频率融合块、变化引导注意力模块和分离Kappa损失。

Result: 在SECOND和Landsat-SCD数据集上分别达到88.62%和96.25%的总体准确率，F_scd和SeK指标也显著提升。

Conclusion: Mamba架构结合新方法在语义变化检测中表现出色，为遥感应用设立了新基准。

Abstract: Semantic Change Detection (SCD) from remote sensing imagery requires models
balancing extensive spatial context, computational efficiency, and sensitivity
to class-imbalanced land-cover transitions. While Convolutional Neural Networks
excel at local feature extraction but lack global context, Transformers provide
global modeling at high computational costs. Recent Mamba architectures based
on state-space models offer compelling solutions through linear complexity and
efficient long-range modeling. In this study, we introduce Mamba-FCS, a SCD
framework built upon Visual State Space Model backbone incorporating, a Joint
Spatio-Frequency Fusion block incorporating log-amplitude frequency domain
features to enhance edge clarity and suppress illumination artifacts, a
Change-Guided Attention (CGA) module that explicitly links the naturally
intertwined BCD and SCD tasks, and a Separated Kappa (SeK) loss tailored for
class-imbalanced performance optimization. Extensive evaluation on SECOND and
Landsat-SCD datasets shows that Mamba-FCS achieves state-of-the-art metrics,
88.62% Overall Accuracy, 65.78% F_scd, and 25.50% SeK on SECOND, 96.25% Overall
Accuracy, 89.27% F_scd, and 60.26% SeK on Landsat-SCD. Ablation analyses
confirm distinct contributions of each novel component, with qualitative
assessments highlighting significant improvements in SCD. Our results underline
the substantial potential of Mamba architectures, enhanced by proposed
techniques, setting a new benchmark for effective and scalable semantic change
detection in remote sensing applications. The complete source code,
configuration files, and pre-trained models will be publicly available upon
publication.

</details>


### [227] [Transfer Learning with EfficientNet for Accurate Leukemia Cell Classification](https://arxiv.org/abs/2508.06535)
*Faisal Ahmed*

Main category: eess.IV

TL;DR: 该研究通过迁移学习和数据增强技术，使用预训练的卷积神经网络（如EfficientNet-B3）对急性淋巴细胞白血病（ALL）进行分类，取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 提高急性淋巴细胞白血病（ALL）的早期诊断准确性，以优化治疗计划。

Method: 采用迁移学习，使用预训练的CNN模型（如ResNet50、ResNet101和EfficientNet变体），并通过数据增强解决类别不平衡问题。

Result: EfficientNet-B3表现最佳，F1分数为94.30%，准确率为92.02%，AUC为94.79%，优于C-NMC挑战中的其他方法。

Conclusion: 结合数据增强和迁移学习（特别是EfficientNet-B3）可有效开发准确且稳健的血液恶性肿瘤诊断工具。

Abstract: Accurate classification of Acute Lymphoblastic Leukemia (ALL) from peripheral
blood smear images is essential for early diagnosis and effective treatment
planning. This study investigates the use of transfer learning with pretrained
convolutional neural networks (CNNs) to improve diagnostic performance. To
address the class imbalance in the dataset of 3,631 Hematologic and 7,644 ALL
images, we applied extensive data augmentation techniques to create a balanced
training set of 10,000 images per class. We evaluated several models, including
ResNet50, ResNet101, and EfficientNet variants B0, B1, and B3. EfficientNet-B3
achieved the best results, with an F1-score of 94.30%, accuracy of 92.02%,
andAUCof94.79%,outperformingpreviouslyreported methods in the C-NMCChallenge.
Thesefindings demonstrate the effectiveness of combining data augmentation with
advanced transfer learning models, particularly EfficientNet-B3, in developing
accurate and robust diagnostic tools for hematologic malignancy detection.

</details>


### [228] [LWT-ARTERY-LABEL: A Lightweight Framework for Automated Coronary Artery Identification](https://arxiv.org/abs/2508.06874)
*Shisheng Zhang,Ramtin Gharleghi,Sonit Singh,Daniel Moses,Dona Adikari,Arcot Sowmya,Susann Beier*

Main category: eess.IV

TL;DR: 提出了一种结合解剖学知识和规则拓扑约束的轻量级方法，用于冠状动脉自动标记，性能优越。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉疾病是全球主要死因，CTCA是重要诊断工具，但传统方法效率低且资源消耗大。

Method: 整合解剖学知识与规则拓扑约束的轻量级方法。

Result: 在基准数据集上达到最优性能。

Conclusion: 该方法为冠状动脉自动标记提供了高效且资源友好的解决方案。

Abstract: Coronary artery disease (CAD) remains the leading cause of death globally,
with computed tomography coronary angiography (CTCA) serving as a key
diagnostic tool. However, coronary arterial analysis using CTCA, such as
identifying artery-specific features from computational modelling, is
labour-intensive and time-consuming. Automated anatomical labelling of coronary
arteries offers a potential solution, yet the inherent anatomical variability
of coronary trees presents a significant challenge. Traditional knowledge-based
labelling methods fall short in leveraging data-driven insights, while recent
deep-learning approaches often demand substantial computational resources and
overlook critical clinical knowledge. To address these limitations, we propose
a lightweight method that integrates anatomical knowledge with rule-based
topology constraints for effective coronary artery labelling. Our approach
achieves state-of-the-art performance on benchmark datasets, providing a
promising alternative for automated coronary artery labelling.

</details>


### [229] [Fusion-Based Brain Tumor Classification Using Deep Learning and Explainable AI, and Rule-Based Reasoning](https://arxiv.org/abs/2508.06891)
*Melika Filvantorkaman,Mohsen Piri,Maral Filvan Torkaman,Ashkan Zabihi,Hamidreza Moradi*

Main category: eess.IV

TL;DR: 提出了一种基于MobileNetV2和DenseNet121的集成深度学习框架，结合XAI模块和临床决策规则，用于脑肿瘤分类，性能优越且可解释性强。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤的准确分类对诊断和治疗至关重要，但现有方法缺乏透明度和临床信任。

Method: 使用MobileNetV2和DenseNet121的集成框架，结合Grad-CAM++和临床决策规则，进行5折交叉验证。

Result: 集成模型准确率达91.7%，Grad-CAM++可视化与专家标注区域高度一致，临床评估得分高。

Conclusion: 该框架为脑肿瘤分类提供了高效、可解释的解决方案，适合临床应用。

Abstract: Accurate and interpretable classification of brain tumors from magnetic
resonance imaging (MRI) is critical for effective diagnosis and treatment
planning. This study presents an ensemble-based deep learning framework that
combines MobileNetV2 and DenseNet121 convolutional neural networks (CNNs) using
a soft voting strategy to classify three common brain tumor types: glioma,
meningioma, and pituitary adenoma. The models were trained and evaluated on the
Figshare dataset using a stratified 5-fold cross-validation protocol. To
enhance transparency and clinical trust, the framework integrates an
Explainable AI (XAI) module employing Grad-CAM++ for class-specific saliency
visualization, alongside a symbolic Clinical Decision Rule Overlay (CDRO) that
maps predictions to established radiological heuristics. The ensemble
classifier achieved superior performance compared to individual CNNs, with an
accuracy of 91.7%, precision of 91.9%, recall of 91.7%, and F1-score of 91.6%.
Grad-CAM++ visualizations revealed strong spatial alignment between model
attention and expert-annotated tumor regions, supported by Dice coefficients up
to 0.88 and IoU scores up to 0.78. Clinical rule activation further validated
model predictions in cases with distinct morphological features. A
human-centered interpretability assessment involving five board-certified
radiologists yielded high Likert-scale scores for both explanation usefulness
(mean = 4.4) and heatmap-region correspondence (mean = 4.0), reinforcing the
framework's clinical relevance. Overall, the proposed approach offers a robust,
interpretable, and generalizable solution for automated brain tumor
classification, advancing the integration of deep learning into clinical
neurodiagnostics.

</details>


### [230] [Spatio-Temporal Conditional Diffusion Models for Forecasting Future Multiple Sclerosis Lesion Masks Conditioned on Treatments](https://arxiv.org/abs/2508.07006)
*Gian Mario Favero,Ge Ya Luo,Nima Fathi,Justin Szeto,Douglas L. Arnold,Brennan Nichyporuk,Chris Pal,Tal Arbel*

Main category: eess.IV

TL;DR: 该论文提出了一种基于图像的治疗感知时空扩散模型，用于预测多发性硬化症（MS）患者的未来病灶演变。


<details>
  <summary>Details</summary>
Motivation: 多发性硬化症的异质性进展需要个性化医疗，而图像生成模型可以为数据驱动的预后提供工具。

Method: 采用体素空间方法，结合多模态患者数据（如MRI和治疗信息），预测未来的新增大T2病灶（NET2）掩模。

Result: 在2131名患者的3D MRI数据集上验证，模型能准确预测六种不同治疗方案的NET2病灶掩模，并展示出临床应用的潜力。

Conclusion: 该研究展示了基于因果关系的图像生成模型在MS数据驱动预后中的潜力。

Abstract: Image-based personalized medicine has the potential to transform healthcare,
particularly for diseases that exhibit heterogeneous progression such as
Multiple Sclerosis (MS). In this work, we introduce the first treatment-aware
spatio-temporal diffusion model that is able to generate future masks
demonstrating lesion evolution in MS. Our voxel-space approach incorporates
multi-modal patient data, including MRI and treatment information, to forecast
new and enlarging T2 (NET2) lesion masks at a future time point. Extensive
experiments on a multi-centre dataset of 2131 patient 3D MRIs from randomized
clinical trials for relapsing-remitting MS demonstrate that our generative
model is able to accurately predict NET2 lesion masks for patients across six
different treatments. Moreover, we demonstrate our model has the potential for
real-world clinical applications through downstream tasks such as future lesion
count and location estimation, binary lesion activity classification, and
generating counterfactual future NET2 masks for several treatments with
different efficacies. This work highlights the potential of causal, image-based
generative models as powerful tools for advancing data-driven prognostics in
MS.

</details>


### [231] [Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities](https://arxiv.org/abs/2508.07031)
*Anindya Bijoy Das,Shahnewaz Karim Sakib,Shibbir Ahmed*

Main category: eess.IV

TL;DR: 研究分析了大型语言模型（LLMs）在医学影像任务中的幻觉问题，包括图像到文本和文本到图像的生成错误，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: LLMs在医学影像任务中常产生幻觉（自信但错误的输出），可能误导临床决策，因此需要系统性研究其错误模式。

Method: 研究通过专家评估标准，分析了LLMs在图像到文本（生成报告）和文本到图像（生成影像）任务中的幻觉问题，包括事实不一致和解剖学错误。

Result: 研究发现两种任务中均存在常见的幻觉模式，并揭示了模型架构和训练数据等因素对错误的影响。

Conclusion: 研究为提升LLM驱动的医学影像系统的安全性和可信度提供了见解。

Abstract: Large Language Models (LLMs) are increasingly applied to medical imaging
tasks, including image interpretation and synthetic image generation. However,
these models often produce hallucinations, which are confident but incorrect
outputs that can mislead clinical decisions. This study examines hallucinations
in two directions: image to text, where LLMs generate reports from X-ray, CT,
or MRI scans, and text to image, where models create medical images from
clinical prompts. We analyze errors such as factual inconsistencies and
anatomical inaccuracies, evaluating outputs using expert informed criteria
across imaging modalities. Our findings reveal common patterns of hallucination
in both interpretive and generative tasks, with implications for clinical
reliability. We also discuss factors contributing to these failures, including
model architecture and training data. By systematically studying both image
understanding and generation, this work provides insights into improving the
safety and trustworthiness of LLM driven medical imaging systems.

</details>


### [232] [3DGS-VBench: A Comprehensive Video Quality Evaluation Benchmark for 3DGS Compression](https://arxiv.org/abs/2508.07038)
*Yuke Xing,William Gordon,Qi Yang,Kaifa Yang,Jiarui Wang,Yiling Xu*

Main category: eess.IV

TL;DR: 3DGS-VBench是一个用于评估3D高斯溅射（3DGS）压缩算法视觉质量的大规模数据集和基准测试，包含660个压缩模型和视频序列，并提供了MOS评分和15种质量评估指标的对比。


<details>
  <summary>Details</summary>
Motivation: 3DGS的高存储需求限制了实际应用，现有压缩技术缺乏系统化的视觉质量评估研究。

Method: 建立了包含660个压缩3DGS模型和视频序列的数据集3DGS-VBench，通过50名参与者标注MOS评分，并验证数据集可靠性。

Result: 评估了6种3DGS压缩算法的存储效率和视觉质量，并对比了15种质量评估指标。

Conclusion: 3DGS-VBench为3DGS压缩和质量评估研究提供了专用工具，推动了相关领域的发展。

Abstract: 3D Gaussian Splatting (3DGS) enables real-time novel view synthesis with high
visual fidelity, but its substantial storage requirements hinder practical
deployment, prompting state-of-the-art (SOTA) 3DGS methods to incorporate
compression modules. However, these 3DGS generative compression techniques
introduce unique distortions lacking systematic quality assessment research. To
this end, we establish 3DGS-VBench, a large-scale Video Quality Assessment
(VQA) Dataset and Benchmark with 660 compressed 3DGS models and video sequences
generated from 11 scenes across 6 SOTA 3DGS compression algorithms with
systematically designed parameter levels. With annotations from 50
participants, we obtained MOS scores with outlier removal and validated dataset
reliability. We benchmark 6 3DGS compression algorithms on storage efficiency
and visual quality, and evaluate 15 quality assessment metrics across multiple
paradigms. Our work enables specialized VQA model training for 3DGS, serving as
a catalyst for compression and quality assessment research. The dataset is
available at https://github.com/YukeXing/3DGS-VBench.

</details>


### [233] [SAGCNet: Spatial-Aware Graph Completion Network for Missing Slice Imputation in Population CMR Imaging](https://arxiv.org/abs/2508.07041)
*Junkai Liu,Nay Aung,Theodoros N. Arvanitis,Stefan K. Piechnik,Joao A C Lima,Steffen E. Petersen,Le Zhang*

Main category: eess.IV

TL;DR: SAGCNet通过图结构和空间适配器解决了MRI缺失切片合成问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: MRI缺失或不可用切片影响诊断准确性，现有方法难以建模3D切片间的依赖关系。

Method: 提出SAGCNet，包含切片图完成模块和空间适配器，捕捉3D空间上下文。

Result: 在心脏MRI数据集上表现优异，定量和定性均优于现有方法。

Conclusion: SAGCNet能有效合成缺失切片，且在数据有限时仍保持高性能。

Abstract: Magnetic resonance imaging (MRI) provides detailed soft-tissue
characteristics that assist in disease diagnosis and screening. However, the
accuracy of clinical practice is often hindered by missing or unusable slices
due to various factors. Volumetric MRI synthesis methods have been developed to
address this issue by imputing missing slices from available ones. The inherent
3D nature of volumetric MRI data, such as cardiac magnetic resonance (CMR),
poses significant challenges for missing slice imputation approaches, including
(1) the difficulty of modeling local inter-slice correlations and dependencies
of volumetric slices, and (2) the limited exploration of crucial 3D spatial
information and global context. In this study, to mitigate these issues, we
present Spatial-Aware Graph Completion Network (SAGCNet) to overcome the
dependency on complete volumetric data, featuring two main innovations: (1) a
volumetric slice graph completion module that incorporates the inter-slice
relationships into a graph structure, and (2) a volumetric spatial adapter
component that enables our model to effectively capture and utilize various
forms of 3D spatial context. Extensive experiments on cardiac MRI datasets
demonstrate that SAGCNet is capable of synthesizing absent CMR slices,
outperforming competitive state-of-the-art MRI synthesis methods both
quantitatively and qualitatively. Notably, our model maintains superior
performance even with limited slice data.

</details>


### [234] [Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications](https://arxiv.org/abs/2508.07165)
*Zelin Qiu,Xi Wang,Zhuoyao Xie,Juan Zhou,Yu Wang,Lingjie Yang,Xinrui Jiang,Juyoung Bae,Moo Hyun Son,Qiang Ye,Dexuan Chen,Rui Zhang,Tao Li,Neeraj Ramesh Mahboobani,Varut Vardhanabhuti,Xiaohui Duan,Yinghua Zhao,Hao Chen*

Main category: eess.IV

TL;DR: PRISM是一个基于大规模多序列MRI预训练的基础模型，通过解耦解剖不变特征和序列特异性变化，提升了模型在不同MRI协议下的泛化能力，在44个下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多序列MRI的异质性对深度学习模型泛化能力的挑战，提升模型在不同采集参数下的性能，增强临床实用性。

Method: 收集64个数据集（34个用于预训练），提出新预训练范式，解耦解剖不变特征与序列特异性变化，构建44个下游任务基准。

Result: PRISM在39/44任务中显著优于非预训练模型和现有基础模型，展现了跨协议数据的鲁棒性和泛化能力。

Conclusion: PRISM为多序列MRI分析提供了可扩展框架，增强了AI在放射学中的转化潜力，具有广泛的临床适用性。

Abstract: Multi-sequence Magnetic Resonance Imaging (MRI) offers remarkable
versatility, enabling the distinct visualization of different tissue types.
Nevertheless, the inherent heterogeneity among MRI sequences poses significant
challenges to the generalization capability of deep learning models. These
challenges undermine model performance when faced with varying acquisition
parameters, thereby severely restricting their clinical utility. In this study,
we present PRISM, a foundation model PRe-trained with large-scale
multI-Sequence MRI. We collected a total of 64 datasets from both public and
private sources, encompassing a wide range of whole-body anatomical structures,
with scans spanning diverse MRI sequences. Among them, 336,476 volumetric MRI
scans from 34 datasets (8 public and 26 private) were curated to construct the
largest multi-organ multi-sequence MRI pretraining corpus to date. We propose a
novel pretraining paradigm that disentangles anatomically invariant features
from sequence-specific variations in MRI, while preserving high-level semantic
representations. We established a benchmark comprising 44 downstream tasks,
including disease diagnosis, image segmentation, registration, progression
prediction, and report generation. These tasks were evaluated on 32 public
datasets and 5 private cohorts. PRISM consistently outperformed both
non-pretrained models and existing foundation models, achieving first-rank
results in 39 out of 44 downstream benchmarks with statistical significance
improvements. These results underscore its ability to learn robust and
generalizable representations across unseen data acquired under diverse MRI
protocols. PRISM provides a scalable framework for multi-sequence MRI analysis,
thereby enhancing the translational potential of AI in radiology. It delivers
consistent performance across diverse imaging protocols, reinforcing its
clinical applicability.

</details>


### [235] [HaDM-ST: Histology-Assisted Differential Modeling for Spatial Transcriptomics Generation](https://arxiv.org/abs/2508.07225)
*Xuepeng Liu,Zheng Jiang,Pinan Zhu,Hanyu Liu,Chao Li*

Main category: eess.IV

TL;DR: HaDM-ST是一种基于H&E图像和低分辨率空间转录组学的高分辨率ST生成框架，通过语义蒸馏网络、空间对齐模块和通道感知对抗学习器解决现有方法的三大挑战。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学（ST）的分辨率受限于现有平台，而现有方法在结合H&E染色组织学图像时面临三大挑战：提取表达相关特征、实现多模态对齐和建模基因特异性变异。

Method: HaDM-ST包括：1）语义蒸馏网络提取H&E图像的预测线索；2）空间对齐模块确保与低分辨率ST的像素级对应；3）通道感知对抗学习器进行细粒度基因级建模。

Result: 在200个基因的实验中，HaDM-ST在多种组织和物种中表现优于现有方法，提高了高分辨率ST预测的空间保真度和基因级一致性。

Conclusion: HaDM-ST通过结合H&E图像和低分辨率ST，有效解决了现有方法的局限性，显著提升了高分辨率ST生成的性能。

Abstract: Spatial transcriptomics (ST) reveals spatial heterogeneity of gene
expression, yet its resolution is limited by current platforms. Recent methods
enhance resolution via H&E-stained histology, but three major challenges
persist: (1) isolating expression-relevant features from visually complex H&E
images; (2) achieving spatially precise multimodal alignment in diffusion-based
frameworks; and (3) modeling gene-specific variation across expression
channels. We propose HaDM-ST (Histology-assisted Differential Modeling for ST
Generation), a high-resolution ST generation framework conditioned on H&E
images and low-resolution ST. HaDM-ST includes: (i) a semantic distillation
network to extract predictive cues from H&E; (ii) a spatial alignment module
enforcing pixel-wise correspondence with low-resolution ST; and (iii) a
channel-aware adversarial learner for fine-grained gene-level modeling.
Experiments on 200 genes across diverse tissues and species show HaDM-ST
consistently outperforms prior methods, enhancing spatial fidelity and
gene-level coherence in high-resolution ST predictions.

</details>


### [236] [DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework](https://arxiv.org/abs/2508.07682)
*Wenzhuo Ma,Zhenzhong Chen*

Main category: eess.IV

TL;DR: DiffVC-OSD是一种基于单步扩散的感知神经视频压缩框架，通过直接输入重建的潜在表示，结合时间上下文和潜在特征，提升感知质量。


<details>
  <summary>Details</summary>
Motivation: 传统多步扩散方法效率较低，DiffVC-OSD旨在通过单步扩散提升压缩效率和感知质量。

Method: 采用单步扩散模型，设计时间上下文适配器以利用时间依赖性，并通过端到端微调优化压缩性能。

Result: 实验表明，DiffVC-OSD在感知压缩性能上达到最优，解码速度提升约20倍，比特率降低86.92%。

Conclusion: DiffVC-OSD通过单步扩散显著提升了视频压缩的效率和感知质量。

Abstract: In this work, we first propose DiffVC-OSD, a One-Step Diffusion-based
Perceptual Neural Video Compression framework. Unlike conventional multi-step
diffusion-based methods, DiffVC-OSD feeds the reconstructed latent
representation directly into a One-Step Diffusion Model, enhancing perceptual
quality through a single diffusion step guided by both temporal context and the
latent itself. To better leverage temporal dependencies, we design a Temporal
Context Adapter that encodes conditional inputs into multi-level features,
offering more fine-grained guidance for the Denoising Unet. Additionally, we
employ an End-to-End Finetuning strategy to improve overall compression
performance. Extensive experiments demonstrate that DiffVC-OSD achieves
state-of-the-art perceptual compression performance, offers about 20$\times$
faster decoding and a 86.92\% bitrate reduction compared to the corresponding
multi-step diffusion-based variant.

</details>


### [237] [Anatomy-Aware Low-Dose CT Denoising via Pretrained Vision Models and Semantic-Guided Contrastive Learning](https://arxiv.org/abs/2508.07788)
*Runze Wang,Zeli Chen,Zhiyun Song,Wei Fang,Jiajin Zhang,Danyang Tu,Yuxing Tang,Minfeng Xu,Xianghua Ye,Le Lu,Dakai Jin*

Main category: eess.IV

TL;DR: ALDEN是一种基于深度学习的低剂量CT去噪方法，通过结合预训练视觉模型的语义特征与对抗和对比学习，显著提升了去噪效果并保留了解剖结构。


<details>
  <summary>Details</summary>
Motivation: 现有低剂量CT去噪方法常忽略解剖语义，导致去噪效果不佳。ALDEN旨在通过引入解剖感知机制解决这一问题。

Method: ALDEN结合了对抗学习和对比学习，利用解剖感知判别器和语义引导对比学习模块，动态融合层次语义特征并保持解剖一致性。

Result: 在多个数据集上，ALDEN实现了最先进的性能，显著减少了过平滑问题，并在下游多器官分割任务中验证了其解剖保留能力。

Conclusion: ALDEN通过解剖感知机制显著提升了低剂量CT去噪效果，为临床诊断提供了更可靠的图像质量。

Abstract: To reduce radiation exposure and improve the diagnostic efficacy of low-dose
computed tomography (LDCT), numerous deep learning-based denoising methods have
been developed to mitigate noise and artifacts. However, most of these
approaches ignore the anatomical semantics of human tissues, which may
potentially result in suboptimal denoising outcomes. To address this problem,
we propose ALDEN, an anatomy-aware LDCT denoising method that integrates
semantic features of pretrained vision models (PVMs) with adversarial and
contrastive learning. Specifically, we introduce an anatomy-aware discriminator
that dynamically fuses hierarchical semantic features from reference
normal-dose CT (NDCT) via cross-attention mechanisms, enabling tissue-specific
realism evaluation in the discriminator. In addition, we propose a
semantic-guided contrastive learning module that enforces anatomical
consistency by contrasting PVM-derived features from LDCT, denoised CT and
NDCT, preserving tissue-specific patterns through positive pairs and
suppressing artifacts via dual negative pairs. Extensive experiments conducted
on two LDCT denoising datasets reveal that ALDEN achieves the state-of-the-art
performance, offering superior anatomy preservation and substantially reducing
over-smoothing issue of previous work. Further validation on a downstream
multi-organ segmentation task (encompassing 117 anatomical structures) affirms
the model's ability to maintain anatomical awareness.

</details>


### [238] [Towards Human-AI Collaboration System for the Detection of Invasive Ductal Carcinoma in Histopathology Images](https://arxiv.org/abs/2508.07875)
*Shuo Han,Ahmed Karam Eldaly,Solomon Sunday Oyelere*

Main category: eess.IV

TL;DR: 提出了一种人机交互（HITL）深度学习系统，结合EfficientNetV2S模型和专家反馈，提高侵袭性导管癌（IDC）检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 早期准确诊断IDC对提高患者生存率至关重要，结合AI与医学专家知识有望提升检测效率和精度。

Method: 采用EfficientNetV2S模型进行初步诊断，专家修正误分类图像并反馈至训练数据，形成迭代优化循环。

Result: 模型初始准确率达93.65%，结合HITL系统后进一步提升了性能。

Conclusion: 人机协作方法为AI辅助医疗诊断提供了高效、准确的新方向。

Abstract: Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer,
and early, accurate diagnosis is critical to improving patient survival rates
by guiding treatment decisions. Combining medical expertise with artificial
intelligence (AI) holds significant promise for enhancing the precision and
efficiency of IDC detection. In this work, we propose a human-in-the-loop
(HITL) deep learning system designed to detect IDC in histopathology images.
The system begins with an initial diagnosis provided by a high-performance
EfficientNetV2S model, offering feedback from AI to the human expert. Medical
professionals then review the AI-generated results, correct any misclassified
images, and integrate the revised labels into the training dataset, forming a
feedback loop from the human back to the AI. This iterative process refines the
model's performance over time. The EfficientNetV2S model itself achieves
state-of-the-art performance compared to existing methods in the literature,
with an overall accuracy of 93.65\%. Incorporating the human-in-the-loop system
further improves the model's accuracy using four experimental groups with
misclassified images. These results demonstrate the potential of this
collaborative approach to enhance AI performance in diagnostic systems. This
work contributes to advancing automated, efficient, and highly accurate methods
for IDC detection through human-AI collaboration, offering a promising
direction for future AI-assisted medical diagnostics.

</details>


### [239] [Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models](https://arxiv.org/abs/2508.07903)
*Johanna P. Müller,Anika Knupfer,Pedro Blöss,Edoardo Berardi Vittur,Bernhard Kainz,Jana Hutter*

Main category: eess.IV

TL;DR: 提出了一种基于扩散模型的子宫MRI合成框架，结合无条件与条件DDPMs和LDMs，生成高保真合成图像，解决了现有模型在女性盆腔图像生成中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成精确的女性盆腔解剖图像方面存在困难，限制了其在妇科影像中的应用，尤其是在数据稀缺和患者隐私问题突出的情况下。

Method: 结合无条件与条件Denoising Diffusion Probabilistic Models (DDPMs)和Latent Diffusion Models (LDMs)，在2D和3D中生成子宫MRI合成图像。

Result: 生成的合成图像在解剖学上一致且高保真，通过感知和分布指标评估，显著提升了诊断准确性，并得到专家盲评的临床真实性验证。

Conclusion: 该框架为妇科影像提供了高质量的合成数据资源，支持可重复研究并推动妇科AI的公平发展。

Abstract: Despite significant progress in generative modelling, existing diffusion
models often struggle to produce anatomically precise female pelvic images,
limiting their application in gynaecological imaging, where data scarcity and
patient privacy concerns are critical. To overcome these barriers, we introduce
a novel diffusion-based framework for uterine MRI synthesis, integrating both
unconditional and conditioned Denoising Diffusion Probabilistic Models (DDPMs)
and Latent Diffusion Models (LDMs) in 2D and 3D. Our approach generates
anatomically coherent, high fidelity synthetic images that closely mimic real
scans and provide valuable resources for training robust diagnostic models. We
evaluate generative quality using advanced perceptual and distributional
metrics, benchmarking against standard reconstruction methods, and demonstrate
substantial gains in diagnostic accuracy on a key classification task. A
blinded expert evaluation further validates the clinical realism of our
synthetic images. We release our models with privacy safeguards and a
comprehensive synthetic uterine MRI dataset to support reproducible research
and advance equitable AI in gynaecology.

</details>


### [240] [A Physics-Driven Neural Network with Parameter Embedding for Generating Quantitative MR Maps from Weighted Images](https://arxiv.org/abs/2508.08123)
*Lingjing Chen,Chengxiu Zhang,Yinqiao Yi,Yida Wang,Yang Song,Xu Yan,Shengfang Xu,Dalin Zhu,Mengqiu Cao,Yan Zhou,Chenglong Wang,Guang Yang*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的MRI定量图像合成方法，通过嵌入MRI序列参数（TR、TE、TI）提升准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统MRI定量图像合成方法在准确性和泛化性上存在不足，尤其是在处理未见过的病理区域时表现不佳。

Method: 采用物理驱动的神经网络，直接嵌入MRI序列参数，学习MRI信号形成的物理原理，输入T1、T2和T2-FLAIR图像，合成T1、T2和PD定量图。

Result: 在内部和外部测试数据集上表现优异，PSNR超过34 dB，SSIM高于0.92，优于传统深度学习模型，并能准确合成未见病理区域的定量图。

Conclusion: 通过参数嵌入显著提升了定量MRI合成的性能和可靠性，具有加速qMRI和提升临床实用性的潜力。

Abstract: We propose a deep learning-based approach that integrates MRI sequence
parameters to improve the accuracy and generalizability of quantitative image
synthesis from clinical weighted MRI. Our physics-driven neural network embeds
MRI sequence parameters -- repetition time (TR), echo time (TE), and inversion
time (TI) -- directly into the model via parameter embedding, enabling the
network to learn the underlying physical principles of MRI signal formation.
The model takes conventional T1-weighted, T2-weighted, and T2-FLAIR images as
input and synthesizes T1, T2, and proton density (PD) quantitative maps.
Trained on healthy brain MR images, it was evaluated on both internal and
external test datasets. The proposed method achieved high performance with PSNR
values exceeding 34 dB and SSIM values above 0.92 for all synthesized parameter
maps. It outperformed conventional deep learning models in accuracy and
robustness, including data with previously unseen brain structures and lesions.
Notably, our model accurately synthesized quantitative maps for these unseen
pathological regions, highlighting its superior generalization capability.
Incorporating MRI sequence parameters via parameter embedding allows the neural
network to better learn the physical characteristics of MR signals,
significantly enhancing the performance and reliability of quantitative MRI
synthesis. This method shows great potential for accelerating qMRI and
improving its clinical utility.

</details>


### [241] [RedDino: A foundation model for red blood cell analysis](https://arxiv.org/abs/2508.08180)
*Luca Zedda,Andrea Loddo,Cecilia Di Ruberto,Carsten Marr*

Main category: eess.IV

TL;DR: RedDino是一种自监督基础模型，专为红细胞图像分析设计，性能优于现有最先进模型。


<details>
  <summary>Details</summary>
Motivation: 红细胞形态分析对血液疾病诊断至关重要，但目前缺乏全面的AI解决方案。

Method: RedDino基于DINOv2框架的自监督学习，训练了125万张红细胞图像。

Result: RedDino在红细胞形状分类中表现优异，具有强大的特征表示和泛化能力。

Conclusion: RedDino为计算血液学提供了可靠工具，代码和预训练模型已开源。

Abstract: Red blood cells (RBCs) are essential to human health, and their precise
morphological analysis is important for diagnosing hematological disorders.
Despite the promise of foundation models in medical diagnostics, comprehensive
AI solutions for RBC analysis remain scarce. We present RedDino, a
self-supervised foundation model designed for RBC image analysis. RedDino uses
an RBC-specific adaptation of the DINOv2 self-supervised learning framework and
is trained on a curated dataset of 1.25 million RBC images from diverse
acquisition modalities and sources. Extensive evaluations show that RedDino
outperforms existing state-of-the-art models on RBC shape classification.
Through assessments including linear probing and nearest neighbor
classification, we confirm its strong feature representations and
generalization ability. Our main contributions are: (1) a foundation model
tailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations
for RBC modeling, and (3) a detailed evaluation of generalization performance.
RedDino addresses key challenges in computational hematology by capturing
nuanced morphological features, advancing the development of reliable
diagnostic tools. The source code and pretrained models for RedDino are
available at https://github.com/Snarci/RedDino, and the pretrained models can
be downloaded from our Hugging Face collection at
https://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [242] [PureSample: Neural Materials Learned by Sampling Microgeometry](https://arxiv.org/abs/2508.07240)
*Zixuan Li,Zixiong Wang,Jian Yang,Milos Hasan,Beibei Wang*

Main category: cs.GR

TL;DR: PureSample是一种新型神经BRDF表示方法，通过随机行走采样学习材料行为，简化了传统BRDF模型的复杂推导过程。


<details>
  <summary>Details</summary>
Motivation: 传统基于物理的材料模型依赖复杂的解析BRDF推导，且忽略了空间变化，难以实现高效采样和评估。

Method: PureSample通过两个可学习组件实现：一是基于流匹配神经网络的采样分布模型，二是轻量级神经网络捕获的视角相关反照率项。

Result: 该方法在多层材料、多次散射微表面材料等多种复杂材料上表现优异。

Conclusion: PureSample提供了一种高效、通用的BRDF表示方法，适用于均质和非均质材料。

Abstract: Traditional physically-based material models rely on analytically derived
bidirectional reflectance distribution functions (BRDFs), typically by
considering statistics of micro-primitives such as facets, flakes, or spheres,
sometimes combined with multi-bounce interactions such as layering and multiple
scattering. These derivations are often complex and model-specific, and
typically consider a statistical aggregate of a large surface area, ignoring
spatial variation. Once an analytic BRDF's evaluation is defined, one still
needs to design an importance sampling method for it, and a way to evaluate the
pdf of that sampling distribution, requiring further model-specific
derivations.
  We present PureSample: a novel neural BRDF representation that allows
learning a material's behavior purely by sampling forward random walks on the
microgeometry, which is usually straightforward to implement. Our
representation allows for efficient importance sampling, pdf evaluation, and
BRDF evaluation, for homogeneous as well as spatially varying materials.
  We achieve this by two learnable components: first, the sampling distribution
is modeled using a flow matching neural network, which allows both importance
sampling and pdf evaluation; second, we introduce a view-dependent albedo term,
captured by a lightweight neural network, which allows for converting a scalar
pdf value to a colored BRDF value for any pair of view and light directions.
  We demonstrate PureSample on challenging materials, including multi-layered
materials, multiple-scattering microfacet materials, and various other
microstructures.

</details>


### [243] [Verification Method for Graph Isomorphism Criteria](https://arxiv.org/abs/2508.07615)
*Chuanfu Hu,Aimin Hou*

Main category: cs.GR

TL;DR: 本文提出了一种验证方法，用于判断前人提出的图同构判定条件是否为充要条件，并提出了一种细分方法以减少回溯空间。


<details>
  <summary>Details</summary>
Motivation: 图同构判定标准对解决图同构问题至关重要，但现有方法存在回溯和证明难度大的问题。

Method: 提出验证方法判断充要条件，并设计细分方法优化必要条件的候选空间。

Result: 验证方法可确保判定条件的正确性，细分方法有效减少回溯空间。

Conclusion: 该方法为图同构判定提供了更高效和可靠的解决方案。

Abstract: The criteria for determining graph isomorphism are crucial for solving graph
isomorphism problems. The necessary condition is that two isomorphic graphs
possess invariants, but their function can only be used to filtrate and
subdivide candidate spaces. The sufficient conditions are used to rebuild the
isomorphic reconstruction of special graphs, but their drawback is that the
isomorphic functions of subgraphs may not form part of the isomorphic functions
of the parent graph. The use of sufficient or necessary conditions generally
results in backtracking to ensure the correctness of the decision algorithm.
The sufficient and necessary conditions can ensure that the determination of
graph isomorphism does not require backtracking, but the correctness of its
proof process is difficult to guarantee. This article proposes a verification
method that can correctly determine whether the judgment conditions proposed by
previous researchers are sufficient and necessary conditions. A subdivision
method has also been proposed in this article, which can obtain more
subdivisions for necessary conditions and effectively reduce the size of
backtracking space.

</details>


### [244] [Vertex Features for Neural Global Illumination](https://arxiv.org/abs/2508.07852)
*Rui Su,Honghao Dong,Haojie Jin,Yisong Chen,Guoping Wang,Sheng Li*

Main category: cs.GR

TL;DR: 提出了一种基于网格顶点的可学习神经表示方法，显著降低了内存占用并保持了渲染质量。


<details>
  <summary>Details</summary>
Motivation: 传统特征网格表示在3D场景重建和神经渲染中内存占用高，限制了并行计算硬件的效率。

Method: 将可学习特征直接存储在网格顶点上，利用几何结构作为紧凑表示，结合任务特定的几何先验优化特征。

Result: 实验表明，该方法内存消耗仅为网格表示的五分之一或更低，同时保持渲染质量并降低推理开销。

Conclusion: 神经顶点特征是一种高效且紧凑的表示方法，适用于神经渲染任务。

Abstract: Recent research on learnable neural representations has been widely adopted
in the field of 3D scene reconstruction and neural rendering applications.
However, traditional feature grid representations often suffer from substantial
memory footprint, posing a significant bottleneck for modern parallel computing
hardware. In this paper, we present neural vertex features, a generalized
formulation of learnable representation for neural rendering tasks involving
explicit mesh surfaces. Instead of uniformly distributing neural features
throughout 3D space, our method stores learnable features directly at mesh
vertices, leveraging the underlying geometry as a compact and structured
representation for neural processing. This not only optimizes memory
efficiency, but also improves feature representation by aligning compactly with
the surface using task-specific geometric priors. We validate our neural
representation across diverse neural rendering tasks, with a specific emphasis
on neural radiosity. Experimental results demonstrate that our method reduces
memory consumption to only one-fifth (or even less) of grid-based
representations, while maintaining comparable rendering quality and lowering
inference overhead.

</details>


### [245] [Emergent morphogenesis via planar fabrication enabled by a reduced model of composites](https://arxiv.org/abs/2508.08198)
*Yupeng Zhang,Adam Alon,M. Khalid Jawed*

Main category: cs.GR

TL;DR: 提出了一种基于双层系统的简化数值和实验框架，通过热响应材料与基里加米图案层的结合，实现从平面到复杂3D形态的可编程控制。


<details>
  <summary>Details</summary>
Motivation: 解决软机器人、可重构设备和功能材料中复杂3D形态的精确设计与制造问题。

Method: 使用热响应热塑性材料与基里加米图案层结合，通过单层简化模型模拟拉伸与弯曲耦合力学。

Result: 实现了多种3D形态（如碗、独木舟、花瓣）的高效计算设计与可扩展制造。

Conclusion: 该框架为复杂3D形态的设计与制造提供了高效且可扩展的解决方案。

Abstract: The ability to engineer complex three-dimensional shapes from planar sheets
with precise, programmable control underpins emerging technologies in soft
robotics, reconfigurable devices, and functional materials. Here, we present a
reduced-order numerical and experimental framework for a bilayer system
consisting of a stimuli-responsive thermoplastic sheet (Shrinky Dink) bonded to
a kirigami-patterned, inert plastic layer. Upon uniform heating, the active
layer contracts while the patterned layer constrains in-plane stretch but
allows out-of-plane bending, yielding programmable 3D morphologies from simple
planar precursors. Our approach enables efficient computational design and
scalable manufacturing of 3D forms with a single-layer reduced model that
captures the coupled mechanics of stretching and bending. Unlike traditional
bilayer modeling, our framework collapses the multilayer composite into a
single layer of nodes and elements, reducing the degrees of freedom and
enabling simulation on a 2D geometry. This is achieved by introducing a novel
energy formulation that captures the coupling between in-plane stretch mismatch
and out-of-plane bending - extending beyond simple isotropic linear elastic
models. Experimentally, we establish a fully planar, repeatable fabrication
protocol using a stimuli-responsive thermoplastic and a laser-cut inert plastic
layer. The programmed strain mismatch drives an array of 3D morphologies, such
as bowls, canoes, and flower petals, all verified by both simulation and
physical prototypes.

</details>


### [246] [LL3M: Large Language 3D Modelers](https://arxiv.org/abs/2508.08228)
*Sining Lu,Guan Chen,Nam Anh Dinh,Itai Lang,Ari Holtzman,Rana Hanocka*

Main category: cs.GR

TL;DR: LL3M是一个多智能体系统，利用预训练大语言模型（LLMs）通过编写可解释的Python代码在Blender中生成3D资产。它将形状生成重新定义为代码编写任务，提供更高的模块化、可编辑性和与艺术家工作流的集成。


<details>
  <summary>Details</summary>
Motivation: 传统生成方法依赖于3D数据学习，而LL3M通过代码生成实现更灵活、可编辑的3D资产创建，并与人类工作流无缝集成。

Method: LL3M协调多个专用LLM智能体，通过规划、检索、编写、调试和优化Blender脚本来生成和编辑几何与外观。系统利用BlenderRAG知识库增强代码正确性和建模能力。

Result: 实验表明，LL3M能生成多样化的形状、材质和场景，支持用户驱动的迭代优化，展示了代码作为生成媒介的强大能力。

Conclusion: LL3M证明了代码作为生成和解释3D资产的媒介的潜力，为3D创作提供了新的协同创作模式。

Abstract: We present LL3M, a multi-agent system that leverages pretrained large
language models (LLMs) to generate 3D assets by writing interpretable Python
code in Blender. We break away from the typical generative approach that learns
from a collection of 3D data. Instead, we reformulate shape generation as a
code-writing task, enabling greater modularity, editability, and integration
with artist workflows. Given a text prompt, LL3M coordinates a team of
specialized LLM agents to plan, retrieve, write, debug, and refine Blender
scripts that generate and edit geometry and appearance. The generated code
works as a high-level, interpretable, human-readable, well-documented
representation of scenes and objects, making full use of sophisticated Blender
constructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse,
unconstrained shapes, materials, and scenes. This code presents many avenues
for further agent and human editing and experimentation via code tweaks or
procedural parameters. This medium naturally enables a co-creative loop in our
system: agents can automatically self-critique using code and visuals, while
iterative user instructions provide an intuitive way to refine assets. A shared
code context across agents enables awareness of previous attempts, and a
retrieval-augmented generation knowledge base built from Blender API
documentation, BlenderRAG, equips agents with examples, types, and functions
empowering advanced modeling operations and code correctness. We demonstrate
the effectiveness of LL3M across diverse shape categories, style and material
edits, and user-driven refinements. Our experiments showcase the power of code
as a generative and interpretable medium for 3D asset creation. Our project
page is at https://threedle.github.io/ll3m.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [247] [Tidal Triggering of Magnitude 7+ Earthquakes by Jupiter](https://arxiv.org/abs/2508.07064)
*E. W. Holt,Eric Newman*

Main category: physics.geo-ph

TL;DR: 该研究通过卡方检验分析1960年至2024年间M7+地震活动是否与地球在绕日轨道上的位置独立，发现木星、金星和土星的特定轨道位置与地震活动显著相关。


<details>
  <summary>Details</summary>
Motivation: 探究地震活动是否与地球在绕日轨道上的位置有关，特别是与木星、金星、土星和火星的特定轨道位置的关系。

Method: 使用卡方独立性检验，分析64年的地震和天文数据，针对木星、金星、土星和火星的特定轨道位置进行28,782次测试。

Result: 发现木星的特定轨道位置（下合和“前小潮”位置）与M7+地震活动显著相关，活动呈现脉冲式变化；金星和土星也观察到类似现象。

Conclusion: 地震活动与某些行星的轨道位置存在显著关联，但未发现与月球周期的明显相关性。

Abstract: This work uses a chi-squared test of independence to determine if days that
include earthquakes greater than or equal to magnitude 7 (M7+) from 1960 to
2024 are truly independent of the position of Earth in its orbit around the
sun. To this end, this study breaks up Earth's orbit into days offset on either
side of two reference Earth-Sun-Planet orientations, or zero-points: opposition
and inferior conjunction. A computer program is used to sample U.S.G.S.
earthquake and N.A.S.A. Horizons ephemeris data for the last 64 years with the
purpose of conducting 28,782 chi-squared tests-of-independence for all
intervals (5 to 45 days) spanning the entirety of Earth's synodic period
relative to these zero points for Jupiter, Venus, Saturn, and Mars. For
Jupiter, 1,071 statistically significant intervals of M7+ activity are
associated with two particular points: the inferior conjunction and what is
here termed the "preceding neap" position. At both of these points, M7+
activity first increases (>125% average) and then sharply decreases in a
pulse-like fashion, with those lulls in M7+ activity (<75% of average) lasting
about a month. Both of these pulses of M7+ activity begin at
Sun-Observer-Target (SOT) Angles near 45 degrees and 135 degrees, and this also
observed for Venus and Saturn; Mars synodic period prevented any comparison of
chi-squared intervals to SOT Angle. Although this study did not observe any
obvious correlation of M7+ activity with the lunar cycle, the medians, means,
and modes of the significant intervals returned by the chi-squared analysis for
Jupiter range from 27 to 34 days, suggesting that intervals are more likely to
be found significant by the chi-squared analysis if they average out the lunar
cycle.

</details>


### [248] [Forward and adjoint calculations of gravitational potential in heterogeneous, aspherical planets](https://arxiv.org/abs/2508.07910)
*Alex D. C. Myhill,Matthew A. Maitra,David Al-Attar*

Main category: physics.geo-ph

TL;DR: 开发了一个计算包，用于精确计算非球形、非均匀行星的内外引力势及其泛函导数和灵敏度核。方法包括泊松方程变换和伪谱/谱元离散化，并以火卫一为例验证。


<details>
  <summary>Details</summary>
Motivation: 解决非球形、非均匀行星引力势计算的精确性问题，为相关研究提供高效工具。

Method: 通过泊松方程变换到参考域，采用伪谱/谱元离散化方法，推导了势的一阶扰动展开和灵敏度核。

Result: 计算了火卫一的引力势，验证了方法的准确性，并量化了误差。推导了灵敏度核和扰动表达式。

Conclusion: 该计算包为行星引力势研究提供了高效、精确的工具，并推导了相关数学关系。

Abstract: We have developed a computational package for the calculation of numerically
exact internal and external gravitational potential, its functional derivatives
and sensitivity kernels, in an aspherical, heterogeneous planet. We detail our
implementation, utilizing a transformation of the Poisson equation into a
reference domain, as well as a pseudospectral/spectral element discretisation.
The use of the forward solver within the package is demonstrated by calculating
the gravitational potential of Phobos with homogeneous and heterogeneous
density models. Equations for the first-order perturbation expansion of
potential in the referential formulation are found, and the magnitude of the
error is quantified based on the exact method. The adjoint Poisson equation is
derived, and from it the sensitivity kernels for objective functionals of the
potential, which are calculated for Phobos. The expression for perturbations to
an objective functional is reduced to a single body and surface integral.
Finally, a relation is obtained between the sensitivity kernels which must be
satisfied when using a computational domain different to the physical domain.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [249] [IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model](https://arxiv.org/abs/2508.06571)
*Anqing Jiang,Yu Gao,Yiru Wang,Zhigang Sun,Shuo Wang,Yuwen Heng,Hao Sun,Shichen Tang,Lijuan Zhu,Jinhao Chai,Jijun Wang,Zichong Gu,Hao Jiang,Li Sun*

Main category: cs.AI

TL;DR: 本文提出IRL-VLA框架，通过逆强化学习构建轻量级奖励世界模型，结合VLA架构和PPO算法，显著提升自动驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA架构基于开环模仿学习，性能受限；闭环训练依赖高保真传感器模拟，存在领域差距和计算效率问题。

Method: 三阶段框架：1) 预训练VLA策略；2) 逆强化学习构建奖励世界模型；3) PPO优化奖励模型指导的强化学习。

Result: 在NAVSIM v2和CVPR2025挑战赛中取得最优性能。

Conclusion: IRL-VLA框架为闭环自动驾驶中的VLA研究提供了高效解决方案。

Abstract: Vision-Language-Action (VLA) models have demonstrated potential in autonomous
driving. However, two critical challenges hinder their development: (1)
Existing VLA architectures are typically based on imitation learning in
open-loop setup which tends to capture the recorded behaviors in the dataset,
leading to suboptimal and constrained performance, (2) Close-loop training
relies heavily on high-fidelity sensor simulation, where domain gaps and
computational inefficiencies pose significant barriers. In this paper, we
introduce IRL-VLA, a novel close-loop Reinforcement Learning via
\textbf{I}nverse \textbf{R}einforcement \textbf{L}earning reward world model
with a self-built VLA approach. Our framework proceeds in a three-stage
paradigm: In the first stage, we propose a VLA architecture and pretrain the
VLA policy via imitation learning. In the second stage, we construct a
lightweight reward world model via inverse reinforcement learning to enable
efficient close-loop reward computation. To further enhance planning
performance, finally, we design specialized reward world model guidence
reinforcement learning via PPO(Proximal Policy Optimization) to effectively
balance the safety incidents, comfortable driving, and traffic efficiency. Our
approach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving
benchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that
our framework will accelerate VLA research in close-loop autonomous driving.

</details>


### [250] [CountQA: How Well Do MLLMs Count in the Wild?](https://arxiv.org/abs/2508.06585)
*Jayant Sravan Tamarapalli,Rynaa Grover,Nilay Pande,Sahiti Yerramilli*

Main category: cs.AI

TL;DR: 多模态大语言模型（MLLMs）在视觉场景理解上表现出色，但在对象计数这一基本认知技能上存在严重缺陷。CountQA是一个新基准，旨在测试和改善这一缺陷。


<details>
  <summary>Details</summary>
Motivation: MLLMs在对象计数方面的不足限制了其在实际应用中的可靠性，现有基准无法全面测试复杂场景下的计数能力。

Method: 提出CountQA基准，包含1,500多个问答对，涵盖高密度、杂乱和遮挡的真实世界图像，评估了15种主流MLLMs。

Result: 表现最佳的模型准确率仅为42.9%，且随着对象数量增加性能下降。

Conclusion: CountQA为诊断和改善MLLMs的计数能力提供了工具，推动开发更具数值和空间感知能力的模型。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in
understanding visual scenes, yet they exhibit a critical lack in a fundamental
cognitive skill: object counting. This blind spot severely limits their
reliability in real-world applications. To date, this capability has been
largely unevaluated in complex scenarios, as existing benchmarks either feature
sparse object densities or are confined to specific visual domains, failing to
test models under realistic conditions. Addressing this gap, we introduce
CountQA, a challenging new benchmark designed to probe this deficiency.
Comprising over 1,500 question-answer pairs, CountQA features real-world images
with high object density, clutter, and occlusion. We investigate this weakness
by evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the
top-performing model achieves a mere 42.9% accuracy, with performance declining
as object counts rise. By providing a dedicated benchmark to diagnose and
rectify this core weakness, CountQA paves the way for a new generation of MLLMs
that are not only descriptively fluent but also numerically grounded and
spatially aware. We will open-source the dataset and code upon paper acceptance
to foster further research.

</details>


### [251] [MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction](https://arxiv.org/abs/2508.06859)
*Shuo Tang,Jian Xu,Jiadong Zhang,Yi Chen,Qizhao Jin,Lingdong Shen,Chenglin Liu,Shiming Xiang*

Main category: cs.AI

TL;DR: 论文提出了一种基于AI的端到端天气预警系统，通过MP-Bench数据集和MMLM模型解决了现有技术中的三大挑战，实现了高效的恶劣天气预测。


<details>
  <summary>Details</summary>
Motivation: 当前天气预警系统依赖人工专家解读，存在主观性和操作负担。AI技术的发展为自动化天气预测提供了新机遇，但面临数据稀缺、高维数据对齐困难以及现有模型处理能力不足等挑战。

Method: 构建了MP-Bench数据集（421,363对气象数据与文本标注），并开发了MMLM模型，该模型直接处理4D气象数据，通过三个自适应融合模块动态提取和整合特征。

Result: 实验表明MMLM在多个任务中表现优异，验证了其在恶劣天气理解和自动化预测中的有效性。

Conclusion: MMLM和MP-Bench为AI驱动的自动化天气预警系统迈出了关键一步，代码和数据集将公开。

Abstract: Timely and accurate severe weather warnings are critical for disaster
mitigation. However, current forecasting systems remain heavily reliant on
manual expert interpretation, introducing subjectivity and significant
operational burdens. With the rapid development of AI technologies, the
end-to-end "AI weather station" is gradually emerging as a new trend in
predicting severe weather events. Three core challenges impede the development
of end-to-end AI severe weather system: (1) scarcity of severe weather event
samples; (2) imperfect alignment between high-dimensional meteorological data
and textual warnings; (3) existing multimodal language models are unable to
handle high-dimensional meteorological data and struggle to fully capture the
complex dependencies across temporal sequences, vertical pressure levels, and
spatial dimensions. To address these challenges, we introduce MP-Bench, the
first large-scale temporal multimodal dataset for severe weather events
prediction, comprising 421,363 pairs of raw multi-year meteorological data and
corresponding text caption, covering a wide range of severe weather scenarios
across China. On top of this dataset, we develop a meteorology multimodal large
model (MMLM) that directly ingests 4D meteorological inputs. In addition, it is
designed to accommodate the unique characteristics of 4D meteorological data
flow, incorporating three plug-and-play adaptive fusion modules that enable
dynamic feature extraction and integration across temporal sequences, vertical
pressure layers, and spatial dimensions. Extensive experiments on MP-Bench
demonstrate that MMLM performs exceptionally well across multiple tasks,
highlighting its effectiveness in severe weather understanding and marking a
key step toward realizing automated, AI-driven weather forecasting systems. Our
source code and dataset will be made publicly available.

</details>


### [252] [Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents](https://arxiv.org/abs/2508.07642)
*Tianyi Ma,Yue Zhang,Zehao Wang,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: SkillNav是一个模块化框架，通过引入基于技能的结构化推理，提升了Transformer-based VLN代理的性能，实现了在R2R和GSA-R2R基准测试中的最佳表现。


<details>
  <summary>Details</summary>
Motivation: 当前VLN方法在复杂空间和时间推理的未见场景中泛化能力不足，需要更结构化的解决方案。

Method: SkillNav将导航分解为可解释的原子技能，每个技能由专门代理处理，并使用VLM-based路由器动态选择代理。

Result: 在R2R和GSA-R2R基准测试中达到新最佳性能，展示了强大的泛化能力。

Conclusion: SkillNav通过模块化和技能分解显著提升了VLN代理的性能和泛化能力。

Abstract: Vision-and-Language Navigation (VLN) poses significant challenges in enabling
agents to interpret natural language instructions and navigate complex 3D
environments. While recent progress has been driven by large-scale pre-training
and data augmentation, current methods still struggle to generalize to unseen
scenarios, particularly when complex spatial and temporal reasoning is
required. In this work, we propose SkillNav, a modular framework that
introduces structured, skill-based reasoning into Transformer-based VLN agents.
Our method decomposes navigation into a set of interpretable atomic skills
(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each
handled by a specialized agent. We then introduce a novel zero-shot
Vision-Language Model (VLM)-based router, which dynamically selects the most
suitable agent at each time step by aligning sub-goals with visual observations
and historical actions. SkillNav achieves a new state-of-the-art performance on
the R2R benchmark and demonstrates strong generalization to the GSA-R2R
benchmark that includes novel instruction styles and unseen environments.

</details>


### [253] [FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis](https://arxiv.org/abs/2508.07950)
*Chen Shen,Wanqing Zhang,Kehan Li,Erwen Huang,Haitao Bi,Aiying Fan,Yiwen Shen,Hongmei Dong,Ji Zhang,Yuming Shao,Zengjia Liu,Xinshe Liu,Tao Li,Chunxia Yan,Shuanliang Fan,Di Wu,Jianhua Ma,Bin Cong,Zhenyuan Wang,Chunfeng Lian*

Main category: cs.AI

TL;DR: FEAT是一个多智能体AI框架，用于自动化和标准化法医死因调查，通过领域适应的大语言模型提升效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决法医死因鉴定中的劳动力短缺和诊断差异问题，特别是在中国的高负荷法医体系中。

Method: FEAT结合任务分解、证据分析、迭代优化和结论合成的多智能体架构，采用工具增强推理、层次检索增强生成和专家反馈。

Result: FEAT在多样化的中国案例中表现优于现有AI系统，具有高专家一致性和地理泛化能力。

Conclusion: FEAT是首个基于LLM的法医AI系统，结合AI效率和人类监督，提升法医服务的可及性和可靠性。

Abstract: Forensic cause-of-death determination faces systemic challenges, including
workforce shortages and diagnostic variability, particularly in high-volume
systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic
AgenT), a multi-agent AI framework that automates and standardizes death
investigations through a domain-adapted large language model. FEAT's
application-oriented architecture integrates: (i) a central Planner for task
decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a
Memory & Reflection module for iterative refinement, and (iv) a Global Solver
for conclusion synthesis. The system employs tool-augmented reasoning,
hierarchical retrieval-augmented generation, forensic-tuned LLMs, and
human-in-the-loop feedback to ensure legal and medical validity. In evaluations
across diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI
systems in both long-form autopsy analyses and concise cause-of-death
conclusions. It demonstrated robust generalization across six geographic
regions and achieved high expert concordance in blinded validations. Senior
pathologists validated FEAT's outputs as comparable to those of human experts,
with improved detection of subtle evidentiary nuances. To our knowledge, FEAT
is the first LLM-based AI agent system dedicated to forensic medicine, offering
scalable, consistent death certification while maintaining expert-level rigor.
By integrating AI efficiency with human oversight, this work could advance
equitable access to reliable medicolegal services while addressing critical
capacity constraints in forensic systems.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [254] [Digital generation of the 3-D pore architecture of isotropic membranes using 2-D cross-sectional scanning electron microscopy images](https://arxiv.org/abs/2508.06664)
*Sima Zeinali Danalou,Hooman Chamani,Arash Rabbani,Patrick C. Lee,Jason Hattrick Simpers,Jay R Werber*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究改进了从单张2D SEM图像重建3D孔隙网络的方法，解决了传统3D断层扫描技术的高成本和复杂性，同时提高了对复杂孔隙形态的还原能力。


<details>
  <summary>Details</summary>
Motivation: 传统2D SEM无法解析3D孔隙结构和连通性，而现有3D重建技术成本高且复杂。研究旨在开发一种更高效、精确的方法。

Method: 开发了一种增强的重建算法，从单张2D SEM图像生成3D孔隙网络，保持关键统计特性并准确还原复杂孔隙形态。

Result: 应用于商业微滤膜，生成高保真3D重建，与X射线断层扫描数据验证显示结构指标高度一致，分辨率更高。

Conclusion: 该方法适用于各向同性多孔膜结构，未来需扩展至各向异性膜。

Abstract: A major limitation of two-dimensional scanning electron microscopy (SEM) in
imaging porous membranes is its inability to resolve three-dimensional pore
architecture and interconnectivity, which are critical factors governing
membrane performance. Although conventional tomographic 3-D reconstruction
techniques can address this limitation, they are often expensive, technically
challenging, and not widely accessible. We previously introduced a
proof-of-concept method for reconstructing a membrane's 3-D pore network from a
single 2-D SEM image, yielding statistically equivalent results to those
obtained from 3-D tomography. However, this initial approach struggled to
replicate the diverse pore geometries commonly observed in real membranes. In
this study, we advance the methodology by developing an enhanced reconstruction
algorithm that not only maintains essential statistical properties (e.g., pore
size distribution), but also accurately reproduces intricate pore morphologies.
Applying this technique to a commercial microfiltration membrane, we generated
a high-fidelity 3-D reconstruction and derived key membrane properties.
Validation with X-ray tomography data revealed excellent agreement in
structural metrics, with our SEM-based approach achieving superior resolution
in resolving fine pore features. The tool can be readily applied to isotropic
porous membrane structures of any pore size, as long as those pores can be
visualized by SEM. Further work is needed for 3-D structure generation of
anisotropic membranes.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [255] [Membership Inference Attacks with False Discovery Rate Control](https://arxiv.org/abs/2508.07066)
*Chenxu Zhao,Wei Qian,Aobo Chen,Mengdi Huai*

Main category: stat.ML

TL;DR: 提出了一种新型成员推理攻击方法，能够提供假发现率（FDR）保证，并可作为现有方法的包装器。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击方法缺乏对假发现率的保证，且分布未知和概率相互依赖增加了挑战。

Method: 设计了一种新型成员推理攻击方法，提供FDR保证，并可作为现有方法的包装器。

Result: 理论分析和多场景实验验证了方法的优越性能。

Conclusion: 该方法在保证FDR的同时，提升了成员推理攻击的实用性。

Abstract: Recent studies have shown that deep learning models are vulnerable to
membership inference attacks (MIAs), which aim to infer whether a data record
was used to train a target model or not. To analyze and study these
vulnerabilities, various MIA methods have been proposed. Despite the
significance and popularity of MIAs, existing works on MIAs are limited in
providing guarantees on the false discovery rate (FDR), which refers to the
expected proportion of false discoveries among the identified positive
discoveries. However, it is very challenging to ensure the false discovery rate
guarantees, because the underlying distribution is usually unknown, and the
estimated non-member probabilities often exhibit interdependence. To tackle the
above challenges, in this paper, we design a novel membership inference attack
method, which can provide the guarantees on the false discovery rate.
Additionally, we show that our method can also provide the marginal probability
guarantee on labeling true non-member data as member data. Notably, our method
can work as a wrapper that can be seamlessly integrated with existing MIA
methods in a post-hoc manner, while also providing the FDR control. We perform
the theoretical analysis for our method. Extensive experiments in various
settings (e.g., the black-box setting and the lifelong learning setting) are
also conducted to verify the desirable performance of our method.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [256] [Extracting Overlapping Microservices from Monolithic Code via Deep Semantic Embeddings and Graph Neural Network-Based Soft Clustering](https://arxiv.org/abs/2508.07486)
*Morteza Ziabakhsh,Kiyan Rezaee,Sadegh Eskandari,Seyed Amir Hossein Tabatabaei,Mohammad M. Ghassemi*

Main category: cs.SE

TL;DR: Mo2oM框架通过软聚类方法将单体应用分解为微服务，显著提升了结构模块化和通信效率。


<details>
  <summary>Details</summary>
Motivation: 现有微服务提取方法采用硬聚类，导致服务间耦合高、内聚低，Mo2oM通过软聚类模拟专家分解方式，优化这一问题。

Method: 结合深度语义嵌入和结构依赖（方法调用图），使用图神经网络进行软聚类，生成微服务。

Result: 在四个开源基准测试中，Mo2oM在结构模块化、通信开销等方面显著优于现有方法。

Conclusion: Mo2oM通过软聚类有效平衡了微服务的内聚与耦合，提升了整体性能。

Abstract: Modern software systems are increasingly shifting from monolithic
architectures to microservices to enhance scalability, maintainability, and
deployment flexibility. Existing microservice extraction methods typically rely
on hard clustering, assigning each software component to a single microservice.
This approach often increases inter-service coupling and reduces intra-service
cohesion. We propose Mo2oM (Monolithic to Overlapping Microservices), a
framework that formulates microservice extraction as a soft clustering problem,
allowing components to belong probabilistically to multiple microservices. This
approach is inspired by expert-driven decompositions, where practitioners
intentionally replicate certain software components across services to reduce
communication overhead. Mo2oM combines deep semantic embeddings with structural
dependencies extracted from methodcall graphs to capture both functional and
architectural relationships. A graph neural network-based soft clustering
algorithm then generates the final set of microservices. We evaluate Mo2oM on
four open-source monolithic benchmarks and compare it against eight
state-of-the-art baselines. Our results demonstrate that Mo2oM achieves
improvements of up to 40.97% in structural modularity (balancing cohesion and
coupling), 58% in inter-service call percentage (communication overhead),
26.16% in interface number (modularity and decoupling), and 38.96% in
non-extreme distribution (service size balance) across all benchmarks.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [257] [Detecting Early Kidney Allograft Fibrosis with Multi-b-value Spectral Diffusion MRI](https://arxiv.org/abs/2508.06644)
*Mira M. Liu,Jonathan Dyke,Thomas Gladytz,Jonas Jasse,Ian Bolger,Sergio Calle,Swathi Pavuluri,Tanner Crews,Surya Seshan,Steven Salvatore,Isaac Stillman,Thangamani Muthukumar,Bachir Taouli,Samira Farouk,Octavia Bane,Sara Lewis*

Main category: physics.med-ph

TL;DR: 研究评估了光谱扩散MRI在检测肾移植早期纤维化中的作用，发现其能有效识别轻度/中度纤维化，甚至在肾功能正常时也能检测到纤维化。


<details>
  <summary>Details</summary>
Motivation: 肾移植纤维化是慢性肾病（CKD）的标志，预测功能下降和移植失败。研究旨在探索光谱扩散MRI是否能早期检测纤维化。

Method: 通过多b值DWI和光谱扩散分析，结合IVIM和ADC，评估成像参数与生物过程的关联，并进行诊断能力分析。

Result: 光谱扩散MRI能检测轻度/中度纤维化（IFTA=2-4）和纤维化（IFTA>0），即使在肾功能正常时（eGFR>45ml/min/1.73m2）。

Conclusion: 光谱扩散MRI是一种有前景的方法，可在功能下降前早期检测纤维化和CKD。

Abstract: Kidney allograft fibrosis is a marker of chronic kidney disease (CKD) and
predicts functional decline, and eventual allograft failure. This study
evaluates if spectral diffusion MRI can help detect early development and
mild/moderate fibrosis in kidney allografts. In a prospective two-center study
of kidney allografts, interstitial fibrosis and tubular atrophy (IFTA) was
scored and eGFR was calculated from serum creatinine. Multi-b-value DWI
(bvalues=[0,10,30,50,80,120,200,400,800mm2/s]) was post-processed with spectral
diffusion, intravoxel incoherent motion (IVIM), and apparent diffusion
coefficient (ADC). Connection between imaging parameters and biological
processes was measured by Mann-Whitney U-test and Spearman's rank; diagnostic
ability was measured by five-fold cross-validation univariate and multi-variate
logistic regression. Quality control analyses included volunteer MRI (n=4) and
inter-observer analysis (n=19). 99 patients were included (50$\pm$13yo,
64M/35F, 39 IFTA=0, 22 IFTA=2, 20 IFTA=4, 18 IFTA=6, 46 eGFR<=45mL/min/1.73m2,
mean eGFR=47.5$\pm$21.3mL/min/1.73m2). Spectral diffusion detected fibrosis
(IFTA>0) in patients with normal/stable eGFR>45ml/min/1.73m2
[AUC(95$\%$CI)=0.72(0.56,0.87),p=0.007]. Spectral diffusion detected
mild/moderate fibrosis (IFTA=2-4) [AUC(95$\%$CI)=0.65(0.52,0.71),p=0.023], as
did ADC [AUC(95$\%$CI)=0.71(0.54,0.87),p=0.013)]. eGFR, time-from-transplant,
and allograft size could not. Interobserver correlation was >0.50 in 24 out of
40 diffusion parameters. Spectral diffusion MRI showed detection of
mild/moderate fibrosis and fibrosis before decline in function. It is a
promising method to detect early development of fibrosis and CKD before
progression.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [258] [AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance](https://arxiv.org/abs/2508.06944)
*Lixuan He,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为AMFT的单阶段算法，通过隐式奖励理论动态平衡监督微调（SFT）和强化学习（RL），以解决传统两阶段方法中的灾难性遗忘和探索-模仿权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段方法（SFT+RL）存在灾难性遗忘和探索-模仿权衡问题，而现有单阶段方法缺乏动态平衡机制。

Method: 提出AMFT算法，利用元梯度自适应权重控制器动态优化SFT和RL的平衡，并通过策略熵正则化确保稳定性。

Result: 在数学推理、抽象视觉推理和视觉语言导航等任务上，AMFT实现了新的SOTA，并在OOD任务上表现出优越的泛化能力。

Conclusion: AMFT通过元学习控制器提供了一种更原则性和有效的LLM对齐范式，显著提升了稳定性和性能。

Abstract: Large Language Models (LLMs) are typically fine-tuned for reasoning tasks
through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by
Reinforcement Learning (RL), a process fraught with catastrophic forgetting and
suboptimal trade-offs between imitation and exploration. Recent single-stage
methods attempt to unify SFT and RL using heuristics, but lack a principled
mechanism for dynamically balancing the two paradigms. In this paper, we
reframe this challenge through the theoretical lens of \textbf{implicit
rewards}, viewing SFT and RL not as distinct methods but as complementary
reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel
single-stage algorithm that learns the optimal balance between SFT's implicit,
path-level reward and RL's explicit, outcome-based reward. The core of AMFT is
a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL
balance as a learnable parameter, dynamically optimizing it to maximize
long-term task performance. This forward-looking approach, regularized by
policy entropy for stability, autonomously discovers an effective training
curriculum. We conduct a comprehensive evaluation on challenging benchmarks
spanning mathematical reasoning, abstract visual reasoning (General Points),
and vision-language navigation (V-IRL). AMFT consistently establishes a new
state-of-the-art and demonstrats superior generalization on out-of-distribution
(OOD) tasks. Ablation studies and training dynamic analysis confirm that the
meta-learning controller is crucial for AMFT's stability, sample efficiency,
and performance, offering a more principled and effective paradigm for LLM
alignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.

</details>


### [259] [Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria](https://arxiv.org/abs/2508.07102)
*Yang Cao,Yubin Chen,Zhao Song,Jiahao Zhang*

Main category: cs.LG

TL;DR: 论文提出了Second-Order MeanFlow，一种将平均加速度场引入MeanFlow目标的新方法，证明了其可行性和高效性，并展示了其在计算复杂度上的优势。


<details>
  <summary>Details</summary>
Motivation: 扩展MeanFlow框架，通过引入高阶动态（平均加速度）来提升生成模型的表达能力和采样效率。

Method: 提出Second-Order MeanFlow，证明其满足一致性条件，并通过电路复杂度分析和注意力计算近似实现高效采样。

Result: 证明了Second-Order MeanFlow的可行性和高效性，展示了其在TC0类电路中的实现能力，以及快速注意力计算的近似方法。

Conclusion: 为高阶流匹配模型奠定了理论基础，结合了丰富的动态特性和实际采样效率。

Abstract: Generative modelling has seen significant advances through simulation-free
paradigms such as Flow Matching, and in particular, the MeanFlow framework,
which replaces instantaneous velocity fields with average velocities to enable
efficient single-step sampling. In this work, we introduce a theoretical study
on Second-Order MeanFlow, a novel extension that incorporates average
acceleration fields into the MeanFlow objective. We first establish the
feasibility of our approach by proving that the average acceleration satisfies
a generalized consistency condition analogous to first-order MeanFlow, thereby
supporting stable, one-step sampling and tractable loss functions. We then
characterize its expressivity via circuit complexity analysis, showing that
under mild assumptions, the Second-Order MeanFlow sampling process can be
implemented by uniform threshold circuits within the $\mathsf{TC}^0$ class.
Finally, we derive provably efficient criteria for scalable implementation by
leveraging fast approximate attention computations: we prove that attention
operations within the Second-Order MeanFlow architecture can be approximated to
within $1/\mathrm{poly}(n)$ error in time $n^{2+o(1)}$. Together, these results
lay the theoretical foundation for high-order flow matching models that combine
rich dynamics with practical sampling efficiency.

</details>


### [260] [Vision-Based Localization and LLM-based Navigation for Indoor Environments](https://arxiv.org/abs/2508.08120)
*Keyan Rahimi,Md. Wasiul Haque,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: 该研究提出了一种结合视觉定位和大型语言模型（LLM）导航的室内导航方法，定位准确率达96%，导航指令准确率为75%。


<details>
  <summary>Details</summary>
Motivation: 解决室内环境中GPS信号不可靠和建筑结构复杂的问题。

Method: 使用ResNet-50卷积神经网络进行视觉定位，结合LLM生成导航指令。

Result: 定位准确率96%，导航指令准确率75%。

Conclusion: 展示了利用普通摄像头和公开平面图实现可扩展、无需基础设施的室内导航潜力。

Abstract: Indoor navigation remains a complex challenge due to the absence of reliable
GPS signals and the architectural intricacies of large enclosed environments.
This study presents an indoor localization and navigation approach that
integrates vision-based localization with large language model (LLM)-based
navigation. The localization system utilizes a ResNet-50 convolutional neural
network fine-tuned through a two-stage process to identify the user's position
using smartphone camera input. To complement localization, the navigation
module employs an LLM, guided by a carefully crafted system prompt, to
interpret preprocessed floor plan images and generate step-by-step directions.
Experimental evaluation was conducted in a realistic office corridor with
repetitive features and limited visibility to test localization robustness. The
model achieved high confidence and an accuracy of 96% across all tested
waypoints, even under constrained viewing conditions and short-duration
queries. Navigation tests using ChatGPT on real building floor maps yielded an
average instruction accuracy of 75%, with observed limitations in zero-shot
reasoning and inference time. This research demonstrates the potential for
scalable, infrastructure-free indoor navigation using off-the-shelf cameras and
publicly available floor plans, particularly in resource-constrained settings
like hospitals, airports, and educational institutions.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [261] [Sensory robustness through top-down feedback and neural stochasticity in recurrent vision models](https://arxiv.org/abs/2508.07115)
*Antonino Greco,Marco D'Alessandro,Karl J. Friston,Giovanni Pezzulo,Markus Siegel*

Main category: q-bio.NC

TL;DR: 研究通过训练卷积循环神经网络（ConvRNN）探究了自上而下反馈在视觉处理中的作用，发现结合随机神经变异性的反馈能显著提升模型的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 探索自上而下反馈在人工视觉模型中的功能意义，尤其是在分类任务中的具体贡献。

Method: 训练带有或不带自上而下反馈的ConvRNN，并引入随机神经变异性（如dropout）来模拟生物系统的随机性。

Result: 带反馈的ConvRNN在速度-准确性权衡、噪声和对抗攻击鲁棒性方面表现更优，且反馈信息显著影响了表示几何和网络活动。

Conclusion: 自上而下反馈与神经随机性共同作用，通过稳定网络活动和高效编码信息，揭示了双机制对鲁棒感知编码的贡献。

Abstract: Biological systems leverage top-down feedback for visual processing, yet most
artificial vision models succeed in image classification using purely
feedforward or recurrent architectures, calling into question the functional
significance of descending cortical pathways. Here, we trained convolutional
recurrent neural networks (ConvRNN) on image classification in the presence or
absence of top-down feedback projections to elucidate the specific
computational contributions of those feedback pathways. We found that ConvRNNs
with top-down feedback exhibited remarkable speed-accuracy trade-off and
robustness to noise perturbations and adversarial attacks, but only when they
were trained with stochastic neural variability, simulated by randomly
silencing single units via dropout. By performing detailed analyses to identify
the reasons for such benefits, we observed that feedback information
substantially shaped the representational geometry of the post-integration
layer, combining the bottom-up and top-down streams, and this effect was
amplified by dropout. Moreover, feedback signals coupled with dropout optimally
constrained network activity onto a low-dimensional manifold and encoded object
information more efficiently in out-of-distribution regimes, with top-down
information stabilizing the representational dynamics at the population level.
Together, these findings uncover a dual mechanism for resilient sensory coding.
On the one hand, neural stochasticity prevents unit-level co-adaptation albeit
at the cost of more chaotic dynamics. On the other hand, top-down feedback
harnesses high-level information to stabilize network activity on compact
low-dimensional manifolds.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [262] [Visualization Vibes: The Socio-Indexical Function of Visualization Design](https://arxiv.org/abs/2508.06775)
*Michelle Morgenstern,Amy Rae Fox,Graham M. Jones,Arvind Satyanarayan*

Main category: cs.HC

TL;DR: 论文探讨了数据可视化不仅传达数据意义，还传递社会意义，影响公众对可视化的接受度。


<details>
  <summary>Details</summary>
Motivation: 在信息生态中，公众对科学的不信任和错误信息的泛滥导致数据传播面临挑战，传统可视化研究未能全面解释人们如何与可视化互动。

Method: 结合语言人类学理论，通过民族志访谈，分析读者如何通过可视化设计推断其社会背景。

Result: 研究发现可视化设计特征引发社会推断，影响读者对可视化的接受度，揭示了社会索引性的重要性。

Conclusion: 论文为公共数据传播提供了新的理论基础和实践干预，强调了社会索引性在可视化中的作用。

Abstract: In contemporary information ecologies saturated with misinformation,
disinformation, and a distrust of science itself, public data communication
faces significant hurdles. Although visualization research has broadened
criteria for effective design, governing paradigms privilege the accurate and
efficient transmission of data. Drawing on theory from linguistic anthropology,
we argue that such approaches-focused on encoding and decoding propositional
content-cannot fully account for how people engage with visualizations and why
particular visualizations might invite adversarial or receptive responses. In
this paper, we present evidence that data visualizations communicate not only
semantic, propositional meaning$\unicode{x2013}$meaning about
data$\unicode{x2013}$but also social, indexical meaning$\unicode{x2013}$meaning
beyond data. From a series of ethnographically-informed interviews, we document
how readers make rich and varied assessments of a visualization's
"vibes"$\unicode{x2013}$inferences about the social provenance of a
visualization based on its design features. Furthermore, these social
attributions have the power to influence reception, as readers' decisions about
how to engage with a visualization concern not only content, or even aesthetic
appeal, but also their sense of alignment or disalignment with the entities
they imagine to be involved in its production and circulation. We argue these
inferences hinge on a function of human sign systems that has thus far been
little studied in data visualization: socio-indexicality, whereby the formal
features (rather than the content) of communication evoke social contexts,
identities, and characteristics. Demonstrating the presence and significance of
this socio-indexical function in visualization, this paper offers both a
conceptual foundation and practical intervention for troubleshooting breakdowns
in public data communication.

</details>


### [263] [Quantifying Visualization Vibes: Measuring Socio-Indexicality at Scale](https://arxiv.org/abs/2508.06786)
*Amy Rae Fox,Michelle Morgenstern,Graham M. Jones,Arvind Satyanarayan*

Main category: cs.HC

TL;DR: 论文探讨了可视化如何传达超出数据本身的社会信息，提出了分析框架并通过调查验证了社会推断的普遍性和对信任的影响。


<details>
  <summary>Details</summary>
Motivation: 研究可视化如何通过设计特征和社会背景传达超出数据的信息，以应对公共数据传播中的挑战。

Method: 采用归因启发调查，分析社会推断的普遍性及其对信任的影响。

Result: 社会推断可异步研究，不受特定文化或数据素养限制，且设计特征与数据主题共同影响解读。

Conclusion: 扩展可视化研究中的人文因素至社会文化现象，可为公共数据传播提供实用设计建议。

Abstract: What impressions might readers form with visualizations that go beyond the
data they encode? In this paper, we build on recent work that demonstrates the
socio-indexical function of visualization, showing that visualizations
communicate more than the data they explicitly encode. Bridging this with prior
work examining public discourse about visualizations, we contribute an analytic
framework for describing inferences about an artifact's social provenance. Via
a series of attribution-elicitation surveys, we offer descriptive evidence that
these social inferences: (1) can be studied asynchronously, (2) are not unique
to a particular sociocultural group or a function of limited data literacy, and
(3) may influence assessments of trust. Further, we demonstrate (4) how design
features act in concert with the topic and underlying messages of an artifact's
data to give rise to such 'beyond-data' readings. We conclude by discussing the
design and research implications of inferences about social provenance, and why
we believe broadening the scope of research on human factors in visualization
to include sociocultural phenomena can yield actionable design recommendations
to address urgent challenges in public data communication.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [264] [MSPT: A Lightweight Face Image Quality Assessment Method with Multi-stage Progressive Training](https://arxiv.org/abs/2508.07590)
*Xiongwei Xiao,Baoying Chen,Jishen Zeng,Jianquan Yang*

Main category: cs.MM

TL;DR: 提出了一种轻量级人脸质量评估网络（MSPT），通过多阶段渐进训练策略，在保持高效推理的同时达到高性能。


<details>
  <summary>Details</summary>
Motivation: 传统人脸质量评估方法泛化能力有限，而学习型方法虽性能优越但计算和存储成本高，难以实际部署。

Method: 采用三阶段渐进训练策略，逐步引入多样化数据样本并提高输入图像分辨率。

Result: 在VQualA 2025基准数据集上取得第二高分，性能与现有最优方法相当或更好。

Conclusion: MSPT在轻量级网络中实现了高性能，有效解决了复杂特征学习和灾难性遗忘问题。

Abstract: Accurately assessing the perceptual quality of face images is crucial,
especially with the rapid progress in face restoration and generation.
Traditional quality assessment methods often struggle with the unique
characteristics of face images, limiting their generalizability. While
learning-based approaches demonstrate superior performance due to their strong
fitting capabilities, their high complexity typically incurs significant
computational and storage costs, hindering practical deployment. To address
this, we propose a lightweight face quality assessment network with Multi-Stage
Progressive Training (MSPT). Our network employs a three-stage progressive
training strategy that gradually introduces more diverse data samples and
increases input image resolution. This novel approach enables lightweight
networks to achieve high performance by effectively learning complex quality
features while significantly mitigating catastrophic forgetting. Our MSPT
achieved the second highest score on the VQualA 2025 face image quality
assessment benchmark dataset, demonstrating that MSPT achieves comparable or
better performance than state-of-the-art methods while maintaining efficient
inference.

</details>


### [265] [AD-AVSR: Asymmetric Dual-stream Enhancement for Robust Audio-Visual Speech Recognition](https://arxiv.org/abs/2508.07608)
*Junxiao Xue,Xiaozhen Liu,Xuecheng Wu,Xinyi Yin,Danlei Huang,Fei Yu*

Main category: cs.MM

TL;DR: 论文提出了一种基于双向模态增强的新AVSR框架AD-AVSR，通过音频双流编码策略和跨模态噪声抑制掩码模块，显著提升了在噪声环境下的语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有AVSR方法多为单向增强或对称融合，难以捕捉音频-视觉数据的异质性和互补性，尤其是在信息不对称条件下。

Method: 采用音频双流编码策略和双向模态增强，包括音频感知视觉细化模块和跨模态噪声抑制掩码模块，并引入阈值选择机制过滤无关信息。

Result: 在LRS2和LRS3数据集上的实验表明，AD-AVSR在性能和噪声鲁棒性上均优于现有方法。

Conclusion: AD-AVSR通过双向信息流和选择性过滤机制，有效提升了音频-视觉语音识别的性能。

Abstract: Audio-visual speech recognition (AVSR) combines audio-visual modalities to
improve speech recognition, especially in noisy environments. However, most
existing methods deploy the unidirectional enhancement or symmetric fusion
manner, which limits their capability to capture heterogeneous and
complementary correlations of audio-visual data-especially under asymmetric
information conditions. To tackle these gaps, we introduce a new AVSR framework
termed AD-AVSR based on bidirectional modality enhancement. Specifically, we
first introduce the audio dual-stream encoding strategy to enrich audio
representations from multiple perspectives and intentionally establish
asymmetry to support subsequent cross-modal interactions. The enhancement
process involves two key components, Audio-aware Visual Refinement Module for
enhanced visual representations under audio guidance, and Cross-modal Noise
Suppression Masking Module which refines audio representations using visual
cues, collaboratively leading to the closed-loop and bidirectional information
flow. To further enhance correlation robustness, we adopt a threshold-based
selection mechanism to filter out irrelevant or weakly correlated audio-visual
pairs. Extensive experimental results on the LRS2 and LRS3 datasets indicate
that our AD-AVSR consistently surpasses SOTA methods in both performance and
noise robustness, highlighting the effectiveness of our model design.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [266] [Codebook-enabled Generative End-to-end Semantic Communication Powered by Transformer](https://arxiv.org/abs/2402.16868)
*Peigen Ye,Yaping Sun,Shumin Yao,Hao Chen,Xiaodong Xu,Shuguang Cui*

Main category: cs.IT

TL;DR: 论文提出了一种基于码本的鲁棒图像语义通信系统，通过联合构建语义编解码器和码本，并设计向量到索引的转换器，以消除信道噪声影响，提升生成图像质量。


<details>
  <summary>Details</summary>
Motivation: 码本生成语义通信系统易受信道噪声影响，需提升系统鲁棒性。

Method: 联合构建语义编解码器和码本，设计向量到索引转换器以消除噪声影响。

Result: 生成的图像在视觉感知上优于JPEG+LDPC和传统JSCC方法。

Conclusion: 该方法在生成图像质量和抗噪声性能上优于现有技术。

Abstract: Codebook-based generative semantic communication attracts increasing
attention, since only indices are required to be transmitted when the codebook
is shared between transmitter and receiver. However, due to the fact that the
semantic relations among code vectors are not necessarily related to the
distance of the corresponding code indices, the performance of the
codebook-enabled semantic communication system is susceptible to the channel
noise. Thus, how to improve the system robustness against the noise requires
careful design. This paper proposes a robust codebook-assisted image semantic
communication system, where semantic codec and codebook are first jointly
constructed, and then vector-to-index transformer is designed guided by the
codebook to eliminate the effects of channel noise, and achieve image
generation. Thanks to the assistance of the high-quality codebook to the
Transformer, the generated images at the receiver outperform those of the
compared methods in terms of visual perception. In the end, numerical results
and generated images demonstrate the advantages of the generative semantic
communication method over JPEG+LDPC and traditional joint source channel coding
(JSCC) methods.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [267] [KLASSify to Verify: Audio-Visual Deepfake Detection Using SSL-based Audio and Handcrafted Visual Features](https://arxiv.org/abs/2508.07337)
*Ivan Kukanov,Jun Wah Ng*

Main category: eess.AS

TL;DR: 论文提出了一种多模态方法用于检测和定位深度伪造内容，结合视觉和音频模态，提高了检测的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着音频驱动的人脸生成器和TTS模型的快速发展，深度伪造技术日益复杂，需要更强大的检测方法，尤其是针对未见过的攻击场景。

Method: 视觉模态采用手工特征以提高可解释性和适应性；音频模态结合自监督学习骨干和图形注意力网络，捕捉丰富的音频表征。

Result: 在AV-Deepfake1M++数据集上，多模态系统在深度伪造分类任务中AUC达到92.78%，仅使用音频模态时时间定位IoU为0.3536。

Conclusion: 该方法在性能和实际部署之间取得了平衡，注重鲁棒性和潜在的可解释性。

Abstract: The rapid development of audio-driven talking head generators and advanced
Text-To-Speech (TTS) models has led to more sophisticated temporal deepfakes.
These advances highlight the need for robust methods capable of detecting and
localizing deepfakes, even under novel, unseen attack scenarios. Current
state-of-the-art deepfake detectors, while accurate, are often computationally
expensive and struggle to generalize to novel manipulation techniques. To
address these challenges, we propose multimodal approaches for the
AV-Deepfake1M 2025 challenge. For the visual modality, we leverage handcrafted
features to improve interpretability and adaptability. For the audio modality,
we adapt a self-supervised learning (SSL) backbone coupled with graph attention
networks to capture rich audio representations, improving detection robustness.
Our approach strikes a balance between performance and real-world deployment,
focusing on resilience and potential interpretability. On the AV-Deepfake1M++
dataset, our multimodal system achieves AUC of 92.78% for deepfake
classification task and IoU of 0.3536 for temporal localization using only the
audio modality.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [268] [Fading the Digital Ink: A Universal Black-Box Attack Framework for 3DGS Watermarking Systems](https://arxiv.org/abs/2508.07263)
*Qingyuan Zeng,Shu Jiang,Jiajing Lin,Zhenzhong Wang,Kay Chen Tan,Min Jiang*

Main category: cs.CR

TL;DR: 本文提出了一种名为GMEA的通用黑盒攻击框架，用于挑战3D高斯泼溅（3DGS）中的数字水印技术，揭示了现有版权保护方案的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着3DGS的兴起，数字水印技术被用于版权保护，但其对抗攻击的鲁棒性尚未充分研究。

Method: 通过将攻击建模为大规模多目标优化问题，提出了一种基于组的进化攻击策略，最小化卷积网络提取特征的标准差以干扰水印检测。

Result: 实验表明，GMEA能有效移除主流3DGS水印方法中的1D和2D水印，同时保持高视觉保真度。

Conclusion: 该研究揭示了现有3DGS水印系统的关键漏洞，呼吁开发更鲁棒的水印技术。

Abstract: With the rise of 3D Gaussian Splatting (3DGS), a variety of digital
watermarking techniques, embedding either 1D bitstreams or 2D images, are used
for copyright protection. However, the robustness of these watermarking
techniques against potential attacks remains underexplored. This paper
introduces the first universal black-box attack framework, the Group-based
Multi-objective Evolutionary Attack (GMEA), designed to challenge these
watermarking systems. We formulate the attack as a large-scale multi-objective
optimization problem, balancing watermark removal with visual quality. In a
black-box setting, we introduce an indirect objective function that blinds the
watermark detector by minimizing the standard deviation of features extracted
by a convolutional network, thus rendering the feature maps uninformative. To
manage the vast search space of 3DGS models, we employ a group-based
optimization strategy to partition the model into multiple, independent
sub-optimization problems. Experiments demonstrate that our framework
effectively removes both 1D and 2D watermarks from mainstream 3DGS watermarking
methods while maintaining high visual fidelity. This work reveals critical
vulnerabilities in existing 3DGS copyright protection schemes and calls for the
development of more robust watermarking systems.

</details>


### [269] [IPBA: Imperceptible Perturbation Backdoor Attack in Federated Self-Supervised Learning](https://arxiv.org/abs/2508.08031)
*Jiayao Wang,Yang Song,Zhendong Zhao,Jiale Zhang,Qilin Wu,Junwu Zhu,Dongfang Zhao*

Main category: cs.CR

TL;DR: 本文提出了一种针对联邦自监督学习（FSSL）的隐蔽且有效的后门攻击方法IPBA，解决了现有方法在隐蔽性和实用性上的不足。


<details>
  <summary>Details</summary>
Motivation: FSSL结合了去中心化建模和无监督表示学习的优势，但研究表明其易受后门攻击，且现有方法隐蔽性不足。

Method: IPBA通过解耦后门样本和增强样本的特征分布，并引入Sliced-Wasserstein距离优化触发生成过程。

Result: 实验表明，IPBA在多个FSSL场景和数据集中显著优于现有后门攻击方法，且对各种防御机制具有强鲁棒性。

Conclusion: IPBA为FSSL中的后门攻击提供了更隐蔽和有效的解决方案。

Abstract: Federated self-supervised learning (FSSL) combines the advantages of
decentralized modeling and unlabeled representation learning, serving as a
cutting-edge paradigm with strong potential for scalability and privacy
preservation. Although FSSL has garnered increasing attention, research
indicates that it remains vulnerable to backdoor attacks. Existing methods
generally rely on visually obvious triggers, which makes it difficult to meet
the requirements for stealth and practicality in real-world deployment. In this
paper, we propose an imperceptible and effective backdoor attack method against
FSSL, called IPBA. Our empirical study reveals that existing imperceptible
triggers face a series of challenges in FSSL, particularly limited
transferability, feature entanglement with augmented samples, and
out-of-distribution properties. These issues collectively undermine the
effectiveness and stealthiness of traditional backdoor attacks in FSSL. To
overcome these challenges, IPBA decouples the feature distributions of backdoor
and augmented samples, and introduces Sliced-Wasserstein distance to mitigate
the out-of-distribution properties of backdoor samples, thereby optimizing the
trigger generation process. Our experimental results on several FSSL scenarios
and datasets show that IPBA significantly outperforms existing backdoor attack
methods in performance and exhibits strong robustness under various defense
mechanisms.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [270] [InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information](https://arxiv.org/abs/2508.07630)
*Anirudh Iyengar Kaniyar Narayana Iyengar,Srija Mukhopadhyay,Adnan Qidwai,Shubhankar Singh,Dan Roth,Vivek Gupta*

Main category: cs.CL

TL;DR: InterChart是一个诊断基准，用于评估视觉语言模型（VLMs）在多个相关图表中的推理能力，揭示了模型在图表复杂性增加时准确率显著下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准多关注孤立、视觉单一的图表，而InterChart旨在评估模型在科学报告、金融分析和政策仪表板等实际应用中的多图表推理能力。

Method: InterChart分为三个难度层级：单图表事实推理、合成对齐图表集的综合分析以及真实复杂图表对的语义推理。

Result: 评估显示，随着图表复杂性增加，VLMs的准确率显著下降，分解多实体图表可提升模型表现。

Conclusion: InterChart为复杂多视觉环境中的多模态推理提供了严格框架，揭示了模型的系统性局限。

Abstract: We introduce InterChart, a diagnostic benchmark that evaluates how well
vision-language models (VLMs) reason across multiple related charts, a task
central to real-world applications such as scientific reporting, financial
analysis, and public policy dashboards. Unlike prior benchmarks focusing on
isolated, visually uniform charts, InterChart challenges models with diverse
question types ranging from entity inference and trend correlation to numerical
estimation and abstract multi-step reasoning grounded in 2-3 thematically or
structurally related charts. We organize the benchmark into three tiers of
increasing difficulty: (1) factual reasoning over individual charts, (2)
integrative analysis across synthetically aligned chart sets, and (3) semantic
inference over visually complex, real-world chart pairs. Our evaluation of
state-of-the-art open and closed-source VLMs reveals consistent and steep
accuracy declines as chart complexity increases. We find that models perform
better when we decompose multi-entity charts into simpler visual units,
underscoring their struggles with cross-chart integration. By exposing these
systematic limitations, InterChart provides a rigorous framework for advancing
multimodal reasoning in complex, multi-visual environments.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [271] [High-background X-ray single particle imaging enabled by holographic enhancement with 2D crystals](https://arxiv.org/abs/2508.07953)
*Abhishek Mall,Zhou Shen,Kartik Ayyer*

Main category: physics.optics

TL;DR: 该论文提出了一种基于全息增强的改进X射线单粒子成像技术，通过强散射二维晶体晶格提高分辨率，适用于高背景散射环境。


<details>
  <summary>Details</summary>
Motivation: 当前X射线自由电子激光（XFEL）的单粒子成像技术因背景散射限制分辨率，且同步辐射源难以检测对象。

Method: 利用强散射二维晶体晶格的全息增强技术，结合定制重建算法恢复潜在参数。

Result: 该方法可在背景信号高达对象信号10^5倍的情况下实现结构恢复，提高分辨率和技术可及性。

Conclusion: 该技术有望扩展单粒子成像的应用范围，支持高分辨率成像和固定靶样品递送。

Abstract: X-ray single particle imaging (SPI) has offered the potential to visualize
structures of biomolecules at near-atomic resolution. However, state-of-the-art
structures at X-ray free electron lasers (XFELs) are limited to moderate
resolution, primarily due to background scattering. We computationally explore
a modified SPI technique based on holographic enhancement from a strongly
scattering 2D crystal lattice placed near the object. The Bragg peaks from the
crystal enable structure retrieval even for background levels up to 10$^{5}$
times higher than the object signal. This method could enable SPI at more
widely accessible synchrotron sources, where even detection of objects before
radiation damage is nearly impossible currently, supports practical
fixed-target sample delivery, and enables high-resolution imaging under
near-native conditions. Numerical simulations with a custom reconstruction
algorithm to recover the latent parameters show the potential to improve the
achievable resolution while also expanding the accessibility to the technique.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [272] [Vibration-Based Energy Metric for Restoring Needle Alignment in Autonomous Robotic Ultrasound](https://arxiv.org/abs/2508.06921)
*Zhongyu Chen,Chenyang Li,Xuesong Li,Dianye Huang,Zhongliang Jiang,Stefanie Speidel,Xiangyu Chu,K. W. Samuel Au*

Main category: cs.RO

TL;DR: 提出一种通过周期性振动针头来恢复超声引导下针头对齐的方法，解决了针头在超声图像中不可见时的对齐问题。


<details>
  <summary>Details</summary>
Motivation: 针头对齐在机器人超声引导手术中至关重要，但超声图像中的噪声、伪影和低分辨率导致针头检测困难，尤其是在针头不可见时。

Method: 通过机械系统周期性振动针头，提出一种基于振动的能量度量，即使针头完全不在平面内也有效，并开发控制策略调整超声探头位置。

Result: 在离体猪组织实验中，平移误差为0.41±0.27毫米，旋转误差为0.51±0.19度。

Conclusion: 该方法在针头不可见时仍能有效恢复对齐，提高了机器人超声引导针头插入的精确性。

Abstract: Precise needle alignment is essential for percutaneous needle insertion in
robotic ultrasound-guided procedures. However, inherent challenges such as
speckle noise, needle-like artifacts, and low image resolution make robust
needle detection difficult, particularly when visibility is reduced or lost. In
this paper, we propose a method to restore needle alignment when the ultrasound
imaging plane and the needle insertion plane are misaligned. Unlike many
existing approaches that rely heavily on needle visibility in ultrasound
images, our method uses a more robust feature by periodically vibrating the
needle using a mechanical system. Specifically, we propose a vibration-based
energy metric that remains effective even when the needle is fully out of
plane. Using this metric, we develop a control strategy to reposition the
ultrasound probe in response to misalignments between the imaging plane and the
needle insertion plane in both translation and rotation. Experiments conducted
on ex-vivo porcine tissue samples using a dual-arm robotic ultrasound-guided
needle insertion system demonstrate the effectiveness of the proposed approach.
The experimental results show the translational error of 0.41$\pm$0.27 mm and
the rotational error of 0.51$\pm$0.19 degrees.

</details>


### [273] [AgriVLN: Vision-and-Language Navigation for Agricultural Robots](https://arxiv.org/abs/2508.07406)
*Xiaobei Zhao,Xingqi Lyu,Xiang Li*

Main category: cs.RO

TL;DR: 论文提出了农业场景下的视觉与语言导航基准A2A和基线方法AgriVLN，通过子任务分解模块提升导航成功率。


<details>
  <summary>Details</summary>
Motivation: 解决农业机器人因依赖人工操作或固定轨道导致的移动受限和适应性差的问题。

Method: 基于视觉语言模型（VLM）设计AgriVLN基线方法，并引入子任务列表（STL）模块分解指令。

Result: AgriVLN在短指令上表现良好，长指令成功率从0.33提升至0.47，优于其他VLN方法。

Conclusion: A2A基准和AgriVLN方法为农业机器人导航提供了有效解决方案，展示了在农业领域的先进性能。

Abstract: Agricultural robots have emerged as powerful members in agricultural tasks,
nevertheless, still heavily rely on manual operation or untransportable railway
for movement, resulting in limited mobility and poor adaptability.
Vision-and-Language Navigation (VLN) enables robots to navigate to the target
destinations following natural language instructions, demonstrating strong
performance on several domains. However, none of the existing benchmarks or
methods is specifically designed for agricultural scenes. To bridge this gap,
we propose Agriculture to Agriculture (A2A) benchmark, containing 1,560
episodes across six diverse agricultural scenes, in which all realistic RGB
videos are captured by front-facing camera on a quadruped robot at a height of
0.38 meters, aligning with the practical deployment conditions. Meanwhile, we
propose Vision-and-Language Navigation for Agricultural Robots (AgriVLN)
baseline based on Vision-Language Model (VLM) prompted with carefully crafted
templates, which can understand both given instructions and agricultural
environments to generate appropriate low-level actions for robot control. When
evaluated on A2A, AgriVLN performs well on short instructions but struggles
with long instructions, because it often fails to track which part of the
instruction is currently being executed. To address this, we further propose
Subtask List (STL) instruction decomposition module and integrate it into
AgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare
AgriVLN with several existing VLN methods, demonstrating the state-of-the-art
performance in the agricultural domain.

</details>


### [274] [Progressive Bird's Eye View Perception for Safety-Critical Autonomous Driving: A Comprehensive Survey](https://arxiv.org/abs/2508.07560)
*Yan Gong,Naibang Wang,Jianli Lu,Xinyu Zhang,Yongsheng Gao,Jie Zhao,Zifan Huang,Haozhi Bai,Nanxin Zeng,Nayu Su,Lei Yang,Ziying Song,Xiaoxi Hu,Xinmin Jiang,Xiaojuan Zhang,Susanto Rahardja*

Main category: cs.RO

TL;DR: 本文综述了自动驾驶中鸟瞰图（BEV）感知的安全关键视角，分析了单模态、多模态和多代理协作感知的框架，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶从受控环境转向现实世界部署，确保BEV感知在复杂场景中的安全性和可靠性成为关键挑战。

Method: 系统分析了BEV感知的三个阶段：单模态车辆端、多模态车辆端和多代理协作感知，并评估了相关数据集。

Result: 提出了开放世界中的关键挑战（如开放集识别、传感器退化等），并指出了未来研究方向（如端到端系统集成、大语言模型等）。

Conclusion: BEV感知在自动驾驶中具有重要潜力，但仍需解决开放世界挑战以实现更安全可靠的部署。

Abstract: Bird's-Eye-View (BEV) perception has become a foundational paradigm in
autonomous driving, enabling unified spatial representations that support
robust multi-sensor fusion and multi-agent collaboration. As autonomous
vehicles transition from controlled environments to real-world deployment,
ensuring the safety and reliability of BEV perception in complex scenarios -
such as occlusions, adverse weather, and dynamic traffic - remains a critical
challenge. This survey provides the first comprehensive review of BEV
perception from a safety-critical perspective, systematically analyzing
state-of-the-art frameworks and implementation strategies across three
progressive stages: single-modality vehicle-side, multimodal vehicle-side, and
multi-agent collaborative perception. Furthermore, we examine public datasets
encompassing vehicle-side, roadside, and collaborative settings, evaluating
their relevance to safety and robustness. We also identify key open-world
challenges - including open-set recognition, large-scale unlabeled data, sensor
degradation, and inter-agent communication latency - and outline future
research directions, such as integration with end-to-end autonomous driving
systems, embodied intelligence, and large language models.

</details>


### [275] [Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning](https://arxiv.org/abs/2508.07885)
*Shoaib Ahmmad,Zubayer Ahmed Aditto,Md Mehrab Hossain,Noushin Yeasmin,Shorower Hossain*

Main category: cs.RO

TL;DR: 该论文提出了一种基于AI的感知系统，用于GPS缺失的室内环境中的自主四轴飞行器导航，结合云计算和定制PCB，实现了高效的数据处理和避障。


<details>
  <summary>Details</summary>
Motivation: 解决GPS缺失环境下四轴飞行器的自主导航问题，提升在狭窄空间中的感知和决策能力。

Method: 系统整合了YOLOv11、Depth Anything V2、ToF传感器、IMU和云端LLM，采用多线程架构和虚拟安全包络技术。

Result: 实验显示，目标检测mAP50为0.6，深度估计MAE为7.2 cm，42次试验中仅有16次安全包络突破，系统延迟低于1秒。

Conclusion: 该框架为GPS缺失环境下的无人机导航提供了高效辅助，补充了现有技术的不足。

Abstract: This paper introduces an advanced AI-driven perception system for autonomous
quadcopter navigation in GPS-denied indoor environments. The proposed framework
leverages cloud computing to offload computationally intensive tasks and
incorporates a custom-designed printed circuit board (PCB) for efficient sensor
data acquisition, enabling robust navigation in confined spaces. The system
integrates YOLOv11 for object detection, Depth Anything V2 for monocular depth
estimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial
Measurement Unit (IMU), and a cloud-based Large Language Model (LLM) for
context-aware decision-making. A virtual safety envelope, enforced by
calibrated sensor offsets, ensures collision avoidance, while a multithreaded
architecture achieves low-latency processing. Enhanced spatial awareness is
facilitated by 3D bounding box estimation with Kalman filtering. Experimental
results in an indoor testbed demonstrate strong performance, with object
detection achieving a mean Average Precision (mAP50) of 0.6, depth estimation
Mean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42
trials over approximately 11 minutes, and end-to-end system latency below 1
second. This cloud-supported, high-intelligence framework serves as an
auxiliary perception and navigation system, complementing state-of-the-art
drone autonomy for GPS-denied confined spaces.

</details>


### [276] [ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks](https://arxiv.org/abs/2508.08240)
*Kaijun Wang,Liqin Lu,Mingyu Liu,Jianuo Jiang,Zeju Li,Bolin Zhang,Wancai Zheng,Xinyi Yu,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: ODYSSEY是一个统一的移动操作框架，结合了高级任务规划和低级全身控制，解决了语言引导的长时程移动操作中的感知、泛化和控制问题。


<details>
  <summary>Details</summary>
Motivation: 移动操作在语义推理、泛化操作和自适应运动方面存在挑战，现有方法局限于桌面场景，泛化能力不足，且缺乏对非结构化环境的适应性。

Method: ODYSSEY框架结合了分层规划器（基于视觉语言模型）和全身控制策略，实现了任务分解和精确执行。

Result: 通过仿真到现实的迁移，系统在多样化的室内外场景中表现出泛化性和鲁棒性。

Conclusion: ODYSSEY提高了腿式操纵器在非结构化环境中执行复杂动态任务的可行性，推动了通用机器人助手的发展。

Abstract: Language-guided long-horizon mobile manipulation has long been a grand
challenge in embodied semantic reasoning, generalizable manipulation, and
adaptive locomotion. Three fundamental limitations hinder progress: First,
although large language models have improved spatial reasoning and task
planning through semantic priors, existing implementations remain confined to
tabletop scenarios, failing to address the constrained perception and limited
actuation ranges of mobile platforms. Second, current manipulation strategies
exhibit insufficient generalization when confronted with the diverse object
configurations encountered in open-world environments. Third, while crucial for
practical deployment, the dual requirement of maintaining high platform
maneuverability alongside precise end-effector control in unstructured settings
remains understudied.
  In this work, we present ODYSSEY, a unified mobile manipulation framework for
agile quadruped robots equipped with manipulators, which seamlessly integrates
high-level task planning with low-level whole-body control. To address the
challenge of egocentric perception in language-conditioned tasks, we introduce
a hierarchical planner powered by a vision-language model, enabling
long-horizon instruction decomposition and precise action execution. At the
control level, our novel whole-body policy achieves robust coordination across
challenging terrains. We further present the first benchmark for long-horizon
mobile manipulation, evaluating diverse indoor and outdoor scenarios. Through
successful sim-to-real transfer, we demonstrate the system's generalization and
robustness in real-world deployments, underscoring the practicality of legged
manipulators in unstructured environments. Our work advances the feasibility of
generalized robotic assistants capable of complex, dynamic tasks. Our project
page: https://kaijwang.github.io/odyssey.github.io/

</details>
