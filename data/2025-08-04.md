<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 89]
- [eess.IV](#eess.IV) [Total: 7]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Quality-Guided Mixture of Score-Fusion Experts Framework for Human Recognition](https://arxiv.org/abs/2508.00053)
*Jie Zhu,Yiyang Su,Minchul Kim,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: 提出了一种名为QME的新型框架，通过可学习的分数融合策略提升全身生物识别性能，结合质量估计和分数三元组损失，实验证明其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统多模态生物识别方法因忽略分数分布差异而性能受限，需改进分数融合策略。

Method: 采用专家混合（MoE）框架，引入伪质量损失和质量估计器（QE），以及分数三元组损失。

Result: 在多个数据集上实现最先进性能，解决了模型对齐和数据质量变化等关键挑战。

Conclusion: QME框架在多模态和多模型场景下表现优异，为生物识别领域提供了有效解决方案。

Abstract: Whole-body biometric recognition is a challenging multimodal task that
integrates various biometric modalities, including face, gait, and body. This
integration is essential for overcoming the limitations of unimodal systems.
Traditionally, whole-body recognition involves deploying different models to
process multiple modalities, achieving the final outcome by score-fusion (e.g.,
weighted averaging of similarity matrices from each model). However, these
conventional methods may overlook the variations in score distributions of
individual modalities, making it challenging to improve final performance. In
this work, we present \textbf{Q}uality-guided \textbf{M}ixture of score-fusion
\textbf{E}xperts (QME), a novel framework designed for improving whole-body
biometric recognition performance through a learnable score-fusion strategy
using a Mixture of Experts (MoE). We introduce a novel pseudo-quality loss for
quality estimation with a modality-specific Quality Estimator (QE), and a score
triplet loss to improve the metric performance. Extensive experiments on
multiple whole-body biometric datasets demonstrate the effectiveness of our
proposed approach, achieving state-of-the-art results across various metrics
compared to baseline methods. Our method is effective for multimodal and
multi-model, addressing key challenges such as model misalignment in the
similarity score domain and variability in data quality.

</details>


### [2] [Punching Bag vs. Punching Person: Motion Transferability in Videos](https://arxiv.org/abs/2508.00085)
*Raiyaan Abdullah,Jared Claypoole,Michael Cogswell,Ajay Divakaran,Yogesh Rawat*

Main category: cs.CV

TL;DR: 研究探讨动作识别模型在跨多样上下文中的高级运动概念迁移能力，发现模型在新情境下性能显著下降，并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 探索动作识别模型是否能有效迁移高级运动概念至不同上下文，尤其是在类似分布中。

Method: 引入包含合成和自然视频的三个数据集，评估13种先进模型，分析性能下降原因。

Result: 模型在新情境下性能显著下降，多模态模型在细粒度动作上表现更差，大模型在空间主导任务中表现更好。

Conclusion: 研究为动作识别中的运动迁移性评估提供了重要基准，并提出了改进方向。

Abstract: Action recognition models demonstrate strong generalization, but can they
effectively transfer high-level motion concepts across diverse contexts, even
within similar distributions? For example, can a model recognize the broad
action "punching" when presented with an unseen variation such as "punching
person"? To explore this, we introduce a motion transferability framework with
three datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2)
Kinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural
video datasets. We evaluate 13 state-of-the-art models on these benchmarks and
observe a significant drop in performance when recognizing high-level actions
in novel contexts. Our analysis reveals: 1) Multimodal models struggle more
with fine-grained unknown actions than with coarse ones; 2) The bias-free
Syn-TA proves as challenging as real-world datasets, with models showing
greater performance drops in controlled settings; 3) Larger models improve
transferability when spatial cues dominate but struggle with intensive temporal
reasoning, while reliance on object and background cues hinders generalization.
We further explore how disentangling coarse and fine motions can improve
recognition in temporally challenging datasets. We believe this study
establishes a crucial benchmark for assessing motion transferability in action
recognition. Datasets and relevant code:
https://github.com/raiyaan-abdullah/Motion-Transfer.

</details>


### [3] [The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking](https://arxiv.org/abs/2508.00088)
*Mateo de Mayo,Daniel Cremers,Taihú Pire*

Main category: cs.CV

TL;DR: 论文介绍了Monado SLAM数据集，旨在解决头戴式传感器在挑战性场景中的跟踪问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉惯性里程计（VIO）和SLAM系统在头戴式应用中难以应对高动态运动、遮挡等复杂场景，缺乏相关数据集。

Method: 提出Monado SLAM数据集，包含多款虚拟现实头显采集的真实序列。

Result: 数据集以CC BY 4.0许可发布，推动VIO/SLAM研究发展。

Conclusion: Monado SLAM数据集填补了现有研究空白，有助于改进头戴式传感器的跟踪性能。

Abstract: Humanoid robots and mixed reality headsets benefit from the use of
head-mounted sensors for tracking. While advancements in visual-inertial
odometry (VIO) and simultaneous localization and mapping (SLAM) have produced
new and high-quality state-of-the-art tracking systems, we show that these are
still unable to gracefully handle many of the challenging settings presented in
the head-mounted use cases. Common scenarios like high-intensity motions,
dynamic occlusions, long tracking sessions, low-textured areas, adverse
lighting conditions, saturation of sensors, to name a few, continue to be
covered poorly by existing datasets in the literature. In this way, systems may
inadvertently overlook these essential real-world issues. To address this, we
present the Monado SLAM dataset, a set of real sequences taken from multiple
virtual reality headsets. We release the dataset under a permissive CC BY 4.0
license, to drive advancements in VIO/SLAM research and development.

</details>


### [4] [Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images](https://arxiv.org/abs/2508.00135)
*Basna Mohammed Salih Hasan,Ramadhan J. Mstafa*

Main category: cs.CV

TL;DR: 论文提出了一种基于卷积神经网络（CNN）的模型，利用眼周区域的颜色图像进行性别分类，在CVBL和（Female and Male）数据集上分别达到99%和96%的准确率。


<details>
  <summary>Details</summary>
Motivation: 性别分类在安全、人机交互等领域至关重要，但化妆品和伪装等因素会影响分类准确性。研究专注于利用眼周区域的特征来解决这一问题。

Method: 采用CNN模型，利用眼周区域的颜色图像提取关键特征进行性别分类，并在两个数据集上验证性能。

Result: 模型在CVBL数据集上达到99%的准确率，在（Female and Male）数据集上达到96%的准确率，且参数量较少（7,235,089）。

Conclusion: 模型表现出色，适用于安全和监控等实际应用。

Abstract: Gender classification has emerged as a crucial aspect in various fields,
including security, human-machine interaction, surveillance, and advertising.
Nonetheless, the accuracy of this classification can be influenced by factors
such as cosmetics and disguise. Consequently, our study is dedicated to
addressing this concern by concentrating on gender classification using color
images of the periocular region. The periocular region refers to the area
surrounding the eye, including the eyelids, eyebrows, and the region between
them. It contains valuable visual cues that can be used to extract key features
for gender classification. This paper introduces a sophisticated Convolutional
Neural Network (CNN) model that utilizes color image databases to evaluate the
effectiveness of the periocular region for gender classification. To validate
the model's performance, we conducted tests on two eye datasets, namely CVBL
and (Female and Male). The recommended architecture achieved an outstanding
accuracy of 99% on the previously unused CVBL dataset while attaining a
commendable accuracy of 96% with a small number of learnable parameters
(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of
our proposed model for gender classification using the periocular region, we
evaluated its performance through an extensive range of metrics and compared it
with other state-of-the-art approaches. The results unequivocally demonstrate
the efficacy of our model, thereby suggesting its potential for practical
application in domains such as security and surveillance.

</details>


### [5] [World Consistency Score: A Unified Metric for Video Generation Quality](https://arxiv.org/abs/2508.00144)
*Akshat Rakheja,Aarsh Ashdhir,Aryan Bhattacharjee,Vanshika Sharma*

Main category: cs.CV

TL;DR: World Consistency Score (WCS) 是一种新的生成视频模型评估指标，强调视频的内部世界一致性，结合了四个可解释的子指标，并通过学习权重公式生成单一分数。


<details>
  <summary>Details</summary>
Motivation: 现有视频评估指标主要关注视觉保真度或提示对齐，而忽略了时间与物理一致性，WCS旨在填补这一空白。

Method: WCS整合了四个子指标（物体持久性、关系稳定性、因果合规性和闪烁惩罚），使用开源工具计算，并通过人类偏好数据训练权重。

Result: WCS通过实验验证（如VBench-2.0等基准测试）显示与人类评估高度相关，且优于现有指标（如FVD、CLIPScore等）。

Conclusion: WCS提供了一个全面且可解释的框架，用于评估视频生成模型在保持世界一致性方面的能力。

Abstract: We introduce World Consistency Score (WCS), a novel unified evaluation metric
for generative video models that emphasizes internal world consistency of the
generated videos. WCS integrates four interpretable sub-components - object
permanence, relation stability, causal compliance, and flicker penalty - each
measuring a distinct aspect of temporal and physical coherence in a video.
These submetrics are combined via a learned weighted formula to produce a
single consistency score that aligns with human judgments. We detail the
motivation for WCS in the context of existing video evaluation metrics,
formalize each submetric and how it is computed with open-source tools
(trackers, action recognizers, CLIP embeddings, optical flow), and describe how
the weights of the WCS combination are trained using human preference data. We
also outline an experimental validation blueprint: using benchmarks like
VBench-2.0, EvalCrafter, and LOVE to test WCS's correlation with human
evaluations, performing sensitivity analyses, and comparing WCS against
established metrics (FVD, CLIPScore, VBench, FVMD). The proposed WCS offers a
comprehensive and interpretable framework for evaluating video generation
models on their ability to maintain a coherent "world" over time, addressing
gaps left by prior metrics focused only on visual fidelity or prompt alignment.

</details>


### [6] [GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration](https://arxiv.org/abs/2508.00152)
*Li Mi,Manon Bechaz,Zeming Chen,Antoine Bosselut,Devis Tuia*

Main category: cs.CV

TL;DR: GeoExplorer提出了一种基于好奇心驱动探索的主动地理定位方法，通过内在奖励提升鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于距离奖励的强化学习方法在距离估计困难或面对新目标与环境时表现不佳，缺乏可靠的探索策略。

Method: 提出GeoExplorer，利用好奇心驱动的内在奖励进行目标无关的探索，增强环境建模能力。

Result: 在四个AGL基准测试中验证了GeoExplorer的有效性，尤其在陌生目标和环境中的定位表现突出。

Conclusion: GeoExplorer通过好奇心驱动探索显著提升了AGL任务的鲁棒性和泛化能力。

Abstract: Active Geo-localization (AGL) is the task of localizing a goal, represented
in various modalities (e.g., aerial images, ground-level images, or text),
within a predefined search area. Current methods approach AGL as a
goal-reaching reinforcement learning (RL) problem with a distance-based reward.
They localize the goal by implicitly learning to minimize the relative distance
from it. However, when distance estimation becomes challenging or when
encountering unseen targets and environments, the agent exhibits reduced
robustness and generalization ability due to the less reliable exploration
strategy learned during training. In this paper, we propose GeoExplorer, an AGL
agent that incorporates curiosity-driven exploration through intrinsic rewards.
Unlike distance-based rewards, our curiosity-driven reward is goal-agnostic,
enabling robust, diverse, and contextually relevant exploration based on
effective environment modeling. These capabilities have been proven through
extensive experiments across four AGL benchmarks, demonstrating the
effectiveness and generalization ability of GeoExplorer in diverse settings,
particularly in localizing unfamiliar targets and environments.

</details>


### [7] [Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs](https://arxiv.org/abs/2508.00169)
*Bhavya Goyal,Felipe Gutierrez-Barragan,Wei Lin,Andreas Velten,Yin Li,Mohit Gupta*

Main category: cs.CV

TL;DR: 论文提出了一种名为概率点云（PPC）的新3D场景表示方法，通过为每个点添加概率属性来封装原始数据中的测量不确定性，从而提升3D物体检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现代LiDAR在远距离或低反光物体等场景中会产生稀疏或错误的点云，这些误差会传播到下游感知模型，导致精度下降。传统3D处理流程未保留原始测量中的不确定性信息。

Method: 提出PPC表示方法，为点云中的每个点添加概率属性，并引入基于PPC的推理方法，可作为轻量级模块集成到3D推理流程中。

Result: 通过仿真和实际捕获数据验证，PPC方法在室内外场景中优于LiDAR及相机-LiDAR融合模型，尤其在处理小、远、低反光物体及强环境光时表现更优。

Conclusion: PPC通过保留不确定性信息，显著提升了3D物体检测的鲁棒性和准确性，适用于多种复杂场景。

Abstract: LiDAR-based 3D sensors provide point clouds, a canonical 3D representation
used in various scene understanding tasks. Modern LiDARs face key challenges in
several real-world scenarios, such as long-distance or low-albedo objects,
producing sparse or erroneous point clouds. These errors, which are rooted in
the noisy raw LiDAR measurements, get propagated to downstream perception
models, resulting in potentially severe loss of accuracy. This is because
conventional 3D processing pipelines do not retain any uncertainty information
from the raw measurements when constructing point clouds.
  We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation
where each point is augmented with a probability attribute that encapsulates
the measurement uncertainty (or confidence) in the raw data. We further
introduce inference approaches that leverage PPC for robust 3D object
detection; these methods are versatile and can be used as computationally
lightweight drop-in modules in 3D inference pipelines. We demonstrate, via both
simulations and real captures, that PPC-based 3D inference methods outperform
several baselines using LiDAR as well as camera-LiDAR fusion models, across
challenging indoor and outdoor scenarios involving small, distant, and
low-albedo objects, as well as strong ambient light.
  Our project webpage is at https://bhavyagoyal.github.io/ppc .

</details>


### [8] [IN2OUT: Fine-Tuning Video Inpainting Model for Video Outpainting Using Hierarchical Discriminator](https://arxiv.org/abs/2508.00418)
*Sangwoo Youn,Minji Lee,Nokap Tony Park,Yeonggyoo Jeon,Taeyoung Na*

Main category: cs.CV

TL;DR: 论文提出了一种改进的视频外绘方法，通过引入分层判别器和专用损失函数，解决了现有方法在生成模糊结果和全局一致性上的问题。


<details>
  <summary>Details</summary>
Motivation: 视频外绘的挑战在于扩展边界时保持内容一致性，现有方法直接应用修复模型效果不佳。

Method: 提出分层判别器，区分全局和局部目标，并设计专用外绘损失函数。

Result: 方法在定量和定性上均优于现有技术。

Conclusion: 通过改进判别器和损失函数，显著提升了视频外绘的质量和一致性。

Abstract: Video outpainting presents a unique challenge of extending the borders while
maintaining consistency with the given content. In this paper, we suggest the
use of video inpainting models that excel in object flow learning and
reconstruction in outpainting rather than solely generating the background as
in existing methods. However, directly applying or fine-tuning inpainting
models to outpainting has shown to be ineffective, often leading to blurry
results. Our extensive experiments on discriminator designs reveal that a
critical component missing in the outpainting fine-tuning process is a
discriminator capable of effectively assessing the perceptual quality of the
extended areas. To tackle this limitation, we differentiate the objectives of
adversarial training into global and local goals and introduce a hierarchical
discriminator that meets both objectives. Additionally, we develop a
specialized outpainting loss function that leverages both local and global
features of the discriminator. Fine-tuning on this adversarial loss function
enhances the generator's ability to produce both visually appealing and
globally coherent outpainted scenes. Our proposed method outperforms
state-of-the-art methods both quantitatively and qualitatively. Supplementary
materials including the demo video and the code are available in SigPort.

</details>


### [9] [On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI](https://arxiv.org/abs/2508.00171)
*David Restrepo,Ira Ktena,Maria Vakalopoulou,Stergios Christodoulidis,Enzo Ferrante*

Main category: cs.CV

TL;DR: 论文提出选择性模态转移（SMS）方法，用于量化视觉-语言模型（VLMs）在二元分类任务中对各模态的依赖，揭示模型对文本输入的显著偏好。


<details>
  <summary>Details</summary>
Motivation: 临床决策依赖于医学图像和相关临床报告的综合分析，但现有VLMs常偏向文本模态，忽视视觉信息。

Method: 通过交换样本中的图像或文本来暴露模态偏差，评估六种开源VLMs在两种医学影像数据集上的表现。

Result: 发现模型对文本输入的依赖显著，视觉信息常被忽视。

Conclusion: 强调设计真正融合视觉和文本信息的医学多模态模型的重要性。

Abstract: Clinical decision-making relies on the integrated analysis of medical images
and the associated clinical reports. While Vision-Language Models (VLMs) can
offer a unified framework for such tasks, they can exhibit strong biases toward
one modality, frequently overlooking critical visual cues in favor of textual
information. In this work, we introduce Selective Modality Shifting (SMS), a
perturbation-based approach to quantify a model's reliance on each modality in
binary classification tasks. By systematically swapping images or text between
samples with opposing labels, we expose modality-specific biases. We assess six
open-source VLMs-four generalist models and two fine-tuned for medical data-on
two medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray)
and FairVLMed (scanning laser ophthalmoscopy). By assessing model performance
and the calibration of every model in both unperturbed and perturbed settings,
we reveal a marked dependency on text input, which persists despite the
presence of complementary visual information. We also perform a qualitative
attention-based analysis which further confirms that image content is often
overshadowed by text details. Our findings highlight the importance of
designing and evaluating multimodal medical models that genuinely integrate
visual and textual cues, rather than relying on single-modality signals.

</details>


### [10] [Semantic and Temporal Integration in Latent Diffusion Space for High-Fidelity Video Super-Resolution](https://arxiv.org/abs/2508.00471)
*Yiwen Wang,Xinning Chai,Yuhong Zhang,Zhengxue Cheng,Jun Zhao,Rong Xie,Li Song*

Main category: cs.CV

TL;DR: SeTe-VSR是一种结合语义和时空指导的视频超分辨率方法，显著提升了细节恢复和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频超分辨率模型在控制生成过程和保持时间一致性方面存在挑战。

Method: 提出SeTe-VSR，在潜在扩散空间中结合语义和时空信息指导。

Result: 实验表明SeTe-VSR在细节恢复和感知质量上优于现有方法。

Conclusion: SeTe-VSR在复杂视频超分辨率任务中表现出色。

Abstract: Recent advancements in video super-resolution (VSR) models have demonstrated
impressive results in enhancing low-resolution videos. However, due to
limitations in adequately controlling the generation process, achieving high
fidelity alignment with the low-resolution input while maintaining temporal
consistency across frames remains a significant challenge. In this work, we
propose Semantic and Temporal Guided Video Super-Resolution (SeTe-VSR), a novel
approach that incorporates both semantic and temporal-spatio guidance in the
latent diffusion space to address these challenges. By incorporating high-level
semantic information and integrating spatial and temporal information, our
approach achieves a seamless balance between recovering intricate details and
ensuring temporal coherence. Our method not only preserves high-reality visual
content but also significantly enhances fidelity. Extensive experiments
demonstrate that SeTe-VSR outperforms existing methods in terms of detail
recovery and perceptual quality, highlighting its effectiveness for complex
video super-resolution tasks.

</details>


### [11] [Graph Lineages and Skeletal Graph Products](https://arxiv.org/abs/2508.00197)
*Eric Mjolsness,Cory B. Scott*

Main category: cs.CV

TL;DR: 该论文提出了一种结构化图谱系（graph lineages）的代数类型理论，用于定义层次化模型架构和算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为机器学习和计算科学中的数学模型架构提供一种基于层次化图结构的通用框架。

Method: 方法包括定义层次化增长的图谱系，引入延长映射和骨架操作符，并推导其代数性质。

Result: 结果为构建了适用于层次化图谱系的代数类型理论，并展示了在深度神经网络和多网格数值方法中的应用。

Conclusion: 结论是该框架适用于定义层次化模型架构（hierarchitectures）和局部采样、搜索或优化算法。

Abstract: Graphs, and sequences of growing graphs, can be used to specify the
architecture of mathematical models in many fields including machine learning
and computational science. Here we define structured graph "lineages" (ordered
by level number) that grow in a hierarchical fashion, so that: (1) the number
of graph vertices and edges increases exponentially in level number; (2)
bipartite graphs connect successive levels within a graph lineage and, as in
multigrid methods, can constrain matrices relating successive levels; (3) using
prolongation maps within a graph lineage, process-derived distance measures
between graphs at successive levels can be defined; (4) a category of "graded
graphs" can be defined, and using it low-cost "skeletal" variants of standard
algebraic graph operations and type constructors (cross product, box product,
disjoint sum, and function types) can be derived for graded graphs and hence
hierarchical graph lineages; (5) these skeletal binary operators have similar
but not identical algebraic and category-theoretic properties to their standard
counterparts; (6) graph lineages and their skeletal product constructors can
approach continuum limit objects. Additional space-efficient unary operators on
graded graphs are also derived: thickening, which creates a graph lineage of
multiscale graphs, and escalation to a graph lineage of search frontiers
(useful as a generalization of adaptive grids and in defining "skeletal"
functions). The result is an algebraic type theory for graded graphs and
(hierarchical) graph lineages. The approach is expected to be well suited to
defining hierarchical model architectures - "hierarchitectures" - and local
sampling, search, or optimization algorithms on them. We demonstrate such
application to deep neural networks (including visual and feature scale spaces)
and to multigrid numerical methods.

</details>


### [12] [A Novel Modeling Framework and Data Product for Extended VIIRS-like Artificial Nighttime Light Image Reconstruction (1986-2024)](https://arxiv.org/abs/2508.00590)
*Yihe Tian,Kwan Man Cheng,Zhengbo Zhang,Tao Zhang,Suju Li,Dongmei Yan,Bing Xu*

Main category: cs.CV

TL;DR: 论文提出了一种新的重建框架（EVAL），用于扩展VIIRS-like夜间灯光数据的时间序列，解决了现有方法低估光强度和结构遗漏的问题，并将数据记录扩展到1986年。


<details>
  <summary>Details</summary>
Motivation: NPP-VIIRS传感器的夜间灯光数据时间覆盖有限（始于2012年），限制了长期时间序列研究，现有方法存在光强度低估和结构遗漏的不足。

Method: 提出了一个两阶段重建框架：构建阶段使用分层融合解码器（HFD）提高初始重建的保真度；细化阶段采用双特征细化器（DFR），利用高分辨率不透水面掩膜增强细节。

Result: 开发的EVAL产品将数据记录扩展到1986年，性能显著优于现有产品（R²从0.68提升至0.80，RMSE从1.27降至0.99），并表现出良好的时间一致性和社会经济参数相关性。

Conclusion: EVAL数据集为研究社区提供了有价值的资源，适用于长期分析，并已公开提供。

Abstract: Artificial Night-Time Light (NTL) remote sensing is a vital proxy for
quantifying the intensity and spatial distribution of human activities.
Although the NPP-VIIRS sensor provides high-quality NTL observations, its
temporal coverage, which begins in 2012, restricts long-term time-series
studies that extend to earlier periods. Despite the progress in extending
VIIRS-like NTL time-series, current methods still suffer from two significant
shortcomings: the underestimation of light intensity and the structural
omission. To overcome these limitations, we propose a novel reconstruction
framework consisting of a two-stage process: construction and refinement. The
construction stage features a Hierarchical Fusion Decoder (HFD) designed to
enhance the fidelity of the initial reconstruction. The refinement stage
employs a Dual Feature Refiner (DFR), which leverages high-resolution
impervious surface masks to guide and enhance fine-grained structural details.
Based on this framework, we developed the Extended VIIRS-like Artificial
Nighttime Light (EVAL) product for China, extending the standard data record
backwards by 26 years to begin in 1986. Quantitative evaluation shows that EVAL
significantly outperforms existing state-of-the-art products, boosting the
$\text{R}^2$ from 0.68 to 0.80 while lowering the RMSE from 1.27 to 0.99.
Furthermore, EVAL exhibits excellent temporal consistency and maintains a high
correlation with socioeconomic parameters, confirming its reliability for
long-term analysis. The resulting EVAL dataset provides a valuable new resource
for the research community and is publicly available at
https://doi.org/10.11888/HumanNat.tpdc.302930.

</details>


### [13] [Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition](https://arxiv.org/abs/2508.00205)
*Xiangyu Kong,Hengde Zhu,Haoqin Sun,Zhihao Guo,Jiayan Gu,Xinyi Ni,Wei Zhang,Shizhe Liu,Siyang Song*

Main category: cs.CV

TL;DR: 本文提出了一种基于个性化内部认知的自动真实人格识别方法，通过模拟目标个体的内部认知来提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常作为外部观察者推断人格印象，与真实人格偏差较大，性能较差。本文受真实人格与内部认知关联的启发，提出新方法。

Method: 通过模拟个性化内部认知（表示为网络权重），构建包含二维节点和边特征矩阵的图，并设计2D-GNN进行人格推断。采用端到端策略联合训练模块。

Result: 提出的方法能够更准确地识别真实人格特质。

Conclusion: 通过模拟内部认知并结合2D-GNN，本文方法显著提升了真实人格识别的性能。

Abstract: Automatic real personality recognition (RPR) aims to evaluate human real
personality traits from their expressive behaviours. However, most existing
solutions generally act as external observers to infer observers' personality
impressions based on target individuals' expressive behaviours, which
significantly deviate from their real personalities and consistently lead to
inferior recognition performance. Inspired by the association between real
personality and human internal cognition underlying the generation of
expressive behaviours, we propose a novel RPR approach that efficiently
simulates personalised internal cognition from easy-accessible external short
audio-visual behaviours expressed by the target individual. The simulated
personalised cognition, represented as a set of network weights that enforce
the personalised network to reproduce the individual-specific facial reactions,
is further encoded as a novel graph containing two-dimensional node and edge
feature matrices, with a novel 2D Graph Neural Network (2D-GNN) proposed for
inferring real personality traits from it. To simulate real personality-related
cognition, an end-to-end strategy is designed to jointly train our cognition
simulation, 2D graph construction, and personality recognition modules.

</details>


### [14] [SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation](https://arxiv.org/abs/2508.00750)
*Prerana Ramkumar*

Main category: cs.CV

TL;DR: SU-ESRGAN结合ESRGAN、DeepLabv3分割损失和蒙特卡洛dropout，为卫星图像提供语义一致性和像素级不确定性，适用于无人机和卫星系统。


<details>
  <summary>Details</summary>
Motivation: 现有GAN在超分辨率图像中缺乏语义一致性和像素级置信度，限制了其在遥感关键应用中的可信度。

Method: 提出SU-ESRGAN框架，整合ESRGAN、DeepLabv3分割损失和蒙特卡洛dropout，生成不确定性地图。

Result: 在PSNR、SSIM和LPIPS指标上与基线ESRGAN相当，并在跨域测试中表现优异。

Conclusion: SU-ESRGAN在卫星和无人机系统中具有实用价值，强调领域感知训练的重要性。

Abstract: Generative Adversarial Networks (GANs) have achieved realistic
super-resolution (SR) of images however, they lack semantic consistency and
per-pixel confidence, limiting their credibility in critical remote sensing
applications such as disaster response, urban planning and agriculture. This
paper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first
SR framework designed for satellite imagery to integrate the ESRGAN,
segmentation loss via DeepLabv3 for class detail preservation and Monte Carlo
dropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results
(PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This
novel model is valuable in satellite systems or UAVs that use wide
field-of-view (FoV) cameras, trading off spatial resolution for coverage. The
modular design allows integration in UAV data pipelines for on-board or
post-processing SR to enhance imagery resulting due to motion blur, compression
and sensor limitations. Further, the model is fine-tuned to evaluate its
performance on cross domain applications. The tests are conducted on two drone
based datasets which differ in altitude and imaging perspective. Performance
evaluation of the fine-tuned models show a stronger adaptation to the Aerial
Maritime Drone Dataset, whose imaging characteristics align with the training
data, highlighting the importance of domain-aware training in SR-applications.

</details>


### [15] [SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters](https://arxiv.org/abs/2508.00213)
*Shayan Jalilian,Abdul Bais*

Main category: cs.CV

TL;DR: SAM-PTx通过轻量级适配器设计，将CLIP文本嵌入引入SAM模型，提升语义引导的分割性能。


<details>
  <summary>Details</summary>
Motivation: 探索语义文本提示在分割任务中的潜力，弥补传统空间提示的不足。

Method: 提出Parallel-Text适配器，将文本嵌入注入SAM的图像编码器，仅修改MLP并行分支。

Result: 在COD10K、COCO和ADE20K数据集上，语义提示显著优于纯空间提示基线。

Conclusion: 语义条件集成是高效适应SAM的实用且可扩展路径。

Abstract: The Segment Anything Model (SAM) has demonstrated impressive generalization
in prompt-based segmentation. Yet, the potential of semantic text prompts
remains underexplored compared to traditional spatial prompts like points and
boxes. This paper introduces SAM-PTx, a parameter-efficient approach for
adapting SAM using frozen CLIP-derived text embeddings as class-level semantic
guidance. Specifically, we propose a lightweight adapter design called
Parallel-Text that injects text embeddings into SAM's image encoder, enabling
semantics-guided segmentation while keeping most of the original architecture
frozen. Our adapter modifies only the MLP-parallel branch of each transformer
block, preserving the attention pathway for spatial reasoning. Through
supervised experiments and ablations on the COD10K dataset as well as low-data
subsets of COCO and ADE20K, we show that incorporating fixed text embeddings as
input improves segmentation performance over purely spatial prompt baselines.
To our knowledge, this is the first work to use text prompts for segmentation
on the COD10K dataset. These results suggest that integrating semantic
conditioning into SAM's architecture offers a practical and scalable path for
efficient adaptation with minimal computational complexity.

</details>


### [16] [Object-Centric Cropping for Visual Few-Shot Classification](https://arxiv.org/abs/2508.00218)
*Aymane Abdali,Bartosz Boguslawski,Lucas Drumetz,Vincent Gripon*

Main category: cs.CV

TL;DR: 在少样本图像分类中，通过引入对象的局部位置信息显著提升性能，且仅需少量标注或完全无监督方法即可实现。


<details>
  <summary>Details</summary>
Motivation: 解决少样本图像分类中因图像模糊（如多对象或复杂背景）导致的性能下降问题。

Method: 利用Segment Anything Model（仅需标注对象的一个像素）或无监督前景对象提取方法，引入对象的局部位置信息。

Result: 在多个基准测试中，分类性能显著提升。

Conclusion: 局部位置信息的引入是提升少样本图像分类性能的有效方法，且实现成本低。

Abstract: In the domain of Few-Shot Image Classification, operating with as little as
one example per class, the presence of image ambiguities stemming from multiple
objects or complex backgrounds can significantly deteriorate performance. Our
research demonstrates that incorporating additional information about the local
positioning of an object within its image markedly enhances classification
across established benchmarks. More importantly, we show that a significant
fraction of the improvement can be achieved through the use of the Segment
Anything Model, requiring only a pixel of the object of interest to be pointed
out, or by employing fully unsupervised foreground object extraction methods.

</details>


### [17] [Guided Depth Map Super-Resolution via Multi-Scale Fusion U-shaped Mamba Network](https://arxiv.org/abs/2508.00248)
*Chenggang Guo,Hao Xu,XianMing Wan*

Main category: cs.CV

TL;DR: 提出了一种多尺度融合U形Mamba（MSF-UM）模型，用于深度图超分辨率，结合Mamba的高效状态空间建模和多尺度U形结构，显著提升重建精度并减少参数。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络在处理长距离依赖和全局上下文信息时存在局限，而Transformer的计算复杂度和内存消耗过高，限制了其在高分辨率深度图上的应用。

Method: 设计了一种结合残差密集通道注意力块和Mamba状态空间模块的结构，利用多尺度跨模态融合策略，通过彩色图像的高频纹理信息引导深度图超分辨率。

Result: MSF-UM在多个公开数据集上验证了其有效性，显著减少了模型参数并提升了重建精度，尤其在大规模深度图超分辨率任务中表现出优秀的泛化能力。

Conclusion: MSF-UM模型通过结合局部特征提取和长距离依赖建模，以及多尺度跨模态融合，为深度图超分辨率提供了一种高效且准确的解决方案。

Abstract: Depth map super-resolution technology aims to improve the spatial resolution
of low-resolution depth maps and effectively restore high-frequency detail
information. Traditional convolutional neural network has limitations in
dealing with long-range dependencies and are unable to fully model the global
contextual information in depth maps. Although transformer can model global
dependencies, its computational complexity and memory consumption are
quadratic, which significantly limits its ability to process high-resolution
depth maps. In this paper, we propose a multi-scale fusion U-shaped Mamba
(MSF-UM) model, a novel guided depth map super-resolution framework. The core
innovation of this model is to integrate Mamba's efficient state-space modeling
capabilities into a multi-scale U-shaped fusion structure guided by a color
image. The structure combining the residual dense channel attention block and
the Mamba state space module is designed, which combines the local feature
extraction capability of the convolutional layer with the modeling advantage of
the state space model for long-distance dependencies. At the same time, the
model adopts a multi-scale cross-modal fusion strategy to make full use of the
high-frequency texture information from the color image to guide the
super-resolution process of the depth map. Compared with existing mainstream
methods, the proposed MSF-UM significantly reduces the number of model
parameters while achieving better reconstruction accuracy. Extensive
experiments on multiple publicly available datasets validate the effectiveness
of the model, especially showing excellent generalization ability in the task
of large-scale depth map super-resolution.

</details>


### [18] [PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting](https://arxiv.org/abs/2508.00259)
*Wentao Sun,Hanqing Xu,Quanyun Wu,Dedong Zhang,Yiping Chen,Lingfei Ma,John S. Zelek,Jonathan Li*

Main category: cs.CV

TL;DR: PointGauss提出了一种基于点云引导的实时多目标分割框架，通过高斯泼溅表示实现高效3D分割，显著提升了多视图一致性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在初始化时间长和多视图一致性不足的问题，PointGauss旨在通过点云分割驱动的流程直接解析高斯基元，解决这些问题。

Method: 1. 基于点云的高斯基元解码器，1分钟内生成3D实例掩码；2. GPU加速的2D掩码渲染系统，确保多视图一致性。

Result: 实验表明，PointGauss在多视图mIoU上比现有方法提升1.89%至31.78%，同时保持高效计算。

Conclusion: PointGauss在3D分割中表现出色，并提出了DesktopObjects-360数据集，弥补了现有基准的不足。

Abstract: We introduce PointGauss, a novel point cloud-guided framework for real-time
multi-object segmentation in Gaussian Splatting representations. Unlike
existing methods that suffer from prolonged initialization and limited
multi-view consistency, our approach achieves efficient 3D segmentation by
directly parsing Gaussian primitives through a point cloud segmentation-driven
pipeline. The key innovation lies in two aspects: (1) a point cloud-based
Gaussian primitive decoder that generates 3D instance masks within 1 minute,
and (2) a GPU-accelerated 2D mask rendering system that ensures multi-view
consistency. Extensive experiments demonstrate significant improvements over
previous state-of-the-art methods, achieving performance gains of 1.89 to
31.78% in multi-view mIoU, while maintaining superior computational efficiency.
To address the limitations of current benchmarks (single-object focus,
inconsistent 3D evaluation, small scale, and partial coverage), we present
DesktopObjects-360, a novel comprehensive dataset for 3D segmentation in
radiance fields, featuring: (1) complex multi-object scenes, (2) globally
consistent 2D annotations, (3) large-scale training data (over 27 thousand 2D
masks), (4) full 360{\deg} coverage, and (5) 3D evaluation masks.

</details>


### [19] [Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models](https://arxiv.org/abs/2508.00260)
*Hyundong Jin,Hyung Jin Chang,Eunwoo Kim*

Main category: cs.CV

TL;DR: 提出了一种新框架，通过混合视觉投影器和专家推荐策略，解决生成式视觉语言模型在持续学习中忽视语言指令的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在持续学习中可能导致模型过度关注视觉输入而忽视语言指令，尤其是在重复文本指令的任务中。

Method: 引入混合视觉投影器作为专家，根据指令上下文进行视觉到语言的翻译，并提出专家推荐和修剪策略以减少干扰。

Result: 在多样化视觉语言任务上的实验表明，该方法优于现有持续学习方法，生成更符合指令的响应。

Conclusion: 该方法有效解决了语言指令被忽视的问题，提升了模型在持续学习中的性能。

Abstract: Continual learning enables pre-trained generative vision-language models
(VLMs) to incorporate knowledge from new tasks without retraining data from
previous ones. Recent methods update a visual projector to translate visual
information for new tasks, connecting pre-trained vision encoders with large
language models. However, such adjustments may cause the models to prioritize
visual inputs over language instructions, particularly learning tasks with
repetitive types of textual instructions. To address the neglect of language
instructions, we propose a novel framework that grounds the translation of
visual information on instructions for language models. We introduce a mixture
of visual projectors, each serving as a specialized visual-to-language
translation expert based on the given instruction context to adapt to new
tasks. To avoid using experts for irrelevant instruction contexts, we propose
an expert recommendation strategy that reuses experts for tasks similar to
those previously learned. Additionally, we introduce expert pruning to
alleviate interference from the use of experts that cumulatively activated in
previous tasks. Extensive experiments on diverse vision-language tasks
demonstrate that our method outperforms existing continual learning approaches
by generating instruction-following responses.

</details>


### [20] [Multimodal Referring Segmentation: A Survey](https://arxiv.org/abs/2508.00265)
*Henghui Ding,Song Tang,Shuting He,Chang Liu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 本文综述了多模态指代分割领域，包括问题定义、数据集、统一架构、代表性方法、广义指代表达（GREx）及相关应用。


<details>
  <summary>Details</summary>
Motivation: 多模态指代分割在基于用户指令的精确对象感知中具有重要应用价值，近年来因深度学习技术的进步而备受关注。

Method: 提出统一元架构，总结图像、视频和3D场景中的代表性方法，并讨论GREx方法以应对现实复杂性。

Result: 提供了标准基准上的性能比较，并持续跟踪相关研究。

Conclusion: 多模态指代分割领域发展迅速，未来需进一步解决现实复杂性问题并扩展应用场景。

Abstract: Multimodal referring segmentation aims to segment target objects in visual
scenes, such as images, videos, and 3D scenes, based on referring expressions
in text or audio format. This task plays a crucial role in practical
applications requiring accurate object perception based on user instructions.
Over the past decade, it has gained significant attention in the multimodal
community, driven by advances in convolutional neural networks, transformers,
and large language models, all of which have substantially improved multimodal
perception capabilities. This paper provides a comprehensive survey of
multimodal referring segmentation. We begin by introducing this field's
background, including problem definitions and commonly used datasets. Next, we
summarize a unified meta architecture for referring segmentation and review
representative methods across three primary visual scenes, including images,
videos, and 3D scenes. We further discuss Generalized Referring Expression
(GREx) methods to address the challenges of real-world complexity, along with
related tasks and practical applications. Extensive performance comparisons on
standard benchmarks are also provided. We continually track related works at
https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.

</details>


### [21] [Towards Robust Semantic Correspondence: A Benchmark and Insights](https://arxiv.org/abs/2508.00272)
*Wenyue Chong*

Main category: cs.CV

TL;DR: 论文提出了一个评估语义对应在恶劣条件下鲁棒性的新基准，发现现有方法在挑战性场景中表现下降，并探讨了大规模视觉模型和融合策略的效果。


<details>
  <summary>Details</summary>
Motivation: 语义对应是计算机视觉中的基础任务，但在恶劣条件下的鲁棒性研究较少。本文旨在填补这一空白。

Method: 建立了一个包含14种挑战性场景的基准数据集，评估了现有方法及大规模视觉模型（如DINO和Stable Diffusion）的表现。

Result: 现有方法在恶劣条件下性能显著下降；大规模视觉模型能提升鲁棒性，但微调会降低相对鲁棒性；DINO优于Stable Diffusion，融合策略效果更好。

Conclusion: 任务特定的鲁棒性增强策略是必要的，通用数据增强效果不佳，研究结果在真实场景中一致。

Abstract: Semantic correspondence aims to identify semantically meaningful
relationships between different images and is a fundamental challenge in
computer vision. It forms the foundation for numerous tasks such as 3D
reconstruction, object tracking, and image editing. With the progress of
large-scale vision models, semantic correspondence has achieved remarkable
performance in controlled and high-quality conditions. However, the robustness
of semantic correspondence in challenging scenarios is much less investigated.
In this work, we establish a novel benchmark for evaluating semantic
correspondence in adverse conditions. The benchmark dataset comprises 14
distinct challenging scenarios that reflect commonly encountered imaging
issues, including geometric distortion, image blurring, digital artifacts, and
environmental occlusion. Through extensive evaluations, we provide several key
insights into the robustness of semantic correspondence approaches: (1) All
existing methods suffer from noticeable performance drops under adverse
conditions; (2) Using large-scale vision models can enhance overall robustness,
but fine-tuning on these models leads to a decline in relative robustness; (3)
The DINO model outperforms the Stable Diffusion in relative robustness, and
their fusion achieves better absolute robustness; Moreover, We evaluate common
robustness enhancement strategies for semantic correspondence and find that
general data augmentations are ineffective, highlighting the need for
task-specific designs. These results are consistent across both our dataset and
real-world benchmarks.

</details>


### [22] [Privacy-Preserving Driver Drowsiness Detection with Spatial Self-Attention and Federated Learning](https://arxiv.org/abs/2508.00287)
*Tran Viet Khoa,Do Hai Son,Mohammad Abu Alsheikh,Yibeltal F Alem,Dinh Thai Hoang*

Main category: cs.CV

TL;DR: 提出了一种基于空间自注意力机制和LSTM的驾驶员疲劳检测框架，结合联邦学习和梯度相似性比较，实现了89.9%的准确率。


<details>
  <summary>Details</summary>
Motivation: 驾驶员疲劳是交通事故的主要原因之一，但在真实场景中，由于数据的分散性和多样性，准确检测疲劳仍具挑战性。

Method: 开发了空间自注意力机制（SSA）与LSTM结合的模型，并采用梯度相似性比较（GSC）优化联邦学习中的模型聚合。

Result: 在联邦学习环境下，框架的检测准确率达到89.9%，优于现有方法。

Conclusion: 该框架能有效处理真实世界数据的多样性，有望应用于智能交通系统以提高道路安全。

Abstract: Driver drowsiness is one of the main causes of road accidents and is
recognized as a leading contributor to traffic-related fatalities. However,
detecting drowsiness accurately remains a challenging task, especially in
real-world settings where facial data from different individuals is
decentralized and highly diverse. In this paper, we propose a novel framework
for drowsiness detection that is designed to work effectively with
heterogeneous and decentralized data. Our approach develops a new Spatial
Self-Attention (SSA) mechanism integrated with a Long Short-Term Memory (LSTM)
network to better extract key facial features and improve detection
performance. To support federated learning, we employ a Gradient Similarity
Comparison (GSC) that selects the most relevant trained models from different
operators before aggregation. This improves the accuracy and robustness of the
global model while preserving user privacy. We also develop a customized tool
that automatically processes video data by extracting frames, detecting and
cropping faces, and applying data augmentation techniques such as rotation,
flipping, brightness adjustment, and zooming. Experimental results show that
our framework achieves a detection accuracy of 89.9% in the federated learning
settings, outperforming existing methods under various deployment scenarios.
The results demonstrate the effectiveness of our approach in handling
real-world data variability and highlight its potential for deployment in
intelligent transportation systems to enhance road safety through early and
reliable drowsiness detection.

</details>


### [23] [TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models](https://arxiv.org/abs/2508.00289)
*Christian Simon,Masato Ishii,Akio Hayakawa,Zhi Zhong,Shusuke Takahashi,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 提出了一种名为TITAN-Guide的训练自由引导方法，用于优化文本到视频扩散模型的推理时间对齐，解决了内存占用大和控制效果不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有训练自由引导框架内存需求高或控制效果不理想，限制了在计算密集型任务（如文本到视频扩散模型）中的应用。

Method: 开发了一种无需反向传播的高效扩散潜在优化方法，研究了前向梯度下降及多种方向指令选项。

Result: 实验证明该方法能高效管理内存并显著提升文本到视频扩散模型的性能。

Conclusion: TITAN-Guide不仅减少了内存需求，还在多个扩散引导基准测试中表现出色。

Abstract: In the recent development of conditional diffusion models still require heavy
supervised fine-tuning for performing control on a category of tasks.
Training-free conditioning via guidance with off-the-shelf models is a
favorable alternative to avoid further fine-tuning on the base model. However,
the existing training-free guidance frameworks either have heavy memory
requirements or offer sub-optimal control due to rough estimation. These
shortcomings limit the applicability to control diffusion models that require
intense computation, such as Text-to-Video (T2V) diffusion models. In this
work, we propose Taming Inference Time Alignment for Guided Text-to-Video
Diffusion Model, so-called TITAN-Guide, which overcomes memory space issues,
and provides more optimal control in the guidance process compared to the
counterparts. In particular, we develop an efficient method for optimizing
diffusion latents without backpropagation from a discriminative guiding model.
In particular, we study forward gradient descents for guided diffusion tasks
with various options on directional directives. In our experiments, we
demonstrate the effectiveness of our approach in efficiently managing memory
during latent optimization, while previous methods fall short. Our proposed
approach not only minimizes memory requirements but also significantly enhances
T2V performance across a range of diffusion guidance benchmarks. Code, models,
and demo are available at https://titanguide.github.io.

</details>


### [24] [AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer](https://arxiv.org/abs/2508.00298)
*Jin Lyu,Liang An,Li Lin,Pujin Cheng,Yebin Liu,Xiaoying Tang*

Main category: cs.CV

TL;DR: AniMer+是一个扩展的统一框架，用于重建哺乳动物和鸟类的姿态与形状，通过高容量的ViT和MoE设计，结合合成数据集提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法网络容量不足和多物种数据集稀缺的问题，以支持生物研究的定量分析。

Method: 采用家族感知的ViT和MoE设计，分区网络层为物种特定和共享组件，并引入扩散模型生成合成数据集。

Result: 在41.3k哺乳动物和12.4k鸟类图像上训练，性能优于现有方法，尤其在Animal Kingdom数据集上表现突出。

Conclusion: AniMer+的网络架构和合成数据集显著提升了实际应用性能。

Abstract: In the era of foundation models, achieving a unified understanding of
different dynamic objects through a single network has the potential to empower
stronger spatial intelligence. Moreover, accurate estimation of animal pose and
shape across diverse species is essential for quantitative analysis in
biological research. However, this topic remains underexplored due to the
limited network capacity of previous methods and the scarcity of comprehensive
multi-species datasets. To address these limitations, we introduce AniMer+, an
extended version of our scalable AniMer framework. In this paper, we focus on a
unified approach for reconstructing mammals (mammalia) and birds (aves). A key
innovation of AniMer+ is its high-capacity, family-aware Vision Transformer
(ViT) incorporating a Mixture-of-Experts (MoE) design. Its architecture
partitions network layers into taxa-specific components (for mammalia and aves)
and taxa-shared components, enabling efficient learning of both distinct and
common anatomical features within a single model. To overcome the critical
shortage of 3D training data, especially for birds, we introduce a
diffusion-based conditional image generation pipeline. This pipeline produces
two large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D for
birds. To note, CtrlAVES3D is the first large-scale, 3D-annotated dataset for
birds, which is crucial for resolving single-view depth ambiguities. Trained on
an aggregated collection of 41.3k mammalian and 12.4k avian images (combining
real and synthetic data), our method demonstrates superior performance over
existing approaches across a wide range of benchmarks, including the
challenging out-of-domain Animal Kingdom dataset. Ablation studies confirm the
effectiveness of both our novel network architecture and the generated
synthetic datasets in enhancing real-world application performance.

</details>


### [25] [Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence](https://arxiv.org/abs/2508.00299)
*Danzhen Fu,Jiagao Hu,Daiguo Zhou,Fei Wang,Zepeng Wang,Wenhua Liao*

Main category: cs.CV

TL;DR: 提出了一种多视角行人视频编辑框架，通过结合视频修复和动作控制技术，增强自动驾驶训练数据的多样性。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统中行人检测模型因训练数据缺乏危险场景而鲁棒性不足的问题。

Method: 识别多视角行人区域，扩展边界框并拼接为统一画布，通过掩码和姿态序列控制实现行人编辑。

Result: 实验表明框架能高质量完成行人编辑，具有视觉真实性、时空一致性和多视角一致性。

Conclusion: 该方法为多视角行人视频生成提供了鲁棒且通用的解决方案，适用于数据增强和场景模拟。

Abstract: Pedestrian detection models in autonomous driving systems often lack
robustness due to insufficient representation of dangerous pedestrian scenarios
in training datasets. To address this limitation, we present a novel framework
for controllable pedestrian video editing in multi-view driving scenarios by
integrating video inpainting and human motion control techniques. Our approach
begins by identifying pedestrian regions of interest across multiple camera
views, expanding detection bounding boxes with a fixed ratio, and resizing and
stitching these regions into a unified canvas while preserving cross-view
spatial relationships. A binary mask is then applied to designate the editable
area, within which pedestrian editing is guided by pose sequence control
conditions. This enables flexible editing functionalities, including pedestrian
insertion, replacement, and removal. Extensive experiments demonstrate that our
framework achieves high-quality pedestrian editing with strong visual realism,
spatiotemporal coherence, and cross-view consistency. These results establish
the proposed method as a robust and versatile solution for multi-view
pedestrian video generation, with broad potential for applications in data
augmentation and scenario simulation in autonomous driving.

</details>


### [26] [Exploring Fourier Prior and Event Collaboration for Low-Light Image Enhancement](https://arxiv.org/abs/2508.00308)
*Chunyan She,Fujun Han,Chengyu Fang,Shukai Duan,Lidan Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于事件相机的低光图像增强方法，通过解耦增强流程为可见性恢复和结构细化两阶段，结合动态对齐和对比损失，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用事件相机和帧相机的模态优势，限制了性能提升。

Method: 将增强流程分为两阶段：1) 可见性恢复网络（利用傅里叶空间的振幅-相位关系）；2) 结构细化（动态对齐融合策略）。此外，引入对比损失以学习判别性表示。

Result: 实验表明，该方法优于现有最优模型。

Conclusion: 通过解耦增强流程和动态对齐策略，有效提升了低光图像增强的性能。

Abstract: The event camera, benefiting from its high dynamic range and low latency,
provides performance gain for low-light image enhancement. Unlike frame-based
cameras, it records intensity changes with extremely high temporal resolution,
capturing sufficient structure information. Currently, existing event-based
methods feed a frame and events directly into a single model without fully
exploiting modality-specific advantages, which limits their performance.
Therefore, by analyzing the role of each sensing modality, the enhancement
pipeline is decoupled into two stages: visibility restoration and structure
refinement. In the first stage, we design a visibility restoration network with
amplitude-phase entanglement by rethinking the relationship between amplitude
and phase components in Fourier space. In the second stage, a fusion strategy
with dynamic alignment is proposed to mitigate the spatial mismatch caused by
the temporal resolution discrepancy between two sensing modalities, aiming to
refine the structure information of the image enhanced by the visibility
restoration network. In addition, we utilize spatial-frequency interpolation to
simulate negative samples with diverse illumination, noise and artifact
degradations, thereby developing a contrastive loss that encourages the model
to learn discriminative representations. Experiments demonstrate that the
proposed method outperforms state-of-the-art models.

</details>


### [27] [DocTron-Formula: Generalized Formula Recognition in Complex and Structured Scenarios](https://arxiv.org/abs/2508.00311)
*Yufeng Zhong,Zhixiong Zeng,Lei Chen,Longrong Yang,Liming Zheng,Jing Huang,Siqi Yang,Lin Ma*

Main category: cs.CV

TL;DR: DocTron-Formula是一个基于通用视觉语言模型的统一框架，用于数学公式OCR，无需专用架构。结合CSFormula数据集，通过监督微调实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 数学公式OCR在科学文献智能分析中至关重要，但现有模型难以处理其结构多样性和复杂性。

Method: 提出DocTron-Formula框架，利用通用视觉语言模型，并引入CSFormula数据集进行监督微调。

Result: 方法在多种风格、科学领域和复杂布局中达到最优性能，超越专用模型。

Conclusion: DocTron-Formula为复杂科学文档的自动理解提供了新范式。

Abstract: Optical Character Recognition (OCR) for mathematical formula is essential for
the intelligent analysis of scientific literature. However, both task-specific
and general vision-language models often struggle to handle the structural
diversity, complexity, and real-world variability inherent in mathematical
content. In this work, we present DocTron-Formula, a unified framework built
upon general vision-language models, thereby eliminating the need for
specialized architectures. Furthermore, we introduce CSFormula, a large-scale
and challenging dataset that encompasses multidisciplinary and structurally
complex formulas at the line, paragraph, and page levels. Through
straightforward supervised fine-tuning, our approach achieves state-of-the-art
performance across a variety of styles, scientific domains, and complex
layouts. Experimental results demonstrate that our method not only surpasses
specialized models in terms of accuracy and robustness, but also establishes a
new paradigm for the automated understanding of complex scientific documents.

</details>


### [28] [GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.00312)
*Suhang Cai,Xiaohao Peng,Chong Wang,Xiaojie Cai,Jiangbo Qian*

Main category: cs.CV

TL;DR: 提出了一种基于生成视频的弱监督视频异常检测框架（GV-VAD），通过文本条件视频生成模型生成可控且真实的合成视频，低成本增强训练数据，并在UCF-Crime数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决视频异常检测中真实异常数据稀缺、标注成本高的问题，提升模型的性能和泛化能力。

Method: 利用文本条件视频生成模型生成合成视频，并通过合成样本损失缩放策略控制其影响。

Result: 在UCF-Crime数据集上优于现有方法。

Conclusion: GV-VAD框架通过低成本合成数据增强，有效提升了视频异常检测的性能。

Abstract: Video anomaly detection (VAD) plays a critical role in public safety
applications such as intelligent surveillance. However, the rarity,
unpredictability, and high annotation cost of real-world anomalies make it
difficult to scale VAD datasets, which limits the performance and
generalization ability of existing models. To address this challenge, we
propose a generative video-enhanced weakly-supervised video anomaly detection
(GV-VAD) framework that leverages text-conditioned video generation models to
produce semantically controllable and physically plausible synthetic videos.
These virtual videos are used to augment training data at low cost. In
addition, a synthetic sample loss scaling strategy is utilized to control the
influence of generated synthetic samples for efficient training. The
experiments show that the proposed framework outperforms state-of-the-art
methods on UCF-Crime datasets. The code is available at
https://github.com/Sumutan/GV-VAD.git.

</details>


### [29] [Steering Guidance for Personalized Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.00319)
*Sunghyun Park,Seokeon Choi,Hyoungwoo Park,Sungrack Yun*

Main category: cs.CV

TL;DR: 提出了一种新的个性化引导方法，通过未学习的弱模型和动态权重插值，平衡目标分布对齐与文本编辑能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如CFG和AG）在少量图像微调时无法有效平衡目标分布对齐和文本编辑能力。

Method: 利用未学习的弱模型和动态权重插值，在推理时控制未学习程度。

Result: 实验表明，该方法能提升文本对齐和目标分布保真度，且无需额外计算开销。

Conclusion: 提出的个性化引导方法简单有效，适用于多种微调策略。

Abstract: Personalizing text-to-image diffusion models is crucial for adapting the
pre-trained models to specific target concepts, enabling diverse image
generation. However, fine-tuning with few images introduces an inherent
trade-off between aligning with the target distribution (e.g., subject
fidelity) and preserving the broad knowledge of the original model (e.g., text
editability). Existing sampling guidance methods, such as classifier-free
guidance (CFG) and autoguidance (AG), fail to effectively guide the output
toward well-balanced space: CFG restricts the adaptation to the target
distribution, while AG compromises text alignment. To address these
limitations, we propose personalization guidance, a simple yet effective method
leveraging an unlearned weak model conditioned on a null text prompt. Moreover,
our method dynamically controls the extent of unlearning in a weak model
through weight interpolation between pre-trained and fine-tuned models during
inference. Unlike existing guidance methods, which depend solely on guidance
scales, our method explicitly steers the outputs toward a balanced latent space
without additional computational overhead. Experimental results demonstrate
that our proposed guidance can improve text alignment and target distribution
fidelity, integrating seamlessly with various fine-tuning strategies.

</details>


### [30] [Spectral Sensitivity Estimation with an Uncalibrated Diffraction Grating](https://arxiv.org/abs/2508.00330)
*Lilika Makabe,Hiroaki Santo,Fumio Okura,Michael S. Brown,Yasuyuki Matsushita*

Main category: cs.CV

TL;DR: 提出一种使用衍射光栅校准相机光谱灵敏度的实用方法，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 相机光谱灵敏度的准确校准对计算机视觉任务至关重要，但现有方法需要专用设备。

Method: 通过未校准的衍射光栅片捕获图像，闭式估计相机光谱灵敏度和光栅参数。

Result: 在合成和真实数据实验中，该方法优于传统基于参考目标的方法。

Conclusion: 该方法有效且实用，无需专用设备。

Abstract: This paper introduces a practical and accurate calibration method for camera
spectral sensitivity using a diffraction grating. Accurate calibration of
camera spectral sensitivity is crucial for various computer vision tasks,
including color correction, illumination estimation, and material analysis.
Unlike existing approaches that require specialized narrow-band filters or
reference targets with known spectral reflectances, our method only requires an
uncalibrated diffraction grating sheet, readily available off-the-shelf. By
capturing images of the direct illumination and its diffracted pattern through
the grating sheet, our method estimates both the camera spectral sensitivity
and the diffraction grating parameters in a closed-form manner. Experiments on
synthetic and real-world data demonstrate that our method outperforms
conventional reference target-based methods, underscoring its effectiveness and
practicality.

</details>


### [31] [Analyze-Prompt-Reason: A Collaborative Agent-Based Framework for Multi-Image Vision-Language Reasoning](https://arxiv.org/abs/2508.00356)
*Angelos Vlachos,Giorgos Filandrianos,Maria Lymperaiou,Nikolaos Spanos,Ilias Mitsouras,Vasileios Karampinis,Athanasios Voulodimos*

Main category: cs.CV

TL;DR: 提出了一种基于双代理的协作框架，用于多图像推理，通过语言提示和视觉推理模型实现自动化、模块化且无需训练的多任务泛化。


<details>
  <summary>Details</summary>
Motivation: 解决跨多样数据集和任务格式的多模态推理挑战。

Method: 采用双代理系统：PromptEngineer生成任务特定提示，VisionReasoner（大型视觉语言模型）进行最终推理。

Result: 在18个数据集上评估，Claude 3.7在TQA、DocVQA和MMCoQA等任务中表现优异。

Conclusion: 大型视觉语言模型在信息提示引导下可有效进行多图像推理，设计选择对性能有显著影响。

Abstract: We present a Collaborative Agent-Based Framework for Multi-Image Reasoning.
Our approach tackles the challenge of interleaved multimodal reasoning across
diverse datasets and task formats by employing a dual-agent system: a
language-based PromptEngineer, which generates context-aware, task-specific
prompts, and a VisionReasoner, a large vision-language model (LVLM) responsible
for final inference. The framework is fully automated, modular, and
training-free, enabling generalization across classification, question
answering, and free-form generation tasks involving one or multiple input
images. We evaluate our method on 18 diverse datasets from the 2025 MIRAGE
Challenge (Track A), covering a broad spectrum of visual reasoning tasks
including document QA, visual comparison, dialogue-based understanding, and
scene-level inference. Our results demonstrate that LVLMs can effectively
reason over multiple images when guided by informative prompts. Notably, Claude
3.7 achieves near-ceiling performance on challenging tasks such as TQA (99.13%
accuracy), DocVQA (96.87%), and MMCoQA (75.28 ROUGE-L). We also explore how
design choices-such as model selection, shot count, and input length-influence
the reasoning performance of different LVLMs.

</details>


### [32] [Stable at Any Speed: Speed-Driven Multi-Object Tracking with Learnable Kalman Filtering](https://arxiv.org/abs/2508.00358)
*Yan Gong,Mengjun Chen,Hao Liu,Gao Yongsheng,Lei Yang,Naibang Wang,Ziying Song,Haoqun Ma*

Main category: cs.CV

TL;DR: 论文提出了一种速度引导的可学习卡尔曼滤波器（SG-LKF），通过动态调整不确定性建模以适应车辆速度，显著提高了动态场景中的跟踪稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统多目标跟踪方法依赖静态坐标变换，忽略了车辆速度对观测噪声和参考帧变化的影响，导致高速动态场景中跟踪性能下降。

Method: 提出了SG-LKF和MotionScaleNet（MSNet），通过自监督轨迹一致性损失增强帧间关联和轨迹连续性。

Result: SG-LKF在KITTI 2D MOT上以79.59% HOTA排名第一，在KITTI 3D MOT上达到82.03% HOTA，并在nuScenes 3D MOT上优于SimpleTrack 2.2% AMOTA。

Conclusion: SG-LKF通过动态适应车辆速度，显著提升了多目标跟踪在动态场景中的性能。

Abstract: Multi-object tracking (MOT) enables autonomous vehicles to continuously
perceive dynamic objects, supplying essential temporal cues for prediction,
behavior understanding, and safe planning. However, conventional
tracking-by-detection methods typically rely on static coordinate
transformations based on ego-vehicle poses, disregarding ego-vehicle
speed-induced variations in observation noise and reference frame changes,
which degrades tracking stability and accuracy in dynamic, high-speed
scenarios. In this paper, we investigate the critical role of ego-vehicle speed
in MOT and propose a Speed-Guided Learnable Kalman Filter (SG-LKF) that
dynamically adapts uncertainty modeling to ego-vehicle speed, significantly
improving stability and accuracy in highly dynamic scenarios. Central to SG-LKF
is MotionScaleNet (MSNet), a decoupled token-mixing and channel-mixing MLP that
adaptively predicts key parameters of SG-LKF. To enhance inter-frame
association and trajectory continuity, we introduce a self-supervised
trajectory consistency loss jointly optimized with semantic and positional
constraints. Extensive experiments show that SG-LKF ranks first among all
vision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results
on KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on
nuScenes 3D MOT.

</details>


### [33] [CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective](https://arxiv.org/abs/2508.00359)
*Zongheng Tang,Yi Liu,Yifan Sun,Yulu Gao,Jinyu Chen,Runsheng Xu,Si Liu*

Main category: cs.CV

TL;DR: 本文提出了一种高效的协作感知方法CoST，通过统一时空空间同时聚合多智能体和多时间观测，提升了传输效率和感知性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法中多智能体和多时间融合分离导致的效率低下和性能不足问题。

Method: 提出协作感知时空变换器（CoST），统一时空空间进行特征传输和融合。

Result: CoST在效率和准确性上均有提升，且兼容多数现有方法。

Conclusion: CoST通过统一时空融合，显著提升了协作感知的效率和性能。

Abstract: Collaborative perception shares information among different agents and helps
solving problems that individual agents may face, e.g., occlusions and small
sensing range. Prior methods usually separate the multi-agent fusion and
multi-time fusion into two consecutive steps. In contrast, this paper proposes
an efficient collaborative perception that aggregates the observations from
different agents (space) and different times into a unified spatio-temporal
space simultanesouly. The unified spatio-temporal space brings two benefits,
i.e., efficient feature transmission and superior feature fusion. 1) Efficient
feature transmission: each static object yields a single observation in the
spatial temporal space, and thus only requires transmission only once (whereas
prior methods re-transmit all the object features multiple times). 2) superior
feature fusion: merging the multi-agent and multi-time fusion into a unified
spatial-temporal aggregation enables a more holistic perspective, thereby
enhancing perception performance in challenging scenarios. Consequently, our
Collaborative perception with Spatio-temporal Transformer (CoST) gains
improvement in both efficiency and accuracy. Notably, CoST is not tied to any
specific method and is compatible with a majority of previous methods,
enhancing their accuracy while reducing the transmission bandwidth.

</details>


### [34] [Honey Classification using Hyperspectral Imaging and Machine Learning](https://arxiv.org/abs/2508.00361)
*Mokhtar A. Al-Awadhi,Ratnadeep R. Deshmukh*

Main category: cs.CV

TL;DR: 提出了一种基于机器学习的蜂蜜植物来源自动分类方法，包括数据准备、特征提取和分类三个主要步骤，使用LDA和SVM/KNN模型，在标准数据集上取得了最高95.13%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 解决蜂蜜植物来源自动分类的问题，提高分类准确性和效率。

Method: 1. 数据准备阶段使用类转换方法增强类间可分性；2. 特征提取阶段采用LDA技术降维；3. 分类阶段使用SVM和KNN模型。

Result: 在标准HSI数据集上，分类准确率达到95.13%（图像分类）和92.80%（实例分类）。

Conclusion: 该方法在蜂蜜植物来源分类任务中表现出色，达到了最先进的性能。

Abstract: In this paper, we propose a machine learning-based method for automatically
classifying honey botanical origins. Dataset preparation, feature extraction,
and classification are the three main steps of the proposed method. We use a
class transformation method in the dataset preparation phase to maximize the
separability across classes. The feature extraction phase employs the Linear
Discriminant Analysis (LDA) technique for extracting relevant features and
reducing the number of dimensions. In the classification phase, we use Support
Vector Machines (SVM) and K-Nearest Neighbors (KNN) models to classify the
extracted features of honey samples into their botanical origins. We evaluate
our system using a standard honey hyperspectral imaging (HSI) dataset.
Experimental findings demonstrate that the proposed system produces
state-of-the-art results on this dataset, achieving the highest classification
accuracy of 95.13% for hyperspectral image-based classification and 92.80% for
hyperspectral instance-based classification.

</details>


### [35] [SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies](https://arxiv.org/abs/2508.00366)
*Liang Han,Xu Zhang,Haichuan Song,Kanle Shi,Yu-Shen Liu,Zhizhong Han*

Main category: cs.CV

TL;DR: SparseRecon提出了一种新的稀疏视图表面重建方法，结合了体积渲染特征一致性和不确定性引导的深度约束，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在稀疏视图下表现不佳：基于泛化的方法对未见视图泛化能力差，基于过拟合的方法受限于几何线索不足。

Method: 提出特征一致性损失和不确定性引导深度约束，前者确保重建完整性和平滑性，后者在遮挡和特征不明显区域补充几何细节。

Result: 实验表明，SparseRecon在稀疏视图输入下优于现有方法，尤其在小重叠视图场景中表现突出。

Conclusion: SparseRecon通过结合两种约束，有效解决了稀疏视图重建的挑战，显著提升了重建质量。

Abstract: Surface reconstruction from sparse views aims to reconstruct a 3D shape or
scene from few RGB images. The latest methods are either generalization-based
or overfitting-based. However, the generalization-based methods do not
generalize well on views that were unseen during training, while the
reconstruction quality of overfitting-based methods is still limited by the
limited geometry clues. To address this issue, we propose SparseRecon, a novel
neural implicit reconstruction method for sparse views with volume
rendering-based feature consistency and uncertainty-guided depth constraint.
Firstly, we introduce a feature consistency loss across views to constrain the
neural implicit field. This design alleviates the ambiguity caused by
insufficient consistency information of views and ensures completeness and
smoothness in the reconstruction results. Secondly, we employ an
uncertainty-guided depth constraint to back up the feature consistency loss in
areas with occlusion and insignificant features, which recovers geometry
details for better reconstruction quality. Experimental results demonstrate
that our method outperforms the state-of-the-art methods, which can produce
high-quality geometry with sparse-view input, especially in the scenarios with
small overlapping views. Project page: https://hanl2010.github.io/SparseRecon/.

</details>


### [36] [Representation Shift: Unifying Token Compression with FlashAttention](https://arxiv.org/abs/2508.00367)
*Joonmyung Choi,Sanghyeok Lee,Byungoh Ko,Eunseo Kim,Jihyung Kil,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: 提出了一种名为Representation Shift的训练无关、模型无关的度量方法，用于测量每个token表示的变化程度，从而在不依赖注意力图的情况下实现token压缩，与FlashAttention兼容。


<details>
  <summary>Details</summary>
Motivation: 随着任务复杂度的增加，Transformer模型的计算成本（尤其是自注意力的二次成本）和GPU内存访问开销显著增加，现有token压缩方法因依赖注意力图而与FlashAttention不兼容。

Method: 提出Representation Shift度量方法，通过测量token表示的变化程度来实现token压缩，无需注意力图或重新训练。

Result: 实验表明，该方法与FlashAttention兼容，在视频-文本检索和视频问答任务中分别实现了5.5%和4.4%的显著加速。

Conclusion: Representation Shift是一种高效且通用的token压缩方法，适用于多种模型（如Transformer、CNN和状态空间模型），并能与FlashAttention无缝集成。

Abstract: Transformers have demonstrated remarkable success across vision, language,
and video. Yet, increasing task complexity has led to larger models and more
tokens, raising the quadratic cost of self-attention and the overhead of GPU
memory access. To reduce the computation cost of self-attention, prior work has
proposed token compression techniques that drop redundant or less informative
tokens. Meanwhile, fused attention kernels such as FlashAttention have been
developed to alleviate memory overhead by avoiding attention map construction
and its associated I/O to HBM. This, however, makes it incompatible with most
training-free token compression methods, which rely on attention maps to
determine token importance. Here, we propose Representation Shift, a
training-free, model-agnostic metric that measures the degree of change in each
token's representation. This seamlessly integrates token compression with
FlashAttention, without attention maps or retraining. Our method further
generalizes beyond Transformers to CNNs and state space models. Extensive
experiments show that Representation Shift enables effective token compression
compatible with FlashAttention, yielding significant speedups of up to 5.5% and
4.4% in video-text retrieval and video QA, respectively. Code is available at
https://github.com/mlvlab/Representation-Shift.

</details>


### [37] [Bidirectional Action Sequence Learning for Long-term Action Anticipation with Large Language Models](https://arxiv.org/abs/2508.00374)
*Yuji Sato,Yasunori Ishii,Takayoshi Yamashita*

Main category: cs.CV

TL;DR: BiAnt方法通过结合前向和后向预测，利用大语言模型改进视频长期动作预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法因单向性限制性能，难以捕捉场景中语义不同的子动作。

Method: BiAnt结合前向和后向预测，使用大语言模型。

Result: 在Ego4D数据集上，BiAnt在编辑距离上优于基线方法。

Conclusion: BiAnt通过双向预测提升了长期动作预测的性能。

Abstract: Video-based long-term action anticipation is crucial for early risk detection
in areas such as automated driving and robotics. Conventional approaches
extract features from past actions using encoders and predict future events
with decoders, which limits performance due to their unidirectional nature.
These methods struggle to capture semantically distinct sub-actions within a
scene. The proposed method, BiAnt, addresses this limitation by combining
forward prediction with backward prediction using a large language model.
Experimental results on Ego4D demonstrate that BiAnt improves performance in
terms of edit distance compared to baseline methods.

</details>


### [38] [Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis](https://arxiv.org/abs/2508.00381)
*Kamal Basha S,Athira Nambiar*

Main category: cs.CV

TL;DR: 本文提出Adapt-WeldNet框架，结合预训练架构、迁移学习和自适应优化器，提升焊接缺陷检测性能，并引入DDIA框架增强系统透明度和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统无损检测方法难以检测细微或内部缺陷，现有神经网络方法缺乏可解释性，影响安全性。

Method: Adapt-WeldNet系统评估预训练架构和优化策略；DDIA框架结合XAI技术和专家验证。

Result: 优化了缺陷检测性能，并通过DDIA提升了系统的透明度和可信度。

Conclusion: 该研究提升了焊接缺陷检测系统的性能与可解释性，增强了在海洋和离岸环境中的安全性和可靠性。

Abstract: Weld defect detection is crucial for ensuring the safety and reliability of
piping systems in the oil and gas industry, especially in challenging marine
and offshore environments. Traditional non-destructive testing (NDT) methods
often fail to detect subtle or internal defects, leading to potential failures
and costly downtime. Furthermore, existing neural network-based approaches for
defect classification frequently rely on arbitrarily selected pretrained
architectures and lack interpretability, raising safety concerns for
deployment. To address these challenges, this paper introduces
``Adapt-WeldNet", an adaptive framework for welding defect detection that
systematically evaluates various pre-trained architectures, transfer learning
strategies, and adaptive optimizers to identify the best-performing model and
hyperparameters, optimizing defect detection and providing actionable insights.
Additionally, a novel Defect Detection Interpretability Analysis (DDIA)
framework is proposed to enhance system transparency. DDIA employs Explainable
AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific
evaluations validated by certified ASNT NDE Level II professionals.
Incorporating a Human-in-the-Loop (HITL) approach and aligning with the
principles of Trustworthy AI, DDIA ensures the reliability, fairness, and
accountability of the defect detection system, fostering confidence in
automated decisions through expert validation. By improving both performance
and interpretability, this work enhances trust, safety, and reliability in
welding defect detection systems, supporting critical operations in offshore
and marine environments.

</details>


### [39] [$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models](https://arxiv.org/abs/2508.00383)
*Won June Cho,Hongjun Yoon,Daeky Jeong,Hyeongyeol Lim,Yosep Chong*

Main category: cs.CV

TL;DR: 论文提出了一种混合架构$MV_{Hybrid}$，结合状态空间模型（SSMs）和ViT，用于从病理图像预测空间基因表达，性能优于现有ViT模型。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学成本高且技术复杂，限制了临床应用。从常规病理图像预测基因表达是一个实用替代方案，但现有ViT模型性能不足。

Method: 提出$MV_{Hybrid}$架构，结合SSMs和ViT，利用负实特征值初始化以增强低频模式捕捉能力。

Result: 在LOSO评估中，$MV_{Hybrid}$比最佳ViT模型相关性提高57%，性能下降减少43%，且在下游任务中表现优异。

Conclusion: $MV_{Hybrid}$是一种有前景的下一代病理视觉基础模型架构。

Abstract: Spatial transcriptomics reveals gene expression patterns within tissue
context, enabling precision oncology applications such as treatment response
prediction, but its high cost and technical complexity limit clinical adoption.
Predicting spatial gene expression (biomarkers) from routine histopathology
images offers a practical alternative, yet current vision foundation models
(VFMs) in pathology based on Vision Transformer (ViT) backbones perform below
clinical standards. Given that VFMs are already trained on millions of diverse
whole slide images, we hypothesize that architectural innovations beyond ViTs
may better capture the low-frequency, subtle morphological patterns correlating
with molecular phenotypes. By demonstrating that state space models initialized
with negative real eigenvalues exhibit strong low-frequency bias, we introduce
$MV_{Hybrid}$, a hybrid backbone architecture combining state space models
(SSMs) with ViT. We compare five other different backbone architectures for
pathology VFMs, all pretrained on identical colorectal cancer datasets using
the DINOv2 self-supervised learning method. We evaluate all pretrained models
using both random split and leave-one-study-out (LOSO) settings of the same
biomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher
correlation than the best-performing ViT and shows 43% smaller performance
degradation compared to random split in gene expression prediction,
demonstrating superior performance and robustness, respectively. Furthermore,
$MV_{Hybrid}$ shows equal or better downstream performance in classification,
patch retrieval, and survival prediction tasks compared to that of ViT, showing
its promise as a next-generation pathology VFM backbone. Our code is publicly
available at: https://github.com/deepnoid-ai/MVHybrid.

</details>


### [40] [Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech Recognition](https://arxiv.org/abs/2508.00391)
*Guanjie Huang,Danny H. K. Tsang,Shan Yang,Guangzhi Lei,Li Liu*

Main category: cs.CV

TL;DR: 提出了一种名为Cued-Agent的多智能体系统，用于自动识别Cued Speech（CS），通过四个子智能体分别处理手势、唇部特征、动态融合和语义转换，显著提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法因数据有限和模态异步问题导致多模态融合效果不佳，多智能体系统为解决这一问题提供了新思路。

Method: 设计了四个子智能体：手势识别、唇部特征提取、动态提示融合和语义转换，通过协作完成CS识别任务。

Result: 实验表明，Cued-Agent在正常和听力障碍场景下均优于现有方法。

Conclusion: Cued-Agent为CS识别提供了一种高效的多智能体解决方案，尤其在数据有限的情况下表现优异。

Abstract: Cued Speech (CS) is a visual communication system that combines lip-reading
with hand coding to facilitate communication for individuals with hearing
impairments. Automatic CS Recognition (ACSR) aims to convert CS hand gestures
and lip movements into text via AI-driven methods. Traditionally, the temporal
asynchrony between hand and lip movements requires the design of complex
modules to facilitate effective multimodal fusion. However, constrained by
limited data availability, current methods demonstrate insufficient capacity
for adequately training these fusion mechanisms, resulting in suboptimal
performance. Recently, multi-agent systems have shown promising capabilities in
handling complex tasks with limited data availability. To this end, we propose
the first collaborative multi-agent system for ACSR, named Cued-Agent. It
integrates four specialized sub-agents: a Multimodal Large Language Model-based
Hand Recognition agent that employs keyframe screening and CS expert prompt
strategies to decode hand movements, a pretrained Transformer-based Lip
Recognition agent that extracts lip features from the input video, a Hand
Prompt Decoding agent that dynamically integrates hand prompts with lip
features during inference in a training-free manner, and a Self-Correction
Phoneme-to-Word agent that enables post-process and end-to-end conversion from
phoneme sequences to natural language sentences for the first time through
semantic refinement. To support this study, we expand the existing Mandarin CS
dataset by collecting data from eight hearing-impaired cuers, establishing a
mixed dataset of fourteen subjects. Extensive experiments demonstrate that our
Cued-Agent performs superbly in both normal and hearing-impaired scenarios
compared with state-of-the-art methods. The implementation is available at
https://github.com/DennisHgj/Cued-Agent.

</details>


### [41] [Decouple before Align: Visual Disentanglement Enhances Prompt Tuning](https://arxiv.org/abs/2508.00395)
*Fei Zhang,Tianfei Zhou,Jiangchao Yao,Ya Zhang,Ivor W. Tsang,Yanfeng Wang*

Main category: cs.CV

TL;DR: 论文提出DAPT框架，通过解耦视觉模态为前景和背景，并分别对齐文本模态，解决了提示调优中的信息不对称问题，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决提示调优（PT）中视觉与文本模态信息不对称的问题，避免模型仅关注上下文区域导致的注意力偏差。

Method: 提出DAPT框架，包括视觉模态解耦为前景和背景，分别与文本对齐，并引入视觉拉推正则化以增强视觉专注。

Result: 在少样本学习、基础到新类泛化和数据高效学习中表现优异，优于现有基准。

Conclusion: DAPT通过解耦和对齐模态，有效解决了信息不对称问题，提升了模型性能，代码将开源。

Abstract: Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm,
has showcased remarkable effectiveness in improving the task-specific
transferability of vision-language models. This paper delves into a previously
overlooked information asymmetry issue in PT, where the visual modality mostly
conveys more context than the object-oriented textual modality.
Correspondingly, coarsely aligning these two modalities could result in the
biased attention, driving the model to merely focus on the context area. To
address this, we propose DAPT, an effective PT framework based on an intuitive
decouple-before-align concept. First, we propose to explicitly decouple the
visual modality into the foreground and background representation via
exploiting coarse-and-fine visual segmenting cues, and then both of these
decoupled patterns are aligned with the original foreground texts and the
hand-crafted background classes, thereby symmetrically strengthening the modal
alignment. To further enhance the visual concentration, we propose a visual
pull-push regularization tailored for the foreground-background patterns,
directing the original visual representation towards unbiased attention on the
region-of-interest object. We demonstrate the power of architecture-free DAPT
through few-shot learning, base-to-novel generalization, and data-efficient
learning, all of which yield superior performance across prevailing benchmarks.
Our code will be released at https://github.com/Ferenas/DAPT.

</details>


### [42] [Video Forgery Detection with Optical Flow Residuals and Spatial-Temporal Consistency](https://arxiv.org/abs/2508.00397)
*Xi Xue,Kunio Suzuki,Nabarun Goswami,Takuya Shintate*

Main category: cs.CV

TL;DR: 提出了一种基于空间-时间一致性的视频伪造检测框架，结合RGB外观特征和光流残差，通过双分支架构检测AI生成视频中的伪造痕迹。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的视频越来越逼真，现有方法难以捕捉时间上的细微不一致性，尤其是视觉保真度高且运动连贯的AI生成视频。

Method: 采用双分支架构，一支分析RGB帧检测外观伪影，另一支处理光流残差揭示时间合成不完美导致的运动异常。

Result: 在多种生成模型的文本到视频和图像到视频任务上表现出鲁棒性和强泛化能力。

Conclusion: 通过结合互补特征，该方法能有效检测多种伪造视频。

Abstract: The rapid advancement of diffusion-based video generation models has led to
increasingly realistic synthetic content, presenting new challenges for video
forgery detection. Existing methods often struggle to capture fine-grained
temporal inconsistencies, particularly in AI-generated videos with high visual
fidelity and coherent motion. In this work, we propose a detection framework
that leverages spatial-temporal consistency by combining RGB appearance
features with optical flow residuals. The model adopts a dual-branch
architecture, where one branch analyzes RGB frames to detect appearance-level
artifacts, while the other processes flow residuals to reveal subtle motion
anomalies caused by imperfect temporal synthesis. By integrating these
complementary features, the proposed method effectively detects a wide range of
forged videos. Extensive experiments on text-to-video and image-to-video tasks
across ten diverse generative models demonstrate the robustness and strong
generalization ability of the proposed approach.

</details>


### [43] [iSafetyBench: A video-language benchmark for safety in industrial environment](https://arxiv.org/abs/2508.00399)
*Raiyaan Abdullah,Yogesh Singh Rawat,Shruti Vyas*

Main category: cs.CV

TL;DR: iSafetyBench是一个新的视频语言基准测试，用于评估视觉语言模型在工业环境中的表现，特别是在常规和危险场景下的识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在高风险工业领域的表现尚未充分探索，特别是在识别常规操作和安全关键异常方面。

Method: 构建了iSafetyBench基准测试，包含1,100个工业场景视频片段，标注了多标签动作类别，并设计了多选问题用于评估。

Result: 评估了8种先进视频语言模型，发现它们在危险活动识别和多标签场景中表现不佳。

Conclusion: iSafetyBench揭示了现有模型的局限性，强调了开发更鲁棒、安全感知的多模态模型的必要性。

Abstract: Recent advances in vision-language models (VLMs) have enabled impressive
generalization across diverse video understanding tasks under zero-shot
settings. However, their capabilities in high-stakes industrial domains-where
recognizing both routine operations and safety-critical anomalies is
essential-remain largely underexplored. To address this gap, we introduce
iSafetyBench, a new video-language benchmark specifically designed to evaluate
model performance in industrial environments across both normal and hazardous
scenarios. iSafetyBench comprises 1,100 video clips sourced from real-world
industrial settings, annotated with open-vocabulary, multi-label action tags
spanning 98 routine and 67 hazardous action categories. Each clip is paired
with multiple-choice questions for both single-label and multi-label
evaluation, enabling fine-grained assessment of VLMs in both standard and
safety-critical contexts. We evaluate eight state-of-the-art video-language
models under zero-shot conditions. Despite their strong performance on existing
video benchmarks, these models struggle with iSafetyBench-particularly in
recognizing hazardous activities and in multi-label scenarios. Our results
reveal significant performance gaps, underscoring the need for more robust,
safety-aware multimodal models for industrial applications. iSafetyBench
provides a first-of-its-kind testbed to drive progress in this direction. The
dataset is available at: https://github.com/raiyaan-abdullah/iSafety-Bench.

</details>


### [44] [Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents](https://arxiv.org/abs/2508.00400)
*Janika Deborah Gajo,Gerarld Paul Merales,Jerome Escarcha,Brenden Ashley Molina,Gian Nartea,Emmanuel G. Maminta,Juan Carlos Roldan,Rowel O. Atienza*

Main category: cs.CV

TL;DR: Sari Sandbox是一个高保真、逼真的3D零售店模拟环境，用于评估具身代理在购物任务中的表现，并与人类表现对比。


<details>
  <summary>Details</summary>
Motivation: 填补零售领域具身代理训练模拟环境的空白，提供更真实的测试平台。

Method: 开发了包含250多种交互式商品的3D零售店模拟环境，支持VR和VLM驱动的具身代理，并提供了SariBench数据集。

Result: 提供了具身代理在导航、检查和操作商品方面的基线表现，并与人类表现对比。

Conclusion: 通过基准测试和性能分析，提出了提升真实性和可扩展性的建议，并开源了代码。

Abstract: We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store
simulation for benchmarking embodied agents against human performance in
shopping tasks. Addressing a gap in retail-specific sim environments for
embodied agent training, Sari Sandbox features over 250 interactive grocery
items across three store configurations, controlled via an API. It supports
both virtual reality (VR) for human interaction and a vision language model
(VLM)-powered embodied agent. We also introduce SariBench, a dataset of
annotated human demonstrations across varied task difficulties. Our sandbox
enables embodied agents to navigate, inspect, and manipulate retail items,
providing baselines against human performance. We conclude with benchmarks,
performance analysis, and recommendations for enhancing realism and
scalability. The source code can be accessed via
https://github.com/upeee/sari-sandbox-env.

</details>


### [45] [PMR: Physical Model-Driven Multi-Stage Restoration of Turbulent Dynamic Videos](https://arxiv.org/abs/2508.00406)
*Tao Wu,Jingyuan Ye,Ying Fu*

Main category: cs.CV

TL;DR: 论文提出了一种动态效率指数（DEI）和多阶段视频恢复框架（PMR），用于解决大气湍流导致的视频质量下降问题，尤其在强湍流和复杂动态场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 大气湍流引起的几何畸变和模糊会降低远距离动态场景视频的质量，现有方法难以恢复边缘细节和消除混合失真。

Method: 提出DEI量化视频动态强度，并开发PMR框架，包括去倾斜、动态区域增强和去模糊三个阶段，采用轻量级主干网络和分阶段联合训练。

Result: 实验表明，该方法能有效抑制运动拖尾伪影、恢复边缘细节，并在高湍流和复杂动态场景中表现出强泛化能力。

Conclusion: PMR框架在恢复湍流视频质量方面表现出色，代码和数据集将公开。

Abstract: Geometric distortions and blurring caused by atmospheric turbulence degrade
the quality of long-range dynamic scene videos. Existing methods struggle with
restoring edge details and eliminating mixed distortions, especially under
conditions of strong turbulence and complex dynamics. To address these
challenges, we introduce a Dynamic Efficiency Index ($DEI$), which combines
turbulence intensity, optical flow, and proportions of dynamic regions to
accurately quantify video dynamic intensity under varying turbulence conditions
and provide a high-dynamic turbulence training dataset. Additionally, we
propose a Physical Model-Driven Multi-Stage Video Restoration ($PMR$) framework
that consists of three stages: \textbf{de-tilting} for geometric stabilization,
\textbf{motion segmentation enhancement} for dynamic region refinement, and
\textbf{de-blurring} for quality restoration. $PMR$ employs lightweight
backbones and stage-wise joint training to ensure both efficiency and high
restoration quality. Experimental results demonstrate that the proposed method
effectively suppresses motion trailing artifacts, restores edge details and
exhibits strong generalization capability, especially in real-world scenarios
characterized by high-turbulence and complex dynamics. We will make the code
and datasets openly available.

</details>


### [46] [Sortblock: Similarity-Aware Feature Reuse for Diffusion Model](https://arxiv.org/abs/2508.00412)
*Hanqi Chen,Xu Zhang,Xiaoliu Guan,Lielin Jiang,Guanzhong Wang,Zeyu Chen,Yi Liu*

Main category: cs.CV

TL;DR: Sortblock是一种无需训练的推理加速框架，通过动态缓存块级特征和选择性跳过冗余计算，显著提升DiTs的推理速度，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers（DiTs）的序列去噪过程导致高推理延迟，限制了实时应用。现有方法忽略了去噪阶段和Transformer块的语义变化。

Method: 提出Sortblock框架，动态缓存相邻时间步的块级特征，通过残差演化排序自适应确定重计算比例，并引入轻量级线性预测减少误差。

Result: 实验表明，Sortblock在多种任务和DiT架构中实现超过2倍的推理加速，且输出质量下降极小。

Conclusion: Sortblock为扩散生成模型提供了一种高效且通用的加速解决方案。

Abstract: Diffusion Transformers (DiTs) have demonstrated remarkable generative
capabilities, particularly benefiting from Transformer architectures that
enhance visual and artistic fidelity. However, their inherently sequential
denoising process results in high inference latency, limiting their deployment
in real-time scenarios. Existing training-free acceleration approaches
typically reuse intermediate features at fixed timesteps or layers, overlooking
the evolving semantic focus across denoising stages and Transformer blocks.To
address this, we propose Sortblock, a training-free inference acceleration
framework that dynamically caches block-wise features based on their similarity
across adjacent timesteps. By ranking the evolution of residuals, Sortblock
adaptively determines a recomputation ratio, selectively skipping redundant
computations while preserving generation quality. Furthermore, we incorporate a
lightweight linear prediction mechanism to reduce accumulated errors in skipped
blocks.Extensive experiments across various tasks and DiT architectures
demonstrate that Sortblock achieves over 2$\times$ inference speedup with
minimal degradation in output quality, offering an effective and generalizable
solution for accelerating diffusion-based generative models.

</details>


### [47] [DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space](https://arxiv.org/abs/2508.00413)
*Junyu Chen,Dongyun Zou,Wenkun He,Junsong Chen,Enze Xie,Song Han,Han Cai*

Main category: cs.CV

TL;DR: DC-AE 1.5是一种新型深度压缩自编码器家族，通过结构化潜在空间和增强扩散训练策略，解决了高分辨率扩散模型中潜在通道增加导致的收敛慢和生成质量下降问题。


<details>
  <summary>Details</summary>
Motivation: 高分辨率扩散模型中，增加自编码器的潜在通道数虽能提升重建质量，但会导致扩散模型收敛变慢，限制生成质量上限。

Method: 提出两种创新：1) 结构化潜在空间，通过训练在潜在空间中实现通道级结构；2) 增强扩散训练，通过额外扩散训练目标加速收敛。

Result: DC-AE 1.5在ImageNet 512x512上比DC-AE生成质量更高且快4倍。

Conclusion: DC-AE 1.5通过结构化潜在空间和增强训练策略，显著提升了高分辨率扩散模型的性能和效率。

Abstract: We present DC-AE 1.5, a new family of deep compression autoencoders for
high-resolution diffusion models. Increasing the autoencoder's latent channel
number is a highly effective approach for improving its reconstruction quality.
However, it results in slow convergence for diffusion models, leading to poorer
generation quality despite better reconstruction quality. This issue limits the
quality upper bound of latent diffusion models and hinders the employment of
autoencoders with higher spatial compression ratios. We introduce two key
innovations to address this challenge: i) Structured Latent Space, a
training-based approach to impose a desired channel-wise structure on the
latent space with front latent channels capturing object structures and latter
latent channels capturing image details; ii) Augmented Diffusion Training, an
augmented diffusion training strategy with additional diffusion training
objectives on object latent channels to accelerate convergence. With these
techniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling
results than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better
image generation quality than DC-AE-f32c32 while being 4x faster. Code:
https://github.com/dc-ai-projects/DC-Gen.

</details>


### [48] [UIS-Mamba: Exploring Mamba for Underwater Instance Segmentation via Dynamic Tree Scan and Hidden State Weaken](https://arxiv.org/abs/2508.00421)
*Runmin Cong,Zongji Yu,Hao Fang,Haoyan Sun,Sam Kwong*

Main category: cs.CV

TL;DR: 提出了一种基于Mamba的水下实例分割模型UIS-Mamba，通过动态树扫描和隐藏状态削弱模块解决水下场景中的挑战。


<details>
  <summary>Details</summary>
Motivation: 水下实例分割任务面临颜色失真和边界模糊等挑战，现有固定补丁扫描机制难以保持实例内部连续性。

Method: 设计了动态树扫描（DTS）模块和隐藏状态削弱（HSW）模块，分别用于动态调整补丁和抑制背景干扰。

Result: 在UIIS和USIS10K数据集上实现了最先进的性能，同时保持了较低的参数和计算复杂度。

Conclusion: UIS-Mamba成功将Mamba迁移至水下任务，为解决水下实例分割问题提供了有效方案。

Abstract: Underwater Instance Segmentation (UIS) tasks are crucial for underwater
complex scene detection. Mamba, as an emerging state space model with
inherently linear complexity and global receptive fields, is highly suitable
for processing image segmentation tasks with long sequence features. However,
due to the particularity of underwater scenes, there are many challenges in
applying Mamba to UIS. The existing fixed-patch scanning mechanism cannot
maintain the internal continuity of scanned instances in the presence of
severely underwater color distortion and blurred instance boundaries, and the
hidden state of the complex underwater background can also inhibit the
understanding of instance objects. In this work, we propose the first
Mamba-based underwater instance segmentation model UIS-Mamba, and design two
innovative modules, Dynamic Tree Scan (DTS) and Hidden State Weaken (HSW), to
migrate Mamba to the underwater task. DTS module maintains the continuity of
the internal features of the instance objects by allowing the patches to
dynamically offset and scale, thereby guiding the minimum spanning tree and
providing dynamic local receptive fields. HSW module suppresses the
interference of complex backgrounds and effectively focuses the information
flow of state propagation to the instances themselves through the Ncut-based
hidden state weakening mechanism. Experimental results show that UIS-Mamba
achieves state-of-the-art performance on both UIIS and USIS10K datasets, while
maintaining a low number of parameters and computational complexity. Code is
available at https://github.com/Maricalce/UIS-Mamba.

</details>


### [49] [Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting](https://arxiv.org/abs/2508.00427)
*Seunggeun Chi,Enna Sachdeva,Pin-Hao Huang,Kwonjoon Lee*

Main category: cs.CV

TL;DR: 提出了一种结合物理先验知识和多区域修复技术的方法，用于动态场景中的人-物交互（HOI）的模态补全，显著提升了生成结果的准确性和真实性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如预训练扩散模型）在动态场景中生成合理的模态补全效果有限，主要因为对人-物交互的理解不足。

Method: 利用物理约束（如人体拓扑和接触信息）定义主次区域，并在扩散模型中采用定制化的去噪策略进行多区域修复。

Result: 实验表明，该方法在HOI场景中显著优于现有方法，且无需真实接触标注也能保持鲁棒性。

Conclusion: 该方法提升了机器对动态环境的感知能力，适用于3D重建和新视角/姿态合成等任务。

Abstract: Amodal completion, which is the process of inferring the full appearance of
objects despite partial occlusions, is crucial for understanding complex
human-object interactions (HOI) in computer vision and robotics. Existing
methods, such as those that use pre-trained diffusion models, often struggle to
generate plausible completions in dynamic scenarios because they have a limited
understanding of HOI. To solve this problem, we've developed a new approach
that uses physical prior knowledge along with a specialized multi-regional
inpainting technique designed for HOI. By incorporating physical constraints
from human topology and contact information, we define two distinct regions:
the primary region, where occluded object parts are most likely to be, and the
secondary region, where occlusions are less probable. Our multi-regional
inpainting method uses customized denoising strategies across these regions
within a diffusion model. This improves the accuracy and realism of the
generated completions in both their shape and visual detail. Our experimental
results show that our approach significantly outperforms existing methods in
HOI scenarios, moving machine perception closer to a more human-like
understanding of dynamic environments. We also show that our pipeline is robust
even without ground-truth contact annotations, which broadens its applicability
to tasks like 3D reconstruction and novel view/pose synthesis.

</details>


### [50] [Reducing the gap between general purpose data and aerial images in concentrated solar power plants](https://arxiv.org/abs/2508.00440)
*M. A. Pérez-Cutiño,J. Valverde,J. Capitán,J. M. Díaz-Báñez*

Main category: cs.CV

TL;DR: 论文提出了一种名为AerialCSP的虚拟数据集，用于模拟CSP（聚光太阳能发电）植物的航拍图像，以减少对大量标注数据的需求，并提升模型在真实场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 由于CSP植物中的高反射表面和特定领域元素，通用数据集训练的机器学习模型难以泛化，而收集和标注真实数据成本高昂。

Method: 通过生成高质量的合成数据集AerialCSP，模拟真实航拍图像，用于预训练模型，并评估其在CSP相关视觉任务中的表现。

Result: 预训练在AerialCSP上的模型显著提高了真实场景中的故障检测能力，尤其是对小而罕见的缺陷，减少了手动标注的需求。

Conclusion: AerialCSP为CSP植物的视觉任务提供了有效的预训练解决方案，降低了数据收集和标注的成本。

Abstract: In the context of Concentrated Solar Power (CSP) plants, aerial images
captured by drones present a unique set of challenges. Unlike urban or natural
landscapes commonly found in existing datasets, solar fields contain highly
reflective surfaces, and domain-specific elements that are uncommon in
traditional computer vision benchmarks. As a result, machine learning models
trained on generic datasets struggle to generalize to this setting without
extensive retraining and large volumes of annotated data. However, collecting
and labeling such data is costly and time-consuming, making it impractical for
rapid deployment in industrial applications.
  To address this issue, we propose a novel approach: the creation of
AerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By
generating synthetic data that closely mimic real-world conditions, our
objective is to facilitate pretraining of models before deployment,
significantly reducing the need for extensive manual labeling. Our main
contributions are threefold: (1) we introduce AerialCSP, a high-quality
synthetic dataset for aerial inspection of CSP plants, providing annotated data
for object detection and image segmentation; (2) we benchmark multiple models
on AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we
demonstrate that pretraining on AerialCSP significantly improves real-world
fault detection, particularly for rare and small defects, reducing the need for
extensive manual labeling. AerialCSP is made publicly available at
https://mpcutino.github.io/aerialcsp/.

</details>


### [51] [TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation](https://arxiv.org/abs/2508.00442)
*Jiale Zhou,Wenhan Wang,Shikun Li,Xiaolei Qu,Xin Guo,Yizhong Liu,Wenzhong Tang,Xun Lin,Yefeng Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种名为TopoTTA的测试时适应框架，专门用于解决管状结构分割中的域偏移问题，通过拓扑增强和硬样本生成策略显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 管状结构分割（TSS）在应用中面临域偏移的挑战，导致在未见目标域中性能下降。拓扑结构的变化和局部特征的差异进一步加剧了这一问题。

Method: TopoTTA分为两个阶段：1）使用TopoMDCs适应跨域拓扑差异；2）通过TopoHG策略和伪标签在伪断裂区域中提升拓扑连续性。

Result: 在四个场景和十个数据集上的实验表明，TopoTTA平均提升了31.81%的clDice分数。

Conclusion: TopoTTA是一种有效的、即插即用的测试时适应解决方案，适用于基于CNN的TSS模型。

Abstract: Tubular structure segmentation (TSS) is important for various applications,
such as hemodynamic analysis and route navigation. Despite significant progress
in TSS, domain shifts remain a major challenge, leading to performance
degradation in unseen target domains. Unlike other segmentation tasks, TSS is
more sensitive to domain shifts, as changes in topological structures can
compromise segmentation integrity, and variations in local features
distinguishing foreground from background (e.g., texture and contrast) may
further disrupt topological continuity. To address these challenges, we propose
Topology-enhanced Test-Time Adaptation (TopoTTA), the first test-time
adaptation framework designed specifically for TSS. TopoTTA consists of two
stages: Stage 1 adapts models to cross-domain topological discrepancies using
the proposed Topological Meta Difference Convolutions (TopoMDCs), which enhance
topological representation without altering pre-trained parameters; Stage 2
improves topological continuity by a novel Topology Hard sample Generation
(TopoHG) strategy and prediction alignment on hard samples with pseudo-labels
in the generated pseudo-break regions. Extensive experiments across four
scenarios and ten datasets demonstrate TopoTTA's effectiveness in handling
topological distribution shifts, achieving an average improvement of 31.81% in
clDice. TopoTTA also serves as a plug-and-play TTA solution for CNN-based TSS
models.

</details>


### [52] [SDMatte: Grafting Diffusion Models for Interactive Matting](https://arxiv.org/abs/2508.00443)
*Longfei Huang,Yu Liang,Hao Zhang,Jinwei Chen,Wei Dong,Lunde Chen,Wanyu Liu,Bo Li,Pengtao Jiang*

Main category: cs.CV

TL;DR: SDMatte利用扩散模型的强大先验，将文本驱动交互能力转化为视觉提示驱动交互能力，提出了一种新的交互式抠图方法。


<details>
  <summary>Details</summary>
Motivation: 现有交互式抠图方法在边缘区域提取精细细节方面表现不足，而扩散模型在复杂数据建模和纹理细节合成方面表现出色。

Method: SDMatte结合扩散模型的先验，引入视觉提示和坐标嵌入，提出掩码自注意力机制以聚焦指定区域。

Result: 在多数据集上的实验验证了SDMatte在交互式抠图中的优越性能。

Conclusion: SDMatte通过扩散模型和视觉提示驱动交互，显著提升了交互式抠图的精细度和效果。

Abstract: Recent interactive matting methods have shown satisfactory performance in
capturing the primary regions of objects, but they fall short in extracting
fine-grained details in edge regions. Diffusion models trained on billions of
image-text pairs, demonstrate exceptional capability in modeling highly complex
data distributions and synthesizing realistic texture details, while exhibiting
robust text-driven interaction capabilities, making them an attractive solution
for interactive matting. To this end, we propose SDMatte, a diffusion-driven
interactive matting model, with three key contributions. First, we exploit the
powerful priors of diffusion models and transform the text-driven interaction
capability into visual prompt-driven interaction capability to enable
interactive matting. Second, we integrate coordinate embeddings of visual
prompts and opacity embeddings of target objects into U-Net, enhancing
SDMatte's sensitivity to spatial position information and opacity information.
Third, we propose a masked self-attention mechanism that enables the model to
focus on areas specified by visual prompts, leading to better performance.
Extensive experiments on multiple datasets demonstrate the superior performance
of our method, validating its effectiveness in interactive matting. Our code
and model are available at https://github.com/vivoCameraResearch/SDMatte.

</details>


### [53] [AutoDebias: Automated Framework for Debiasing Text-to-Image Models](https://arxiv.org/abs/2508.00445)
*Hongyi Cai,Mohammad Mahdinur Rahman,Mingkang Dong,Jie Li,Muxin Pu,Zhili Fang,Yinan Peng,Hanjun Luo,Yang Liu*

Main category: cs.CV

TL;DR: AutoDebias框架自动检测并减少文本到图像模型中的社会偏见，无需预先了解偏见类型，通过生成包容性提示和CLIP引导训练实现。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理复杂或重叠的偏见，AutoDebias旨在解决这一问题。

Method: 利用视觉语言模型检测偏见模式，生成包容性提示，并通过CLIP引导训练优化模型。

Result: 在25种偏见场景中，AutoDebias检测准确率达91.6%，显著减少偏见输出，同时保持图像质量。

Conclusion: AutoDebias能有效处理复杂偏见，提升模型公平性而不影响生成质量。

Abstract: Text-to-Image (T2I) models generate high-quality images from text prompts but
often exhibit unintended social biases, such as gender or racial stereotypes,
even when these attributes are not mentioned. Existing debiasing methods work
well for simple or well-known cases but struggle with subtle or overlapping
biases. We propose AutoDebias, a framework that automatically identifies and
mitigates harmful biases in T2I models without prior knowledge of specific bias
types. Specifically, AutoDebias leverages vision-language models to detect
biased visual patterns and constructs fairness guides by generating inclusive
alternative prompts that reflect balanced representations. These guides drive a
CLIP-guided training process that promotes fairer outputs while preserving the
original model's image quality and diversity. Unlike existing methods,
AutoDebias effectively addresses both subtle stereotypes and multiple
interacting biases. We evaluate the framework on a benchmark covering over 25
bias scenarios, including challenging cases where multiple biases occur
simultaneously. AutoDebias detects harmful patterns with 91.6% accuracy and
reduces biased outputs from 90% to negligible levels, while preserving the
visual fidelity of the original model.

</details>


### [54] [CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text](https://arxiv.org/abs/2508.00447)
*Anju Rani,Daniel Ortiz-Arroyo,Petar Durdevic*

Main category: cs.CV

TL;DR: CLIPTime是一个基于CLIP的多模态多任务框架，用于预测真菌生长的发育阶段和时间戳，无需显式时间输入。


<details>
  <summary>Details</summary>
Motivation: 理解生物生长的时序动态对多个领域至关重要，但现有视觉语言模型在捕捉时序进展方面表现有限。

Method: 基于CLIP架构，学习视觉-文本联合嵌入，通过分类和回归预测生长阶段和时间戳。

Result: 实验表明CLIPTime能有效建模生物进展，生成可解释的时间相关输出。

Conclusion: CLIPTime展示了视觉语言模型在生物监测应用中的潜力。

Abstract: Understanding the temporal dynamics of biological growth is critical across
diverse fields such as microbiology, agriculture, and biodegradation research.
Although vision-language models like Contrastive Language Image Pretraining
(CLIP) have shown strong capabilities in joint visual-textual reasoning, their
effectiveness in capturing temporal progression remains limited. To address
this, we propose CLIPTime, a multimodal, multitask framework designed to
predict both the developmental stage and the corresponding timestamp of fungal
growth from image and text inputs. Built upon the CLIP architecture, our model
learns joint visual-textual embeddings and enables time-aware inference without
requiring explicit temporal input during testing. To facilitate training and
evaluation, we introduce a synthetic fungal growth dataset annotated with
aligned timestamps and categorical stage labels. CLIPTime jointly performs
classification and regression, predicting discrete growth stages alongside
continuous timestamps. We also propose custom evaluation metrics, including
temporal accuracy and regression error, to assess the precision of time-aware
predictions. Experimental results demonstrate that CLIPTime effectively models
biological progression and produces interpretable, temporally grounded outputs,
highlighting the potential of vision-language models in real-world biological
monitoring applications.

</details>


### [55] [PIF-Net: Ill-Posed Prior Guided Multispectral and Hyperspectral Image Fusion via Invertible Mamba and Fusion-Aware LoRA](https://arxiv.org/abs/2508.00453)
*Baisong Li,Xingwang Wang,Haixiao Xu*

Main category: cs.CV

TL;DR: PIF-Net提出了一种新的多光谱和高光谱图像融合框架，通过引入不适定先验和可逆Mamba架构，有效解决了数据不对齐问题，并在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多光谱和高光谱图像融合（MHIF）任务因光谱与空间信息的固有权衡及观测数据有限而具有不适定性，现有方法未能有效解决数据不对齐问题。

Method: PIF-Net结合不适定先验，采用可逆Mamba架构保持信息一致性，并设计轻量级的Fusion-Aware Low-Rank Adaptation模块动态校准特征。

Result: 在多个基准数据集上，PIF-Net显著优于现有方法，同时保持模型高效性。

Conclusion: PIF-Net通过创新架构和模块设计，成功解决了MHIF任务的不适定性，实现了高性能的图像融合。

Abstract: The goal of multispectral and hyperspectral image fusion (MHIF) is to
generate high-quality images that simultaneously possess rich spectral
information and fine spatial details. However, due to the inherent trade-off
between spectral and spatial information and the limited availability of
observations, this task is fundamentally ill-posed. Previous studies have not
effectively addressed the ill-posed nature caused by data misalignment. To
tackle this challenge, we propose a fusion framework named PIF-Net, which
explicitly incorporates ill-posed priors to effectively fuse multispectral
images and hyperspectral images. To balance global spectral modeling with
computational efficiency, we design a method based on an invertible Mamba
architecture that maintains information consistency during feature
transformation and fusion, ensuring stable gradient flow and process
reversibility. Furthermore, we introduce a novel fusion module called the
Fusion-Aware Low-Rank Adaptation module, which dynamically calibrates spectral
and spatial features while keeping the model lightweight. Extensive experiments
on multiple benchmark datasets demonstrate that PIF-Net achieves significantly
better image restoration performance than current state-of-the-art methods
while maintaining model efficiency.

</details>


### [56] [HyPCV-Former: Hyperbolic Spatio-Temporal Transformer for 3D Point Cloud Video Anomaly Detection](https://arxiv.org/abs/2508.00473)
*Jiaping Cao,Kangkang Zhou,Juan Du*

Main category: cs.CV

TL;DR: 提出了一种基于双曲时空变换器（HyPCV-Former）的视频异常检测方法，利用双曲空间捕捉事件的分层结构和时空连续性，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在RGB或深度域中使用欧几里得表示，难以捕捉事件的分层结构和时空连续性，因此需要一种更有效的表示方法。

Method: 通过点云提取器提取每帧空间特征，并将其嵌入洛伦兹双曲空间；引入双曲多头自注意力机制（HMHA）建模时间动态，直接在双曲空间中进行特征变换和异常评分。

Result: 在TIMo和DAD数据集上分别实现了7%和5.6%的性能提升，达到最先进水平。

Conclusion: HyPCV-Former通过双曲空间建模，显著提升了视频异常检测的性能，为复杂场景下的异常检测提供了新思路。

Abstract: Video anomaly detection is a fundamental task in video surveillance, with
broad applications in public safety and intelligent monitoring systems.
Although previous methods leverage Euclidean representations in RGB or depth
domains, such embeddings are inherently limited in capturing hierarchical event
structures and spatio-temporal continuity. To address these limitations, we
propose HyPCV-Former, a novel hyperbolic spatio-temporal transformer for
anomaly detection in 3D point cloud videos. Our approach first extracts
per-frame spatial features from point cloud sequences via point cloud
extractor, and then embeds them into Lorentzian hyperbolic space, which better
captures the latent hierarchical structure of events. To model temporal
dynamics, we introduce a hyperbolic multi-head self-attention (HMHA) mechanism
that leverages Lorentzian inner products and curvature-aware softmax to learn
temporal dependencies under non-Euclidean geometry. Our method performs all
feature transformations and anomaly scoring directly within full Lorentzian
space rather than via tangent space approximation. Extensive experiments
demonstrate that HyPCV-Former achieves state-of-the-art performance across
multiple anomaly categories, with a 7\% improvement on the TIMo dataset and a
5.6\% gain on the DAD dataset compared to benchmarks. The code will be released
upon paper acceptance.

</details>


### [57] [LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer](https://arxiv.org/abs/2508.00477)
*Yuzhuo Chen,Zehua Ma,Jianhua Wang,Kai Kang,Shunyu Yao,Weiming Zhang*

Main category: cs.CV

TL;DR: LAMIC是一个无需训练的布局感知多图像合成框架，通过两种注意力机制（GIA和RMA）实现多参考场景下的图像生成，并在多个指标上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多参考图像合成中布局一致性和实体解耦的挑战。

Method: 基于MMDiT模型，引入GIA和RMA两种注意力机制，无需训练即可扩展单参考扩散模型到多参考场景。

Result: 在ID-S、BG-S、IN-R等指标上优于现有基线，展示了零样本泛化能力。

Conclusion: LAMIC为可控多图像合成提供了新的无需训练范式，具有强大的泛化能力。

Abstract: In controllable image synthesis, generating coherent and consistent images
from multiple references with spatial layout awareness remains an open
challenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework
that, for the first time, extends single-reference diffusion models to
multi-reference scenarios in a training-free manner. Built upon the MMDiT
model, LAMIC introduces two plug-and-play attention mechanisms: 1) Group
Isolation Attention (GIA) to enhance entity disentanglement; and 2)
Region-Modulated Attention (RMA) to enable layout-aware generation. To
comprehensively evaluate model capabilities, we further introduce three
metrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout
control; and 2) Background Similarity (BG-S) for measuring background
consistency. Extensive experiments show that LAMIC achieves state-of-the-art
performance across most major metrics: it consistently outperforms existing
multi-reference baselines in ID-S, BG-S, IN-R and AVG scores across all
settings, and achieves the best DPG in complex composition tasks. These results
demonstrate LAMIC's superior abilities in identity keeping, background
preservation, layout control, and prompt-following, all achieved without any
training or fine-tuning, showcasing strong zero-shot generalization ability. By
inheriting the strengths of advanced single-reference models and enabling
seamless extension to multi-image scenarios, LAMIC establishes a new
training-free paradigm for controllable multi-image composition. As foundation
models continue to evolve, LAMIC's performance is expected to scale
accordingly. Our implementation is available at:
https://github.com/Suchenl/LAMIC.

</details>


### [58] [SAMSA 2.0: Prompting Segment Anything with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation](https://arxiv.org/abs/2508.00493)
*Alfie Roddan,Tobias Czempiel,Chi Xu,Daniel S. Elson,Stamatia Giannarou*

Main category: cs.CV

TL;DR: SAMSA 2.0是一种用于高光谱医学图像的交互式分割框架，通过光谱角度提示结合空间线索引导SAM模型，提高了分割的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决高光谱医学图像分割中光谱信息利用不足的问题，提升分割性能。

Method: 引入光谱角度提示，将光谱信息与空间线索早期融合，无需重新训练模型。

Result: 相比仅使用RGB的模型，Dice分数提高了3.8%；相比先前的光谱融合方法，提高了3.1%。

Conclusion: SAMSA 2.0在低数据和噪声场景下表现出色，增强了少样本和零样本性能。

Abstract: We present SAMSA 2.0, an interactive segmentation framework for hyperspectral
medical imaging that introduces spectral angle prompting to guide the Segment
Anything Model (SAM) using spectral similarity alongside spatial cues. This
early fusion of spectral information enables more accurate and robust
segmentation across diverse spectral datasets. Without retraining, SAMSA 2.0
achieves up to +3.8% higher Dice scores compared to RGB-only models and up to
+3.1% over prior spectral fusion methods. Our approach enhances few-shot and
zero-shot performance, demonstrating strong generalization in challenging
low-data and noisy scenarios common in clinical imaging.

</details>


### [59] [LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI](https://arxiv.org/abs/2508.00496)
*Mohammed Kamran,Maria Bernathova,Raoul Varga,Christian Singer,Zsuzsanna Bago-Horvath,Thomas Helbich,Georg Langs,Philipp Seeböck*

Main category: cs.CV

TL;DR: LesiOnTime是一种新型3D分割方法，结合纵向影像和BI-RADS评分，显著提升了小病灶分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法主要针对大病灶，忽略了纵向和临床信息，而真实筛查中需要结合时间点和临床评估（如BI-RADS评分）检测细微病灶。

Method: 提出Temporal Prior Attention（TPA）块动态整合历史与当前扫描信息，以及BI-RADS一致性正则化（BCR）损失函数，将领域知识嵌入训练过程。

Result: 在内部纵向数据集上，Dice分数比现有方法提升5%，TPA和BCR均贡献互补性能增益。

Conclusion: 结合时间和临床背景对乳腺癌筛查中的早期病灶分割至关重要。

Abstract: Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced
MRI (DCE-MRI) is critical for early cancer detection, especially in high-risk
patients. While recent deep learning methods have advanced lesion segmentation,
they primarily target large lesions and neglect valuable longitudinal and
clinical information routinely used by radiologists. In real-world screening,
detecting subtle or emerging lesions requires radiologists to compare across
timepoints and consider previous radiology assessments, such as the BI-RADS
score. We propose LesiOnTime, a novel 3D segmentation approach that mimics
clinical diagnostic workflows by jointly leveraging longitudinal imaging and
BIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA)
block that dynamically integrates information from previous and current scans;
and (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent
space alignment for scans with similar radiological assessments, thus embedding
domain knowledge into the training process. Evaluated on a curated in-house
longitudinal dataset of high-risk patients with DCE-MRI, our approach
outperforms state-of-the-art single-timepoint and longitudinal baselines by 5%
in terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute
complementary performance gains. These results highlight the importance of
incorporating temporal and clinical context for reliable early lesion
segmentation in real-world breast cancer screening. Our code is publicly
available at https://github.com/cirmuw/LesiOnTime

</details>


### [60] [Leveraging Convolutional and Graph Networks for an Unsupervised Remote Sensing Labelling Tool](https://arxiv.org/abs/2508.00506)
*Tulsi Patel,Mark W. Jones,Thomas Redfern*

Main category: cs.CV

TL;DR: 提出了一种无监督的遥感图像标注方法，结合卷积和图神经网络，提升标注的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 遥感图像标注耗时且昂贵，传统方法依赖预标注数据，限制了其应用。

Method: 使用卷积和图神经网络对图像进行分割和特征编码，结合颜色和空间相似性分组像素区域。

Result: 减少了标注中的异常值，支持细粒度标注，并形成旋转不变的语义关系。

Conclusion: 该方法克服了传统方法的局限性，为遥感图像标注提供了更高效的解决方案。

Abstract: Machine learning for remote sensing imaging relies on up-to-date and accurate
labels for model training and testing. Labelling remote sensing imagery is time
and cost intensive, requiring expert analysis. Previous labelling tools rely on
pre-labelled data for training in order to label new unseen data. In this work,
we define an unsupervised pipeline for finding and labelling geographical areas
of similar context and content within Sentinel-2 satellite imagery. Our
approach removes limitations of previous methods by utilising segmentation with
convolutional and graph neural networks to encode a more robust feature space
for image comparison. Unlike previous approaches we segment the image into
homogeneous regions of pixels that are grouped based on colour and spatial
similarity. Graph neural networks are used to aggregate information about the
surrounding segments enabling the feature representation to encode the local
neighbourhood whilst preserving its own local information. This reduces
outliers in the labelling tool, allows users to label at a granular level, and
allows a rotationally invariant semantic relationship at the image level to be
formed within the encoding space.

</details>


### [61] [Fine-grained Spatiotemporal Grounding on Egocentric Videos](https://arxiv.org/abs/2508.00518)
*Shuo Liang,Yiwu Zhong,Zi-Yuan Hu,Yeyao Tao,Liwei Wang*

Main category: cs.CV

TL;DR: 论文提出了EgoMask，首个针对自我中心视频的像素级时空定位基准，通过自动标注管道构建，并展示了现有模型在该基准上的表现不佳，但通过微调可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 自我中心视频在增强现实和机器人等应用中日益重要，但现有研究多关注外中心视频，自我中心视频的时空定位仍待探索。

Method: 提出EgoMask基准和自动标注管道，构建EgoMask-Train训练数据集，并分析自我中心视频的独特挑战。

Result: 实验显示现有模型在EgoMask上表现不佳，但通过微调可显著改进，同时不影响外中心数据集的表现。

Conclusion: 研究为自我中心视频理解提供了关键资源和见解，推动了该领域的发展。

Abstract: Spatiotemporal video grounding aims to localize target entities in videos
based on textual queries. While existing research has made significant progress
in exocentric videos, the egocentric setting remains relatively underexplored,
despite its growing importance in applications such as augmented reality and
robotics. In this work, we conduct a systematic analysis of the discrepancies
between egocentric and exocentric videos, revealing key challenges such as
shorter object durations, sparser trajectories, smaller object sizes, and
larger positional shifts. To address these challenges, we introduce EgoMask,
the first pixel-level benchmark for fine-grained spatiotemporal grounding in
egocentric videos. It is constructed by our proposed automatic annotation
pipeline, which annotates referring expressions and object masks across short-,
medium-, and long-term videos. Additionally, we create EgoMask-Train, a
large-scale training dataset to facilitate model development. Experiments
demonstrate that the state-of-the-art spatiotemporal grounding models perform
poorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields
significant improvements, while preserving performance on exocentric datasets.
Our work thus provides essential resources and insights for advancing
egocentric video understanding. Our code is available at
https://github.com/LaVi-Lab/EgoMask .

</details>


### [62] [EPANet: Efficient Path Aggregation Network for Underwater Fish Detection](https://arxiv.org/abs/2508.00528)
*Jinsong Yang,Zeyuan Hu,Yichen Li*

Main category: cs.CV

TL;DR: 提出了一种高效路径聚合网络（EPANet），用于解决水下鱼类检测中的低分辨率、背景干扰和目标相似性问题，通过互补特征集成实现轻量级且准确的检测。


<details>
  <summary>Details</summary>
Motivation: 水下鱼类检测（UFD）因低分辨率、背景干扰和目标与环境的视觉相似性而具有挑战性，现有方法通常以增加模型复杂性和降低效率为代价。

Method: EPANet包含高效路径聚合特征金字塔网络（EPA-FPN）和多尺度多样化短路径瓶颈（MS-DDSP瓶颈），前者通过长距离跳跃连接提升语义-空间互补性，后者通过细粒度特征划分和多样化卷积操作增强局部特征多样性。

Result: 在基准UFD数据集上的实验表明，EPANet在检测精度和推理速度上优于现有方法，同时保持较低参数复杂度。

Conclusion: EPANet通过高效的特征集成和轻量级设计，显著提升了水下鱼类检测的性能和效率。

Abstract: Underwater fish detection (UFD) remains a challenging task in computer vision
due to low object resolution, significant background interference, and high
visual similarity between targets and surroundings. Existing approaches
primarily focus on local feature enhancement or incorporate complex attention
mechanisms to highlight small objects, often at the cost of increased model
complexity and reduced efficiency. To address these limitations, we propose an
efficient path aggregation network (EPANet), which leverages complementary
feature integration to achieve accurate and lightweight UFD. EPANet consists of
two key components: an efficient path aggregation feature pyramid network
(EPA-FPN) and a multi-scale diverse-division short path bottleneck (MS-DDSP
bottleneck). The EPA-FPN introduces long-range skip connections across
disparate scales to improve semantic-spatial complementarity, while cross-layer
fusion paths are adopted to enhance feature integration efficiency. The MS-DDSP
bottleneck extends the conventional bottleneck structure by introducing
finer-grained feature division and diverse convolutional operations, thereby
increasing local feature diversity and representation capacity. Extensive
experiments on benchmark UFD datasets demonstrate that EPANet outperforms
state-of-the-art methods in terms of detection accuracy and inference speed,
while maintaining comparable or even lower parameter complexity.

</details>


### [63] [Video Color Grading via Look-Up Table Generation](https://arxiv.org/abs/2508.00548)
*Seunghyun Shin,Dongmin Shin,Jisu Shin,Hae-Gon Jeon,Joon-Young Lee*

Main category: cs.CV

TL;DR: 提出了一种基于参考的视频色彩分级框架，利用扩散模型生成查找表（LUT）以实现色彩属性对齐，并结合用户偏好进行低层次特征增强。


<details>
  <summary>Details</summary>
Motivation: 视频色彩分级通常需要专业技能，限制了非专业人士的使用，因此需要一种自动化且高效的方法。

Method: 通过扩散模型生成LUT，对齐参考场景与输入视频的色彩属性，并结合文本提示优化低层次特征。

Result: 实验和用户研究表明，该方法能有效实现视频色彩分级，且保持结构细节并快速推理。

Conclusion: 提出的框架为视频色彩分级提供了一种高效且用户友好的解决方案，代码已开源。

Abstract: Different from color correction and transfer, color grading involves
adjusting colors for artistic or storytelling purposes in a video, which is
used to establish a specific look or mood. However, due to the complexity of
the process and the need for specialized editing skills, video color grading
remains primarily the domain of professional colorists. In this paper, we
present a reference-based video color grading framework. Our key idea is
explicitly generating a look-up table (LUT) for color attribute alignment
between reference scenes and input video via a diffusion model. As a training
objective, we enforce that high-level features of the reference scenes like
look, mood, and emotion should be similar to that of the input video. Our
LUT-based approach allows for color grading without any loss of structural
details in the whole video frames as well as achieving fast inference. We
further build a pipeline to incorporate a user-preference via text prompts for
low-level feature enhancement such as contrast and brightness, etc.
Experimental results, including extensive user studies, demonstrate the
effectiveness of our approach for video color grading. Codes are publicly
available at https://github.com/seunghyuns98/VideoColorGrading.

</details>


### [64] [Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images](https://arxiv.org/abs/2508.00549)
*Daniel Wolf,Heiko Hillenhagen,Billurvan Taskin,Alex Bäuerle,Meinrad Beer,Michael Götz,Timo Ropinski*

Main category: cs.CV

TL;DR: 评估了当前先进的视觉语言模型（VLMs）在医学图像中定位相对位置的能力，发现所有模型均表现不佳。尝试使用视觉提示改进效果有限，并提出了MIRP基准数据集以促进研究。


<details>
  <summary>Details</summary>
Motivation: 临床决策依赖于对解剖结构相对位置的理解，但VLMs在此任务上的能力尚未充分探索，因此需要评估和改进。

Method: 评估了GPT-4o、Llama3.2、Pixtral和JanusPro等模型，并尝试使用视觉提示（如标记）提升性能。

Result: 所有模型在医学图像中表现不佳，视觉提示仅带来有限改进，模型更多依赖先验知识而非图像内容。

Conclusion: 提出了MIRP基准数据集，以系统评估和改进VLMs在医学图像中的相对定位能力。

Abstract: Clinical decision-making relies heavily on understanding relative positions
of anatomical structures and anomalies. Therefore, for Vision-Language Models
(VLMs) to be applicable in clinical practice, the ability to accurately
determine relative positions on medical images is a fundamental prerequisite.
Despite its importance, this capability remains highly underexplored. To
address this gap, we evaluate the ability of state-of-the-art VLMs, GPT-4o,
Llama3.2, Pixtral, and JanusPro, and find that all models fail at this
fundamental task. Inspired by successful approaches in computer vision, we
investigate whether visual prompts, such as alphanumeric or colored markers
placed on anatomical structures, can enhance performance. While these markers
provide moderate improvements, results remain significantly lower on medical
images compared to observations made on natural images. Our evaluations suggest
that, in medical imaging, VLMs rely more on prior anatomical knowledge than on
actual image content for answering relative position questions, often leading
to incorrect conclusions. To facilitate further research in this area, we
introduce the MIRP , Medical Imaging Relative Positioning, benchmark dataset,
designed to systematically evaluate the capability to identify relative
positions in medical images.

</details>


### [65] [DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable Adversarial Purification](https://arxiv.org/abs/2508.00552)
*Chihan Huang,Belal Alsinglawi,Islam Al-qudah*

Main category: cs.CV

TL;DR: 提出了一种高效的基于扩散的对抗净化框架DBLP，通过噪声桥蒸馏和自适应语义增强实现实时净化。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络易受对抗扰动影响，现有扩散净化方法计算成本高，限制了实际应用。

Method: 采用噪声桥蒸馏目标，在潜在一致性模型中构建对抗噪声与干净数据的对齐，并结合多尺度金字塔边缘图进行语义增强。

Result: 在多个数据集上达到SOTA鲁棒精度和图像质量，推理时间约0.2秒。

Conclusion: DBLP为实时对抗净化提供了有效解决方案。

Abstract: Recent advances in deep neural networks (DNNs) have led to remarkable success
across a wide range of tasks. However, their susceptibility to adversarial
perturbations remains a critical vulnerability. Existing diffusion-based
adversarial purification methods often require intensive iterative denoising,
severely limiting their practical deployment. In this paper, we propose
Diffusion Bridge Distillation for Purification (DBLP), a novel and efficient
diffusion-based framework for adversarial purification. Central to our approach
is a new objective, noise bridge distillation, which constructs a principled
alignment between the adversarial noise distribution and the clean data
distribution within a latent consistency model (LCM). To further enhance
semantic fidelity, we introduce adaptive semantic enhancement, which fuses
multi-scale pyramid edge maps as conditioning input to guide the purification
process. Extensive experiments across multiple datasets demonstrate that DBLP
achieves state-of-the-art (SOTA) robust accuracy, superior image quality, and
around 0.2s inference time, marking a significant step toward real-time
adversarial purification.

</details>


### [66] [HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models](https://arxiv.org/abs/2508.00553)
*Jizhihui Liu,Feiyi Du,Guangdao Zhu,Niu Lian,Jun Li,Bin Chen*

Main category: cs.CV

TL;DR: HiPrune是一种无需训练、模型无关的视觉令牌修剪框架，利用视觉编码器的分层注意力结构，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLMs）因视觉令牌序列过长导致计算开销大、推理效率低，现有方法依赖特殊令牌或任务特定训练，缺乏普适性。

Method: HiPrune通过分层注意力结构选择三类信息令牌：锚点令牌（对象中心层高注意力）、缓冲令牌（空间连续性）和注册令牌（全局上下文层高注意力）。

Result: 实验表明，HiPrune在LLaVA-1.5等模型上保持99.3%任务准确率仅需33.3%令牌，减少9倍FLOPs和延迟。

Conclusion: HiPrune无需训练，适用于任何ViT-based VLM，在令牌修剪性能和效率上达到最优。

Abstract: Vision-Language Models (VLMs) encode images into lengthy sequences of visual
tokens, leading to excessive computational overhead and limited inference
efficiency. While prior efforts prune or merge tokens to address this issue,
they often rely on special tokens (e.g., CLS) or require task-specific
training, hindering scalability across architectures. In this paper, we propose
HiPrune, a training-free and model-agnostic token Pruning framework that
exploits the Hierarchical attention structure within vision encoders. We
identify that middle layers attend to object-centric regions, while deep layers
capture global contextual features. Based on this observation, HiPrune selects
three types of informative tokens: (1) Anchor tokens with high attention in
object-centric layers, (2) Buffer tokens adjacent to anchors for spatial
continuity, and (3) Register tokens with strong attention in deep layers for
global summarization. Our method requires no retraining and integrates
seamlessly with any ViT-based VLM. Extensive experiments on LLaVA-1.5,
LLaVA-NeXT, and Qwen2.5-VL demonstrate that HiPrune achieves state-of-the-art
pruning performance, preserving up to 99.3% task accuracy with only 33.3%
tokens, and maintaining 99.5% accuracy with just 11.1% tokens. Meanwhile, it
reduces inference FLOPs and latency by up to 9$\times$, showcasing strong
generalization across models and tasks. Code is available at
https://github.com/Danielement321/HiPrune.

</details>


### [67] [Training-Free Class Purification for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2508.00557)
*Qi Chen,Lingxiao Yang,Yun Chen,Nailong Zhao,Jianhuang Lai,Jie Shao,Xiaohua Xie*

Main category: cs.CV

TL;DR: FreeCP是一种无需训练的分类净化框架，旨在解决开放词汇语义分割中的类别冗余和视觉语言模糊性问题，显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视类别冗余和视觉语言模糊性，导致次优的类别激活图。FreeCP旨在解决这些问题。

Method: FreeCP通过净化语义类别和纠正冗余与模糊性引起的错误，生成最终分割预测。

Result: 在八个基准测试中，FreeCP作为即插即用模块，显著提升了分割性能。

Conclusion: FreeCP有效解决了开放词汇语义分割中的关键挑战，提升了性能。

Abstract: Fine-tuning pre-trained vision-language models has emerged as a powerful
approach for enhancing open-vocabulary semantic segmentation (OVSS). However,
the substantial computational and resource demands associated with training on
large datasets have prompted interest in training-free methods for OVSS.
Existing training-free approaches primarily focus on modifying model
architectures and generating prototypes to improve segmentation performance.
However, they often neglect the challenges posed by class redundancy, where
multiple categories are not present in the current test image, and
visual-language ambiguity, where semantic similarities among categories create
confusion in class activation. These issues can lead to suboptimal class
activation maps and affinity-refined activation maps. Motivated by these
observations, we propose FreeCP, a novel training-free class purification
framework designed to address these challenges. FreeCP focuses on purifying
semantic categories and rectifying errors caused by redundancy and ambiguity.
The purified class representations are then leveraged to produce final
segmentation predictions. We conduct extensive experiments across eight
benchmarks to validate FreeCP's effectiveness. Results demonstrate that FreeCP,
as a plug-and-play module, significantly boosts segmentation performance when
combined with other OVSS methods.

</details>


### [68] [Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints](https://arxiv.org/abs/2508.00558)
*Jens U. Kreber,Joerg Stueckler*

Main category: cs.CV

TL;DR: PhysNAP是一种基于扩散模型的新方法，用于生成与部分点云对齐且物理合理的铰接物体。


<details>
  <summary>Details</summary>
Motivation: 铰接物体是日常环境中重要的交互对象，但现有方法在物理合理性和点云对齐方面存在不足。

Method: 使用符号距离函数（SDFs）表示部件形状，通过点云对齐损失和非穿透性、移动性约束引导反向扩散过程，并引入类别信息优化对齐。

Result: 在PartNet-Mobility数据集上评估，PhysNAP在约束一致性和生成能力之间取得了平衡，优于无引导的基线扩散模型。

Conclusion: PhysNAP能够生成物理合理且与点云对齐的铰接物体，为相关领域提供了有效工具。

Abstract: Articulated objects are an important type of interactable objects in everyday
environments. In this paper, we propose PhysNAP, a novel diffusion model-based
approach for generating articulated objects that aligns them with partial point
clouds and improves their physical plausibility. The model represents part
shapes by signed distance functions (SDFs). We guide the reverse diffusion
process using a point cloud alignment loss computed using the predicted SDFs.
Additionally, we impose non-penetration and mobility constraints based on the
part SDFs for guiding the model to generate more physically plausible objects.
We also make our diffusion approach category-aware to further improve point
cloud alignment if category information is available. We evaluate the
generative ability and constraint consistency of samples generated with PhysNAP
using the PartNet-Mobility dataset. We also compare it with an unguided
baseline diffusion model and demonstrate that PhysNAP can improve constraint
consistency and provides a tradeoff with generative ability.

</details>


### [69] [Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images](https://arxiv.org/abs/2508.00563)
*Hannah Kniesel,Leon Sick,Tristan Payer,Tim Bergner,Kavitha Shaga Devan,Clarissa Read,Paul Walther,Timo Ropinski*

Main category: cs.CV

TL;DR: 提出了一种基于弱监督的对象检测方法，仅需图像级标注，通过预训练模型生成伪标签，优化了标注成本并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有对象检测方法依赖大量标注框，成本高且需专家知识，因此提出仅需图像级标注的弱监督方法。

Method: 利用预训练模型预测病毒存在与否，生成伪标签，通过优化方法和缩小感受野提取病毒颗粒。

Result: 伪标签方法在标注时间有限的情况下优于其他弱标注方法，甚至优于真实标注。

Conclusion: 该方法显著降低了标注成本，同时提升了检测性能，适用于标注资源有限的场景。

Abstract: Current state-of-the-art methods for object detection rely on annotated
bounding boxes of large data sets for training. However, obtaining such
annotations is expensive and can require up to hundreds of hours of manual
labor. This poses a challenge, especially since such annotations can only be
provided by experts, as they require knowledge about the scientific domain. To
tackle this challenge, we propose a domain-specific weakly supervised object
detection algorithm that only relies on image-level annotations, which are
significantly easier to acquire. Our method distills the knowledge of a
pre-trained model, on the task of predicting the presence or absence of a virus
in an image, to obtain a set of pseudo-labels that can be used to later train a
state-of-the-art object detection model. To do so, we use an optimization
approach with a shrinking receptive field to extract virus particles directly
without specific network architectures. Through a set of extensive studies, we
show how the proposed pseudo-labels are easier to obtain, and, more
importantly, are able to outperform other existing weak labeling methods, and
even ground truth labels, in cases where the time to obtain the annotation is
limited.

</details>


### [70] [CoProU-VO: Combining Projected Uncertainty for End-to-End Unsupervised Monocular Visual Odometry](https://arxiv.org/abs/2508.00568)
*Jingchao Xie,Oussema Dhaouadi,Weirong Chen,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: 提出了一种新的端到端视觉里程计方法CoProU-VO，通过跨帧不确定性传播解决动态场景中的姿态估计问题。


<details>
  <summary>Details</summary>
Motivation: 动态物体和遮挡导致传统无监督视觉里程计方法在静态场景假设下失效，需要更鲁棒的不确定性建模。

Method: 结合目标帧和参考帧的不确定性，采用概率公式和视觉Transformer架构，同时学习深度、不确定性和相机姿态。

Result: 在KITTI和nuScenes数据集上表现优于现有无监督单目方法，尤其在高速公路场景中表现突出。

Conclusion: 跨帧不确定性传播显著提升了动态场景中的视觉里程计性能。

Abstract: Visual Odometry (VO) is fundamental to autonomous navigation, robotics, and
augmented reality, with unsupervised approaches eliminating the need for
expensive ground-truth labels. However, these methods struggle when dynamic
objects violate the static scene assumption, leading to erroneous pose
estimations. We tackle this problem by uncertainty modeling, which is a
commonly used technique that creates robust masks to filter out dynamic objects
and occlusions without requiring explicit motion segmentation. Traditional
uncertainty modeling considers only single-frame information, overlooking the
uncertainties across consecutive frames. Our key insight is that uncertainty
must be propagated and combined across temporal frames to effectively identify
unreliable regions, particularly in dynamic scenes. To address this challenge,
we introduce Combined Projected Uncertainty VO (CoProU-VO), a novel end-to-end
approach that combines target frame uncertainty with projected reference frame
uncertainty using a principled probabilistic formulation. Built upon vision
transformer backbones, our model simultaneously learns depth, uncertainty
estimation, and camera poses. Consequently, experiments on the KITTI and
nuScenes datasets demonstrate significant improvements over previous
unsupervised monocular end-to-end two-frame-based methods and exhibit strong
performance in challenging highway scenes where other approaches often fail.
Additionally, comprehensive ablation studies validate the effectiveness of
cross-frame uncertainty propagation.

</details>


### [71] [Uncertainty-Aware Likelihood Ratio Estimation for Pixel-Wise Out-of-Distribution Detection](https://arxiv.org/abs/2508.00587)
*Marc Hölle,Walter Kellermann,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: 提出了一种基于不确定性感知的似然比估计方法，用于区分语义分割模型中的已知和未知像素特征，显著降低了误报率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在复杂场景中难以区分罕见已知类和真正未知对象的问题。

Method: 使用证据分类器结合似然比测试，生成概率分布以捕捉不确定性，并有效利用异常暴露。

Result: 在五个标准数据集上，误报率最低（2.5%），平均精度高（90.91%），计算开销可忽略。

Conclusion: 通过显式建模不确定性，该方法显著提升了未知对象的检测性能。

Abstract: Semantic segmentation models trained on known object classes often fail in
real-world autonomous driving scenarios by confidently misclassifying unknown
objects. While pixel-wise out-of-distribution detection can identify unknown
objects, existing methods struggle in complex scenes where rare object classes
are often confused with truly unknown objects. We introduce an
uncertainty-aware likelihood ratio estimation method that addresses these
limitations. Our approach uses an evidential classifier within a likelihood
ratio test to distinguish between known and unknown pixel features from a
semantic segmentation model, while explicitly accounting for uncertainty.
Instead of producing point estimates, our method outputs probability
distributions that capture uncertainty from both rare training examples and
imperfect synthetic outliers. We show that by incorporating uncertainty in this
way, outlier exposure can be leveraged more effectively. Evaluated on five
standard benchmark datasets, our method achieves the lowest average false
positive rate (2.5%) among state-of-the-art while maintaining high average
precision (90.91%) and incurring only negligible computational overhead. Code
is available at https://github.com/glasbruch/ULRE.

</details>


### [72] [Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving](https://arxiv.org/abs/2508.00589)
*Stefan Englmeier,Max A. Büttner,Katharina Winter,Fabian B. Flohr*

Main category: cs.CV

TL;DR: 提出了一种基于上下文感知的运动检索框架，用于自动驾驶系统中罕见人类行为的识别，并引入数据集WayMoCo。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需在安全关键场景中可靠运行，但大规模数据集中罕见人类行为的检索具有挑战性。

Method: 结合SMPL运动序列和视频帧，编码到多模态嵌入空间，支持通过文本查询检索人类行为及其上下文。

Result: 在WayMoCo数据集上，方法比现有模型准确率提升27.5%。

Conclusion: 该方法为自动驾驶系统提供了高效的人类行为检索工具，支持更鲁棒的评估和泛化。

Abstract: Autonomous driving systems must operate reliably in safety-critical
scenarios, particularly those involving unusual or complex behavior by
Vulnerable Road Users (VRUs). Identifying these edge cases in driving datasets
is essential for robust evaluation and generalization, but retrieving such rare
human behavior scenarios within the long tail of large-scale datasets is
challenging. To support targeted evaluation of autonomous driving systems in
diverse, human-centered scenarios, we propose a novel context-aware motion
retrieval framework. Our method combines Skinned Multi-Person Linear
(SMPL)-based motion sequences and corresponding video frames before encoding
them into a shared multimodal embedding space aligned with natural language.
Our approach enables the scalable retrieval of human behavior and their context
through text queries. This work also introduces our dataset WayMoCo, an
extension of the Waymo Open Dataset. It contains automatically labeled motion
and scene context descriptions derived from generated pseudo-ground-truth SMPL
sequences and corresponding image data. Our approach outperforms
state-of-the-art models by up to 27.5% accuracy in motion-context retrieval,
when evaluated on the WayMoCo dataset.

</details>


### [73] [Wukong Framework for Not Safe For Work Detection in Text-to-Image systems](https://arxiv.org/abs/2508.00591)
*Mingrui Liu,Sixiao Zhang,Cheng Long*

Main category: cs.CV

TL;DR: Wukong是一个基于Transformer的NSFW检测框架，利用扩散模型早期去噪步骤的中间输出，实现高效且准确的NSFW内容检测。


<details>
  <summary>Details</summary>
Motivation: 现有外部保护措施（文本过滤器和图像过滤器）存在效率或准确性不足的问题，需要一种更高效且准确的NSFW检测方法。

Method: Wukong利用扩散模型早期去噪步骤的中间输出和预训练的U-Net跨注意力参数，在图像生成过程中进行早期检测。

Result: Wukong在准确率上与图像过滤器相当，但效率显著更高，显著优于基于文本的过滤器。

Conclusion: Wukong提供了一种高效且准确的NSFW检测方法，适用于现代T2I系统。

Abstract: Text-to-Image (T2I) generation is a popular AI-generated content (AIGC)
technology enabling diverse and creative image synthesis. However, some outputs
may contain Not Safe For Work (NSFW) content (e.g., violence), violating
community guidelines. Detecting NSFW content efficiently and accurately, known
as external safeguarding, is essential. Existing external safeguards fall into
two types: text filters, which analyze user prompts but overlook T2I
model-specific variations and are prone to adversarial attacks; and image
filters, which analyze final generated images but are computationally costly
and introduce latency. Diffusion models, the foundation of modern T2I systems
like Stable Diffusion, generate images through iterative denoising using a
U-Net architecture with ResNet and Transformer blocks. We observe that: (1)
early denoising steps define the semantic layout of the image, and (2)
cross-attention layers in U-Net are crucial for aligning text and image
regions. Based on these insights, we propose Wukong, a transformer-based NSFW
detection framework that leverages intermediate outputs from early denoising
steps and reuses U-Net's pre-trained cross-attention parameters. Wukong
operates within the diffusion process, enabling early detection without waiting
for full image generation. We also introduce a new dataset containing prompts,
seeds, and image-specific NSFW labels, and evaluate Wukong on this and two
public benchmarks. Results show that Wukong significantly outperforms
text-based safeguards and achieves comparable accuracy of image filters, while
offering much greater efficiency.

</details>


### [74] [GeoMoE: Divide-and-Conquer Motion Field Modeling with Mixture-of-Experts for Two-View Geometry](https://arxiv.org/abs/2508.00592)
*Jiajun Le,Jiayi Ma*

Main category: cs.CV

TL;DR: GeoMoE利用混合专家模型（MoE）分解运动场，通过概率先验引导分解和双路径校正器，显著提升了双视几何中的运动场建模效果。


<details>
  <summary>Details</summary>
Motivation: 在复杂场景中，现有方法难以处理运动场的异质性，导致估计结果偏离真实结构。

Method: 提出GeoMoE框架，包括概率先验引导分解和MoE增强的双路径校正器，以分而治之的方式建模异质运动模式。

Result: 在相对位姿和单应性估计任务中优于现有方法，并表现出强泛化能力。

Conclusion: GeoMoE通过分治策略和专家定制化建模，有效解决了运动场异质性问题，提升了双视几何的性能。

Abstract: Recent progress in two-view geometry increasingly emphasizes enforcing
smoothness and global consistency priors when estimating motion fields between
pairs of images. However, in complex real-world scenes, characterized by
extreme viewpoint and scale changes as well as pronounced depth
discontinuities, the motion field often exhibits diverse and heterogeneous
motion patterns. Most existing methods lack targeted modeling strategies and
fail to explicitly account for this variability, resulting in estimated motion
fields that diverge from their true underlying structure and distribution. We
observe that Mixture-of-Experts (MoE) can assign dedicated experts to motion
sub-fields, enabling a divide-and-conquer strategy for heterogeneous motion
patterns. Building on this insight, we re-architect motion field modeling in
two-view geometry with GeoMoE, a streamlined framework. Specifically, we first
devise a Probabilistic Prior-Guided Decomposition strategy that exploits inlier
probability signals to perform a structure-aware decomposition of the motion
field into heterogeneous sub-fields, sharply curbing outlier-induced bias.
Next, we introduce an MoE-Enhanced Bi-Path Rectifier that enhances each
sub-field along spatial-context and channel-semantic paths and routes it to a
customized expert for targeted modeling, thereby decoupling heterogeneous
motion regimes, suppressing cross-sub-field interference and representational
entanglement, and yielding fine-grained motion-field rectification. With this
minimalist design, GeoMoE outperforms prior state-of-the-art methods in
relative pose and homography estimation and shows strong generalization. The
source code and pre-trained models are available at
https://github.com/JiajunLe/GeoMoE.

</details>


### [75] [DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior](https://arxiv.org/abs/2508.00599)
*Junzhe Lu,Jing Lin,Hongkun Dou,Ailing Zeng,Yue Deng,Xian Liu,Zhongang Cai,Lei Yang,Yulun Zhang,Haoqian Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: DPoser-X是一种基于扩散的3D全身人体姿态先验模型，通过变分扩散采样解决姿态任务，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 构建一个通用且鲁棒的全身人体姿态先验模型具有挑战性，主要由于人体姿态的复杂性和高质量数据集的稀缺。

Method: 采用扩散模型（DPoser）并扩展为DPoser-X，提出截断时间步调度和掩码训练机制，结合全身和局部数据集。

Result: 在身体、手、脸和全身姿态建模的多个基准测试中表现优于现有方法。

Conclusion: DPoser-X为全身人体姿态先验建模设立了新标杆。

Abstract: We present DPoser-X, a diffusion-based prior model for 3D whole-body human
poses. Building a versatile and robust full-body human pose prior remains
challenging due to the inherent complexity of articulated human poses and the
scarcity of high-quality whole-body pose datasets. To address these
limitations, we introduce a Diffusion model as body Pose prior (DPoser) and
extend it to DPoser-X for expressive whole-body human pose modeling. Our
approach unifies various pose-centric tasks as inverse problems, solving them
through variational diffusion sampling. To enhance performance on downstream
applications, we introduce a novel truncated timestep scheduling method
specifically designed for pose data characteristics. We also propose a masked
training mechanism that effectively combines whole-body and part-specific
datasets, enabling our model to capture interdependencies between body parts
while avoiding overfitting to specific actions. Extensive experiments
demonstrate DPoser-X's robustness and versatility across multiple benchmarks
for body, hand, face, and full-body pose modeling. Our model consistently
outperforms state-of-the-art alternatives, establishing a new benchmark for
whole-body human pose prior modeling.

</details>


### [76] [Backdoor Attacks on Deep Learning Face Detection](https://arxiv.org/abs/2508.00620)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi*

Main category: cs.CV

TL;DR: 论文研究了针对人脸检测系统的攻击方法，包括Face Generation Attacks和Landmark Shift Attack，并提出了防御措施。


<details>
  <summary>Details</summary>
Motivation: 在无约束环境中，人脸识别系统面临光照、姿态等挑战，需要依赖人脸检测模块。然而，这些模块容易受到攻击，因此研究其脆弱性及防御方法至关重要。

Method: 提出了两种攻击方法：Face Generation Attacks和Landmark Shift Attack，用于干扰人脸检测中的边界框和关键点坐标回归任务。

Result: 实验证明了这些攻击方法的有效性，能够成功干扰人脸检测系统。

Conclusion: 论文揭示了人脸检测系统的安全漏洞，并提出了相应的防御措施，为未来的安全研究提供了方向。

Abstract: Face Recognition Systems that operate in unconstrained environments capture
images under varying conditions,such as inconsistent lighting, or diverse face
poses. These challenges require including a Face Detection module that
regresses bounding boxes and landmark coordinates for proper Face Alignment.
This paper shows the effectiveness of Object Generation Attacks on Face
Detection, dubbed Face Generation Attacks, and demonstrates for the first time
a Landmark Shift Attack that backdoors the coordinate regression task performed
by face detectors. We then offer mitigations against these vulnerabilities.

</details>


### [77] [Minimum Data, Maximum Impact: 20 annotated samples for explainable lung nodule classification](https://arxiv.org/abs/2508.00639)
*Luisa Gallée,Catharina Silvia Lisson,Christoph Gerhard Lisson,Daniela Drees,Felix Weig,Daniel Vogele,Meinrad Beer,Michael Götz*

Main category: cs.CV

TL;DR: 论文提出通过生成模型合成带有病理视觉属性的医学图像数据，以解决标注数据稀缺问题，提升可解释模型的性能。


<details>
  <summary>Details</summary>
Motivation: 增强医学图像诊断中可解释模型的透明性和临床适用性，解决标注数据不足的限制。

Method: 使用扩散模型生成带有属性标注的合成数据，仅需少量真实标注样本（20个肺结节样本）。

Result: 合成数据显著提升模型性能，属性预测准确率提高13.4%，目标预测准确率提高1.8%。

Conclusion: 合成数据可有效克服数据集限制，推动可解释模型在医学图像分析中的应用。

Abstract: Classification models that provide human-interpretable explanations enhance
clinicians' trust and usability in medical image diagnosis. One research focus
is the integration and prediction of pathology-related visual attributes used
by radiologists alongside the diagnosis, aligning AI decision-making with
clinical reasoning. Radiologists use attributes like shape and texture as
established diagnostic criteria and mirroring these in AI decision-making both
enhances transparency and enables explicit validation of model outputs.
However, the adoption of such models is limited by the scarcity of large-scale
medical image datasets annotated with these attributes. To address this
challenge, we propose synthesizing attribute-annotated data using a generative
model. We enhance the Diffusion Model with attribute conditioning and train it
using only 20 attribute-labeled lung nodule samples from the LIDC-IDRI dataset.
Incorporating its generated images into the training of an explainable model
boosts performance, increasing attribute prediction accuracy by 13.4% and
target prediction accuracy by 1.8% compared to training with only the small
real attribute-annotated dataset. This work highlights the potential of
synthetic data to overcome dataset limitations, enhancing the applicability of
explainable models in medical image analysis.

</details>


### [78] [Revisiting Adversarial Patch Defenses on Object Detectors: Unified Evaluation, Large-Scale Dataset, and New Insights](https://arxiv.org/abs/2508.00649)
*Junhao Zheng,Jiahao Sun,Chenhao Lin,Zhengyu Zhao,Chen Ma,Chong Zhang,Cong Wang,Qian Wang,Chao Shen*

Main category: cs.CV

TL;DR: 论文提出了首个针对目标检测器补丁攻击的防御基准，分析了11种防御方法，揭示了新见解，并发布了大规模对抗补丁数据集。


<details>
  <summary>Details</summary>
Motivation: 现有防御评估缺乏统一框架，导致评估不一致且不完整，需建立全面基准以改进防御方法。

Method: 重新评估11种代表性防御方法，构建包含2种攻击目标、13种补丁攻击、11种目标检测器和4种指标的基准，并发布包含94种补丁和94,000张图像的数据集。

Result: 发现防御自然补丁的难点在于数据分布而非高频特征；新数据集可提升现有防御性能15.09% AP@0.5；自适应攻击能显著绕过现有防御。

Conclusion: 研究为补丁攻击/防御的评估和设计提供了指导，并开源了代码和数据集以促进未来研究。

Abstract: Developing reliable defenses against patch attacks on object detectors has
attracted increasing interest. However, we identify that existing defense
evaluations lack a unified and comprehensive framework, resulting in
inconsistent and incomplete assessments of current methods. To address this
issue, we revisit 11 representative defenses and present the first patch
defense benchmark, involving 2 attack goals, 13 patch attacks, 11 object
detectors, and 4 diverse metrics. This leads to the large-scale adversarial
patch dataset with 94 types of patches and 94,000 images. Our comprehensive
analyses reveal new insights: (1) The difficulty in defending against
naturalistic patches lies in the data distribution, rather than the commonly
believed high frequencies. Our new dataset with diverse patch distributions can
be used to improve existing defenses by 15.09% AP@0.5. (2) The average
precision of the attacked object, rather than the commonly pursued patch
detection accuracy, shows high consistency with defense performance. (3)
Adaptive attacks can substantially bypass existing defenses, and defenses with
complex/stochastic models or universal patch properties are relatively robust.
We hope that our analyses will serve as guidance on properly evaluating patch
attacks/defenses and advancing their design. Code and dataset are available at
https://github.com/Gandolfczjh/APDE, where we will keep integrating new
attacks/defenses.

</details>


### [79] [Can Large Pretrained Depth Estimation Models Help With Image Dehazing?](https://arxiv.org/abs/2508.00698)
*Hongfei Zhang,Kun Zhou,Ruizheng Wu,Jiangbo Lu*

Main category: cs.CV

TL;DR: 论文提出了一种基于预训练深度特征的图像去雾方法，通过RGB-D融合模块提升去雾效果和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有去雾方法受限于特定架构设计，难以适应不同场景的需求。

Method: 利用预训练的深度特征，设计了一个即插即用的RGB-D融合模块。

Result: 实验验证了该方法在多个基准数据集上的有效性和广泛适用性。

Conclusion: 预训练的深度特征在去雾任务中具有显著的一致性和适应性。

Abstract: Image dehazing remains a challenging problem due to the spatially varying
nature of haze in real-world scenes. While existing methods have demonstrated
the promise of large-scale pretrained models for image dehazing, their
architecture-specific designs hinder adaptability across diverse scenarios with
different accuracy and efficiency requirements. In this work, we systematically
investigate the generalization capability of pretrained depth
representations-learned from millions of diverse images-for image dehazing. Our
empirical analysis reveals that the learned deep depth features maintain
remarkable consistency across varying haze levels. Building on this insight, we
propose a plug-and-play RGB-D fusion module that seamlessly integrates with
diverse dehazing architectures. Extensive experiments across multiple
benchmarks validate both the effectiveness and broad applicability of our
approach.

</details>


### [80] [D3: Training-Free AI-Generated Video Detection Using Second-Order Features](https://arxiv.org/abs/2508.00701)
*Chende Zheng,Ruiqi suo,Chenhao Lin,Zhengyu Zhao,Le Yang,Shuai Liu,Minghui Yang,Cong Wang,Chao Shen*

Main category: cs.CV

TL;DR: 论文提出了一种基于二阶动力学分析的无训练检测方法D3，用于识别AI生成视频中的时间伪影，并在多个数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法对合成视频中时间伪影的探索不足，导致检测效果有限。

Method: 通过牛顿力学下的二阶动力学分析建立理论框架，提出基于二阶中心差分特征的D3方法。

Result: 在4个开源数据集上，D3表现优于现有方法，例如在Gen-Video上平均精度提升10.39%。

Conclusion: D3在计算效率和鲁棒性方面表现出色，为合成视频检测提供了新思路。

Abstract: The evolution of video generation techniques, such as Sora, has made it
increasingly easy to produce high-fidelity AI-generated videos, raising public
concern over the dissemination of synthetic content. However, existing
detection methodologies remain limited by their insufficient exploration of
temporal artifacts in synthetic videos. To bridge this gap, we establish a
theoretical framework through second-order dynamical analysis under Newtonian
mechanics, subsequently extending the Second-order Central Difference features
tailored for temporal artifact detection. Building on this theoretical
foundation, we reveal a fundamental divergence in second-order feature
distributions between real and AI-generated videos. Concretely, we propose
Detection by Difference of Differences (D3), a novel training-free detection
method that leverages the above second-order temporal discrepancies. We
validate the superiority of our D3 on 4 open-source datasets (Gen-Video,
VideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo,
D3 outperforms the previous best method by 10.39% (absolute) mean Average
Precision. Additional experiments on time cost and post-processing operations
demonstrate D3's exceptional computational efficiency and strong robust
performance. Our code is available at https://github.com/Zig-HS/D3.

</details>


### [81] [MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2508.00726)
*Jiale Li,Mingrui Wu,Zixiang Jin,Hao Chen,Jiayi Ji,Xiaoshuai Sun,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: 该论文首次系统研究了多图像多模态大语言模型（MLLMs）中的幻觉问题，并提出了专门用于评估多图像场景下对象相关幻觉的基准MIHBench。通过动态注意力平衡机制，有效减少了幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单图像场景下的幻觉问题，多图像场景下的幻觉研究尚属空白。论文旨在填补这一研究空白。

Method: 提出MIHBench基准，包含三个核心任务；设计动态注意力平衡机制，调整图像间注意力分布。

Result: 实验表明，动态注意力平衡机制能有效减少幻觉现象，提升多图像场景下的语义整合和推理稳定性。

Conclusion: 论文为多图像MLLMs中的幻觉问题提供了系统研究，并提出了一种有效的解决方案。

Abstract: Despite growing interest in hallucination in Multimodal Large Language
Models, existing studies primarily focus on single-image settings, leaving
hallucination in multi-image scenarios largely unexplored. To address this gap,
we conduct the first systematic study of hallucinations in multi-image MLLMs
and propose MIHBench, a benchmark specifically tailored for evaluating
object-related hallucinations across multiple images. MIHBench comprises three
core tasks: Multi-Image Object Existence Hallucination, Multi-Image Object
Count Hallucination, and Object Identity Consistency Hallucination, targeting
semantic understanding across object existence, quantity reasoning, and
cross-view identity consistency. Through extensive evaluation, we identify key
factors associated with the occurrence of multi-image hallucinations,
including: a progressive relationship between the number of image inputs and
the likelihood of hallucination occurrences; a strong correlation between
single-image hallucination tendencies and those observed in multi-image
contexts; and the influence of same-object image ratios and the positional
placement of negative samples within image sequences on the occurrence of
object identity consistency hallucination. To address these challenges, we
propose a Dynamic Attention Balancing mechanism that adjusts inter-image
attention distributions while preserving the overall visual attention
proportion. Experiments across multiple state-of-the-art MLLMs demonstrate that
our method effectively reduces hallucination occurrences and enhances semantic
integration and reasoning stability in multi-image scenarios.

</details>


### [82] [YOLO-Count: Differentiable Object Counting for Text-to-Image Generation](https://arxiv.org/abs/2508.00728)
*Guanning Zeng,Xiang Zhang,Zirui Wang,Haiyang Xu,Zeyuan Chen,Bingnan Li,Zhuowen Tu*

Main category: cs.CV

TL;DR: YOLO-Count是一个可微分的开放词汇对象计数模型，解决了通用计数问题并为文本到图像生成提供精确数量控制。


<details>
  <summary>Details</summary>
Motivation: 解决开放词汇计数与文本到图像生成控制之间的差距，同时应对对象大小和空间分布的变化。

Method: 提出'基数'图作为回归目标，结合表示对齐和强弱监督混合方案，采用完全可微分架构。

Result: 在实验中表现出最先进的计数准确性，并为文本到图像系统提供有效的数量控制。

Conclusion: YOLO-Count在计数和生成控制方面均表现出色，具有广泛的应用潜力。

Abstract: We propose YOLO-Count, a differentiable open-vocabulary object counting model
that tackles both general counting challenges and enables precise quantity
control for text-to-image (T2I) generation. A core contribution is the
'cardinality' map, a novel regression target that accounts for variations in
object size and spatial distribution. Leveraging representation alignment and a
hybrid strong-weak supervision scheme, YOLO-Count bridges the gap between
open-vocabulary counting and T2I generation control. Its fully differentiable
architecture facilitates gradient-based optimization, enabling accurate object
count estimation and fine-grained guidance for generative models. Extensive
experiments demonstrate that YOLO-Count achieves state-of-the-art counting
accuracy while providing robust and effective quantity control for T2I systems.

</details>


### [83] [Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR](https://arxiv.org/abs/2508.00744)
*Adwait Chandorkar,Hasan Tercan,Tobias Meisen*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级骨干网络Dense Backbone，用于3D目标检测，显著降低了计算成本，同时保持了高检测精度。


<details>
  <summary>Details</summary>
Motivation: 尽管LiDAR-based 3D目标检测取得了进展，但现有方法仍依赖复杂的主干网络（如VGG或ResNet），增加了模型复杂度。轻量级主干在2D检测中已有研究，但在3D检测中仍有限。

Method: 提出Dense Backbone，结合高速处理、轻量架构和鲁棒检测精度，并适配了多种SoTA 3D检测器（如PillarNet）。

Result: DensePillarNet在nuScenes测试集上减少了29%的参数量和28%的延迟，仅损失2%的检测精度。

Conclusion: Dense Backbone是一种专为点云数据设计的轻量级主干网络，具有即插即用的特性，易于集成到现有架构中。

Abstract: Recent advancements in LiDAR-based 3D object detection have significantly
accelerated progress toward the realization of fully autonomous driving in
real-world environments. Despite achieving high detection performance, most of
the approaches still rely on a VGG-based or ResNet-based backbone for feature
exploration, which increases the model complexity. Lightweight backbone design
is well-explored for 2D object detection, but research on 3D object detection
still remains limited. In this work, we introduce Dense Backbone, a lightweight
backbone that combines the benefits of high processing speed, lightweight
architecture, and robust detection accuracy. We adapt multiple SoTA 3d object
detectors, such as PillarNet, with our backbone and show that with our
backbone, these models retain most of their detection capability at a
significantly reduced computational cost. To our knowledge, this is the first
dense-layer-based backbone tailored specifically for 3D object detection from
point cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29%
reduction in model parameters and a 28% reduction in latency with just a 2%
drop in detection accuracy on the nuScenes test set. Furthermore, Dense
Backbone's plug-and-play design allows straightforward integration into
existing architectures, requiring no modifications to other network components.

</details>


### [84] [GECO: Geometrically Consistent Embedding with Lightspeed Inference](https://arxiv.org/abs/2508.00746)
*Regine Hartwig,Dominik Muhle,Riccardo Marin,Daniel Cremers*

Main category: cs.CV

TL;DR: GECO提出了一种基于最优传输的训练框架，生成几何一致的特征，显著提升了语义对应任务的性能，并引入了新的几何感知评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前自监督视觉基础模型在捕捉语义对应时缺乏对3D几何的感知，GECO旨在填补这一空白。

Method: 采用最优传输的训练框架，生成几何一致的特征，支持遮挡和去遮挡情况下的监督。

Result: GECO在PFPascal、APK和CUB数据集上实现了6.0%、6.2%和4.1%的PCK提升，运行速度比现有方法快98.2%。

Conclusion: GECO不仅提升了性能，还揭示了PCK指标的局限性，并提出了更全面的几何感知评估方法。

Abstract: Recent advances in feature learning have shown that self-supervised vision
foundation models can capture semantic correspondences but often lack awareness
of underlying 3D geometry. GECO addresses this gap by producing geometrically
coherent features that semantically distinguish parts based on geometry (e.g.,
left/right eyes, front/back legs). We propose a training framework based on
optimal transport, enabling supervision beyond keypoints, even under occlusions
and disocclusions. With a lightweight architecture, GECO runs at 30 fps, 98.2%
faster than prior methods, while achieving state-of-the-art performance on
PFPascal, APK, and CUB, improving PCK by 6.0%, 6.2%, and 4.1%, respectively.
Finally, we show that PCK alone is insufficient to capture geometric quality
and introduce new metrics and insights for more geometry-aware feature
learning. Link to project page:
https://reginehartwig.github.io/publications/geco/

</details>


### [85] [Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos](https://arxiv.org/abs/2508.00748)
*Laura Pedrouzo-Rodriguez,Pedro Delgado-DeRobles,Luis F. Gomez,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez*

Main category: cs.CV

TL;DR: 论文探讨了在逼真头像交流中，通过面部运动模式作为行为生物特征进行身份验证的可行性，并提出了一种轻量级的时空图卷积网络架构。


<details>
  <summary>Details</summary>
Motivation: 逼真头像在虚拟交流中广泛应用，但存在冒充风险，需要可靠的身份验证方法。

Method: 使用GAGAvatar生成真实和冒充头像视频数据集，提出基于面部标志的时空图卷积网络架构。

Result: 实验显示面部运动特征可实现有效身份验证，AUC值接近80%。

Conclusion: 研究强调了头像交流中行为生物特征防御的重要性，并提供了公开数据集和系统。

Abstract: Photorealistic talking-head avatars are becoming increasingly common in
virtual meetings, gaming, and social platforms. These avatars allow for more
immersive communication, but they also introduce serious security risks. One
emerging threat is impersonation: an attacker can steal a user's
avatar-preserving their appearance and voice-making it nearly impossible to
detect its fraudulent usage by sight or sound alone. In this paper, we explore
the challenge of biometric verification in such avatar-mediated scenarios. Our
main question is whether an individual's facial motion patterns can serve as
reliable behavioral biometrics to verify their identity when the avatar's
visual appearance is a facsimile of its owner. To answer this question, we
introduce a new dataset of realistic avatar videos created using a
state-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and
impostor avatar videos. We also propose a lightweight, explainable
spatio-temporal Graph Convolutional Network architecture with temporal
attention pooling, that uses only facial landmarks to model dynamic facial
gestures. Experimental results demonstrate that facial motion cues enable
meaningful identity verification with AUC values approaching 80%. The proposed
benchmark and biometric system are available for the research community in
order to bring attention to the urgent need for more advanced behavioral
biometric defenses in avatar-based communication systems.

</details>


### [86] [Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation](https://arxiv.org/abs/2508.00766)
*Irene Iele,Francesco Di Feola,Valerio Guarrasi,Paolo Soda*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的测试时间适应（TTA）框架，用于医学图像到图像转换任务，动态调整翻译过程以处理分布外样本，同时保持分布内样本的性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像转换在处理分布外样本时性能下降，现有方法无法动态适应不同样本的需求。

Method: 引入重建模块量化域偏移，并通过动态适应块选择性修改预训练模型的内部特征。

Result: 在低剂量CT去噪和T1到T2 MRI转换任务中，性能优于基线模型和现有TTA方法。

Conclusion: 动态、样本特定的调整是提高模型在真实场景中鲁棒性的有效途径。

Abstract: Image-to-image translation has emerged as a powerful technique in medical
imaging, enabling tasks such as image denoising and cross-modality conversion.
However, it suffers from limitations in handling out-of-distribution samples
without causing performance degradation. To address this limitation, we propose
a novel Test-Time Adaptation (TTA) framework that dynamically adjusts the
translation process based on the characteristics of each test sample. Our
method introduces a Reconstruction Module to quantify the domain shift and a
Dynamic Adaptation Block that selectively modifies the internal features of a
pretrained translation model to mitigate the shift without compromising the
performance on in-distribution samples that do not require adaptation. We
evaluate our approach on two medical image-to-image translation tasks: low-dose
CT denoising and T1 to T2 MRI translation, showing consistent improvements over
both the baseline translation model without TTA and prior TTA methods. Our
analysis highlights the limitations of the state-of-the-art that uniformly
apply the adaptation to both out-of-distribution and in-distribution samples,
demonstrating that dynamic, sample-specific adjustment offers a promising path
to improve model resilience in real-world scenarios. The code is available at:
https://github.com/cosbidev/Sample-Aware_TTA.

</details>


### [87] [Zero-Shot Anomaly Detection with Dual-Branch Prompt Learning](https://arxiv.org/abs/2508.00777)
*Zihan Wang,Samira Ebrahimi Kahou,Narges Armanfard*

Main category: cs.CV

TL;DR: PILOT框架通过双分支提示学习和无标签测试时适应策略，解决了零样本异常检测在域偏移下的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有零样本异常检测方法在域偏移下表现不佳，因其训练数据有限且无法适应新分布。

Method: PILOT采用双分支提示学习机制和测试时适应策略，动态整合可学习提示与语义属性。

Result: 在13个工业和医学基准测试中，PILOT在域偏移下的异常检测和定位性能达到最佳。

Conclusion: PILOT通过创新方法显著提升了零样本异常检测在域偏移下的表现。

Abstract: Zero-shot anomaly detection (ZSAD) enables identifying and localizing defects
in unseen categories by relying solely on generalizable features rather than
requiring any labeled examples of anomalies. However, existing ZSAD methods,
whether using fixed or learned prompts, struggle under domain shifts because
their training data are derived from limited training domains and fail to
generalize to new distributions. In this paper, we introduce PILOT, a framework
designed to overcome these challenges through two key innovations: (1) a novel
dual-branch prompt learning mechanism that dynamically integrates a pool of
learnable prompts with structured semantic attributes, enabling the model to
adaptively weight the most relevant anomaly cues for each input image; and (2)
a label-free test-time adaptation strategy that updates the learnable prompt
parameters using high-confidence pseudo-labels from unlabeled test data.
Extensive experiments on 13 industrial and medical benchmarks demonstrate that
PILOT achieves state-of-the-art performance in both anomaly detection and
localization under domain shift.

</details>


### [88] [Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning](https://arxiv.org/abs/2508.00822)
*Alexander Nikitas Dimopoulos,Joseph Grasso*

Main category: cs.CV

TL;DR: 研究分析了异构标注点云数据集在公共安全应用中的语义分割性能，发现几何大物体分割效果较好，而小物体识别率较低，需改进标注标准化和技术。


<details>
  <summary>Details</summary>
Motivation: 公共安全应用中的点云数据标注异构性导致语义分割性能不稳定，需研究统一标注和提升小物体检测的方法。

Method: 采用KPConv架构和分级标注方案，通过IoU指标评估公共安全相关特征的性能。

Result: 大物体（如楼梯、窗户）分割效果较好，小物体识别率低，受类别不平衡和几何区分度限制。

Conclusion: 公共安全点云语义分割需标准化标注协议和改进标注技术，以解决数据异构性和小物体检测问题。

Abstract: This study analyzes semantic segmentation performance across heterogeneously
labeled point-cloud datasets relevant to public safety applications, including
pre-incident planning systems derived from lidar scans. Using NIST's Point
Cloud City dataset (Enfield and Memphis collections), we investigate challenges
in unifying differently labeled 3D data. Our methodology employs a graded
schema with the KPConv architecture, evaluating performance through IoU metrics
on safety-relevant features. Results indicate performance variability:
geometrically large objects (e.g. stairs, windows) achieve higher segmentation
performance, suggesting potential for navigational context, while smaller
safety-critical features exhibit lower recognition rates. Performance is
impacted by class imbalance and the limited geometric distinction of smaller
objects in typical lidar scans, indicating limitations in detecting certain
safety-relevant features using current point-cloud methods. Key identified
challenges include insufficient labeled data, difficulties in unifying class
labels across datasets, and the need for standardization. Potential directions
include automated labeling and multi-dataset learning strategies. We conclude
that reliable point-cloud semantic segmentation for public safety necessitates
standardized annotation protocols and improved labeling techniques to address
data heterogeneity and the detection of small, safety-critical elements.

</details>


### [89] [IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation](https://arxiv.org/abs/2508.00823)
*Wenxuan Guo,Xiuwei Xu,Hang Yin,Ziwei Wang,Jianjiang Feng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: IGL-Nav提出了一种基于增量3D高斯定位的高效图像目标导航框架，通过逐步更新场景表示和几何信息匹配，显著提升了导航性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以充分建模3D环境与目标图像之间的几何关系，且计算效率低。

Method: 采用增量式3D高斯表示，结合粗定位和细优化策略，实现高效导航。

Result: IGL-Nav在多种实验配置下大幅超越现有方法，并能处理自由视角图像目标导航。

Conclusion: IGL-Nav是一种高效、3D感知的图像目标导航解决方案，适用于实际机器人平台。

Abstract: Visual navigation with an image as goal is a fundamental and challenging
problem. Conventional methods either rely on end-to-end RL learning or
modular-based policy with topological graph or BEV map as memory, which cannot
fully model the geometric relationship between the explored 3D environment and
the goal image. In order to efficiently and accurately localize the goal image
in 3D space, we build our navigation system upon the renderable 3D gaussian
(3DGS) representation. However, due to the computational intensity of 3DGS
optimization and the large search space of 6-DoF camera pose, directly
leveraging 3DGS for image localization during agent exploration process is
prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D
Gaussian Localization framework for efficient and 3D-aware image-goal
navigation. Specifically, we incrementally update the scene representation as
new images arrive with feed-forward monocular prediction. Then we coarsely
localize the goal by leveraging the geometric information for discrete space
matching, which can be equivalent to efficient 3D convolution. When the agent
is close to the goal, we finally solve the fine target pose with optimization
via differentiable rendering. The proposed IGL-Nav outperforms existing
state-of-the-art methods by a large margin across diverse experimental
configurations. It can also handle the more challenging free-view image-goal
setting and be deployed on real-world robotic platform using a cellphone to
capture goal image at arbitrary pose. Project page:
https://gwxuan.github.io/IGL-Nav/.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [90] [GEPAR3D: Geometry Prior-Assisted Learning for 3D Tooth Segmentation](https://arxiv.org/abs/2508.00155)
*Tomasz Szczepański,Szymon Płotka,Michal K. Grzeszczyk,Arleta Adamowicz,Piotr Fudalej,Przemysław Korzeniowski,Tomasz Trzciński,Arkadiusz Sitek*

Main category: eess.IV

TL;DR: GEPAR3D是一种新颖的方法，将实例检测和多类分割统一为一步，显著提高了CBCT中牙齿根尖的细分精度。


<details>
  <summary>Details</summary>
Motivation: CBCT中的牙齿分割，尤其是根尖等精细结构的分割，对正畸中根吸收评估至关重要，但现有方法存在挑战。

Method: GEPAR3D结合了统计形状模型作为几何先验，并采用深度分水岭方法，将每颗牙齿建模为连续的3D能量盆地。

Result: 在多个测试集上，GEPAR3D的平均Dice相似系数达到95.0%，召回率提高9.5%，显著优于其他方法。

Conclusion: GEPAR3D在根尖分割质量上有显著提升，有望为临床决策提供更准确的根吸收评估。

Abstract: Tooth segmentation in Cone-Beam Computed Tomography (CBCT) remains
challenging, especially for fine structures like root apices, which is critical
for assessing root resorption in orthodontics. We introduce GEPAR3D, a novel
approach that unifies instance detection and multi-class segmentation into a
single step tailored to improve root segmentation. Our method integrates a
Statistical Shape Model of dentition as a geometric prior, capturing anatomical
context and morphological consistency without enforcing restrictive adjacency
constraints. We leverage a deep watershed method, modeling each tooth as a
continuous 3D energy basin encoding voxel distances to boundaries. This
instance-aware representation ensures accurate segmentation of narrow, complex
root apices. Trained on publicly available CBCT scans from a single center, our
method is evaluated on external test sets from two in-house and two public
medical centers. GEPAR3D achieves the highest overall segmentation performance,
averaging a Dice Similarity Coefficient (DSC) of 95.0% (+2.8% over the
second-best method) and increasing recall to 95.2% (+9.5%) across all test
sets. Qualitative analyses demonstrated substantial improvements in root
segmentation quality, indicating significant potential for more accurate root
resorption assessment and enhanced clinical decision-making in orthodontics. We
provide the implementation and dataset at https://github.com/tomek1911/GEPAR3D.

</details>


### [91] [On the Utility of Virtual Staining for Downstream Applications as it relates to Task Network Capacity](https://arxiv.org/abs/2508.00164)
*Sourya Sengupta,Jianquan Xu,Phuong Nguyen,Frank J. Brooks,Yang Liu,Mark A. Anastasio*

Main category: eess.IV

TL;DR: 虚拟染色技术通过深度学习生成荧光图像，但其对下游任务（如分割或分类）的效用取决于任务网络的容量。


<details>
  <summary>Details</summary>
Motivation: 研究虚拟染色在生物医学成像中对临床相关下游任务的实用性，考虑深度学习网络的容量影响。

Method: 使用生物数据集，比较无标记、虚拟染色和真实荧光图像在下游任务中的表现。

Result: 虚拟染色的效用与任务网络提取信息的能力相关，高容量网络下虚拟染色可能无益甚至有害。

Conclusion: 决定是否使用虚拟染色时，应考虑任务网络的容量。

Abstract: Virtual staining, or in-silico-labeling, has been proposed to computationally
generate synthetic fluorescence images from label-free images by use of deep
learning-based image-to-image translation networks. In most reported studies,
virtually stained images have been assessed only using traditional image
quality measures such as structural similarity or signal-to-noise ratio.
However, in biomedical imaging, images are typically acquired to facilitate an
image-based inference, which we refer to as a downstream biological or clinical
task. This study systematically investigates the utility of virtual staining
for facilitating clinically relevant downstream tasks (like segmentation or
classification) with consideration of the capacity of the deep neural networks
employed to perform the tasks. Comprehensive empirical evaluations were
conducted using biological datasets, assessing task performance by use of
label-free, virtually stained, and ground truth fluorescence images. The
results demonstrated that the utility of virtual staining is largely dependent
on the ability of the segmentation or classification task network to extract
meaningful task-relevant information, which is related to the concept of
network capacity. Examples are provided in which virtual staining does not
improve, or even degrades, segmentation or classification performance when the
capacity of the associated task network is sufficiently large. The results
demonstrate that task network capacity should be considered when deciding
whether to perform virtual staining.

</details>


### [92] [Weakly Supervised Intracranial Aneurysm Detection and Segmentation in MR angiography via Multi-task UNet with Vesselness Prior](https://arxiv.org/abs/2508.00235)
*Erin Rainville,Amirhossein Rasoulian,Hassan Rivaz,Yiming Xiao*

Main category: eess.IV

TL;DR: 提出了一种弱监督3D多任务UNet模型，结合血管先验知识，用于颅内动脉瘤的检测和分割。


<details>
  <summary>Details</summary>
Motivation: 颅内动脉瘤（IAs）在影像中难以准确检测和分析，且缺乏大规模标注数据集，阻碍了深度学习算法的开发。

Method: 使用Frangi血管过滤器生成软脑血管先验，结合注意力机制，通过多任务UNet同时进行检测和分割。

Result: 在Lausanne和ADAM数据集上表现优异，分割（Dice=0.614）和检测（灵敏度92.9%）均优于现有技术。

Conclusion: 该方法在颅内动脉瘤的检测和分割中表现出高效性和泛化能力。

Abstract: Intracranial aneurysms (IAs) are abnormal dilations of cerebral blood vessels
that, if ruptured, can lead to life-threatening consequences. However, their
small size and soft contrast in radiological scans often make it difficult to
perform accurate and efficient detection and morphological analyses, which are
critical in the clinical care of the disorder. Furthermore, the lack of large
public datasets with voxel-wise expert annotations pose challenges for
developing deep learning algorithms to address the issues. Therefore, we
proposed a novel weakly supervised 3D multi-task UNet that integrates
vesselness priors to jointly perform aneurysm detection and segmentation in
time-of-flight MR angiography (TOF-MRA). Specifically, to robustly guide IA
detection and segmentation, we employ the popular Frangi's vesselness filter to
derive soft cerebrovascular priors for both network input and an attention
block to conduct segmentation from the decoder and detection from an auxiliary
branch. We train our model on the Lausanne dataset with coarse ground truth
segmentation, and evaluate it on the test set with refined labels from the same
database. To further assess our model's generalizability, we also validate it
externally on the ADAM dataset. Our results demonstrate the superior
performance of the proposed technique over the SOTA techniques for aneurysm
segmentation (Dice = 0.614, 95%HD =1.38mm) and detection (false positive rate =
1.47, sensitivity = 92.9%).

</details>


### [93] [Diffusion-Based User-Guided Data Augmentation for Coronary Stenosis Detection](https://arxiv.org/abs/2508.00438)
*Sumin Seo,In Kyu Lee,Hyun-Woo Kim,Jaesik Min,Chung-Hwan Jung*

Main category: eess.IV

TL;DR: 提出了一种基于扩散模型的数据增强方法，用于生成真实的冠状动脉狭窄病变，提升检测和分类性能。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉狭窄是缺血性心脏病的主要风险因素，但现有深度学习方法受限于标记数据不足和类别不平衡。

Method: 使用扩散模型进行数据增强，生成用户可控严重程度的病变图像。

Result: 在大规模内部和公共数据集上表现出色，尤其在数据有限时仍保持高性能。

Conclusion: 该方法能优化狭窄严重性评估，提升临床决策支持可靠性。

Abstract: Coronary stenosis is a major risk factor for ischemic heart events leading to
increased mortality, and medical treatments for this condition require
meticulous, labor-intensive analysis. Coronary angiography provides critical
visual cues for assessing stenosis, supporting clinicians in making informed
decisions for diagnosis and treatment. Recent advances in deep learning have
shown great potential for automated localization and severity measurement of
stenosis. In real-world scenarios, however, the success of these competent
approaches is often hindered by challenges such as limited labeled data and
class imbalance. In this study, we propose a novel data augmentation approach
that uses an inpainting method based on a diffusion model to generate realistic
lesions, allowing user-guided control of severity. Extensive evaluation on
lesion detection and severity classification across various synthetic dataset
sizes shows superior performance of our method on both a large-scale in-house
dataset and a public coronary angiography dataset. Furthermore, our approach
maintains high detection and classification performance even when trained with
limited data, highlighting its clinical importance in improving the assessment
of severity of stenosis and optimizing data utilization for more reliable
decision support.

</details>


### [94] [FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems](https://arxiv.org/abs/2508.00721)
*Yuxiang Wan,Ryan Devera,Wenjie Zhang,Ju Sun*

Main category: eess.IV

TL;DR: FMPlug是一个新颖的插件框架，通过利用观察与目标对象的相似性和生成流的高斯性，增强基础流匹配先验，解决不适定逆问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖领域特定或无训练先验，FMPlug通过智能利用简单但强大的洞察力，释放领域无关基础模型的潜力。

Method: 引入时间自适应预热策略和锐利高斯性正则化，优化基础流匹配先验。

Result: 在图像超分辨率和高斯去模糊任务上显著优于现有方法。

Conclusion: FMPlug通过新颖的框架设计，成功提升了基础模型在逆问题中的表现。

Abstract: We present FMPlug, a novel plug-in framework that enhances foundation
flow-matching (FM) priors for solving ill-posed inverse problems. Unlike
traditional approaches that rely on domain-specific or untrained priors, FMPlug
smartly leverages two simple but powerful insights: the similarity between
observed and desired objects and the Gaussianity of generative flows. By
introducing a time-adaptive warm-up strategy and sharp Gaussianity
regularization, FMPlug unlocks the true potential of domain-agnostic foundation
models. Our method beats state-of-the-art methods that use foundation FM priors
by significant margins, on image super-resolution and Gaussian deblurring.

</details>


### [95] [AI-Driven Collaborative Satellite Object Detection for Space Sustainability](https://arxiv.org/abs/2508.00755)
*Peng Hu,Wenxuan Zhang*

Main category: eess.IV

TL;DR: 提出了一种基于卫星集群的深度学习空间目标检测方法，通过协作优化检测性能，同时保持低SWaP。


<details>
  <summary>Details</summary>
Motivation: 低地球轨道卫星密度增加导致碰撞风险上升，传统地面跟踪系统存在延迟和覆盖限制，需要星载视觉检测能力。

Method: 构建高保真数据集模拟卫星集群成像场景，引入距离感知视角选择策略，评估深度学习模型。

Result: 集群方法在检测精度上优于单星和现有方法，同时保持低SWaP。

Conclusion: 分布式AI星载系统有望提升空间态势感知能力，促进长期空间可持续性。

Abstract: The growing density of satellites in low-Earth orbit (LEO) presents serious
challenges to space sustainability, primarily due to the increased risk of
in-orbit collisions. Traditional ground-based tracking systems are constrained
by latency and coverage limitations, underscoring the need for onboard,
vision-based space object detection (SOD) capabilities. In this paper, we
propose a novel satellite clustering framework that enables the collaborative
execution of deep learning (DL)-based SOD tasks across multiple satellites. To
support this approach, we construct a high-fidelity dataset simulating imaging
scenarios for clustered satellite formations. A distance-aware viewpoint
selection strategy is introduced to optimize detection performance, and recent
DL models are used for evaluation. Experimental results show that the
clustering-based method achieves competitive detection accuracy compared to
single-satellite and existing approaches, while maintaining a low size, weight,
and power (SWaP) footprint. These findings underscore the potential of
distributed, AI-enabled in-orbit systems to enhance space situational awareness
and contribute to long-term space sustainability.

</details>


### [96] [CADS: A Comprehensive Anatomical Dataset and Segmentation for Whole-Body Anatomy in Computed Tomography](https://arxiv.org/abs/2507.22953)
*Murong Xu,Tamaz Amiranashvili,Fernando Navarro,Maksym Fritsak,Ibrahim Ethem Hamamci,Suprosanna Shit,Bastian Wittmann,Sezgin Er,Sebastian M. Christ,Ezequiel de la Rosa,Julian Deseoe,Robert Graf,Hendrik Möller,Anjany Sekuboyina,Jan C. Peeken,Sven Becker,Giulia Baldini,Johannes Haubold,Felix Nensa,René Hosch,Nikhil Mirajkar,Saad Khalid,Stefan Zachow,Marc-André Weber,Georg Langs,Jakob Wasserthal,Mehmet Kemal Ozdemir,Andrey Fedorov,Ron Kikinis,Stephanie Tanadini-Lang,Jan S. Kirschke,Stephanie E. Combs,Bjoern Menze*

Main category: eess.IV

TL;DR: CADS是一个开源框架，专注于整合和标准化异构数据，用于全身CT分割，提供大规模数据集和模型，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前AI分割方法针对单个结构，导致模型碎片化且性能不一，缺乏全面训练数据。CADS旨在解决这些问题。

Method: 通过系统整合和标准化异构数据，构建包含22,022个CT扫描的大规模数据集，开发基于现有架构的CADS模型。

Result: CADS在18个公共数据集和真实医院队列中表现优于现有方法，并在放射肿瘤学任务中验证了临床实用性。

Conclusion: CADS通过开源数据集、模型和工具，推动了放射学中稳健AI解决方案的发展，使全面解剖分析更易获取。

Abstract: Accurate delineation of anatomical structures in volumetric CT scans is
crucial for diagnosis and treatment planning. While AI has advanced automated
segmentation, current approaches typically target individual structures,
creating a fragmented landscape of incompatible models with varying performance
and disparate evaluation protocols. Foundational segmentation models address
these limitations by providing a holistic anatomical view through a single
model. Yet, robust clinical deployment demands comprehensive training data,
which is lacking in existing whole-body approaches, both in terms of data
heterogeneity and, more importantly, anatomical coverage. In this work, rather
than pursuing incremental optimizations in model architecture, we present CADS,
an open-source framework that prioritizes the systematic integration,
standardization, and labeling of heterogeneous data sources for whole-body CT
segmentation. At its core is a large-scale dataset of 22,022 CT volumes with
complete annotations for 167 anatomical structures, representing a significant
advancement in both scale and coverage, with 18 times more scans than existing
collections and 60% more distinct anatomical targets. Building on this diverse
dataset, we develop the CADS-model using established architectures for
accessible and automated full-body CT segmentation. Through comprehensive
evaluation across 18 public datasets and an independent real-world hospital
cohort, we demonstrate advantages over SoTA approaches. Notably, thorough
testing of the model's performance in segmentation tasks from radiation
oncology validates its direct utility for clinical interventions. By making our
large-scale dataset, our segmentation models, and our clinical software tool
publicly available, we aim to advance robust AI solutions in radiology and make
comprehensive anatomical analysis accessible to clinicians and researchers
alike.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [97] [Occlusion-robust Stylization for Drawing-based 3D Animation](https://arxiv.org/abs/2508.00398)
*Sunjae Yoon,Gwanhyeong Koo,Younghwan Lee,Ji Woo Hong,Chang D. Yoo*

Main category: cs.GR

TL;DR: 本文提出了一种名为OSF的遮挡鲁棒风格化框架，用于解决基于绘图的3D动画中因遮挡导致的风格退化问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法在训练和推理时存在风格化姿态差距，导致在动态运动中遮挡情况下风格属性（如粗糙轮廓和独特笔触）退化。

Method: OSF利用光流提供遮挡鲁棒的边缘引导，替代传统的两阶段方法，实现单次运行。

Result: OSF在遮挡情况下保持风格一致性，推理速度提升2.4倍，内存减少2.1倍。

Conclusion: OSF有效解决了风格化姿态差距问题，提升了基于绘图的3D动画的质量和效率。

Abstract: 3D animation aims to generate a 3D animated video from an input image and a
target 3D motion sequence. Recent advances in image-to-3D models enable the
creation of animations directly from user-hand drawings. Distinguished from
conventional 3D animation, drawing-based 3D animation is crucial to preserve
artist's unique style properties, such as rough contours and distinct stroke
patterns. However, recent methods still exhibit quality deterioration in style
properties, especially under occlusions caused by overlapping body parts,
leading to contour flickering and stroke blurring. This occurs due to a
`stylization pose gap' between training and inference in stylization networks
designed to preserve drawing styles in drawing-based 3D animation systems. The
stylization pose gap denotes that input target poses used to train the
stylization network are always in occlusion-free poses, while target poses
encountered in an inference include diverse occlusions under dynamic motions.
To this end, we propose Occlusion-robust Stylization Framework (OSF) for
drawing-based 3D animation. We found that while employing object's edge can be
effective input prior for guiding stylization, it becomes notably inaccurate
when occlusions occur at inference. Thus, our proposed OSF provides
occlusion-robust edge guidance for stylization network using optical flow,
ensuring a consistent stylization even under occlusions. Furthermore, OSF
operates in a single run instead of the previous two-stage method, achieving
2.4x faster inference and 2.1x less memory.

</details>


### [98] [CrossSet: Unveiling the Complex Interplay of Two Set-typed Dimensions in Multivariate Data](https://arxiv.org/abs/2508.00424)
*Kresimir Matkovic,Rainer Splechtna,Denis Gracanin,Helwig Hauser*

Main category: cs.GR

TL;DR: CrossSet是一种新颖的方法，用于联合研究两个集合类型维度及其相互作用，通过分层矩阵布局实现多尺度交互式可视化分析。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅支持单个集合类型维度的研究，而联合分析两个集合类型维度的需求尚未得到充分满足。

Method: 基于任务分析，采用分层矩阵布局，结合概览和细化功能，实现多尺度交互式可视化。

Result: CrossSet在多个应用场景中验证了其有效性和效率，支持从概览到细节的多层次分析。

Conclusion: CrossSet为集合类型数据的交互式可视化分析提供了新的解决方案，尤其适用于双变量集合类型数据的多尺度探索。

Abstract: The interactive visual analysis of set-typed data, i.e., data with attributes
that are of type set, is a rewarding area of research and applications.
Valuable prior work has contributed solutions that enable the study of such
data with individual set-typed dimensions. In this paper, we present CrossSet,
a novel method for the joint study of two set-typed dimensions and their
interplay. Based on a task analysis, we describe a new, multi-scale approach to
the interactive visual exploration and analysis of such data. Two set-typed
data dimensions are jointly visualized using a hierarchical matrix layout,
enabling the analysis of the interactions between two set-typed attributes at
several levels, in addition to the analysis of individual such dimensions.
CrossSet is anchored at a compact, large-scale overview that is complemented by
drill-down opportunities to study the relations between and within the
set-typed dimensions, enabling an interactive visual multi-scale exploration
and analysis of bivariate set-typed data. Such an interactive approach makes it
possible to study single set-typed dimensions in detail, to gain an overview of
the interaction and association between two such dimensions, to refine one of
the dimensions to gain additional details at several levels, and to drill down
to the specific interactions of individual set-elements from the set-typed
dimensions. To demonstrate the effectiveness and efficiency of CrossSet, we
have evaluated the new method in the context of several application scenarios.

</details>


### [99] [Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation](https://arxiv.org/abs/2508.00428)
*Nan Xiang,Tianyi Liang,Haiwen Huang,Shiqi Jiang,Hao Huang,Yifei Huang,Liangyu Chen,Changbo Wang,Chenhui Li*

Main category: cs.GR

TL;DR: Sel3DCraft是一个用于文本到3D生成的视觉提示工程系统，通过双分支结构、多视图评分和视觉分析工具，提升了3D模型的生成效率和设计创造力。


<details>
  <summary>Details</summary>
Motivation: 当前文本到3D生成过程依赖盲目的试错提示，结果不可预测，亟需一种能够提供多视图一致性和空间理解的视觉提示工程方法。

Method: 提出Sel3DCraft系统，包含双分支结构（检索与生成）、多视图混合评分方法（结合MLLMs和高层次指标）以及视觉分析工具。

Result: 实验和用户研究表明，Sel3DCraft在支持设计师创造力方面优于其他文本到3D生成系统。

Conclusion: Sel3DCraft通过结构化视觉提示工程，显著提升了文本到3D生成的效率和设计质量。

Abstract: Text-to-3D (T23D) generation has transformed digital content creation, yet
remains bottlenecked by blind trial-and-error prompting processes that yield
unpredictable results. While visual prompt engineering has advanced in
text-to-image domains, its application to 3D generation presents unique
challenges requiring multi-view consistency evaluation and spatial
understanding. We present Sel3DCraft, a visual prompt engineering system for
T23D that transforms unstructured exploration into a guided visual process. Our
approach introduces three key innovations: a dual-branch structure combining
retrieval and generation for diverse candidate exploration; a multi-view hybrid
scoring approach that leverages MLLMs with innovative high-level metrics to
assess 3D models with human-expert consistency; and a prompt-driven visual
analytics suite that enables intuitive defect identification and refinement.
Extensive testing and user studies demonstrate that Sel3DCraft surpasses other
T23D systems in supporting creativity for designers.

</details>


### [100] [SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation](https://arxiv.org/abs/2508.00782)
*Kien T. Pham,Yingqing He,Yazhou Xing,Qifeng Chen,Long Chen*

Main category: cs.GR

TL;DR: SpA2V是一个利用音频空间线索生成视频的框架，通过两阶段方法实现语义和空间对齐。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注语义信息，忽略了音频中的空间属性（如位置和运动方向），而人类能自然感知这些信息。SpA2V旨在填补这一空白。

Method: 1）音频引导的视频规划：利用MLLM从音频中提取空间和语义线索，生成视频场景布局（VSL）；2）布局引导的视频生成：将VSL作为条件输入预训练扩散模型，无需额外训练。

Result: 实验表明，SpA2V能生成与输入音频语义和空间对齐的真实视频。

Conclusion: SpA2V通过利用音频的空间线索，显著提升了视频生成的准确性和真实性。

Abstract: Audio-driven video generation aims to synthesize realistic videos that align
with input audio recordings, akin to the human ability to visualize scenes from
auditory input. However, existing approaches predominantly focus on exploring
semantic information, such as the classes of sounding sources present in the
audio, limiting their ability to generate videos with accurate content and
spatial composition. In contrast, we humans can not only naturally identify the
semantic categories of sounding sources but also determine their deeply encoded
spatial attributes, including locations and movement directions. This useful
information can be elucidated by considering specific spatial indicators
derived from the inherent physical properties of sound, such as loudness or
frequency. As prior methods largely ignore this factor, we present SpA2V, the
first framework explicitly exploits these spatial auditory cues from audios to
generate videos with high semantic and spatial correspondence. SpA2V decomposes
the generation process into two stages: 1) Audio-guided Video Planning: We
meticulously adapt a state-of-the-art MLLM for a novel task of harnessing
spatial and semantic cues from input audio to construct Video Scene Layouts
(VSLs). This serves as an intermediate representation to bridge the gap between
the audio and video modalities. 2) Layout-grounded Video Generation: We develop
an efficient and effective approach to seamlessly integrate VSLs as conditional
guidance into pre-trained diffusion models, enabling VSL-grounded video
generation in a training-free manner. Extensive experiments demonstrate that
SpA2V excels in generating realistic videos with semantic and spatial alignment
to the input audios.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [101] [DiSC-Med: Diffusion-based Semantic Communications for Robust Medical Image Transmission](https://arxiv.org/abs/2508.00172)
*Fupei Guo,Hao Zheng,Xiang Zhang,Li Chen,Yue Wang,Songyang Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于扩散的语义通信框架DiSC-Med，用于高效且鲁棒的医学图像传输。


<details>
  <summary>Details</summary>
Motivation: 人工智能和无线通信技术的发展推动了远程医疗的需求，但医学数据在有限带宽和噪声信道中的高效传输仍是一个关键挑战。

Method: 开发了医学增强的压缩和去噪模块，通过语义通信框架DiSC-Med实现高效带宽利用和鲁棒性。

Result: 在真实医学数据集上的实验表明，DiSC-Med能够高效传输关键语义信息，并在噪声信道中实现卓越的重建性能。

Conclusion: DiSC-Med框架为远程医疗应用提供了高效且鲁棒的解决方案。

Abstract: The rapid development of artificial intelligence has driven smart health with
next-generation wireless communication technologies, stimulating exciting
applications in remote diagnosis and intervention. To enable a timely and
effective response for remote healthcare, efficient transmission of medical
data through noisy channels with limited bandwidth emerges as a critical
challenge. In this work, we propose a novel diffusion-based semantic
communication framework, namely DiSC-Med, for the medical image transmission,
where medical-enhanced compression and denoising blocks are developed for
bandwidth efficiency and robustness, respectively. Unlike conventional
pixel-wise communication framework, our proposed DiSC-Med is able to capture
the key semantic information and achieve superior reconstruction performance
with ultra-high bandwidth efficiency against noisy channels. Extensive
experiments on real-world medical datasets validate the effectiveness of our
framework, demonstrating its potential for robust and efficient telehealth
applications.

</details>


### [102] [Stress-Aware Resilient Neural Training](https://arxiv.org/abs/2508.00098)
*Ashkan Shakarami,Yousef Yeganeh,Azade Farshad,Lorenzo Nicole,Stefano Ghidoni,Nassir Navab*

Main category: cs.LG

TL;DR: 论文提出了一种基于材料科学中结构疲劳概念的弹性-塑性变形启发的神经网络训练方法，通过动态调整优化行为提升模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 受材料科学中结构疲劳现象的启发，旨在解决深度神经网络在训练过程中可能遇到的停滞问题，帮助模型逃离尖锐极小值，收敛到更平坦、泛化性更好的区域。

Method: 提出了Plastic Deformation Optimizer，一种应力感知机制，通过内部应力信号检测训练停滞，并自适应地向模型参数注入噪声。

Result: 在六种架构、四种优化器和七个视觉基准测试中验证了方法的有效性，显著提升了鲁棒性和泛化能力，且计算开销极小。

Conclusion: Stress-Aware Learning为神经网络训练提供了一种新的弹性优化范式，具有实际应用潜力。

Abstract: This paper introduces Stress-Aware Learning, a resilient neural training
paradigm in which deep neural networks dynamically adjust their optimization
behavior - whether under stable training regimes or in settings with uncertain
dynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic)
Deformation, inspired by structural fatigue in materials science. To
instantiate this concept, we propose Plastic Deformation Optimizer, a
stress-aware mechanism that injects adaptive noise into model parameters
whenever an internal stress signal - reflecting stagnation in training loss and
accuracy - indicates persistent optimization difficulty. This enables the model
to escape sharp minima and converge toward flatter, more generalizable regions
of the loss landscape. Experiments across six architectures, four optimizers,
and seven vision benchmarks demonstrate improved robustness and generalization
with minimal computational overhead. The code and 3D visuals will be available
on GitHub: https://github.com/Stress-Aware-Learning/SAL.

</details>


### [103] [Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product](https://arxiv.org/abs/2508.00230)
*Paul Albert,Frederic Z. Zhang,Hemanth Saratchandran,Anton van den Hengel,Ehsan Abbasnejad*

Main category: cs.LG

TL;DR: KRAdapter是一种新型参数高效微调（PEFT）算法，通过Khatri-Rao乘积生成权重更新，解决了LoRA在近似高有效秩矩阵时的局限性，并在视觉语言模型和大型语言模型上表现优异。


<details>
  <summary>Details</summary>
Motivation: LoRA在适应多模态和大型语言模型时存在局限性，尤其是在处理高有效秩矩阵时表现不佳。

Method: 提出KRAdapter算法，利用Khatri-Rao乘积生成权重更新，以更好地近似高有效秩矩阵。

Result: KRAdapter在1B参数的视觉语言模型和8B参数的大型语言模型上表现优于LoRA，尤其在未见过的常识推理任务中。

Conclusion: KRAdapter是一种高效且实用的PEFT方法，适用于大规模参数模型的微调。

Abstract: Parameter-efficient fine-tuning (PEFT) has become a standard approach for
adapting large pre-trained models. Amongst PEFT methods, low-rank adaptation
(LoRA) has achieved notable success. However, recent studies have highlighted
its limitations compared against full-rank alternatives, particularly when
applied to multimodal and large language models. In this work, we present a
quantitative comparison amongst full-rank and low-rank PEFT methods using a
synthetic matrix approximation benchmark with controlled spectral properties.
Our results confirm that LoRA struggles to approximate matrices with relatively
flat spectrums or high frequency components -- signs of high effective ranks.
To this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the
Khatri-Rao product to produce weight updates, which, by construction, tends to
produce matrix product with a high effective rank. We demonstrate performance
gains with KRAdapter on vision-language models up to 1B parameters and on large
language models up to 8B parameters, particularly on unseen common-sense
reasoning tasks. In addition, KRAdapter maintains the memory and compute
efficiency of LoRA, making it a practical and robust alternative to fine-tune
billion-scale parameter models.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [104] [AudioGen-Omni: A Unified Multimodal Diffusion Transformer for Video-Synchronized Audio, Speech, and Song Generation](https://arxiv.org/abs/2508.00733)
*Le Wang,Jun Wang,Feng Deng,Chen Zhang,Kun Gai,Di Zhang*

Main category: cs.SD

TL;DR: AudioGen-Omni是一个基于多模态扩散变换器（MMDit）的统一方法，能够生成与输入视频同步的高保真音频、语音和歌曲。


<details>
  <summary>Details</summary>
Motivation: 解决多模态音频生成的语义对齐和跨模态条件限制问题，提升生成质量和效率。

Method: 采用联合训练范式，结合视频-文本-音频数据，使用统一的歌词-转录编码器和PAAPI增强的注意力机制。

Result: 在文本到音频/语音/歌曲任务上达到最优性能，生成时间快（1.91秒生成8秒音频）。

Conclusion: AudioGen-Omni在多模态音频生成中实现了高质量、高效和通用性。

Abstract: We present AudioGen-Omni - a unified approach based on multimodal diffusion
transformers (MMDit), capable of generating high-fidelity audio, speech, and
songs coherently synchronized with the input video. AudioGen-Omni introduces a
novel joint training paradigm that seamlessly integrates large-scale
video-text-audio corpora, enabling a model capable of generating semantically
rich, acoustically diverse audio conditioned on multimodal inputs and adaptable
to a wide range of audio generation tasks. AudioGen-Omni employs a unified
lyrics-transcription encoder that encodes graphemes and phonemes from both sung
and spoken inputs into dense frame-level representations. Dense frame-level
representations are fused using an AdaLN-based joint attention mechanism
enhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein
RoPE is selectively applied to temporally structured modalities to ensure
precise and robust cross-modal alignment. By unfreezing all modalities and
masking missing inputs, AudioGen-Omni mitigates the semantic constraints of
text-frozen paradigms, enabling effective cross-modal conditioning. This joint
training approach enhances audio quality, semantic alignment, and lip-sync
accuracy, while also achieving state-of-the-art results on
Text-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8
seconds of audio, it offers substantial improvements in both efficiency and
generality.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [105] [Jet Image Generation in High Energy Physics Using Diffusion Models](https://arxiv.org/abs/2508.00250)
*Victor D. Martinez,Vidya Manian,Sudhir Malik*

Main category: hep-ph

TL;DR: 本文首次将扩散模型应用于生成LHC质子-质子碰撞事件的喷注图像，比较了基于分数的扩散模型和一致性模型的性能，发现后者在生成质量和稳定性上更优。


<details>
  <summary>Details</summary>
Motivation: 研究目的是利用扩散模型生成高能物理实验中的喷注图像，以改进计算效率和生成准确性。

Method: 将JetNet模拟数据集中的喷注运动学变量映射为二维图像，并训练扩散模型学习喷注成分的空间分布。

Result: 一致性模型在FID等指标上表现优于基于分数的扩散模型，生成图像质量更高且更稳定。

Conclusion: 该方法为高能物理研究提供了高效且准确的工具，尤其在喷注图像生成方面具有显著优势。

Abstract: This article presents, for the first time, the application of diffusion
models for generating jet images corresponding to proton-proton collision
events at the Large Hadron Collider (LHC). The kinematic variables of quark,
gluon, W-boson, Z-boson, and top quark jets from the JetNet simulation dataset
are mapped to two-dimensional image representations. Diffusion models are
trained on these images to learn the spatial distribution of jet constituents.
We compare the performance of score-based diffusion models and consistency
models in accurately generating class-conditional jet images. Unlike approaches
based on latent distributions, our method operates directly in image space. The
fidelity of the generated images is evaluated using several metrics, including
the Fr\'echet Inception Distance (FID), which demonstrates that consistency
models achieve higher fidelity and generation stability compared to score-based
diffusion models. These advancements offer significant improvements in
computational efficiency and generation accuracy, providing valuable tools for
High Energy Physics (HEP) research.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [106] [On the interaction of dilatancy and friction in the behavior of fluid-saturated sheared granular materials: a coupled Computational Fluid Dynamics--Discrete Element Method study](https://arxiv.org/abs/2508.00786)
*Bimal Chhushyabaga,Behrooz Ferdowsi*

Main category: cond-mat.soft

TL;DR: 研究通过CFD-DEM模拟分析了流体饱和颗粒材料在不同环境下的破坏行为，揭示了孔隙压力演化的关键作用。


<details>
  <summary>Details</summary>
Motivation: 理解流体饱和颗粒材料在陆地和水中环境下的破坏机制，以减少相关自然灾害（如海底滑坡和地震）的风险。

Method: 采用三维耦合CFD-DEM模拟，分析密实和松散颗粒组在不同条件下的坍塌和流动动力学。

Result: 密实组通过膨胀稳定，松散组在孔隙压力作用下快速压实和流体化；流体拖曳和孔隙压力主导水下系统。

Conclusion: 研究为饱和颗粒介质的破坏机制提供了新见解，支持开发基于物理的灾害缓解模型。

Abstract: Frictional instabilities in fluid-saturated granular materials underlie
critical natural hazards such as submarine landslides and earthquake
initiation. Distinct failure behaviors emerge under subaerial and subaqueous
conditions due to the coupled effects of mechanical deformation, interparticle
friction, and fluid interactions. This study employs three-dimensional coupled
Computational Fluid Dynamics-Discrete Element Method (CFD-DEM) simulations to
investigate the collapse and runout dynamics of dense and loose granular
assemblies across these settings. Parametric analyses reveal that pore pressure
evolution plays a central role in governing failure mechanisms: dense
assemblies stabilize through dilation, while loose assemblies undergo rapid
compaction and fluidization, particularly under subaqueous conditions.
Spatiotemporal analyses of coarse-grained fields further highlight
strain-rate-dependent behavior driven by evolving porosity and effective
stress. Both environments exhibit rate-strengthening behavior that scales with
the inertial number (In) and viscous number (Iv), though driven by distinct
mechanisms: subaerial systems are dominated by interparticle contact networks,
whereas subaqueous systems are influenced by fluid drag, pore-pressure buildup,
and lubrication. An analytical solution for excess pore pressure is compared
with breaching-induced pressure distributions from CFD-DEM simulations, using
input parameters derived from numerical triaxial DEM tests. The model captures
fluid-particle coupling effectively, reproducing comparable excess pore
pressures at steady state, while early-time discrepancies underscore the
complexities of transient interactions. These findings advance the
understanding of failure mechanics in saturated granular media and support the
development of physics-based models for mitigating hazards associated with
subaqueous granular flows.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [107] [STF: Shallow-Level Temporal Feedback to Enhance Spiking Transformers](https://arxiv.org/abs/2508.00387)
*Zeqi Zheng,Zizheng Zhu,Yingchao Yu,Yanchen Huang,Changze Lv,Junfeng Tang,Zhaofei Yu,Yaochu Jin*

Main category: cs.NE

TL;DR: 论文提出了一种轻量级的浅层时间反馈模块（STF），用于提升基于Transformer的脉冲神经网络（SNNs）的性能，解决了现有深度反馈设计的高成本和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 现有的深度反馈设计在提升SNNs性能时存在高参数开销、高能耗和长延迟的问题，需要一种更高效的解决方案。

Method: 提出了STF模块，包含时空位置嵌入（TSPE）和时间反馈（TF），用于编码层，以轻量级方式提升性能。

Result: 实验表明，STF在多种静态数据集（如CIFAR-10、CIFAR-100和ImageNet-1K）上显著提升了性能，并增强了脉冲模式的多样性。

Conclusion: STF是一种高效的编码方案，适用于静态场景，具有对抗鲁棒性和时间敏感性优势。

Abstract: Transformer-based Spiking Neural Networks (SNNs) suffer from a great
performance gap compared to floating-point Artificial Neural Networks (ANNs)
due to the binary nature of spike trains. Recent efforts have introduced
deep-level feedback loops to transmit high-level semantic information to narrow
this gap. However, these designs often span multiple deep layers, resulting in
costly feature transformations, higher parameter overhead, increased energy
consumption, and longer inference latency. To address this issue, we propose
Shallow-level Temporal Feedback (STF), a lightweight plug-and-play module for
the encoding layer, which consists of Temporal-Spatial Position Embedding
(TSPE) and Temporal Feedback (TF).Extensive experiments show that STF
consistently improves performance across various Transformer-based SNN
backbones on static datasets, including CIFAR-10, CIFAR-100, and ImageNet-1K,
under different spike timestep settings. Further analysis reveals that STF
enhances the diversity of the spike patterns, which is key to performance gain.
Moreover, evaluations on adversarial robustness and temporal sensitivity
confirm that STF outperforms direct coding and its variants, highlighting its
potential as a new spike encoding scheme for static scenarios. Our code will be
released upon acceptance.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [108] [The Repeated-Stimulus Confound in Electroencephalography](https://arxiv.org/abs/2508.00531)
*Jack A. Kilgallen,Barak A. Pearlmutter,Jeffrey Mark Siskind*

Main category: q-bio.NC

TL;DR: 论文指出，在神经解码研究中，重复使用相同刺激训练和评估模型会导致‘重复刺激混淆’，从而高估模型性能。


<details>
  <summary>Details</summary>
Motivation: 揭示神经解码研究中因重复使用相同刺激导致的性能高估问题，并评估其对文献结果的影响。

Method: 分析了16篇受影响的文献，并使用其模型进行实验，量化性能高估程度。

Result: 发现模型解码准确率被高估了4.46-7.42%，且高估程度与准确率增长呈正相关。

Conclusion: 重复刺激混淆不仅高估性能，还削弱了研究结论的有效性，甚至可能支持伪科学主张。

Abstract: In neural-decoding studies, recordings of participants' responses to stimuli
are used to train models. In recent years, there has been an explosion of
publications detailing applications of innovations from deep-learning research
to neural-decoding studies. The data-hungry models used in these experiments
have resulted in a demand for increasingly large datasets. Consequently, in
some studies, the same stimuli are presented multiple times to each participant
to increase the number of trials available for use in model training. However,
when a decoding model is trained and subsequently evaluated on responses to the
same stimuli, stimulus identity becomes a confounder for accuracy. We term this
the repeated-stimulus confound. We identify a susceptible dataset, and 16
publications which report model performance based on evaluation procedures
affected by the confound. We conducted experiments using models from the
affected studies to investigate the likely extent to which results in the
literature have been misreported. Our findings suggest that the decoding
accuracies of these models were overestimated by between 4.46-7.42%. Our
analysis also indicates that per 1% increase in accuracy under the confound,
the magnitude of the overestimation increases by 0.26%. The confound not only
results in optimistic estimates of decoding performance, but undermines the
validity of several claims made within the affected publications. We conducted
further experiments to investigate the implications of the confound in
alternative contexts. We found that the same methodology used within the
affected studies could also be used to justify an array of pseudoscientific
claims, such as the existence of extrasensory perception.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [109] [UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents](https://arxiv.org/abs/2508.00288)
*Jianqiang Xiao,Yuexuan Sun,Yixin Shao,Boxi Gan,Rongqiang Liu,Yanjing Wu,Weili Gua,Xiang Deng*

Main category: cs.RO

TL;DR: UAV-ON是一个用于无人机在开放环境中进行目标导航的基准测试，摆脱了对语言指令的依赖，引入了语义目标驱动的导航挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究多依赖语言指令（如VLN），限制了无人机导航的扩展性和自主性。UAV-ON旨在填补这一空白，推动基于语义目标的无人机自主导航研究。

Method: UAV-ON包含14个高保真环境，定义了1270个目标对象，每个对象通过实例级指令描述。提出了Aerial ObjectNav Agent（AOA）作为基线方法，结合语义目标和观察进行导航。

Result: 实验表明，现有基线方法在UAV-ON中表现不佳，突显了无人机导航与语义目标结合的复杂性。

Conclusion: UAV-ON为复杂环境中基于语义目标的无人机自主导航研究提供了新方向，强调了这一领域的挑战。

Abstract: Aerial navigation is a fundamental yet underexplored capability in embodied
intelligence, enabling agents to operate in large-scale, unstructured
environments where traditional navigation paradigms fall short. However, most
existing research follows the Vision-and-Language Navigation (VLN) paradigm,
which heavily depends on sequential linguistic instructions, limiting its
scalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark
for large-scale Object Goal Navigation (ObjectNav) by aerial agents in
open-world environments, where agents operate based on high-level semantic
goals without relying on detailed instructional guidance as in VLN. UAV-ON
comprises 14 high-fidelity Unreal Engine environments with diverse semantic
regions and complex spatial layouts, covering urban, natural, and mixed-use
settings. It defines 1270 annotated target objects, each characterized by an
instance-level instruction that encodes category, physical footprint, and
visual descriptors, allowing grounded reasoning. These instructions serve as
semantic goals, introducing realistic ambiguity and complex reasoning
challenges for aerial agents. To evaluate the benchmark, we implement several
baseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that
integrates instruction semantics with egocentric observations for long-horizon,
goal-directed exploration. Empirical results show that all baselines struggle
in this setting, highlighting the compounded challenges of aerial navigation
and semantic goal grounding. UAV-ON aims to advance research on scalable UAV
autonomy driven by semantic goal descriptions in complex real-world
environments.

</details>


### [110] [Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging](https://arxiv.org/abs/2508.00354)
*Tianshuang Qiu,Zehan Ma,Karim El-Refai,Hiya Shah,Chung Min Kim,Justin Kerr,Ken Goldberg*

Main category: cs.RO

TL;DR: Omni-Scan是一种利用双机械臂机器人抓取和旋转物体以生成高质量3D高斯溅射模型的流程，适用于零件缺陷检测。


<details>
  <summary>Details</summary>
Motivation: 传统3D物体扫描方法受限于设备工作空间，需要多相机阵列或激光扫描仪。Omni-Scan旨在通过机器人抓取和旋转物体，实现全方位扫描。

Method: 使用双机械臂机器人抓取物体并旋转，结合DepthAnything、Segment Anything和RAFT光流模型去除背景和夹持器遮挡，改进3DGS训练流程以支持拼接数据集。

Result: 在12种工业和家用物体的缺陷检测中，平均准确率达到83%。

Conclusion: Omni-Scan提供了一种高效的3D物体建模方法，适用于缺陷检测等应用。

Abstract: 3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view
images. Such "digital twins" are useful for simulations, virtual reality,
marketing, robot policy fine-tuning, and part inspection. 3D object scanning
usually requires multi-camera arrays, precise laser scanners, or robot
wrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,
a pipeline for producing high-quality 3D Gaussian Splat models using a
bi-manual robot that grasps an object with one gripper and rotates the object
with respect to a stationary camera. The object is then re-grasped by a second
gripper to expose surfaces that were occluded by the first gripper. We present
the Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as
RAFT optical flow models to identify and isolate objects held by a robot
gripper while removing the gripper and the background. We then modify the 3DGS
training pipeline to support concatenated datasets with gripper occlusion,
producing an omni-directional (360 degree view) model of the object. We apply
Omni-Scan to part defect inspection, finding that it can identify visual or
geometric defects in 12 different industrial and household objects with an
average accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be
found at https://berkeleyautomation.github.io/omni-scan/

</details>


### [111] [On-Device Diffusion Transformer Policy for Efficient Robot Manipulation](https://arxiv.org/abs/2508.00697)
*Yiming Wu,Huan Wang,Zhenghao Chen,Jianxin Pang,Dong Xu*

Main category: cs.RO

TL;DR: LightDP是一种专为移动设备设计的轻量级框架，通过压缩去噪模块和减少采样步骤，显著提升了Diffusion Policies的计算效率，实现了实时动作预测。


<details>
  <summary>Details</summary>
Motivation: Diffusion Policies在资源受限的移动平台上应用时存在计算效率低和内存占用大的问题，限制了其实际部署。

Method: LightDP采用网络压缩和采样步骤减少策略，包括统一的剪枝与再训练流程和一致性蒸馏技术。

Result: 在多个标准数据集上，LightDP实现了实时动作预测，性能与现有Diffusion Policies相当。

Conclusion: LightDP为资源有限环境中Diffusion Policies的实际部署提供了可行方案。

Abstract: Diffusion Policies have significantly advanced robotic manipulation tasks via
imitation learning, but their application on resource-constrained mobile
platforms remains challenging due to computational inefficiency and extensive
memory footprint. In this paper, we propose LightDP, a novel framework
specifically designed to accelerate Diffusion Policies for real-time deployment
on mobile devices. LightDP addresses the computational bottleneck through two
core strategies: network compression of the denoising modules and reduction of
the required sampling steps. We first conduct an extensive computational
analysis on existing Diffusion Policy architectures, identifying the denoising
network as the primary contributor to latency. To overcome performance
degradation typically associated with conventional pruning methods, we
introduce a unified pruning and retraining pipeline, optimizing the model's
post-pruning recoverability explicitly. Furthermore, we combine pruning
techniques with consistency distillation to effectively reduce sampling steps
while maintaining action prediction accuracy. Experimental evaluations on the
standard datasets, \ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that
LightDP achieves real-time action prediction on mobile devices with competitive
performance, marking an important step toward practical deployment of
diffusion-based policies in resource-limited environments. Extensive real-world
experiments also show the proposed LightDP can achieve performance comparable
to state-of-the-art Diffusion Policies.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [112] [Electrical and Thermal Conductivity of Earth's Iron-enriched Basal Magma Ocean](https://arxiv.org/abs/2508.00791)
*Francis Dragulet,Lars Stixrude*

Main category: astro-ph.EP

TL;DR: 地球最早的磁场可能起源于基底岩浆海洋，这是一个围绕地核的硅酸盐熔融层，可能持续了数十亿年。研究表明，基底岩浆海洋条件下硅酸盐液体的电导率超过10000 S/m，可能达到发电机活动的阈值。然而，由于铁在下地幔矿物组合中的不相容性，基底岩浆海洋在其大部分历史中比整体硅酸盐地球更富含铁。通过从头算分子动力学计算，研究了铁含量如何影响硅酸盐发电机假说，并探讨了电导率随铁富集的变化。此外，还计算了电子对热导率的贡献，以评估基底岩浆海洋的对流不稳定性。最后，应用这些结果模拟了地球基底岩浆海洋的热和磁演化过程。


<details>
  <summary>Details</summary>
Motivation: 研究地球最早磁场的起源及其与基底岩浆海洋的关系，探讨铁含量对硅酸盐发电机活动的影响。

Method: 使用从头算分子动力学计算，分析铁富集对硅酸盐液体电导率的影响，并计算电子对热导率的贡献。

Result: 发现基底岩浆海洋条件下硅酸盐液体的电导率可能达到发电机活动的阈值，铁富集会显著影响电导率。

Conclusion: 基底岩浆海洋可能是地球最早磁场的来源，铁含量对其发电机活动有重要影响，模拟结果支持这一假说。

Abstract: The Earth's earliest magnetic field may have originated in a basal magma
ocean, a layer of silicate melt surround the core that could have persisted for
billions of years. Recent studies show that the electrical conductivity of
liquid with a bulk silicate Earth composition exceeds 10000 S/m at basal magma
ocean conditions, potentially surprising the threshold for dynamo activity.
Over most of its history however, the basal magma ocean is more enriched in
iron than the bulk silicate Earth, due to iron's incompatibility in the mineral
assemblages of the lower mantle. Using ab-initio molecular dynamics
calculations, we examine how iron content affects the silicate dynamo
hypothesis. We investigate how the electrical conductivity of silicate liquid
changes with iron enrichment, at pressures and temperatures relevant for
Earth's basal magma ocean. We also compute the electronic contribution to the
thermal conductivity , to evaluate convective instability of basal magma
oceans. Finally, we apply our results to model the thermal and magnetic
evolution of Earth's basal magma ocean over time.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [113] [Numerical Uncertainty in Linear Registration: An Experimental Study](https://arxiv.org/abs/2508.00781)
*Niusha Mirhakimi,Yohan Chatelain,Jean-Baptiste Poline,Tristan Glatard*

Main category: q-bio.QM

TL;DR: 该论文通过蒙特卡洛算术模拟评估了SPM、FSL和ANTs等常用线性配准工具在数值稳定性上的表现，发现SPM稳定性最高，而ANTs对数值扰动敏感。研究还表明健康人群与帕金森病患者的数值稳定性无显著差异，并探讨了数值不确定性在自动质量控制中的应用。


<details>
  <summary>Details</summary>
Motivation: 线性配准在MRI预处理中至关重要，但其数值不确定性研究不足，因此需要评估常用工具的稳定性。

Method: 使用蒙特卡洛算术模拟，评估SPM、FSL和ANTs在不同图像相似性度量、脑模板及健康与帕金森病队列中的表现。

Result: SPM稳定性最高，ANTs对数值扰动敏感且可能失败；健康与帕金森病队列无显著差异；数值不确定性可用于质量控制。

Conclusion: 研究为线性配准的数值稳定性提供了实验依据，并支持未来不确定性分析及自动质量控制的应用。

Abstract: While linear registration is a critical step in MRI preprocessing pipelines,
its numerical uncertainty is understudied. Using Monte-Carlo Arithmetic (MCA)
simulations, we assessed the most commonly used linear registration tools
within major software packages (SPM, FSL, and ANTs) across multiple image
similarity measures, two brain templates, and both healthy control (HC, n=50)
and Parkinson's Disease (PD, n=50) cohorts. Our findings highlight the
influence of linear registration tools and similarity measures on numerical
stability. Among the evaluated tools and with default similarity measures, SPM
exhibited the highest stability. FSL and ANTs showed greater and similar ranges
of variability, with ANTs demonstrating particular sensitivity to numerical
perturbations that occasionally led to registration failure. Furthermore, no
significant differences were observed between healthy and PD cohorts,
suggesting that numerical stability analyses obtained with healthy subjects may
generalise to clinical populations. Finally, we also demonstrated how numerical
uncertainty measures may support automated quality control (QC) of linear
registration results. Overall, our experimental results characterize the
numerical stability of linear registration experimentally and can serve as a
basis for future uncertainty analyses.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [114] [CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding](https://arxiv.org/abs/2508.00378)
*Shixin Yi,Lin Shang*

Main category: cs.AI

TL;DR: CoRGI框架通过引入视觉验证机制，解决了视觉语言模型在推理过程中缺乏视觉内容基础的问题，提升了推理性能和解释的可信度。


<details>
  <summary>Details</summary>
Motivation: 现有Chain-of-Thought (CoT)方法在视觉语言模型中生成的解释虽然语言流畅，但缺乏视觉内容的验证，导致幻觉问题。

Method: 提出CoRGI框架，包含三个阶段：生成文本推理链、通过VEVM模块提取视觉证据、结合文本和视觉证据生成验证后的答案。

Result: 在VCR基准测试中，CoRGI提升了Qwen-2.5VL和LLaVA-1.6的性能，并通过消融实验验证了各步骤的有效性。

Conclusion: 视觉验证对提升多模态推理的鲁棒性至关重要，CoRGI框架为未来研究提供了实用方向。

Abstract: Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in
vision-language models (VLMs), but it often produces explanations that are
linguistically fluent yet lack grounding in visual content. We observe that
such hallucinations arise in part from the absence of an explicit verification
mechanism during multi-step reasoning. To address this, we propose
\textbf{CoRGI}(\textbf{C}hain \textbf{o}f \textbf{R}easoning with
\textbf{G}rounded \textbf{I}nsights), a modular framework that introduces
visual verification into the reasoning process. CoRGI follows a three-stage
pipeline: it first generates a textual reasoning chain, then extracts
supporting visual evidence for each reasoning step via a dedicated module
(VEVM), and finally synthesizes the textual rationale with visual evidence to
generate a grounded, verified answer. The framework can be integrated with
existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR
benchmark and find that it improves reasoning performance on two representative
open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm
the contribution of each step in the verification module, and human evaluations
suggest that CoRGI leads to more factual and helpful explanations. We also
examine alternative designs for the visual verification step and discuss
potential limitations of post-hoc verification frameworks. These findings
highlight the importance of grounding intermediate reasoning steps in visual
evidence to enhance the robustness of multimodal reasoning.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [115] [Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications](https://arxiv.org/abs/2508.00669)
*Wenxuan Wang,Zizhan Ma,Meidan Ding,Shiyi Zheng,Shengyuan Liu,Jie Liu,Jiaming Ji,Wenting Chen,Xiang Li,Linlin Shen,Yixuan Yuan*

Main category: cs.CL

TL;DR: 本文系统综述了大型语言模型（LLMs）在医学推理领域的发展，提出了训练时和测试时的增强技术分类，并分析了其在多种数据模态和临床应用中的表现。


<details>
  <summary>Details</summary>
Motivation: 医学实践中系统性、透明且可验证的推理能力是LLMs的关键短板，促使研究转向专门用于医学推理的LLM开发。

Method: 提出分类法，包括训练时策略（如监督微调、强化学习）和测试时机制（如提示工程、多智能体系统），并分析其在文本、图像、代码等数据模态及临床应用中的表现。

Result: 基于60项2022-2025年的研究，总结了评估基准的演变，并指出关键挑战如忠实性与合理性的差距、多模态推理需求。

Conclusion: 未来方向包括构建高效、稳健且社会技术负责任的医学AI，解决现有挑战。

Abstract: The proliferation of Large Language Models (LLMs) in medicine has enabled
impressive capabilities, yet a critical gap remains in their ability to perform
systematic, transparent, and verifiable reasoning, a cornerstone of clinical
practice. This has catalyzed a shift from single-step answer generation to the
development of LLMs explicitly designed for medical reasoning. This paper
provides the first systematic review of this emerging field. We propose a
taxonomy of reasoning enhancement techniques, categorized into training-time
strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time
mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how
these techniques are applied across different data modalities (text, image,
code) and in key clinical applications such as diagnosis, education, and
treatment planning. Furthermore, we survey the evolution of evaluation
benchmarks from simple accuracy metrics to sophisticated assessments of
reasoning quality and visual interpretability. Based on an analysis of 60
seminal studies from 2022-2025, we conclude by identifying critical challenges,
including the faithfulness-plausibility gap and the need for native multimodal
reasoning, and outlining future directions toward building efficient, robust,
and sociotechnically responsible medical AI.

</details>
