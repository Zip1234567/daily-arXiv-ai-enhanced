<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 156]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.GR](#cs.GR) [Total: 5]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [physics.optics](#physics.optics) [Total: 2]
- [stat.ME](#stat.ME) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.RO](#cs.RO) [Total: 5]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.CY](#cs.CY) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Deep Learning-Based CCTV System for Automatic Smoking Detection in Fire Exit Zones](https://arxiv.org/abs/2508.11696)
*Sami Sadat,Mohammad Irtiza Hossain,Junaid Ahmed Sifat,Suhail Haque Rafi,Md. Waseq Alauddin Alvi,Md. Khalilur Rhaman*

Main category: cs.CV

TL;DR: 基于YOLOv8的自定义模型在CCTV监控中实时检测火灾途径区吸烟行为，达到78.90%回归率和83.70% mAP性能，在Jetson Xavier NX边缘设处上实现52-97ms的处理速度


<details>
  <summary>Details</summary>
Motivation: 为了满足火灾安全要求，需要开发能够在CCTV监控中实时检测火灾途径区吸烟行为的系统，以确保公共安全和自动遵规

Method: 使用8,124张图片和2,708个明暗区域样本的数据集，评估YOLOv8、YOLOv11、YOLOv12三种目标检测模型，并在YOLOv8基础上开发自定义模型以满足监控场景的挑战性需求

Result: 提出的自定义模型表现最优，达到78.90%的回归率和83.70% mAP@50，在多种边缘设处上进行多线程性能评估，Jetson Xavier NX的处理速度为52-97ms/次推理

Conclusion: 该系统为监控公共安全和自动遵规提供了稳健且适应性强的平台，适合时间敏感的实时操作场景

Abstract: A deep learning real-time smoking detection system for CCTV surveillance of
fire exit areas is proposed due to critical safety requirements. The dataset
contains 8,124 images from 20 different scenarios along with 2,708 raw samples
demonstrating low-light areas. We evaluated three advanced object detection
models: YOLOv8, YOLOv11, and YOLOv12, followed by development of a custom model
derived from YOLOv8 with added structures for challenging surveillance
contexts. The proposed model outperformed the others, achieving a recall of
78.90 percent and mAP at 50 of 83.70 percent, delivering optimal object
detection across varied environments. Performance evaluation on multiple edge
devices using multithreaded operations showed the Jetson Xavier NX processed
data at 52 to 97 milliseconds per inference, establishing its suitability for
time-sensitive operations. This system offers a robust and adaptable platform
for monitoring public safety and enabling automatic regulatory compliance.

</details>


### [2] [Separating Knowledge and Perception with Procedural Data](https://arxiv.org/abs/2508.11697)
*Adrián Rodríguez-Muñoz,Manel Baradad,Phillip Isola,Antonio Torralba*

Main category: cs.CV

TL;DR: 通过仅使用程序生成数据训练表征模型，结合视觉记忆库技术，在不需额外训练的情况下实现了与实际图像模型相当的性能表现，同时完全隔离实际图像数据。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度学习模型对大量实际图像数据的依赖问题，通过程序生成数据实现数据隔离和隐私保护，同时保持模型性能。

Method: 仅使用程序生成的数据训练表征模型，结合显式的视觉记忆库（参考图像嵌入数据库）来处理视视相似性、分类和语义分割任务，无需额外训练。

Result: 在NIGHTS视视相似性任务上与Places训练模型相差仅1%；在CUB200和Flowers102细粒度分类任务上分别超15%和8%；在ImageNet-1K分类任务上相差10%；在COCO零样本分割任务上R²指标与实际数据训练模型相差10%以内。

Conclusion: 程序模型能够在完全隔离实际图像数据的前提下实现强劲性能，但由于同一物体不同部分的表征相伙异，导致内存搜索错误，还存在一定性能差距。

Abstract: We train representation models with procedural data only, and apply them on
visual similarity, classification, and semantic segmentation tasks without
further training by using visual memory -- an explicit database of reference
image embeddings. Unlike prior work on visual memory, our approach achieves
full compartmentalization with respect to all real-world images while retaining
strong performance. Compared to a model trained on Places, our procedural model
performs within $1\%$ on NIGHTS visual similarity, outperforms by $8\%$ and
$15\%$ on CUB200 and Flowers102 fine-grained classification, and is within
$10\%$ on ImageNet-1K classification. It also demonstrates strong zero-shot
segmentation, achieving an $R^2$ on COCO within $10\%$ of the models trained on
real data. Finally, we analyze procedural versus real data models, showing that
parts of the same object have dissimilar representations in procedural models,
resulting in incorrect searches in memory and explaining the remaining
performance gap.

</details>


### [3] [FusionFM: Fusing Eye-specific Foundational Models for Optimized Ophthalmic Diagnosis](https://arxiv.org/abs/2508.11721)
*Ke Zou,Jocelyn Hui Lin Goh,Yukun Zhou,Tian Lin,Samantha Min Er Yew,Sahana Srinivasan,Meng Wang,Rui Santos,Gabor M. Somfai,Huazhu Fu,Haoyu Chen,Pearse A. Keane,Ching-Yu Cheng,Yih Chung Tham*

Main category: cs.CV

TL;DR: 这是首次系统性评估眼科基础模型的研究，提出了FusionFM评估框架和两种融合策略，发现DINORET和RetiZero在眼部和系统性疾病任务中表现最佳，融合策略在某些疾病预测中有突破。


<details>
  <summary>Details</summary>
Motivation: 眼科基础模型虽然发展快速，但对于哪个模型表现最佳、是否在不同任务上都良好、以及融合所有模型的效果等基本问题仍缺乏系统性评估。

Method: 提出FusionFM评估框架，包括两种融合方法来整合不同眼科基础模型。评估范围涵盖眼科疾病检测（青光眼、糖尿病视网膜病变、鹿鹿眼等）和系统性疾病预测（糖尿病、高血压）。对四个先进基础模型（RETFound、VisionFM、RetiZero、DINORET）进行标准化测试。

Result: DINORET和RetiZero在眼科和系统性疾病任务中表现最佳，RetiZero在外部数据集上显示更强的沿半性。Gating-based融合策略在预测青光眼、AMD和高血压时提供了突破。预测系统性疾病（特别是外部群体中的高血压）仍面临挑战。

Conclusion: 研究提供了基于证据的眼科基础模型评估，强调了模型融合的优势，并指出了提高临床应用性的策略。

Abstract: Foundation models (FMs) have shown great promise in medical image analysis by
improving generalization across diverse downstream tasks. In ophthalmology,
several FMs have recently emerged, but there is still no clear answer to
fundamental questions: Which FM performs the best? Are they equally good across
different tasks? What if we combine all FMs together? To our knowledge, this is
the first study to systematically evaluate both single and fused ophthalmic
FMs. To address these questions, we propose FusionFM, a comprehensive
evaluation suite, along with two fusion approaches to integrate different
ophthalmic FMs. Our framework covers both ophthalmic disease detection
(glaucoma, diabetic retinopathy, and age-related macular degeneration) and
systemic disease prediction (diabetes and hypertension) based on retinal
imaging. We benchmarked four state-of-the-art FMs (RETFound, VisionFM,
RetiZero, and DINORET) using standardized datasets from multiple countries and
evaluated their performance using AUC and F1 metrics. Our results show that
DINORET and RetiZero achieve superior performance in both ophthalmic and
systemic disease tasks, with RetiZero exhibiting stronger generalization on
external datasets. Regarding fusion strategies, the Gating-based approach
provides modest improvements in predicting glaucoma, AMD, and hypertension.
Despite these advances, predicting systemic diseases, especially hypertension
in external cohort remains challenging. These findings provide an
evidence-based evaluation of ophthalmic FMs, highlight the benefits of model
fusion, and point to strategies for enhancing their clinical applicability.

</details>


### [4] [UniDCF: A Foundation Model for Comprehensive Dentocraniofacial Hard Tissue Reconstruction](https://arxiv.org/abs/2508.11728)
*Chunxia Ren,Ning Zhu,Yue Lai,Gui Chen,Ruijie Wang,Yangyi Hu,Suyao Liu,Shuwen Mao,Hong Su,Yu Zhang,Li Xiao*

Main category: cs.CV

TL;DR: UniDCF是一个统一的多模态深度学习框架，能够通过点云和多视角图像的融合编码来重建多种牙颌面硬组织，解决了现有单模态方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 牙颌面硬组织缺损严重影响患者生理功能、面部美观和心理健康，当前深度学习模型局限于单组织和单模态输入，导致泛化性差且在解剖保真度、计算效率和跨组织适应性之间存在权衡。

Method: 采用多模态融合编码技术，结合点云和多视角图像的互补优势，并引入基于分数的去噪模块来优化表面平滑度。构建了包含6,609名患者、54,555个标注实例的最大多模态数据集。

Result: UniDCF在几何精度、结构完整性和空间准确性方面优于现有最先进方法。临床模拟显示重建设计时间减少99%，临床医生接受度超过94%。

Conclusion: UniDCF实现了快速、自动化、高保真的重建，支持个性化和精确的修复治疗，简化临床工作流程，改善患者治疗效果。

Abstract: Dentocraniofacial hard tissue defects profoundly affect patients'
physiological functions, facial aesthetics, and psychological well-being,
posing significant challenges for precise reconstruction. Current deep learning
models are limited to single-tissue scenarios and modality-specific imaging
inputs, resulting in poor generalizability and trade-offs between anatomical
fidelity, computational efficiency, and cross-tissue adaptability. Here we
introduce UniDCF, a unified framework capable of reconstructing multiple
dentocraniofacial hard tissues through multimodal fusion encoding of point
clouds and multi-view images. By leveraging the complementary strengths of each
modality and incorporating a score-based denoising module to refine surface
smoothness, UniDCF overcomes the limitations of prior single-modality
approaches. We curated the largest multimodal dataset, comprising intraoral
scans, CBCT, and CT from 6,609 patients, resulting in 54,555 annotated
instances. Evaluations demonstrate that UniDCF outperforms existing
state-of-the-art methods in terms of geometric precision, structural
completeness, and spatial accuracy. Clinical simulations indicate UniDCF
reduces reconstruction design time by 99% and achieves clinician-rated
acceptability exceeding 94%. Overall, UniDCF enables rapid, automated, and
high-fidelity reconstruction, supporting personalized and precise restorative
treatments, streamlining clinical workflows, and enhancing patient outcomes.

</details>


### [5] [Ovis2.5 Technical Report](https://arxiv.org/abs/2508.11737)
*Shiyin Lu,Yang Li,Yu Xia,Yuwei Hu,Shanshan Zhao,Yanqing Ma,Zhichao Wei,Yinglun Li,Lunhao Duan,Jianshan Zhao,Yuxuan Han,Haijun Li,Wanying Chen,Junke Tang,Chengkun Hou,Zhixing Du,Tianli Zhou,Wenjie Zhang,Huping Ding,Jiahe Li,Wen Li,Gui Hu,Yiliang Gu,Siran Yang,Jiamang Wang,Hailong Sun,Yibo Wang,Hui Sun,Jinlong Huang,Yuping He,Shengze Shi,Weihong Zhang,Guodong Zheng,Junpeng Jiang,Sensen Gao,Yi-Feng Wu,Sijia Chen,Yuhui Chen,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: Ovis2.5是一个先进的多模态大语言模型，采用原生分辨率视觉transformer处理图像，避免固定分辨率分块带来的质量损失，并通过反思机制增强推理能力。该模型在OpenCompass排行榜上取得78.3分的优异成绩，在40B参数以下的开源模型中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统视觉模型在处理高密度视觉内容（如复杂图表）时因固定分辨率分块导致的细节丢失和全局布局破坏问题，同时提升多模态推理能力，开发了Ovis2.5模型。

Method: 采用原生分辨率视觉transformer处理可变分辨率图像，引入反思机制（包括自检和修订）作为可选"思考模式"，通过五阶段课程学习（包括视觉预训练、大规模指令微调、DPO和GRPO对齐）进行训练，并使用多模态数据打包和混合并行技术提升效率。

Result: Ovis2.5-9B在OpenCompass多模态排行榜上平均得分78.3，显著超越前代Ovis2-8B，在40B参数以下开源MLLM中达到SOTA；Ovis2.5-2B得分73.9，在同规模模型中创下SOTA。在STEM基准测试、 grounding任务、视频任务和复杂图表分析方面均取得领先结果。

Conclusion: Ovis2.5通过原生分辨率视觉处理和增强推理机制，在多模态理解任务上实现了显著的性能提升，特别是在处理视觉密集内容方面表现出色，为资源受限设备提供了高性能的小模型解决方案。

Abstract: We present Ovis2.5, a successor to Ovis2 designed for native-resolution
visual perception and strong multimodal reasoning. Ovis2.5 integrates a
native-resolution vision transformer that processes images at their native,
variable resolutions, avoiding the degradation from fixed-resolution tiling and
preserving both fine detail and global layout -- crucial for visually dense
content like complex charts. To strengthen reasoning, we train the model to
move beyond linear chain-of-thought and perform reflection -- including
self-checking and revision. This advanced capability is exposed as an optional
"thinking mode" at inference time, allowing users to trade latency for enhanced
accuracy on difficult inputs. The model is trained via a comprehensive
five-phase curriculum that progressively builds its skills. The process begins
with foundational visual and multimodal pretraining, advances through
large-scale instruction tuning, and culminates in alignment and reasoning
enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ
multimodal data packing and hybrid parallelism, yielding a significant
end-to-end speedup. We release two open-source models: Ovis2.5-9B and
Ovis2.5-2B. The latter continues the "small model, big performance" philosophy
of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the
OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a
substantial improvement over its predecessor, Ovis2-8B, and achieving
state-of-the-art results among open-source MLLMs in the sub-40B parameter
range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate
scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong
capabilities on grounding and video tasks, and achieves open-source SOTA at its
scale for complex chart analysis.

</details>


### [6] [VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models](https://arxiv.org/abs/2508.11801)
*Ming Cheng,Tong Wu,Jiazhen Hu,Jiaying Gong,Hoda Eldardiry*

Main category: cs.CV

TL;DR: 首个公开视频到文本的电子商务属性值提取数据集VideoAVE，涵盖14个领域和172个属性，通过CLIP-MoE系统精经数据质量，并建立了视频语言模型的评测基准。


<details>
  <summary>Details</summary>
Motivation: 现有的属性值提取数据集主要限于文本到文本或图像到文本设置，缺乏对产品视频、多样化属性覆盖和公开可用性的支持。

Method: 提出VideoAVE数据集，涵盖14个领域和172个唯一属性，使用CLIP基于的混合专家系统(CLIP-MoE)过滤失配的视频-产品对，并建立评测基准来测试各种视频语言模型。

Result: 得到了精经的224k训练数据和25k评估数据，评测结果显示视频到文本属性值提取仍然是个挑战性问题，特别是在开放设置下。

Conclusion: 视频到文本属性值提取仍有很大提升空间，需要发展更加高级的视频语言模型来利用有效的时间信息。

Abstract: Attribute Value Extraction (AVE) is important for structuring product
information in e-commerce. However, existing AVE datasets are primarily limited
to text-to-text or image-to-text settings, lacking support for product videos,
diverse attribute coverage, and public availability. To address these gaps, we
introduce VideoAVE, the first publicly available video-to-text e-commerce AVE
dataset across 14 different domains and covering 172 unique attributes. To
ensure data quality, we propose a post-hoc CLIP-based Mixture of Experts
filtering system (CLIP-MoE) to remove the mismatched video-product pairs,
resulting in a refined dataset of 224k training data and 25k evaluation data.
In order to evaluate the usability of the dataset, we further establish a
comprehensive benchmark by evaluating several state-of-the-art video vision
language models (VLMs) under both attribute-conditioned value prediction and
open attribute-value pair extraction tasks. Our results analysis reveals that
video-to-text AVE remains a challenging problem, particularly in open settings,
and there is still room for developing more advanced VLMs capable of leveraging
effective temporal information. The dataset and benchmark code for VideoAVE are
available at: https://github.com/gjiaying/VideoAVE

</details>


### [7] [An MLP Baseline for Handwriting Recognition Using Planar Curvature and Gradient Orientation](https://arxiv.org/abs/2508.11803)
*Azam Nouri*

Main category: cs.CV

TL;DR: 使用二阶几何特征（曲率大小、曲率符号和梯度方向）的MLP在手写字符识别中取得优异效果，证明手工特征也能实现深度学习优势


<details>
  <summary>Details</summary>
Motivation: 研究二阶几何特征是否足以驱动MLP进行手写字符识别，为CNN提供替代方案，探索可解释的手工特征的潜力

Method: 使用三个手工特征图（平面曲率大小、曲率符号和梯度方向）作为输入，构建曲率-方向MLP分类器

Result: 在MNIST数字上达到97%准确率，在EMNIST字母上达到89%准确率

Conclusion: 曲率基表示对手写字符图像具有强大判别力，深度学习优势可以通过可解释的手工工程特征实现

Abstract: This study investigates whether second-order geometric cues - planar
curvature magnitude, curvature sign, and gradient orientation - are sufficient
on their own to drive a multilayer perceptron (MLP) classifier for handwritten
character recognition (HCR), offering an alternative to convolutional neural
networks (CNNs). Using these three handcrafted feature maps as inputs, our
curvature-orientation MLP achieves 97 percent accuracy on MNIST digits and 89
percent on EMNIST letters. These results underscore the discriminative power of
curvature-based representations for handwritten character images and
demonstrate that the advantages of deep learning can be realized even with
interpretable, hand-engineered features.

</details>


### [8] [Labels or Input? Rethinking Augmentation in Multimodal Hate Detection](https://arxiv.org/abs/2508.11808)
*Sahajpreet Singh,Rongxin Ouyang,Subhayan Mukerjee,Kokil Jaidka*

Main category: cs.CV

TL;DR: 通过提示优化和多模态数据增帽来提高多模态恨恨图片检测的准确性和稳健性


<details>
  <summary>Details</summary>
Motivation: 现代网络中多模态恨恨内容涉及文本和图像组合的细微相互作用，现有视觉-语言模型缺乏细粒度监督并容易误检隐式恨恨话语

Method: 双重方法：1)提示优化框架，系统变化提示结构、监督粒度和训练模态；2)多模态数据增帽流水线，通过多代理LLM-VLM设置生成2479个反事实中立图片

Result: 结构化提示提高了小模型的稳健性，InternVL2在二元和细粒度设置中获得最佳F1分数，增帽流水线成功减少偏偏相关性并改善分类器通用性

Conclusion: 提示结构和数据组成与模型大小同样关键，目标增帽能支持更可信豖和上下文敏感的恨恨检测方向

Abstract: The modern web is saturated with multimodal content, intensifying the
challenge of detecting hateful memes, where harmful intent is often conveyed
through subtle interactions between text and image under the guise of humor or
satire. While recent advances in Vision-Language Models (VLMs) show promise,
these models lack support for fine-grained supervision and remain susceptible
to implicit hate speech. In this paper, we present a dual-pronged approach to
improve multimodal hate detection. First, we propose a prompt optimization
framework that systematically varies prompt structure, supervision granularity,
and training modality. We show that prompt design and label scaling both
influence performance, with structured prompts improving robustness even in
small models, and InternVL2 achieving the best F1-scores across binary and
scaled settings. Second, we introduce a multimodal data augmentation pipeline
that generates 2,479 counterfactually neutral memes by isolating and rewriting
the hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup,
successfully reduces spurious correlations and improves classifier
generalization. Our approaches inspire new directions for building synthetic
data to train robust and fair vision-language models. Our findings demonstrate
that prompt structure and data composition are as critical as model size, and
that targeted augmentation can support more trustworthy and context-sensitive
hate detection.

</details>


### [9] [Recent Advances in Transformer and Large Language Models for UAV Applications](https://arxiv.org/abs/2508.11834)
*Hamza Kheddar,Yassine Habchi,Mohamed Chahine Ghanem,Mustapha Hemis,Dusit Niyato*

Main category: cs.CV

TL;DR: 本文系统综述了Transformer架构在无人机系统中的应用，包括分类评估、性能比较、数据集分析，并指出了当前挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer模型的快速发展，其在无人机感知、决策和自主性方面的应用日益广泛，但缺乏系统的分类和评估，需要统一的框架来指导研究和实践。

Method: 采用系统性文献综述方法，对Transformer在无人机中的应用进行分类（注意力机制、CNN-Transformer混合、强化学习Transformer、大语言模型），通过结构化表格和性能基准进行比较分析。

Result: 提出了基于Transformer的无人机模型统一分类法，总结了在精准农业、自主导航等新兴应用中的进展，并识别了计算效率和实时部署等关键挑战。

Conclusion: 该综述为研究人员和从业者提供了理解和发展Transformer驱动的无人机技术的全面指导，指出了未来研究方向以推动该领域的发展。

Abstract: The rapid advancement of Transformer-based models has reshaped the landscape
of uncrewed aerial vehicle (UAV) systems by enhancing perception,
decision-making, and autonomy. This review paper systematically categorizes and
evaluates recent developments in Transformer architectures applied to UAVs,
including attention mechanisms, CNN-Transformer hybrids, reinforcement learning
Transformers, and large language models (LLMs). Unlike previous surveys, this
work presents a unified taxonomy of Transformer-based UAV models, highlights
emerging applications such as precision agriculture and autonomous navigation,
and provides comparative analyses through structured tables and performance
benchmarks. The paper also reviews key datasets, simulators, and evaluation
metrics used in the field. Furthermore, it identifies existing gaps in the
literature, outlines critical challenges in computational efficiency and
real-time deployment, and offers future research directions. This comprehensive
synthesis aims to guide researchers and practitioners in understanding and
advancing Transformer-driven UAV technologies.

</details>


### [10] [Towards Understanding 3D Vision: the Role of Gaussian Curvature](https://arxiv.org/abs/2508.11825)
*Sherlon Almeida da Silva,Davi Geiger,Luiz Velho,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: 该论文研究高斯曲率在3D表面建模中的作用，发现它提供了稀疏紧凑的表面描述，可作为几何先验改进3D重建，并可能作为立体方法的无监督度量指标


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的计算机视觉方法缺乏显式的3D几何模型，无法直接分析、跨模态迁移或进行受控实验

Method: 利用Middlebury立体数据集研究高斯曲率的特性，分析其在3D表面建模中的潜在价值

Result: 发现高斯曲率具有：(i)稀疏紧凑的表面描述能力，(ii)现有方法隐含考虑但无法显式提取，(iii)可作为几何先验改进重建，(iv)可能作为无监督立体度量

Conclusion: 高斯曲率作为不变几何量，在3D表面建模中具有重要价值，可为深度学习提供显式几何约束和评估指标

Abstract: Recent advances in computer vision have predominantly relied on data-driven
approaches that leverage deep learning and large-scale datasets. Deep neural
networks have achieved remarkable success in tasks such as stereo matching and
monocular depth reconstruction. However, these methods lack explicit models of
3D geometry that can be directly analyzed, transferred across modalities, or
systematically modified for controlled experimentation. We investigate the role
of Gaussian curvature in 3D surface modeling. Besides Gaussian curvature being
an invariant quantity under change of observers or coordinate systems, we
demonstrate using the Middlebury stereo dataset that it offers: (i) a sparse
and compact description of 3D surfaces, (ii) state-of-the-art monocular and
stereo methods seem to implicitly consider it, but no explicit module of such
use can be extracted, (iii) a form of geometric prior that can inform and
improve 3D surface reconstruction, and (iv) a possible use as an unsupervised
metric for stereo methods.

</details>


### [11] [EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models](https://arxiv.org/abs/2508.11886)
*Wenhui Zhu,Xiwen Chen,Zhipeng Wang,Shao Tang,Sayan Ghosh,Xuanzhao Dong,Rajat Koner,Yalin Wang*

Main category: cs.CV

TL;DR: 提出EVTP-IV方法，通过视觉token剪枝在保持精度的同时实现5倍视频和3.5倍图像分割加速，仅使用20%的token


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在指令视觉分割任务中推理成本过高，特别是视频处理成为主要瓶颈，需要高效的加速方法

Method: 基于k-center算法设计视觉token剪枝方法，整合空间信息确保更好的覆盖度，通过信息论分析支持设计

Result: 在标准IVS基准测试中达到5倍视频加速和3.5倍图像加速，仅使用20%的token仍保持可比精度，优于现有剪枝方法

Conclusion: EVTP-IV是简单有效的视觉token剪枝方法，显著加速推理同时保持性能，为高效多模态处理提供实用解决方案

Abstract: Instructed Visual Segmentation (IVS) tasks require segmenting objects in
images or videos based on natural language instructions. While recent
multimodal large language models (MLLMs) have achieved strong performance on
IVS, their inference cost remains a major bottleneck, particularly in video. We
empirically analyze visual token sampling in MLLMs and observe a strong
correlation between subset token coverage and segmentation performance. This
motivates our design of a simple and effective token pruning method that
selects a compact yet spatially representative subset of tokens to accelerate
inference. In this paper, we introduce a novel visual token pruning method for
IVS, called EVTP-IV, which builds upon the k-center by integrating spatial
information to ensure better coverage. We further provide an
information-theoretic analysis to support our design. Experiments on standard
IVS benchmarks show that our method achieves up to 5X speed-up on video tasks
and 3.5X on image tasks, while maintaining comparable accuracy using only 20%
of the tokens. Our method also consistently outperforms state-of-the-art
pruning baselines under varying pruning ratios.

</details>


### [12] [From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images](https://arxiv.org/abs/2508.11826)
*Dehn Xu,Tim Katzke,Emmanuel Müller*

Main category: cs.CV

TL;DR: 这篇论文系统性评估了多种图像-图象转换方法在图神经网络基于图级异常检测中的效果，发现颜色描述符表现最佳，加入形状和纹理特征能进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 虽然GNN已应用于图像演算的图表示下流任务，但还没有研究系统比较不同图像-图象转换方法在GNN基于图级异常检测中的效果。

Method: 系统性评估多种分割方案、边缘构建策略和节点特征集（包括颜色、纹理、形状描述符），在皮肤镜图像上进行广泛实验，测试无监督、弱监督和全监督模式下的性能。

Result: 颜色描述符单独表现最佳，加入形状和纹理特征能持续提升检测效果。最佳无监督配置达到0.805 AUC-ROC，弱监督提升至0.872，全监督达到0.914。

Conclusion: 这项研究为图像-图象转换方法的选择提供了系统性指南，表明GNN基于图级异常检测在皮肤镜图像分析中具有强大潜力，甚至在无监督情况下也能达到竞争性能。

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful approach for
graph-based machine learning tasks. Previous work applied GNNs to image-derived
graph representations for various downstream tasks such as classification or
anomaly detection. These transformations include segmenting images, extracting
features from segments, mapping them to nodes, and connecting them. However, to
the best of our knowledge, no study has rigorously compared the effectiveness
of the numerous potential image-to-graph transformation approaches for
GNN-based graph-level anomaly detection (GLAD). In this study, we
systematically evaluate the efficacy of multiple segmentation schemes, edge
construction strategies, and node feature sets based on color, texture, and
shape descriptors to produce suitable image-derived graph representations to
perform graph-level anomaly detection. We conduct extensive experiments on
dermoscopic images using state-of-the-art GLAD models, examining performance
and efficiency in purely unsupervised, weakly supervised, and fully supervised
regimes. Our findings reveal, for example, that color descriptors contribute
the best standalone performance, while incorporating shape and texture features
consistently enhances detection efficacy. In particular, our best unsupervised
configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805
without relying on pretrained backbones like comparable image-based approaches.
With the inclusion of sparse labels, the performance increases substantially to
0.872 and with full supervision to 0.914 AUC-ROC.

</details>


### [13] [Large Kernel Modulation Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.11893)
*Quanwei Hu,Yinggan Tang,Xuguang Zhang*

Main category: cs.CV

TL;DR: 提出基于纯CNN的大核调制网络（LKMN），通过增强部分大核块和交叉门控前馈网络，在轻量级图像超分辨率任务中平衡性能与效率，超越现有SOTA方法


<details>
  <summary>Details</summary>
Motivation: 解决资源受限场景下图像超分辨率的轻量化需求，CNN延迟低但缺乏非局部特征捕获能力，Transformer非局部建模能力强但推理速度慢

Method: LKMN包含两个核心组件：增强部分大核块（EPLKB）使用通道混洗增强通道交互、通道注意力聚焦关键信息、部分通道大核条带卷积提取非局部特征；交叉门控前馈网络（CGFN）通过可学习缩放因子动态调整特征差异，采用交叉门策略调制融合特征

Result: 在Manga109数据集4倍超分辨率上，LKMN-L比DAT-light提升0.23dB PSNR，推理速度快约4.8倍

Conclusion: 该方法在保持轻量化的同时实现了性能提升，证明了纯CNN架构在平衡质量与效率方面的优势

Abstract: Image super-resolution (SR) in resource-constrained scenarios demands
lightweight models balancing performance and latency. Convolutional neural
networks (CNNs) offer low latency but lack non-local feature capture, while
Transformers excel at non-local modeling yet suffer slow inference. To address
this trade-off, we propose the Large Kernel Modulation Network (LKMN), a pure
CNN-based model. LKMN has two core components: Enhanced Partial Large Kernel
Block (EPLKB) and Cross-Gate Feed-Forward Network (CGFN). The EPLKB utilizes
channel shuffle to boost inter-channel interaction, incorporates channel
attention to focus on key information, and applies large kernel strip
convolutions on partial channels for non-local feature extraction with reduced
complexity. The CGFN dynamically adjusts discrepancies between input, local,
and non-local features via a learnable scaling factor, then employs a
cross-gate strategy to modulate and fuse these features, enhancing their
complementarity. Extensive experiments demonstrate that our method outperforms
existing state-of-the-art (SOTA) lightweight SR models while balancing quality
and efficiency. Specifically, LKMN-L achieves 0.23 dB PSNR improvement over
DAT-light on the Manga109 dataset at $\times$4 upscale, with nearly $\times$4.8
times faster. Codes are in the supplementary materials. The code is available
at https://github.com/Supereeeee/LKMN.

</details>


### [14] [ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages](https://arxiv.org/abs/2508.11854)
*Matthew Hull,Haoyang Yang,Pratham Mehta,Mansi Phute,Aeree Cho,Haorang Wang,Matthew Lau,Wenke Lee,Wilian Lunardi,Martin Andreoni,Polo Chau*

Main category: cs.CV

TL;DR: 首个黑盒攻击方法ComplicitSplat，利用3D高斯抽射的渲染特性在特定视角嵌入伪装内容，攻击各种目标检测器


<details>
  <summary>Details</summary>
Motivation: 3D高斯抽射技术在安全关键任务中快速应用，需要研究可能的恶意攻击风险

Method: 利用3DGS的渲染方法创建视角特异性伪装，无需模型架构或权重，在特定视角显示恶意内容

Result: 攻击成功演示在真实物体和合成场景上，包括单阶段、多阶段和Transformer检测器

Conclusion: 曝露了3DGS在自主导航等关键系统中的新安全风险

Abstract: As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks
for efficient novel-view synthesis from static images, how might an adversary
tamper images to cause harm? We introduce ComplicitSplat, the first attack that
exploits standard 3DGS shading methods to create viewpoint-specific camouflage
- colors and textures that change with viewing angle - to embed adversarial
content in scene objects that are visible only from specific viewpoints and
without requiring access to model architecture or weights. Our extensive
experiments show that ComplicitSplat generalizes to successfully attack a
variety of popular detector - both single-stage, multi-stage, and
transformer-based models on both real-world capture of physical objects and
synthetic scenes. To our knowledge, this is the first black-box attack on
downstream object detectors using 3DGS, exposing a novel safety risk for
applications like autonomous navigation and other mission-critical robotic
systems.

</details>


### [15] [Impact of Clinical Image Quality on Efficient Foundation Model Finetuning](https://arxiv.org/abs/2508.11864)
*Yucheng Tang,Pawel Rajwa,Alexander Ng,Yipei Wang,Wen Yan,Natasha Thorley,Aqua Asif,Clare Allen,Louise Dickinson,Francesco Giganti,Shonit Punwani,Daniel C. Alexander,Veeru Kasivisvanathan,Yipeng Hu*

Main category: cs.CV

TL;DR: 该研究评估了医学影像基础模型ProFound在前列腺MRI中的标签效率，发现图像质量分布及其在微调与测试集之间的不匹配显著影响模型性能。当质量比例一致时，微调所需标注数据远少于从头训练，但标签效率取决于图像质量分布。


<details>
  <summary>Details</summary>
Motivation: 评估医学影像基础模型在不同图像质量条件下的标签效率，探究图像质量分布对模型泛化能力的影响，为实际部署提供质量标准和分布对齐指导。

Method: 使用在前列腺MRI大数据集上预训练的ProFound基础模型，系统性地变化微调和测试集中高/低质量图像的比例，测量微调模型的泛化性能，比较不同下游任务（自动放射学报告和前列腺癌检测）的表现。

Result: 研究发现：a) 微调集和测试集之间高/低质量图像比例的变化会导致下游性能显著差异；b) 微调集中足够高质量图像对维持强性能至关重要；c) 当质量比例一致时，微调所需标注数据大幅减少；d) 如果没有足够高质量微调数据，预训练模型可能无法超越无预训练的模型。

Conclusion: 图像质量分布及其在微调与部署环境之间的匹配对基础模型的性能至关重要。需要为特定下游任务制定微调数据的质量标准，并在微调和部署时量化图像质量，以充分实现基础模型的数据和计算效率优势。

Abstract: Foundation models in medical imaging have shown promising label efficiency,
achieving high downstream performance with only a fraction of annotated data.
Here, we evaluate this in prostate multiparametric MRI using ProFound, a
domain-specific vision foundation model pretrained on large-scale prostate MRI
datasets. We investigate how variable image quality affects label-efficient
finetuning by measuring the generalisability of finetuned models. Experiments
systematically vary high-/low-quality image ratios in finetuning and evaluation
sets. Our findings indicate that image quality distribution and its
finetune-and-test mismatch significantly affect model performance. In
particular: a) Varying the ratio of high- to low-quality images between
finetuning and test sets leads to notable differences in downstream
performance; and b) The presence of sufficient high-quality images in the
finetuning set is critical for maintaining strong performance, whilst the
importance of matched finetuning and testing distribution varies between
different downstream tasks, such as automated radiology reporting and prostate
cancer detection.When quality ratios are consistent, finetuning needs far less
labeled data than training from scratch, but label efficiency depends on image
quality distribution. Without enough high-quality finetuning data, pretrained
models may fail to outperform those trained without pretraining. This
highlights the importance of assessing and aligning quality distributions
between finetuning and deployment, and the need for quality standards in
finetuning data for specific downstream tasks. Using ProFound, we show the
value of quantifying image quality in both finetuning and deployment to fully
realise the data and compute efficiency benefits of foundation models.

</details>


### [16] [AdaRing: Towards Ultra-Light Vision-Language Adaptation via Cross-Layer Tensor Ring Decomposition](https://arxiv.org/abs/2508.11870)
*Ying Huang,Yuanbin Man,Wenqi Jia,Zhengzhong Tu,Junzhou Huang,Miao Yin*

Main category: cs.CV

TL;DR: AdaRing是一个基于张量环分解的视觉语言模型轻量化微调框架，通过跨层共享张量核心和层特定切片来消除冗余，同时使用多样化秩驱动适配器处理不同表示需求的任务，在减少90%训练参数的同时达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有适配器方法存在两个主要限制：1）由于忽略跨层冗余而导致压缩率有限；2）同质适配器的表示能力有限。需要开发更高效的参数微调方法。

Method: 提出基于跨层张量环分解（TRD）的框架，将适配器表示为层共享的张量核心和层特定切片，利用张量级低秩性消除跨层冗余，并通过泛化感知微调指导多样化秩驱动适配器协作。

Result: 实验表明AdaRing在减少平均训练参数90%的同时，达到了最先进的性能水平。

Conclusion: AdaRing框架成功实现了VLMs的超轻量参数高效适应，通过张量环分解和多样化适配器协作有效解决了现有方法的局限性。

Abstract: Adapter-based fine-tuning has gained remarkable attention in adapting large
pre-trained vision language models (VLMs) for a wide range of downstream tasks
efficiently. In this paradigm, only the inserted adapters are fine-tuned,
without the need for training the original VLM backbone. Existing works scale
adapters by integrating them into every layer of VLMs to increase the capacity
of adapters. However, these methods face two primary limitations: 1) limited
compression rate due to ignoring cross-layer redundancy, and 2) limited
representational capacity across homogeneous adapters. In this paper, we
propose a novel vision-language fine-tuning framework based on cross-layer
tensor ring decomposition (TRD) with the integration and collaboration of
diverse adapters, called AdaRing, achieving ultra-light parameter-efficient
adaptation of VLMs on various tasks. To remove the high redundancy that exists
among adapters across layers, we exploit the tensor-level low-rankness to
formulate adapters as layer-shared tensor cores and layer-specific slices.
Moreover, guided by generalization-aware fine-tuning, diverse rank-driven
adapters cooperate to handle tasks that require different representations. Our
experiments show that the proposed AdaRing achieves the state-of-the-art
performance while reducing average training parameters by 90%.

</details>


### [17] [A Sobel-Gradient MLP Baseline for Handwritten Character Recognition](https://arxiv.org/abs/2508.11902)
*Azam Nouri*

Main category: cs.CV

TL;DR: 使用Sobel算子提取的一阶边缘图作为输入，全连接MLP网络在手写字符识别任务上达到接近CNN的性能，证明一阶梯度已包含足够的分类信息


<details>
  <summary>Details</summary>
Motivation: 探索是否仅使用一阶边缘信息就能驱动MLP网络实现手写字符识别，作为CNN的替代方案

Method: 使用水平和垂直Sobel导数作为输入，训练全连接MLP网络在MNIST和EMNIST数据集上进行字符识别

Result: 在MNIST数字上达到98%准确率，EMNIST字母上达到92%准确率，接近CNN性能但内存占用更小、特征更透明

Conclusion: 一阶梯度已包含手写字符图像中大部分类别判别信息，边缘感知MLP是HCR的有力选择

Abstract: We revisit the classical Sobel operator to ask a simple question: Are
first-order edge maps sufficient to drive an all-dense multilayer perceptron
(MLP) for handwritten character recognition (HCR), as an alternative to
convolutional neural networks (CNNs)? Using only horizontal and vertical Sobel
derivatives as input, we train an MLP on MNIST and EMNIST Letters. Despite its
extreme simplicity, the resulting network reaches 98% accuracy on MNIST digits
and 92% on EMNIST letters -- approaching CNNs while offering a smaller memory
footprint and transparent features. Our findings highlight that much of the
class-discriminative information in handwritten character images is already
captured by first-order gradients, making edge-aware MLPs a compelling option
for HCR.

</details>


### [18] [OVG-HQ: Online Video Grounding with Hybrid-modal Queries](https://arxiv.org/abs/2508.11903)
*Runhao Zeng,Jiaqi Mao,Minghao Lai,Minh Hieu Phan,Yanjie Dong,Wei Wang,Qi Chen,Xiping Hu*

Main category: cs.CV

TL;DR: 提出了一种新的在线视频基准任务OVG-HQ，支持使用文本、图像、视频等混合模态查询，并提出了统一框架OVG-HQ-Unify来解决在线设置下的上下文限制和模态不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统视频基准任务在流媒体视频或使用视觉索引的查询场景下表现不佳，需要一种能够处理混合模态查询的在线解决方案。

Method: 提出OVG-HQ-Unify统一框架，包含参数化记忆块(PMB)来保留历史知识，以及交叉模态蒸馈策略来平衡不同模态的学习。构建了QVHighlights-Unify数据集和新的在线评估指标。

Result: 实验结果显示OVG-HQ-Unify在准确性和效率方面都超过现有模型，为在线混合模态视频基准提供了稳健解决方案。

Conclusion: 该研究成功开发了能够处理多模态查询的在线视频基准系统，解决了传统方法在实时场景下的限制，为该领域带来了重要进展。

Abstract: Video grounding (VG) task focuses on locating specific moments in a video
based on a query, usually in text form. However, traditional VG struggles with
some scenarios like streaming video or queries using visual cues. To fill this
gap, we present a new task named Online Video Grounding with Hybrid-modal
Queries (OVG-HQ), which enables online segment localization using text, images,
video segments, and their combinations. This task poses two new challenges:
limited context in online settings and modality imbalance during training,
where dominant modalities overshadow weaker ones. To address these, we propose
OVG-HQ-Unify, a unified framework featuring a Parametric Memory Block (PMB)
that retain previously learned knowledge to enhance current decision and a
cross-modal distillation strategy that guides the learning of non-dominant
modalities. This design enables a single model to effectively handle
hybrid-modal queries. Due to the lack of suitable datasets, we construct
QVHighlights-Unify, an expanded dataset with multi-modal queries. Besides,
since offline metrics overlook prediction timeliness, we adapt them to the
online setting, introducing oR@n, IoU=m, and online mean Average Precision
(omAP) to evaluate both accuracy and efficiency. Experiments show that our
OVG-HQ-Unify outperforms existing models, offering a robust solution for
online, hybrid-modal video grounding. Source code and datasets are available at
https://github.com/maojiaqi2324/OVG-HQ.

</details>


### [19] [SafeCtrl: Region-Based Safety Control for Text-to-Image Diffusion via Detect-Then-Suppress](https://arxiv.org/abs/2508.11904)
*Lingyun Zhang,Yu Xie,Yanwei Fu,Ping Chen*

Main category: cs.CV

TL;DR: SafeCtrl是一个轻量级的非侵入式插件，通过检测-抑制范式来提升文本到图像模型的安全性，在保持生成质量的同时有效防止有害内容生成


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型安全方法存在安全性与保真度之间的权衡问题，基于概念替换的定位方法可能导致语义不一致，需要更灵活的解决方案

Method: 采用检测-抑制范式，首先精确定位不安全内容，然后抑制有害语义而非硬替换，使用DPO训练策略利用图像级偏好数据学习抑制行为，无需像素级标注

Result: 在安全性和保真度保持方面显著优于现有最先进方法

Conclusion: 解耦的基于抑制的控制是构建更负责任生成模型的高效且可扩展方向

Abstract: The widespread deployment of text-to-image models is challenged by their
potential to generate harmful content. While existing safety methods, such as
prompt rewriting or model fine-tuning, provide valuable interventions, they
often introduce a trade-off between safety and fidelity. Recent
localization-based approaches have shown promise, yet their reliance on
explicit ``concept replacement" can sometimes lead to semantic incongruity. To
address these limitations, we explore a more flexible detect-then-suppress
paradigm. We introduce SafeCtrl, a lightweight, non-intrusive plugin that first
precisely localizes unsafe content. Instead of performing a hard A-to-B
substitution, SafeCtrl then suppresses the harmful semantics, allowing the
generative process to naturally and coherently resolve into a safe,
context-aware alternative. A key aspect of our work is a novel training
strategy using Direct Preference Optimization (DPO). We leverage readily
available, image-level preference data to train our module, enabling it to
learn nuanced suppression behaviors and perform region-guided interventions at
inference without requiring costly, pixel-level annotations. Extensive
experiments show that SafeCtrl significantly outperforms state-of-the-art
methods in both safety efficacy and fidelity preservation. Our findings suggest
that decoupled, suppression-based control is a highly effective and scalable
direction for building more responsible generative models.

</details>


### [20] [TimeSenCLIP: A Vision-Language Model for Remote Sensing Using Single-Pixel Time Series](https://arxiv.org/abs/2508.11919)
*Pallavi Jain,Diego Marcos,Dino Ienco,Roberto Interdonato,Tristan Berchoux*

Main category: cs.CV

TL;DR: TimeSenCLIP是一个轻量级框架，通过利用单个像素的时空和光谱信息进行土地利用分类，减少了对大空间瓦片和文本监督的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在遥感应用中面临两个关键挑战：依赖大空间瓦片增加计算成本，以及依赖文本监督但文本数据往往不易获得。

Method: 利用Sentinel-2影像的光谱和时间信息，结合地理标记的地面照片进行跨视角学习，最小化基于标题的训练需求，同时保持卫星和地面视角之间的语义对齐。

Result: 实验证明，单个像素输入结合时空和光谱线索足以进行专题制图，为大规模遥感应用提供了可扩展且高效的替代方案。

Conclusion: 该方法在LUCAS和Sen4Map数据集上评估了土地利用、作物类型和生态系统类型分类任务，展示了单像素时空光谱信息在遥感分类中的有效性。

Abstract: Vision-language models have shown significant promise in remote sensing
applications, particularly for land-use and land-cover (LULC) via zero-shot
classification and retrieval. However, current approaches face two key
challenges: reliance on large spatial tiles that increase computational cost,
and dependence on text-based supervision, which is often not readily available.
In this work, we present TimeSenCLIP, a lightweight framework that reevaluate
the role of spatial context by evaluating the effectiveness of a single pixel
by leveraging its temporal and spectral dimensions, for classifying LULC and
ecosystem types. By leveraging spectral and temporal information from
Sentinel-2 imagery and cross-view learning with geo-tagged ground-level photos,
we minimises the need for caption-based training while preserving semantic
alignment between overhead (satellite) and ground perspectives. Our approach is
grounded in the LUCAS and Sen4Map datasets, and evaluated on classification
tasks including LULC, crop type, and ecosystem type. We demonstrate that single
pixel inputs, when combined with temporal and spectral cues, are sufficient for
thematic mapping, offering a scalable and efficient alternative for large-scale
remote sensing applications. Code is available at
https://github.com/pallavijain-pj/TimeSenCLIP

</details>


### [21] [Assessment of Using Synthetic Data in Brain Tumor Segmentation](https://arxiv.org/abs/2508.11922)
*Aditi Jahagirdar,Sameer Joshi*

Main category: cs.CV

TL;DR: 使用GAN生成的合成MRI数据作为数据增帽策略，在脑部脱病分割任务中进行验证。虽然数量性能指标相似，但合成数据能够改善整体脱病边界分割质量。


<details>
  <summary>Details</summary>
Motivation: 手动脑部脱病分割面临脱病异质性、标注数据稀缺和类别不平衡等挑战，合成数据有潜力提升数据多样性。

Method: 使用预训练GAN模型生成合成MRI数据，将其与BraTS 2020实际数据以不同比例组合训练U-Net分割网络，评估数据增帽效果。

Result: 数量性指标（Dice系数、IoU等）在纯实际数据和混合数据之间相似，但质性分析显示40%实际+60%合成数据的组合能够改善整体脱病边界分割质量。脱病核心和增强区域的分割精度仍较低。

Conclusion: 合成数据作为数据增帽策略在脑部脱病分割中具有可行性，但需要进一步解决类别不平衡问题、进行更大规模实验和确保体积数据一致性。

Abstract: Manual brain tumor segmentation from MRI scans is challenging due to tumor
heterogeneity, scarcity of annotated data, and class imbalance in medical
imaging datasets. Synthetic data generated by generative models has the
potential to mitigate these issues by improving dataset diversity. This study
investigates, as a proof of concept, the impact of incorporating synthetic MRI
data, generated using a pre-trained GAN model, into training a U-Net
segmentation network. Experiments were conducted using real data from the BraTS
2020 dataset, synthetic data generated with the medigan library, and hybrid
datasets combining real and synthetic samples in varying proportions. While
overall quantitative performance (Dice coefficient, IoU, precision, recall,
accuracy) was comparable between real-only and hybrid-trained models,
qualitative inspection suggested that hybrid datasets, particularly with 40%
real and 60% synthetic data, improved whole tumor boundary delineation.
However, region-wise accuracy for the tumor core and the enhancing tumor
remained lower, indicating a persistent class imbalance. The findings support
the feasibility of synthetic data as an augmentation strategy for brain tumor
segmentation, while highlighting the need for larger-scale experiments,
volumetric data consistency, and mitigating class imbalance in future work.

</details>


### [22] [Deep Learning For Point Cloud Denoising: A Survey](https://arxiv.org/abs/2508.11932)
*Chengwei Zhang,Xueyi Zhang,Mingrui Lao,Tao Jiang,Xinhao Xu,Wenjie Li,Fubo Zhang,Longyong Chen*

Main category: cs.CV

TL;DR: 这篇论文是一个关于深度学习基于点云去噪的综述性研究，系统总结了该领域的发展状况、分析了关键挑战、提出了分类体系，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 实际环境中的点云数据存在多种模态和强度的噪声，点云去噪作为预处理步骤对下游任务至关重要。深度学习方法虽然表现优异，但缺乏系统的综述性研究来总结该领域的发展。

Method: 将点云去噪模型化为两步过程：离群点移除和表面噪声恢复。通过这种分析框架，对现有方法进行系统分类和比较分析，识别其相似性、差异性和各自优势。

Result: 论文提出了一个专门针对点云去噪任务的分类体系，包含了大部分点云去噪的常见场景和需求。通过系统化的总结和对比分析，揭示了不同方法的特点和适用性。

Conclusion: 论文填补了深度学习基于点云去噪领域缺乏综述性研究的空白，为该领域提供了系统的分析框架和分类体系。未来研究应关注方法的局限性和可扩展性，以促进点云去噪技术的进一步发展。

Abstract: Real-world environment-derived point clouds invariably exhibit noise across
varying modalities and intensities. Hence, point cloud denoising (PCD) is
essential as a preprocessing step to improve downstream task performance. Deep
learning (DL)-based PCD models, known for their strong representation
capabilities and flexible architectures, have surpassed traditional methods in
denoising performance. To our best knowledge, despite recent advances in
performance, no comprehensive survey systematically summarizes the developments
of DL-based PCD. To fill the gap, this paper seeks to identify key challenges
in DL-based PCD, summarizes the main contributions of existing methods, and
proposes a taxonomy tailored to denoising tasks. To achieve this goal, we
formulate PCD as a two-step process: outlier removal and surface noise
restoration, encompassing most scenarios and requirements of PCD. Additionally,
we compare methods in terms of similarities, differences, and respective
advantages. Finally, we discuss research limitations and future directions,
offering insights for further advancements in PCD.

</details>


### [23] [DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects](https://arxiv.org/abs/2508.11950)
*Tingbang Liang,Yixin Zeng,Jiatong Xie,Boyu Zhou*

Main category: cs.CV

TL;DR: DynamicPose是一个无需重新训练的6D位姿跟踪框架，专门针对快速移动的相机和物体场景，通过视觉惯性里程计、深度信息2D跟踪器和VIO引导的卡尔曼滤波器实现鲁棒跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要适用于静态或准静态场景，在相机和物体都快速移动时性能显著下降，需要解决快速运动场景下的6D位姿跟踪鲁棒性问题。

Method: 提出三个协同组件：1）视觉惯性里程计补偿相机运动引起的ROI偏移；2）深度信息2D跟踪器校正大物体平移引起的ROI偏差；3）VIO引导的卡尔曼滤波器预测物体旋转并生成候选位姿进行分层精炼。形成闭环系统确保精确位姿初始化和跟踪。

Result: 仿真和真实世界实验证明该方法有效，能够实现快速移动相机和物体的实时鲁棒6D位姿跟踪。

Conclusion: DynamicPose框架成功解决了快速运动场景下的6D位姿跟踪挑战，通过多组件协同和闭环系统设计实现了鲁棒且实时的性能表现。

Abstract: We present DynamicPose, a retraining-free 6D pose tracking framework that
improves tracking robustness in fast-moving camera and object scenarios.
Previous work is mainly applicable to static or quasi-static scenes, and its
performance significantly deteriorates when both the object and the camera move
rapidly. To overcome these challenges, we propose three synergistic components:
(1) A visual-inertial odometry compensates for the shift in the Region of
Interest (ROI) caused by camera motion; (2) A depth-informed 2D tracker
corrects ROI deviations caused by large object translation; (3) A VIO-guided
Kalman filter predicts object rotation, generates multiple candidate poses, and
then obtains the final pose by hierarchical refinement. The 6D pose tracking
results guide subsequent 2D tracking and Kalman filter updates, forming a
closed-loop system that ensures accurate pose initialization and precise pose
tracking. Simulation and real-world experiments demonstrate the effectiveness
of our method, achieving real-time and robust 6D pose tracking for fast-moving
cameras and objects.

</details>


### [24] [Transferable Class Statistics and Multi-scale Feature Approximation for 3D Object Detection](https://arxiv.org/abs/2508.11951)
*Hao Peng,Hong Sang,Yajing Ma,Ping Qiu,Chao Ji*

Main category: cs.CV

TL;DR: 通过知识蓄粉近似单邻域的多尺度特征学习，采用可转移特征嵌入和中心加权IoU优化，在保持检测性能的同时大幅节省计算成本。


<details>
  <summary>Details</summary>
Motivation: 多尺度特征对点云物体检测至关重要，但传统多尺度学习需要多次邻域搜索和尺度感知层，计算成本高且不利于轻量级模型发展。

Method: 1）基于知识蓄粉从单个邻域近似点基多尺度特征
2）设计可转移特征嵌入机制，采用类别统计特征补偿单邻域构造性不足
3）提出中心加权交并比（central weighted IoU）优化定位准确性

Result: 在公开数据集上进行了广泛实验，验证了方法的有效性，在保持检测性能的同时节省了计算成本。

Conclusion: 该方法通过创新的特征近似和转移机制，成功解决了多尺度特征学习的计算效率问题，为轻量级点云物体检测提供了有效解决方案。

Abstract: This paper investigates multi-scale feature approximation and transferable
features for object detection from point clouds. Multi-scale features are
critical for object detection from point clouds. However, multi-scale feature
learning usually involves multiple neighborhood searches and scale-aware
layers, which can hinder efforts to achieve lightweight models and may not be
conducive to research constrained by limited computational resources. This
paper approximates point-based multi-scale features from a single neighborhood
based on knowledge distillation. To compensate for the loss of constructive
diversity in a single neighborhood, this paper designs a transferable feature
embedding mechanism. Specifically, class-aware statistics are employed as
transferable features given the small computational cost. In addition, this
paper introduces the central weighted intersection over union for localization
to alleviate the misalignment brought by the center offset in optimization.
Note that the method presented in this paper saves computational costs.
Extensive experiments on public datasets demonstrate the effectiveness of the
proposed method.

</details>


### [25] [UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding](https://arxiv.org/abs/2508.11952)
*Yueming Xu,Jiahui Zhang,Ze Huang,Yurui Chen,Yanpeng Zhou,Zhenyu Chen,Yu-Jie Yuan,Pengxiang Xia,Guowei Huang,Xinyue Cai,Zhongang Qi,Xingyue Quan,Jianye Hao,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: UniUGG是首个统一理解和生成3D模态的框架，使用LLM处理文本和3D表示，核心采用潜在扩散模型生成高质量3D内容，支持基于参考图像和视角变换的3D场景生成，同时保持空间视觉问答能力。


<details>
  <summary>Details</summary>
Motivation: 尽管近期统一架构在图像理解和生成方面取得显著进展，但3D任务的整合仍然具有挑战性且探索不足，需要开发能够同时处理3D理解和生成的统一框架。

Method: 提出UniUGG框架，使用LLM理解和解码句子与3D表示；核心采用空间解码器利用潜在扩散模型生成高质量3D表示；提出几何语义学习策略预训练视觉编码器，联合捕获输入的语义和几何线索。

Result: 大量实验结果表明该方法在视觉表示、空间理解和3D生成方面具有优越性。

Conclusion: UniUGG成功实现了3D理解和生成的统一，通过几何语义学习策略有效提升了空间理解和生成能力，为3D多模态任务提供了有效的解决方案。

Abstract: Despite the impressive progress on understanding and generating images shown
by the recent unified architectures, the integration of 3D tasks remains
challenging and largely unexplored. In this paper, we introduce UniUGG, the
first unified understanding and generation framework for 3D modalities. Our
unified framework employs an LLM to comprehend and decode sentences and 3D
representations. At its core, we propose a spatial decoder leveraging a latent
diffusion model to generate high-quality 3D representations. This allows for
the generation and imagination of 3D scenes based on a reference image and an
arbitrary view transformation, while remaining supports for spatial visual
question answering (VQA) tasks. Additionally, we propose a geometric-semantic
learning strategy to pretrain the vision encoder. This design jointly captures
the input's semantic and geometric cues, enhancing both spatial understanding
and generation. Extensive experimental results demonstrate the superiority of
our method in visual representation, spatial understanding, and 3D generation.
The source code will be released upon paper acceptance.

</details>


### [26] [SAMDWICH: Moment-aware Video-text Alignment for Referring Video Object Segmentation](https://arxiv.org/abs/2508.11955)
*Seunghun Lee,Jiwan Seo,Jeonghoon Kim,Siwon Kim,Haeun Yun,Hyogyeong Jeon,Wonhyeok Choi,Jaehoon Jeong,Zane Durante,Sang Hyun Park,Sunghoon Im*

Main category: cs.CV

TL;DR: SAMDWICH是一个基于时刻感知的Referring Video Object Segmentation框架，通过引入MeViS-M数据集和MDP传播策略、OSS监督策略，解决了现有方法中的语义错位问题，在复杂场景下实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的Referring Video Object Segmentation方法存在语义错位问题，主要原因是训练时对所有可见对象进行无差别帧采样和监督，而不考虑它们与文本查询的实际相关性。

Method: 1) 构建MeViS-M数据集，手动标注每个对象被表达式引用的时间时刻；2) 提出Moment-guided Dual-path Propagation (MDP)策略，通过时刻中心记忆机制在相关和不相关帧上进行训练；3) 引入Object-level Selective Supervision (OSS)，只在时间对齐的帧上监督相关对象。

Result: 在具有挑战性的MeViS基准测试中实现了最先进的性能，特别是在涉及多样化表达式的复杂场景中表现出色。

Conclusion: 通过时刻感知的监督和选择性训练策略，SAMDWICH有效增强了视频-文本对齐，显著提升了引用视频对象分割的准确性和鲁棒性。

Abstract: Referring Video Object Segmentation (RVOS) aims to segment and track objects
in videos based on natural language expressions, requiring precise alignment
between visual content and textual queries. However, existing methods often
suffer from semantic misalignment, largely due to indiscriminate frame sampling
and supervision of all visible objects during training -- regardless of their
actual relevance to the expression. To address this, we introduce a
moment-aware RVOS framework named SAMDWICH, along with a newly annotated
dataset, MeViS-M, built upon the challenging MeViS benchmark. We manually
annotate temporal moments indicating when each object is referred to by the
expression, enabling semantically grounded supervision that strengthens
video-text alignment. SAMDWICH leverages these aligned text-to-clip pairs to
guide training, significantly enhancing referential understanding. Building
upon this framework, we propose Moment-guided Dual-path Propagation (MDP), a
moment-aware propagation strategy that improves both object grounding and
tracking by training on both relevant and irrelevant frames through a
moment-centric memory mechanism. In addition, we introduce Object-level
Selective Supervision (OSS), an object-level filtering strategy that supervises
only the objects temporally aligned with the expression in each training clip.
This selective supervision reduces semantic noise and reinforces
language-conditioned learning. Extensive experiments show that SAMDWICH
achieves state-of-the-art performance on challenging MeViS benchmark,
particularly excelling in complex scenarios involving diverse expressions.

</details>


### [27] [PEdger++: Practical Edge Detection via Assembling Cross Information](https://arxiv.org/abs/2508.11961)
*Yuanbin Fu,Liang Li,Xiaojie Guo*

Main category: cs.CV

TL;DR: PEdger++是一个协作学习框架，通过利用异构架构、不同训练时刻和多重参数采样的跨信息来提升边缘检测性能，在保持高精度的同时显著降低计算成本和模型大小。


<details>
  <summary>Details</summary>
Motivation: 边缘检测是计算机视觉应用的基础，但现有深度学习方法计算成本高，难以在资源受限设备上部署。需要平衡高精度和低计算复杂度。

Method: 提出PEdger++协作学习框架，利用异构架构、多样训练时刻和多重参数采样的跨信息，从集成学习角度增强特征学习能力。

Result: 在BSDS500、NYUD和Multicue数据集上的实验表明，该方法在定量和定性评估上都优于现有方法，并提供不同计算需求的多个模型版本。

Conclusion: PEdger++有效解决了边缘检测中精度与计算效率的平衡问题，展示了在资源受限设备上的良好适应性。

Abstract: Edge detection serves as a critical foundation for numerous computer vision
applications, including object detection, semantic segmentation, and image
editing, by extracting essential structural cues that define object boundaries
and salient edges. To be viable for broad deployment across devices with
varying computational capacities, edge detectors shall balance high accuracy
with low computational complexity. While deep learning has evidently improved
accuracy, they often suffer from high computational costs, limiting their
applicability on resource-constrained devices. This paper addresses the
challenge of achieving that balance: \textit{i.e.}, {how to efficiently capture
discriminative features without relying on large-size and sophisticated
models}. We propose PEdger++, a collaborative learning framework designed to
reduce computational costs and model sizes while improving edge detection
accuracy. The core principle of our PEdger++ is that cross-information derived
from heterogeneous architectures, diverse training moments, and multiple
parameter samplings, is beneficial to enhance learning from an ensemble
perspective. Extensive experimental results on the BSDS500, NYUD and Multicue
datasets demonstrate the effectiveness of our approach, both quantitatively and
qualitatively, showing clear improvements over existing methods. We also
provide multiple versions of the model with varying computational requirements,
highlighting PEdger++'s adaptability with respect to different resource
constraints. Codes are accessible at
https://github.com/ForawardStar/EdgeDetectionviaPEdgerPlus/.

</details>


### [28] [Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis](https://arxiv.org/abs/2508.11988)
*Nicolas Mastropasqua,Ignacio Bugueno-Cordova,Rodrigo Verschae,Daniel Acevedo,Pablo Negri,Maria E. Buemi*

Main category: cs.CV

TL;DR: 这篇论文提出了一个新的多分辨率多模态微表情数据集，利用事件相机收集同步RGB和事件数据，在变化光照条件下录制微表情。基础实验结果显示事件数据在行为单元分类和帧重建任务中都显著优于RGB数据。


<details>
  <summary>Details</summary>
Motivation: 微表情分析在人机交互和驾驶员监测等领域有重要应用，但依靠RGB相机捕捉细微快速面部运动存在困难，而事件相机具有微秒级精度、高动态范围和低延迟优势，但目前公开的事件基行为单元数据集仍然稀缺。

Method: 构建了一个新的多分辨率多模态微表情数据集，使用同步的RGB和事件相机在变化光照条件下进行录制。评估了两个基准任务：1）使用疯粉神经网络进行行为单元分类；2）使用条件变分自动编码器进行帧重建。

Result: 行为单元分类任务中，使用事件数据达到51.23%的准确率（RGB数据仅23.12%）。帧重建任务中，高分辨率事件输入获得SSIM=0.8513和PSNR=26.89dB的良好性能。

Conclusion: 事件基数据可以有效用于微表情识别和帧重建，为微表情分析领域提供了更优异的数据源。

Abstract: Micro-expression analysis has applications in domains such as Human-Robot
Interaction and Driver Monitoring Systems. Accurately capturing subtle and fast
facial movements remains difficult when relying solely on RGB cameras, due to
limitations in temporal resolution and sensitivity to motion blur. Event
cameras offer an alternative, with microsecond-level precision, high dynamic
range, and low latency. However, public datasets featuring event-based
recordings of Action Units are still scarce. In this work, we introduce a
novel, preliminary multi-resolution and multi-modal micro-expression dataset
recorded with synchronized RGB and event cameras under variable lighting
conditions. Two baseline tasks are evaluated to explore the spatial-temporal
dynamics of micro-expressions: Action Unit classification using Spiking Neural
Networks (51.23\% accuracy with events vs. 23.12\% with RGB), and frame
reconstruction using Conditional Variational Autoencoders, achieving SSIM =
0.8513 and PSNR = 26.89 dB with high-resolution event input. These promising
results show that event-based data can be used for micro-expression recognition
and frame reconstruction.

</details>


### [29] [MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2508.11999)
*Daoze Zhang,Zhanheng Nie,Jianyu Liu,Chenghan Fu,Wanxian Guan,Yuan Gao,Jun Song,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: 提出了首个基于生成式多模态大语言模型的产品表示学习模型MOON，通过引导式混合专家模块、核心语义区域检测和专业化负采样策略，解决了产品图像文本多对一对齐、背景噪声干扰等挑战，并在新构建的大规模基准上展现了优异的零样本性能。


<details>
  <summary>Details</summary>
Motivation: 现有判别式双流架构难以建模产品多图像与文本之间的多对一对齐关系，而生成式多模态大语言模型在产品表示学习方面具有巨大潜力，但面临缺乏多模态建模模块、产品图像背景噪声干扰以及缺乏标准评估基准等挑战。

Method: 1) 采用引导式混合专家(MoE)模块进行多模态和特定方面的产品内容建模；2) 有效检测产品图像中的核心语义区域以减少背景噪声干扰；3) 引入专业化负采样策略增加负样本的难度和多样性；4) 发布大规模多模态基准MBE。

Result: 模型在新建基准和公共数据集上均表现出竞争力的零样本性能，在跨模态检索、产品分类和属性预测等多种下游任务中展现出强大的泛化能力。案例研究和可视化证明了MOON在产品理解方面的有效性。

Conclusion: MOON作为首个基于生成式MLLM的产品表示学习模型，成功解决了多对一对齐、背景噪声等关键问题，为产品理解领域提供了新的解决方案和评估基准。

Abstract: With the rapid advancement of e-commerce, exploring general representations
rather than task-specific ones has attracted increasing research attention. For
product understanding, although existing discriminative dual-flow architectures
drive progress in this field, they inherently struggle to model the many-to-one
alignment between multiple images and texts of products. Therefore, we argue
that generative Multimodal Large Language Models (MLLMs) hold significant
potential for improving product representation learning. Nevertheless,
achieving this goal still remains non-trivial due to several key challenges:
the lack of multimodal and aspect-aware modeling modules in typical LLMs; the
common presence of background noise in product images; and the absence of a
standard benchmark for evaluation. To address these issues, we propose the
first generative MLLM-based model named MOON for product representation
learning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for
targeted modeling of multimodal and aspect-specific product content; (2)
effectively detects core semantic regions in product images to mitigate the
distraction and interference caused by background noise; and (3) introduces the
specialized negative sampling strategy to increase the difficulty and diversity
of negative samples. In addition, we release a large-scale multimodal benchmark
MBE for various product understanding tasks. Experimentally, our model
demonstrates competitive zero-shot performance on both our benchmark and the
public dataset, showcasing strong generalization across various downstream
tasks, including cross-modal retrieval, product classification, and attribute
prediction. Furthermore, the case study and visualization illustrate the
effectiveness of MOON for product understanding.

</details>


### [30] [InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes](https://arxiv.org/abs/2508.12015)
*Hongyuan Liu,Haochen Yu,Jianfei Jiang,Qiankun Liu,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: InstDrive是一个面向动态驾驶场景的实例感知3D高斯泼溅框架，首次实现了开放世界动态驾驶场景的3D实例分割，无需数据预处理或复杂优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法将背景元素统一为单一表示，阻碍了实例级理解和灵活场景编辑；现有室内场景方法不适用于室外驾驶场景；需要避免依赖预处理实例ID或复杂管道。

Method: 使用SAM生成的掩码作为伪真值，通过对比损失和伪监督目标指导2D特征学习；在3D层面引入正则化隐式编码实例身份，通过体素损失强制一致性；使用轻量级静态码本桥接连续特征和离散身份。

Result: 定量和定性实验证明了InstDrive的有效性，是首个在动态开放世界驾驶场景中实现3D实例分割的框架。

Conclusion: InstDrive成功解决了动态驾驶场景中实例感知重建的挑战，为自动驾驶和场景理解提供了有效的解决方案。

Abstract: Reconstructing dynamic driving scenes from dashcam videos has attracted
increasing attention due to its significance in autonomous driving and scene
understanding. While recent advances have made impressive progress, most
methods still unify all background elements into a single representation,
hindering both instance-level understanding and flexible scene editing. Some
approaches attempt to lift 2D segmentation into 3D space, but often rely on
pre-processed instance IDs or complex pipelines to map continuous features to
discrete identities. Moreover, these methods are typically designed for indoor
scenes with rich viewpoints, making them less applicable to outdoor driving
scenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian
Splatting framework tailored for the interactive reconstruction of dynamic
driving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D
feature learning via contrastive loss and pseudo-supervised objectives. At the
3D level, we introduce regularization to implicitly encode instance identities
and enforce consistency through a voxel-based loss. A lightweight static
codebook further bridges continuous features and discrete identities without
requiring data pre-processing or complex optimization. Quantitative and
qualitative experiments demonstrate the effectiveness of InstDrive, and to the
best of our knowledge, it is the first framework to achieve 3D instance
segmentation in dynamic, open-world driving scenes.More visualizations are
available at our project page.

</details>


### [31] [WiseLVAM: A Novel Framework For Left Ventricle Automatic Measurements](https://arxiv.org/abs/2508.12023)
*Durgesh Kumar Singh,Qing Cao,Sarina Thomas,Ahcène Boubekki,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: 这篇论文提出了一种全自动化的WiseLVAM框架，通过结合B模式图像和动态解剖模式来提高左室线性测量的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统的自动化方法在B模式图像中直接预测标记点，小的位置偏差也会导致显著的测量误差，影响临床可靠性。

Method: 首先通过弱监督B模式标记点检测器估计左室轮廓，然后推断左室长轴和基底水平来放置扫描线。在动态解剖模式图像中预测标记点进行测量。

Result: 该方法结合了B模式的结构感知能力和动态解剖模式的运动感知能力，提高了测量的稳健性和准确性。

Conclusion: WiseLVAM框架为左室线性测量提供了一种全自动化但又可手动调整的实用解决方案，有潜力在常规临床应用中实施。

Abstract: Clinical guidelines recommend performing left ventricular (LV) linear
measurements in B-mode echocardiographic images at the basal level -- typically
at the mitral valve leaflet tips -- and aligned perpendicular to the LV long
axis along a virtual scanline (SL). However, most automated methods estimate
landmarks directly from B-mode images for the measurement task, where even
small shifts in predicted points along the LV walls can lead to significant
measurement errors, reducing their clinical reliability. A recent
semi-automatic method, EnLVAM, addresses this limitation by constraining
landmark prediction to a clinician-defined SL and training on generated
Anatomical Motion Mode (AMM) images to predict LV landmarks along the same. To
enable full automation, a contour-aware SL placement approach is proposed in
this work, in which the LV contour is estimated using a weakly supervised
B-mode landmark detector. SL placement is then performed by inferring the LV
long axis and the basal level-mimicking clinical guidelines. Building on this
foundation, we introduce \textit{WiseLVAM} -- a novel, fully automated yet
manually adaptable framework for automatically placing the SL and then
automatically performing the LV linear measurements in the AMM mode.
\textit{WiseLVAM} utilizes the structure-awareness from B-mode images and the
motion-awareness from AMM mode to enhance robustness and accuracy with the
potential to provide a practical solution for the routine clinical application.

</details>


### [32] [Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering](https://arxiv.org/abs/2508.12036)
*Rakesh Thakur,Yusra Tariq*

Main category: cs.CV

TL;DR: Q-FSRU是一个结合频域表示和量子检索增强生成的医疗视觉问答模型，在VQA-RAD数据集上表现优异


<details>
  <summary>Details</summary>
Motivation: 解决需要同时理解图像和文本的复杂临床问题仍然是医疗AI的主要挑战

Method: 使用FFT将医学图像和文本特征转换到频域以提取有意义信息，结合量子检索系统从外部源获取医学事实，并将两者融合进行推理

Result: 在VQA-RAD数据集上超越了先前模型，特别是在需要图像-文本推理的复杂案例中表现突出

Conclusion: 频域和量子信息的结合提高了性能和可解释性，为构建智能、清晰、有用的医生AI工具提供了有前景的方法

Abstract: Solving tough clinical questions that require both image and text
understanding is still a major challenge in healthcare AI. In this work, we
propose Q-FSRU, a new model that combines Frequency Spectrum Representation and
Fusion (FSRU) with a method called Quantum Retrieval-Augmented Generation
(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in
features from medical images and related text, then shifts them into the
frequency domain using Fast Fourier Transform (FFT). This helps it focus on
more meaningful data and filter out noise or less useful information. To
improve accuracy and ensure that answers are based on real knowledge, we add a
quantum-inspired retrieval system. It fetches useful medical facts from
external sources using quantum-based similarity techniques. These details are
then merged with the frequency-based features for stronger reasoning. We
evaluated our model using the VQA-RAD dataset, which includes real radiology
images and questions. The results showed that Q-FSRU outperforms earlier
models, especially on complex cases needing image-text reasoning. The mix of
frequency and quantum information improves both performance and explainability.
Overall, this approach offers a promising way to build smart, clear, and
helpful AI tools for doctors.

</details>


### [33] [VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models](https://arxiv.org/abs/2508.12081)
*Haidong Xu,Guangwei Xu,Zhedong Zheng,Xiatian Zhu,Wei Ji,Xiangtai Li,Ruijie Guo,Meishan Zhang,Min zhang,Hao Fei*

Main category: cs.CV

TL;DR: VimoRAG是一个基于视频检索增强的运动生成框架，通过从大规模视频数据库中检索相关2D人体运动信号来解决运动大语言模型的数据稀缺问题，显著提升了仅文本输入条件下的3D运动生成性能。


<details>
  <summary>Details</summary>
Motivation: 运动大语言模型由于标注数据有限而面临严重的域外/词汇外问题，需要利用大规模野外视频数据库来增强3D运动生成能力。

Method: 设计了Gemini Motion Video Retriever机制进行有效的运动中心视频检索，以及Motion-centric Dual-alignment DPO Trainer来缓解检索结果不佳导致的错误传播问题。

Result: 实验结果表明VimoRAG显著提升了仅文本输入条件下运动大语言模型的性能。

Conclusion: 视频检索增强方法有效解决了运动生成中的数据稀缺问题，为运动大语言模型提供了新的增强途径。

Abstract: This paper introduces VimoRAG, a novel video-based retrieval-augmented motion
generation framework for motion large language models (LLMs). As motion LLMs
face severe out-of-domain/out-of-vocabulary issues due to limited annotated
data, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D
motion generation by retrieving relevant 2D human motion signals. While
video-based motion RAG is nontrivial, we address two key bottlenecks: (1)
developing an effective motion-centered video retrieval model that
distinguishes human poses and actions, and (2) mitigating the issue of error
propagation caused by suboptimal retrieval results. We design the Gemini Motion
Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,
enabling effective retrieval and generation processes. Experimental results
show that VimoRAG significantly boosts the performance of motion LLMs
constrained to text-only input.

</details>


### [34] [Automated Model Evaluation for Object Detection via Prediction Consistency and Reliablity](https://arxiv.org/abs/2508.12082)
*Seungju Yoo,Hyuk Kwon,Joong-Won Hwang,Kibok Lee*

Main category: cs.CV

TL;DR: 自动化对识检测器性能评估的PCR方法，通过分析NMS前后棒围框的空间一致性和可靠性来估计性能，避免人工标注成本


<details>
  <summary>Details</summary>
Motivation: 对识检测器在实际应用中的性能评估依然依赖成本昂贵的人工标注，需要一种自动化的性能评估方法

Method: 提出预测一致性和可靠性(PCR)方法，利用非最大压制(NMS)前后的多个候选棒围框，聚合测量空间一致性和重叠棒围框的信心度可靠性

Result: PCR方法比现有的AutoEval方法能够提供更准确的性能估计，构建的元数据集覆盖了更广泛的检测性能范围

Conclusion: 该研究为对识检测器的自动化性能评估提供了一种高效的方法，通过PCR指标可以在不依赖真实标签的情况下准确预测检测器性能

Abstract: Recent advances in computer vision have made training object detectors more
efficient and effective; however, assessing their performance in real-world
applications still relies on costly manual annotation. To address this
limitation, we develop an automated model evaluation (AutoEval) framework for
object detection. We propose Prediction Consistency and Reliability (PCR),
which leverages the multiple candidate bounding boxes that conventional
detectors generate before non-maximum suppression (NMS). PCR estimates
detection performance without ground-truth labels by jointly measuring 1) the
spatial consistency between boxes before and after NMS, and 2) the reliability
of the retained boxes via the confidence scores of overlapping boxes. For a
more realistic and scalable evaluation, we construct a meta-dataset by applying
image corruptions of varying severity. Experimental results demonstrate that
PCR yields more accurate performance estimates than existing AutoEval methods,
and the proposed meta-dataset covers a wider range of detection performance.
The code is available at https://github.com/YonseiML/autoeval-det.

</details>


### [35] [Generic Event Boundary Detection via Denoising Diffusion](https://arxiv.org/abs/2508.12084)
*Jaejun Hwang,Dayoung Gong,Manjin Kim,Minsu Cho*

Main category: cs.CV

TL;DR: DiffGEBD是一个基于扩散模型的通用事件边界检测方法，通过生成式视角处理事件边界检测的主观性和多样性问题，在Kinetics-GEBD和TAPOS基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的事件边界检测方法专注于确定性预测，忽略了事件边界的主观性和解决方案的多样性。本文旨在从生成式角度解决这个问题。

Method: 提出DiffGEBD扩散模型：通过时间自相似性编码相邻帧的相关变化，然后以编码特征为条件，迭代地将随机噪声解码为合理的事件边界。使用无分类器引导来控制去噪扩散的多样性程度。

Result: 在Kinetics-GEBD和TAPOS两个标准基准测试中取得了强劲性能，能够生成多样且合理的事件边界。

Conclusion: 扩散模型为通用事件边界检测提供了一个有效的生成式解决方案，能够处理事件边界的主观性和多样性问题，并引入了新的评估指标来同时考虑多样性和保真度。

Abstract: Generic event boundary detection (GEBD) aims to identify natural boundaries
in a video, segmenting it into distinct and meaningful chunks. Despite the
inherent subjectivity of event boundaries, previous methods have focused on
deterministic predictions, overlooking the diversity of plausible solutions. In
this paper, we introduce a novel diffusion-based boundary detection model,
dubbed DiffGEBD, that tackles the problem of GEBD from a generative
perspective. The proposed model encodes relevant changes across adjacent frames
via temporal self-similarity and then iteratively decodes random noise into
plausible event boundaries being conditioned on the encoded features.
Classifier-free guidance allows the degree of diversity to be controlled in
denoising diffusion. In addition, we introduce a new evaluation metric to
assess the quality of predictions considering both diversity and fidelity.
Experiments show that our method achieves strong performance on two standard
benchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event
boundaries.

</details>


### [36] [Enhancing 3D point accuracy of laser scanner through multi-stage convolutional neural network for applications in construction](https://arxiv.org/abs/2508.12089)
*Qinyuan Fan,Clemens Gühmann*

Main category: cs.CV

TL;DR: 使用多步卷积神经网络组合高低端扫描仪数据，通过统计学习模型精确缩减粗糕室内环境中光电扫描仪的位置错误，提升低端设备测量精度


<details>
  <summary>Details</summary>
Motivation: 高端和低端光电扫描仪在粗糕室内环境中都存在位置错误，影响高精度几何模型构建和改造工程的准确性

Method: 将高精度扫描仪作为参考，与低精度扫描仪在同一环境下配对测量，通过统计关系建立测量差异与空间分布的关联，结合传统几何处理和神经网经细化进行系统错误等正

Result: 在粗糕室内数据集中，平均方差错误减少超过70%，峰值信噪比提升约6分贝，低端设备可达到接近高端设备的测量不确定性水平

Conclusion: 该方法通过软件算法方式有效提升低端光电扫描设备的测量精度，无需硬件改造即可实现高精度测量能力

Abstract: We propose a multi-stage convolutional neural network (MSCNN) based
integrated method for reducing uncertainty of 3D point accuracy of lasar
scanner (LS) in rough indoor rooms, providing more accurate spatial
measurements for high-precision geometric model creation and renovation. Due to
different equipment limitations and environmental factors, high-end and low-end
LS have positional errors. Our approach pairs high-accuracy scanners (HAS) as
references with corresponding low-accuracy scanners (LAS) of measurements in
identical environments to quantify specific error patterns. By establishing a
statistical relationship between measurement discrepancies and their spatial
distribution, we develop a correction framework that combines traditional
geometric processing with targeted neural network refinement. This method
transforms the quantification of systematic errors into a supervised learning
problem, allowing precise correction while preserving critical geometric
features. Experimental results in our rough indoor rooms dataset show
significant improvements in measurement accuracy, with mean square error (MSE)
reductions exceeding 70% and peak signal-to-noise ratio (PSNR) improvements of
approximately 6 decibels. This approach enables low-end devices to achieve
measurement uncertainty levels approaching those of high-end devices without
hardware modifications.

</details>


### [37] [Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion](https://arxiv.org/abs/2508.12094)
*Songwei Liu,Hong Liu,Fangmin Chen,Xurui Peng,Chenqian Yan,Lean Fu,Xing Mei*

Main category: cs.CV

TL;DR: 这篇论文提出了一种时间步感知的累积错误补偿方案，通过理论分析错误传播机制，有效减少散布模型在低精度量化中的性能损失，实现了SOTA的量化性能。


<details>
  <summary>Details</summary>
Motivation: 散布模型在图像合成领域取得了突破性进展，但迭代去噪过程导致计算成本高。虽然训练后量化(PTQ)可以加速金取样，但迭代特性使得步进量化错误在生成过程中累积，影响输出质量。

Method: 建立理论框架数学地形容错误传播机制，求解每步量化错误传播方程，并得到累积错误的闭式解。基于理论基础，提出时间步感知的累积错误补偿方案。

Result: 在多个图像数据集上进行的广泛实验表明，该补偿策略有效减少了错误传播，显著提升了现有PTQ方法的性能，在低精度散布模型上达到了最先进的性能水平。

Conclusion: 通过理论分析错误传播机制并提出时间步感知的补偿方案，有效解决了散布模型量化中的累积错误问题，为散布模型的大规模部署提供了有效的加速解决方案。

Abstract: Diffusion models have transformed image synthesis by establishing
unprecedented quality and creativity benchmarks. Nevertheless, their
large-scale deployment faces challenges due to computationally intensive
iterative denoising processes. Although post-training quantization(PTQ)
provides an effective pathway for accelerating sampling, the iterative nature
of diffusion models causes stepwise quantization errors to accumulate
progressively during generation, inevitably compromising output fidelity. To
address this challenge, we develop a theoretical framework that mathematically
formulates error propagation in Diffusion Models (DMs), deriving per-step
quantization error propagation equations and establishing the first closed-form
solution for cumulative error. Building on this theoretical foundation, we
propose a timestep-aware cumulative error compensation scheme. Extensive
experiments across multiple image datasets demonstrate that our compensation
strategy effectively mitigates error propagation, significantly enhancing
existing PTQ methods to achieve state-of-the-art(SOTA) performance on
low-precision diffusion models.

</details>


### [38] [VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine](https://arxiv.org/abs/2508.12108)
*Ziyang Zhang,Yang Yu,Xulei Yang,Si Yong Yeo*

Main category: cs.CV

TL;DR: VELVET-Med是一个针对有限3D医学影像数据（如CT扫描）的视觉语言预训练框架，通过创新的预训练目标和架构设计，在仅使用38,875个扫描-报告对的情况下实现了优异的跨模态理解和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 医学领域中3D影像（如CT扫描）与文本报告的配对数据收集困难且耗时，限制了视觉语言模型在医学下游任务中的性能表现。

Method: 提出VELVET-Med框架：1）将单模态自监督学习融入VLP框架；2）设计TriBERT语言编码器学习多层次文本语义；3）开发分层对比学习捕获多层次视觉-语言对应关系。

Result: 模型表现出强大的迁移能力，在3D分割、跨模态检索、视觉问答和报告生成等广泛下游任务中达到最先进性能。

Conclusion: 该方法证明了即使使用有限的3D医学数据，通过精心设计的预训练目标和架构，也能有效挖掘体积医学影像和临床叙述中的丰富空间和语义关系，提升编码器的泛化能力。

Abstract: Vision-and-language models (VLMs) have been increasingly explored in the
medical domain, particularly following the success of CLIP in general domain.
However, unlike the relatively straightforward pairing of 2D images and text,
curating large-scale paired data in the medical field for volumetric modalities
such as CT scans remains a challenging and time-intensive process. This
difficulty often limits the performance on downstream tasks. To address these
challenges, we propose a novel vision-language pre-training (VLP) framework,
termed as \textbf{VELVET-Med}, specifically designed for limited volumetric
data such as 3D CT and associated radiology reports. Instead of relying on
large-scale data collection, our method focuses on the development of effective
pre-training objectives and model architectures. The key contributions are: 1)
We incorporate uni-modal self-supervised learning into VLP framework, which are
often underexplored in the existing literature. 2) We propose a novel language
encoder, termed as \textbf{TriBERT}, for learning multi-level textual
semantics. 3) We devise the hierarchical contrastive learning to capture
multi-level vision-language correspondence. Using only 38,875 scan-report
pairs, our approach seeks to uncover rich spatial and semantic relationships
embedded in volumetric medical images and corresponding clinical narratives,
thereby enhancing the generalization ability of the learned encoders. The
resulting encoders exhibit strong transferability, achieving state-of-the-art
performance across a wide range of downstream tasks, including 3D segmentation,
cross-modal retrieval, visual question answering, and report generation.

</details>


### [39] [Simple o3: Towards Interleaved Vision-Language Reasoning](https://arxiv.org/abs/2508.12109)
*Ye Wang,Qianglong Chen,Zejun Li,Siyuan Wang,Shijie Guo,Zhirui Zhang,Zhongyu Wei*

Main category: cs.CV

TL;DR: Simple o3是一个端到端的多模态推理框架，通过集成动态视觉工具（裁剪、缩放、重用）和交错视觉语言推理，显著提升多模态大语言模型的链式思维能力


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉语言任务上表现优异，但其在多模态场景下的长链式思维能力尚未充分探索，需要模拟人类"图像思考"的迭代视觉转换和语言推理过程

Method: 提出Simple o3框架，通过监督微调集成动态工具交互；开发可扩展的数据合成流水线，基于"观察-推理-行动"循环生成高质量交错视觉语言推理链；构建TWI-Tools-146K开源数据集

Result: 在多个基准测试中表现出色，超越现有方法；通过引入额外视觉token和视觉工具重用，显著提升视觉推理和细粒度感知能力；基于精确视觉定位的图像裁剪使模型能有效关注关键实体

Conclusion: Simple o3建立了强大且计算成本合理的多模态推理范式，首次深入分析了不同交错推理策略对模型性能的影响，为多模态推理发展提供了重要见解

Abstract: Multimodal Large Language Models (MLLMs) have shown impressive performance on
vision-language tasks, but their long Chain-of-Thought (CoT) capabilities in
multimodal scenarios remain underexplored. Inspired by OpenAI's o3 model, which
emulates human-like ''thinking with image'' through iterative visual
transformations and linguistic reasoning, we propose Simple o3, an end-to-end
framework that integrates dynamic tool interactions (e.g., cropping, zooming,
and reusing) into interleaved vision-language reasoning via supervised
fine-tuning (SFT). Our approach features a scalable data synthesis pipeline
that generates high-quality interleaved vision-language reasoning chains via an
''observe-reason-act'' cycle, complete with executable visual operations and
rigorous verification, yielding the open-source TWI-Tools-146K dataset.
Experimental results demonstrate Simple o3's superior performance on diverse
benchmarks, outperforming existing approaches. By combining enhanced reasoning
capabilities, Simple o3 establishes a powerful yet computationally affordable
paradigm for advancing multimodal reasoning. Remarkably, we provide the first
in-depth analysis of different interleaved reasoning strategies, offering
insights into their impact on model performance. We found that by introducing
additional visual tokens for interleaved vision-language reasoning, reusing and
magnifying the original image significantly improves the model's visual
reasoning and fine-grained perception, while image cropping based on precise
visual grounding allows the model to effectively focus on key entities or
regions, further enhancing its capabilities.

</details>


### [40] [DualFit: A Two-Stage Virtual Try-On via Warping and Synthesis](https://arxiv.org/abs/2508.12131)
*Minh Tran,Johnmark Clements,Annie Prasanna,Tri Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: DualFit是一个两阶段的虚拟试穿混合管道，通过流场变形和保真度合成模块，在保持服装细节完整性的同时实现视觉无缝的试穿效果


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的免变形虚拟试穿方法虽然提升了感知质量，但往往无法保留服装的精细细节（如logo和印刷文字），这些细节对于品牌完整性和客户信任至关重要

Method: 采用两阶段方法：第一阶段使用学习到的流场将目标服装变形以对齐人物图像；第二阶段通过保真度保持试穿模块，将变形后的服装与保留的人体区域进行融合，使用保留区域输入和修复掩码来指导过程

Result: 广泛的定性结果表明，DualFit实现了视觉无缝的试穿效果，同时忠实地保持了高频服装细节，在重建准确性和感知真实性之间取得了有效平衡

Conclusion: DualFit通过混合方法成功解决了虚拟试穿中细节保持的问题，为在线时尚零售提供了更可靠的解决方案

Abstract: Virtual Try-On technology has garnered significant attention for its
potential to transform the online fashion retail experience by allowing users
to visualize how garments would look on them without physical trials. While
recent advances in diffusion-based warping-free methods have improved
perceptual quality, they often fail to preserve fine-grained garment details
such as logos and printed text elements that are critical for brand integrity
and customer trust. In this work, we propose DualFit, a hybrid VTON pipeline
that addresses this limitation by two-stage approach. In the first stage,
DualFit warps the target garment to align with the person image using a learned
flow field, ensuring high-fidelity preservation. In the second stage, a
fidelity-preserving try-on module synthesizes the final output by blending the
warped garment with preserved human regions. Particularly, to guide this
process, we introduce a preserved-region input and an inpainting mask, enabling
the model to retain key areas and regenerate only where necessary, particularly
around garment seams. Extensive qualitative results show that DualFit achieves
visually seamless try-on results while faithfully maintaining high-frequency
garment details, striking an effective balance between reconstruction accuracy
and perceptual realism.

</details>


### [41] [TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks](https://arxiv.org/abs/2508.12132)
*Amira Guesmi,Bassem Ouni,Muhammad Shafique*

Main category: cs.CV

TL;DR: TriQDef是一个三级量化感知防御框架，通过特征错位惩罚、梯度感知失谐惩罚和联合量化感知训练，有效降低QNN中基于补丁的对抗攻击在不同比特宽度间的可迁移性，攻击成功率降低40%以上。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络(QNNs)虽然在边缘设备中部署广泛，但对基于补丁的对抗攻击防御有限，现有方法要么过拟合固定量化设置，要么无法解决跨比特泛化漏洞。

Method: TriQDef包含三个组件：1)特征错位惩罚(FDP)通过惩罚中间表示的感知相似性来强制语义不一致；2)梯度感知失谐惩罚(GPDP)通过边缘IoU和HOG余弦度量最小化结构方向一致性来错位输入梯度；3)联合量化感知训练协议在多个量化级别上统一这些惩罚。

Result: 在CIFAR-10和ImageNet上的广泛实验表明，TriQDef在未见过的补丁和量化组合上降低攻击成功率(ASR)超过40%，同时保持高清洁准确率。

Conclusion: 研究强调了破坏语义和感知梯度对齐对于减轻QNN中补丁可迁移性的重要性，TriQDef框架有效解决了跨比特攻击转移问题。

Abstract: Quantized Neural Networks (QNNs) are increasingly deployed in edge and
resource-constrained environments due to their efficiency in computation and
memory usage. While shown to distort the gradient landscape and weaken
conventional pixel-level attacks, it provides limited robustness against
patch-based adversarial attacks-localized, high-saliency perturbations that
remain surprisingly transferable across bit-widths. Existing defenses either
overfit to fixed quantization settings or fail to address this cross-bit
generalization vulnerability. We introduce \textbf{TriQDef}, a tri-level
quantization-aware defense framework designed to disrupt the transferability of
patch-based adversarial attacks across QNNs. TriQDef consists of: (1) a Feature
Disalignment Penalty (FDP) that enforces semantic inconsistency by penalizing
perceptual similarity in intermediate representations; (2) a Gradient
Perceptual Dissonance Penalty (GPDP) that explicitly misaligns input gradients
across bit-widths by minimizing structural and directional agreement via Edge
IoU and HOG Cosine metrics; and (3) a Joint Quantization-Aware Training
Protocol that unifies these penalties within a shared-weight training scheme
across multiple quantization levels. Extensive experiments on CIFAR-10 and
ImageNet demonstrate that TriQDef reduces Attack Success Rates (ASR) by over
40\% on unseen patch and quantization combinations, while preserving high clean
accuracy. Our findings underscore the importance of disrupting both semantic
and perceptual gradient alignment to mitigate patch transferability in QNNs.

</details>


### [42] [Infusing fine-grained visual knowledge to Vision-Language Models](https://arxiv.org/abs/2508.12137)
*Nikolaos-Antonios Ypsilantis,Kaifeng Chen,André Araujo,Ondřej Chum*

Main category: cs.CV

TL;DR: 这篇论文提出了一种细粒度调优方法，在保持视觉-语言模型多模态知识的同时，提升细粒度领域适配性能。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型在细粒度开放集检索任务中表现不佳，传统细调方法容易导致灾难性遗忘，严重影响模型的通用能力。

Method: 受续续学习文献启发，系统分析并组合多种正则化技术，同时关注验证集设计和超参数调优的关键细节。

Result: 在细粒度和粗粒度图像-图像/图像-文本检索测试中均获得了坚固的结果，在不使用文本数据或原始文本编码器的情况下保持了视觉-文本对齐能力。

Conclusion: 该方法能够有效平衡细粒度领域适配和领域外知识保留，为视觉-语言模型的细调提供了可靠的解决方案。

Abstract: Large-scale contrastive pre-training produces powerful Vision-and-Language
Models (VLMs) capable of generating representations (embeddings) effective for
a wide variety of visual and multimodal tasks. However, these pretrained
embeddings remain suboptimal for fine-grained open-set visual retrieval, where
state-of-the-art results require fine-tuning the vision encoder using annotated
domain-specific samples. Naively performing such fine-tuning typically leads to
catastrophic forgetting, severely diminishing the model's general-purpose
visual and cross-modal capabilities.
  In this work, we propose a fine-tuning method explicitly designed to achieve
optimal balance between fine-grained domain adaptation and retention of the
pretrained VLM's broad multimodal knowledge. Drawing inspiration from continual
learning literature, we systematically analyze standard regularization
techniques aimed at knowledge retention and propose an efficient and effective
combination strategy. Additionally, we address the commonly overlooked yet
critical aspects of validation set design and hyperparameter tuning to ensure
reproducibility and robust generalization across datasets and pretrained
models. We extensively evaluate our method on both fine-grained and
coarse-grained image-image and image-text retrieval benchmarks. Our approach
consistently achieves strong results, notably retaining the visual-text
alignment without utilizing any text data or the original text encoder during
fine-tuning. Code and model checkpoints: https://github.com/nikosips/infusing .

</details>


### [43] [KP-INR: A Dual-Branch Implicit Neural Representation Model for Cardiac Cine MRI Reconstruction](https://arxiv.org/abs/2508.12147)
*Donghang Lyu,Marius Staring,Mariya Doneva,Hildo J. Lamb,Nicola Pezzotti*

Main category: cs.CV

TL;DR: 基于隐式神经表示的双分支KP-INR方法，通过结合坐标位置嵌入和局部多尺度k空间特征，在心脏动态磁共振成像重建中实现了更高质量的图像恢复。


<details>
  <summary>Details</summary>
Motivation: 当前INR方法主要关注坐标基于位置嵌入，忽视了目标点和其周围上下文的特征表示，导致在心脏动态MRI重建中性能有限。

Method: 提出KP-INR双分支网络：一支处理k空间坐标的位置嵌入，另一支学习局部多尺度k空间特征表示，通过跨分支交互近似k空间值。

Result: 在CMRxRecon2024数据集上验证了KP-INR的优势，在具有挑战性的卡尔坐标k空间数据上实现了更好的性能，超过基线模型。

Conclusion: KP-INR通过结合坐标位置信息和局部特征表示，为心脏动态MRI重建提供了更有效的解决方案，在该领域具有应用潜力。

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a non-invasive method for
assessing cardiac structure, function, and blood flow. Cine MRI extends this by
capturing heart motion, providing detailed insights into cardiac mechanics. To
reduce scan time and breath-hold discomfort, fast acquisition techniques have
been utilized at the cost of lowering image quality. Recently, Implicit Neural
Representation (INR) methods have shown promise in unsupervised reconstruction
by learning coordinate-to-value mappings from undersampled data, enabling
high-quality image recovery. However, current existing INR methods primarily
focus on using coordinate-based positional embeddings to learn the mapping,
while overlooking the feature representations of the target point and its
neighboring context. In this work, we propose KP-INR, a dual-branch INR method
operating in k-space for cardiac cine MRI reconstruction: one branch processes
the positional embedding of k-space coordinates, while the other learns from
local multi-scale k-space feature representations at those coordinates. By
enabling cross-branch interaction and approximating the target k-space values
from both branches, KP-INR can achieve strong performance on challenging
Cartesian k-space data. Experiments on the CMRxRecon2024 dataset confirms its
improved performance over baseline models and highlights its potential in this
field.

</details>


### [44] [Demystifying Foreground-Background Memorization in Diffusion Models](https://arxiv.org/abs/2508.12148)
*Jimmy Z. Di,Yiwei Lu,Yaoliang Yu,Gautam Kamath,Adam Dziedzic,Franziska Boenisch*

Main category: cs.CV

TL;DR: 提出了FB-Mem方法，通过分割技术量化扩散模型中局部记忆现象，发现记忆比之前认为的更普遍，现有缓解方法效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法只能识别完全相同的记忆复制，无法量化图像局部区域的记忆现象，也无法捕捉超越特定提示-图像对的复杂记忆模式。

Method: 提出基于分割的FB-Mem度量方法，对生成图像中的记忆区域进行分类和量化分析，并使用聚类方法进行更强的缓解。

Result: 发现记忆现象比之前理解的更普遍：单个提示生成可能与多个训练图像相关；现有缓解方法无法消除局部记忆，特别是在前景区域。

Conclusion: 建立了有效的扩散模型记忆测量框架，证明了当前缓解方法的不足，提出了基于聚类的更强缓解方法。

Abstract: Diffusion models (DMs) memorize training images and can reproduce
near-duplicates during generation. Current detection methods identify verbatim
memorization but fail to capture two critical aspects: quantifying partial
memorization occurring in small image regions, and memorization patterns beyond
specific prompt-image pairs. To address these limitations, we propose
Foreground Background Memorization (FB-Mem), a novel segmentation-based metric
that classifies and quantifies memorized regions within generated images. Our
method reveals that memorization is more pervasive than previously understood:
(1) individual generations from single prompts may be linked to clusters of
similar training images, revealing complex memorization patterns that extend
beyond one-to-one correspondences; and (2) existing model-level mitigation
methods, such as neuron deactivation and pruning, fail to eliminate local
memorization, which persists particularly in foreground regions. Our work
establishes an effective framework for measuring memorization in diffusion
models, demonstrates the inadequacy of current mitigation approaches, and
proposes a stronger mitigation method using a clustering approach.

</details>


### [45] [RealTalk: Realistic Emotion-Aware Lifelike Talking-Head Synthesis](https://arxiv.org/abs/2508.12163)
*Wenqing Wang,Yun Fu*

Main category: cs.CV

TL;DR: RealTalk是一个用于合成情感化说话头像的新框架，通过结合VAE、ResNet和NeRF技术，在情感准确性、可控性和身份保持方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前方法在唇形同步和图像质量方面表现出色，但在生成准确可控的情感表达同时保持主体身份方面存在不足，需要解决情感化说话头像合成的挑战。

Method: 使用变分自编码器(VAE)从音频生成3D面部标志点，通过ResNet基础的地标变形模型(LDM)与情感标签嵌入结合产生情感标志点，最后通过三平面注意力神经辐射场(NeRF)合成高度真实的情感说话头像。

Result: 大量实验证明RealTalk在情感准确性、可控性和身份保持方面优于现有方法。

Conclusion: RealTalk推动了社交智能AI系统的发展，为情感化说话头像合成提供了有效的解决方案。

Abstract: Emotion is a critical component of artificial social intelligence. However,
while current methods excel in lip synchronization and image quality, they
often fail to generate accurate and controllable emotional expressions while
preserving the subject's identity. To address this challenge, we introduce
RealTalk, a novel framework for synthesizing emotional talking heads with high
emotion accuracy, enhanced emotion controllability, and robust identity
preservation. RealTalk employs a variational autoencoder (VAE) to generate 3D
facial landmarks from driving audio, which are concatenated with emotion-label
embeddings using a ResNet-based landmark deformation model (LDM) to produce
emotional landmarks. These landmarks and facial blendshape coefficients jointly
condition a novel tri-plane attention Neural Radiance Field (NeRF) to
synthesize highly realistic emotional talking heads. Extensive experiments
demonstrate that RealTalk outperforms existing methods in emotion accuracy,
controllability, and identity preservation, advancing the development of
socially intelligent AI systems.

</details>


### [46] [Scalable RF Simulation in Generative 4D Worlds](https://arxiv.org/abs/2508.12176)
*Zhiwei Zheng,Dongyin Hu,Mingmin Zhao*

Main category: cs.CV

TL;DR: WaveVerse是一个基于提示的RF信号仿真框架，通过语言引导的4D世界生成器和相位相干射线追踪模拟器，从生成的室内场景和人体运动中模拟真实RF信号，解决了RF数据采集难题。


<details>
  <summary>Details</summary>
Motivation: RF传感作为隐私保护的室内感知替代方案面临高质量数据采集困难的问题，特别是在动态多样的室内环境中。

Method: 采用语言引导的4D世界生成器（包含状态感知因果变换器用于条件化人体运动生成）和相位相干射线追踪模拟器，实现准确的RF信号仿真。

Result: 实验证明该方法在条件化人体运动生成方面有效，相位相干性成功应用于波束成形和呼吸监测，在RF成像和人体活动识别任务中均取得性能提升。

Conclusion: WaveVerse首次实现了RF成像数据生成，在数据有限和数据充足场景下都能获得一致的性能增益，为RF感知提供了有效的仿真解决方案。

Abstract: Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving
alternative to vision-based methods for indoor perception tasks. However,
collecting high-quality RF data in dynamic and diverse indoor environments
remains a major challenge. To address this, we introduce WaveVerse, a
prompt-based, scalable framework that simulates realistic RF signals from
generated indoor scenes with human motions. WaveVerse introduces a
language-guided 4D world generator, which includes a state-aware causal
transformer for human motion generation conditioned on spatial constraints and
texts, and a phase-coherent ray tracing simulator that enables the simulation
of accurate and coherent RF signals. Experiments demonstrate the effectiveness
of our approach in conditioned human motion generation and highlight how phase
coherence is applied to beamforming and respiration monitoring. We further
present two case studies in ML-based high-resolution imaging and human activity
recognition, demonstrating that WaveVerse not only enables data generation for
RF imaging for the first time, but also consistently achieves performance gain
in both data-limited and data-adequate scenarios.

</details>


### [47] [Splat Feature Solver](https://arxiv.org/abs/2508.12216)
*Butian Xiong,Rong Liu,Kenneth Xu,Meida Chen,Andrew Feng*

Main category: cs.CV

TL;DR: 通过统一的稀疏线性逆问题形式化，提出了一种核心和特征无关的特征提升方法，能够高效地将多视角图像特征附着到体积表示中，并通过正则化策略提高语义保真性


<details>
  <summary>Details</summary>
Motivation: 解决多视角图像特征向3D表示附着过程中的不一致性问题，实现高质量的特征提升，以支持3D场景理解任务

Method: 将特征提升问题形式化为稀疏线性逆问题，可以高效地求解闭式解。采用两种正则化策略：Tikhonov指导通过软对角优势确保数值稳定性，后提升聚合通过特征聚类筛除噪声输入

Result: 在开放词汇的3D分割测试集上达到了最先进的性能，超过了基于训练、分组和经验前向的基线方法，且只需几分钟就能生成提升后的特征

Conclusion: 该方法为特征提升提供了一种统一、效率高且理论保证的解决方案，通过正则化技术有效处理多视角不一致性，在3D场景理解任务中表现突出

Abstract: Feature lifting has emerged as a crucial component in 3D scene understanding,
enabling the attachment of rich image feature descriptors (e.g., DINO, CLIP)
onto splat-based 3D representations. The core challenge lies in optimally
assigning rich general attributes to 3D primitives while addressing the
inconsistency issues from multi-view images. We present a unified, kernel- and
feature-agnostic formulation of the feature lifting problem as a sparse linear
inverse problem, which can be solved efficiently in closed form. Our approach
admits a provable upper bound on the global optimal error under convex losses
for delivering high quality lifted features. To address inconsistencies and
noise in multi-view observations, we introduce two complementary regularization
strategies to stabilize the solution and enhance semantic fidelity. Tikhonov
Guidance enforces numerical stability through soft diagonal dominance, while
Post-Lifting Aggregation filters noisy inputs via feature clustering. Extensive
experiments demonstrate that our approach achieves state-of-the-art performance
on open-vocabulary 3D segmentation benchmarks, outperforming training-based,
grouping-based, and heuristic-forward baselines while producing the lifted
features in minutes. Code is available at
\href{https://github.com/saliteta/splat-distiller.git}{\textbf{github}}. We
also have a \href{https://splat-distiller.pages.dev/}

</details>


### [48] [C2PSA-Enhanced YOLOv11 Architecture: A Novel Approach for Small Target Detection in Cotton Disease Diagnosis](https://arxiv.org/abs/2508.12219)
*Kaiyuan Wang,Jixing Liu,Xiaobo Cai*

Main category: cs.CV

TL;DR: 基于深度学习的YOLOv11优化方法，用于棉花病害检测，通过改进小目标检测、样本不平衡处理和数据增强，显著提升了检测精度和速度。


<details>
  <summary>Details</summary>
Motivation: 解决棉花病害检测中的三个关键挑战：早期斑点检测精度低（5mm²以下斑点漏检率35%）、田间条件下性能下降（准确率下降25%）、多病害场景错误率高（34.7%）。

Method: 提出C2PSA模块增强小目标特征提取；动态类别权重处理样本不平衡；改进的Mosaic-MixUp缩放数据增强方法。

Result: 在4,078张图像数据集上测试：mAP50达到0.820（提升8.0%）；mAP50-95达到0.705（提升10.5%）；推理速度158 FPS。

Conclusion: 开发的移动部署系统可实现农业应用中的实时病害监测和精准处理，为智能农业监测提供了有效解决方案。

Abstract: This study presents a deep learning-based optimization of YOLOv11 for cotton
disease detection, developing an intelligent monitoring system. Three key
challenges are addressed: (1) low precision in early spot detection (35%
leakage rate for sub-5mm2 spots), (2) performance degradation in field
conditions (25% accuracy drop), and (3) high error rates (34.7%) in
multi-disease scenarios. The proposed solutions include: C2PSA module for
enhanced small-target feature extraction; Dynamic category weighting to handle
sample imbalance; Improved data augmentation via Mosaic-MixUp scaling.
Experimental results on a 4,078-image dataset show: mAP50: 0.820 (+8.0%
improvement); mAP50-95: 0.705 (+10.5% improvement); Inference speed: 158 FPS.
The mobile-deployed system enables real-time disease monitoring and precision
treatment in agricultural applications.

</details>


### [49] [In vivo 3D ultrasound computed tomography of musculoskeletal tissues with generative neural physics](https://arxiv.org/abs/2508.12226)
*Zhijun Zeng,Youjia Zheng,Chang Su,Qianhang Wu,Hao Hu,Zeyuan Dong,Shan Gao,Yang Lv,Rui Tang,Ligang Cui,Zhiyong Hou,Weijun Lin,Zuoqiang Shi,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: 使用生成式神经物理框架结合神经网络和物理模拟，实现了高保真度的3D超声计算断层扫描，免除了传统光线重建方法忽略强散射的限制，在10分钟内完成肌肉骨骼组织的定量成像。


<details>
  <summary>Details</summary>
Motivation: 超声计算断层扫描(USCT)虽然无放射、分辨玉高，但在肌肉骨骼成像中受限于传统光线基重建方法忽略强散射效应，导致成像准确性不足。

Method: 提出了一种生成式神经物理框架，将生成式网络与物理信息神经模拟相结合。从仅数十张跨模态图像中学习超声波传播的简洁代理模型，融合了波动模拟的准确性与深度学习的效率和稳定性。

Result: 在合成数据和in vivo数据（乳腺、手臂、腿部）上，方法在10分钟内重建了组织参数的3D分布图，对肌肉和骨骼的生物力学性质保持敏感性，分辨玉可与MRI相比。能够生成超越反射模式图像的声学特性空间分布图。

Conclusion: 该方法克服了强散射情况下的计算瓶颈，推动了USCT技术向肌肉骨骼疾病常规临床评估的发展，为无放射高分辨玉的组织定量成像提供了新的解决方案。

Abstract: Ultrasound computed tomography (USCT) is a radiation-free, high-resolution
modality but remains limited for musculoskeletal imaging due to conventional
ray-based reconstructions that neglect strong scattering. We propose a
generative neural physics framework that couples generative networks with
physics-informed neural simulation for fast, high-fidelity 3D USCT. By learning
a compact surrogate of ultrasonic wave propagation from only dozens of
cross-modality images, our method merges the accuracy of wave modeling with the
efficiency and stability of deep learning. This enables accurate quantitative
imaging of in vivo musculoskeletal tissues, producing spatial maps of acoustic
properties beyond reflection-mode images. On synthetic and in vivo data
(breast, arm, leg), we reconstruct 3D maps of tissue parameters in under ten
minutes, with sensitivity to biomechanical properties in muscle and bone and
resolution comparable to MRI. By overcoming computational bottlenecks in
strongly scattering regimes, this approach advances USCT toward routine
clinical assessment of musculoskeletal disease.

</details>


### [50] [WXSOD: A Benchmark for Robust Salient Object Detection in Adverse Weather Conditions](https://arxiv.org/abs/2508.12250)
*Quan Chen,Xiong Yang,Rongfeng Lu,Qianyu Zhang,Yu Liu,Xiaofei Zhou,Bolun Zheng*

Main category: cs.CV

TL;DR: 这篇论文提出了一个新的天气噪声显著物体检测数据集WXSOD和基线模型WFANet，专门处理复杂天气条件下的显著物体检测问题。


<details>
  <summary>Details</summary>
Motivation: 现有的显著物体检测方法在复杂天气噪声环境下性能会受到影响，但缺乏有相应的数据集来进行研究。

Method: 构建了包含14,945张RGB图片的WXSOD数据集，包括合成和真实天气噪声图片。提出了两支流架构的WFANet模型，通过天气预测分支挖掘天气相关特征，再与显著检测分支融合。

Result: 在WXSOD数据集上与17种SOD方法进行了综合比较，WFANet实现了更优的性能。

Conclusion: 该研究为复杂天气条件下的显著物体检测提供了重要的数据集资源和有效的基线方法。

Abstract: Salient object detection (SOD) in complex environments remains a challenging
research topic. Most existing methods perform well in natural scenes with
negligible noise, and tend to leverage multi-modal information (e.g., depth and
infrared) to enhance accuracy. However, few studies are concerned with the
damage of weather noise on SOD performance due to the lack of dataset with
pixel-wise annotations. To bridge this gap, this paper introduces a novel
Weather-eXtended Salient Object Detection (WXSOD) dataset. It consists of
14,945 RGB images with diverse weather noise, along with the corresponding
ground truth annotations and weather labels. To verify algorithm
generalization, WXSOD contains two test sets, i.e., a synthesized test set and
a real test set. The former is generated by adding weather noise to clean
images, while the latter contains real-world weather noise. Based on WXSOD, we
propose an efficient baseline, termed Weather-aware Feature Aggregation Network
(WFANet), which adopts a fully supervised two-branch architecture.
Specifically, the weather prediction branch mines weather-related deep
features, while the saliency detection branch fuses semantic features extracted
from the backbone with weather features for SOD. Comprehensive comparisons
against 17 SOD methods shows that our WFANet achieves superior performance on
WXSOD. The code and benchmark results will be made publicly available at
https://github.com/C-water/WXSOD

</details>


### [51] [Superpixel-informed Continuous Low-Rank Tensor Representation for Multi-Dimensional Data Recovery](https://arxiv.org/abs/2508.12261)
*Zhizhou Wang,Ruijing Zheng,Zhenyu Wu,Jianli Wang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种超像素信息化的连续低秩张量表示框架(SCTR)，解决了传统低秩张量方法在实际应用中的两大局限性：整体低秩假设不合理和仅能处理网格数据的问题。


<details>
  <summary>Details</summary>
Motivation: 传统低秩张量表示方法假设整体数据都是低秩的，但实际场景中存在显著的空间变化；只能处理离散网格数据，灵活性和适用性受限。

Method: 使用超像素作为基本建模单元，因为语义一致区域具有更强的低秩特性；提出不对称低秩张量分解(ALTF)，通过共享神经网络与专门头部来参数化超像素特定因子矩阵，分离全局模式学习和局部适配。

Result: 在多谱图像、视频和色彩图像等多个标准数据集上，SCTR模型比现有低秩张量方法提高3-5 dB的PSNR收益。

Conclusion: SCTR框架能够连续灵活地建模多维数据，突破传统网格限制，通过超像素单元和不对称分解结构，高效地捐据跨超像素共性和内部变化，实现了表达力与紧凑性的平衡。

Abstract: Low-rank tensor representation (LRTR) has emerged as a powerful tool for
multi-dimensional data processing. However, classical LRTR-based methods face
two critical limitations: (1) they typically assume that the holistic data is
low-rank, this assumption is often violated in real-world scenarios with
significant spatial variations; and (2) they are constrained to discrete
meshgrid data, limiting their flexibility and applicability. To overcome these
limitations, we propose a Superpixel-informed Continuous low-rank Tensor
Representation (SCTR) framework, which enables continuous and flexible modeling
of multi-dimensional data beyond traditional grid-based constraints. Our
approach introduces two main innovations: First, motivated by the observation
that semantically coherent regions exhibit stronger low-rank characteristics
than holistic data, we employ superpixels as the basic modeling units. This
design not only encodes rich semantic information, but also enhances
adaptability to diverse forms of data streams. Second, we propose a novel
asymmetric low-rank tensor factorization (ALTF) where superpixel-specific
factor matrices are parameterized by a shared neural network with specialized
heads. By strategically separating global pattern learning from local
adaptation, this framework efficiently captures both cross-superpixel
commonalities and within-superpixel variations. This yields a representation
that is both highly expressive and compact, balancing model efficiency with
adaptability. Extensive experiments on several benchmark datasets demonstrate
that SCTR achieves 3-5 dB PSNR improvements over existing LRTR-based methods
across multispectral images, videos, and color images.

</details>


### [52] [Region-Level Context-Aware Multimodal Understanding](https://arxiv.org/abs/2508.12263)
*Hongliang Wei,Xianqi Zhang,Xingtao Wang,Xiaopeng Fan,Debin Zhao*

Main category: cs.CV

TL;DR: 本文提出了区域级上下文感知多模态理解(RCMU)的新概念，并通过Region-level Context-aware Visual Instruction Tuning (RCVIT)方法和RCMU数据集来增强MLLM模型的上下文感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型主要关注通用视觉理解，缺乏将对象的视觉内容与文本上下文相结合的能力，影响了更深入的多模态理解。

Method: 提出RCVIT方法，将对象信息整合到模型输入中，利用边框坐标关联对象的视觉内容和文本信息；构建了RCMU大规模数据集和RC&P-Bench评测标准。

Result: 基于Qwen2-VL模型训练的RC-Qwen2-VL模型在多个RCMU任务上表现出艰涉，并成功应用于多模态RAG和个性化对话场景。

Conclusion: 该研究有效提升了MLLM模型的区域级上下文感知能力，为更深入的多模态理解提供了新的解决方案。

Abstract: Despite significant progress, existing research on Multimodal Large Language
Models (MLLMs) mainly focuses on general visual understanding, overlooking the
ability to integrate textual context associated with objects for a more
context-aware multimodal understanding -- an ability we refer to as
Region-level Context-aware Multimodal Understanding (RCMU). To address this
limitation, we first formulate the RCMU task, which requires models to respond
to user instructions by integrating both image content and textual information
of regions or objects. To equip MLLMs with RCMU capabilities, we propose
Region-level Context-aware Visual Instruction Tuning (RCVIT), which
incorporates object information into the model input and enables the model to
utilize bounding box coordinates to effectively associate objects' visual
content with their textual information. To address the lack of datasets, we
introduce the RCMU dataset, a large-scale visual instruction tuning dataset
that covers multiple RCMU tasks. We also propose RC\&P-Bench, a comprehensive
benchmark that can evaluate the performance of MLLMs in RCMU and multimodal
personalized understanding tasks. Additionally, we propose a reference-free
evaluation metric to perform a comprehensive and fine-grained evaluation of the
region-level context-aware image descriptions. By performing RCVIT on Qwen2-VL
models with the RCMU dataset, we developed RC-Qwen2-VL models. Experimental
results indicate that RC-Qwen2-VL models not only achieve outstanding
performance on multiple RCMU tasks but also demonstrate successful applications
in multimodal RAG and personalized conversation. Our data, model and benchmark
are available at https://github.com/hongliang-wei/RC-MLLM

</details>


### [53] [SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration](https://arxiv.org/abs/2508.12271)
*Ronghua Xu,Jin Xie,Jing Nie,Jiale Cao,Yanwei Pang*

Main category: cs.CV

TL;DR: 这篇论文提出了SNNSIR，一种全骨体龄的疯粉神经网络方案，用于立体图像恢复，通过疯粉驱动设计实现低功耗和硬件友好的计算。


<details>
  <summary>Details</summary>
Motivation: 现有的混合SNN-ANN模型仍依赖浮点数矩阵除法或指数运算，这与SNN的二进制和事件驱动性质不相容。SNNSIR目标实现全骨体疯粉驱动的低功耗硬件友好计算。

Method: 设计了轻量级疯粉殊差基础块(SRBB)增强信息流，疯粉立体卷积调制模块(SSCM)通过元素乘法实现简化非线性，以及疯粉立体交叉注意力模块(SSCA)促进双向特征交互。

Result: 在多种立体图像恢复任务(去雨纹、去雨滴、低光增强、超分辨率)中展示了竞争性的恢复性能，同时显著降低计算开销。

Conclusion: 该模型显示了在实时、低功耗立体视觉应用中的潜力，为疯粉神经网络在计算密集型任务中的应用提供了有效解决方案。

Abstract: Spiking Neural Networks (SNNs), characterized by discrete binary activations,
offer high computational efficiency and low energy consumption, making them
well-suited for computation-intensive tasks such as stereo image restoration.
In this work, we propose SNNSIR, a simple yet effective Spiking Neural Network
for Stereo Image Restoration, specifically designed under the spike-driven
paradigm where neurons transmit information through sparse, event-based binary
spikes. In contrast to existing hybrid SNN-ANN models that still rely on
operations such as floating-point matrix division or exponentiation, which are
incompatible with the binary and event-driven nature of SNNs, our proposed
SNNSIR adopts a fully spike-driven architecture to achieve low-power and
hardware-friendly computation. To address the expressiveness limitations of
binary spiking neurons, we first introduce a lightweight Spike Residual Basic
Block (SRBB) to enhance information flow via spike-compatible residual
learning. Building on this, the Spike Stereo Convolutional Modulation (SSCM)
module introduces simplified nonlinearity through element-wise multiplication
and highlights noise-sensitive regions via cross-view-aware modulation.
Complementing this, the Spike Stereo Cross-Attention (SSCA) module further
improves stereo correspondence by enabling efficient bidirectional feature
interaction across views within a spike-compatible framework. Extensive
experiments on diverse stereo image restoration tasks, including rain streak
removal, raindrop removal, low-light enhancement, and super-resolution
demonstrate that our model achieves competitive restoration performance while
significantly reducing computational overhead. These results highlight the
potential for real-time, low-power stereo vision applications. The code will be
available after the article is accepted.

</details>


### [54] [TSLA: A Task-Specific Learning Adaptation for Semantic Segmentation on Autonomous Vehicles Platform](https://arxiv.org/abs/2508.12279)
*Jun Liu,Zhenglun Kong,Pu Zhao,Weihao Zeng,Hao Tang,Xuan Shen,Changdi Yang,Wenbin Zhang,Geng Yuan,Wei Niu,Xue Lin,Yanzhi Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种针对自动驾驶硬件平台的动态可适应语义分割网络，通过三层控制机制和贝叶斯优化来实现计算资源的高效分配和任务特定优化。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶平台面临多样化的驾驶场景，每个场景都有不同的硬件资源和精度要求。由于嵌入式设备的计算限制，在目标平台（如NVIDIA DRIVE PX 2）上部署时需要考虑计算成本。

Method: 采用三层控制机制（宽度乘数、分类器深度、分类器核）实现动态适应性，结合贝叶斯优化和代理模型在有限计算预算下高效探索超参数空间。

Result: 实现了任务特定学习适应（TSLA），能够根据不同的自动驾驶任务生成定制化的配置方案，最大化计算能力和模型精度。

Conclusion: 该方法能够有效优化硬件利用率，为自动驾驶场景提供计算效率和性能之间的最佳平衡，满足特定场景和任务的独特计算复杂度和精度需求。

Abstract: Autonomous driving platforms encounter diverse driving scenarios, each with
varying hardware resources and precision requirements. Given the computational
limitations of embedded devices, it is crucial to consider computing costs when
deploying on target platforms like the NVIDIA\textsuperscript{\textregistered}
DRIVE PX 2. Our objective is to customize the semantic segmentation network
according to the computing power and specific scenarios of autonomous driving
hardware. We implement dynamic adaptability through a three-tier control
mechanism -- width multiplier, classifier depth, and classifier kernel --
allowing fine-grained control over model components based on hardware
constraints and task requirements. This adaptability facilitates broad model
scaling, targeted refinement of the final layers, and scenario-specific
optimization of kernel sizes, leading to improved resource allocation and
performance.
  Additionally, we leverage Bayesian Optimization with surrogate modeling to
efficiently explore hyperparameter spaces under tight computational budgets.
Our approach addresses scenario-specific and task-specific requirements through
automatic parameter search, accommodating the unique computational complexity
and accuracy needs of autonomous driving. It scales its Multiply-Accumulate
Operations (MACs) for Task-Specific Learning Adaptation (TSLA), resulting in
alternative configurations tailored to diverse self-driving tasks. These TSLA
customizations maximize computational capacity and model accuracy, optimizing
hardware utilization.

</details>


### [55] [CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval](https://arxiv.org/abs/2508.12290)
*Chor Boon Tan,Conghui Hu,Gim Hee Lee*

Main category: cs.CV

TL;DR: 本文提出CLAIR方法，通过CLIP置信度评分优化噪声伪标签，设计多重对比损失和跨域映射函数，在弱监督零样本跨域图像检索任务中取得优异性能


<details>
  <summary>Details</summary>
Motivation: 大型基础模型能轻松生成大量未标注数据的伪标签，使得无监督零样本跨域图像检索变得不那么相关，因此转向研究使用CLIP等模型生成噪声伪标签的弱监督方法

Method: 1) 使用CLIP文本和图像特征相似度计算置信度来优化噪声伪标签 2) 设计实例间和簇间对比损失编码到类别感知潜在空间 3) 设计域间对比损失缓解域差异 4) 学习闭式跨域映射函数，仅使用CLIP文本嵌入将图像特征从一个域投影到另一个域 5) 引入可学习提示增强零样本泛化能力

Result: 在TUBerlin、Sketchy、Quickdraw和DomainNet零样本数据集上的大量实验表明，CLAIR相比现有最先进方法始终表现出优越性能

Conclusion: CLAIR方法有效解决了弱监督零样本跨域图像检索问题，通过噪声标签优化、多重对比学习和跨域映射等技术显著提升了检索性能

Abstract: The recent growth of large foundation models that can easily generate
pseudo-labels for huge quantity of unlabeled data makes unsupervised Zero-Shot
Cross-Domain Image Retrieval (UZS-CDIR) less relevant. In this paper, we
therefore turn our attention to weakly supervised ZS-CDIR (WSZS-CDIR) with
noisy pseudo labels generated by large foundation models such as CLIP. To this
end, we propose CLAIR to refine the noisy pseudo-labels with a confidence score
from the similarity between the CLIP text and image features. Furthermore, we
design inter-instance and inter-cluster contrastive losses to encode images
into a class-aware latent space, and an inter-domain contrastive loss to
alleviate domain discrepancies. We also learn a novel cross-domain mapping
function in closed-form, using only CLIP text embeddings to project image
features from one domain to another, thereby further aligning the image
features for retrieval. Finally, we enhance the zero-shot generalization
ability of our CLAIR to handle novel categories by introducing an extra set of
learnable prompts. Extensive experiments are carried out using TUBerlin,
Sketchy, Quickdraw, and DomainNet zero-shot datasets, where our CLAIR
consistently shows superior performance compared to existing state-of-the-art
methods.

</details>


### [56] [Improving Densification in 3D Gaussian Splatting for High-Fidelity Rendering](https://arxiv.org/abs/2508.12313)
*Xiaobin Deng,Changyu Diao,Min Li,Ruohan Yu,Duanqing Xu*

Main category: cs.CV

TL;DR: 改善3D高斯拟合的密化策略，通过边缘感知分数、长轴分割和抗过拟合技术提升渲染质量，在不增加计算开销的情况下实现更高的重建质量。


<details>
  <summary>Details</summary>
Motivation: 3D高斯拟合的密化策略导致重建质量不佳，需要从密化时机、密化方式和抗过拟合三个方面进行全面改进。

Method: 提出边缘感知分数选择分割候选高斯，长轴分割策略减少几何失真，以及恢复感知剪枝、多步更新和增长控制等抗过拟合技术。

Result: 方法在不增加训练或推理开销的情况下，使用更少的高斯实现了独创的渲染保真度。

Conclusion: 该研究通过系统性的密化流程改进，显著提升了3D高斯拟合的重建质量和效率，为实时渲染领域提供了有效的解决方案。

Abstract: Although 3D Gaussian Splatting (3DGS) has achieved impressive performance in
real-time rendering, its densification strategy often results in suboptimal
reconstruction quality. In this work, we present a comprehensive improvement to
the densification pipeline of 3DGS from three perspectives: when to densify,
how to densify, and how to mitigate overfitting. Specifically, we propose an
Edge-Aware Score to effectively select candidate Gaussians for splitting. We
further introduce a Long-Axis Split strategy that reduces geometric distortions
introduced by clone and split operations. To address overfitting, we design a
set of techniques, including Recovery-Aware Pruning, Multi-step Update, and
Growth Control. Our method enhances rendering fidelity without introducing
additional training or inference overhead, achieving state-of-the-art
performance with fewer Gaussians.

</details>


### [57] [Neural Cellular Automata for Weakly Supervised Segmentation of White Blood Cells](https://arxiv.org/abs/2508.12322)
*Michael Deutges,Chen Yang,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: 基于神经细胞自动机(NCA)的弱监督分割方法，无需分割标签即可从分类特征图中提取白血球分割掩码，在三个数据集上表现超过现有弱监督方法。


<details>
  <summary>Details</summary>
Motivation: 血液染片图像中白血球检测和分割对医学诊断至关重要，但需要大量标签数据训练模型，而标注费时费钱。

Method: 提出NCA-WSS方法，利用神经细胞自动机在分类过程中生成的特征图，无需重新训练即可提取分割掩码。

Result: 在三个白血球显微镜数据集上评估，NCA-WSS方法显著超过现有的弱监督分割方法。

Conclusion: 该方法呈现了NCA在弱监督框架下同时进行分类和分割的潜力，为医学图像分析提供了可扩展和高效的解决方案。

Abstract: The detection and segmentation of white blood cells in blood smear images is
a key step in medical diagnostics, supporting various downstream tasks such as
automated blood cell counting, morphological analysis, cell classification, and
disease diagnosis and monitoring. Training robust and accurate models requires
large amounts of labeled data, which is both time-consuming and expensive to
acquire. In this work, we propose a novel approach for weakly supervised
segmentation using neural cellular automata (NCA-WSS). By leveraging the
feature maps generated by NCA during classification, we can extract
segmentation masks without the need for retraining with segmentation labels. We
evaluate our method on three white blood cell microscopy datasets and
demonstrate that NCA-WSS significantly outperforms existing weakly supervised
approaches. Our work illustrates the potential of NCA for both classification
and segmentation in a weakly supervised framework, providing a scalable and
efficient solution for medical image analysis.

</details>


### [58] [Attention Pooling Enhances NCA-based Classification of Microscopy Images](https://arxiv.org/abs/2508.12324)
*Chen Yang,Michael Deutges,Jingsong Liu,Han Li,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: 通过将注意力池化机制与神经元胞自动机(NCA)结合，提升显微镜图像分类性能，在保持参数效率的同时显著超越现有NCA方法


<details>
  <summary>Details</summary>
Motivation: 神经细胞自动机(NCA)为图像分类提供了鲁棒且可解释的方法，但在性能上与更复杂的架构存在差距，需要提升特征提取能力和分类准确性

Method: 集成注意力池化机制到NCA中，通过关注信息最丰富的区域来精炼特征提取，在八个不同的显微镜图像数据集上进行评估

Result: 显著超越现有NCA方法，与传统轻量级卷积神经网络和视觉变换器架构相比，在保持显著更低参数数量的同时展现出改进的性能

Conclusion: 基于NCA的模型具有作为可解释图像分类替代方案的潜力，注意力池化的集成有效提升了性能同时保持了参数效率

Abstract: Neural Cellular Automata (NCA) offer a robust and interpretable approach to
image classification, making them a promising choice for microscopy image
analysis. However, a performance gap remains between NCA and larger, more
complex architectures. We address this challenge by integrating attention
pooling with NCA to enhance feature extraction and improve classification
accuracy. The attention pooling mechanism refines the focus on the most
informative regions, leading to more accurate predictions. We evaluate our
method on eight diverse microscopy image datasets and demonstrate that our
approach significantly outperforms existing NCA methods while remaining
parameter-efficient and explainable. Furthermore, we compare our method with
traditional lightweight convolutional neural network and vision transformer
architectures, showing improved performance while maintaining a significantly
lower parameter count. Our results highlight the potential of NCA-based models
an alternative for explainable image classification.

</details>


### [59] [DoppDrive: Doppler-Driven Temporal Aggregation for Improved Radar Object Detection](https://arxiv.org/abs/2508.12330)
*Yuval Haitman,Oded Bialer*

Main category: cs.CV

TL;DR: DoppDrive是一种基于多普勒效应的雷达点云时间聚合方法，通过径向移动消除动态物体散射，提高点云密度，从而显著提升各种检测器的目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 雷达在自动驾驶中具有长距离检测优势，但远距离点云稀疏问题严重。现有时间聚合方法会引入动态物体散射，降低检测性能。

Method: 提出DoppDrive方法：1）根据多普勒动态分量径向移动历史帧点云以消除径向散射；2）基于多普勒和角度为每个点分配唯一聚合时长以减少切向散射

Result: 该方法显著提高了各种检测器和数据集上的目标检测性能，且作为点云密度增强步骤可与任何检测器兼容

Conclusion: DoppDrive通过多普勒驱动的智能时间聚合，有效解决了雷达点云稀疏和动态散射问题，为雷达目标检测提供了有效的预处理方案

Abstract: Radar-based object detection is essential for autonomous driving due to
radar's long detection range. However, the sparsity of radar point clouds,
especially at long range, poses challenges for accurate detection. Existing
methods increase point density through temporal aggregation with ego-motion
compensation, but this approach introduces scatter from dynamic objects,
degrading detection performance. We propose DoppDrive, a novel Doppler-Driven
temporal aggregation method that enhances radar point cloud density while
minimizing scatter. Points from previous frames are shifted radially according
to their dynamic Doppler component to eliminate radial scatter, with each point
assigned a unique aggregation duration based on its Doppler and angle to
minimize tangential scatter. DoppDrive is a point cloud density enhancement
step applied before detection, compatible with any detector, and we demonstrate
that it significantly improves object detection performance across various
detectors and datasets.

</details>


### [60] [Geometry-Aware Video Inpainting for Joint Headset Occlusion Removal and Face Reconstruction in Social XR](https://arxiv.org/abs/2508.12336)
*Fatemeh Ghorbani Lohesara,Karen Eguiazarian,Sebastian Knorr*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于深度学习的几何感知框架，能够从单视点RGB视频中同时去除头成显示器遮挡并重建完整的3D面部几何，为社交XR应用提供更好的面部表情重现能力。


<details>
  <summary>Details</summary>
Motivation: 头成显示器(HMDs)遮挡用户面部上部分，影响视频录制和社交XR应用中面部表情传达，需要解决遮挡移除和面部重建问题。

Method: 集成GAN基础的视频修复网络，通过密集面部关键点和单张无遮挡参照帧指导，恢复缺失面部区域保持身份识别。继而使用SynergyNet基础模块从修复后帧回归3D形态模型参数，实现准确的3D面部重建。整个流程中融入密集关键点优化提升质量。

Result: 实验结果显示该框架能成功去除HMD遮挡，保持面部身份识别和现实感，生成逐帧实际的3D面部几何输出。在不同关键点密度下都保持了稳健性。

Conclusion: 该研究提供了一种有效的解决方案，能够在单视点RGB视频中同时实现HMD遮挡移除和高质量3D面部重建，为社交XR应用的沉浸式体验提供了重要技术支撑。

Abstract: Head-mounted displays (HMDs) are essential for experiencing extended reality
(XR) environments and observing virtual content. However, they obscure the
upper part of the user's face, complicating external video recording and
significantly impacting social XR applications such as teleconferencing, where
facial expressions and eye gaze details are crucial for creating an immersive
experience. This study introduces a geometry-aware learning-based framework to
jointly remove HMD occlusions and reconstruct complete 3D facial geometry from
RGB frames captured from a single viewpoint. The method integrates a GAN-based
video inpainting network, guided by dense facial landmarks and a single
occlusion-free reference frame, to restore missing facial regions while
preserving identity. Subsequently, a SynergyNet-based module regresses 3D
Morphable Model (3DMM) parameters from the inpainted frames, enabling accurate
3D face reconstruction. Dense landmark optimization is incorporated throughout
the pipeline to improve both the inpainting quality and the fidelity of the
recovered geometry. Experimental results demonstrate that the proposed
framework can successfully remove HMDs from RGB facial videos while maintaining
facial identity and realism, producing photorealistic 3D face geometry outputs.
Ablation studies further show that the framework remains robust across
different landmark densities, with only minor quality degradation under sparse
landmark configurations.

</details>


### [61] [Semantic Discrepancy-aware Detector for Image Forgery Identification](https://arxiv.org/abs/2508.12341)
*Ziye Wang,Minghang Yu,Chunyan Xu,Zhen Cui*

Main category: cs.CV

TL;DR: 提出SDD检测器，通过重建学习在细粒度视觉层面对齐伪造和语义概念空间，利用预训练视觉语言模型的概念知识来提升伪造图像检测性能


<details>
  <summary>Details</summary>
Motivation: 随着图像生成技术的快速发展，需要强大的伪造检测来确保数字媒体的可信度。现有方法中伪造空间和语义概念空间的不对齐阻碍了检测性能

Method: 提出语义差异感知检测器(SDD)：1)语义标记采样模块缓解空间偏移；2)概念级伪造差异学习模块加强视觉语义概念与伪造痕迹的交互；3)低级伪造特征增强器整合学习到的概念级差异

Result: 在两个标准图像伪造数据集上的实验证明SDD的有效性，相比现有方法取得了更优越的结果

Conclusion: SDD通过重建学习有效对齐伪造和语义概念空间，显著提升了伪造图像检测性能，为解决空间不对齐问题提供了有效方案

Abstract: With the rapid advancement of image generation techniques, robust forgery
detection has become increasingly imperative to ensure the trustworthiness of
digital media. Recent research indicates that the learned semantic concepts of
pre-trained models are critical for identifying fake images. However, the
misalignment between the forgery and semantic concept spaces hinders the
model's forgery detection performance. To address this problem, we propose a
novel Semantic Discrepancy-aware Detector (SDD) that leverages reconstruction
learning to align the two spaces at a fine-grained visual level. By exploiting
the conceptual knowledge embedded in the pre-trained vision language model, we
specifically design a semantic token sampling module to mitigate the space
shifts caused by features irrelevant to both forgery traces and semantic
concepts. A concept-level forgery discrepancy learning module, built upon a
visual reconstruction paradigm, is proposed to strengthen the interaction
between visual semantic concepts and forgery traces, effectively capturing
discrepancies under the concepts' guidance. Finally, the low-level forgery
feature enhancemer integrates the learned concept level forgery discrepancies
to minimize redundant forgery information. Experiments conducted on two
standard image forgery datasets demonstrate the efficacy of the proposed SDD,
which achieves superior results compared to existing methods. The code is
available at https://github.com/wzy1111111/SSD.

</details>


### [62] [AquaFeat: A Features-Based Image Enhancement Model for Underwater Object Detection](https://arxiv.org/abs/2508.12343)
*Emanuel C. Silva,Tatiana T. Schein,Stephanie L. Brião,Guilherme L. M. Costa,Felipe G. Oliveira,Gustavo P. Almeida,Eduardo L. Silva,Sam S. Devincenzi,Karina S. Machado,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: AquaFeat是一个即插即用的任务驱动特征增强模块，专门针对水下目标检测任务优化，通过多尺度特征增强网络和端到端训练，显著提升检测精度同时保持实时处理速度。


<details>
  <summary>Details</summary>
Motivation: 水下环境的严重图像退化会影响目标检测模型的性能，传统图像增强方法通常没有针对下游检测任务进行优化。

Method: 提出AquaFeat模块，集成多尺度特征增强网络，与检测器损失函数进行端到端训练，确保增强过程明确指导以优化与检测任务最相关的特征。

Result: 在YOLOv8m上集成AquaFeat，在挑战性水下数据集上达到SOTA性能：Precision 0.877、Recall 0.624、mAP@0.5 0.677、mAP@[0.5:0.95] 0.421，处理速度46.5 FPS。

Conclusion: AquaFeat为海洋生态系统监测和基础设施检查等实际应用提供了有效且计算效率高的解决方案，在保持实用处理速度的同时提供显著的精度提升。

Abstract: The severe image degradation in underwater environments impairs object
detection models, as traditional image enhancement methods are often not
optimized for such downstream tasks. To address this, we propose AquaFeat, a
novel, plug-and-play module that performs task-driven feature enhancement. Our
approach integrates a multi-scale feature enhancement network trained
end-to-end with the detector's loss function, ensuring the enhancement process
is explicitly guided to refine features most relevant to the detection task.
When integrated with YOLOv8m on challenging underwater datasets, AquaFeat
achieves state-of-the-art Precision (0.877) and Recall (0.624), along with
competitive mAP scores (mAP@0.5 of 0.677 and mAP@[0.5:0.95] of 0.421). By
delivering these accuracy gains while maintaining a practical processing speed
of 46.5 FPS, our model provides an effective and computationally efficient
solution for real-world applications, such as marine ecosystem monitoring and
infrastructure inspection.

</details>


### [63] [MBMamba: When Memory Buffer Meets Mamba for Structure-Aware Image Deblurring](https://arxiv.org/abs/2508.12346)
*Hu Gao,Depeng Dang*

Main category: cs.CV

TL;DR: 提出MBMamba网络，通过内存缓冲机制和Ising受需正则化损失，解决Mamba在图像去橡糖中的局部像素遗忘和频道冗余问题，在保持原有架构的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: Mamba架构在图像去橡糖中存在局部像素遗忘和频道冗余问题，而现有改进方法通常会增加计算复杂度影响实时性能。

Method: 设计内存缓冲机制保留历史信息以支持相邻特征间的关联建模，并引入Ising受需正则化损失模拟物理系统能量最小化过程以维持图像结构一致性。

Result: 在广泛使用的测试集上超越了当前最先进的方法。

Conclusion: MBMamba无需改变原有Mamba架构即能有效解决局部信息遗忘问题，同时保持了计算效率。

Abstract: The Mamba architecture has emerged as a promising alternative to CNNs and
Transformers for image deblurring. However, its flatten-and-scan strategy often
results in local pixel forgetting and channel redundancy, limiting its ability
to effectively aggregate 2D spatial information. Although existing methods
mitigate this by modifying the scan strategy or incorporating local feature
modules, it increase computational complexity and hinder real-time performance.
In this paper, we propose a structure-aware image deblurring network without
changing the original Mamba architecture. Specifically, we design a memory
buffer mechanism to preserve historical information for later fusion, enabling
reliable modeling of relevance between adjacent features. Additionally, we
introduce an Ising-inspired regularization loss that simulates the energy
minimization of the physical system's "mutual attraction" between pixels,
helping to maintain image structure and coherence. Building on this, we develop
MBMamba. Experimental results show that our method outperforms state-of-the-art
approaches on widely used benchmarks.

</details>


### [64] [EgoLoc: A Generalizable Solution for Temporal Interaction Localization in Egocentric Videos](https://arxiv.org/abs/2508.12349)
*Junyi Ma,Erhang Zhang,Yin-Dong Zheng,Yuchen Xie,Yixuan Zhou,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出了EgoLoc，一种零样本方法，用于在自我中心视频中定位手-物体接触和分离的时间戳，解决了现有方法依赖类别标注和物体掩码的问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注交互动作的行为范式（如何交互），但对手与目标物体接触和分离的关键时刻（何时交互）这一更精细的问题研究不足，这对于混合现实的沉浸式交互体验和机器人运动规划至关重要。

Method: 提出EgoLoc方法，引入手动力学引导采样生成高质量视觉提示，利用视觉语言模型识别接触/分离属性、定位特定时间戳，并提供闭环反馈进行进一步细化。该方法无需物体掩码和动词-名词分类法。

Result: 在公共数据集和新基准上的综合实验表明，EgoLoc能够实现合理的时序交互定位，并能有效促进自我中心视觉和机器人操作任务中的多个下游应用。

Conclusion: EgoLoc提供了一种无需类别标注的零样本解决方案，能够精确地定位手-物体交互的关键时刻，具有很好的泛化性和实用性。

Abstract: Analyzing hand-object interaction in egocentric vision facilitates VR/AR
applications and human-robot policy transfer. Existing research has mostly
focused on modeling the behavior paradigm of interactive actions (i.e., ``how
to interact''). However, the more challenging and fine-grained problem of
capturing the critical moments of contact and separation between the hand and
the target object (i.e., ``when to interact'') is still underexplored, which is
crucial for immersive interactive experiences in mixed reality and robotic
motion planning. Therefore, we formulate this problem as temporal interaction
localization (TIL). Some recent works extract semantic masks as TIL references,
but suffer from inaccurate object grounding and cluttered scenarios. Although
current temporal action localization (TAL) methods perform well in detecting
verb-noun action segments, they rely on category annotations during training
and exhibit limited precision in localizing hand-object contact/separation
moments. To address these issues, we propose a novel zero-shot approach dubbed
EgoLoc to localize hand-object contact and separation timestamps in egocentric
videos. EgoLoc introduces hand-dynamics-guided sampling to generate
high-quality visual prompts. It exploits the vision-language model to identify
contact/separation attributes, localize specific timestamps, and provide
closed-loop feedback for further refinement. EgoLoc eliminates the need for
object masks and verb-noun taxonomies, leading to generalizable zero-shot
implementation. Comprehensive experiments on the public dataset and our novel
benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric
videos. It is also validated to effectively facilitate multiple downstream
applications in egocentric vision and robotic manipulation tasks. Code and
relevant data will be released at https://github.com/IRMVLab/EgoLoc.

</details>


### [65] [Synthetic Data is Sufficient for Zero-Shot Visual Generalization from Offline Data](https://arxiv.org/abs/2508.12356)
*Ahmet H. Güzel,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.CV

TL;DR: 通过两步数据增帽方法（元数据增帽+潜空间激光生成）来提升视觉离线强化学习的绝震分析性能，无需改变现有算法


<details>
  <summary>Details</summary>
Motivation: 视觉离线RL中的数据不充足和噪音问题导致攻策法太过拟合且无法良好法到未见环境

Method: 首先对原始离线数据进行增帽增加多样性，然后使用激光模型在潜空间生成合成训练数据

Result: 在Visual D4RL和Procgen数据集上显著提升了绝震分析性能，减小了法差距离同时保持计算效率

Conclusion: 该方法为使用合成数据训练更具法性的智能体提供了有前景的方向

Abstract: Offline reinforcement learning (RL) offers a promising framework for training
agents using pre-collected datasets without the need for further environment
interaction. However, policies trained on offline data often struggle to
generalise due to limited exposure to diverse states. The complexity of visual
data introduces additional challenges such as noise, distractions, and spurious
correlations, which can misguide the policy and increase the risk of
overfitting if the training data is not sufficiently diverse. Indeed, this
makes it challenging to leverage vision-based offline data in training robust
agents that can generalize to unseen environments. To solve this problem, we
propose a simple approach generating additional synthetic training data. We
propose a two-step process, first augmenting the originally collected offline
data to improve zero-shot generalization by introducing diversity, then using a
diffusion model to generate additional data in latent space. We test our method
across both continuous action spaces (Visual D4RL) and discrete action spaces
(Procgen), demonstrating that it significantly improves generalization without
requiring any algorithmic changes to existing model-free offline RL methods. We
show that our method not only increases the diversity of the training data but
also significantly reduces the generalization gap at test time while
maintaining computational efficiency. We believe this approach could fuel
additional progress in generating synthetic data to train more general agents
in the future.

</details>


### [66] [IPGPhormer: Interpretable Pathology Graph-Transformer for Survival Analysis](https://arxiv.org/abs/2508.12381)
*Guo Tang,Songhan Jiang,Jinpeng Lu,Linghan Cai,Yongbing Zhang*

Main category: cs.CV

TL;DR: 提出IPGPhormer框架，通过图-Transformer结构同时捕获肿瘤微环境的长距离空间关系和局部上下文依赖，在四个公开数据集上实现了预测准确性和可解释性的提升


<details>
  <summary>Details</summary>
Motivation: 现有生存分析方法难以平衡长距离空间关系建模与局部上下文依赖，且缺乏内在可解释性，限制了临床实用性

Method: Interpretable Pathology Graph-Transformer (IPGPhormer)框架，无需后处理手动标注即可在组织和细胞层面提供可解释性

Result: 在四个公开基准数据集上，IPGPhormer在预测准确性和可解释性方面均优于最先进方法

Conclusion: IPGPhormer为癌症预后评估提供了有前景的工具，为病理学中更可靠和可解释的决策支持系统铺平了道路

Abstract: Pathological images play an essential role in cancer prognosis, while
survival analysis, which integrates computational techniques, can predict
critical clinical events such as patient mortality or disease recurrence from
whole-slide images (WSIs). Recent advancements in multiple instance learning
have significantly improved the efficiency of survival analysis. However,
existing methods often struggle to balance the modeling of long-range spatial
relationships with local contextual dependencies and typically lack inherent
interpretability, limiting their clinical utility. To address these challenges,
we propose the Interpretable Pathology Graph-Transformer (IPGPhormer), a novel
framework that captures the characteristics of the tumor microenvironment and
models their spatial dependencies across the tissue. IPGPhormer uniquely
provides interpretability at both tissue and cellular levels without requiring
post-hoc manual annotations, enabling detailed analyses of individual WSIs and
cross-cohort assessments. Comprehensive evaluations on four public benchmark
datasets demonstrate that IPGPhormer outperforms state-of-the-art methods in
both predictive accuracy and interpretability. In summary, our method,
IPGPhormer, offers a promising tool for cancer prognosis assessment, paving the
way for more reliable and interpretable decision-support systems in pathology.
The code is publicly available at
https://anonymous.4open.science/r/IPGPhormer-6EEB.

</details>


### [67] [ViT-EnsembleAttack: Augmenting Ensemble Models for Stronger Adversarial Transferability in Vision Transformers](https://arxiv.org/abs/2508.12384)
*Hanwen Cao,Haobo Lu,Xiaosen Wang,Kun He*

Main category: cs.CV

TL;DR: 提出ViT-EnsembleAttack方法，通过对ViT模型进行对抗性增强来提升集成攻击的迁移性，包括多头丢弃、注意力分数缩放和MLP特征混合三种策略，并使用贝叶斯优化优化参数。


<details>
  <summary>Details</summary>
Motivation: 现有的集成攻击方法主要关注优化集成权重或集成路径，而忽略了通过探索集成模型来增强对抗攻击迁移性的可能性。同时，集成Vision Transformer模型的研究较少受到关注。

Method: 1) 对每个代理ViT模型使用三种对抗性增强策略：多头丢弃、注意力分数缩放和MLP特征混合；2) 使用贝叶斯优化优化相关参数；3) 集成这些对抗性增强模型生成对抗样本；4) 引入自动重加权和步长放大模块提升迁移性。

Result: 大量实验表明，ViT-EnsembleAttack显著增强了基于ViT的集成攻击的对抗迁移性，大幅优于现有方法。

Conclusion: 该方法通过对抗性增强集成模型有效提升了ViT集成攻击的迁移性，填补了该领域的研究空白，为Vision Transformer的安全研究提供了新的思路。

Abstract: Ensemble-based attacks have been proven to be effective in enhancing
adversarial transferability by aggregating the outputs of models with various
architectures. However, existing research primarily focuses on refining
ensemble weights or optimizing the ensemble path, overlooking the exploration
of ensemble models to enhance the transferability of adversarial attacks. To
address this gap, we propose applying adversarial augmentation to the surrogate
models, aiming to boost overall generalization of ensemble models and reduce
the risk of adversarial overfitting. Meanwhile, observing that ensemble Vision
Transformers (ViTs) gain less attention, we propose ViT-EnsembleAttack based on
the idea of model adversarial augmentation, the first ensemble-based attack
method tailored for ViTs to the best of our knowledge. Our approach generates
augmented models for each surrogate ViT using three strategies: Multi-head
dropping, Attention score scaling, and MLP feature mixing, with the associated
parameters optimized by Bayesian optimization. These adversarially augmented
models are ensembled to generate adversarial examples. Furthermore, we
introduce Automatic Reweighting and Step Size Enlargement modules to boost
transferability. Extensive experiments demonstrate that ViT-EnsembleAttack
significantly enhances the adversarial transferability of ensemble-based
attacks on ViTs, outperforming existing methods by a substantial margin. Code
is available at https://github.com/Trustworthy-AI-Group/TransferAttack.

</details>


### [68] [DeCoT: Decomposing Complex Instructions for Enhanced Text-to-Image Generation with Large Language Models](https://arxiv.org/abs/2508.12396)
*Xiaochuan Lin,Xiangyong Chen,Xuan Li,Yichen Su*

Main category: cs.CV

TL;DR: DeCoT是一个通过大语言模型分解复杂文本指令来提升文生图模型性能的框架，在LongBench-T2I基准测试中显著改善了文本理解和构图准确性。


<details>
  <summary>Details</summary>
Motivation: 当前文生图模型在处理复杂长文本指令时表现不佳，经常无法准确渲染细节、空间关系和特定约束，需要一种方法来提升对复杂指令的理解能力。

Method: DeCoT采用两阶段框架：1）复杂指令分解和语义增强，使用LLM将原始指令分解为结构化语义单元；2）多阶段提示集成和自适应生成，将语义单元转换为适合现有T2I模型的分层或优化提示。

Result: 在LongBench-T2I数据集上，DeCoT显著提升了主流T2I模型的性能，特别是在"文本"和"构图"等挑战性维度。与Infinity-8B集成时平均得分3.52，优于基线3.44。人工评估也证实了感知质量和指令保真度的提升。

Conclusion: DeCoT有效弥合了高级用户意图与T2I模型需求之间的差距，实现了更忠实和准确的图像生成，证明了LLM提示在增强T2I模型性能中的关键作用。

Abstract: Despite remarkable advancements, current Text-to-Image (T2I) models struggle
with complex, long-form textual instructions, frequently failing to accurately
render intricate details, spatial relationships, or specific constraints. This
limitation is highlighted by benchmarks such as LongBench-T2I, which reveal
deficiencies in handling composition, specific text, and fine textures. To
address this, we propose DeCoT (Decomposition-CoT), a novel framework that
leverages Large Language Models (LLMs) to significantly enhance T2I models'
understanding and execution of complex instructions. DeCoT operates in two core
stages: first, Complex Instruction Decomposition and Semantic Enhancement,
where an LLM breaks down raw instructions into structured, actionable semantic
units and clarifies ambiguities; second, Multi-Stage Prompt Integration and
Adaptive Generation, which transforms these units into a hierarchical or
optimized single prompt tailored for existing T2I models. Extensive experiments
on the LongBench-T2I dataset demonstrate that DeCoT consistently and
substantially improves the performance of leading T2I models across all
evaluated dimensions, particularly in challenging aspects like "Text" and
"Composition". Quantitative results, validated by multiple MLLM evaluators
(Gemini-2.0-Flash and InternVL3-78B), show that DeCoT, when integrated with
Infinity-8B, achieves an average score of 3.52, outperforming the baseline
Infinity-8B (3.44). Ablation studies confirm the critical contribution of each
DeCoT component and the importance of sophisticated LLM prompting. Furthermore,
human evaluations corroborate these findings, indicating superior perceptual
quality and instruction fidelity. DeCoT effectively bridges the gap between
high-level user intent and T2I model requirements, leading to more faithful and
accurate image generation.

</details>


### [69] [Federated Cross-Modal Style-Aware Prompt Generation](https://arxiv.org/abs/2508.12399)
*Suraj Prasad,Navyansh Mahla,Sunny Gupta,Amit Sethi*

Main category: cs.CV

TL;DR: FedCSAP是一个联邦学习的跨模态风格感知提示生成框架，利用CLIP视觉编码器的多尺度特征和客户端特定风格指标来生成鲁棒的提示词，在多个图像分类数据集上优于现有联邦提示学习方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅依赖最终层特征，无法充分利用分散客户端数据中的丰富多尺度视觉线索和领域特定风格变化，需要一种能够整合多层级视觉特征和风格信息的方法来提升联邦学习中的提示学习效果。

Method: FedCSAP框架利用CLIP视觉编码器的低、中、高层级特征，结合从批量统计中提取的客户端特定风格指标，将精细的视觉细节与文本上下文融合，生成独特且非冗余的上下文感知提示词。

Result: 在多个图像分类数据集上的综合实验证实，FedCSAP在准确性和整体泛化能力方面均优于现有的联邦提示学习方法。

Conclusion: FedCSAP通过整合多尺度视觉特征和风格信息，成功提升了联邦提示学习的性能，能够有效处理非独立同分布类分布和多样化的领域特定风格，同时确保数据隐私保护。

Abstract: Prompt learning has propelled vision-language models like CLIP to excel in
diverse tasks, making them ideal for federated learning due to computational
efficiency. However, conventional approaches that rely solely on final-layer
features miss out on rich multi-scale visual cues and domain-specific style
variations in decentralized client data. To bridge this gap, we introduce
FedCSAP (Federated Cross-Modal Style-Aware Prompt Generation). Our framework
harnesses low, mid, and high-level features from CLIP's vision encoder
alongside client-specific style indicators derived from batch-level statistics.
By merging intricate visual details with textual context, FedCSAP produces
robust, context-aware prompt tokens that are both distinct and non-redundant,
thereby boosting generalization across seen and unseen classes. Operating
within a federated learning paradigm, our approach ensures data privacy through
local training and global aggregation, adeptly handling non-IID class
distributions and diverse domain-specific styles. Comprehensive experiments on
multiple image classification datasets confirm that FedCSAP outperforms
existing federated prompt learning methods in both accuracy and overall
generalization.

</details>


### [70] [MPCAR: Multi-Perspective Contextual Augmentation for Enhanced Visual Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2508.12400)
*Amirul Rahman,Qiang Xu,Xueying Huang*

Main category: cs.CV

TL;DR: MPCAR是一种无需微调的推理时策略，通过多角度生成描述来增强大型视觉语言模型的上下文理解能力，在复杂视觉推理任务中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在需要深度上下文理解、多角度分析或精细细节识别的复杂视觉推理任务中存在局限，单一图像编码和提示方法难以充分捕捉细微视觉信息

Method: 三阶段方法：1）从不同角度生成N个多样化的描述或初步推理路径；2）智能整合这些描述与原问题构建上下文增强提示；3）使用增强提示指导最终深度推理和答案生成

Result: 在GQA、VQA-CP v2和ScienceQA等挑战性VQA数据集上的实验表明，MPCAR持续优于现有基线方法，特别是在需要强上下文理解的任务上获得显著准确率提升

Conclusion: 利用LVLMs固有的生成能力来丰富输入上下文，可以有效释放其在复杂多模态任务中的潜在推理潜力，无需修改模型参数即可实现性能提升

Abstract: Despite significant advancements, Large Vision-Language Models (LVLMs)
continue to face challenges in complex visual reasoning tasks that demand deep
contextual understanding, multi-angle analysis, or meticulous detail
recognition. Existing approaches often rely on single-shot image encoding and
prompts, limiting their ability to fully capture nuanced visual information.
Inspired by the notion that strategically generated "additional" information
can serve as beneficial contextual augmentation, we propose Multi-Perspective
Contextual Augmentation for Reasoning (MPCAR), a novel inference-time strategy
designed to enhance LVLM performance. MPCAR operates in three stages: first, an
LVLM generates N diverse and complementary descriptions or preliminary
reasoning paths from various angles; second, these descriptions are
intelligently integrated with the original question to construct a
comprehensive context-augmented prompt; and finally, this enriched prompt
guides the ultimate LVLM for deep reasoning and final answer generation.
Crucially, MPCAR achieves these enhancements without requiring any fine-tuning
of the underlying LVLM's parameters. Extensive experiments on challenging
Visual Question Answering (VQA) datasets, including GQA, VQA-CP v2, and
ScienceQA (Image-VQA), demonstrate that MPCAR consistently outperforms
established baseline methods. Our quantitative results show significant
accuracy gains, particularly on tasks requiring robust contextual
understanding, while human evaluations confirm improved coherence and
completeness of the generated answers. Ablation studies further highlight the
importance of diverse prompt templates and the number of generated
perspectives. This work underscores the efficacy of leveraging LVLMs' inherent
generative capabilities to enrich input contexts, thereby unlocking their
latent reasoning potential for complex multimodal tasks.

</details>


### [71] [LMAD: Integrated End-to-End Vision-Language Model for Explainable Autonomous Driving](https://arxiv.org/abs/2508.12404)
*Nan Song,Bozhou Zhang,Xiatian Zhu,Jiankang Deng,Li Zhang*

Main category: cs.CV

TL;DR: LMAD是一个针对自动驾驶场景设计的视觉语言框架，通过引入初步场景交互和专家适配器，显著提升了现有VLM在驾驶推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的VLM方法主要在车载多视角图像和场景推理文本上进行微调，但缺乏自动驾驶所需的整体细致场景识别和强大空间感知能力，特别是在复杂情况下。

Method: 提出LMAD框架，模拟现代端到端驾驶范式，结合全面场景理解和任务专用结构，引入初步场景交互和专家适配器，更好地将VLM与自动驾驶场景对齐。

Result: 在DriveLM和nuScenes-QA数据集上的大量实验表明，LMAD显著提升了现有VLM在驾驶推理任务中的性能，为可解释自动驾驶设立了新标准。

Conclusion: LMAD框架有效解决了现有VLM在自动驾驶场景中的局限性，提供了一个与现有VLM完全兼容且能与规划导向驾驶系统无缝集成的解决方案。

Abstract: Large vision-language models (VLMs) have shown promising capabilities in
scene understanding, enhancing the explainability of driving behaviors and
interactivity with users. Existing methods primarily fine-tune VLMs on on-board
multi-view images and scene reasoning text, but this approach often lacks the
holistic and nuanced scene recognition and powerful spatial awareness required
for autonomous driving, especially in complex situations. To address this gap,
we propose a novel vision-language framework tailored for autonomous driving,
called LMAD. Our framework emulates modern end-to-end driving paradigms by
incorporating comprehensive scene understanding and a task-specialized
structure with VLMs. In particular, we introduce preliminary scene interaction
and specialized expert adapters within the same driving task structure, which
better align VLMs with autonomous driving scenarios. Furthermore, our approach
is designed to be fully compatible with existing VLMs while seamlessly
integrating with planning-oriented driving systems. Extensive experiments on
the DriveLM and nuScenes-QA datasets demonstrate that LMAD significantly boosts
the performance of existing VLMs on driving reasoning tasks,setting a new
standard in explainable autonomous driving.

</details>


### [72] [S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing](https://arxiv.org/abs/2508.12409)
*Liang Lv,Di Wang,Jing Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: 这篇论文提出了S5框架，首个可扩展的遥感半监督语义分割方案，通过大规模数据集RS4P-1M和基础模型预训练，在多个遥感测试集上达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有半监督语义分割方法依赖小规模数据集和模型的限制，充分利用大量未标注的地球观测数据来提升实际应用价值。

Method: 构建RS4P-1M大规模数据集，采用基于熵的数据选择策略，预训练不同规模的遥感基础模型，并在微调时使用混合专家系统进行多数据集微调。

Result: 在土地覆盖分割和目标检测任务上显著提升了性能，在所有测试集上达到了最先进水平，证明了扩展半监督学习在遥感应用中的可行性。

Conclusion: S5框架通过大规模数据集和基础模型预训练，成功实现了遥感半监督语义分割的可扩展性，为充分利用未标注地球观测数据提供了有效途径。

Abstract: Semi-supervised semantic segmentation (S4) has advanced remote sensing (RS)
analysis by leveraging unlabeled data through pseudo-labeling and consistency
learning. However, existing S4 studies often rely on small-scale datasets and
models, limiting their practical applicability. To address this, we propose S5,
the first scalable framework for semi-supervised semantic segmentation in RS,
which unlocks the potential of vast unlabeled Earth observation data typically
underutilized due to costly pixel-level annotations. Built upon existing
large-scale RS datasets, S5 introduces a data selection strategy that
integrates entropy-based filtering and diversity expansion, resulting in the
RS4P-1M dataset. Using this dataset, we systematically scales S4 methods by
pre-training RS foundation models (RSFMs) of varying sizes on this extensive
corpus, significantly boosting their performance on land cover segmentation and
object detection tasks. Furthermore, during fine-tuning, we incorporate a
Mixture-of-Experts (MoE)-based multi-dataset fine-tuning approach, which
enables efficient adaptation to multiple RS benchmarks with fewer parameters.
This approach improves the generalization and versatility of RSFMs across
diverse RS benchmarks. The resulting RSFMs achieve state-of-the-art performance
across all benchmarks, underscoring the viability of scaling semi-supervised
learning for RS applications. All datasets, code, and models will be released
at https://github.com/MiliLab/S5

</details>


### [73] [SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological Liver Segmentation in MRI Volumes](https://arxiv.org/abs/2508.12410)
*Jun Zeng,Yannan Huang,Elif Keles,Halil Ertugrul Aktas,Gorkem Durak,Nikhil Kumar Tomar,Quoc-Huy Trinh,Deepak Ranjan Nayak,Ulas Bagci,Debesh Jha*

Main category: cs.CV

TL;DR: 一种新的Mamba基础网络SRMA-Mamba，通过空间解剖基础Mamba模块和空间反向注意力模块，实现了高效的3D疾病感染肝肿化组织分割，效果超越现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 肝硫化对慢性肝病预后关键，早期发现和干预能显著降低死亡率。但现有方法没有充分利用体积MRI数据中的空间解剖细节，影响了临床效果和可解释性。

Method: 设计SRMA-Mamba网络，集成空间解剖基础Mamba模块(SABMamba)，在肝硫化组织内进行选择性Mamba扫描，结合夸状面、颚状面和横断面的解剖信息构建全局空间上下文表征。同时使用空间反向注意力模块(SRMA)逐步精炼分割结果中的硫化细节。

Result: 大量实验证明SRMA-Mamba在3D病理性肝脏分割任务中超越了现有最佳方法，展现出艰涉的性能。

Conclusion: SRMA-Mamba通过深度利用MRI数据的空间解剖特征，提供了一种高效且可解释的肝硫化病变分割方案，为早期诊断和干预提供了有力技术支撑。

Abstract: Liver Cirrhosis plays a critical role in the prognosis of chronic liver
disease. Early detection and timely intervention are critical in significantly
reducing mortality rates. However, the intricate anatomical architecture and
diverse pathological changes of liver tissue complicate the accurate detection
and characterization of lesions in clinical settings. Existing methods
underutilize the spatial anatomical details in volumetric MRI data, thereby
hindering their clinical effectiveness and explainability. To address this
challenge, we introduce a novel Mamba-based network, SRMA-Mamba, designed to
model the spatial relationships within the complex anatomical structures of MRI
volumes. By integrating the Spatial Anatomy-Based Mamba module (SABMamba),
SRMA-Mamba performs selective Mamba scans within liver cirrhotic tissues and
combines anatomical information from the sagittal, coronal, and axial planes to
construct a global spatial context representation, enabling efficient
volumetric segmentation of pathological liver structures. Furthermore, we
introduce the Spatial Reverse Attention module (SRMA), designed to
progressively refine cirrhotic details in the segmentation map, utilizing both
the coarse segmentation map and hierarchical encoding features. Extensive
experiments demonstrate that SRMA-Mamba surpasses state-of-the-art methods,
delivering exceptional performance in 3D pathological liver segmentation. Our
code is available for public:
{\color{blue}{https://github.com/JunZengz/SRMA-Mamba}}.

</details>


### [74] [TiP4GEN: Text to Immersive Panorama 4D Scene Generation](https://arxiv.org/abs/2508.12415)
*Ke Xing,Hanwen Liang,Dejia Xu,Yuyang Yin,Konstantinos N. Plataniotis,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: TiP4GEN是一个文本到动态全景场景生成的先进框架，通过整合全景视频生成和动态场景重建技术，能够创建具有精细内容控制和几何一致性的360度沉浸式4D场景。


<details>
  <summary>Details</summary>
Motivation: 随着VR/AR技术的快速发展，对高质量沉浸式动态场景的需求日益增长。现有生成工作主要集中在静态场景或窄视角动态场景，无法提供真正360度全视角的沉浸体验。

Method: 采用双分支生成模型（全景分支和透视分支）进行视频生成，通过双向交叉注意力机制实现信息交换；基于3D高斯泼溅的几何对齐重建模型，利用度量深度图对齐时空点云，并通过估计姿态初始化场景相机。

Result: 大量实验证明了所提设计的有效性，TiP4GEN在生成视觉吸引人且运动连贯的动态全景场景方面表现出优越性。

Conclusion: TiP4GEN框架成功解决了动态全景场景生成的挑战，为创建高质量360度沉浸式虚拟环境提供了有效的解决方案。

Abstract: With the rapid advancement and widespread adoption of VR/AR technologies,
there is a growing demand for the creation of high-quality, immersive dynamic
scenes. However, existing generation works predominantly concentrate on the
creation of static scenes or narrow perspective-view dynamic scenes, falling
short of delivering a truly 360-degree immersive experience from any viewpoint.
In this paper, we introduce \textbf{TiP4GEN}, an advanced text-to-dynamic
panorama scene generation framework that enables fine-grained content control
and synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GEN
integrates panorama video generation and dynamic scene reconstruction to create
360-degree immersive virtual environments. For video generation, we introduce a
\textbf{Dual-branch Generation Model} consisting of a panorama branch and a
perspective branch, responsible for global and local view generation,
respectively. A bidirectional cross-attention mechanism facilitates
comprehensive information exchange between the branches. For scene
reconstruction, we propose a \textbf{Geometry-aligned Reconstruction Model}
based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds using
metric depth maps and initializing scene cameras with estimated poses, our
method ensures geometric consistency and temporal coherence for the
reconstructed scenes. Extensive experiments demonstrate the effectiveness of
our proposed designs and the superiority of TiP4GEN in generating visually
compelling and motion-coherent dynamic panoramic scenes. Our project page is at
https://ke-xing.github.io/TiP4GEN/.

</details>


### [75] [Illusions in Humans and AI: How Visual Perception Aligns and Diverges](https://arxiv.org/abs/2508.12422)
*Jianyi Yang,Junyi Ye,Ankan Dash,Guiling Wang*

Main category: cs.CV

TL;DR: 通过对比生物与人工智能视觉系统在视觉幻觉上的差异，揭示了AI视视的特有脏弱点和对齐问题，为发展更健壮、可解释的人类对齐AI视视系统提供见解。


<details>
  <summary>Details</summary>
Motivation: 理解生物与人工视觉系统在构建视觉现实方面的根本差异，以发展更健壮、可解释的人类对齐人工智能视视系统。

Method: 通过系统性对比人类和AI对经典视觉幻觉（包括颜色、大小、形状、运动）的响应，分析AI是否体验幻觉、是否有独特幻觉。

Result: 发现AI可能通过目标训练或模式识别产生类似幻觉的效果，同时识别出AI独有的幻觉如像素级敏感性和幻觉，揭示了人类视觉无法感知的AI特定视觉脏弱点。

Conclusion: 这些发现为开发保持人类有益觉知偏见、避免破坏信任和安全的抖动的视觉系统提供了重要见解，有助于建设更健壮、可解释的人类对齐AI。

Abstract: By comparing biological and artificial perception through the lens of
illusions, we highlight critical differences in how each system constructs
visual reality. Understanding these divergences can inform the development of
more robust, interpretable, and human-aligned artificial intelligence (AI)
vision systems. In particular, visual illusions expose how human perception is
based on contextual assumptions rather than raw sensory data. As artificial
vision systems increasingly perform human-like tasks, it is important to ask:
does AI experience illusions, too? Does it have unique illusions? This article
explores how AI responds to classic visual illusions that involve color, size,
shape, and motion. We find that some illusion-like effects can emerge in these
models, either through targeted training or as by-products of pattern
recognition. In contrast, we also identify illusions unique to AI, such as
pixel-level sensitivity and hallucinations, that lack human counterparts. By
systematically comparing human and AI responses to visual illusions, we uncover
alignment gaps and AI-specific perceptual vulnerabilities invisible to human
perception. These findings provide insights for future research on vision
systems that preserve human-beneficial perceptual biases while avoiding
distortions that undermine trust and safety.

</details>


### [76] [Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations](https://arxiv.org/abs/2508.12430)
*Yahsin Yeh,Yilun Wu,Bokai Ruan,Honghan Shuai*

Main category: cs.CV

TL;DR: 这篇论文提出了视觉问题回答中自然语言解释系统的安全漏洞，通过敏感性攻击和知识基础防御方法来提高模型稳健性。


<details>
  <summary>Details</summary>
Motivation: 现有VQA-NLE系统存在解释不一致和理解浅薄的问题，曝露了模型的弱点和安全风险。

Method: 使用对抗性问题扰动和新的图像微调攻击策略，并提出基于外部知识的缓觡方法来减少不一致性。

Result: 在两个标准测试集和两个广泛使用的VQA-NLE模型上证明了攻击的有效性和知识基础防御的潜力。

Conclusion: 当前VQA-NLE系统存在严重的安全和可靠性问题，知识基础方法为提高模型稳健性提供了有效途径。

Abstract: Natural language explanations in visual question answering (VQA-NLE) aim to
make black-box models more transparent by elucidating their decision-making
processes. However, we find that existing VQA-NLE systems can produce
inconsistent explanations and reach conclusions without genuinely understanding
the underlying context, exposing weaknesses in either their inference pipeline
or explanation-generation mechanism. To highlight these vulnerabilities, we not
only leverage an existing adversarial strategy to perturb questions but also
propose a novel strategy that minimally alters images to induce contradictory
or spurious outputs. We further introduce a mitigation method that leverages
external knowledge to alleviate these inconsistencies, thereby bolstering model
robustness. Extensive evaluations on two standard benchmarks and two widely
used VQA-NLE models underscore the effectiveness of our attacks and the
potential of knowledge-based defenses, ultimately revealing pressing security
and reliability concerns in current VQA-NLE systems.

</details>


### [77] [X-Ray-CoT: Interpretable Chest X-ray Diagnosis with Vision-Language Models via Chain-of-Thought Reasoning](https://arxiv.org/abs/2508.12455)
*Chee Ng,Liliang Sun,Shaoqing Tang*

Main category: cs.CV

TL;DR: X-Ray-CoT是一个基于视觉语言大模型的框架，通过模拟放射科医生的思维链过程，实现胸部X光片的智能诊断和可解释报告生成，在保持竞争力的诊断准确性的同时提供透明化的决策过程。


<details>
  <summary>Details</summary>
Motivation: 胸部X光片诊断需要丰富的临床经验且存在观察者间差异，虽然深度学习模型诊断准确率高，但其黑盒特性阻碍了在高风险医疗环境中的临床应用。

Method: 提出X-Ray-CoT框架，首先提取多模态特征和视觉概念，然后使用基于LLM的组件配合结构化思维链提示策略进行推理，生成详细自然语言诊断报告。

Result: 在CORDA数据集上，疾病诊断的平衡准确率达到80.52%，F1分数为78.65%，略优于现有黑盒模型，并能生成高质量的可解释报告。

Conclusion: 该工作代表了在医学影像中构建可信赖和临床可操作AI系统的重要进展，消融研究证实了多模态融合和思维链推理对构建稳健透明医疗AI的必要性。

Abstract: Chest X-ray imaging is crucial for diagnosing pulmonary and cardiac diseases,
yet its interpretation demands extensive clinical experience and suffers from
inter-observer variability. While deep learning models offer high diagnostic
accuracy, their black-box nature hinders clinical adoption in high-stakes
medical settings. To address this, we propose X-Ray-CoT (Chest X-Ray
Chain-of-Thought), a novel framework leveraging Vision-Language Large Models
(LVLMs) for intelligent chest X-ray diagnosis and interpretable report
generation. X-Ray-CoT simulates human radiologists' "chain-of-thought" by first
extracting multi-modal features and visual concepts, then employing an
LLM-based component with a structured Chain-of-Thought prompting strategy to
reason and produce detailed natural language diagnostic reports. Evaluated on
the CORDA dataset, X-Ray-CoT achieves competitive quantitative performance,
with a Balanced Accuracy of 80.52% and F1 score of 78.65% for disease
diagnosis, slightly surpassing existing black-box models. Crucially, it
uniquely generates high-quality, explainable reports, as validated by
preliminary human evaluations. Our ablation studies confirm the integral role
of each proposed component, highlighting the necessity of multi-modal fusion
and CoT reasoning for robust and transparent medical AI. This work represents a
significant step towards trustworthy and clinically actionable AI systems in
medical imaging.

</details>


### [78] [Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping](https://arxiv.org/abs/2508.12466)
*Xuhui Zhan,Tyler Derr*

Main category: cs.CV

TL;DR: Inverse-LLaVA是一种新颖的多模态学习方法，它消除了传统对齐预训练的需求，通过将文本嵌入映射到视觉表示空间而非相反方向，在推理密集型任务上表现优异，同时计算需求减少45%。


<details>
  <summary>Details</summary>
Motivation: 挑战传统多模态学习需要昂贵对齐预训练的范式，避免将视觉特征投影到离散文本标记空间，寻求更高效的多模态融合方法。

Method: 提出逆向映射方法，将文本嵌入映射到连续视觉表示空间，在transformer中间层进行融合，通过注意力机制中的选择性加法组件实现动态集成。

Result: 在9个多模态基准测试中显示差异化性能：推理和认知任务显著提升（MM-VET: +0.2%, VizWiz: +1.8%, ScienceQA: +0.2%, 认知推理: +27.2%），但感知任务性能下降（名人识别: -49.5%, OCR: -21.3%）。

Conclusion: 首次实证证明对齐预训练对有效多模态学习并非必要，特别是复杂推理任务；建立了减少45%计算需求的新范式，挑战了模态融合的传统观念。

Abstract: Traditional multimodal learning approaches require expensive alignment
pre-training to bridge vision and language modalities, typically projecting
visual features into discrete text token spaces. We challenge both fundamental
assumptions underlying this paradigm by proposing Inverse-LLaVA, a novel
approach that eliminates alignment pre-training entirely while inverting the
conventional mapping direction. Rather than projecting visual features to text
space, our method maps text embeddings into continuous visual representation
space and performs fusion within transformer intermediate layers. Through
selective additive components in attention mechanisms, we enable dynamic
integration of visual and textual representations without requiring massive
image-text alignment datasets. Comprehensive experiments across nine multimodal
benchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves
notable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%,
VizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing
expected decreases in perception tasks requiring memorized visual-text
associations (celebrity recognition: -49.5%, OCR: -21.3%). These results
provide the first empirical evidence that alignment pre-training is not
necessary for effective multimodal learning, particularly for complex reasoning
tasks. Our work establishes the feasibility of a new paradigm that reduces
computational requirements by 45%, challenges conventional wisdom about
modality fusion, and opens new research directions for efficient multimodal
architectures that preserve modality-specific characteristics. Our project
website with code and additional resources is available at
https://inverse-llava.github.io.

</details>


### [79] [Standardization of Neuromuscular Reflex Analysis -- Role of Fine-Tuned Vision-Language Model Consortium and OpenAI gpt-oss Reasoning LLM Enabled Decision Support System](https://arxiv.org/abs/2508.12473)
*Eranga Bandara,Ross Gore,Sachin Shetty,Ravi Mukkamala,Christopher Rhea,Atmaram Yarlagadda,Shaifali Kaushik,L. H. M. P. De Silva,Andriy Maznychenko,Inna Sokolowska,Amin Hass,Kasun De Zoysa*

Main category: cs.CV

TL;DR: 使用细调视觉-语言模型联盟和理解大语言模型构建自动化H-反射电机图象解释诊断系统，提高神经肌肉评估的准确性和标准化水平


<details>
  <summary>Details</summary>
Motivation: 传统H-反射EMG波形分析存在主观性和变异性问题，影响诊断的可靠性和标准化，需要自动化解决方案

Method: 使用多个细调VLM模型构成联盟，分析波形图像和临床数据，通过共识机制聚合诊断结果，再由专门理解LLM进行精炼和解释

Result: 混合系统实现了高准确、一致性和可解释的H-反射评估，显著提升了神经肌肉诊断的自动化和标准化水平

Conclusion: 这是首次将细调VLM联盟与理解LLM集成用于图像基H-反射分析，为下一代AI辅助神经肌肉评估平台奠定了基础

Abstract: Accurate assessment of neuromuscular reflexes, such as the H-reflex, plays a
critical role in sports science, rehabilitation, and clinical neurology.
Traditional analysis of H-reflex EMG waveforms is subject to variability and
interpretation bias among clinicians and researchers, limiting reliability and
standardization. To address these challenges, we propose a Fine-Tuned
Vision-Language Model (VLM) Consortium and a reasoning Large-Language Model
(LLM)-enabled Decision Support System for automated H-reflex waveform
interpretation and diagnosis. Our approach leverages multiple VLMs, each
fine-tuned on curated datasets of H-reflex EMG waveform images annotated with
clinical observations, recovery timelines, and athlete metadata. These models
are capable of extracting key electrophysiological features and predicting
neuromuscular states, including fatigue, injury, and recovery, directly from
EMG images and contextual metadata. Diagnostic outputs from the VLM consortium
are aggregated using a consensus-based method and refined by a specialized
reasoning LLM, which ensures robust, transparent, and explainable decision
support for clinicians and sports scientists. The end-to-end platform
orchestrates seamless communication between the VLM ensemble and the reasoning
LLM, integrating prompt engineering strategies and automated reasoning
workflows using LLM Agents. Experimental results demonstrate that this hybrid
system delivers highly accurate, consistent, and interpretable H-reflex
assessments, significantly advancing the automation and standardization of
neuromuscular diagnostics. To our knowledge, this work represents the first
integration of a fine-tuned VLM consortium with a reasoning LLM for image-based
H-reflex analysis, laying the foundation for next-generation AI-assisted
neuromuscular assessment and athlete monitoring platforms.

</details>


### [80] [Skin Cancer Classification: Hybrid CNN-Transformer Models with KAN-Based Fusion](https://arxiv.org/abs/2508.12484)
*Shubhi Agarwal,Amulya Kumar Mahto*

Main category: cs.CV

TL;DR: 这篇论文探索了序列和并行混合CNN-Transformer模型在皮肤癌分类中的应用，结合卷积科尔莫戈洛夫-阿诺德网络(CKAN)进行非线性特征融合，在多个数据集上取得了竞争性能的分类结果。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌分类在医学图像分析中至关重要，需要精确区分恶性和非恶性病变。传统模型在特征表征学习和全局依赖建模方面有限，需要更有效的混合模型来提升分类性能。

Method: 提出序列和并行混合CNN-Transformer模型，集成转移学习和数据增强。CNN提取局部空间特征，Transformer建模全局依赖关系，CKAN通过可学习激活函数进行非线性特征融合。

Result: 在三个标准数据集上取得优异结果：HAM10000数据集准确率92.81%、F1分92.47%；PAD-UFES数据集准确率97.83%、F1分97.83%；BCN20000数据集准确率91.17%、F1分91.79%。

Conclusion: 混合CNN-Transformer架构能够有效捕捉空间和上下文特征，CKAN的集成通过可学习激活函数增强了特征融合能力，显示了特征表征和模型设计在医学图像分类中的重要性。

Abstract: Skin cancer classification is a crucial task in medical image analysis, where
precise differentiation between malignant and non-malignant lesions is
essential for early diagnosis and treatment. In this study, we explore
Sequential and Parallel Hybrid CNN-Transformer models with Convolutional
Kolmogorov-Arnold Network (CKAN). Our approach integrates transfer learning and
extensive data augmentation, where CNNs extract local spatial features,
Transformers model global dependencies, and CKAN facilitates nonlinear feature
fusion for improved representation learning. To assess generalization, we
evaluate our models on multiple benchmark datasets (HAM10000,BCN20000 and
PAD-UFES) under varying data distributions and class imbalances. Experimental
results demonstrate that hybrid CNN-Transformer architectures effectively
capture both spatial and contextual features, leading to improved
classification performance. Additionally, the integration of CKAN enhances
feature fusion through learnable activation functions, yielding more
discriminative representations. Our proposed approach achieves competitive
performance in skin cancer classification, demonstrating 92.81% accuracy and
92.47% F1-score on the HAM10000 dataset, 97.83% accuracy and 97.83% F1-score on
the PAD-UFES dataset, and 91.17% accuracy with 91.79% F1- score on the BCN20000
dataset highlighting the effectiveness and generalizability of our model across
diverse datasets. This study highlights the significance of feature
representation and model design in advancing robust and accurate medical image
classification.

</details>


### [81] [Design and Validation of a Responsible Artificial Intelligence-based System for the Referral of Diabetic Retinopathy Patients](https://arxiv.org/abs/2508.12506)
*E. Ulises Moya-Sánchez,Abraham Sánchez-Perez,Raúl Nanclares Da Veiga,Alejandro Zarate-Macías,Edgar Villareal,Alejandro Sánchez-Montes,Edtna Jauregui-Ulloa,Héctor Moreno,Ulises Cortés*

Main category: cs.CV

TL;DR: RAIS-DR是一个负责任的人工智能系统，用于糖尿病视网膜病变筛查，在准确性和公平性方面显著优于FDA批准的EyeArt系统。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是工作年龄人群视力丧失的主要原因，早期检测可降低95%的视力丧失风险。但由于视网膜专家短缺和及时检查的挑战，检测变得复杂。AI模型虽提供解决方案，但临床应用中存在数据质量低和偏见问题。

Method: 开发RAIS-DR系统，整合高效的卷积模型进行预处理、质量评估和三个专门的DR分类模型。在1,046名患者的本地数据集上评估，与FDA批准的EyeArt系统进行比较。

Result: RAIS-DR表现出显著改进：F1分数提高5-12%，准确率提高6-19%，特异性提高10-20%。公平性指标显示在不同人口统计亚组中表现公平。

Conclusion: RAIS-DR是一个强大且符合伦理的DR筛查解决方案，具有减少医疗保健差异的潜力，代码和权重已开源提供。

Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss in working-age
individuals. Early detection of DR can reduce the risk of vision loss by up to
95%, but a shortage of retinologists and challenges in timely examination
complicate detection. Artificial Intelligence (AI) models using retinal fundus
photographs (RFPs) offer a promising solution. However, adoption in clinical
settings is hindered by low-quality data and biases that may lead AI systems to
learn unintended features. To address these challenges, we developed RAIS-DR, a
Responsible AI System for DR screening that incorporates ethical principles
across the AI lifecycle. RAIS-DR integrates efficient convolutional models for
preprocessing, quality assessment, and three specialized DR classification
models. We evaluated RAIS-DR against the FDA-approved EyeArt system on a local
dataset of 1,046 patients, unseen by both systems. RAIS-DR demonstrated
significant improvements, with F1 scores increasing by 5-12%, accuracy by
6-19%, and specificity by 10-20%. Additionally, fairness metrics such as
Disparate Impact and Equal Opportunity Difference indicated equitable
performance across demographic subgroups, underscoring RAIS-DR's potential to
reduce healthcare disparities. These results highlight RAIS-DR as a robust and
ethically aligned solution for DR screening in clinical settings. The code,
weights of RAIS-DR are available at
https://gitlab.com/inteligencia-gubernamental-jalisco/jalisco-retinopathy with
RAIL.

</details>


### [82] [LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision Language Models](https://arxiv.org/abs/2508.12512)
*Krishna Teja Chitty-Venkata,Murali Emani,Venkatram Vishwanath*

Main category: cs.CV

TL;DR: LangVision-LoRA-NAS是一个新颖的框架，将神经架构搜索（NAS）与LoRA相结合，为视觉语言模型（VLMs）优化可变秩适应，通过动态搜索最优LoRA秩配置来平衡性能与计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前LoRA实现采用固定秩，可能限制了在不同任务中的灵活性和效率。需要一种能够根据具体多模态任务动态优化LoRA秩配置的方法。

Method: 集成神经架构搜索（NAS）与LoRA，通过NAS动态搜索针对特定多模态任务的最优LoRA秩配置，实现可变秩适应。

Result: 在多个数据集上使用LLaMA-3.2-11B模型进行广泛实验，LangVision-LoRA-NAS显著提升了模型性能，同时降低了微调成本。

Conclusion: LangVision-LoRA-NAS框架通过动态秩优化，为视觉语言模型的高效微调提供了有效的解决方案，在性能和计算效率之间取得了良好平衡。

Abstract: Vision Language Models (VLMs) integrate visual and text modalities to enable
multimodal understanding and generation. These models typically combine a
Vision Transformer (ViT) as an image encoder and a Large Language Model (LLM)
for text generation. LoRA (Low-Rank Adaptation) is an efficient fine-tuning
method to adapt pre-trained models to new tasks by introducing low-rank updates
to their weights. While LoRA has emerged as a powerful technique for
fine-tuning large models by introducing low-rank updates, current
implementations assume a fixed rank, potentially limiting flexibility and
efficiency across diverse tasks. This paper introduces
\textit{LangVision-LoRA-NAS}, a novel framework that integrates Neural
Architecture Search (NAS) with LoRA to optimize VLMs for variable-rank
adaptation. Our approach leverages NAS to dynamically search for the optimal
LoRA rank configuration tailored to specific multimodal tasks, balancing
performance and computational efficiency. Through extensive experiments using
the LLaMA-3.2-11B model on several datasets, LangVision-LoRA-NAS demonstrates
notable improvement in model performance while reducing fine-tuning costs. Our
Base and searched fine-tuned models on LLaMA-3.2-11B-Vision-Instruct can be
found
\href{https://huggingface.co/collections/krishnateja95/llama-32-11b-vision-instruct-langvision-lora-nas-6786cac480357a6a6fcc59ee}{\textcolor{blue}{here}}
and the code for LangVision-LoRA-NAS can be found
\href{https://github.com/krishnateja95/LangVision-NAS}{\textcolor{blue}{here}}.

</details>


### [83] [An Initial Study of Bird's-Eye View Generation for Autonomous Vehicles using Cross-View Transformers](https://arxiv.org/abs/2508.12520)
*Felipe Carlos dos Santos,Eric Aislan Antonelo,Gustavo Claudio Karl Couto*

Main category: cs.CV

TL;DR: 通过跨视角变换器学习将摄像头图像映射到平面视图地图，包括道路、车道标记咈规划轨迹三个通道


<details>
  <summary>Details</summary>
Motivation: 平面视图地图为自动驾驶感知提供结构化的顶视抽象，重点考察在未见城市的演化能力咈不同摄像头配置的影响

Method: 使用跨视角变换器(CVT)，通过丰富的城市驾驶模拟器生成训练数据，比较了两种损失函数（focal损失咈L1损失）的效果

Result: 在仅使用一个城市训练数据的情况下，配备四台摄像头并使用L1损失训练的CVT模型在新城市中表现最为稳健

Conclusion: 跨视角变换器在将摄像头输入映射到准确平面视图地图方面显示出很大潜力

Abstract: Bird's-Eye View (BEV) maps provide a structured, top-down abstraction that is
crucial for autonomous-driving perception. In this work, we employ Cross-View
Transformers (CVT) for learning to map camera images to three BEV's channels -
road, lane markings, and planned trajectory - using a realistic simulator for
urban driving. Our study examines generalization to unseen towns, the effect of
different camera layouts, and two loss formulations (focal and L1). Using
training data from only a town, a four-camera CVT trained with the L1 loss
delivers the most robust test performance, evaluated in a new town. Overall,
our results underscore CVT's promise for mapping camera inputs to reasonably
accurate BEV maps.

</details>


### [84] [MuSACo: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training](https://arxiv.org/abs/2508.12522)
*Muhammad Osama Zeeshan,Natacha Gillet,Alessandro Lameiras Koerich,Marco Pedersoli,Francois Bremond,Eric Granger*

Main category: cs.CV

TL;DR: MuSACo是一种基于协同训练的多模态个性化表情识别方法，通过选择相关源主体并利用多模态互补信息进行主体特异性适应，在BioVid和StressID数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多源域适应方法往往忽略多模态信息或将多个源混合为单一域，限制了主体多样性，无法显式捕捉主体特异性特征。个性化表情识别需要适应主体间的高度变异性。

Method: 基于协同训练的多模态主体特异性选择和适应方法：选择与目标相关的源主体；使用主导模态生成伪标签进行类感知学习；结合类不可知损失从低置信度目标样本中学习；对齐每个模态的源特征，仅组合置信的目标特征。

Result: 在BioVid和StressID两个具有挑战性的多模态表情识别数据集上，MuSACo能够超越无监督域适应（混合）和最先进的多源域适应方法。

Conclusion: MuSACo通过有效利用多模态信息和多源域信息，成功解决了个性化表情识别中的主体特异性适应问题，特别适用于数字健康中的情感计算应用，如患者特异性压力或疼痛评估。

Abstract: Personalized expression recognition (ER) involves adapting a machine learning
model to subject-specific data for improved recognition of expressions with
considerable interpersonal variability. Subject-specific ER can benefit
significantly from multi-source domain adaptation (MSDA) methods, where each
domain corresponds to a specific subject, to improve model accuracy and
robustness. Despite promising results, state-of-the-art MSDA approaches often
overlook multimodal information or blend sources into a single domain, limiting
subject diversity and failing to explicitly capture unique subject-specific
characteristics. To address these limitations, we introduce MuSACo, a
multi-modal subject-specific selection and adaptation method for ER based on
co-training. It leverages complementary information across multiple modalities
and multiple source domains for subject-specific adaptation. This makes MuSACo
particularly relevant for affective computing applications in digital health,
such as patient-specific assessment for stress or pain, where subject-level
nuances are crucial. MuSACo selects source subjects relevant to the target and
generates pseudo-labels using the dominant modality for class-aware learning,
in conjunction with a class-agnostic loss to learn from less confident target
samples. Finally, source features from each modality are aligned, while only
confident target features are combined. Our experimental results on challenging
multimodal ER datasets: BioVid and StressID, show that MuSACo can outperform
UDA (blending) and state-of-the-art MSDA methods.

</details>


### [85] [REVEAL -- Reasoning and Evaluation of Visual Evidence through Aligned Language](https://arxiv.org/abs/2508.12543)
*Ipsita Praharaj,Yukta Butala,Yash Butala*

Main category: cs.CV

TL;DR: REVEAL框架利用大型视觉语言模型，将图像伪造检测构建为提示驱动的视觉推理任务，通过整体场景评估和区域异常检测两种方法来提高跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 生成模型的快速发展使得视觉伪造检测和解释变得更加困难，现有方法在跨域泛化方面存在挑战，需要既能检测伪造又能提供推理和定位的鲁棒框架。

Method: 提出REVEAL框架，采用两种互补方法：(1)整体场景级评估：基于图像的物理特性、语义、透视和整体真实性；(2)区域级异常检测：将图像分割成多个区域并分别分析。利用大型视觉语言模型的语义对齐能力。

Result: 在多个领域数据集（Photoshop、DeepFake和AIGC编辑）上进行实验，与竞争基线进行比较，并分析了模型提供的推理能力。

Conclusion: 通过提示驱动的视觉推理方法，REVEAL框架能够有效检测图像伪造并提供解释，在跨域泛化方面表现出色，为图像真实性验证提供了新的解决方案。

Abstract: The rapid advancement of generative models has intensified the challenge of
detecting and interpreting visual forgeries, necessitating robust frameworks
for image forgery detection while providing reasoning as well as localization.
While existing works approach this problem using supervised training for
specific manipulation or anomaly detection in the embedding space,
generalization across domains remains a challenge. We frame this problem of
forgery detection as a prompt-driven visual reasoning task, leveraging the
semantic alignment capabilities of large vision-language models. We propose a
framework, `REVEAL` (Reasoning and Evaluation of Visual Evidence through
Aligned Language), that incorporates generalized guidelines. We propose two
tangential approaches - (1) Holistic Scene-level Evaluation that relies on the
physics, semantics, perspective, and realism of the image as a whole and (2)
Region-wise anomaly detection that splits the image into multiple regions and
analyzes each of them. We conduct experiments over datasets from different
domains (Photoshop, DeepFake and AIGC editing). We compare the Vision Language
Models against competitive baselines and analyze the reasoning provided by
them.

</details>


### [86] [Structure-preserving Feature Alignment for Old Photo Colorization](https://arxiv.org/abs/2508.12570)
*Yingxue Pang,Xin Jin,Jun Fu,Zhibo Chen*

Main category: cs.CV

TL;DR: 提出SFAC算法，仅需两张图像即可实现老照片着色，通过特征对齐和结构保持机制解决领域差异问题


<details>
  <summary>Details</summary>
Motivation: 传统深度学习着色方法需要大规模数据集，但老照片缺乏真实标签且与自然灰度图像存在领域差异，难以直接应用

Method: SFAC算法：1）特征分布对齐损失建立语义对应；2）特征级感知约束和像素级冻结-更新金字塔结构保持机制防止结构扭曲

Result: 大量实验证明该方法在老照片着色任务上效果显著，定性和定量指标均验证了有效性

Conclusion: SFAC算法成功解决了老照片着色的领域差异问题，无需大数据依赖，仅需两张图像即可实现高质量着色

Abstract: Deep learning techniques have made significant advancements in
reference-based colorization by training on large-scale datasets. However,
directly applying these methods to the task of colorizing old photos is
challenging due to the lack of ground truth and the notorious domain gap
between natural gray images and old photos. To address this issue, we propose a
novel CNN-based algorithm called SFAC, i.e., Structure-preserving Feature
Alignment Colorizer. SFAC is trained on only two images for old photo
colorization, eliminating the reliance on big data and allowing direct
processing of the old photo itself to overcome the domain gap problem. Our
primary objective is to establish semantic correspondence between the two
images, ensuring that semantically related objects have similar colors. We
achieve this through a feature distribution alignment loss that remains robust
to different metric choices. However, utilizing robust semantic correspondence
to transfer color from the reference to the old photo can result in inevitable
structure distortions. To mitigate this, we introduce a structure-preserving
mechanism that incorporates a perceptual constraint at the feature level and a
frozen-updated pyramid at the pixel level. Extensive experiments demonstrate
the effectiveness of our method for old photo colorization, as confirmed by
qualitative and quantitative metrics.

</details>


### [87] [Foundation Model for Skeleton-Based Human Action Understanding](https://arxiv.org/abs/2508.12586)
*Hongsong Wang,Wanjiang Weng,Junbo Wang,Fang Zhao,Guo-Sen Xie,Xin Geng,Liang Wang*

Main category: cs.CV

TL;DR: 提出了USDRL框架，首个骨架基础模型，通过Transformer编码器、多粒度特征解相关和多视角一致性训练，在25个基准测试中显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有骨架动作理解方法缺乏可扩展性和泛化能力，没有能够适应广泛动作理解任务的基础模型

Method: 使用Transformer密集时空编码器学习时空特征，多粒度特征解相关减少维度冗余，多视角一致性训练增强语义学习和多模态特征提取

Result: 在9个骨架动作理解任务的25个基准测试中显著超越当前最先进方法

Conclusion: USDRL框架扩展了骨架动作理解的研究范围，鼓励更多关注密集预测任务

Abstract: Human action understanding serves as a foundational pillar in the field of
intelligent motion perception. Skeletons serve as a modality- and
device-agnostic representation for human modeling, and skeleton-based action
understanding has potential applications in humanoid robot control and
interaction. \RED{However, existing works often lack the scalability and
generalization required to handle diverse action understanding tasks. There is
no skeleton foundation model that can be adapted to a wide range of action
understanding tasks}. This paper presents a Unified Skeleton-based Dense
Representation Learning (USDRL) framework, which serves as a foundational model
for skeleton-based human action understanding. USDRL consists of a
Transformer-based Dense Spatio-Temporal Encoder (DSTE), Multi-Grained Feature
Decorrelation (MG-FD), and Multi-Perspective Consistency Training (MPCT). The
DSTE module adopts two parallel streams to learn temporal dynamic and spatial
structure features. The MG-FD module collaboratively performs feature
decorrelation across temporal, spatial, and instance domains to reduce
dimensional redundancy and enhance information extraction. The MPCT module
employs both multi-view and multi-modal self-supervised consistency training.
The former enhances the learning of high-level semantics and mitigates the
impact of low-level discrepancies, while the latter effectively facilitates the
learning of informative multimodal features. We perform extensive experiments
on 25 benchmarks across across 9 skeleton-based action understanding tasks,
covering coarse prediction, dense prediction, and transferred prediction. Our
approach significantly outperforms the current state-of-the-art methods. We
hope that this work would broaden the scope of research in skeleton-based
action understanding and encourage more attention to dense prediction tasks.

</details>


### [88] [Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models](https://arxiv.org/abs/2508.12587)
*Tan-Hanh Pham,Chris Ngo*

Main category: cs.CV

TL;DR: 多模态连续思维链(MCOUT)通过在聚合潜在空间中进行连续向量推理，充分利用视觉和文本特征，在多模态理由任务上获得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型推理方法(如思维链提示)在多模态环境中效果不佳，难以动态对齐音频、视觉和文本信息。

Method: 提出MCOUT方法，将推理状态表示为连续隐藏向量，迭代精炼并与视觉和文本嵌入对齐。包括两个变种：MCOUT-Base(重用语言模型隐藏状态)和MCOUT-Multi(集成多模态注意力机制)。

Result: 在MMMU、ScienceQA和MMStar等测试集上，MCOUT持续提升多模态理由性能，准确率最高提升8.23%，BLEU指标提升达8.27%，在选择题和开放式任务中都取得显著改进。

Conclusion: 连续潜在空间推理是提升大型多模态模型理由能力的有前景方向，为实现类人反思类多模态推理提供了可扩展框架。

Abstract: Many reasoning techniques for large multimodal models adapt language model
approaches, such as Chain-of-Thought (CoT) prompting, which express reasoning
as word sequences. While effective for text, these methods are suboptimal for
multimodal contexts, struggling to align audio, visual, and textual information
dynamically. To explore an alternative paradigm, we propose the Multimodal
Chain of Continuous Thought (MCOUT), which enables reasoning directly in a
joint latent space rather than in natural language. In MCOUT, the reasoning
state is represented as a continuous hidden vector, iteratively refined and
aligned with visual and textual embeddings, inspired by human reflective
cognition. We develop two variants: MCOUT-Base, which reuses the language
model`s last hidden state as the continuous thought for iterative reasoning,
and MCOUT-Multi, which integrates multimodal latent attention to strengthen
cross-modal alignment between visual and textual features. Experiments on
benchmarks including MMMU, ScienceQA, and MMStar show that MCOUT consistently
improves multimodal reasoning, yielding up to 8.23% accuracy gains over strong
baselines and improving BLEU scores up to 8.27% across multiple-choice and
open-ended tasks. These findings highlight latent continuous reasoning as a
promising direction for advancing LMMs beyond language-bound CoT, offering a
scalable framework for human-like reflective multimodal inference. Code is
available at https://github.com/Hanhpt23/OmniMod.

</details>


### [89] [ViLaD: A Large Vision Language Diffusion Framework for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.12603)
*Can Cui,Yupeng Zhou,Juntong Peng,Sung-Yeon Park,Zichong Yang,Prashanth Sankaranarayanan,Jiaru Zhang,Ruqi Zhang,Ziran Wang*

Main category: cs.CV

TL;DR: ViLaD是一个基于扩散模型的新型端到端自动驾驶框架，通过并行生成驾驶决策序列显著降低延迟，支持双向推理，在nuScenes数据集上表现优于现有自回归VLM方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉语言模型的端到端自动驾驶系统采用自回归架构，存在推理延迟高、无法进行双向推理的问题，不适合动态的安全关键环境。

Method: 提出ViLaD框架，利用掩码扩散模型并行生成完整的驾驶决策序列，支持双向推理和渐进式简单优先生成策略。

Result: 在nuScenes数据集上，ViLaD在规划准确性和推理速度方面均优于最先进的自回归VLM基线，接近零失败率，并在真实自动驾驶车辆上验证了实用性。

Conclusion: ViLaD代表了自动驾驶领域的范式转变，通过扩散模型架构解决了自回归方法的局限性，为实际应用提供了高效可靠的解决方案。

Abstract: End-to-end autonomous driving systems built on Vision Language Models (VLMs)
have shown significant promise, yet their reliance on autoregressive
architectures introduces some limitations for real-world applications. The
sequential, token-by-token generation process of these models results in high
inference latency and cannot perform bidirectional reasoning, making them
unsuitable for dynamic, safety-critical environments. To overcome these
challenges, we introduce ViLaD, a novel Large Vision Language Diffusion (LVLD)
framework for end-to-end autonomous driving that represents a paradigm shift.
ViLaD leverages a masked diffusion model that enables parallel generation of
entire driving decision sequences, significantly reducing computational
latency. Moreover, its architecture supports bidirectional reasoning, allowing
the model to consider both past and future simultaneously, and supports
progressive easy-first generation to iteratively improve decision quality. We
conduct comprehensive experiments on the nuScenes dataset, where ViLaD
outperforms state-of-the-art autoregressive VLM baselines in both planning
accuracy and inference speed, while achieving a near-zero failure rate.
Furthermore, we demonstrate the framework's practical viability through a
real-world deployment on an autonomous vehicle for an interactive parking task,
confirming its effectiveness and soundness for practical applications.

</details>


### [90] [ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images](https://arxiv.org/abs/2508.12605)
*Wenjie Liao,Jieyu Yuan,Yifang Xu,Chunle Guo,Zilong Zhang,Jihong Li,Jiachen Fu,Haotian Fan,Tao Li,Junhui Cui,Chongyi Li*

Main category: cs.CV

TL;DR: 这篇论文提出了首个大规模的用户生成内容图像视觉失真评估指令微调数据集ViDA-UGC，通过链式思绪框架生成细粒度质量描述，从而提升多模态大语言模型在图像质量评估上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的可解释图像质量评估方法存在两个主要问题：一是将用户生成内容（UGC）和AI生成内容（AIGC）图像混淆使用相同的失真标准进行评估，二是缺乏细致的质量分析以支持图像质量监控和图像恢复指导。

Method: 研究构建了首个大规模的ViDA-UGC数据集，包含11K带有细粒度质量基准、详细质量感知和推理质量描述的图像。通过失真导向的流水线和链式思绪评估框架，指导GPT-4o生成质量描述。还选择476张图像构建了ViDA-UGC-Bench标准测试集。

Result: 实验结果表明，ViDA-UGC数据集和CoT框架能够一致地提升多个基础MLLM模型在ViDA-UGC-Bench和Q-Bench上的各种图像质量分析能力，甚至超越了GPT-4o的性能。

Conclusion: 该研究为用户生成内容图像的视觉失真评估提供了一个重要的数据集和框架，通过细粒度的质量描述和失真分析，有效地提升了多模态大语言模型在图像质量评估任务上的表现。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have introduced a
paradigm shift for Image Quality Assessment (IQA) from unexplainable image
quality scoring to explainable IQA, demonstrating practical applications like
quality control and optimization guidance. However, current explainable IQA
methods not only inadequately use the same distortion criteria to evaluate both
User-Generated Content (UGC) and AI-Generated Content (AIGC) images, but also
lack detailed quality analysis for monitoring image quality and guiding image
restoration. In this study, we establish the first large-scale Visual
Distortion Assessment Instruction Tuning Dataset for UGC images, termed
ViDA-UGC, which comprises 11K images with fine-grained quality grounding,
detailed quality perception, and reasoning quality description data. This
dataset is constructed through a distortion-oriented pipeline, which involves
human subject annotation and a Chain-of-Thought (CoT) assessment framework.
This framework guides GPT-4o to generate quality descriptions by identifying
and analyzing UGC distortions, which helps capturing rich low-level visual
features that inherently correlate with distortion patterns. Moreover, we
carefully select 476 images with corresponding 6,149 question answer pairs from
ViDA-UGC and invite a professional team to ensure the accuracy and quality of
GPT-generated information. The selected and revised data further contribute to
the first UGC distortion assessment benchmark, termed ViDA-UGC-Bench.
Experimental results demonstrate the effectiveness of the ViDA-UGC and CoT
framework for consistently enhancing various image quality analysis abilities
across multiple base MLLMs on ViDA-UGC-Bench and Q-Bench, even surpassing
GPT-4o.

</details>


### [91] [OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion](https://arxiv.org/abs/2508.12610)
*Chen Qian,Danyang Li,Xinran Yu,Zheng Yang,Qiang Ma*

Main category: cs.CV

TL;DR: 通过提出CMU-Occlu数据集和OpenMoCap模型，解决了光学动作抓取中大规模标记点遮挡问题，提升了动作解析的稳健性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有光学动作抓取系统在真实应用中遇到大规模标记点遮挡时性能伞然下降，主要问题在于缺乏反映真实遮挡模式的训练数据集和无法抓取标记点间长程依赖关系的训练策略。

Method: 首先创建了CMU-Occlu数据集，利用光线追踪技术模拟真实的标记点遮挡模式。然后提出OpenMoCap模型，采用标记-关节链推理机制，实现标记与关节间深度约束的同时优化和构建。

Result: 经过广泛的对比实验，OpenMoCap在多种场景下都一质地超越了竞争方法。CMU-Occlu数据集为未来稳健动作解析研究打开了新天地。

Conclusion: 该研究通过创新的数据集和模型设计，有效解决了光学动作抓取中遮挡问题的核心挑战，提供了一个实用且高效的解决方案，并已集成到实际系统中部署。

Abstract: Optical motion capture is a foundational technology driving advancements in
cutting-edge fields such as virtual reality and film production. However,
system performance suffers severely under large-scale marker occlusions common
in real-world applications. An in-depth analysis identifies two primary
limitations of current models: (i) the lack of training datasets accurately
reflecting realistic marker occlusion patterns, and (ii) the absence of
training strategies designed to capture long-range dependencies among markers.
To tackle these challenges, we introduce the CMU-Occlu dataset, which
incorporates ray tracing techniques to realistically simulate practical marker
occlusion patterns. Furthermore, we propose OpenMoCap, a novel motion-solving
model designed specifically for robust motion capture in environments with
significant occlusions. Leveraging a marker-joint chain inference mechanism,
OpenMoCap enables simultaneous optimization and construction of deep
constraints between markers and joints. Extensive comparative experiments
demonstrate that OpenMoCap consistently outperforms competing methods across
diverse scenarios, while the CMU-Occlu dataset opens the door for future
studies in robust motion solving. The proposed OpenMoCap is integrated into the
MoSen MoCap system for practical deployment. The code is released at:
https://github.com/qianchen214/OpenMoCap.

</details>


### [92] [WIPES: Wavelet-based Visual Primitives](https://arxiv.org/abs/2508.12615)
*Wenhao Zhang,Hao Zhu,Delong Wu,Di Kang,Linchao Bao,Zhan Ma,Xun Cao*

Main category: cs.CV

TL;DR: WIPES是一种基于小波的通用视觉基元表示方法，通过小波的空间-频率局部化优势有效捕获高低频信息，并开发了基于小波的可微分光栅化器实现快速渲染，在多种视觉任务中表现出优于现有方法的渲染质量和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉表示方法往往依赖频率引导或复杂神经网络解码，导致频谱损失或渲染速度慢，需要一种能同时提供灵活频率调制和快速渲染速度的连续视觉表示方法。

Method: 提出基于小波的通用视觉基元WIPES，利用小波的空间-频率局部化特性捕获多尺度频率信息，并开发了基于小波的可微分光栅化器来实现快速视觉渲染。

Result: 在2D图像表示、5D静态和6D动态新视角合成等多种视觉任务中，WIPES相比基于INR的方法提供更高的渲染质量和更快的推理速度，在渲染质量上也优于基于高斯表示的方法。

Conclusion: WIPES作为一种视觉基元表示，通过小波变换的优势成功解决了现有方法在频谱保持和渲染速度方面的局限性，为多维度视觉信号表示提供了有效的解决方案。

Abstract: Pursuing a continuous visual representation that offers flexible frequency
modulation and fast rendering speed has recently garnered increasing attention
in the fields of 3D vision and graphics. However, existing representations
often rely on frequency guidance or complex neural network decoding, leading to
spectrum loss or slow rendering. To address these limitations, we propose
WIPES, a universal Wavelet-based vIsual PrimitivES for representing
multi-dimensional visual signals. Building on the spatial-frequency
localization advantages of wavelets, WIPES effectively captures both the
low-frequency "forest" and the high-frequency "trees." Additionally, we develop
a wavelet-based differentiable rasterizer to achieve fast visual rendering.
Experimental results on various visual tasks, including 2D image
representation, 5D static and 6D dynamic novel view synthesis, demonstrate that
WIPES, as a visual primitive, offers higher rendering quality and faster
inference than INR-based methods, and outperforms Gaussian-based
representations in rendering quality.

</details>


### [93] [Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning](https://arxiv.org/abs/2508.12628)
*Yukang Lin,Xiang Zhang,Shichang Jia,Bowen Wan,Chenghan Fu,Xudong Ren,Yueran Liu,Wanxian Guan,Pengji Wang,Jian Xu,Bo Zheng,Baolin Liu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种可解释性创意图片评估与选择新范式，基于多模态大语言模型开发了Creative4U系统，通过理性选择训练方法实现了准确的创意图片选择。


<details>
  <summary>Details</summary>
Motivation: 广告主虽然可以使用AIGC技术低成本生产大量创意图片，但缺乏有效的质量评估方法来选择最优创意，现有方法主要关注排名而无法提供可解释的选择。

Method: 基于多模态大语言模型，将创意图片评估与选择集成为自然语言生成任务。构建CreativePair数据集（8k带注释图片对），开发Creative4U系统，采用Reason-to-Select RFT训练方法（包括CoT-SFT和GRPO强化学习）。

Result: 离线和在线实验都证明了该方法的有效性，能够准确评估和选择创意图片。

Conclusion: 该研究为创意图片选择提供了可解释的新范式，通过MLLMs技术实现了根据用户兴趣的个性化创意选择，对于提升电子商务平台的广告效果具有重要意义。

Abstract: Creative image in advertising is the heart and soul of e-commerce platform.
An eye-catching creative image can enhance the shopping experience for users,
boosting income for advertisers and advertising revenue for platforms. With the
advent of AIGC technology, advertisers can produce large quantities of creative
images at minimal cost. However, they struggle to assess the creative quality
to select. Existing methods primarily focus on creative ranking, which fails to
address the need for explainable creative selection.
  In this work, we propose the first paradigm for explainable creative
assessment and selection. Powered by multimodal large language models (MLLMs),
our approach integrates the assessment and selection of creative images into a
natural language generation task. To facilitate this research, we construct
CreativePair, the first comparative reasoning-induced creative dataset
featuring 8k annotated image pairs, with each sample including a label
indicating which image is superior. Additionally, we introduce Creative4U
(pronounced Creative for You), a MLLMs-based creative selector that takes into
account users' interests. Through Reason-to-Select RFT, which includes
supervised fine-tuning with Chain-of-Thought (CoT-SFT) and Group Relative
Policy Optimization (GRPO) based reinforcement learning, Creative4U is able to
evaluate and select creative images accurately. Both offline and online
experiments demonstrate the effectiveness of our approach. Our code and dataset
will be made public to advance research and industrial applications.

</details>


### [94] [SpotVLM: Cloud-edge Collaborative Real-time VLM based on Context Transfer](https://arxiv.org/abs/2508.12638)
*Chen Qian,Xinran Yu,Zewen Huang,Danyang Li,Qiang Ma,Fan Dang,Xuan Ding,Guangyong Shang,Zheng Yang*

Main category: cs.CV

TL;DR: 提出了一种新的云边协同范式Context Transfer，将大型视觉-语言模型的延迟输出作为历史上下文来指导小型模型的实时推理


<details>
  <summary>Details</summary>
Motivation: 现有的云边协同方案无法适应云端延迟波动，且忽视了延迟但准确的大型模型输出的潜在价值

Method: 设计了SpotVLM，包含上下文替换模块和视觉聚焦模块，用于精炼历史文本输入和提升视觉基准一致性

Result: 在四个数据集上的三个实时视觉任务中验证了框架的有效性

Conclusion: 该新范式为未来VLM系统中更有效和延迟感知的协同策略奠定了基础

Abstract: Vision-Language Models (VLMs) are increasingly deployed in real-time
applications such as autonomous driving and human-computer interaction, which
demand fast and reliable responses based on accurate perception. To meet these
requirements, existing systems commonly employ cloud-edge collaborative
architectures, such as partitioned Large Vision-Language Models (LVLMs) or task
offloading strategies between Large and Small Vision-Language Models (SVLMs).
However, these methods fail to accommodate cloud latency fluctuations and
overlook the full potential of delayed but accurate LVLM responses. In this
work, we propose a novel cloud-edge collaborative paradigm for VLMs, termed
Context Transfer, which treats the delayed outputs of LVLMs as historical
context to provide real-time guidance for SVLMs inference. Based on this
paradigm, we design SpotVLM, which incorporates both context replacement and
visual focus modules to refine historical textual input and enhance visual
grounding consistency. Extensive experiments on three real-time vision tasks
across four datasets demonstrate the effectiveness of the proposed framework.
The new paradigm lays the groundwork for more effective and latency-aware
collaboration strategies in future VLM systems.

</details>


### [95] [Synthesizing Accurate and Realistic T1-weighted Contrast-Enhanced MR Images using Posterior-Mean Rectified Flow](https://arxiv.org/abs/2508.12640)
*Bastian Brandstötter,Erich Kobler*

Main category: cs.CV

TL;DR: 一种两阶段流基模型管道，通过后验均值预测和流精细化，从非对比前T1杂强度MRI生成对比前T1杂强度MRI，避免使甠镁毒性对比前


<details>
  <summary>Details</summary>
Motivation: 对比前增强MRI在神经脱粘病学诊断中至关重要，但需要使甠镁基对比前，这会增加成本和扫描时间，造成环境问题，并可能对患者造成风险

Method: 两阶段流基模型管道：首先用片基3D U-Net预测像素后验均值（最小化MSE），然后通过时间条件化3D流模型进行精细化，以结合现实细节和结构保真性

Result: 在360个测试集数据上，最佳精细输出达到轴向FID 12.46和KID 0.007（比后验均值降低68.7%），保持低体积MSE 0.057（比后验均值高出27%），能够实际恢复病变边缘和血管细节

Conclusion: 该方法有效解决了临床部署中的感知-形变損失的调和问题，为无镁对比前增强MRI生成提供了可行方案

Abstract: Contrast-enhanced (CE) T1-weighted MRI is central to neuro-oncologic
diagnosis but requires gadolinium-based agents, which add cost and scan time,
raise environmental concerns, and may pose risks to patients. In this work, we
propose a two-stage Posterior-Mean Rectified Flow (PMRF) pipeline for
synthesizing volumetric CE brain MRI from non-contrast inputs. First, a
patch-based 3D U-Net predicts the voxel-wise posterior mean (minimizing MSE).
Then, this initial estimate is refined by a time-conditioned 3D rectified flow
to incorporate realistic textures without compromising structural fidelity. We
train this model on a multi-institutional collection of paired pre- and
post-contrast T1w volumes (BraTS 2023-2025). On a held-out test set of 360
diverse volumes, our best refined outputs achieve an axial FID of $12.46$ and
KID of $0.007$ ($\sim 68.7\%$ lower FID than the posterior mean) while
maintaining low volumetric MSE of $0.057$ ($\sim 27\%$ higher than the
posterior mean). Qualitative comparisons confirm that our method restores
lesion margins and vascular details realistically, effectively navigating the
perception-distortion trade-off for clinical deployment.

</details>


### [96] [Learn Faster and Remember More: Balancing Exploration and Exploitation for Continual Test-time Adaptation](https://arxiv.org/abs/2508.12643)
*Pinci Yang,Peisong Wen,Ke Ma,Qianqian Xu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种在持续测试时适配(CTTA)中平衡探索和利用的方法BEE，通过多层次一致性正则化和补充锐回放机制来提升适配速度和知识保持能力。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法在平衡探索新域和利用历史知识方面遇到两大挑战：1)域偏移主要影响浅层特征，而现有方法从深层预测调整效果差；2)单一模型在探索时容易遗忘历史域知识。

Method: 提出了一种基于mean teacher框架的BEE方法：1)多层次一致性正则化(MCR)捐失，对齐学生和教师模型的中间特征；2)补充锐回放(CAR)机制，重用历史检查点来恢复多样化域的补充知识。

Result: 在多个标准数据集上的实验表明，该方法显著超越了现有最先进方法。

Conclusion: BEE方法通过MCR和CAR机制有效解决了CTTA中探索与利用的平衡问题，为持续测试时适配任务提供了有效的解决方案。

Abstract: Continual Test-Time Adaptation (CTTA) aims to adapt a source pre-trained
model to continually changing target domains during inference. As a fundamental
principle, an ideal CTTA method should rapidly adapt to new domains
(exploration) while retaining and exploiting knowledge from previously
encountered domains to handle similar domains in the future. Despite
significant advances, balancing exploration and exploitation in CTTA is still
challenging: 1) Existing methods focus on adjusting predictions based on
deep-layer outputs of neural networks. However, domain shifts typically affect
shallow features, which are inefficient to be adjusted from deep predictions,
leading to dilatory exploration; 2) A single model inevitably forgets knowledge
of previous domains during the exploration, making it incapable of exploiting
historical knowledge to handle similar future domains. To address these
challenges, this paper proposes a mean teacher framework that strikes an
appropriate Balance between Exploration and Exploitation (BEE) during the CTTA
process. For the former challenge, we introduce a Multi-level Consistency
Regularization (MCR) loss that aligns the intermediate features of the student
and teacher models, accelerating adaptation to the current domain. For the
latter challenge, we employ a Complementary Anchor Replay (CAR) mechanism to
reuse historical checkpoints (anchors), recovering complementary knowledge for
diverse domains. Experiments show that our method significantly outperforms
state-of-the-art methods on several benchmarks, demonstrating its effectiveness
for CTTA tasks.

</details>


### [97] [DyCrowd: Towards Dynamic Crowd Reconstruction from a Large-scene Video](https://arxiv.org/abs/2508.12644)
*Hao Wen,Hongbo Kang,Jian Ma,Jing Huang,Yuanwang Yang,Haozhe Lin,Yu-Kun Lai,Kun Li*

Main category: cs.CV

TL;DR: DyCrowd是首个从大场景视频中进行时空一致3D人群重建的框架，通过群体引导的运动优化策略和VAE运动先验，解决了遮挡和时序不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法从静态图像重建3D人群，缺乏时序一致性且无法有效处理遮挡问题，需要开发能够处理大场景动态人群重建的解决方案。

Method: 采用粗到细的群体引导运动优化策略，结合VAE人体运动先验和分段群体引导优化，利用异步运动一致性损失和集体行为来解决长期动态遮挡问题。

Result: 实验结果表明该方法在大场景动态人群重建任务中达到了最先进的性能水平。

Conclusion: 提出的DyCrowd框架能够有效处理大场景视频中的动态人群重建，解决了遮挡和时序一致性问题，并贡献了VirtualCrowd虚拟基准数据集。

Abstract: 3D reconstruction of dynamic crowds in large scenes has become increasingly
important for applications such as city surveillance and crowd analysis.
However, current works attempt to reconstruct 3D crowds from a static image,
causing a lack of temporal consistency and inability to alleviate the typical
impact caused by occlusions. In this paper, we propose DyCrowd, the first
framework for spatio-temporally consistent 3D reconstruction of hundreds of
individuals' poses, positions and shapes from a large-scene video. We design a
coarse-to-fine group-guided motion optimization strategy for occlusion-robust
crowd reconstruction in large scenes. To address temporal instability and
severe occlusions, we further incorporate a VAE (Variational Autoencoder)-based
human motion prior along with a segment-level group-guided optimization. The
core of our strategy leverages collective crowd behavior to address long-term
dynamic occlusions. By jointly optimizing the motion sequences of individuals
with similar motion segments and combining this with the proposed Asynchronous
Motion Consistency (AMC) loss, we enable high-quality unoccluded motion
segments to guide the motion recovery of occluded ones, ensuring robust and
plausible motion recovery even in the presence of temporal desynchronization
and rhythmic inconsistencies. Additionally, in order to fill the gap of no
existing well-annotated large-scene video dataset, we contribute a virtual
benchmark dataset, VirtualCrowd, for evaluating dynamic crowd reconstruction
from large-scene videos. Experimental results demonstrate that the proposed
method achieves state-of-the-art performance in the large-scene dynamic crowd
reconstruction task. The code and dataset will be available for research
purposes.

</details>


### [98] [Stable Diffusion-Based Approach for Human De-Occlusion](https://arxiv.org/abs/2508.12663)
*Seung Young Noh,Ju Yong Chang*

Main category: cs.CV

TL;DR: 提出了一种两阶段的人体去遮挡方法，先通过扩散模型和关节热图完成遮挡掩码，再利用稳定扩散模型和人类特定文本特征完成RGB重建，有效解决了严重遮挡下的人体外观恢复问题。


<details>
  <summary>Details</summary>
Motivation: 人类能够利用先验知识和可见线索推断被遮挡物体的缺失部分，但让深度学习模型准确预测被遮挡区域仍然具有挑战性。现有方法在处理人体遮挡时存在像素级退化等问题。

Method: 两阶段方法：1) 掩码补全阶段使用基于扩散的人体先验和遮挡关节热图重建完整掩码；2) RGB补全阶段使用稳定扩散模型，以重建掩码为条件，并加入VQA模型提取的人类特定文本特征，通过解码器微调避免像素级退化。

Result: 该方法在严重遮挡下有效重建人体外观，在掩码和RGB补全方面均优于现有方法，生成的去遮挡图像能够提升下游任务（如2D姿态估计和3D人体重建）的性能。

Conclusion: 提出的两阶段人体去遮挡方法通过结合扩散先验、空间线索和人类特定特征，成功解决了遮挡区域重建的挑战，为人体中心任务提供了有效的预处理方案。

Abstract: Humans can infer the missing parts of an occluded object by leveraging prior
knowledge and visible cues. However, enabling deep learning models to
accurately predict such occluded regions remains a challenging task.
De-occlusion addresses this problem by reconstructing both the mask and RGB
appearance. In this work, we focus on human de-occlusion, specifically
targeting the recovery of occluded body structures and appearances. Our
approach decomposes the task into two stages: mask completion and RGB
completion. The first stage leverages a diffusion-based human body prior to
provide a comprehensive representation of body structure, combined with
occluded joint heatmaps that offer explicit spatial cues about missing regions.
The reconstructed amodal mask then serves as a conditioning input for the
second stage, guiding the model on which areas require RGB reconstruction. To
further enhance RGB generation, we incorporate human-specific textual features
derived using a visual question answering (VQA) model and encoded via a CLIP
encoder. RGB completion is performed using Stable Diffusion, with decoder
fine-tuning applied to mitigate pixel-level degradation in visible regions -- a
known limitation of prior diffusion-based de-occlusion methods caused by latent
space transformations. Our method effectively reconstructs human appearances
even under severe occlusions and consistently outperforms existing methods in
both mask and RGB completion. Moreover, the de-occluded images generated by our
approach can improve the performance of downstream human-centric tasks, such as
2D pose estimation and 3D human reconstruction. The code will be made publicly
available.

</details>


### [99] [WP-CLIP: Leveraging CLIP to Predict Wölfflin's Principles in Visual Art](https://arxiv.org/abs/2508.12668)
*Abhijay Ghildyal,Li-Yun Wang,Feng Liu*

Main category: cs.CV

TL;DR: 本文研究了CLIP模型是否能够理解和预测Wölfflin的五个艺术风格原则，发现预训练CLIP无法捕捉这些细微风格元素，因此通过微调创建了WP-CLIP模型，在GAN生成画作和Pandora-18K数据集上验证了其跨风格泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有指标无法有效预测Wölfflin的五个艺术风格原则，而视觉语言模型在评估抽象图像属性方面显示出潜力，需要探索CLIP模型是否能够理解和预测这些艺术风格元素。

Method: 在真实艺术图像的标注数据集上微调预训练的CLIP模型，为每个Wölfflin原则预测评分，构建WP-CLIP模型。

Result: WP-CLIP模型在GAN生成画作和Pandora-18K艺术数据集上表现出良好的泛化能力，能够有效预测Wölfflin的五个艺术风格原则。

Conclusion: 视觉语言模型在自动化艺术分析方面具有巨大潜力，微调后的CLIP模型能够成功捕捉和理解复杂的艺术风格元素。

Abstract: W\"olfflin's five principles offer a structured approach to analyzing
stylistic variations for formal analysis. However, no existing metric
effectively predicts all five principles in visual art. Computationally
evaluating the visual aspects of a painting requires a metric that can
interpret key elements such as color, composition, and thematic choices. Recent
advancements in vision-language models (VLMs) have demonstrated their ability
to evaluate abstract image attributes, making them promising candidates for
this task. In this work, we investigate whether CLIP, pre-trained on
large-scale data, can understand and predict W\"olfflin's principles. Our
findings indicate that it does not inherently capture such nuanced stylistic
elements. To address this, we fine-tune CLIP on annotated datasets of real art
images to predict a score for each principle. We evaluate our model, WP-CLIP,
on GAN-generated paintings and the Pandora-18K art dataset, demonstrating its
ability to generalize across diverse artistic styles. Our results highlight the
potential of VLMs for automated art analysis.

</details>


### [100] [Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation](https://arxiv.org/abs/2508.12680)
*Yuheng Zha,Kun Zhou,Yujia Wu,Yushu Wang,Jie Feng,Zhi Xu,Shibo Hao,Zhengzhong Liu,Eric P. Xing,Zhiting Hu*

Main category: cs.CV

TL;DR: 提出了Vision-G1视觉推理模型，通过构建包含46个数据源、8个维度的综合RL数据集，采用影响函数数据选择和难度过滤策略，结合多轮RL训练和数据课程学习，在多个视觉推理基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的训练主要集中在数学和逻辑推理等有限任务上，缺乏跨领域的泛化能力，主要原因是超出这些狭窄领域的高质量可验证奖励数据稀缺，且多领域数据整合困难。

Method: 构建了涵盖8个维度（信息图表、数学、空间、跨图像、图形用户界面、医学、常识和通用科学）的综合RL数据集；采用影响函数进行数据选择和基于难度的过滤策略；使用多轮RL训练和数据课程学习来迭代提升模型能力。

Result: Vision-G1模型在各种视觉推理基准测试中实现了最先进的性能，超越了同规模的视觉语言模型，甚至优于GPT-4o和Gemini-1.5 Flash等专有模型。

Conclusion: 通过构建全面的多领域数据集和创新的训练策略，成功提升了视觉语言模型的跨领域推理能力，证明了该方法在提升视觉推理性能方面的有效性。

Abstract: Despite their success, current training pipelines for reasoning VLMs focus on
a limited range of tasks, such as mathematical and logical reasoning. As a
result, these models face difficulties in generalizing their reasoning
capabilities to a wide range of domains, primarily due to the scarcity of
readily available and verifiable reward data beyond these narrowly defined
areas. Moreover, integrating data from multiple domains is challenging, as the
compatibility between domain-specific datasets remains uncertain. To address
these limitations, we build a comprehensive RL-ready visual reasoning dataset
from 46 data sources across 8 dimensions, covering a wide range of tasks such
as infographic, mathematical, spatial, cross-image, graphic user interface,
medical, common sense and general science. We propose an influence function
based data selection and difficulty based filtering strategy to identify
high-quality training samples from this dataset. Subsequently, we train the
VLM, referred to as Vision-G1, using multi-round RL with a data curriculum to
iteratively improve its visual reasoning capabilities. Our model achieves
state-of-the-art performance across various visual reasoning benchmarks,
outperforming similar-sized VLMs and even proprietary models like GPT-4o and
Gemini-1.5 Flash. The model, code and dataset are publicly available at
https://github.com/yuh-zha/Vision-G1.

</details>


### [101] [Refine-and-Contrast: Adaptive Instance-Aware BEV Representations for Multi-UAV Collaborative Object Detection](https://arxiv.org/abs/2508.12684)
*Zhongyao Li,Peirui Cheng,Liangjin Zhao,Chen Chen,Yundu Li,Zhechao Wang,Xue Yang,Xian Sun,Zhirui Wang*

Main category: cs.CV

TL;DR: AdaBEV是一个多无人机协同3D检测框架，通过自适应实例感知的BEV表示学习，实现了精度与计算效率的优越平衡。


<details>
  <summary>Details</summary>
Motivation: 多无人机协同3D检测虽然能通过多视角融合提供准确鲁棒的感知，但在资源受限的无人机平台上计算负担重，需要更高效的方法。

Method: 提出Box-Guided Refinement Module (BG-RM) 仅精炼前景实例相关的BEV网格，以及Instance-Background Contrastive Learning (IBCL) 通过对比学习增强前景背景特征分离。

Result: 在Air-Co-Pred数据集上，AdaBEV在不同模型规模下都实现了优越的精度-计算权衡，在低分辨率下超越其他SOTA方法，接近上限性能。

Conclusion: AdaBEV通过自适应的实例感知BEV表示学习，有效解决了多无人机3D检测中的计算效率问题，同时保持了高精度性能。

Abstract: Multi-UAV collaborative 3D detection enables accurate and robust perception
by fusing multi-view observations from aerial platforms, offering significant
advantages in coverage and occlusion handling, while posing new challenges for
computation on resource-constrained UAV platforms. In this paper, we present
AdaBEV, a novel framework that learns adaptive instance-aware BEV
representations through a refine-and-contrast paradigm. Unlike existing methods
that treat all BEV grids equally, AdaBEV introduces a Box-Guided Refinement
Module (BG-RM) and an Instance-Background Contrastive Learning (IBCL) to
enhance semantic awareness and feature discriminability. BG-RM refines only BEV
grids associated with foreground instances using 2D supervision and spatial
subdivision, while IBCL promotes stronger separation between foreground and
background features via contrastive learning in BEV space. Extensive
experiments on the Air-Co-Pred dataset demonstrate that AdaBEV achieves
superior accuracy-computation trade-offs across model scales, outperforming
other state-of-the-art methods at low resolutions and approaching upper bound
performance while maintaining low-resolution BEV inputs and negligible
overhead.

</details>


### [102] [TTA-DAME: Test-Time Adaptation with Domain Augmentation and Model Ensemble for Dynamic Driving Conditions](https://arxiv.org/abs/2508.12690)
*Dongjae Jeon,Taeheon Kim,Seongwon Cho,Minhyuk Seo,Jonghyun Choi*

Main category: cs.CV

TL;DR: TTA-DAME通过源域数据增强、域判别器和专门域检测器处理驾驶场景中的天气域迁移，特别是在白天到夜晚的剧烈变化，通过多检测器和NMS提升性能，在SHIFT基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决测试时适应(TTA)在现实驾驶场景中频繁天气域迁移的挑战，特别是应对从白天到夜晚的剧烈域变化。

Method: 利用源域数据增强到目标域，引入域判别器和专门域检测器缓解剧烈域迁移，训练多个检测器并通过非极大值抑制(NMS)整合预测结果。

Result: 在SHIFT基准测试中显示出显著性能提升，验证了方法的有效性。

Conclusion: TTA-DAME方法能够有效处理驾驶场景中的动态域迁移问题，特别是在应对白天到夜晚的剧烈变化方面表现出色。

Abstract: Test-time Adaptation (TTA) poses a challenge, requiring models to dynamically
adapt and perform optimally on shifting target domains. This task is
particularly emphasized in real-world driving scenes, where weather domain
shifts occur frequently. To address such dynamic changes, our proposed method,
TTA-DAME, leverages source domain data augmentation into target domains.
Additionally, we introduce a domain discriminator and a specialized domain
detector to mitigate drastic domain shifts, especially from daytime to
nighttime conditions. To further improve adaptability, we train multiple
detectors and consolidate their predictions through Non-Maximum Suppression
(NMS). Our empirical validation demonstrates the effectiveness of our method,
showing significant performance enhancements on the SHIFT Benchmark.

</details>


### [103] [Multi-Level Knowledge Distillation and Dynamic Self-Supervised Learning for Continual Learning](https://arxiv.org/abs/2508.12692)
*Taeheon Kim,San Kim,Minhyuk Seo,Dongjae Jeon,Wonje Jeong,Jonghyun Choi*

Main category: cs.CV

TL;DR: 本文提出了两种组件（多级知识蒸馏和动态自监督损失）来解决重复类增量学习问题，在CVPR CLVISION挑战赛中获得了第二名。


<details>
  <summary>Details</summary>
Motivation: 传统的类增量学习假设每个任务都包含新类别，而重复类增量学习(CIR)更现实，允许先前训练过的类别在后续任务中重复出现，并且可以访问大量未标注的外部数据。

Method: 1. 多级知识蒸馏(MLKD)：从多个先前模型的特征和logits等多个角度提取知识
2. 动态自监督损失(SSL)：利用未标注数据加速新类学习，通过动态权重保持对主要任务的关注

Result: 所提出的两个组件显著提高了CIR设置下的性能，在CVPR第五届CLVISION挑战赛中获得了第二名。

Conclusion: 通过有效利用未标注数据和多角度知识蒸馏，可以在重复类增量学习场景中同时保持模型的高稳定性和可塑性。

Abstract: Class-incremental with repetition (CIR), where previously trained classes
repeatedly introduced in future tasks, is a more realistic scenario than the
traditional class incremental setup, which assumes that each task contains
unseen classes. CIR assumes that we can easily access abundant unlabeled data
from external sources, such as the Internet. Therefore, we propose two
components that efficiently use the unlabeled data to ensure the high stability
and the plasticity of models trained in CIR setup. First, we introduce
multi-level knowledge distillation (MLKD) that distills knowledge from multiple
previous models across multiple perspectives, including features and logits, so
the model can maintain much various previous knowledge. Moreover, we implement
dynamic self-supervised loss (SSL) to utilize the unlabeled data that
accelerates the learning of new classes, while dynamic weighting of SSL keeps
the focus of training to the primary task. Both of our proposed components
significantly improve the performance in CIR setup, achieving 2nd place in the
CVPR 5th CLVISION Challenge.

</details>


### [104] [Neural Rendering for Sensor Adaptation in 3D Object Detection](https://arxiv.org/abs/2508.12695)
*Felix Embacher,David Holtz,Jonas Uhrig,Marius Cordts,Markus Enzweiler*

Main category: cs.CV

TL;DR: 研究发现不同传感器配置导致自动驾驶3D检测器性能漏流，提出了基于神经渲染的数据适配方案来减少域间差距


<details>
  <summary>Details</summary>
Motivation: 自动驾驶不同车辆类型的传感器配置存在差异，导致训练好的感知模型在新传感器环境下性能漏流，需要解决跨传感器域间差距问题

Method: 创建CamShift数据集模拟SUV和小型车的传感器差异，分析不同3D检测器的稳健性，提出基于神经渲染的数据适配流水线来转换数据集以匹配不同传感器配置

Result: 证明了显著的跨传感器性能漏流，发现基于密集BEV表示的模型更稳健，神经渲染适配方案能大幅提升所有检测器的性能

Conclusion: 传感器配置差异对自动驾驶3D检测有重大影响，神经渲染技术提供了高效的数据重用方案，减少了重新收集数据的需求

Abstract: Autonomous vehicles often have varying camera sensor setups, which is
inevitable due to restricted placement options for different vehicle types.
Training a perception model on one particular setup and evaluating it on a new,
different sensor setup reveals the so-called cross-sensor domain gap, typically
leading to a degradation in accuracy. In this paper, we investigate the impact
of the cross-sensor domain gap on state-of-the-art 3D object detectors. To this
end, we introduce CamShift, a dataset inspired by nuScenes and created in CARLA
to specifically simulate the domain gap between subcompact vehicles and sport
utility vehicles (SUVs). Using CamShift, we demonstrate significant
cross-sensor performance degradation, identify robustness dependencies on model
architecture, and propose a data-driven solution to mitigate the effect. On the
one hand, we show that model architectures based on a dense Bird's Eye View
(BEV) representation with backward projection, such as BEVFormer, are the most
robust against varying sensor configurations. On the other hand, we propose a
novel data-driven sensor adaptation pipeline based on neural rendering, which
can transform entire datasets to match different camera sensor setups. Applying
this approach improves performance across all investigated 3D object detectors,
mitigating the cross-sensor domain gap by a large margin and reducing the need
for new data collection by enabling efficient data reusability across vehicles
with different sensor setups. The CamShift dataset and the sensor adaptation
benchmark are available at https://dmholtz.github.io/camshift/.

</details>


### [105] [Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection](https://arxiv.org/abs/2508.12711)
*Fanxiao Li,Jiaying Wu,Tingchao Fu,Yunyun Dong,Bingbing Song,Wei Zhou*

Main category: cs.CV

TL;DR: GenAI驱动的新闻多样性导致多级漂移，显著降低了现有LVLM多模态虚假信息检测系统的性能，需要更鲁棒的检测方法


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具产生的新闻内容多样性给多模态虚假信息检测带来了新挑战，需要系统研究这种多样性对检测系统的影响

Method: 构建DriftBench大规模基准数据集（16,000个新闻实例，6种多样化类别），设计三个评估任务：真实性验证鲁棒性、对抗性证据污染敏感性、推理一致性分析

Result: 六个最先进的LVLM检测器性能显著下降（平均F1下降14.8%），推理轨迹不稳定，在对抗性证据注入下表现更差

Conclusion: 现有MMD系统存在根本性漏洞，在GenAI时代迫切需要更具弹性的检测方法

Abstract: The proliferation of multimodal misinformation poses growing threats to
public discourse and societal trust. While Large Vision-Language Models (LVLMs)
have enabled recent progress in multimodal misinformation detection (MMD), the
rise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven
news diversity, characterized by highly varied and complex content. We show
that this diversity induces multi-level drift, comprising (1) model-level
misperception drift, where stylistic variations disrupt a model's internal
reasoning, and (2) evidence-level drift, where expression diversity degrades
the quality or relevance of retrieved external evidence. These drifts
significantly degrade the robustness of current LVLM-based MMD systems. To
systematically study this problem, we introduce DriftBench, a large-scale
benchmark comprising 16,000 news instances across six categories of
diversification. We design three evaluation tasks: (1) robustness of truth
verification under multi-level drift; (2) susceptibility to adversarial
evidence contamination generated by GenAI; and (3) analysis of reasoning
consistency across diverse inputs. Experiments with six state-of-the-art
LVLM-based detectors show substantial performance drops (average F1 -14.8%) and
increasingly unstable reasoning traces, with even more severe failures under
adversarial evidence injection. Our findings uncover fundamental
vulnerabilities in existing MMD systems and suggest an urgent need for more
resilient approaches in the GenAI era.

</details>


### [106] [Real-Time Sign Language Gestures to Speech Transcription using Deep Learning](https://arxiv.org/abs/2508.12713)
*Brandone Fonya*

Main category: cs.CV

TL;DR: 基于CNN的实时手语识别系统，通过摄像头捕捉手势并转换为文本和语音输出，帮助听障人士沟通


<details>
  <summary>Details</summary>
Motivation: 解决听力和言语障碍人士在日常环境中的沟通障碍，提升他们的自主性和社会融入

Method: 使用卷积神经网络(CNN)在Sign Language MNIST数据集上训练，实时通过摄像头捕捉手势并进行分类识别，结合文本转语音技术

Result: 实验显示系统具有高准确性和实时性能（存在一些延迟），证明其作为辅助工具的实用性和可靠性

Conclusion: 该系统为手语使用者提供了一个易用、可靠的实时沟通辅助工具，有效提升了他们在不同社交场景中的沟通能力

Abstract: Communication barriers pose significant challenges for individuals with
hearing and speech impairments, often limiting their ability to effectively
interact in everyday environments. This project introduces a real-time
assistive technology solution that leverages advanced deep learning techniques
to translate sign language gestures into textual and audible speech. By
employing convolution neural networks (CNN) trained on the Sign Language MNIST
dataset, the system accurately classifies hand gestures captured live via
webcam. Detected gestures are instantaneously translated into their
corresponding meanings and transcribed into spoken language using
text-to-speech synthesis, thus facilitating seamless communication.
Comprehensive experiments demonstrate high model accuracy and robust real-time
performance with some latency, highlighting the system's practical
applicability as an accessible, reliable, and user-friendly tool for enhancing
the autonomy and integration of sign language users in diverse social settings.

</details>


### [107] [Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score](https://arxiv.org/abs/2508.12718)
*Syed Muhmmad Israr,Feng Zhao*

Main category: cs.CV

TL;DR: 提出了Dual Contrastive Denoising Score框架，利用文本到图像扩散模型的生成先验，通过双对比损失实现真实图像编辑，既能灵活修改内容又能保持结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在编辑真实图像时面临两个挑战：用户难以准确描述图像细节的文本提示，以及模型在修改特定区域时会意外改变不需要修改的区域。

Method: 基于对比学习方法，在潜在扩散模型中引入双对比损失，利用自注意力层的空间信息，无需额外网络即可实现图像到图像转换。

Result: 通过大量实验证明，该方法在真实图像编辑方面优于现有方法，能够保持预训练文本到图像扩散模型的能力而无需额外训练。

Conclusion: 提出的框架成功解决了真实图像编辑中的关键问题，实现了内容修改和结构保持的平衡，为零样本图像到图像转换提供了有效解决方案。

Abstract: Large-scale text-to-image generative models have shown remarkable ability to
synthesize diverse and high-quality images. However, it is still challenging to
directly apply these models for editing real images for two reasons. First, it
is difficult for users to come up with a perfect text prompt that accurately
describes every visual detail in the input image. Second, while existing models
can introduce desirable changes in certain regions, they often dramatically
alter the input content and introduce unexpected changes in unwanted regions.
To address these challenges, we present Dual Contrastive Denoising Score, a
simple yet powerful framework that leverages the rich generative prior of
text-to-image diffusion models. Inspired by contrastive learning approaches for
unpaired image-to-image translation, we introduce a straightforward dual
contrastive loss within the proposed framework. Our approach utilizes the
extensive spatial information from the intermediate representations of the
self-attention layers in latent diffusion models without depending on auxiliary
networks. Our method achieves both flexible content modification and structure
preservation between input and output images, as well as zero-shot
image-to-image translation. Through extensive experiments, we show that our
approach outperforms existing methods in real image editing while maintaining
the capability to directly utilize pretrained text-to-image diffusion models
without further training.

</details>


### [108] [Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting](https://arxiv.org/abs/2508.12720)
*Kangjie Chen,Yingji Zhong,Zhihao Li,Jiaqi Lin,Youyu Chen,Minghan Qin,Haoqian Wang*

Main category: cs.CV

TL;DR: 本文分析了稀疏视图3D高斯泼溅中的外观伪影问题，发现高斯之间的过度纠缠（共适应）是主要原因，提出了量化指标CA和两种轻量级解决方案。


<details>
  <summary>Details</summary>
Motivation: 3DGS在密集视图下表现优异，但在稀疏视图场景中会出现外观伪影，需要探究其根本原因并提出解决方案。

Method: 提出共适应评分(CA)指标来量化高斯纠缠程度，并设计两种轻量级策略：随机高斯丢弃和透明度乘性噪声注入。

Result: 分析表明共适应程度随训练视图数量增加而自然缓解，提出的两种策略在各种方法和基准测试中验证有效。

Conclusion: 对共适应效应的深入理解将有助于社区更全面地认识稀疏视图3DGS，提出的轻量级解决方案可有效缓解外观伪影问题。

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel
view synthesis under dense-view settings. However, in sparse-view scenarios,
despite the realistic renderings in training views, 3DGS occasionally manifests
appearance artifacts in novel views. This paper investigates the appearance
artifacts in sparse-view 3DGS and uncovers a core limitation of current
approaches: the optimized Gaussians are overly-entangled with one another to
aggressively fit the training views, which leads to a neglect of the real
appearance distribution of the underlying scene and results in appearance
artifacts in novel views. The analysis is based on a proposed metric, termed
Co-Adaptation Score (CA), which quantifies the entanglement among Gaussians,
i.e., co-adaptation, by computing the pixel-wise variance across multiple
renderings of the same viewpoint, with different random subsets of Gaussians.
The analysis reveals that the degree of co-adaptation is naturally alleviated
as the number of training views increases. Based on the analysis, we propose
two lightweight strategies to explicitly mitigate the co-adaptation in
sparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise
injection to the opacity. Both strategies are designed to be plug-and-play, and
their effectiveness is validated across various methods and benchmarks. We hope
that our insights into the co-adaptation effect will inspire the community to
achieve a more comprehensive understanding of sparse-view 3DGS.

</details>


### [109] [Frequency-Driven Inverse Kernel Prediction for Single Image Defocus Deblurring](https://arxiv.org/abs/2508.12736)
*Ying Zhang,Xiongxin Tang,Chongyi Li,Qiao Chen,Yuquan Wu*

Main category: cs.CV

TL;DR: 提出FDIKP网络，通过频率域表示和双分支逆核预测策略，结合位置自适应卷积和双域尺度循环模块，显著提升单图像散焦去模糊性能


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖空间特征进行核估计，但在严重模糊区域由于局部高频细节缺失导致性能下降，需要利用频率域对模糊建模的优越判别能力

Method: FDIKP网络包含：1）双分支逆核预测策略（DIKP）提升核估计精度和稳定性；2）位置自适应卷积（PAC）增强反卷积适应性；3）双域尺度循环模块（DSRM）融合反卷积结果并逐步提升去模糊质量

Result: 大量实验表明该方法优于现有方法

Conclusion: 通过频率域表示和创新的网络架构设计，有效解决了散焦去模糊中空间变化模糊核建模的关键挑战

Abstract: Single image defocus deblurring aims to recover an all-in-focus image from a
defocus counterpart, where accurately modeling spatially varying blur kernels
remains a key challenge. Most existing methods rely on spatial features for
kernel estimation, but their performance degrades in severely blurry regions
where local high-frequency details are missing. To address this, we propose a
Frequency-Driven Inverse Kernel Prediction network (FDIKP) that incorporates
frequency-domain representations to enhance structural identifiability in
kernel modeling. Given the superior discriminative capability of the frequency
domain for blur modeling, we design a Dual-Branch Inverse Kernel Prediction
(DIKP) strategy that improves the accuracy of kernel estimation while
maintaining stability. Moreover, considering the limited number of predicted
inverse kernels, we introduce a Position Adaptive Convolution (PAC) to enhance
the adaptability of the deconvolution process. Finally, we propose a
Dual-Domain Scale Recurrent Module (DSRM) to fuse deconvolution results and
progressively improve deblurring quality from coarse to fine. Extensive
experiments demonstrate that our method outperforms existing approaches. Code
will be made publicly available.

</details>


### [110] [DCSCR: A Class-Specific Collaborative Representation based Network for Image Set Classification](https://arxiv.org/abs/2508.12745)
*Xizhan Gao,Wei Hu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种深度类别特徚协同表征网络(DCSCR)，用于解决少样本图像集分类中的特征表征和集间相似性计算问题。


<details>
  <summary>Details</summary>
Motivation: 图像集分类中存在两个关键挑战：如何学习有效特征表征和如何探索不同图像集间的相似性。传统方法忽视特征学习，而现有深度方法在测量集距离时无法自适应调整特征。

Method: 结合传统图像集分类方法与深度模型，提出DCSCR网络。包含三个模块：全卷积深度特征提取器、全局特征学习模块和类别特征协同表征基于的距离学习模块。前两个模块学习帧级特征表征，后者学习概念级特征表征并通过新的CSCR对比损失函数获取集间相似性。

Result: 在多个知名的少样本图像集分类数据集上进行了广泛实验，结果表明该方法相比于一些最先进的图像集分类算法更有效。

Conclusion: DCSCR能够同时学习图像集的帧级和概念级特征表征，以及不同集之间的距离相似性，有效解决了少样本图像集分类中的关键挑战。

Abstract: Image set classification (ISC), which can be viewed as a task of comparing
similarities between sets consisting of unordered heterogeneous images with
variable quantities and qualities, has attracted growing research attention in
recent years. How to learn effective feature representations and how to explore
the similarities between different image sets are two key yet challenging
issues in this field. However, existing traditional ISC methods classify image
sets based on raw pixel features, ignoring the importance of feature learning.
Existing deep ISC methods can learn deep features, but they fail to adaptively
adjust the features when measuring set distances, resulting in limited
performance in few-shot ISC. To address the above issues, this paper combines
traditional ISC methods with deep models and proposes a novel few-shot ISC
approach called Deep Class-specific Collaborative Representation (DCSCR)
network to simultaneously learn the frame- and concept-level feature
representations of each image set and the distance similarities between
different sets. Specifically, DCSCR consists of a fully convolutional deep
feature extractor module, a global feature learning module, and a
class-specific collaborative representation-based metric learning module. The
deep feature extractor and global feature learning modules are used to learn
(local and global) frame-level feature representations, while the
class-specific collaborative representation-based metric learning module is
exploit to adaptively learn the concept-level feature representation of each
image set and thus obtain the distance similarities between different sets by
developing a new CSCR-based contrastive loss function. Extensive experiments on
several well-known few-shot ISC datasets demonstrate the effectiveness of the
proposed method compared with some state-of-the-art image set classification
algorithms.

</details>


### [111] [D2-Mamba: Dual-Scale Fusion and Dual-Path Scanning with SSMs for Shadow Removal](https://arxiv.org/abs/2508.12750)
*Linhao Li,Boya Jin,Zizhe Li,Lanqing Guo,Hao Cheng,Bo Li,Yongfeng Dong*

Main category: cs.CV

TL;DR: 提出一种基于Mamba的双谱融合和双路扫描等网络结构，通过选择性传播上下文信息和适应性模型化来改善阴影移除效果


<details>
  <summary>Details</summary>
Motivation: 阴影移除任务中，阴影区域的修复变换与光照良好区域存在显著差异，统一的修复策略效果不佳，需要有效整合非局部上下文线索和适应性地模型区域特定变换

Method: 设计了双谱融合Mamba块(DFMB)和双路Mamba组(DPMG)，前者通过融合原始特征与低分辨率特征来提升多尺度特征表示和减少边界假影，后者通过水平扫描获取全局特征并采用面具感知的适应扫描策略来改善结构连续性和细粒度区域模型

Result: 在阴影移除标准数据集上，方法显著超越了现有的最先进方法

Conclusion: 提出的Mamba基网结构通过双谱融合和双路扫描等技术，能够有效地选择性传播上下文信息并适应性地模型区域特定变换，从而在阴影移除任务中取得了更优的效果

Abstract: Shadow removal aims to restore images that are partially degraded by shadows,
where the degradation is spatially localized and non-uniform. Unlike general
restoration tasks that assume global degradation, shadow removal can leverage
abundant information from non-shadow regions for guidance. However, the
transformation required to correct shadowed areas often differs significantly
from that of well-lit regions, making it challenging to apply uniform
correction strategies. This necessitates the effective integration of non-local
contextual cues and adaptive modeling of region-specific transformations. To
this end, we propose a novel Mamba-based network featuring dual-scale fusion
and dual-path scanning to selectively propagate contextual information based on
transformation similarity across regions. Specifically, the proposed Dual-Scale
Fusion Mamba Block (DFMB) enhances multi-scale feature representation by fusing
original features with low-resolution features, effectively reducing boundary
artifacts. The Dual-Path Mamba Group (DPMG) captures global features via
horizontal scanning and incorporates a mask-aware adaptive scanning strategy,
which improves structural continuity and fine-grained region modeling.
Experimental results demonstrate that our method significantly outperforms
existing state-of-the-art approaches on shadow removal benchmarks.

</details>


### [112] [CLAIRE-DSA: Fluoroscopic Image Classification for Quality Assurance of Computer Vision Pipelines in Acute Ischemic Stroke](https://arxiv.org/abs/2508.12755)
*Cristo J. van den Berg,Frank G. te Nijenhuis,Mirre J. Blaauboer,Daan T. W. van Erp,Carlijn M. Keppels,Matthijs van der Sluijs,Bob Roozenbeek,Wim van Zwam,Sandra Cornelissen,Danny Ruijters,Ruisheng Su,Theo van Walsum*

Main category: cs.CV

TL;DR: CLAIRE-DSA是一个基于深度学习的框架，用于在急性缺血性卒中机械取栓术中分类数字减影血管造影图像的关键属性，显著提高下游分割任务的性能。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉模型在机械取栓术中可以提供辅助，但图像质量差会降低性能。需要一种自动化工具来评估和分类DSA图像质量，支持质量控制和工作流程优化。

Method: 使用预训练的ResNet骨干网络进行微调，预测9个图像属性（如对比度存在、投影角度、运动伪影严重程度等）。在包含1,758个荧光镜最小强度投影的标注数据集上训练单独的分类器。

Result: 模型在所有标签上表现优异，ROC-AUC范围为0.91到0.98，精确度范围为0.70到1.00。在分割任务中，通过过滤低质量图像，分割成功率从42%提高到69%（p < 0.001）。

Conclusion: CLAIRE-DSA作为自动化工具在准确分类急性缺血性卒中患者DSA序列图像属性方面表现出强大潜力，支持临床和研究应用中的图像标注和质量控制。

Abstract: Computer vision models can be used to assist during mechanical thrombectomy
(MT) for acute ischemic stroke (AIS), but poor image quality often degrades
performance. This work presents CLAIRE-DSA, a deep learning--based framework
designed to categorize key image properties in minimum intensity projections
(MinIPs) acquired during MT for AIS, supporting downstream quality control and
workflow optimization. CLAIRE-DSA uses pre-trained ResNet backbone models,
fine-tuned to predict nine image properties (e.g., presence of contrast,
projection angle, motion artefact severity). Separate classifiers were trained
on an annotated dataset containing $1,758$ fluoroscopic MinIPs. The model
achieved excellent performance on all labels, with ROC-AUC ranging from $0.91$
to $0.98$, and precision ranging from $0.70$ to $1.00$. The ability of
CLAIRE-DSA to identify suitable images was evaluated on a segmentation task by
filtering poor quality images and comparing segmentation performance on
filtered and unfiltered datasets. Segmentation success rate increased from
$42%$ to $69%$, $p < 0.001$. CLAIRE-DSA demonstrates strong potential as an
automated tool for accurately classifying image properties in DSA series of
acute ischemic stroke patients, supporting image annotation and quality control
in clinical and research applications. Source code is available at
https://gitlab.com/icai-stroke-lab/wp3_neurointerventional_ai/claire-dsa.

</details>


### [113] [Harnessing Group-Oriented Consistency Constraints for Semi-Supervised Semantic Segmentation in CdZnTe Semiconductors](https://arxiv.org/abs/2508.12766)
*Peihao Li,Yan Fang,Man Liu,Huihui Bai,Anhong Wang,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: 这篇论文提出了一种专门处理CdZnTe半导体图像的半监督语义分割方法ICAF，通过组内一致性增帽框架解决了低对比度缺陷边界标注难题，在仅使用2组标注数据的情况下达到了70.6% mIoU的高精度。


<details>
  <summary>Details</summary>
Motivation: 因为CdZnTe半导体图像存在低对比度缺陷边界问题，需要注释者参考多个视图来确定真实标签，形成了独特的"多对一"关系。现有的半监督语义分割方法基于"一对一"关系，在这种场景下效果不佳，容易在低对比度区域产生错误积累和确认偏差。

Method: 提出了组向视角的内部一致性增帽框架(ICAF)。首先通过组内视图采样(IVS)建立基线模型，然后组建伪标签纠正网络(PCN)。PCN包含两个模块：视图增帽模块(VAM)通过聚合多个视图动态合成边界敏感视图改善边界细节；视图纠正模块(VCM)将合成视图与其他视图进行信息交互，强化显著区域并减少噪声。

Result: 在CdZnTe数据集上进行了涉广实验，使用DeepLabV3+与ResNet-101作为分割模型。在仅使用2组标注数据(约5丯之一)的情况下，达到了70.6% mIoU的高精度。

Conclusion: ICAF框架通过组内一致性增帽有效解决了CdZnTe半导体图像分割中的"多对一"标注挑战，显著提升了在极少标注数据下的分割性能，为类似的多视图共享真实标签的语义分割问题提供了有效解决方案。

Abstract: Labeling Cadmium Zinc Telluride (CdZnTe) semiconductor images is challenging
due to the low-contrast defect boundaries, necessitating annotators to
cross-reference multiple views. These views share a single ground truth (GT),
forming a unique ``many-to-one'' relationship. This characteristic renders
advanced semi-supervised semantic segmentation (SSS) methods suboptimal, as
they are generally limited by a ``one-to-one'' relationship, where each image
is independently associated with its GT. Such limitation may lead to error
accumulation in low-contrast regions, further exacerbating confirmation bias.
To address this issue, we revisit the SSS pipeline from a group-oriented
perspective and propose a human-inspired solution: the Intra-group Consistency
Augmentation Framework (ICAF). First, we experimentally validate the inherent
consistency constraints within CdZnTe groups, establishing a group-oriented
baseline using the Intra-group View Sampling (IVS). Building on this insight,
we introduce the Pseudo-label Correction Network (PCN) to enhance consistency
representation, which consists of two key modules. The View Augmentation Module
(VAM) improves boundary details by dynamically synthesizing a boundary-aware
view through the aggregation of multiple views. In the View Correction Module
(VCM), this synthesized view is paired with other views for information
interaction, effectively emphasizing salient regions while minimizing noise.
Extensive experiments demonstrate the effectiveness of our solution for CdZnTe
materials. Leveraging DeepLabV3+ with a ResNet-101 backbone as our segmentation
model, we achieve a 70.6\% mIoU on the CdZnTe dataset using only 2
group-annotated data (5\textperthousand). The code is available at
\href{https://github.com/pipixiapipi/ICAF}{https://github.com/pipixiapipi/ICAF}.

</details>


### [114] [SocialTrack: Multi-Object Tracking in Complex Urban Traffic Scenes Inspired by Social Behavior](https://arxiv.org/abs/2508.12777)
*Wenguang Tao,Xiaotian Wang,Tian Yan,Jie Yan,Guodong Li,Kun Bai*

Main category: cs.CV

TL;DR: SocialTrack是一个针对无人机视角下多目标跟踪的创新框架，通过多尺度特征增强、速度自适应卡尔曼滤波、群体运动补偿和时空记忆预测等技术，显著提升了复杂城市交通环境中小目标的跟踪精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 无人机视角下的多目标跟踪在智能交通系统分析中具有重要价值，但复杂场景中的小目标尺度变化、遮挡、非线性交叉运动和运动模糊等问题严重影响了跟踪稳定性。

Method: 提出SocialTrack框架，包含：1）多尺度特征增强的小目标检测器；2）速度自适应容积卡尔曼滤波(VACKF)用于轨迹预测；3）群体运动补偿策略(GMCS)建模社会群体运动先验；4）时空记忆预测(STMP)利用历史轨迹信息预测未来状态。

Result: 在UAVDT和MOT17数据集上的实验表明，SocialTrack在多个关键指标上优于现有最先进方法，特别是在MOTA和IDF1等核心性能指标上有显著提升。

Conclusion: SocialTrack框架在复杂无人机视角下的小目标跟踪中表现出优异的鲁棒性和适应性，且具有高度模块化和兼容性，可与现有跟踪器无缝集成以进一步提升性能。

Abstract: As a key research direction in the field of multi-object tracking (MOT),
UAV-based multi-object tracking has significant application value in the
analysis and understanding of urban intelligent transportation systems.
However, in complex UAV perspectives, challenges such as small target scale
variations, occlusions, nonlinear crossing motions, and motion blur severely
hinder the stability of multi-object tracking. To address these challenges,
this paper proposes a novel multi-object tracking framework, SocialTrack, aimed
at enhancing the tracking accuracy and robustness of small targets in complex
urban traffic environments. The specialized small-target detector enhances the
detection performance by employing a multi-scale feature enhancement mechanism.
The Velocity Adaptive Cubature Kalman Filter (VACKF) improves the accuracy of
trajectory prediction by incorporating a velocity dynamic modeling mechanism.
The Group Motion Compensation Strategy (GMCS) models social group motion priors
to provide stable state update references for low-quality tracks, significantly
improving the target association accuracy in complex dynamic environments.
Furthermore, the Spatio-Temporal Memory Prediction (STMP) leverages historical
trajectory information to predict the future state of low-quality tracks,
effectively mitigating identity switching issues. Extensive experiments on the
UAVDT and MOT17 datasets demonstrate that SocialTrack outperforms existing
state-of-the-art (SOTA) methods across several key metrics. Significant
improvements in MOTA and IDF1, among other core performance indicators,
highlight its superior robustness and adaptability. Additionally, SocialTrack
is highly modular and compatible, allowing for seamless integration with
existing trackers to further enhance performance.

</details>


### [115] [Leveraging Diffusion Models for Stylization using Multiple Style Images](https://arxiv.org/abs/2508.12784)
*Dan Ruta,Abdelaziz Djelouah,Raphael Ortiz,Christopher Schroers*

Main category: cs.CV

TL;DR: 基于多样式图片的潜在扩散模型风格转换方法，通过图像提示适配器和统计对齐技术，在去噪UNet的交叉注意力和自注意力层进行干预，实现了更准确的风格匹配和内容-风格解耦合。


<details>
  <summary>Details</summary>
Motivation: 现有风格转换方法在准确匹配风格、支持的样式图片数量、内容-风格解耦合方面仍遇到困难，需要解决这些问题提升性能。

Method: 利用多样式图片表征风格特征，设计了结合图像提示适配器和去噪过程中特征统计对齐的方法，在UNet的交叉注意力和自注意力层进行干预，使用聚类技术从样式样本中提炼小代表性注意力特征集。

Result: 该方法在风格化任务中达到了最先进的结果。

Conclusion: 通过多样式图片、图像提示适配器和统计对齐技术的结合，成功提升了风格转换的准确性和效果，解决了现有方法的主要限制。

Abstract: Recent advances in latent diffusion models have enabled exciting progress in
image style transfer. However, several key issues remain. For example, existing
methods still struggle to accurately match styles. They are often limited in
the number of style images that can be used. Furthermore, they tend to entangle
content and style in undesired ways. To address this, we propose leveraging
multiple style images which helps better represent style features and prevent
content leaking from the style images. We design a method that leverages both
image prompt adapters and statistical alignment of the features during the
denoising process. With this, our approach is designed such that it can
intervene both at the cross-attention and the self-attention layers of the
denoising UNet. For the statistical alignment, we employ clustering to distill
a small representative set of attention features from the large number of
attention values extracted from the style samples. As demonstrated in our
experimental section, the resulting method achieves state-of-the-art results
for stylization.

</details>


### [116] [Vehicle detection from GSV imagery: Predicting travel behaviour for cycling and motorcycling using Computer Vision](https://arxiv.org/abs/2508.12794)
*Kyriaki,Kokka,Rahul Goel,Ali Abbas,Kerry A. Nice,Luca Martial,SM Labib,Rihuan Ke,Carola Bibiane Schönlieb,James Woodcock*

Main category: cs.CV

TL;DR: 使用深度学习分析Google街景图像来估算全球各城市的骑行和摩托车使用水平，得到了与传统调查数据高度相关的结果


<details>
  <summary>Details</summary>
Motivation: 交通方式影响健康，但全球范围内的骑行和摩托车行为数据缺乏，需要一种高效的方法来收集这些数据

Method: 使用YOLOv4模型分析185个城市的Google街景图像（每个城市8000张图片），识别自行车和摩托车，然后使用beta回归模型预测交通方式分享率

Result: 模型检测自行车和摩托车的准确率达89%，预测模型的R²值分别为0.614（骑行）和0.612（摩托车），中位绝对误差分别为1.3%和1.4%

Conclusion: 通过计算机视觉分析Google街景图像可以高效地获取交通行为数据，为中东、拉丁美洲和东亚等地区的城市提供了可靠的估算结果

Abstract: Transportation influence health by shaping exposure to physical activity, air
pollution and injury risk.Comparative data on cycling and motorcycling
behaviours is scarce, particularly at a global scale.Street view imagery, such
as Google Street View (GSV), combined with computer vision, is a valuable
resource for efficiently capturing travel behaviour data.This study
demonstrates a novel approach using deep learning on street view images to
estimate cycling and motorcycling levels across diverse cities worldwide.We
utilized data from 185 global cities.The data on mode shares of cycling and
motorcycling estimated using travel surveys or censuses.We used GSV images to
detect cycles and motorcycles in sampled locations, using 8000 images per
city.The YOLOv4 model, fine-tuned using images from six cities, achieved a mean
average precision of 89% for detecting cycles and motorcycles in GSV images.A
global prediction model was developed using beta regression with city-level
mode shares as outcome, with log transformed explanatory variables of counts of
GSV-detected images with cycles and motorcycles, while controlling for
population density.We found strong correlations between GSV motorcycle counts
and motorcycle mode share (0.78) and moderate correlations between GSV cycle
counts and cycling mode share (0.51).Beta regression models predicted mode
shares with $R^2$ values of 0.614 for cycling and 0.612 for motorcycling,
achieving median absolute errors (MDAE) of 1.3% and 1.4%,
respectively.Scatterplots demonstrated consistent prediction accuracy, though
cities like Utrecht and Cali were outliers.The model was applied to 60 cities
globally for which we didn't have recent mode share data.We provided estimates
for some cities in the Middle East, Latin America and East Asia.With computer
vision, GSV images capture travel modes and activity, providing insights
alongside traditional data sources.

</details>


### [117] [Morphological classification of eclipsing binary stars using computer vision methods](https://arxiv.org/abs/2508.12802)
*Štefan Parimucha,Maksim Gabdeev,Yanna Markus,Martin Vaňko,Pavol Gajdoš*

Main category: cs.CV

TL;DR: 使用预训练卷积神经网络和视觉Transformer模型，通过极坐标转换和hexbin可视化技术，对蔽双星光变曲进行分离双星和过接触双星的分类，准确率达96%以上。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够自动分析大规模天文调查数据中蔽双星光变曲的计算机视觉方法，提高分类效率和准确性。

Method: 采用预训练的ResNet50和vision transformer模型，通过极坐标转换和hexbin可视化技术处理阶段折叠光变曲图像，使用层次分类方法先分类双星系统类型，再识别斑点存在性。

Result: 在Gaia G、I和TESS等多个通道上验证数据达到>96%的高准确率，在OGLE、DEBCat和WUMaCat观测数据上表现优异（>94%，TESS达100%），但自动斑点检测性能差。

Conclusion: 计算机视觉在大规模蔽双星形态分类中展现巨大潜力，但需要进一步研究更稳健的自动斑点检测方法。

Abstract: We present an application of computer vision methods to classify the light
curves of eclipsing binaries (EB). We have used pre-trained models based on
convolutional neural networks ($\textit{ResNet50}$) and vision transformers
($\textit{vit\_base\_patch16\_224}$), which were fine-tuned on images created
from synthetic datasets. To improve model generalisation and reduce
overfitting, we developed a novel image representation by transforming
phase-folded light curves into polar coordinates combined with hexbin
visualisation. Our hierarchical approach in the first stage classifies systems
into detached and overcontact types, and in the second stage identifies the
presence or absence of spots. The binary classification models achieved high
accuracy ($>96\%$) on validation data across multiple passbands (Gaia~$G$, $I$,
and $TESS$) and demonstrated strong performance ($>94\%$, up to $100\%$ for
$TESS$) when tested on extensive observational data from the OGLE, DEBCat, and
WUMaCat catalogues. While the primary binary classification was highly
successful, the secondary task of automated spot detection performed poorly,
revealing a significant limitation of our models for identifying subtle
photometric features. This study highlights the potential of computer vision
for EB morphological classification in large-scale surveys, but underscores the
need for further research into robust, automated spot detection.

</details>


### [118] [Next Visual Granularity Generation](https://arxiv.org/abs/2508.12811)
*Yikai Wang,Zhouxia Wang,Zhonghua Wu,Qingyi Tao,Kang Liao,Chen Change Loy*

Main category: cs.CV

TL;DR: 提出了一种新的图像生成方法NVG，通过将图像分解为结构化序列，从全局布局到细节逐步细化生成，在ImageNet上实现了优于VAR系列的FID分数


<details>
  <summary>Details</summary>
Motivation: 传统图像生成方法缺乏对生成过程的多粒度控制，需要一种能够从粗到细逐步细化的结构化生成框架

Method: 将图像分解为共享相同空间分辨率但使用不同数量token的结构化序列，通过Next Visual Granularity (NVG)框架从空图像开始逐步生成视觉粒度序列

Result: 在ImageNet数据集上训练的NVG模型显示出明显的扩展性，FID分数优于VAR系列（3.30->3.03, 2.57->2.44, 2.09->2.06）

Conclusion: NVG框架提供了跨多个粒度级别的细粒度控制能力，展示了分层表示的优势，代码和模型将公开发布

Abstract: We propose a novel approach to image generation by decomposing an image into
a structured sequence, where each element in the sequence shares the same
spatial resolution but differs in the number of unique tokens used, capturing
different level of visual granularity. Image generation is carried out through
our newly introduced Next Visual Granularity (NVG) generation framework, which
generates a visual granularity sequence beginning from an empty image and
progressively refines it, from global layout to fine details, in a structured
manner. This iterative process encodes a hierarchical, layered representation
that offers fine-grained control over the generation process across multiple
granularity levels. We train a series of NVG models for class-conditional image
generation on the ImageNet dataset and observe clear scaling behavior. Compared
to the VAR series, NVG consistently outperforms it in terms of FID scores (3.30
-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to
showcase the capability and potential of the NVG framework. Our code and models
will be released.

</details>


### [119] [SIS-Challenge: Event-based Spatio-temporal Instance Segmentation Challenge at the CVPR 2025 Event-based Vision Workshop](https://arxiv.org/abs/2508.12813)
*Friedhelm Hamann,Emil Mededovic,Fabian Gülhan,Yuli Wu,Johannes Stegmaier,Jing He,Yiqing Wang,Kexin Zhang,Lingling Li,Licheng Jiao,Mengru Ma,Hongxiang Huang,Yuhao Yan,Hongwei Ren,Xiaopeng Lin,Yulong Huang,Bojun Cheng,Se Hyun Lee,Gyu Sung Ham,Kanghan Oh,Gi Hyun Lim,Boxuan Yang,Bowen Du,Guillermo Gallego*

Main category: cs.CV

TL;DR: CVPR 2025事件视觉研讨会举办的时空实例分割挑战赛概述，包含任务定义、数据集、挑战细节和结果分析，以及前5名团队的方法介绍


<details>
  <summary>Details</summary>
Motivation: 推动事件相机和灰度相机数据融合的时空实例分割技术发展，为计算机视觉社区提供标准基准和评估平台

Method: 基于时空对齐的事件相机和灰度相机数据，预测精确的像素级分割掩码，定义了具体的对象类别分割任务

Result: 提供了完整的挑战赛结果分析，展示了前5名团队的性能表现和方法特点

Conclusion: 该挑战赛成功推动了多模态时空实例分割技术的发展，为研究者提供了宝贵的基准数据和创新方法参考

Abstract: We present an overview of the Spatio-temporal Instance Segmentation (SIS)
challenge held in conjunction with the CVPR 2025 Event-based Vision Workshop.
The task is to predict accurate pixel-level segmentation masks of defined
object classes from spatio-temporally aligned event camera and grayscale camera
data. We provide an overview of the task, dataset, challenge details and
results. Furthermore, we describe the methods used by the top-5 ranking teams
in the challenge. More resources and code of the participants' methods are
available here:
https://github.com/tub-rip/MouseSIS/blob/main/docs/challenge_results.md

</details>


### [120] [DEEP-SEA: Deep-Learning Enhancement for Environmental Perception in Submerged Aquatics](https://arxiv.org/abs/2508.12824)
*Shuang Chen,Ronald Thenius,Farshad Arvin,Amir Atapour-Abarghouei*

Main category: cs.CV

TL;DR: DEEP-SEA是一个基于深度学习的海底图像恢复模型，通过双频增强自注意力机制在空间和频率域中自适应优化特征表示，有效解决水下环境的光散射、吸收和浑浊问题，提升图像清晰度和色彩准确性。


<details>
  <summary>Details</summary>
Motivation: 水下监测平台依赖视觉数据进行海洋生物多样性分析和生态评估，但水下环境的光散射、吸收和浑浊问题严重降低图像质量，影响准确观测。

Method: 提出DEEP-SEA模型，采用双频增强自注意力空间和频率调制器，在频率域和空间域同时自适应优化特征表示，保持空间结构的同时增强低频和高频信息。

Result: 在EUVP和LSUI数据集上的综合实验表明，该模型在恢复精细图像细节和结构一致性方面优于现有最先进方法。

Conclusion: DEEP-SEA通过有效减轻水下视觉退化，有潜力提高水下监测平台的可靠性，实现更准确的生态观测、物种识别和自主导航。

Abstract: Continuous and reliable underwater monitoring is essential for assessing
marine biodiversity, detecting ecological changes and supporting autonomous
exploration in aquatic environments. Underwater monitoring platforms rely on
mainly visual data for marine biodiversity analysis, ecological assessment and
autonomous exploration. However, underwater environments present significant
challenges due to light scattering, absorption and turbidity, which degrade
image clarity and distort colour information, which makes accurate observation
difficult. To address these challenges, we propose DEEP-SEA, a novel deep
learning-based underwater image restoration model to enhance both low- and
high-frequency information while preserving spatial structures. The proposed
Dual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator aims to
adaptively refine feature representations in frequency domains and
simultaneously spatial information for better structural preservation. Our
comprehensive experiments on EUVP and LSUI datasets demonstrate the superiority
over the state of the art in restoring fine-grained image detail and structural
consistency. By effectively mitigating underwater visual degradation, DEEP-SEA
has the potential to improve the reliability of underwater monitoring platforms
for more accurate ecological observation, species identification and autonomous
navigation.

</details>


### [121] [Multi-source Multimodal Progressive Domain Adaption for Audio-Visual Deception Detection](https://arxiv.org/abs/2508.12842)
*Ronghao Lin,Sijie Mai,Ying Zeng,Qiaolin He,Aolin Xiong,Haifeng Hu*

Main category: cs.CV

TL;DR: 提出MMPDA框架解决多模态欺骗检测中的领域偏移问题，通过渐进式领域适应方法在特征和决策层面进行对齐，在MMDD挑战赛中取得Top-2成绩


<details>
  <summary>Details</summary>
Motivation: 解决多模态欺骗检测中源域和目标域之间的领域偏移问题，特别是在多样化的多模态数据集之间

Method: 多源多模态渐进式领域适应(MMPDA)框架，通过在特征和决策层面逐步对齐源域和目标域来转移音频-视觉知识

Result: 在竞赛第二阶段达到60.43%准确率和56.99% F1分数，F1分数比第一名高5.59%，准确率比第三名高6.75%

Conclusion: MMPDA框架有效解决了跨域多模态欺骗检测问题，证明了渐进式领域适应方法在处理多样化多模态数据集领域偏移方面的有效性

Abstract: This paper presents the winning approach for the 1st MultiModal Deception
Detection (MMDD) Challenge at the 1st Workshop on Subtle Visual Computing
(SVC). Aiming at the domain shift issue across source and target domains, we
propose a Multi-source Multimodal Progressive Domain Adaptation (MMPDA)
framework that transfers the audio-visual knowledge from diverse source domains
to the target domain. By gradually aligning source and the target domain at
both feature and decision levels, our method bridges domain shifts across
diverse multimodal datasets. Extensive experiments demonstrate the
effectiveness of our approach securing Top-2 place. Our approach reaches 60.43%
on accuracy and 56.99\% on F1-score on competition stage 2, surpassing the 1st
place team by 5.59% on F1-score and the 3rd place teams by 6.75% on accuracy.
Our code is available at https://github.com/RH-Lin/MMPDA.

</details>


### [122] [Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization with Vision-Language Models](https://arxiv.org/abs/2508.12861)
*Dexia Chen,Wentao Zhang,Qianjie Zhu,Ping Hu,Weibing Li,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: 通过多视角特征提取咄一致性约束机制，CoMuCo方法提升了视觉-语言模型在跨领域少样本识别任务中的性能


<details>
  <summary>Details</summary>
Motivation: 现有的VLM迅速迁移学习方法在标准图像数据集上表现良好，但在跨领域任务中效果有限，因为成像领域与自然图像差异较大

Method: 提出CoMuCo策略，使用两个功能互补的专家模块提取多视角特征，结合先验知识的一致性约束咄信息几何基于的共识机制来增强特征学习的稳健性

Result: 在旧的咄新建的跨领域少样本测试集上，CoMuCo方法都一致超过了当前的最佳方法

Conclusion: CoMuCo通过多视角协同优化咄一致性约束，有效解决了VLM在跨领域少样本识别中的挑战，为跨领域小样本学习提供了新的解决方案

Abstract: Vision-language models (VLMs) pre-trained on natural image and language data,
such as CLIP, have exhibited significant potential in few-shot image
recognition tasks, leading to development of various efficient transfer
learning methods. These methods exploit inherent pre-learned knowledge in VLMs
and have achieved strong performance on standard image datasets. However, their
effectiveness is often limited when confronted with cross-domain tasks where
imaging domains differ from natural images. To address this limitation, we
propose Consistency-guided Multi-view Collaborative Optimization (CoMuCo), a
novel fine-tuning strategy for VLMs. This strategy employs two functionally
complementary expert modules to extract multi-view features, while
incorporating prior knowledge-based consistency constraints and information
geometry-based consensus mechanisms to enhance the robustness of feature
learning. Additionally, a new cross-domain few-shot benchmark is established to
help comprehensively evaluate methods on imaging domains distinct from natural
images. Extensive empirical evaluations on both existing and newly proposed
benchmarks suggest CoMuCo consistently outperforms current methods in few-shot
tasks. The code and benchmark will be released.

</details>


### [123] [Preserve and Sculpt: Manifold-Aligned Fine-tuning of Vision-Language Models for Few-Shot Learning](https://arxiv.org/abs/2508.12877)
*Dexia Chen,Qianjie Zhu,Weibing Li,Yue Yu,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: 基于预训练视觉-语言模型的精调方法MPS-Tuning，通过保持语义流形的几何结构和增强类别可分辨性来提升少样本分类性能


<details>
  <summary>Details</summary>
Motivation: 现有的参数效率微调或一致性约束方法忽视了数据分布的几何结构，导致整体语义表征的扭曲

Method: MPS-Tuning方法将特征空间中的数据分布视为语义流形，通过对齐微调前后特征的Gram矩阵来保持流形的宏观和微观拓扑结构，同时优化图像和文本模态的成对相似性来增强类别可分辨性

Result: 大量实验证明MPS-Tuning显著提升了模型性能，同时有效保持了语义流形的结构

Conclusion: MPS-Tuning通过显式约束语义流形的内在几何结构和增强类别可分辨性，为基于预训练VLMs的少样本分类提供了有效的精调方法

Abstract: Pretrained vision-language models (VLMs), such as CLIP, have shown remarkable
potential in few-shot image classification and led to numerous effective
transfer learning strategies. These methods leverage the pretrained knowledge
of VLMs to enable effective domain adaptation while mitigating overfitting
through parameter-efficient tuning or instance-based consistency constraints.
However, such regularizations often neglect the geometric structure of data
distribution, which may lead to distortion of the overall semantic
representation. To overcome this limitation, we propose a novel fine-tuning
method, Manifold-Preserving and Sculpting Tuning (MPS-Tuning). Regarding the
data distribution in feature space as a semantic manifold, MPS-Tuning
explicitly constrains the intrinsic geometry of this manifold while further
sculpting it to enhance class separability. Specifically, MPS-Tuning preserves
both macroscopic and microscopic topological structures of the original
manifold by aligning Gram matrices of features before and after fine-tuning.
Theoretically, this constraint is shown to approximate an upper bound of the
Gromov-Wasserstein distance. Furthermore, features from the image and text
modalities are paired, and pairwise similarities are optimized to enhance the
manifold's class discriminability. Extensive experiments demonstrate that
MPS-Tuning significantly improves model performance while effectively
preserving the structure of the semantic manifold. The code will be released.

</details>


### [124] [S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models](https://arxiv.org/abs/2508.12880)
*Chubin Chen,Jiashu Zhu,Xiaokun Feng,Nisha Huang,Meiqi Wu,Fangyuan Mao,Jiahong Wu,Xiangxiang Chu,Xiu Li*

Main category: cs.CV

TL;DR: 通过对CFG方法的实证分析，发现其产生次优预测导致语义不一致和输出质量低的问题，提出S^2-Guidance方法通过随机块投弃构建子网络来精炼预测，在文本到图像/视频生成任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 因为现代扩散模型中广泛使用的分类器免指导(CFG)技术存在产生次优预测的问题，导致语义不一致和低质量输出，需要找到更有效的指导方法。

Method: 提出S^2-Guidance方法，利用随机块投弃在前向过程中构建随机子网络，通过这些子网络精炼模型的次优预测，指导模型避免潜在的低质量预测并向高质量输出发展。

Result: 在文本到图像和文本到视频生成任务上进行了广泛的定性和定量实验，结果显示S^2-Guidance能够提供更优秀的性能，一贯超过CFG和其他先进的指导策略。

Conclusion: S^2-Guidance通过利用模型自身的子网络来精炼次优预测，有效解决了CFG方法存在的问题，在多个生成任务中都显示出优异的性能，为扩散模型的指导技术提供了新的解决方案。

Abstract: Classifier-free Guidance (CFG) is a widely used technique in modern diffusion
models for enhancing sample quality and prompt adherence. However, through an
empirical analysis on Gaussian mixture modeling with a closed-form solution, we
observe a discrepancy between the suboptimal results produced by CFG and the
ground truth. The model's excessive reliance on these suboptimal predictions
often leads to semantic incoherence and low-quality outputs. To address this
issue, we first empirically demonstrate that the model's suboptimal predictions
can be effectively refined using sub-networks of the model itself. Building on
this insight, we propose S^2-Guidance, a novel method that leverages stochastic
block-dropping during the forward process to construct stochastic sub-networks,
effectively guiding the model away from potential low-quality predictions and
toward high-quality outputs. Extensive qualitative and quantitative experiments
on text-to-image and text-to-video generation tasks demonstrate that
S^2-Guidance delivers superior performance, consistently surpassing CFG and
other advanced guidance strategies. Our code will be released.

</details>


### [125] [ONG: One-Shot NMF-based Gradient Masking for Efficient Model Sparsification](https://arxiv.org/abs/2508.12891)
*Sankar Behera,Yamuna Prasad*

Main category: cs.CV

TL;DR: ONG是一种基于非负矩阵分解的一次性剪枝方法，通过梯度掩码在训练过程中严格保持目标稀疏度，在CIFAR数据集上取得了与现有方法相当或更好的性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络规模庞大导致部署困难，现有剪枝方法存在迭代过程复杂、需要专门标准或在训练中难以有效保持稀疏度的问题。

Method: 使用非负矩阵分解(NMF)一次性识别重要权重结构进行剪枝，然后采用精确的梯度掩码机制确保只有未剪枝权重被更新，严格保持目标稀疏度。

Result: 在CIFAR-10和CIFAR-100数据集上使用ResNet56、ResNet34和ResNet18进行测试，ONG在各种稀疏度水平下都能达到相当或更优的性能，同时保持剪枝后的结构完整性。

Conclusion: ONG提供了一种有效的一次性剪枝解决方案，能够精确控制目标稀疏度，在训练过程中严格保持稀疏结构，性能表现优异。

Abstract: Deep Neural Networks (DNNs) have achieved remarkable success but their large
size poses deployment challenges. While various pruning techniques exist, many
involve complex iterative processes, specialized criteria, or struggle to
maintain sparsity effectively during training. We introduce ONG (One-shot
NMF-based Gradient Masking), a novel sparsification strategy that identifies
salient weight structures using Non-negative Matrix Factorization (NMF) for
one-shot pruning at the outset of training. Subsequently, ONG employs a precise
gradient masking mechanism to ensure that only unpruned weights are updated,
strictly preserving the target sparsity throughout the training phase. We
integrate ONG into the BIMP comparative framework and evaluate it on CIFAR-10
and CIFAR-100 with ResNet56, ResNet34, and ResNet18 against established stable
sparsification methods. Our experiments demonstrate ONG's ability to achieve
comparable or superior performance at various sparsity levels while maintaining
structural integrity post-pruning and offering a clear mechanism for targeting
desired sparsities.

</details>


### [126] [CTFlow: Video-Inspired Latent Flow Matching for 3D CT Synthesis](https://arxiv.org/abs/2508.12900)
*Jiayi Wang,Hadrien Reynaud,Franciskus Xaverius Erick,Bernhard Kainz*

Main category: cs.CV

TL;DR: CTFlow是一个基于临床报告生成完整3D CT体积的0.5B潜在流匹配变换器模型，在时间一致性、图像多样性和文本-图像对齐方面优于现有方法


<details>
  <summary>Details</summary>
Motivation: 通过临床报告条件生成整个CT体积可以加速医学研究，实现数据增强、隐私保护合成，同时减少对患者数据的监管限制，同时保留诊断信号

Method: 使用FLUX的A-VAE定义潜在空间，CT-Clip文本编码器编码临床报告，采用自定义自回归方法生成一致的整个CT体积，先基于纯文本预测第一个切片序列，然后基于先前生成的切片序列和文本来预测后续序列

Result: 在FID、FVD、IS分数和CLIP分数等指标上，CTFlow在时间一致性、图像多样性和文本-图像对齐方面优于最先进的生成CT模型

Conclusion: CTFlow展示了基于临床报告生成高质量3D CT体积的可行性，为医学影像数据增强和隐私保护应用提供了有效解决方案

Abstract: Generative modelling of entire CT volumes conditioned on clinical reports has
the potential to accelerate research through data augmentation,
privacy-preserving synthesis and reducing regulator-constraints on patient data
while preserving diagnostic signals. With the recent release of CT-RATE, a
large-scale collection of 3D CT volumes paired with their respective clinical
reports, training large text-conditioned CT volume generation models has become
achievable. In this work, we introduce CTFlow, a 0.5B latent flow matching
transformer model, conditioned on clinical reports. We leverage the A-VAE from
FLUX to define our latent space, and rely on the CT-Clip text encoder to encode
the clinical reports. To generate consistent whole CT volumes while keeping the
memory constraints tractable, we rely on a custom autoregressive approach,
where the model predicts the first sequence of slices of the volume from
text-only, and then relies on the previously generated sequence of slices and
the text, to predict the following sequence. We evaluate our results against
state-of-the-art generative CT model, and demonstrate the superiority of our
approach in terms of temporal coherence, image diversity and text-image
alignment, with FID, FVD, IS scores and CLIP score.

</details>


### [127] [CMF-IoU: Multi-Stage Cross-Modal Fusion 3D Object Detection with IoU Joint Prediction](https://arxiv.org/abs/2508.12917)
*Zhiwei Ning,Zhaojiang Liu,Xuanang Gao,Yifan Zuo,Jie Yang,Yuming Fang,Wei Liu*

Main category: cs.CV

TL;DR: 一种多步骤跨模态融合的3D检测框架CMF-IOU，通过伪点云、双边增强背骡网络和迭代粒度池化等技术，有效解决3D空间与2D语义信息对齐挑战，在多个数据集上表现优异


<details>
  <summary>Details</summary>
Motivation: 现有多模态检测方法主要集中在单阶段或部分融合，导致特征提取不充分和性能次优

Method: 1）通过深度补全网络将像素信息投影到3D空间生成伪点云 2）设计双边跨视图增强3D背骡网络（S2D分支和ResVC分支） 3）迭代粒度-点知觉细粒度池化模块 4）集成IoU联合预测分支和新的建议框生成技术

Result: 在KITTI、nuScenes和Waymo数据集上进行了涉及广法实验，显示方法具有优异的性能

Conclusion: CMF-IOU框架通过多步骤跨模态融合有效解决了3D空间与2D语义信息对齐问题，在多个标准数据集上达到了领先水平

Abstract: Multi-modal methods based on camera and LiDAR sensors have garnered
significant attention in the field of 3D detection. However, many prevalent
works focus on single or partial stage fusion, leading to insufficient feature
extraction and suboptimal performance. In this paper, we introduce a
multi-stage cross-modal fusion 3D detection framework, termed CMF-IOU, to
effectively address the challenge of aligning 3D spatial and 2D semantic
information. Specifically, we first project the pixel information into 3D space
via a depth completion network to get the pseudo points, which unifies the
representation of the LiDAR and camera information. Then, a bilateral
cross-view enhancement 3D backbone is designed to encode LiDAR points and
pseudo points. The first sparse-to-distant (S2D) branch utilizes an
encoder-decoder structure to reinforce the representation of sparse LiDAR
points. The second residual view consistency (ResVC) branch is proposed to
mitigate the influence of inaccurate pseudo points via both the 3D and 2D
convolution processes. Subsequently, we introduce an iterative voxel-point
aware fine grained pooling module, which captures the spatial information from
LiDAR points and textural information from pseudo points in the proposal
refinement stage. To achieve more precise refinement during iteration, an
intersection over union (IoU) joint prediction branch integrated with a novel
proposals generation technique is designed to preserve the bounding boxes with
both high IoU and classification scores. Extensive experiments show the
superior performance of our method on the KITTI, nuScenes and Waymo datasets.

</details>


### [128] [7Bench: a Comprehensive Benchmark for Layout-guided Text-to-image Models](https://arxiv.org/abs/2508.12919)
*Elena Izzo,Luca Parolari,Davide Vezzaro,Lamberto Ballan*

Main category: cs.CV

TL;DR: 提出了首个同时评估语义和空间对齐的基准测试7Bench，用于评估布局引导的文本到图像生成模型，涵盖7个挑战性场景并包含新的布局对齐评分方法。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成基准主要关注文本对齐，而忽略了布局对齐评估，这限制了评估模型空间保真度的能力，特别是在合成数据生成等应用中。

Method: 创建包含7个挑战性场景的文本-布局对数据集，提出结合布局对齐分数的评估协议来评估空间准确性。

Result: 使用7Bench评估了多个最先进的扩散模型，揭示了它们在不同对齐任务中的优势和局限性。

Conclusion: 7Bench填补了布局引导文本到图像生成评估的重要空白，为模型的空间保真度评估提供了标准化工具。

Abstract: Layout-guided text-to-image models offer greater control over the generation
process by explicitly conditioning image synthesis on the spatial arrangement
of elements. As a result, their adoption has increased in many computer vision
applications, ranging from content creation to synthetic data generation. A
critical challenge is achieving precise alignment between the image, textual
prompt, and layout, ensuring semantic fidelity and spatial accuracy. Although
recent benchmarks assess text alignment, layout alignment remains overlooked,
and no existing benchmark jointly evaluates both. This gap limits the ability
to evaluate a model's spatial fidelity, which is crucial when using
layout-guided generation for synthetic data, as errors can introduce noise and
degrade data quality. In this work, we introduce 7Bench, the first benchmark to
assess both semantic and spatial alignment in layout-guided text-to-image
generation. It features text-and-layout pairs spanning seven challenging
scenarios, investigating object generation, color fidelity, attribute
recognition, inter-object relationships, and spatial control. We propose an
evaluation protocol that builds on existing frameworks by incorporating the
layout alignment score to assess spatial accuracy. Using 7Bench, we evaluate
several state-of-the-art diffusion models, uncovering their respective
strengths and limitations across diverse alignment tasks. The benchmark is
available at https://github.com/Elizzo/7Bench.

</details>


### [129] [Towards High-Resolution Industrial Image Anomaly Detection](https://arxiv.org/abs/2508.12931)
*Ximiao Zhang,Min Xu,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: HiAD是一个针对高分辨率图像异常检测的通用框架，通过双分支架构和多分辨率特征融合策略，有效检测不同大小的异常区域，在计算资源有限的情况下实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 当前异常检测方法主要针对低分辨率场景，高分辨率图像的传统下采样会导致细粒度判别信息丢失，造成细微异常区域漏检。现有方法在检测精度和效率方面难以满足工业场景实际需求。

Method: 采用双分支架构整合不同尺度的异常线索，结合多分辨率特征融合策略处理高分辨率图像的细粒度纹理变化。使用检测器池和多种检测器分配策略，根据图像块特征自适应分配检测器。

Result: 在专门构建的高分辨率异常检测基准（MVTec-HD、VisA-HD和RealIAD-HD）上进行广泛实验，证明了HiAD的优越性能。

Conclusion: HiAD框架能够有效解决高分辨率图像异常检测中的挑战，在保持检测性能的同时有效控制计算成本，适用于工业实际应用场景。

Abstract: Current anomaly detection methods primarily focus on low-resolution
scenarios. For high-resolution images, conventional downsampling often results
in missed detections of subtle anomalous regions due to the loss of
fine-grained discriminative information. Despite some progress, recent studies
have attempted to improve detection resolution by employing lightweight
networks or using simple image tiling and ensemble methods. However, these
approaches still struggle to meet the practical demands of industrial scenarios
in terms of detection accuracy and efficiency. To address the above issues, we
propose HiAD, a general framework for high-resolution anomaly detection. HiAD
is capable of detecting anomalous regions of varying sizes in high-resolution
images under limited computational resources. Specifically, HiAD employs a
dual-branch architecture that integrates anomaly cues across different scales
to comprehensively capture both subtle and large-scale anomalies. Furthermore,
it incorporates a multi-resolution feature fusion strategy to tackle the
challenges posed by fine-grained texture variations in high-resolution images.
To enhance both adaptability and efficiency, HiAD utilizes a detector pool in
conjunction with various detector assignment strategies, enabling detectors to
be adaptively assigned based on patch features, ensuring detection performance
while effectively controlling computational costs. We conduct extensive
experiments on our specifically constructed high-resolution anomaly detection
benchmarks, including MVTec-HD, VisA-HD, and the real-world benchmark
RealIAD-HD, demonstrating the superior performance of HiAD. The code is
available at https://github.com/cnulab/HiAD.

</details>


### [130] [SEDEG:Sequential Enhancement of Decoder and Encoder's Generality for Class Incremental Learning with Small Memory](https://arxiv.org/abs/2508.12932)
*Hongyang Chen,Shaoling Pu,Lingyu Zheng,Zhongwu Sun*

Main category: cs.CV

TL;DR: SEDEG是一个两阶段训练的ViT增量学习框架，通过特征增强和知识蒸馏技术同时提升编码器和解码器的泛化能力，有效缓解灾难性遗忘问题，特别是在小内存场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的增量学习方法大多只关注编码器或解码器中的一个组件，限制了缓解灾难性遗忘的效果，特别是在小内存场景下表现更差。需要同时提升编码器和解码器的泛化能力来更好地适应动态数据输入。

Method: 提出两阶段训练框架：第一阶段通过特征增强训练集成编码器学习泛化表示，提升解码器泛化能力并平衡分类器；第二阶段使用平衡知识蒸馏和特征知识蒸馏策略压缩集成编码器，开发新的更泛化的编码器。

Result: 在三个基准数据集上的广泛实验显示SEDEG具有优越性能，消融研究确认了各组件有效性。

Conclusion: SEDEG通过同时提升编码器和解码器的泛化能力，有效解决了增量学习中的灾难性遗忘问题，特别是在小内存场景下表现出色。

Abstract: In incremental learning, enhancing the generality of knowledge is crucial for
adapting to dynamic data inputs. It can develop generalized representations or
more balanced decision boundaries, preventing the degradation of long-term
knowledge over time and thus mitigating catastrophic forgetting. Some emerging
incremental learning methods adopt an encoder-decoder architecture and have
achieved promising results. In the encoder-decoder achitecture, improving the
generalization capabilities of both the encoder and decoder is critical, as it
helps preserve previously learned knowledge while ensuring adaptability and
robustness to new, diverse data inputs. However, many existing continual
methods focus solely on enhancing one of the two components, which limits their
effectiveness in mitigating catastrophic forgetting. And these methods perform
even worse in small-memory scenarios, where only a limited number of historical
samples can be stored. To mitigate this limitation, we introduces SEDEG, a
two-stage training framework for vision transformers (ViT), focusing on
sequentially improving the generality of both Decoder and Encoder. Initially,
SEDEG trains an ensembled encoder through feature boosting to learn generalized
representations, which subsequently enhance the decoder's generality and
balance the classifier. The next stage involves using knowledge distillation
(KD) strategies to compress the ensembled encoder and develop a new, more
generalized encoder. This involves using a balanced KD approach and feature KD
for effective knowledge transfer. Extensive experiments on three benchmark
datasets show SEDEG's superior performance, and ablation studies confirm the
efficacy of its components. The code is available at
https://github.com/ShaolingPu/CIL.

</details>


### [131] [Fully Automated Segmentation of Fiber Bundles in Anatomic Tracing Data](https://arxiv.org/abs/2508.12942)
*Kyriaki-Margarita Bintsi,Yaël Balbastre,Jingjing Wu,Julia F. Lehman,Suzanne N. Haber,Anastasia Yendiki*

Main category: cs.CV

TL;DR: 基于U-Net等深度学习技术的全自动化框架，用于猴子神经追踪数据中的纤维束分割，显著提升稀疏束检测能力和减少误检率


<details>
  <summary>Details</summary>
Motivation: 解决神经纤维束手动注释劳动密集的问题，充分利用神经追踪数据来验证和改进滿散磁共振成像技术

Method: 基于U-Net网络构造，采用大补丁尺寸、前景感知采样和半监督预训练等技术

Result: 纤维束检测提升20%以上，假发现率降低40%，避免了将终端误标为束等常见错误

Conclusion: 该框架能够大规模自动分析神经追踪数据，为滿散磁共振成像技术提供更多真实地面验证数据

Abstract: Anatomic tracer studies are critical for validating and improving diffusion
MRI (dMRI) tractography. However, large-scale analysis of data from such
studies is hampered by the labor-intensive process of annotating fiber bundles
manually on histological slides. Existing automated methods often miss sparse
bundles or require complex post-processing across consecutive sections,
limiting their flexibility and generalizability. We present a streamlined,
fully automated framework for fiber bundle segmentation in macaque tracer data,
based on a U-Net architecture with large patch sizes, foreground aware
sampling, and semisupervised pre-training. Our approach eliminates common
errors such as mislabeling terminals as bundles, improves detection of sparse
bundles by over 20% and reduces the False Discovery Rate (FDR) by 40% compared
to the state-of-the-art, all while enabling analysis of standalone slices. This
new framework will facilitate the automated analysis of anatomic tracing data
at a large scale, generating more ground-truth data that can be used to
validate and optimize dMRI tractography methods.

</details>


### [132] [Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models](https://arxiv.org/abs/2508.12945)
*Jianshu Zeng,Yuxuan Liu,Yutong Feng,Chenxuan Miao,Zixiang Gao,Jiwang Qu,Jianzhang Zhang,Bin Wang,Kun Yuan*

Main category: cs.CV

TL;DR: Lumen是一个端到端的视频重光照框架，基于大规模视频生成模型，通过文本描述控制光照和背景，使用混合真实和合成视频数据集进行训练，实现前景保持和时序一致的重光照效果。


<details>
  <summary>Details</summary>
Motivation: 视频重光照是一个具有挑战性但有价值的任务，需要在替换视频背景的同时相应地调整前景光照并实现和谐融合。现有方法缺乏高质量配对视频数据，且难以保持前景原始属性和时序一致性。

Method: 构建大规模混合数据集（合成+真实视频），利用3D渲染引擎生成合成视频对，采用HDR光照模拟补充真实视频。设计联合训练课程，注入域感知适配器来解耦重光照学习和域外观分布学习。

Result: 实验结果表明Lumen能够有效将输入编辑为具有一致光照和严格前景保持的电影级重光照视频，在综合基准测试中优于现有方法。

Conclusion: Lumen框架通过创新的数据集构建和训练策略，成功解决了视频重光照中的前景保持和时序一致性问题，为视频编辑提供了高质量的解决方案。

Abstract: Video relighting is a challenging yet valuable task, aiming to replace the
background in videos while correspondingly adjusting the lighting in the
foreground with harmonious blending. During translation, it is essential to
preserve the original properties of the foreground, e.g., albedo, and propagate
consistent relighting among temporal frames. In this paper, we propose Lumen,
an end-to-end video relighting framework developed on large-scale video
generative models, receiving flexible textual description for instructing the
control of lighting and background. Considering the scarcity of high-qualified
paired videos with the same foreground in various lighting conditions, we
construct a large-scale dataset with a mixture of realistic and synthetic
videos. For the synthetic domain, benefiting from the abundant 3D assets in the
community, we leverage advanced 3D rendering engine to curate video pairs in
diverse environments. For the realistic domain, we adapt a HDR-based lighting
simulation to complement the lack of paired in-the-wild videos. Powered by the
aforementioned dataset, we design a joint training curriculum to effectively
unleash the strengths of each domain, i.e., the physical consistency in
synthetic videos, and the generalized domain distribution in realistic videos.
To implement this, we inject a domain-aware adapter into the model to decouple
the learning of relighting and domain appearance distribution. We construct a
comprehensive benchmark to evaluate Lumen together with existing methods, from
the perspectives of foreground preservation and video consistency assessment.
Experimental results demonstrate that Lumen effectively edit the input into
cinematic relighted videos with consistent lighting and strict foreground
preservation. Our project page: https://lumen-relight.github.io/

</details>


### [133] [MaskSem: Semantic-Guided Masking for Learning 3D Hybrid High-Order Motion Representation](https://arxiv.org/abs/2508.12948)
*Wei Wei,Shaojie Zhang,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: MaskSem是一种新颖的语义引导掩码方法，通过Grad-CAM指导关节掩码，使用混合高阶运动作为重建目标，提升了自监督骨架动作识别的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督骨架动作识别方法主要关注有限关节集和低阶运动模式，限制了模型对复杂运动模式的理解能力。

Method: 提出语义引导掩码方法MaskSem，利用基于相对运动的Grad-CAM指导关节掩码，并使用混合高阶运动（速度和加速度）作为重建目标。

Result: 在NTU60、NTU120和PKU-MMD数据集上的实验表明，MaskSem结合普通transformer能够提升骨架动作识别性能。

Conclusion: 该方法能够更全面地描述动态运动过程，增强对运动模式的理解，更适合人机交互应用。

Abstract: Human action recognition is a crucial task for intelligent robotics,
particularly within the context of human-robot collaboration research. In
self-supervised skeleton-based action recognition, the mask-based
reconstruction paradigm learns the spatial structure and motion patterns of the
skeleton by masking joints and reconstructing the target from unlabeled data.
However, existing methods focus on a limited set of joints and low-order motion
patterns, limiting the model's ability to understand complex motion patterns.
To address this issue, we introduce MaskSem, a novel semantic-guided masking
method for learning 3D hybrid high-order motion representations. This novel
framework leverages Grad-CAM based on relative motion to guide the masking of
joints, which can be represented as the most semantically rich temporal
orgions. The semantic-guided masking process can encourage the model to explore
more discriminative features. Furthermore, we propose using hybrid high-order
motion as the reconstruction target, enabling the model to learn multi-order
motion patterns. Specifically, low-order motion velocity and high-order motion
acceleration are used together as the reconstruction target. This approach
offers a more comprehensive description of the dynamic motion process,
enhancing the model's understanding of motion patterns. Experiments on the
NTU60, NTU120, and PKU-MMD datasets show that MaskSem, combined with a vanilla
transformer, improves skeleton-based action recognition, making it more
suitable for applications in human-robot interaction.

</details>


### [134] [Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination](https://arxiv.org/abs/2508.12957)
*Yizhou Liu,Jingwei Wei,Zizhi Chen,Minghao Han,Xukun Zhang,Keliang Liu,Lihua Zhang*

Main category: cs.CV

TL;DR: ARMed是一个用于开放式医学视觉问答的强化学习框架，通过结合文本正确性和自适应语义奖励来提升医学推理质量，在多个基准测试中显著提高了准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学影像中的强化学习应用不足，现有方法主要针对封闭式视觉问答，而开放式医学VQA更能反映临床实践但研究有限。基于模型的语义奖励存在奖励崩溃问题，即语义差异显著的响应获得相似分数。

Method: ARMed首先通过监督微调在思维链数据上融入领域知识，然后应用强化学习结合文本正确性和自适应语义奖励来增强推理质量。

Result: 在六个医学VQA基准测试中，ARMed在领域内任务上提升了32.64%，在领域外基准上获得了11.65%的增益。

Conclusion: 研究强调了奖励可区分性在医学强化学习中的关键作用，以及语义引导奖励在实现稳健且具有临床意义的多模态推理方面的潜力。

Abstract: Reinforcement learning (RL) with rule-based rewards has demonstrated strong
potential in enhancing the reasoning and generalization capabilities of
vision-language models (VLMs) and large language models (LLMs), while reducing
computational overhead. However, its application in medical imaging remains
underexplored. Existing reinforcement fine-tuning (RFT) approaches in this
domain primarily target closed-ended visual question answering (VQA), limiting
their applicability to real-world clinical reasoning. In contrast, open-ended
medical VQA better reflects clinical practice but has received limited
attention. While some efforts have sought to unify both formats via
semantically guided RL, we observe that model-based semantic rewards often
suffer from reward collapse, where responses with significant semantic
differences receive similar scores. To address this, we propose ARMed (Adaptive
Reinforcement for Medical Reasoning), a novel RL framework for open-ended
medical VQA. ARMed first incorporates domain knowledge through supervised
fine-tuning (SFT) on chain-of-thought data, then applies reinforcement learning
with textual correctness and adaptive semantic rewards to enhance reasoning
quality. We evaluate ARMed on six challenging medical VQA benchmarks. Results
show that ARMed consistently boosts both accuracy and generalization, achieving
a 32.64% improvement on in-domain tasks and an 11.65% gain on out-of-domain
benchmarks. These results highlight the critical role of reward
discriminability in medical RL and the promise of semantically guided rewards
for enabling robust and clinically meaningful multimodal reasoning.

</details>


### [135] [Multi-Phase Automated Segmentation of Dental Structures in CBCT Using a Lightweight Auto3DSeg and SegResNet Implementation](https://arxiv.org/abs/2508.12962)
*Dominic LaBella,Keshav Jha,Jared Robbins,Esther Yu*

Main category: cs.CV

TL;DR: DLaBella29团队在MICCAI 2025 ToothFairy3挑战赛中提出的基于3D SegResNet架构的深度学习管道，用于CBCT牙齿多类别分割，在验证集上达到平均Dice分数0.87


<details>
  <summary>Details</summary>
Motivation: 自动化CBCT牙齿结构分割可有效辅助病理识别（如牙髓或根尖周病变）和头颈癌患者放射治疗规划，提高患者护理质量

Method: 使用MONAI Auto3DSeg框架和3D SegResNet架构，采用5折交叉验证训练，关键预处理包括图像重采样和强度裁剪，采用两阶段分割策略：第一阶段使用Multi-Label STAPLE集成融合，第二阶段在已分割的下颌骨周围进行紧密裁剪以分割小神经结构

Result: 在ToothFairy3挑战赛的样本外验证集上取得了平均Dice分数0.87的优异性能

Conclusion: 该方法展示了自动化牙齿分割在放射肿瘤学中改善患者护理的相关性和实用性，为临床诊断和治疗规划提供了有效的技术支撑

Abstract: Cone-beam computed tomography (CBCT) has become an invaluable imaging
modality in dentistry, enabling 3D visualization of teeth and surrounding
structures for diagnosis and treatment planning. Automated segmentation of
dental structures in CBCT can efficiently assist in identifying pathology
(e.g., pulpal or periapical lesions) and facilitate radiation therapy planning
in head and neck cancer patients. We describe the DLaBella29 team's approach
for the MICCAI 2025 ToothFairy3 Challenge, which involves a deep learning
pipeline for multi-class tooth segmentation. We utilized the MONAI Auto3DSeg
framework with a 3D SegResNet architecture, trained on a subset of the
ToothFairy3 dataset (63 CBCT scans) with 5-fold cross-validation. Key
preprocessing steps included image resampling to 0.6 mm isotropic resolution
and intensity clipping. We applied an ensemble fusion using Multi-Label STAPLE
on the 5-fold predictions to infer a Phase 1 segmentation and then conducted
tight cropping around the easily segmented Phase 1 mandible to perform Phase 2
segmentation on the smaller nerve structures. Our method achieved an average
Dice of 0.87 on the ToothFairy3 challenge out-of-sample validation set. This
paper details the clinical context, data preparation, model development,
results of our approach, and discusses the relevance of automated dental
segmentation for improving patient care in radiation oncology.

</details>


### [136] [GazeDETR: Gaze Detection using Disentangled Head and Gaze Representations](https://arxiv.org/abs/2508.12966)
*Ryan Anthony Jalova de Belen,Gelareh Mohammadi,Arcot Sowmya*

Main category: cs.CV

TL;DR: GazeDETR是一个新颖的端到端架构，使用两个解耦的解码器分别处理头部定位和视线预测任务，在多个数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有端到端视线检测模型使用单一解码器同时定位头部和预测视线，导致表示纠缠。需要解耦这两个任务以获得更好的性能。

Method: 提出GazeDETR架构，包含两个独立的解码器：一个专门用于头部定位（利用局部信息），另一个用于视线预测（结合局部和全局信息），使用连贯注意力场。

Result: 在GazeFollow、VideoAttentionTarget和ChildPlay数据集上达到最先进性能，显著优于现有端到端模型。

Conclusion: 通过任务解耦和专门化解码器设计，GazeDETR有效提升了视线检测的准确性和性能，证明了分离表示学习的重要性。

Abstract: Gaze communication plays a crucial role in daily social interactions.
Quantifying this behavior can help in human-computer interaction and digital
phenotyping. While end-to-end models exist for gaze target detection, they only
utilize a single decoder to simultaneously localize human heads and predict
their corresponding gaze (e.g., 2D points or heatmap) in a scene. This
multitask learning approach generates a unified and entangled representation
for human head localization and gaze location prediction. Herein, we propose
GazeDETR, a novel end-to-end architecture with two disentangled decoders that
individually learn unique representations and effectively utilize coherent
attentive fields for each subtask. More specifically, we demonstrate that its
human head predictor utilizes local information, while its gaze decoder
incorporates both local and global information. Our proposed architecture
achieves state-of-the-art results on the GazeFollow, VideoAttentionTarget and
ChildPlay datasets. It outperforms existing end-to-end models with a notable
margin.

</details>


### [137] [Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation](https://arxiv.org/abs/2508.12969)
*Qirui Li,Guangcong Zheng,Qi Zhao,Jie Li,Bin Dong,Yiwu Yao,Xi Li*

Main category: cs.CV

TL;DR: 通过分析视频波散Transformer的注意力矩阵结构，发现其存在异质性稀疏模式，提出硬件感知的Compact Attention加速框架，实现1.6~2.5倍注意力计算加速而保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制的计算需求给Transformer基于的视频生成带来严重挑战，特别是在合成超长序列时。现有方法无法充分利用视频数据的内在时空冗余性。

Method: 提出Compact Attention加速框架：1)适应性分块策略通过动态分组近似多样化空间交互模式；2)时间变化窗口根据帧距离调整稀疏程度；3)自动化配置搜索算法优化稀疏模式保留关键注意力途径。

Result: 在单GPU环境下实现了注意力计算1.6~2.5倍加速，同时保持了与全注意力基线相当的视觉质量。

Conclusion: 这项工作提供了一种基于结构化稀疏利用的原则性方法，用于开启高效的长形式视频生成。

Abstract: The computational demands of self-attention mechanisms pose a critical
challenge for transformer-based video generation, particularly in synthesizing
ultra-long sequences. Current approaches, such as factorized attention and
fixed sparse patterns, fail to fully exploit the inherent spatio-temporal
redundancies in video data. Through systematic analysis of video diffusion
transformers (DiT), we uncover a key insight: Attention matrices exhibit
structured, yet heterogeneous sparsity patterns, where specialized heads
dynamically attend to distinct spatiotemporal regions (e.g., local pattern,
cross-shaped pattern, or global pattern). Existing sparse attention methods
either impose rigid constraints or introduce significant overhead, limiting
their effectiveness. To address this, we propose Compact Attention, a
hardware-aware acceleration framework featuring three innovations: 1) Adaptive
tiling strategies that approximate diverse spatial interaction patterns via
dynamic tile grouping, 2) Temporally varying windows that adjust sparsity
levels based on frame proximity, and 3) An automated configuration search
algorithm that optimizes sparse patterns while preserving critical attention
pathways. Our method achieves 1.6~2.5x acceleration in attention computation on
single-GPU setups while maintaining comparable visual quality with
full-attention baselines. This work provides a principled approach to unlocking
efficient long-form video generation through structured sparsity exploitation.
Project Page: https://yo-ava.github.io/Compact-Attention.github.io/

</details>


### [138] [Dextr: Zero-Shot Neural Architecture Search with Singular Value Decomposition and Extrinsic Curvature](https://arxiv.org/abs/2508.12977)
*Rohan Asthana,Joschua Conrad,Maurits Ortmanns,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: 这篇论文提出了一种无需标签数据的零成本神经网络查找代理方法，通过结合汇聚性、普遍性和表达能力来预测网络性能，在多个标准数据集上表现優异。


<details>
  <summary>Details</summary>
Motivation: 现有的零成本代理方法多依赖标签数据，而这在实际应用中通常不可用。同时当前方法要么只关注汇聚性和普遍性，要么只关注网络表达能力，需要一种综合考虑这三者的方法。

Method: 通过分析频道共线性对汇聚性和普遍性的影响，利用神经网络层特征的奇异值分解(SVD)和网络输出的外在曲率来设计代理。该代理是两个关键组件的对数和调均均值的简化调和平均数。

Result: 在NAS-Bench-101、NAS-Bench-201、TransNAS-Bench-101-micro等多个相关性测试中表现優异，同时在DARTS和AutoFormer搜索空间的NAS任务中也显示出良好性能。该方法仅需一个无标签数据样本就能准确预测网络在测试数据上的性能，且效率显著。

Conclusion: 该研究提出的零成本代理方法成功解决了对标签数据的依赖问题，并综合考虑了汇聚性、普遍性和表达能力三个关键因素，在多个标准数据集和搜索空间上都表现出優异的相关性和效果。

Abstract: Zero-shot Neural Architecture Search (NAS) typically optimises the
architecture search process by exploiting the network or gradient properties at
initialisation through zero-cost proxies. The existing proxies often rely on
labelled data, which is usually unavailable in real-world settings.
Furthermore, the majority of the current methods focus either on optimising the
convergence and generalisation attributes or solely on the expressivity of the
network architectures. To address both limitations, we first demonstrate how
channel collinearity affects the convergence and generalisation properties of a
neural network. Then, by incorporating the convergence, generalisation and
expressivity in one approach, we propose a zero-cost proxy that omits the
requirement of labelled data for its computation. In particular, we leverage
the Singular Value Decomposition (SVD) of the neural network layer features and
the extrinsic curvature of the network output to design our proxy. %As a
result, the proposed proxy is formulated as the simplified harmonic mean of the
logarithms of two key components: the sum of the inverse of the feature
condition number and the extrinsic curvature of the network output. Our
approach enables accurate prediction of network performance on test data using
only a single label-free data sample. Our extensive evaluation includes a total
of six experiments, including the Convolutional Neural Network (CNN) search
space, i.e. DARTS and the Transformer search space, i.e. AutoFormer. The
proposed proxy demonstrates a superior performance on multiple correlation
benchmarks, including NAS-Bench-101, NAS-Bench-201, and
TransNAS-Bench-101-micro; as well as on the NAS task within the DARTS and the
AutoFormer search space, all while being notably efficient. The code is
available at https://github.com/rohanasthana/Dextr.

</details>


### [139] [Omni Survey for Multimodality Analysis in Visual Object Tracking](https://arxiv.org/abs/2508.13000)
*Zhangyong Tang,Tianyang Xu,Xuefeng Zhu,Hui Li,Shaochuan Zhao,Tao Zhou,Chunyang Cheng,Xiaojun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: 本文是一个关于多模态视觉目标跟踪(MMVOT)的全面调研报告，涵盖了数据采集、模态对齐、模型设计和评估等关键方面，包含6个MMVOT任务和338个参考文献。


<details>
  <summary>Details</summary>
Motivation: 智慧城市产生了大量多模态数据，需要综合监测城市基础设施和服务。多模态视觉目标跟踪作为关键任务，需要从多模态角度进行分析和研究。

Method: 从四个关键方面对比单模态跟踪：数据采集、模态对齐和标注、模型设计、评估。对现有MMVOT方法按处理可见光(RGB)和其他模态(X)的不同方式进行分类，其中X包括热红外(T)、深度(D)、事件(E)、近红外(NIR)、语言(L)或声纳(S)。

Result: 调研发现现有MMVOT数据集中对象类别呈现明显的长尾分布，与RGB数据集相比动物类别显著缺乏。同时探讨了多模态跟踪是否总能提供更优解决方案的基本问题。

Conclusion: 本文进行了多模态视觉目标跟踪领域的全面调研，为该领域提供了系统性的分析和指南，并首次揭示了相关数据集的分布特征和存在的问题。

Abstract: The development of smart cities has led to the generation of massive amounts
of multi-modal data in the context of a range of tasks that enable a
comprehensive monitoring of the smart city infrastructure and services. This
paper surveys one of the most critical tasks, multi-modal visual object
tracking (MMVOT), from the perspective of multimodality analysis. Generally,
MMVOT differs from single-modal tracking in four key aspects, data collection,
modality alignment and annotation, model designing, and evaluation.
Accordingly, we begin with an introduction to the relevant data modalities,
laying the groundwork for their integration. This naturally leads to a
discussion of challenges of multi-modal data collection, alignment, and
annotation. Subsequently, existing MMVOT methods are categorised, based on
different ways to deal with visible (RGB) and X modalities: programming the
auxiliary X branch with replicated or non-replicated experimental
configurations from the RGB branch. Here X can be thermal infrared (T), depth
(D), event (E), near infrared (NIR), language (L), or sonar (S). The final part
of the paper addresses evaluation and benchmarking. In summary, we undertake an
omni survey of all aspects of multi-modal visual object tracking (VOT),
covering six MMVOT tasks and featuring 338 references in total. In addition, we
discuss the fundamental rhetorical question: Is multi-modal tracking always
guaranteed to provide a superior solution to unimodal tracking with the help of
information fusion, and if not, in what circumstances its application is
beneficial. Furthermore, for the first time in this field, we analyse the
distributions of the object categories in the existing MMVOT datasets,
revealing their pronounced long-tail nature and a noticeable lack of animal
categories when compared with RGB datasets.

</details>


### [140] [Empirical Evidences for the Effects of Feature Diversity in Open Set Recognition and Continual Learning](https://arxiv.org/abs/2508.13005)
*Jiawen Xu,Odej Kao*

Main category: cs.CV

TL;DR: 本文通过实证研究表明，增强特征多样性可以改善开放集识别性能，并促进持续学习中的知识保留和新数据整合


<details>
  <summary>Details</summary>
Motivation: 开放集识别和持续学习是机器学习中的两个关键挑战，虽然许多方法通过启发式促进特征多样性来解决这些问题，但很少有研究直接探讨特征多样性在其中扮演的角色

Method: 通过实证研究分析特征多样性对开放集识别和持续学习的影响，提供经验证据

Result: 增强特征多样性可以改善开放集样本的识别，同时也有利于持续学习中旧知识的保留和新数据的整合

Conclusion: 研究结果可为这两个领域的实践方法和理论理解提供启发，推动进一步研究

Abstract: Open set recognition (OSR) and continual learning are two critical challenges
in machine learning, focusing respectively on detecting novel classes at
inference time and updating models to incorporate the new classes. While many
recent approaches have addressed these problems, particularly OSR, by
heuristically promoting feature diversity, few studies have directly examined
the role that feature diversity plays in tackling them. In this work, we
provide empirical evidence that enhancing feature diversity improves the
recognition of open set samples. Moreover, increased feature diversity also
facilitates both the retention of previously learned data and the integration
of new data in continual learning. We hope our findings can inspire further
research into both practical methods and theoretical understanding in these
domains.

</details>


### [141] [SlimComm: Doppler-Guided Sparse Queries for Bandwidth-Efficient Cooperative 3-D Perception](https://arxiv.org/abs/2508.13007)
*Melih Yazgan,Qiyuan Wu,Iramm Hamdard,Shiqi Li,J. Marius Zoellner*

Main category: cs.CV

TL;DR: SlimComm是一个通信高效的协作感知框架，通过整合4D雷达多普勒信息和查询驱动稀疏方案，大幅降低带宽需求同时保持感知精度。


<details>
  <summary>Details</summary>
Motivation: 协作感知中传输密集的BEV特征图会严重占用车辆间通信带宽，需要一种既能保持感知精度又能显著减少通信负载的方法。

Method: 构建运动中心动态地图区分动静物体，生成参考查询（动态和高置信区域）和探索查询（遮挡区域），仅交换查询特定BEV特征并通过多尺度门控可变形注意力融合。

Result: 带宽降低达90%，在多种交通密度和遮挡场景下性能匹配或超越现有基线方法。

Conclusion: SlimComm有效解决了协作感知中的通信瓶颈问题，通过智能查询机制和雷达多普勒信息实现了高效的特征交换。

Abstract: Collaborative perception allows connected autonomous vehicles (CAVs) to
overcome occlusion and limited sensor range by sharing intermediate features.
Yet transmitting dense Bird's-Eye-View (BEV) feature maps can overwhelm the
bandwidth available for inter-vehicle communication. We present SlimComm, a
communication-efficient framework that integrates 4D radar Doppler with a
query-driven sparse scheme. SlimComm builds a motion-centric dynamic map to
distinguish moving from static objects and generates two query types: (i)
reference queries on dynamic and high-confidence regions, and (ii) exploratory
queries probing occluded areas via a two-stage offset. Only query-specific BEV
features are exchanged and fused through multi-scale gated deformable
attention, reducing payload while preserving accuracy. For evaluation, we
release OPV2V-R and Adver-City-R, CARLA-based datasets with per-point Doppler
radar. SlimComm achieves up to 90% lower bandwidth than full-map sharing while
matching or surpassing prior baselines across varied traffic densities and
occlusions. Dataset and code will be available at: https://url.fzi.de/SlimComm.

</details>


### [142] [Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model](https://arxiv.org/abs/2508.13009)
*Xianglong He,Chunli Peng,Zexiang Liu,Boyang Wang,Yifan Zhang,Qi Cui,Fei Kang,Biao Jiang,Mengyin An,Yangyang Ren,Baixin Xu,Hao-Xiang Guo,Kaixiong Gong,Cyrus Wu,Wei Li,Xuchen Song,Yang Liu,Eric Li,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-Game 2.0是一个实时交互式世界模型，通过少步自回归扩散生成高质量长视频，速度达到25FPS，解决了现有模型推理速度慢的问题。


<details>
  <summary>Details</summary>
Motivation: 现有交互式世界模型依赖双向注意力和冗长推理步骤，严重限制了实时性能，难以模拟需要基于历史上下文和当前动作即时更新结果的真实世界动态。

Method: 包含三个关键组件：(1)可扩展的数据生产流水线，从Unreal Engine和GTA5环境生成大量带交互标注的视频数据；(2)动作注入模块，支持帧级鼠标键盘输入作为交互条件；(3)基于因果架构的少步蒸馏，实现实时流式视频生成。

Result: 能够以25FPS的超快速度生成高质量分钟级视频，覆盖多样化场景，并开源模型权重和代码库。

Conclusion: Matrix-Game 2.0通过创新的少步蒸馏和因果架构，显著提升了交互式世界模型的实时性能，为交互式世界建模研究提供了重要进展。

Abstract: Recent advances in interactive video generations have demonstrated diffusion
model's potential as world models by capturing complex physical dynamics and
interactive behaviors. However, existing interactive world models depend on
bidirectional attention and lengthy inference steps, severely limiting
real-time performance. Consequently, they are hard to simulate real-world
dynamics, where outcomes must update instantaneously based on historical
context and current actions. To address this, we present Matrix-Game 2.0, an
interactive world model generates long videos on-the-fly via few-step
auto-regressive diffusion. Our framework consists of three key components: (1)
A scalable data production pipeline for Unreal Engine and GTA5 environments to
effectively produce massive amounts (about 1200 hours) of video data with
diverse interaction annotations; (2) An action injection module that enables
frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step
distillation based on the casual architecture for real-time and streaming video
generation. Matrix Game 2.0 can generate high-quality minute-level videos
across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our
model weights and codebase to advance research in interactive world modeling.

</details>


### [143] [EgoTwin: Dreaming Body and View in First Person](https://arxiv.org/abs/2508.13013)
*Jingqiao Xiu,Fangzhou Hong,Yicong Li,Mengze Li,Wentao Wang,Sirui Han,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: EgoTwin是一个基于扩散变换器的联合视频-运动生成框架，专门解决第一人称视角视频和人体运动生成任务，通过头部中心运动表示和网络交互机制确保视角对齐和因果交互。


<details>
  <summary>Details</summary>
Motivation: 虽然外中心视频合成取得了很大进展，但第一人称视角视频生成仍然很少被探索，需要同时建模第一人称视角内容和穿戴者身体运动引起的相机运动模式。

Method: 提出EgoTwin框架，使用扩散变换器架构，引入头部中心运动表示将人体运动锚定到头部关节，并采用网络交互机制在注意力操作中显式捕捉视频和运动之间的因果交互。

Result: 构建了大规模真实世界同步文本-视频-运动三元组数据集，设计了新颖的指标来评估视频-运动一致性，大量实验证明了EgoTwin框架的有效性。

Conclusion: EgoTwin成功解决了第一人称视频生成中的视角对齐和因果交互挑战，为联合视频-运动生成任务提供了有效的解决方案。

Abstract: While exocentric video synthesis has achieved great progress, egocentric
video generation remains largely underexplored, which requires modeling
first-person view content along with camera motion patterns induced by the
wearer's body movements. To bridge this gap, we introduce a novel task of joint
egocentric video and human motion generation, characterized by two key
challenges: 1) Viewpoint Alignment: the camera trajectory in the generated
video must accurately align with the head trajectory derived from human motion;
2) Causal Interplay: the synthesized human motion must causally align with the
observed visual dynamics across adjacent video frames. To address these
challenges, we propose EgoTwin, a joint video-motion generation framework built
on the diffusion transformer architecture. Specifically, EgoTwin introduces a
head-centric motion representation that anchors the human motion to the head
joint and incorporates a cybernetics-inspired interaction mechanism that
explicitly captures the causal interplay between video and motion within
attention operations. For comprehensive evaluation, we curate a large-scale
real-world dataset of synchronized text-video-motion triplets and design novel
metrics to assess video-motion consistency. Extensive experiments demonstrate
the effectiveness of the EgoTwin framework.

</details>


### [144] [HierAdaptMR: Cross-Center Cardiac MRI Reconstruction with Hierarchical Feature Adapters](https://arxiv.org/abs/2508.13026)
*Ruru Xu,Ilkay Oksuz*

Main category: cs.CV

TL;DR: HierAdaptMR是一个分层特征适应框架，通过参数高效的适配器解决多中心心脏MRI重建中的域偏移问题，在CMRxRecon2025数据集上表现出优秀的跨中心泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习心脏MRI重建在多个临床中心部署时面临显著的域偏移挑战，不同扫描仪配置和成像协议导致性能下降。

Method: 采用分层特征适应框架：协议级适配器处理序列特异性特征，中心级适配器处理扫描仪相关变异，基于变分展开骨干网络。通用适配器通过随机训练学习中心不变适应，实现完全未见中心的泛化。使用多尺度SSIM损失和频域增强进行优化。

Result: 在CMRxRecon2025数据集（5+中心、10+扫描仪、9种模态）上的综合评估显示，该方法在保持重建质量的同时实现了优越的跨中心泛化性能。

Conclusion: HierAdaptMR通过分层适配器有效解决了多中心心脏MRI重建的域偏移问题，为临床部署提供了实用的解决方案。

Abstract: Deep learning-based cardiac MRI reconstruction faces significant domain shift
challenges when deployed across multiple clinical centers with heterogeneous
scanner configurations and imaging protocols. We propose HierAdaptMR, a
hierarchical feature adaptation framework that addresses multi-level domain
variations through parameter-efficient adapters. Our method employs
Protocol-Level Adapters for sequence-specific characteristics and Center-Level
Adapters for scanner-dependent variations, built upon a variational unrolling
backbone. A Universal Adapter enables generalization to entirely unseen centers
through stochastic training that learns center-invariant adaptations. The
framework utilizes multi-scale SSIM loss with frequency domain enhancement and
contrast-adaptive weighting for robust optimization. Comprehensive evaluation
on the CMRxRecon2025 dataset spanning 5+ centers, 10+ scanners, and 9
modalities demonstrates superior cross-center generalization while maintaining
reconstruction quality. code: https://github.com/Ruru-Xu/HierAdaptMR

</details>


### [145] [IntelliCap: Intelligent Guidance for Consistent View Sampling](https://arxiv.org/abs/2508.13043)
*Ayaka Yasunaga,Hideo Saito,Dieter Schmalstieg,Shohei Mori*

Main category: cs.CV

TL;DR: 一种基于视觉-语言模型的扫描指导技术，通过识别重要物体并生成球面代理来指导用户获取更包容视角依赖性外观的图像数据


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在指导用户采集视角合成输入图像时的不足，特别是对单个物体偏重而忽视视角依赖材质特性的问题

Method: 利用语义分割和类别识别，通过视视-语言模型评分重要物体，然后生成球面代理来指导用户扫描

Result: 在真实场景中表现出艰优于传统视角采样策略的性能

Conclusion: 该方法能够有效指导用户获取高质量的多视角合成输入数据，特别是对具有视角依赖性外观的重要物体

Abstract: Novel view synthesis from images, for example, with 3D Gaussian splatting,
has made great progress. Rendering fidelity and speed are now ready even for
demanding virtual reality applications. However, the problem of assisting
humans in collecting the input images for these rendering algorithms has
received much less attention. High-quality view synthesis requires uniform and
dense view sampling. Unfortunately, these requirements are not easily addressed
by human camera operators, who are in a hurry, impatient, or lack understanding
of the scene structure and the photographic process. Existing approaches to
guide humans during image acquisition concentrate on single objects or neglect
view-dependent material characteristics. We propose a novel situated
visualization technique for scanning at multiple scales. During the scanning of
a scene, our method identifies important objects that need extended image
coverage to properly represent view-dependent appearance. To this end, we
leverage semantic segmentation and category identification, ranked by a
vision-language model. Spherical proxies are generated around highly ranked
objects to guide the user during scanning. Our results show superior
performance in real scenes compared to conventional view sampling strategies.

</details>


### [146] [Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping](https://arxiv.org/abs/2508.13065)
*Siddharth Khandelwal,Sridhar Kamath,Arjun Jain*

Main category: cs.CV

TL;DR: 人体形状编辑的新方法Odo，通过洞洞模型和深度地图控制，实现了更准确和超过基线方法的人体形态编辑效果


<details>
  <summary>Details</summary>
Motivation: 现有人体形状编辑方法存在不真实体型比例、纹理扭曲和背景不一致等问题，且缺乏大规模专门数据集进行训练和评估

Method: 构建了18,573张图片的大规模数据集，提出Odo方法：结合冻结UNet保留外观细节，使用ControlNet通过目标SMPL深度地图导航形状变换

Result: 方法在顶点重建错误上达到7.5mm，显著低于基线方法的13.6mm，能大幅提升形态编辑的准确性和真实性

Conclusion: 该研究为人体形状编辑领域提供了量化评估标准和高效方法，Odo方法通过深度地图控制实现了更准确的形态编辑效果

Abstract: Human shape editing enables controllable transformation of a person's body
shape, such as thin, muscular, or overweight, while preserving pose, identity,
clothing, and background. Unlike human pose editing, which has advanced
rapidly, shape editing remains relatively underexplored. Current approaches
typically rely on 3D morphable models or image warping, often introducing
unrealistic body proportions, texture distortions, and background
inconsistencies due to alignment errors and deformations. A key limitation is
the lack of large-scale, publicly available datasets for training and
evaluating body shape manipulation methods. In this work, we introduce the
first large-scale dataset of 18,573 images across 1523 subjects, specifically
designed for controlled human shape editing. It features diverse variations in
body shape, including fat, muscular and thin, captured under consistent
identity, clothing, and background conditions. Using this dataset, we propose
Odo, an end-to-end diffusion-based method that enables realistic and intuitive
body reshaping guided by simple semantic attributes. Our approach combines a
frozen UNet that preserves fine-grained appearance and background details from
the input image with a ControlNet that guides shape transformation using target
SMPL depth maps. Extensive experiments demonstrate that our method outperforms
prior approaches, achieving per-vertex reconstruction errors as low as 7.5mm,
significantly lower than the 13.6mm observed in baseline methods, while
producing realistic results that accurately match the desired target shapes.

</details>


### [147] [Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray Diagnosis and Report Generation](https://arxiv.org/abs/2508.13068)
*Tanjim Islam Riju,Shuchismita Anwar,Saman Sarker Joy,Farig Sadeque,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: 通过视线指导的对比学习和区域感知报告生成框架，结合医生眼动数据显著提升胸部X光疾病分类和报告生成的性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 利用医生诊时的眼动信息来改善医学影像分析，提升疾病分类的准确性和医学报告生成的临床相关性。

Method: 两阶段多模态框架：第一阶段使用视线指导的对比学习进行疾病分类，结合多项眼动注意力损失函数；第二阶段通过模块化报告生成管道，提取信心度加权的诊断关键词并映射到解剖区域。

Result: 结合眼动数据后，F1分数从0.597提升到0.631（+5.70%），AUC从0.821提升到0.849（+3.41%），同时改善了精度和召回率，报告质量在临床关键词召回率和ROUGE指标上都有提升。

Conclusion: 集成眼动数据能够同时提高疾病分类的性能和生成医学报告的可解释性，证明了眼动信息在医学影像分析中的重要价值。

Abstract: We propose a two-stage multimodal framework that enhances disease
classification and region-aware radiology report generation from chest X-rays,
leveraging the MIMIC-Eye dataset. In the first stage, we introduce a
gaze-guided contrastive learning architecture for disease classification. It
integrates visual features, clinical labels, bounding boxes, and radiologist
eye-tracking signals and is equipped with a novel multi-term gaze-attention
loss combining MSE, KL divergence, correlation, and center-of-mass alignment.
Incorporating fixations improves F1 score from 0.597 to 0.631 (+5.70%) and AUC
from 0.821 to 0.849 (+3.41%), while also improving precision and recall,
highlighting the effectiveness of gaze-informed attention supervision. In the
second stage, we present a modular report generation pipeline that extracts
confidence-weighted diagnostic keywords, maps them to anatomical regions using
a curated dictionary constructed from domain-specific priors, and generates
region-aligned sentences via structured prompts. This pipeline improves report
quality as measured by clinical keyword recall and ROUGE overlap. Our results
demonstrate that integrating gaze data improves both classification performance
and the interpretability of generated medical reports.

</details>


### [148] [ID-Card Synthetic Generation: Toward a Simulated Bona fide Dataset](https://arxiv.org/abs/2508.13078)
*Qingwen Zeng,Juan E. Tapia,Izan Garcia,Juan M. Espin,Christoph Busch*

Main category: cs.CV

TL;DR: 通过稳定散布生成合成真实ID卡图像来解决实际真实样本缺乏问题，提升出示攻击检测系统的性能


<details>
  <summary>Details</summary>
Motivation: ID卡出示攻击检测系统面临真实样本数量有限和攻击手段多样化的挑战，需要新方法来缓解数据限制问题

Method: 使用稳定散布(Stable Diffusion)生成合成的真实ID卡图像，并在从头训练的系统和商业解决方案中评估这些图像

Result: 生成的合成图像被系统识别为真实样本，对检测性能有积极影响，有效缓解了数据限制问题

Conclusion: 使用稳定散布生成合成真实ID卡图像是一种有效方法，能够提升PAD系统的通用性能和解决真实样本数据缺乏问题

Abstract: Nowadays, the development of a Presentation Attack Detection (PAD) system for
ID cards presents a challenge due to the lack of images available to train a
robust PAD system and the increase in diversity of possible attack instrument
species. Today, most algorithms focus on generating attack samples and do not
take into account the limited number of bona fide images. This work is one of
the first to propose a method for mimicking bona fide images by generating
synthetic versions of them using Stable Diffusion, which may help improve the
generalisation capabilities of the detector. Furthermore, the new images
generated are evaluated in a system trained from scratch and in a commercial
solution. The PAD system yields an interesting result, as it identifies our
images as bona fide, which has a positive impact on detection performance and
data restrictions.

</details>


### [149] [Checkmate: interpretable and explainable RSVQA is the endgame](https://arxiv.org/abs/2508.13086)
*Lucrezia Tosato,Christel Tartini Chappuis,Syrielle Montariol,Flora Weissgerber,Sylvain Lobry,Devis Tuia*

Main category: cs.CV

TL;DR: 提出了一个新的遥感视觉问答数据集Chessboard和可解释模型Checkmate，通过细粒度视觉推理提高模型的可解释性和可信过程


<details>
  <summary>Details</summary>
Motivation: 解决遥感视觉问答系统中模型可解释性不足、数据集偏差导致短路学习等问题

Method: 构建包含312万个问题的平衡数据集Chessboard，每个答案都与图像单元相关联；开发Checkmate模型能够识别决策相关的图像区域

Result: 通过多种模型架构的实验验证，该方法提高了系统的透明度和可信度

Conclusion: 该研究为遥感视觉问答系统提供了更可解释和可信的决策支持

Abstract: Remote Sensing Visual Question Answering (RSVQA) presents unique challenges
in ensuring that model decisions are both understandable and grounded in visual
content. Current models often suffer from a lack of interpretability and
explainability, as well as from biases in dataset distributions that lead to
shortcut learning. In this work, we tackle these issues by introducing a novel
RSVQA dataset, Chessboard, designed to minimize biases through 3'123'253
questions and a balanced answer distribution. Each answer is linked to one or
more cells within the image, enabling fine-grained visual reasoning.
  Building on this dataset, we develop an explainable and interpretable model
called Checkmate that identifies the image cells most relevant to its
decisions. Through extensive experiments across multiple model architectures,
we show that our approach improves transparency and supports more trustworthy
decision-making in RSVQA systems.

</details>


### [150] [DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation](https://arxiv.org/abs/2508.13091)
*Zihua Liu,Yizhou Li,Songyan Zhang,Masatoshi Okutomi*

Main category: cs.CV

TL;DR: 提出DMS方法，利用扩散模型生成新视角图像来解决自监督立体匹配中的遮挡问题，无需额外标注数据即可显著提升性能


<details>
  <summary>Details</summary>
Motivation: 自监督立体匹配和单目深度估计方法在遮挡区域存在对应像素缺失问题，导致光度重建模糊性，需要解决这一挑战

Method: 微调Stable Diffusion模型，通过方向提示沿极线方向合成新视角图像（左-左视图、右-右视图和中间视图），补充遮挡像素以建立显式光度对应关系

Result: 在多个基准数据集上达到最先进性能，异常值减少高达35%

Conclusion: DMS是一种即插即用的模型无关方法，仅需无标注立体图像对即可有效提升自监督立体匹配和单目深度估计性能

Abstract: While supervised stereo matching and monocular depth estimation have advanced
significantly with learning-based algorithms, self-supervised methods using
stereo images as supervision signals have received relatively less focus and
require further investigation. A primary challenge arises from ambiguity
introduced during photometric reconstruction, particularly due to missing
corresponding pixels in ill-posed regions of the target view, such as
occlusions and out-of-frame areas. To address this and establish explicit
photometric correspondences, we propose DMS, a model-agnostic approach that
utilizes geometric priors from diffusion models to synthesize novel views along
the epipolar direction, guided by directional prompts. Specifically, we
finetune a Stable Diffusion model to simulate perspectives at key positions:
left-left view shifted from the left camera, right-right view shifted from the
right camera, along with an additional novel view between the left and right
cameras. These synthesized views supplement occluded pixels, enabling explicit
photometric reconstruction. Our proposed DMS is a cost-free, ''plug-and-play''
method that seamlessly enhances self-supervised stereo matching and monocular
depth estimation, and relies solely on unlabeled stereo image pairs for both
training and synthesizing. Extensive experiments demonstrate the effectiveness
of our approach, with up to 35% outlier reduction and state-of-the-art
performance across multiple benchmark datasets.

</details>


### [151] [Real-Time Beach Litter Detection and Counting: A Comparative Analysis of RT-DETR Model Variants](https://arxiv.org/abs/2508.13101)
*Miftahul Huda,Arsyiah Azahra,Putri Maulida Chairani,Dimas Rizky Ramadhani,Nabila Azhari,Ade Lailani*

Main category: cs.CV

TL;DR: RT-DETR-L模型在准确性和推理速度之间提供了更优的平衡，比RT-DETR-X更适合实时海滩垃圾检测部署


<details>
  <summary>Details</summary>
Motivation: 海岸污染是全球紧迫的环境问题，需要可扩展的自动化监测解决方案

Method: 使用RT-DETR-L和RT-DETR-X两种变体模型，在公开的海岸垃圾数据集上进行训练和比较分析

Result: RT-DETR-X准确度略高(mAP@50=0.816, mAP@50-95=0.612)，但RT-DETR-L推理速度更快(20.1ms vs 34.5ms)，在速度与精度间平衡更佳

Conclusion: RT-DETR-L模型因其优越的处理速度和检测精度平衡，为实时野外部署提供了更实用的高效解决方案

Abstract: Coastal pollution is a pressing global environmental issue, necessitating
scalable and automated solutions for monitoring and management. This study
investigates the efficacy of the Real-Time Detection Transformer (RT-DETR), a
state-of-the-art, end-to-end object detection model, for the automated
detection and counting of beach litter. A rigorous comparative analysis is
conducted between two model variants, RT-DETR-Large (RT-DETR-L) and
RT-DETR-Extra-Large (RT-DETR-X), trained on a publicly available dataset of
coastal debris. The evaluation reveals that the RT-DETR-X model achieves
marginally superior accuracy, with a mean Average Precision at 50\% IoU
(mAP@50) of 0.816 and a mAP@50-95 of 0.612, compared to the RT-DETR-L model's
0.810 and 0.606, respectively. However, this minor performance gain is realized
at a significant computational cost; the RT-DETR-L model demonstrates a
substantially faster inference time of 20.1 ms versus 34.5 ms for the
RT-DETR-X. The findings suggest that the RT-DETR-L model offers a more
practical and efficient solution for real-time, in-field deployment due to its
superior balance of processing speed and detection accuracy. This research
provides valuable insights into the application of advanced Transformer-based
detectors for environmental conservation, highlighting the critical trade-offs
between model complexity and operational viability.

</details>


### [152] [Precise Action-to-Video Generation Through Visual Action Prompts](https://arxiv.org/abs/2508.13104)
*Yuang Wang,Chao Wen,Haoyu Guo,Sida Peng,Minghan Qin,Hujun Bao,Xiaowei Zhou,Ruizhen Hu*

Main category: cs.CV

TL;DR: 视觉动作提示（VAP）通过将动作渲染为视觉骨架来平衡动作生成的精确性和跨领域转移性，支持复杂高自由度交互的视频生成。


<details>
  <summary>Details</summary>
Motivation: 解决动作驱动视频生成中的精确性-通用性二元投竞问题：现有方法要么精确但缺乏跨领域转移性，要么通用但缺乏细粒度控制。

Method: 将动作渲染为领域无关的视觉骨架表示，通过轻量级微调整合到预训练视频生成模型中，支持人类-物体交互和机器人操作的跨领域训练。

Result: 在EgoVid、RT-1和DROID数据集上验证了方法的有效性，能够精确控制复杂交互动作同时保持跨领域动态学习能力。

Conclusion: 视觉动作提示提供了一种统一的动作表示方式，成功解决了动作生成的精确性与跨领域转移性之间的投竞问题。

Abstract: We present visual action prompts, a unified action representation for
action-to-video generation of complex high-DoF interactions while maintaining
transferable visual dynamics across domains. Action-driven video generation
faces a precision-generality trade-off: existing methods using text, primitive
actions, or coarse masks offer generality but lack precision, while
agent-centric action signals provide precision at the cost of cross-domain
transferability. To balance action precision and dynamic transferability, we
propose to "render" actions into precise visual prompts as domain-agnostic
representations that preserve both geometric precision and cross-domain
adaptability for complex actions; specifically, we choose visual skeletons for
their generality and accessibility. We propose robust pipelines to construct
skeletons from two interaction-rich data sources - human-object interactions
(HOI) and dexterous robotic manipulation - enabling cross-domain training of
action-driven generative models. By integrating visual skeletons into
pretrained video generation models via lightweight fine-tuning, we enable
precise action control of complex interaction while preserving the learning of
cross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the
effectiveness of our proposed approach. Project page:
https://zju3dv.github.io/VAP/.

</details>


### [153] [Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence](https://arxiv.org/abs/2508.13139)
*Ling-Hao Chen,Yuhong Zhang,Zixin Yin,Zhiyang Dou,Xin Chen,Jingbo Wang,Taku Komura,Lei Zhang*

Main category: cs.CV

TL;DR: Motion2Motion是一个无需训练的框架，用于在不同骨骼拓扑结构的角色之间进行动画迁移，只需目标骨骼的少量示例运动即可实现高效可靠的跨物种骨骼迁移。


<details>
  <summary>Details</summary>
Motivation: 解决不同骨骼拓扑结构角色间动画迁移的挑战，现有技术难以处理骨骼拓扑不一致和缺乏大规模配对运动数据集的问题。

Method: 提出训练免费的Motion2Motion框架，通过源骨骼和目标骨骼之间的稀疏骨骼对应关系，仅需目标骨骼的一个或几个示例运动即可工作。

Result: 在相似骨骼和跨物种骨骼迁移场景中都实现了高效可靠的性能，成功集成到下游应用和用户界面中。

Conclusion: 该方法具有工业应用潜力，为解决跨拓扑结构动画迁移问题提供了有效的解决方案，代码和数据已开源。

Abstract: This work studies the challenge of transfer animations between characters
whose skeletal topologies differ substantially. While many techniques have
advanced retargeting techniques in decades, transfer motions across diverse
topologies remains less-explored. The primary obstacle lies in the inherent
topological inconsistency between source and target skeletons, which restricts
the establishment of straightforward one-to-one bone correspondences. Besides,
the current lack of large-scale paired motion datasets spanning different
topological structures severely constrains the development of data-driven
approaches. To address these limitations, we introduce Motion2Motion, a novel,
training-free framework. Simply yet effectively, Motion2Motion works with only
one or a few example motions on the target skeleton, by accessing a sparse set
of bone correspondences between the source and target skeletons. Through
comprehensive qualitative and quantitative evaluations, we demonstrate that
Motion2Motion achieves efficient and reliable performance in both
similar-skeleton and cross-species skeleton transfer scenarios. The practical
utility of our approach is further evidenced by its successful integration in
downstream applications and user interfaces, highlighting its potential for
industrial applications. Code and data are available at
https://lhchen.top/Motion2Motion.

</details>


### [154] [Has GPT-5 Achieved Spatial Intelligence? An Empirical Study](https://arxiv.org/abs/2508.13142)
*Zhongang Cai,Yubo Wang,Qingping Sun,Ruisi Wang,Chenyang Gu,Wanqi Yin,Zhiqian Lin,Zhitao Yang,Chen Wei,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Jiaqi Li,Xiangyu Fan,Hanming Deng,Lewei Lu,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: GPT-5在多模态空间智能方面取得显著进步但仍不及人类水平，专有模型在最困难问题上并无决定性优势


<details>
  <summary>Details</summary>
Motivation: 评估当前最先进多模态模型在空间理解和推理能力方面的表现，特别是新发布的GPT-5模型，这是实现通用人工智能的关键能力

Method: 提出统一的空间任务分类法，在8个关键基准测试上评估最先进的专有和开源模型，消耗超过10亿个token，并进行定性评估

Result: GPT-5展现出前所未有的空间智能强度，但在广泛任务上仍落后于人类；识别出多模态模型更具挑战性的空间智能问题；专有模型在最困难问题上没有决定性优势

Conclusion: 多模态模型在空间智能方面仍有显著局限，需要进一步研究来弥合与人类能力的差距

Abstract: Multi-modal models have achieved remarkable progress in recent years.
Nevertheless, they continue to exhibit notable limitations in spatial
understanding and reasoning, which are fundamental capabilities to achieving
artificial general intelligence. With the recent release of GPT-5, allegedly
the most powerful AI model to date, it is timely to examine where the leading
models stand on the path toward spatial intelligence. First, we propose a
comprehensive taxonomy of spatial tasks that unifies existing benchmarks and
discuss the challenges in ensuring fair evaluation. We then evaluate
state-of-the-art proprietary and open-source models on eight key benchmarks, at
a cost exceeding one billion total tokens. Our empirical study reveals that (1)
GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)
still falls short of human performance across a broad spectrum of tasks.
Moreover, we (3) identify the more challenging spatial intelligence problems
for multi-modal models, and (4) proprietary models do not exhibit a decisive
advantage when facing the most difficult problems. In addition, we conduct a
qualitative evaluation across a diverse set of scenarios that are intuitive for
humans yet fail even the most advanced multi-modal models.

</details>


### [155] [IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion](https://arxiv.org/abs/2508.13153)
*Wenhao Hu,Zesheng Li,Haonan Zhou,Liu Liu,Xuexiang Wen,Zhizhong Su,Xi Li,Gaoang Wang*

Main category: cs.CV

TL;DR: IGFuse是一个新颖的3D场景重建框架，通过融合多视角扫描数据来重建交互式高斯场景，解决了物体遮挡和传感器覆盖限制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多阶段重建流程容易出错且难以扩展，单次扫描无法捕捉完整的结构细节，需要一种能够有效处理遮挡并支持场景操作的方法。

Method: 构建分割感知的高斯场，通过双向光度和语义一致性约束，引入伪中间场景状态进行统一对齐，并采用协作共剪枝策略优化几何结构。

Result: 实验验证了框架对新场景配置的强泛化能力，实现了高保真渲染和物体级场景操作，无需密集观测或复杂流程。

Conclusion: IGFuse在真实世界3D重建和实景到仿真转换方面表现出色，为交互式3D场景重建提供了有效的解决方案。

Abstract: Reconstructing complete and interactive 3D scenes remains a fundamental
challenge in computer vision and robotics, particularly due to persistent
object occlusions and limited sensor coverage. Multiview observations from a
single scene scan often fail to capture the full structural details. Existing
approaches typically rely on multi stage pipelines, such as segmentation,
background completion, and inpainting or require per-object dense scanning,
both of which are error-prone, and not easily scalable. We propose IGFuse, a
novel framework that reconstructs interactive Gaussian scene by fusing
observations from multiple scans, where natural object rearrangement between
captures reveal previously occluded regions. Our method constructs segmentation
aware Gaussian fields and enforces bi-directional photometric and semantic
consistency across scans. To handle spatial misalignments, we introduce a
pseudo-intermediate scene state for unified alignment, alongside collaborative
co-pruning strategies to refine geometry. IGFuse enables high fidelity
rendering and object level scene manipulation without dense observations or
complex pipelines. Extensive experiments validate the framework's strong
generalization to novel scene configurations, demonstrating its effectiveness
for real world 3D reconstruction and real-to-simulation transfer. Our project
page is available online.

</details>


### [156] [4DNeX: Feed-Forward 4D Generative Modeling Made Easy](https://arxiv.org/abs/2508.13154)
*Zhaoxi Chen,Tianqi Liu,Long Zhuo,Jiawei Ren,Zeng Tao,He Zhu,Fangzhou Hong,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 4DNeX是首个从单张图像生成4D（动态3D）场景表示的端到端前馈框架，通过微调预训练视频扩散模型实现高效生成


<details>
  <summary>Details</summary>
Motivation: 解决现有4D生成方法依赖计算密集型优化或需要多帧视频输入的问题，提供更高效的图像到4D转换方案

Method: 1)构建大规模4D数据集4DNeX-10M；2)引入统一6D视频表示联合建模RGB和XYZ序列；3)提出适配策略将视频扩散模型重新用于4D建模

Result: 生成高质量动态点云，支持新颖视角视频合成，在效率和泛化性方面优于现有4D生成方法

Conclusion: 为图像到4D建模提供了可扩展解决方案，为生成式4D世界模型模拟动态场景演化奠定了基础

Abstract: We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,
dynamic 3D) scene representations from a single image. In contrast to existing
methods that rely on computationally intensive optimization or require
multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D
generation by fine-tuning a pretrained video diffusion model. Specifically, 1)
to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale
dataset with high-quality 4D annotations generated using advanced
reconstruction approaches. 2) we introduce a unified 6D video representation
that jointly models RGB and XYZ sequences, facilitating structured learning of
both appearance and geometry. 3) we propose a set of simple yet effective
adaptation strategies to repurpose pretrained video diffusion models for 4D
modeling. 4DNeX produces high-quality dynamic point clouds that enable
novel-view video synthesis. Extensive experiments demonstrate that 4DNeX
outperforms existing 4D generation methods in efficiency and generalizability,
offering a scalable solution for image-to-4D modeling and laying the foundation
for generative 4D world models that simulate dynamic scene evolution.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [157] [DermINO: Hybrid Pretraining for a Versatile Dermatology Foundation Model](https://arxiv.org/abs/2508.12190)
*Jingkai Xu,De Cheng,Xiangqian Zhao,Jungang Yang,Zilong Wang,Xinyang Jiang,Xufang Luo,Lili Chen,Xiaoli Ning,Chengxu Li,Xinzhu Zhou,Xuejiao Song,Ang Li,Qingyue Xia,Zhou Zhuang,Hongfei Ouyang,Ke Xue,Yujun Sheng,Rusong Meng,Feng Xu,Xi Yang,Weimin Ma,Yusheng Lee,Dongsheng Li,Xinbo Gao,Jianming Liang,Lili Qiu,Nannan Wang,Xianbo Zuo,Cui Yong*

Main category: eess.IV

TL;DR: DermNIO是一个用于皮肤病学的多功能基础模型，通过混合预训练框架在432,776张图像上训练，在20个数据集上优于现有模型，诊断准确率达到95.79%，比临床医生高22.13%。


<details>
  <summary>Details</summary>
Motivation: 皮肤病对全球医疗系统造成沉重负担，患病率高且诊断复杂，资源有限地区皮肤科医生短缺。现有AI模型依赖大量人工标注数据且功能单一，在真实场景中效果有限。

Method: 提出DermNIO基础模型，使用432,776张图像训练，采用新颖的混合预训练框架，结合自监督学习、半监督学习和知识引导的原型初始化，增强对复杂皮肤病情的理解和泛化能力。

Result: 在20个数据集上评估，DermNIO在恶性肿瘤分类、疾病严重程度分级、多类别诊断和皮肤病图像描述等高级临床应用中表现优异，在皮肤病变分割等低级任务中也达到最先进水平。在隐私保护联邦学习场景和不同皮肤类型、性别中表现出强大鲁棒性。

Conclusion: DermNIO显著提升了皮肤病诊断的准确性和泛化能力，AI辅助将临床医生诊断性能提高了17.21%，为解决皮肤病诊断挑战提供了有效的解决方案。

Abstract: Skin diseases impose a substantial burden on global healthcare systems,
driven by their high prevalence (affecting up to 70% of the population),
complex diagnostic processes, and a critical shortage of dermatologists in
resource-limited areas. While artificial intelligence(AI) tools have
demonstrated promise in dermatological image analysis, current models face
limitations-they often rely on large, manually labeled datasets and are built
for narrow, specific tasks, making them less effective in real-world settings.
To tackle these limitations, we present DermNIO, a versatile foundation model
for dermatology. Trained on a curated dataset of 432,776 images from three
sources (public repositories, web-sourced images, and proprietary collections),
DermNIO incorporates a novel hybrid pretraining framework that augments the
self-supervised learning paradigm through semi-supervised learning and
knowledge-guided prototype initialization. This integrated method not only
deepens the understanding of complex dermatological conditions, but also
substantially enhances the generalization capability across various clinical
tasks. Evaluated across 20 datasets, DermNIO consistently outperforms
state-of-the-art models across a wide range of tasks. It excels in high-level
clinical applications including malignancy classification, disease severity
grading, multi-category diagnosis, and dermatological image caption, while also
achieving state-of-the-art performance in low-level tasks such as skin lesion
segmentation. Furthermore, DermNIO demonstrates strong robustness in
privacy-preserving federated learning scenarios and across diverse skin types
and sexes. In a blinded reader study with 23 dermatologists, DermNIO achieved
95.79% diagnostic accuracy (versus clinicians' 73.66%), and AI assistance
improved clinician performance by 17.21%.

</details>


### [158] [FractMorph: A Fractional Fourier-Based Multi-Domain Transformer for Deformable Image Registration](https://arxiv.org/abs/2508.12445)
*Shayan Kebriti,Shahabedin Nabavi,Ali Gooya*

Main category: eess.IV

TL;DR: FractMorph是一种基于3D双并行transformer的新型可变形图像配准架构，通过多域分数傅里叶变换分支增强跨图像特征匹配，在ACDC心脏MRI数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在统一框架中同时捕捉细粒度局部变形和大尺度全局变形，需要一种能够同时处理不同尺度变形的端到端解决方案。

Method: 提出双并行transformer架构，使用分数傅里叶变换在0°、45°、90°角度以及对数幅度分支并行提取局部、半全局和全局特征，通过交叉注意力融合固定图像和移动图像的特征，最后通过轻量级U-Net预测密集变形场。

Result: 在ACDC心脏MRI数据集上，总体Dice相似系数达到86.45%，平均每结构DSC为75.15%，95%Hausdorff距离为1.54mm。轻量版FractMorph-Light仅需29.6M参数，保持相似精度但内存使用减半。

Conclusion: 多域谱空间注意力机制能够在单一端到端网络中稳健高效地建模医学图像中的复杂非刚性变形，无需场景特定调优或多尺度层次网络。

Abstract: Deformable image registration (DIR) is a crucial and challenging technique
for aligning anatomical structures in medical images and is widely applied in
diverse clinical applications. However, existing approaches often struggle to
capture fine-grained local deformations and large-scale global deformations
simultaneously within a unified framework. We present FractMorph, a novel 3D
dual-parallel transformer-based architecture that enhances cross-image feature
matching through multi-domain fractional Fourier transform (FrFT) branches.
Each Fractional Cross-Attention (FCA) block applies parallel FrFTs at
fractional angles of 0{\deg}, 45{\deg}, 90{\deg}, along with a log-magnitude
branch, to effectively extract local, semi-global, and global features at the
same time. These features are fused via cross-attention between the fixed and
moving image streams. A lightweight U-Net style network then predicts a dense
deformation field from the transformer-enriched features. On the ACDC cardiac
MRI dataset, FractMorph achieves state-of-the-art performance with an overall
Dice Similarity Coefficient (DSC) of 86.45%, an average per-structure DSC of
75.15%, and a 95th-percentile Hausdorff distance (HD95) of 1.54 mm on our data
split. We also introduce FractMorph-Light, a lightweight variant of our model
with only 29.6M parameters, which maintains the superior accuracy of the main
model while using approximately half the memory. Our results demonstrate that
multi-domain spectral-spatial attention in transformers can robustly and
efficiently model complex non-rigid deformations in medical images using a
single end-to-end network, without the need for scenario-specific tuning or
hierarchical multi-scale networks. The source code of our implementation is
available at https://github.com/shayankebriti/FractMorph.

</details>


### [159] [Segmenting Thalamic Nuclei: T1 Maps Provide a Reliable and Efficient Solution](https://arxiv.org/abs/2508.12508)
*Anqi Feng,Zhangxing Bian,Samuel W. Remedios,Savannah P. Hays,Blake E. Dewey,Jiachen Zhuo,Dan Benjamini,Jerry L. Prince*

Main category: eess.IV

TL;DR: 本研究系统评估了多种MRI对比度对丘脑核团分割的效果，发现T1 mapping单独使用即可获得最佳性能，而PD mapping没有额外价值。


<details>
  <summary>Details</summary>
Motivation: 丘脑核团的精确分割对神经系统疾病研究和临床干预至关重要，但最佳的MRI输入序列尚不明确。

Method: 使用3D U-Net模型，评估包括MPRAGE、FGATIR序列、定量PD和T1 mapping以及多反转时间T1加权图像等多种MRI对比度，并对多TI图像采用基于梯度的显著性分析和蒙特卡洛dropout来选择最重要的图像。

Result: T1 mapping单独使用就能获得强大的定量性能和优越的定性结果，PD mapping没有提供额外价值。

Conclusion: T1 mapping是评估选项中可靠且高效的输入选择，为优化丘脑结构相关的临床或研究成像方案提供了重要指导。

Abstract: Accurate thalamic nuclei segmentation is crucial for understanding
neurological diseases, brain functions, and guiding clinical interventions.
However, the optimal inputs for segmentation remain unclear. This study
systematically evaluates multiple MRI contrasts, including MPRAGE and FGATIR
sequences, quantitative PD and T1 maps, and multiple T1-weighted images at
different inversion times (multi-TI), to determine the most effective inputs.
For multi-TI images, we employ a gradient-based saliency analysis with Monte
Carlo dropout and propose an Overall Importance Score to select the images
contributing most to segmentation. A 3D U-Net is trained on each of these
configurations. Results show that T1 maps alone achieve strong quantitative
performance and superior qualitative outcomes, while PD maps offer no added
value. These findings underscore the value of T1 maps as a reliable and
efficient input among the evaluated options, providing valuable guidance for
optimizing imaging protocols when thalamic structures are of clinical or
research interest.

</details>


### [160] [Anatomic Feature Fusion Model for Diagnosing Calcified Pulmonary Nodules on Chest X-Ray](https://arxiv.org/abs/2508.12562)
*Hyeonjin Choi,Yang-gon Kim,Dong-yeon Yoo,Ju-sung Sun,Jung-won Lee*

Main category: eess.IV

TL;DR: 这篇论文提出了一种肿结钙化分类模型，通过融合原始图像和结构压制变体的特征，在胸部X光片上实现了更准确的肿结钙化识别，超越了传统方法。


<details>
  <summary>Details</summary>
Motivation: 胸部X光片上肿结钙化识别对医疗诊断至关重要，钙化是良性肿结的关键指标。但伴随着肌肉、骨骼等结构的叠加干扰，以及医生视觉评估的主观性，导致诊断结果存在显著差异。

Method: 研究提出了一种钙化分类模型，利用原始图像和结构压制变体的融合特征来减少结构干扰。数据集包含2,517张无痕痕图像和656张肿结图像（151个钙化肿结和550个非钙化肿结）。

Result: 建议的模型在钙化诊断中达到了86.52%的准确率和0.8889的AUC值，超过仅使用原始图像训练的模型3.54%和0.0385。

Conclusion: 该研究开发的钙化分类模型通过融合多源特征有效减少结构干扰，在胸部X光片肿结钙化识别中显示出优秀的诊断性能，为临床诊断提供了更准确的工具。

Abstract: Accurate and timely identification of pulmonary nodules on chest X-rays can
differentiate between life-saving early treatment and avoidable invasive
procedures. Calcification is a definitive indicator of benign nodules and is
the primary foundation for diagnosis. In actual practice, diagnosing pulmonary
nodule calcification on chest X-rays predominantly depends on the physician's
visual assessment, resulting in significant diversity in interpretation.
Furthermore, overlapping anatomical elements, such as ribs and spine,
complicate the precise identification of calcification patterns. This study
presents a calcification classification model that attains strong diagnostic
performance by utilizing fused features derived from raw images and their
structure-suppressed variants to reduce structural interference. We used 2,517
lesion-free images and 656 nodule images (151 calcified nodules and 550
non-calcified nodules), all obtained from Ajou University Hospital. The
suggested model attained an accuracy of 86.52% and an AUC of 0.8889 in
calcification diagnosis, surpassing the model trained on raw images by 3.54%
and 0.0385, respectively.

</details>


### [161] [Learning local and global prototypes with optimal transport for unsupervised anomaly detection and localization](https://arxiv.org/abs/2508.12927)
*Robin Trombetta,Carole Lartizien*

Main category: eess.IV

TL;DR: 基于原型学习和最优运输的无监督异常检测方法，通过学习局部和全局原型来提高工业图像异常检测性能


<details>
  <summary>Details</summary>
Motivation: 解决无监督异常检测中标签获取成本高和避免异常类型偏差的问题，适用于工业检测和医学影像等领域

Method: 提出一种基于原型学习的新方法，使用特征基成本和空间基成本的结构化嵌入比较指标，利用最优运输从预训练图像编码器中学习局部和全局原型

Result: 在两个工业图像异常检测标准测试集上达到了与强基准线相当的性能水平

Conclusion: 方法能够通过结构约束学习原型来捕捌正常样本的基础组织结构，有效提高图像中不一致性的检测能力

Abstract: Unsupervised anomaly detection aims to detect defective parts of a sample by
having access, during training, to a set of normal, i.e. defect-free, data. It
has many applications in fields, such as industrial inspection or medical
imaging, where acquiring labels is costly or when we want to avoid introducing
biases in the type of anomalies that can be spotted. In this work, we propose a
novel UAD method based on prototype learning and introduce a metric to compare
a structured set of embeddings that balances a feature-based cost and a
spatial-based cost. We leverage this metric to learn local and global
prototypes with optimal transport from latent representations extracted with a
pre-trained image encoder. We demonstrate that our approach can enforce a
structural constraint when learning the prototypes, allowing to capture the
underlying organization of the normal samples, thus improving the detection of
incoherencies in images. Our model achieves performance that is on par with
strong baselines on two reference benchmarks for anomaly detection on
industrial images. The code is available at
https://github.com/robintrmbtt/pradot.

</details>


### [162] [From Transthoracic to Transesophageal: Cross-Modality Generation using LoRA Diffusion](https://arxiv.org/abs/2508.13077)
*Emmanuel Oladokun,Yuxuan Ou,Anna Novikova,Daria Kulikova,Sarina Thomas,Jurica Šprem,Vicente Grau*

Main category: eess.IV

TL;DR: 通过小参数适配器和轻量级控制层MaskR²，将TTE训练的模型迅速适配到TEE预处理，以极少的真实数据生成高保真度合成图像，并提升下游分割任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度潜移模型在数据稀缺领域如经食道心超声动态图（TEE）中需要大量训练数据的问题，这个领域正缺乏数据支持。

Method: 采用Low-Rank Adaptation技术结合轻量级控制层MaskR²，将训练好的事问心超声动态图（TTE）模型适配到TEE预处理，仅需极少量的新数据和小规模适配器。

Result: 仅适配MLP层即可实现高保真度TEE合成，混合不到200帹真实TEE帧样本后，在多类分割任务上显著提升了Dice分数，尤其在下背心脏结构上表现更佳。

Conclusion: 该方法能够以低开销生成语义可控的TEE图像，MaskR²能够有效转换未见的掩码格式，且生成的图像对下游多类分割任务有显著提升效果。

Abstract: Deep diffusion models excel at realistic image synthesis but demand large
training sets-an obstacle in data-scarce domains like transesophageal
echocardiography (TEE). While synthetic augmentation has boosted performance in
transthoracic echo (TTE), TEE remains critically underrepresented, limiting the
reach of deep learning in this high-impact modality.
  We address this gap by adapting a TTE-trained, mask-conditioned diffusion
backbone to TEE with only a limited number of new cases and adapters as small
as $10^5$ parameters. Our pipeline combines Low-Rank Adaptation with MaskR$^2$,
a lightweight remapping layer that aligns novel mask formats with the
pretrained model's conditioning channels. This design lets users adapt models
to new datasets with a different set of anatomical structures to the base
model's original set.
  Through a targeted adaptation strategy, we find that adapting only MLP layers
suffices for high-fidelity TEE synthesis. Finally, mixing less than 200 real
TEE frames with our synthetic echoes improves the dice score on a multiclass
segmentation task, particularly boosting performance on underrepresented
right-heart structures. Our results demonstrate that (1) semantically
controlled TEE images can be generated with low overhead, (2) MaskR$^2$
effectively transforms unseen mask formats into compatible formats without
damaging downstream task performance, and (3) our method generates images that
are effective for improving performance on a downstream task of multiclass
segmentation.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [163] [RefAdGen: High-Fidelity Advertising Image Generation](https://arxiv.org/abs/2508.11695)
*Yiyun Chen,Weikai Yang*

Main category: cs.GR

TL;DR: 通过构建AdProd-100K数据集和提出RefAdGen框架，解决了AIGC在生成高保真广告图像时的保真性与效率两难问题


<details>
  <summary>Details</summary>
Motivation: 现有AIGC技术要么需要大量细调才能达到高保真度，要么在多样化产品上无法保持保真性，影响在电子商务和营销行业的应用

Method: 首先构建AdProd-100K大规模广告图像生成数据集，采用双重数据增强策略；提出RefAdGen生成框架，通过在U-Net输入注入产品掩码实现精确空间控制，并使用高效的注意力融合模块（AFM）集成产品特征

Result: 实验结果显示RefAdGen达到了最先进的性能，能够在未见过的产品和具有挑战性的实际图像上保持高保真度和显著的视觉效果

Conclusion: RefAdGen为传统工作流程提供了一种可扩展、成本效益高的替代方案，有效解决了现有方法在保真性与效率之间的两难问题

Abstract: The rapid advancement of Artificial Intelligence Generated Content (AIGC)
techniques has unlocked opportunities in generating diverse and compelling
advertising images based on referenced product images and textual scene
descriptions. This capability substantially reduces human labor and production
costs in traditional marketing workflows. However, existing AIGC techniques
either demand extensive fine-tuning for each referenced image to achieve high
fidelity, or they struggle to maintain fidelity across diverse products, making
them impractical for e-commerce and marketing industries. To tackle this
limitation, we first construct AdProd-100K, a large-scale advertising image
generation dataset. A key innovation in its construction is our dual data
augmentation strategy, which fosters robust, 3D-aware representations crucial
for realistic and high-fidelity image synthesis. Leveraging this dataset, we
propose RefAdGen, a generation framework that achieves high fidelity through a
decoupled design. The framework enforces precise spatial control by injecting a
product mask at the U-Net input, and employs an efficient Attention Fusion
Module (AFM) to integrate product features. This design effectively resolves
the fidelity-efficiency dilemma present in existing methods. Extensive
experiments demonstrate that RefAdGen achieves state-of-the-art performance,
showcasing robust generalization by maintaining high fidelity and remarkable
visual results for both unseen products and challenging real-world, in-the-wild
images. This offers a scalable and cost-effective alternative to traditional
workflows. Code and datasets are publicly available at
https://github.com/Anonymous-Name-139/RefAdgen.

</details>


### [164] [Substepping the Material Point Method](https://arxiv.org/abs/2508.11722)
*Chenfanfu Jiang*

Main category: cs.GR

TL;DR: 通过子步长时间步进方法，将显式MPM集成器包装成伪隐式集成器，支持大时间步长计算


<details>
  <summary>Details</summary>
Motivation: 显式时间积分在MPM中普遍使用，但在与其他大步长汇合汇合、约束施加、投影或多物理求解时，大时间步长很有必要

Method: 提出简单的即插即用算法，通过子步方式使用大时间步长来推进MPM，将显式MPM集成器包装成伪隐式集成器

Result: 该算法能够有效地在大时间步长下进行MPM计算

Conclusion: 通过子步方式将显式MPM集成器转换为伪隐式集成器，为大时间步长计算提供了一种简单有效的解决方案

Abstract: Many Material Point Method implementations favor explicit time integration.
However large time steps are often desirable for special reasons - for example,
for partitioned coupling with another large-step solver, or for imposing
constraints, projections, or multiphysics solves. We present a simple,
plug-and-play algorithm that advances MPM with a large time step using
substeps, effectively wrapping an explicit MPM integrator into a
pseudo-implicit one.

</details>


### [165] [Mesh Processing Non-Meshes via Neural Displacement Fields](https://arxiv.org/abs/2508.12179)
*Yuta Noma,Zhecheng Wang,Chenxi Liu,Karan Singh,Alec Jacobson*

Main category: cs.GR

TL;DR: 一种细包的神经场表示方法，能够在不同表面表示之间进行几何处理，解决传统网格处理流水线在新型表面表示中的问题。


<details>
  <summary>Details</summary>
Motivation: 传统网格处理流水线已成熟，但适配到更新的非网格表面表示时需要克服网格转换或大量数据传输的成本，这会失去新表示在流媒体应用中的核心优势。

Method: 学习一个神经映射，从粗糕网格近似映射到原始表面。该表示仅占几百千字节，适合轻量传输。支持快速提取流形和Delaunay网格进行本质形状分析，并压缩标量场。

Result: 实验和应用表明，该方法实现了快速、细包和准确的几何处理，为交互式几何处理开启了新可能性。

Conclusion: 该神经场方法提供了一种细包的解决方案，能够在不同表面表示之间进行通用的几何处理任务，适用于流媒体应用中的轻量传输和交互式处理。

Abstract: Mesh processing pipelines are mature, but adapting them to newer non-mesh
surface representations -- which enable fast rendering with compact file size
-- requires costly meshing or transmitting bulky meshes, negating their core
benefits for streaming applications.
  We present a compact neural field that enables common geometry processing
tasks across diverse surface representations. Given an input surface, our
method learns a neural map from its coarse mesh approximation to the surface.
The full representation totals only a few hundred kilobytes, making it ideal
for lightweight transmission. Our method enables fast extraction of manifold
and Delaunay meshes for intrinsic shape analysis, and compresses scalar fields
for efficient delivery of costly precomputed results. Experiments and
applications show that our fast, compact, and accurate approach opens up new
possibilities for interactive geometry processing.

</details>


### [166] [Express4D: Expressive, Friendly, and Extensible 4D Facial Motion Generation Benchmark](https://arxiv.org/abs/2508.12438)
*Yaron Aloni,Rotem Shalev-Arkushin,Yonatan Shafir,Guy Tevet,Ohad Fried,Amit Haim Bermano*

Main category: cs.GR

TL;DR: 提出了Express4D数据集，这是一个使用消费级设备和LLM生成的自然语言指令捕获的细粒度面部表情数据集，解决了现有数据集要么是语音驱动要么仅限于粗糙情感标签的问题。


<details>
  <summary>Details</summary>
Motivation: 当前的面部表情生成模型存在数据集限制问题，要么是语音驱动，要么只有粗糙的情感标签，缺乏细粒度控制和表达性描述，且数据采集设备昂贵复杂。

Method: 使用消费级设备采集面部运动序列，采用ARKit blendshape格式，通过LLM生成自然语言指令进行语义标注，构建了包含丰富表达性表演和标签的数据集。

Result: 训练了两个基线模型，能够实现有意义的文本到表情运动生成，并捕捉到两种模态之间的多对多映射关系。

Conclusion: Express4D数据集为细粒度面部表情生成提供了高质量基准，数据集、代码和视频示例已公开，可用于未来基准测试和研究。

Abstract: Dynamic facial expression generation from natural language is a crucial task
in Computer Graphics, with applications in Animation, Virtual Avatars, and
Human-Computer Interaction. However, current generative models suffer from
datasets that are either speech-driven or limited to coarse emotion labels,
lacking the nuanced, expressive descriptions needed for fine-grained control,
and were captured using elaborate and expensive equipment. We hence present a
new dataset of facial motion sequences featuring nuanced performances and
semantic annotation. The data is easily collected using commodity equipment and
LLM-generated natural language instructions, in the popular ARKit blendshape
format. This provides riggable motion, rich with expressive performances and
labels. We accordingly train two baseline models, and evaluate their
performance for future benchmarking. Using our Express4D dataset, the trained
models can learn meaningful text-to-expression motion generation and capture
the many-to-many mapping of the two modalities. The dataset, code, and video
examples are available on our webpage: https://jaron1990.github.io/Express4D/

</details>


### [167] [MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration](https://arxiv.org/abs/2508.12691)
*Yuanxin Wei,Lansong Diao,Bujiao Chen,Shenggan Cheng,Zhengping Qian,Wenyuan Yu,Nong Xiao,Wei Lin,Jiangsu Du*

Main category: cs.GR

TL;DR: 这篇论文提出了MixCache框架，通过多粒度缓存策略和自适应缓存决策，在保持视频生成质量的同时显著提升了视频DiT模型的推理效率，实现了近1.9倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 视频DiT模型的多步迭代去噪过程导致计算成本高和推理延迟。现有的单一粒度缓存策略无法灵活地平衡生成质量和推理速度。

Method: 提出MixCache框架，包括上下文感知缓存触发策略和自适应混合缓存决策策略，能够动态选择最优的缓存粒度，无需训练。

Result: 在多种模型上进行了广泛实验，MixCache能够显著加速视频生成（Wan 14B模型1.94倍速度提升，HunyuanVideo模型1.97倍速度提升），同时保持优秀的生成质量和推理效率。

Conclusion: MixCache作为一种无需训练的缓存基础框架，通过多粒度缓存策略有效解决了视频DiT模型的效率问题，在保持高质量视频生成的同时实现了显著的速度提升。

Abstract: Leveraging the Transformer architecture and the diffusion process, video DiT
models have emerged as a dominant approach for high-quality video generation.
However, their multi-step iterative denoising process incurs high computational
cost and inference latency. Caching, a widely adopted optimization method in
DiT models, leverages the redundancy in the diffusion process to skip
computations in different granularities (e.g., step, cfg, block). Nevertheless,
existing caching methods are limited to single-granularity strategies,
struggling to balance generation quality and inference speed in a flexible
manner. In this work, we propose MixCache, a training-free caching-based
framework for efficient video DiT inference. It first distinguishes the
interference and boundary between different caching strategies, and then
introduces a context-aware cache triggering strategy to determine when caching
should be enabled, along with an adaptive hybrid cache decision strategy for
dynamically selecting the optimal caching granularity. Extensive experiments on
diverse models demonstrate that, MixCache can significantly accelerate video
generation (e.g., 1.94$\times$ speedup on Wan 14B, 1.97$\times$ speedup on
HunyuanVideo) while delivering both superior generation quality and inference
efficiency compared to baseline methods.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [168] [SPIDER: Scalable Probabilistic Inference for Differential Earthquake Relocation](https://arxiv.org/abs/2508.12117)
*Zachary E. Ross,John D. Wilding,Kamyar Azizzadenesheli,Aitaro Kato*

Main category: physics.geo-ph

TL;DR: SPIDER是一个可扩展的贝叶斯推断框架，用于双差震源重定位，结合物理信息神经网络和高效采样器来处理大规模地震目录。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯方法无法扩展到高维参数空间（参数可达数百万），无法满足双差重定位的需求，而现代地震目录规模越来越大。

Method: 使用物理信息神经网络Eikonal求解器和随机梯度朗之万动力学高效采样器，在多GPU上并行处理整个地震目录的后验采样。

Result: 在加州和日本的合成地震目录和三个真实数据目录上验证了SPIDER的能力，并提出了分析高维后验分布的方法。

Conclusion: SPIDER提供了一个可扩展的贝叶斯框架，能够有效处理大规模地震目录的双差重定位问题，并支持科学解释和评估。

Abstract: Seismicity catalogs are larger than ever due to an explosion of techniques
for enhanced earthquake detection and an abundance of high-quality datasets.
Bayesian inference is a popular framework for locating earthquakes due to its
ability to propagate and quantify uncertainty into the inversion results, but
traditional methods do not scale well to high-dimensional parameter spaces,
making them unsuitable for double-difference relocation where the number of
parameters can reach the millions. Here we introduce SPIDER, a scalable
Bayesian inference framework for double difference hypocenter relocation.
SPIDER uses a physics-informed neural network Eikonal solver together with a
highly efficient sampler called Stochastic Gradient Langevin Dynamics to
generate posterior samples for entire seismicity catalogs jointly. It is
readily parallelized over multiple GPUs for enhanced computational efficiency.
We demonstrate the capabilities of SPIDER on a rigorous synthetic seismicity
catalog and three real data catalogs from California and Japan. We introduce
several ways to analyze high-dimensional posterior distributions to aid in
scientific interpretation and evaluation.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [169] [Evidence for ultra-water-rich ammonia hydrates stabilized in icy exoplanetary mantles](https://arxiv.org/abs/2508.11924)
*Anshuman Mondal,Katharina Mohrbach,Timofey Fedotenko,Mandy Bethkenhagen,Hanns-Peter Liermann,Carmen Sanchez-Valle*

Main category: astro-ph.EP

TL;DR: 在高压高温条件下，水-氨氮系统中会发生化学反应产生超富水氨氮水合物，这对外星水冰地壳模型有重要意义


<details>
  <summary>Details</summary>
Motivation: 研究水-氨氮系统在高压高温条件下的行为，以更准确地模拟外行星冰地壳的内部动力学过程

Method: 在高压高温条件下（超过16 GPa和750 K）研究氨氮半水合物（AHH）与冰VII之间的化学反应，分析稳定性和相平表

Result: 发现了新型超富水氨氮水合物NH3·6H2O（氮水比为1:6），该组合在30 GPa和1600 K下仍稳定并可冷印到室温

Conclusion: 超富水氨氮水合物在1-2地球质量外行星的冰地壳中是优先形成的，它们与冰VII的浮力差异可能导致冰地壳的化学层化，影响外行星的热演化过程

Abstract: Understanding the behavior of the water-ammonia system at high pressure-high
temperature conditions is important for modeling the internal dynamics of
exoplanet icy mantles. Conventionally, mixtures of ammonia hemihydrate AHH (2:1
ammonia-water molar ratio) and H2O ice VII have been regarded as the ultimate
solid phase assembly in the system. Here we report evidence for chemical
reactions between AHH and ice VII above 750 K and 16 GPa that stabilize
water-rich ammonia hydrates, including a novel ultra-water rich hydrate
NH3.6H2O (1:6 ratio) coexisting with ammonia dihydrate ADH (1:2 ratio) and
excess ice VII. This assembly is stable up to at least 30 GPa and 1600 K and
can be quenched to room temperature. Our results demonstrate that water-rich
ammonia hydrates are favored in the icy mantle of 1-2 MEarth exoplanets
regardless of the ammonia content of the hydrate crystallized during accretion
and/or evolution as long as excess H2O ice is available. The buoyancy contrast
between water-rich hydrates and ice VII may lead to chemical stratification in
exoplanet icy mantles, hence affecting their thermal evolution.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [170] [Hybrid Deep Reconstruction for Vignetting-Free Upconversion Imaging through Scattering in ENZ Materials](https://arxiv.org/abs/2508.13096)
*Hao Zhang,Yang Xu,Wenwen Zhang,Saumya Choudhary,M. Zahirul Alam,Long D. Nguyen,Matthew Klein,Shivashankar Vangala,J. Keith Miller,Eric G. Johnson,Joshua R. Hendrickson,Robert W. Boyd,Sergio Carbajo*

Main category: physics.optics

TL;DR: 通过混合监督深度学习框架DeepTimeGate，结合时间门控的epsilon-near-zero成像系统，成功重建穿透复杂媒仅的高保真图像


<details>
  <summary>Details</summary>
Motivation: 解决光学成像在濁浪或复杂媒仅中遇到的散射问题，散射会打乱空间和相位信息，影响图像质量

Method: 使用时间门控的ENZ成像系统（利用四波混合技术在ITO膜中隔离球形光子），结合U-Net基础的监督模型DeepTimeGate进行初始重建，再用自监督的Deep Image Prior精炼阶段

Result: 在各种散射条件下表现优异：PSNR提升124%，SSIM提升231%，IoU提高10倍，清除旋转效应并扩大视野

Conclusion: 该方法在生物医学成像、液体诊断等领域具有广泛应用前景，可解决传统光学成像因散射而失效的问题

Abstract: Optical imaging through turbid or heterogeneous environments (collectively
referred to as complex media) is fundamentally challenged by scattering, which
scrambles structured spatial and phase information. To address this, we propose
a hybrid-supervised deep learning framework to reconstruct high-fidelity images
from nonlinear scattering measurements acquired with a time-gated
epsilon-near-zero (ENZ) imaging system. The system leverages four-wave mixing
(FWM) in subwavelength indium tin oxide (ITO) films to temporally isolate
ballistic photons, thus rejecting multiply scattered light and enhancing
contrast. To recover structured features from these signals, we introduce
DeepTimeGate, a U-Net-based supervised model that performs initial
reconstruction, followed by a Deep Image Prior (DIP) refinement stage using
self-supervised learning. Our approach demonstrates strong performance across
different imaging scenarios, including binary resolution patterns and complex
vortex-phase masks, under varied scattering conditions. Compared to raw
scattering inputs, it boosts average PSNR by 124%, SSIM by 231%, and achieves a
10 times improvement in intersection-over-union (IoU). Beyond enhancing
fidelity, our method removes the vignetting effect and expands the effective
field-of-view compared to the ENZ-based optical time gate output. These results
suggest broad applicability in biomedical imaging, in-solution diagnostics, and
other scenarios where conventional optical imaging fails due to scattering.

</details>


### [171] [Point upsampling networks for single-photon sensing](https://arxiv.org/abs/2508.12986)
*Jinyi Liu,Guoyang Zhao,Lijun Liu,Yiguang Hong,Weiping Zhang,Shuming Cheng*

Main category: physics.optics

TL;DR: 基于狄宝模型的点云上量网络，通过多路扫描机制、双向Mamba背链和适配上量移位模块，提升单元军感知点云的密度和减少空间偏差，开创了单元军感知的上量化新方向。


<details>
  <summary>Details</summary>
Motivation: 单元军感知作为长距离超敏感图像技术，产生的点云密度稀疏且空间偏差明显，限制了其实际应用价值。

Method: 构建基于状态空间模型的网络，集成多路扫描机制以丰富空间上下文，双向Mamba背链以捐捕全局几何和局部细节，以及适配上量移位模块以缩正偏移引起的失真。

Result: 在常用数据集上进行的大量实验确认了高重建准确性和对失真噪声的强壁垛性，并通过实际数据证明模型能够生成视觉一致、保持细节且压制噪声的点云。

Conclusion: 本研究首次建立了单元军感知的上量化框架，为单元军感知及其在下游任务中的实际应用开启了新方向。

Abstract: Single-photon sensing has generated great interest as a prominent technique
of long-distance and ultra-sensitive imaging, however, it tends to yield sparse
and spatially biased point clouds, thus limiting its practical utility. In this
work, we propose using point upsampling networks to increase point density and
reduce spatial distortion in single-photon point cloud. Particularly, our
network is built on the state space model which integrates a multi-path
scanning mechanism to enrich spatial context, a bidirectional Mamba backbone to
capture global geometry and local details, and an adaptive upsample shift
module to correct offset-induced distortions. Extensive experiments are
implemented on commonly-used datasets to confirm its high reconstruction
accuracy and strong robustness to the distortion noise, and also on real-world
data to demonstrate that our model is able to generate visually consistent,
detail-preserving, and noise suppressed point clouds. Our work is the first to
establish the upsampling framework for single-photon sensing, and hence opens a
new avenue for single-photon sensing and its practical applications in the
downstreaming tasks.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [172] [Statistical analysis of multivariate planar curves and applications to X-ray classification](https://arxiv.org/abs/2508.11780)
*Moindjié Issam-Ali,Descary Marie-Hélène,Beaulac Cédric*

Main category: stat.ME

TL;DR: 本文提出了一种新的多元平面曲线分析方法，利用分割图像的边缘形状作为预测因子来进行医学图像分类识别，并通过心脏扩大症检测验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析中常要解决预测问题，而分割图像的边缘形状具有重要的诊断价值。本文想要利用这些形状信息来进行监督分类。

Method: 提出了一种新的多元平面曲线形式化方法，将单个随机平面曲线扩展到多个曲线的聚合分析。解决了统计形态分析中的对齐问题，并通过切空间投影将获得的多元形态变量用于功能分类方法。

Result: 在分割X光图像中进行心脏扩大症检测，以及在合成数据上进行的数值实验都证明了所提方法的吸引力和稳健性。

Conclusion: 该研究为医学图像分析预测问题提供了一种基于形状特征的新方法，通过多元平面曲线分析和形态对齐技术，在心脏扩大症识别等应用中表现出良好的效果。

Abstract: Recent developments in computer vision have enabled the availability of
segmented images across various domains, such as medicine, where segmented
radiography images play an important role in diagnosis-making. As prediction
problems are common in medical image analysis, this work explores the use of
segmented images (through the associated contours they highlight) as predictors
in a supervised classification context. Consequently, we develop a new approach
for image analysis that takes into account the shape of objects within images.
For this aim, we introduce a new formalism that extends the study of single
random planar curves to the joint analysis of multiple planar curves-referred
to here as multivariate planar curves. In this framework, we propose a solution
to the alignment issue in statistical shape analysis. The obtained multivariate
shape variables are then used in functional classification methods through
tangent projections. Detection of cardiomegaly in segmented X-rays and
numerical experiments on synthetic data demonstrate the appeal and robustness
of the proposed method.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [173] [Data-driven RF Tomography via Cross-modal Sensing and Continual Learning](https://arxiv.org/abs/2508.11654)
*Yang Zhao,Tao Wang,Said Elhadi*

Main category: eess.SP

TL;DR: 数据驱动的无线电频成像技术，通过跨模态感知和持续学习方法，在动态环境下实现了更准确的地下根块藤重建效果，比现有最佳方法提升23.2%。


<details>
  <summary>Details</summary>
Motivation: 虽然数据驱动的RF成像在地下目标检测方面展现出强大潜力，但在动态环境下实现准确和稳健的性能仍然面临挑战。需要开发能够在RF信号发生显著变化时仍然能够重建地下根块藤截面图像的方法。

Method: 提出了一个包含关键组件的DRIFT框架：1）设计了集成RF和视觉传感器的跨模态感知系统；2）采用跨模态学习方法训练RF成像深度神经网络模型；3）在动态环境中检测到环境变化时，应用持续学习自动更新DNN模型。

Result: 实验结果显示，该方法平均相当直径误差为2.29cm，比现有最佳方法提升23.2%的性能改善。

Conclusion: DRIFT框架通过跨模态感知和持续学习组合，能够在动态环境下实现更加准确和稳健的地下根块藤重建，为地下目标检测领域提供了有效的解决方案。

Abstract: Data-driven radio frequency (RF) tomography has demonstrated significant
potential for underground target detection, due to the penetrative nature of RF
signals through soil. However, it is still challenging to achieve accurate and
robust performance in dynamic environments. In this work, we propose a
data-driven radio frequency tomography (DRIFT) framework with the following key
components to reconstruct cross section images of underground root tubers, even
with significant changes in RF signals. First, we design a cross-modal sensing
system with RF and visual sensors, and propose to train an RF tomography deep
neural network (DNN) model following the cross-modal learning approach. Then we
propose to apply continual learning to automatically update the DNN model, once
environment changes are detected in a dynamic environment. Experimental results
show that our approach achieves an average equivalent diameter error of 2.29
cm, 23.2% improvement upon the state-of-the-art approach. Our DRIFT code and
dataset are publicly available on https://github.com/Data-driven-RTI/DRIFT.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [174] [iTrace: Click-Based Gaze Visualization on the Apple Vision Pro](https://arxiv.org/abs/2508.12268)
*Esra Mehmedova,Santiago Berrezueta-Guzman,Stefan Wagner*

Main category: cs.HC

TL;DR: iTrace是一个在Apple Vision Pro上通过点击式方法提取用户注视数据的系统，能够生成动态热图来分析个人和集体的注意力模式，在保持91%精确度的同时克服了设备隐私限制。


<details>
  <summary>Details</summary>
Motivation: Apple Vision Pro具有精确的眼动追踪能力，但由于隐私限制无法直接获取连续的用户注视数据，需要开发替代方法来提取和分析注视信息。

Method: 开发了iTrace系统，采用客户端-服务器架构，通过手动捏合手势、停留控制和游戏控制器等点击式方法提取注视坐标，并将其转换为动态热图进行视频和空间眼动追踪。

Result: 8BitDo控制器实现了14.22次点击/秒的数据收集率，显著高于停留控制的0.45次点击/秒，能够生成更密集的热图可视化。系统达到91%的注视精确度，揭示了讲座视频中的集中注意力和问题解决任务中的广泛扫描等不同注意力模式。

Conclusion: iTrace在保持高精度的同时有效克服了Apple Vision Pro的隐私限制，在教育内容参与、环境设计评估、营销分析和临床认知评估等领域具有广泛应用潜力，但目前建议仅在研究环境中使用。

Abstract: The Apple Vision Pro is equipped with accurate eye-tracking capabilities, yet
the privacy restrictions on the device prevent direct access to continuous user
gaze data. This study introduces iTrace, a novel application that overcomes
these limitations through click-based gaze extraction techniques, including
manual methods like a pinch gesture, and automatic approaches utilizing dwell
control or a gaming controller. We developed a system with a client-server
architecture that captures the gaze coordinates and transforms them into
dynamic heatmaps for video and spatial eye tracking. The system can generate
individual and averaged heatmaps, enabling analysis of personal and collective
attention patterns.
  To demonstrate its effectiveness and evaluate the usability and performance,
a study was conducted with two groups of 10 participants, each testing
different clicking methods. The 8BitDo controller achieved higher average data
collection rates at 14.22 clicks/s compared to 0.45 clicks/s with dwell
control, enabling significantly denser heatmap visualizations. The resulting
heatmaps reveal distinct attention patterns, including concentrated focus in
lecture videos and broader scanning during problem-solving tasks. By allowing
dynamic attention visualization while maintaining a high gaze precision of 91
%, iTrace demonstrates strong potential for a wide range of applications in
educational content engagement, environmental design evaluation, marketing
analysis, and clinical cognitive assessment. Despite the current gaze data
restrictions on the Apple Vision Pro, we encourage developers to use iTrace
only in research settings.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [175] [XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads](https://arxiv.org/abs/2508.13049)
*Tejas Chaudhari,Akarsh J.,Tanushree Dewangan,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: XR-NPE是一个面向XR感知工作负载的高吞吐量混合精度SIMD神经处理引擎，支持多种低精度格式，通过创新的硬件设计和量化感知训练实现高效能计算。


<details>
  <summary>Details</summary>
Motivation: 随着扩展现实(XR)设备的发展，需要高效能的神经处理引擎来处理视觉惯性里程计、物体分类和眼动追踪等感知任务，同时降低功耗和面积开销。

Method: 采用混合精度算法支持FP4、Posit(4,1)、Posit(8,0)、Posit(16,1)格式，设计可重构尾数乘法和指数处理电路(RMMEC)，结合选择性电源门控技术降低能耗。

Result: 在28nm CMOS工艺下达到1.72GHz最大工作频率，面积0.016mm²，算术强度14pJ，相比现有最佳MAC方法减少42%面积和38%功耗，在VCU129上相比SoTA加速器减少1.4x LUTs和1.77x FFs。

Conclusion: XR-NPE证明是一个可扩展、精度自适应的计算引擎，适用于未来资源受限的XR设备，为设计者和研究人员提供了公开的代码库。

Abstract: This work proposes XR-NPE, a high-throughput Mixed-precision SIMD Neural
Processing Engine, designed for extended reality (XR) perception workloads like
visual inertial odometry (VIO), object classification, and eye gaze extraction.
XR-NPE is first to support FP4, Posit (4,1), Posit (8,0), and Posit (16,1)
formats, with layer adaptive hybrid-algorithmic implementation supporting
ultra-low bit precision to significantly reduce memory bandwidth requirements,
and accompanied by quantization-aware training for minimal accuracy loss. The
proposed Reconfigurable Mantissa Multiplication and Exponent processing
Circuitry (RMMEC) reduces dark silicon in the SIMD MAC compute engine, assisted
by selective power gating to reduce energy consumption, providing 2.85x
improved arithmetic intensity. XR-NPE achieves a maximum operating frequency of
1.72 GHz, area 0.016 mm2 , and arithmetic intensity 14 pJ at CMOS 28nm,
reducing 42% area, 38% power compared to the best of state-of-the-art MAC
approaches. The proposed XR-NPE based AXI-enabled Matrix-multiplication
co-processor consumes 1.4x fewer LUTs, 1.77x fewer FFs, and provides 1.2x
better energy efficiency compared to SoTA accelerators on VCU129. The proposed
co-processor provides 23% better energy efficiency and 4% better compute
density for VIO workloads. XR-NPE establishes itself as a scalable,
precision-adaptive compute engine for future resource-constrained XR devices.
The complete set for codes for results reproducibility are released publicly,
enabling designers and researchers to readily adopt and build upon them.
https://github.com/mukullokhande99/XR-NPE.

</details>


### [176] [HOMI: Ultra-Fast EdgeAI platform for Event Cameras](https://arxiv.org/abs/2508.12637)
*Shankaranarayanan H,Satyapreet Singh Yadav,Adithya Krishna,Ajay Vikram P,Mahesh Mehendale,Chetan Singh Thakur*

Main category: cs.AR

TL;DR: HOMI是一个超低延迟的端到端边缘AI平台，使用事件相机和FPGA芯片，通过硬件优化的预处理管道实现高效手势识别，在DVS Gesture数据集上达到94%准确率和1000fps吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有事件处理解决方案存在局限性，缺乏完整的端到端实现、延迟高且未能充分利用事件数据的稀疏性，需要开发更高效的事件相机处理平台。

Method: 使用Prophesee IMX636事件传感器芯片和Xilinx Zynq UltraScale+ MPSoC FPGA芯片，部署自主研发的AI加速器，开发硬件优化的预处理管道，支持恒定时间和恒定事件模式的直方图累积、线性和指数时间表面。

Result: 在DVS Gesture数据集上达到94%准确率（高精度配置），低延迟配置下提供1000fps吞吐量，仅使用33%的FPGA LUT资源，内存占用紧凑。

Conclusion: HOMI平台为边缘机器人应用提供了高效的端到端事件处理解决方案，具有低延迟、高精度和资源效率的优势，为更复杂架构的集成留下了充足空间。

Abstract: Event cameras offer significant advantages for edge robotics applications due
to their asynchronous operation and sparse, event-driven output, making them
well-suited for tasks requiring fast and efficient closed-loop control, such as
gesture-based human-robot interaction. Despite this potential, existing event
processing solutions remain limited, often lacking complete end-to-end
implementations, exhibiting high latency, and insufficiently exploiting event
data sparsity. In this paper, we present HOMI, an ultra-low latency, end-to-end
edge AI platform comprising a Prophesee IMX636 event sensor chip with an Xilinx
Zynq UltraScale+MPSoC FPGA chip, deploying an in-house developed AI
accelerator. We have developed hardware-optimized pre-processing pipelines
supporting both constant-time and constant-event modes for histogram
accumulation, linear and exponential time surfaces. Our general-purpose
implementation caters to both accuracy-driven and low-latency applications.
HOMI achieves 94% accuracy on the DVS Gesture dataset as a use case when
configured for high accuracy operation and provides a throughput of 1000 fps
for low-latency configuration. The hardware-optimised pipeline maintains a
compact memory footprint and utilises only 33% of the available LUT resources
on the FPGA, leaving ample headroom for further latency reduction, model
parallelisation, multi-task deployments, or integration of more complex
architectures.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [177] [Data Shift of Object Detection in Autonomous Driving](https://arxiv.org/abs/2508.11868)
*Lida Xu*

Main category: cs.RO

TL;DR: 本文研究自驾驶对象检测任务中的数据偏移问题，通过数据偏移检测和CycleGAN数据增帽技术优化YOLOv5模型，在BDD100K数据集上获得更优的性能。


<details>
  <summary>Details</summary>
Motivation: 自驾驶系统中机器学习模型对训练和测试数据分布一致性偏敏，实际应用中季节、天气等因素导致的数据偏移很容易影响模型性能。

Method: 系统分析数据偏移问题的复杂性和表现形式，综述数据偏移检测方法，并使用偏移检测技术进行数据集分类和平衡。结合CycleGAN数据增帽技术与YOLOv5框架进行模型优化。

Result: 在BDD100K数据集上的实验结果显示，该方法在对象检测任务中达到了更优的性能，超过了基线模型。

Conclusion: 通过系统化的数据偏移检测和数据增帽技术，可以有效提升自驾驶对象检测模型在实际应用中的适应性和稳健性。

Abstract: With the widespread adoption of machine learning technologies in autonomous
driving systems, their role in addressing complex environmental perception
challenges has become increasingly crucial. However, existing machine learning
models exhibit significant vulnerability, as their performance critically
depends on the fundamental assumption that training and testing data satisfy
the independent and identically distributed condition, which is difficult to
guarantee in real-world applications. Dynamic variations in data distribution
caused by seasonal changes, weather fluctuations lead to data shift problems in
autonomous driving systems. This study investigates the data shift problem in
autonomous driving object detection tasks, systematically analyzing its
complexity and diverse manifestations. We conduct a comprehensive review of
data shift detection methods and employ shift detection analysis techniques to
perform dataset categorization and balancing. Building upon this foundation, we
construct an object detection model. To validate our approach, we optimize the
model by integrating CycleGAN-based data augmentation techniques with the
YOLOv5 framework. Experimental results demonstrate that our method achieves
superior performance compared to baseline models on the BDD100K dataset.

</details>


### [178] [Mechanical Automation with Vision: A Design for Rubik's Cube Solver](https://arxiv.org/abs/2508.12469)
*Abhinav Chalise,Nimesh Gopal Pradhan,Nishan Khanal,Prashant Raj Bista,Dinesh Baniya Kshatri*

Main category: cs.RO

TL;DR: 基于YOLOv8实时检测和Kociemba算法的三阶魔方自动解机械系统，通过三个步进电机和微控制器实现物理操控，平均解决时间约2.2分钟


<details>
  <summary>Details</summary>
Motivation: 开发一种能够自动解决三阶魔方的机械系统，通过计算机视觉和算法自动识别魔方状态并执行解决操作

Method: 使用YOLOv8模型进行实时魔方状态检测（精度0.98443，召回0.98419），通过Unity开发GUI界面，采用Kociemba算法求解，使用三个步进电机和微控制器实现物理操控

Result: 系统能够准确检测魔方状态，平均解决时间约2.2分钟，YOLOv8模型表现优异（Box Loss 0.42051, Class Loss 0.2611）

Conclusion: 该系统成功实现了高效的三阶魔方自动解决，结合了计算机视觉、算法求解和机械控制技术，为自动化魔方解决提供了可行的方案

Abstract: The core mechanical system is built around three stepper motors for physical
manipulation, a microcontroller for hardware control, a camera and YOLO
detection model for real-time cube state detection. A significant software
component is the development of a user-friendly graphical user interface (GUI)
designed in Unity. The initial state after detection from real-time YOLOv8
model (Precision 0.98443, Recall 0.98419, Box Loss 0.42051, Class Loss 0.2611)
is virtualized on GUI. To get the solution, the system employs the Kociemba's
algorithm while physical manipulation with a single degree of freedom is done
by combination of stepper motors' interaction with the cube achieving the
average solving time of ~2.2 minutes.

</details>


### [179] [PROD: Palpative Reconstruction of Deformable Objects through Elastostatic Signed Distance Functions](https://arxiv.org/abs/2508.12554)
*Hamza El-Kebir*

Main category: cs.RO

TL;DR: PROD是一种通过触觉交互重建可变形物体形状和力学特性的新方法，使用弹性静力学SDF从力和姿态测量中恢复未变形形状和材料刚度


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖纯几何或视觉数据，无法有效估计软材料的静态和动态响应，需要整合触觉交互信息来改进可变形物体重建

Method: 将物体变形建模为弹性静力学过程，推导控制泊松方程，从稀疏的姿势和力测量中估计SDF，结合稳态弹性动力学假设恢复未变形SDF

Result: PROD能够处理姿态误差、非法向力施加和曲率误差，在模拟软体交互中表现出鲁棒性，成功估计材料刚度

Conclusion: PROD为机器人操作、医学成像和触觉反馈系统等应用中的可变形物体重建提供了强大工具

Abstract: We introduce PROD (Palpative Reconstruction of Deformables), a novel method
for reconstructing the shape and mechanical properties of deformable objects
using elastostatic signed distance functions (SDFs). Unlike traditional
approaches that rely on purely geometric or visual data, PROD integrates
palpative interaction -- measured through force-controlled surface probing --
to estimate both the static and dynamic response of soft materials. We model
the deformation of an object as an elastostatic process and derive a governing
Poisson equation for estimating its SDF from a sparse set of pose and force
measurements. By incorporating steady-state elastodynamic assumptions, we show
that the undeformed SDF can be recovered from deformed observations with
provable convergence. Our approach also enables the estimation of material
stiffness by analyzing displacement responses to varying force inputs. We
demonstrate the robustness of PROD in handling pose errors, non-normal force
application, and curvature errors in simulated soft body interactions. These
capabilities make PROD a powerful tool for reconstructing deformable objects in
applications ranging from robotic manipulation to medical imaging and haptic
feedback systems.

</details>


### [180] [Temporal and Rotational Calibration for Event-Centric Multi-Sensor Systems](https://arxiv.org/abs/2508.12564)
*Jiayao Mai,Xiuyuan Lu,Kuan Dai,Shaojie Shen,Yi Zhou*

Main category: cs.RO

TL;DR: 事件相机外参标定方法，通过观测角速度和运动估计，无需标定物即可完成时间偏移和旋转外参的高精度标定


<details>
  <summary>Details</summary>
Motivation: 事件相机作为低延迟感知器在多感知器融合中具有重要价值，但其外参标定方法研究较少，传统方法需要专门标定物

Method: 基于运动的两步标定框架：首先利用动力学相关性通过CCA初始化时间偏移和旋转外参，然后采用SO(3)连续时间参数化进行非线性优化精细调整

Result: 在公开和自收数据集上验证，方法达到了与基于标定物方法相当的标定精度，但稳定性超过纯CCA方法，显示了高精度、稳健性和灵活性

Conclusion: 该方法为事件相机多感知器系统提供了一种无需标定物的高精度外参标定方案，具有强大的实用价值和推广潜力

Abstract: Event cameras generate asynchronous signals in response to pixel-level
brightness changes, offering a sensing paradigm with theoretically
microsecond-scale latency that can significantly enhance the performance of
multi-sensor systems. Extrinsic calibration is a critical prerequisite for
effective sensor fusion; however, the configuration that involves event cameras
remains an understudied topic. In this paper, we propose a motion-based
temporal and rotational calibration framework tailored for event-centric
multi-sensor systems, eliminating the need for dedicated calibration targets.
Our method uses as input the rotational motion estimates obtained from event
cameras and other heterogeneous sensors, respectively. Different from
conventional approaches that rely on event-to-frame conversion, our method
efficiently estimates angular velocity from normal flow observations, which are
derived from the spatio-temporal profile of event data. The overall calibration
pipeline adopts a two-step approach: it first initializes the temporal offset
and rotational extrinsics by exploiting kinematic correlations in the spirit of
Canonical Correlation Analysis (CCA), and then refines both temporal and
rotational parameters through a joint non-linear optimization using a
continuous-time parametrization in SO(3). Extensive evaluations on both
publicly available and self-collected datasets validate that the proposed
method achieves calibration accuracy comparable to target-based methods, while
exhibiting superior stability over purely CCA-based methods, and highlighting
its precision, robustness and flexibility. To facilitate future research, our
implementation will be made open-source. Code:
https://github.com/NAIL-HNU/EvMultiCalib.

</details>


### [181] [Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy](https://arxiv.org/abs/2508.13103)
*Tianyi Zhang,Haonan Duan,Haoran Hao,Yu Qiao,Jifeng Dai,Zhi Hou*

Main category: cs.RO

TL;DR: OC-VLA框架通过将动作预测直接建立在相机观测空间中，解决了VLA模型在观察和动作空间不一致的问题，提高了模型对相机视角变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: VLA模型在处理真实世界环境时经常遇到泛化问题，主要原因是观察空间和动作空间之间存在固有差异。虽然训练数据来自不同的相机视角，但模型通常在机器人基坐标系中预测末端执行器位姿，导致空间不一致。

Method: 提出OC-VLA框架，利用相机外参标定矩阵将末端执行器位姿从机器人基坐标系转换到相机坐标系，从而统一不同视角的预测目标。这是一个轻量级、即插即用的策略。

Result: 在模拟和真实世界的机器人操作任务上的综合评估表明，OC-VLA加速了收敛速度，提高了任务成功率，并改善了跨视角泛化能力。

Conclusion: OC-VLA框架有效解决了VLA模型的空间不一致问题，提高了模型对相机视角变化的鲁棒性，且与现有VLA架构兼容，无需重大修改。

Abstract: Vision-Language-Action (VLA) models frequently encounter challenges in
generalizing to real-world environments due to inherent discrepancies between
observation and action spaces. Although training data are collected from
diverse camera perspectives, the models typically predict end-effector poses
within the robot base coordinate frame, resulting in spatial inconsistencies.
To mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA)
framework, which grounds action predictions directly in the camera observation
space. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms
end-effector poses from the robot base coordinate system into the camera
coordinate system, thereby unifying prediction targets across heterogeneous
viewpoints. This lightweight, plug-and-play strategy ensures robust alignment
between perception and action, substantially improving model resilience to
camera viewpoint variations. The proposed approach is readily compatible with
existing VLA architectures, requiring no substantial modifications.
Comprehensive evaluations on both simulated and real-world robotic manipulation
tasks demonstrate that OC-VLA accelerates convergence, enhances task success
rates, and improves cross-view generalization. The code will be publicly
available.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [182] [Contrastive Regularization over LoRA for Multimodal Biomedical Image Incremental Learning](https://arxiv.org/abs/2508.11673)
*Haojie Zhang,Yixiong Liang,Hulin Kuang,Lihui Cen,Zhe Qu,Yigang Cen,Min Zeng,Shichao Kan*

Main category: cs.LG

TL;DR: 提出了MSLoRA-CR方法，通过模态特定的LoRA模块和对比正则化来解决多模态生物医学图像增量学习中的知识保留和跨模态知识迁移问题


<details>
  <summary>Details</summary>
Motivation: 多模态生物医学图像增量学习需要统一模型处理不同模态，避免为每个模态单独训练模型带来的高推理成本，但现有方法主要关注单模态内的任务扩展

Method: 基于大型视觉语言模型，冻结预训练模型，为每个模态增量适配新的LoRA模块，同时使用对比正则化增强模态内知识共享和促进模态间知识区分

Result: 在生物医学图像增量学习实验中，MSLoRA-CR优于为每个模态单独训练模型的方法和通用的增量学习方法，整体性能提升1.88%，同时保持计算效率

Conclusion: MSLoRA-CR方法有效解决了多模态生物医学图像增量学习的挑战，在性能和效率方面都表现出色

Abstract: Multimodal Biomedical Image Incremental Learning (MBIIL) is essential for
handling diverse tasks and modalities in the biomedical domain, as training
separate models for each modality or task significantly increases inference
costs. Existing incremental learning methods focus on task expansion within a
single modality, whereas MBIIL seeks to train a unified model incrementally
across modalities. The MBIIL faces two challenges: I) How to preserve
previously learned knowledge during incremental updates? II) How to effectively
leverage knowledge acquired from existing modalities to support new modalities?
To address these challenges, we propose MSLoRA-CR, a method that fine-tunes
Modality-Specific LoRA modules while incorporating Contrastive Regularization
to enhance intra-modality knowledge sharing and promote inter-modality
knowledge differentiation. Our approach builds upon a large vision-language
model (LVLM), keeping the pretrained model frozen while incrementally adapting
new LoRA modules for each modality or task. Experiments on the incremental
learning of biomedical images demonstrate that MSLoRA-CR outperforms both the
state-of-the-art (SOTA) approach of training separate models for each modality
and the general incremental learning method (incrementally fine-tuning LoRA).
Specifically, MSLoRA-CR achieves a 1.88% improvement in overall performance
compared to unconstrained incremental learning methods while maintaining
computational efficiency. Our code is publicly available at
https://github.com/VentusAislant/MSLoRA_CR.

</details>


### [183] [Scalable Geospatial Data Generation Using AlphaEarth Foundations Model](https://arxiv.org/abs/2508.11739)
*Luc Houriez,Sebastian Pilarski,Behzad Vahedi,Ali Ahmadalipour,Teo Honda Scully,Nicholas Aflitto,David Andre,Caroline Jaffe,Martha Wedner,Rich Mazzola,Josh Jeffery,Ben Messinger,Sage McGinley-Smith,Sarah Russell*

Main category: cs.LG

TL;DR: 利用Google DeepMind的AlphaEarth Foundations (AEF) 全球地理空间表示，通过随机森林和逻辑回归等基础模型，将美国LANDFIRE植被类型数据集扩展到加拿大，在13类和80类分类任务上分别达到81%和73%的准确率。


<details>
  <summary>Details</summary>
Motivation: 高质量的地理空间标记数据集通常只覆盖特定地理区域，无法全球覆盖。需要一种方法来扩展这些数据集的地理覆盖范围。

Method: 利用AEF作为信息密集的全球地理空间表示，使用随机森林和逻辑回归等基础机器学习模型，将美国现有的植被类型分类模型扩展到加拿大地区。

Result: 在EvtPhys（13类）任务上，模型在美国和加拿大的验证集上分别达到81%和73%的分类准确率，预测结果与地面实况定性一致。

Conclusion: 即使使用基础模型，AEF也能有效支持地理空间标记数据集的跨区域扩展，为全球地理空间分析提供了可行的方法。

Abstract: High-quality labeled geospatial datasets are essential for extracting
insights and understanding our planet. Unfortunately, these datasets often do
not span the entire globe and are limited to certain geographic regions where
data was collected. Google DeepMind's recently released AlphaEarth Foundations
(AEF) provides an information-dense global geospatial representation designed
to serve as a useful input across a wide gamut of tasks. In this article we
propose and evaluate a methodology which leverages AEF to extend geospatial
labeled datasets beyond their initial geographic regions. We show that even
basic models like random forests or logistic regression can be used to
accomplish this task. We investigate a case study of extending LANDFIRE's
Existing Vegetation Type (EVT) dataset beyond the USA into Canada at two levels
of granularity: EvtPhys (13 classes) and EvtGp (80 classes). Qualitatively, for
EvtPhys, model predictions align with ground truth. Trained models achieve 81%
and 73% classification accuracy on EvtPhys validation sets in the USA and
Canada, despite discussed limitations.

</details>


### [184] [ENA: Efficient N-dimensional Attention](https://arxiv.org/abs/2508.11921)
*Yibo Zhong*

Main category: cs.LG

TL;DR: 通过组合线性递归和滑动窗口注意力机制，提出了高效的N维注意力机制（ENA）来处理超长高阶数据序列


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型在处理长序列高阶数据时效率低下，需要更高效的架构来扩展线性递归模型的应用范围

Method: 研究了扫描策略和注意力混合架构两个关键方面，采用线性递归压缩全局信息到状态中，并通过沿用高阶滑动窗口注意力来补充局部建模

Result: 实验结果显示扫描策略提供的改善有限，而注意力混合模型显示出有前景的结果，特别是沿用高阶滑动窗口注意力在理论和实践中都很高效

Conclusion: ENA架构通过线性递归和滑动窗口注意力的组合，为超长高阶数据序列建模提供了一个简单、高效且实用的解决方案

Abstract: Efficient modeling of long sequences of high-order data requires a more
efficient architecture than Transformer. In this paper, we investigate two key
aspects of extending linear recurrent models, especially those originally
designed for language modeling, to high-order data (1D to ND): scanning
strategies and attention-hybrid architectures. Empirical results suggest that
scanning provides limited benefits, while attention-hybrid models yield
promising results. Focusing on the latter, we further evaluate types of
attention and find that tiled high-order sliding window attention (SWA) is
efficient in both theory and practice. We term the resulting hybrid
architecture of linear recurrence and high-order SWA as Efficient N-dimensional
Attention (ENA). We then conduct several experiments to demonstrate its
effectiveness. The intuition behind ENA is that linear recurrence compresses
global information into a state, while SWA complements it by enforcing strict
local modeling. Together, they form a simple framework that offers a promising
and practical solution for ultra-long high-order data modeling.

</details>


### [185] [L-SR1: Learned Symmetric-Rank-One Preconditioning](https://arxiv.org/abs/2508.12270)
*Gal Lifshitz,Shahar Zuler,Ori Fouks,Dan Raviv*

Main category: cs.LG

TL;DR: 提出一种新的学习二阶优化器，通过可训练的预处理单元改进经典SR1算法，在人体网格恢复任务上超过现有学习优化方法


<details>
  <summary>Details</summary>
Motivation: 绘制端到端深度学习存在对大量标签数据的依赖、漫逆性差和计算要求高等问题，而经典优化方法虽然轻量但收敛慢，学习二阶优化器领域仍很少有研究

Method: 使用可训练预处理单元生成数据驱动的向量，构建正半定秩一矩阵，通过学习投影与离线约束对齐，改进SR1算法

Result: 在分析实验和单目人体网格恢复任务上表现超过现有学习优化方法，具有轻量、无需标注数据或微调、强漫逆性等优点

Conclusion: 该方法为学习二阶优化器领域提供了新的视角，适合集成到更广泛的优化框架中，在保持高效的同时提升了性能

Abstract: End-to-end deep learning has achieved impressive results but remains limited
by its reliance on large labeled datasets, poor generalization to unseen
scenarios, and growing computational demands. In contrast, classical
optimization methods are data-efficient and lightweight but often suffer from
slow convergence. While learned optimizers offer a promising fusion of both
worlds, most focus on first-order methods, leaving learned second-order
approaches largely unexplored.
  We propose a novel learned second-order optimizer that introduces a trainable
preconditioning unit to enhance the classical Symmetric-Rank-One (SR1)
algorithm. This unit generates data-driven vectors used to construct positive
semi-definite rank-one matrices, aligned with the secant constraint via a
learned projection. Our method is evaluated through analytic experiments and on
the real-world task of Monocular Human Mesh Recovery (HMR), where it
outperforms existing learned optimization-based approaches. Featuring a
lightweight model and requiring no annotated data or fine-tuning, our approach
offers strong generalization and is well-suited for integration into broader
optimization-based frameworks.

</details>


### [186] [Toward Architecture-Agnostic Local Control of Posterior Collapse in VAEs](https://arxiv.org/abs/2508.12530)
*Hyunsoo Song,Seungwhan Kim,Seungkyu Lee*

Main category: cs.LG

TL;DR: 通过提出Latent Reconstruction(LR)损失函数来解决VAE后验摘套问题，无需网络结构限制


<details>
  <summary>Details</summary>
Motivation: 解决VAE模型中的后验摘套问题，提高生成样本的多样性，免去现有方法的网络结构限制

Method: 定义局部后验摘套概念，基于注射和复合函数的数学性质提出Latent Reconstruction(LR)损失函数

Result: 在MNIST、fashionMNIST、Omniglot、CelebA、FFHQ等多个数据集上有效控制后验摘套

Conclusion: LR损失函数能够无需特定网络结构限制地有效控制VAE的后验摘套问题

Abstract: Variational autoencoders (VAEs), one of the most widely used generative
models, are known to suffer from posterior collapse, a phenomenon that reduces
the diversity of generated samples. To avoid posterior collapse, many prior
works have tried to control the influence of regularization loss. However, the
trade-off between reconstruction and regularization is not satisfactory. For
this reason, several methods have been proposed to guarantee latent
identifiability, which is the key to avoiding posterior collapse. However, they
require structural constraints on the network architecture. For further
clarification, we define local posterior collapse to reflect the importance of
individual sample points in the data space and to relax the network constraint.
Then, we propose Latent Reconstruction(LR) loss, which is inspired by
mathematical properties of injective and composite functions, to control
posterior collapse without restriction to a specific architecture. We
experimentally evaluate our approach, which controls posterior collapse on
varied datasets such as MNIST, fashionMNIST, Omniglot, CelebA, and FFHQ.

</details>


### [187] [Argos: A Decentralized Federated System for Detection of Traffic Signs in CAVs](https://arxiv.org/abs/2508.12712)
*Seyed Mahdi Haji Seyed Hossein,Alireza Hosseini,Soheil Hajian Manesh,Amirali Shahriary*

Main category: cs.LG

TL;DR: 本文提出了一个用于车辆网络中交通标志检测的联邦学习框架，通过分散式训练避免原始数据共享，解决了隐私和通信挑战。


<details>
  <summary>Details</summary>
Motivation: 联网和自动驾驶车辆每天产生大量传感器数据，集中式机器学习方法在感知任务中存在显著的隐私和通信问题，需要去中心化的解决方案。

Method: 使用轻量级目标检测器在车辆间划分交通标志类别进行专门化本地训练，通过Flower框架在模拟环境中使用FedProx、FedAdam和FedAVG等算法聚合模型参数，评估不同配置包括服务器轮次、本地训练轮数、客户端参与比例和数据分布。

Result: 实验显示：服务器轮次从2增加到20时准确率从0.1以下提升至0.8以上；适中的本地训练轮数（8-10轮）在0.67准确率下提供最优效率；更高的客户端参与比例将泛化能力提升至0.83；FedProx在处理异构性方面优于其他聚合器；非独立同分布数据相比独立同分布数据性能下降；训练时长主要与轮次数而非聚合策略相关。

Conclusion: 这种联邦学习方法可为实际车辆部署提供可扩展的隐私保护解决方案，未来可通过集成鲁棒聚合和通信优化来推进智能交通系统发展。

Abstract: Connected and automated vehicles generate vast amounts of sensor data daily,
raising significant privacy and communication challenges for centralized
machine learning approaches in perception tasks. This study presents a
decentralized, federated learning framework tailored for traffic sign detection
in vehicular networks to enable collaborative model training without sharing
raw data. The framework partitioned traffic sign classes across vehicles for
specialized local training using lightweight object detectors, aggregated model
parameters via algorithms like FedProx, FedAdam and FedAVG in a simulated
environment with the Flower framework, and evaluated multiple configurations
including varying server rounds, local epochs, client participation fractions,
and data distributions. Experiments demonstrated that increasing server rounds
from 2 to 20 boosted accuracy from below 0.1 to over 0.8, moderate local epochs
(8-10) provided optimal efficiency with accuracies around 0.67, higher client
participation fractions enhanced generalization up to 0.83, FedProx
outperformed other aggregators in handling heterogeneity, non-IID data
distributions reduced performance compared to IID, and training duration
primarily scaled with the number of rounds rather than aggregation strategy. We
conclude that this federated approach may offer a scalable, privacy-preserving
solution for real-world vehicular deployments, potentially guiding future
integrations of robust aggregation and communication optimizations to advance
intelligent transportation systems.

</details>


### [188] [A Shift in Perspective on Causality in Domain Generalization](https://arxiv.org/abs/2508.12798)
*Damian Machlanski,Stephanie Riley,Edward Moroshko,Kurt Butler,Panagiotis Dimitrakopoulos,Thomas Melistas,Akchunya Chanchal,Steven McDonagh,Ricardo Silva,Sotirios A. Tsaftaris*

Main category: cs.LG

TL;DR: 本文重新审视因果建模在AI泛化中的作用，挑战了现有领域泛化基准的结论，提出了更细致的因果理论框架


<details>
  <summary>Details</summary>
Motivation: 近期领域泛化基准研究对因果建模能否带来稳健AI泛化的承诺提出了挑战，需要重新审视因果性与泛化的关系

Method: 通过理论分析和文献综述，调和因果性与领域泛化文献中的表面矛盾，建立更细致的理论框架

Result: 提出了对因果性在泛化中作用的更细致理解，并提供了交互式演示

Conclusion: 因果建模在AI泛化中仍具有重要价值，但需要更细致的理论框架来理解其作用机制

Abstract: The promise that causal modelling can lead to robust AI generalization has
been challenged in recent work on domain generalization (DG) benchmarks. We
revisit the claims of the causality and DG literature, reconciling apparent
contradictions and advocating for a more nuanced theory of the role of
causality in generalization. We also provide an interactive demo at
https://chai-uk.github.io/ukairs25-causal-predictors/.

</details>


### [189] [Learning to Steer: Input-dependent Steering for Multimodal LLMs](https://arxiv.org/abs/2508.12815)
*Jayneel Parekh,Pegah Khayatan,Mustafa Shukor,Arnaud Dapogny,Alasdair Newson,Matthieu Cord*

Main category: cs.LG

TL;DR: L2S方法通过训练小型辅助模块预测输入特定的导向向量，实现多模态大语言模型的细粒度导向控制，在减少幻觉和增强安全性方面优于静态基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有导向技术（如均值导向）依赖单一导向向量且独立于输入查询，在处理依赖具体示例的期望行为时存在局限性，例如安全回答需要根据具体问题内容采取不同策略。

Method: 提出L2S方法，使用对比性输入特定提示计算输入特定的线性偏移，并训练小型辅助模块来预测这些在测试时未知的输入特定导向向量。

Result: L2S方法在减少多模态大语言模型的幻觉和增强安全性方面表现出色，性能优于其他静态基线方法。

Conclusion: 输入特定的细粒度导向方法能够更有效地控制多模态大语言模型的行为，特别是在需要根据具体输入内容调整响应策略的场景中。

Abstract: Steering has emerged as a practical approach to enable post-hoc guidance of
LLMs towards enforcing a specific behavior. However, it remains largely
underexplored for multimodal LLMs (MLLMs); furthermore, existing steering
techniques, such as mean steering, rely on a single steering vector, applied
independently of the input query. This paradigm faces limitations when the
desired behavior is dependent on the example at hand. For example, a safe
answer may consist in abstaining from answering when asked for an illegal
activity, or may point to external resources or consultation with an expert
when asked about medical advice. In this paper, we investigate a fine-grained
steering that uses an input-specific linear shift. This shift is computed using
contrastive input-specific prompting. However, the input-specific prompts
required for this approach are not known at test time. Therefore, we propose to
train a small auxiliary module to predict the input-specific steering vector.
Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces
hallucinations and enforces safety in MLLMs, outperforming other static
baselines.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [190] [Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems](https://arxiv.org/abs/2508.12026)
*Szymon Pawlonka,Mikołaj Małkiński,Jacek Mańdziuk*

Main category: cs.AI

TL;DR: 提出了Bongard-RWR+数据集，包含5400个实例，使用VLM生成真实世界图像来表示原始Bongard问题的抽象概念，评估发现VLM在细粒度概念识别上存在困难。


<details>
  <summary>Details</summary>
Motivation: 现有的Bongard问题数据集要么使用合成图像无法完全捕捉真实世界复杂性，要么使用真实图像但概念过于简单，且Bongard-RWR数据集规模太小限制了评估鲁棒性。

Method: 使用Pixtral-12B描述手动策划的图像并生成新描述，用Flux.1-dev从描述合成图像，手动验证生成图像是否忠实反映目标概念，构建了5400个实例的数据集。

Result: 评估显示VLM能够识别粗粒度视觉概念，但在辨别细粒度概念方面持续存在困难。

Conclusion: VLM在抽象视觉推理方面存在局限性，特别是在细粒度概念识别上表现不佳，需要进一步提升推理能力。

Abstract: Bongard Problems (BPs) provide a challenging testbed for abstract visual
reasoning (AVR), requiring models to identify visual concepts fromjust a few
examples and describe them in natural language. Early BP benchmarks featured
synthetic black-and-white drawings, which might not fully capture the
complexity of real-world scenes. Subsequent BP datasets employed real-world
images, albeit the represented concepts are identifiable from high-level image
features, reducing the task complexity. Differently, the recently released
Bongard-RWR dataset aimed at representing abstract concepts formulated in the
original BPs using fine-grained real-world images. Its manual construction,
however, limited the dataset size to just $60$ instances, constraining
evaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset
composed of $5\,400$ instances that represent original BP abstract concepts
using real-world-like images generated via a vision language model (VLM)
pipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually
curated images and generate new descriptions aligned with the underlying
concepts, use Flux.1-dev to synthesize images from these descriptions, and
manually verify that the generated images faithfully reflect the intended
concepts. We evaluate state-of-the-art VLMs across diverse BP formulations,
including binary and multiclass classification, as well as textual answer
generation. Our findings reveal that while VLMs can recognize coarse-grained
visual concepts, they consistently struggle with discerning fine-grained
concepts, highlighting limitations in their reasoning capabilities.

</details>


### [191] [EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding](https://arxiv.org/abs/2508.12687)
*Ashish Seth,Utkarsh Tyagi,Ramaneswaran Selvakumar,Nishit Anand,Sonal Kumar,Sreyan Ghosh,Ramani Duraiswami,Chirag Agarwal,Dinesh Manocha*

Main category: cs.AI

TL;DR: EgoIllusion是首个评估多模态大语言模型在自我中心视频中幻觉问题的基准，包含1400个视频和8000个人工标注问题，测试显示包括GPT-4o和Gemini在内的顶级模型准确率仅59%


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在第三人称和自我中心视频中表现出色，但容易产生连贯但不准确的幻觉响应，需要专门基准来评估和改进这一问题

Method: 构建包含1400个自我中心视频和8000个人工标注问题的基准数据集，设计开放性和封闭性问题来触发视觉和听觉线索的幻觉

Result: 对10个MLLM的评估显示显著挑战，最强模型如GPT-4o和Gemini准确率仅为59%，表明当前模型在自我中心视频理解方面存在严重幻觉问题

Conclusion: EgoIllusion为评估MLLM有效性提供了基础基准，将促进开发幻觉率更低的自我中心MLLM，该基准将开源以确保可复现性

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
performance in complex multimodal tasks. While MLLMs excel at visual perception
and reasoning in third-person and egocentric videos, they are prone to
hallucinations, generating coherent yet inaccurate responses. We present
EgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentric
videos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotated
open and closed-ended questions designed to trigger hallucinations in both
visual and auditory cues in egocentric videos. Evaluations across ten MLLMs
reveal significant challenges, including powerful models like GPT-4o and
Gemini, achieving only 59% accuracy. EgoIllusion lays the foundation in
developing robust benchmarks to evaluate the effectiveness of MLLMs and spurs
the development of better egocentric MLLMs with reduced hallucination rates.
Our benchmark will be open-sourced for reproducibility.

</details>


### [192] [E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model](https://arxiv.org/abs/2508.12854)
*Ronghao Lin,Shuai Shen,Weipeng Hu,Qiaolin He,Aolin Xiong,Li Huang,Haifeng Hu,Yap-peng Tan*

Main category: cs.AI

TL;DR: E3RG是一个基于多模态大语言模型的显式情感驱动共情响应生成系统，通过将多模态共情任务分解为理解、记忆检索和生成三部分，无需额外训练即可产生自然、情感丰富且身份一致的多模态响应。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在文本共情响应生成方面有所改进，但在处理多模态情感内容和保持身份一致性方面仍存在挑战，需要开发更有效的多模态共情响应生成系统。

Method: 将多模态共情响应生成任务分解为三个部分：多模态共情理解、共情记忆检索和多模态响应生成，并整合先进的表达性语音和视频生成模型。

Result: 在零样本和少样本设置下均表现出优越性能，在ACM MM 25的Avatar-based多模态共情挑战中获得Top-1排名。

Conclusion: E3RG系统能够有效生成自然、情感丰富且身份一致的多模态共情响应，无需额外训练，为构建情感智能的人机交互提供了有效解决方案。

Abstract: Multimodal Empathetic Response Generation (MERG) is crucial for building
emotionally intelligent human-computer interactions. Although large language
models (LLMs) have improved text-based ERG, challenges remain in handling
multimodal emotional content and maintaining identity consistency. Thus, we
propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System
based on multimodal LLMs which decomposes MERG task into three parts:
multimodal empathy understanding, empathy memory retrieval, and multimodal
response generation. By integrating advanced expressive speech and video
generative models, E3RG delivers natural, emotionally rich, and
identity-consistent responses without extra training. Experiments validate the
superiority of our system on both zero-shot and few-shot settings, securing
Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.
Our code is available at https://github.com/RH-Lin/E3RG.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [193] [Artificial Intelligence in Rural Healthcare Delivery: Bridging Gaps and Enhancing Equity through Innovation](https://arxiv.org/abs/2508.11738)
*Kiruthika Balakrishnan,Durgadevi Velusamy,Hana E. Hinkle,Zhi Li,Karthikeyan Ramasamy,Hikmat Khan,Srini Ramaswamy,Pir Masoom Shah*

Main category: cs.CY

TL;DR: 这篇系统综述研究探讨了人工智能（AI）在改善农村医疗保健服务中的变革潜力，分析了109项研究，发现AI技术如预测分析、远程医疗平台和自动诊断工具能显著提升农村医疗的可及性、质量和效率。


<details>
  <summary>Details</summary>
Motivation: 农村医疗面临基础设施不足、人力资源短缺和社会经济差距等持续挑战，阻碍了基本医疗服务的获取，需要探索创新解决方案。

Method: 采用系统综述方法，检索2019-2024年间PubMed、Embase、Web of Science、IEEE Xplore和Scopus数据库的109项研究，使用PRISMA指南和Covidence软件进行筛选，并进行主题分析。

Result: 研究发现AI应用（包括多模态基础模型和大型语言模型）在提升农村医疗可及性、质量和效率方面具有显著潜力，能够整合多种数据源支持决策，促进临床文档、分诊、翻译和虚拟辅助等功能。

Conclusion: AI技术有望通过增强人力能力、减少诊断延迟和普及专业知识来革命化农村医疗，但需要克服基础设施限制、数据质量问题和伦理考虑等障碍，需要跨学科合作、数字基础设施投资和监管框架发展。

Abstract: Rural healthcare faces persistent challenges, including inadequate
infrastructure, workforce shortages, and socioeconomic disparities that hinder
access to essential services. This study investigates the transformative
potential of artificial intelligence (AI) in addressing these issues in
underserved rural areas. We systematically reviewed 109 studies published
between 2019 and 2024 from PubMed, Embase, Web of Science, IEEE Xplore, and
Scopus. Articles were screened using PRISMA guidelines and Covidence software.
A thematic analysis was conducted to identify key patterns and insights
regarding AI implementation in rural healthcare delivery. The findings reveal
significant promise for AI applications, such as predictive analytics,
telemedicine platforms, and automated diagnostic tools, in improving healthcare
accessibility, quality, and efficiency. Among these, advanced AI systems,
including Multimodal Foundation Models (MFMs) and Large Language Models (LLMs),
offer particularly transformative potential. MFMs integrate diverse data
sources, such as imaging, clinical records, and bio signals, to support
comprehensive decision-making, while LLMs facilitate clinical documentation,
patient triage, translation, and virtual assistance. Together, these
technologies can revolutionize rural healthcare by augmenting human capacity,
reducing diagnostic delays, and democratizing access to expertise. However,
barriers remain, including infrastructural limitations, data quality concerns,
and ethical considerations. Addressing these challenges requires
interdisciplinary collaboration, investment in digital infrastructure, and the
development of regulatory frameworks. This review offers actionable
recommendations and highlights areas for future research to ensure equitable
and sustainable integration of AI in rural healthcare systems.

</details>


### [194] [Vitamin N: Benefits of Different Forms of Public Greenery for Urban Health](https://arxiv.org/abs/2508.12998)
*Sanja Šćepanović,Sagar Joglekar,Stephen Law,Daniele Quercia,Ke Zhou,Alice Battiston,Rossano Schifanella*

Main category: cs.CY

TL;DR: 城市绿化与健康关联研究，提出了在路绿化和离路绿化的新分类方法，发现日常见到的绿化比传统指标更能预测健康改善效果。


<details>
  <summary>Details</summary>
Motivation: 过去研究中城市绿化与健康关系不一致，原因是官方绿化指标只量化绿地数量或近住性，而忽视了人们日常生活中实际看到和使用绿化的频率。

Method: 结合來自伦敦的航空影像、OpenStreetMap绿化数据、超过10万张Google街景图像的量化绿化以及基于16万条道路段的可达性估计，建立了在路绿化和离路绿化的新分类。将这些测量与国家健康服务系统发放的74.5亿份医疗处方连接进行分析。

Result: 在路绿化比四种广泛使用的官方指标更能预测健康改善效果。高于城市中位数的在路绿化区域高血压药物处方量减少3.68%。如果所有低于中位数的区域达到城市中位数水平，每年处方成本可减少约315万英镑。

Conclusion: 日常生活中看到的绿化比公共但隐秘的绿化更具健康相关性，存在于文献中的常用官方指标有重要限制。

Abstract: Urban greenery is often linked to better health, yet findings from past
research have been inconsistent. One reason is that official greenery metrics
measure the amount or nearness of greenery but ignore how often people actually
may potentially see or use it in daily life. To address this gap, we introduced
a new classification that separates on-road greenery, which people see while
walking through streets, from off-road greenery, which requires planned visits.
We did so by combining aerial imagery of Greater London and greenery data from
OpenStreetMap with quantified greenery from over 100,000 Google Street View
images and accessibility estimates based on 160,000 road segments. We linked
these measures to 7.45 billion medical prescriptions issued by the National
Health Service and processed through our methodology. These prescriptions cover
five conditions: diabetes, hypertension, asthma, depression, and anxiety, as
well as opioid use. As hypothesized, we found that green on-road was more
strongly linked to better health than four widely used official measures. For
example, hypertension prescriptions dropped by 3.68% in wards with on-road
greenery above the median citywide level compared to those below it. If all
below-median wards reached the citywide median in on-road greenery,
prescription costs could fall by up to {\pounds}3.15 million each year. These
results suggest that greenery seen in daily life may be more relevant than
public yet secluded greenery, and that official metrics commonly used in the
literature have important limitations.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [195] [BeeNet: Reconstructing Flower Shapes from Electric Fields using Deep Learning](https://arxiv.org/abs/2508.11724)
*Jake Turley,Ryan A. Palmer,Isaac V. Chenchiah,Daniel Robert*

Main category: q-bio.QM

TL;DR: 使用深度学习模型从蜜蜂与花朵交互产生的电场中重建花朵形状，证明电感觉能够传输丰富的空间信息


<details>
  <summary>Details</summary>
Motivation: 研究节肢动物如传粉者如何利用电场信息来识别环境特征，特别是花朵形状

Method: 模拟蜜蜂与不同花瓣形状花朵的电场交互，使用UNet深度学习模型训练从电场数据重建花朵形状

Result: 模型能够准确重建多种花朵形状，包括训练集外的复杂形状，重建效果在最佳距离处达到峰值

Conclusion: 电感觉能够传输丰富的空间细节信息，为节肢动物环境知觉提供新见解

Abstract: Arthropods, including pollinators, respond to environmental electrical
fields. Here, we show that electric field information can be decoded to
reconstruct environmental features. We develop an algorithm capable of
inferring the shapes of polarisable flowers from the electric field generated
by a nearby charged bee. We simulated electric fields arising from bee flower
interactions for flowers with varying petal geometries. These simulated data
were used to train a deep learning UNet model to recreate petal shapes. The
model accurately reconstructed diverse flower shapes including more complex
flower shapes not included in training. Reconstruction performance peaked at an
optimal bee flower distance, indicating distance-dependent encoding of shape
information. These findings show that electroreception can impart rich spatial
detail, offering insights into arthropod environmental perception.

</details>


### [196] [On the Importance of Behavioral Nuances: Amplifying Non-Obvious Motor Noise Under True Empirical Considerations May Lead to Briefer Assays and Faster Classification Processes](https://arxiv.org/abs/2508.12742)
*Theodoros Bermperidis,Joe Vero,Elizabeth B Torres*

Main category: q-bio.QM

TL;DR: 基于瞬时面部视频的彩集微峰数据类型，通过几何和非线性动力系统方法分析动力学，实现了在短时间采样中保持个性化统计功率的情感计算平台


<details>
  <summary>Details</summary>
Motivation: 解决大规模数据集收集困难与短时间采样可扩展性之间的争议，避免传统大汇总技术因假设正态分布和线性过程而丢失重要信息

Method: 结合从短时(5秒)面部视频中提取的微峰新数据类型与AI驱动的面部网格估计方法，采用几何和非线性动力系统方法分析动力学(速度数据)以捕捉所有面部微峰

Result: 新方法能够捕捉包括不同情感微表情细差在内的所有面部微峰，并提供了区分自闭症个体与普通发育个体动力学和几何模式的新方法

Conclusion: 该情感计算平台成功实现了在短时间采样中保持高统计功率，为自闭症诊断和情感识别提供了更有效的技术方案

Abstract: There is a tradeoff between attaining statistical power with large, difficult
to gather data sets, and producing highly scalable assays that register brief
data samples. Often, as grand-averaging techniques a priori assume
normally-distributed parameters and linear, stationary processes in
biorhythmic, time series data, important information is lost, averaged out as
gross data. We developed an affective computing platform that enables taking
brief data samples while maintaining personalized statistical power. This is
achieved by combining a new data type derived from the micropeaks present in
time series data registered from brief (5-second-long) face videos with recent
advances in AI-driven face-grid estimation methods. By adopting geometric and
nonlinear dynamical systems approaches to analyze the kinematics, especially
the speed data, the new methods capture all facial micropeaks. These include as
well the nuances of different affective micro expressions. We offer new ways to
differentiate dynamical and geometric patterns present in autistic individuals
from those found more commonly in neurotypical development.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [197] [Privacy-Aware Detection of Fake Identity Documents: Methodology, Benchmark, and Improved Detection Methods (FakeIDet2)](https://arxiv.org/abs/2508.11716)
*Javier Muñoz-Haro,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez*

Main category: cs.CR

TL;DR: 本文提出了一种基于片段的隐私保护方法来解决假证件检测中真实数据缺乏问题，并提供了包含90万个真假ID片段的公共数据库FakeIDet2-db以及新的隐私意识假证件检测方法FakeIDet2。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术发展，攻击者能够创造极其真实的假证件，但真实ID数据因隐私问题而缺乏，很难用于训练假证件检测系统。

Method: 提出基于片段的隐私保护方法，构建包含超过900K真假ID片段的公共数据库，并设计新的隐私意识假证件检测方法FakeIDet2。

Result: 提供了一个标准可复现的测试基准，考虑了物理和合成攻击，包括打印、屏幕和复合攻击等多种情况。

Conclusion: 该研究为假证件检测领域提供了解决数据缺乏和隐私问题的有效方法，通过片段化处理和公共数据库的建立，推动了该领域的研究进展。

Abstract: Remote user verification in Internet-based applications is becoming
increasingly important nowadays. A popular scenario for it consists of
submitting a picture of the user's Identity Document (ID) to a service
platform, authenticating its veracity, and then granting access to the
requested digital service. An ID is well-suited to verify the identity of an
individual, since it is government issued, unique, and nontransferable.
However, with recent advances in Artificial Intelligence (AI), attackers can
surpass security measures in IDs and create very realistic physical and
synthetic fake IDs. Researchers are now trying to develop methods to detect an
ever-growing number of these AI-based fakes that are almost indistinguishable
from authentic (bona fide) IDs. In this counterattack effort, researchers are
faced with an important challenge: the difficulty in using real data to train
fake ID detectors. This real data scarcity for research and development is
originated by the sensitive nature of these documents, which are usually kept
private by the ID owners (the users) and the ID Holders (e.g., government,
police, bank, etc.). The main contributions of our study are: 1) We propose and
discuss a patch-based methodology to preserve privacy in fake ID detection
research. 2) We provide a new public database, FakeIDet2-db, comprising over
900K real/fake ID patches extracted from 2,000 ID images, acquired using
different smartphone sensors, illumination and height conditions, etc. In
addition, three physical attacks are considered: print, screen, and composite.
3) We present a new privacy-aware fake ID detection method, FakeIDet2. 4) We
release a standard reproducible benchmark that considers physical and synthetic
attacks from popular databases in the literature.

</details>
