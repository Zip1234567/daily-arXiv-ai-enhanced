<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 52]
- [eess.IV](#eess.IV) [Total: 7]
- [cs.GR](#cs.GR) [Total: 3]
- [physics.geo-ph](#physics.geo-ph) [Total: 3]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Facial Emotion Recognition does not detect feeling unsafe in automated driving](https://arxiv.org/abs/2509.04490)
*Abel van Elburg,Konstantinos Gkentsidis,Mathieu Sarrazin,Sarah Barendswaard,Varun Kotian,Riender Happee*

Main category: cs.CV

TL;DR: 通过驾驶模拟器实验研究自动驾驶风根据风根感知风险，发现车辆运动和皮电导可以预测风险感知，而面部表情识别效果不佳


<details>
  <summary>Details</summary>
Motivation: 研究互联网车辆的公众接受度，重点关注信任和感知安全性在自动驾驶中的作用

Method: 使用32名参与者进行驾驶模拟器实验，测量主观舒适度、车辆运动、面部表情、皮电导、心率和眼动跟踪等多种数据

Result: 动态驾驶风根导致更强的不舒适感；面部表情识别对风险感知评估不可靠；基于车辆运动和皮电导的神经网络模型能够有效预测风险感知

Conclusion: 车辆运动和皮电导可以作为客观风险感知评估的有效方法，减少主观偏差，为自动驾驶安全研究提供新方向

Abstract: Trust and perceived safety play a crucial role in the public acceptance of
automated vehicles. To understand perceived risk, an experiment was conducted
using a driving simulator under two automated driving styles and optionally
introducing a crossing pedestrian. Data was collected from 32 participants,
consisting of continuous subjective comfort ratings, motion, webcam footage for
facial expression, skin conductance, heart rate, and eye tracking. The
continuous subjective perceived risk ratings showed significant discomfort
associated with perceived risk during cornering and braking followed by relief
or even positive comfort on continuing the ride. The dynamic driving style
induced a stronger discomfort as compared to the calm driving style. The
crossing pedestrian did not affect discomfort with the calm driving style but
doubled the comfort decrement with the dynamic driving style. This illustrates
the importance of consequences of critical interactions in risk perception.
Facial expression was successfully analyzed for 24 participants but most
(15/24) did not show any detectable facial reaction to the critical event.
Among the 9 participants who did, 8 showed a Happy expression, and only 4
showed a Surprise expression. Fear was never dominant. This indicates that
facial expression recognition is not a reliable method for assessing perceived
risk in automated vehicles. To predict perceived risk a neural network model
was implemented using vehicle motion and skin conductance. The model correlated
well with reported perceived risk, demonstrating its potential for objective
perceived risk assessment in automated vehicles, reducing subjective bias and
highlighting areas for future research.

</details>


### [2] [PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting](https://arxiv.org/abs/2509.04545)
*Linqing Wang,Ximing Xing,Yiji Cheng,Zhiyuan Zhao,Jiale Tao,Qixun Wang,Ruihuang Li,Xin Li,Mingrui Wu,Xinchi Deng,Chunyu Wang,Qinglin Lu*

Main category: cs.CV

TL;DR: PromptEnhancer是一个通用的提示词重写框架，通过强化学习训练思维链重写器，显著提升文本到图像模型对复杂提示的理解和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在处理复杂用户提示时存在属性绑定、否定和组合关系等方面的困难，导致用户意图与生成结果不匹配。

Method: 提出PromptEnhancer框架，通过训练思维链重写器进行强化学习，使用专门的AlignEvaluator奖励模型提供细粒度反馈，基于24个关键点的系统分类法优化提示重写。

Result: 在HunyuanImage 2.1模型上的实验表明，PromptEnhancer显著提高了图像-文本对齐度，在各种语义和组合挑战中表现优异。

Conclusion: 该框架无需修改预训练模型权重即可提升任何T2I模型性能，并引入了新的人类偏好基准数据集以促进未来研究。

Abstract: Recent advancements in text-to-image (T2I) diffusion models have demonstrated
remarkable capabilities in generating high-fidelity images. However, these
models often struggle to faithfully render complex user prompts, particularly
in aspects like attribute binding, negation, and compositional relationships.
This leads to a significant mismatch between user intent and the generated
output. To address this challenge, we introduce PromptEnhancer, a novel and
universal prompt rewriting framework that enhances any pretrained T2I model
without requiring modifications to its weights. Unlike prior methods that rely
on model-specific fine-tuning or implicit reward signals like image-reward
scores, our framework decouples the rewriter from the generator. We achieve
this by training a Chain-of-Thought (CoT) rewriter through reinforcement
learning, guided by a dedicated reward model we term the AlignEvaluator. The
AlignEvaluator is trained to provide explicit and fine-grained feedback based
on a systematic taxonomy of 24 key points, which are derived from a
comprehensive analysis of common T2I failure modes. By optimizing the CoT
rewriter to maximize the reward from our AlignEvaluator, our framework learns
to generate prompts that are more precisely interpreted by T2I models.
Extensive experiments on the HunyuanImage 2.1 model demonstrate that
PromptEnhancer significantly improves image-text alignment across a wide range
of semantic and compositional challenges. Furthermore, we introduce a new,
high-quality human preference benchmark to facilitate future research in this
direction.

</details>


### [3] [Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model](https://arxiv.org/abs/2509.04548)
*Hongyang Wei,Baixin Xu,Hongbo Liu,Cyrus Wu,Jie Liu,Yi Peng,Peiyu Wang,Zexiang Liu,Jingwen He,Yidan Xietian,Chuanxin Tang,Zidong Wang,Yichen Wei,Liang Hu,Boyi Jiang,William Li,Ying He,Yang Liu,Xuchen Song,Eric Li,Yahui Zhou*

Main category: cs.CV

TL;DR: UniPic2-SD3.5M-Kontext是一个2B参数的DiT模型，基于SD3.5-Medium架构改进，通过渐进式双任务强化策略和大规模预训练，在图像生成和编辑任务上达到SOTA性能，同时可扩展为统一的多模态框架。


<details>
  <summary>Details</summary>
Motivation: 现有的开源多模态模型过于注重参数规模扩展而忽视训练策略优化，限制了效率和性能。需要开发更高效的训练方法来提升模型能力。

Method: 1) 对SD3.5-Medium进行架构修改和大规模高质量数据预训练；2) 提出渐进式双任务强化策略(PDTR)分阶段增强指令跟随和编辑一致性；3) 通过连接器将图像模型与Qwen2.5-VL-7B连接进行联合训练，构建统一多模态模型。

Result: 模型在图像生成和编辑能力上超越了参数规模更大的BAGEL(7B)和Flux-Kontext(12B)模型。构建的UniPic2-Metaquery在多任务上达到顶级性能，验证了训练范式的有效性。

Conclusion: 提出的Skywork UniPic 2.0训练范式通过优化的架构设计和渐进式强化策略，实现了高效的多模态模型训练，在保持较小参数规模的同时获得了卓越的性能表现。

Abstract: Recent advances in multimodal models have demonstrated impressive
capabilities in unified image generation and editing. However, many prominent
open-source models prioritize scaling model parameters over optimizing training
strategies, limiting their efficiency and performance. In this work, we present
UniPic2-SD3.5M-Kontext, a 2B-parameter DiT model based on SD3.5-Medium, which
achieves state-of-the-art image generation and editing while extending
seamlessly into a unified multimodal framework. Our approach begins with
architectural modifications to SD3.5-Medium and large-scale pre-training on
high-quality data, enabling joint text-to-image generation and editing
capabilities. To enhance instruction following and editing consistency, we
propose a novel Progressive Dual-Task Reinforcement strategy (PDTR), which
effectively strengthens both tasks in a staged manner. We empirically validate
that the reinforcement phases for different tasks are mutually beneficial and
do not induce negative interference. After pre-training and reinforcement
strategies, UniPic2-SD3.5M-Kontext demonstrates stronger image generation and
editing capabilities than models with significantly larger generation
parameters-including BAGEL (7B) and Flux-Kontext (12B). Furthermore, following
the MetaQuery, we connect the UniPic2-SD3.5M-Kontext and Qwen2.5-VL-7B via a
connector and perform joint training to launch a unified multimodal model
UniPic2-Metaquery. UniPic2-Metaquery integrates understanding, generation, and
editing, achieving top-tier performance across diverse tasks with a simple and
scalable training paradigm. This consistently validates the effectiveness and
generalizability of our proposed training paradigm, which we formalize as
Skywork UniPic 2.0.

</details>


### [4] [Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping](https://arxiv.org/abs/2509.04582)
*Jingyi Lu,Kai Han*

Main category: cs.CV

TL;DR: Inpaint4Drag是一个基于像素空间的拖拽图像编辑框架，通过双向变形和图像修复实现实时编辑，相比现有方法大幅提升交互体验和精度


<details>
  <summary>Details</summary>
Motivation: 现有拖拽图像编辑方法主要依赖生成模型的潜在空间操作，导致精度有限、反馈延迟和模型特定约束，需要更直接高效的像素空间解决方案

Method: 将拖拽编辑分解为像素空间的双向变形和图像修复，将图像区域视为可变形材料，保持自然形状，并将拖拽输入转换为标准修复格式

Result: 实现实时变形预览(0.01秒)和高效修复(0.3秒)，在512x512分辨率下显著优于需要数分钟编辑的现有方法

Conclusion: 该方法作为通用适配器可兼容任何修复模型，无需架构修改，自动继承修复技术的未来改进，在视觉质量和精确控制方面表现优异

Abstract: Drag-based image editing has emerged as a powerful paradigm for intuitive
image manipulation. However, existing approaches predominantly rely on
manipulating the latent space of generative models, leading to limited
precision, delayed feedback, and model-specific constraints. Accordingly, we
present Inpaint4Drag, a novel framework that decomposes drag-based editing into
pixel-space bidirectional warping and image inpainting. Inspired by elastic
object deformation in the physical world, we treat image regions as deformable
materials that maintain natural shape under user manipulation. Our method
achieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at
512x512 resolution, significantly improving the interaction experience compared
to existing methods that require minutes per edit. By transforming drag inputs
directly into standard inpainting formats, our approach serves as a universal
adapter for any inpainting model without architecture modification,
automatically inheriting all future improvements in inpainting technology.
Extensive experiments demonstrate that our method achieves superior visual
quality and precise control while maintaining real-time performance. Project
page: https://visual-ai.github.io/inpaint4drag/

</details>


### [5] [DisPatch: Disarming Adversarial Patches in Object Detection with Diffusion Models](https://arxiv.org/abs/2509.04597)
*Jin Ma,Mohammed Aldeen,Christopher Salas,Feng Luo,Mashrur Chowdhury,Mert Pesé,Long Cheng*

Main category: cs.CV

TL;DR: DISPATCH是一个基于扩散模型的物体检测防御框架，采用"再生和修正"策略来对抗对抗性补丁攻击，无需先验知识即可有效防御多种攻击类型。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的物体检测器容易受到对抗性补丁攻击，这些攻击可以隐藏真实物体或创建虚假物体，造成严重后果。现有防御方法需要针对特定攻击设计，缺乏通用性和鲁棒性。

Method: 利用扩散模型的生成能力重新生成整个图像，使其与良性数据对齐，然后通过修正过程识别并替换对抗性区域为重新生成的良性对应部分。该方法与攻击无关，无需了解现有补丁的先验知识。

Result: 在多个检测器和攻击方法上的实验表明，DISPATCH在隐藏攻击上达到89.3%的mAP.5分数，在非目标创建攻击上将攻击成功率降低至24.8%，始终优于现有最先进防御方法。

Conclusion: DISPATCH是对抗性补丁攻击的有效、通用且鲁棒的防御方案，对自适应攻击保持强鲁棒性，是物体检测系统实用可靠的防御方法。

Abstract: Object detection is fundamental to various real-world applications, such as
security monitoring and surveillance video analysis. Despite their
advancements, state-of-theart object detectors are still vulnerable to
adversarial patch attacks, which can be easily applied to real-world objects to
either conceal actual items or create non-existent ones, leading to severe
consequences. Given the current diversity of adversarial patch attacks and
potential unknown threats, an ideal defense method should be effective,
generalizable, and robust against adaptive attacks. In this work, we introduce
DISPATCH, the first diffusion-based defense framework for object detection.
Unlike previous works that aim to "detect and remove" adversarial patches,
DISPATCH adopts a "regenerate and rectify" strategy, leveraging generative
models to disarm attack effects while preserving the integrity of the input
image. Specifically, we utilize the in-distribution generative power of
diffusion models to regenerate the entire image, aligning it with benign data.
A rectification process is then employed to identify and replace adversarial
regions with their regenerated benign counterparts. DISPATCH is attack-agnostic
and requires no prior knowledge of the existing patches. Extensive experiments
across multiple detectors and attacks demonstrate that DISPATCH consistently
outperforms state-of-the-art defenses on both hiding attacks and creating
attacks, achieving the best overall mAP.5 score of 89.3% on hiding attacks, and
lowering the attack success rate to 24.8% on untargeted creating attacks.
Moreover, it maintains strong robustness against adaptive attacks, making it a
practical and reliable defense for object detection systems.

</details>


### [6] [WATCH: World-aware Allied Trajectory and pose reconstruction for Camera and Human](https://arxiv.org/abs/2509.04600)
*Qijun Ying,Zhongyuan Hu,Rui Zhang,Ronghui Li,Yu Lu,Zijiao Zeng*

Main category: cs.CV

TL;DR: WATCH是一个从单目视频重建全球人体运动的统一框架，通过解析航向角分解和相机轨迹整合机制，解决了相机方向信息利用不足和相机平移线索整合无效的问题，在野外基准测试中达到了最先进的轨迹重建性能。


<details>
  <summary>Details</summary>
Motivation: 从野外单目视频重建全球人体运动在VR、图形和机器人应用中需求日益增长，但面临深度模糊、运动模糊以及相机与人体运动纠缠等挑战。现有以人体运动为中心的方法在利用相机方向信息和整合相机平移线索方面存在不足。

Method: 提出了WATCH框架，包含两个核心创新：1）解析航向角分解技术，相比现有几何方法具有更好的效率和可扩展性；2）受世界模型启发的相机轨迹整合机制，提供有效利用相机平移信息的途径。

Result: 在野外基准测试中，WATCH在端到端轨迹重建方面达到了最先进的性能，证明了联合建模相机-人体运动关系的有效性。

Conclusion: 该工作为解决相机平移整合这一长期挑战提供了新的见解，展示了联合建模相机-人体运动关系的有效性，代码将公开提供。

Abstract: Global human motion reconstruction from in-the-wild monocular videos is
increasingly demanded across VR, graphics, and robotics applications, yet
requires accurate mapping of human poses from camera to world coordinates-a
task challenged by depth ambiguity, motion ambiguity, and the entanglement
between camera and human movements. While human-motion-centric approaches excel
in preserving motion details and physical plausibility, they suffer from two
critical limitations: insufficient exploitation of camera orientation
information and ineffective integration of camera translation cues. We present
WATCH (World-aware Allied Trajectory and pose reconstruction for Camera and
Human), a unified framework addressing both challenges. Our approach introduces
an analytical heading angle decomposition technique that offers superior
efficiency and extensibility compared to existing geometric methods.
Additionally, we design a camera trajectory integration mechanism inspired by
world models, providing an effective pathway for leveraging camera translation
information beyond naive hard-decoding approaches. Through experiments on
in-the-wild benchmarks, WATCH achieves state-of-the-art performance in
end-to-end trajectory reconstruction. Our work demonstrates the effectiveness
of jointly modeling camera-human motion relationships and offers new insights
for addressing the long-standing challenge of camera translation integration in
global human motion reconstruction. The code will be available publicly.

</details>


### [7] [Sali4Vid: Saliency-Aware Video Reweighting and Adaptive Caption Retrieval for Dense Video Captioning](https://arxiv.org/abs/2509.04602)
*MinJu Jeon,Si-Woo Kim,Ye-Chan Kim,HyunGee Kim,Dong-Jin Kim*

Main category: cs.CV

TL;DR: Sali4Vid是一个基于显著性的密集视频字幕框架，通过视频重加权和自适应字幕检索来解决现有方法的时间戳监督不足和固定大小视频块检索的问题，在YouCook2和ViTT数据集上达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有端到端密集视频字幕方法存在两个主要问题：(1)时间戳监督仅应用于文本，而将所有视频帧同等对待；(2)从固定大小的视频块中检索字幕，忽略了场景转换。

Method: 提出Sali4Vid框架，包含两个核心组件：显著性感知视频重加权（将时间戳标注转换为基于sigmoid的帧重要性权重）和基于语义的自适应字幕检索（通过帧相似性分割视频以捕捉场景转换并改进字幕检索）。

Result: 在YouCook2和ViTT数据集上取得了最先进的结果，证明了联合改进视频加权和检索对密集视频字幕的益处。

Conclusion: Sali4Vid通过简单而有效的显著性感知框架，成功解决了现有方法的局限性，显著提升了密集视频字幕的性能，为视频理解任务提供了新的思路。

Abstract: Dense video captioning aims to temporally localize events in video and
generate captions for each event. While recent works propose end-to-end models,
they suffer from two limitations: (1) applying timestamp supervision only to
text while treating all video frames equally, and (2) retrieving captions from
fixed-size video chunks, overlooking scene transitions. To address these, we
propose Sali4Vid, a simple yet effective saliency-aware framework. We introduce
Saliency-aware Video Reweighting, which converts timestamp annotations into
sigmoid-based frame importance weights, and Semantic-based Adaptive Caption
Retrieval, which segments videos by frame similarity to capture scene
transitions and improve caption retrieval. Sali4Vid achieves state-of-the-art
results on YouCook2 and ViTT, demonstrating the benefit of jointly improving
video weighting and retrieval for dense video captioning

</details>


### [8] [UAV-Based Intelligent Traffic Surveillance System: Real-Time Vehicle Detection, Classification, Tracking, and Behavioral Analysis](https://arxiv.org/abs/2509.04624)
*Ali Khanpour,Tianyi Wang,Afra Vahidi-Shams,Wim Ectors,Farzam Nakhaie,Amirhossein Taheri,Christian Claudel*

Main category: cs.CV

TL;DR: 基于无人机的交通监控系统，通过多尺度模板匹配和卡尔曼滤波实现车辆检测跟踪，准确率达91.8%，并能自动识别交通违法行为，支持智能城市交通管理


<details>
  <summary>Details</summary>
Motivation: 传统交通监控系统存在覆盖范围有限、适应性差和可扩展性不足的问题，需要开发更先进的监控解决方案

Method: 利用多尺度多角度模板匹配、卡尔曼滤波和单应性校准处理无人机采集的航拍视频数据，结合地理围栏、运动滤波和轨迹偏差分析技术

Result: 在200米高空实现91.8%检测精度、90.5% F1分数，跟踪指标MOTA 92.1%、MOTP 93.7%，能分类5种车辆类型并自动检测交通违法行为

Conclusion: 该系统具有可扩展性、准确性和实用性，可作为下一代智能城市的基础设施独立交通监控解决方案

Abstract: Traffic congestion and violations pose significant challenges for urban
mobility and road safety. Traditional traffic monitoring systems, such as fixed
cameras and sensor-based methods, are often constrained by limited coverage,
low adaptability, and poor scalability. To address these challenges, this paper
introduces an advanced unmanned aerial vehicle (UAV)-based traffic surveillance
system capable of accurate vehicle detection, classification, tracking, and
behavioral analysis in real-world, unconstrained urban environments. The system
leverages multi-scale and multi-angle template matching, Kalman filtering, and
homography-based calibration to process aerial video data collected from
altitudes of approximately 200 meters. A case study in urban area demonstrates
robust performance, achieving a detection precision of 91.8%, an F1-score of
90.5%, and tracking metrics (MOTA/MOTP) of 92.1% and 93.7%, respectively.
Beyond precise detection, the system classifies five vehicle types and
automatically detects critical traffic violations, including unsafe lane
changes, illegal double parking, and crosswalk obstructions, through the fusion
of geofencing, motion filtering, and trajectory deviation analysis. The
integrated analytics module supports origin-destination tracking, vehicle count
visualization, inter-class correlation analysis, and heatmap-based congestion
modeling. Additionally, the system enables entry-exit trajectory profiling,
vehicle density estimation across road segments, and movement direction
logging, supporting comprehensive multi-scale urban mobility analytics.
Experimental results confirms the system's scalability, accuracy, and practical
relevance, highlighting its potential as an enforcement-aware,
infrastructure-independent traffic monitoring solution for next-generation
smart cities.

</details>


### [9] [VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation](https://arxiv.org/abs/2509.04669)
*Mustafa Munir,Alex Zhang,Radu Marculescu*

Main category: cs.CV

TL;DR: VCMamba是一个结合CNN和Mamba SSM优势的新型视觉骨干网络，通过卷积块提取局部特征，多向Mamba块建模长距离依赖，在ImageNet和ADE20K上取得了优异性能


<details>
  <summary>Details</summary>
Motivation: ViTs和SSMs在全局建模方面表现出色但缺乏局部特征提取能力，CNNs擅长局部特征但缺乏全局推理能力，需要结合两者优势

Method: 使用卷积stem和分层结构，早期阶段用卷积块提取局部特征，后期阶段用多向Mamba块建模长距离依赖和全局上下文

Result: VCMamba-B在ImageNet-1K达到82.6% top-1准确率，比PlainMamba-L3高0.3%且参数少37%；在ADE20K达到47.1 mIoU，比EfficientFormer-L7高2.0 mIoU且参数少62%

Conclusion: VCMamba成功结合了CNN的局部特征提取能力和Mamba SSM的全局建模能力，在保持线性复杂度的同时实现了优异的性能表现

Abstract: Recent advances in Vision Transformers (ViTs) and State Space Models (SSMs)
have challenged the dominance of Convolutional Neural Networks (CNNs) in
computer vision. ViTs excel at capturing global context, and SSMs like Mamba
offer linear complexity for long sequences, yet they do not capture
fine-grained local features as effectively as CNNs. Conversely, CNNs possess
strong inductive biases for local features but lack the global reasoning
capabilities of transformers and Mamba. To bridge this gap, we introduce
\textit{VCMamba}, a novel vision backbone that integrates the strengths of CNNs
and multi-directional Mamba SSMs. VCMamba employs a convolutional stem and a
hierarchical structure with convolutional blocks in its early stages to extract
rich local features. These convolutional blocks are then processed by later
stages incorporating multi-directional Mamba blocks designed to efficiently
model long-range dependencies and global context. This hybrid design allows for
superior feature representation while maintaining linear complexity with
respect to image resolution. We demonstrate VCMamba's effectiveness through
extensive experiments on ImageNet-1K classification and ADE20K semantic
segmentation. Our VCMamba-B achieves 82.6% top-1 accuracy on ImageNet-1K,
surpassing PlainMamba-L3 by 0.3% with 37% fewer parameters, and outperforming
Vision GNN-B by 0.3% with 64% fewer parameters. Furthermore, VCMamba-B obtains
47.1 mIoU on ADE20K, exceeding EfficientFormer-L7 by 2.0 mIoU while utilizing
62% fewer parameters. Code is available at
https://github.com/Wertyuui345/VCMamba.

</details>


### [10] [Guideline-Consistent Segmentation via Multi-Agent Refinement](https://arxiv.org/abs/2509.04687)
*Vanshika Vats,Ashwani Rathee,James Davis*

Main category: cs.CV

TL;DR: 一种多段落长度标注指南的语义分割方法，通过Worker-Supervisor多段代理协同工作架构和迭代精炼，实现了不需重新训练的指南遵循性分割。


<details>
  <summary>Details</summary>
Motivation: 解决实际应用中语义分割需要遵循复杂长文本标注指南的挑战，传统方法需要费用昂贵的任务特定重训练，而现有开放词汇分割方法在复杂指南前表现异常。

Method: 提出多段代理、训练免费框架，采用Worker-Supervisor精炼架构：Worker负责分割，Supervisor根据检索到的指南评估分割结果，轻量级强化学习停止策略决定循环终止时机。

Result: 在Waymo和ReasonSeg数据集上显著超越了最先进的基线方法，展现出强大的遵循指令能力和良好的泛化性能。

Conclusion: 该方法有效解决了复杂标注指南的遵循问题，为实际应用中的语义分割提供了一种灵活、高效的解决方案。

Abstract: Semantic segmentation in real-world applications often requires not only
accurate masks but also strict adherence to textual labeling guidelines. These
guidelines are typically complex and long, and both human and automated
labeling often fail to follow them faithfully. Traditional approaches depend on
expensive task-specific retraining that must be repeated as the guidelines
evolve. Although recent open-vocabulary segmentation methods excel with simple
prompts, they often fail when confronted with sets of paragraph-length
guidelines that specify intricate segmentation rules. To address this, we
introduce a multi-agent, training-free framework that coordinates
general-purpose vision-language models within an iterative Worker-Supervisor
refinement architecture. The Worker performs the segmentation, the Supervisor
critiques it against the retrieved guidelines, and a lightweight reinforcement
learning stop policy decides when to terminate the loop, ensuring
guideline-consistent masks while balancing resource use. Evaluated on the Waymo
and ReasonSeg datasets, our method notably outperforms state-of-the-art
baselines, demonstrating strong generalization and instruction adherence.

</details>


### [11] [Domain Adaptation for Different Sensor Configurations in 3D Object Detection](https://arxiv.org/abs/2509.04711)
*Satoshi Tanaka,Kok Seang Tan,Isamu Yamashita*

Main category: cs.CV

TL;DR: 提出了两种技术来解决3D目标检测中不同传感器配置的领域适应问题：下游微调和多数据集训练后的部分层微调，在相同地理区域的多传感器配置数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 不同车辆平台部署不同的传感器配置导致点云分布变化，使得在一个配置上训练的模型应用到另一个配置时性能下降，而现有工作主要关注环境领域差距和单一LiDAR内的密度变化，不同传感器配置的领域差距尚未充分探索。

Method: 提出两种技术：1）下游微调（在多数据集训练后进行数据集特定微调）；2）部分层微调（只更新部分层以提高跨配置泛化能力）。使用相同地理区域收集的多个传感器配置配对数据集进行验证。

Result: 联合训练结合下游微调和部分层微调始终优于每个配置的简单联合训练。

Conclusion: 为将3D目标检测模型适应到多样化车辆平台提供了实用且可扩展的解决方案。

Abstract: Recent advances in autonomous driving have underscored the importance of
accurate 3D object detection, with LiDAR playing a central role due to its
robustness under diverse visibility conditions. However, different vehicle
platforms often deploy distinct sensor configurations, causing performance
degradation when models trained on one configuration are applied to another
because of shifts in the point cloud distribution. Prior work on multi-dataset
training and domain adaptation for 3D object detection has largely addressed
environmental domain gaps and density variation within a single LiDAR; in
contrast, the domain gap for different sensor configurations remains largely
unexplored. In this work, we address domain adaptation across different sensor
configurations in 3D object detection. We propose two techniques: Downstream
Fine-tuning (dataset-specific fine-tuning after multi-dataset training) and
Partial Layer Fine-tuning (updating only a subset of layers to improve
cross-configuration generalization). Using paired datasets collected in the
same geographic region with multiple sensor configurations, we show that joint
training with Downstream Fine-tuning and Partial Layer Fine-tuning consistently
outperforms naive joint training for each configuration. Our findings provide a
practical and scalable solution for adapting 3D object detection models to the
diverse vehicle platforms.

</details>


### [12] [CD-Mamba: Cloud detection with long-range spatial dependency modeling](https://arxiv.org/abs/2509.04729)
*Tianxiang Xue,Jiayi Zhao,Jingsheng Li,Changlu Chen,Kun Zhan*

Main category: cs.CV

TL;DR: 提出CD-Mamba混合模型，结合卷积神经网络和Mamba状态空间模型，用于遮蓝遮免遮雾检测，同时抓取矩离空间依赖性和长程大气相似性


<details>
  <summary>Details</summary>
Motivation: 遮蓝遮免遮雾对遥感图像数据完整性和可靠性造成挑战，需要同时处理矩离空间冗余性和长程大气相似性

Method: 设计CD-Mamba混合模型，集成卷积操作和Mamba状态空间模型，能同时抓取像素级细节和片段级长程依赖关系

Result: 大量实验验证了CD-Mamba的有效性，在多种空间尺度上显示出更高的检测精度，性能超过现有方法

Conclusion: CD-Mamba通过结合卷积和Mamba的优势，成功实现了对矩离空间关系和长程依赖性的全面抓取，为遮蓝遮免遮雾检测提供了有效解决方案

Abstract: Remote sensing images are frequently obscured by cloud cover, posing
significant challenges to data integrity and reliability. Effective cloud
detection requires addressing both short-range spatial redundancies and
long-range atmospheric similarities among cloud patches. Convolutional neural
networks are effective at capturing local spatial dependencies, while Mamba has
strong capabilities in modeling long-range dependencies. To fully leverage both
local spatial relations and long-range dependencies, we propose CD-Mamba, a
hybrid model that integrates convolution and Mamba's state-space modeling into
a unified cloud detection network. CD-Mamba is designed to comprehensively
capture pixelwise textural details and long term patchwise dependencies for
cloud detection. This design enables CD-Mamba to manage both pixel-wise
interactions and extensive patch-wise dependencies simultaneously, improving
detection accuracy across diverse spatial scales. Extensive experiments
validate the effectiveness of CD-Mamba and demonstrate its superior performance
over existing methods.

</details>


### [13] [Exploiting Unlabeled Structures through Task Consistency Training for Versatile Medical Image Segmentation](https://arxiv.org/abs/2509.04732)
*Shengqian Zhu,Jiafei Wu,Xiaogang Xu,Chengrong Yu,Ying Song,Zhang Yi,Guangjun Li,Junjie Hu*

Main category: cs.CV

TL;DR: 提出Task Consistency Training (TCT)框架解决医学图像分割中的类别不平衡问题，无需额外模型，通过一致性约束和过滤策略有效利用未标注数据


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中获取所有类别的完整标注成本高昂，部分标注数据集存在严重的类别不平衡问题，现有方法需要额外模型且易受标签噪声影响

Method: TCT框架包含主分割头和多个辅助任务头，通过一致性约束利用未标注解剖结构，采用过滤策略排除低一致性噪声数据，使用统一辅助不确定性加权损失防止特定任务主导

Result: 在8个腹部数据集上的广泛实验证明了方法的有效性

Conclusion: TCT框架能够有效解决类别不平衡问题，无需额外模型，在多样化临床数据集上表现出色

Abstract: Versatile medical image segmentation (VMIS) targets the segmentation of
multiple classes, while obtaining full annotations for all classes is often
impractical due to the time and labor required. Leveraging partially labeled
datasets (PLDs) presents a promising alternative; however, current VMIS
approaches face significant class imbalance due to the unequal category
distribution in PLDs. Existing methods attempt to address this by generating
pseudo-full labels. Nevertheless, these typically require additional models and
often result in potential performance degradation from label noise. In this
work, we introduce a Task Consistency Training (TCT) framework to address class
imbalance without requiring extra models. TCT includes a backbone network with
a main segmentation head (MSH) for multi-channel predictions and multiple
auxiliary task heads (ATHs) for task-specific predictions. By enforcing a
consistency constraint between the MSH and ATH predictions, TCT effectively
utilizes unlabeled anatomical structures. To avoid error propagation from
low-consistency, potentially noisy data, we propose a filtering strategy to
exclude such data. Additionally, we introduce a unified auxiliary
uncertainty-weighted loss (UAUWL) to mitigate segmentation quality declines
caused by the dominance of specific tasks. Extensive experiments on eight
abdominal datasets from diverse clinical sites demonstrate our approach's
effectiveness.

</details>


### [14] [Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A Dual Uncertainty-Aware Training Approach to SAM Optimization](https://arxiv.org/abs/2509.04735)
*Dharsan Ravindran,Kevin Wang,Zhuoyuan Cao,Saleh Abdelrahman,Jeffery Wu*

Main category: cs.CV

TL;DR: SAM和SAM2在恶劣天气条件下表现不佳，本文通过引入不确定性量化方法，提出了两种改进方案：多步微调SAM2和适配UAT方法，在自动驾驶场景中显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉基础模型如SAM和SAM2在一般图像分割基准上表现优异，但在视觉模糊度高的恶劣天气条件下表现不佳，缺乏不确定性量化能力，这限制了其在安全关键型自动驾驶应用中的可靠性。

Method: 1. 为SAM2设计多步微调程序，将不确定性指标直接融入损失函数；2. 将医学影像中的不确定性感知适配器(UAT)适配到驾驶场景中。

Result: 在CamVid、BDD100K和GTA驾驶数据集上的实验表明，UAT-SAM在极端天气条件下优于标准SAM，而带有不确定性感知损失的SAM2在各种驾驶场景中都取得了改进性能。

Conclusion: 明确的不确定性建模对于在挑战性环境中进行安全关键型自动驾驶具有重要价值，不确定性量化方法能显著提升分割模型在恶劣条件下的鲁棒性。

Abstract: Recent advances in vision foundation models, such as the Segment Anything
Model (SAM) and its successor SAM2, have achieved state-of-the-art performance
on general image segmentation benchmarks. However, these models struggle in
adverse weather conditions where visual ambiguity is high, largely due to their
lack of uncertainty quantification. Inspired by progress in medical imaging,
where uncertainty-aware training has improved reliability in ambiguous cases,
we investigate two approaches to enhance segmentation robustness for autonomous
driving. First, we introduce a multi-step finetuning procedure for SAM2 that
incorporates uncertainty metrics directly into the loss function, improving
overall scene recognition. Second, we adapt the Uncertainty-Aware Adapter
(UAT), originally designed for medical image segmentation, to driving contexts.
We evaluate both methods on CamVid, BDD100K, and GTA driving datasets.
Experiments show that UAT-SAM outperforms standard SAM in extreme weather,
while SAM2 with uncertainty-aware loss achieves improved performance across
diverse driving scenes. These findings underscore the value of explicit
uncertainty modeling for safety-critical autonomous driving in challenging
environments.

</details>


### [15] [WatchHAR: Real-time On-device Human Activity Recognition System for Smartwatches](https://arxiv.org/abs/2509.04736)
*Taeyoung Yeon,Vasco Xu,Henry Hoffmann,Karan Ahuja*

Main category: cs.CV

TL;DR: WatchHAR是一个完全在智能手表上运行的音频和惯性传感器融合的细粒度人类活动识别系统，解决了外部数据处理带来的隐私和延迟问题，实现了5倍处理速度提升和超过90%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决智能手表在无约束环境中完全本地化运行活动识别系统的挑战，特别是隐私保护和延迟问题，实现智能手表作为独立、隐私感知的连续活动跟踪设备的潜力。

Method: 提出新颖的端到端可训练架构，统一传感器数据预处理和推理，优化流水线的每个组件，实现复合性能增益。

Result: 在25个以上活动类别中保持超过90%准确率的同时，处理速度提升5倍；活动事件检测处理时间9.3毫秒，多模态活动分类11.8毫秒，优于最先进模型。

Conclusion: WatchHAR推进了设备端活动识别技术，实现了智能手表作为独立、隐私感知和最小侵入性连续活动跟踪设备的潜力。

Abstract: Despite advances in practical and multimodal fine-grained Human Activity
Recognition (HAR), a system that runs entirely on smartwatches in unconstrained
environments remains elusive. We present WatchHAR, an audio and inertial-based
HAR system that operates fully on smartwatches, addressing privacy and latency
issues associated with external data processing. By optimizing each component
of the pipeline, WatchHAR achieves compounding performance gains. We introduce
a novel architecture that unifies sensor data preprocessing and inference into
an end-to-end trainable module, achieving 5x faster processing while
maintaining over 90% accuracy across more than 25 activity classes. WatchHAR
outperforms state-of-the-art models for event detection and activity
classification while running directly on the smartwatch, achieving 9.3 ms
processing time for activity event detection and 11.8 ms for multimodal
activity classification. This research advances on-device activity recognition,
realizing smartwatches' potential as standalone, privacy-aware, and
minimally-invasive continuous activity tracking devices.

</details>


### [16] [MCANet: A Multi-Scale Class-Specific Attention Network for Multi-Label Post-Hurricane Damage Assessment using UAV Imagery](https://arxiv.org/abs/2509.04757)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: MCANet是一个用于飓风后无人机图像多标签分类的多尺度注意力网络，通过Res2Net骨干网络和多头类别特定注意力机制，在RescueNet数据集上达到91.75%的mAP，优于多个基准模型


<details>
  <summary>Details</summary>
Motivation: 现有CNN方法难以捕捉多尺度空间特征，无法有效区分视觉相似或共现的灾害损伤类型，需要开发更精确的灾后损伤评估方法

Method: 提出MCANet框架：使用Res2Net分层骨干网络丰富多尺度空间上下文，采用多头类别特定残差注意力模块增强判别能力，每个注意力分支关注不同空间粒度

Result: 在RescueNet数据集（4,494张无人机图像）上达到91.75% mAP，优于ResNet、Res2Net、VGG等模型；使用8个注意力头时性能提升至92.35%，对Road Blocked等困难类别的AP提升超过6%

Conclusion: MCANet能够准确定位损伤相关区域，支持可解释性，输出可用于灾后风险制图、应急路由和数字孪生灾害响应。未来可整合灾害知识图谱和多模态大语言模型提升适应性

Abstract: Rapid and accurate post-hurricane damage assessment is vital for disaster
response and recovery. Yet existing CNN-based methods struggle to capture
multi-scale spatial features and to distinguish visually similar or
co-occurring damage types. To address these issues, we propose MCANet, a
multi-label classification framework that learns multi-scale representations
and adaptively attends to spatially relevant regions for each damage category.
MCANet employs a Res2Net-based hierarchical backbone to enrich spatial context
across scales and a multi-head class-specific residual attention module to
enhance discrimination. Each attention branch focuses on different spatial
granularities, balancing local detail with global context. We evaluate MCANet
on the RescueNet dataset of 4,494 UAV images collected after Hurricane Michael.
MCANet achieves a mean average precision (mAP) of 91.75%, outperforming ResNet,
Res2Net, VGG, MobileNet, EfficientNet, and ViT. With eight attention heads,
performance further improves to 92.35%, boosting average precision for
challenging classes such as Road Blocked by over 6%. Class activation mapping
confirms MCANet's ability to localize damage-relevant regions, supporting
interpretability. Outputs from MCANet can inform post-disaster risk mapping,
emergency routing, and digital twin-based disaster response. Future work could
integrate disaster-specific knowledge graphs and multimodal large language
models to improve adaptability to unseen disasters and enrich semantic
understanding for real-world decision-making.

</details>


### [17] [Dynamic Group Detection using VLM-augmented Temporal Groupness Graph](https://arxiv.org/abs/2509.04758)
*Kaname Yokoyama,Chihiro Nakatani,Norimichi Ukita*

Main category: cs.CV

TL;DR: 这篇论文提出了一种动态视频中人群检测方法，通过结合视觉-语言模型和全局优化图解决了动态变化人群的检测问题，在公开数据集上超越了现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 检测复杂人群需要考虑组内成员的局部外观特征和场景的全局上下文。同时，以往方法假设人群在视频中不变，无法处理动态变化的人群结构。

Method: 使用组检测增强的视觉-语言模型(VLM)提取每帧的局部和全局外观特征，然后通过包含所有帧组性概率的图进行全局优化，实现动态人群检测。

Result: 在公开数据集上的实验结果显示，该方法在人群检测任务上超越了现有的最佳方法。

Conclusion: 该方法通过结合VLM特征提取和时序全局优化，有效地解决了动态人群检测的挑战，为视频中复杂人群分析提供了新的解决方案。

Abstract: This paper proposes dynamic human group detection in videos. For detecting
complex groups, not only the local appearance features of in-group members but
also the global context of the scene are important. Such local and global
appearance features in each frame are extracted using a Vision-Language Model
(VLM) augmented for group detection in our method. For further improvement, the
group structure should be consistent over time. While previous methods are
stabilized on the assumption that groups are not changed in a video, our method
detects dynamically changing groups by global optimization using a graph with
all frames' groupness probabilities estimated by our groupness-augmented CLIP
features. Our experimental results demonstrate that our method outperforms
state-of-the-art group detection methods on public datasets. Code:
https://github.com/irajisamurai/VLM-GroupDetection.git

</details>


### [18] [FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph](https://arxiv.org/abs/2509.04772)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: FloodVision是一个零样本洪水深度估计框架，结合GPT-4o的视觉语言能力和结构化知识图谱，通过识别参考物体和统计过滤实现准确深度估计，在110张图像上达到8.17厘米平均绝对误差。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉洪水检测方法存在精度限制和泛化能力差的问题，依赖于固定物体检测器和任务特定训练，需要开发能够跨不同洪水场景准确估计深度的通用方法。

Method: 结合基础视觉语言模型GPT-4o的语义推理能力和结构化领域知识图谱，知识图谱编码常见城市物体的标准真实尺寸，动态识别RGB图像中的参考物体，检索验证高度，估计淹没比例，应用统计异常值过滤计算最终深度值。

Result: 在MyCoast New York的110张众包图像上评估，平均绝对误差为8.17厘米，比GPT-4o基线(10.28厘米)降低20.5%，超越先前基于CNN的方法，在不同场景中泛化良好，近实时运行。

Conclusion: FloodVision系统适合未来集成到数字孪生平台和公民报告应用中，为智慧城市洪水韧性提供有效解决方案，展示了基础模型与领域知识结合在环境监测中的潜力。

Abstract: Timely and accurate floodwater depth estimation is critical for road
accessibility and emergency response. While recent computer vision methods have
enabled flood detection, they suffer from both accuracy limitations and poor
generalization due to dependence on fixed object detectors and task-specific
training. To enable accurate depth estimation that can generalize across
diverse flood scenarios, this paper presents FloodVision, a zero-shot framework
that combines the semantic reasoning abilities of the foundation
vision-language model GPT-4o with a structured domain knowledge graph. The
knowledge graph encodes canonical real-world dimensions for common urban
objects including vehicles, people, and infrastructure elements to ground the
model's reasoning in physical reality. FloodVision dynamically identifies
visible reference objects in RGB images, retrieves verified heights from the
knowledge graph to mitigate hallucination, estimates submergence ratios, and
applies statistical outlier filtering to compute final depth values. Evaluated
on 110 crowdsourced images from MyCoast New York, FloodVision achieves a mean
absolute error of 8.17 cm, reducing the GPT-4o baseline 10.28 cm by 20.5% and
surpassing prior CNN-based methods. The system generalizes well across varying
scenes and operates in near real-time, making it suitable for future
integration into digital twin platforms and citizen-reporting apps for smart
city flood resilience.

</details>


### [19] [Hybrid-Tower: Fine-grained Pseudo-query Interaction and Generation for Text-to-Video Retrieval](https://arxiv.org/abs/2509.04773)
*Bangxiang Lan,Ruobing Xie,Ruixiang Zhao,Xingwu Sun,Zhanhui Kang,Gang Yang,Xirong Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的混合塔框架PIG，通过伪查询生成和细粒度交互，同时实现文本到视频检索的高效果性和高效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有CLIP基础的两塔框架（效果低）和单塔框架（效率低）在文本到视频检索任务中的缺陷，尝试统一两者优势。

Method: 设计了一种伪查询生成器，为每个视频生成伪查询，使得视频特征和文本特征能够在真实查询到达前就进行细粒度交互，同时保持了两塔框架的高效率特性。

Result: 在5个常用的文本-视频检索数据集上，方法较基线在R@1指标上提升1.6%~3.9%，效果接近state-of-the-art级别，同时保持了与两塔框架相当的效率。

Conclusion: 混合塔框架PIG成功地统一了单塔框架的高效果性和两塔框架的高效率，为文本到视频检索领域提供了一种新的解决方案。

Abstract: The Text-to-Video Retrieval (T2VR) task aims to retrieve unlabeled videos by
textual queries with the same semantic meanings. Recent CLIP-based approaches
have explored two frameworks: Two-Tower versus Single-Tower framework, yet the
former suffers from low effectiveness, while the latter suffers from low
efficiency. In this study, we explore a new Hybrid-Tower framework that can
hybridize the advantages of the Two-Tower and Single-Tower framework, achieving
high effectiveness and efficiency simultaneously. We propose a novel hybrid
method, Fine-grained Pseudo-query Interaction and Generation for T2VR, ie, PIG,
which includes a new pseudo-query generator designed to generate a pseudo-query
for each video. This enables the video feature and the textual features of
pseudo-query to interact in a fine-grained manner, similar to the Single-Tower
approaches to hold high effectiveness, even before the real textual query is
received. Simultaneously, our method introduces no additional storage or
computational overhead compared to the Two-Tower framework during the inference
stage, thus maintaining high efficiency. Extensive experiments on five commonly
used text-video retrieval benchmarks demonstrate that our method achieves a
significant improvement over the baseline, with an increase of $1.6\% \sim
3.9\%$ in R@1. Furthermore, our method matches the efficiency of Two-Tower
models while achieving near state-of-the-art performance, highlighting the
advantages of the Hybrid-Tower framework.

</details>


### [20] [Comparative Evaluation of Traditional and Deep Learning Feature Matching Algorithms using Chandrayaan-2 Lunar Data](https://arxiv.org/abs/2509.04775)
*R. Makharia,J. G. Singla,Amitabh,N. Dube,H. Sharma*

Main category: cs.CV

TL;DR: 评估五种特征匹配算法在月球多模态图像配准中的性能，发现深度学习算法SuperGlue在精度和速度上表现最佳，传统方法在极区光照条件下性能下降


<details>
  <summary>Details</summary>
Motivation: 月球探测需要精确的图像配准来进行表面测绘、资源定位和任务规划，但不同传感器（光学、高光谱、雷达）的数据由于分辨率、光照和传感器畸变差异而难以配准

Method: 提出预处理流程（地理参考、分辨率对齐、强度归一化等），评估SIFT、ASIFT、AKAZE、RIFT2和SuperGlue五种特征匹配算法在赤道和极区多模态图像对上的性能

Result: SuperGlue始终获得最低的均方根误差和最快的运行时间，传统方法如SIFT和AKAZE在赤道附近表现良好但在极区光照条件下性能下降

Conclusion: 预处理和学习型方法对于在不同条件下实现鲁棒的月球图像配准至关重要，深度学习算法在跨模态配准中具有显著优势

Abstract: Accurate image registration is critical for lunar exploration, enabling
surface mapping, resource localization, and mission planning. Aligning data
from diverse lunar sensors -- optical (e.g., Orbital High Resolution Camera,
Narrow and Wide Angle Cameras), hyperspectral (Imaging Infrared Spectrometer),
and radar (e.g., Dual-Frequency Synthetic Aperture Radar, Selene/Kaguya
mission) -- is challenging due to differences in resolution, illumination, and
sensor distortion. We evaluate five feature matching algorithms: SIFT, ASIFT,
AKAZE, RIFT2, and SuperGlue (a deep learning-based matcher), using
cross-modality image pairs from equatorial and polar regions. A preprocessing
pipeline is proposed, including georeferencing, resolution alignment, intensity
normalization, and enhancements like adaptive histogram equalization, principal
component analysis, and shadow correction. SuperGlue consistently yields the
lowest root mean square error and fastest runtimes. Classical methods such as
SIFT and AKAZE perform well near the equator but degrade under polar lighting.
The results highlight the importance of preprocessing and learning-based
approaches for robust lunar image registration across diverse conditions.

</details>


### [21] [Toward Accessible Dermatology: Skin Lesion Classification Using Deep Learning Models on Mobile-Acquired Images](https://arxiv.org/abs/2509.04800)
*Asif Newaz,Masum Mushfiq Ishti,A Z M Ashraful Azam,Asif Ur Rahman Adib*

Main category: cs.CV

TL;DR: 本文提出了基于Transformer的移动设备皮肤病变分类方法，在包含50多种皮肤疾病的大型数据集上取得了优异性能，并通过Grad-CAM增强模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统皮肤疾病诊断方法成本高、复杂且在资源有限地区不可用，现有深度学习研究主要局限于皮肤镜数据集和有限疾病类别，需要更贴近真实场景的移动设备采集解决方案。

Method: 构建了移动设备采集的大型皮肤疾病数据集（50+类别），评估了多种CNN和Transformer架构，重点采用Swin Transformer捕获全局上下文特征，并集成Grad-CAM提供可解释性。

Result: Transformer模型（特别是Swin Transformer）表现出最优性能，能够有效捕获全局特征，Grad-CAM成功突出临床相关区域，为模型预测提供透明度。

Conclusion: 基于Transformer的方法在移动设备皮肤病变分类中具有巨大潜力，为资源有限环境下的AI辅助皮肤病筛查和早期诊断提供了可行路径。

Abstract: Skin diseases are among the most prevalent health concerns worldwide, yet
conventional diagnostic methods are often costly, complex, and unavailable in
low-resource settings. Automated classification using deep learning has emerged
as a promising alternative, but existing studies are mostly limited to
dermoscopic datasets and a narrow range of disease classes. In this work, we
curate a large dataset of over 50 skin disease categories captured with mobile
devices, making it more representative of real-world conditions. We evaluate
multiple convolutional neural networks and Transformer-based architectures,
demonstrating that Transformer models, particularly the Swin Transformer,
achieve superior performance by effectively capturing global contextual
features. To enhance interpretability, we incorporate Gradient-weighted Class
Activation Mapping (Grad-CAM), which highlights clinically relevant regions and
provides transparency in model predictions. Our results underscore the
potential of Transformer-based approaches for mobile-acquired skin lesion
classification, paving the way toward accessible AI-assisted dermatological
screening and early diagnosis in resource-limited environments.

</details>


### [22] [Extracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentation](https://arxiv.org/abs/2509.04816)
*Svetlana Pavlitska,Beyza Keskin,Alwin Faßbender,Christian Hubschneider,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 本文提出从混合专家模型(MoE)中提取预测不确定性的方法，无需修改架构即可获得良好校准的不确定性估计，在OOD数据下表现优于集成方法


<details>
  <summary>Details</summary>
Motivation: 提高计算机视觉模型在安全关键应用（如交通场景感知）中的可靠性，需要准确且良好校准的预测不确定性估计

Method: 研究三种从MoE中提取预测不确定性的方法：预测熵、互信息和专家方差；使用两个专家的MoE在A2D2数据集上进行训练和评估

Result: MoE比集成方法产生更可靠的不确定性估计；简单门控机制比复杂类别门控具有更好的路由不确定性校准；增加专家数量可进一步提升不确定性校准

Conclusion: 混合专家模型是量化预测不确定性的有效方法，能够提供良好校准的不确定性估计，特别适用于安全关键应用场景

Abstract: Estimating accurate and well-calibrated predictive uncertainty is important
for enhancing the reliability of computer vision models, especially in
safety-critical applications like traffic scene perception. While ensemble
methods are commonly used to quantify uncertainty by combining multiple models,
a mixture of experts (MoE) offers an efficient alternative by leveraging a
gating network to dynamically weight expert predictions based on the input.
Building on the promising use of MoEs for semantic segmentation in our previous
works, we show that well-calibrated predictive uncertainty estimates can be
extracted from MoEs without architectural modifications. We investigate three
methods to extract predictive uncertainty estimates: predictive entropy, mutual
information, and expert variance. We evaluate these methods for an MoE with two
experts trained on a semantical split of the A2D2 dataset. Our results show
that MoEs yield more reliable uncertainty estimates than ensembles in terms of
conditional correctness metrics under out-of-distribution (OOD) data.
Additionally, we evaluate routing uncertainty computed via gate entropy and
find that simple gating mechanisms lead to better calibration of routing
uncertainty estimates than more complex classwise gates. Finally, our
experiments on the Cityscapes dataset suggest that increasing the number of
experts can further enhance uncertainty calibration. Our code is available at
https://github.com/KASTEL-MobilityLab/mixtures-of-experts/.

</details>


### [23] [Exploring Non-Local Spatial-Angular Correlations with a Hybrid Mamba-Transformer Framework for Light Field Super-Resolution](https://arxiv.org/abs/2509.04824)
*Haosong Liu,Xiancheng Zhu,Huanqiang Zeng,Jianqing Zhu,Jiuwen Cao,Junhui Hou*

Main category: cs.CV

TL;DR: 提出LFMT框架，结合Mamba和Transformer优势，通过子空间简单扫描策略和双阶段建模，在光场图像超分辨率任务中实现高效特征提取和性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有Mamba方法在光场图像超分辨率中存在多方向扫描策略效率低下、特征提取冗余的问题，且状态空间模型在保持空间-角度和视差信息方面存在局限。

Method: 提出子空间简单扫描策略(Sub-SS)和子空间简单Mamba块(SSMB)；采用双阶段建模：第一阶段使用空间-角度残差子空间Mamba块(SA-RSMB)进行浅层特征提取，第二阶段使用极平面Mamba块(EPMB)和极平面Transformer块(EPTB)的双分支并行结构进行深度特征精炼。

Result: LFMT在真实和合成光场数据集上显著优于当前最先进方法，在保持低计算复杂度的同时实现了性能的实质性提升。

Conclusion: LFMT框架成功整合了Mamba和Transformer的优势，能够全面探索空间、角度和极平面域的信息，为光场图像超分辨率提供了有效的解决方案。

Abstract: Recently, Mamba-based methods, with its advantage in long-range information
modeling and linear complexity, have shown great potential in optimizing both
computational cost and performance of light field image super-resolution
(LFSR). However, current multi-directional scanning strategies lead to
inefficient and redundant feature extraction when applied to complex LF data.
To overcome this challenge, we propose a Subspace Simple Scanning (Sub-SS)
strategy, based on which we design the Subspace Simple Mamba Block (SSMB) to
achieve more efficient and precise feature extraction. Furthermore, we propose
a dual-stage modeling strategy to address the limitation of state space in
preserving spatial-angular and disparity information, thereby enabling a more
comprehensive exploration of non-local spatial-angular correlations.
Specifically, in stage I, we introduce the Spatial-Angular Residual Subspace
Mamba Block (SA-RSMB) for shallow spatial-angular feature extraction; in stage
II, we use a dual-branch parallel structure combining the Epipolar Plane Mamba
Block (EPMB) and Epipolar Plane Transformer Block (EPTB) for deep epipolar
feature refinement. Building upon meticulously designed modules and strategies,
we introduce a hybrid Mamba-Transformer framework, termed LFMT. LFMT integrates
the strengths of Mamba and Transformer models for LFSR, enabling comprehensive
information exploration across spatial, angular, and epipolar-plane domains.
Experimental results demonstrate that LFMT significantly outperforms current
state-of-the-art methods in LFSR, achieving substantial improvements in
performance while maintaining low computational complexity on both real-word
and synthetic LF datasets.

</details>


### [24] [PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination](https://arxiv.org/abs/2509.04833)
*Ming Dai,Wenxuan Cheng,Jiedong Zhuang,Jiang-jiang Liu,Hongshen Zhao,Zhenhua Feng,Wankou Yang*

Main category: cs.CV

TL;DR: PropVG是一个端到端的基于proposal的视觉定位框架，首次将前景目标proposal生成与指代对象理解无缝集成，无需额外检测器，通过对比学习和多粒度判别提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有端到端视觉定位方法仅依赖指代目标进行监督，忽略了潜在前景目标的益处，且缺乏多粒度判别能力，在复杂场景中对象识别不够鲁棒。

Method: 提出PropVG框架，包含对比式指代评分模块（CRS）在句子和单词级别进行对比学习，以及多粒度目标判别模块（MTD）融合对象和语义级信息。

Result: 在gRefCOCO、Ref-ZOM、R-RefCOCO和RefCOCO等多个基准测试上进行了广泛实验，证明了PropVG的有效性。

Conclusion: PropVG成功解决了现有方法的局限性，通过集成proposal生成和对比学习机制，显著提升了视觉定位的性能和鲁棒性。

Abstract: Recent advances in visual grounding have largely shifted away from
traditional proposal-based two-stage frameworks due to their inefficiency and
high computational complexity, favoring end-to-end direct reference paradigms.
However, these methods rely exclusively on the referred target for supervision,
overlooking the potential benefits of prominent prospective targets. Moreover,
existing approaches often fail to incorporate multi-granularity discrimination,
which is crucial for robust object identification in complex scenarios. To
address these limitations, we propose PropVG, an end-to-end proposal-based
framework that, to the best of our knowledge, is the first to seamlessly
integrate foreground object proposal generation with referential object
comprehension without requiring additional detectors. Furthermore, we introduce
a Contrastive-based Refer Scoring (CRS) module, which employs contrastive
learning at both sentence and word levels to enhance the capability in
understanding and distinguishing referred objects. Additionally, we design a
Multi-granularity Target Discrimination (MTD) module that fuses object- and
semantic-level information to improve the recognition of absent targets.
Extensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO
(REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes and
models are available at https://github.com/Dmmm1997/PropVG.

</details>


### [25] [TemporalFlowViz: Parameter-Aware Visual Analytics for Interpreting Scramjet Combustion Evolution](https://arxiv.org/abs/2509.04834)
*Yifei Jia,Shiyu Cheng,Yu Dong,Guan Li,Dong Tian,Ruixiao Peng,Xuyi Lu,Yu Wang,Wei Yao,Guihua Shan*

Main category: cs.CV

TL;DR: TemporalFlowViz是一个可视化分析工作流和系统，用于分析超燃冲压发动机燃烧模拟中的时间流场数据，通过视觉转换器提取特征、聚类分析和视觉语言模型生成自然语言摘要，支持专家驱动的燃烧模式发现和解释。


<details>
  <summary>Details</summary>
Motivation: 超燃冲压发动机燃烧模拟产生的大规模高维时间流场数据对视觉解释、特征区分和跨案例比较提出了重大挑战，需要专门的可视化分析工具来支持专家理解复杂的燃烧动力学。

Method: 使用预训练的Vision Transformers从时间序列流场图像中提取高维嵌入，应用降维和基于密度的聚类来发现潜在燃烧模式，构建嵌入空间中的时间轨迹来跟踪模拟演化，并通过视觉语言模型生成自然语言摘要。

Result: 通过两个专家案例研究和专家反馈证明，TemporalFlowViz能够增强假设生成、支持可解释的模式发现，并促进大规模超燃冲压发动机燃烧分析中的知识发现。

Conclusion: TemporalFlowViz提供了一个有效的参数感知可视化分析工作流，成功桥接了潜在表示与专家推理之间的差距，为复杂燃烧动力学的理解提供了有力工具。

Abstract: Understanding the complex combustion dynamics within scramjet engines is
critical for advancing high-speed propulsion technologies. However, the large
scale and high dimensionality of simulation-generated temporal flow field data
present significant challenges for visual interpretation, feature
differentiation, and cross-case comparison. In this paper, we present
TemporalFlowViz, a parameter-aware visual analytics workflow and system
designed to support expert-driven clustering, visualization, and interpretation
of temporal flow fields from scramjet combustion simulations. Our approach
leverages hundreds of simulated combustion cases with varying initial
conditions, each producing time-sequenced flow field images. We use pretrained
Vision Transformers to extract high-dimensional embeddings from these frames,
apply dimensionality reduction and density-based clustering to uncover latent
combustion modes, and construct temporal trajectories in the embedding space to
track the evolution of each simulation over time. To bridge the gap between
latent representations and expert reasoning, domain specialists annotate
representative cluster centroids with descriptive labels. These annotations are
used as contextual prompts for a vision-language model, which generates
natural-language summaries for individual frames and full simulation cases. The
system also supports parameter-based filtering, similarity-based case
retrieval, and coordinated multi-view exploration to facilitate in-depth
analysis. We demonstrate the effectiveness of TemporalFlowViz through two
expert-informed case studies and expert feedback, showing TemporalFlowViz
enhances hypothesis generation, supports interpretable pattern discovery, and
enhances knowledge discovery in large-scale scramjet combustion analysis.

</details>


### [26] [Pose-Free 3D Quantitative Phase Imaging of Flowing Cellular Populations](https://arxiv.org/abs/2509.04848)
*Enze Ye,Wei Lin,Shaochi Ren,Yakun Liu,Xiaoping Li,Hao Wang,He Sun,Feng Pan*

Main category: cs.CV

TL;DR: OmniFHT是一个无需姿态信息的3D折射率重建框架，通过傅里叶衍射定理和隐式神经表示，实现了对流动细胞的高通量断层成像，支持任意几何形状和多轴旋转。


<details>
  <summary>Details</summary>
Motivation: 当前3D定量相位成像方法假设细胞进行均匀单轴旋转且需要已知每个帧的姿态，这限制了方法对近球形细胞的适用性，无法准确成像具有复杂旋转的不规则形状细胞，只能分析细胞群体的子集。

Method: 利用傅里叶衍射定理和隐式神经表示(INRs)，通过联合优化每个细胞的未知旋转轨迹和体积结构，在弱散射假设下实现3D折射率重建。

Result: OmniFHT能够从稀疏采样投影和有限角度覆盖中实现准确重建，仅需10个视图或120度角度范围即可获得高保真结果。

Conclusion: OmniFHT首次实现了对整个流动细胞群体的原位高通量断层成像，为流式细胞术平台提供了可扩展且无偏见的无标记形态计量分析解决方案。

Abstract: High-throughput 3D quantitative phase imaging (QPI) in flow cytometry enables
label-free, volumetric characterization of individual cells by reconstructing
their refractive index (RI) distributions from multiple viewing angles during
flow through microfluidic channels. However, current imaging methods assume
that cells undergo uniform, single-axis rotation, which require their poses to
be known at each frame. This assumption restricts applicability to
near-spherical cells and prevents accurate imaging of irregularly shaped cells
with complex rotations. As a result, only a subset of the cellular population
can be analyzed, limiting the ability of flow-based assays to perform robust
statistical analysis. We introduce OmniFHT, a pose-free 3D RI reconstruction
framework that leverages the Fourier diffraction theorem and implicit neural
representations (INRs) for high-throughput flow cytometry tomographic imaging.
By jointly optimizing each cell's unknown rotational trajectory and volumetric
structure under weak scattering assumptions, OmniFHT supports arbitrary cell
geometries and multi-axis rotations. Its continuous representation also allows
accurate reconstruction from sparsely sampled projections and restricted
angular coverage, producing high-fidelity results with as few as 10 views or
only 120 degrees of angular range. OmniFHT enables, for the first time, in
situ, high-throughput tomographic imaging of entire flowing cell populations,
providing a scalable and unbiased solution for label-free morphometric analysis
in flow cytometry platforms.

</details>


### [27] [CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus](https://arxiv.org/abs/2509.04859)
*Hannah Schieber,Dominik Frischmann,Simon Boche,Victor Schaack,Angela Schoellig,Stefan Leutenegger,Daniel Roth*

Main category: cs.CV

TL;DR: CoRe-GS是一种用于移动自主空中机器人的语义3D高斯重建方法，通过粗分割和颜色过滤实现快速高质量的对象重建，训练时间比完整语义GS减少约25%。


<details>
  <summary>Details</summary>
Motivation: 移动自主机器人应用（如远程指导和灾难响应）需要准确的3D重建和快速场景处理，传统方法重建整个场景效率低下，而专注于特定兴趣点(PoIs)更为高效。

Method: 首先使用语义高斯泼溅生成粗分割场景，然后通过新颖的基于颜色的有效过滤方法对语义对象进行细化，实现有效的对象隔离。

Result: 在两个数据集（SCRREAM真实户外和NeRDS 360合成室内）上评估，显示运行时间减少且新视角合成质量更高，训练时间比完整语义GS训练周期减少约四分之一。

Conclusion: CoRe-GS方法在保持高质量重建的同时显著减少了训练时间，为移动机器人应用提供了高效的语义3D重建解决方案。

Abstract: Mobile reconstruction for autonomous aerial robotics holds strong potential
for critical applications such as tele-guidance and disaster response. These
tasks demand both accurate 3D reconstruction and fast scene processing. Instead
of reconstructing the entire scene in detail, it is often more efficient to
focus on specific objects, i.e., points of interest (PoIs). Mobile robots
equipped with advanced sensing can usually detect these early during data
acquisition or preliminary analysis, reducing the need for full-scene
optimization. Gaussian Splatting (GS) has recently shown promise in delivering
high-quality novel view synthesis and 3D representation by an incremental
learning process. Extending GS with scene editing, semantics adds useful
per-splat features to isolate objects effectively.
  Semantic 3D Gaussian editing can already be achieved before the full training
cycle is completed, reducing the overall training time. Moreover, the
semantically relevant area, the PoI, is usually already known during capturing.
To balance high-quality reconstruction with reduced training time, we propose
CoRe-GS. We first generate a coarse segmentation-ready scene with semantic GS
and then refine it for the semantic object using our novel color-based
effective filtering for effective object isolation. This is speeding up the
training process to be about a quarter less than a full training cycle for
semantic GS. We evaluate our approach on two datasets, SCRREAM (real-world,
outdoor) and NeRDS 360 (synthetic, indoor), showing reduced runtime and higher
novel-view-synthesis quality.

</details>


### [28] [Cryo-RL: automating prostate cancer cryoablation planning with reinforcement learning](https://arxiv.org/abs/2509.04886)
*Trixia Simangan,Ahmed Nadeem Abbasi,Yipeng Hu,Shaheer U. Saeed*

Main category: cs.CV

TL;DR: Cryo-RL是一个基于强化学习的冷冻消融针放置规划框架，通过模拟临床环境和奖励函数学习最优放置策略，在583例前列腺癌病例中表现优于自动化基线方法，达到专家水平且规划时间大幅减少。


<details>
  <summary>Details</summary>
Motivation: 当前前列腺癌冷冻消融治疗的手动规划过程依赖专家经验、耗时且质量不稳定，需要自动化解决方案来提高治疗质量和可扩展性。

Method: 将冷冻消融规划建模为马尔可夫决策过程，在模拟环境中通过强化学习代理顺序选择冷冻消融针位置和冰球直径，基于肿瘤覆盖率的奖励函数指导学习最优策略。

Result: 在583例回顾性病例中，Cryo-RL相比最佳几何优化基线方法Dice系数提升超过8个百分点，达到人类专家水平，且规划时间显著减少。

Conclusion: 强化学习能够提供临床可行、可重复且高效的冷冻消融规划方案，具有重要的临床应用潜力。

Abstract: Cryoablation is a minimally invasive localised treatment for prostate cancer
that destroys malignant tissue during de-freezing, while sparing surrounding
healthy structures. Its success depends on accurate preoperative planning of
cryoprobe placements to fully cover the tumour and avoid critical anatomy. This
planning is currently manual, expertise-dependent, and time-consuming, leading
to variability in treatment quality and limited scalability. In this work, we
introduce Cryo-RL, a reinforcement learning framework that models cryoablation
planning as a Markov decision process and learns an optimal policy for
cryoprobe placement. Within a simulated environment that models clinical
constraints and stochastic intraoperative variability, an agent sequentially
selects cryoprobe positions and ice sphere diameters. Guided by a reward
function based on tumour coverage, this agent learns a cryoablation strategy
that leads to optimal cryoprobe placements without the need for any
manually-designed plans. Evaluated on 583 retrospective prostate cancer cases,
Cryo-RL achieved over 8 percentage-point Dice improvements compared with the
best automated baselines, based on geometric optimisation, and matched human
expert performance while requiring substantially less planning time. These
results highlight the potential of reinforcement learning to deliver clinically
viable, reproducible, and efficient cryoablation plans.

</details>


### [29] [SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models](https://arxiv.org/abs/2509.04889)
*Dominik Pegler,David Steyrl,Mengfan Zhang,Alexander Karner,Jozsef Arato,Frank Scharnowski,Filip Melinscak*

Main category: cs.CV

TL;DR: 使用预训练计算机视觉模型通过迁移学习预测蜘蛛图像引发的恐惧程度，平均绝对误差10.1-11.0，模型解释性良好但需要足够数据量


<details>
  <summary>Details</summary>
Motivation: 为开发自适应计算机化暴露疗法系统，需要能够根据视觉刺激准确预测患者恐惧水平的模型，这是实现动态调整治疗刺激的关键步骤

Method: 采用三种不同的预训练计算机视觉模型进行迁移学习，使用包含313张蜘蛛图像的标准数据集预测0-100分的恐惧评分，通过交叉验证评估性能

Result: 模型平均绝对误差在10.1到11.0之间，学习曲线分析显示减少数据集会显著降低性能，但增加数据量无显著提升。解释性评估证实模型基于蜘蛛相关特征进行预测，错误分析识别出高误差的视觉条件

Conclusion: 研究证明了可解释计算机视觉模型在预测恐惧评分方面的潜力，强调了模型可解释性和足够数据集大小对于开发有效情感感知治疗技术的重要性

Abstract: Advances in computer vision have opened new avenues for clinical
applications, particularly in computerized exposure therapy where visual
stimuli can be dynamically adjusted based on patient responses. As a critical
step toward such adaptive systems, we investigated whether pretrained computer
vision models can accurately predict fear levels from spider-related images. We
adapted three diverse models using transfer learning to predict human fear
ratings (on a 0-100 scale) from a standardized dataset of 313 images. The
models were evaluated using cross-validation, achieving an average mean
absolute error (MAE) between 10.1 and 11.0. Our learning curve analysis
revealed that reducing the dataset size significantly harmed performance,
though further increases yielded no substantial gains. Explainability
assessments showed the models' predictions were based on spider-related
features. A category-wise error analysis further identified visual conditions
associated with higher errors (e.g., distant views and artificial/painted
spiders). These findings demonstrate the potential of explainable computer
vision models in predicting fear ratings, highlighting the importance of both
model explainability and a sufficient dataset size for developing effective
emotion-aware therapeutic technologies.

</details>


### [30] [SynGen-Vision: Synthetic Data Generation for training industrial vision models](https://arxiv.org/abs/2509.04894)
*Alpana Dubey,Suma Mani Kuriakose,Nitish Bhardwaj*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose an approach to generate synthetic data to train computer vision
(CV) models for industrial wear and tear detection. Wear and tear detection is
an important CV problem for predictive maintenance tasks in any industry.
However, data curation for training such models is expensive and time-consuming
due to the unavailability of datasets for different wear and tear scenarios.
Our approach employs a vision language model along with a 3D simulation and
rendering engine to generate synthetic data for varying rust conditions. We
evaluate our approach by training a CV model for rust detection using the
generated dataset and tested the trained model on real images of rusted
industrial objects. The model trained with the synthetic data generated by our
approach, outperforms the other approaches with a mAP50 score of 0.87. The
approach is customizable and can be easily extended to other industrial wear
and tear detection scenarios

</details>


### [31] [Evaluating Multiple Instance Learning Strategies for Automated Sebocyte Droplet Counting](https://arxiv.org/abs/2509.04895)
*Maryam Adelipour,Gustavo Carneiro,Jeongkwon Kim*

Main category: cs.CV

TL;DR: 这篇论文提出了一种基于注意力机制的多实例学习框架，用于自动化计数皮腺细胞中的脂谴粒，并与基线模型进行了性能比较。


<details>
  <summary>Details</summary>
Motivation: 手动计数皮腺细胞中的脂谴粒非常耗时且容易受主观因素影响，需要自动化解决方案来提高效率和准确性。

Method: 采用Nile Red染色的皮腺细胞图像，分为14个类别，通过数据增广扩展到约50,000个细胞。对比了两种模型：基线MLP模型和基于ResNet-50特征的注意力MIL模型，使用五折交叉验证。

Result: 基线MLP模型表现更稳定（平均MAE=5.6），而注意力MIL模型不太稳定（平均MAE=10.7）但在某些折叠中表现更优。

Conclusion: 简单的集合级判断提供了稳健的基线方法，而注意力MIL需要任务对齐的池化和正则化才能充分发挥其潜力。

Abstract: Sebocytes are lipid-secreting cells whose differentiation is marked by the
accumulation of intracellular lipid droplets, making their quantification a key
readout in sebocyte biology. Manual counting is labor-intensive and subjective,
motivating automated solutions. Here, we introduce a simple attention-based
multiple instance learning (MIL) framework for sebocyte image analysis. Nile
Red-stained sebocyte images were annotated into 14 classes according to droplet
counts, expanded via data augmentation to about 50,000 cells. Two models were
benchmarked: a baseline multi-layer perceptron (MLP) trained on aggregated
patch-level counts, and an attention-based MIL model leveraging ResNet-50
features with instance weighting. Experiments using five-fold cross-validation
showed that the baseline MLP achieved more stable performance (mean MAE = 5.6)
compared with the attention-based MIL, which was less consistent (mean MAE =
10.7) but occasionally superior in specific folds. These findings indicate that
simple bag-level aggregation provides a robust baseline for slide-level droplet
counting, while attention-based MIL requires task-aligned pooling and
regularization to fully realize its potential in sebocyte image analysis.

</details>


### [32] [UniView: Enhancing Novel View Synthesis From A Single Image By Unifying Reference Features](https://arxiv.org/abs/2509.04932)
*Haowang Cui,Rui Chen,Tao Luo,Rui Li,Jiaze Wang*

Main category: cs.CV

TL;DR: UniView是一种新颖的单图像新视角合成方法，通过检索相似物体的参考图像提供强先验信息，利用多模态大语言模型选择参考图像，并采用多级隔离层适配器和解耦三重注意力机制来提升合成质量。


<details>
  <summary>Details</summary>
Motivation: 单图像新视角合成任务由于未观察区域存在多重解释而高度不适定，现有方法往往基于模糊先验和输入视图附近的插值生成不可见区域，导致严重失真。

Method: 1) 构建检索和增强系统，使用多模态大语言模型选择符合要求的参考图像；2) 引入具有多级隔离层的即插即用适配器模块，动态为目标视图生成参考特征；3) 设计解耦三重注意力机制，有效对齐和整合多分支特征以保留原始输入图像细节。

Result: 大量实验表明，UniView显著提升了新视角合成性能，在具有挑战性的数据集上优于最先进的方法。

Conclusion: 通过利用相似物体的参考图像提供强先验信息，结合先进的适配器模块和注意力机制，UniView有效解决了单图像新视角合成中的失真问题，取得了优异的性能表现。

Abstract: The task of synthesizing novel views from a single image is highly ill-posed
due to multiple explanations for unobserved areas. Most current methods tend to
generate unseen regions from ambiguity priors and interpolation near input
views, which often lead to severe distortions. To address this limitation, we
propose a novel model dubbed as UniView, which can leverage reference images
from a similar object to provide strong prior information during view
synthesis. More specifically, we construct a retrieval and augmentation system
and employ a multimodal large language model (MLLM) to assist in selecting
reference images that meet our requirements. Additionally, a plug-and-play
adapter module with multi-level isolation layers is introduced to dynamically
generate reference features for the target views. Moreover, in order to
preserve the details of an original input image, we design a decoupled triple
attention mechanism, which can effectively align and integrate multi-branch
features into the synthesis process. Extensive experiments have demonstrated
that our UniView significantly improves novel view synthesis performance and
outperforms state-of-the-art methods on the challenging datasets.

</details>


### [33] [Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper](https://arxiv.org/abs/2509.04957)
*Gehui Chen,Guan'an Wang,Xiaowen Huang,Jitao Sang*

Main category: cs.CV

TL;DR: MFM-Mapper是一个高效的视频到音频生成方法，通过融合双视觉编码器特征和使用GPT-2映射器，以仅16%的训练量实现了更好的语义和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频到音频生成方法需要大量资源从头训练，而利用预训练基础模型可以更好地进行跨模态知识迁移和泛化。

Method: 提出MFM-Mapper方法：1）使用双视觉编码器融合更丰富的语义和时间特征；2）用GPT-2替换线性映射器，将跨模态特征映射视为自回归翻译任务；3）保持轻量级训练。

Result: 仅需先前映射器方法16%的训练规模，就能达到竞争性性能，在语义和时间一致性方面表现更好。

Conclusion: MFM-Mapper通过有效利用多个基础模型和先进的映射架构，显著提高了视频到音频生成的训练效率和性能。

Abstract: Recent Video-to-Audio (V2A) generation relies on extracting semantic and
temporal features from video to condition generative models. Training these
models from scratch is resource intensive. Consequently, leveraging foundation
models (FMs) has gained traction due to their cross-modal knowledge transfer
and generalization capabilities. One prior work has explored fine-tuning a
lightweight mapper network to connect a pre-trained visual encoder with a
text-to-audio generation model for V2A. Inspired by this, we introduce the
Multiple Foundation Model Mapper (MFM-Mapper). Compared to the previous mapper
approach, MFM-Mapper benefits from richer semantic and temporal information by
fusing features from dual visual encoders. Furthermore, by replacing a linear
mapper with GPT-2, MFM-Mapper improves feature alignment, drawing parallels
between cross-modal features mapping and autoregressive translation tasks. Our
MFM-Mapper exhibits remarkable training efficiency. It achieves better
performance in semantic and temporal consistency with fewer training consuming,
requiring only 16\% of the training scale compared to previous mapper-based
work, yet achieves competitive performance with models trained on a much larger
scale.

</details>


### [34] [Dual-Domain Perspective on Degradation-Aware Fusion: A VLM-Guided Robust Infrared and Visible Image Fusion Framework](https://arxiv.org/abs/2509.05000)
*Tianpei Zhang,Jufeng Zhao,Yiming Zhu,Guangmang Cui*

Main category: cs.CV

TL;DR: 提出GD^2Fusion框架，通过视觉语言模型感知退化并联合优化频域和空域，解决双源退化场景下的红外-可见光图像融合问题


<details>
  <summary>Details</summary>
Motivation: 现有方法假设高质量输入，无法处理双源退化场景，需要手动选择多个预增强步骤，导致误差累积和性能下降

Method: 设计GFMSE模块进行频域退化感知和抑制，提取融合相关子带特征；GSMAF模块进行跨模态退化滤波和自适应多源特征聚合

Result: 在双源退化场景下取得了优于现有算法和策略的融合性能

Conclusion: GD^2Fusion框架通过协同集成视觉语言模型和双域联合优化，有效解决了双源退化场景下的图像融合问题

Abstract: Most existing infrared-visible image fusion (IVIF) methods assume
high-quality inputs, and therefore struggle to handle dual-source degraded
scenarios, typically requiring manual selection and sequential application of
multiple pre-enhancement steps. This decoupled pre-enhancement-to-fusion
pipeline inevitably leads to error accumulation and performance degradation. To
overcome these limitations, we propose Guided Dual-Domain Fusion (GD^2Fusion),
a novel framework that synergistically integrates vision-language models (VLMs)
for degradation perception with dual-domain (frequency/spatial) joint
optimization. Concretely, the designed Guided Frequency Modality-Specific
Extraction (GFMSE) module performs frequency-domain degradation perception and
suppression and discriminatively extracts fusion-relevant sub-band features.
Meanwhile, the Guided Spatial Modality-Aggregated Fusion (GSMAF) module carries
out cross-modal degradation filtering and adaptive multi-source feature
aggregation in the spatial domain to enhance modality complementarity and
structural consistency. Extensive qualitative and quantitative experiments
demonstrate that GD^2Fusion achieves superior fusion performance compared with
existing algorithms and strategies in dual-source degraded scenarios. The code
will be publicly released after acceptance of this paper.

</details>


### [35] [Interpretable Deep Transfer Learning for Breast Ultrasound Cancer Detection: A Multi-Dataset Study](https://arxiv.org/abs/2509.05004)
*Mohammad Abbadi,Yassine Himeur,Shadi Atalla,Wathiq Mansoor*

Main category: cs.CV

TL;DR: 本文研究了机器学习和深度学习在乳腺癌超声图像分类中的应用，ResNet-18模型达到99.7%的最高准确率，支持AI诊断工具在临床工作流程中的整合。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性癌症相关死亡的主要原因，超声成像因其安全性和成本效益在早期检测中发挥关键作用，特别是在致密乳腺组织患者中。

Method: 使用BUSI、BUS-BRA和BrEaST-Lesions USG数据集，评估经典机器学习模型（SVM、KNN）和深度卷积神经网络（ResNet-18、EfficientNet-B0、GoogLeNet），并采用Grad-CAM可视化技术。

Result: ResNet-18实现最高准确率（99.7%）和对恶性病变的完美敏感性，经典ML模型在深度特征提取增强下表现竞争力，Grad-CAM可视化提高了模型透明度。

Conclusion: 研究结果支持AI诊断工具整合到临床工作流程中，展示了部署高性能、可解释的超声乳腺癌检测系统的可行性。

Abstract: Breast cancer remains a leading cause of cancer-related mortality among women
worldwide. Ultrasound imaging, widely used due to its safety and
cost-effectiveness, plays a key role in early detection, especially in patients
with dense breast tissue. This paper presents a comprehensive study on the
application of machine learning and deep learning techniques for breast cancer
classification using ultrasound images. Using datasets such as BUSI, BUS-BRA,
and BrEaST-Lesions USG, we evaluate classical machine learning models (SVM,
KNN) and deep convolutional neural networks (ResNet-18, EfficientNet-B0,
GoogLeNet). Experimental results show that ResNet-18 achieves the highest
accuracy (99.7%) and perfect sensitivity for malignant lesions. Classical ML
models, though outperformed by CNNs, achieve competitive performance when
enhanced with deep feature extraction. Grad-CAM visualizations further improve
model transparency by highlighting diagnostically relevant image regions. These
findings support the integration of AI-based diagnostic tools into clinical
workflows and demonstrate the feasibility of deploying high-performing,
interpretable systems for ultrasound-based breast cancer detection.

</details>


### [36] [A biologically inspired separable learning vision model for real-time traffic object perception in Dark](https://arxiv.org/abs/2509.05012)
*Hulin Li,Qiliang Ren,Jun Li,Hanbing Wei,Zheng Liu,Linfang Fan*

Main category: cs.CV

TL;DR: 这篇论文提出了一种可分离学习视觉模型SLVM，专门处理低光照交通场景的对象检测、实例分割和光流估计任务，并构建了最大的低光照交通场景数据集Dark-traffic。


<details>
  <summary>Details</summary>
Motivation: 低光照交通场景中对象感知面临严重的照明退化问题，现有方法适应性差且缺乏专门的大规模数据集。

Method: 提出SLVM模型，包含四个核心组件：光适应孔径机制、特征层可分离学习策略、任务特定解耦分支和空间对齐感知融合模块。构建Dark-traffic数据集支持多任务学习。

Result: SLVM在Dark-traffic数据集上显著优于现有方法：检测性能超过RT-DETR 11.2%，实例分割超过YOLOv12 6.1%，光流估计EPE锐减12.37%。在LIS指标上平均超11%。

Conclusion: SLVM模型能够高效处理低光照交通场景的多任务感知问题，具有较强的适应性和减少计算开销的优势。

Abstract: Fast and accurate object perception in low-light traffic scenes has attracted
increasing attention. However, due to severe illumination degradation and the
lack of reliable visual cues, existing perception models and methods struggle
to quickly adapt to and accurately predict in low-light environments. Moreover,
there is the absence of available large-scale benchmark specifically focused on
low-light traffic scenes. To bridge this gap, we introduce a physically
grounded illumination degradation method tailored to real-world low-light
settings and construct Dark-traffic, the largest densely annotated dataset to
date for low-light traffic scenes, supporting object detection, instance
segmentation, and optical flow estimation. We further propose the Separable
Learning Vision Model (SLVM), a biologically inspired framework designed to
enhance perception under adverse lighting. SLVM integrates four key components:
a light-adaptive pupillary mechanism for illumination-sensitive feature
extraction, a feature-level separable learning strategy for efficient
representation, task-specific decoupled branches for multi-task separable
learning, and a spatial misalignment-aware fusion module for precise
multi-feature alignment. Extensive experiments demonstrate that SLVM achieves
state-of-the-art performance with reduced computational overhead. Notably, it
outperforms RT-DETR by 11.2 percentage points in detection, YOLOv12 by 6.1
percentage points in instance segmentation, and reduces endpoint error (EPE) of
baseline by 12.37% on Dark-traffic. On the LIS benchmark, the end-to-end
trained SLVM surpasses Swin Transformer+EnlightenGAN and
ConvNeXt-T+EnlightenGAN by an average of 11 percentage points across key
metrics, and exceeds Mask RCNN (with light enhancement) by 3.1 percentage
points. The Dark-traffic dataset and complete code is released at
https://github.com/alanli1997/slvm.

</details>


### [37] [Leveraging Transfer Learning and Mobile-enabled Convolutional Neural Networks for Improved Arabic Handwritten Character Recognition](https://arxiv.org/abs/2509.05019)
*Mohsine El Khayati,Ayyad Maafiri,Yassine Himeur,Hamzah Ali Alkhazaleh,Shadi Atalla,Wathiq Mansoor*

Main category: cs.CV

TL;DR: 通过载体移植学习与轻量级卷积神经网络结合，提升阿拉伯手写字符识别的准确性和效率，MobileNet表现最优，完整微调策略效果最好。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯手写字符识别中面临的计算资源需求大、数据集稀缺等挑战，寻找高效质量的解决方案。

Method: 采用三种载体移植学习策略（完整微调、部分微调、从头训练）结合四种轻量级MbNets模型（MobileNet、SqueezeNet、MnasNet、ShuffleNet），在三个标准数据集上进行实验。

Result: MobileNet表现最优，稳定且高效；ShuffleNet在汇总性方面突出；IFHCDB数据集达到99%准确率（MnasNet）；完整微调策略效果最好，部分微调表现最差。

Conclusion: 载体移植学习与轻量级CNN结合在阿拉伯手写字符识别中具有强大潜力，为资源受限环境下的高效识别提供了可行方案，未来将深入研究模型优化和数据增强等方向。

Abstract: The study explores the integration of transfer learning (TL) with
mobile-enabled convolutional neural networks (MbNets) to enhance Arabic
Handwritten Character Recognition (AHCR). Addressing challenges like extensive
computational requirements and dataset scarcity, this research evaluates three
TL strategies--full fine-tuning, partial fine-tuning, and training from
scratch--using four lightweight MbNets: MobileNet, SqueezeNet, MnasNet, and
ShuffleNet. Experiments were conducted on three benchmark datasets: AHCD,
HIJJA, and IFHCDB. MobileNet emerged as the top-performing model, consistently
achieving superior accuracy, robustness, and efficiency, with ShuffleNet
excelling in generalization, particularly under full fine-tuning. The IFHCDB
dataset yielded the highest results, with 99% accuracy using MnasNet under full
fine-tuning, highlighting its suitability for robust character recognition. The
AHCD dataset achieved competitive accuracy (97%) with ShuffleNet, while HIJJA
posed significant challenges due to its variability, achieving a peak accuracy
of 92% with ShuffleNet. Notably, full fine-tuning demonstrated the best overall
performance, balancing accuracy and convergence speed, while partial
fine-tuning underperformed across metrics. These findings underscore the
potential of combining TL and MbNets for resource-efficient AHCR, paving the
way for further optimizations and broader applications. Future work will
explore architectural modifications, in-depth dataset feature analysis, data
augmentation, and advanced sensitivity analysis to enhance model robustness and
generalizability.

</details>


### [38] [LUIVITON: Learned Universal Interoperable VIrtual Try-ON](https://arxiv.org/abs/2509.05030)
*Cong Cao,Xianhang Cheng,Jingyuan Liu,Yujian Zheng,Zhenhui Lin,Meriem Chkir,Hao Li*

Main category: cs.CV

TL;DR: LUIVITON是一个端到端的全自动虚拟试穿系统，能够将复杂的多层服装覆盖到多样化的人形角色上，使用SMPL作为代理表示，通过几何学习和扩散模型方法解决服装到身体的对应问题。


<details>
  <summary>Details</summary>
Motivation: 解决将复杂服装与任意形状和姿态的人形角色进行自动对齐的挑战，支持包括人类、机器人、卡通角色、生物和外星人在内的多样化角色，并提供无需人工干预的高质量3D服装拟合。

Method: 使用SMPL作为代理表示，将服装到身体的覆盖问题分解为两个对应任务：1）使用基于几何学习的方法处理服装到SMPL的对应，2）使用基于扩散模型的方法处理身体到SMPL的对应，利用多视角一致的外观特征和预训练的2D基础模型。

Result: 系统能够处理复杂几何形状、非流形网格，有效泛化到广泛的人形角色，保持计算效率，支持快速定制服装尺寸和材质属性，即使在没有2D服装缝制图案的情况下也能产生高质量的3D服装拟合。

Conclusion: LUIVITON提供了一个完全自动化的虚拟试穿解决方案，能够处理多样化的人形角色和复杂服装，具有实用性和高效性，为虚拟试穿领域带来了重要进展。

Abstract: We present LUIVITON, an end-to-end system for fully automated virtual try-on,
capable of draping complex, multi-layer clothing onto diverse and arbitrarily
posed humanoid characters. To address the challenge of aligning complex
garments with arbitrary and highly diverse body shapes, we use SMPL as a proxy
representation and separate the clothing-to-body draping problem into two
correspondence tasks: 1) clothing-to-SMPL and 2) body-to-SMPL correspondence,
where each has its unique challenges. While we address the clothing-to-SMPL
fitting problem using a geometric learning-based approach for
partial-to-complete shape correspondence prediction, we introduce a diffusion
model-based approach for body-to-SMPL correspondence using multi-view
consistent appearance features and a pre-trained 2D foundation model. Our
method can handle complex geometries, non-manifold meshes, and generalizes
effectively to a wide range of humanoid characters -- including humans, robots,
cartoon subjects, creatures, and aliens, while maintaining computational
efficiency for practical adoption. In addition to offering a fully automatic
fitting solution, LUIVITON supports fast customization of clothing size,
allowing users to adjust clothing sizes and material properties after they have
been draped. We show that our system can produce high-quality 3D clothing
fittings without any human labor, even when 2D clothing sewing patterns are not
available.

</details>


### [39] [Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization](https://arxiv.org/abs/2509.05034)
*Jingqi Wu,Hanxi Li,Lin Yuanbo Wu,Hao Chen,Deyin Liu,Peng Wang*

Main category: cs.CV

TL;DR: ADClick是一个用于工业异常检测的交互式图像分割算法，通过少量用户点击和文本描述生成像素级异常标注，显著提升异常检测模型性能。ADClick-Seg进一步通过跨模态框架结合视觉特征和文本提示，在多项指标上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 工业产品检测通常只使用正常样本训练异常检测模型，虽然有缺陷样本但需要像素级标注限制了可扩展性。需要一种能够利用少量用户交互和文本描述来高效生成精确标注的方法。

Method: 提出ADClick交互式图像分割算法，通过用户点击和文本描述生成像素级异常标注。进一步开发ADClick-Seg跨模态框架，使用基于原型的方法对齐视觉特征和文本提示，结合像素级先验和语言引导线索。

Result: ADClick在MVTec AD上达到AP=96.1%的优异性能。ADClick-Seg在多类别异常检测任务上取得AP=80.0%、PRO=97.5%、Pixel-AUROC=99.1%的state-of-the-art结果。

Conclusion: ADClick系列方法通过交互式标注和跨模态融合，有效解决了工业异常检测中标注成本高的问题，显著提升了检测性能，为实际工业应用提供了高效解决方案。

Abstract: Industrial product inspection is often performed using Anomaly Detection (AD)
frameworks trained solely on non-defective samples. Although defective samples
can be collected during production, leveraging them usually requires
pixel-level annotations, limiting scalability. To address this, we propose
ADClick, an Interactive Image Segmentation (IIS) algorithm for industrial
anomaly detection. ADClick generates pixel-wise anomaly annotations from only a
few user clicks and a brief textual description, enabling precise and efficient
labeling that significantly improves AD model performance (e.g., AP = 96.1\% on
MVTec AD). We further introduce ADClick-Seg, a cross-modal framework that
aligns visual features and textual prompts via a prototype-based approach for
anomaly detection and localization. By combining pixel-level priors with
language-guided cues, ADClick-Seg achieves state-of-the-art results on the
challenging ``Multi-class'' AD task (AP = 80.0\%, PRO = 97.5\%, Pixel-AUROC =
99.1\% on MVTec AD).

</details>


### [40] [Systematic Review and Meta-analysis of AI-driven MRI Motion Artifact Detection and Correction](https://arxiv.org/abs/2509.05071)
*Mojtaba Safari,Zach Eidex,Richard L. J. Qiu,Matthew Goette,Tonghe Wang,Xiaofeng Yang*

Main category: cs.CV

TL;DR: AI驱动的深度学习方法，特别是生成模型，在检测和校正MRI运动伪影方面显示出巨大潜力，但面临泛化性有限、依赖配对训练数据和视觉失真等挑战，需要标准化数据集和报告协议。


<details>
  <summary>Details</summary>
Motivation: 系统评估人工智能方法在MRI运动伪影检测和校正方面的有效性、当前发展状况、面临的挑战以及未来研究方向，为改进MRI图像质量和诊断准确性提供依据。

Method: 采用系统性综述和荟萃分析方法，重点关注深度学习特别是生成模型在MRI运动伪影检测和校正中的应用，提取数据集、深度学习架构和性能指标的定量数据。

Result: 深度学习特别是生成模型在减少运动伪影和改善图像质量方面表现出潜力，但存在泛化性有限、依赖配对训练数据和视觉失真风险等关键挑战。

Conclusion: AI驱动方法特别是深度学习生成模型在改善MRI图像质量方面具有显著潜力，但需要解决公共数据集缺乏、标准化报告协议和更先进的自适应技术等关键挑战，以提升诊断准确性、降低医疗成本并改善患者护理结果。

Abstract: Background: To systematically review and perform a meta-analysis of
artificial intelligence (AI)-driven methods for detecting and correcting
magnetic resonance imaging (MRI) motion artifacts, assessing current
developments, effectiveness, challenges, and future research directions.
Methods: A comprehensive systematic review and meta-analysis were conducted,
focusing on deep learning (DL) approaches, particularly generative models, for
the detection and correction of MRI motion artifacts. Quantitative data were
extracted regarding utilized datasets, DL architectures, and performance
metrics. Results: DL, particularly generative models, show promise for reducing
motion artifacts and improving image quality; however, limited
generalizability, reliance on paired training data, and risk of visual
distortions remain key challenges that motivate standardized datasets and
reporting. Conclusions: AI-driven methods, particularly DL generative models,
show significant potential for improving MRI image quality by effectively
addressing motion artifacts. However, critical challenges must be addressed,
including the need for comprehensive public datasets, standardized reporting
protocols for artifact levels, and more advanced, adaptable DL techniques to
reduce reliance on extensive paired datasets. Addressing these aspects could
substantially enhance MRI diagnostic accuracy, reduce healthcare costs, and
improve patient care outcomes.

</details>


### [41] [GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting](https://arxiv.org/abs/2509.05075)
*Yangming Li,Chaoyu Liu,Lihao Liu,Simon Masnou,Carola-Bibian Schönlieb*

Main category: cs.CV

TL;DR: GeoSplat是一个几何约束优化框架，利用一阶和二阶几何先验改进高斯溅射的整个训练流程，包括初始化、梯度更新和密集化，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用低阶几何先验（如法向量），且通过噪声敏感方法估计不可靠，需要更鲁棒的几何约束来改进高斯溅射优化

Method: 提出基于主曲率初始化高斯尺度，引入高效且抗噪声的几何结构估计方法，为框架提供动态几何先验，改进整个训练流程

Result: 在多个数据集上的新颖视图合成实验中，GeoSplat显著提升了高斯溅射性能，超越了之前的基线方法

Conclusion: GeoSplat框架通过引入高阶几何先验和鲁棒估计方法，有效解决了现有几何约束方法的局限性，为高斯溅射优化提供了更好的几何指导

Abstract: A few recent works explored incorporating geometric priors to regularize the
optimization of Gaussian splatting, further improving its performance. However,
those early studies mainly focused on the use of low-order geometric priors
(e.g., normal vector), and they are also unreliably estimated by
noise-sensitive methods, like local principal component analysis. To address
their limitations, we first present GeoSplat, a general geometry-constrained
optimization framework that exploits both first-order and second-order
geometric quantities to improve the entire training pipeline of Gaussian
splatting, including Gaussian initialization, gradient update, and
densification. As an example, we initialize the scales of 3D Gaussian
primitives in terms of principal curvatures, leading to a better coverage of
the object surface than random initialization. Secondly, based on certain
geometric structures (e.g., local manifold), we introduce efficient and
noise-robust estimation methods that provide dynamic geometric priors for our
framework. We conduct extensive experiments on multiple datasets for novel view
synthesis, showing that our framework: GeoSplat, significantly improves the
performance of Gaussian splatting and outperforms previous baselines.

</details>


### [42] [Scale-interaction transformer: a hybrid cnn-transformer model for facial beauty prediction](https://arxiv.org/abs/2509.05078)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的混合深度学习模型SIT，结合CNN的特征提取能力和Transformer的关系建模能力，通过多尺度特征交互模型在面部美学预测任务上达到了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决CNN在面部美学预测任务中因固定尺度处理而忽略不同级别特征间互相依赖关系的问题。

Method: 设计了Scale-Interaction Transformer (SIT)模型，先使用并行卷积的多尺度模块提取不同感知野的面部特征，然后将多尺度表征构造为序列使用Transformer编码器通过自注意力机制模型其互动关系。

Result: 在SCUT-FBP5500数据集上达到新的state-of-the-art结果，Pearson相关系数为0.9187，超过了之前的方法。

Conclusion: 明确建模多尺度视觉线索间的交互对高性能面部美学预测至关重要，SIT模型的成功显示了混合CNN-Transformer模型在需要全局上下文理解的复杂图像回归任务中的潜力。

Abstract: Automated Facial Beauty Prediction (FBP) is a challenging computer vision
task due to the complex interplay of local and global facial features that
influence human perception. While Convolutional Neural Networks (CNNs) excel at
feature extraction, they often process information at a fixed scale,
potentially overlooking the critical inter-dependencies between features at
different levels of granularity. To address this limitation, we introduce the
Scale-Interaction Transformer (SIT), a novel hybrid deep learning architecture
that synergizes the feature extraction power of CNNs with the relational
modeling capabilities of Transformers. The SIT first employs a multi-scale
module with parallel convolutions to capture facial characteristics at varying
receptive fields. These multi-scale representations are then framed as a
sequence and processed by a Transformer encoder, which explicitly models their
interactions and contextual relationships via a self-attention mechanism. We
conduct extensive experiments on the widely-used SCUT-FBP5500 benchmark
dataset, where the proposed SIT model establishes a new state-of-the-art. It
achieves a Pearson Correlation of 0.9187, outperforming previous methods. Our
findings demonstrate that explicitly modeling the interplay between multi-scale
visual cues is crucial for high-performance FBP. The success of the SIT
architecture highlights the potential of hybrid CNN-Transformer models for
complex image regression tasks that demand a holistic, context-aware
understanding.

</details>


### [43] [Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers](https://arxiv.org/abs/2509.05086)
*Svetlana Pavlitska,Haixi Fan,Konstantin Ditschuneit,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 使用稀疏混合专家(MoE)层替换CNN中的残差块或卷积层，在不增加推理成本的情况下提升模型对抗攻击的鲁棒性。研究发现单个MoE层结合对抗训练能显著提升PGD和AutoPGD攻击下的鲁棒性，并发现专家路由会集中在少数专家上，形成专门的鲁棒子路径。


<details>
  <summary>Details</summary>
Motivation: 提升卷积神经网络对抗对抗攻击的鲁棒性，同时避免资源密集型对策，通过增加模型容量但不增加推理成本的方式来实现。

Method: 在ResNet架构中使用稀疏混合专家(MoE)层替换选定的残差块或卷积层，结合对抗训练，并在CIFAR-100数据集上进行实验，使用PGD和AutoPGD攻击评估鲁棒性。

Result: 在深层阶段插入单个MoE层能持续提升对抗攻击鲁棒性；使用switch loss会导致路由集中在少数过度使用的专家上，使这些路径更加鲁棒；某些单个专家的鲁棒性甚至超过门控MoE模型。

Conclusion: 稀疏MoE层能有效提升CNN对抗攻击的鲁棒性，专家路由的集中现象促使形成了专门的鲁棒子路径，这为构建更鲁棒的神经网络提供了新思路。

Abstract: Robustifying convolutional neural networks (CNNs) against adversarial attacks
remains challenging and often requires resource-intensive countermeasures. We
explore the use of sparse mixture-of-experts (MoE) layers to improve robustness
by replacing selected residual blocks or convolutional layers, thereby
increasing model capacity without additional inference cost. On ResNet
architectures trained on CIFAR-100, we find that inserting a single MoE layer
in the deeper stages leads to consistent improvements in robustness under PGD
and AutoPGD attacks when combined with adversarial training. Furthermore, we
discover that when switch loss is used for balancing, it causes routing to
collapse onto a small set of overused experts, thereby concentrating
adversarial training on these paths and inadvertently making them more robust.
As a result, some individual experts outperform the gated MoE model in
robustness, suggesting that robust subpaths emerge through specialization. Our
code is available at https://github.com/KASTEL-MobilityLab/robust-sparse-moes.

</details>


### [44] [Semi-supervised Deep Transfer for Regression without Domain Alignment](https://arxiv.org/abs/2509.05092)
*Mainak Biswas,Ambedkar Dukkipati,Devarajan Sridharan*

Main category: cs.CV

TL;DR: CRAFT是一种源数据不可用的半监督域适应方法，专门针对回归任务设计，在神经科学数据上表现优异，比现有方法提升3-9%的RMSE性能


<details>
  <summary>Details</summary>
Motivation: 解决现实应用中源数据无法共享（隐私或计算成本限制）且目标标签稀缺的问题，特别是在医学和生物学中的连续值预测任务

Method: 基于Contradistinguisher框架，开发了CRAFT方法，通过正则化方法实现源数据不可用情况下的半监督迁移学习，适用于回归任务

Result: 在两个神经科学数据集（EEG眼动预测和MRI脑龄预测）上，CRAFT比微调模型提升9%的RMSE，比四种最先进的源无关域适应方法提升3%以上

Conclusion: CRAFT是生物学和医学中普遍存在的回归任务的高效源无关半监督深度迁移方法

Abstract: Deep learning models deployed in real-world applications (e.g., medicine)
face challenges because source models do not generalize well to domain-shifted
target data. Many successful domain adaptation (DA) approaches require full
access to source data. Yet, such requirements are unrealistic in scenarios
where source data cannot be shared either because of privacy concerns or
because it is too large and incurs prohibitive storage or computational costs.
Moreover, resource constraints may limit the availability of labeled targets.
We illustrate this challenge in a neuroscience setting where source data are
unavailable, labeled target data are meager, and predictions involve
continuous-valued outputs. We build upon Contradistinguisher (CUDA), an
efficient framework that learns a shared model across the labeled source and
unlabeled target samples, without intermediate representation alignment. Yet,
CUDA was designed for unsupervised DA, with full access to source data, and for
classification tasks. We develop CRAFT -- a Contradistinguisher-based
Regularization Approach for Flexible Training -- for source-free (SF),
semi-supervised transfer of pretrained models in regression tasks. We showcase
the efficacy of CRAFT in two neuroscience settings: gaze prediction with
electroencephalography (EEG) data and ``brain age'' prediction with structural
MRI data. For both datasets, CRAFT yielded up to 9% improvement in
root-mean-squared error (RMSE) over fine-tuned models when labeled training
examples were scarce. Moreover, CRAFT leveraged unlabeled target data and
outperformed four competing state-of-the-art source-free domain adaptation
models by more than 3%. Lastly, we demonstrate the efficacy of CRAFT on two
other real-world regression benchmarks. We propose CRAFT as an efficient
approach for source-free, semi-supervised deep transfer for regression that is
ubiquitous in biology and medicine.

</details>


### [45] [A Scalable Attention-Based Approach for Image-to-3D Texture Mapping](https://arxiv.org/abs/2509.05131)
*Arianna Rampini,Kanika Madan,Bruno Roy,AmirHossein Zamani,Derek Cheung*

Main category: cs.CV

TL;DR: 一种从单张图像和网格直接生成高质量3D纹理的Transformer框架，避免UV映射，速度快且保持参考图像的准确性


<details>
  <summary>Details</summary>
Motivation: 现有的3D纹理生成方法速度慢、依赖UV映射、且对参考图像的准确性不高

Method: 使用Transformer框架直接从单张图像和网格预测3D纹理场，集成三平面表示和基于深度的反向投影损失

Result: 每个形状只需0.2秒生成高保真纹理，在输入图像准确性和感知质量方面超越现有最佳方法

Conclusion: 该方法具有可扩展性、高质量和可控制的3D内容创建实用性

Abstract: High-quality textures are critical for realistic 3D content creation, yet
existing generative methods are slow, rely on UV maps, and often fail to remain
faithful to a reference image. To address these challenges, we propose a
transformer-based framework that predicts a 3D texture field directly from a
single image and a mesh, eliminating the need for UV mapping and differentiable
rendering, and enabling faster texture generation. Our method integrates a
triplane representation with depth-based backprojection losses, enabling
efficient training and faster inference. Once trained, it generates
high-fidelity textures in a single forward pass, requiring only 0.2s per shape.
Extensive qualitative, quantitative, and user preference evaluations
demonstrate that our method outperforms state-of-the-art baselines on
single-image texture reconstruction in terms of both fidelity to the input
image and perceptual quality, highlighting its practicality for scalable,
high-quality, and controllable 3D content creation.

</details>


### [46] [SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing](https://arxiv.org/abs/2509.05144)
*Chaolei Wang,Yang Luo,Jing Du,Siyu Chen,Yiping Chen,Ting Han*

Main category: cs.CV

TL;DR: SGS-3D是一个训练自由的3D实例分割精炼框架，通过'分割-生长'策略融合语义和几何信息，显著提升分割精度和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 解决基于2D到3D提升方法的3D实例分割精度不足问题，这些方法由于语义引导模糊和深度约束不足，在提升过程中会产生累积误差

Method: 提出'分割-生长'框架：首先利用几何基元净化和分割模糊的提升掩码，然后在场景中将其生长为完整实例；引入掩码过滤策略和几何细化方法

Result: 在ScanNet200、ScanNet++和KITTI-360数据集上实验表明，SGS-3D显著提高了分割精度和鲁棒性，对预训练模型的不准确掩码具有强鲁棒性

Conclusion: SGS-3D通过联合融合语义和几何信息，实现了高保真度的3D对象实例分割，在室内外环境中保持强泛化能力

Abstract: Accurate 3D instance segmentation is crucial for high-quality scene
understanding in the 3D vision domain. However, 3D instance segmentation based
on 2D-to-3D lifting approaches struggle to produce precise instance-level
segmentation, due to accumulated errors introduced during the lifting process
from ambiguous semantic guidance and insufficient depth constraints. To tackle
these challenges, we propose splitting and growing reliable semantic mask for
high-fidelity 3D instance segmentation (SGS-3D), a novel "split-then-grow"
framework that first purifies and splits ambiguous lifted masks using geometric
primitives, and then grows them into complete instances within the scene.
Unlike existing approaches that directly rely on raw lifted masks and sacrifice
segmentation accuracy, SGS-3D serves as a training-free refinement method that
jointly fuses semantic and geometric information, enabling effective
cooperation between the two levels of representation. Specifically, for
semantic guidance, we introduce a mask filtering strategy that leverages the
co-occurrence of 3D geometry primitives to identify and remove ambiguous masks,
thereby ensuring more reliable semantic consistency with the 3D object
instances. For the geometric refinement, we construct fine-grained object
instances by exploiting both spatial continuity and high-level features,
particularly in the case of semantic ambiguity between distinct objects.
Experimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate that
SGS-3D substantially improves segmentation accuracy and robustness against
inaccurate masks from pre-trained models, yielding high-fidelity object
instances while maintaining strong generalization across diverse indoor and
outdoor environments. Code is available in the supplementary materials.

</details>


### [47] [SL-SLR: Self-Supervised Representation Learning for Sign Language Recognition](https://arxiv.org/abs/2509.05188)
*Ariel Basso Madjoukeng,Jérôme Fink,Pierre Poitier,Edith Belise Kenmogne,Benoit Frenay*

Main category: cs.CV

TL;DR: 本文提出了一种针对手语识别(SLR)的自监督学习框架，通过自由负样本对和新的数据增强技术，解决了对比学习中忽视关键区域和负样本相似度过高的问题，显著提升了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 手语识别面临标注数据稀缺的问题，现有对比学习方法存在两个主要问题：(1)平等对待视频所有部分，忽视关键信息区域的重要性；(2)不同手语间的共享动作导致负样本过于相似，难以区分。这些问题导致学习到的特征缺乏判别性。

Method: 提出了包含两个关键组件的自监督学习框架：1)使用自由负样本对的新自监督方法；2)新的数据增强技术。这两个组件协同工作，专注于学习对手语识别有意义的表示。

Result: 与多种对比学习和自监督方法相比，该方法在线性评估、半监督学习以及跨手语迁移任务中都显示出显著的准确率提升。

Conclusion: 该框架通过解决对比学习在手语识别中的特定问题，成功学习了更具判别性的特征表示，为手语识别任务提供了有效的自监督解决方案。

Abstract: Sign language recognition (SLR) is a machine learning task aiming to identify
signs in videos. Due to the scarcity of annotated data, unsupervised methods
like contrastive learning have become promising in this field. They learn
meaningful representations by pulling positive pairs (two augmented versions of
the same instance) closer and pushing negative pairs (different from the
positive pairs) apart. In SLR, in a sign video, only certain parts provide
information that is truly useful for its recognition. Applying contrastive
methods to SLR raises two issues: (i) contrastive learning methods treat all
parts of a video in the same way, without taking into account the relevance of
certain parts over others; (ii) shared movements between different signs make
negative pairs highly similar, complicating sign discrimination. These issues
lead to learning non-discriminative features for sign recognition and poor
results in downstream tasks. In response, this paper proposes a self-supervised
learning framework designed to learn meaningful representations for SLR. This
framework consists of two key components designed to work together: (i) a new
self-supervised approach with free-negative pairs; (ii) a new data augmentation
technique. This approach shows a considerable gain in accuracy compared to
several contrastive and self-supervised methods, across linear evaluation,
semi-supervised learning, and transferability between sign languages.

</details>


### [48] [Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet](https://arxiv.org/abs/2509.05198)
*Mohammad Saeid,Amir Salarpour,Pedram MohajerAnsari*

Main category: cs.CV

TL;DR: 本文提出了ModelNet-R数据集来解决ModelNet40的问题，并设计了轻量级图神经网络Point-SkipNet，在降低计算开销的同时实现了最先进的分类精度。


<details>
  <summary>Details</summary>
Motivation: 现有的ModelNet40数据集存在标签不一致、2D数据、尺寸不匹配和类别区分不足等问题，限制了3D点云分类模型的性能。

Method: 提出了精心优化的ModelNet-R数据集，并设计了基于图的轻量级神经网络Point-SkipNet，采用高效采样、邻域分组和跳跃连接技术。

Result: 在ModelNet-R上训练的模型性能显著提升，Point-SkipNet以更少的参数量实现了最先进的分类精度。

Conclusion: 数据集质量对于优化3D点云分类模型效率至关重要，ModelNet-R和Point-SkipNet为相关研究提供了更可靠的基准和高效解决方案。

Abstract: The classification of 3D point clouds is crucial for applications such as
autonomous driving, robotics, and augmented reality. However, the commonly used
ModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D
data, size mismatches, and inadequate class differentiation, which hinder model
performance. This paper introduces ModelNet-R, a meticulously refined version
of ModelNet40 designed to address these issues and serve as a more reliable
benchmark. Additionally, this paper proposes Point-SkipNet, a lightweight
graph-based neural network that leverages efficient sampling, neighborhood
grouping, and skip connections to achieve high classification accuracy with
reduced computational overhead. Extensive experiments demonstrate that models
trained in ModelNet-R exhibit significant performance improvements. Notably,
Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a
substantially lower parameter count compared to contemporary models. This
research highlights the crucial role of dataset quality in optimizing model
efficiency for 3D point cloud classification. For more details, see the code
at: https://github.com/m-saeid/ModeNetR_PointSkipNet.

</details>


### [49] [Symbolic Graphics Programming with Large Language Models](https://arxiv.org/abs/2509.05208)
*Yamei Chen,Haoquan Zhang,Yangyi Huang,Zeju Qiu,Kaipeng Zhang,Yandong Wen,Weiyang Liu*

Main category: cs.CV

TL;DR: 该论文研究了大型语言模型生成符号图形程序（SVG）的能力，提出了SGP-GenBench基准测试，并开发了基于强化学习的改进方法，显著提升了SVG生成质量。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在生成符号图形程序方面的能力，特别是从自然语言描述生成可渲染精确视觉内容的SVG程序，这有助于理解LLMs如何通过程序生成来理解视觉世界。

Method: 引入SGP-GenBench基准测试评估LLMs的SVG生成能力；提出基于强化学习的方法，使用格式有效性验证和跨模态奖励（SigLIP和DINO编码器）来优化SVG生成质量。

Result: 前沿专有模型显著优于开源模型，性能与通用编码能力相关；提出的RL方法在Qwen-2.5-7B上大幅提升SVG生成质量和语义准确性，达到与前沿系统相当的性能。

Conclusion: 符号图形编程为跨模态 grounding 提供了精确且可解释的研究视角，RL训练能够诱导更精细的对象分解和上下文细节，提升场景连贯性。

Abstract: Large language models (LLMs) excel at program synthesis, yet their ability to
produce symbolic graphics programs (SGPs) that render into precise visual
content remains underexplored. We study symbolic graphics programming, where
the goal is to generate an SGP from a natural-language description. This task
also serves as a lens into how LLMs understand the visual world by prompting
them to generate images rendered from SGPs. Among various SGPs, our paper
sticks to scalable vector graphics (SVGs). We begin by examining the extent to
which LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a
comprehensive benchmark covering object fidelity, scene fidelity, and
compositionality (attribute binding, spatial relations, numeracy). On
SGP-GenBench, we discover that frontier proprietary models substantially
outperform open-source models, and performance correlates well with general
coding capabilities. Motivated by this gap, we aim to improve LLMs' ability to
generate SGPs. We propose a reinforcement learning (RL) with verifiable rewards
approach, where a format-validity gate ensures renderable SVG, and a
cross-modal reward aligns text and the rendered image via strong vision
encoders (e.g., SigLIP for text-image and DINO for image-image). Applied to
Qwen-2.5-7B, our method substantially improves SVG generation quality and
semantics, achieving performance on par with frontier systems. We further
analyze training dynamics, showing that RL induces (i) finer decomposition of
objects into controllable primitives and (ii) contextual details that improve
scene coherence. Our results demonstrate that symbolic graphics programming
offers a precise and interpretable lens on cross-modal grounding.

</details>


### [50] [COGITAO: A Visual Reasoning Framework To Study Compositionality & Generalization](https://arxiv.org/abs/2509.05249)
*Yassine Taoudi-Benchekroun,Klim Troyan,Pascal Sager,Stefan Gerber,Lukas Tuggener,Benjamin Grewe*

Main category: cs.CV

TL;DR: COGITAO是一个模块化、可扩展的数据生成框架和基准测试，用于系统研究视觉领域的组合性和泛化能力，通过基于规则的任务构建和可组合变换来评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前机器学习模型在组合学习概念并将其应用于新场景方面的局限性，这是人类智能的关键能力但在现有模型中仍存在不足。

Method: 基于ARC-AGI的问题设置，构建基于规则的任务，应用28种可互操作的变换对网格环境中的对象进行处理，支持可调整深度的组合，并提供对网格参数和对象属性的广泛控制。

Result: 能够创建数百万个独特的任务规则，远超现有数据集规模，生成几乎无限的任务样本。基线实验显示最先进的视觉模型虽然域内性能强劲，但在新组合的泛化方面持续失败。

Conclusion: COGITAO作为一个开源框架，为组合性和泛化研究提供了强大的工具，揭示了当前模型在组合泛化方面的根本性挑战，支持该领域的持续研究。

Abstract: The ability to compose learned concepts and apply them in novel settings is
key to human intelligence, but remains a persistent limitation in
state-of-the-art machine learning models. To address this issue, we introduce
COGITAO, a modular and extensible data generation framework and benchmark
designed to systematically study compositionality and generalization in visual
domains. Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs
rule-based tasks which apply a set of transformations to objects in grid-like
environments. It supports composition, at adjustable depth, over a set of 28
interoperable transformations, along with extensive control over grid
parametrization and object properties. This flexibility enables the creation of
millions of unique task rules -- surpassing concurrent datasets by several
orders of magnitude -- across a wide range of difficulties, while allowing
virtually unlimited sample generation per rule. We provide baseline experiments
using state-of-the-art vision models, highlighting their consistent failures to
generalize to novel combinations of familiar elements, despite strong in-domain
performance. COGITAO is fully open-sourced, including all code and datasets, to
support continued research in this field.

</details>


### [51] [WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool](https://arxiv.org/abs/2509.05296)
*Zizun Li,Jianjun Zhou,Yifan Wang,Haoyu Guo,Wenzheng Chang,Yang Zhou,Haoyi Zhu,Junyi Chen,Chunhua Shen,Tong He*

Main category: cs.CV

TL;DR: WinT3R是一个前馈重建模型，能够在在线预测精确相机位姿和高质量点云地图，在重建质量和实时性能之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法在重建质量和实时性能之间存在权衡问题，需要一种既能保证高质量几何预测又不牺牲计算效率的解决方案。

Method: 采用滑动窗口机制确保窗口内帧间充分信息交换，使用紧凑相机表示并维护全局相机令牌池来提升相机位姿估计的可靠性。

Result: 在多个数据集上的广泛实验验证，WinT3R在在线重建质量、相机位姿估计和重建速度方面达到了最先进的性能。

Conclusion: WinT3R通过创新的滑动窗口机制和相机表示方法，成功解决了重建质量与实时性能的权衡问题，实现了高效高质量的在线重建。

Abstract: We present WinT3R, a feed-forward reconstruction model capable of online
prediction of precise camera poses and high-quality point maps. Previous
methods suffer from a trade-off between reconstruction quality and real-time
performance. To address this, we first introduce a sliding window mechanism
that ensures sufficient information exchange among frames within the window,
thereby improving the quality of geometric predictions without large
computation. In addition, we leverage a compact representation of cameras and
maintain a global camera token pool, which enhances the reliability of camera
pose estimation without sacrificing efficiency. These designs enable WinT3R to
achieve state-of-the-art performance in terms of online reconstruction quality,
camera pose estimation, and reconstruction speed, as validated by extensive
experiments on diverse datasets. Code and model are publicly available at
https://github.com/LiZizun/WinT3R.

</details>


### [52] [FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases](https://arxiv.org/abs/2509.05297)
*Matteo Poggi,Fabio Tosi*

Main category: cs.CV

TL;DR: FlowSeek是一个新颖的光流框架，只需单个消费级GPU即可训练，硬件需求比现有方法低8倍，但在多个数据集上实现了10-15%的性能提升


<details>
  <summary>Details</summary>
Motivation: 为了解决光流训练需要大量硬件资源的问题，开发一个既紧凑又准确的光流架构，降低训练成本同时保持优异性能

Method: 结合光流网络设计空间的最新进展、先进的单图像深度基础模型和经典的低维运动参数化方法

Result: 在Sintel Final、KITTI、Spring和LayeredFlow数据集上实现了优于之前最先进方法SEA-RAFT的性能，相对改进达到10%和15%

Conclusion: FlowSeek证明了通过巧妙结合现有技术，可以在大幅降低硬件需求的同时实现更好的跨数据集泛化性能

Abstract: We present FlowSeek, a novel framework for optical flow requiring minimal
hardware resources for training. FlowSeek marries the latest advances on the
design space of optical flow networks with cutting-edge single-image depth
foundation models and classical low-dimensional motion parametrization,
implementing a compact, yet accurate architecture. FlowSeek is trained on a
single consumer-grade GPU, a hardware budget about 8x lower compared to most
recent methods, and still achieves superior cross-dataset generalization on
Sintel Final and KITTI, with a relative improvement of 10 and 15% over the
previous state-of-the-art SEA-RAFT, as well as on Spring and LayeredFlow
datasets.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [53] [Inferring the Graph Structure of Images for Graph Neural Networks](https://arxiv.org/abs/2509.04677)
*Mayur S Gowda,John Shi,Augusto Santos,José M. F. Moura*

Main category: eess.IV

TL;DR: 该论文提出通过寻找替代网格图的图表示方法来改进MNIST和Fashion-MNIST数据集的图神经网络分类准确率，使用行相关、列相关和乘积图等新图结构。


<details>
  <summary>Details</summary>
Motivation: 传统方法将图像表示为网格图，每个节点代表像素，边连接相邻像素。但这种方法可能不是最优的图表示，需要寻找更好的图结构来提高GNN性能。

Method: 基于像素值之间的相关性，为每个图像构建行相关图、列相关图和乘积图，替代传统的网格图和超像素方法。

Result: 实验表明，使用这些不同的图表示作为GNN输入，相比传统网格图和超像素方法，分类准确率有所提高。

Conclusion: 通过优化图像数据的图表示结构，可以有效提升图神经网络在下游任务中的性能表现。

Abstract: Image datasets such as MNIST are a key benchmark for testing Graph Neural
Network (GNN) architectures. The images are traditionally represented as a grid
graph with each node representing a pixel and edges connecting neighboring
pixels (vertically and horizontally). The graph signal is the values
(intensities) of each pixel in the image. The graphs are commonly used as input
to graph neural networks (e.g., Graph Convolutional Neural Networks (Graph
CNNs) [1, 2], Graph Attention Networks (GAT) [3], GatedGCN [4]) to classify the
images. In this work, we improve the accuracy of downstream graph neural
network tasks by finding alternative graphs to the grid graph and superpixel
methods to represent the dataset images, following the approach in [5, 6]. We
find row correlation, column correlation, and product graphs for each image in
MNIST and Fashion-MNIST using correlations between the pixel values building on
the method in [5, 6]. Experiments show that using these different graph
representations and features as input into downstream GNN models improves the
accuracy over using the traditional grid graph and superpixel methods in the
literature.

</details>


### [54] [AURAD: Anatomy-Pathology Unified Radiology Synthesis with Progressive Representations](https://arxiv.org/abs/2509.04819)
*Shuhan Ding,Jingjing Fu,Yu Gu,Naiteek Sangani,Mu Wei,Paul Vozila,Nan Liu,Jiang Bian,Hoifung Poon*

Main category: eess.IV

TL;DR: AURAD是一个可控的放射学合成框架，能够联合生成高保真胸部X光片和伪语义掩码，解决了医学图像合成中细粒度控制和临床相关性的挑战。


<details>
  <summary>Details</summary>
Motivation: 医学图像合成在数据稀缺的临床环境中至关重要，但现有方法难以处理胸部X光片中形态多样且与解剖结构紧密交织的疾病模式，限制了合成图像的多样性和临床实用性。

Method: 采用渐进式流程：首先基于解剖结构从临床提示生成伪掩码，然后利用这些掩码指导图像合成。使用预训练的专家医学模型过滤输出以确保临床合理性，生成的掩码还可用于下游检测和分割任务。

Result: 78%的合成图像被认证放射科医生分类为真实图像，超过40%的分割覆盖被评定为临床有用。方法在任务和数据集上展现出有效性和泛化能力。

Conclusion: AURAD框架不仅实现了高质量的医学图像合成，还通过生成的伪掩码连接了生成建模与真实临床应用，为数据增强和模型泛化提供了有效解决方案。

Abstract: Medical image synthesis has become an essential strategy for augmenting
datasets and improving model generalization in data-scarce clinical settings.
However, fine-grained and controllable synthesis remains difficult due to
limited high-quality annotations and domain shifts across datasets. Existing
methods, often designed for natural images or well-defined tumors, struggle to
generalize to chest radiographs, where disease patterns are morphologically
diverse and tightly intertwined with anatomical structures. To address these
challenges, we propose AURAD, a controllable radiology synthesis framework that
jointly generates high-fidelity chest X-rays and pseudo semantic masks. Unlike
prior approaches that rely on randomly sampled masks-limiting diversity,
controllability, and clinical relevance-our method learns to generate masks
that capture multi-pathology coexistence and anatomical-pathological
consistency. It follows a progressive pipeline: pseudo masks are first
generated from clinical prompts conditioned on anatomical structures, and then
used to guide image synthesis. We also leverage pretrained expert medical
models to filter outputs and ensure clinical plausibility. Beyond visual
realism, the synthesized masks also serve as labels for downstream tasks such
as detection and segmentation, bridging the gap between generative modeling and
real-world clinical applications. Extensive experiments and blinded radiologist
evaluations demonstrate the effectiveness and generalizability of our method
across tasks and datasets. In particular, 78% of our synthesized images are
classified as authentic by board-certified radiologists, and over 40% of
predicted segmentation overlays are rated as clinically useful. All code,
pre-trained models, and the synthesized dataset will be released upon
publication.

</details>


### [55] [Multi-modal Uncertainty Robust Tree Cover Segmentation For High-Resolution Remote Sensing Images](https://arxiv.org/abs/2509.04870)
*Yuanyuan Gui,Wei Li,Yinjian Wang,Xiang-Gen Xia,Mauro Marty,Christian Ginzler,Zuyuan Wang*

Main category: eess.IV

TL;DR: 提出MURTreeFormer框架，通过概率潜在表示和VAE重采样机制处理多模态遥感图像中的时间错位不确定性，显著提升树冠覆盖分割精度


<details>
  <summary>Details</summary>
Motivation: 多模态遥感图像采集时间差异导致的时间错位问题会引入跨模态不确定性，严重影响高分辨率图像的语义分割精度，特别是在植被监测等应用中

Method: MURTreeFormer框架将一种模态作为主要模态，其他作为辅助模态，通过概率潜在表示建模辅助模态的块级不确定性，使用VAE重采样机制重构不确定区域，并集成梯度幅度注意力模块和轻量级细化头

Result: 在上海和苏黎世的多模态数据集上进行广泛实验，证明MURTreeFormer显著提高了分割性能，有效减少了时间诱导的随机不确定性影响

Conclusion: 该方法为多模态遥感图像语义分割中的时间错位问题提供了有效解决方案，通过显式建模和处理不确定性来提升树冠覆盖制图的鲁棒性

Abstract: Recent advances in semantic segmentation of multi-modal remote sensing images
have significantly improved the accuracy of tree cover mapping, supporting
applications in urban planning, forest monitoring, and ecological assessment.
Integrating data from multiple modalities-such as optical imagery, light
detection and ranging (LiDAR), and synthetic aperture radar (SAR)-has shown
superior performance over single-modality methods. However, these data are
often acquired days or even months apart, during which various changes may
occur, such as vegetation disturbances (e.g., logging, and wildfires) and
variations in imaging quality. Such temporal misalignments introduce
cross-modal uncertainty, especially in high-resolution imagery, which can
severely degrade segmentation accuracy. To address this challenge, we propose
MURTreeFormer, a novel multi-modal segmentation framework that mitigates and
leverages aleatoric uncertainty for robust tree cover mapping. MURTreeFormer
treats one modality as primary and others as auxiliary, explicitly modeling
patch-level uncertainty in the auxiliary modalities via a probabilistic latent
representation. Uncertain patches are identified and reconstructed from the
primary modality's distribution through a VAE-based resampling mechanism,
producing enhanced auxiliary features for fusion. In the decoder, a gradient
magnitude attention (GMA) module and a lightweight refinement head (RH) are
further integrated to guide attention toward tree-like structures and to
preserve fine-grained spatial details. Extensive experiments on multi-modal
datasets from Shanghai and Zurich demonstrate that MURTreeFormer significantly
improves segmentation performance and effectively reduces the impact of
temporally induced aleatoric uncertainty.

</details>


### [56] [INR meets Multi-Contrast MRI Reconstruction](https://arxiv.org/abs/2509.04888)
*Natascha Niessen,Carolin M. Pirkl,Ana Beatriz Solana,Hannah Eichhorn,Veronika Spieker,Wenqi Huang,Tim Sprenger,Marion I. Menzel,Julia A. Schnabel*

Main category: eess.IV

TL;DR: 提出一种基于隐式神经表示(INR)的网络方法，用于多对比度MRI的联合重建，通过互补欠采样模式实现更高加速因子，在MPnRAGE序列上优于现有并行成像压缩感知方法。


<details>
  <summary>Details</summary>
Motivation: 多对比度MRI扫描时间过长限制了临床应用，需要通过欠采样加速，但传统重建方法在高加速因子下效果有限。利用多对比度序列中冗余的解剖信息可以实现更高加速率。

Method: 使用捕获k空间中心对比度信息的欠采样模式，在不同对比度间进行互补的高频欠采样。提出INR网络联合重建所有对比度图像，充分利用跨对比度的互补信息。

Result: 在MPnRAGE多对比度MRI序列上，该方法在更高加速因子下仍能获得高质量图像，性能优于最先进的并行成像压缩感知(PICS)重建方法。

Conclusion: 基于INR的联合重建方法能够有效利用多对比度MRI的互补信息，显著提高加速能力，为临床常规应用多对比度序列提供了可行方案。

Abstract: Multi-contrast MRI sequences allow for the acquisition of images with varying
tissue contrast within a single scan. The resulting multi-contrast images can
be used to extract quantitative information on tissue microstructure. To make
such multi-contrast sequences feasible for clinical routine, the usually very
long scan times need to be shortened e.g. through undersampling in k-space.
However, this comes with challenges for the reconstruction. In general,
advanced reconstruction techniques such as compressed sensing or deep
learning-based approaches can enable the acquisition of high-quality images
despite the acceleration. In this work, we leverage redundant anatomical
information of multi-contrast sequences to achieve even higher acceleration
rates. We use undersampling patterns that capture the contrast information
located at the k-space center, while performing complementary undersampling
across contrasts for high frequencies. To reconstruct this highly sparse
k-space data, we propose an implicit neural representation (INR) network that
is ideal for using the complementary information acquired across contrasts as
it jointly reconstructs all contrast images. We demonstrate the benefits of our
proposed INR method by applying it to multi-contrast MRI using the MPnRAGE
sequence, where it outperforms the state-of-the-art parallel imaging compressed
sensing (PICS) reconstruction method, even at higher acceleration factors.

</details>


### [57] [VLSM-Ensemble: Ensembling CLIP-based Vision-Language Models for Enhanced Medical Image Segmentation](https://arxiv.org/abs/2509.05154)
*Julia Dietlmeier,Oluwabukola Grace Adegboro,Vayangi Ganepola,Claudia Mazo,Noel E. O'Connor*

Main category: eess.IV

TL;DR: 通过将视觉语言分割模型与低复杂度CNN集成，在医学图像分割任务中显著提升了性能，特别是在BKAI息肉数据集上Dice分数提高了6.3%。


<details>
  <summary>Details</summary>
Motivation: 当前基于CLIP和BiomedCLIP的视觉语言分割模型性能仍落后于CRIS等复杂架构，研究试图通过模型集成而非文本提示工程来缩小这一差距。

Method: 采用集成学习方法，将视觉语言分割模型（VLSMs）与低复杂度的卷积神经网络（CNN）进行组合。

Result: 在BKAI息肉数据集上Dice分数提升6.3%，其他数据集提升1%-6%。在四个放射学和非放射学数据集上提供了初步结果，集成效果因数据集而异。

Conclusion: 模型集成在不同数据集上表现差异显著（从优于到劣于CRIS模型），这为社区未来的研究提供了重要方向。

Abstract: Vision-language models and their adaptations to image segmentation tasks
present enormous potential for producing highly accurate and interpretable
results. However, implementations based on CLIP and BiomedCLIP are still
lagging behind more sophisticated architectures such as CRIS. In this work,
instead of focusing on text prompt engineering as is the norm, we attempt to
narrow this gap by showing how to ensemble vision-language segmentation models
(VLSMs) with a low-complexity CNN. By doing so, we achieve a significant Dice
score improvement of 6.3% on the BKAI polyp dataset using the ensembled
BiomedCLIPSeg, while other datasets exhibit gains ranging from 1% to 6%.
Furthermore, we provide initial results on additional four radiology and
non-radiology datasets. We conclude that ensembling works differently across
these datasets (from outperforming to underperforming the CRIS model),
indicating a topic for future investigation by the community. The code is
available at https://github.com/juliadietlmeier/VLSM-Ensemble.

</details>


### [58] [Exploring Autoregressive Vision Foundation Models for Image Compression](https://arxiv.org/abs/2509.05169)
*Huu-Tai Phung,Yu-Hsiang Lin,Yen-Kuan Ho,Wen-Hsiao Peng*

Main category: eess.IV

TL;DR: 首次尝试将视觉基础模型重新用于图像压缩，在极低码率下获得更好的感知质量


<details>
  <summary>Details</summary>
Motivation: 视视基础模型在生成任务中表现突出，具备编码器-解码器结构，类似于学习型图像压缩算法，可能适用于低码率压缩

Method: 利用VFM中的自回归模型进行编码，基于已编码的token来预测和编码下一个token，这种方法不同于以前仅依赖条件生成的语义压缩方法

Result: 实验结果显示，某些预训练的通用VFM在极低码率下比专门的学习型图像压缩算法具有更优的感知质量

Conclusion: 这为利用视觉基础模型进行低码率、语义丰富的图像压缩开启了一条有前景的研究方向

Abstract: This work presents the first attempt to repurpose vision foundation models
(VFMs) as image codecs, aiming to explore their generation capability for
low-rate image compression. VFMs are widely employed in both conditional and
unconditional generation scenarios across diverse downstream tasks, e.g.,
physical AI applications. Many VFMs employ an encoder-decoder architecture
similar to that of end-to-end learned image codecs and learn an autoregressive
(AR) model to perform next-token prediction. To enable compression, we
repurpose the AR model in VFM for entropy coding the next token based on
previously coded tokens. This approach deviates from early semantic compression
efforts that rely solely on conditional generation for reconstructing input
images. Extensive experiments and analysis are conducted to compare VFM-based
codec to current SOTA codecs optimized for distortion or perceptual quality.
Notably, certain pre-trained, general-purpose VFMs demonstrate superior
perceptual quality at extremely low bitrates compared to specialized learned
image codecs. This finding paves the way for a promising research direction
that leverages VFMs for low-rate, semantically rich image compression.

</details>


### [59] [Generation of realistic cardiac ultrasound sequences with ground truth motion and speckle decorrelation](https://arxiv.org/abs/2509.05261)
*Thierry Judge,Nicolas Duchateau,Khuram Faraz,Pierre-Marc Jodoin,Olivier Bernard*

Main category: eess.IV

TL;DR: 提出了一种改进的超声图像模拟框架，通过显式考虑散斑去相关效应来增强模拟图像序列的真实性。


<details>
  <summary>Details</summary>
Motivation: 现有的超声图像模拟管道在生成左心室应变估计的训练数据时，由于未考虑散斑去相关效应，导致模拟图像的真实性有限。

Method: 在现有超声模拟管道基础上，引入动态散斑变化模型。从真实超声序列和心肌分割出发生成网格指导图像形成，使用基于真实超声数据相关性测量得到的相干图来局部自适应调整散射体分布。

Result: 在CAMUS数据库98名患者数据上评估，通过比较真实和模拟图像的相关性曲线，新方法相比基线管道获得了更低的平均绝对误差。

Conclusion: 所提出的方法能够更准确地重现临床数据中观察到的散斑去相关行为，显著提高了超声模拟图像的真实性。

Abstract: Simulated ultrasound image sequences are key for training and validating
machine learning algorithms for left ventricular strain estimation. Several
simulation pipelines have been proposed to generate sequences with
corresponding ground truth motion, but they suffer from limited realism as they
do not consider speckle decorrelation. In this work, we address this limitation
by proposing an improved simulation framework that explicitly accounts for
speckle decorrelation. Our method builds on an existing ultrasound simulation
pipeline by incorporating a dynamic model of speckle variation. Starting from
real ultrasound sequences and myocardial segmentations, we generate meshes that
guide image formation. Instead of applying a fixed ratio of myocardial and
background scatterers, we introduce a coherence map that adapts locally over
time. This map is derived from correlation values measured directly from the
real ultrasound data, ensuring that simulated sequences capture the
characteristic temporal changes observed in practice. We evaluated the realism
of our approach using ultrasound data from 98 patients in the CAMUS database.
Performance was assessed by comparing correlation curves from real and
simulated images. The proposed method achieved lower mean absolute error
compared to the baseline pipeline, indicating that it more faithfully
reproduces the decorrelation behavior seen in clinical data.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [60] [Narrative-to-Scene Generation: An LLM-Driven Pipeline for 2D Game Environments](https://arxiv.org/abs/2509.04481)
*Yi-Chun Chen,Arnav Jhala*

Main category: cs.GR

TL;DR: 这篇论文提出了一种轻量级流水线，能够将短文本故事提示转换为可玩的2D绘制游戏场景序列，实现故事驱动的程序内容生成。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在故事生成方面取得了进展，但将史述性文本与可玩视觉环境连接起来仍是程序内容生成领域的挑战。

Method: 系统先从LLM生成的故事中识别三个关键时间帧，提取"对象-关系-对象"三元组形式的空间谓词，然后使用GameTileNet数据集的affordance语义嵌入来检索视觉资产。使用细胞自动机生成层状地形，并基于谓词结构的空间规则来放置对象。

Result: 在十个多样化故事中评估了系统性能，分析了地瓦-对象匹配、affordance-层对齐和空间约束满足情况。

Conclusion: 这个原型系统提供了一种可扩展的史述驱动场景生成方法，为未来在多帧连续性、符号跟踪和多代理协调方面的研究奠定了基础。

Abstract: Recent advances in large language models(LLMs) enable compelling story
generation, but connecting narrative text to playable visual environments
remains an open challenge in procedural content generation(PCG). We present a
lightweight pipeline that transforms short narrative prompts into a sequence of
2D tile-based game scenes, reflecting the temporal structure of stories. Given
an LLM-generated narrative, our system identifies three key time frames,
extracts spatial predicates in the form of "Object-Relation-Object" triples,
and retrieves visual assets using affordance-aware semantic embeddings from the
GameTileNet dataset. A layered terrain is generated using Cellular Automata,
and objects are placed using spatial rules grounded in the predicate structure.
We evaluated our system in ten diverse stories, analyzing tile-object matching,
affordance-layer alignment, and spatial constraint satisfaction across frames.
This prototype offers a scalable approach to narrative-driven scene generation
and lays the foundation for future work on multi-frame continuity, symbolic
tracking, and multi-agent coordination in story-centered PCG.

</details>


### [61] [Fidelity-preserving enhancement of ptychography with foundational text-to-image models](https://arxiv.org/abs/2509.04513)
*Ming Du,Volker Rose,Junjing Deng,Dileep Singh,Si Chen,Mathew J. Cherukara*

Main category: cs.GR

TL;DR: 提出了一种结合物理模型相位恢复和文本引导扩散模型的PnP框架，用于消除ptychographic成像中的伪影，通过ADMM算法确保数据保真度和伪影去除的共识，显著提升了图像质量。


<details>
  <summary>Details</summary>
Motivation: ptychographic相位恢复技术虽然能实现高分辨率成像，但经常受到网格病理和多层串扰等伪影的影响，导致重建图像质量下降。需要一种既能保持物理一致性又能有效去除伪影的方法。

Method: 采用交替方向乘子法(ADMM)将基于物理模型的相位恢复与文本引导的图像编辑相结合，使用预训练的基础扩散模型(LEDITS++)进行伪影去除，用户可以通过自然语言指定需要去除的伪影类型。

Result: 在模拟和实验数据集上的演示显示，该方法在伪影抑制和结构保真度方面都有显著改进，通过PSNR和衍射图案一致性等指标验证了其有效性。

Conclusion: 这项工作展示了文本引导生成模型与基于模型的相位恢复算法相结合，可作为高质量衍射成像的可迁移且保真度保持的方法。

Abstract: Ptychographic phase retrieval enables high-resolution imaging of complex
samples but often suffers from artifacts such as grid pathology and multislice
crosstalk, which degrade reconstructed images. We propose a plug-and-play (PnP)
framework that integrates physics model-based phase retrieval with text-guided
image editing using foundational diffusion models. By employing the alternating
direction method of multipliers (ADMM), our approach ensures consensus between
data fidelity and artifact removal subproblems, maintaining physics consistency
while enhancing image quality. Artifact removal is achieved using a text-guided
diffusion image editing method (LEDITS++) with a pre-trained foundational
diffusion model, allowing users to specify artifacts for removal in natural
language. Demonstrations on simulated and experimental datasets show
significant improvements in artifact suppression and structural fidelity,
validated by metrics such as peak signal-to-noise ratio (PSNR) and diffraction
pattern consistency. This work highlights the combination of text-guided
generative models and model-based phase retrieval algorithms as a transferable
and fidelity-preserving method for high-quality diffraction imaging.

</details>


### [62] [Improved 3D Scene Stylization via Text-Guided Generative Image Editing with Region-Based Control](https://arxiv.org/abs/2509.05285)
*Haruo Fujiwara,Yusuke Mukuta,Tatsuya Harada*

Main category: cs.GR

TL;DR: 通过改进的注意力机制和多游规图技术，提高了文本驱动3D场景粉刷的质量和视图一致性，并支持区域控制的多样式混合


<details>
  <summary>Details</summary>
Motivation: 解决现有文本驱动3D场景美化方法在高质量粉刷、视图一致性和区域语义对应方面的挑战

Method: 扩展样式对齐深度条件视图生成框架，使用单一参考基础的注意力共享机制，采用多深度地图网格加强视图一致性，并提出多区域重要性切片水远距离损失

Result: 实验评估显示该方法在文本驱动3D粉刷方面有效改善了结果质量

Conclusion: 该管线通过创新的注意力对齐和视图一致性技术，成功解决了高质量3D粉刷的关键挑战

Abstract: Recent advances in text-driven 3D scene editing and stylization, which
leverage the powerful capabilities of 2D generative models, have demonstrated
promising outcomes. However, challenges remain in ensuring high-quality
stylization and view consistency simultaneously. Moreover, applying style
consistently to different regions or objects in the scene with semantic
correspondence is a challenging task. To address these limitations, we
introduce techniques that enhance the quality of 3D stylization while
maintaining view consistency and providing optional region-controlled style
transfer. Our method achieves stylization by re-training an initial 3D
representation using stylized multi-view 2D images of the source views.
Therefore, ensuring both style consistency and view consistency of stylized
multi-view images is crucial. We achieve this by extending the style-aligned
depth-conditioned view generation framework, replacing the fully shared
attention mechanism with a single reference-based attention-sharing mechanism,
which effectively aligns style across different viewpoints. Additionally,
inspired by recent 3D inpainting methods, we utilize a grid of multiple depth
maps as a single-image reference to further strengthen view consistency among
stylized images. Finally, we propose Multi-Region Importance-Weighted Sliced
Wasserstein Distance Loss, allowing styles to be applied to distinct image
regions using segmentation masks from off-the-shelf models. We demonstrate that
this optional feature enhances the faithfulness of style transfer and enables
the mixing of different styles across distinct regions of the scene.
Experimental evaluations, both qualitative and quantitative, demonstrate that
our pipeline effectively improves the results of text-driven 3D stylization.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [63] [Relativity of Time in Earthquake Physics](https://arxiv.org/abs/2509.04858)
*A. V. Guglielmi,O. D. Zotov*

Main category: physics.geo-ph

TL;DR: 本文通过借用相对论中的固有时间概念，开发了地震源的固有时间理论框架，并利用前震和余震观测数据测量固有时间，发现源参数的非平稳性。


<details>
  <summary>Details</summary>
Motivation: 纪念爱因斯坦相对论120周年，将相对论中的固有时间概念应用于非相对论的地球物理对象——地震源的研究，以丰富地震实验研究的可能性。

Method: 基于地震现象学理论框架，开发源固有时间概念，利用前震和余震观测数据测量固有时间，通过同步地下时钟和世界时钟来研究时间流动。

Result: 发现地震源固有时间相对于世界时间流动不均匀，识别出主震前源演化的两个阶段，补充了之前建立的主震后三相弛豫图像，表明源参数具有非平稳性。

Conclusion: 源固有时间的概念有效丰富了地震实验研究的工具和方法，证明了这一跨学科概念在地球物理研究中的实用性。

Abstract: In this paper we develop an understanding of the proper time of the Tohoku
earthquake source. The paper is dedicated to the 120th anniversary of
Einstein's theory of relativity, but the dedication is symbolic, since we are
investigating a purely non-relativistic geophysical object. We still found it
possible to borrow the terms time relativity and proper time from the theory of
relativity. The concept of the proper time of the source is developed by us
within the framework of the phenomenological theory of earthquakes. The paper
describes a procedure for measuring proper time using observation data of
foreshocks and aftershocks. We synchronized the imaginary underground clock
that counted the proper time and the clock that showed world time. The idea of
a hypothetical underground clock turned out to be effective. We have shown that
the proper time of the source flows unevenly relative to the flow of world
time. Two phases of the evolution of the source before the main shock of the
earthquake were discovered. This complements the picture of three-phase
relaxation of the source after the main shock, which we established earlier.
The uneven flow of proper time relative to world time indicates the
non-stationarity of the source parameters. The overall conclusion is that the
concept of the proper time of the source has enriched the possibilities of
experimental study of earthquakes.

</details>


### [64] [Spectrum of slip dynamics, scaling & statistical laws emerge from simplified model of fault and damage zone architecture](https://arxiv.org/abs/2509.04909)
*M. Almakari,N. Kheirdast,C. Villafuerte,M. Y. Thomas,P. Dubernet,J. Cheng,A. Gupta,P. Romanet,S. Chaillat,H. S. Bhat*

Main category: physics.geo-ph

TL;DR: 通过引入主断层周围的损伤带（分布式裂缝），即使断层摩擦性质均匀，也能自然产生从慢滑到快速破裂的连续谱，重现地震观测中的多种统计规律和现象。


<details>
  <summary>Details</summary>
Motivation: 地震和大地测量观测显示断层滑动存在多种动态行为和统计规律，但背后的物理机制尚不清楚。研究旨在探索断层周围损伤带如何影响滑动动力学。

Method: 建立二维剪切断层带模型，主断层周围分布遵循幂律尺寸和密度分布的裂缝，所有裂缝采用速率-状态摩擦定律，使用准动态边界积分模拟和分层矩阵加速计算。

Result: 模拟重现了Omori定律、逆Omori定律、Gutenberg-Richter标度律和矩-持续时间标度律，观察到从慢滑到快速破裂的自然连续谱，以及震颤、VLFE、LFE、SSE和地震等多种现象。

Conclusion: 断层周围损伤带是解释观测到的多种滑动动态和统计规律的关键因素，即使在没有流体和空间异质性的情况下，也能自然产生复杂的地震活动模式。

Abstract: Seismological and geodetic observations of a fault zone reveal a wide range
of slip dynamics, scaling, and statistical laws. However, the underlying
physical mechanisms remain unclear. In this study, we show that incorporating
an off-fault damage zone-characterized by distributed fractures surrounding a
main fault-can reproduce many key features observed in seismic and geodetic
data. We model a 2D shear fault zone in which off-fault cracks follow power-law
size and density distributions, and are oriented either optimally or parallel
to the main fault. All fractures follow the rate-and-state friction law with
parameters chosen such that each can host slip instabilities. We do not
introduce spatial heterogeneities in the frictional properties of the fault.
Using quasi-dynamic boundary integral simulations accelerated by hierarchical
matrices, we simulate slip dynamics of this system and analyze the events
produced both on and off the main fault. Despite the spatially uniform
frictional properties, we observe a natural continuum from slow to fast
ruptures, as observed in nature. Our simulations reproduce the Omori law, the
inverse Omori law, the Gutenberg-Richter scaling, and the moment-duration
scaling. We also observe seismicity localizing toward the main fault when an
event is about to nucleate on the main fault. During slow slip events,
off-fault seismicity migrates in a pattern resembling a fluid diffusion front,
despite the absence of fluids in the model. We also show that tremors, Very Low
Frequency Earthquakes (VLFEs), Low Frequency Earthquakes (LFEs), Slow Slip
Events (SSEs), and earthquakes (EQs) can all emerge naturally in the `digital
twin' framework.

</details>


### [65] [Ice-free geomorphometry of Enderby Land, East Antarctica: 2. Coastal oases](https://arxiv.org/abs/2509.05141)
*I. V. Florinsky,S. O. Zharnova*

Main category: physics.geo-ph

TL;DR: 本研究对南极恩德比地五个沿海绿洲进行了地貌计量建模与制图，生成了11种重要地貌变量的60幅地图，为南极研究提供了定量化地形数据。


<details>
  <summary>Details</summary>
Motivation: 为南极冰原地区创建科学参考地貌计量图集，获取这些独特景观的定量地形知识，支持南极多学科研究。

Method: 使用REMA高程数据，对五个沿海绿洲进行地貌计量建模，生成坡度、坡向、曲率、汇水面积、地形湿度指数等11种地貌变量的地图。

Result: 成功制作了60幅1:5万和1:7.5万比例尺的地图，以严谨、定量和可重复的方式描述了恩德比地沿海绿洲的地貌特征。

Conclusion: 新的地貌计量数据对该地区的地质、地貌、冰川、生态和水文研究具有重要价值，为南极研究提供了基础数据支持。

Abstract: Geomorphometric modeling and mapping of ice-free Antarctic areas can be
applied for obtaining new quantitative knowledge about the topography of these
unique landscapes and for the further use of morphometric information in
Antarctic research. Within the framework of a project of creating a physical
geographical thematic scientific reference geomorphometric atlas of ice-free
areas of Antarctica, we performed geomorphometric modeling and mapping of five
key coastal oases of Enderby Land, East Antarctica. These include, from west to
east, the Konovalov Oasis, Thala Hills (Molodezhny and Vecherny Oases), Fyfe
Hills, and Howard Hills. As input data, we used five fragments of the Reference
Elevation Model of Antarctica (REMA). For the coastal oases and adjacent ice
sheet and glaciers, we derived models and maps of eleven, most scientifically
important morphometric variables (i.e., slope, aspect, horizontal curvature,
vertical curvature, minimal curvature, maximal curvature, catchment area,
topographic wetness index, stream power index, total insolation, and wind
exposition index). In total, we derived 60 maps in 1:50,000 and 1:75,000
scales. The obtained models and maps describe the coastal oases of Enderby Land
in a rigorous, quantitative, and reproducible manner. New morphometric data can
be useful for further geological, geomorphological, glaciological, ecological,
and hydrological studies of this region.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [66] [Sample-efficient Integration of New Modalities into Large Language Models](https://arxiv.org/abs/2509.04606)
*Osman Batur İnce,André F. T. Martins,Oisin Mac Aodha,Edoardo M. Ponti*

Main category: cs.CL

TL;DR: 提出SEMI方法，通过超网络适配器实现少样本高效集成新模态到LLM中，显著提升样本效率


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型需要处理不断演变的模态空间，但从头训练不现实，且现有方法需要大量配对数据，低资源模态难以集成

Method: 设计超网络来适配共享投影器，通过等距变换增加训练模态多样性，利用高资源模态训练，在推理时用少量样本适配任意新模态

Result: SEMI在少样本集成新模态时显著提升效率，达到相同准确率所需数据量减少64倍，支持任意嵌入维度的编码器

Conclusion: SEMI有望扩展基础模型的模态覆盖范围，为低资源模态集成提供高效解决方案

Abstract: Multimodal foundation models can process several modalities. However, since
the space of possible modalities is large and evolving over time, training a
model from scratch to encompass all modalities is unfeasible. Moreover,
integrating a modality into a pre-existing foundation model currently requires
a significant amount of paired data, which is often not available for
low-resource modalities. In this paper, we introduce a method for
sample-efficient modality integration (SEMI) into Large Language Models (LLMs).
To this end, we devise a hypernetwork that can adapt a shared projector --
placed between modality-specific encoders and an LLM -- to any modality. The
hypernetwork, trained on high-resource modalities (i.e., text, speech, audio,
video), is conditioned on a few samples from any arbitrary modality at
inference time to generate a suitable adapter. To increase the diversity of
training modalities, we artificially multiply the number of encoders through
isometric transformations. We find that SEMI achieves a significant boost in
sample efficiency during few-shot integration of new modalities (i.e.,
satellite images, astronomical images, inertial measurements, and molecules)
with encoders of arbitrary embedding dimensionality. For instance, to reach the
same accuracy as 32-shot SEMI, training the projector from scratch needs
64$\times$ more data. As a result, SEMI holds promise to extend the modality
coverage of foundation models.

</details>


### [67] [Phonological Representation Learning for Isolated Signs Improves Out-of-Vocabulary Generalization](https://arxiv.org/abs/2509.04745)
*Lee Kezar,Zed Sehyr,Jesse Thomason*

Main category: cs.CL

TL;DR: 通过语音式归纳偏置（参数解耦和半监督学习）改善短语识别和未见手语重建质量，提高向量量化自动编码器的表征学习能力


<details>
  <summary>Details</summary>
Motivation: 手语数据集常常误差较大，需要模型能够沿用到未见手语上，而向量量化方法的表征学习能力需要进一步验证和改善

Method: 探索两种语音式归纳偏置：参数解耦（结构偏置）和语音半监督学习（正则化技术），通过向量量化自动编码器进行学习

Result: 提出模型学到的表征在未见手语的一次重建和手语识别任务上表现更好，较基线模型更有辨别力

Conclusion: 明确的语言学动机偏置可以有效提高手语表征学习的沿用能力，为语音式归纳提供了量化分析

Abstract: Sign language datasets are often not representative in terms of vocabulary,
underscoring the need for models that generalize to unseen signs. Vector
quantization is a promising approach for learning discrete, token-like
representations, but it has not been evaluated whether the learned units
capture spurious correlations that hinder out-of-vocabulary performance. This
work investigates two phonological inductive biases: Parameter Disentanglement,
an architectural bias, and Phonological Semi-Supervision, a regularization
technique, to improve isolated sign recognition of known signs and
reconstruction quality of unseen signs with a vector-quantized autoencoder. The
primary finding is that the learned representations from the proposed model are
more effective for one-shot reconstruction of unseen signs and more
discriminative for sign identification compared to a controlled baseline. This
work provides a quantitative analysis of how explicit, linguistically-motivated
biases can improve the generalization of learned representations of sign
language.

</details>


### [68] [PRIM: Towards Practical In-Image Multilingual Machine Translation](https://arxiv.org/abs/2509.05146)
*Yanzhi Tian,Zeming Liu,Zhengyang Liu,Chong Feng,Xin Li,Heyan Huang,Yuhang Guo*

Main category: cs.CL

TL;DR: 本文提出了实用的图像内多语言机器翻译(IIMMT)任务和PRIM数据集，以解决现有研究在合成数据上的局限性，并开发了VisTrans模型来处理真实场景中的复杂背景和多语言翻译挑战。


<details>
  <summary>Details</summary>
Motivation: 当前图像内机器翻译研究主要在合成数据上进行，具有简单背景、单一字体、固定文本位置和双语翻译等特点，无法充分反映真实世界情况，导致研究与实际应用之间存在显著差距。

Method: 提出了VisTrans端到端模型，该模型分别处理图像中的视觉文本和背景信息，确保多语言翻译能力的同时提高视觉质量。还构建了PRIM数据集，包含真实世界捕获的单行文本图像，具有复杂背景、多种字体、多样化文本位置，并支持多语言翻译方向。

Result: 实验结果表明，VisTrans相比其他模型在翻译质量和视觉效果方面都取得了更好的表现。

Conclusion: 该研究通过构建PRIM数据集和提出VisTrans模型，推动了图像内多语言机器翻译在真实场景中的研究，为解决实际应用中的复杂挑战提供了有效解决方案。

Abstract: In-Image Machine Translation (IIMT) aims to translate images containing texts
from one language to another. Current research of end-to-end IIMT mainly
conducts on synthetic data, with simple background, single font, fixed text
position, and bilingual translation, which can not fully reflect real world,
causing a significant gap between the research and practical conditions. To
facilitate research of IIMT in real-world scenarios, we explore Practical
In-Image Multilingual Machine Translation (IIMMT). In order to convince the
lack of publicly available data, we annotate the PRIM dataset, which contains
real-world captured one-line text images with complex background, various
fonts, diverse text positions, and supports multilingual translation
directions. We propose an end-to-end model VisTrans to handle the challenge of
practical conditions in PRIM, which processes visual text and background
information in the image separately, ensuring the capability of multilingual
translation while improving the visual quality. Experimental results indicate
the VisTrans achieves a better translation quality and visual effect compared
to other models. The code and dataset are available at:
https://github.com/BITHLP/PRIM.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [69] [Towards an Accurate and Effective Robot Vision (The Problem of Topological Localization for Mobile Robots)](https://arxiv.org/abs/2509.04948)
*Emanuela Boros*

Main category: cs.RO

TL;DR: 这篇论文系统性评估了多种视觉描述子在办公环境中的拓扑定位性能，包括Color Histograms、SIFT、ASIFT、RGB-SIFT和Bag-of-Visual-Words方法，并在ImageCLEF评测中验证了最佳配置的效果。


<details>
  <summary>Details</summary>
Motivation: 解决移动机器人在办公环境中的拓扑定位问题，应对视觉定位中的感知模糊性、传感器噪声和照明变化带来的挑战。

Method: 使用单目彩色相机获取图像，系统性比较多种视觉描述子、距离测量和分类器，包括颜色直方图、SIFT、ASIFT、RGB-SIFT和视觉词袋方法。

Result: 结果显示适当配置外观描述子、相似性测量和分类器能够提升定位性能，并在ImageCLEF评估中成功识别新图像序列的最可能位置。

Conclusion: 未来工作将探索层次模型、排名方法和特征组合，以建立更稳健的定位系统，减少训练和运行时间，避免维数灾难，实现多样照明条件和更长路径上的实时定位。

Abstract: Topological localization is a fundamental problem in mobile robotics, since
robots must be able to determine their position in order to accomplish tasks.
Visual localization and place recognition are challenging due to perceptual
ambiguity, sensor noise, and illumination variations. This work addresses
topological localization in an office environment using only images acquired
with a perspective color camera mounted on a robot platform, without relying on
temporal continuity of image sequences. We evaluate state-of-the-art visual
descriptors, including Color Histograms, SIFT, ASIFT, RGB-SIFT, and
Bag-of-Visual-Words approaches inspired by text retrieval. Our contributions
include a systematic, quantitative comparison of these features, distance
measures, and classifiers. Performance was analyzed using standard evaluation
metrics and visualizations, extending previous experiments. Results demonstrate
the advantages of proper configurations of appearance descriptors, similarity
measures, and classifiers. The quality of these configurations was further
validated in the Robot Vision task of the ImageCLEF evaluation campaign, where
the system identified the most likely location of novel image sequences. Future
work will explore hierarchical models, ranking methods, and feature
combinations to build more robust localization systems, reducing training and
runtime while avoiding the curse of dimensionality. Ultimately, this aims
toward integrated, real-time localization across varied illumination and longer
routes.

</details>


### [70] [Pointing-Guided Target Estimation via Transformer-Based Attention](https://arxiv.org/abs/2509.05031)
*Luca Müller,Hassan Ali,Philipp Allgeuer,Lukáš Gajdošech,Stefan Wermter*

Main category: cs.RO

TL;DR: 提出MM-ITF多模态转换器架构，通过单目RGB数据准确预测人类指向手势的目标物体，实现直观的人机协作


<details>
  <summary>Details</summary>
Motivation: 指向手势是非语言交流的基本形式，在HRI中机器人需要预测人类意图并做出适当响应，但目前缺乏有效的目标物体预测方法

Method: 采用多模态交互转换器(MM-ITF)架构，利用模态间注意力机制将2D指向手势映射到物体位置，为每个物体分配可能性分数并识别最可能目标

Result: 方法能够准确预测意图目标物体，使用单目RGB数据即可实现，并引入了补丁混淆矩阵来评估模型在不同候选物体位置上的预测性能

Conclusion: MM-ITF架构能够有效处理指向手势的目标识别问题，为直观和可访问的人机协作提供了技术基础

Abstract: Deictic gestures, like pointing, are a fundamental form of non-verbal
communication, enabling humans to direct attention to specific objects or
locations. This capability is essential in Human-Robot Interaction (HRI), where
robots should be able to predict human intent and anticipate appropriate
responses. In this work, we propose the Multi-Modality Inter-TransFormer
(MM-ITF), a modular architecture to predict objects in a controlled tabletop
scenario with the NICOL robot, where humans indicate targets through natural
pointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing
gestures to object locations, assigns a likelihood score to each, and
identifies the most likely target. Our results demonstrate that the method can
accurately predict the intended object using monocular RGB data, thus enabling
intuitive and accessible human-robot collaboration. To evaluate the
performance, we introduce a patch confusion matrix, providing insights into the
model's predictions across candidate object locations. Code available at:
https://github.com/lucamuellercode/MMITF.

</details>


### [71] [Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers](https://arxiv.org/abs/2509.05201)
*Nariman Niknejad,Gokul S. Sankar,Bahare Kiumarsi,Hamidreza Modares*

Main category: cs.RO

TL;DR: 提出了一种针对深度学习感知模块非高斯噪声的鲁棒模型预测控制框架，使用约束zonotopes进行基于集合的状态估计，通过线性规划和Minkowski-Lyapunov方法确保稳定性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设感知误差为零均值高斯噪声，但实际深度学习感知模块存在有偏、重尾的非高斯噪声，需要更准确的感知不确定性量化来保证反馈控制的安全性。

Method: 使用约束zonotopes进行基于集合的状态估计来捕获有偏重尾不确定性；将鲁棒MPC重构为线性规划问题，采用Minkowski-Lyapunov成本函数和松弛变量；通过Minkowski-Lyapunov不等式和收缩zonotopic不变集确保闭环稳定性。

Result: 在全方位移动机器人上进行仿真和硬件实验验证，结果表明该感知感知MPC在重尾噪声条件下提供稳定准确的控制性能，在状态估计误差边界和整体控制性能方面显著优于传统高斯噪声设计。

Conclusion: 所提出的框架能够有效处理深度学习感知模块的非高斯噪声，通过准确的感知不确定性量化和鲁棒控制设计，实现了在复杂噪声环境下的安全稳定控制。

Abstract: This paper presents a robust model predictive control (MPC) framework that
explicitly addresses the non-Gaussian noise inherent in deep learning-based
perception modules used for state estimation. Recognizing that accurate
uncertainty quantification of the perception module is essential for safe
feedback control, our approach departs from the conventional assumption of
zero-mean noise quantification of the perception error. Instead, it employs
set-based state estimation with constrained zonotopes to capture biased,
heavy-tailed uncertainties while maintaining bounded estimation errors. To
improve computational efficiency, the robust MPC is reformulated as a linear
program (LP), using a Minkowski-Lyapunov-based cost function with an added
slack variable to prevent degenerate solutions. Closed-loop stability is
ensured through Minkowski-Lyapunov inequalities and contractive zonotopic
invariant sets. The largest stabilizing terminal set and its corresponding
feedback gain are then derived via an ellipsoidal approximation of the
zonotopes. The proposed framework is validated through both simulations and
hardware experiments on an omnidirectional mobile robot along with a camera and
a convolutional neural network-based perception module implemented within a
ROS2 framework. The results demonstrate that the perception-aware MPC provides
stable and accurate control performance under heavy-tailed noise conditions,
significantly outperforming traditional Gaussian-noise-based designs in terms
of both state estimation error bounding and overall control performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [72] [SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing](https://arxiv.org/abs/2509.04908)
*Hongyi Jing,Jiafu Chen,Chen Rao,Ziqiang Dang,Jiajie Teng,Tianyi Chu,Juncheng Mo,Shuo Fang,Huaizhong Lin,Rui Lv,Chenguang Ma,Lei Zhao*

Main category: cs.AI

TL;DR: SparkUI-Parser是一个新型端到端GUI解析框架，通过连续坐标建模和拒绝机制，显著提高了定位精度和解析能力，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有MLLM在GUI感知中的两个主要问题：1）基于文本自回归机制的离散坐标建模导致定位精度低和推理速度慢；2）只能定位预定义元素集合，无法解析整个界面，限制了广泛应用。

Method: 提出基于预训练MLLM的连续坐标建模方法，增加token路由器和坐标解码器；引入基于改进匈牙利匹配算法的拒绝机制来识别和拒绝不存在元素；构建ScreenParse基准测试系统评估GUI模型的结构感知能力。

Result: 在ScreenSpot、ScreenSpot-v2、CAGUI-Grounding和ScreenParse等多个基准测试中始终优于最先进方法，显著提高了准确性和推理速度。

Conclusion: SparkUI-Parser通过连续建模和拒绝机制有效解决了离散输出特性和逐token生成过程的限制，实现了更高的定位精度和细粒度界面解析能力，为下游任务提供了更好的支持。

Abstract: The existing Multimodal Large Language Models (MLLMs) for GUI perception have
made great progress. However, the following challenges still exist in prior
methods: 1) They model discrete coordinates based on text autoregressive
mechanism, which results in lower grounding accuracy and slower inference
speed. 2) They can only locate predefined sets of elements and are not capable
of parsing the entire interface, which hampers the broad application and
support for downstream tasks. To address the above issues, we propose
SparkUI-Parser, a novel end-to-end framework where higher localization
precision and fine-grained parsing capability of the entire interface are
simultaneously achieved. Specifically, instead of using probability-based
discrete modeling, we perform continuous modeling of coordinates based on a
pre-trained Multimodal Large Language Model (MLLM) with an additional token
router and coordinate decoder. This effectively mitigates the limitations
inherent in the discrete output characteristics and the token-by-token
generation process of MLLMs, consequently boosting both the accuracy and the
inference speed. To further enhance robustness, a rejection mechanism based on
a modified Hungarian matching algorithm is introduced, which empowers the model
to identify and reject non-existent elements, thereby reducing false positives.
Moreover, we present ScreenParse, a rigorously constructed benchmark to
systematically assess structural perception capabilities of GUI models across
diverse scenarios. Extensive experiments demonstrate that our approach
consistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2,
CAGUI-Grounding and ScreenParse benchmarks. The resources are available at
https://github.com/antgroup/SparkUI-Parser.

</details>


### [73] [LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation](https://arxiv.org/abs/2509.05263)
*Yinglin Duan,Zhengxia Zou,Tongwei Gu,Wei Jia,Zhan Zhao,Luyi Xu,Xinzhu Liu,Hao Jiang,Kang Chen,Shuang Qiu*

Main category: cs.AI

TL;DR: LatticeWorld是一个基于轻量级LLM和游戏引擎的3D世界生成框架，通过多模态输入创建动态交互式3D环境，大幅提升工业生产效率


<details>
  <summary>Details</summary>
Motivation: 传统手动建模方法效率低下，需要开发能够基于用户指令自动生成高质量3D虚拟世界的系统，以缩小仿真与现实之间的差距

Method: 使用LLaMA-2-7B轻量级语言模型结合Unreal Engine 5渲染引擎，接受文本描述和视觉指令作为多模态输入，生成包含动态代理的大规模3D交互世界

Result: 在场景布局生成和视觉保真度方面达到优异精度，相比传统手动生产方法实现90倍以上的工业生产效率提升，同时保持高创意质量

Conclusion: LatticeWorld提供了一个简单有效的3D世界生成框架，显著提升了3D环境的生产效率，为虚拟世界创建提供了新的解决方案

Abstract: Recent research has been increasingly focusing on developing 3D world models
that simulate complex real-world scenarios. World models have found broad
applications across various domains, including embodied AI, autonomous driving,
entertainment, etc. A more realistic simulation with accurate physics will
effectively narrow the sim-to-real gap and allow us to gather rich information
about the real world conveniently. While traditional manual modeling has
enabled the creation of virtual 3D scenes, modern approaches have leveraged
advanced machine learning algorithms for 3D world generation, with most recent
advances focusing on generative methods that can create virtual worlds based on
user instructions. This work explores such a research direction by proposing
LatticeWorld, a simple yet effective 3D world generation framework that
streamlines the industrial production pipeline of 3D environments. LatticeWorld
leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering
engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed
framework accepts textual descriptions and visual instructions as multimodal
inputs and creates large-scale 3D interactive worlds with dynamic agents,
featuring competitive multi-agent interaction, high-fidelity physics
simulation, and real-time rendering. We conduct comprehensive experiments to
evaluate LatticeWorld, showing that it achieves superior accuracy in scene
layout generation and visual fidelity. Moreover, LatticeWorld achieves over a
$90\times$ increase in industrial production efficiency while maintaining high
creative quality compared with traditional manual production methods. Our demo
video is available at https://youtu.be/8VWZXpERR18

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [74] [Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning](https://arxiv.org/abs/2509.04734)
*Jasmine Shone,Shaden Alshammari,Mark Hamilton,Zhening Li,William Freeman*

Main category: cs.LG

TL;DR: 提出了Beyond I-Con框架，通过探索不同的统计散度和相似性核来系统发现新的损失函数，在无监督聚类、监督对比学习和降维任务中均取得了优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: I-Con框架发现现有表示学习方法隐含地最小化数据分布与学习分布之间的KL散度，但KL散度的不对称性和无界性可能导致优化问题，需要探索其他统计散度来改进表示学习。

Method: 开发Beyond I-Con框架，系统地探索替代的统计散度（如总变差距离、有界f-散度）和相似性核（如基于距离的核），并在不同表示学习任务中进行实验验证。

Result: 在无监督聚类中修改PMI算法使用总变差距离达到SOTA；在监督对比学习中使用总变差距离和距离核优于标准方法；在降维中使用有界f-散度比SNE获得更好的下游任务性能。

Conclusion: 统计散度和相似性核的选择对表示学习优化至关重要，Beyond I-Con框架为系统探索这些选择提供了有效途径。

Abstract: The Information Contrastive (I-Con) framework revealed that over 23
representation learning methods implicitly minimize KL divergence between data
and learned distributions that encode similarities between data points.
However, a KL-based loss may be misaligned with the true objective, and
properties of KL divergence such as asymmetry and unboundedness may create
optimization challenges. We present Beyond I-Con, a framework that enables
systematic discovery of novel loss functions by exploring alternative
statistical divergences and similarity kernels. Key findings: (1) on
unsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-art
results by modifying the PMI algorithm to use total variation (TV) distance;
(2) on supervised contrastive learning, we outperform the standard approach by
using TV and a distance-based similarity kernel instead of KL and an angular
kernel; (3) on dimensionality reduction, we achieve superior qualitative
results and better performance on downstream tasks than SNE by replacing KL
with a bounded f-divergence. Our results highlight the importance of
considering divergence and similarity kernel choices in representation learning
optimization.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [75] [Ecologically Valid Benchmarking and Adaptive Attention: Scalable Marine Bioacoustic Monitoring](https://arxiv.org/abs/2509.04682)
*Nicholas R. Rasmussen,Rodrigue Rizk,Longwei Wang,KC Santosh*

Main category: cs.SD

TL;DR: 提出了GetNetUPAM框架和ARPA-N网络，通过分层嵌套交叉验证和自适应池化注意力机制，显著提升水下被动声学监测的模型稳定性和检测精度


<details>
  <summary>Details</summary>
Motivation: 水下被动声学监测存在固有噪声和复杂信号依赖性问题，现有方法在环境变化下稳定性不足，需要更鲁棒的架构和评估框架

Method: 开发GetNetUPAM分层嵌套交叉验证框架，按站点-年份分区数据；提出ARPA-N神经网络，采用自适应分辨率池化和空间注意力机制处理不规则频谱图

Result: ARPA-N在GetNetUPAM评估下比DenseNet基线平均精度提升14.4%，所有指标变异性降低数量级，实现跨站点-年份折叠的一致检测

Conclusion: 该框架和网络显著提升了水下生物声学监测的扩展性和准确性，为生态现实变异性下的模型稳定性提供了量化评估方法

Abstract: Underwater Passive Acoustic Monitoring (UPAM) provides rich spatiotemporal
data for long-term ecological analysis, but intrinsic noise and complex signal
dependencies hinder model stability and generalization. Multilayered windowing
has improved target sound localization, yet variability from shifting ambient
noise, diverse propagation effects, and mixed biological and anthropogenic
sources demands robust architectures and rigorous evaluation. We introduce
GetNetUPAM, a hierarchical nested cross-validation framework designed to
quantify model stability under ecologically realistic variability. Data are
partitioned into distinct site-year segments, preserving recording
heterogeneity and ensuring each validation fold reflects a unique environmental
subset, reducing overfitting to localized noise and sensor artifacts. Site-year
blocking enforces evaluation against genuine environmental diversity, while
standard cross-validation on random subsets measures generalization across
UPAM's full signal distribution, a dimension absent from current benchmarks.
Using GetNetUPAM as the evaluation backbone, we propose the Adaptive Resolution
Pooling and Attention Network (ARPA-N), a neural architecture for irregular
spectrogram dimensions. Adaptive pooling with spatial attention extends the
receptive field, capturing global context without excessive parameters. Under
GetNetUPAM, ARPA-N achieves a 14.4% gain in average precision over DenseNet
baselines and a log2-scale order-of-magnitude drop in variability across all
metrics, enabling consistent detection across site-year folds and advancing
scalable, accurate bioacoustic monitoring.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [76] [Histogram Driven Amplitude Embedding for Qubit Efficient Quantum Image Compression](https://arxiv.org/abs/2509.04849)
*Sahil Tomar,Sandeep Kumar*

Main category: quant-ph

TL;DR: 提出了一种使用近期量子设备压缩彩色图像的紧凑硬件高效方法，通过分块处理、全局直方图构建和振幅嵌入，实现了恒定量子比特需求的高质量图像重建。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统像素级编码在量子图像处理中资源消耗大的问题，开发一种适用于当前NISQ时代量子系统的实用图像压缩方法，在保持重建质量的同时显著提高量子比特效率。

Method: 将图像分割为固定大小的bixels块，计算每个块的总强度；构建具有B个bin的全局直方图；将归一化的bin计数平方根作为振幅编码到n量子比特量子态中；使用PennyLane进行振幅嵌入并在IBM量子硬件上执行；通过测量重建直方图来恢复块强度和重组完整图像。

Result: 实证结果显示使用仅5-7个量子比特即可获得高质量重建，显著优于传统像素级编码的量子比特效率，验证了该方法在当前NISQ时代量子系统中的实际应用价值。

Conclusion: 该方法通过调整直方图bin数量B，用户可以控制保真度与资源使用之间的权衡，实现了独立于图像分辨率的恒定量子比特需求，为量子图像处理提供了实用的硬件高效解决方案。

Abstract: This work introduces a compact and hardware efficient method for compressing
color images using near term quantum devices. The approach segments the image
into fixed size blocks called bixels, and computes the total intensity within
each block. A global histogram with B bins is then constructed from these block
intensities, and the normalized square roots of the bin counts are encoded as
amplitudes into an n qubit quantum state. Amplitude embedding is performed
using PennyLane and executed on real IBM Quantum hardware. The resulting state
is measured to reconstruct the histogram, enabling approximate recovery of
block intensities and full image reassembly. The method maintains a constant
qubit requirement based solely on the number of histogram bins, independent of
the resolution of the image. By adjusting B, users can control the trade off
between fidelity and resource usage. Empirical results demonstrate high quality
reconstructions using as few as 5 to 7 qubits, significantly outperforming
conventional pixel level encodings in terms of qubit efficiency and validating
the practical application of the method for current NISQ era quantum systems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [77] [STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous GPUs](https://arxiv.org/abs/2509.04719)
*Han Liang,Jiahui Zhou,Zicheng Zhou,Xiaoxi Zhang,Xu Chen*

Main category: cs.DC

TL;DR: STADI是一种针对异构多GPU环境的扩散模型推理加速框架，通过时空自适应调度机制，在时间维度减少较慢GPU的去噪步骤，在空间维度弹性分配图像块，实现负载均衡，相比现有方法可降低45%延迟并提高资源利用率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成等应用中计算成本高昂，现有并行推理方案在异构多GPU环境中资源利用率低，硬件差异或后台任务导致负载不均衡。

Method: 提出混合调度器，在时间维度使用计算感知步骤分配器和最小公倍数最小化量化技术减少较慢GPU的去噪步骤；在空间维度采用弹性块并行机制，根据GPU计算能力分配不同大小的图像块。

Result: 在负载不均衡和异构多GPU集群上的实验验证了STADI的有效性，相比最先进的块并行框架，端到端推理延迟最多降低45%，异构GPU资源利用率显著提升。

Conclusion: STADI框架通过时空自适应调度有效解决了异构环境中的负载均衡问题，显著加速了扩散模型推理并提高了资源利用效率。

Abstract: The escalating adoption of diffusion models for applications such as image
generation demands efficient parallel inference techniques to manage their
substantial computational cost. However, existing diffusion parallelism
inference schemes often underutilize resources in heterogeneous multi-GPU
environments, where varying hardware capabilities or background tasks cause
workload imbalance. This paper introduces Spatio-Temporal Adaptive Diffusion
Inference (STADI), a novel framework to accelerate diffusion model inference in
such settings. At its core is a hybrid scheduler that orchestrates fine-grained
parallelism across both temporal and spatial dimensions. Temporally, STADI
introduces a novel computation-aware step allocator applied after warmup
phases, using a least-common-multiple-minimizing quantization technique to
reduce denoising steps on slower GPUs and execution synchronization. To further
minimize GPU idle periods, STADI executes an elastic patch parallelism
mechanism that allocates variably sized image patches to GPUs according to
their computational capability, ensuring balanced workload distribution through
a complementary spatial mechanism. Extensive experiments on both
load-imbalanced and heterogeneous multi-GPU clusters validate STADI's efficacy,
demonstrating improved load balancing and mitigation of performance
bottlenecks. Compared to patch parallelism, a state-of-the-art diffusion
inference framework, our method significantly reduces end-to-end inference
latency by up to 45% and significantly improves resource utilization on
heterogeneous GPUs.

</details>
