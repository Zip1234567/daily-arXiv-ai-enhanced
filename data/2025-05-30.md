<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 141]
- [eess.IV](#eess.IV) [Total: 14]
- [cs.GR](#cs.GR) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [cs.LG](#cs.LG) [Total: 9]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CL](#cs.CL) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.RO](#cs.RO) [Total: 5]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Using Cross-Domain Detection Loss to Infer Multi-Scale Information for Improved Tiny Head Tracking](https://arxiv.org/abs/2505.22677)
*Jisu Kim,Alex Mattingly,Eung-Joo Lee,Benjamin S. Riggan*

Main category: cs.CV

TL;DR: 提出一种优化性能与效率平衡的框架，用于增强微小头部检测与跟踪，通过跨域检测损失、多尺度模块和小感受野检测机制实现。


<details>
  <summary>Details</summary>
Motivation: 当前方法计算成本高，延迟大，资源占用多，需优化性能与效率的平衡。

Method: 集成跨域检测损失、多尺度模块和小感受野检测机制。

Result: 在CroHD和CrowdHuman数据集上提升了MOTA和mAP。

Conclusion: 该框架在拥挤场景中有效提升了微小头部检测与跟踪的性能。

Abstract: Head detection and tracking are essential for downstream tasks, but current
methods often require large computational budgets, which increase latencies and
ties up resources (e.g., processors, memory, and bandwidth). To address this,
we propose a framework to enhance tiny head detection and tracking by
optimizing the balance between performance and efficiency. Our framework
integrates (1) a cross-domain detection loss, (2) a multi-scale module, and (3)
a small receptive field detection mechanism. These innovations enhance
detection by bridging the gap between large and small detectors, capturing
high-frequency details at multiple scales during training, and using filters
with small receptive fields to detect tiny heads. Evaluations on the CroHD and
CrowdHuman datasets show improved Multiple Object Tracking Accuracy (MOTA) and
mean Average Precision (mAP), demonstrating the effectiveness of our approach
in crowded scenes.

</details>


### [2] [Frequency-Adaptive Discrete Cosine-ViT-ResNet Architecture for Sparse-Data Vision](https://arxiv.org/abs/2505.22701)
*Ziyue Kang,Weichuan Zhang*

Main category: cs.CV

TL;DR: 提出了一种混合深度学习框架，结合自适应DCT预处理模块、ViT-B16和ResNet50主干网络以及贝叶斯线性分类头，用于解决稀有动物图像分类中的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 稀有动物图像分类面临数据稀缺的挑战，许多物种仅有少量标记样本。

Method: 设计了自适应DCT预处理模块，学习适合后续主干网络的最优频率边界；结合ViT-B16和ResNet50分别提取全局和局部特征；通过跨级融合策略整合特征，并使用贝叶斯线性分类器输出预测。

Result: 在自建的50类野生动物数据集上，该方法在样本稀缺情况下优于传统CNN和固定频带DCT方法，达到最先进准确率。

Conclusion: 该方法通过自适应频率选择和混合特征提取，有效提升了稀有动物图像分类的性能。

Abstract: A major challenge in rare animal image classification is the scarcity of
data, as many species usually have only a small number of labeled samples.
  To address this challenge, we designed a hybrid deep-learning framework
comprising a novel adaptive DCT preprocessing module, ViT-B16 and ResNet50
backbones, and a Bayesian linear classification head. To our knowledge, we are
the first to introduce an adaptive frequency-domain selection mechanism that
learns optimal low-, mid-, and high-frequency boundaries suited to the
subsequent backbones.
  Our network first captures image frequency-domain cues via this adaptive DCT
partitioning. The adaptively filtered frequency features are then fed into
ViT-B16 to model global contextual relationships, while ResNet50 concurrently
extracts local, multi-scale spatial representations from the original image. A
cross-level fusion strategy seamlessly integrates these frequency- and
spatial-domain embeddings, and the fused features are passed through a Bayesian
linear classifier to output the final category predictions. On our self-built
50-class wildlife dataset, this approach outperforms conventional CNN and
fixed-band DCT pipelines, achieving state-of-the-art accuracy under extreme
sample scarcity.

</details>


### [3] [HiDream-I1: A High-Efficient Image Generative Foundation Model with Sparse Diffusion Transformer](https://arxiv.org/abs/2505.22705)
*Qi Cai,Jingwen Chen,Yang Chen,Yehao Li,Fuchen Long,Yingwei Pan,Zhaofan Qiu,Yiheng Zhang,Fengbin Gao,Peihan Xu,Yimeng Wang,Kai Yu,Wenxuan Chen,Ziwei Feng,Zijian Gong,Jianzhuang Pan,Yi Peng,Rui Tian,Siyu Wang,Bo Zhao,Ting Yao,Tao Mei*

Main category: cs.CV

TL;DR: HiDream-I1是一个17B参数的开源图像生成基础模型，采用稀疏扩散变换器结构，实现了高质量图像生成与低延迟的平衡，并支持指令式图像编辑。


<details>
  <summary>Details</summary>
Motivation: 解决现有图像生成模型在质量和计算效率之间的权衡问题。

Method: 采用双流解耦设计和动态MoE架构的稀疏DiT结构，支持多模态交互。

Result: 实现了高质量的图像生成和指令式编辑，提供三种变体以适应不同需求。

Conclusion: HiDream-I1及其衍生产品形成了一个全面的图像代理，推动了多模态AIGC研究的发展。

Abstract: Recent advancements in image generative foundation models have prioritized
quality improvements but often at the cost of increased computational
complexity and inference latency. To address this critical trade-off, we
introduce HiDream-I1, a new open-source image generative foundation model with
17B parameters that achieves state-of-the-art image generation quality within
seconds. HiDream-I1 is constructed with a new sparse Diffusion Transformer
(DiT) structure. Specifically, it starts with a dual-stream decoupled design of
sparse DiT with dynamic Mixture-of-Experts (MoE) architecture, in which two
separate encoders are first involved to independently process image and text
tokens. Then, a single-stream sparse DiT structure with dynamic MoE
architecture is adopted to trigger multi-model interaction for image generation
in a cost-efficient manner. To support flexiable accessibility with varied
model capabilities, we provide HiDream-I1 in three variants: HiDream-I1-Full,
HiDream-I1-Dev, and HiDream-I1-Fast.
  Furthermore, we go beyond the typical text-to-image generation and remould
HiDream-I1 with additional image conditions to perform precise,
instruction-based editing on given images, yielding a new instruction-based
image editing model namely HiDream-E1. Ultimately, by integrating text-to-image
generation and instruction-based image editing, HiDream-I1 evolves to form a
comprehensive image agent (HiDream-A1) capable of fully interactive image
creation and refinement. To accelerate multi-modal AIGC research, we have
open-sourced all the codes and model weights of HiDream-I1-Full,
HiDream-I1-Dev, HiDream-I1-Fast, HiDream-E1 through our project websites:
https://github.com/HiDream-ai/HiDream-I1 and
https://github.com/HiDream-ai/HiDream-E1. All features can be directly
experienced via https://vivago.ai/studio.

</details>


### [4] [One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory](https://arxiv.org/abs/2505.23617)
*Chenhao Zheng,Jieyu Zhang,Mohammadreza Salehi,Ziqi Gao,Vishnu Iyengar,Norimasa Kobori,Quan Kong,Ranjay Krishna*

Main category: cs.CV

TL;DR: 论文提出了一种基于轨迹的视频标记化方法TrajViT，显著减少了冗余标记并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于时空块的方法标记视频时会产生过多标记且效率低下，而最佳标记减少策略在相机移动时效果不佳。

Method: 提出基于全景子对象轨迹的标记化方法TrajViT，通过对比学习训练，提取对象轨迹并转换为语义标记。

Result: TrajViT在多个视频理解任务中显著优于ViT3D，例如在视频-文本检索任务中平均提升6% top-5召回率，同时标记减少10倍。

Conclusion: TrajViT是首个在多样化视频分析任务中一致优于ViT3D的高效编码器，具有鲁棒性和可扩展性。

Abstract: Effective video tokenization is critical for scaling transformer models for
long videos. Current approaches tokenize videos using space-time patches,
leading to excessive tokens and computational inefficiencies. The best token
reduction strategies degrade performance and barely reduce the number of tokens
when the camera moves. We introduce grounded video tokenization, a paradigm
that organizes tokens based on panoptic sub-object trajectories rather than
fixed patches. Our method aligns with fundamental perceptual principles,
ensuring that tokenization reflects scene complexity rather than video
duration. We propose TrajViT, a video encoder that extracts object trajectories
and converts them into semantically meaningful tokens, significantly reducing
redundancy while maintaining temporal coherence. Trained with contrastive
learning, TrajViT significantly outperforms space-time ViT (ViT3D) across
multiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a
large margin of 6% top-5 recall in average at video-text retrieval task with
10x token deduction. We also show TrajViT as a stronger model than ViT3D for
being the video encoder for modern VideoLLM, obtaining an average of 5.2%
performance improvement across 6 VideoQA benchmarks while having 4x faster
training time and 18x less inference FLOPs. TrajViT is the first efficient
encoder to consistently outperform ViT3D across diverse video analysis tasks,
making it a robust and scalable solution.

</details>


### [5] [MIAS-SAM: Medical Image Anomaly Segmentation without thresholding](https://arxiv.org/abs/2505.22762)
*Marco Colussi,Dragan Ahmetovic,Sergio Mascetti*

Main category: cs.CV

TL;DR: MIAS-SAM是一种用于医学图像异常区域分割的新方法，通过基于补丁的记忆库和SAM编码器提取特征，无需阈值即可生成精确分割。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像异常分割中需要手动设定阈值的问题，提高分割的准确性和自动化程度。

Method: 使用SAM编码器提取正常数据的特征并存储于记忆库，推理时通过比较特征生成异常图，利用异常图的重心提示SAM解码器完成分割。

Result: 在三个公开数据集（脑MRI、肝脏CT和视网膜OCT）上表现出高精度的异常分割能力，DICE分数验证了其有效性。

Conclusion: MIAS-SAM无需手动设定阈值即可实现精确的异常分割，具有广泛的应用潜力。

Abstract: This paper presents MIAS-SAM, a novel approach for the segmentation of
anomalous regions in medical images. MIAS-SAM uses a patch-based memory bank to
store relevant image features, which are extracted from normal data using the
SAM encoder. At inference time, the embedding patches extracted from the SAM
encoder are compared with those in the memory bank to obtain the anomaly map.
Finally, MIAS-SAM computes the center of gravity of the anomaly map to prompt
the SAM decoder, obtaining an accurate segmentation from the previously
extracted features. Differently from prior works, MIAS-SAM does not require to
define a threshold value to obtain the segmentation from the anomaly map.
Experimental results conducted on three publicly available datasets, each with
a different imaging modality (Brain MRI, Liver CT, and Retina OCT) show
accurate anomaly segmentation capabilities measured using DICE score. The code
is available at: https://github.com/warpcut/MIAS-SAM

</details>


### [6] [Rhetorical Text-to-Image Generation via Two-layer Diffusion Policy Optimization](https://arxiv.org/abs/2505.22792)
*Yuxi Zhang,Yueting Li,Xinyu Du,Sibo Wang*

Main category: cs.CV

TL;DR: Rhet2Pix是一个解决修辞语言生成图像问题的框架，通过多步策略优化和双层MDP扩散模块，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型无法捕捉修辞语言的隐含意义，导致生成图像偏向字面表达。

Method: 提出Rhet2Pix框架，采用多步策略优化和双层MDP扩散模块，逐步细化子句并优化图像生成动作。

Result: Rhet2Pix在定性和定量评估中均优于GPT-4o、Grok-3等SOTA模型。

Conclusion: Rhet2Pix有效解决了修辞语言生成图像的挑战，代码和数据集已公开。

Abstract: Generating images from rhetorical languages remains a critical challenge for
text-to-image models. Even state-of-the-art (SOTA) multimodal large language
models (MLLM) fail to generate images based on the hidden meaning inherent in
rhetorical language--despite such content being readily mappable to visual
representations by humans. A key limitation is that current models emphasize
object-level word embedding alignment, causing metaphorical expressions to
steer image generation towards their literal visuals and overlook the intended
semantic meaning. To address this, we propose Rhet2Pix, a framework that
formulates rhetorical text-to-image generation as a multi-step policy
optimization problem, incorporating a two-layer MDP diffusion module. In the
outer layer, Rhet2Pix converts the input prompt into incrementally elaborated
sub-sentences and executes corresponding image-generation actions, constructing
semantically richer visuals. In the inner layer, Rhet2Pix mitigates reward
sparsity during image generation by discounting the final reward and optimizing
every adjacent action pair along the diffusion denoising trajectory. Extensive
experiments demonstrate the effectiveness of Rhet2Pix in rhetorical
text-to-image generation. Our model outperforms SOTA MLLMs such as GPT-4o,
Grok-3 and leading academic baselines across both qualitative and quantitative
evaluations. The code and dataset used in this work are publicly available.

</details>


### [7] [GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion](https://arxiv.org/abs/2505.23085)
*Gwanghyun Kim,Xueting Li,Ye Yuan,Koki Nagano,Tianye Li,Jan Kautz,Se Young Chun,Umar Iqbal*

Main category: cs.CV

TL;DR: GeoMan是一种新型架构，用于从单目视频中生成准确且时间一致的3D人体几何估计，解决了现有方法在时间一致性和细节捕捉上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对单图像优化，常出现时间不一致和动态细节捕捉不足的问题。

Method: GeoMan结合图像模型和视频扩散模型，首帧由图像模型估计深度和法线，后续帧由视频模型生成，并采用根相对深度表示以保留人体尺度细节。

Result: GeoMan在定性和定量评估中均达到最先进水平，显著提升了时间一致性和泛化能力。

Conclusion: GeoMan通过创新架构和数据表示，有效解决了3D人体几何估计中的关键挑战。

Abstract: Estimating accurate and temporally consistent 3D human geometry from videos
is a challenging problem in computer vision. Existing methods, primarily
optimized for single images, often suffer from temporal inconsistencies and
fail to capture fine-grained dynamic details. To address these limitations, we
present GeoMan, a novel architecture designed to produce accurate and
temporally consistent depth and normal estimations from monocular human videos.
GeoMan addresses two key challenges: the scarcity of high-quality 4D training
data and the need for metric depth estimation to accurately model human size.
To overcome the first challenge, GeoMan employs an image-based model to
estimate depth and normals for the first frame of a video, which then
conditions a video diffusion model, reframing video geometry estimation task as
an image-to-video generation problem. This design offloads the heavy lifting of
geometric estimation to the image model and simplifies the video model's role
to focus on intricate details while using priors learned from large-scale video
datasets. Consequently, GeoMan improves temporal consistency and
generalizability while requiring minimal 4D training data. To address the
challenge of accurate human size estimation, we introduce a root-relative depth
representation that retains critical human-scale details and is easier to be
estimated from monocular inputs, overcoming the limitations of traditional
affine-invariant and metric depth representations. GeoMan achieves
state-of-the-art performance in both qualitative and quantitative evaluations,
demonstrating its effectiveness in overcoming longstanding challenges in 3D
human geometry estimation from videos.

</details>


### [8] [Cultural Evaluations of Vision-Language Models Have a Lot to Learn from Cultural Theory](https://arxiv.org/abs/2505.22793)
*Srishti Yadav,Lauren Tilton,Maria Antoniak,Taylor Arnold,Jiaang Li,Siddhesh Milind Pawar,Antonia Karamolegkou,Stella Frank,Zhaochong An,Negar Rostamzadeh,Daniel Hershcovich,Serge Belongie,Ekaterina Shutova*

Main category: cs.CV

TL;DR: 本文提出视觉文化研究方法对视觉语言模型（VLMs）的文化能力分析至关重要，并提出了五个文化维度框架。


<details>
  <summary>Details</summary>
Motivation: 现代视觉语言模型在文化能力评估中表现不佳，需要系统性框架分析图像中的文化细微差别。

Method: 基于视觉文化研究（文化研究、符号学、视觉研究）的方法论，提出五个文化维度框架。

Result: 提出了一个更全面的文化能力分析框架，以改进VLMs的文化理解。

Conclusion: 视觉文化研究方法对VLMs的文化分析不可或缺，未来研究应进一步验证这些框架的有效性。

Abstract: Modern vision-language models (VLMs) often fail at cultural competency
evaluations and benchmarks. Given the diversity of applications built upon
VLMs, there is renewed interest in understanding how they encode cultural
nuances. While individual aspects of this problem have been studied, we still
lack a comprehensive framework for systematically identifying and annotating
the nuanced cultural dimensions present in images for VLMs. This position paper
argues that foundational methodologies from visual culture studies (cultural
studies, semiotics, and visual studies) are necessary for cultural analysis of
images. Building upon this review, we propose a set of five frameworks,
corresponding to cultural dimensions, that must be considered for a more
complete analysis of the cultural competencies of VLMs.

</details>


### [9] [How Animals Dance (When You're Not Looking)](https://arxiv.org/abs/2505.23738)
*Xiaojuan Wang,Aleksander Holynski,Brian Curless,Ira Kemelmacher,Steve Seitz*

Main category: cs.CV

TL;DR: 提出了一种基于关键帧的框架，用于生成音乐同步、舞蹈感知的动物舞蹈视频，通过优化关键帧结构和视频扩散模型实现。


<details>
  <summary>Details</summary>
Motivation: 解决从少量关键帧生成高质量、音乐同步的动物舞蹈视频的挑战。

Method: 将舞蹈合成建模为图优化问题，结合文本到图像提示或GPT-4o生成关键帧，使用视频扩散模型生成中间帧。

Result: 仅需6个输入关键帧即可生成长达30秒的舞蹈视频，适用于多种动物和音乐。

Conclusion: 该方法在生成舞蹈视频方面具有高效性和广泛适用性。

Abstract: We present a keyframe-based framework for generating music-synchronized,
choreography aware animal dance videos. Starting from a few keyframes
representing distinct animal poses -- generated via text-to-image prompting or
GPT-4o -- we formulate dance synthesis as a graph optimization problem: find
the optimal keyframe structure that satisfies a specified choreography pattern
of beats, which can be automatically estimated from a reference dance video. We
also introduce an approach for mirrored pose image generation, essential for
capturing symmetry in dance. In-between frames are synthesized using an video
diffusion model. With as few as six input keyframes, our method can produce up
to 30 second dance videos across a wide range of animals and music tracks.

</details>


### [10] [Fast Trajectory-Independent Model-Based Reconstruction Algorithm for Multi-Dimensional Magnetic Particle Imaging](https://arxiv.org/abs/2505.22797)
*Vladyslav Gapyak,Thomas März,Andreas Weinmann*

Main category: cs.CV

TL;DR: 提出了一种轨迹无关的模型重建算法，用于2D磁粒子成像（MPI），并结合Plug-and-Play（PnP）算法处理反卷积问题，展示了在不同扫描场景下的强大重建能力。


<details>
  <summary>Details</summary>
Motivation: 传统MPI重建方法依赖耗时校准或特定轨迹的模型模拟，限制了其灵活性和通用性。

Method: 开发了轨迹无关的模型重建算法，并采用零-shot PnP算法，利用自然图像训练的降噪器处理反卷积问题。

Result: 在公开数据集和自定义数据上验证了算法的有效性，展示了跨场景的强重建能力。

Conclusion: 该研究为通用、灵活的模型MPI重建奠定了基础。

Abstract: Magnetic Particle Imaging (MPI) is a promising tomographic technique for
visualizing the spatio-temporal distribution of superparamagnetic
nanoparticles, with applications ranging from cancer detection to real-time
cardiovascular monitoring. Traditional MPI reconstruction relies on either
time-consuming calibration (measured system matrix) or model-based simulation
of the forward operator. Recent developments have shown the applicability of
Chebyshev polynomials to multi-dimensional Lissajous Field-Free Point (FFP)
scans. This method is bound to the particular choice of sinusoidal scanning
trajectories. In this paper, we present the first reconstruction on real 2D MPI
data with a trajectory-independent model-based MPI reconstruction algorithm. We
further develop the zero-shot Plug-and-Play (PnP) algorithm of the authors --
with automatic noise level estimation -- to address the present deconvolution
problem, leveraging a state-of-the-art denoiser trained on natural images
without retraining on MPI-specific data. We evaluate our method on the publicly
available 2D FFP MPI dataset ``MPIdata: Equilibrium Model with Anisotropy",
featuring scans of six phantoms acquired using a Bruker preclinical scanner.
Moreover, we show reconstruction performed on custom data on a 2D scanner with
additional high-frequency excitation field and partial data. Our results
demonstrate strong reconstruction capabilities across different scanning
scenarios -- setting a precedent for general-purpose, flexible model-based MPI
reconstruction.

</details>


### [11] [LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization](https://arxiv.org/abs/2505.23740)
*Ronghuan Wu,Wanchao Su,Jing Liao*

Main category: cs.CV

TL;DR: LayerPeeler是一种新颖的分层图像矢量化方法，通过渐进式简化策略解决现有工具在遮挡区域处理上的不足，生成完整路径和连贯层结构的矢量图形。


<details>
  <summary>Details</summary>
Motivation: 现有图像矢量化工具在处理遮挡区域时效果不佳，导致生成的矢量图形不完整或碎片化，限制了编辑灵活性。

Method: LayerPeeler采用自回归剥离策略，结合视觉语言模型构建层图以捕获遮挡关系，并通过微调图像扩散模型移除非遮挡层。

Result: 实验表明，LayerPeeler在路径语义、几何规则性和视觉保真度上显著优于现有技术。

Conclusion: LayerPeeler通过创新方法提升了图像矢量化的质量和灵活性，为相关领域提供了有效工具。

Abstract: Image vectorization is a powerful technique that converts raster images into
vector graphics, enabling enhanced flexibility and interactivity. However,
popular image vectorization tools struggle with occluded regions, producing
incomplete or fragmented shapes that hinder editability. While recent
advancements have explored rule-based and data-driven layer-wise image
vectorization, these methods face limitations in vectorization quality and
flexibility. In this paper, we introduce LayerPeeler, a novel layer-wise image
vectorization approach that addresses these challenges through a progressive
simplification paradigm. The key to LayerPeeler's success lies in its
autoregressive peeling strategy: by identifying and removing the topmost
non-occluded layers while recovering underlying content, we generate vector
graphics with complete paths and coherent layer structures. Our method
leverages vision-language models to construct a layer graph that captures
occlusion relationships among elements, enabling precise detection and
description for non-occluded layers. These descriptive captions are used as
editing instructions for a finetuned image diffusion model to remove the
identified layers. To ensure accurate removal, we employ localized attention
control that precisely guides the model to target regions while faithfully
preserving the surrounding content. To support this, we contribute a
large-scale dataset specifically designed for layer peeling tasks. Extensive
quantitative and qualitative experiments demonstrate that LayerPeeler
significantly outperforms existing techniques, producing vectorization results
with superior path semantics, geometric regularity, and visual fidelity.

</details>


### [12] [Weakly-supervised Localization of Manipulated Image Regions Using Multi-resolution Learned Features](https://arxiv.org/abs/2505.23586)
*Ziyong Wang,Charith Abhayaratne*

Main category: cs.CV

TL;DR: 提出了一种弱监督方法，结合图像级检测网络和预训练分割模型，无需像素级标注即可定位图像篡改区域。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习方法在图像篡改检测中缺乏可解释性和定位能力的问题，同时克服真实场景中像素级标注不足的限制。

Method: 基于WCBnet生成多视角特征图，结合预训练分割模型（如DeepLab、SegmentAnything等）进行粗定位，并通过贝叶斯推理细化定位结果。

Result: 实验证明该方法有效，无需像素级标签即可实现篡改区域的定位。

Conclusion: 该方法为弱监督条件下的图像篡改定位提供了可行方案。

Abstract: The explosive growth of digital images and the widespread availability of
image editing tools have made image manipulation detection an increasingly
critical challenge. Current deep learning-based manipulation detection methods
excel in achieving high image-level classification accuracy, they often fall
short in terms of interpretability and localization of manipulated regions.
Additionally, the absence of pixel-wise annotations in real-world scenarios
limits the existing fully-supervised manipulation localization techniques. To
address these challenges, we propose a novel weakly-supervised approach that
integrates activation maps generated by image-level manipulation detection
networks with segmentation maps from pre-trained models. Specifically, we build
on our previous image-level work named WCBnet to produce multi-view feature
maps which are subsequently fused for coarse localization. These coarse maps
are then refined using detailed segmented regional information provided by
pre-trained segmentation models (such as DeepLab, SegmentAnything and PSPnet),
with Bayesian inference employed to enhance the manipulation localization.
Experimental results demonstrate the effectiveness of our approach,
highlighting the feasibility to localize image manipulations without relying on
pixel-level labels.

</details>


### [13] [VidText: Towards Comprehensive Evaluation for Video Text Understanding](https://arxiv.org/abs/2505.22810)
*Zhoufaran Yang,Yan Shu,Zhifei Yang,Yan Zhang,Yu Li,Keyang Lu,Gangyan Zeng,Shaohui Liu,Yu Zhou,Nicu Sebe*

Main category: cs.CV

TL;DR: VidText是一个新的视频文本理解基准，填补了现有视频理解和OCR基准的不足，支持多语言和多层次任务评估。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解基准忽视文本信息，OCR基准局限于静态图像，无法捕捉动态视觉与文本的交互。

Method: 提出VidText基准，涵盖多场景、多语言内容，并引入分层评估框架和感知推理任务。

Result: 实验显示现有大型多模态模型在多数任务上表现不佳，需改进输入分辨率和OCR能力等因素。

Conclusion: VidText填补了视频理解基准的空白，为未来动态环境中的多模态推理研究奠定基础。

Abstract: Visual texts embedded in videos carry rich semantic information, which is
crucial for both holistic video understanding and fine-grained reasoning about
local human actions. However, existing video understanding benchmarks largely
overlook textual information, while OCR-specific benchmarks are constrained to
static images, limiting their ability to capture the interaction between text
and dynamic visual contexts. To address this gap, we propose VidText, a new
benchmark designed for comprehensive and in-depth evaluation of video text
understanding. VidText offers the following key features: 1) It covers a wide
range of real-world scenarios and supports multilingual content, encompassing
diverse settings where video text naturally appears. 2) It introduces a
hierarchical evaluation framework with video-level, clip-level, and
instance-level tasks, enabling assessment of both global summarization and
local retrieval capabilities. 3) The benchmark also introduces a set of paired
perception reasoning tasks, ranging from visual text perception to cross-modal
reasoning between textual and visual information. Extensive experiments on 18
state-of-the-art Large Multimodal Models (LMMs) reveal that current models
struggle across most tasks, with significant room for improvement. Further
analysis highlights the impact of both model-intrinsic factors, such as input
resolution and OCR capability, and external factors, including the use of
auxiliary information and Chain-of-Thought reasoning strategies. We hope
VidText will fill the current gap in video understanding benchmarks and serve
as a foundation for future research on multimodal reasoning with video text in
dynamic environments.

</details>


### [14] [IMTS is Worth Time $\times$ Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction](https://arxiv.org/abs/2505.22815)
*Zhangyi Hu,Jiemin Wu,Hua Xu,Mingqian Liao,Ninghui Feng,Bo Gao,Songning Lai,Yutao Yue*

Main category: cs.CV

TL;DR: VIMTS是一个基于视觉MAE的框架，用于处理不规则多变量时间序列（IMTS）预测，通过补全缺失数据和利用跨通道依赖关系提升预测性能。


<details>
  <summary>Details</summary>
Motivation: IMTS预测因多通道信号未对齐和大量缺失数据而具有挑战性，现有方法难以捕捉可靠的时序模式。视觉MAE在稀疏多通道数据处理中的成功启发了VIMTS的提出。

Method: VIMTS将IMTS沿时间线分割为等间隔特征块，利用跨通道依赖关系补全缺失数据，并采用视觉MAE进行块重建，最后通过粗到细技术生成精确预测。

Result: 实验表明VIMTS在IMTS预测中表现优异，并具备少样本学习能力。

Conclusion: VIMTS扩展了视觉基础模型在时间序列任务中的应用，为IMTS预测提供了有效解决方案。

Abstract: Irregular Multivariate Time Series (IMTS) forecasting is challenging due to
the unaligned nature of multi-channel signals and the prevalence of extensive
missing data. Existing methods struggle to capture reliable temporal patterns
from such data due to significant missing values. While pre-trained foundation
models show potential for addressing these challenges, they are typically
designed for Regularly Sampled Time Series (RTS). Motivated by the visual Mask
AutoEncoder's (MAE) powerful capability for modeling sparse multi-channel
information and its success in RTS forecasting, we propose VIMTS, a framework
adapting Visual MAE for IMTS forecasting. To mitigate the effect of missing
values, VIMTS first processes IMTS along the timeline into feature patches at
equal intervals. These patches are then complemented using learned
cross-channel dependencies. Then it leverages visual MAE's capability in
handling sparse multichannel data for patch reconstruction, followed by a
coarse-to-fine technique to generate precise predictions from focused contexts.
In addition, we integrate self-supervised learning for improved IMTS modeling
by adapting the visual MAE to IMTS data. Extensive experiments demonstrate
VIMTS's superior performance and few-shot capability, advancing the application
of visual foundation models in more general time series tasks. Our code is
available at https://github.com/WHU-HZY/VIMTS.

</details>


### [15] [Improving Contrastive Learning for Referring Expression Counting](https://arxiv.org/abs/2505.22850)
*Kostas Triaridis,Panagiotis Kaliosis,E-Ro Nguyen,Jingyi Xu,Hieu Le,Dimitris Samaras*

Main category: cs.CV

TL;DR: 论文提出C-REX，一种基于对比学习的框架，用于解决Referring Expression Counting（REC）任务，通过增强判别性表示学习，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以区分视觉相似但属于不同指代表达的对象，因此需要一种新的方法来提升细粒度属性与上下文差异的区分能力。

Method: 提出C-REX框架，基于监督对比学习，完全在图像空间内操作，避免图像-文本对比学习的对齐问题，并提供更大的负样本池。

Result: C-REX在REC任务中表现优异，MAE和RMSE分别提升22%和10%以上，同时在类无关计数任务中也表现良好。

Conclusion: C-REX通过对比学习显著提升了REC任务的性能，并展示了其在其他类似任务中的通用性。

Abstract: Object counting has progressed from class-specific models, which count only
known categories, to class-agnostic models that generalize to unseen
categories. The next challenge is Referring Expression Counting (REC), where
the goal is to count objects based on fine-grained attributes and contextual
differences. Existing methods struggle with distinguishing visually similar
objects that belong to the same category but correspond to different referring
expressions. To address this, we propose C-REX, a novel contrastive learning
framework, based on supervised contrastive learning, designed to enhance
discriminative representation learning. Unlike prior works, C-REX operates
entirely within the image space, avoiding the misalignment issues of image-text
contrastive learning, thus providing a more stable contrastive signal. It also
guarantees a significantly larger pool of negative samples, leading to improved
robustness in the learned representations. Moreover, we showcase that our
framework is versatile and generic enough to be applied to other similar tasks
like class-agnostic counting. To support our approach, we analyze the key
components of sota detection-based models and identify that detecting object
centroids instead of bounding boxes is the key common factor behind their
success in counting tasks. We use this insight to design a simple yet effective
detection-based baseline to build upon. Our experiments show that C-REX
achieves state-of-the-art results in REC, outperforming previous methods by
more than 22\% in MAE and more than 10\% in RMSE, while also demonstrating
strong performance in class-agnostic counting. Code is available at
https://github.com/cvlab-stonybrook/c-rex.

</details>


### [16] [CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian Splatting](https://arxiv.org/abs/2505.22854)
*Kornel Howil,Joanna Waczyńska,Piotr Borycki,Tadeusz Dziarmaga,Marcin Mazur,Przemysław Spurek*

Main category: cs.CV

TL;DR: CLIPGaussians是一个统一的多模态风格迁移框架，支持文本和图像引导的风格化，适用于2D图像、视频、3D对象和4D场景。


<details>
  <summary>Details</summary>
Motivation: 尽管高斯泼溅（GS）在渲染3D场景方面表现高效，但对其实现复杂风格迁移仍具挑战性。

Method: 直接在GS基元上操作，无需大型生成模型或从头训练，通过联合优化颜色和几何实现风格化。

Result: 在3D和4D场景中实现风格保真和一致性，视频中保持时间连贯性。

Conclusion: CLIPGaussians是一种通用且高效的多模态风格迁移解决方案。

Abstract: Gaussian Splatting (GS) has recently emerged as an efficient representation
for rendering 3D scenes from 2D images and has been extended to images, videos,
and dynamic 4D content. However, applying style transfer to GS-based
representations, especially beyond simple color changes, remains challenging.
In this work, we introduce CLIPGaussians, the first unified style transfer
framework that supports text- and image-guided stylization across multiple
modalities: 2D images, videos, 3D objects, and 4D scenes. Our method operates
directly on Gaussian primitives and integrates into existing GS pipelines as a
plug-in module, without requiring large generative models or retraining from
scratch. CLIPGaussians approach enables joint optimization of color and
geometry in 3D and 4D settings, and achieves temporal coherence in videos,
while preserving a model size. We demonstrate superior style fidelity and
consistency across all tasks, validating CLIPGaussians as a universal and
efficient solution for multimodal style transfer.

</details>


### [17] [A Probabilistic Jump-Diffusion Framework for Open-World Egocentric Activity Recognition](https://arxiv.org/abs/2505.22858)
*Sanjoy Kundu,Shanmukha Vellamcheti,Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: ProbRes是一种基于跳扩散的概率残差搜索框架，用于开放世界的自我中心活动识别，通过平衡先验引导的探索和似然驱动的利用，高效导航搜索空间。


<details>
  <summary>Details</summary>
Motivation: 开放世界的自我中心活动识别具有挑战性，因为其无约束性质要求模型从部分观察的广阔搜索空间中推断未见活动。

Method: ProbRes整合了结构化常识先验构建语义连贯的搜索空间，利用视觉语言模型自适应细化预测，并通过随机搜索机制高效定位高似然活动标签。

Result: 在多个开放级别（L0-L3）上系统评估，ProbRes在多个基准数据集上达到最先进性能，并建立了开放世界识别的清晰分类。

Conclusion: ProbRes为自我中心活动理解提供了方法论进步，并明确了开放世界识别的挑战和需求。

Abstract: Open-world egocentric activity recognition poses a fundamental challenge due
to its unconstrained nature, requiring models to infer unseen activities from
an expansive, partially observed search space. We introduce ProbRes, a
Probabilistic Residual search framework based on jump-diffusion that
efficiently navigates this space by balancing prior-guided exploration with
likelihood-driven exploitation. Our approach integrates structured commonsense
priors to construct a semantically coherent search space, adaptively refines
predictions using Vision-Language Models (VLMs) and employs a stochastic search
mechanism to locate high-likelihood activity labels while minimizing exhaustive
enumeration efficiently. We systematically evaluate ProbRes across multiple
openness levels (L0--L3), demonstrating its adaptability to increasing search
space complexity. In addition to achieving state-of-the-art performance on
benchmark datasets (GTEA Gaze, GTEA Gaze+, EPIC-Kitchens, and Charades-Ego), we
establish a clear taxonomy for open-world recognition, delineating the
challenges and methodological advancements necessary for egocentric activity
understanding.

</details>


### [18] [4DTAM: Non-Rigid Tracking and Mapping via Dynamic Surface Gaussians](https://arxiv.org/abs/2505.22859)
*Hidenobu Matsuki,Gwangbin Bae,Andrew J. Davison*

Main category: cs.CV

TL;DR: 提出首个基于可微分渲染的4D跟踪与建图方法，联合优化相机定位与非刚性表面重建，并引入合成数据集以解决评估难题。


<details>
  <summary>Details</summary>
Motivation: 自然环境中复杂的非刚性运动使得4D-SLAM研究较少，且缺乏可靠的真实数据和评估方法。

Method: 使用高斯表面基元进行SLAM，结合MLP表示的变形场和新型相机位姿估计技术，优化场景几何、外观和动态。

Result: 实现了准确的表面重建和非刚性变形建模，并提供了开源合成数据集。

Conclusion: 通过新方法和评估协议推动了现代4D-SLAM研究。

Abstract: We propose the first 4D tracking and mapping method that jointly performs
camera localization and non-rigid surface reconstruction via differentiable
rendering. Our approach captures 4D scenes from an online stream of color
images with depth measurements or predictions by jointly optimizing scene
geometry, appearance, dynamics, and camera ego-motion. Although natural
environments exhibit complex non-rigid motions, 4D-SLAM remains relatively
underexplored due to its inherent challenges; even with 2.5D signals, the
problem is ill-posed because of the high dimensionality of the optimization
space. To overcome these challenges, we first introduce a SLAM method based on
Gaussian surface primitives that leverages depth signals more effectively than
3D Gaussians, thereby achieving accurate surface reconstruction. To further
model non-rigid deformations, we employ a warp-field represented by a
multi-layer perceptron (MLP) and introduce a novel camera pose estimation
technique along with surface regularization terms that facilitate
spatio-temporal reconstruction. In addition to these algorithmic challenges, a
significant hurdle in 4D SLAM research is the lack of reliable ground truth and
evaluation protocols, primarily due to the difficulty of 4D capture using
commodity sensors. To address this, we present a novel open synthetic dataset
of everyday objects with diverse motions, leveraging large-scale object models
and animation modeling. In summary, we open up the modern 4D-SLAM research by
introducing a novel method and evaluation protocols grounded in modern vision
and rendering techniques.

</details>


### [19] [CFP-Gen: Combinatorial Functional Protein Generation via Diffusion Language Models](https://arxiv.org/abs/2505.22869)
*Junbo Yin,Chao Zha,Wenjia He,Chencheng Xu,Xin Gao*

Main category: cs.CV

TL;DR: CFP-Gen是一种新型扩散语言模型，用于组合功能蛋白质生成，通过整合多模态条件（功能、序列和结构约束）实现蛋白质设计。


<details>
  <summary>Details</summary>
Motivation: 现有PLM仅基于单一条件约束生成蛋白质序列，难以同时满足多模态的多种约束。

Method: 引入AGFM模块动态调整蛋白质特征分布，RCFE模块捕获残基间相互作用，并集成3D结构编码器施加几何约束。

Result: CFP-Gen能够高通量生成功能与天然蛋白质相当的新型蛋白质，并在设计多功能蛋白质时具有高成功率。

Conclusion: CFP-Gen为蛋白质设计提供了一种高效的多约束集成方法。

Abstract: Existing PLMs generate protein sequences based on a single-condition
constraint from a specific modality, struggling to simultaneously satisfy
multiple constraints across different modalities. In this work, we introduce
CFP-Gen, a novel diffusion language model for Combinatorial Functional Protein
GENeration. CFP-Gen facilitates the de novo protein design by integrating
multimodal conditions with functional, sequence, and structural constraints.
Specifically, an Annotation-Guided Feature Modulation (AGFM) module is
introduced to dynamically adjust the protein feature distribution based on
composable functional annotations, e.g., GO terms, IPR domains and EC numbers.
Meanwhile, the Residue-Controlled Functional Encoding (RCFE) module captures
residue-wise interaction to ensure more precise control. Additionally,
off-the-shelf 3D structure encoders can be seamlessly integrated to impose
geometric constraints. We demonstrate that CFP-Gen enables high-throughput
generation of novel proteins with functionality comparable to natural proteins,
while achieving a high success rate in designing multifunctional proteins. Code
and data available at https://github.com/yinjunbo/cfpgen.

</details>


### [20] [3DGS Compression with Sparsity-guided Hierarchical Transform Coding](https://arxiv.org/abs/2505.22908)
*Hao Xu,Xiaolin Wu,Xi Zhang*

Main category: cs.CV

TL;DR: SHTC是一种端到端优化的变换编码框架，用于3D高斯泼溅（3DGS）压缩，通过联合优化3DGS、变换和轻量级上下文模型，显著提升了率失真性能。


<details>
  <summary>Details</summary>
Motivation: 3DGS因其快速高质量渲染而流行，但内存占用大，传输和存储开销高。现有神经压缩方法未采用端到端优化的分析-合成变换，导致性能不佳。

Method: SHTC框架包含基频层（KLT用于数据去相关）和稀疏编码增强层（压缩KLT残差）。增强编码器学习线性变换，解码器使用ISTA重建残差。

Result: SHTC显著提升了率失真性能，同时参数和计算开销最小。

Conclusion: SHTC是首个端到端优化的3DGS压缩框架，通过可解释设计实现了高效压缩。

Abstract: 3D Gaussian Splatting (3DGS) has gained popularity for its fast and
high-quality rendering, but it has a very large memory footprint incurring high
transmission and storage overhead. Recently, some neural compression methods,
such as Scaffold-GS, were proposed for 3DGS but they did not adopt the approach
of end-to-end optimized analysis-synthesis transforms which has been proven
highly effective in neural signal compression. Without an appropriate analysis
transform, signal correlations cannot be removed by sparse representation.
Without such transforms the only way to remove signal redundancies is through
entropy coding driven by a complex and expensive context modeling, which
results in slower speed and suboptimal rate-distortion (R-D) performance. To
overcome this weakness, we propose Sparsity-guided Hierarchical Transform
Coding (SHTC), the first end-to-end optimized transform coding framework for
3DGS compression. SHTC jointly optimizes the 3DGS, transforms and a lightweight
context model. This joint optimization enables the transform to produce
representations that approach the best R-D performance possible. The SHTC
framework consists of a base layer using KLT for data decorrelation, and a
sparsity-coded enhancement layer that compresses the KLT residuals to refine
the representation. The enhancement encoder learns a linear transform to
project high-dimensional inputs into a low-dimensional space, while the decoder
unfolds the Iterative Shrinkage-Thresholding Algorithm (ISTA) to reconstruct
the residuals. All components are designed to be interpretable, allowing the
incorporation of signal priors and fewer parameters than black-box transforms.
This novel design significantly improves R-D performance with minimal
additional parameters and computational overhead.

</details>


### [21] [Hierarchical Material Recognition from Local Appearance](https://arxiv.org/abs/2505.22911)
*Matthew Beveridge,Shree K. Nayar*

Main category: cs.CV

TL;DR: 论文提出了一种基于物理特性的材料分类法，并构建了一个多样化的数据集。通过图注意力网络实现分层材料识别，性能优异，且能适应复杂环境和少样本学习。


<details>
  <summary>Details</summary>
Motivation: 为视觉应用提供一种基于物理特性的材料分类法，并解决实际场景中的材料识别问题。

Method: 使用图注意力网络，结合分类法和深度图数据，实现分层材料识别。

Result: 模型在性能上达到最优，并能适应复杂环境和少样本学习。

Conclusion: 提出的分类法和模型在材料识别中表现优异，具有实际应用潜力。

Abstract: We introduce a taxonomy of materials for hierarchical recognition from local
appearance. Our taxonomy is motivated by vision applications and is arranged
according to the physical traits of materials. We contribute a diverse,
in-the-wild dataset with images and depth maps of the taxonomy classes.
Utilizing the taxonomy and dataset, we present a method for hierarchical
material recognition based on graph attention networks. Our model leverages the
taxonomic proximity between classes and achieves state-of-the-art performance.
We demonstrate the model's potential to generalize to adverse, real-world
imaging conditions, and that novel views rendered using the depth maps can
enhance this capability. Finally, we show the model's capacity to rapidly learn
new materials in a few-shot learning setting.

</details>


### [22] [cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning](https://arxiv.org/abs/2505.22914)
*Maksim Kolodiazhnyi,Denis Tarasov,Dmitrii Zhemchuzhnikov,Alexander Nikulin,Ilya Zisman,Anna Vorontsova,Anton Konushin,Vladislav Kurenkov,Danila Rukhovich*

Main category: cs.CV

TL;DR: 提出了一种多模态CAD重建模型，结合点云、图像和文本输入，利用视觉语言模型（VLM）和两阶段训练（监督微调+强化学习），在DeepCAD基准测试中表现优于现有单模态方法。


<details>
  <summary>Details</summary>
Motivation: 现有CAD重建方法通常仅支持单一输入模态，限制了通用性和鲁棒性。多模态输入可以提升设计的民主化。

Method: 采用两阶段训练：1) 监督微调（SFT）基于大规模生成数据；2) 强化学习（RL）微调，使用在线反馈（如GRPO算法）。

Result: SFT模型在DeepCAD基准测试中超越单模态方法；RL微调后，在三个数据集（包括真实世界数据）上达到新SOTA。

Conclusion: 多模态输入结合两阶段训练显著提升CAD重建性能，强化学习微调效果优于离线方法。

Abstract: Computer-Aided Design (CAD) plays a central role in engineering and
manufacturing, making it possible to create precise and editable 3D models.
Using a variety of sensor or user-provided data as inputs for CAD
reconstruction can democratize access to design applications. However, existing
methods typically focus on a single input modality, such as point clouds,
images, or text, which limits their generalizability and robustness. Leveraging
recent advances in vision-language models (VLM), we propose a multi-modal CAD
reconstruction model that simultaneously processes all three input modalities.
Inspired by large language model (LLM) training paradigms, we adopt a two-stage
pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated
data, followed by reinforcement learning (RL) fine-tuning using online
feedback, obtained programatically. Furthermore, we are the first to explore RL
fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such
as Group Relative Preference Optimization (GRPO) outperform offline
alternatives. In the DeepCAD benchmark, our SFT model outperforms existing
single-modal approaches in all three input modalities simultaneously. More
importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three
challenging datasets, including a real-world one.

</details>


### [23] [Re-ttention: Ultra Sparse Visual Generation via Attention Statistical Reshape](https://arxiv.org/abs/2505.22918)
*Ruichen Chen,Keith G. Mills,Liyao Jiang,Chao Gao,Di Niu*

Main category: cs.CV

TL;DR: Re-ttention是一种稀疏注意力机制，用于解决DiT模型中注意力计算复杂度高的问题，通过利用扩散模型的时间冗余性，在极高稀疏度下保持视觉质量。


<details>
  <summary>Details</summary>
Motivation: 解决DiT模型中注意力机制计算复杂度随分辨率和视频长度呈二次方增长的问题，同时避免现有稀疏注意力技术在极高稀疏度下视觉质量下降和计算开销增加的问题。

Method: 提出Re-ttention，通过基于历史softmax分布重塑注意力分数，克服注意力机制中的概率归一化偏移，实现极高稀疏度下的视觉质量保持。

Result: 实验表明，Re-ttention在推理时仅需3.1%的token，优于FastDiTAttn等方法，并在H100 GPU上实现45%端到端延迟和92%自注意力延迟降低。

Conclusion: Re-ttention在极高稀疏度下显著降低了计算开销，同时保持了视觉质量，为高效视觉生成提供了可行方案。

Abstract: Diffusion Transformers (DiT) have become the de-facto model for generating
high-quality visual content like videos and images. A huge bottleneck is the
attention mechanism where complexity scales quadratically with resolution and
video length. One logical way to lessen this burden is sparse attention, where
only a subset of tokens or patches are included in the calculation. However,
existing techniques fail to preserve visual quality at extremely high sparsity
levels and might even incur non-negligible compute overheads. % To address this
concern, we propose Re-ttention, which implements very high sparse attention
for visual generation models by leveraging the temporal redundancy of Diffusion
Models to overcome the probabilistic normalization shift within the attention
mechanism. Specifically, Re-ttention reshapes attention scores based on the
prior softmax distribution history in order to preserve the visual quality of
the full quadratic attention at very high sparsity levels. % Experimental
results on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate
that Re-ttention requires as few as 3.1\% of the tokens during inference,
outperforming contemporary methods like FastDiTAttn, Sparse VideoGen and
MInference. Further, we measure latency to show that our method can attain over
45\% end-to-end % and over 92\% self-attention latency reduction on an H100 GPU
at negligible overhead cost.
  Code available online here:
\href{https://github.com/cccrrrccc/Re-ttention}{https://github.com/cccrrrccc/Re-ttention}

</details>


### [24] [Leveraging Diffusion Models for Synthetic Data Augmentation in Protein Subcellular Localization Classification](https://arxiv.org/abs/2505.22926)
*Sylvey Lin,Zhi-Yi Cao*

Main category: cs.CV

TL;DR: 研究探讨扩散模型生成的合成图像是否能提升蛋白质亚细胞定位的多标签分类，发现混合训练策略在验证集表现良好，但测试集泛化性差，强调真实数据生成和稳健监督的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型生成的合成图像在生物医学图像分类中的潜力，尤其是蛋白质亚细胞定位任务。

Method: 使用简化的类条件DDPM生成标签一致的样本，并尝试两种混合训练策略（Mix Loss和Mix Representation）。

Result: 混合策略在验证集表现良好，但测试集泛化性差；基于ResNet的基线模型表现更稳定。

Conclusion: 在生物医学图像分类中，真实数据生成和稳健监督对有效利用生成增强至关重要。

Abstract: We investigate whether synthetic images generated by diffusion models can
enhance multi-label classification of protein subcellular localization.
Specifically, we implement a simplified class-conditional denoising diffusion
probabilistic model (DDPM) to produce label-consistent samples and explore
their integration with real data via two hybrid training strategies: Mix Loss
and Mix Representation. While these approaches yield promising validation
performance, our proposed MixModel exhibits poor generalization to unseen test
data, underscoring the challenges of leveraging synthetic data effectively. In
contrast, baseline classifiers built on ResNet backbones with conventional loss
functions demonstrate greater stability and test-time performance. Our findings
highlight the importance of realistic data generation and robust supervision
when incorporating generative augmentation into biomedical image
classification.

</details>


### [25] [Fast Isotropic Median Filtering](https://arxiv.org/abs/2505.22938)
*Ben Weiss*

Main category: cs.CV

TL;DR: 提出了一种高效的中值滤波方法，克服了传统方法在比特深度、核大小和核形状上的限制。


<details>
  <summary>Details</summary>
Motivation: 传统中值滤波算法存在比特深度、核大小和核形状的限制，导致实际应用受限。

Method: 提出了一种新方法，支持任意比特深度、核大小和凸核形状（包括圆形）。

Result: 该方法高效且无传统方法的局限性。

Conclusion: 新方法为中值滤波提供了更灵活和高效的解决方案。

Abstract: Median filtering is a cornerstone of computational image processing. It
provides an effective means of image smoothing, with minimal blurring or
softening of edges, invariance to monotonic transformations such as gamma
adjustment, and robustness to noise and outliers. However, known algorithms
have all suffered from practical limitations: the bit depth of the image data,
the size of the filter kernel, or the kernel shape itself. Square-kernel
implementations tend to produce streaky cross-hatching artifacts, and nearly
all known efficient algorithms are in practice limited to square kernels. We
present for the first time a method that overcomes all of these limitations.
Our method operates efficiently on arbitrary bit-depth data, arbitrary kernel
sizes, and arbitrary convex kernel shapes, including circular shapes.

</details>


### [26] [ATI: Any Trajectory Instruction for Controllable Video Generation](https://arxiv.org/abs/2505.22944)
*Angtian Wang,Haibin Huang,Jacob Zhiyuan Fang,Yiding Yang,Chongyang Ma*

Main category: cs.CV

TL;DR: 提出了一种统一的视频生成运动控制框架，通过轨迹输入整合相机运动、物体平移和局部精细运动。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法需通过独立模块或任务特定设计处理不同运动类型的问题，提供一种统一的解决方案。

Method: 通过轻量级运动注入器将用户定义的轨迹投影到预训练图像到视频生成模型的潜在空间中。

Result: 在多种视频运动控制任务中表现优异，包括风格化运动效果、动态视角变化和精确局部运动操控。

Conclusion: 该方法在可控性和视觉质量上显著优于现有方法，且兼容多种先进视频生成模型。

Abstract: We propose a unified framework for motion control in video generation that
seamlessly integrates camera movement, object-level translation, and
fine-grained local motion using trajectory-based inputs. In contrast to prior
methods that address these motion types through separate modules or
task-specific designs, our approach offers a cohesive solution by projecting
user-defined trajectories into the latent space of pre-trained image-to-video
generation models via a lightweight motion injector. Users can specify
keypoints and their motion paths to control localized deformations, entire
object motion, virtual camera dynamics, or combinations of these. The injected
trajectory signals guide the generative process to produce temporally
consistent and semantically aligned motion sequences. Our framework
demonstrates superior performance across multiple video motion control tasks,
including stylized motion effects (e.g., motion brushes), dynamic viewpoint
changes, and precise local motion manipulation. Experiments show that our
method provides significantly better controllability and visual quality
compared to prior approaches and commercial solutions, while remaining broadly
compatible with various state-of-the-art video generation backbones. Project
page: https://anytraj.github.io/.

</details>


### [27] [Toward Memory-Aided World Models: Benchmarking via Spatial Consistency](https://arxiv.org/abs/2505.22976)
*Kewei Lian,Shaofei Cai,Yilun Du,Yitao Liang*

Main category: cs.CV

TL;DR: 论文提出了一种用于增强空间一致性的数据集和基准测试，基于Minecraft环境收集了20百万帧的导航视频，支持世界模型的学习和评估。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏对空间一致性的关注，无法有效支持记忆模块的开发。

Method: 构建了一个包含150个Minecraft位置的导航视频数据集，采用课程设计逐步增加序列长度。

Result: 评估了四种世界模型基线，数据集和代码已开源。

Conclusion: 该数据集填补了空间一致性研究的空白，支持未来世界模型的开发。

Abstract: The ability to simulate the world in a spatially consistent manner is a
crucial requirements for effective world models. Such a model enables
high-quality visual generation, and also ensures the reliability of world
models for downstream tasks such as simulation and planning. Designing a memory
module is a crucial component for addressing spatial consistency: such a model
must not only retain long-horizon observational information, but also enables
the construction of explicit or implicit internal spatial representations.
However, there are no dataset designed to promote the development of memory
modules by explicitly enforcing spatial consistency constraints. Furthermore,
most existing benchmarks primarily emphasize visual coherence or generation
quality, neglecting the requirement of long-range spatial consistency. To
bridge this gap, we construct a dataset and corresponding benchmark by sampling
150 distinct locations within the open-world environment of Minecraft,
collecting about 250 hours (20 million frames) of loop-based navigation videos
with actions. Our dataset follows a curriculum design of sequence lengths,
allowing models to learn spatial consistency on increasingly complex navigation
trajectories. Furthermore, our data collection pipeline is easily extensible to
new Minecraft environments and modules. Four representative world model
baselines are evaluated on our benchmark. Dataset, benchmark, and code are
open-sourced to support future research.

</details>


### [28] [HyperMotion: DiT-Based Pose-Guided Human Image Animation of Complex Motions](https://arxiv.org/abs/2505.22977)
*Shuolin Xu,Siming Zheng,Ziyi Wang,HC Yu,Jinwei Chen,Huaqi Zhang,Bo Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 论文提出了Open-HyperMotionX数据集和HyperMotionX Bench，用于评估和改进复杂人体运动条件下的姿态引导图像动画模型，并提出了一种基于DiT的视频生成基线方法，通过空间低频增强RoPE模块显著提升了动态人体运动序列的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂人体运动（如Hypermotion）中表现不佳，且缺乏高质量评估基准。

Method: 提出Open-HyperMotionX数据集和HyperMotionX Bench，设计基于DiT的视频生成基线方法，引入空间低频增强RoPE模块。

Result: 实验表明，该方法显著提升了复杂人体运动图像动画的生成质量。

Conclusion: 提出的数据集和方法有效解决了复杂人体运动条件下的动画生成问题，代码和数据集将公开。

Abstract: Recent advances in diffusion models have significantly improved conditional
video generation, particularly in the pose-guided human image animation task.
Although existing methods are capable of generating high-fidelity and
time-consistent animation sequences in regular motions and static scenes, there
are still obvious limitations when facing complex human body motions
(Hypermotion) that contain highly dynamic, non-standard motions, and the lack
of a high-quality benchmark for evaluation of complex human motion animations.
To address this challenge, we introduce the \textbf{Open-HyperMotionX Dataset}
and \textbf{HyperMotionX Bench}, which provide high-quality human pose
annotations and curated video clips for evaluating and improving pose-guided
human image animation models under complex human motion conditions.
Furthermore, we propose a simple yet powerful DiT-based video generation
baseline and design spatial low-frequency enhanced RoPE, a novel module that
selectively enhances low-frequency spatial feature modeling by introducing
learnable frequency scaling. Our method significantly improves structural
stability and appearance consistency in highly dynamic human motion sequences.
Extensive experiments demonstrate the effectiveness of our dataset and proposed
approach in advancing the generation quality of complex human motion image
animations. Code and dataset will be made publicly available.

</details>


### [29] [Pose-free 3D Gaussian splatting via shape-ray estimation](https://arxiv.org/abs/2505.22978)
*Youngju Na,Taeyeon Kim,Jumin Lee,Kyu Beom Han,Woo Jae Kim,Sung-eui Yoon*

Main category: cs.CV

TL;DR: SHARE是一种无需相机姿态的3D高斯泼溅框架，通过联合形状和光线估计解决姿态不准确导致的几何错位问题。


<details>
  <summary>Details</summary>
Motivation: 在真实场景中，精确的相机姿态难以获取，导致几何错位，SHARE旨在解决这一问题。

Method: SHARE通过构建姿态感知的规范体积表示，整合多视角信息，并利用锚点对齐的高斯预测优化局部几何。

Result: 在多样化的真实数据集上，SHARE在无需姿态的情况下实现了鲁棒的高斯泼溅性能。

Conclusion: SHARE通过联合估计和局部优化，有效解决了姿态不准确带来的挑战。

Abstract: While generalizable 3D Gaussian splatting enables efficient, high-quality
rendering of unseen scenes, it heavily depends on precise camera poses for
accurate geometry. In real-world scenarios, obtaining accurate poses is
challenging, leading to noisy pose estimates and geometric misalignments. To
address this, we introduce SHARE, a pose-free, feed-forward Gaussian splatting
framework that overcomes these ambiguities by joint shape and camera rays
estimation. Instead of relying on explicit 3D transformations, SHARE builds a
pose-aware canonical volume representation that seamlessly integrates
multi-view information, reducing misalignment caused by inaccurate pose
estimates. Additionally, anchor-aligned Gaussian prediction enhances scene
reconstruction by refining local geometry around coarse anchors, allowing for
more precise Gaussian placement. Extensive experiments on diverse real-world
datasets show that our method achieves robust performance in pose-free
generalizable Gaussian splatting.

</details>


### [30] [MOVi: Training-free Text-conditioned Multi-Object Video Generation](https://arxiv.org/abs/2505.22980)
*Aimon Rahman,Jiang Liu,Ze Wang,Ximeng Sun,Jialian Wu,Xiaodong Yu,Yusheng Su,Vishal M. Patel,Zicheng Liu,Emad Barsoum*

Main category: cs.CV

TL;DR: 提出了一种无需训练的多对象视频生成方法，利用扩散模型和大型语言模型（LLM）的开放世界知识，显著提升了多对象生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有扩散基文本到视频（T2V）模型在多对象生成中存在挑战，如对象交互复杂、对象特征混淆等问题。

Method: 使用LLM作为对象轨迹的“导演”，通过噪声重新初始化和注意力机制优化，实现精确控制和特征分离。

Result: 实验显示，该方法在多对象生成准确性和运动动态性上提升了42%，同时保持了高保真度和运动平滑性。

Conclusion: 该方法为多对象视频生成提供了一种高效且无需训练的解决方案，显著提升了现有模型的性能。

Abstract: Recent advances in diffusion-based text-to-video (T2V) models have
demonstrated remarkable progress, but these models still face challenges in
generating videos with multiple objects. Most models struggle with accurately
capturing complex object interactions, often treating some objects as static
background elements and limiting their movement. In addition, they often fail
to generate multiple distinct objects as specified in the prompt, resulting in
incorrect generations or mixed features across objects. In this paper, we
present a novel training-free approach for multi-object video generation that
leverages the open world knowledge of diffusion models and large language
models (LLMs). We use an LLM as the ``director'' of object trajectories, and
apply the trajectories through noise re-initialization to achieve precise
control of realistic movements. We further refine the generation process by
manipulating the attention mechanism to better capture object-specific features
and motion patterns, and prevent cross-object feature interference. Extensive
experiments validate the effectiveness of our training free approach in
significantly enhancing the multi-object generation capabilities of existing
video diffusion models, resulting in 42% absolute improvement in motion
dynamics and object generation accuracy, while also maintaining high fidelity
and motion smoothness.

</details>


### [31] [Synthetic Document Question Answering in Hungarian](https://arxiv.org/abs/2505.23008)
*Jonathan Li,Zoltan Csaki,Nidhi Hiremath,Etash Guha,Fenglu Hong,Edward Ma,Urmish Thakker*

Main category: cs.CV

TL;DR: 论文提出了针对匈牙利语的文档视觉问答（VQA）数据集HuDocVQA和HuDocVQA-manual，以及用于OCR训练的HuCCPDF数据集，通过微调提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（如匈牙利语）在文档VQA任务中缺乏训练和评估数据的问题。

Method: 通过人工和合成方法构建匈牙利语数据集，并进行质量过滤和去重；同时提供OCR训练数据集。

Result: 微调Llama 3.2 11B Instruct模型在HuDocVQA上的准确率提升了7.2%。

Conclusion: 数据集和代码将公开，以促进多语言文档VQA的研究。

Abstract: Modern VLMs have achieved near-saturation accuracy in English document visual
question-answering (VQA). However, this task remains challenging in lower
resource languages due to a dearth of suitable training and evaluation data. In
this paper we present scalable methods for curating such datasets by focusing
on Hungarian, approximately the 17th highest resource language on the internet.
Specifically, we present HuDocVQA and HuDocVQA-manual, document VQA datasets
that modern VLMs significantly underperform on compared to English DocVQA.
HuDocVQA-manual is a small manually curated dataset based on Hungarian
documents from Common Crawl, while HuDocVQA is a larger synthetically generated
VQA data set from the same source. We apply multiple rounds of quality
filtering and deduplication to HuDocVQA in order to match human-level quality
in this dataset. We also present HuCCPDF, a dataset of 117k pages from
Hungarian Common Crawl PDFs along with their transcriptions, which can be used
for training a model for Hungarian OCR. To validate the quality of our
datasets, we show how finetuning on a mixture of these datasets can improve
accuracy on HuDocVQA for Llama 3.2 11B Instruct by +7.2%. Our datasets and code
will be released to the public to foster further research in multilingual
DocVQA.

</details>


### [32] [SeG-SR: Integrating Semantic Knowledge into Remote Sensing Image Super-Resolution via Vision-Language Model](https://arxiv.org/abs/2505.23010)
*Bowen Chen,Keyan Chen,Mohan Yang,Zhengxia Zou,Zhenwei Shi*

Main category: cs.CV

TL;DR: 论文提出了一种基于语义引导的遥感图像超分辨率框架（SeG-SR），利用视觉语言模型提取语义知识以提升超分辨率效果。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像超分辨率方法主要关注像素空间低层特征，忽视了高层语义理解，导致重建结果语义不一致。

Method: 提出SeG-SR框架，包含语义特征提取模块（SFEM）、语义定位模块（SLM）和可学习调制模块（LMM），通过语义知识引导超分辨率过程。

Result: 在多个数据集上达到最优性能，且适用于不同超分辨率架构。

Conclusion: SeG-SR通过引入高层语义知识，显著提升了遥感图像超分辨率的性能。

Abstract: High-resolution (HR) remote sensing imagery plays a vital role in a wide
range of applications, including urban planning and environmental monitoring.
However, due to limitations in sensors and data transmission links, the images
acquired in practice often suffer from resolution degradation. Remote Sensing
Image Super-Resolution (RSISR) aims to reconstruct HR images from
low-resolution (LR) inputs, providing a cost-effective and efficient
alternative to direct HR image acquisition. Existing RSISR methods primarily
focus on low-level characteristics in pixel space, while neglecting the
high-level understanding of remote sensing scenes. This may lead to
semantically inconsistent artifacts in the reconstructed results. Motivated by
this observation, our work aims to explore the role of high-level semantic
knowledge in improving RSISR performance. We propose a Semantic-Guided
Super-Resolution framework, SeG-SR, which leverages Vision-Language Models
(VLMs) to extract semantic knowledge from input images and uses it to guide the
super resolution (SR) process. Specifically, we first design a Semantic Feature
Extraction Module (SFEM) that utilizes a pretrained VLM to extract semantic
knowledge from remote sensing images. Next, we propose a Semantic Localization
Module (SLM), which derives a series of semantic guidance from the extracted
semantic knowledge. Finally, we develop a Learnable Modulation Module (LMM)
that uses semantic guidance to modulate the features extracted by the SR
network, effectively incorporating high-level scene understanding into the SR
pipeline. We validate the effectiveness and generalizability of SeG-SR through
extensive experiments: SeG-SR achieves state-of-the-art performance on two
datasets and consistently delivers performance improvements across various SR
architectures. Codes can be found at https://github.com/Mr-Bamboo/SeG-SR.

</details>


### [33] [Spatio-Temporal Joint Density Driven Learning for Skeleton-Based Action Recognition](https://arxiv.org/abs/2505.23012)
*Shanaka Ramesh Gunasekara,Wanqing Li,Philip Ogunbona,Jack Yang*

Main category: cs.CV

TL;DR: 论文提出了一种新的空间-时间关节密度（STJD）测量方法，用于量化骨骼序列中动态与静态关节的交互作用，并开发了两种自监督学习方法（STJD-CL和STJD-MP），显著提升了动作分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法主要关注骨骼序列的动态特征，而忽略了动态与静态关节交互的判别潜力。本文旨在利用这种交互作用提升动作分类性能。

Method: 提出STJD测量方法，识别关键关节（prime joints），并开发了对比学习策略STJD-CL和结合重建框架的STJD-MP方法。

Result: 在NTU RGB+D 60、NTU RGB+D 120和PKUMMD数据集上，STJD-CL和STJD-MP性能显著提升，尤其在NTU RGB+D 120上分别比现有方法提高了3.5和3.6个百分点。

Conclusion: STJD方法有效利用了动态与静态关节的交互作用，显著提升了动作分类性能，为自监督学习提供了新思路。

Abstract: Traditional approaches in unsupervised or self supervised learning for
skeleton-based action classification have concentrated predominantly on the
dynamic aspects of skeletal sequences. Yet, the intricate interaction between
the moving and static elements of the skeleton presents a rarely tapped
discriminative potential for action classification. This paper introduces a
novel measurement, referred to as spatial-temporal joint density (STJD), to
quantify such interaction. Tracking the evolution of this density throughout an
action can effectively identify a subset of discriminative moving and/or static
joints termed "prime joints" to steer self-supervised learning. A new
contrastive learning strategy named STJD-CL is proposed to align the
representation of a skeleton sequence with that of its prime joints while
simultaneously contrasting the representations of prime and nonprime joints. In
addition, a method called STJD-MP is developed by integrating it with a
reconstruction-based framework for more effective learning. Experimental
evaluations on the NTU RGB+D 60, NTU RGB+D 120, and PKUMMD datasets in various
downstream tasks demonstrate that the proposed STJD-CL and STJD-MP improved
performance, particularly by 3.5 and 3.6 percentage points over the
state-of-the-art contrastive methods on the NTU RGB+D 120 dataset using X-sub
and X-set evaluations, respectively.

</details>


### [34] [Towards Privacy-Preserving Fine-Grained Visual Classification via Hierarchical Learning from Label Proportions](https://arxiv.org/abs/2505.23031)
*Jinyi Chang,Dongliang Chang,Lei Chen,Bingyao Yu,Zhanyu Ma*

Main category: cs.CV

TL;DR: 本文提出了一种无需实例级标签的细粒度视觉分类方法LHFGLP，利用分层标签比例学习（LLP）框架，通过分层特征细化和分层比例损失提升分类精度。


<details>
  <summary>Details</summary>
Motivation: 现有细粒度分类方法依赖实例级标签，不适用于隐私敏感场景（如医学图像分析）。本文旨在解决这一问题。

Method: 提出LHFGLP框架，结合分层稀疏字典学习和分层比例损失，实现渐进式特征细化和分类优化。

Result: 在三个细粒度数据集上的实验表明，LHFGLP优于现有LLP方法。

Conclusion: LHFGLP为隐私保护的细粒度分类提供了有效解决方案，代码和数据集将公开以促进研究。

Abstract: In recent years, Fine-Grained Visual Classification (FGVC) has achieved
impressive recognition accuracy, despite minimal inter-class variations.
However, existing methods heavily rely on instance-level labels, making them
impractical in privacy-sensitive scenarios such as medical image analysis. This
paper aims to enable accurate fine-grained recognition without direct access to
instance labels. To achieve this, we leverage the Learning from Label
Proportions (LLP) paradigm, which requires only bag-level labels for efficient
training. Unlike existing LLP-based methods, our framework explicitly exploits
the hierarchical nature of fine-grained datasets, enabling progressive feature
granularity refinement and improving classification accuracy. We propose
Learning from Hierarchical Fine-Grained Label Proportions (LHFGLP), a framework
that incorporates Unrolled Hierarchical Fine-Grained Sparse Dictionary
Learning, transforming handcrafted iterative approximation into learnable
network optimization. Additionally, our proposed Hierarchical Proportion Loss
provides hierarchical supervision, further enhancing classification
performance. Experiments on three widely-used fine-grained datasets, structured
in a bag-based manner, demonstrate that our framework consistently outperforms
existing LLP-based methods. We will release our code and datasets to foster
further research in privacy-preserving fine-grained classification.

</details>


### [35] [Deep Modeling and Optimization of Medical Image Classification](https://arxiv.org/abs/2505.23040)
*Yihang Wu,Muhammad Owais,Reem Kateb,Ahmad Chaddad*

Main category: cs.CV

TL;DR: 论文提出了一种改进的CLIP变体，结合多种深度模型和联邦学习技术，用于医学图像分类，同时利用传统机器学习方法提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决医学领域因数据隐私问题导致的大数据需求不足，以及CLIP在医学领域潜力未被充分挖掘的问题。

Method: 1) 提出CLIP变体，使用四种CNN和八种ViT作为图像编码器；2) 结合12种深度模型与两种联邦学习技术；3) 引入传统ML方法提升泛化能力。

Result: MaxViT在HAM10000数据集中表现最佳（AVG=87.03%），ConvNeXt_L在FL模型中F1-score达83.98%，SVM提升Swin Transformer系列性能约2%。

Conclusion: 该方法在医学图像分类中表现出色，同时兼顾数据隐私和泛化能力。

Abstract: Deep models, such as convolutional neural networks (CNNs) and vision
transformer (ViT), demonstrate remarkable performance in image classification.
However, those deep models require large data to fine-tune, which is
impractical in the medical domain due to the data privacy issue. Furthermore,
despite the feasible performance of contrastive language image pre-training
(CLIP) in the natural domain, the potential of CLIP has not been fully
investigated in the medical field. To face these challenges, we considered
three scenarios: 1) we introduce a novel CLIP variant using four CNNs and eight
ViTs as image encoders for the classification of brain cancer and skin cancer,
2) we combine 12 deep models with two federated learning techniques to protect
data privacy, and 3) we involve traditional machine learning (ML) methods to
improve the generalization ability of those deep models in unseen domain data.
The experimental results indicate that maxvit shows the highest averaged (AVG)
test metrics (AVG = 87.03\%) in HAM10000 dataset with multimodal learning,
while convnext\_l demonstrates remarkable test with an F1-score of 83.98\%
compared to swin\_b with 81.33\% in FL model. Furthermore, the use of support
vector machine (SVM) can improve the overall test metrics with AVG of $\sim
2\%$ for swin transformer series in ISIC2018. Our codes are available at
https://github.com/AIPMLab/SkinCancerSimulation.

</details>


### [36] [Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation](https://arxiv.org/abs/2505.23043)
*Jihai Zhang,Tianle Li,Linjie Li,Zhengyuan Yang,Yu Cheng*

Main category: cs.CV

TL;DR: 本文探讨了统一视觉语言模型（VLMs）中理解与生成任务的相互增强，发现混合数据训练能带来双向提升，且多模态对齐和跨任务知识转移是关键。


<details>
  <summary>Details</summary>
Motivation: 研究统一VLMs中理解与生成任务的相互增强假设，填补此前研究的空白。

Method: 设计真实场景数据集，评估多种统一VLM架构，进行定量实验。

Result: 混合数据训练带来双向提升，多模态对齐和跨任务知识转移显著影响性能。

Conclusion: 统一理解与生成对VLMs至关重要，为模型设计与优化提供新见解。

Abstract: Recent advancements in unified vision-language models (VLMs), which integrate
both visual understanding and generation capabilities, have attracted
significant attention. The underlying hypothesis is that a unified architecture
with mixed training on both understanding and generation tasks can enable
mutual enhancement between understanding and generation. However, this
hypothesis remains underexplored in prior works on unified VLMs. To address
this gap, this paper systematically investigates the generalization across
understanding and generation tasks in unified VLMs. Specifically, we design a
dataset closely aligned with real-world scenarios to facilitate extensive
experiments and quantitative evaluations. We evaluate multiple unified VLM
architectures to validate our findings. Our key findings are as follows. First,
unified VLMs trained with mixed data exhibit mutual benefits in understanding
and generation tasks across various architectures, and this mutual benefits can
scale up with increased data. Second, better alignment between multimodal input
and output spaces will lead to better generalization. Third, the knowledge
acquired during generation tasks can transfer to understanding tasks, and this
cross-task generalization occurs within the base language model, beyond
modality adapters. Our findings underscore the critical necessity of unifying
understanding and generation in VLMs, offering valuable insights for the design
and optimization of unified VLMs.

</details>


### [37] [SpatialSplat: Efficient Semantic 3D from Sparse Unposed Images](https://arxiv.org/abs/2505.23044)
*Yu Sheng,Jiajun Deng,Xinran Zhang,Yu Zhang,Bei Hua,Yanyong Zhang,Jianmin Ji*

Main category: cs.CV

TL;DR: SpatialSplat 是一种前馈框架，通过双场语义表示和选择性高斯机制，减少冗余并提升语义3D重建的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在压缩语义特征时损失表达能力，且像素级预测导致冗余，增加了内存开销。

Method: 提出双场语义表示（粗粒度特征场和细粒度特征场）和选择性高斯机制，消除冗余高斯。

Result: 实验表明，场景表示参数减少60%，性能优于现有方法。

Conclusion: SpatialSplat 实现了更紧凑的3D高斯表示，提升了语义3D重建的实用性。

Abstract: A major breakthrough in 3D reconstruction is the feedforward paradigm to
generate pixel-wise 3D points or Gaussian primitives from sparse, unposed
images. To further incorporate semantics while avoiding the significant memory
and storage costs of high-dimensional semantic features, existing methods
extend this paradigm by associating each primitive with a compressed semantic
feature vector. However, these methods have two major limitations: (a) the
naively compressed feature compromises expressiveness, affecting the model's
ability to capture fine-grained semantics, and (b) the pixel-wise primitive
prediction introduces redundancy in overlapping areas, causing unnecessary
memory overhead. To this end, we introduce \textbf{SpatialSplat}, a feedforward
framework that produces redundancy-aware Gaussians and capitalizes on a
dual-field semantic representation. Particularly, with the insight that
primitives within the same instance exhibit high semantic consistency, we
decompose the semantic representation into a coarse feature field that encodes
uncompressed semantics with minimal primitives, and a fine-grained yet
low-dimensional feature field that captures detailed inter-instance
relationships. Moreover, we propose a selective Gaussian mechanism, which
retains only essential Gaussians in the scene, effectively eliminating
redundant primitives. Our proposed Spatialsplat learns accurate semantic
information and detailed instances prior with more compact 3D Gaussians, making
semantic 3D reconstruction more applicable. We conduct extensive experiments to
evaluate our method, demonstrating a remarkable 60\% reduction in scene
representation parameters while achieving superior performance over
state-of-the-art methods. The code will be made available for future
investigation.

</details>


### [38] [Multi-Sourced Compositional Generalization in Visual Question Answering](https://arxiv.org/abs/2505.23045)
*Chuanhao Li,Wenbo Ye,Zhen Li,Yuwei Wu,Yunde Jia*

Main category: cs.CV

TL;DR: 该论文研究了多模态视觉与语言任务中的多源组合泛化能力（MSCG），提出了一种检索增强的训练框架，通过统一不同模态的原语表示来提升VQA模型的MSCG能力。


<details>
  <summary>Details</summary>
Motivation: 由于视觉与语言任务的多模态特性，组合泛化能力在多源情况下的研究尚未深入，论文旨在填补这一空白。

Method: 提出检索增强训练框架，通过检索语义等价的原语并聚合其特征，统一不同模态的原语表示。

Result: 实验结果表明该框架有效提升了VQA模型的MSCG能力，并发布了新的GQA-MSCG数据集。

Conclusion: 论文成功探索了MSCG问题，提出的框架和数据集为未来研究提供了基础。

Abstract: Compositional generalization is the ability of generalizing novel
compositions from seen primitives, and has received much attention in
vision-and-language (V\&L) recently. Due to the multi-modal nature of V\&L
tasks, the primitives composing compositions source from different modalities,
resulting in multi-sourced novel compositions. However, the generalization
ability over multi-sourced novel compositions, \textit{i.e.}, multi-sourced
compositional generalization (MSCG) remains unexplored. In this paper, we
explore MSCG in the context of visual question answering (VQA), and propose a
retrieval-augmented training framework to enhance the MSCG ability of VQA
models by learning unified representations for primitives from different
modalities. Specifically, semantically equivalent primitives are retrieved for
each primitive in the training samples, and the retrieved features are
aggregated with the original primitive to refine the model. This process helps
the model learn consistent representations for the same semantic primitives
across different modalities. To evaluate the MSCG ability of VQA models, we
construct a new GQA-MSCG dataset based on the GQA dataset, in which samples
include three types of novel compositions composed of primitives from different
modalities. Experimental results demonstrate the effectiveness of the proposed
framework. We release GQA-MSCG at https://github.com/NeverMoreLCH/MSCG.

</details>


### [39] [Zero-P-to-3: Zero-Shot Partial-View Images to 3D Object](https://arxiv.org/abs/2505.23054)
*Yuxuan Lin,Ruihang Chu,Zhenyu Chen,Xiao Tang,Lei Ke,Haoling Li,Yingji Zhong,Zhihao Li,Shiyong Liu,Xiaofei Wu,Jianzhuang Liu,Yujiu Yang*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练的方法\method，通过融合局部密集观测和多源先验来解决部分视角3D重建中的视角范围有限和生成不一致问题。


<details>
  <summary>Details</summary>
Motivation: 部分视角3D重建在稀疏视角和单图像重建研究较多，但部分观测视角的重建仍未被充分探索，尤其是视角范围有限和生成不一致的挑战。

Method: 提出\method方法，融合局部密集观测和多源先验，通过DDIM采样对齐先验生成多视角一致图像，并设计迭代细化策略利用几何结构提升重建质量。

Result: 在多个数据集上的实验表明，该方法在不可见区域的表现优于现有技术。

Conclusion: \method方法通过融合先验和迭代细化，有效解决了部分视角3D重建中的关键挑战。

Abstract: Generative 3D reconstruction shows strong potential in incomplete
observations. While sparse-view and single-image reconstruction are
well-researched, partial observation remains underexplored. In this context,
dense views are accessible only from a specific angular range, with other
perspectives remaining inaccessible. This task presents two main challenges:
(i) limited View Range: observations confined to a narrow angular scope prevent
effective traditional interpolation techniques that require evenly distributed
perspectives. (ii) inconsistent Generation: views created for invisible regions
often lack coherence with both visible regions and each other, compromising
reconstruction consistency. To address these challenges, we propose \method, a
novel training-free approach that integrates the local dense observations and
multi-source priors for reconstruction. Our method introduces a fusion-based
strategy to effectively align these priors in DDIM sampling, thereby generating
multi-view consistent images to supervise invisible views. We further design an
iterative refinement strategy, which uses the geometric structures of the
object to enhance reconstruction quality. Extensive experiments on multiple
datasets show the superiority of our method over SOTAs, especially in invisible
regions.

</details>


### [40] [URWKV: Unified RWKV Model with Multi-state Perspective for Low-light Image Restoration](https://arxiv.org/abs/2505.23068)
*Rui Xu,Yuzhen Niu,Yuezhou Li,Huangbiao Xu,Wenxi Liu,Yuzhong Chen*

Main category: cs.CV

TL;DR: 提出了一种统一的多状态感知模型URWKV，用于低光图像增强和去模糊，通过自适应归一化和选择性融合模块，显著提升了性能并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强和去模糊模型在处理动态耦合的退化问题时表现受限，需要更灵活的解决方案。

Method: 设计了Luminance-adaptive Normalization（LAN）和State-aware Selective Fusion（SSF）模块，结合多状态感知机制，动态调整归一化参数和特征融合。

Result: URWKV模型在多个基准测试中表现优于现有方法，同时参数和计算资源需求显著减少。

Conclusion: URWKV模型通过多状态感知和动态调整机制，有效解决了低光图像增强和去模糊中的复杂退化问题。

Abstract: Existing low-light image enhancement (LLIE) and joint LLIE and deblurring
(LLIE-deblur) models have made strides in addressing predefined degradations,
yet they are often constrained by dynamically coupled degradations. To address
these challenges, we introduce a Unified Receptance Weighted Key Value (URWKV)
model with multi-state perspective, enabling flexible and effective degradation
restoration for low-light images. Specifically, we customize the core URWKV
block to perceive and analyze complex degradations by leveraging multiple
intra- and inter-stage states. First, inspired by the pupil mechanism in the
human visual system, we propose Luminance-adaptive Normalization (LAN) that
adjusts normalization parameters based on rich inter-stage states, allowing for
adaptive, scene-aware luminance modulation. Second, we aggregate multiple
intra-stage states through exponential moving average approach, effectively
capturing subtle variations while mitigating information loss inherent in the
single-state mechanism. To reduce the degradation effects commonly associated
with conventional skip connections, we propose the State-aware Selective Fusion
(SSF) module, which dynamically aligns and integrates multi-state features
across encoder stages, selectively fusing contextual information. In comparison
to state-of-the-art models, our URWKV model achieves superior performance on
various benchmarks, while requiring significantly fewer parameters and
computational resources.

</details>


### [41] [LeMoRe: Learn More Details for Lightweight Semantic Segmentation](https://arxiv.org/abs/2505.23093)
*Mian Muhammad Naeem Abid,Nancy Mehta,Zongwei Wu,Radu Timofte*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级语义分割方法LeMoRe，通过结合显式和隐式建模平衡计算效率与表示保真度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在特征建模复杂度上难以平衡效率与性能，且受限于刚性架构和隐式表示学习。

Method: 结合明确的笛卡尔方向和显式建模视图与隐式推断中间表示，通过嵌套注意力机制高效捕获全局依赖。

Result: 在ADE20K、CityScapes等数据集上实验表明，LeMoRe在性能与效率间取得了良好平衡。

Conclusion: LeMoRe为轻量级语义分割提供了一种高效且性能优越的解决方案。

Abstract: Lightweight semantic segmentation is essential for many downstream vision
tasks. Unfortunately, existing methods often struggle to balance efficiency and
performance due to the complexity of feature modeling. Many of these existing
approaches are constrained by rigid architectures and implicit representation
learning, often characterized by parameter-heavy designs and a reliance on
computationally intensive Vision Transformer-based frameworks. In this work, we
introduce an efficient paradigm by synergizing explicit and implicit modeling
to balance computational efficiency with representational fidelity. Our method
combines well-defined Cartesian directions with explicitly modeled views and
implicitly inferred intermediate representations, efficiently capturing global
dependencies through a nested attention mechanism. Extensive experiments on
challenging datasets, including ADE20K, CityScapes, Pascal Context, and
COCO-Stuff, demonstrate that LeMoRe strikes an effective balance between
performance and efficiency.

</details>


### [42] [CURVE: CLIP-Utilized Reinforcement Learning for Visual Image Enhancement via Simple Image Processing](https://arxiv.org/abs/2505.23102)
*Yuka Ogino,Takahiro Toizumi,Atsushi Ito*

Main category: cs.CV

TL;DR: 论文提出了一种基于CLIP和强化学习的低光图像增强方法CURVE，通过Bézier曲线调整全局色调，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决零参考低光图像增强中如何利用CLIP模型生成感知良好的图像并保持计算效率的问题。

Method: 使用Bézier曲线调整全局图像色调，并通过强化学习训练参数估计器，奖励由CLIP文本嵌入设计。

Result: 在低光和多曝光数据集上，CURVE在增强质量和处理速度上优于传统方法。

Conclusion: CURVE结合CLIP和强化学习，有效提升了低光图像增强的性能和效率。

Abstract: Low-Light Image Enhancement (LLIE) is crucial for improving both human
perception and computer vision tasks. This paper addresses two challenges in
zero-reference LLIE: obtaining perceptually 'good' images using the Contrastive
Language-Image Pre-Training (CLIP) model and maintaining computational
efficiency for high-resolution images. We propose CLIP-Utilized Reinforcement
learning-based Visual image Enhancement (CURVE). CURVE employs a simple image
processing module which adjusts global image tone based on B\'ezier curve and
estimates its processing parameters iteratively. The estimator is trained by
reinforcement learning with rewards designed using CLIP text embeddings.
Experiments on low-light and multi-exposure datasets demonstrate the
performance of CURVE in terms of enhancement quality and processing speed
compared to conventional methods.

</details>


### [43] [EAD: An EEG Adapter for Automated Classification](https://arxiv.org/abs/2505.23107)
*Pushapdeep Singh,Jyoti Nigam,Medicherla Vamsi Krishna,Arnav Bhavsar,Aditya Nigam*

Main category: cs.CV

TL;DR: 论文提出了一种名为EEG Adapter (EAD)的灵活框架，用于学习EEG信号的嵌入表示，适用于不同设备和任务。


<details>
  <summary>Details</summary>
Motivation: 传统EEG分类方法依赖于特定任务的数据采集和通道数量，限制了统一管道的开发。

Method: 利用EEG基础模型进行适应性调整，学习鲁棒的EEG表示。

Result: 在两个公开数据集上达到99.33%和92.31%的准确率，展示了跨任务的泛化能力。

Conclusion: EAD框架在多样EEG数据集上表现优异，具有零样本分类能力。

Abstract: While electroencephalography (EEG) has been a popular modality for neural
decoding, it often involves task specific acquisition of the EEG data. This
poses challenges for the development of a unified pipeline to learn embeddings
for various EEG signal classification, which is often involved in various
decoding tasks. Traditionally, EEG classification involves the step of signal
preprocessing and the use of deep learning techniques, which are highly
dependent on the number of EEG channels in each sample. However, the same
pipeline cannot be applied even if the EEG data is collected for the same
experiment but with different acquisition devices. This necessitates the
development of a framework for learning EEG embeddings, which could be highly
beneficial for tasks involving multiple EEG samples for the same task but with
varying numbers of EEG channels. In this work, we propose EEG Adapter (EAD), a
flexible framework compatible with any signal acquisition device. More
specifically, we leverage a recent EEG foundational model with significant
adaptations to learn robust representations from the EEG data for the
classification task. We evaluate EAD on two publicly available datasets
achieving state-of-the-art accuracies 99.33% and 92.31% on EEG-ImageNet and
BrainLat respectively. This illustrates the effectiveness of the proposed
framework across diverse EEG datasets containing two different perception
tasks: stimulus and resting-state EEG signals. We also perform zero-shot EEG
classification on EEG-ImageNet task to demonstrate the generalization
capability of the proposed approach.

</details>


### [44] [Identification of Patterns of Cognitive Impairment for Early Detection of Dementia](https://arxiv.org/abs/2505.23109)
*Anusha A. S.,Uma Ranjan,Medha Sharma,Siddharth Dutt*

Main category: cs.CV

TL;DR: 提出了一种个性化认知测试方案，通过识别个体认知障碍模式，为早期痴呆检测提供高效工具。


<details>
  <summary>Details</summary>
Motivation: 早期痴呆检测对干预至关重要，但传统认知测试耗时且难以适用于大规模人群，尤其是需要定期评估时。个体认知障碍模式差异进一步增加了复杂性。

Method: 从正常人和轻度认知障碍（MCI）人群中学习认知障碍模式，采用两步法（集成包装特征选择和聚类分析）识别模式，并将其用于个性化测试设计。

Result: 识别出的模式与临床认可的MCI变体一致，可用于预测无症状或正常人群的认知障碍路径。研究基于NACC数据库的24,000名受试者数据。

Conclusion: 该方法为痴呆早期检测提供了高效、个性化的解决方案，适用于大规模人群的定期评估。

Abstract: Early detection of dementia is crucial to devise effective interventions.
Comprehensive cognitive tests, while being the most accurate means of
diagnosis, are long and tedious, thus limiting their applicability to a large
population, especially when periodic assessments are needed. The problem is
compounded by the fact that people have differing patterns of cognitive
impairment as they progress to different forms of dementia. This paper presents
a novel scheme by which individual-specific patterns of impairment can be
identified and used to devise personalized tests for periodic follow-up.
Patterns of cognitive impairment are initially learned from a population
cluster of combined normals and MCIs, using a set of standardized cognitive
tests. Impairment patterns in the population are identified using a 2-step
procedure involving an ensemble wrapper feature selection followed by cluster
identification and analysis. These patterns have been shown to correspond to
clinically accepted variants of MCI, a prodrome of dementia. The learned
clusters of patterns can subsequently be used to identify the most likely route
of cognitive impairment, even for pre-symptomatic and apparently normal people.
Baseline data of 24,000 subjects from the NACC database was used for the study.

</details>


### [45] [Diffusion-Based Generative Models for 3D Occupancy Prediction in Autonomous Driving](https://arxiv.org/abs/2505.23115)
*Yunshen Wang,Yicheng Liu,Tianyuan Yuan,Yucheng Mao,Yingshi Liang,Xiuyu Yang,Honggang Zhang,Hang Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散模型的生成方法，用于预测3D占用网格，解决了传统判别方法在噪声数据、不完整观测和复杂3D场景结构中的问题。


<details>
  <summary>Details</summary>
Motivation: 当前判别方法在3D占用网格预测中存在噪声敏感、观测不完整和复杂结构处理不足的问题，影响了自动驾驶的准确性。

Method: 将3D占用预测重新定义为生成建模任务，利用扩散模型学习数据分布并融入3D场景先验。

Result: 实验表明，扩散模型在预测一致性、噪声鲁棒性和复杂结构处理上优于现有判别方法，尤其在遮挡或低可见度区域表现更优。

Conclusion: 该方法显著提升了3D占用预测的准确性和实用性，对自动驾驶的下游规划任务具有重要价值。

Abstract: Accurately predicting 3D occupancy grids from visual inputs is critical for
autonomous driving, but current discriminative methods struggle with noisy
data, incomplete observations, and the complex structures inherent in 3D
scenes. In this work, we reframe 3D occupancy prediction as a generative
modeling task using diffusion models, which learn the underlying data
distribution and incorporate 3D scene priors. This approach enhances prediction
consistency, noise robustness, and better handles the intricacies of 3D spatial
structures. Our extensive experiments show that diffusion-based generative
models outperform state-of-the-art discriminative approaches, delivering more
realistic and accurate occupancy predictions, especially in occluded or
low-visibility regions. Moreover, the improved predictions significantly
benefit downstream planning tasks, highlighting the practical advantages of our
method for real-world autonomous driving applications.

</details>


### [46] [TextSR: Diffusion Super-Resolution with Multilingual OCR Guidance](https://arxiv.org/abs/2505.23119)
*Keren Ye,Ignacio Garcia Dorado,Michalis Raptis,Mauricio Delbracio,Irene Zhu,Peyman Milanfar,Hossein Talebi*

Main category: cs.CV

TL;DR: TextSR是一种针对多语言场景文本图像超分辨率的多模态扩散模型，通过结合文本检测、OCR和字符形状先验，显著提升了文本超分辨率的准确性和可读性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的图像超分辨率方法在场景文本图像上表现不佳，存在文本定位不准和字符形状建模不足的问题，导致生成质量下降。

Method: TextSR利用文本检测器定位文本区域，通过OCR提取多语言文本，并使用UTF-8编码器和交叉注意力将字符转换为视觉形状。还提出了两种方法增强模型鲁棒性。

Result: 在TextZoom和TextVQA数据集上，TextSR表现优异，为场景文本图像超分辨率设立了新基准。

Conclusion: TextSR通过结合文本先验和图像信息，有效提升了文本超分辨率的细节和可读性，展示了其在实际应用中的潜力。

Abstract: While recent advancements in Image Super-Resolution (SR) using diffusion
models have shown promise in improving overall image quality, their application
to scene text images has revealed limitations. These models often struggle with
accurate text region localization and fail to effectively model image and
multilingual character-to-shape priors. This leads to inconsistencies, the
generation of hallucinated textures, and a decrease in the perceived quality of
the super-resolved text.
  To address these issues, we introduce TextSR, a multimodal diffusion model
specifically designed for Multilingual Scene Text Image Super-Resolution.
TextSR leverages a text detector to pinpoint text regions within an image and
then employs Optical Character Recognition (OCR) to extract multilingual text
from these areas. The extracted text characters are then transformed into
visual shapes using a UTF-8 based text encoder and cross-attention. Recognizing
that OCR may sometimes produce inaccurate results in real-world scenarios, we
have developed two innovative methods to enhance the robustness of our model.
By integrating text character priors with the low-resolution text images, our
model effectively guides the super-resolution process, enhancing fine details
within the text and improving overall legibility. The superior performance of
our model on both the TextZoom and TextVQA datasets sets a new benchmark for
STISR, underscoring the efficacy of our approach.

</details>


### [47] [MMGT: Motion Mask Guided Two-Stage Network for Co-Speech Gesture Video Generation](https://arxiv.org/abs/2505.23120)
*Siyuan Wang,Jiawei Liu,Wei Wang,Yeying Jin,Jinsong Du,Zhi Han*

Main category: cs.CV

TL;DR: MMGT模型通过两阶段网络（SMGA和MM-HAA）结合音频、运动掩码和运动特征，生成高质量的同步语音手势视频，解决了传统方法在细节控制和运动生成上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅依赖音频信号难以捕捉大幅手势动作，导致视频失真和伪影，限制了实际应用。

Method: 提出MMGT模型，分为两阶段：SMGA网络生成高质量姿势视频和运动掩码；MM-HAA结合稳定扩散视频生成模型，优化细节控制。

Result: 实验表明，模型在视频质量、唇同步和手势生成方面表现更优。

Conclusion: MMGT通过多模态输入和分层注意力机制，显著提升了语音手势视频的生成质量。

Abstract: Co-Speech Gesture Video Generation aims to generate vivid speech videos from
audio-driven still images, which is challenging due to the diversity of
different parts of the body in terms of amplitude of motion, audio relevance,
and detailed features. Relying solely on audio as the control signal often
fails to capture large gesture movements in video, leading to more pronounced
artifacts and distortions. Existing approaches typically address this issue by
introducing additional a priori information, but this can limit the practical
application of the task. Specifically, we propose a Motion Mask-Guided
Two-Stage Network (MMGT) that uses audio, as well as motion masks and motion
features generated from the audio signal to jointly drive the generation of
synchronized speech gesture videos. In the first stage, the Spatial Mask-Guided
Audio Pose Generation (SMGA) Network generates high-quality pose videos and
motion masks from audio, effectively capturing large movements in key regions
such as the face and gestures. In the second stage, we integrate the Motion
Masked Hierarchical Audio Attention (MM-HAA) into the Stabilized Diffusion
Video Generation model, overcoming limitations in fine-grained motion
generation and region-specific detail control found in traditional methods.
This guarantees high-quality, detailed upper-body video generation with
accurate texture and motion details. Evaluations show improved video quality,
lip-sync, and gesture. The model and code are available at
https://github.com/SIA-IDE/MMGT.

</details>


### [48] [HMAD: Advancing E2E Driving with Anchored Offset Proposals and Simulation-Supervised Multi-target Scoring](https://arxiv.org/abs/2505.23129)
*Bin Wang,Pingjun Li,Jinkun Liu,Jun Cheng,Hailong Lei,Yinze Rong,Huan-ang Gao,Kangliang Chen,Xing Pan,Weihao Gu*

Main category: cs.CV

TL;DR: HMAD框架通过BEV轨迹生成和学习的多标准评分，解决了自动驾驶中轨迹多样性和路径选择的问题，并在CVPR 2025测试集上取得了44.5%的驾驶分数。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在生成多样且合规的轨迹以及通过多维度评估选择最优路径方面存在挑战。

Method: HMAD结合BEVFormer和可学习锚定查询生成候选轨迹，并通过模拟监督评分模块评估轨迹的安全性、合规性和舒适度。

Result: 在CVPR 2025测试集上，HMAD的驾驶分数达到44.5%。

Conclusion: HMAD展示了将轨迹生成与安全评分解耦的优势，为高级自动驾驶提供了有效解决方案。

Abstract: End-to-end autonomous driving faces persistent challenges in both generating
diverse, rule-compliant trajectories and robustly selecting the optimal path
from these options via learned, multi-faceted evaluation. To address these
challenges, we introduce HMAD, a framework integrating a distinctive
Bird's-Eye-View (BEV) based trajectory proposal mechanism with learned
multi-criteria scoring. HMAD leverages BEVFormer and employs learnable anchored
queries, initialized from a trajectory dictionary and refined via iterative
offset decoding (inspired by DiffusionDrive), to produce numerous diverse and
stable candidate trajectories. A key innovation, our simulation-supervised
scorer module, then evaluates these proposals against critical metrics
including no at-fault collisions, drivable area compliance, comfortableness,
and overall driving quality (i.e., extended PDM score). Demonstrating its
efficacy, HMAD achieves a 44.5% driving score on the CVPR 2025 private test
set. This work highlights the benefits of effectively decoupling robust
trajectory generation from comprehensive, safety-aware learned scoring for
advanced autonomous driving.

</details>


### [49] [PhotoArtAgent: Intelligent Photo Retouching with Language Model-Based Artist Agents](https://arxiv.org/abs/2505.23130)
*Haoyu Chen,Keda Tao,Yizao Wang,Xinlei Wang,Lei Zhu,Jinjin Gu*

Main category: cs.CV

TL;DR: PhotoArtAgent是一个结合视觉语言模型和自然语言推理的智能系统，模拟专业艺术家的创作过程，提供透明且交互性强的照片修饰方案。


<details>
  <summary>Details</summary>
Motivation: 非专业用户依赖的自动化工具缺乏解释深度和交互透明性，而专业艺术家能通过调整创造独特视觉效果。PhotoArtAgent旨在填补这一差距。

Method: 系统结合视觉语言模型和自然语言推理，分析艺术性、规划修饰策略，并通过API输出参数至Lightroom，迭代优化直至达成艺术目标。

Result: 实验表明，PhotoArtAgent不仅优于现有自动化工具，还能达到专业艺术家的修饰水平。

Conclusion: PhotoArtAgent通过透明解释和用户交互，成功模拟专业艺术家的创作过程，提升了照片修饰的艺术性和可控性。

Abstract: Photo retouching is integral to photographic art, extending far beyond simple
technical fixes to heighten emotional expression and narrative depth. While
artists leverage expertise to create unique visual effects through deliberate
adjustments, non-professional users often rely on automated tools that produce
visually pleasing results but lack interpretative depth and interactive
transparency. In this paper, we introduce PhotoArtAgent, an intelligent system
that combines Vision-Language Models (VLMs) with advanced natural language
reasoning to emulate the creative process of a professional artist. The agent
performs explicit artistic analysis, plans retouching strategies, and outputs
precise parameters to Lightroom through an API. It then evaluates the resulting
images and iteratively refines them until the desired artistic vision is
achieved. Throughout this process, PhotoArtAgent provides transparent,
text-based explanations of its creative rationale, fostering meaningful
interaction and user control. Experimental results show that PhotoArtAgent not
only surpasses existing automated tools in user studies but also achieves
results comparable to those of professional human artists.

</details>


### [50] [Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing](https://arxiv.org/abs/2505.23134)
*Tongtong Su,Chengyu Wang,Jun Huang,Dongming Lu*

Main category: cs.CV

TL;DR: 论文提出了一种名为Zero-to-Hero的参考视频编辑方法，通过分解编辑过程为两个阶段，解决了现有文本引导方法的模糊性和细粒度控制不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导的视频编辑方法存在用户意图模糊和细粒度控制不足的问题，需要一种更精确和一致的方法。

Method: 方法分为两个阶段：Zero阶段通过编辑锚帧作为参考图像，并利用原始帧的对应关系引导注意力机制；Hero阶段通过条件生成模型进行视频修复。

Result: 在PSNR指标上优于最佳基线方法2.6 dB，并通过Blender构建的视频集验证了外观一致性。

Conclusion: Zero-to-Hero方法在视频编辑中实现了更高的准确性和时间一致性，解决了现有方法的局限性。

Abstract: Appearance editing according to user needs is a pivotal task in video
editing. Existing text-guided methods often lead to ambiguities regarding user
intentions and restrict fine-grained control over editing specific aspects of
objects. To overcome these limitations, this paper introduces a novel approach
named {Zero-to-Hero}, which focuses on reference-based video editing that
disentangles the editing process into two distinct problems. It achieves this
by first editing an anchor frame to satisfy user requirements as a reference
image and then consistently propagating its appearance across other frames. We
leverage correspondence within the original frames to guide the attention
mechanism, which is more robust than previously proposed optical flow or
temporal modules in memory-friendly video generative models, especially when
dealing with objects exhibiting large motions. It offers a solid ZERO-shot
initialization that ensures both accuracy and temporal consistency. However,
intervention in the attention mechanism results in compounded imaging
degradation with over-saturated colors and unknown blurring issues. Starting
from Zero-Stage, our Hero-Stage Holistically learns a conditional generative
model for vidEo RestOration. To accurately evaluate the consistency of the
appearance, we construct a set of videos with multiple appearances using
Blender, enabling a fine-grained and deterministic evaluation. Our method
outperforms the best-performing baseline with a PSNR improvement of 2.6 dB. The
project page is at https://github.com/Tonniia/Zero2Hero.

</details>


### [51] [Interpreting Chest X-rays Like a Radiologist: A Benchmark with Clinical Reasoning](https://arxiv.org/abs/2505.23143)
*Jinquan Guan,Qi Chen,Lizhou Liang,Yuhang Liu,Vu Minh Hieu Phan,Minh-Son To,Jian Chen,Yutong Xie*

Main category: cs.CV

TL;DR: 论文提出了CXRTrek数据集和CXRTrekNet模型，模拟放射科医生的多阶段诊断推理过程，解决了现有医学AI模型在胸片解读中缺乏上下文推理的问题。


<details>
  <summary>Details</summary>
Motivation: 现有医学AI模型在胸片解读中采用简单的输入-输出模式，忽略了诊断推理的序列性和上下文信息，导致与临床场景不匹配。

Method: 构建CXRTrek数据集，包含8个诊断阶段的42.8万样本和1100万问答对；提出CXRTrekNet模型，整合临床推理流程。

Result: CXRTrekNet在CXRTrek基准测试中优于现有医学VLLM，并在多个外部数据集上表现出优越的泛化能力。

Conclusion: CXRTrek数据集和模型成功模拟了临床诊断推理，显著提升了胸片解读的准确性和上下文感知能力。

Abstract: Artificial intelligence (AI)-based chest X-ray (CXR) interpretation
assistants have demonstrated significant progress and are increasingly being
applied in clinical settings. However, contemporary medical AI models often
adhere to a simplistic input-to-output paradigm, directly processing an image
and an instruction to generate a result, where the instructions may be integral
to the model's architecture. This approach overlooks the modeling of the
inherent diagnostic reasoning in chest X-ray interpretation. Such reasoning is
typically sequential, where each interpretive stage considers the images, the
current task, and the contextual information from previous stages. This
oversight leads to several shortcomings, including misalignment with clinical
scenarios, contextless reasoning, and untraceable errors. To fill this gap, we
construct CXRTrek, a new multi-stage visual question answering (VQA) dataset
for CXR interpretation. The dataset is designed to explicitly simulate the
diagnostic reasoning process employed by radiologists in real-world clinical
settings for the first time. CXRTrek covers 8 sequential diagnostic stages,
comprising 428,966 samples and over 11 million question-answer (Q&A) pairs,
with an average of 26.29 Q&A pairs per sample. Building on the CXRTrek dataset,
we propose a new vision-language large model (VLLM), CXRTrekNet, specifically
designed to incorporate the clinical reasoning flow into the VLLM framework.
CXRTrekNet effectively models the dependencies between diagnostic stages and
captures reasoning patterns within the radiological context. Trained on our
dataset, the model consistently outperforms existing medical VLLMs on the
CXRTrek benchmarks and demonstrates superior generalization across multiple
tasks on five diverse external datasets. The dataset and model can be found in
our repository (https://github.com/guanjinquan/CXRTrek).

</details>


### [52] [FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing](https://arxiv.org/abs/2505.23145)
*Jeongsol Kim,Yeobin Hong,Jong Chul Ye*

Main category: cs.CV

TL;DR: FlowAlign是一种基于流的无反转图像编辑框架，通过流匹配损失实现更稳定和一致的编辑轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的无反转图像编辑方法（如FlowEdit）虽然避免了潜在反转问题，但编辑轨迹不稳定且源一致性差。

Method: FlowAlign引入流匹配损失作为正则化机制，平衡编辑提示的语义对齐和源图像的结构一致性，并支持反向编辑。

Result: 实验表明，FlowAlign在源保留和编辑可控性方面优于现有方法。

Conclusion: FlowAlign通过流匹配损失实现了更稳定、一致的图像编辑，并支持可逆操作。

Abstract: Recent inversion-free, flow-based image editing methods such as FlowEdit
leverages a pre-trained noise-to-image flow model such as Stable Diffusion 3,
enabling text-driven manipulation by solving an ordinary differential equation
(ODE). While the lack of exact latent inversion is a core advantage of these
methods, it often results in unstable editing trajectories and poor source
consistency. To address this limitation, we propose FlowAlign, a novel
inversion-free flow-based framework for consistent image editing with
principled trajectory control. FlowAlign introduces a flow-matching loss as a
regularization mechanism to promote smoother and more stable trajectories
during the editing process. Notably, the flow-matching loss is shown to
explicitly balance semantic alignment with the edit prompt and structural
consistency with the source image along the trajectory. Furthermore, FlowAlign
naturally supports reverse editing by simply reversing the ODE trajectory,
highlighting the reversible and consistent nature of the transformation.
Extensive experiments demonstrate that FlowAlign outperforms existing methods
in both source preservation and editing controllability.

</details>


### [53] [PreFM: Online Audio-Visual Event Parsing via Predictive Future Modeling](https://arxiv.org/abs/2505.23155)
*Xiao Yu,Yan Fang,Xiaojie Jin,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: 论文提出了一种在线音频-视觉事件解析（On-AVEP）的新范式，通过预测未来建模（PreFM）框架实现实时高效的多模态视频理解。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖离线处理和大模型，限制了实时应用。

Method: 提出PreFM框架，包括预测未来多模态建模和模态无关的鲁棒表示，以提升上下文理解和实时效率。

Result: 在UnAV-100和LLP数据集上，PreFM显著优于现有方法，且参数更少。

Conclusion: PreFM为实时多模态视频理解提供了有效解决方案。

Abstract: Audio-visual event parsing plays a crucial role in understanding multimodal
video content, but existing methods typically rely on offline processing of
entire videos with huge model sizes, limiting their real-time applicability. We
introduce Online Audio-Visual Event Parsing (On-AVEP), a novel paradigm for
parsing audio, visual, and audio-visual events by sequentially analyzing
incoming video streams. The On-AVEP task necessitates models with two key
capabilities: (1) Accurate online inference, to effectively distinguish events
with unclear and limited context in online settings, and (2) Real-time
efficiency, to balance high performance with computational constraints. To
cultivate these, we propose the Predictive Future Modeling (PreFM) framework
featured by (a) predictive multimodal future modeling to infer and integrate
beneficial future audio-visual cues, thereby enhancing contextual understanding
and (b) modality-agnostic robust representation along with focal temporal
prioritization to improve precision and generalization. Extensive experiments
on the UnAV-100 and LLP datasets show PreFM significantly outperforms
state-of-the-art methods by a large margin with significantly fewer parameters,
offering an insightful approach for real-time multimodal video understanding.
Code is available at https://github.com/XiaoYu-1123/PreFM.

</details>


### [54] [LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient Rendering](https://arxiv.org/abs/2505.23158)
*Jonas Kulhanek,Marie-Julie Rakotosaona,Fabian Manhardt,Christina Tsalicoglou,Michael Niemeyer,Torsten Sattler,Songyou Peng,Federico Tombari*

Main category: cs.CV

TL;DR: 提出了一种新颖的3D高斯泼溅LOD方法，通过分层LOD表示和动态加载技术，显著降低渲染时间和GPU内存占用。


<details>
  <summary>Details</summary>
Motivation: 解决内存受限设备上大规模场景实时渲染的挑战。

Method: 采用分层LOD表示，结合深度感知3D平滑滤波、重要性剪枝和微调，动态加载空间分块数据。

Result: 在户外和室内数据集上实现最先进性能，降低延迟和内存需求。

Conclusion: 该方法高效且实用，适用于资源受限环境的高质量实时渲染。

Abstract: In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian
Splatting that enables real-time rendering of large-scale scenes on
memory-constrained devices. Our approach introduces a hierarchical LOD
representation that iteratively selects optimal subsets of Gaussians based on
camera distance, thus largely reducing both rendering time and GPU memory
usage. We construct each LOD level by applying a depth-aware 3D smoothing
filter, followed by importance-based pruning and fine-tuning to maintain visual
fidelity. To further reduce memory overhead, we partition the scene into
spatial chunks and dynamically load only relevant Gaussians during rendering,
employing an opacity-blending mechanism to avoid visual artifacts at chunk
boundaries. Our method achieves state-of-the-art performance on both outdoor
(Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality
renderings with reduced latency and memory requirements.

</details>


### [55] [Implicit Inversion turns CLIP into a Decoder](https://arxiv.org/abs/2505.23161)
*Antonio D'Orazio,Maria Rosaria Briglia,Donato Crisostomi,Dario Loi,Emanuele Rodolà,Iacopo Masi*

Main category: cs.CV

TL;DR: CLIP模型无需解码器或训练即可实现图像合成，通过优化频率感知的隐式神经表示和引入稳定技术，解锁了文本到图像生成等功能。


<details>
  <summary>Details</summary>
Motivation: 探索CLIP模型在无需额外训练或解码器的情况下，是否具备生成图像的潜力。

Method: 采用频率感知的隐式神经表示，结合对抗性鲁棒初始化、正交投影和混合损失等技术，优化图像合成过程。

Result: 成功实现了文本到图像生成、风格迁移和图像重建等功能，无需修改CLIP的权重。

Conclusion: 判别式模型可能隐藏着未被开发的生成潜力。

Abstract: CLIP is a discriminative model trained to align images and text in a shared
embedding space. Due to its multimodal structure, it serves as the backbone of
many generative pipelines, where a decoder is trained to map from the shared
space back to images. In this work, we show that image synthesis is
nevertheless possible using CLIP alone -- without any decoder, training, or
fine-tuning. Our approach optimizes a frequency-aware implicit neural
representation that encourages coarse-to-fine generation by stratifying
frequencies across network layers. To stabilize this inverse mapping, we
introduce adversarially robust initialization, a lightweight Orthogonal
Procrustes projection to align local text and image embeddings, and a blending
loss that anchors outputs to natural image statistics. Without altering CLIP's
weights, this framework unlocks capabilities such as text-to-image generation,
style transfer, and image reconstruction. These findings suggest that
discriminative models may hold untapped generative potential, hidden in plain
sight.

</details>


### [56] [RoboTransfer: Geometry-Consistent Video Diffusion for Robotic Visual Policy Transfer](https://arxiv.org/abs/2505.23171)
*Liu Liu,Xiaofeng Wang,Guosheng Zhao,Keyu Li,Wenkang Qin,Jiaxiong Qiu,Zheng Zhu,Guan Huang,Zhizhong Su*

Main category: cs.CV

TL;DR: RoboTransfer是一个基于扩散的视频生成框架，用于机器人数据合成，解决了模拟到现实的差距问题，并通过多视角几何和场景组件控制提升了数据质量。


<details>
  <summary>Details</summary>
Motivation: 模仿学习在机器人操作中很重要，但收集大规模真实演示成本高，模拟器又存在模拟到现实的差距问题。

Method: RoboTransfer结合多视角几何和场景组件控制，通过交叉视角特征交互和全局深度/法线条件确保几何一致性。

Result: 实验显示，RoboTransfer生成的多视角视频具有更好的几何一致性和视觉保真度，训练的策略在DIFF-OBJ和DIFF-ALL场景中分别提升了33.3%和251%的成功率。

Conclusion: RoboTransfer为机器人数据合成提供了高效且可控的解决方案，显著提升了模仿学习的性能。

Abstract: Imitation Learning has become a fundamental approach in robotic manipulation.
However, collecting large-scale real-world robot demonstrations is
prohibitively expensive. Simulators offer a cost-effective alternative, but the
sim-to-real gap make it extremely challenging to scale. Therefore, we introduce
RoboTransfer, a diffusion-based video generation framework for robotic data
synthesis. Unlike previous methods, RoboTransfer integrates multi-view geometry
with explicit control over scene components, such as background and object
attributes. By incorporating cross-view feature interactions and global
depth/normal conditions, RoboTransfer ensures geometry consistency across
views. This framework allows fine-grained control, including background edits
and object swaps. Experiments demonstrate that RoboTransfer is capable of
generating multi-view videos with enhanced geometric consistency and visual
fidelity. In addition, policies trained on the data generated by RoboTransfer
achieve a 33.3% relative improvement in the success rate in the DIFF-OBJ
setting and a substantial 251% relative improvement in the more challenging
DIFF-ALL scenario. Explore more demos on our project page:
https://horizonrobotics.github.io/robot_lab/robotransfer

</details>


### [57] [DIP-R1: Deep Inspection and Perception with RL Looking Through and Understanding Complex Scenes](https://arxiv.org/abs/2505.23179)
*Sungjune Park,Hyunjun Kim,Junho Kim,Seongho Kim,Yong Man Ro*

Main category: cs.CV

TL;DR: 论文提出了一种基于强化学习的框架DIP-R1，用于增强多模态大语言模型（MLLMs）在复杂场景中的细粒度视觉感知能力。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在视觉理解方面表现出色，但在复杂现实场景（如密集人群）中的细粒度感知仍有限，因此探索如何通过强化学习提升其能力。

Method: 开发了DIP-R1框架，通过三种基于规则的奖励模型指导MLLMs：标准推理奖励、方差引导观察奖励和加权精确召回奖励。

Result: DIP-R1在多种细粒度目标检测数据上表现优异，显著优于现有基线模型和监督微调方法。

Conclusion: 研究表明，将强化学习融入MLLMs可显著提升其在复杂现实感知任务中的能力。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant visual
understanding capabilities, yet their fine-grained visual perception in complex
real-world scenarios, such as densely crowded public areas, remains limited.
Inspired by the recent success of reinforcement learning (RL) in both LLMs and
MLLMs, in this paper, we explore how RL can enhance visual perception ability
of MLLMs. Then we develop a novel RL-based framework, Deep Inspection and
Perception with RL (DIP-R1) designed to enhance the visual perception
capabilities of MLLMs, by comprehending complex scenes and looking through
visual instances closely. DIP-R1 guides MLLMs through detailed inspection of
visual scene via three simply designed rule-based reward modelings. First, we
adopt a standard reasoning reward encouraging the model to include three
step-by-step processes: 1) reasoning for understanding visual scenes, 2)
observing for looking through interested but ambiguous regions, and 3)
decision-making for predicting answer. Second, a variance-guided looking reward
is designed to examine uncertain regions for the second observing process. It
explicitly enables the model to inspect ambiguous areas, improving its ability
to mitigate perceptual uncertainties. Third, we model a weighted
precision-recall accuracy reward enhancing accurate decision-making. We explore
its effectiveness across diverse fine-grained object detection data consisting
of challenging real-world environments, such as densely crowded scenes. Built
upon existing MLLMs, DIP-R1 achieves consistent and significant improvement
across various in-domain and out-of-domain scenarios. It also outperforms
various existing baseline models and supervised fine-tuning methods. Our
findings highlight the substantial potential of integrating RL into MLLMs for
enhancing capabilities in complex real-world perception tasks.

</details>


### [58] [HiGarment: Cross-modal Harmony Based Diffusion Model for Flat Sketch to Realistic Garment Image](https://arxiv.org/abs/2505.23186)
*Junyi Guo,Jingxuan Zhang,Fangyu Wu,Huanda Lu,Qiufeng Wang,Wenmian Yang,Eng Gee Lim,Dongming Lu*

Main category: cs.CV

TL;DR: 论文提出了一种新任务FS2RG，通过结合平面草图和文本指导生成真实服装图像，并提出了HiGarment框架解决其中的挑战。


<details>
  <summary>Details</summary>
Motivation: 填补服装生产过程中基于扩散模型的合成任务的研究空白。

Method: HiGarment框架包含多模态语义增强机制和协调交叉注意力机制。

Result: 实验和用户研究证明了HiGarment的有效性。

Conclusion: HiGarment在服装合成任务中表现优异，代码和数据集将公开。

Abstract: Diffusion-based garment synthesis tasks primarily focus on the design phase
in the fashion domain, while the garment production process remains largely
underexplored. To bridge this gap, we introduce a new task: Flat Sketch to
Realistic Garment Image (FS2RG), which generates realistic garment images by
integrating flat sketches and textual guidance. FS2RG presents two key
challenges: 1) fabric characteristics are solely guided by textual prompts,
providing insufficient visual supervision for diffusion-based models, which
limits their ability to capture fine-grained fabric details; 2) flat sketches
and textual guidance may provide conflicting information, requiring the model
to selectively preserve or modify garment attributes while maintaining
structural coherence. To tackle this task, we propose HiGarment, a novel
framework that comprises two core components: i) a multi-modal semantic
enhancement mechanism that enhances fabric representation across textual and
visual modalities, and ii) a harmonized cross-attention mechanism that
dynamically balances information from flat sketches and text prompts, allowing
controllable synthesis by generating either sketch-aligned (image-biased) or
text-guided (text-biased) outputs. Furthermore, we collect Multi-modal Detailed
Garment, the largest open-source dataset for garment generation. Experimental
results and user studies demonstrate the effectiveness of HiGarment in garment
synthesis. The code and dataset will be released.

</details>


### [59] [Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks](https://arxiv.org/abs/2505.23192)
*Run Hao,Peng Ying*

Main category: cs.CV

TL;DR: 提出了一种基于语法树和蒙特卡洛树搜索的对抗性提示生成框架，用于逃避AIGC检测器，并在实际竞赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像模型生成的肖像可能被滥用的问题，以及测试AIGC检测器的鲁棒性。

Method: 利用语法树结构和蒙特卡洛树搜索变体，系统探索语义提示空间，生成多样且可控的对抗性提示。

Result: 方法在多个T2I模型上验证有效，并在实际对抗性AIGC检测竞赛中排名第一。

Conclusion: 该方法不仅可用于攻击场景，还能构建高质量对抗数据集，助力更鲁棒的AIGC检测与防御系统开发。

Abstract: The rise of text-to-image (T2I) models has enabled the synthesis of
photorealistic human portraits, raising serious concerns about identity misuse
and the robustness of AIGC detectors. In this work, we propose an automated
adversarial prompt generation framework that leverages a grammar tree structure
and a variant of the Monte Carlo tree search algorithm to systematically
explore the semantic prompt space. Our method generates diverse, controllable
prompts that consistently evade both open-source and commercial AIGC detectors.
Extensive experiments across multiple T2I models validate its effectiveness,
and the approach ranked first in a real-world adversarial AIGC detection
competition. Beyond attack scenarios, our method can also be used to construct
high-quality adversarial datasets, providing valuable resources for training
and evaluating more robust AIGC detection and defense systems.

</details>


### [60] [Language-guided Learning for Object Detection Tackling Multiple Variations in Aerial Images](https://arxiv.org/abs/2505.23193)
*Sungjune Park,Hyunjun Kim,Beomchan Park,Yong Man Ro*

Main category: cs.CV

TL;DR: 论文提出了一种名为LANGO的语言引导对象检测框架，用于解决航空图像中因光照和视角变化导致的对象检测挑战。


<details>
  <summary>Details</summary>
Motivation: 航空图像中的对象检测面临多种变化（如光照和视角）带来的挑战，导致对象定位和类别识别复杂化。

Method: 设计了视觉语义推理器以理解图像场景的语义，并提出关系学习损失来处理实例级变化（如视角和尺度）。

Result: 通过实验验证，该方法显著提升了检测性能。

Conclusion: LANGO框架有效缓解了航空图像中场景和实例级变化对对象检测的影响。

Abstract: Despite recent advancements in computer vision research, object detection in
aerial images still suffers from several challenges. One primary challenge to
be mitigated is the presence of multiple types of variation in aerial images,
for example, illumination and viewpoint changes. These variations result in
highly diverse image scenes and drastic alterations in object appearance, so
that it becomes more complicated to localize objects from the whole image scene
and recognize their categories. To address this problem, in this paper, we
introduce a novel object detection framework in aerial images, named
LANGuage-guided Object detection (LANGO). Upon the proposed language-guided
learning, the proposed framework is designed to alleviate the impacts from both
scene and instance-level variations. First, we are motivated by the way humans
understand the semantics of scenes while perceiving environmental factors in
the scenes (e.g., weather). Therefore, we design a visual semantic reasoner
that comprehends visual semantics of image scenes by interpreting conditions
where the given images were captured. Second, we devise a training objective,
named relation learning loss, to deal with instance-level variations, such as
viewpoint angle and scale changes. This training objective aims to learn
relations in language representations of object categories, with the help of
the robust characteristics against such variations. Through extensive
experiments, we demonstrate the effectiveness of the proposed method, and our
method obtains noticeable detection performance improvements.

</details>


### [61] [WTEFNet: Real-Time Low-Light Object Detection for Advanced Driver-Assistance Systems](https://arxiv.org/abs/2505.23201)
*Hao Wu,Junzhou Chen,Ronghui Zhang,Nengchao Lyu,Hongyu Hu,Yanyong Guo,Tony Z. Qiu*

Main category: cs.CV

TL;DR: WTEFNet是一个专为低光场景设计的实时目标检测框架，包含低光增强、小波特征提取和自适应融合检测模块，在多个数据集上表现优异，适合实时ADAS应用。


<details>
  <summary>Details</summary>
Motivation: 解决现有RGB摄像头在低光条件下性能下降的问题，提升ADAS系统的环境感知能力。

Method: 提出WTEFNet框架，包含低光增强（LLE）、小波特征提取（WFE）和自适应融合检测（AFFD）三个核心模块。

Result: 在BDD100K、SHIFT、nuScenes和GSN数据集上达到最先进精度，并在嵌入式平台上验证了实时性。

Conclusion: WTEFNet在低光条件下表现出色，适合实时ADAS应用。

Abstract: Object detection is a cornerstone of environmental perception in advanced
driver assistance systems(ADAS). However, most existing methods rely on RGB
cameras, which suffer from significant performance degradation under low-light
conditions due to poor image quality. To address this challenge, we proposes
WTEFNet, a real-time object detection framework specifically designed for
low-light scenarios, with strong adaptability to mainstream detectors. WTEFNet
comprises three core modules: a Low-Light Enhancement (LLE) module, a
Wavelet-based Feature Extraction (WFE) module, and an Adaptive Fusion Detection
(AFFD) module. The LLE enhances dark regions while suppressing overexposed
areas; the WFE applies multi-level discrete wavelet transforms to isolate high-
and low-frequency components, enabling effective denoising and structural
feature retention; the AFFD fuses semantic and illumination features for robust
detection. To support training and evaluation, we introduce GSN, a manually
annotated dataset covering both clear and rainy night-time scenes. Extensive
experiments on BDD100K, SHIFT, nuScenes, and GSN demonstrate that WTEFNet
achieves state-of-the-art accuracy under low-light conditions. Furthermore,
deployment on a embedded platform (NVIDIA Jetson AGX Orin) confirms the
framework's suitability for real-time ADAS applications.

</details>


### [62] [HyperPointFormer: Multimodal Fusion in 3D Space with Dual-Branch Cross-Attention Transformers](https://arxiv.org/abs/2505.23206)
*Aldino Rizaldy,Richard Gloaguen,Fabian Ewald Fassnacht,Pedram Ghamisi*

Main category: cs.CV

TL;DR: 提出了一种基于3D点云的多模态融合方法，使用双分支Transformer模型直接学习几何和光谱特征，并通过跨注意力机制实现多尺度特征融合。


<details>
  <summary>Details</summary>
Motivation: 现有方法将3D数据降维到2D处理，未能充分利用3D数据的潜力，限制了模型的3D特征学习和预测能力。

Method: 提出了一种完全基于3D点云的多模态融合方法，采用双分支Transformer模型和跨注意力机制。

Result: 在多个数据集上验证，3D融合方法性能与2D方法相当，且能生成3D预测。

Conclusion: 3D融合方法不仅性能优越，还提供了更灵活的预测能力，支持从3D到2D的投影。

Abstract: Multimodal remote sensing data, including spectral and lidar or
photogrammetry, is crucial for achieving satisfactory land-use / land-cover
classification results in urban scenes. So far, most studies have been
conducted in a 2D context. When 3D information is available in the dataset, it
is typically integrated with the 2D data by rasterizing the 3D data into 2D
formats. Although this method yields satisfactory classification results, it
falls short in fully exploiting the potential of 3D data by restricting the
model's ability to learn 3D spatial features directly from raw point clouds.
Additionally, it limits the generation of 3D predictions, as the dimensionality
of the input data has been reduced. In this study, we propose a fully 3D-based
method that fuses all modalities within the 3D point cloud and employs a
dedicated dual-branch Transformer model to simultaneously learn geometric and
spectral features. To enhance the fusion process, we introduce a
cross-attention-based mechanism that fully operates on 3D points, effectively
integrating features from various modalities across multiple scales. The
purpose of cross-attention is to allow one modality to assess the importance of
another by weighing the relevant features. We evaluated our method by comparing
it against both 3D and 2D methods using the 2018 IEEE GRSS Data Fusion Contest
(DFC2018) dataset. Our findings indicate that 3D fusion delivers competitive
results compared to 2D methods and offers more flexibility by providing 3D
predictions. These predictions can be projected onto 2D maps, a capability that
is not feasible in reverse. Additionally, we evaluated our method on different
datasets, specifically the ISPRS Vaihingen 3D and the IEEE 2019 Data Fusion
Contest. Our code will be published here:
https://github.com/aldinorizaldy/hyperpointformer.

</details>


### [63] [Navigating the Accuracy-Size Trade-Off with Flexible Model Merging](https://arxiv.org/abs/2505.23209)
*Akash Dhasade,Divyansh Jhunjhunwala,Milos Vujasinovic,Gauri Joshi,Anne-Marie Kermarrec*

Main category: cs.CV

TL;DR: FlexMerge是一种无需数据的新型模型合并框架，灵活生成不同大小的合并模型，平衡精度与成本。


<details>
  <summary>Details</summary>
Motivation: 解决单一合并模型精度不足与部署多个独立模型成本高的问题。

Method: 将微调模型视为顺序块集合，逐步合并，支持多种合并算法。

Result: 实验表明，适度增大的合并模型能显著提升精度，优于单一模型。

Conclusion: FlexMerge为多样化部署提供了灵活、高效且无需数据的解决方案。

Abstract: Model merging has emerged as an efficient method to combine multiple
single-task fine-tuned models. The merged model can enjoy multi-task
capabilities without expensive training. While promising, merging into a single
model often suffers from an accuracy gap with respect to individual fine-tuned
models. On the other hand, deploying all individual fine-tuned models incurs
high costs. We propose FlexMerge, a novel data-free model merging framework to
flexibly generate merged models of varying sizes, spanning the spectrum from a
single merged model to retaining all individual fine-tuned models. FlexMerge
treats fine-tuned models as collections of sequential blocks and progressively
merges them using any existing data-free merging method, halting at a desired
size. We systematically explore the accuracy-size trade-off exhibited by
different merging algorithms in combination with FlexMerge. Extensive
experiments on vision and NLP benchmarks, with up to 30 tasks, reveal that even
modestly larger merged models can provide substantial accuracy improvements
over a single model. By offering fine-grained control over fused model size,
FlexMerge provides a flexible, data-free, and high-performance solution for
diverse deployment scenarios.

</details>


### [64] [SAMamba: Adaptive State Space Modeling with Hierarchical Vision for Infrared Small Target Detection](https://arxiv.org/abs/2505.23214)
*Wenhao Xu,Shuchen Zheng,Changwei Wang,Zherui Zhang,Chuan Ren,Rongtao Xu,Shibiao Xu*

Main category: cs.CV

TL;DR: SAMamba是一种结合SAM2层次特征学习和Mamba选择性序列建模的新框架，通过FS-Adapter、CSI模块和DPCF模块解决了红外小目标检测中的信息丢失和全局上下文建模问题。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测在军事和预警应用中至关重要，但现有深度学习方法存在信息丢失和全局上下文建模效率低的问题。

Method: 提出SAMamba框架，包含FS-Adapter用于域适应、CSI模块用于全局上下文建模、DPCF模块用于多尺度特征融合。

Result: 在多个数据集上显著优于现有方法，尤其在复杂背景和多尺度目标场景中表现突出。

Conclusion: SAMamba通过创新模块设计有效解决了红外小目标检测的核心挑战，具有实际应用潜力。

Abstract: Infrared small target detection (ISTD) is vital for long-range surveillance
in military, maritime, and early warning applications. ISTD is challenged by
targets occupying less than 0.15% of the image and low distinguishability from
complex backgrounds. Existing deep learning methods often suffer from
information loss during downsampling and inefficient global context modeling.
This paper presents SAMamba, a novel framework integrating SAM2's hierarchical
feature learning with Mamba's selective sequence modeling. Key innovations
include: (1) A Feature Selection Adapter (FS-Adapter) for efficient
natural-to-infrared domain adaptation via dual-stage selection (token-level
with a learnable task embedding and channel-wise adaptive transformations); (2)
A Cross-Channel State-Space Interaction (CSI) module for efficient global
context modeling with linear complexity using selective state space modeling;
and (3) A Detail-Preserving Contextual Fusion (DPCF) module that adaptively
combines multi-scale features with a gating mechanism to balance
high-resolution and low-resolution feature contributions. SAMamba addresses
core ISTD challenges by bridging the domain gap, maintaining fine-grained
details, and efficiently modeling long-range dependencies. Experiments on
NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets show SAMamba significantly
outperforms state-of-the-art methods, especially in challenging scenarios with
heterogeneous backgrounds and varying target scales. Code:
https://github.com/zhengshuchen/SAMamba.

</details>


### [65] [UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes](https://arxiv.org/abs/2505.23253)
*Yixun Liang,Kunming Luo,Xiao Chen,Rui Chen,Hongyu Yan,Weiyu Li,Jiarui Liu,Ping Tan*

Main category: cs.CV

TL;DR: UniTEX是一种新颖的两阶段3D纹理生成框架，通过直接在统一的3D功能空间中操作，避免了UV映射的局限性，生成高质量、一致的3D纹理。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖UV映射进行纹理细化，但存在拓扑模糊性问题。UniTEX旨在绕过UV映射的限制，直接在3D空间中生成纹理。

Method: 1. 提出纹理函数（TFs）将纹理生成提升到3D空间；2. 使用基于Transformer的大规模纹理模型（LTM）从图像和几何输入预测TFs；3. 采用LoRA策略高效适配扩散变换器（DiTs）进行高质量多视图纹理合成。

Result: 实验表明，UniTEX在视觉质量和纹理完整性上优于现有方法，提供了可扩展的自动化3D纹理生成解决方案。

Conclusion: UniTEX通过创新的3D功能空间方法，解决了现有技术的局限性，为3D纹理生成提供了高效且高质量的方案。

Abstract: We present UniTEX, a novel two-stage 3D texture generation framework to
create high-quality, consistent textures for 3D assets. Existing approaches
predominantly rely on UV-based inpainting to refine textures after reprojecting
the generated multi-view images onto the 3D shapes, which introduces challenges
related to topological ambiguity. To address this, we propose to bypass the
limitations of UV mapping by operating directly in a unified 3D functional
space. Specifically, we first propose that lifts texture generation into 3D
space via Texture Functions (TFs)--a continuous, volumetric representation that
maps any 3D point to a texture value based solely on surface proximity,
independent of mesh topology. Then, we propose to predict these TFs directly
from images and geometry inputs using a transformer-based Large Texturing Model
(LTM). To further enhance texture quality and leverage powerful 2D priors, we
develop an advanced LoRA-based strategy for efficiently adapting large-scale
Diffusion Transformers (DiTs) for high-quality multi-view texture synthesis as
our first stage. Extensive experiments demonstrate that UniTEX achieves
superior visual quality and texture integrity compared to existing approaches,
offering a generalizable and scalable solution for automated 3D texture
generation. Code will available in: https://github.com/YixunLiang/UniTEX.

</details>


### [66] [Image Aesthetic Reasoning: A New Benchmark for Medical Image Screening with MLLMs](https://arxiv.org/abs/2505.23265)
*Zheng Sun,Yi Wei,Long Yu*

Main category: cs.CV

TL;DR: 论文提出了一种结合数据和方法的解决方案，以提升多模态大语言模型（MLLMs）在医学图像筛选中的美学推理能力，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在图像美学推理方面表现不佳，尤其是医学图像筛选领域缺乏数据和模型能力不足。

Method: 收集了包含1500+样本的医学图像数据集，并提出了DPA-GRPO方法（结合长链思维和动态比例准确度奖励的强化学习）来提升MLLMs的美学推理能力。

Result: 实验表明，即使是最先进的闭源MLLMs（如GPT-4o和Qwen-VL-Max）在美学推理上表现接近随机猜测，而提出的方法能以更小的模型超越这些大模型的性能。

Conclusion: 该研究为图像美学推理提供了一种新的解决方案，并希望未来能成为医学图像筛选的常规配置。

Abstract: Multimodal Large Language Models (MLLMs) are of great application across many
domains, such as multimodal understanding and generation. With the development
of diffusion models (DM) and unified MLLMs, the performance of image generation
has been significantly improved, however, the study of image screening is rare
and its performance with MLLMs is unsatisfactory due to the lack of data and
the week image aesthetic reasoning ability in MLLMs. In this work, we propose a
complete solution to address these problems in terms of data and methodology.
For data, we collect a comprehensive medical image screening dataset with 1500+
samples, each sample consists of a medical image, four generated images, and a
multiple-choice answer. The dataset evaluates the aesthetic reasoning ability
under four aspects: \textit{(1) Appearance Deformation, (2) Principles of
Physical Lighting and Shadow, (3) Placement Layout, (4) Extension Rationality}.
For methodology, we utilize long chains of thought (CoT) and Group Relative
Policy Optimization with Dynamic Proportional Accuracy reward, called DPA-GRPO,
to enhance the image aesthetic reasoning ability of MLLMs. Our experimental
results reveal that even state-of-the-art closed-source MLLMs, such as GPT-4o
and Qwen-VL-Max, exhibit performance akin to random guessing in image aesthetic
reasoning. In contrast, by leveraging the reinforcement learning approach, we
are able to surpass the score of both large-scale models and leading
closed-source models using a much smaller model. We hope our attempt on medical
image screening will serve as a regular configuration in image aesthetic
reasoning in the future.

</details>


### [67] [Unsupervised Transcript-assisted Video Summarization and Highlight Detection](https://arxiv.org/abs/2505.23268)
*Spyros Barbakos,Charalampos Antoniadis,Gerasimos Potamianos,Gianluca Setti*

Main category: cs.CV

TL;DR: 论文提出了一种结合视频帧和转录文本的多模态RL框架，用于视频摘要和高光检测，优于仅依赖视觉内容的方法。


<details>
  <summary>Details</summary>
Motivation: 视频消费是日常生活的重要组成部分，但观看完整视频可能乏味。现有方法未将视频帧和转录文本结合在RL框架中。

Method: 提出多模态管道，利用视频帧和转录文本，通过RL框架训练模型，奖励生成多样且具代表性的摘要。

Result: 实验表明，结合转录文本的视频摘要和高光检测优于仅依赖视觉内容的方法。

Conclusion: 多模态RL框架在视频摘要和高光检测中表现优越，且能利用大规模未标注数据训练。

Abstract: Video consumption is a key part of daily life, but watching entire videos can
be tedious. To address this, researchers have explored video summarization and
highlight detection to identify key video segments. While some works combine
video frames and transcripts, and others tackle video summarization and
highlight detection using Reinforcement Learning (RL), no existing work, to the
best of our knowledge, integrates both modalities within an RL framework. In
this paper, we propose a multimodal pipeline that leverages video frames and
their corresponding transcripts to generate a more condensed version of the
video and detect highlights using a modality fusion mechanism. The pipeline is
trained within an RL framework, which rewards the model for generating diverse
and representative summaries while ensuring the inclusion of video segments
with meaningful transcript content. The unsupervised nature of the training
allows for learning from large-scale unannotated datasets, overcoming the
challenge posed by the limited size of existing annotated datasets. Our
experiments show that using the transcript in video summarization and highlight
detection achieves superior results compared to relying solely on the visual
content of the video.

</details>


### [68] [LADA: Scalable Label-Specific CLIP Adapter for Continual Learning](https://arxiv.org/abs/2505.23271)
*Mao-Lin Luo,Zi-Hao Zhou,Tong Wei,Min-Ling Zhang*

Main category: cs.CV

TL;DR: LADA（Label-specific ADApter）是一种用于持续学习的轻量级方法，通过为冻结的CLIP图像编码器添加标签特定的记忆单元，避免参数分区错误，并通过特征蒸馏防止灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有基于CLIP的方法在持续学习中通过分区参数适应任务，容易导致推理错误和性能下降。LADA旨在解决这一问题。

Method: LADA在冻结的CLIP图像编码器后添加标签特定的记忆单元，通过特征蒸馏防止新类别干扰已学类别。

Result: LADA在持续学习任务中实现了最先进的性能。

Conclusion: LADA通过轻量级设计和特征蒸馏，有效解决了持续学习中的参数分区和遗忘问题。

Abstract: Continual learning with vision-language models like CLIP offers a pathway
toward scalable machine learning systems by leveraging its transferable
representations. Existing CLIP-based methods adapt the pre-trained image
encoder by adding multiple sets of learnable parameters, with each task using a
partial set of parameters. This requires selecting the expected parameters for
input images during inference, which is prone to error that degrades
performance. To address this problem, we introduce LADA (Label-specific
ADApter). Instead of partitioning parameters across tasks, LADA appends
lightweight, label-specific memory units to the frozen CLIP image encoder,
enabling discriminative feature generation by aggregating task-agnostic
knowledge. To prevent catastrophic forgetting, LADA employs feature
distillation for seen classes, preventing their features from being interfered
with by new classes. Positioned after the image encoder, LADA prevents gradient
flow to the frozen CLIP parameters, ensuring efficient training. Extensive
results show that LADA achieves state-of-the-art performance in continual
learning settings. The implementation code is available at
https://github.com/MaolinLuo/LADA.

</details>


### [69] [Are MLMs Trapped in the Visual Room?](https://arxiv.org/abs/2505.23272)
*Yazhou Zhang,Chunwang Zou,Qimeng Liu,Lu Rong,Ben Yao,Zheng Lian,Qiuchi Li,Peng Zhang,Jing Qin*

Main category: cs.CV

TL;DR: 论文探讨多模态大模型（MLMs）是否能真正“理解”图像，提出“视觉房间”论点，并通过感知与认知双层次评估框架验证其局限性。


<details>
  <summary>Details</summary>
Motivation: 挑战现有假设，即感知能力等同于真正理解，通过实验验证MLMs在感知与认知任务中的表现差异。

Method: 提出双层次评估框架（感知与认知），并构建高质量多模态讽刺数据集，评估8种SoTA MLMs。

Result: MLMs在感知任务表现良好，但在讽刺理解上平均错误率达16.1%，揭示感知与理解的差距。

Conclusion: 支持“视觉房间”论点，为MLMs提供新评估范式，强调情感推理、常识推断和上下文对齐的不足。

Abstract: Can multi-modal large models (MLMs) that can ``see'' an image be said to
``understand'' it? Drawing inspiration from Searle's Chinese Room, we propose
the \textbf{Visual Room} argument: a system may process and describe every
detail of visual inputs by following algorithmic rules, without genuinely
comprehending the underlying intention. This dilemma challenges the prevailing
assumption that perceptual mastery implies genuine understanding. In
implementation, we introduce a two-tier evaluation framework spanning
perception and cognition. The perception component evaluates whether MLMs can
accurately capture the surface-level details of visual contents, where the
cognitive component examines their ability to infer sarcasm polarity. To
support this framework, We further introduce a high-quality multi-modal sarcasm
dataset comprising both 924 static images and 100 dynamic videos. All sarcasm
labels are annotated by the original authors and verified by independent
reviewers to ensure clarity and consistency. We evaluate eight state-of-the-art
(SoTA) MLMs. Our results highlight three key findings: (1) MLMs perform well on
perception tasks; (2) even with correct perception, models exhibit an average
error rate of ~16.1\% in sarcasm understanding, revealing a significant gap
between seeing and understanding; (3) error analysis attributes this gap to
deficiencies in emotional reasoning, commonsense inference, and context
alignment. This work provides empirical grounding for the proposed Visual Room
argument and offers a new evaluation paradigm for MLMs.

</details>


### [70] [Holistic Large-Scale Scene Reconstruction via Mixed Gaussian Splatting](https://arxiv.org/abs/2505.23280)
*Chuandong Liu,Huijiao Wang,Lei Yu,Gui-Song Xia*

Main category: cs.CV

TL;DR: MixGS提出了一种新的整体优化框架，用于大规模3D场景重建，解决了现有方法因分治策略导致的全局信息丢失和复杂参数调整问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模场景重建方法依赖分治策略，导致全局信息丢失且需要复杂参数调整，MixGS旨在解决这些问题。

Method: MixGS通过将相机姿态和高斯属性整合为视图感知表示，并引入混合操作，同时保持全局一致性和局部保真度。

Result: 实验表明，MixGS在大规模场景中实现了最先进的渲染质量和竞争性速度，显著降低了计算需求。

Conclusion: MixGS是一种高效的大规模3D场景重建方法，能够在单个24GB显存的GPU上训练。

Abstract: Recent advances in 3D Gaussian Splatting have shown remarkable potential for
novel view synthesis. However, most existing large-scale scene reconstruction
methods rely on the divide-and-conquer paradigm, which often leads to the loss
of global scene information and requires complex parameter tuning due to scene
partitioning and local optimization. To address these limitations, we propose
MixGS, a novel holistic optimization framework for large-scale 3D scene
reconstruction. MixGS models the entire scene holistically by integrating
camera pose and Gaussian attributes into a view-aware representation, which is
decoded into fine-detailed Gaussians. Furthermore, a novel mixing operation
combines decoded and original Gaussians to jointly preserve global coherence
and local fidelity. Extensive experiments on large-scale scenes demonstrate
that MixGS achieves state-of-the-art rendering quality and competitive speed,
while significantly reducing computational requirements, enabling large-scale
scene reconstruction training on a single 24GB VRAM GPU. The code will be
released at https://github.com/azhuantou/MixGS.

</details>


### [71] [RSFAKE-1M: A Large-Scale Dataset for Detecting Diffusion-Generated Remote Sensing Forgeries](https://arxiv.org/abs/2505.23283)
*Zhihong Tan,Jiayi Wang,Huiying Shi,Binyuan Huang,Hongchen Wei,Zhenzhong Chen*

Main category: cs.CV

TL;DR: 论文介绍了RSFAKE-1M数据集，用于检测基于扩散模型的伪造遥感图像，填补了现有研究的空白，并展示了其对于提升检测方法泛化性和鲁棒性的重要性。


<details>
  <summary>Details</summary>
Motivation: 遥感图像在环境监测、城市规划等领域至关重要，但现有伪造检测方法主要针对GAN或自然图像，缺乏对扩散模型生成伪造图像的研究。

Method: 构建了包含50万伪造和50万真实遥感图像的RSFAKE-1M数据集，伪造图像由10种扩散模型生成，涵盖多种生成条件。使用现有检测器和统一基线进行实验评估。

Result: 实验表明，当前方法对基于扩散模型的伪造遥感图像检测效果有限，而基于RSFAKE-1M训练的模型在泛化性和鲁棒性上显著提升。

Conclusion: RSFAKE-1M为遥感图像伪造检测领域的研究提供了重要基础，推动了下一代检测方法的发展。

Abstract: Detecting forged remote sensing images is becoming increasingly critical, as
such imagery plays a vital role in environmental monitoring, urban planning,
and national security. While diffusion models have emerged as the dominant
paradigm for image generation, their impact on remote sensing forgery detection
remains underexplored. Existing benchmarks primarily target GAN-based forgeries
or focus on natural images, limiting progress in this critical domain. To
address this gap, we introduce RSFAKE-1M, a large-scale dataset of 500K forged
and 500K real remote sensing images. The fake images are generated by ten
diffusion models fine-tuned on remote sensing data, covering six generation
conditions such as text prompts, structural guidance, and inpainting. This
paper presents the construction of RSFAKE-1M along with a comprehensive
experimental evaluation using both existing detectors and unified baselines.
The results reveal that diffusion-based remote sensing forgeries remain
challenging for current methods, and that models trained on RSFAKE-1M exhibit
notably improved generalization and robustness. Our findings underscore the
importance of RSFAKE-1M as a foundation for developing and evaluating
next-generation forgery detection approaches in the remote sensing domain. The
dataset and other supplementary materials are available at
https://huggingface.co/datasets/TZHSW/RSFAKE/.

</details>


### [72] [GenCAD-Self-Repairing: Feasibility Enhancement for 3D CAD Generation](https://arxiv.org/abs/2505.23287)
*Chikaha Tsuji,Enrique Flores Medina,Harshit Gupta,Md Ferdous Alam*

Main category: cs.CV

TL;DR: GenCAD-Self-Repairing通过扩散引导和自我修复流程提升生成CAD模型的可行性，将不可行设计的三分之二转化为可行设计。


<details>
  <summary>Details</summary>
Motivation: 解决GenCAD生成CAD模型时约10%设计不可行的问题。

Method: 结合扩散引导去噪过程和基于回归的修正机制，优化CAD命令序列。

Result: 将不可行设计的三分之二转化为可行设计，显著提升可行性率并保持几何精度。

Conclusion: 该方法提高了生成CAD模型的可行性，扩展了高质量训练数据的可用性，增强了AI驱动CAD生成的应用潜力。

Abstract: With the advancement of generative AI, research on its application to 3D
model generation has gained traction, particularly in automating the creation
of Computer-Aided Design (CAD) files from images. GenCAD is a notable model in
this domain, leveraging an autoregressive transformer-based architecture with a
contrastive learning framework to generate CAD programs.
  However, a major limitation of GenCAD is its inability to consistently
produce feasible boundary representations (B-reps), with approximately 10% of
generated designs being infeasible. To address this, we propose
GenCAD-Self-Repairing, a framework that enhances the feasibility of generative
CAD models through diffusion guidance and a self-repairing pipeline. This
framework integrates a guided diffusion denoising process in the latent space
and a regression-based correction mechanism to refine infeasible CAD command
sequences while preserving geometric accuracy. Our approach successfully
converted two-thirds of infeasible designs in the baseline method into feasible
ones, significantly improving the feasibility rate while simultaneously
maintaining a reasonable level of geometric accuracy between the point clouds
of ground truth models and generated models.
  By significantly improving the feasibility rate of generating CAD models, our
approach helps expand the availability of high-quality training data and
enhances the applicability of AI-driven CAD generation in manufacturing,
architecture, and product design.

</details>


### [73] [Federated Unsupervised Semantic Segmentation](https://arxiv.org/abs/2505.23292)
*Evangelos Charalampakis,Vasileios Mygdalis,Ioannis Pitas*

Main category: cs.CV

TL;DR: FUSS是首个完全去中心化、无标签的联邦学习框架，用于无监督语义图像分割，通过特征和原型空间的一致性优化，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 探索联邦学习在无监督语义图像分割中的应用，解决分布式客户端特征对齐的挑战。

Method: 提出FUSS框架，通过联合优化本地分割头和共享语义中心，实现特征和原型空间的全局一致性。

Result: 在多种数据集上，FUSS表现优于本地训练和传统联邦学习算法。

Conclusion: FUSS为无监督联邦语义分割提供了有效解决方案，代码将开源以支持复现。

Abstract: This work explores the application of Federated Learning (FL) in Unsupervised
Semantic image Segmentation (USS). Recent USS methods extract pixel-level
features using frozen visual foundation models and refine them through
self-supervised objectives that encourage semantic grouping. These features are
then grouped to semantic clusters to produce segmentation masks. Extending
these ideas to federated settings requires feature representation and cluster
centroid alignment across distributed clients -- an inherently difficult task
under heterogeneous data distributions in the absence of supervision. To
address this, we propose FUSS Federated Unsupervised image Semantic
Segmentation) which is, to our knowledge, the first framework to enable fully
decentralized, label-free semantic segmentation training. FUSS introduces novel
federation strategies that promote global consistency in feature and prototype
space, jointly optimizing local segmentation heads and shared semantic
centroids. Experiments on both benchmark and real-world datasets, including
binary and multi-class segmentation tasks, show that FUSS consistently
outperforms local-only client trainings as well as extensions of classical FL
algorithms under varying client data distributions. To support reproducibility,
full code will be released upon manuscript acceptance.

</details>


### [74] [TRACE: Trajectory-Constrained Concept Erasure in Diffusion Models](https://arxiv.org/abs/2505.23312)
*Finn Carter*

Main category: cs.CV

TL;DR: TRACE是一种新方法，用于从扩散模型中擦除特定概念，同时保持生成质量。它结合理论框架和微调程序，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型中生成不良内容（如色情、敏感身份、版权风格）的问题，以确保隐私、公平和安全。

Method: TRACE通过理论框架和微调程序，在扩散过程中抑制目标概念，同时保持其他内容的生成质量。具体包括注意力层更新和轨迹感知微调目标。

Result: 在多个基准测试中，TRACE表现优于现有方法（如ANT、EraseAnything和MACE），在擦除效果和输出质量上达到最优。

Conclusion: TRACE是一种有效的概念擦除方法，具有理论和实践优势，适用于多种扩散模型。

Abstract: Text-to-image diffusion models have shown unprecedented generative
capability, but their ability to produce undesirable concepts
(e.g.~pornographic content, sensitive identities, copyrighted styles) poses
serious concerns for privacy, fairness, and safety. {Concept erasure} aims to
remove or suppress specific concept information in a generative model. In this
paper, we introduce \textbf{TRACE (Trajectory-Constrained Attentional Concept
Erasure)}, a novel method to erase targeted concepts from diffusion models
while preserving overall generative quality. Our approach combines a rigorous
theoretical framework, establishing formal conditions under which a concept can
be provably suppressed in the diffusion process, with an effective fine-tuning
procedure compatible with both conventional latent diffusion (Stable Diffusion)
and emerging rectified flow models (e.g.~FLUX). We first derive a closed-form
update to the model's cross-attention layers that removes hidden
representations of the target concept. We then introduce a trajectory-aware
finetuning objective that steers the denoising process away from the concept
only in the late sampling stages, thus maintaining the model's fidelity on
unrelated content. Empirically, we evaluate TRACE on multiple benchmarks used
in prior concept erasure studies (object classes, celebrity faces, artistic
styles, and explicit content from the I2P dataset). TRACE achieves
state-of-the-art performance, outperforming recent methods such as ANT,
EraseAnything, and MACE in terms of removal efficacy and output quality.

</details>


### [75] [Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition](https://arxiv.org/abs/2505.23313)
*Weizhe Kong,Xiao Wang,Ruichong Gao,Chenglong Li,Yu Zhang,Xing Yang,Yaowei Wang,Jin Tang*

Main category: cs.CV

TL;DR: 本文提出了首个针对行人属性识别（PAR）的对抗攻击与防御框架，结合全局和局部攻击，并设计防御策略。


<details>
  <summary>Details</summary>
Motivation: 尽管PAR在深度神经网络推动下取得进展，但其抗干扰能力和潜在脆弱性尚未充分研究。

Method: 基于CLIP的PAR框架，采用多模态Transformer融合视觉和文本特征，提出对抗语义和标签扰动攻击（ASL-PAR），并设计语义偏移防御策略。

Result: 在数字和物理领域的多个数据集上验证了攻击与防御策略的有效性。

Conclusion: 提出的框架显著提升了PAR的对抗鲁棒性，代码将开源。

Abstract: Pedestrian Attribute Recognition (PAR) is an indispensable task in
human-centered research and has made great progress in recent years with the
development of deep neural networks. However, the potential vulnerability and
anti-interference ability have still not been fully explored. To bridge this
gap, this paper proposes the first adversarial attack and defense framework for
pedestrian attribute recognition. Specifically, we exploit both global- and
patch-level attacks on the pedestrian images, based on the pre-trained
CLIP-based PAR framework. It first divides the input pedestrian image into
non-overlapping patches and embeds them into feature embeddings using a
projection layer. Meanwhile, the attribute set is expanded into sentences using
prompts and embedded into attribute features using a pre-trained CLIP text
encoder. A multi-modal Transformer is adopted to fuse the obtained vision and
text tokens, and a feed-forward network is utilized for attribute recognition.
Based on the aforementioned PAR framework, we adopt the adversarial semantic
and label-perturbation to generate the adversarial noise, termed ASL-PAR. We
also design a semantic offset defense strategy to suppress the influence of
adversarial attacks. Extensive experiments conducted on both digital domains
(i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the
effectiveness of our proposed adversarial attack and defense strategies for the
pedestrian attribute recognition. The source code of this paper will be
released on https://github.com/Event-AHU/OpenPAR.

</details>


### [76] [Dimension-Reduction Attack! Video Generative Models are Experts on Controllable Image Synthesis](https://arxiv.org/abs/2505.23325)
*Hengyuan Cao,Yutong Feng,Biao Gong,Yijing Tian,Yunhong Lu,Chuang Liu,Bin Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为DRA-Ctrl的视频到图像知识压缩与任务适应范式，利用视频模型的长范围上下文建模能力支持图像生成任务。


<details>
  <summary>Details</summary>
Motivation: 探索训练好的高维视频生成模型是否能有效支持低维任务（如可控图像生成）。

Method: 引入基于mixup的过渡策略解决视频帧与图像生成的差异，并重新设计注意力结构和掩码机制以对齐文本提示与图像控制。

Result: 实验表明，改造后的视频模型在多种图像生成任务中表现优于直接训练的图像模型。

Conclusion: DRA-Ctrl展示了大规模视频生成器在更广泛视觉应用中的潜力，为跨模态统一生成模型奠定了基础。

Abstract: Video generative models can be regarded as world simulators due to their
ability to capture dynamic, continuous changes inherent in real-world
environments. These models integrate high-dimensional information across
visual, temporal, spatial, and causal dimensions, enabling predictions of
subjects in various status. A natural and valuable research direction is to
explore whether a fully trained video generative model in high-dimensional
space can effectively support lower-dimensional tasks such as controllable
image generation. In this work, we propose a paradigm for video-to-image
knowledge compression and task adaptation, termed \textit{Dimension-Reduction
Attack} (\texttt{DRA-Ctrl}), which utilizes the strengths of video models,
including long-range context modeling and flatten full-attention, to perform
various generation tasks. Specially, to address the challenging gap between
continuous video frames and discrete image generation, we introduce a
mixup-based transition strategy that ensures smooth adaptation. Moreover, we
redesign the attention structure with a tailored masking mechanism to better
align text prompts with image-level control. Experiments across diverse image
generation tasks, such as subject-driven and spatially conditioned generation,
show that repurposed video models outperform those trained directly on images.
These results highlight the untapped potential of large-scale video generators
for broader visual applications. \texttt{DRA-Ctrl} provides new insights into
reusing resource-intensive video models and lays foundation for future unified
generative models across visual modalities. The project page is
https://dra-ctrl-2025.github.io/DRA-Ctrl/.

</details>


### [77] [Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization](https://arxiv.org/abs/2505.23331)
*Matteo Gallici,Haitz Sáez de Ocáriz Borde*

Main category: cs.CV

TL;DR: 使用强化学习（RL）微调预训练生成模型，结合GRPO方法优化视觉自回归模型（VAR），显著提升图像质量并实现风格控制。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过RL微调方法使生成模型更符合人类偏好，尤其是针对复杂的奖励信号（如美学预测和CLIP嵌入）。

Method: 采用Group Relative Policy Optimization（GRPO）对VAR模型进行微调，利用CLIP嵌入和美学预测器生成奖励信号。

Result: 实验表明，该方法能显著提升图像质量，并实现生成风格的精确控制，还能超越预训练数据分布生成新风格的图像。

Conclusion: RL微调对VAR模型高效有效，尤其适合在线采样，优于扩散模型。

Abstract: Fine-tuning pre-trained generative models with Reinforcement Learning (RL)
has emerged as an effective approach for aligning outputs more closely with
nuanced human preferences. In this paper, we investigate the application of
Group Relative Policy Optimization (GRPO) to fine-tune next-scale visual
autoregressive (VAR) models. Our empirical results demonstrate that this
approach enables alignment to intricate reward signals derived from aesthetic
predictors and CLIP embeddings, significantly enhancing image quality and
enabling precise control over the generation style. Interestingly, by
leveraging CLIP, our method can help VAR models generalize beyond their initial
ImageNet distribution: through RL-driven exploration, these models can generate
images aligned with prompts referencing image styles that were absent during
pre-training. In summary, we show that RL-based fine-tuning is both efficient
and effective for VAR models, benefiting particularly from their fast inference
speeds, which are advantageous for online sampling, an aspect that poses
significant challenges for diffusion-based alternatives.

</details>


### [78] [DSAGL: Dual-Stream Attention-Guided Learning for Weakly Supervised Whole Slide Image Classification](https://arxiv.org/abs/2505.23341)
*Daoxi Cao,Hangbei Cheng,Yijin Li,Ruolin Zhou,Xinyi Li,Xuehan Zhang,Binwei Li,Xuancheng Gu,Xueyu Liu,Yongfei Wu*

Main category: cs.CV

TL;DR: DSAGL是一种弱监督分类框架，通过双流设计和注意力机制解决全切片图像分类中的实例级模糊性和包级语义一致性问题。


<details>
  <summary>Details</summary>
Motivation: 全切片图像（WSIs）因其超高分辨率和丰富语义内容对癌症诊断至关重要，但大尺寸和细粒度标注稀缺限制了传统监督学习的应用。

Method: 提出DSAGL框架，结合教师-学生架构和双流设计，生成多尺度注意力伪标签指导实例级学习，使用轻量编码器VSSMamba和融合注意力模块FASA。

Result: 在CIFAR-10、NCT-CRC和TCGA-Lung数据集上，DSAGL表现优于现有弱监督学习方法，具有更强的判别性能和鲁棒性。

Conclusion: DSAGL通过双流注意力引导学习有效解决了WSIs分类中的挑战，为弱监督学习提供了新思路。

Abstract: Whole-slide images (WSIs) are critical for cancer diagnosis due to their
ultra-high resolution and rich semantic content. However, their massive size
and the limited availability of fine-grained annotations pose substantial
challenges for conventional supervised learning. We propose DSAGL (Dual-Stream
Attention-Guided Learning), a novel weakly supervised classification framework
that combines a teacher-student architecture with a dual-stream design. DSAGL
explicitly addresses instance-level ambiguity and bag-level semantic
consistency by generating multi-scale attention-based pseudo labels and guiding
instance-level learning. A shared lightweight encoder (VSSMamba) enables
efficient long-range dependency modeling, while a fusion-attentive module
(FASA) enhances focus on sparse but diagnostically relevant regions. We further
introduce a hybrid loss to enforce mutual consistency between the two streams.
Experiments on CIFAR-10, NCT-CRC, and TCGA-Lung datasets demonstrate that DSAGL
consistently outperforms state-of-the-art MIL baselines, achieving superior
discriminative performance and robustness under weak supervision.

</details>


### [79] [Diffusion Sampling Path Tells More: An Efficient Plug-and-Play Strategy for Sample Filtering](https://arxiv.org/abs/2505.23343)
*Sixian Wang,Zhiwei Tang,Tsung-Hui Chang*

Main category: cs.CV

TL;DR: 论文提出了一种名为CFG-Rejection的高效方法，通过利用去噪轨迹中的累积分数差异（ASD）来提前过滤低质量样本，无需外部奖励信号或模型重训练。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在采样过程中存在样本质量不一致的问题，现有方法（如DDPO和推理时对齐技术）计算成本高且依赖外部奖励信号，限制了广泛应用。

Method: 通过分析去噪轨迹中的累积分数差异（ASD），发现其与样本质量高度相关，并基于此提出CFG-Rejection方法，在去噪早期阶段过滤低质量样本。

Result: 实验表明，CFG-Rejection在图像生成中显著提升了人类偏好评分（HPSv2, PickScore）和挑战性基准（GenEval, DPG-Bench）的表现。

Conclusion: CFG-Rejection是一种高效、即插即用的方法，适用于多种生成任务，为高质量样本生成提供了更可靠的途径。

Abstract: Diffusion models often exhibit inconsistent sample quality due to stochastic
variations inherent in their sampling trajectories. Although training-based
fine-tuning (e.g. DDPO [1]) and inference-time alignment techniques[2] aim to
improve sample fidelity, they typically necessitate full denoising processes
and external reward signals. This incurs substantial computational costs,
hindering their broader applicability. In this work, we unveil an intriguing
phenomenon: a previously unobserved yet exploitable link between sample quality
and characteristics of the denoising trajectory during classifier-free guidance
(CFG). Specifically, we identify a strong correlation between high-density
regions of the sample distribution and the Accumulated Score Differences
(ASD)--the cumulative divergence between conditional and unconditional scores.
Leveraging this insight, we introduce CFG-Rejection, an efficient,
plug-and-play strategy that filters low-quality samples at an early stage of
the denoising process, crucially without requiring external reward signals or
model retraining. Importantly, our approach necessitates no modifications to
model architectures or sampling schedules and maintains full compatibility with
existing diffusion frameworks. We validate the effectiveness of CFG-Rejection
in image generation through extensive experiments, demonstrating marked
improvements on human preference scores (HPSv2, PickScore) and challenging
benchmarks (GenEval, DPG-Bench). We anticipate that CFG-Rejection will offer
significant advantages for diverse generative modalities beyond images, paving
the way for more efficient and reliable high-quality sample generation.

</details>


### [80] [Beyond Optimal Transport: Model-Aligned Coupling for Flow Matching](https://arxiv.org/abs/2505.23346)
*Yexiong Lin,Yu Yao,Tongliang Liu*

Main category: cs.CV

TL;DR: Flow Matching (FM) 框架通过训练模型学习向量场，将样本从源分布传输到目标分布。传统方法使用随机耦合，导致路径交叉和非直线轨迹，而基于最优传输 (OT) 的方法虽减少路径交叉，但耦合与模型偏好轨迹不一致。为此，提出模型对齐耦合 (MAC)，结合几何距离和模型预测误差选择耦合，显著提升生成质量和效率。


<details>
  <summary>Details</summary>
Motivation: 传统 FM 方法使用随机耦合导致路径交叉和非直线轨迹，而基于 OT 的方法虽减少交叉，但耦合与模型偏好轨迹不一致，影响学习效果。

Method: 提出模型对齐耦合 (MAC)，结合几何距离和模型预测误差选择耦合，并选择误差最低的 top-k 耦合进行训练。

Result: 实验表明，MAC 在少步生成设置中显著提升了生成质量和效率。

Conclusion: MAC 通过结合几何距离和模型偏好，有效解决了耦合与模型轨迹不一致的问题，提升了 FM 的性能。

Abstract: Flow Matching (FM) is an effective framework for training a model to learn a
vector field that transports samples from a source distribution to a target
distribution. To train the model, early FM methods use random couplings, which
often result in crossing paths and lead the model to learn non-straight
trajectories that require many integration steps to generate high-quality
samples. To address this, recent methods adopt Optimal Transport (OT) to
construct couplings by minimizing geometric distances, which helps reduce path
crossings. However, we observe that such geometry-based couplings do not
necessarily align with the model's preferred trajectories, making it difficult
to learn the vector field induced by these couplings, which prevents the model
from learning straight trajectories. Motivated by this, we propose
Model-Aligned Coupling (MAC), an effective method that matches training
couplings based not only on geometric distance but also on alignment with the
model's preferred transport directions based on its prediction error. To avoid
the time-costly match process, MAC proposes to select the top-$k$ fraction of
couplings with the lowest error for training. Extensive experiments show that
MAC significantly improves generation quality and efficiency in few-step
settings compared to existing methods. Project page:
https://yexionglin.github.io/mac

</details>


### [81] [Beam-Guided Knowledge Replay for Knowledge-Rich Image Captioning using Vision-Language Model](https://arxiv.org/abs/2505.23358)
*Reem AlJunaid,Muzammil Behzad*

Main category: cs.CV

TL;DR: KRCapVLM是一种基于知识回放的图像描述生成框架，通过视觉语言模型和束搜索解码生成更丰富、连贯的描述，提升了知识识别和描述质量。


<details>
  <summary>Details</summary>
Motivation: 现有图像描述模型生成的描述通常缺乏具体性和上下文深度，KRCapVLM旨在解决这一问题。

Method: 结合束搜索解码、注意力模块和训练调度器，增强特征表示和训练稳定性。

Result: 模型在知识识别准确性和描述质量上均有显著提升，能更好地泛化到新知识概念。

Conclusion: KRCapVLM有效提升了生成知识丰富、上下文相关描述的能力。

Abstract: Generating informative and knowledge-rich image captions remains a challenge
for many existing captioning models, which often produce generic descriptions
that lack specificity and contextual depth. To address this limitation, we
propose KRCapVLM, a knowledge replay-based novel image captioning framework
using vision-language model. We incorporate beam search decoding to generate
more diverse and coherent captions. We also integrate attention-based modules
into the image encoder to enhance feature representation. Finally, we employ
training schedulers to improve stability and ensure smoother convergence during
training. These proposals accelerate substantial gains in both caption quality
and knowledge recognition. Our proposed model demonstrates clear improvements
in both the accuracy of knowledge recognition and the overall quality of
generated captions. It shows a stronger ability to generalize to previously
unseen knowledge concepts, producing more informative and contextually relevant
descriptions. These results indicate the effectiveness of our approach in
enhancing the model's capacity to generate meaningful, knowledge-grounded
captions across a range of scenarios.

</details>


### [82] [VideoReasonBench: Can MLLMs Perform Vision-Centric Complex Video Reasoning?](https://arxiv.org/abs/2505.23359)
*Yuanxin Liu,Kun Ouyang,Haoning Wu,Yi Liu,Lin Sui,Xinhao Li,Yan Zhong,Y. Charles,Xinyu Zhou,Xu Sun*

Main category: cs.CV

TL;DR: 论文提出了VideoReasonBench，一个专注于视觉中心复杂视频推理的基准测试，填补了现有视频理解任务中缺乏深度推理的空白。通过评估18种多模态大语言模型，发现大多数模型在复杂视频推理上表现不佳，而Gemini-2.5-Pro表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解任务缺乏深度推理需求，无法展示长链思维推理的优势。为此，论文旨在填补这一空白，设计一个视觉丰富且推理复杂的基准测试。

Method: 引入VideoReasonBench，包含视觉丰富且推理复杂的视频任务，评估模型在三个递进层次的视频推理能力：视觉信息回忆、潜在状态推断和视频外信息预测。

Result: 评估显示大多数多模态大语言模型在复杂视频推理上表现不佳（如GPT-4o准确率仅6.9%），而Gemini-2.5-Pro以56.0%准确率显著优于其他模型。

Conclusion: VideoReasonBench填补了视频推理任务的空白，证明了长链思维推理在复杂视频任务中的重要性，同时揭示了测试时间扩展对性能提升的关键作用。

Abstract: Recent studies have shown that long chain-of-thought (CoT) reasoning can
significantly enhance the performance of large language models (LLMs) on
complex tasks. However, this benefit is yet to be demonstrated in the domain of
video understanding, since most existing benchmarks lack the reasoning depth
required to demonstrate the advantages of extended CoT chains. While recent
efforts have proposed benchmarks aimed at video reasoning, the tasks are often
knowledge-driven and do not rely heavily on visual content. To bridge this gap,
we introduce VideoReasonBench, a benchmark designed to evaluate vision-centric,
complex video reasoning. To ensure visual richness and high reasoning
complexity, each video in VideoReasonBench depicts a sequence of fine-grained
operations on a latent state that is only visible in part of the video. The
questions evaluate three escalating levels of video reasoning skills: recalling
observed visual information, inferring the content of latent states, and
predicting information beyond the video. Under such task setting, models have
to precisely recall multiple operations in the video, and perform step-by-step
reasoning to get correct final answers for these questions. Using
VideoReasonBench, we comprehensively evaluate 18 state-of-the-art multimodal
LLMs (MLLMs), finding that most perform poorly on complex video reasoning,
e.g., GPT-4o achieves only 6.9% accuracy, while the thinking-enhanced
Gemini-2.5-Pro significantly outperforms others with 56.0% accuracy. Our
investigations on "test-time scaling" further reveal that extended thinking
budget, while offering none or minimal benefits on existing video benchmarks,
is essential for improving the performance on VideoReasonBench.

</details>


### [83] [MCFNet: A Multimodal Collaborative Fusion Network for Fine-Grained Semantic Classification](https://arxiv.org/abs/2505.23365)
*Yang Qiao,Xiaoyu Zhong,Xiaofeng Gu,Zhiguo Yu*

Main category: cs.CV

TL;DR: 提出了一种新型多模态协作融合网络（MCFNet），通过模态特定正则化和混合注意力机制提升细粒度分类性能，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 多模态信息处理对图像分类性能提升至关重要，但传统方法难以捕捉细粒度语义交互，限制了高精度分类任务的应用。

Method: 设计了MCFNet，包含正则化集成融合模块（模态特定正则化策略）和多模态决策分类模块（混合损失函数加权投票）。

Result: 在基准数据集上的实验表明，MCFNet在分类准确率上取得显著提升。

Conclusion: MCFNet能有效建模跨模态的细微语义，适用于高精度分类任务。

Abstract: Multimodal information processing has become increasingly important for
enhancing image classification performance. However, the intricate and implicit
dependencies across different modalities often hinder conventional methods from
effectively capturing fine-grained semantic interactions, thereby limiting
their applicability in high-precision classification tasks. To address this
issue, we propose a novel Multimodal Collaborative Fusion Network (MCFNet)
designed for fine-grained classification. The proposed MCFNet architecture
incorporates a regularized integrated fusion module that improves intra-modal
feature representation through modality-specific regularization strategies,
while facilitating precise semantic alignment via a hybrid attention mechanism.
Additionally, we introduce a multimodal decision classification module, which
jointly exploits inter-modal correlations and unimodal discriminative features
by integrating multiple loss functions within a weighted voting paradigm.
Extensive experiments and ablation studies on benchmark datasets demonstrate
that the proposed MCFNet framework achieves consistent improvements in
classification accuracy, confirming its effectiveness in modeling subtle
cross-modal semantics.

</details>


### [84] [PAN-Crafter: Learning Modality-Consistent Alignment for PAN-Sharpening](https://arxiv.org/abs/2505.23367)
*Jeonghyeok Do,Sungpyo Kim,Geunhyuk Youk,Jaehyup Lee,Munchurl Kim*

Main category: cs.CV

TL;DR: PAN-Crafter提出了一种模态一致性对齐框架，通过MARs和CM3A机制解决PAN和MS图像间的错位问题，显著提升了HRMS图像的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 解决PAN和MS图像因传感器放置、采集时间和分辨率差异导致的跨模态错位问题，避免传统方法因假设完美对齐而导致的频谱失真和模糊。

Method: 提出Modality-Adaptive Reconstruction (MARs)联合重建HRMS和PAN图像，并引入Cross-Modality Alignment-Aware Attention (CM3A)双向对齐特征。

Result: 在多个基准数据集上表现优于现有方法，推理速度快50.11倍，内存占用减少0.63倍，且在未见卫星数据集上表现出强泛化能力。

Conclusion: PAN-Crafter通过模态一致性对齐和自适应特征细化，显著提升了PAN-sharpening的性能和鲁棒性。

Abstract: PAN-sharpening aims to fuse high-resolution panchromatic (PAN) images with
low-resolution multi-spectral (MS) images to generate high-resolution
multi-spectral (HRMS) outputs. However, cross-modality misalignment -- caused
by sensor placement, acquisition timing, and resolution disparity -- induces a
fundamental challenge. Conventional deep learning methods assume perfect
pixel-wise alignment and rely on per-pixel reconstruction losses, leading to
spectral distortion, double edges, and blurring when misalignment is present.
To address this, we propose PAN-Crafter, a modality-consistent alignment
framework that explicitly mitigates the misalignment gap between PAN and MS
modalities. At its core, Modality-Adaptive Reconstruction (MARs) enables a
single network to jointly reconstruct HRMS and PAN images, leveraging PAN's
high-frequency details as auxiliary self-supervision. Additionally, we
introduce Cross-Modality Alignment-Aware Attention (CM3A), a novel mechanism
that bidirectionally aligns MS texture to PAN structure and vice versa,
enabling adaptive feature refinement across modalities. Extensive experiments
on multiple benchmark datasets demonstrate that our PAN-Crafter outperforms the
most recent state-of-the-art method in all metrics, even with 50.11$\times$
faster inference time and 0.63$\times$ the memory size. Furthermore, it
demonstrates strong generalization performance on unseen satellite datasets,
showing its robustness across different conditions.

</details>


### [85] [UniRL: Self-Improving Unified Multimodal Models via Supervised and Reinforcement Learning](https://arxiv.org/abs/2505.23380)
*Weijia Mao,Zhenheng Yang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: UniRL是一种自改进的后训练方法，通过模型生成图像作为训练数据，无需外部数据，同时优化生成和理解任务。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型依赖大规模数据和计算，且后训练方法常需外部数据或局限于特定任务。

Method: 采用自生成图像作为训练数据，结合监督微调（SFT）和Group Relative Policy Optimization（GRPO）优化模型。

Result: 在Show-o和Janus上分别达到GenEval分数0.77和0.65。

Conclusion: UniRL无需外部数据，提升任务性能并减少任务间不平衡，仅需少量额外训练步骤。

Abstract: Unified multimodal large language models such as Show-o and Janus have
achieved strong performance across both generation and understanding tasks.
However, these models typically rely on large-scale datasets and require
substantial computation during the pretraining stage. In addition, several
post-training methods have been proposed, but they often depend on external
data or are limited to task-specific customization. In this work, we introduce
UniRL, a self-improving post-training approach. Our approach enables the model
to generate images from prompts and use them as training data in each
iteration, without relying on any external image data. Moreover, it enables the
two tasks to enhance each other: the generated images are used for
understanding, and the understanding results are used to supervise generation.
We explore supervised fine-tuning (SFT) and Group Relative Policy Optimization
(GRPO) to optimize the models. UniRL offers three key advantages: (1) it
requires no external image data, as all training samples are generated by the
model itself during training; (2) it not only improves individual task
performance, but also reduces the imbalance between generation and
understanding; and (3) it requires only several additional training steps
during the post-training stage. We evaluate UniRL on top of Show-o and Janus,
achieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and
models will be released in https://github.com/showlab/UniRL.

</details>


### [86] [VModA: An Effective Framework for Adaptive NSFW Image Moderation](https://arxiv.org/abs/2505.23386)
*Han Bao,Qinying Wang,Zhi Chen,Qingming Li,Xuhong Zhang,Changjiang Li,Zonghui Wang,Shouling Ji,Wenzhi Chen*

Main category: cs.CV

TL;DR: VModA是一种通用的NSFW内容检测框架，适应多样化的审核规则，显著提升检测准确性，尤其在复杂语义内容上表现优异。


<details>
  <summary>Details</summary>
Motivation: NSFW内容在社交网络上泛滥，对用户尤其是未成年人造成危害，现有检测方法难以应对复杂语义和多样化的审核规则。

Method: 提出VModA框架，适应不同审核规则，处理复杂语义的NSFW内容，并在实验中验证其性能。

Result: VModA在NSFW检测中准确性提升54.3%，表现出跨类别、场景和基础模型的强适应性。

Conclusion: VModA是一种高效且适应性强的NSFW检测解决方案，已在真实场景中验证其有效性。

Abstract: Not Safe/Suitable for Work (NSFW) content is rampant on social networks and
poses serious harm to citizens, especially minors. Current detection methods
mainly rely on deep learning-based image recognition and classification.
However, NSFW images are now presented in increasingly sophisticated ways,
often using image details and complex semantics to obscure their true nature or
attract more views. Although still understandable to humans, these images often
evade existing detection methods, posing a significant threat. Further
complicating the issue, varying regulations across platforms and regions create
additional challenges for effective moderation, leading to detection bias and
reduced accuracy. To address this, we propose VModA, a general and effective
framework that adapts to diverse moderation rules and handles complex,
semantically rich NSFW content across categories. Experimental results show
that VModA significantly outperforms existing methods, achieving up to a 54.3%
accuracy improvement across NSFW types, including those with complex semantics.
Further experiments demonstrate that our method exhibits strong adaptability
across categories, scenarios, and base VLMs. We also identified inconsistent
and controversial label samples in public NSFW benchmark datasets, re-annotated
them, and submitted corrections to the original maintainers. Two datasets have
confirmed the updates so far. Additionally, we evaluate VModA in real-world
scenarios to demonstrate its practical effectiveness.

</details>


### [87] [Robust and Annotation-Free Wound Segmentation on Noisy Real-World Pressure Ulcer Images: Towards Automated DESIGN-R\textsuperscript{\textregistered} Assessment](https://arxiv.org/abs/2505.23392)
*Yun-Cheng Tsai*

Main category: cs.CV

TL;DR: 提出了一种结合YOLOv11n检测器和FUSegNet分割模型的轻量级方法，仅需500个标注框即可实现跨身体部位的伤口分割，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型（如FUSegNet）在非足部伤口上泛化能力差，需高效且标注量少的解决方案。

Method: 结合YOLOv11n检测器和预训练FUSegNet，无需像素级标注或微调，仅用500个标注框实现跨域泛化。

Result: 在多个测试集上，平均IoU提升23个百分点，DESIGN-R尺寸估计准确率从71%提高到94%。

Conclusion: 该方法仅需少量标注即可实现高效泛化，为临床伤口评分自动化提供了实用解决方案。

Abstract: Purpose: Accurate wound segmentation is essential for automated DESIGN-R
scoring. However, existing models such as FUSegNet, which are trained primarily
on foot ulcer datasets, often fail to generalize to wounds on other body sites.
  Methods: We propose an annotation-efficient pipeline that combines a
lightweight YOLOv11n-based detector with the pre-trained FUSegNet segmentation
model. Instead of relying on pixel-level annotations or retraining for new
anatomical regions, our method achieves robust performance using only 500
manually labeled bounding boxes. This zero fine-tuning approach effectively
bridges the domain gap and enables direct deployment across diverse wound
types. This is an advance not previously demonstrated in the wound segmentation
literature.
  Results: Evaluated on three real-world test sets spanning foot, sacral, and
trochanter wounds, our YOLO plus FUSegNet pipeline improved mean IoU by 23
percentage points over vanilla FUSegNet and increased end-to-end DESIGN-R size
estimation accuracy from 71 percent to 94 percent (see Table 3 for details).
  Conclusion: Our pipeline generalizes effectively across body sites without
task-specific fine-tuning, demonstrating that minimal supervision, with 500
annotated ROIs, is sufficient for scalable, annotation-light wound
segmentation. This capability paves the way for real-world DESIGN-R automation,
reducing reliance on pixel-wise labeling, streamlining documentation workflows,
and supporting objective and consistent wound scoring in clinical practice. We
will publicly release the trained detector weights and configuration to promote
reproducibility and facilitate downstream deployment.

</details>


### [88] [Point or Line? Using Line-based Representation for Panoptic Symbol Spotting in CAD Drawings](https://arxiv.org/abs/2505.23395)
*Xingguang Wei,Haomin Wang,Shenglong Ye,Ruifeng Luo,Yanting Zhang,Lixin Gu,Jifeng Dai,Yu Qiao,Wenhai Wang,Hongjie Zhang*

Main category: cs.CV

TL;DR: VecFormer提出了一种基于线表示的CAD图纸全景符号识别方法，解决了现有方法的高计算成本、泛化性差和几何信息丢失问题，并通过分支融合模块提升预测一致性，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有CAD图纸符号识别方法依赖图像栅格化、图构建或点表示，存在高计算成本、泛化性差和几何信息丢失问题。

Method: VecFormer采用线表示原始图元，保留几何连续性，并引入分支融合模块整合实例与语义预测。

Result: 实验显示VecFormer达到91.1 PQ，Stuff-PQ分别提升9.6和21.2分，优于现有方法。

Conclusion: 线表示是矢量图形理解的有效基础，VecFormer为CAD图纸符号识别提供了新思路。

Abstract: We study the task of panoptic symbol spotting, which involves identifying
both individual instances of countable things and the semantic regions of
uncountable stuff in computer-aided design (CAD) drawings composed of vector
graphical primitives. Existing methods typically rely on image rasterization,
graph construction, or point-based representation, but these approaches often
suffer from high computational costs, limited generality, and loss of geometric
structural information. In this paper, we propose VecFormer, a novel method
that addresses these challenges through line-based representation of
primitives. This design preserves the geometric continuity of the original
primitive, enabling more accurate shape representation while maintaining a
computation-friendly structure, making it well-suited for vector graphic
understanding tasks. To further enhance prediction reliability, we introduce a
Branch Fusion Refinement module that effectively integrates instance and
semantic predictions, resolving their inconsistencies for more coherent
panoptic outputs. Extensive experiments demonstrate that our method establishes
a new state-of-the-art, achieving 91.1 PQ, with Stuff-PQ improved by 9.6 and
21.2 points over the second-best results under settings with and without prior
information, respectively, highlighting the strong potential of line-based
representation as a foundation for vector graphic understanding.

</details>


### [89] [Bridging Geometric and Semantic Foundation Models for Generalized Monocular Depth Estimation](https://arxiv.org/abs/2505.23400)
*Sanggyun Ma,Wonjoon Choi,Jihun Park,Jaeyeul Kim,Seunghun Lee,Jiwan Seo,Sunghoon Im*

Main category: cs.CV

TL;DR: BriGeS方法通过融合几何和语义信息提升单目深度估计（MDE）性能，利用Bridging Gate和Attention Temperature Scaling技术优化注意力机制，显著减少资源需求并提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有MDE方法在处理复杂场景时表现不足，BriGeS旨在通过结合几何和语义信息提升模型性能。

Method: BriGeS通过Bridging Gate整合深度和分割基础模型的优势，并采用Attention Temperature Scaling技术优化注意力机制，仅训练Bridging Gate以减少资源消耗。

Result: 在多个数据集上的实验表明，BriGeS在复杂场景中的MDE性能优于现有方法。

Conclusion: BriGeS通过高效融合几何和语义信息，显著提升了MDE的性能和泛化能力。

Abstract: We present Bridging Geometric and Semantic (BriGeS), an effective method that
fuses geometric and semantic information within foundation models to enhance
Monocular Depth Estimation (MDE). Central to BriGeS is the Bridging Gate, which
integrates the complementary strengths of depth and segmentation foundation
models. This integration is further refined by our Attention Temperature
Scaling technique. It finely adjusts the focus of the attention mechanisms to
prevent over-concentration on specific features, thus ensuring balanced
performance across diverse inputs. BriGeS capitalizes on pre-trained foundation
models and adopts a strategy that focuses on training only the Bridging Gate.
This method significantly reduces resource demands and training time while
maintaining the model's ability to generalize effectively. Extensive
experiments across multiple challenging datasets demonstrate that BriGeS
outperforms state-of-the-art methods in MDE for complex scenes, effectively
handling intricate structures and overlapping objects.

</details>


### [90] [Video Editing for Audio-Visual Dubbing](https://arxiv.org/abs/2505.23406)
*Binyamin Manela,Sharon Gannot,Ethan Fetyaya*

Main category: cs.CV

TL;DR: EdiDub是一种新颖的视觉配音框架，通过内容感知编辑任务改进现有方法，显著提升身份保留和同步效果。


<details>
  <summary>Details</summary>
Motivation: 当前视觉配音方法在无缝集成到原始场景或保留复杂视觉信息方面存在局限，EdiDub旨在解决这些问题。

Method: EdiDub将视觉配音重新定义为内容感知编辑任务，采用专门的条件方案保留原始视频上下文。

Result: 在多个基准测试中，EdiDub在身份保留和同步方面表现优异，人类评估也确认其优于现有方法。

Conclusion: 内容感知编辑方法在保持复杂视觉元素和准确唇同步方面优于传统生成或修复方法。

Abstract: Visual dubbing, the synchronization of facial movements with new speech, is
crucial for making content accessible across different languages, enabling
broader global reach. However, current methods face significant limitations.
Existing approaches often generate talking faces, hindering seamless
integration into original scenes, or employ inpainting techniques that discard
vital visual information like partial occlusions and lighting variations. This
work introduces EdiDub, a novel framework that reformulates visual dubbing as a
content-aware editing task. EdiDub preserves the original video context by
utilizing a specialized conditioning scheme to ensure faithful and accurate
modifications rather than mere copying. On multiple benchmarks, including a
challenging occluded-lip dataset, EdiDub significantly improves identity
preservation and synchronization. Human evaluations further confirm its
superiority, achieving higher synchronization and visual naturalness scores
compared to the leading methods. These results demonstrate that our
content-aware editing approach outperforms traditional generation or
inpainting, particularly in maintaining complex visual elements while ensuring
accurate lip synchronization.

</details>


### [91] [UrbanCraft: Urban View Extrapolation via Hierarchical Sem-Geometric Priors](https://arxiv.org/abs/2505.23434)
*Tianhang Wang,Fan Lu,Sanqing Qu,Guo Yu,Shihang Du,Ya Wu,Yuan Huang,Guang Chen*

Main category: cs.CV

TL;DR: UrbanCraft通过分层语义几何表示解决外推视图合成问题，提升城市重建的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法在训练相机分布外的视图合成表现不佳，限制了城市重建的通用性。

Method: 利用部分可观测场景重建粗粒度语义几何基元，结合实例级3D边界框先验，提出HSG-VSD方法整合语义几何约束。

Result: 定性和定量实验验证了方法在外推视图合成问题上的有效性。

Conclusion: UrbanCraft通过分层先验和HSG-VSD方法显著提升了外推视图合成的性能。

Abstract: Existing neural rendering-based urban scene reconstruction methods mainly
focus on the Interpolated View Synthesis (IVS) setting that synthesizes from
views close to training camera trajectory. However, IVS can not guarantee the
on-par performance of the novel view outside the training camera distribution
(\textit{e.g.}, looking left, right, or downwards), which limits the
generalizability of the urban reconstruction application. Previous methods have
optimized it via image diffusion, but they fail to handle text-ambiguous or
large unseen view angles due to coarse-grained control of text-only diffusion.
In this paper, we design UrbanCraft, which surmounts the Extrapolated View
Synthesis (EVS) problem using hierarchical sem-geometric representations
serving as additional priors. Specifically, we leverage the partially
observable scene to reconstruct coarse semantic and geometric primitives,
establishing a coarse scene-level prior through an occupancy grid as the base
representation. Additionally, we incorporate fine instance-level priors from 3D
bounding boxes to enhance object-level details and spatial relationships.
Building on this, we propose the \textbf{H}ierarchical
\textbf{S}emantic-Geometric-\textbf{G}uided Variational Score Distillation
(HSG-VSD), which integrates semantic and geometric constraints from pretrained
UrbanCraft2D into the score distillation sampling process, forcing the
distribution to be consistent with the observable scene. Qualitative and
quantitative comparisons demonstrate the effectiveness of our methods on EVS
problem.

</details>


### [92] [Adaptive Spatial Augmentation for Semi-supervised Semantic Segmentation](https://arxiv.org/abs/2505.23438)
*Lingyan Ran,Yali Li,Tao Zhuo,Shizhou Zhang,Yanning Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种自适应空间增强（ASAug）方法，用于半监督语义分割（SSSS），通过动态调整增强策略提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有强增强方法主要关注强度扰动，对语义掩码影响小，而空间增强在SSSS中被忽视。作者发现空间增强对SSSS也有帮助，并提出自适应策略。

Method: 提出ASAug，基于熵动态调整每个实例的空间增强（如平移、旋转），解决弱增强与强增强间掩码不一致问题。

Result: ASAug作为可插拔模块，显著提升现有方法性能，在PASCAL VOC 2012、Cityscapes和COCO上达到SOTA。

Conclusion: 空间增强对SSSS有效，自适应策略进一步优化性能，ASAug具有通用性和实用性。

Abstract: In semi-supervised semantic segmentation (SSSS), data augmentation plays a
crucial role in the weak-to-strong consistency regularization framework, as it
enhances diversity and improves model generalization. Recent strong
augmentation methods have primarily focused on intensity-based perturbations,
which have minimal impact on the semantic masks. In contrast, spatial
augmentations like translation and rotation have long been acknowledged for
their effectiveness in supervised semantic segmentation tasks, but they are
often ignored in SSSS. In this work, we demonstrate that spatial augmentation
can also contribute to model training in SSSS, despite generating inconsistent
masks between the weak and strong augmentations. Furthermore, recognizing the
variability among images, we propose an adaptive augmentation strategy that
dynamically adjusts the augmentation for each instance based on entropy.
Extensive experiments show that our proposed Adaptive Spatial Augmentation
(\textbf{ASAug}) can be integrated as a pluggable module, consistently
improving the performance of existing methods and achieving state-of-the-art
results on benchmark datasets such as PASCAL VOC 2012, Cityscapes, and COCO.

</details>


### [93] [VITON-DRR: Details Retention Virtual Try-on via Non-rigid Registration](https://arxiv.org/abs/2505.23439)
*Ben Li,Minqi Li,Jie Ren,Kaibing Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于精确非刚性配准的虚拟试穿方法（VITON-DRR），通过双金字塔结构特征提取器和变形模块，实现了更准确的衣物变形和细节保留。


<details>
  <summary>Details</summary>
Motivation: 虚拟试穿在电商和时尚行业有巨大应用潜力，但现有方法在衣物变形时难以保留细节，导致不真实的试穿效果。

Method: 使用双金字塔结构特征提取器重建人体语义分割，设计变形模块提取衣物关键点并通过非刚性配准算法变形，最后通过图像合成模块生成试穿图像。

Result: 实验表明，VITON-DRR在衣物变形和细节保留上优于现有方法。

Conclusion: VITON-DRR通过精确的非刚性配准，显著提升了虚拟试穿的质量和细节保留能力。

Abstract: Image-based virtual try-on aims to fit a target garment to a specific person
image and has attracted extensive research attention because of its huge
application potential in the e-commerce and fashion industries. To generate
high-quality try-on results, accurately warping the clothing item to fit the
human body plays a significant role, as slight misalignment may lead to
unrealistic artifacts in the fitting image. Most existing methods warp the
clothing by feature matching and thin-plate spline (TPS). However, it often
fails to preserve clothing details due to self-occlusion, severe misalignment
between poses, etc. To address these challenges, this paper proposes a detail
retention virtual try-on method via accurate non-rigid registration (VITON-DRR)
for diverse human poses. Specifically, we reconstruct a human semantic
segmentation using a dual-pyramid-structured feature extractor. Then, a novel
Deformation Module is designed for extracting the cloth key points and warping
them through an accurate non-rigid registration algorithm. Finally, the Image
Synthesis Module is designed to synthesize the deformed garment image and
generate the human pose information adaptively. {Compared with} traditional
methods, the proposed VITON-DRR can make the deformation of fitting images more
accurate and retain more garment details. The experimental results demonstrate
that the proposed method performs better than state-of-the-art methods.

</details>


### [94] [CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis](https://arxiv.org/abs/2505.23444)
*Runmin Jiang,Genpei Zhang,Yuntian Yang,Siqi Wu,Yuheng Zhang,Wanyue Feng,Yizhou Zhao,Xi Xiao,Xiao Wang,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: CryoCCD是一个结合生物物理建模与生成技术的合成框架，用于生成多尺度冷冻电镜显微图像，解决了现有方法在结构多样性和噪声复杂性上的不足。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜（cryo-EM）在近原子分辨率成像方面有优势，但高质量标注数据的稀缺阻碍了模型的开发。合成数据生成虽是一种潜在解决方案，但现有方法难以同时捕捉生物样本的结构多样性和复杂的空间变化噪声。

Method: CryoCCD通过生物物理建模和生成技术生成多尺度冷冻电镜显微图像，利用条件扩散模型生成真实噪声，并通过循环一致性和掩码感知对比学习保持结构保真度和噪声模式。

Result: 实验表明，CryoCCD生成的图像结构准确，并在下游任务（如粒子挑选和重建）中表现优于现有基线方法。

Conclusion: CryoCCD为冷冻电镜图像合成提供了一种有效解决方案，显著提升了生成图像的质量和下游任务的性能。

Abstract: Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of
macromolecules, but developing robust models for downstream analysis is
hindered by the scarcity of high-quality annotated data. While synthetic data
generation has emerged as a potential solution, existing methods often fail to
capture both the structural diversity of biological specimens and the complex,
spatially varying noise inherent in cryo-EM imaging. To overcome these
limitations, we propose CryoCCD, a synthesis framework that integrates
biophysical modeling with generative techniques. Specifically, CryoCCD produces
multi-scale cryo-EM micrographs that reflect realistic biophysical variability
through compositional heterogeneity, cellular context, and physics-informed
imaging. To generate realistic noise, we employ a conditional diffusion model,
enhanced by cycle consistency to preserve structural fidelity and mask-aware
contrastive learning to capture spatially adaptive noise patterns. Extensive
experiments show that CryoCCD generates structurally accurate micrographs and
enhances performance in downstream tasks, outperforming state-of-the-art
baselines in both particle picking and reconstruction.

</details>


### [95] [A Reverse Causal Framework to Mitigate Spurious Correlations for Debiasing Scene Graph Generation](https://arxiv.org/abs/2505.23451)
*Shuzhou Sun,Li Liu,Tianpeng Liu,Shuaifeng Zhi,Ming-Ming Cheng,Janne Heikkilä,Yongxiang Liu*

Main category: cs.CV

TL;DR: 论文提出了一种反向因果框架（RcSGG）来解决场景图生成（SGG）中因因果链结构导致的虚假相关性，通过干预混淆变量和增强反向因果估计，显著提高了平均召回率。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段SGG框架因因果链结构导致虚假相关性，表现为尾部关系被预测为头部关系、前景关系被预测为背景关系。论文旨在解决这一问题。

Method: 提出RcSGG框架，包括Active Reverse Estimation（ARE）干预混淆变量和Maximum Information Sampling（MIS）增强反向因果估计。

Result: 在多个基准测试和不同SGG框架中实现了最先进的平均召回率。

Conclusion: RcSGG通过重构因果结构有效消除了SGG中的虚假相关性，显著提升了性能。

Abstract: Existing two-stage Scene Graph Generation (SGG) frameworks typically
incorporate a detector to extract relationship features and a classifier to
categorize these relationships; therefore, the training paradigm follows a
causal chain structure, where the detector's inputs determine the classifier's
inputs, which in turn influence the final predictions. However, such a causal
chain structure can yield spurious correlations between the detector's inputs
and the final predictions, i.e., the prediction of a certain relationship may
be influenced by other relationships. This influence can induce at least two
observable biases: tail relationships are predicted as head ones, and
foreground relationships are predicted as background ones; notably, the latter
bias is seldom discussed in the literature. To address this issue, we propose
reconstructing the causal chain structure into a reverse causal structure,
wherein the classifier's inputs are treated as the confounder, and both the
detector's inputs and the final predictions are viewed as causal variables.
Specifically, we term the reconstructed causal paradigm as the Reverse causal
Framework for SGG (RcSGG). RcSGG initially employs the proposed Active Reverse
Estimation (ARE) to intervene on the confounder to estimate the reverse
causality, \ie the causality from final predictions to the classifier's inputs.
Then, the Maximum Information Sampling (MIS) is suggested to enhance the
reverse causality estimation further by considering the relationship
information. Theoretically, RcSGG can mitigate the spurious correlations
inherent in the SGG framework, subsequently eliminating the induced biases.
Comprehensive experiments on popular benchmarks and diverse SGG frameworks show
the state-of-the-art mean recall rate.

</details>


### [96] [LAFR: Efficient Diffusion-based Blind Face Restoration via Latent Codebook Alignment Adapter](https://arxiv.org/abs/2505.23462)
*Runyi Li,Bin Chen,Jian Zhang,Radu Timofte*

Main category: cs.CV

TL;DR: 论文提出LAFR方法，通过潜在空间对齐和轻量级微调，高效实现低质量图像的高保真人脸恢复。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在低质量图像输入时存在语义不匹配问题，且传统方法计算成本高。

Method: 提出LAFR，基于码本的潜在空间适配器对齐高低质量图像的潜在分布，并结合多级恢复损失增强身份保留。

Result: 实验表明LAFR在合成和真实数据集上均能高效实现高质量、身份保留的人脸恢复。

Conclusion: LAFR为低质量人脸图像恢复提供了一种高效且高性能的解决方案。

Abstract: Blind face restoration from low-quality (LQ) images is a challenging task
that requires not only high-fidelity image reconstruction but also the
preservation of facial identity. While diffusion models like Stable Diffusion
have shown promise in generating high-quality (HQ) images, their VAE modules
are typically trained only on HQ data, resulting in semantic misalignment when
encoding LQ inputs. This mismatch significantly weakens the effectiveness of LQ
conditions during the denoising process. Existing approaches often tackle this
issue by retraining the VAE encoder, which is computationally expensive and
memory-intensive. To address this limitation efficiently, we propose LAFR
(Latent Alignment for Face Restoration), a novel codebook-based latent space
adapter that aligns the latent distribution of LQ images with that of HQ
counterparts, enabling semantically consistent diffusion sampling without
altering the original VAE. To further enhance identity preservation, we
introduce a multi-level restoration loss that combines constraints from
identity embeddings and facial structural priors. Additionally, by leveraging
the inherent structural regularity of facial images, we show that lightweight
finetuning of diffusion prior on just 0.9% of FFHQ dataset is sufficient to
achieve results comparable to state-of-the-art methods, reduce training time by
70%. Extensive experiments on both synthetic and real-world face restoration
benchmarks demonstrate the effectiveness and efficiency of LAFR, achieving
high-quality, identity-preserving face reconstruction from severely degraded
inputs.

</details>


### [97] [Revisiting Reweighted Risk for Calibration: AURC, Focal Loss, and Inverse Focal Loss](https://arxiv.org/abs/2505.23463)
*Han Zhou,Sebastian G. Gruber,Teodora Popordanoska,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: 论文探讨了加权风险函数与校准误差的关系，提出优化正则化AURC可改善校准性能，并通过SoftRank技术实现梯度优化。


<details>
  <summary>Details</summary>
Motivation: 研究加权风险函数（如焦点损失、逆焦点损失和AURC）的校准性质，揭示其与校准误差的联系。

Method: 分析加权风险函数类，建立其与校准误差的理论联系，提出正则化AURC优化方法，并使用SoftRank实现可微性。

Result: 正则化AURC优化在多种数据集和模型架构中表现出竞争性的校准性能。

Conclusion: 优化正则化AURC是改善模型校准的有效方法，且逆焦点损失的加权策略更具理论支持。

Abstract: Several variants of reweighted risk functionals, such as focal losss, inverse
focal loss, and the Area Under the Risk-Coverage Curve (AURC), have been
proposed in the literature and claims have been made in relation to their
calibration properties. However, focal loss and inverse focal loss propose
vastly different weighting schemes. In this paper, we revisit a broad class of
weighted risk functions commonly used in deep learning and establish a
principled connection between these reweighting schemes and calibration errors.
We show that minimizing calibration error is closely linked to the selective
classification paradigm and demonstrate that optimizing a regularized variant
of the AURC naturally leads to improved calibration. This regularized AURC
shares a similar reweighting strategy with inverse focal loss, lending support
to the idea that focal loss is less principled when calibration is a desired
outcome. Direct AURC optimization offers greater flexibility through the choice
of confidence score functions (CSFs). To enable gradient-based optimization, we
introduce a differentiable formulation of the regularized AURC using the
SoftRank technique. Empirical evaluations demonstrate that our AURC-based loss
achieves competitive class-wise calibration performance across a range of
datasets and model architectures.

</details>


### [98] [A Divide-and-Conquer Approach for Global Orientation of Non-Watertight Scene-Level Point Clouds Using 0-1 Integer Optimization](https://arxiv.org/abs/2505.23469)
*Zhuodong Li,Fei Hou,Wencheng Wang,Xuequan Lu,Ying He*

Main category: cs.CV

TL;DR: DACPO是一种新颖的分治框架，用于解决大规模非封闭3D场景的点云定向问题，通过分块处理和全局优化实现高效定向。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对封闭的物体级3D模型，而大规模非封闭3D场景的定向问题尚未充分研究。

Method: DACPO将点云分割为小块，通过随机贪婪法和迭代泊松表面重建两步处理每块，再通过全局优化整合结果。

Result: 在基准数据集上，DACPO在挑战性的大规模非封闭场景中表现优异。

Conclusion: DACPO填补了大规模非封闭场景定向的空白，具有高效性和鲁棒性。

Abstract: Orienting point clouds is a fundamental problem in computer graphics and 3D
vision, with applications in reconstruction, segmentation, and analysis. While
significant progress has been made, existing approaches mainly focus on
watertight, object-level 3D models. The orientation of large-scale,
non-watertight 3D scenes remains an underexplored challenge. To address this
gap, we propose DACPO (Divide-And-Conquer Point Orientation), a novel framework
that leverages a divide-and-conquer strategy for scalable and robust point
cloud orientation. Rather than attempting to orient an unbounded scene at once,
DACPO segments the input point cloud into smaller, manageable blocks, processes
each block independently, and integrates the results through a global
optimization stage. For each block, we introduce a two-step process: estimating
initial normal orientations by a randomized greedy method and refining them by
an adapted iterative Poisson surface reconstruction. To achieve consistency
across blocks, we model inter-block relationships using an an undirected graph,
where nodes represent blocks and edges connect spatially adjacent blocks. To
reliably evaluate orientation consistency between adjacent blocks, we introduce
the concept of the visible connected region, which defines the region over
which visibility-based assessments are performed. The global integration is
then formulated as a 0-1 integer-constrained optimization problem, with block
flip states as binary variables. Despite the combinatorial nature of the
problem, DACPO remains scalable by limiting the number of blocks (typically a
few hundred for 3D scenes) involved in the optimization. Experiments on
benchmark datasets demonstrate DACPO's strong performance, particularly in
challenging large-scale, non-watertight scenarios where existing methods often
fail. The source code is available at https://github.com/zd-lee/DACPO.

</details>


### [99] [TimePoint: Accelerated Time Series Alignment via Self-Supervised Keypoint and Descriptor Learning](https://arxiv.org/abs/2505.23475)
*Ron Shapira Weber,Shahar Ben Ishay,Andrey Lavrinenko,Shahaf E. Finder,Oren Freifeld*

Main category: cs.CV

TL;DR: TimePoint是一种自监督方法，通过从合成数据中学习关键点和描述符，显著加速基于DTW的时间序列对齐，同时提高准确性。


<details>
  <summary>Details</summary>
Motivation: 动态时间规整（DTW）在时间序列对齐中存在可扩展性差和对噪声敏感的问题，需要一种更高效且鲁棒的方法。

Method: TimePoint利用1D微分同胚生成合成训练数据，结合全卷积和小波卷积架构提取关键点和描述符，再应用DTW进行稀疏表示对齐。

Result: TimePoint比标准DTW更快且更准确，在合成数据上训练后能泛化到真实数据，微调后性能进一步提升。

Conclusion: TimePoint为时间序列分析提供了一种可扩展的解决方案，显著优于传统DTW方法。

Abstract: Fast and scalable alignment of time series is a fundamental challenge in many
domains. The standard solution, Dynamic Time Warping (DTW), struggles with poor
scalability and sensitivity to noise. We introduce TimePoint, a self-supervised
method that dramatically accelerates DTW-based alignment while typically
improving alignment accuracy by learning keypoints and descriptors from
synthetic data. Inspired by 2D keypoint detection but carefully adapted to the
unique challenges of 1D signals, TimePoint leverages efficient 1D
diffeomorphisms, which effectively model nonlinear time warping, to generate
realistic training data. This approach, along with fully convolutional and
wavelet convolutional architectures, enables the extraction of informative
keypoints and descriptors. Applying DTW to these sparse representations yield
major speedups and typically higher alignment accuracy than standard DTW
applied to the full signals. TimePoint demonstrates strong generalization to
real-world time series when trained solely on synthetic data, and further
improves with fine-tuning on real data. Extensive experiments demonstrate that
TimePoint consistently achieves faster and more accurate alignments than
standard DTW, making it a scalable solution for time-series analysis. Our code
is available at https://github.com/BGU-CS-VIL/TimePoint

</details>


### [100] [PhysicsNeRF: Physics-Guided 3D Reconstruction from Sparse Views](https://arxiv.org/abs/2505.23481)
*Mohamed Rayan Barhdadi,Hasan Kurban,Hussein Alnuweiri*

Main category: cs.CV

TL;DR: PhysicsNeRF通过物理约束改进NeRF，在稀疏视图下实现更好的3D重建，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决标准NeRF在稀疏视图下表现不佳的问题，提出物理约束框架以提高重建质量和泛化能力。

Method: 结合深度排序、RegNeRF一致性、稀疏先验和跨视图对齐四种约束，采用0.67M参数架构。

Result: 仅用8视图实现21.4 dB平均PSNR，泛化差距为5.7-6.2 dB，揭示了稀疏重建的局限性。

Conclusion: PhysicsNeRF提供物理一致的3D表示，适用于交互与仿真，并阐明了约束NeRF模型的表达力-泛化权衡。

Abstract: PhysicsNeRF is a physically grounded framework for 3D reconstruction from
sparse views, extending Neural Radiance Fields with four complementary
constraints: depth ranking, RegNeRF-style consistency, sparsity priors, and
cross-view alignment. While standard NeRFs fail under sparse supervision,
PhysicsNeRF employs a compact 0.67M-parameter architecture and achieves 21.4 dB
average PSNR using only 8 views, outperforming prior methods. A generalization
gap of 5.7-6.2 dB is consistently observed and analyzed, revealing fundamental
limitations of sparse-view reconstruction. PhysicsNeRF enables physically
consistent, generalizable 3D representations for agent interaction and
simulation, and clarifies the expressiveness-generalization trade-off in
constrained NeRF models.

</details>


### [101] [VCapsBench: A Large-scale Fine-grained Benchmark for Video Caption Quality Evaluation](https://arxiv.org/abs/2505.23484)
*Shi-Xue Zhang,Hongfa Wang,Duojun Huang,Xin Li,Xiaobin Zhu,Xu-Cheng Yin*

Main category: cs.CV

TL;DR: VCapsBench是一个用于细粒度视频字幕评估的大规模基准，包含5K+视频和100K+ QA对，旨在提升文本到视频生成的质量。


<details>
  <summary>Details</summary>
Motivation: 现有基准在细粒度评估（尤其是对视频生成关键的空间-时间细节）上不足，影响了字幕质量对生成视频的语义一致性和视觉保真度。

Method: 引入VCapsBench基准，包含5,677视频和109,796 QA对，覆盖21个细粒度维度，并提出AR、IR、CR三个指标及基于LLM的自动评估流程。

Result: 基准通过对比QA对分析验证字幕质量，为字幕优化提供可操作建议。

Conclusion: VCapsBench有助于推动鲁棒的文本到视频模型的发展，数据集和代码已开源。

Abstract: Video captions play a crucial role in text-to-video generation tasks, as
their quality directly influences the semantic coherence and visual fidelity of
the generated videos. Although large vision-language models (VLMs) have
demonstrated significant potential in caption generation, existing benchmarks
inadequately address fine-grained evaluation, particularly in capturing
spatial-temporal details critical for video generation. To address this gap, we
introduce the Fine-grained Video Caption Evaluation Benchmark (VCapsBench), the
first large-scale fine-grained benchmark comprising 5,677 (5K+) videos and
109,796 (100K+) question-answer pairs. These QA-pairs are systematically
annotated across 21 fine-grained dimensions (e.g., camera movement, and shot
type) that are empirically proven critical for text-to-video generation. We
further introduce three metrics (Accuracy (AR), Inconsistency Rate (IR),
Coverage Rate (CR)), and an automated evaluation pipeline leveraging large
language model (LLM) to verify caption quality via contrastive QA-pairs
analysis. By providing actionable insights for caption optimization, our
benchmark can advance the development of robust text-to-video models. The
dataset and codes are available at website: https://github.com/GXYM/VCapsBench.

</details>


### [102] [R2I-Bench: Benchmarking Reasoning-Driven Text-to-Image Generation](https://arxiv.org/abs/2505.23493)
*Kaijie Chen,Zihao Lin,Zhiyang Xu,Ying Shen,Yuguang Yao,Joy Rimchala,Jiaxin Zhang,Lifu Huang*

Main category: cs.CV

TL;DR: R2I-Bench是一个专门评估文本到图像生成模型推理能力的基准，包含多类推理任务，并设计了细粒度评估指标R2IScore。实验显示当前模型的推理能力有限。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在推理能力上表现不足，缺乏系统评估。

Method: 提出R2I-Bench基准，涵盖多种推理类别，并设计R2IScore评估指标。

Result: 16个代表性模型的实验结果显示推理能力普遍不足。

Conclusion: 需要开发更具推理能力的下一代文本到图像生成系统。

Abstract: Reasoning is a fundamental capability often required in real-world
text-to-image (T2I) generation, e.g., generating ``a bitten apple that has been
left in the air for more than a week`` necessitates understanding temporal
decay and commonsense concepts. While recent T2I models have made impressive
progress in producing photorealistic images, their reasoning capability remains
underdeveloped and insufficiently evaluated. To bridge this gap, we introduce
R2I-Bench, a comprehensive benchmark specifically designed to rigorously assess
reasoning-driven T2I generation. R2I-Bench comprises meticulously curated data
instances, spanning core reasoning categories, including commonsense,
mathematical, logical, compositional, numerical, causal, and concept mixing. To
facilitate fine-grained evaluation, we design R2IScore, a QA-style metric based
on instance-specific, reasoning-oriented evaluation questions that assess three
critical dimensions: text-image alignment, reasoning accuracy, and image
quality. Extensive experiments with 16 representative T2I models, including a
strong pipeline-based framework that decouples reasoning and generation using
the state-of-the-art language and image generation models, demonstrate
consistently limited reasoning performance, highlighting the need for more
robust, reasoning-aware architectures in the next generation of T2I systems.
Project Page: https://r2i-bench.github.io

</details>


### [103] [VAU-R1: Advancing Video Anomaly Understanding via Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.23504)
*Liyun Zhu,Qixiang Chen,Xi Shen,Xiaodong Cun*

Main category: cs.CV

TL;DR: VAU-R1是一个基于多模态大语言模型的数据高效框架，通过强化微调提升视频异常理解能力，并提出了首个视频异常推理基准VAU-Bench。


<details>
  <summary>Details</summary>
Motivation: 视频异常理解（VAU）在智能城市、安全监控等领域至关重要，但现有方法缺乏可解释性且难以捕捉异常事件的因果和上下文关系。

Method: 提出VAU-R1框架，利用多模态大语言模型和强化微调（RFT）提升异常推理能力，并构建VAU-Bench基准测试。

Result: 实验表明，VAU-R1显著提高了问答准确性、时间定位和推理连贯性。

Conclusion: VAU-R1和VAU-Bench为可解释和推理感知的视频异常理解奠定了基础。

Abstract: Video Anomaly Understanding (VAU) is essential for applications such as smart
cities, security surveillance, and disaster alert systems, yet remains
challenging due to its demand for fine-grained spatio-temporal perception and
robust reasoning under ambiguity. Despite advances in anomaly detection,
existing methods often lack interpretability and struggle to capture the causal
and contextual aspects of abnormal events. This limitation is further
compounded by the absence of comprehensive benchmarks for evaluating reasoning
ability in anomaly scenarios. To address both challenges, we introduce VAU-R1,
a data-efficient framework built upon Multimodal Large Language Models (MLLMs),
which enhances anomaly reasoning through Reinforcement Fine-Tuning (RFT).
Besides, we propose VAU-Bench, the first Chain-of-Thought benchmark tailored
for video anomaly reasoning, featuring multiple-choice QA, detailed rationales,
temporal annotations, and descriptive captions. Empirical results show that
VAU-R1 significantly improves question answering accuracy, temporal grounding,
and reasoning coherence across diverse contexts. Together, our method and
benchmark establish a strong foundation for interpretable and reasoning-aware
video anomaly understanding. Our code is available at
https://github.com/GVCLab/VAU-R1.

</details>


### [104] [OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data](https://arxiv.org/abs/2505.23522)
*Fengxiang Wang,Mingshuo Chen,Xuming He,YiFan Zhang,Feng Liu,Zijie Guo,Zhenghao Hu,Jiong Wang,Jingyi Xu,Zhangrui Li,Fenghua Ling,Ben Fei,Weijia Li,Long Lan,Wenjing Yang,Wenlong Zhang,Lei Bai*

Main category: cs.CV

TL;DR: OmniEarth-Bench是一个全面的地球科学多模态基准测试，覆盖六大地球圈层及其相互作用，包含100个专家策划的评估维度，现有最先进模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有地球科学多模态学习基准在系统覆盖和评估维度上存在局限，无法全面反映地球系统的复杂性。

Method: 利用卫星和实地观测数据，整合29,779个标注，涵盖感知、推理、科学知识和链式推理四个层级，通过专家和众包协作减少标签模糊性。

Result: 测试9种最先进的多模态模型，表现均不理想，最高准确率低于35%，部分跨圈层任务中GPT-4o准确率为0%。

Conclusion: OmniEarth-Bench为地球科学AI设立了新标准，推动了科学发现和环境监测的实际应用。

Abstract: Existing benchmarks for Earth science multimodal learning exhibit critical
limitations in systematic coverage of geosystem components and cross-sphere
interactions, often constrained to isolated subsystems (only in
Human-activities sphere or atmosphere) with limited evaluation dimensions (less
than 16 tasks). To address these gaps, we introduce OmniEarth-Bench, the first
comprehensive multimodal benchmark spanning all six Earth science spheres
(atmosphere, lithosphere, Oceansphere, cryosphere, biosphere and
Human-activities sphere) and cross-spheres with one hundred expert-curated
evaluation dimensions. Leveraging observational data from satellite sensors and
in-situ measurements, OmniEarth-Bench integrates 29,779 annotations across four
tiers: perception, general reasoning, scientific knowledge reasoning and
chain-of-thought (CoT) reasoning. This involves the efforts of 2-5 experts per
sphere to establish authoritative evaluation dimensions and curate relevant
observational datasets, 40 crowd-sourcing annotators to assist experts for
annotations, and finally, OmniEarth-Bench is validated via hybrid expert-crowd
workflows to reduce label ambiguity. Experiments on 9 state-of-the-art MLLMs
reveal that even the most advanced models struggle with our benchmarks, where
none of them reach 35\% accuracy. Especially, in some cross-spheres tasks, the
performance of leading models like GPT-4o drops to 0.0\%. OmniEarth-Bench sets
a new standard for geosystem-aware AI, advancing both scientific discovery and
practical applications in environmental monitoring and disaster prediction. The
dataset, source code, and trained models were released.

</details>


### [105] [CLIP-AE: CLIP-assisted Cross-view Audio-Visual Enhancement for Unsupervised Temporal Action Localization](https://arxiv.org/abs/2505.23524)
*Rui Xia,Dan Jiang,Quan Zhang,Ke Zhang,Chun Yuan*

Main category: cs.CV

TL;DR: 提出了一种基于CLIP的跨视角视听增强的无监督时序动作定位方法，解决了现有方法过度关注高区分性区域和缺乏上下文边界信息的问题。


<details>
  <summary>Details</summary>
Motivation: 现有监督或弱监督方法依赖耗时的人工标注，而无监督方法面临特征过度关注高区分性区域和缺乏多模态信息的挑战。

Method: 结合视觉语言预训练和分类预训练，引入音频感知，并通过自监督跨视角学习实现多视角增强。

Result: 在两个公开数据集上的实验表明，该方法优于多个先进模型。

Conclusion: 提出的方法有效解决了无监督时序动作定位中的关键问题，并显著提升了性能。

Abstract: Temporal Action Localization (TAL) has garnered significant attention in
information retrieval. Existing supervised or weakly supervised methods heavily
rely on labeled temporal boundaries and action categories, which are
labor-intensive and time-consuming. Consequently, unsupervised temporal action
localization (UTAL) has gained popularity. However, current methods face two
main challenges: 1) Classification pre-trained features overly focus on highly
discriminative regions; 2) Solely relying on visual modality information makes
it difficult to determine contextual boundaries. To address these issues, we
propose a CLIP-assisted cross-view audiovisual enhanced UTAL method.
Specifically, we introduce visual language pre-training (VLP) and
classification pre-training-based collaborative enhancement to avoid excessive
focus on highly discriminative regions; we also incorporate audio perception to
provide richer contextual boundary information. Finally, we introduce a
self-supervised cross-view learning paradigm to achieve multi-view perceptual
enhancement without additional annotations. Extensive experiments on two public
datasets demonstrate our model's superiority over several state-of-the-art
competitors.

</details>


### [106] [Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization and Temporal Motion Modulation](https://arxiv.org/abs/2505.23525)
*Jiahao Cui,Yan Chen,Mingwang Xu,Hanlin Shang,Yuxuan Chen,Yun Zhan,Zilong Dong,Yao Yao,Jingdong Wang,Siyu Zhu*

Main category: cs.CV

TL;DR: 提出了一种基于人类偏好的扩散框架，用于生成高动态和逼真的肖像动画，解决了唇同步、自然表情和高保真身体运动的挑战。


<details>
  <summary>Details</summary>
Motivation: 生成高度动态和逼真的肖像动画在唇同步、自然表情和身体运动动态方面存在挑战，需要一种能够对齐人类偏好的方法。

Method: 通过直接偏好优化和时序运动调制，利用人类偏好数据集和特征重塑技术，提升动画的自然性和同步性。

Result: 实验表明，该方法在唇音频同步、表情生动性和身体运动连贯性上优于基线方法，并在人类偏好指标上有显著提升。

Conclusion: 提出的框架有效解决了肖像动画生成中的关键问题，并通过人类偏好对齐和时序调制实现了高质量输出。

Abstract: Generating highly dynamic and photorealistic portrait animations driven by
audio and skeletal motion remains challenging due to the need for precise lip
synchronization, natural facial expressions, and high-fidelity body motion
dynamics. We propose a human-preference-aligned diffusion framework that
addresses these challenges through two key innovations. First, we introduce
direct preference optimization tailored for human-centric animation, leveraging
a curated dataset of human preferences to align generated outputs with
perceptual metrics for portrait motion-video alignment and naturalness of
expression. Second, the proposed temporal motion modulation resolves
spatiotemporal resolution mismatches by reshaping motion conditions into
dimensionally aligned latent features through temporal channel redistribution
and proportional feature expansion, preserving the fidelity of high-frequency
motion details in diffusion-based synthesis. The proposed mechanism is
complementary to existing UNet and DiT-based portrait diffusion approaches, and
experiments demonstrate obvious improvements in lip-audio synchronization,
expression vividness, body motion coherence over baseline methods, alongside
notable gains in human preference metrics. Our model and source code can be
found at: https://github.com/xyz123xyz456/hallo4.

</details>


### [107] [Position Paper: Metadata Enrichment Model: Integrating Neural Networks and Semantic Knowledge Graphs for Cultural Heritage Applications](https://arxiv.org/abs/2505.23543)
*Jan Ignatowicz,Krzysztof Kutt,Grzegorz J. Nalepa*

Main category: cs.CV

TL;DR: 论文提出了一种结合神经网络与语义技术的Metadata Enrichment Model (MEM)，用于增强文化遗产数字化元数据，并通过实验验证其潜力。


<details>
  <summary>Details</summary>
Motivation: 文化遗产数字化的元数据不足限制了其可访问性和协作性，现有视觉分析模型在特定领域（如手稿）的应用有限。

Method: 提出MEM框架，结合计算机视觉模型、大语言模型和知识图谱，通过Multilayer Vision Mechanism (MVM)动态提取嵌套特征。

Result: 在Jagiellonian数字图书馆的印本数据集上验证MEM，并发布105页手稿标注数据集。

Conclusion: MEM为文化遗产研究提供了灵活可扩展的方法，展示了AI与语义技术在实践中的潜力。

Abstract: The digitization of cultural heritage collections has opened new directions
for research, yet the lack of enriched metadata poses a substantial challenge
to accessibility, interoperability, and cross-institutional collaboration. In
several past years neural networks models such as YOLOv11 and Detectron2 have
revolutionized visual data analysis, but their application to domain-specific
cultural artifacts - such as manuscripts and incunabula - remains limited by
the absence of methodologies that address structural feature extraction and
semantic interoperability. In this position paper, we argue, that the
integration of neural networks with semantic technologies represents a paradigm
shift in cultural heritage digitization processes. We present the Metadata
Enrichment Model (MEM), a conceptual framework designed to enrich metadata for
digitized collections by combining fine-tuned computer vision models, large
language models (LLMs) and structured knowledge graphs. The Multilayer Vision
Mechanism (MVM) appears as the key innovation of MEM. This iterative process
improves visual analysis by dynamically detecting nested features, such as text
within seals or images within stamps. To expose MEM's potential, we apply it to
a dataset of digitized incunabula from the Jagiellonian Digital Library and
release a manually annotated dataset of 105 manuscript pages. We examine the
practical challenges of MEM's usage in real-world GLAM institutions, including
the need for domain-specific fine-tuning, the adjustment of enriched metadata
with Linked Data standards and computational costs. We present MEM as a
flexible and extensible methodology. This paper contributes to the discussion
on how artificial intelligence and semantic web technologies can advance
cultural heritage research, and also use these technologies in practice.

</details>


### [108] [Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information](https://arxiv.org/abs/2505.23558)
*Xu Chu,Xinrong Chen,Guanyu Wang,Zhijie Tan,Kui Huang,Wenyu Lv,Tong Mo,Weiping Li*

Main category: cs.CV

TL;DR: Qwen-LA通过引入视觉-文本反思过程，减少视觉语言推理模型中的幻觉问题，并通过BRPO和视觉令牌操作增强视觉注意力。


<details>
  <summary>Details</summary>
Motivation: 长推理过程会稀释视觉令牌，导致视觉信息被忽视并引发幻觉，仅靠文本反思不足以解决这一问题。

Method: 提出Qwen-LA模型，结合BRPO强化学习方法，引入视觉令牌COPY和ROUTE操作，强制模型在推理中重新关注视觉信息。

Result: 在多个视觉问答数据集和幻觉指标上，Qwen-LA表现出领先的准确性并减少幻觉。

Conclusion: 视觉-文本反思和视觉令牌操作有效提升了视觉语言推理模型的性能和可靠性。

Abstract: Inference time scaling drives extended reasoning to enhance the performance
of Vision-Language Models (VLMs), thus forming powerful Vision-Language
Reasoning Models (VLRMs). However, long reasoning dilutes visual tokens,
causing visual information to receive less attention and may trigger
hallucinations. Although introducing text-only reflection processes shows
promise in language models, we demonstrate that it is insufficient to suppress
hallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain
(Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a
vision-text reflection process that guides the model to re-attention visual
information during reasoning. We first propose a reinforcement learning method
Balanced Reflective Policy Optimization (BRPO), which guides the model to
decide when to generate vision-text reflection on its own and balance the
number and length of reflections. Then, we formally prove that VLRMs lose
attention to visual tokens as reasoning progresses, and demonstrate that
supplementing visual information during reflection enhances visual attention.
Therefore, during training and inference, Visual Token COPY and Visual Token
ROUTE are introduced to force the model to re-attention visual information at
the visual level, addressing the limitations of text-only reflection.
Experiments on multiple visual QA datasets and hallucination metrics indicate
that Qwen-LA achieves leading accuracy performance while reducing
hallucinations. Our code is available at:
https://github.com/Liar406/Look_Again.

</details>


### [109] [Uni-MuMER: Unified Multi-Task Fine-Tuning of Vision-Language Model for Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2505.23566)
*Yu Li,Jin Jiang,Jianhua Zhu,Shuai Peng,Baole Wei,Yuxuan Zhou,Liangcai Gao*

Main category: cs.CV

TL;DR: Uni-MuMER通过微调预训练的视觉语言模型（VLM）解决手写数学表达式识别（HMER）问题，无需修改模型架构，结合了Tree-CoT、EDL和SC三种任务，实现了新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: HMER因符号布局自由和手写风格多变而具有挑战性，现有方法难以整合成统一框架，而预训练VLM的跨任务泛化能力为解决这一问题提供了可能。

Method: Uni-MuMER通过完全微调VLM，结合Tree-CoT（结构化空间推理）、EDL（减少视觉相似字符混淆）和SC（提升长表达式识别一致性）三种任务。

Result: 在CROHME和HME100K数据集上，Uni-MuMER性能超越SSAN（16.31%）和Gemini2.5-flash（24.42%），达到SOTA。

Conclusion: Uni-MuMER展示了预训练VLM在HMER任务中的潜力，通过数据驱动任务整合实现了显著性能提升。

Abstract: Handwritten Mathematical Expression Recognition (HMER) remains a persistent
challenge in Optical Character Recognition (OCR) due to the inherent freedom of
symbol layout and variability in handwriting styles. Prior methods have faced
performance bottlenecks, proposing isolated architectural modifications that
are difficult to integrate coherently into a unified framework. Meanwhile,
recent advances in pretrained vision-language models (VLMs) have demonstrated
strong cross-task generalization, offering a promising foundation for
developing unified solutions. In this paper, we introduce Uni-MuMER, which
fully fine-tunes a VLM for the HMER task without modifying its architecture,
effectively injecting domain-specific knowledge into a generalist framework.
Our method integrates three data-driven tasks: Tree-Aware Chain-of-Thought
(Tree-CoT) for structured spatial reasoning, Error-Driven Learning (EDL) for
reducing confusion among visually similar characters, and Symbol Counting (SC)
for improving recognition consistency in long expressions. Experiments on the
CROHME and HME100K datasets show that Uni-MuMER achieves new state-of-the-art
performance, surpassing the best lightweight specialized model SSAN by 16.31%
and the top-performing VLM Gemini2.5-flash by 24.42% in the zero-shot setting.
Our datasets, models, and code are open-sourced at:
https://github.com/BFlameSwift/Uni-MuMER

</details>


### [110] [Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles](https://arxiv.org/abs/2505.23590)
*Zifu Wang,Junyi Zhu,Bo Tang,Zhiyu Li,Feiyu Xiong,Jiaqian Yu,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: 论文研究了基于规则的强化学习在多模态大语言模型（MLLMs）中的应用，以拼图任务为实验框架，发现MLLMs通过微调能显著提升性能并泛化到复杂任务，且强化学习比监督微调更具泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索多模态大语言模型在视觉任务中的表现，特别是基于规则的强化学习在拼图任务中的应用，以填补文本领域与多模态领域的研究空白。

Method: 使用拼图任务作为结构化实验框架，通过微调和强化学习训练MLLMs，分析其性能、泛化能力和推理模式。

Result: MLLMs在拼图任务中从随机猜测提升到近乎完美准确率，并能泛化到复杂任务；强化学习比监督微调更有效；推理模式多为预存而非涌现。

Conclusion: 研究表明基于规则的强化学习在多模态任务中具有潜力，但初始监督微调可能阻碍后续强化学习优化，为多模态学习提供了重要见解。

Abstract: The application of rule-based reinforcement learning (RL) to multimodal large
language models (MLLMs) introduces unique challenges and potential deviations
from findings in text-only domains, particularly for perception-heavy tasks.
This paper provides a comprehensive study of rule-based visual RL using jigsaw
puzzles as a structured experimental framework, revealing several key findings.
\textit{Firstly,} we find that MLLMs, initially performing near to random
guessing on simple puzzles, achieve near-perfect accuracy and generalize to
complex, unseen configurations through fine-tuning. \textit{Secondly,} training
on jigsaw puzzles can induce generalization to other visual tasks, with
effectiveness tied to specific task configurations. \textit{Thirdly,} MLLMs can
learn and generalize with or without explicit reasoning, though open-source
models often favor direct answering. Consequently, even when trained for
step-by-step reasoning, they can ignore the thinking process in deriving the
final answer. \textit{Fourthly,} we observe that complex reasoning patterns
appear to be pre-existing rather than emergent, with their frequency increasing
alongside training and task difficulty. \textit{Finally,} our results
demonstrate that RL exhibits more effective generalization than Supervised
Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL
optimization. Although these observations are based on jigsaw puzzles and may
vary across other visual tasks, this research contributes a valuable piece of
jigsaw to the larger puzzle of collective understanding rule-based visual RL
and its potential in multimodal learning. The code is available at:
\href{https://github.com/zifuwanggg/Jigsaw-R1}{https://github.com/zifuwanggg/Jigsaw-R1}.

</details>


### [111] [DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task Learning in Chest X-ray Classification](https://arxiv.org/abs/2505.23595)
*Youssef Mohamed,Noran Mohamed,Khaled Abouhashad,Feilong Tang,Sara Atito,Shoaib Jameel,Imran Razzak,Ahmed B. Zaky*

Main category: cs.CV

TL;DR: DeepChest是一种动态任务加权框架，用于多标签胸部X光分类，通过性能驱动的权重机制提升多任务学习效率，显著减少内存使用并提高训练速度。


<details>
  <summary>Details</summary>
Motivation: 多任务学习在医学影像领域具有优势，但任务贡献平衡是一个挑战。

Method: DeepChest利用任务特定损失趋势分析，动态调整任务权重，无需梯度访问，适用于多种网络架构。

Result: 在大型CXR数据集上，DeepChest比现有方法准确率提高7%，并显著减少任务损失。

Conclusion: DeepChest为医学诊断中的深度学习提供了更高效和稳健的解决方案。

Abstract: While Multi-Task Learning (MTL) offers inherent advantages in complex domains
such as medical imaging by enabling shared representation learning, effectively
balancing task contributions remains a significant challenge. This paper
addresses this critical issue by introducing DeepChest, a novel,
computationally efficient and effective dynamic task-weighting framework
specifically designed for multi-label chest X-ray (CXR) classification. Unlike
existing heuristic or gradient-based methods that often incur substantial
overhead, DeepChest leverages a performance-driven weighting mechanism based on
effective analysis of task-specific loss trends. Given a network architecture
(e.g., ResNet18), our model-agnostic approach adaptively adjusts task
importance without requiring gradient access, thereby significantly reducing
memory usage and achieving a threefold increase in training speed. It can be
easily applied to improve various state-of-the-art methods. Extensive
experiments on a large-scale CXR dataset demonstrate that DeepChest not only
outperforms state-of-the-art MTL methods by 7% in overall accuracy but also
yields substantial reductions in individual task losses, indicating improved
generalization and effective mitigation of negative transfer. The efficiency
and performance gains of DeepChest pave the way for more practical and robust
deployment of deep learning in critical medical diagnostic applications. The
code is publicly available at https://github.com/youssefkhalil320/DeepChest-MTL

</details>


### [112] [Bridging Classical and Modern Computer Vision: PerceptiveNet for Tree Crown Semantic Segmentation](https://arxiv.org/abs/2505.23597)
*Georgios Voulgaris*

Main category: cs.CV

TL;DR: 论文提出了一种名为PerceptiveNet的新模型，结合了对数Gabor参数化卷积层和宽感受野的主干网络，用于解决树冠语义分割中的复杂性问题，并在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 精确的树冠语义分割对森林管理和生态研究至关重要，但传统方法和现有深度学习模型难以处理树冠的复杂特征（如阴影、背景干扰等）。

Method: 提出PerceptiveNet模型，结合对数Gabor参数化卷积层和宽感受野的主干网络，并通过实验比较不同卷积层的效果，同时评估其在混合CNN-Transformer模型中的表现。

Result: PerceptiveNet在树冠数据集和多个基准数据集上表现优于现有方法，展示了显著的性能提升和跨领域泛化能力。

Conclusion: PerceptiveNet通过创新的卷积层设计和宽感受野主干网络，有效解决了树冠分割中的复杂性问题，具有广泛的应用潜力。

Abstract: The accurate semantic segmentation of tree crowns within remotely sensed data
is crucial for scientific endeavours such as forest management, biodiversity
studies, and carbon sequestration quantification. However, precise segmentation
remains challenging due to complexities in the forest canopy, including
shadows, intricate backgrounds, scale variations, and subtle spectral
differences among tree species. Compared to the traditional methods, Deep
Learning models improve accuracy by extracting informative and discriminative
features, but often fall short in capturing the aforementioned complexities.
  To address these challenges, we propose PerceptiveNet, a novel model
incorporating a Logarithmic Gabor-parameterised convolutional layer with
trainable filter parameters, alongside a backbone that extracts salient
features while capturing extensive context and spatial information through a
wider receptive field. We investigate the impact of Log-Gabor, Gabor, and
standard convolutional layers on semantic segmentation performance through
extensive experimentation. Additionally, we conduct an ablation study to assess
the contributions of individual layers and their combinations to overall model
performance, and we evaluate PerceptiveNet as a backbone within a novel hybrid
CNN-Transformer model. Our results outperform state-of-the-art models,
demonstrating significant performance improvements on a tree crown dataset
while generalising across domains, including two benchmark aerial scene
semantic segmentation datasets with varying complexities.

</details>


### [113] [A Comprehensive Evaluation of Multi-Modal Large Language Models for Endoscopy Analysis](https://arxiv.org/abs/2505.23601)
*Shengyuan Liu,Boyun Zheng,Wenting Chen,Zhihao Peng,Zhenfei Yin,Jing Shao,Jiancong Hu,Yixuan Yuan*

Main category: cs.CV

TL;DR: EndoBench是一个全面的基准测试，旨在评估多模态大语言模型（MLLMs）在内窥镜实践中的多维能力，覆盖多种场景和任务，揭示模型与临床专家之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前的内窥镜分析基准测试覆盖范围有限，无法反映真实世界的多样性和临床工作流程的需求，因此需要更全面的评估工具。

Method: EndoBench包含4种内窥镜场景、12项临床任务和5种视觉提示粒度，通过6,832个验证的VQA对进行评估，涵盖解剖识别、病变分析等维度。

Result: 实验显示专有MLLMs优于开源和医学专用模型，但仍不及人类专家；医学领域微调显著提升准确性；模型性能受提示格式和任务复杂度影响。

Conclusion: EndoBench为内窥镜领域的MLLMs评估设立了新标准，揭示了当前模型与临床专家之间的差距，并公开了基准和代码。

Abstract: Endoscopic procedures are essential for diagnosing and treating internal
diseases, and multi-modal large language models (MLLMs) are increasingly
applied to assist in endoscopy analysis. However, current benchmarks are
limited, as they typically cover specific endoscopic scenarios and a small set
of clinical tasks, failing to capture the real-world diversity of endoscopic
scenarios and the full range of skills needed in clinical workflows. To address
these issues, we introduce EndoBench, the first comprehensive benchmark
specifically designed to assess MLLMs across the full spectrum of endoscopic
practice with multi-dimensional capacities. EndoBench encompasses 4 distinct
endoscopic scenarios, 12 specialized clinical tasks with 12 secondary subtasks,
and 5 levels of visual prompting granularities, resulting in 6,832 rigorously
validated VQA pairs from 21 diverse datasets. Our multi-dimensional evaluation
framework mirrors the clinical workflow--spanning anatomical recognition,
lesion analysis, spatial localization, and surgical operations--to holistically
gauge the perceptual and diagnostic abilities of MLLMs in realistic scenarios.
We benchmark 23 state-of-the-art models, including general-purpose,
medical-specialized, and proprietary MLLMs, and establish human clinician
performance as a reference standard. Our extensive experiments reveal: (1)
proprietary MLLMs outperform open-source and medical-specialized models
overall, but still trail human experts; (2) medical-domain supervised
fine-tuning substantially boosts task-specific accuracy; and (3) model
performance remains sensitive to prompt format and clinical task complexity.
EndoBench establishes a new standard for evaluating and advancing MLLMs in
endoscopy, highlighting both progress and persistent gaps between current
models and expert clinical reasoning. We publicly release our benchmark and
code.

</details>


### [114] [Color Image Set Recognition Based on Quaternionic Grassmannians](https://arxiv.org/abs/2505.23629)
*Xiang Xiang Wang,Tin-Yau Tam*

Main category: cs.CV

TL;DR: 提出了一种基于四元数Grassmannian的彩色图像集识别方法，利用四元数捕捉颜色信息，并通过计算Grassmannian上的最短距离构建分类框架。


<details>
  <summary>Details</summary>
Motivation: 利用四元数的优势更有效地捕捉和表示彩色图像集中的颜色信息。

Method: 将彩色图像集表示为四元数Grassmannian上的点，并计算最短距离用于分类。

Result: 在ETH-80数据集上取得了良好的识别效果。

Conclusion: 方法有效但稳定性有待改进，未来可进一步优化。

Abstract: We propose a new method for recognizing color image sets using quaternionic
Grassmannians, which use the power of quaternions to capture color information
and represent each color image set as a point on the quaternionic Grassmannian.
We provide a direct formula to calculate the shortest distance between two
points on the quaternionic Grassmannian, and use this distance to build a new
classification framework. Experiments on the ETH-80 benchmark dataset show that
our method achieves good recognition results. We also discuss some limitations
in stability and suggest ways the method can be improved in the future.

</details>


### [115] [Comparing the Effects of Persistence Barcodes Aggregation and Feature Concatenation on Medical Imaging](https://arxiv.org/abs/2505.23637)
*Dashti A. Ali,Richard K. G. Do,William R. Jarnagin,Aras T. Asaad,Amber L. Simpson*

Main category: cs.CV

TL;DR: 论文比较了两种基于持久同调的特征向量构建方法在医学图像分类中的效果，发现特征拼接方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统特征提取方法对输入的小变化敏感，而持久同调（PH）能稳定提取拓扑和几何特征，但如何从多个持久条形码构建最终特征向量尚需研究。

Method: 通过聚合持久条形码或拼接特征向量两种方法构建拓扑特征向量，并在多样医学图像数据集上比较其分类性能。

Result: 特征拼接方法保留了更多细节拓扑信息，分类性能更优。

Conclusion: 在类似实验中，特征拼接是更优的选择。

Abstract: In medical image analysis, feature engineering plays an important role in the
design and performance of machine learning models. Persistent homology (PH),
from the field of topological data analysis (TDA), demonstrates robustness and
stability to data perturbations and addresses the limitation from traditional
feature extraction approaches where a small change in input results in a large
change in feature representation. Using PH, we store persistent topological and
geometrical features in the form of the persistence barcode whereby large bars
represent global topological features and small bars encapsulate geometrical
information of the data. When multiple barcodes are computed from 2D or 3D
medical images, two approaches can be used to construct the final topological
feature vector in each dimension: aggregating persistence barcodes followed by
featurization or concatenating topological feature vectors derived from each
barcode. In this study, we conduct a comprehensive analysis across diverse
medical imaging datasets to compare the effects of the two aforementioned
approaches on the performance of classification models. The results of this
analysis indicate that feature concatenation preserves detailed topological
information from individual barcodes, yields better classification performance
and is therefore a preferred approach when conducting similar experiments.

</details>


### [116] [Radiant Triangle Soup with Soft Connectivity Forces for 3D Reconstruction and Novel View Synthesis](https://arxiv.org/abs/2505.23642)
*Nathaniel Burgdorfer,Philippos Mordohai*

Main category: cs.CV

TL;DR: 提出了一种基于三角形的推理时优化框架，用于表示场景的几何和外观，优于当前广泛使用的高斯泼溅方法。


<details>
  <summary>Details</summary>
Motivation: 三角形能够实现更丰富的颜色插值，并便于下游任务处理，同时能自然形成表面。

Method: 开发了一种针对三角形汤（不连接的半透明三角形集合）的场景优化算法，并在优化过程中引入三角形间的连接力以促进表面连续性。

Result: 在代表性3D重建数据集上实验，展示了具有竞争力的光度和几何结果。

Conclusion: 三角形作为场景表示基元具有优势，尤其在颜色插值和表面形成方面表现优异。

Abstract: In this work, we introduce an inference-time optimization framework utilizing
triangles to represent the geometry and appearance of the scene. More
specifically, we develop a scene optimization algorithm for triangle soup, a
collection of disconnected semi-transparent triangle primitives. Compared to
the current most-widely used primitives for 3D scene representation, namely
Gaussian splats, triangles allow for more expressive color interpolation, and
benefit from a large algorithmic infrastructure for downstream tasks.
Triangles, unlike full-rank Gaussian kernels, naturally combine to form
surfaces. We formulate connectivity forces between triangles during
optimization, encouraging explicit, but soft, surface continuity in 3D. We
perform experiments on a representative 3D reconstruction dataset and show
competitive photometric and geometric results.

</details>


### [117] [VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models](https://arxiv.org/abs/2505.23656)
*Xiangdong Zhang,Jiaqi Liao,Shaofeng Zhang,Fanqing Meng,Xiangpeng Wan,Junchi Yan,Yu Cheng*

Main category: cs.CV

TL;DR: VideoREPA框架通过Token Relation Distillation损失，将视频理解基础模型中的物理知识蒸馏到T2V模型中，显著提升了生成视频的物理合理性。


<details>
  <summary>Details</summary>
Motivation: 当前T2V模型在生成物理合理内容方面表现不足，其物理理解能力落后于视频自监督学习方法。

Method: 提出VideoREPA框架，通过Token Relation Distillation (TRD)损失对齐token级关系，将物理知识从视频理解基础模型蒸馏到T2V模型中。

Result: VideoREPA显著提升了基线方法CogVideoX的物理常识，在相关基准测试中表现优异。

Conclusion: VideoREPA是首个专为T2V模型设计的REPA方法，成功注入物理知识，生成更符合直觉物理的视频。

Abstract: Recent advancements in text-to-video (T2V) diffusion models have enabled
high-fidelity and realistic video synthesis. However, current T2V models often
struggle to generate physically plausible content due to their limited inherent
ability to accurately understand physics. We found that while the
representations within T2V models possess some capacity for physics
understanding, they lag significantly behind those from recent video
self-supervised learning methods. To this end, we propose a novel framework
called VideoREPA, which distills physics understanding capability from video
understanding foundation models into T2V models by aligning token-level
relations. This closes the physics understanding gap and enable more
physics-plausible generation. Specifically, we introduce the Token Relation
Distillation (TRD) loss, leveraging spatio-temporal alignment to provide soft
guidance suitable for finetuning powerful pre-trained T2V models, a critical
departure from prior representation alignment (REPA) methods. To our knowledge,
VideoREPA is the first REPA method designed for finetuning T2V models and
specifically for injecting physical knowledge. Empirical evaluations show that
VideoREPA substantially enhances the physics commonsense of baseline method,
CogVideoX, achieving significant improvement on relevant benchmarks and
demonstrating a strong capacity for generating videos consistent with intuitive
physics. More video results are available at https://videorepa.github.io/.

</details>


### [118] [D-AR: Diffusion via Autoregressive Models](https://arxiv.org/abs/2505.23660)
*Ziteng Gao,Mike Zheng Shou*

Main category: cs.CV

TL;DR: D-AR将图像扩散过程重新定义为标准的自回归过程，通过离散化图像为序列令牌，利用扩散特性实现粗到细的自回归建模，无需修改底层设计。


<details>
  <summary>Details</summary>
Motivation: 探索一种统一的自回归架构，将图像合成与语言模型结合，简化扩散过程。

Method: 设计令牌化器将图像转换为离散令牌序列，利用扩散特性实现自回归建模，直接解码令牌为去噪步骤。

Result: 在ImageNet基准测试中，使用775M Llama骨干和256离散令牌，达到2.09 FID。

Conclusion: D-AR展示了自回归架构在视觉合成中的潜力，为未来研究提供了新方向。

Abstract: This paper presents Diffusion via Autoregressive models (D-AR), a new
paradigm recasting the image diffusion process as a vanilla autoregressive
procedure in the standard next-token-prediction fashion. We start by designing
the tokenizer that converts images into sequences of discrete tokens, where
tokens in different positions can be decoded into different diffusion denoising
steps in the pixel space. Thanks to the diffusion properties, these tokens
naturally follow a coarse-to-fine order, which directly lends itself to
autoregressive modeling. Therefore, we apply standard next-token prediction on
these tokens, without modifying any underlying designs (either causal masks or
training/inference strategies), and such sequential autoregressive token
generation directly mirrors the diffusion procedure in image space. That is,
once the autoregressive model generates an increment of tokens, we can directly
decode these tokens into the corresponding diffusion denoising step in the
streaming manner. Our pipeline naturally reveals several intriguing properties,
for example, it supports consistent previews when generating only a subset of
tokens and enables zero-shot layout-controlled synthesis. On the standard
ImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone
with 256 discrete tokens. We hope our work can inspire future research on
unified autoregressive architectures of visual synthesis, especially with large
language models. Code and models will be available at
https://github.com/showlab/D-AR

</details>


### [119] [OpenUni: A Simple Baseline for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2505.23661)
*Size Wu,Zhonghua Wu,Zerui Gong,Qingyi Tao,Sheng Jin,Qinyue Li,Wei Li,Chen Change Loy*

Main category: cs.CV

TL;DR: OpenUni是一个轻量级、开源的基线模型，用于统一多模态理解和生成，通过高效训练策略和简单架构实现高质量图像生成和卓越性能。


<details>
  <summary>Details</summary>
Motivation: 受统一模型学习实践的启发，旨在简化多模态任务的复杂性，同时保持高性能。

Method: 采用可学习查询和轻量级Transformer连接器，桥接现有多模态大语言模型和扩散模型。

Result: 生成高质量图像并在标准基准测试中表现优异，仅需1.1B和3.1B激活参数。

Conclusion: OpenUni为多模态研究提供了高效、开放的解决方案，并公开了所有资源以促进社区发展。

Abstract: In this report, we present OpenUni, a simple, lightweight, and fully
open-source baseline for unifying multimodal understanding and generation.
Inspired by prevailing practices in unified model learning, we adopt an
efficient training strategy that minimizes the training complexity and overhead
by bridging the off-the-shelf multimodal large language models (LLMs) and
diffusion models through a set of learnable queries and a light-weight
transformer-based connector. With a minimalist choice of architecture, we
demonstrate that OpenUni can: 1) generate high-quality and instruction-aligned
images, and 2) achieve exceptional performance on standard benchmarks such as
GenEval, DPG- Bench, and WISE, with only 1.1B and 3.1B activated parameters. To
support open research and community advancement, we release all model weights,
training code, and our curated training datasets (including 23M image-text
pairs) at https://github.com/wusize/OpenUni.

</details>


### [120] [Grounded Reinforcement Learning for Visual Reasoning](https://arxiv.org/abs/2505.23678)
*Gabriel Sarch,Snigdha Saha,Naitik Khandelwal,Ayush Jain,Michael J. Tarr,Aviral Kumar,Katerina Fragkiadaki*

Main category: cs.CV

TL;DR: ViGoRL是一种基于视觉的强化学习模型，通过空间锚定推理步骤提升视觉推理能力，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 视觉推理任务需要模型具备视觉注意力、感知输入解释和空间证据推理能力，现有方法缺乏明确的视觉锚定机制。

Method: ViGoRL通过多轮强化学习框架动态锚定视觉坐标，并结合放大视觉反馈优化推理。

Result: ViGoRL在多个视觉推理基准测试中表现优于传统方法，尤其在视觉搜索和小GUI元素定位任务中达到86.4%的准确率。

Conclusion: 视觉锚定的强化学习是提升通用视觉推理能力的有效范式。

Abstract: While reinforcement learning (RL) over chains of thought has significantly
advanced language models in tasks such as mathematics and coding, visual
reasoning introduces added complexity by requiring models to direct visual
attention, interpret perceptual inputs, and ground abstract reasoning in
spatial evidence. We introduce ViGoRL (Visually Grounded Reinforcement
Learning), a vision-language model trained with RL to explicitly anchor each
reasoning step to specific visual coordinates. Inspired by human visual
decision-making, ViGoRL learns to produce spatially grounded reasoning traces,
guiding visual attention to task-relevant regions at each step. When
fine-grained exploration is required, our novel multi-turn RL framework enables
the model to dynamically zoom into predicted coordinates as reasoning unfolds.
Across a diverse set of visual reasoning benchmarks--including SAT-2 and BLINK
for spatial reasoning, V*bench for visual search, and ScreenSpot and
VisualWebArena for web-based grounding--ViGoRL consistently outperforms both
supervised fine-tuning and conventional RL baselines that lack explicit
grounding mechanisms. Incorporating multi-turn RL with zoomed-in visual
feedback significantly improves ViGoRL's performance on localizing small GUI
elements and visual search, achieving 86.4% on V*Bench. Additionally, we find
that grounding amplifies other visual behaviors such as region exploration,
grounded subgoal setting, and visual verification. Finally, human evaluations
show that the model's visual references are not only spatially accurate but
also helpful for understanding model reasoning steps. Our results show that
visually grounded RL is a strong paradigm for imbuing models with
general-purpose visual reasoning.

</details>


### [121] [VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos](https://arxiv.org/abs/2505.23693)
*Tingyu Song,Tongyan Hu,Guo Gan,Yilun Zhao*

Main category: cs.CV

TL;DR: 论文提出了VF-Eval基准，用于全面评估MLLMs在AIGC视频上的能力，发现现有模型表现不佳，并通过RePrompt实验展示了其在视频生成中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注自然视频，忽视了AIGC视频的评估，且MLLMs在AIGC视频上的能力尚未充分探索。

Method: 提出VF-Eval基准，包含四项任务（连贯性验证、错误意识、错误类型检测和推理评估），并评估了13种前沿MLLMs。

Result: 即使表现最佳的GPT-4.1模型在所有任务中表现也不一致，表明基准的挑战性。

Conclusion: VF-Eval为MLLMs在AIGC视频上的能力提供了全面评估，并通过RePrompt实验展示了其在视频生成中的实际应用价值。

Abstract: MLLMs have been widely studied for video question answering recently.
However, most existing assessments focus on natural videos, overlooking
synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in
video generation rely on MLLMs to evaluate the quality of generated videos, but
the capabilities of MLLMs on interpreting AIGC videos remain largely
underexplored. To address this, we propose a new benchmark, VF-Eval, which
introduces four tasks-coherence validation, error awareness, error type
detection, and reasoning evaluation-to comprehensively evaluate the abilities
of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that
even the best-performing model, GPT-4.1, struggles to achieve consistently good
performance across all tasks. This highlights the challenging nature of our
benchmark. Additionally, to investigate the practical applications of VF-Eval
in improving video generation, we conduct an experiment, RePrompt,
demonstrating that aligning MLLMs more closely with human feedback can benefit
video generation.

</details>


### [122] [DA-VPT: Semantic-Guided Visual Prompt Tuning for Vision Transformers](https://arxiv.org/abs/2505.23694)
*Li Ren,Chen Chen,Liqiang Wang,Kien Hua*

Main category: cs.CV

TL;DR: 本文提出了一种名为DA-VPT的新框架，通过度量学习技术研究提示分布对微调性能的影响，并利用语义信息引导提示学习，从而提升ViT模型在下游视觉任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究未深入探索视觉提示与图像标记之间的相关性和分布关系，本文旨在填补这一空白。

Method: 提出DA-VPT框架，通过学习类相关语义数据的距离度量来引导提示的分布。

Result: 实验表明，DA-VPT能更高效地微调ViT模型，并在识别和分割任务中取得更好性能。

Conclusion: 通过语义信息引导提示学习，DA-VPT显著提升了ViT模型在下游任务中的表现。

Abstract: Visual Prompt Tuning (VPT) has become a promising solution for
Parameter-Efficient Fine-Tuning (PEFT) approach for Vision Transformer (ViT)
models by partially fine-tuning learnable tokens while keeping most model
parameters frozen. Recent research has explored modifying the connection
structures of the prompts. However, the fundamental correlation and
distribution between the prompts and image tokens remain unexplored. In this
paper, we leverage metric learning techniques to investigate how the
distribution of prompts affects fine-tuning performance. Specifically, we
propose a novel framework, Distribution Aware Visual Prompt Tuning (DA-VPT), to
guide the distributions of the prompts by learning the distance metric from
their class-related semantic data. Our method demonstrates that the prompts can
serve as an effective bridge to share semantic information between image
patches and the class token. We extensively evaluated our approach on popular
benchmarks in both recognition and segmentation tasks. The results demonstrate
that our approach enables more effective and efficient fine-tuning of ViT
models by leveraging semantic information to guide the learning of the prompts,
leading to improved performance on various downstream vision tasks.

</details>


### [123] [CLDTracker: A Comprehensive Language Description for Visual Tracking](https://arxiv.org/abs/2505.23704)
*Mohamad Alansari,Sajid Javed,Iyyakutti Iyappan Ganapathi,Sara Alansari,Muzammal Naseer*

Main category: cs.CV

TL;DR: 论文提出了一种名为CLDTracker的新型视觉目标跟踪框架，通过结合视觉和语言信息解决传统跟踪器在复杂场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 视觉目标跟踪（VOT）在动态外观变化、遮挡和背景干扰下表现不佳，传统方法依赖视觉线索，难以应对复杂场景。

Method: 提出CLDTracker，采用双分支架构（文本分支和视觉分支），利用CLIP和GPT-4V等模型生成丰富的文本描述，增强语义理解。

Result: 在六个标准VOT基准测试中，CLDTracker实现了最先进的性能。

Conclusion: CLDTracker通过结合视觉和语言信息，显著提升了跟踪性能，验证了其在复杂场景中的有效性。

Abstract: VOT remains a fundamental yet challenging task in computer vision due to
dynamic appearance changes, occlusions, and background clutter. Traditional
trackers, relying primarily on visual cues, often struggle in such complex
scenarios. Recent advancements in VLMs have shown promise in semantic
understanding for tasks like open-vocabulary detection and image captioning,
suggesting their potential for VOT. However, the direct application of VLMs to
VOT is hindered by critical limitations: the absence of a rich and
comprehensive textual representation that semantically captures the target
object's nuances, limiting the effective use of language information;
inefficient fusion mechanisms that fail to optimally integrate visual and
textual features, preventing a holistic understanding of the target; and a lack
of temporal modeling of the target's evolving appearance in the language
domain, leading to a disconnect between the initial description and the
object's subsequent visual changes. To bridge these gaps and unlock the full
potential of VLMs for VOT, we propose CLDTracker, a novel Comprehensive
Language Description framework for robust visual Tracking. Our tracker
introduces a dual-branch architecture consisting of a textual and a visual
branch. In the textual branch, we construct a rich bag of textual descriptions
derived by harnessing the powerful VLMs such as CLIP and GPT-4V, enriched with
semantic and contextual cues to address the lack of rich textual
representation. Experiments on six standard VOT benchmarks demonstrate that
CLDTracker achieves SOTA performance, validating the effectiveness of
leveraging robust and temporally-adaptive vision-language representations for
tracking. Code and models are publicly available at:
https://github.com/HamadYA/CLDTracker

</details>


### [124] [Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning](https://arxiv.org/abs/2505.23709)
*Dionysis Christopoulos,Sotiris Spanos,Eirini Baltzi,Valsamis Ntouskos,Konstantinos Karantzalos*

Main category: cs.CV

TL;DR: SLIMP通过结合皮肤病变图像和元数据的嵌套对比学习方法，提升皮肤病变分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决仅依赖图像进行黑色素瘤检测和皮肤病变分类的挑战，如成像条件差异大和缺乏临床背景。

Method: 提出嵌套对比学习，结合病变图像、个体元数据和患者级元数据。

Result: 相比其他预训练策略，SLIMP在下游分类任务中表现更优。

Conclusion: SLIMP通过充分利用多模态数据，提升了皮肤病变分类的准确性。

Abstract: We introduce SLIMP (Skin Lesion Image-Metadata Pre-training) for learning
rich representations of skin lesions through a novel nested contrastive
learning approach that captures complex relationships between images and
metadata. Melanoma detection and skin lesion classification based solely on
images, pose significant challenges due to large variations in imaging
conditions (lighting, color, resolution, distance, etc.) and lack of clinical
and phenotypical context. Clinicians typically follow a holistic approach for
assessing the risk level of the patient and for deciding which lesions may be
malignant and need to be excised, by considering the patient's medical history
as well as the appearance of other lesions of the patient. Inspired by this,
SLIMP combines the appearance and the metadata of individual skin lesions with
patient-level metadata relating to their medical record and other clinically
relevant information. By fully exploiting all available data modalities
throughout the learning process, the proposed pre-training strategy improves
performance compared to other pre-training strategies on downstream skin
lesions classification tasks highlighting the learned representations quality.

</details>


### [125] [AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views](https://arxiv.org/abs/2505.23716)
*Lihan Jiang,Yucheng Mao,Linning Xu,Tao Lu,Kerui Ren,Yichen Jin,Xudong Xu,Mulin Yu,Jiangmiao Pang,Feng Zhao,Dahua Lin,Bo Dai*

Main category: cs.CV

TL;DR: AnySplat是一种前馈网络，用于从未校准的图像集合中进行新视角合成，无需已知相机姿态或逐场景优化。


<details>
  <summary>Details</summary>
Motivation: 传统神经渲染流程需要已知相机姿态和逐场景优化，而现有前馈方法在密集视图下计算负担大。AnySplat旨在解决这些问题。

Method: 通过单次前向传播预测3D高斯基元（编码场景几何和外观）及每张输入图像的相机内外参数。

Result: 在零样本评估中，AnySplat在稀疏和密集视图场景下均达到与已知姿态基线相当的质量，并超越现有无姿态方法。

Conclusion: AnySplat显著降低了基于优化的神经场的渲染延迟，为无约束捕获设置下的实时新视角合成提供了可能。

Abstract: We introduce AnySplat, a feed forward network for novel view synthesis from
uncalibrated image collections. In contrast to traditional neural rendering
pipelines that demand known camera poses and per scene optimization, or recent
feed forward methods that buckle under the computational weight of dense views,
our model predicts everything in one shot. A single forward pass yields a set
of 3D Gaussian primitives encoding both scene geometry and appearance, and the
corresponding camera intrinsics and extrinsics for each input image. This
unified design scales effortlessly to casually captured, multi view datasets
without any pose annotations. In extensive zero shot evaluations, AnySplat
matches the quality of pose aware baselines in both sparse and dense view
scenarios while surpassing existing pose free approaches. Moreover, it greatly
reduce rendering latency compared to optimization based neural fields, bringing
real time novel view synthesis within reach for unconstrained capture
settings.Project page: https://city-super.github.io/anysplat/

</details>


### [126] [FMG-Det: Foundation Model Guided Robust Object Detection](https://arxiv.org/abs/2505.23726)
*Darryl Hannan,Timothy Doster,Henry Kvinge,Adam Attarian,Yijing Watkins*

Main category: cs.CV

TL;DR: 论文提出FMG-Det方法，通过结合多实例学习框架和预训练基础模型，高效处理噪声标注，提升目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 目标检测任务中标注边界的主观性导致数据质量难以保证，噪声标注显著影响模型性能，尤其是在少样本场景下。

Method: 结合多实例学习（MIL）框架和预训练基础模型的预处理流程，修正标注后再训练，同时对检测头进行微调。

Result: 在多个数据集上实现了最先进的性能，适用于标准和少样本场景，且方法更简单高效。

Conclusion: FMG-Det通过高效处理噪声标注，显著提升了目标检测模型的性能，尤其在少样本场景下表现突出。

Abstract: Collecting high quality data for object detection tasks is challenging due to
the inherent subjectivity in labeling the boundaries of an object. This makes
it difficult to not only collect consistent annotations across a dataset but
also to validate them, as no two annotators are likely to label the same object
using the exact same coordinates. These challenges are further compounded when
object boundaries are partially visible or blurred, which can be the case in
many domains. Training on noisy annotations significantly degrades detector
performance, rendering them unusable, particularly in few-shot settings, where
just a few corrupted annotations can impact model performance. In this work, we
propose FMG-Det, a simple, efficient methodology for training models with noisy
annotations. More specifically, we propose combining a multiple instance
learning (MIL) framework with a pre-processing pipeline that leverages powerful
foundation models to correct labels prior to training. This pre-processing
pipeline, along with slight modifications to the detector head, results in
state-of-the-art performance across a number of datasets, for both standard and
few-shot scenarios, while being much simpler and more efficient than other
approaches.

</details>


### [127] [PixelThink: Towards Efficient Chain-of-Pixel Reasoning](https://arxiv.org/abs/2505.23727)
*Song Wang,Gongfan Fang,Lingdong Kong,Xiangtai Li,Jianyun Xu,Sheng Yang,Qiang Li,Jianke Zhu,Xinchao Wang*

Main category: cs.CV

TL;DR: PixelThink通过结合任务难度和模型不确定性优化推理生成，提升分割性能和推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分布外场景泛化能力有限且推理过程冗长，导致计算成本高和质量控制困难。

Method: 提出PixelThink，利用外部任务难度和内部模型不确定性调节推理生成，压缩推理长度。

Result: 实验表明，该方法提升了推理效率和分割性能。

Conclusion: PixelThink为高效可解释的多模态理解提供了新视角。

Abstract: Existing reasoning segmentation approaches typically fine-tune multimodal
large language models (MLLMs) using image-text pairs and corresponding mask
labels. However, they exhibit limited generalization to out-of-distribution
scenarios without an explicit reasoning process. Although recent efforts
leverage reinforcement learning through group-relative policy optimization
(GRPO) to enhance reasoning ability, they often suffer from overthinking -
producing uniformly verbose reasoning chains irrespective of task complexity.
This results in elevated computational costs and limited control over reasoning
quality. To address this problem, we propose PixelThink, a simple yet effective
scheme that integrates externally estimated task difficulty and internally
measured model uncertainty to regulate reasoning generation within a
reinforcement learning paradigm. The model learns to compress reasoning length
in accordance with scene complexity and predictive confidence. To support
comprehensive evaluation, we introduce ReasonSeg-Diff, an extended benchmark
with annotated reasoning references and difficulty scores, along with a suite
of metrics designed to assess segmentation accuracy, reasoning quality, and
efficiency jointly. Experimental results demonstrate that the proposed approach
improves both reasoning efficiency and overall segmentation performance. Our
work contributes novel perspectives towards efficient and interpretable
multimodal understanding. The code and model will be publicly available.

</details>


### [128] [ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS](https://arxiv.org/abs/2505.23734)
*Weijie Wang,Donny Y. Chen,Zeyu Zhang,Duochao Shi,Akide Liu,Bohan Zhuang*

Main category: cs.CV

TL;DR: ZPressor是一个轻量级模块，通过压缩多视图输入为紧凑潜在状态Z，提升3D高斯泼溅模型的扩展性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有前馈3D高斯泼溅模型因编码器容量有限导致的性能下降或内存消耗过高问题。

Method: 通过分区视图为锚点和支持集，利用交叉注意力压缩信息，形成潜在状态Z。

Result: 在DL3DV-10K和RealEstate10K基准测试中，ZPressor显著提升了模型性能和鲁棒性。

Conclusion: ZPressor有效解决了3D高斯泼溅模型的扩展性问题，适用于多视图输入场景。

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a
promising solution for novel view synthesis, enabling one-pass inference
without the need for per-scene 3DGS optimization. However, their scalability is
fundamentally constrained by the limited capacity of their encoders, leading to
degraded performance or excessive memory consumption as the number of input
views increases. In this work, we analyze feed-forward 3DGS frameworks through
the lens of the Information Bottleneck principle and introduce ZPressor, a
lightweight architecture-agnostic module that enables efficient compression of
multi-view inputs into a compact latent state $Z$ that retains essential scene
information while discarding redundancy. Concretely, ZPressor enables existing
feed-forward 3DGS models to scale to over 100 input views at 480P resolution on
an 80GB GPU, by partitioning the views into anchor and support sets and using
cross attention to compress the information from the support views into anchor
views, forming the compressed latent state $Z$. We show that integrating
ZPressor into several state-of-the-art feed-forward 3DGS models consistently
improves performance under moderate input views and enhances robustness under
dense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K.
The video results, code and trained models are available on our project page:
https://lhmd.top/zpressor.

</details>


### [129] [MAGREF: Masked Guidance for Any-Reference Video Generation](https://arxiv.org/abs/2505.23742)
*Yufan Deng,Xun Guo,Yuanyang Yin,Jacob Zhiyuan Fang,Yiding Yang,Yizhi Wang,Shenghai Yuan,Angtian Wang,Bo Liu,Haibin Huang,Chongyang Ma*

Main category: cs.CV

TL;DR: MAGREF提出了一种基于掩码引导的统一框架，用于多参考视频生成，解决了多主体一致性和生成质量的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成在多参考主体条件下难以保持一致性且生成质量有限，需要一种更灵活且高质量的解决方案。

Method: 提出区域感知动态掩码机制和像素级通道拼接机制，支持多主体视频合成。

Result: 模型在多主体视频生成中表现优异，超越现有开源和商业基线。

Conclusion: MAGREF为可扩展、可控且高保真的多主体视频合成提供了有效途径。

Abstract: Video generation has made substantial strides with the emergence of deep
generative models, especially diffusion-based approaches. However, video
generation based on multiple reference subjects still faces significant
challenges in maintaining multi-subject consistency and ensuring high
generation quality. In this paper, we propose MAGREF, a unified framework for
any-reference video generation that introduces masked guidance to enable
coherent multi-subject video synthesis conditioned on diverse reference images
and a textual prompt. Specifically, we propose (1) a region-aware dynamic
masking mechanism that enables a single model to flexibly handle various
subject inference, including humans, objects, and backgrounds, without
architectural changes, and (2) a pixel-wise channel concatenation mechanism
that operates on the channel dimension to better preserve appearance features.
Our model delivers state-of-the-art video generation quality, generalizing from
single-subject training to complex multi-subject scenarios with coherent
synthesis and precise control over individual subjects, outperforming existing
open-source and commercial baselines. To facilitate evaluation, we also
introduce a comprehensive multi-subject video benchmark. Extensive experiments
demonstrate the effectiveness of our approach, paving the way for scalable,
controllable, and high-fidelity multi-subject video synthesis. Code and model
can be found at: https://github.com/MAGREF-Video/MAGREF

</details>


### [130] [DarkDiff: Advancing Low-Light Raw Enhancement by Retasking Diffusion Models for Camera ISP](https://arxiv.org/abs/2505.23743)
*Amber Yijia Zheng,Yu Zhang,Jun Hu,Raymond A. Yeh,Chen Chen*

Main category: cs.CV

TL;DR: 提出了一种基于预训练生成扩散模型的新框架，用于增强低光原始图像，显著提升了感知质量。


<details>
  <summary>Details</summary>
Motivation: 在极端低光条件下拍摄高质量照片具有挑战性但意义重大，现有回归模型容易导致图像过度平滑或阴影过深。

Method: 通过重新利用预训练的生成扩散模型与相机ISP结合，增强低光原始图像。

Result: 在三个低光原始图像基准测试中，该方法在感知质量上优于现有最优方法。

Conclusion: 该方法有效解决了低光图像增强中的细节恢复和颜色准确性难题。

Abstract: High-quality photography in extreme low-light conditions is challenging but
impactful for digital cameras. With advanced computing hardware, traditional
camera image signal processor (ISP) algorithms are gradually being replaced by
efficient deep networks that enhance noisy raw images more intelligently.
However, existing regression-based models often minimize pixel errors and
result in oversmoothing of low-light photos or deep shadows. Recent work has
attempted to address this limitation by training a diffusion model from
scratch, yet those models still struggle to recover sharp image details and
accurate colors. We introduce a novel framework to enhance low-light raw images
by retasking pre-trained generative diffusion models with the camera ISP.
Extensive experiments demonstrate that our method outperforms the
state-of-the-art in perceptual quality across three challenging low-light raw
image benchmarks.

</details>


### [131] [Boosting Domain Incremental Learning: Selecting the Optimal Parameters is All You Need](https://arxiv.org/abs/2505.23744)
*Qiang Wang,Xiang Song,Yuhang He,Jizhou Han,Chenhao Ding,Xinyuan Gao,Yihong Gong*

Main category: cs.CV

TL;DR: SOYO是一个轻量级框架，通过高斯混合压缩器和域特征重采样器改进参数隔离域增量学习中的域选择问题，实验证明其在多个任务中优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在动态环境中表现不佳，参数隔离域增量学习（PIDIL）虽能减少知识冲突，但现有方法在参数选择准确性上存在问题。

Method: SOYO引入高斯混合压缩器（GMC）和域特征重采样器（DFR）存储和平衡先验数据，并通过多级域特征融合网络（MDFN）增强特征提取。

Result: 在六个基准测试中，SOYO表现优于现有基线，展示了其在复杂动态环境中的鲁棒性和适应性。

Conclusion: SOYO为域增量学习提供了一种高效、轻量级的解决方案，适用于多种任务。

Abstract: Deep neural networks (DNNs) often underperform in real-world, dynamic
settings where data distributions change over time. Domain Incremental Learning
(DIL) offers a solution by enabling continual model adaptation, with
Parameter-Isolation DIL (PIDIL) emerging as a promising paradigm to reduce
knowledge conflicts. However, existing PIDIL methods struggle with parameter
selection accuracy, especially as the number of domains and corresponding
classes grows. To address this, we propose SOYO, a lightweight framework that
improves domain selection in PIDIL. SOYO introduces a Gaussian Mixture
Compressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior
domain data efficiently, while a Multi-level Domain Feature Fusion Network
(MDFN) enhances domain feature extraction. Our framework supports multiple
Parameter-Efficient Fine-Tuning (PEFT) methods and is validated across tasks
such as image classification, object detection, and speech enhancement.
Experimental results on six benchmarks demonstrate SOYO's consistent
superiority over existing baselines, showcasing its robustness and adaptability
in complex, evolving environments. The codes will be released in
https://github.com/qwangcv/SOYO.

</details>


### [132] [To Trust Or Not To Trust Your Vision-Language Model's Prediction](https://arxiv.org/abs/2505.23745)
*Hao Dong,Moru Liu,Jian Liang,Eleni Chatzi,Olga Fink*

Main category: cs.CV

TL;DR: TrustVLM是一个无需训练的框架，旨在解决视觉语言模型（VLM）预测可信度评估的关键挑战，通过利用图像嵌入空间的特性提升误分类检测。


<details>
  <summary>Details</summary>
Motivation: VLM在零样本和迁移学习中表现优异，但在安全关键领域可能产生高置信度的错误预测，带来严重后果。

Method: 提出了一种基于图像嵌入空间的置信度评分函数，利用模态间隙和概念在嵌入空间中的区分性。

Result: 在17个数据集上验证，性能显著提升（AURC提高51.87%，AUROC提高9.14%，FPR95降低32.42%）。

Conclusion: TrustVLM无需重新训练即可提升VLM的可靠性，为其在现实应用中的安全部署铺平道路。

Abstract: Vision-Language Models (VLMs) have demonstrated strong capabilities in
aligning visual and textual modalities, enabling a wide range of applications
in multimodal understanding and generation. While they excel in zero-shot and
transfer learning scenarios, VLMs remain susceptible to misclassification,
often yielding confident yet incorrect predictions. This limitation poses a
significant risk in safety-critical domains, where erroneous predictions can
lead to severe consequences. In this work, we introduce TrustVLM, a
training-free framework designed to address the critical challenge of
estimating when VLM's predictions can be trusted. Motivated by the observed
modality gap in VLMs and the insight that certain concepts are more distinctly
represented in the image embedding space, we propose a novel confidence-scoring
function that leverages this space to improve misclassification detection. We
rigorously evaluate our approach across 17 diverse datasets, employing 4
architectures and 2 VLMs, and demonstrate state-of-the-art performance, with
improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95
compared to existing baselines. By improving the reliability of the model
without requiring retraining, TrustVLM paves the way for safer deployment of
VLMs in real-world applications. The code will be available at
https://github.com/EPFL-IMOS/TrustVLM.

</details>


### [133] [Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence](https://arxiv.org/abs/2505.23747)
*Diankun Wu,Fangfu Liu,Yi-Hsin Hung,Yueqi Duan*

Main category: cs.CV

TL;DR: Spatial-MLLM是一个新型框架，通过纯2D观察实现视觉空间推理，无需依赖额外的3D数据。


<details>
  <summary>Details</summary>
Motivation: 现有的3D MLLM依赖额外3D或2.5D数据，限制了在仅有2D输入（如图像或视频）场景中的应用。

Method: 提出双编码器架构：语义编码器和空间编码器，结合空间感知帧采样策略。

Result: 在多种真实数据集上实现最先进的视觉空间理解和推理性能。

Conclusion: Spatial-MLLM为纯2D输入场景提供了高效的视觉空间推理解决方案。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have
significantly enhanced performance on 2D visual tasks. However, improving their
spatial intelligence remains a challenge. Existing 3D MLLMs always rely on
additional 3D or 2.5D data to incorporate spatial awareness, restricting their
utility in scenarios with only 2D inputs, such as images or videos. In this
paper, we present Spatial-MLLM, a novel framework for visual-based spatial
reasoning from purely 2D observations. Unlike conventional video MLLMs which
rely on CLIP-based visual encoders optimized for semantic understanding, our
key insight is to unleash the strong structure prior from the feed-forward
visual geometry foundation model. Specifically, we propose a dual-encoder
architecture: a pretrained 2D visual encoder to extract semantic features, and
a spatial encoder-initialized from the backbone of the visual geometry model-to
extract 3D structure features. A connector then integrates both features into
unified visual tokens for enhanced spatial understanding. Furthermore, we
propose a space-aware frame sampling strategy at inference time, which selects
the spatially informative frames of a video sequence, ensuring that even under
limited token length, the model focuses on frames critical for spatial
reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k
dataset and train the model on it using supervised fine-tuning and GRPO.
Extensive experiments on various real-world datasets demonstrate that our
spatial-MLLM achieves state-of-the-art performance in a wide range of
visual-based spatial understanding and reasoning tasks. Project page:
https://diankun-wu.github.io/Spatial-MLLM/.

</details>


### [134] [ThinkGeo: Evaluating Tool-Augmented Agents for Remote Sensing Tasks](https://arxiv.org/abs/2505.23752)
*Akashah Shabbir,Muhammad Akhtar Munir,Akshay Dudhane,Muhammad Umer Sheikh,Muhammad Haris Khan,Paolo Fraccaro,Juan Bernabe Moreno,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: ThinkGeo是一个评估LLM驱动代理在遥感任务中工具使用能力的基准测试，填补了领域特定评估的空白。


<details>
  <summary>Details</summary>
Motivation: 现有评估多关注通用或多模态场景，缺乏针对复杂遥感用例的工具使用能力评估。

Method: 通过结构化工具使用和多步规划，设计包含436个任务的基准测试，采用ReAct式交互循环评估开源和闭源LLM。

Result: 分析显示不同模型在工具准确性和规划一致性上存在显著差异。

Conclusion: ThinkGeo为评估工具增强LLM在遥感中的空间推理能力提供了首个广泛测试平台。

Abstract: Recent progress in large language models (LLMs) has enabled tool-augmented
agents capable of solving complex real-world tasks through step-by-step
reasoning. However, existing evaluations often focus on general-purpose or
multimodal scenarios, leaving a gap in domain-specific benchmarks that assess
tool-use capabilities in complex remote sensing use cases. We present ThinkGeo,
an agentic benchmark designed to evaluate LLM-driven agents on remote sensing
tasks via structured tool use and multi-step planning. Inspired by
tool-interaction paradigms, ThinkGeo includes human-curated queries spanning a
wide range of real-world applications such as urban planning, disaster
assessment and change analysis, environmental monitoring, transportation
analysis, aviation monitoring, recreational infrastructure, and industrial site
analysis. Each query is grounded in satellite or aerial imagery and requires
agents to reason through a diverse toolset. We implement a ReAct-style
interaction loop and evaluate both open and closed-source LLMs (e.g., GPT-4o,
Qwen2.5) on 436 structured agentic tasks. The benchmark reports both step-wise
execution metrics and final answer correctness. Our analysis reveals notable
disparities in tool accuracy and planning consistency across models. ThinkGeo
provides the first extensive testbed for evaluating how tool-enabled LLMs
handle spatial reasoning in remote sensing. Our code and dataset are publicly
available

</details>


### [135] [Rooms from Motion: Un-posed Indoor 3D Object Detection as Localization and Mapping](https://arxiv.org/abs/2505.23756)
*Justin Lazarow,Kai Kang,Afshin Dehghan*

Main category: cs.CV

TL;DR: Rooms from Motion (RfM) 是一种基于物体中心的框架，通过3D定向框实现场景级3D物体检测、定位和映射，无需预先已知相机位姿。


<details>
  <summary>Details</summary>
Motivation: 现有3D物体检测方法依赖全局信息和预先已知的相机位姿，而RfM旨在从无位姿图像集合中实现物体检测和映射。

Method: RfM用基于3D框的物体中心匹配器替代传统2D关键点匹配器，估计相机位姿和物体轨迹，生成全局语义3D物体地图。若已知位姿，可通过优化全局3D框提升地图质量。

Result: RfM在CA-1M和ScanNet++数据集上表现优于基于点和多视图的3D物体检测方法，生成更高质量的地图。

Conclusion: RfM提供了一种通用的物体中心表示，支持稀疏定位和参数化映射，适用于场景中的物体数量。

Abstract: We revisit scene-level 3D object detection as the output of an object-centric
framework capable of both localization and mapping using 3D oriented boxes as
the underlying geometric primitive. While existing 3D object detection
approaches operate globally and implicitly rely on the a priori existence of
metric camera poses, our method, Rooms from Motion (RfM) operates on a
collection of un-posed images. By replacing the standard 2D keypoint-based
matcher of structure-from-motion with an object-centric matcher based on
image-derived 3D boxes, we estimate metric camera poses, object tracks, and
finally produce a global, semantic 3D object map. When a priori pose is
available, we can significantly improve map quality through optimization of
global 3D boxes against individual observations. RfM shows strong localization
performance and subsequently produces maps of higher quality than leading
point-based and multi-view 3D object detection methods on CA-1M and ScanNet++,
despite these global methods relying on overparameterization through point
clouds or dense volumes. Rooms from Motion achieves a general, object-centric
representation which not only extends the work of Cubify Anything to full
scenes but also allows for inherently sparse localization and parametric
mapping proportional to the number of objects in a scene.

</details>


### [136] [Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models](https://arxiv.org/abs/2505.23757)
*Haohan Chi,Huan-ang Gao,Ziming Liu,Jianing Liu,Chenyu Liu,Jinwei Li,Kaisen Yang,Yangcheng Yu,Zeda Wang,Wenyi Li,Leichen Wang,Xingtao Hu,Hao Sun,Hang Zhao,Hao Zhao*

Main category: cs.CV

TL;DR: Impromptu VLA提出了一种新数据集，用于提升Vision-Language-Action模型在自动驾驶中的表现，特别是在非结构化场景下。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在非结构化极端场景中表现不佳，缺乏针对性基准测试。

Method: 构建了包含80,000个视频片段的Impromptu VLA数据集，涵盖四类非结构化场景，并包含规划导向的问答注释和动作轨迹。

Result: 实验表明，使用该数据集训练的VLA模型在性能上有显著提升，包括闭环NeuroNCAP评分、碰撞率及开环nuScenes轨迹预测精度。

Conclusion: Impromptu VLA数据集有效提升了VLA模型的性能，并为感知、预测和规划提供了诊断工具。

Abstract: Vision-Language-Action (VLA) models for autonomous driving show promise but
falter in unstructured corner case scenarios, largely due to a scarcity of
targeted benchmarks. To address this, we introduce Impromptu VLA. Our core
contribution is the Impromptu VLA Dataset: over 80,000 meticulously curated
video clips, distilled from over 2M source clips sourced from 8 open-source
large-scale datasets. This dataset is built upon our novel taxonomy of four
challenging unstructured categories and features rich, planning-oriented
question-answering annotations and action trajectories. Crucially, experiments
demonstrate that VLAs trained with our dataset achieve substantial performance
gains on established benchmarks--improving closed-loop NeuroNCAP scores and
collision rates, and reaching near state-of-the-art L2 accuracy in open-loop
nuScenes trajectory prediction. Furthermore, our Q&A suite serves as an
effective diagnostic, revealing clear VLM improvements in perception,
prediction, and planning. Our code, data and models are available at
https://github.com/ahydchh/Impromptu-VLA.

</details>


### [137] [LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers](https://arxiv.org/abs/2505.23758)
*Yusuf Dalva,Hidir Yesiltepe,Pinar Yanardag*

Main category: cs.CV

TL;DR: LoRAShop是一个基于LoRA模型的多概念图像编辑框架，通过解耦潜在掩码和局部权重混合实现高效编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多概念图像编辑中难以保持全局一致性和细节，LoRAShop旨在解决这一问题。

Method: 利用Flux-style扩散变换器的特征交互模式，生成解耦潜在掩码，并在局部区域混合LoRA权重。

Result: 实验表明，LoRAShop在身份保持和编辑效果上优于基线方法。

Conclusion: LoRAShop为个性化扩散模型提供了一种实用的编辑工具，推动了视觉创作的发展。

Abstract: We introduce LoRAShop, the first framework for multi-concept image editing
with LoRA models. LoRAShop builds on a key observation about the feature
interaction patterns inside Flux-style diffusion transformers: concept-specific
transformer features activate spatially coherent regions early in the denoising
process. We harness this observation to derive a disentangled latent mask for
each concept in a prior forward pass and blend the corresponding LoRA weights
only within regions bounding the concepts to be personalized. The resulting
edits seamlessly integrate multiple subjects or styles into the original scene
while preserving global context, lighting, and fine details. Our experiments
demonstrate that LoRAShop delivers better identity preservation compared to
baselines. By eliminating retraining and external constraints, LoRAShop turns
personalized diffusion models into a practical `photoshop-with-LoRAs' tool and
opens new avenues for compositional visual storytelling and rapid creative
iteration.

</details>


### [138] [Sketch Down the FLOPs: Towards Efficient Networks for Human Sketch](https://arxiv.org/abs/2505.23763)
*Aneeshan Sain,Subhajit Maity,Pinaki Nath Chowdhury,Subhadeep Koley,Ayan Kumar Bhunia,Yi-Zhe Song*

Main category: cs.CV

TL;DR: 论文提出两种针对草图数据的组件，通过跨模态知识蒸馏和基于强化学习的画布选择器，显著降低计算量（FLOPs减少99.37%），同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 现有针对照片的高效轻量模型不适用于草图数据，缺乏专门为草图设计的高效推理研究。

Method: 1. 跨模态知识蒸馏网络将照片高效网络适配到草图数据；2. 基于强化学习的画布选择器动态调整抽象级别。

Result: FLOPs减少99.37%（从40.18G降至0.254G），精度几乎不变（33.03% vs 32.77%）。

Conclusion: 提出的方法成功为稀疏草图数据设计了高效网络，计算量甚至低于最佳照片模型。

Abstract: As sketch research has collectively matured over time, its adaptation for
at-mass commercialisation emerges on the immediate horizon. Despite an already
mature research endeavour for photos, there is no research on the efficient
inference specifically designed for sketch data. In this paper, we first
demonstrate existing state-of-the-art efficient light-weight models designed
for photos do not work on sketches. We then propose two sketch-specific
components which work in a plug-n-play manner on any photo efficient network to
adapt them to work on sketch data. We specifically chose fine-grained
sketch-based image retrieval (FG-SBIR) as a demonstrator as the most recognised
sketch problem with immediate commercial value. Technically speaking, we first
propose a cross-modal knowledge distillation network to transfer existing photo
efficient networks to be compatible with sketch, which brings down number of
FLOPs and model parameters by 97.96% percent and 84.89% respectively. We then
exploit the abstract trait of sketch to introduce a RL-based canvas selector
that dynamically adjusts to the abstraction level which further cuts down
number of FLOPs by two thirds. The end result is an overall reduction of 99.37%
of FLOPs (from 40.18G to 0.254G) when compared with a full network, while
retaining the accuracy (33.03% vs 32.77%) -- finally making an efficient
network for the sparse sketch data that exhibit even fewer FLOPs than the best
photo counterpart.

</details>


### [139] [MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence](https://arxiv.org/abs/2505.23764)
*Sihan Yang,Runsen Xu,Yiman Xie,Sizhe Yang,Mo Li,Jingli Lin,Chenming Zhu,Xiaochen Chen,Haodong Duan,Xiangyu Yue,Dahua Lin,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: MMSI-Bench是一个专注于多图像空间智能的VQA基准测试，旨在评估MLLMs在复杂物理世界中的多图像空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅关注单图像关系，无法满足实际部署中对多图像空间推理的需求，因此需要开发新的评估工具。

Method: 通过6名3D视觉研究人员耗时300多小时，从12万张图像中精心设计1000个具有挑战性的多选题，并评估34个开源和专有MLLMs。

Result: 实验结果显示，开源模型最高准确率约30%，OpenAI的o3推理模型达到40%，而人类得分高达97%，表明MMSI-Bench的挑战性和未来研究空间。

Conclusion: MMSI-Bench为多图像空间智能研究提供了重要基准，并通过错误分析揭示了主要失败模式，为未来研究指明了方向。

Abstract: Spatial intelligence is essential for multimodal large language models
(MLLMs) operating in the complex physical world. Existing benchmarks, however,
probe only single-image relations and thus fail to assess the multi-image
spatial reasoning that real-world deployments demand. We introduce MMSI-Bench,
a VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision
researchers spent more than 300 hours meticulously crafting 1,000 challenging,
unambiguous multiple-choice questions from over 120,000 images, each paired
with carefully designed distractors and a step-by-step reasoning process. We
conduct extensive experiments and thoroughly evaluate 34 open-source and
proprietary MLLMs, observing a wide gap: the strongest open-source model
attains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while
humans score 97%. These results underscore the challenging nature of MMSI-Bench
and the substantial headroom for future research. Leveraging the annotated
reasoning processes, we also provide an automated error analysis pipeline that
diagnoses four dominant failure modes, including (1) grounding errors, (2)
overlap-matching and scene-reconstruction errors, (3) situation-transformation
reasoning errors, and (4) spatial-logic errors, offering valuable insights for
advancing multi-image spatial intelligence. Project page:
https://runsenxu.com/projects/MMSI_Bench .

</details>


### [140] [Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought](https://arxiv.org/abs/2505.23766)
*Yunze Man,De-An Huang,Guilin Liu,Shiwei Sheng,Shilong Liu,Liang-Yan Gui,Jan Kautz,Yu-Xiong Wang,Zhiding Yu*

Main category: cs.CV

TL;DR: Argus通过视觉注意力机制改进MLLMs在视觉中心任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在需要精确视觉聚焦的任务中表现不佳。

Method: 提出Argus，采用对象中心视觉链式思维信号，增强目标条件视觉注意力。

Result: 在多种基准测试中表现优异，验证了设计的有效性。

Conclusion: 语言引导的视觉兴趣区域聚焦对MLLMs的视觉中心智能发展至关重要。

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated
remarkable capabilities in vision-language tasks, yet they often struggle with
vision-centric scenarios where precise visual focus is needed for accurate
reasoning. In this paper, we introduce Argus to address these limitations with
a new visual attention grounding mechanism. Our approach employs object-centric
grounding as visual chain-of-thought signals, enabling more effective
goal-conditioned visual attention during multimodal reasoning tasks.
Evaluations on diverse benchmarks demonstrate that Argus excels in both
multimodal reasoning tasks and referring object grounding tasks. Extensive
analysis further validates various design choices of Argus, and reveals the
effectiveness of explicit language-guided visual region-of-interest engagement
in MLLMs, highlighting the importance of advancing multimodal intelligence from
a visual-centric perspective. Project page: https://yunzeman.github.io/argus/

</details>


### [141] [TextRegion: Text-Aligned Region Tokens from Frozen Image-Text Models](https://arxiv.org/abs/2505.23769)
*Yao Xiao,Qiqian Fu,Heyi Tao,Yuqun Wu,Zhen Zhu,Derek Hoiem*

Main category: cs.CV

TL;DR: TextRegion结合图像-文本模型和SAM2的优势，生成文本对齐的区域标记，支持详细视觉理解，无需训练，适用于多种下游任务。


<details>
  <summary>Details</summary>
Motivation: 图像-文本模型在图像级任务表现优秀，但在详细视觉理解方面不足，而分割模型（如SAM2）能提供精确的空间边界。结合两者优势以提升性能。

Method: 提出TextRegion框架，结合图像-文本模型和SAM2，生成文本对齐的区域标记，保留开放词汇能力。

Result: 在开放世界语义分割、指代表达理解等任务中表现优异或与最先进的无训练方法竞争。

Conclusion: TextRegion简单有效，兼容多种图像-文本模型，实用且易于扩展。

Abstract: Image-text models excel at image-level tasks but struggle with detailed
visual understanding. While these models provide strong visual-language
alignment, segmentation models like SAM2 offer precise spatial boundaries for
objects. To this end, we propose TextRegion, a simple, effective, and
training-free framework that combines the strengths of image-text models and
SAM2 to generate powerful text-aligned region tokens. These tokens enable
detailed visual understanding while preserving open-vocabulary capabilities.
They can be directly applied to various downstream tasks, including open-world
semantic segmentation, referring expression comprehension, and grounding. We
conduct extensive evaluations and consistently achieve superior or competitive
performance compared to state-of-the-art training-free methods. Additionally,
our framework is compatible with many image-text models, making it highly
practical and easily extensible as stronger models emerge. Code is available
at: https://github.com/avaxiao/TextRegion.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [142] [MRI Image Generation Based on Text Prompts](https://arxiv.org/abs/2505.22682)
*Xinxian Fan,Mengye Lyu*

Main category: eess.IV

TL;DR: 研究利用Stable Diffusion模型通过文本提示生成MRI图像，解决了真实MRI数据集获取的高成本、稀有样本少和隐私问题。模型在3T和0.3T数据集上微调后，生成的图像质量提升，且能有效辅助MRI分类任务。


<details>
  <summary>Details</summary>
Motivation: 解决真实MRI数据集获取的高成本、稀有样本少和隐私问题，探索文本提示生成MRI图像的可行性。

Method: 使用Stable Diffusion模型，在3T fastMRI和0.3T M4Raw数据集上微调，生成不同磁场强度的T1、T2和FLAIR图像，并通过FID和MS-SSIM评估性能。

Result: 微调后的模型在图像质量和语义一致性上表现更好，生成的合成图像能有效增强训练数据集，提升MRI分类任务性能。

Conclusion: 文本提示生成MRI图像可行，可作为医学AI应用的有用工具。

Abstract: This study explores the use of text-prompted MRI image generation with the
Stable Diffusion (SD) model to address challenges in acquiring real MRI
datasets, such as high costs, limited rare case samples, and privacy concerns.
The SD model, pre-trained on natural images, was fine-tuned using the 3T
fastMRI dataset and the 0.3T M4Raw dataset, with the goal of generating brain
T1, T2, and FLAIR images across different magnetic field strengths. The
performance of the fine-tuned model was evaluated using quantitative
metrics,including Fr\'echet Inception Distance (FID) and Multi-Scale Structural
Similarity (MS-SSIM), showing improvements in image quality and semantic
consistency with the text prompts. To further evaluate the model's potential, a
simple classification task was carried out using a small 0.35T MRI dataset,
demonstrating that the synthetic images generated by the fine-tuned SD model
can effectively augment training datasets and improve the performance of MRI
constrast classification tasks. Overall, our findings suggest that
text-prompted MRI image generation is feasible and can serve as a useful tool
for medical AI applications.

</details>


### [143] [DeepMultiConnectome: Deep Multi-Task Prediction of Structural Connectomes Directly from Diffusion MRI Tractography](https://arxiv.org/abs/2505.22685)
*Marcus J. Vroemen,Yuqian Chen,Yui Lo,Tengfei Xu,Weidong Cai,Fan Zhang,Josien P. W. Pluim,Lauren J. O'Donnell*

Main category: eess.IV

TL;DR: DeepMultiConnectome是一种深度学习模型，直接从纤维追踪预测结构连接组，无需灰质分区，支持多种分区方案，速度快且可扩展。


<details>
  <summary>Details</summary>
Motivation: 传统连接组生成方法耗时且依赖灰质分区，难以适用于大规模研究。

Method: 采用基于点云的神经网络和多任务学习，分类流线并共享学习表示，支持两种灰质分区方案。

Result: 预测的连接组与传统方法生成的连接组高度相关（r=0.992和r=0.986），保留了网络特性，且测试-重测分析显示可重复性相当。

Conclusion: DeepMultiConnectome为跨多种分区方案生成个体化连接组提供了快速、可扩展的解决方案。

Abstract: Diffusion MRI (dMRI) tractography enables in vivo mapping of brain structural
connections, but traditional connectome generation is time-consuming and
requires gray matter parcellation, posing challenges for large-scale studies.
We introduce DeepMultiConnectome, a deep-learning model that predicts
structural connectomes directly from tractography, bypassing the need for gray
matter parcellation while supporting multiple parcellation schemes. Using a
point-cloud-based neural network with multi-task learning, the model classifies
streamlines according to their connected regions across two parcellation
schemes, sharing a learned representation. We train and validate
DeepMultiConnectome on tractography from the Human Connectome Project Young
Adult dataset ($n = 1000$), labeled with an 84 and 164 region gray matter
parcellation scheme. DeepMultiConnectome predicts multiple structural
connectomes from a whole-brain tractogram containing 3 million streamlines in
approximately 40 seconds. DeepMultiConnectome is evaluated by comparing
predicted connectomes with traditional connectomes generated using the
conventional method of labeling streamlines using a gray matter parcellation.
The predicted connectomes are highly correlated with traditionally generated
connectomes ($r = 0.992$ for an 84-region scheme; $r = 0.986$ for a 164-region
scheme) and largely preserve network properties. A test-retest analysis of
DeepMultiConnectome demonstrates reproducibility comparable to traditionally
generated connectomes. The predicted connectomes perform similarly to
traditionally generated connectomes in predicting age and cognitive function.
Overall, DeepMultiConnectome provides a scalable, fast model for generating
subject-specific connectomes across multiple parcellation schemes.

</details>


### [144] [Plug-and-Play Posterior Sampling for Blind Inverse Problems](https://arxiv.org/abs/2505.22923)
*Anqi Li,Weijie Gan,Ulugbek S. Kamilov*

Main category: eess.IV

TL;DR: Blind-PnPDM是一种解决盲逆问题的新框架，通过交替高斯去噪方案和扩散模型作为先验，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决盲逆问题中目标图像和测量算子均未知的挑战，避免依赖显式先验或单独参数估计。

Method: 使用两个扩散模型作为先验，分别捕捉目标图像分布和测量算子参数，通过交替高斯去噪方案进行后验采样。

Result: 在盲图像去模糊实验中，Blind-PnPDM在定量指标和视觉保真度上优于现有方法。

Conclusion: 将盲逆问题视为一系列去噪子问题，并利用扩散先验的表达能力，是一种有效的方法。

Abstract: We introduce Blind Plug-and-Play Diffusion Models (Blind-PnPDM) as a novel
framework for solving blind inverse problems where both the target image and
the measurement operator are unknown. Unlike conventional methods that rely on
explicit priors or separate parameter estimation, our approach performs
posterior sampling by recasting the problem into an alternating Gaussian
denoising scheme. We leverage two diffusion models as learned priors: one to
capture the distribution of the target image and another to characterize the
parameters of the measurement operator. This PnP integration of diffusion
models ensures flexibility and ease of adaptation. Our experiments on blind
image deblurring show that Blind-PnPDM outperforms state-of-the-art methods in
terms of both quantitative metrics and visual fidelity. Our results highlight
the effectiveness of treating blind inverse problems as a sequence of denoising
subproblems while harnessing the expressive power of diffusion-based priors.

</details>


### [145] [Synthetic Generation and Latent Projection Denoising of Rim Lesions in Multiple Sclerosis](https://arxiv.org/abs/2505.23353)
*Alexandra G. Roberts,Ha M. Luu,Mert Şişman,Alexey V. Dimov,Ceren Tozlu,Ilhami Kovanlikaya,Susan A. Gauthier,Thanh D. Nguyen,Yi Wang*

Main category: eess.IV

TL;DR: 该论文提出了一种通过合成定量磁化率图（QSM）来改善多发性硬化症中边缘性铁沉积病变（PRLs）检测的方法，并引入了一种新的去噪技术以提高分类器性能。


<details>
  <summary>Details</summary>
Motivation: 多发性硬化症中的PRLs是一种新兴的生物标志物，但由于其罕见性，检测和分割面临类别不平衡问题。

Method: 通过生成合成QSM数据，扩展多通道对比，并利用生成网络的投影能力提出去噪方法。

Result: 合成数据和去噪方法显著提高了PRLs的检测性能，并更好地模拟了真实分布。

Conclusion: 该方法为PRLs的临床检测提供了更可靠的解决方案，并公开了代码和生成数据。

Abstract: Quantitative susceptibility maps from magnetic resonance images can provide
both prognostic and diagnostic information in multiple sclerosis, a
neurodegenerative disease characterized by the formation of lesions in white
matter brain tissue. In particular, susceptibility maps provide adequate
contrast to distinguish between "rim" lesions, surrounded by deposited
paramagnetic iron, and "non-rim" lesion types. These paramagnetic rim lesions
(PRLs) are an emerging biomarker in multiple sclerosis. Much effort has been
devoted to both detection and segmentation of such lesions to monitor
longitudinal change. As paramagnetic rim lesions are rare, addressing this
problem requires confronting the class imbalance between rim and non-rim
lesions. We produce synthetic quantitative susceptibility maps of paramagnetic
rim lesions and show that inclusion of such synthetic data improves classifier
performance and provide a multi-channel extension to generate accompanying
contrasts and probabilistic segmentation maps. We exploit the projection
capability of our trained generative network to demonstrate a novel denoising
approach that allows us to train on ambiguous rim cases and substantially
increase the minority class. We show that both synthetic lesion synthesis and
our proposed rim lesion label denoising method best approximate the unseen rim
lesion distribution and improve detection in a clinically interpretable manner.
We release our code and generated data at https://github.com/agr78/PRLx-GAN
upon publication.

</details>


### [146] [Self-supervised feature learning for cardiac Cine MR image reconstruction](https://arxiv.org/abs/2505.23408)
*Siying Xu,Marcel Früh,Kerstin Hammernik,Andreas Lingg,Jens Kübler,Patrick Krumm,Daniel Rueckert,Sergios Gatidis,Thomas Küstner*

Main category: eess.IV

TL;DR: 提出了一种自监督特征学习辅助重建（SSFL-Recon）框架，用于MRI重建，解决了现有监督学习方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有监督学习方法需要全采样图像，但实际中难以获取且可能引入偏差，而大量临床中的欠采样数据未被充分利用。

Method: 首先在欠采样图像上训练自监督特征提取器，学习采样不敏感特征，然后将这些特征嵌入自监督重建网络以辅助去除伪影。

Result: 实验表明，SSFL-Recon优于现有自监督方法，甚至在16倍欠采样下表现与监督学习相当或更好。

Conclusion: 该框架能有效提取全局表征，有助于去除伪影并提高重建的泛化能力。

Abstract: We propose a self-supervised feature learning assisted reconstruction
(SSFL-Recon) framework for MRI reconstruction to address the limitation of
existing supervised learning methods. Although recent deep learning-based
methods have shown promising performance in MRI reconstruction, most require
fully-sampled images for supervised learning, which is challenging in practice
considering long acquisition times under respiratory or organ motion. Moreover,
nearly all fully-sampled datasets are obtained from conventional reconstruction
of mildly accelerated datasets, thus potentially biasing the achievable
performance. The numerous undersampled datasets with different accelerations in
clinical practice, hence, remain underutilized. To address these issues, we
first train a self-supervised feature extractor on undersampled images to learn
sampling-insensitive features. The pre-learned features are subsequently
embedded in the self-supervised reconstruction network to assist in removing
artifacts. Experiments were conducted retrospectively on an in-house 2D cardiac
Cine dataset, including 91 cardiovascular patients and 38 healthy subjects. The
results demonstrate that the proposed SSFL-Recon framework outperforms existing
self-supervised MRI reconstruction methods and even exhibits comparable or
better performance to supervised learning up to $16\times$ retrospective
undersampling. The feature learning strategy can effectively extract global
representations, which have proven beneficial in removing artifacts and
increasing generalization ability during reconstruction.

</details>


### [147] [Low-Complexity Transform Adjustments For Video Coding](https://arxiv.org/abs/2505.23618)
*Amir Said,Hilmi E. Egilmez,Yung-Hsuan Chao*

Main category: eess.IV

TL;DR: 提出了一种基于DCT-2的低复杂度近似方法，用于设计三角变换，显著降低计算复杂度，同时保持编码效率。


<details>
  <summary>Details</summary>
Motivation: 现有视频编解码器使用多种可分离变换（如DCT和DST）虽能提升编码增益，但在大尺寸变换（32点及以上）时计算和内存复杂度过高。

Method: 利用DCT-2计算，并通过正交调整近似目标变换的最重要基向量。

Result: 在VVC参考软件中，该方法显著降低了计算复杂度，同时编码效率几乎不变。

Conclusion: 该方法为高效实现大尺寸三角变换提供了可行的低复杂度解决方案。

Abstract: Recent video codecs with multiple separable transforms can achieve
significant coding gains using asymmetric trigonometric transforms (DCTs and
DSTs), because they can exploit diverse statistics of residual block signals.
However, they add excessive computational and memory complexity on large
transforms (32-point and larger), since their practical software and hardware
implementations are not as efficient as of the DCT-2. This article introduces a
novel technique to design low-complexity approximations of trigonometric
transforms. The proposed method uses DCT-2 computations, and applies orthogonal
adjustments to approximate the most important basis vectors of the desired
transform. Experimental results on the Versatile Video Coding (VVC) reference
software show that the proposed approach significantly reduces the
computational complexity, while providing practically identical coding
efficiency.

</details>


### [148] [Position Dependent Prediction Combination For Intra-Frame Video Coding](https://arxiv.org/abs/2505.23672)
*Amir Said,Xin Zhao,Marta Karczewicz,Jianle Chen,Feng Zou*

Main category: eess.IV

TL;DR: 论文提出了一种改进HEVC帧内预测的方法，通过非递归滤波器提升性能，实现2.0%的平均比特率降低。


<details>
  <summary>Details</summary>
Motivation: 递归滤波器虽能改进预测效果，但阻碍了像素预测的并行计算，因此需要非递归的替代方案。

Method: 分析递归滤波器有效性，设计非递归预测器，并结合未滤波和滤波参考样本进行预测。

Result: 在AI配置下，比特率平均降低2.0%。

Conclusion: 非递归滤波器在保持并行计算能力的同时，显著提升了HEVC帧内预测性能。

Abstract: Intra-frame prediction in the High Efficiency Video Coding (HEVC) standard
can be empirically improved by applying sets of recursive two-dimensional
filters to the predicted values. However, this approach does not allow (or
complicates significantly) the parallel computation of pixel predictions. In
this work we analyze why the recursive filters are effective, and use the
results to derive sets of non-recursive predictors that have superior
performance. We present an extension to HEVC intra prediction that combines
values predicted using non-filtered and filtered (smoothed) reference samples,
depending on the prediction mode, and block size. Simulations using the HEVC
common test conditions show that a 2.0% bit rate average reduction can be
achieved compared to HEVC, for All Intra (AI) configurations.

</details>


### [149] [IRS: Incremental Relationship-guided Segmentation for Digital Pathology](https://arxiv.org/abs/2505.22855)
*Ruining Deng,Junchao Zhu,Juming Xiong,Can Cui,Tianyuan Yao,Junlin Guo,Siqi Lu,Marilyn Lionts,Mengmeng Yin,Yu Wang,Shilin Zhao,Yucheng Tang,Yihe Yang,Paul Dennis Simonson,Mert R. Sabuncu,Haichun Yang,Yuankai Huo*

Main category: eess.IV

TL;DR: 提出了一种新的增量关系引导分割（IRS）学习方案，用于处理数字病理学中的部分标注数据和持续学习问题。


<details>
  <summary>Details</summary>
Motivation: 数字病理学中全景分割面临部分标注和持续学习的挑战，需要模型能够处理新类别和未见疾病。

Method: 通过数学建模解剖关系，利用增量通用命题矩阵实现空间-时间OOD持续学习。

Result: IRS方法在多尺度病理分割中表现优异，能精确分割肾脏结构和OOD病变。

Conclusion: IRS显著提升了领域泛化能力，适用于实际数字病理学应用。

Abstract: Continual learning is rapidly emerging as a key focus in computer vision,
aiming to develop AI systems capable of continuous improvement, thereby
enhancing their value and practicality in diverse real-world applications. In
healthcare, continual learning holds great promise for continuously acquired
digital pathology data, which is collected in hospitals on a daily basis.
However, panoramic segmentation on digital whole slide images (WSIs) presents
significant challenges, as it is often infeasible to obtain comprehensive
annotations for all potential objects, spanning from coarse structures (e.g.,
regions and unit objects) to fine structures (e.g., cells). This results in
temporally and partially annotated data, posing a major challenge in developing
a holistic segmentation framework. Moreover, an ideal segmentation model should
incorporate new phenotypes, unseen diseases, and diverse populations, making
this task even more complex. In this paper, we introduce a novel and unified
Incremental Relationship-guided Segmentation (IRS) learning scheme to address
temporally acquired, partially annotated data while maintaining
out-of-distribution (OOD) continual learning capacity in digital pathology. The
key innovation of IRS lies in its ability to realize a new spatial-temporal OOD
continual learning paradigm by mathematically modeling anatomical relationships
between existing and newly introduced classes through a simple incremental
universal proposition matrix. Experimental results demonstrate that the IRS
method effectively handles the multi-scale nature of pathological segmentation,
enabling precise kidney segmentation across various structures (regions, units,
and cells) as well as OOD disease lesions at multiple magnifications. This
capability significantly enhances domain generalization, making IRS a robust
approach for real-world digital pathology applications.

</details>


### [150] [iHDR: Iterative HDR Imaging with Arbitrary Number of Exposures](https://arxiv.org/abs/2505.22971)
*Yu Yuan,Yiheng Chi,Xingguang Zhang,Stanley Chan*

Main category: eess.IV

TL;DR: 提出了一种名为iHDR的迭代融合框架，用于处理动态场景中灵活数量的输入帧，通过DiHDR和ToneNet实现高质量的HDR成像。


<details>
  <summary>Details</summary>
Motivation: 现有HDR成像方法通常针对固定数量输入设计，无法适应灵活输入帧数的场景。

Method: iHDR框架包括DiHDR（双输入HDR融合网络）和ToneNet（物理域映射网络），通过迭代融合逐步生成HDR图像。

Result: 实验证明，iHDR在灵活输入帧数情况下优于现有HDR去鬼影方法。

Conclusion: iHDR提供了一种高效且灵活的HDR成像解决方案，适用于动态场景。

Abstract: High dynamic range (HDR) imaging aims to obtain a high-quality HDR image by
fusing information from multiple low dynamic range (LDR) images. Numerous
learning-based HDR imaging methods have been proposed to achieve this for
static and dynamic scenes. However, their architectures are mostly tailored for
a fixed number (e.g., three) of inputs and, therefore, cannot apply directly to
situations beyond the pre-defined limited scope. To address this issue, we
propose a novel framework, iHDR, for iterative fusion, which comprises a
ghost-free Dual-input HDR fusion network (DiHDR) and a physics-based domain
mapping network (ToneNet). DiHDR leverages a pair of inputs to estimate an
intermediate HDR image, while ToneNet maps it back to the nonlinear domain and
serves as the reference input for the next pairwise fusion. This process is
iteratively executed until all input frames are utilized. Qualitative and
quantitative experiments demonstrate the effectiveness of the proposed method
as compared to existing state-of-the-art HDR deghosting approaches given
flexible numbers of input frames.

</details>


### [151] [Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction Networks for Single-Pixel Imaging](https://arxiv.org/abs/2505.23180)
*Ping Wang,Lishun Wang,Gang Qu,Xiaodong Wang,Yulun Zhang,Xin Yuan*

Main category: eess.IV

TL;DR: 论文提出了一种结合深度展开（unrolling）和即插即用（PnP）方法优势的解决方案，通过设计高效的深度图像恢复器（DIR）和提出通用的近端轨迹（PT）损失函数，实现了在单像素成像（SPI）逆问题中灵活处理不同压缩比（CR）的同时，提高重建精度和速度。


<details>
  <summary>Details</summary>
Motivation: PnP方法虽然灵活但精度和速度有限，而展开方法虽然精度高但需要针对不同CR重新训练。论文旨在结合两者的优势。

Method: 设计了高效的深度图像恢复器（DIR）用于展开HQS和ADMM，并提出通用的近端轨迹（PT）损失函数训练网络。

Result: 实验表明，提出的近端展开网络不仅能灵活处理不同CR，还能在重建精度和速度上超越之前的CR特定展开网络。

Conclusion: 论文成功整合了PnP和展开方法的优势，为SPI逆问题提供了更高效的解决方案。

Abstract: Deep-unrolling and plug-and-play (PnP) approaches have become the de-facto
standard solvers for single-pixel imaging (SPI) inverse problem. PnP
approaches, a class of iterative algorithms where regularization is implicitly
performed by an off-the-shelf deep denoiser, are flexible for varying
compression ratios (CRs) but are limited in reconstruction accuracy and speed.
Conversely, unrolling approaches, a class of multi-stage neural networks where
a truncated iterative optimization process is transformed into an end-to-end
trainable network, typically achieve better accuracy with faster inference but
require fine-tuning or even retraining when CR changes. In this paper, we
address the challenge of integrating the strengths of both classes of solvers.
To this end, we design an efficient deep image restorer (DIR) for the unrolling
of HQS (half quadratic splitting) and ADMM (alternating direction method of
multipliers). More importantly, a general proximal trajectory (PT) loss
function is proposed to train HQS/ADMM-unrolling networks such that learned DIR
approximates the proximal operator of an ideal explicit restoration
regularizer. Extensive experiments demonstrate that, the resulting proximal
unrolling networks can not only flexibly handle varying CRs with a single model
like PnP algorithms, but also outperform previous CR-specific unrolling
networks in both reconstruction accuracy and speed. Source codes and models are
available at https://github.com/pwangcs/ProxUnroll.

</details>


### [152] [Advancing Image Super-resolution Techniques in Remote Sensing: A Comprehensive Survey](https://arxiv.org/abs/2505.23248)
*Yunliang Qi,Meng Lou,Yimin Liu,Lu Li,Zhen Yang,Wen Nie*

Main category: eess.IV

TL;DR: 本文对遥感图像超分辨率（RSISR）方法进行了系统综述，涵盖方法、数据集和评估指标，分析了现有方法的局限性，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来提出了许多RSISR方法，但缺乏系统全面的综述，本文旨在填补这一空白，帮助研究人员了解当前趋势和挑战。

Method: 对RSISR方法进行分类（监督、无监督和质量评估），并分析其方法、数据集和评估指标。

Result: 揭示了现有方法在保留细粒度纹理和几何结构方面的局限性，尤其是在大规模退化情况下。

Conclusion: 未来研究需要开发领域特定的架构和鲁棒的评估协议，以缩小合成与真实场景之间的差距。

Abstract: Remote sensing image super-resolution (RSISR) is a crucial task in remote
sensing image processing, aiming to reconstruct high-resolution (HR) images
from their low-resolution (LR) counterparts. Despite the growing number of
RSISR methods proposed in recent years, a systematic and comprehensive review
of these methods is still lacking. This paper presents a thorough review of
RSISR algorithms, covering methodologies, datasets, and evaluation metrics. We
provide an in-depth analysis of RSISR methods, categorizing them into
supervised, unsupervised, and quality evaluation approaches, to help
researchers understand current trends and challenges. Our review also discusses
the strengths, limitations, and inherent challenges of these techniques.
Notably, our analysis reveals significant limitations in existing methods,
particularly in preserving fine-grained textures and geometric structures under
large-scale degradation. Based on these findings, we outline future research
directions, highlighting the need for domain-specific architectures and robust
evaluation protocols to bridge the gap between synthetic and real-world RSISR
scenarios.

</details>


### [153] [Can Large Language Models Challenge CNNS in Medical Image Analysis?](https://arxiv.org/abs/2505.23503)
*Shibbir Ahmed,Shahnewaz Karim Sakib,Anindya Bijoy Das*

Main category: eess.IV

TL;DR: 本文提出了一种多模态AI框架，用于精确分类医学诊断图像，比较了CNN和LLM的性能差异，发现结合过滤的LLM能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 提升医学诊断图像的分类准确性和效率，同时评估不同模型的环境影响。

Method: 使用公开数据集，比较CNN和LLM在准确性、执行效率、能耗和碳排放方面的表现。

Result: CNN在某些情况下优于多模态技术，但结合过滤的LLM能显著提升性能。

Conclusion: 多模态AI系统有望提高医学诊断的可靠性、效率和可扩展性。

Abstract: This study presents a multimodal AI framework designed for precisely
classifying medical diagnostic images. Utilizing publicly available datasets,
the proposed system compares the strengths of convolutional neural networks
(CNNs) and different large language models (LLMs). This in-depth comparative
analysis highlights key differences in diagnostic performance, execution
efficiency, and environmental impacts. Model evaluation was based on accuracy,
F1-score, average execution time, average energy consumption, and estimated
$CO_2$ emission. The findings indicate that although CNN-based models can
outperform various multimodal techniques that incorporate both images and
contextual information, applying additional filtering on top of LLMs can lead
to substantial performance gains. These findings highlight the transformative
potential of multimodal AI systems to enhance the reliability, efficiency, and
scalability of medical diagnostics in clinical settings.

</details>


### [154] [PCA for Enhanced Cross-Dataset Generalizability in Breast Ultrasound Tumor Segmentation](https://arxiv.org/abs/2505.23587)
*Christian Schmidt,Heinrich Martin Overhoff*

Main category: eess.IV

TL;DR: 论文提出了一种基于主成分分析（PCA）的新方法，用于提升医学超声图像分割模型在未见数据集上的外部有效性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割模型在跨数据集部署时外部有效性不足，尤其在超声图像领域，现有方法如域适应和GAN风格迁移在数据量小且多样化的医学领域效果有限。

Method: 通过PCA预处理降噪并保留90%的数据方差，生成PCA重建数据集，并基于U-Net模型在原始和PCA数据集上进行训练与测试。

Result: 实验表明，PCA重建数据集显著提升了召回率（0.57→0.70）和Dice分数（0.50→0.58），并减少了外部验证导致的召回率下降33%。

Conclusion: PCA重建可作为提升医学图像分割模型外部有效性的有效方法，尤其在挑战性案例中表现突出。

Abstract: In medical image segmentation, limited external validity remains a critical
obstacle when models are deployed across unseen datasets, an issue particularly
pronounced in the ultrasound image domain. Existing solutions-such as domain
adaptation and GAN-based style transfer-while promising, often fall short in
the medical domain where datasets are typically small and diverse. This paper
presents a novel application of principal component analysis (PCA) to address
this limitation. PCA preprocessing reduces noise and emphasizes essential
features by retaining approximately 90\% of the dataset variance. We evaluate
our approach across six diverse breast tumor ultrasound datasets comprising
3,983 B-mode images and corresponding expert tumor segmentation masks. For each
dataset, a corresponding dimensionality reduced PCA-dataset is created and
U-Net-based segmentation models are trained on each of the twelve datasets.
Each model trained on an original dataset was inferenced on the remaining five
out-of-domain original datasets (baseline results), while each model trained on
a PCA dataset was inferenced on five out-of-domain PCA datasets. Our
experimental results indicate that using PCA reconstructed datasets, instead of
original images, improves the model's recall and Dice scores, particularly for
model-dataset pairs where baseline performance was lowest, achieving
statistically significant gains in recall (0.57 $\pm$ 0.07 vs. 0.70 $\pm$ 0.05,
$p = 0.0004$) and Dice scores (0.50 $\pm$ 0.06 vs. 0.58 $\pm$ 0.06, $p =
0.03$). Our method reduced the decline in recall values due to external
validation by $33\%$. These findings underscore the potential of PCA
reconstruction as a safeguard to mitigate declines in segmentation performance,
especially in challenging cases, with implications for enhancing external
validity in real-world medical applications.

</details>


### [155] [ImmunoDiff: A Diffusion Model for Immunotherapy Response Prediction in Lung Cancer](https://arxiv.org/abs/2505.23675)
*Moinak Bhattacharya,Judy Huang,Amna F. Sher,Gagandeep Singh,Chao Chen,Prateek Prasanna*

Main category: eess.IV

TL;DR: ImmunoDiff是一种基于扩散模型的框架，通过整合解剖学先验和临床数据，从基线CT扫描合成治疗后图像，显著提高了NSCLC免疫治疗反应预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 准确预测非小细胞肺癌（NSCLC）的免疫治疗反应是一个未满足的临床需求，现有模型难以捕捉治疗引起的复杂形态和纹理变化。

Method: 提出ImmunoDiff，结合解剖学先验（如肺叶和血管结构）和临床数据（如PD-L1表达），通过cbi-Adapter模块实现多模态数据整合。

Result: 在内部NSCLC队列中，反应预测的平衡准确率提高了21.24%，生存预测的c指数增加了0.03。

Conclusion: ImmunoDiff通过整合解剖和临床数据，显著提升了免疫治疗反应预测的准确性，具有临床应用潜力。

Abstract: Accurately predicting immunotherapy response in Non-Small Cell Lung Cancer
(NSCLC) remains a critical unmet need. Existing radiomics and deep
learning-based predictive models rely primarily on pre-treatment imaging to
predict categorical response outcomes, limiting their ability to capture the
complex morphological and textural transformations induced by immunotherapy.
This study introduces ImmunoDiff, an anatomy-aware diffusion model designed to
synthesize post-treatment CT scans from baseline imaging while incorporating
clinically relevant constraints. The proposed framework integrates anatomical
priors, specifically lobar and vascular structures, to enhance fidelity in CT
synthesis. Additionally, we introduce a novel cbi-Adapter, a conditioning
module that ensures pairwise-consistent multimodal integration of imaging and
clinical data embeddings, to refine the generative process. Additionally, a
clinical variable conditioning mechanism is introduced, leveraging demographic
data, blood-based biomarkers, and PD-L1 expression to refine the generative
process. Evaluations on an in-house NSCLC cohort treated with immune checkpoint
inhibitors demonstrate a 21.24% improvement in balanced accuracy for response
prediction and a 0.03 increase in c-index for survival prediction. Code will be
released soon.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [156] [Quality assessment of 3D human animation: Subjective and objective evaluation](https://arxiv.org/abs/2505.23301)
*Rim Rekik,Stefanie Wuhrer,Ludovic Hoyet,Katja Zibrek,Anne-Hélène Olivier*

Main category: cs.GR

TL;DR: 提出了一种基于数据驱动的质量评估方法，用于评估非参数化虚拟人动画的真实感。


<details>
  <summary>Details</summary>
Motivation: 虚拟人动画在虚拟和增强现实中有广泛应用，但缺乏针对非参数化生成动画的质量评估方法。

Method: 通过用户研究收集主观真实感评分，并利用线性回归模型预测评分。

Result: 线性回归模型在数据集上达到90%的相关性，优于现有的深度学习方法。

Conclusion: 该方法为非参数化虚拟人动画的质量评估提供了有效工具。

Abstract: Virtual human animations have a wide range of applications in virtual and
augmented reality. While automatic generation methods of animated virtual
humans have been developed, assessing their quality remains challenging.
Recently, approaches introducing task-oriented evaluation metrics have been
proposed, leveraging neural network training. However, quality assessment
measures for animated virtual humans that are not generated with parametric
body models have yet to be developed. In this context, we introduce a first
such quality assessment measure leveraging a novel data-driven framework.
First, we generate a dataset of virtual human animations together with their
corresponding subjective realism evaluation scores collected with a user study.
Second, we use the resulting dataset to learn predicting perceptual evaluation
scores. Results indicate that training a linear regressor on our dataset
results in a correlation of 90%, which outperforms a state of the art deep
learning baseline.

</details>


### [157] [To Measure What Isn't There -- Visual Exploration of Missingness Structures Using Quality Metrics](https://arxiv.org/abs/2505.23447)
*Sara Johansson Fernstad,Sarah Alsufyani,Silvia Del Din,Alison Yarnall,Lynn Rochester*

Main category: cs.GR

TL;DR: 本文提出了一组质量指标，用于识别和可视化高维数据中的结构化缺失模式，支持数据质量分析和决策。


<details>
  <summary>Details</summary>
Motivation: 高维数据中的缺失值是常见问题，可能导致分析问题。结构化缺失可能反映数据收集或预处理问题，也可能揭示重要特征。现有研究多关注统计填补方法，而可视化在理解缺失结构方面潜力巨大，但相关研究较少且缺乏可扩展性。

Method: 提出一组质量指标，用于识别结构化缺失模式，并通过可视化分析支持数据探索。

Result: 通过实际步行监测研究案例展示了质量指标在可视化分析中的应用。

Conclusion: 本文的质量指标为高维数据中结构化缺失的可视化分析提供了有效工具，支持数据质量决策。

Abstract: This paper contributes a set of quality metrics for identification and visual
analysis of structured missingness in high-dimensional data. Missing values in
data are a frequent challenge in most data generating domains and may cause a
range of analysis issues. Structural missingness in data may indicate issues in
data collection and pre-processing, but may also highlight important data
characteristics. While research into statistical methods for dealing with
missing data are mainly focusing on replacing missing values with plausible
estimated values, visualization has great potential to support a more in-depth
understanding of missingness structures in data. Nonetheless, while the
interest in missing data visualization has increased in the last decade, it is
still a relatively overlooked research topic with a comparably small number of
publications, few of which address scalability issues. Efficient visual
analysis approaches are needed to enable exploration of missingness structures
in large and high-dimensional data, and to support informed decision-making in
context of potential data quality issues. This paper suggests a set of quality
metrics for identification of patterns of interest for understanding of
structural missingness in data. These quality metrics can be used as guidance
in visual analysis, as demonstrated through a use case exploring structural
missingness in data from a real-life walking monitoring study. All supplemental
materials for this paper are available at
https://doi.org/10.25405/data.ncl.c.7741829.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [158] [Depth to magnetic source estimation using TDX contour](https://arxiv.org/abs/2505.22780)
*Hammed Oyekan*

Main category: physics.geo-ph

TL;DR: 提出了一种基于TDX导数的快速简单方法，用于估计磁性体的深度，提高了深度分辨率和准确性。


<details>
  <summary>Details</summary>
Motivation: 磁性体深度估计在地球物理应用中至关重要，如矿产勘探和地质填图。

Method: 利用TDX导数（总磁场的一阶导数）进行深度估计，该方法对噪声不敏感且独立于磁化强度。

Result: 通过垂直接触源模型验证了方法的有效性，结果与已知深度一致。

Conclusion: 该方法可靠，尽管假设条件严格，但在复杂地质结构中仍表现良好。

Abstract: Accurate depth estimation of magnetic sources plays a crucial role in various
geophysical applications, including mineral exploration, resource assessments,
regional hydrocarbon exploration, and geological mapping. Thus, this abstract
presents a fast and simple method of estimating the depth of a magnetic body
using the TDX derivative of the total magnetic field. TDX is a first-order
derivative of the magnetic field that, in addition to edge detection, is less
affected by noise, allowing for better depth resolution. The reduced
sensitivity to noise enables a clearer estimation of depth and enhances the
accuracy of the depth determination process. The TDX, as a variant of the phase
derivative, is independent of magnetization and can be used to identify the
edge of a magnetic body. In addition to excelling at edge detection, they can
also estimate the depth of the magnetic source producing the anomalies. In this
study, we explore the utilization of contour of the TDX derivative for
estimating depth, assuming a vertical contact source. We demonstrate the
effectiveness of the method using a two-prism block model and a simple bishop
model with a uniform susceptibility of 0.001 cgs. The results agree with the
known depth, providing evidence of the reliability of the method despite the
restrictive nature of the assumption, especially for the Bishop model, where
there are numerous fault structures.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [159] [Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion](https://arxiv.org/abs/2505.23266)
*Chunlong Xie,Jialing He,Shangwei Guo,Jiacheng Wang,Shudong Zhang,Tianwei Zhang,Tao Xiang*

Main category: cs.CR

TL;DR: AdvOF是一种针对视觉与语言导航（VLN）代理的新型攻击框架，通过生成对抗性3D对象来研究其对VLM感知模块的影响。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击未考虑服务计算环境中的可靠性和服务质量（QoS），AdvOF填补了这一空白。

Method: AdvOF通过精确聚合和对齐2D与3D空间中的目标对象位置，定义并渲染对抗对象，并通过多视角优化和正则化协作优化对抗对象。

Result: 实验表明，AdvOF能有效降低代理在对抗条件下的性能，同时对正常导航任务干扰最小。

Conclusion: AdvOF为VLM驱动的导航系统服务安全性提供了新见解，并为物理世界部署中的鲁棒服务组合奠定了基础。

Abstract: We present Adversarial Object Fusion (AdvOF), a novel attack framework
targeting vision-and-language navigation (VLN) agents in service-oriented
environments by generating adversarial 3D objects. While foundational models
like Large Language Models (LLMs) and Vision Language Models (VLMs) have
enhanced service-oriented navigation systems through improved perception and
decision-making, their integration introduces vulnerabilities in
mission-critical service workflows. Existing adversarial attacks fail to
address service computing contexts, where reliability and quality-of-service
(QoS) are paramount. We utilize AdvOF to investigate and explore the impact of
adversarial environments on the VLM-based perception module of VLN agents. In
particular, AdvOF first precisely aggregates and aligns the victim object
positions in both 2D and 3D space, defining and rendering adversarial objects.
Then, we collaboratively optimize the adversarial object with regularization
between the adversarial and victim object across physical properties and VLM
perceptions. Through assigning importance weights to varying views, the
optimization is processed stably and multi-viewedly by iterative fusions from
local updates and justifications. Our extensive evaluations demonstrate AdvOF
can effectively degrade agent performance under adversarial conditions while
maintaining minimal interference with normal navigation tasks. This work
advances the understanding of service security in VLM-powered navigation
systems, providing computational foundations for robust service composition in
physical-world deployments.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [160] [ConnectomeDiffuser: Generative AI Enables Brain Network Construction from Diffusion Tensor Imaging](https://arxiv.org/abs/2505.22683)
*Xuhang Chen,Michael Kwok-Po Ng,Kim-Fung Tsang,Chi-Man Pun,Shuqiang Wang*

Main category: q-bio.NC

TL;DR: ConnectomeDiffuser是一个基于扩散的自动化框架，用于从DTI构建脑网络，克服了现有方法的局限性，提高了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在构建脑网络时存在主观性、工作流程繁琐及无法捕捉复杂拓扑特征等问题，需要更高效、自动化的解决方案。

Method: 结合模板网络、扩散模型和图卷积网络分类器，从DTI扫描中提取拓扑特征并生成全面的脑网络。

Result: 在两种神经退行性疾病数据集上验证，性能显著优于其他方法，能更敏感地分析个体差异。

Conclusion: ConnectomeDiffuser为神经退行性疾病的诊断和研究提供了更准确、通用的工具。

Abstract: Brain network analysis plays a crucial role in diagnosing and monitoring
neurodegenerative disorders such as Alzheimer's disease (AD). Existing
approaches for constructing structural brain networks from diffusion tensor
imaging (DTI) often rely on specialized toolkits that suffer from inherent
limitations: operator subjectivity, labor-intensive workflows, and restricted
capacity to capture complex topological features and disease-specific
biomarkers. To overcome these challenges and advance computational neuroimaging
instrumentation, ConnectomeDiffuser is proposed as a novel diffusion-based
framework for automated end-to-end brain network construction from DTI. The
proposed model combines three key components: (1) a Template Network that
extracts topological features from 3D DTI scans using Riemannian geometric
principles, (2) a diffusion model that generates comprehensive brain networks
with enhanced topological fidelity, and (3) a Graph Convolutional Network
classifier that incorporates disease-specific markers to improve diagnostic
accuracy. ConnectomeDiffuser demonstrates superior performance by capturing a
broader range of structural connectivity and pathology-related information,
enabling more sensitive analysis of individual variations in brain networks.
Experimental validation on datasets representing two distinct neurodegenerative
conditions demonstrates significant performance improvements over other brain
network methods. This work contributes to the advancement of instrumentation in
the context of neurological disorders, providing clinicians and researchers
with a robust, generalizable measurement framework that facilitates more
accurate diagnosis, deeper mechanistic understanding, and improved therapeutic
monitoring of neurodegenerative diseases such as AD.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [161] [Errors in Stereo Geometry Induce Distance Misperception](https://arxiv.org/abs/2505.23685)
*Raffles Xingqi Zhu,Charlie S. Burlingham,Olivier Mercier,Phillip Guan*

Main category: cs.HC

TL;DR: 论文提出了一种几何框架，用于预测由于HMD透视几何不准确导致的距离感知误差，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究HMD渲染和显示过程中可能存在的相机和视点误差，及其对用户深度和距离感知的影响。

Method: 构建了一个几何框架预测误差，并在Quest 3 HMD平台上模拟和测试这些误差。

Result: 实验表明透视几何误差会导致距离感知的偏差，实时视觉反馈可动态校准视动映射。

Conclusion: 几何框架能有效预测误差，动态反馈可改善HMD中的距离感知问题。

Abstract: Stereoscopic head-mounted displays (HMDs) render and present binocular images
to create an egocentric, 3D percept to the HMD user. Within this render and
presentation pipeline there are potential rendering camera and viewing position
errors that can induce deviations in the depth and distance that a user
perceives compared to the underlying intended geometry. For example, rendering
errors can arise when HMD render cameras are incorrectly positioned relative to
the assumed centers of projections of the HMD displays and viewing errors can
arise when users view stereo geometry from the incorrect location in the HMD
eyebox. In this work we present a geometric framework that predicts errors in
distance perception arising from inaccurate HMD perspective geometry and build
an HMD platform to reliably simulate render and viewing error in a Quest 3 HMD
with eye tracking to experimentally test these predictions. We present a series
of five experiments to explore the efficacy of this geometric framework and
show that errors in perspective geometry can induce both under- and
over-estimations in perceived distance. We further demonstrate how real-time
visual feedback can be used to dynamically recalibrate visuomotor mapping so
that an accurate reach distance is achieved even if the perceived visual
distance is negatively impacted by geometric error.

</details>


### [162] [MAC-Gaze: Motion-Aware Continual Calibration for Mobile Gaze Tracking](https://arxiv.org/abs/2505.22769)
*Yaxiong Lei,Mingyue Zhao,Yuheng Wang,Shijing He,Yusuke Sugano,Yafei Wang,Kaixing Zhao,Mohamed Khamis,Juan Ye*

Main category: cs.HC

TL;DR: MAC-Gaze是一种基于运动感知的持续校准方法，通过智能手机IMU传感器和持续学习技术动态调整视线跟踪模型，显著提升了移动场景下的视线估计准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的一次性校准方法无法适应用户姿势和设备方向的动态变化，导致性能下降。

Method: 结合预训练的视觉视线估计器和基于IMU的活动识别模型，采用聚类混合决策机制触发重新校准，并通过基于回放的持续学习避免灾难性遗忘。

Result: 在RGBDGaze和MotionGaze数据集上，视线估计误差分别降低了19.9%和31.7%。

Conclusion: MAC-Gaze为移动场景下的视线估计提供了一种鲁棒的解决方案。

Abstract: Mobile gaze tracking faces a fundamental challenge: maintaining accuracy as
users naturally change their postures and device orientations. Traditional
calibration approaches, like one-off, fail to adapt to these dynamic
conditions, leading to degraded performance over time. We present MAC-Gaze, a
Motion-Aware continual Calibration approach that leverages smartphone Inertial
measurement unit (IMU) sensors and continual learning techniques to
automatically detect changes in user motion states and update the gaze tracking
model accordingly. Our system integrates a pre-trained visual gaze estimator
and an IMU-based activity recognition model with a clustering-based hybrid
decision-making mechanism that triggers recalibration when motion patterns
deviate significantly from previously encountered states. To enable
accumulative learning of new motion conditions while mitigating catastrophic
forgetting, we employ replay-based continual learning, allowing the model to
maintain performance across previously encountered motion conditions. We
evaluate our system through extensive experiments on the publicly available
RGBDGaze dataset and our own 10-hour multimodal MotionGaze dataset (481K+
images, 800K+ IMU readings), encompassing a wide range of postures under
various motion conditions including sitting, standing, lying, and walking.
Results demonstrate that our method reduces gaze estimation error by 19.9% on
RGBDGaze (from 1.73 cm to 1.41 cm) and by 31.7% on MotionGaze (from 2.81 cm to
1.92 cm) compared to traditional calibration approaches. Our framework provides
a robust solution for maintaining gaze estimation accuracy in mobile scenarios.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [163] [Physiology-Informed Generative Multi-Task Network for Contrast-Free CT Perfusion](https://arxiv.org/abs/2505.22673)
*Wasif Khan,Kyle B. See,Simon Kato,Ziqian Huang,Amy Lazarte,Kyle Douglas,Xiangyang Lou,Teng J. Peng,Dhanashree Rajderkar,John Rees,Pina Sanelli,Amita Singh,Ibrahim Tuna,Christina A. Wilson,Ruogu Fang*

Main category: q-bio.TO

TL;DR: 提出了一种名为MAGIC的深度学习框架，用于将非对比CT图像映射为多模态无对比CT灌注图像，以减少对比剂的使用及其副作用。


<details>
  <summary>Details</summary>
Motivation: 对比剂在CT灌注成像中可能导致过敏反应和副作用，且成本高昂。

Method: 结合生成式AI和生理信息，通过改进损失函数提升图像保真度。

Result: 在双盲研究中，MAGIC在视觉质量和诊断准确性上表现优于传统对比剂灌注成像。

Conclusion: MAGIC有望通过无对比、经济高效的灌注成像革新医疗保健。

Abstract: Perfusion imaging is extensively utilized to assess hemodynamic status and
tissue perfusion in various organs. Computed tomography perfusion (CTP) imaging
plays a key role in the early assessment and planning of stroke treatment.
While CTP provides essential perfusion parameters to identify abnormal blood
flow in the brain, the use of contrast agents in CTP can lead to allergic
reactions and adverse side effects, along with costing USD 4.9 billion
worldwide in 2022. To address these challenges, we propose a novel deep
learning framework called Multitask Automated Generation of Intermodal CT
perfusion maps (MAGIC). This framework combines generative artificial
intelligence and physiological information to map non-contrast computed
tomography (CT) imaging to multiple contrast-free CTP imaging maps. We
demonstrate enhanced image fidelity by incorporating physiological
characteristics into the loss terms. Our network was trained and validated
using CT image data from patients referred for stroke at UF Health and
demonstrated robustness to abnormalities in brain perfusion activity. A
double-blinded study was conducted involving seven experienced
neuroradiologists and vascular neurologists. This study validated MAGIC's
visual quality and diagnostic accuracy showing favorable performance compared
to clinical perfusion imaging with intravenous contrast injection. Overall,
MAGIC holds great promise in revolutionizing healthcare by offering
contrast-free, cost-effective, and rapid perfusion imaging.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [164] [Test-time augmentation improves efficiency in conformal prediction](https://arxiv.org/abs/2505.22764)
*Divya Shanmugam,Helen Lu,Swami Sankaranarayanan,John Guttag*

Main category: cs.LG

TL;DR: 本文提出了一种通过测试时增强（TTA）减少共形分类器预测集大小的方法，无需重新训练模型，平均减少10%-14%的预测集大小。


<details>
  <summary>Details</summary>
Motivation: 共形分类器通常会生成信息量不足的大预测集，影响实用性。

Method: 利用测试时增强（TTA）引入归纳偏置，结合共形评分方法，无需模型重新训练。

Result: 在三个数据集、三种模型、两种共形评分方法及多种分布偏移下验证，TTA平均减少预测集大小10%-14%。

Conclusion: 测试时增强是共形分类器流程中的有效补充，灵活且计算高效。

Abstract: A conformal classifier produces a set of predicted classes and provides a
probabilistic guarantee that the set includes the true class. Unfortunately, it
is often the case that conformal classifiers produce uninformatively large
sets. In this work, we show that test-time augmentation (TTA)--a technique that
introduces inductive biases during inference--reduces the size of the sets
produced by conformal classifiers. Our approach is flexible, computationally
efficient, and effective. It can be combined with any conformal score, requires
no model retraining, and reduces prediction set sizes by 10%-14% on average. We
conduct an evaluation of the approach spanning three datasets, three models,
two established conformal scoring methods, different guarantee strengths, and
several distribution shifts to show when and why test-time augmentation is a
useful addition to the conformal pipeline.

</details>


### [165] [Number of Clusters in a Dataset: A Regularized K-means Approach](https://arxiv.org/abs/2505.22991)
*Behzad Kamgar-Parsi,Behrooz Kamgar-Parsi*

Main category: cs.LG

TL;DR: 本文研究了正则化k-means算法中关键超参数λ的设定问题，提出了基于理想簇假设的严格边界，并分析了加性和乘性正则化对多解问题的影响。


<details>
  <summary>Details</summary>
Motivation: 在无标签数据集中确定有意义的簇数量是许多应用中的重要问题，但目前缺乏设定正则化超参数λ的原则性指导。

Method: 假设簇为理想簇（相同半径的d维球体），推导了λ的严格边界，并分析了加性和乘性正则化对k-means算法多解问题的影响。

Result: 实验表明，加性正则化的k-means算法常产生多解，而加性和乘性正则化的共识可减少多解模糊性。

Conclusion: 本文为λ的设定提供了理论依据，并通过实验验证了正则化k-means算法在非理想簇情况下的性能。

Abstract: Finding the number of meaningful clusters in an unlabeled dataset is
important in many applications. Regularized k-means algorithm is a possible
approach frequently used to find the correct number of distinct clusters in
datasets. The most common formulation of the regularization function is the
additive linear term $\lambda k$, where $k$ is the number of clusters and
$\lambda$ a positive coefficient. Currently, there are no principled guidelines
for setting a value for the critical hyperparameter $\lambda$. In this paper,
we derive rigorous bounds for $\lambda$ assuming clusters are {\em ideal}.
Ideal clusters (defined as $d$-dimensional spheres with identical radii) are
close proxies for k-means clusters ($d$-dimensional spherically symmetric
distributions with identical standard deviations). Experiments show that the
k-means algorithm with additive regularizer often yields multiple solutions.
Thus, we also analyze k-means algorithm with multiplicative regularizer. The
consensus among k-means solutions with additive and multiplicative
regularizations reduces the ambiguity of multiple solutions in certain cases.
We also present selected experiments that demonstrate performance of the
regularized k-means algorithms as clusters deviate from the ideal assumption.

</details>


### [166] [Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift](https://arxiv.org/abs/2505.23027)
*Minh Nguyen Nhat To,Paul F RWilson,Viet Nguyen,Mohamed Harmanani,Michael Cooper,Fahimeh Fooladgar,Purang Abolmaesumi,Parvin Mousavi,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: 论文提出了一种名为Diverse Prototypical Ensembles（DPEs）的方法，通过使用多样化的原型分类器集合来应对子群体分布偏移问题，显著提升了最差群体准确率。


<details>
  <summary>Details</summary>
Motivation: 子群体分布偏移会显著降低机器学习模型的性能，现有方法依赖对子群体数量和性质的假设及标注，而这些信息在许多现实数据集中不可用。

Method: 用多样化的原型分类器集合替换标准线性分类层，每个分类器专注于不同的特征和样本，从而自适应地捕捉子群体风险。

Result: 在九个现实数据集上的实验表明，DPEs在最差群体准确率上优于现有方法。

Conclusion: DPEs是一种无需依赖子群体标注的有效方法，能够显著提升模型在子群体分布偏移下的性能。

Abstract: The subpopulationtion shift, characterized by a disparity in subpopulation
distributibetween theween the training and target datasets, can significantly
degrade the performance of machine learning models. Current solutions to
subpopulation shift involve modifying empirical risk minimization with
re-weighting strategies to improve generalization. This strategy relies on
assumptions about the number and nature of subpopulations and annotations on
group membership, which are unavailable for many real-world datasets. Instead,
we propose using an ensemble of diverse classifiers to adaptively capture risk
associated with subpopulations. Given a feature extractor network, we replace
its standard linear classification layer with a mixture of prototypical
classifiers, where each member is trained to classify the data while focusing
on different features and samples from other members. In empirical evaluation
on nine real-world datasets, covering diverse domains and kinds of
subpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often
outperforms the prior state-of-the-art in worst-group accuracy. The code is
available at https://github.com/minhto2802/dpe4subpop

</details>


### [167] [Pseudo Multi-Source Domain Generalization: Bridging the Gap Between Single and Multi-Source Domain Generalization](https://arxiv.org/abs/2505.23173)
*Shohei Enomoto*

Main category: cs.LG

TL;DR: 提出了一种名为PMDG的新框架，通过风格迁移和数据增强技术从单一源域生成多个伪域，以在单源域泛化（SDG）设置中应用多源域泛化（MDG）算法。实验表明，PMDG性能与MDG正相关，且伪域性能在足够数据下可匹配或超越实际多域性能。


<details>
  <summary>Details</summary>
Motivation: 解决多源域泛化（MDG）在实际应用中因多域数据集构建成本高而受限的问题。

Method: 提出PMDG框架，通过风格迁移和数据增强从单一源域生成伪域，构建合成多域数据集，并利用PseudoDomainBed进行实验验证。

Result: 实验结果显示PMDG性能与MDG正相关，伪域在足够数据下可匹配或超越实际多域性能。

Conclusion: PMDG为单源域泛化提供了一种实用解决方案，为未来域泛化研究提供了有价值的见解。

Abstract: Deep learning models often struggle to maintain performance when deployed on
data distributions different from their training data, particularly in
real-world applications where environmental conditions frequently change. While
Multi-source Domain Generalization (MDG) has shown promise in addressing this
challenge by leveraging multiple source domains during training, its practical
application is limited by the significant costs and difficulties associated
with creating multi-domain datasets. To address this limitation, we propose
Pseudo Multi-source Domain Generalization (PMDG), a novel framework that
enables the application of sophisticated MDG algorithms in more practical
Single-source Domain Generalization (SDG) settings. PMDG generates multiple
pseudo-domains from a single source domain through style transfer and data
augmentation techniques, creating a synthetic multi-domain dataset that can be
used with existing MDG algorithms. Through extensive experiments with
PseudoDomainBed, our modified version of the DomainBed benchmark, we analyze
the effectiveness of PMDG across multiple datasets and architectures. Our
analysis reveals several key findings, including a positive correlation between
MDG and PMDG performance and the potential of pseudo-domains to match or exceed
actual multi-domain performance with sufficient data. These comprehensive
empirical results provide valuable insights for future research in domain
generalization. Our code is available at
https://github.com/s-enmt/PseudoDomainBed.

</details>


### [168] [Buffer-free Class-Incremental Learning with Out-of-Distribution Detection](https://arxiv.org/abs/2505.23412)
*Srishti Gupta,Daniele Angioni,Maura Pintor,Ambra Demontis,Lea Schönherr,Battista Biggio,Fabio Roli*

Main category: cs.LG

TL;DR: 论文提出了一种无需内存缓冲区的后处理OOD检测方法，用于开放世界中的类增量学习，性能优于或等同于基于缓冲区的方法。


<details>
  <summary>Details</summary>
Motivation: 解决开放世界场景中类增量学习的挑战，特别是避免使用内存缓冲区带来的隐私、可扩展性和训练时间问题。

Method: 深入分析后处理OOD检测方法，并在推理时应用，替代基于缓冲区的OOD检测。

Result: 在CIFAR-10、CIFAR-100和Tiny ImageNet数据集上，该方法性能与基于缓冲区的方法相当或更优。

Conclusion: 后处理OOD检测方法为高效且保护隐私的开放世界类增量学习系统提供了新思路。

Abstract: Class-incremental learning (CIL) poses significant challenges in open-world
scenarios, where models must not only learn new classes over time without
forgetting previous ones but also handle inputs from unknown classes that a
closed-set model would misclassify. Recent works address both issues by
(i)~training multi-head models using the task-incremental learning framework,
and (ii) predicting the task identity employing out-of-distribution (OOD)
detectors. While effective, the latter mainly relies on joint training with a
memory buffer of past data, raising concerns around privacy, scalability, and
increased training time. In this paper, we present an in-depth analysis of
post-hoc OOD detection methods and investigate their potential to eliminate the
need for a memory buffer. We uncover that these methods, when applied
appropriately at inference time, can serve as a strong substitute for
buffer-based OOD detection. We show that this buffer-free approach achieves
comparable or superior performance to buffer-based methods both in terms of
class-incremental learning and the rejection of unknown samples. Experimental
results on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets support our findings,
offering new insights into the design of efficient and privacy-preserving CIL
systems for open-world settings.

</details>


### [169] [Network Inversion for Uncertainty-Aware Out-of-Distribution Detection](https://arxiv.org/abs/2505.23448)
*Pirzada Suhail,Rehna Afroz,Amit Sethi*

Main category: cs.LG

TL;DR: 提出了一种结合网络反演和分类器训练的新框架，用于同时解决OOD检测和不确定性估计问题。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，意外输入不可避免，OOD检测和不确定性估计对构建安全的机器学习系统至关重要。

Method: 通过引入“垃圾”类，结合网络反演和迭代训练，逐步优化分类器边界，将OOD样本归入垃圾类。

Result: 模型能有效检测并拒绝OOD样本，同时提供预测的不确定性估计。

Conclusion: 该方法无需外部OOD数据集或后校准技术，为OOD检测和不确定性估计提供了统一解决方案。

Abstract: Out-of-distribution (OOD) detection and uncertainty estimation (UE) are
critical components for building safe machine learning systems, especially in
real-world scenarios where unexpected inputs are inevitable. In this work, we
propose a novel framework that combines network inversion with classifier
training to simultaneously address both OOD detection and uncertainty
estimation. For a standard n-class classification task, we extend the
classifier to an (n+1)-class model by introducing a "garbage" class, initially
populated with random gaussian noise to represent outlier inputs. After each
training epoch, we use network inversion to reconstruct input images
corresponding to all output classes that initially appear as noisy and
incoherent and are therefore excluded to the garbage class for retraining the
classifier. This cycle of training, inversion, and exclusion continues
iteratively till the inverted samples begin to resemble the in-distribution
data more closely, suggesting that the classifier has learned to carve out
meaningful decision boundaries while sanitising the class manifolds by pushing
OOD content into the garbage class. During inference, this training scheme
enables the model to effectively detect and reject OOD samples by classifying
them into the garbage class. Furthermore, the confidence scores associated with
each prediction can be used to estimate uncertainty for both in-distribution
and OOD inputs. Our approach is scalable, interpretable, and does not require
access to external OOD datasets or post-hoc calibration techniques while
providing a unified solution to the dual challenges of OOD detection and
uncertainty estimation.

</details>


### [170] [Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model](https://arxiv.org/abs/2505.23606)
*Qingyu Shi,Jinbin Bai,Zhuoran Zhao,Wenhao Chai,Kaidong Yu,Jianzong Wu,Shuangyong Song,Yunhai Tong,Xiangtai Li,Xuelong Li,Shuicheng Yan*

Main category: cs.LG

TL;DR: Muddit是一种统一的离散扩散变压器，支持快速并行生成文本和图像，结合预训练视觉先验和轻量级文本解码器，性能优于传统自回归模型。


<details>
  <summary>Details</summary>
Motivation: 解决自回归统一模型推理慢和非自回归模型泛化能力弱的问题，探索离散扩散作为统一生成任务的有效方法。

Method: 提出Muddit，结合预训练文本到图像骨干和轻量级文本解码器，实现跨模态并行生成。

Result: 在质量和效率上优于更大的自回归模型，展示了离散扩散在统一生成中的潜力。

Conclusion: 离散扩散结合强视觉先验是统一生成任务的可扩展且有效的方法。

Abstract: Unified generation models aim to handle diverse tasks across modalities --
such as text generation, image generation, and vision-language reasoning --
within a single architecture and decoding paradigm. Autoregressive unified
models suffer from slow inference due to sequential decoding, and
non-autoregressive unified models suffer from weak generalization due to
limited pretrained backbones. We introduce Muddit, a unified discrete diffusion
transformer that enables fast and parallel generation across both text and
image modalities. Unlike prior unified diffusion models trained from scratch,
Muddit integrates strong visual priors from a pretrained text-to-image backbone
with a lightweight text decoder, enabling flexible and high-quality multimodal
generation under a unified architecture. Empirical results show that Muddit
achieves competitive or superior performance compared to significantly larger
autoregressive models in both quality and efficiency. The work highlights the
potential of purely discrete diffusion, when equipped with strong visual
priors, as a scalable and effective backbone for unified generation.

</details>


### [171] [Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation](https://arxiv.org/abs/2505.23651)
*Juncheol Shin,Minsang Seok,Seonggon Kim,Eunhyeok Park*

Main category: cs.LG

TL;DR: 研究分析了量化对模型合并的影响，并提出了一种新的后训练量化方法HDRQ，以支持多目标域适应的模型合并。


<details>
  <summary>Details</summary>
Motivation: 量化在目标特定数据上的应用限制了兴趣域并引入离散化效应，使模型合并变得复杂。

Method: 通过误差屏障分析量化影响，提出HDRQ方法，结合Hessian和远距离正则化量化，确保量化过程偏离源预训练模型最小，并平滑损失表面。

Result: 实验证实HDRQ在多目标域适应中有效。

Conclusion: HDRQ是首个针对量化模型合并挑战的研究，效果显著。

Abstract: Model merging has emerged as a powerful technique for combining task-specific
weights, achieving superior performance in multi-target domain adaptation.
However, when applied to practical scenarios, such as quantized models, new
challenges arise. In practical scenarios, quantization is often applied to
target-specific data, but this process restricts the domain of interest and
introduces discretization effects, making model merging highly non-trivial. In
this study, we analyze the impact of quantization on model merging through the
lens of error barriers. Leveraging these insights, we propose a novel
post-training quantization, HDRQ - Hessian and distant regularizing
quantization - that is designed to consider model merging for multi-target
domain adaptation. Our approach ensures that the quantization process incurs
minimal deviation from the source pre-trained model while flattening the loss
surface to facilitate smooth model merging. To our knowledge, this is the first
study on this challenge, and extensive experiments confirm its effectiveness.

</details>


### [172] [REOrdering Patches Improves Vision Models](https://arxiv.org/abs/2505.23751)
*Declan Kutscher,David M. Chan,Yutong Bai,Trevor Darrell,Ritwik Gupta*

Main category: cs.LG

TL;DR: 论文提出REOrder框架，通过信息论先验和强化学习优化图像块的排列顺序，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有序列模型（如Transformer）对图像块的固定顺序（如行优先）敏感，影响性能，需探索更优的排列方式。

Method: 提出两阶段框架：1）基于信息论评估图像块序列的可压缩性；2）使用REINFORCE优化Plackett-Luce策略学习排列策略。

Result: 在ImageNet-1K上，REOrder比行优先排列提升3.01%的top-1准确率；在Functional Map of the World上提升13.35%。

Conclusion: REOrder通过优化图像块排列顺序，显著提升模型性能，为序列模型在视觉任务中的应用提供了新思路。

Abstract: Sequence models such as transformers require inputs to be represented as
one-dimensional sequences. In vision, this typically involves flattening images
using a fixed row-major (raster-scan) order. While full self-attention is
permutation-equivariant, modern long-sequence transformers increasingly rely on
architectural approximations that break this invariance and introduce
sensitivity to patch ordering. We show that patch order significantly affects
model performance in such settings, with simple alternatives like column-major
or Hilbert curves yielding notable accuracy shifts. Motivated by this, we
propose REOrder, a two-stage framework for discovering task-optimal patch
orderings. First, we derive an information-theoretic prior by evaluating the
compressibility of various patch sequences. Then, we learn a policy over
permutations by optimizing a Plackett-Luce policy using REINFORCE. This
approach enables efficient learning in a combinatorial permutation space.
REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to
3.01% and Functional Map of the World by 13.35%.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [173] [CF-DETR: Coarse-to-Fine Transformer for Real-Time Object Detection](https://arxiv.org/abs/2505.23317)
*Woojin Shin,Donghwa Kang,Byeongyun Park,Brent Byunghoon Kang,Jinkyu Lee,Hyeongboo Baek*

Main category: eess.SY

TL;DR: CF-DETR是一种针对自动驾驶感知系统的集成解决方案，通过粗到细的Transformer架构和实时调度框架NPFP**，解决了多DETR任务在实时性和准确性上的挑战。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶感知系统需要同时满足实时性和高准确性的要求，而现有方法未能充分利用Transformer的特性进行高效资源分配。

Method: CF-DETR采用粗到细推理、选择性细推理和多级批处理推理策略，结合NPFP**调度框架动态调整资源分配。

Result: 实验表明，CF-DETR在多种平台上均能满足实时性要求，并显著提高了整体和关键对象的检测精度。

Conclusion: CF-DETR通过创新的架构和调度策略，成功解决了自动驾驶感知系统中的实时性和准确性权衡问题。

Abstract: Detection Transformers (DETR) are increasingly adopted in autonomous vehicle
(AV) perception systems due to their superior accuracy over convolutional
networks. However, concurrently executing multiple DETR tasks presents
significant challenges in meeting firm real-time deadlines (R1) and high
accuracy requirements (R2), particularly for safety-critical objects, while
navigating the inherent latency-accuracy trade-off under resource constraints.
Existing real-time DNN scheduling approaches often treat models generically,
failing to leverage Transformer-specific properties for efficient resource
allocation. To address these challenges, we propose CF-DETR, an integrated
system featuring a novel coarse-to-fine Transformer architecture and a
dedicated real-time scheduling framework NPFP**. CF-DETR employs three key
strategies (A1: coarse-to-fine inference, A2: selective fine inference, A3:
multi-level batch inference) that exploit Transformer properties to dynamically
adjust patch granularity and attention scope based on object criticality,
aiming to satisfy R2. The NPFP** scheduling framework (A4) orchestrates these
adaptive mechanisms A1-A3. It partitions each DETR task into a safety-critical
coarse subtask for guaranteed critical object detection within its deadline
(ensuring R1), and an optional fine subtask for enhanced overall accuracy (R2),
while managing individual and batched execution. Our extensive evaluations on
server, GPU-enabled embedded platforms, and actual AV platforms demonstrate
that CF-DETR, under an NPFP** policy, successfully meets strict timing
guarantees for critical operations and achieves significantly higher overall
and critical object detection accuracy compared to existing baselines across
diverse AV workloads.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [174] [ZeroGUI: Automating Online GUI Learning at Zero Human Cost](https://arxiv.org/abs/2505.23762)
*Chenyu Yang,Shiqian Su,Shi Liu,Xuan Dong,Yue Yu,Weijie Su,Xuehui Wang,Zhaoyang Liu,Jinguo Zhu,Hao Li,Wenhai Wang,Yu Qiao,Xizhou Zhu,Jifeng Dai*

Main category: cs.AI

TL;DR: ZeroGUI是一个在线学习框架，用于自动化GUI代理训练，无需人工标注，通过VLM生成任务和奖励，结合强化学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理方法依赖人工标注且适应性有限，ZeroGUI旨在解决这些问题。

Method: ZeroGUI结合VLM自动生成任务和奖励，采用两阶段在线强化学习。

Result: 在UI-TARS和Aguvis上测试，ZeroGUI显著提升了OSWorld和AndroidLab环境中的性能。

Conclusion: ZeroGUI为GUI代理训练提供了一种高效、零人工成本的解决方案。

Abstract: The rapid advancement of large Vision-Language Models (VLMs) has propelled
the development of pure-vision-based GUI Agents, capable of perceiving and
operating Graphical User Interfaces (GUI) to autonomously fulfill user
instructions. However, existing approaches usually adopt an offline learning
framework, which faces two core limitations: (1) heavy reliance on high-quality
manual annotations for element grounding and action supervision, and (2)
limited adaptability to dynamic and interactive environments. To address these
limitations, we propose ZeroGUI, a scalable, online learning framework for
automating GUI Agent training at Zero human cost. Specifically, ZeroGUI
integrates (i) VLM-based automatic task generation to produce diverse training
goals from the current environment state, (ii) VLM-based automatic reward
estimation to assess task success without hand-crafted evaluation functions,
and (iii) two-stage online reinforcement learning to continuously interact with
and learn from GUI environments. Experiments on two advanced GUI Agents
(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance
across OSWorld and AndroidLab environments. The code is available at
https://github.com/OpenGVLab/ZeroGUI.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [175] [Spoken question answering for visual queries](https://arxiv.org/abs/2505.23308)
*Nimrod Shabtay,Zvi Kons,Avihu Dekel,Hagai Aronowitz,Ron Hoory,Assaf Arbelle*

Main category: eess.AS

TL;DR: 该论文提出了一种多模态模型（SVQA），结合文本、语音和图像输入，用于回答基于图像的语音问题。通过合成数据集训练，模型性能接近基于文本的上限模型。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够同时处理语音和图像输入的问答系统，填补现有数据集的空白。

Method: 融合文本、语音和图像模态，使用零样本TTS模型合成数据集进行训练。

Result: 合成语音训练的模型性能接近文本上限模型，TTS模型选择对准确性影响较小。

Conclusion: 多模态SVQA模型在合成数据集上表现良好，为未来研究提供了可行方向。

Abstract: Question answering (QA) systems are designed to answer natural language
questions. Visual QA (VQA) and Spoken QA (SQA) systems extend the textual QA
system to accept visual and spoken input respectively.
  This work aims to create a system that enables user interaction through both
speech and images. That is achieved through the fusion of text, speech, and
image modalities to tackle the task of spoken VQA (SVQA). The resulting
multi-modal model has textual, visual, and spoken inputs and can answer spoken
questions on images.
  Training and evaluating SVQA models requires a dataset for all three
modalities, but no such dataset currently exists. We address this problem by
synthesizing VQA datasets using two zero-shot TTS models. Our initial findings
indicate that a model trained only with synthesized speech nearly reaches the
performance of the upper-bounding model trained on textual QAs. In addition, we
show that the choice of the TTS model has a minor impact on accuracy.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [176] [Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates](https://arxiv.org/abs/2505.22943)
*Jaewoo Ahn,Heeseung Yun,Dayoon Ko,Gunhee Kim*

Main category: cs.CL

TL;DR: 论文提出MAC基准，利用LLM生成欺骗性文本样本以评估多模态表示的组合漏洞，并提出自训练方法提升零样本性能。


<details>
  <summary>Details</summary>
Motivation: 预训练多模态表示（如CLIP）存在组合漏洞，导致反直觉判断，需系统性评估和改进。

Method: 利用LLM生成欺骗性文本样本，通过自训练方法（拒绝采样微调和多样性过滤）提升攻击成功率和样本多样性。

Result: 使用较小语言模型（如Llama-3.1-8B），方法在多模态表示（图像、视频、音频）中展现出更好的组合漏洞检测性能。

Conclusion: MAC基准和自训练方法有效揭示并改进了多模态表示的组合漏洞，为未来研究提供了工具和方向。

Abstract: While pre-trained multimodal representations (e.g., CLIP) have shown
impressive capabilities, they exhibit significant compositional vulnerabilities
leading to counterintuitive judgments. We introduce Multimodal Adversarial
Compositionality (MAC), a benchmark that leverages large language models (LLMs)
to generate deceptive text samples to exploit these vulnerabilities across
different modalities and evaluates them through both sample-wise attack success
rate and group-wise entropy-based diversity. To improve zero-shot methods, we
propose a self-training approach that leverages rejection-sampling fine-tuning
with diversity-promoting filtering, which enhances both attack success rate and
sample diversity. Using smaller language models like Llama-3.1-8B, our approach
demonstrates superior performance in revealing compositional vulnerabilities
across various multimodal representations, including images, videos, and
audios.

</details>


### [177] [NegVQA: Can Vision Language Models Understand Negation?](https://arxiv.org/abs/2505.22946)
*Yuhui Zhang,Yuchang Su,Yiming Liu,Serena Yeung-Levy*

Main category: cs.CL

TL;DR: NegVQA是一个评估视觉语言模型（VLM）理解否定能力的基准测试，包含7,379个二选一问题。研究发现，现有VLM在否定问题上表现显著下降，并揭示了模型规模与性能的U型关系。


<details>
  <summary>Details</summary>
Motivation: 评估VLM在否定理解上的能力，以应对其在高风险应用中的部署需求。

Method: 利用大语言模型生成否定问题，构建NegVQA基准，并测试20个先进VLM的性能。

Result: VLM在否定问题上表现显著下降，且模型规模与性能呈U型关系。

Conclusion: NegVQA揭示了VLM在否定理解上的不足，为未来VLM发展提供了方向。

Abstract: Negation is a fundamental linguistic phenomenon that can entirely reverse the
meaning of a sentence. As vision language models (VLMs) continue to advance and
are deployed in high-stakes applications, assessing their ability to comprehend
negation becomes essential. To address this, we introduce NegVQA, a visual
question answering (VQA) benchmark consisting of 7,379 two-choice questions
covering diverse negation scenarios and image-question distributions. We
construct NegVQA by leveraging large language models to generate negated
versions of questions from existing VQA datasets. Evaluating 20
state-of-the-art VLMs across seven model families, we find that these models
struggle significantly with negation, exhibiting a substantial performance drop
compared to their responses to the original questions. Furthermore, we uncover
a U-shaped scaling trend, where increasing model size initially degrades
performance on NegVQA before leading to improvements. Our benchmark reveals
critical gaps in VLMs' negation understanding and offers insights into future
VLM development. Project page available at
https://yuhui-zh15.github.io/NegVQA/.

</details>


### [178] [Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint](https://arxiv.org/abs/2505.23759)
*Heekyung Lee,Jiaxin Ge,Tsung-Han Wu,Minwoo Kang,Trevor Darrell,David M. Chan*

Main category: cs.CL

TL;DR: 本文研究了当前视觉语言模型（VLMs）在解决Rebus谜题（一种通过图像、空间排列和符号替换编码语言的视觉谜题）时的能力，发现模型在简单视觉线索解码上有一定能力，但在抽象推理、横向思维和视觉隐喻理解上表现不佳。


<details>
  <summary>Details</summary>
Motivation: Rebus谜题对VLMs提出了独特挑战，需要多模态抽象、符号推理以及对文化、语音和语言双关的理解。本文旨在评估VLMs在此类任务中的表现。

Method: 构建了一个手工生成和标注的多样化Rebus谜题基准，涵盖从简单图像替换到空间依赖线索的多种类型，并分析不同VLMs的表现。

Result: VLMs在解码简单视觉线索时表现出一定能力，但在需要抽象推理、横向思维和视觉隐喻理解的任务中表现显著不足。

Conclusion: 当前VLMs在解决复杂Rebus谜题时存在局限性，尤其是在抽象推理和视觉隐喻理解方面，未来研究需进一步改进模型的多模态推理能力。

Abstract: Rebus puzzles, visual riddles that encode language through imagery, spatial
arrangement, and symbolic substitution, pose a unique challenge to current
vision-language models (VLMs). Unlike traditional image captioning or question
answering tasks, rebus solving requires multi-modal abstraction, symbolic
reasoning, and a grasp of cultural, phonetic and linguistic puns. In this
paper, we investigate the capacity of contemporary VLMs to interpret and solve
rebus puzzles by constructing a hand-generated and annotated benchmark of
diverse English-language rebus puzzles, ranging from simple pictographic
substitutions to spatially-dependent cues ("head" over "heels"). We analyze how
different VLMs perform, and our findings reveal that while VLMs exhibit some
surprising capabilities in decoding simple visual clues, they struggle
significantly with tasks requiring abstract reasoning, lateral thinking, and
understanding visual metaphors.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [179] [PdNeuRAM: Forming-Free, Multi-Bit Pd/HfO2 ReRAM for Energy-Efficient Computing](https://arxiv.org/abs/2505.22789)
*Erbing Hua,Theofilos Spyrou,Majid Ahmadi,Abdul Momin Syed,Hanzhi Xun,Laurentiu Braic,Ewout van der Veer,Nazek Elatab,Anteneh Gebregiorgis,Georgi Gaydadjiev,Beatriz Noheda,Said Hamdioui,Ryoichi Ishihara,Heba Abunahla*

Main category: cond-mat.mtrl-sci

TL;DR: 研究提出了一种新型HfO2基无电形成ReRAM器件PdNeuRAM，具有低电压操作、多比特功能和低变异性，通过Pd-O-Hf结构实现电荷重新分布，无需电形成步骤。


<details>
  <summary>Details</summary>
Motivation: 解决传统ReRAM器件中电形成步骤带来的高功耗、面积开销和低耐久性问题。

Method: 通过深入的材料表征，发现Pd-O-Hf结构的关键作用，并开发了PdNeuRAM器件。

Result: 器件实现了多比特功能，编程和读取能耗分别降低43%和73%，且变异性降低。

Conclusion: 研究为高效节能且经济实惠的ReRAM器件提供了新的机理见解和实现路径。

Abstract: Memristor technology shows great promise for energy-efficient computing, yet
it grapples with challenges like resistance drift and inherent variability. For
filamentary Resistive RAM (ReRAM), one of the most investigated types of
memristive devices, the expensive electroforming step required to create
conductive pathways results in increased power and area overheads and reduced
endurance. In this study, we present novel HfO2-based forming-free ReRAM
devices, PdNeuRAM, that operate at low voltages, support multi-bit
functionality, and display reduced variability. Through a deep understanding
and comprehensive material characterization, we discover the key process that
allows this unique behavior: a Pd-O-Hf configuration that capitalizes on Pd
innate affinity for integrating into HfO2. This structure actively facilitates
charge redistribution at room temperature, effectively eliminating the need for
electroforming. Moreover, the fabricated ReRAM device provides tunable
resistance states for dense memory and reduces programming and reading energy
by 43% and 73%, respectively, using spiking neural networks (SNN). This study
reveals novel mechanistic insights and delineates a strategic roadmap for the
realization of power-efficient and cost-effective ReRAM devices.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [180] [AMOR: Adaptive Character Control through Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2505.23708)
*Lucas N. Alegre,Agon Serifi,Ruben Grandia,David Müller,Espen Knoop,Moritz Bächer*

Main category: cs.RO

TL;DR: 提出了一种多目标强化学习框架，通过训练一个基于权重调节的策略，显著减少调参时间，并支持动态权重选择和分层应用。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法依赖加权奖励函数，调参耗时且难以适应现实世界中的sim-to-real差距。

Method: 提出多目标强化学习框架，训练一个基于权重调节的策略，覆盖奖励权衡的Pareto前沿。

Result: 该框架支持训练后动态调整权重，实现高效迭代，并在机器人动态运动中验证了其有效性。

Conclusion: 多目标策略编码了多样行为，支持高效适应新任务，尤其在分层设置中表现优异。

Abstract: Reinforcement learning (RL) has significantly advanced the control of
physics-based and robotic characters that track kinematic reference motion.
However, methods typically rely on a weighted sum of conflicting reward
functions, requiring extensive tuning to achieve a desired behavior. Due to the
computational cost of RL, this iterative process is a tedious, time-intensive
task. Furthermore, for robotics applications, the weights need to be chosen
such that the policy performs well in the real world, despite inevitable
sim-to-real gaps. To address these challenges, we propose a multi-objective
reinforcement learning framework that trains a single policy conditioned on a
set of weights, spanning the Pareto front of reward trade-offs. Within this
framework, weights can be selected and tuned after training, significantly
speeding up iteration time. We demonstrate how this improved workflow can be
used to perform highly dynamic motions with a robot character. Moreover, we
explore how weight-conditioned policies can be leveraged in hierarchical
settings, using a high-level policy to dynamically select weights according to
the current task. We show that the multi-objective policy encodes a diverse
spectrum of behaviors, facilitating efficient adaptation to novel tasks.

</details>


### [181] [Anomalies by Synthesis: Anomaly Detection using Generative Diffusion Models for Off-Road Navigation](https://arxiv.org/abs/2505.22805)
*Siddharth Ancha,Sunshine Jiang,Travis Manderson,Laura Brandt,Yilun Du,Philip R. Osteen,Nicholas Roy*

Main category: cs.RO

TL;DR: 提出了一种基于生成扩散模型的像素级异常检测方法，无需对OOD数据做任何假设，通过分析扩散模型修改的图像片段实现异常检测。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，机器人需要检测与训练数据分布不同的异常，以确保安全可靠的导航。

Method: 使用生成扩散模型合成去除异常的图像，并通过分析修改的片段检测异常；提出了一种新的引导扩散推理方法。

Result: 方法无需重新训练或微调，可直接集成到现有工作流中，结合视觉语言基础模型实现准确的异常检测。

Conclusion: 该方法为机器人导航提供了一种有效的异常检测解决方案。

Abstract: In order to navigate safely and reliably in off-road and unstructured
environments, robots must detect anomalies that are out-of-distribution (OOD)
with respect to the training data. We present an analysis-by-synthesis approach
for pixel-wise anomaly detection without making any assumptions about the
nature of OOD data. Given an input image, we use a generative diffusion model
to synthesize an edited image that removes anomalies while keeping the
remaining image unchanged. Then, we formulate anomaly detection as analyzing
which image segments were modified by the diffusion model. We propose a novel
inference approach for guided diffusion by analyzing the ideal guidance
gradient and deriving a principled approximation that bootstraps the diffusion
model to predict guidance gradients. Our editing technique is purely test-time
that can be integrated into existing workflows without the need for retraining
or fine-tuning. Finally, we use a combination of vision-language foundation
models to compare pixels in a learned feature space and detect semantically
meaningful edits, enabling accurate anomaly detection for off-road navigation.
Project website: https://siddancha.github.io/anomalies-by-diffusion-synthesis/

</details>


### [182] [TrackVLA: Embodied Visual Tracking in the Wild](https://arxiv.org/abs/2505.23189)
*Shaoan Wang,Jiazhao Zhang,Minghan Li,Jiahang Liu,Anqi Li,Kui Wu,Fangwei Zhong,Junzhi Yu,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: TrackVLA是一种视觉-语言-动作（VLA）模型，通过结合目标识别和轨迹规划，在动态环境中实现高效的视觉跟踪。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在目标识别和轨迹规划分离时的性能限制，特别是在严重遮挡和高动态场景下。

Method: 利用共享的LLM骨干网络，结合语言建模头和基于锚点的扩散模型，实现识别与规划的协同。

Result: 在合成和真实环境中表现出SOTA性能，零样本下显著优于现有方法，并在10 FPS下保持鲁棒性。

Conclusion: TrackVLA展示了在动态和遮挡场景中的高效跟踪能力，为Embodied AI提供了新思路。

Abstract: Embodied visual tracking is a fundamental skill in Embodied AI, enabling an
agent to follow a specific target in dynamic environments using only egocentric
vision. This task is inherently challenging as it requires both accurate target
recognition and effective trajectory planning under conditions of severe
occlusion and high scene dynamics. Existing approaches typically address this
challenge through a modular separation of recognition and planning. In this
work, we propose TrackVLA, a Vision-Language-Action (VLA) model that learns the
synergy between object recognition and trajectory planning. Leveraging a shared
LLM backbone, we employ a language modeling head for recognition and an
anchor-based diffusion model for trajectory planning. To train TrackVLA, we
construct an Embodied Visual Tracking Benchmark (EVT-Bench) and collect diverse
difficulty levels of recognition samples, resulting in a dataset of 1.7 million
samples. Through extensive experiments in both synthetic and real-world
environments, TrackVLA demonstrates SOTA performance and strong
generalizability. It significantly outperforms existing methods on public
benchmarks in a zero-shot manner while remaining robust to high dynamics and
occlusion in real-world scenarios at 10 FPS inference speed. Our project page
is: https://pku-epic.github.io/TrackVLA-web.

</details>


### [183] [Autoregressive Meta-Actions for Unified Controllable Trajectory Generation](https://arxiv.org/abs/2505.23612)
*Jianbo Zhao,Taiyu Ban,Xiyang Wang,Qibin Zhou,Hangning Zhou,Zhihao Liu,Mu Yang,Lei Liu,Bin Li*

Main category: cs.RO

TL;DR: 论文提出了一种自回归元动作方法，用于解决自动驾驶系统中元动作与轨迹时间不对齐的问题，通过分解长间隔元动作为帧级元动作，实现更精确的轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统依赖固定时间间隔的元动作，导致元动作与实际轨迹时间不对齐，影响任务一致性和模型性能。

Method: 提出自回归元动作方法，将长间隔元动分解为帧级元动作，结合自回归轨迹生成框架，并采用分阶段预训练策略。

Result: 实验证明该方法提高了轨迹的自适应性和对动态决策场景的响应能力。

Conclusion: 该方法通过严格对齐元动作与轨迹，显著提升了自动驾驶系统的任务一致性和性能。

Abstract: Controllable trajectory generation guided by high-level semantic decisions,
termed meta-actions, is crucial for autonomous driving systems. A significant
limitation of existing frameworks is their reliance on invariant meta-actions
assigned over fixed future time intervals, causing temporal misalignment with
the actual behavior trajectories. This misalignment leads to irrelevant
associations between the prescribed meta-actions and the resulting
trajectories, disrupting task coherence and limiting model performance. To
address this challenge, we introduce Autoregressive Meta-Actions, an approach
integrated into autoregressive trajectory generation frameworks that provides a
unified and precise definition for meta-action-conditioned trajectory
prediction. Specifically, We decompose traditional long-interval meta-actions
into frame-level meta-actions, enabling a sequential interplay between
autoregressive meta-action prediction and meta-action-conditioned trajectory
generation. This decomposition ensures strict alignment between each trajectory
segment and its corresponding meta-action, achieving a consistent and unified
task formulation across the entire trajectory span and significantly reducing
complexity. Moreover, we propose a staged pre-training process to decouple the
learning of basic motion dynamics from the integration of high-level decision
control, which offers flexibility, stability, and modularity. Experimental
results validate our framework's effectiveness, demonstrating improved
trajectory adaptivity and responsiveness to dynamic decision-making scenarios.
We provide the video document and dataset, which are available at
https://arma-traj.github.io/.

</details>


### [184] [Mobi-$π$: Mobilizing Your Robot Learning Policy](https://arxiv.org/abs/2505.23692)
*Jingyun Yang,Isabella Huang,Brandon Vu,Max Bajracharya,Rika Antonova,Jeannette Bohg*

Main category: cs.RO

TL;DR: 论文提出了一种解决机器人策略在新环境中泛化能力不足的方法，通过优化机器人基座姿态使其符合训练数据的分布，从而避免重新训练策略。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉运动策略在训练时数据来源有限，导致在新机器人位置或视角下泛化能力差，限制了在移动平台上的应用。

Method: 提出了Mobi-π框架，包括评估指标、模拟任务、可视化工具和基线方法，并提出了一种基于3D高斯泼溅和采样优化的基座姿态优化方法。

Result: 实验表明，该方法在仿真和真实环境中均优于基线方法，有效提升了策略的泛化能力。

Conclusion: 通过优化机器人基座姿态，可以在不重新训练策略的情况下提升其在新环境中的表现，为移动平台上的精确任务提供了解决方案。

Abstract: Learned visuomotor policies are capable of performing increasingly complex
manipulation tasks. However, most of these policies are trained on data
collected from limited robot positions and camera viewpoints. This leads to
poor generalization to novel robot positions, which limits the use of these
policies on mobile platforms, especially for precise tasks like pressing
buttons or turning faucets. In this work, we formulate the policy mobilization
problem: find a mobile robot base pose in a novel environment that is in
distribution with respect to a manipulation policy trained on a limited set of
camera viewpoints. Compared to retraining the policy itself to be more robust
to unseen robot base pose initializations, policy mobilization decouples
navigation from manipulation and thus does not require additional
demonstrations. Crucially, this problem formulation complements existing
efforts to improve manipulation policy robustness to novel viewpoints and
remains compatible with them. To study policy mobilization, we introduce the
Mobi-$\pi$ framework, which includes: (1) metrics that quantify the difficulty
of mobilizing a given policy, (2) a suite of simulated mobile manipulation
tasks based on RoboCasa to evaluate policy mobilization, (3) visualization
tools for analysis, and (4) several baseline methods. We also propose a novel
approach that bridges navigation and manipulation by optimizing the robot's
base pose to align with an in-distribution base pose for a learned policy. Our
approach utilizes 3D Gaussian Splatting for novel view synthesis, a score
function to evaluate pose suitability, and sampling-based optimization to
identify optimal robot poses. We show that our approach outperforms baselines
in both simulation and real-world environments, demonstrating its effectiveness
for policy mobilization.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [185] [Multilook Coherent Imaging: Theoretical Guarantees and Algorithms](https://arxiv.org/abs/2505.23594)
*Xi Chen,Soham Jana,Christopher A. Metzler,Arian Maleki,Shirin Jalali*

Main category: stat.ML

TL;DR: 论文研究了多视相干成像中基于似然的方法，提出了理论框架和高效算法，显著提升了图像质量。


<details>
  <summary>Details</summary>
Motivation: 多视相干成像中乘性噪声（散斑）会降低图像质量，但理论基础尚未充分探索，因此需要研究理论和方法。

Method: 采用投影梯度下降（PGD）计算最大似然解，结合Newton-Schulz算法和bagging策略优化性能。

Result: 提出了首个理论上的MSE上界，并通过算法优化实现了最先进的性能。

Conclusion: 研究为多视相干成像提供了理论和算法支持，显著提升了图像质量。

Abstract: Multilook coherent imaging is a widely used technique in applications such as
digital holography, ultrasound imaging, and synthetic aperture radar. A central
challenge in these systems is the presence of multiplicative noise, commonly
known as speckle, which degrades image quality. Despite the widespread use of
coherent imaging systems, their theoretical foundations remain relatively
underexplored. In this paper, we study both the theoretical and algorithmic
aspects of likelihood-based approaches for multilook coherent imaging,
providing a rigorous framework for analysis and method development. Our
theoretical contributions include establishing the first theoretical upper
bound on the Mean Squared Error (MSE) of the maximum likelihood estimator under
the deep image prior hypothesis. Our results capture the dependence of MSE on
the number of parameters in the deep image prior, the number of looks, the
signal dimension, and the number of measurements per look. On the algorithmic
side, we employ projected gradient descent (PGD) as an efficient method for
computing the maximum likelihood solution. Furthermore, we introduce two key
ideas to enhance the practical performance of PGD. First, we incorporate the
Newton-Schulz algorithm to compute matrix inverses within the PGD iterations,
significantly reducing computational complexity. Second, we develop a bagging
strategy to mitigate projection errors introduced during PGD updates. We
demonstrate that combining these techniques with PGD yields state-of-the-art
performance. Our code is available at
https://github.com/Computational-Imaging-RU/Bagged-DIP-Speckle.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [186] [Wav2Sem: Plug-and-Play Audio Semantic Decoupling for 3D Speech-Driven Facial Animation](https://arxiv.org/abs/2505.23290)
*Hao Li,Ju Dai,Xin Zhao,Feng Zhou,Junjun Pan,Lei Li*

Main category: cs.SD

TL;DR: 论文提出Wav2Sem模块，通过语义解耦改善3D语音驱动面部动画中音似音节的平均效应。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用自监督音频模型编码器时，音似音节在特征空间中耦合，导致唇形生成的平均效应。

Method: 提出Wav2Sem模块，提取音频序列的语义特征，解耦特征空间中的音频编码。

Result: 实验表明Wav2Sem有效解耦音频特征，显著提升唇形生成的精确性和自然度。

Conclusion: Wav2Sem模块能有效解决音似音节的耦合问题，提升面部动画质量。

Abstract: In 3D speech-driven facial animation generation, existing methods commonly
employ pre-trained self-supervised audio models as encoders. However, due to
the prevalence of phonetically similar syllables with distinct lip shapes in
language, these near-homophone syllables tend to exhibit significant coupling
in self-supervised audio feature spaces, leading to the averaging effect in
subsequent lip motion generation. To address this issue, this paper proposes a
plug-and-play semantic decorrelation module-Wav2Sem. This module extracts
semantic features corresponding to the entire audio sequence, leveraging the
added semantic information to decorrelate audio encodings within the feature
space, thereby achieving more expressive audio features. Extensive experiments
across multiple Speech-driven models indicate that the Wav2Sem module
effectively decouples audio features, significantly alleviating the averaging
effect of phonetically similar syllables in lip shape generation, thereby
enhancing the precision and naturalness of facial animations. Our source code
is available at https://github.com/wslh852/Wav2Sem.git.

</details>


### [187] [Semantics-Aware Human Motion Generation from Audio Instructions](https://arxiv.org/abs/2505.23465)
*Zi-An Wang,Shihao Zou,Shiyao Yu,Mingyuan Zhang,Chao Dong*

Main category: cs.SD

TL;DR: 本文提出了一种利用音频信号作为条件输入生成语义对齐运动的新任务，并通过端到端框架和记忆检索注意力模块解决了音频语义与运动生成之间的弱关联问题。


<details>
  <summary>Details</summary>
Motivation: 音频信号作为一种更自然直观的通信方式，其语义与生成运动之间的关联性较弱，现有方法多关注节奏匹配而非语义对齐。

Method: 采用掩码生成变换器和记忆检索注意力模块处理稀疏长音频输入，并通过丰富数据集（将描述转换为对话风格并生成多样化音频）提升性能。

Result: 实验证明该框架有效且高效，音频指令能传达与文本相似的语义，同时提供更实用和用户友好的交互。

Conclusion: 音频信号可作为语义编码的有效条件输入，为交互技术提供新的可能性。

Abstract: Recent advances in interactive technologies have highlighted the prominence
of audio signals for semantic encoding. This paper explores a new task, where
audio signals are used as conditioning inputs to generate motions that align
with the semantics of the audio. Unlike text-based interactions, audio provides
a more natural and intuitive communication method. However, existing methods
typically focus on matching motions with music or speech rhythms, which often
results in a weak connection between the semantics of the audio and generated
motions. We propose an end-to-end framework using a masked generative
transformer, enhanced by a memory-retrieval attention module to handle sparse
and lengthy audio inputs. Additionally, we enrich existing datasets by
converting descriptions into conversational style and generating corresponding
audio with varied speaker identities. Experiments demonstrate the effectiveness
and efficiency of the proposed framework, demonstrating that audio instructions
can convey semantics similar to text while providing more practical and
user-friendly interactions.

</details>


### [188] [ZeroSep: Separate Anything in Audio with Zero Training](https://arxiv.org/abs/2505.23625)
*Chao Huang,Yuesheng Ma,Junxuan Huang,Susan Liang,Yunlong Tang,Jing Bi,Wenqiang Liu,Nima Mesgarani,Chenliang Xu*

Main category: cs.SD

TL;DR: ZeroSep利用预训练的文本引导音频扩散模型，实现零样本音频源分离，无需任务特定训练，性能超越监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前监督深度学习方法需要大量标注数据且难以泛化到开放集场景，而生成基础模型可能克服这些限制。

Method: 通过将混合音频反转到扩散模型的潜在空间，利用文本条件引导去噪过程恢复单个源。

Result: ZeroSep在多个分离基准上表现优异，甚至超过监督方法。

Conclusion: 预训练的文本引导扩散模型可用于零样本音频源分离，支持开放集场景。

Abstract: Audio source separation is fundamental for machines to understand complex
acoustic environments and underpins numerous audio applications. Current
supervised deep learning approaches, while powerful, are limited by the need
for extensive, task-specific labeled data and struggle to generalize to the
immense variability and open-set nature of real-world acoustic scenes. Inspired
by the success of generative foundation models, we investigate whether
pre-trained text-guided audio diffusion models can overcome these limitations.
We make a surprising discovery: zero-shot source separation can be achieved
purely through a pre-trained text-guided audio diffusion model under the right
configuration. Our method, named ZeroSep, works by inverting the mixed audio
into the diffusion model's latent space and then using text conditioning to
guide the denoising process to recover individual sources. Without any
task-specific training or fine-tuning, ZeroSep repurposes the generative
diffusion model for a discriminative separation task and inherently supports
open-set scenarios through its rich textual priors. ZeroSep is compatible with
a variety of pre-trained text-guided audio diffusion backbones and delivers
strong separation performance on multiple separation benchmarks, surpassing
even supervised methods.

</details>
