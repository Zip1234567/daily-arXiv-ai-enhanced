<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 98]
- [eess.IV](#eess.IV) [Total: 8]
- [cs.GR](#cs.GR) [Total: 7]
- [physics.optics](#physics.optics) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts](https://arxiv.org/abs/2506.08048)
*Zheng Han,Jun Zhou,Jialun Pei,Jing Qin,Yingfang Fan,Qi Dou*

Main category: cs.CV

TL;DR: 提出了一种数据驱动的生物力学算法，结合人机交互机制，用于AR手术导航中的器官变形建模，提高了计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在手术中处理大范围解剖变化时效果不佳，且计算成本高，影响AR导航的可靠性。

Method: 结合数据驱动的生物力学算法和交互式人机机制，动态修正解剖对齐。

Result: 平均目标配准误差为3.42 mm，结合交互提示后降至2.78 mm，优于现有方法。

Conclusion: 该框架实现了高效准确的变形建模，增强了手术导航的可靠性，为计算机辅助手术提供了新方向。

Abstract: In augmented reality (AR)-guided surgical navigation, preoperative organ
models are superimposed onto the patient's intraoperative anatomy to visualize
critical structures such as vessels and tumors. Accurate deformation modeling
is essential to maintain the reliability of AR overlays by ensuring alignment
between preoperative models and the dynamically changing anatomy. Although the
finite element method (FEM) offers physically plausible modeling, its high
computational cost limits intraoperative applicability. Moreover, existing
algorithms often fail to handle large anatomical changes, such as those induced
by pneumoperitoneum or ligament dissection, leading to inaccurate anatomical
correspondences and compromised AR guidance. To address these challenges, we
propose a data-driven biomechanics algorithm that preserves FEM-level accuracy
while improving computational efficiency. In addition, we introduce a novel
human-in-the-loop mechanism into the deformation modeling process. This enables
surgeons to interactively provide prompts to correct anatomical misalignments,
thereby incorporating clinical expertise and allowing the model to adapt
dynamically to complex surgical scenarios. Experiments on a publicly available
dataset demonstrate that our algorithm achieves a mean target registration
error of 3.42 mm. Incorporating surgeon prompts through the interactive
framework further reduces the error to 2.78 mm, surpassing state-of-the-art
methods in volumetric accuracy. These results highlight the ability of our
framework to deliver efficient and accurate deformation modeling while
enhancing surgeon-algorithm collaboration, paving the way for safer and more
reliable computer-assisted surgeries.

</details>


### [2] [ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving](https://arxiv.org/abs/2506.08052)
*Yongkang Li,Kaixin Xiong,Xiangyu Guo,Fang Li,Sixu Yan,Gangwei Xu,Lijun Zhou,Long Chen,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: ReCogDrive提出了一种结合视觉语言模型（VLMs）和扩散规划器的自动驾驶系统，通过三阶段训练解决领域差距、维度不匹配和模仿学习问题，并在NAVSIM基准测试中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶在罕见和长尾场景中性能下降，且现有基于VLMs的方法存在领域差距、维度不匹配和模仿学习局限。

Method: 采用三阶段训练：1）用驾驶问答数据集训练VLMs；2）扩散规划器进行模仿学习；3）强化学习微调扩散规划器。

Result: 在NAVSIM基准测试中，PDMS达到89.6，超越之前最优方法5.6 PDMS。

Conclusion: ReCogDrive通过结合VLMs和扩散规划器，显著提升了自动驾驶在复杂场景中的性能。

Abstract: Although end-to-end autonomous driving has made remarkable progress, its
performance degrades significantly in rare and long-tail scenarios. Recent
approaches attempt to address this challenge by leveraging the rich world
knowledge of Vision-Language Models (VLMs), but these methods suffer from
several limitations: (1) a significant domain gap between the pre-training data
of VLMs and real-world driving data, (2) a dimensionality mismatch between the
discrete language space and the continuous action space, and (3) imitation
learning tends to capture the average behavior present in the dataset, which
may be suboptimal even dangerous. In this paper, we propose ReCogDrive, an
autonomous driving system that integrates VLMs with diffusion planner, which
adopts a three-stage paradigm for training. In the first stage, we use a
large-scale driving question-answering datasets to train the VLMs, mitigating
the domain discrepancy between generic content and real-world driving
scenarios. In the second stage, we employ a diffusion-based planner to perform
imitation learning, mapping representations from the latent language space to
continuous driving actions. Finally, we fine-tune the diffusion planner using
reinforcement learning with NAVSIM non-reactive simulator, enabling the model
to generate safer, more human-like driving trajectories. We evaluate our
approach on the planning-oriented NAVSIM benchmark, achieving a PDMS of 89.6
and setting a new state-of-the-art that surpasses the previous vision-only SOTA
by 5.6 PDMS.

</details>


### [3] [CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems](https://arxiv.org/abs/2506.08071)
*Aniket Rege,Zinnia Nie,Mahesh Ramesh,Unmesh Raskar,Zhuoran Yu,Aditya Kusupati,Yong Jae Lee,Ramya Korlakai Vinayak*

Main category: cs.CV

TL;DR: CuRe是一个用于评估文本到图像（T2I）系统文化代表性的新基准和评分套件，通过分析系统对文本条件增加的响应来量化文化偏见。


<details>
  <summary>Details</summary>
Motivation: 现有的T2I系统训练数据偏向欧美文化，忽视了全球南方的文化多样性，CuRe旨在填补这一空白。

Method: CuRe利用Wikimedia知识图谱构建了一个包含300种文化物品的层次化数据集，并通过分析T2I系统对文本条件变化的响应来评估其文化代表性。

Result: CuRe评分与人类对感知相似性、图像-文本对齐和文化多样性的判断高度相关，适用于多种图像编码器和T2I系统。

Conclusion: CuRe为评估和改进T2I系统的文化多样性提供了有效工具，数据集和代码已开源。

Abstract: Popular text-to-image (T2I) systems are trained on web-scraped data, which is
heavily Amero and Euro-centric, underrepresenting the cultures of the Global
South. To analyze these biases, we introduce CuRe, a novel and scalable
benchmarking and scoring suite for cultural representativeness that leverages
the marginal utility of attribute specification to T2I systems as a proxy for
human judgments. Our CuRe benchmark dataset has a novel categorical hierarchy
built from the crowdsourced Wikimedia knowledge graph, with 300 cultural
artifacts across 32 cultural subcategories grouped into six broad cultural axes
(food, art, fashion, architecture, celebrations, and people). Our dataset's
categorical hierarchy enables CuRe scorers to evaluate T2I systems by analyzing
their response to increasing the informativeness of text conditioning, enabling
fine-grained cultural comparisons. We empirically observe much stronger
correlations of our class of scorers to human judgments of perceptual
similarity, image-text alignment, and cultural diversity across image encoders
(SigLIP 2, AIMV2 and DINOv2), vision-language models (OpenCLIP, SigLIP 2,
Gemini 2.0 Flash) and state-of-the-art text-to-image systems, including three
variants of Stable Diffusion (1.5, XL, 3.5 Large), FLUX.1 [dev], Ideogram 2.0,
and DALL-E 3. The code and dataset is open-sourced and available at
https://aniketrege.github.io/cure/.

</details>


### [4] [IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation](https://arxiv.org/abs/2506.08137)
*Oishee Bintey Hoque,Abhijin Adiga,Aniruddha Adiga,Siddharth Chaudhary,Madhav V. Marathe,S. S. Ravi,Kirti Rajagopalan,Amanda Wilson,Samarth Swarup*

Main category: cs.CV

TL;DR: IGraSS是一个结合语义分割和图优化的迭代框架，用于改进运河网络映射的准确性，显著减少不可达运河段并提升识别效果。


<details>
  <summary>Details</summary>
Motivation: 现有语义分割模型依赖高质量标注数据，但实际数据常不完整或噪声大。基础设施网络（如运河、道路）具有图级特性（如可达性、连通性），可用于优化标注数据。

Method: IGraSS结合RGB和多模态数据（NDWI、DEM）的语义分割模块与基于图的标注优化模块，迭代处理卫星图像并优化网络结构。

Result: IGraSS将不可达运河段从18%降至3%，优化后的标注数据显著提升运河识别效果。框架也适用于道路网络。

Conclusion: IGraSS是一个通用框架，既能优化噪声标注数据，又能从遥感图像中准确映射基础设施网络。

Abstract: Accurate canal network mapping is essential for water management, including
irrigation planning and infrastructure maintenance. State-of-the-art semantic
segmentation models for infrastructure mapping, such as roads, rely on large,
well-annotated remote sensing datasets. However, incomplete or inadequate
ground truth can hinder these learning approaches. Many infrastructure networks
have graph-level properties such as reachability to a source (like canals) or
connectivity (roads) that can be leveraged to improve these existing ground
truth. This paper develops a novel iterative framework IGraSS, combining a
semantic segmentation module-incorporating RGB and additional modalities (NDWI,
DEM)-with a graph-based ground-truth refinement module. The segmentation module
processes satellite imagery patches, while the refinement module operates on
the entire data viewing the infrastructure network as a graph. Experiments show
that IGraSS reduces unreachable canal segments from around 18% to 3%, and
training with refined ground truth significantly improves canal identification.
IGraSS serves as a robust framework for both refining noisy ground truth and
mapping canal networks from remote sensing imagery. We also demonstrate the
effectiveness and generalizability of IGraSS using road networks as an example,
applying a different graph-theoretic constraint to complete road networks.

</details>


### [5] [Spectral Domain Neural Reconstruction for Passband FMCW Radars](https://arxiv.org/abs/2506.08163)
*Harshvardhan Takawale,Nirupam Roy*

Main category: cs.CV

TL;DR: SpINRv2是一个基于神经网络的框架，用于通过FMCW雷达实现高保真体积重建，改进了前作SpINR，解决了高频下的相位混叠和子区间模糊问题。


<details>
  <summary>Details</summary>
Motivation: 高频FMCW雷达在体积重建中面临相位混叠和子区间模糊的挑战，需要一种更精确的方法来处理这些问题。

Method: 提出了一种完全可微的频域前向模型，结合隐式神经表示（INR）进行连续体积建模，并引入稀疏性和平滑性正则化以消除子区间模糊。

Result: SpINRv2在高频条件下显著优于经典和基于学习的方法，为基于神经雷达的3D成像设立了新基准。

Conclusion: SpINRv2通过频域建模和正则化技术，在高频FMCW雷达的体积重建中实现了更高的精度和效率。

Abstract: We present SpINRv2, a neural framework for high-fidelity volumetric
reconstruction using Frequency-Modulated Continuous-Wave (FMCW) radar.
Extending our prior work (SpINR), this version introduces enhancements that
allow accurate learning under high start frequencies-where phase aliasing and
sub-bin ambiguity become prominent. Our core contribution is a fully
differentiable frequency-domain forward model that captures the complex radar
response using closed-form synthesis, paired with an implicit neural
representation (INR) for continuous volumetric scene modeling. Unlike
time-domain baselines, SpINRv2 directly supervises the complex frequency
spectrum, preserving spectral fidelity while drastically reducing computational
overhead. Additionally, we introduce sparsity and smoothness regularization to
disambiguate sub-bin ambiguities that arise at fine range resolutions.
Experimental results show that SpINRv2 significantly outperforms both classical
and learning-based baselines, especially under high-frequency regimes,
establishing a new benchmark for neural radar-based 3D imaging.

</details>


### [6] [Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework](https://arxiv.org/abs/2506.08185)
*Huixin Zhan,Jason H. Moore*

Main category: cs.CV

TL;DR: 论文提出了一种基于离散扩散框架和视觉-语言-动作（VLA）管道的方法，用于建模外科医生的个性化手术风格指纹，同时探讨了隐私风险。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统常忽略外科医生的个性化信号，而手术风格因训练、经验和行为差异而各异。

Method: 通过离散扩散框架结合VLA管道，将手势预测建模为结构化序列去噪任务，利用内窥镜视频、手术意图语言和隐私感知的外科医生身份嵌入作为多模态输入。

Result: 在JIGSAWS数据集上验证了方法能准确重建手势序列并学习到每位外科医生的独特运动指纹，但个性化嵌入会增加身份泄露风险。

Conclusion: 个性化嵌入虽提升性能，但也增加隐私风险，需在手术建模中平衡个性化与隐私保护。

Abstract: Surgeons exhibit distinct operating styles due to differences in training,
experience, and motor behavior - yet current AI systems often ignore this
personalization signal. We propose a novel approach to model fine-grained,
surgeon-specific fingerprinting in robotic surgery using a discrete diffusion
framework integrated with a vision-language-action (VLA) pipeline. Our method
formulates gesture prediction as a structured sequence denoising task,
conditioned on multimodal inputs including endoscopic video, surgical intent
language, and a privacy-aware embedding of surgeon identity and skill.
Personalized surgeon fingerprinting is encoded through natural language prompts
using third-party language models, allowing the model to retain individual
behavioral style without exposing explicit identity. We evaluate our method on
the JIGSAWS dataset and demonstrate that it accurately reconstructs gesture
sequences while learning meaningful motion fingerprints unique to each surgeon.
To quantify the privacy implications of personalization, we perform membership
inference attacks and find that more expressive embeddings improve task
performance but simultaneously increase susceptibility to identity leakage.
These findings demonstrate that while personalized embeddings improve
performance, they also increase vulnerability to identity leakage, revealing
the importance of balancing personalization with privacy risk in surgical
modeling. Code is available at:
https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.

</details>


### [7] [Open World Scene Graph Generation using Vision Language Models](https://arxiv.org/abs/2506.08189)
*Amartya Dutta,Kazi Sajeed Mehrab,Medha Sawhney,Abhilash Neog,Mridul Khurana,Sepideh Fatemi,Aanish Pradhan,M. Maruf,Ismini Lourentzou,Arka Daw,Anuj Karpatne*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练的开放世界场景图生成方法，利用预训练视觉语言模型实现零样本推理。


<details>
  <summary>Details</summary>
Motivation: 现有场景图生成方法依赖数据集特定监督，限制了在开放世界中的适用性。

Method: 结合多模态提示、嵌入对齐和轻量级配对优化策略，实现零样本结构化推理。

Result: 在多个数据集上验证了预训练模型无需任务级训练即可进行关系理解的能力。

Conclusion: 该方法为开放世界场景图生成提供了高效、模型无关的解决方案。

Abstract: Scene-Graph Generation (SGG) seeks to recognize objects in an image and
distill their salient pairwise relationships. Most methods depend on
dataset-specific supervision to learn the variety of interactions, restricting
their usefulness in open-world settings, involving novel objects and/or
relations. Even methods that leverage large Vision Language Models (VLMs)
typically require benchmark-specific fine-tuning. We introduce Open-World SGG,
a training-free, efficient, model-agnostic framework that taps directly into
the pretrained knowledge of VLMs to produce scene graphs with zero additional
learning. Casting SGG as a zero-shot structured-reasoning problem, our method
combines multimodal prompting, embedding alignment, and a lightweight
pair-refinement strategy, enabling inference over unseen object vocabularies
and relation sets. To assess this setting, we formalize an Open-World
evaluation protocol that measures performance when no SGG-specific data have
been observed either in terms of objects and relations. Experiments on Visual
Genome, Open Images V6, and the Panoptic Scene Graph (PSG) dataset demonstrate
the capacity of pretrained VLMs to perform relational understanding without
task-level training.

</details>


### [8] [A PDE-Based Image Dehazing Method via Atmospheric Scattering Theory](https://arxiv.org/abs/2506.08793)
*Zhuoran Zheng*

Main category: cs.CV

TL;DR: 提出了一种基于偏微分方程（PDE）的单幅图像去雾方法，结合非局部正则化和暗通道先验，改进了PDE模型，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决单幅图像去雾问题，结合大气散射模型和非局部正则化，提出更高效的PDE框架。

Method: 改进的PDE模型，包含边缘保持扩散系数和高斯卷积算子，利用Lax-Milgram定理证明解的存在唯一性，并通过PyTorch GPU加速实现固定点迭代。

Result: 实验证明该方法是一种有效的去雾解决方案，并可推广至深度学习模型。

Conclusion: 提出的PDE框架在单幅图像去雾中表现优异，具有理论和实践的双重价值。

Abstract: This paper presents a novel partial differential equation (PDE) framework for
single-image dehazing. By integrating the atmospheric scattering model with
nonlocal regularization and dark channel prior, we propose the improved PDE: \[
-\text{div}\left(D(\nabla u)\nabla u\right) + \lambda(t) G(u) = \Phi(I,t,A) \]
where $D(\nabla u) = (|\nabla u| + \epsilon)^{-1}$ is the edge-preserving
diffusion coefficient, $G(u)$ is the Gaussian convolution operator, and
$\lambda(t)$ is the adaptive regularization parameter based on transmission map
$t$. We prove the existence and uniqueness of weak solutions in $H_0^1(\Omega)$
using Lax-Milgram theorem, and implement an efficient fixed-point iteration
scheme accelerated by PyTorch GPU computation. The experimental results
demonstrate that this method is a promising deghazing solution that can be
generalized to the deep model paradigm.

</details>


### [9] [Generative Learning of Differentiable Object Models for Compositional Interpretation of Complex Scenes](https://arxiv.org/abs/2506.08191)
*Antoni Nowinowski,Krzysztof Krawiec*

Main category: cs.CV

TL;DR: 扩展了DVP架构，使其能处理多物体场景，并通过潜在空间采样和多种训练模式提升训练效果，提出新基准测试并验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决原DVP无法处理多物体场景的问题，并利用潜在空间的解释性改进训练方法。

Method: 扩展DVP架构，引入潜在空间采样和多训练模式，定义图像和潜在空间的损失函数。

Result: 在重建质量和分解重叠物体方面优于基线模型（MONet和LIVE）。

Conclusion: 通过潜在空间优化和多样化训练模式有效提升性能，但仍存在可微渲染的限制。

Abstract: This study builds on the architecture of the Disentangler of Visual Priors
(DVP), a type of autoencoder that learns to interpret scenes by decomposing the
perceived objects into independent visual aspects of shape, size, orientation,
and color appearance. These aspects are expressed as latent parameters which
control a differentiable renderer that performs image reconstruction, so that
the model can be trained end-to-end with gradient using reconstruction loss. In
this study, we extend the original DVP so that it can handle multiple objects
in a scene. We also exploit the interpretability of its latent by using the
decoder to sample additional training examples and devising alternative
training modes that rely on loss functions defined not only in the image space,
but also in the latent space. This significantly facilitates training, which is
otherwise challenging due to the presence of extensive plateaus in the
image-space reconstruction loss. To examine the performance of this approach,
we propose a new benchmark featuring multiple 2D objects, which subsumes the
previously proposed Multi-dSprites dataset while being more parameterizable. We
compare the DVP extended in these ways with two baselines (MONet and LIVE) and
demonstrate its superiority in terms of reconstruction quality and capacity to
decompose overlapping objects. We also analyze the gradients induced by the
considered loss functions, explain how they impact the efficacy of training,
and discuss the limitations of differentiable rendering in autoencoders and the
ways in which they can be addressed.

</details>


### [10] [HiSin: Efficient High-Resolution Sinogram Inpainting via Resolution-Guided Progressive Inference](https://arxiv.org/abs/2506.08809)
*Jiaze E,Srutarshi Banerjee,Tekin Bicer,Guannan Wang,Yanfu Zhang,Bin Ren*

Main category: cs.CV

TL;DR: HiSin是一种基于扩散模型的高效正弦图修复框架，通过分辨率引导的渐进推理实现内存高效修复，显著减少内存和计算需求。


<details>
  <summary>Details</summary>
Motivation: 高分辨率正弦图修复对CT重建至关重要，但现有扩散模型因高内存和计算需求难以应用于高分辨率输入。

Method: HiSin采用分辨率引导的渐进推理，先在低分辨率提取全局结构，再在高分辨率处理小区域，结合频率感知跳过和结构自适应步长分配以减少冗余计算。

Result: 实验显示HiSin峰值内存使用减少31.25%，推理时间减少18.15%，并在不同数据集、分辨率和掩码条件下保持修复精度。

Conclusion: HiSin通过高效推理策略解决了高分辨率正弦图修复的内存和计算瓶颈，具有实际应用价值。

Abstract: High-resolution sinogram inpainting is essential for computed tomography
reconstruction, as missing high-frequency projections can lead to visible
artifacts and diagnostic errors. Diffusion models are well-suited for this task
due to their robustness and detail-preserving capabilities, but their
application to high-resolution inputs is limited by excessive memory and
computational demands. To address this limitation, we propose HiSin, a novel
diffusion based framework for efficient sinogram inpainting via
resolution-guided progressive inference. It progressively extracts global
structure at low resolution and defers high-resolution inference to small
patches, enabling memory-efficient inpainting. It further incorporates
frequency-aware patch skipping and structure-adaptive step allocation to reduce
redundant computation. Experimental results show that HiSin reduces peak memory
usage by up to 31.25% and inference time by up to 18.15%, and maintains
inpainting accuracy across datasets, resolutions, and mask conditions.

</details>


### [11] [GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra](https://arxiv.org/abs/2506.08194)
*Mateusz Michalkiewicz,Anekha Sokhal,Tadeusz Michalkiewicz,Piotr Pawlikowski,Mahsa Baktashmotlagh,Varun Jampani,Guha Balakrishnan*

Main category: cs.CV

TL;DR: GIQ是一个评估视觉和视觉语言基础模型几何推理能力的综合基准，揭示了当前模型在几何任务中的显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在标准基准上表现良好，但对几何属性的真实理解尚不明确，因此需要专门评估几何推理能力。

Method: GIQ包含224种多样多面体的合成和真实图像，通过单目3D重建、3D对称检测、心理旋转测试和零样本形状分类任务进行系统实验。

Result: 当前模型在重建基本几何形状、详细几何区分和复杂多面体分类任务中表现不佳。

Conclusion: GIQ为未来几何感知表示学习提供了结构化平台，揭示了关键差距。

Abstract: Monocular 3D reconstruction methods and vision-language models (VLMs)
demonstrate impressive results on standard benchmarks, yet their true
understanding of geometric properties remains unclear. We introduce GIQ , a
comprehensive benchmark specifically designed to evaluate the geometric
reasoning capabilities of vision and vision-language foundation models. GIQ
comprises synthetic and real-world images of 224 diverse polyhedra - including
Platonic, Archimedean, Johnson, and Catalan solids, as well as stellations and
compound shapes - covering varying levels of complexity and symmetry. Through
systematic experiments involving monocular 3D reconstruction, 3D symmetry
detection, mental rotation tests, and zero-shot shape classification tasks, we
reveal significant shortcomings in current models. State-of-the-art
reconstruction algorithms trained on extensive 3D datasets struggle to
reconstruct even basic geometric forms accurately. While foundation models
effectively detect specific 3D symmetry elements via linear probing, they
falter significantly in tasks requiring detailed geometric differentiation,
such as mental rotation. Moreover, advanced vision-language assistants exhibit
remarkably low accuracy on complex polyhedra, systematically misinterpreting
basic properties like face geometry, convexity, and compound structures. GIQ is
publicly available, providing a structured platform to highlight and address
critical gaps in geometric intelligence, facilitating future progress in
robust, geometry-aware representation learning.

</details>


### [12] [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/abs/2506.08210)
*Andrew Z. Wang,Songwei Ge,Tero Karras,Ming-Yu Liu,Yogesh Balaji*

Main category: cs.CV

TL;DR: 研究了使用现代仅解码器LLM作为文本编码器在文本到图像扩散模型中的效果，发现层归一化平均嵌入优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型仍使用过时的T5和CLIP作为文本编码器，探索现代LLM的潜力。

Method: 构建标准化训练和评估流程，训练27个模型，分析不同文本嵌入的影响。

Result: 层归一化平均嵌入显著提升复杂提示的对齐效果，多数LLM优于T5基线。

Conclusion: 现代LLM作为文本编码器在文本到图像生成中表现更优，尤其是层归一化平均嵌入方法。

Abstract: Both text-to-image generation and large language models (LLMs) have made
significant advancements. However, many text-to-image models still employ the
somewhat outdated T5 and CLIP as their text encoders. In this work, we
investigate the effectiveness of using modern decoder-only LLMs as text
encoders for text-to-image diffusion models. We build a standardized training
and evaluation pipeline that allows us to isolate and evaluate the effect of
different text embeddings. We train a total of 27 text-to-image models with 12
different text encoders to analyze the critical aspects of LLMs that could
impact text-to-image generation, including the approaches to extract
embeddings, different LLMs variants, and model sizes. Our experiments reveal
that the de facto way of using last-layer embeddings as conditioning leads to
inferior performance. Instead, we explore embeddings from various layers and
find that using layer-normalized averaging across all layers significantly
improves alignment with complex prompts. Most LLMs with this conditioning
outperform the baseline T5 model, showing enhanced performance in advanced
visio-linguistic reasoning skills.

</details>


### [13] [Using Satellite Images And Self-supervised Machine Learning Networks To Detect Water Hidden Under Vegetation](https://arxiv.org/abs/2506.08214)
*Ioannis Iakovidis,Zahra Kalantari,Amir Hossein Payberah,Fernando Jaramillo,Francisco Pena Escobar*

Main category: cs.CV

TL;DR: 本文提出了一种结合深度聚类和负采样的自监督方法，用于雷达卫星图像的水陆分割，无需人工标注，并通过集成模型提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统模型依赖大量人工标注数据的问题，降低标注成本和时间。

Method: 结合深度聚类和负采样的自监督训练方法，并采用集成模型减少方差。

Result: 自监督集成模型在测试集上的IOU指标比全监督单模型提升了0.02。

Conclusion: 自监督方法在减少标注需求的同时，能够实现接近甚至优于全监督模型的性能。

Abstract: In recent years the wide availability of high-resolution radar satellite
images along with the advancement of computer vision models have enabled the
remote monitoring of the surface area of wetlands. However, these models
require large amounts of manually annotated satellite images, which are slow
and expensive to produce. To overcome this problem, self-supervised training
methods have been deployed to train models without using annotated data. In
this paper we use a combination of deep clustering and negative sampling to
train a model to segment radar satellite images into areas that separate water
from land without the use of any manual annotations. Furthermore, we implement
an ensemble version of the model to reduce variance and improve performance.
Compared to a single fully-supervised model using the same architecture, our
ensemble of self-supervised models achieves a 0.02 improvement in the
Intersection Over Union metric over our test dataset.

</details>


### [14] [Jamais Vu: Exposing the Generalization Gap in Supervised Semantic Correspondence](https://arxiv.org/abs/2506.08220)
*Octave Mariotti,Zhipeng Du,Yash Bhalgat,Oisin Mac Aodha,Hakan Bilen*

Main category: cs.CV

TL;DR: 论文提出了一种通过单目深度估计将2D关键点提升到3D空间的方法，以学习密集语义对应关系，并在未见过的关键点上显著优于监督基线。


<details>
  <summary>Details</summary>
Motivation: 现有的监督语义对应方法局限于稀疏标注的关键点，难以泛化到未标注区域。

Method: 利用单目深度估计将2D关键点映射到3D空间，构建连续规范流形，无需显式3D监督或相机标注。

Result: 模型在未见过的关键点上显著优于监督基线，且无监督基线在跨数据集泛化中表现更优。

Conclusion: 提出的方法能有效学习鲁棒的语义对应关系，尤其在泛化能力上表现突出。

Abstract: Semantic correspondence (SC) aims to establish semantically meaningful
matches across different instances of an object category. We illustrate how
recent supervised SC methods remain limited in their ability to generalize
beyond sparsely annotated training keypoints, effectively acting as keypoint
detectors. To address this, we propose a novel approach for learning dense
correspondences by lifting 2D keypoints into a canonical 3D space using
monocular depth estimation. Our method constructs a continuous canonical
manifold that captures object geometry without requiring explicit 3D
supervision or camera annotations. Additionally, we introduce SPair-U, an
extension of SPair-71k with novel keypoint annotations, to better assess
generalization. Experiments not only demonstrate that our model significantly
outperforms supervised baselines on unseen keypoints, highlighting its
effectiveness in learning robust correspondences, but that unsupervised
baselines outperform supervised counterparts when generalized across different
datasets.

</details>


### [15] [A Good CREPE needs more than just Sugar: Investigating Biases in Compositional Vision-Language Benchmarks](https://arxiv.org/abs/2506.08227)
*Vishaal Udandarao,Mehdi Cherti,Shyamgopal Karthik,Jenia Jitsev,Samuel Albanie,Matthias Bethge*

Main category: cs.CV

TL;DR: 论文分析了17个用于评估视觉语言模型组合理解能力的基准，发现其设计存在偏见，导致简单启发式方法与CLIP模型表现相当，并提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示现有视觉语言组合理解基准的设计缺陷及其对模型评估的影响。

Method: 通过分析基准的数据来源和构建过程（如负面图像/标题的生成），识别其中的偏见，并比较启发式方法与CLIP模型的表现。

Result: 发现基准的正负样本分布不对称，导致简单启发式方法表现与CLIP模型相当，表明基准未能有效衡量组合理解能力。

Conclusion: 提出改进基准设计的建议，以减少偏见并增强评估的鲁棒性。

Abstract: We investigate 17 benchmarks (e.g. SugarCREPE, VALSE) commonly used for
measuring compositional understanding capabilities of vision-language models
(VLMs). We scrutinize design choices in their construction, including data
source (e.g. MS-COCO) and curation procedures (e.g. constructing negative
images/captions), uncovering several inherent biases across most benchmarks. We
find that blind heuristics (e.g. token-length, log-likelihood under a language
model) perform on par with CLIP models, indicating that these benchmarks do not
effectively measure compositional understanding. We demonstrate that the
underlying factor is a distribution asymmetry between positive and negative
images/captions, induced by the benchmark construction procedures. To mitigate
these issues, we provide a few key recommendations for constructing more robust
vision-language compositional understanding benchmarks, that would be less
prone to such simple attacks.

</details>


### [16] [Highly Compressed Tokenizer Can Generate Without Training](https://arxiv.org/abs/2506.08257)
*L. Lao Beyer,T. Li,X. Chen,S. Karaman,K. He*

Main category: cs.CV

TL;DR: 1D图像标记器通过高度压缩的一维序列表示图像，支持启发式标记操作实现图像编辑和生成，无需训练生成模型。


<details>
  <summary>Details</summary>
Motivation: 探索1D图像标记器在高度压缩下的表达能力及其在图像编辑和生成中的潜力。

Method: 利用向量量化的1D标记器，通过启发式操作和梯度优化的测试时优化技术，结合重建或CLIP相似性损失函数。

Result: 实现了精细的图像编辑和多样化、逼真的样本生成，适用于修复和文本引导编辑。

Conclusion: 1D标记器在图像编辑和生成中具有高效性和灵活性，无需依赖生成模型的训练。

Abstract: Commonly used image tokenizers produce a 2D grid of spatially arranged
tokens. In contrast, so-called 1D image tokenizers represent images as highly
compressed one-dimensional sequences of as few as 32 discrete tokens. We find
that the high degree of compression achieved by a 1D tokenizer with vector
quantization enables image editing and generative capabilities through
heuristic manipulation of tokens, demonstrating that even very crude
manipulations -- such as copying and replacing tokens between latent
representations of images -- enable fine-grained image editing by transferring
appearance and semantic attributes. Motivated by the expressivity of the 1D
tokenizer's latent space, we construct an image generation pipeline leveraging
gradient-based test-time optimization of tokens with plug-and-play loss
functions such as reconstruction or CLIP similarity. Our approach is
demonstrated for inpainting and text-guided image editing use cases, and can
generate diverse and realistic samples without requiring training of any
generative model.

</details>


### [17] [Seeing Voices: Generating A-Roll Video from Audio with Mirage](https://arxiv.org/abs/2506.08279)
*Aditi Sundararaman,Amogh Adishesha,Andrew Jaegle,Dan Bigioi,Hyoung-Kyu Song,Jon Kyl,Justin Mao,Kevin Lan,Mojtaba Komeili,ShahRukh Athar,Sheila Babayan,Stanislau Beliasau,William Buchwalter*

Main category: cs.CV

TL;DR: Mirage是一种音频到视频的基础模型，能够根据音频输入生成逼真、富有表现力的视频内容，尤其适用于语音驱动的视频生成。


<details>
  <summary>Details</summary>
Motivation: 视频的感染力依赖于音频与视觉的和谐结合，但现有方法要么忽略音频，要么局限于特定领域（如配音）。Mirage旨在填补这一空白，实现通用且高质量的音频到视频生成。

Method: Mirage采用基于自注意力的统一训练方法，支持从头训练或基于现有权重微调，无需特定于音频或视觉的架构或损失组件。

Result: Mirage生成的视频在主观质量上优于其他方法，尤其在语音驱动的视频生成中表现突出。

Conclusion: Mirage为音频到视频生成提供了通用且高质量的解决方案，其技术方法具有广泛适用性。

Abstract: From professional filmmaking to user-generated content, creators and
consumers have long recognized that the power of video depends on the
harmonious integration of what we hear (the video's audio track) with what we
see (the video's image sequence). Current approaches to video generation either
ignore sound to focus on general-purpose but silent image sequence generation
or address both visual and audio elements but focus on restricted application
domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation
model that excels at generating realistic, expressive output imagery from
scratch given an audio input. When integrated with existing methods for speech
synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal
video. When trained on audio-video footage of people talking (A-roll) and
conditioned on audio containing speech, Mirage generates video of people
delivering a believable interpretation of the performance implicit in input
audio. Our central technical contribution is a unified method for training
self-attention-based audio-to-video generation models, either from scratch or
given existing weights. This methodology allows Mirage to retain generality as
an approach to audio-to-video generation while producing outputs of superior
subjective quality to methods that incorporate audio-specific architectures or
loss components specific to people, speech, or details of how images or audio
are captured. We encourage readers to watch and listen to the results of Mirage
for themselves (see paper and comments for links).

</details>


### [18] [SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging](https://arxiv.org/abs/2506.08297)
*Nhat Thanh Tran,Fanghui Xue,Shuai Zhang,Jiancheng Lyu,Yunling Zheng,Yingyong Qi,Jack Xin*

Main category: cs.CV

TL;DR: 论文提出了一种名为SEMA的新型注意力机制，解决了传统注意力机制在计算复杂度和聚焦能力上的问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制在计算机视觉任务中存在计算复杂度高和线性注意力无法聚焦的问题，需要一种更高效的替代方案。

Method: 通过数学定义广义注意力，提出SEMA机制，利用令牌定位避免注意力分散，并结合算术平均捕获全局注意力。

Result: 在Imagenet-1k上的实验表明，SEMA在更大规模的图像上优于线性注意力和近期视觉Mamba模型。

Conclusion: SEMA是一种可扩展且高效的注意力机制，适用于大规模视觉任务。

Abstract: Attention is the critical component of a transformer. Yet the quadratic
computational complexity of vanilla full attention in the input size and the
inability of its linear attention variant to focus have been challenges for
computer vision tasks. We provide a mathematical definition of generalized
attention and formulate both vanilla softmax attention and linear attention
within the general framework. We prove that generalized attention disperses,
that is, as the number of keys tends to infinity, the query assigns equal
weights to all keys. Motivated by the dispersion property and recent
development of Mamba form of attention, we design Scalable and Efficient Mamba
like Attention (SEMA) which utilizes token localization to avoid dispersion and
maintain focusing, complemented by theoretically consistent arithmetic
averaging to capture global aspect of attention. We support our approach on
Imagenet-1k where classification results show that SEMA is a scalable and
effective alternative beyond linear attention, outperforming recent vision
Mamba models on increasingly larger scales of images at similar model parameter
sizes.

</details>


### [19] [OpenRR-1k: A Scalable Dataset for Real-World Reflection Removal](https://arxiv.org/abs/2506.08299)
*Kangning Yang,Ling Ouyang,Huiming Sun,Jie Cai,Lan Fu,Jiaming Ding,Chiu Man Ho,Zibo Meng*

Main category: cs.CV

TL;DR: 论文提出了一种新颖的反射数据集收集范式，并构建了一个高质量、多样化的OpenRR-1k数据集，用于提升反射去除技术的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有反射去除技术因缺乏高质量的真实场景数据集而受限，因此需要一种便捷、经济且可扩展的数据收集方法。

Method: 提出了一种新的数据集收集范式，确保数据高质量、像素对齐且多样化，并构建了包含1,000对图像的OpenRR-1k数据集。

Result: 通过实验验证，OpenRR-1k数据集能有效提升反射去除方法在真实场景中的鲁棒性。

Conclusion: 该数据集为反射去除技术提供了重要的数据支持，未来可进一步扩展应用。

Abstract: Reflection removal technology plays a crucial role in photography and
computer vision applications. However, existing techniques are hindered by the
lack of high-quality in-the-wild datasets. In this paper, we propose a novel
paradigm for collecting reflection datasets from a fresh perspective. Our
approach is convenient, cost-effective, and scalable, while ensuring that the
collected data pairs are of high quality, perfectly aligned, and represent
natural and diverse scenarios. Following this paradigm, we collect a
Real-world, Diverse, and Pixel-aligned dataset (named OpenRR-1k dataset), which
contains 1,000 high-quality transmission-reflection image pairs collected in
the wild. Through the analysis of several reflection removal methods and
benchmark evaluation experiments on our dataset, we demonstrate its
effectiveness in improving robustness in challenging real-world environments.
Our dataset is available at https://github.com/caijie0620/OpenRR-1k.

</details>


### [20] [Hyperspectral Image Classification via Transformer-based Spectral-Spatial Attention Decoupling and Adaptive Gating](https://arxiv.org/abs/2506.08324)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: STNet是一种新型网络架构，通过空间-光谱Transformer模块有效解决高光谱图像分类中的过拟合和泛化能力问题。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临高维数据、地物稀疏分布和光谱冗余等挑战，导致过拟合和泛化能力受限。

Method: STNet采用空间-光谱Transformer模块，通过解耦空间和光谱注意力及智能门控机制，实现高效特征提取与融合。

Result: STNet在IN、UP和KSC数据集上表现优于主流方法，提升了分类性能。

Conclusion: STNet通过创新设计提升了特征提取能力，减少了过拟合风险，适用于小样本和高噪声场景。

Abstract: Deep neural networks face several challenges in hyperspectral image
classification, including high-dimensional data, sparse distribution of ground
objects, and spectral redundancy, which often lead to classification
overfitting and limited generalization capability. To more effectively extract
and fuse spatial context with fine spectral information in hyperspectral image
(HSI) classification, this paper proposes a novel network architecture called
STNet. The core advantage of STNet stems from the dual innovative design of its
Spatial-Spectral Transformer module: first, the fundamental explicit decoupling
of spatial and spectral attention ensures targeted capture of key information
in HSI; second, two functionally distinct gating mechanisms perform intelligent
regulation at both the fusion level of attention flows (adaptive attention
fusion gating) and the internal level of feature transformation (GFFN). This
characteristic demonstrates superior feature extraction and fusion capabilities
compared to traditional convolutional neural networks, while reducing
overfitting risks in small-sample and high-noise scenarios. STNet enhances
model representation capability without increasing network depth or width. The
proposed method demonstrates superior performance on IN, UP, and KSC datasets,
outperforming mainstream hyperspectral image classification approaches.

</details>


### [21] [Locating Tennis Ball Impact on the Racket in Real Time Using an Event Camera](https://arxiv.org/abs/2506.08327)
*Yuto Kase,Kai Ishibe,Ryoma Yasuda,Yudai Washida,Sakiko Hashimoto*

Main category: cs.CV

TL;DR: 提出了一种使用事件相机实时定位网球拍击球位置的方法，解决了高速相机内存消耗大和手动数字化耗时的问题。


<details>
  <summary>Details</summary>
Motivation: 在网球等球拍运动中，准确测量击球位置对分析球员表现和个性化装备设计至关重要，但现有方法存在内存消耗大和人工误差问题。

Method: 通过事件相机高效捕捉亮度变化，结合传统计算机视觉技术和原创的事件处理（PATS）分三步识别：挥拍时间范围、击球时机、球和球拍轮廓。

Result: 实验结果在测量网球球员表现的可接受范围内，且计算时间满足实时应用需求。

Conclusion: 该方法能够高效、准确地实时定位击球位置，适用于长时间监控球员表现。

Abstract: In racket sports, such as tennis, locating the ball's position at impact is
important in clarifying player and equipment characteristics, thereby aiding in
personalized equipment design. High-speed cameras are used to measure the
impact location; however, their excessive memory consumption limits prolonged
scene capture, and manual digitization for position detection is time-consuming
and prone to human error. These limitations make it difficult to effectively
capture the entire playing scene, hindering the ability to analyze the player's
performance. We propose a method for locating the tennis ball impact on the
racket in real time using an event camera. Event cameras efficiently measure
brightness changes (called `events') with microsecond accuracy under high-speed
motion while using lower memory consumption. These cameras enable users to
continuously monitor their performance over extended periods. Our method
consists of three identification steps: time range of swing, timing at impact,
and contours of ball and racket. Conventional computer vision techniques are
utilized along with an original event-based processing to detect the timing at
impact (PATS: the amount of polarity asymmetry in time symmetry). The results
of the experiments were within the permissible range for measuring tennis
players' performance. Moreover, the computation time was sufficiently short for
real-time applications.

</details>


### [22] [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](https://arxiv.org/abs/2506.08351)
*Huixuan Zhang,Junzhe Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: 提出了一种名为Step AG的自适应引导策略，通过在前几步去噪中应用分类器自由引导，显著提升了生成效率，同时保持图像质量和文本对齐。


<details>
  <summary>Details</summary>
Motivation: 分类器自由引导方法在文本到视觉生成扩散模型中效率较低，现有自适应引导方法缺乏分析和普适性。

Method: 提出Step AG策略，限制分类器自由引导在前几步去噪中使用，减少计算成本。

Result: 实验表明，Step AG在图像质量和文本对齐上表现良好，平均提速20%至30%，适用于多种模型和设置。

Conclusion: Step AG是一种简单且普适的高效引导策略，显著提升了生成效率。

Abstract: With the rapid development of text-to-vision generation diffusion models,
classifier-free guidance has emerged as the most prevalent method for
conditioning. However, this approach inherently requires twice as many steps
for model forwarding compared to unconditional generation, resulting in
significantly higher costs. While previous study has introduced the concept of
adaptive guidance, it lacks solid analysis and empirical results, making
previous method unable to be applied to general diffusion models. In this work,
we present another perspective of applying adaptive guidance and propose Step
AG, which is a simple, universally applicable adaptive guidance strategy. Our
evaluations focus on both image quality and image-text alignment. whose results
indicate that restricting classifier-free guidance to the first several
denoising steps is sufficient for generating high-quality, well-conditioned
images, achieving an average speedup of 20% to 30%. Such improvement is
consistent across different settings such as inference steps, and various
models including video generation models, highlighting the superiority of our
method.

</details>


### [23] [MedMoE: Modality-Specialized Mixture of Experts for Medical Vision-Language Understanding](https://arxiv.org/abs/2506.08356)
*Shivang Chopra,Lingchao Mao,Gabriela Sanchez-Rodriguez,Andrew J Feola,Jing Li,Zsolt Kira*

Main category: cs.CV

TL;DR: MedMoE是一个动态适应不同医学成像模态的视觉语言处理框架，通过Mixture-of-Experts模块和多尺度特征提取，提升跨模态的对齐和检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言框架采用统一的局部特征提取策略，忽视了不同成像模态的特定需求，限制了诊断信息的有效捕捉。

Method: MedMoE基于Swin Transformer提取多尺度图像特征，并通过报告类型条件化的MoE模块路由到专门训练的专家分支，实现模态特定的视觉语义捕捉。

Result: 实验表明，MedMoE在多种医学基准测试中提升了跨模态的对齐和检索性能。

Conclusion: MedMoE证明了模态专用视觉表征在临床视觉语言系统中的价值，无需推理时的模态特定监督。

Abstract: Different medical imaging modalities capture diagnostic information at
varying spatial resolutions, from coarse global patterns to fine-grained
localized structures. However, most existing vision-language frameworks in the
medical domain apply a uniform strategy for local feature extraction,
overlooking the modality-specific demands. In this work, we present MedMoE, a
modular and extensible vision-language processing framework that dynamically
adapts visual representation based on the diagnostic context. MedMoE
incorporates a Mixture-of-Experts (MoE) module conditioned on the report type,
which routes multi-scale image features through specialized expert branches
trained to capture modality-specific visual semantics. These experts operate
over feature pyramids derived from a Swin Transformer backbone, enabling
spatially adaptive attention to clinically relevant regions. This framework
produces localized visual representations aligned with textual descriptions,
without requiring modality-specific supervision at inference. Empirical results
on diverse medical benchmarks demonstrate that MedMoE improves alignment and
retrieval performance across imaging modalities, underscoring the value of
modality-specialized visual representations in clinical vision-language
systems.

</details>


### [24] [Image Demoiréing Using Dual Camera Fusion on Mobile Phones](https://arxiv.org/abs/2506.08361)
*Yanting Mei,Zhilu Zhang,Xiaohe Wu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 提出了一种利用双摄像头融合（DCID）去除图像摩尔纹的方法，通过超广角（UW）图像辅助广角（W）图像去摩尔纹，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代智能手机通常配备双摄像头，且超广角图像在广角图像出现摩尔纹时能提供正常颜色和纹理。

Method: 提出轻量级UW图像编码器，集成到现有去摩尔纹网络中，并采用快速两阶段图像对齐方式。

Result: 在包含约9,000个样本的真实数据集上实验，效果优于现有方法。

Conclusion: DCID方法在去除大范围摩尔纹方面表现优异，代码和数据集已开源。

Abstract: When shooting electronic screens, moir\'e patterns usually appear in captured
images, which seriously affects the image quality. Existing image demoir\'eing
methods face great challenges in removing large and heavy moir\'e. To address
the issue, we propose to utilize Dual Camera fusion for Image Demoir\'eing
(DCID), \ie, using the ultra-wide-angle (UW) image to assist the moir\'e
removal of wide-angle (W) image. This is inspired by two motivations: (1) the
two lenses are commonly equipped with modern smartphones, (2) the UW image
generally can provide normal colors and textures when moir\'e exists in the W
image mainly due to their different focal lengths. In particular, we propose an
efficient DCID method, where a lightweight UW image encoder is integrated into
an existing demoir\'eing network and a fast two-stage image alignment manner is
present. Moreover, we construct a large-scale real-world dataset with diverse
mobile phones and monitors, containing about 9,000 samples. Experiments on the
dataset show our method performs better than state-of-the-art methods. Code and
dataset are available at https://github.com/Mrduckk/DCID.

</details>


### [25] [SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding](https://arxiv.org/abs/2506.08391)
*Woohyeon Park,Woojin Kim,Jaeik Kim,Jaeyoung Do*

Main category: cs.CV

TL;DR: SECOND是一种选择性对比解码方法，通过多尺度视觉信息减少视觉语言模型中的对象幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型因对象幻觉问题导致性能受限，需要更准确的视觉理解方法。

Method: SECOND采用选择性对比解码，逐步整合多尺度视觉信息，并以对象为中心对齐人类视觉感知。

Result: SECOND显著减少感知幻觉，在多个基准测试中表现优异。

Conclusion: 多尺度视觉信息的优先对比应用在视觉语言模型中具有巨大潜力，优于现有方法。

Abstract: Despite significant advancements in Vision-Language Models (VLMs), the
performance of existing VLMs remains hindered by object hallucination, a
critical challenge to achieving accurate visual understanding. To address this
issue, we propose SECOND: Selective and Contrastive Decoding, a novel approach
that enables VLMs to effectively leverage multi-scale visual information with
an object-centric manner, closely aligning with human visual perception. SECOND
progressively selects and integrates multi-scale visual information,
facilitating a more precise interpretation of images. By contrasting these
visual information iteratively, SECOND significantly reduces perceptual
hallucinations and outperforms a wide range of benchmarks. Our theoretical
analysis and experiments highlight the largely unexplored potential of
multi-scale application in VLMs, showing that prioritizing and contrasting
across scales outperforms existing methods.

</details>


### [26] [RadioDUN: A Physics-Inspired Deep Unfolding Network for Radio Map Estimation](https://arxiv.org/abs/2506.08418)
*Taiqin Chen,Zikun Zhou,Zheng Fang,Wenzhen Zou,Kanjun Liu,Ke Chen,Yongbing Zhang,Yaowei Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于稀疏信号恢复和物理传播模型的无线电地图估计方法，通过深度展开网络（RadioDUN）和动态重加权模块（DRM）提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以结合无线电地图的物理特性，稀疏样本导致密集地图构建困难。

Method: 将无线电地图估计建模为稀疏信号恢复问题，结合物理传播模型分解优化子问题，提出RadioDUN和DRM模块，并引入阴影损失。

Result: 实验表明，该方法优于现有技术。

Conclusion: RadioDUN通过结合物理特性和自适应学习，显著提升了无线电地图估计的准确性。

Abstract: The radio map represents the spatial distribution of spectrum resources
within a region, supporting efficient resource allocation and interference
mitigation. However, it is difficult to construct a dense radio map as a
limited number of samples can be measured in practical scenarios. While
existing works have used deep learning to estimate dense radio maps from sparse
samples, they are hard to integrate with the physical characteristics of the
radio map. To address this challenge, we cast radio map estimation as the
sparse signal recovery problem. A physical propagation model is further
incorporated to decompose the problem into multiple factor optimization
sub-problems, thereby reducing recovery complexity. Inspired by the existing
compressive sensing methods, we propose the Radio Deep Unfolding Network
(RadioDUN) to unfold the optimization process, achieving adaptive parameter
adjusting and prior fitting in a learnable manner. To account for the radio
propagation characteristics, we develop a dynamic reweighting module (DRM) to
adaptively model the importance of each factor for the radio map. Inspired by
the shadowing factor in the physical propagation model, we integrate
obstacle-related factors to express the obstacle-induced signal stochastic
decay. The shadowing loss is further designed to constrain the factor
prediction and act as a supplementary supervised objective, which enhances the
performance of RadioDUN. Extensive experiments have been conducted to
demonstrate that the proposed method outperforms the state-of-the-art methods.
Our code will be made publicly available upon publication.

</details>


### [27] [Better Reasoning with Less Data: Enhancing VLMs Through Unified Modality Scoring](https://arxiv.org/abs/2506.08429)
*Mingjie Xu,Andrew Estornell,Hongzheng Yang,Yuzhi Zhao,Zhaowei Zhu,Qi Xuan,Jiaheng Wei*

Main category: cs.CV

TL;DR: SCALE提出了一种基于数据质量驱动的视觉语言模型（VLM）指令调优数据集选择方法，通过跨模态评估框架解决图像与文本对齐问题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）的性能依赖于高质量数据集，但存在图像与文本对齐噪声和文本模糊问题，影响模型效果。

Method: SCALE通过跨模态评估框架，为数据条目分配任务、生成多类型描述，并评估对齐性、清晰度等指标。

Result: 发现当前单模态评估方法低估了特定任务关键样本，而生成的多模态描述可有效统一任务为文本模态。

Conclusion: SCALE通过数据质量驱动方法提升了VLM指令调优数据集的质量和模型性能。

Abstract: The application of visual instruction tuning and other post-training
techniques has significantly enhanced the capabilities of Large Language Models
(LLMs) in visual understanding, enriching Vision-Language Models (VLMs) with
more comprehensive visual language datasets. However, the effectiveness of VLMs
is highly dependent on large-scale, high-quality datasets that ensure precise
recognition and accurate reasoning. Two key challenges hinder progress: (1)
noisy alignments between images and the corresponding text, which leads to
misinterpretation, and (2) ambiguous or misleading text, which obscures visual
content. To address these challenges, we propose SCALE (Single modality data
quality and Cross modality Alignment Evaluation), a novel quality-driven data
selection pipeline for VLM instruction tuning datasets. Specifically, SCALE
integrates a cross-modality assessment framework that first assigns each data
entry to its appropriate vision-language task, generates general and
task-specific captions (covering scenes, objects, style, etc.), and evaluates
the alignment, clarity, task rarity, text coherence, and image clarity of each
entry based on the generated captions. We reveal that: (1) current unimodal
quality assessment methods evaluate one modality while overlooking the rest,
which can underestimate samples essential for specific tasks and discard the
lower-quality instances that help build model robustness; and (2) appropriately
generated image captions provide an efficient way to transfer the image-text
multimodal task into a unified text modality.

</details>


### [28] [Enhancing Motion Dynamics of Image-to-Video Models via Adaptive Low-Pass Guidance](https://arxiv.org/abs/2506.08456)
*June Suk Choi,Kyungmin Lee,Sihyun Yu,Yisol Choi,Jinwoo Shin,Kimin Lee*

Main category: cs.CV

TL;DR: 论文提出了一种自适应低通引导（ALG）方法，解决了图像到视频（I2V）生成中动态性不足的问题，显著提升了生成视频的动态性。


<details>
  <summary>Details</summary>
Motivation: 现有I2V方法在微调T2V模型时，常导致生成的视频动态性不足，原因是输入图像的高频细节过早影响采样过程。

Method: 提出ALG方法，通过在去噪早期阶段自适应地对条件图像进行低通滤波，调节其频率内容。

Result: 实验表明，ALG显著提升了视频的动态性（VBench-I2V测试中动态度平均提升36%），同时保持了图像质量和文本对齐。

Conclusion: ALG是一种简单有效的方法，能够在不牺牲图像质量的情况下提升I2V生成视频的动态性。

Abstract: Recent text-to-video (T2V) models have demonstrated strong capabilities in
producing high-quality, dynamic videos. To improve the visual controllability,
recent works have considered fine-tuning pre-trained T2V models to support
image-to-video (I2V) generation. However, such adaptation frequently suppresses
motion dynamics of generated outputs, resulting in more static videos compared
to their T2V counterparts. In this work, we analyze this phenomenon and
identify that it stems from the premature exposure to high-frequency details in
the input image, which biases the sampling process toward a shortcut trajectory
that overfits to the static appearance of the reference image. To address this,
we propose adaptive low-pass guidance (ALG), a simple fix to the I2V model
sampling procedure to generate more dynamic videos without compromising
per-frame image quality. Specifically, ALG adaptively modulates the frequency
content of the conditioning image by applying low-pass filtering at the early
stage of denoising. Extensive experiments demonstrate that ALG significantly
improves the temporal dynamics of generated videos, while preserving image
fidelity and text alignment. Especially, under VBench-I2V test suite, ALG
achieves an average improvement of 36% in dynamic degree without a significant
drop in video quality or image fidelity.

</details>


### [29] [MARMOT: Masked Autoencoder for Modeling Transient Imaging](https://arxiv.org/abs/2506.08470)
*Siyuan Shen,Ziheng Wang,Xingyue Peng,Suan Xia,Ruiqian Li,Shiying Li,Jingyi Yu*

Main category: cs.CV

TL;DR: MARMOT是一个基于掩码自编码器的自监督模型，用于处理非视距（NLOS）瞬态成像，通过预训练学习特征并适应下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法在NLOS瞬态成像中主要优化体积密度或表面重建，未利用数据集学习的先验知识，MARMOT旨在填补这一空白。

Method: 使用基于Transformer的编码器-解码器结构，通过扫描模式掩码（SPM）从部分掩码的瞬态数据中学习特征，并预测完整测量。

Result: 在合成数据集TransVerse上预训练后，MARMOT通过直接特征迁移或解码器微调适应下游任务，实验表明其优于现有方法。

Conclusion: MARMOT在NLOS瞬态成像中表现出高效性，为相关应用提供了新的解决方案。

Abstract: Pretrained models have demonstrated impressive success in many modalities
such as language and vision. Recent works facilitate the pretraining paradigm
in imaging research. Transients are a novel modality, which are captured for an
object as photon counts versus arrival times using a precisely time-resolved
sensor. In particular for non-line-of-sight (NLOS) scenarios, transients of
hidden objects are measured beyond the sensor's direct line of sight. Using
NLOS transients, the majority of previous works optimize volume density or
surfaces to reconstruct the hidden objects and do not transfer priors learned
from datasets. In this work, we present a masked autoencoder for modeling
transient imaging, or MARMOT, to facilitate NLOS applications. Our MARMOT is a
self-supervised model pretrianed on massive and diverse NLOS transient
datasets. Using a Transformer-based encoder-decoder, MARMOT learns features
from partially masked transients via a scanning pattern mask (SPM), where the
unmasked subset is functionally equivalent to arbitrary sampling, and predicts
full measurements. Pretrained on TransVerse-a synthesized transient dataset of
500K 3D models-MARMOT adapts to downstream imaging tasks using direct feature
transfer or decoder finetuning. Comprehensive experiments are carried out in
comparisons with state-of-the-art methods. Quantitative and qualitative results
demonstrate the efficiency of our MARMOT.

</details>


### [30] [Context-aware TFL: A Universal Context-aware Contrastive Learning Framework for Temporal Forgery Localization](https://arxiv.org/abs/2506.08493)
*Qilin Yin,Wei Lu,Xiangyang Luo,Xiaochun Cao*

Main category: cs.CV

TL;DR: 论文提出了一种通用上下文感知对比学习框架（UniCaCLF），用于解决多媒体取证领域中部分视频片段篡改的定位问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注伪造音视频内容的检测，但忽略了部分片段篡改的情况，而实际应用中更需定位篡改的时间段。

Method: 采用监督对比学习，结合上下文感知感知层和自适应上下文更新器，通过对比真实与伪造片段特征的距离来定位篡改。

Result: 在五个公开数据集上的实验表明，UniCaCLF显著优于现有算法。

Conclusion: UniCaCLF通过上下文感知对比学习，有效提升了时间伪造定位的性能。

Abstract: Most research efforts in the multimedia forensics domain have focused on
detecting forgery audio-visual content and reached sound achievements. However,
these works only consider deepfake detection as a classification task and
ignore the case where partial segments of the video are tampered with. Temporal
forgery localization (TFL) of small fake audio-visual clips embedded in real
videos is still challenging and more in line with realistic application
scenarios. To resolve this issue, we propose a universal context-aware
contrastive learning framework (UniCaCLF) for TFL. Our approach leverages
supervised contrastive learning to discover and identify forged instants by
means of anomaly detection, allowing for the precise localization of temporal
forged segments. To this end, we propose a novel context-aware perception layer
that utilizes a heterogeneous activation operation and an adaptive context
updater to construct a context-aware contrastive objective, which enhances the
discriminability of forged instant features by contrasting them with genuine
instant features in terms of their distances to the global context. An
efficient context-aware contrastive coding is introduced to further push the
limit of instant feature distinguishability between genuine and forged instants
in a supervised sample-by-sample manner, suppressing the cross-sample influence
to improve temporal forgery localization performance. Extensive experimental
results over five public datasets demonstrate that our proposed UniCaCLF
significantly outperforms the state-of-the-art competing algorithms.

</details>


### [31] [MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding](https://arxiv.org/abs/2506.08512)
*Zhiyi Zhu,Xiaoyu Wu,Zihao Liu,Linlin Yang*

Main category: cs.CV

TL;DR: MLVTG框架通过MambaAligner和LLMRefiner模块解决了视频时间定位中的冗余注意力和多模态对齐问题，实现了更精确的定位。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的方法存在冗余注意力和多模态对齐不足的问题，需要更高效的解决方案。

Method: MLVTG结合了MambaAligner（使用Vision Mamba块建模时间依赖）和LLMRefiner（利用预训练LLM的语义先验增强对齐），无需微调。

Result: 在QVHighlights、Charades-STA和TVSum数据集上，MLVTG表现优于现有基线，达到SOTA性能。

Conclusion: MLVTG通过双对齐策略显著提升了视频时间定位的精度和效率。

Abstract: Video Temporal Grounding (VTG), which aims to localize video clips
corresponding to natural language queries, is a fundamental yet challenging
task in video understanding. Existing Transformer-based methods often suffer
from redundant attention and suboptimal multi-modal alignment. To address these
limitations, we propose MLVTG, a novel framework that integrates two key
modules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba
blocks as a backbone instead of Transformers to model temporal dependencies and
extract robust video representations for multi-modal alignment. LLMRefiner
leverages the specific frozen layer of a pre-trained Large Language Model (LLM)
to implicitly transfer semantic priors, enhancing multi-modal alignment without
fine-tuning. This dual alignment strategy, temporal modeling via structured
state-space dynamics and semantic purification via textual priors, enables more
precise localization. Extensive experiments on QVHighlights, Charades-STA, and
TVSum demonstrate that MLVTG achieves state-of-the-art performance and
significantly outperforms existing baselines.

</details>


### [32] [Robust Visual Localization via Semantic-Guided Multi-Scale Transformer](https://arxiv.org/abs/2506.08526)
*Zhongtao Tian,Wenhao Huang,Zhidong Chen,Xiao Wei Sun*

Main category: cs.CV

TL;DR: 提出了一种结合多尺度特征学习和语义场景理解的框架，用于动态环境中的视觉定位，通过分层Transformer和语义监督提升性能。


<details>
  <summary>Details</summary>
Motivation: 动态环境中光照变化、天气条件和移动物体干扰视觉定位，现有绝对姿态回归方法难以保持一致性。

Method: 使用分层Transformer和跨尺度注意力融合几何细节与上下文线索，结合语义监督训练网络学习视图不变特征。

Result: 在TartanAir数据集上，该方法在动态物体、光照变化和遮挡等挑战性场景中优于现有姿态回归方法。

Conclusion: 多尺度处理与语义指导的结合为动态环境中的鲁棒视觉定位提供了有效策略。

Abstract: Visual localization remains challenging in dynamic environments where
fluctuating lighting, adverse weather, and moving objects disrupt appearance
cues. Despite advances in feature representation, current absolute pose
regression methods struggle to maintain consistency under varying conditions.
To address this challenge, we propose a framework that synergistically combines
multi-scale feature learning with semantic scene understanding. Our approach
employs a hierarchical Transformer with cross-scale attention to fuse geometric
details and contextual cues, preserving spatial precision while adapting to
environmental changes. We improve the performance of this architecture with
semantic supervision via neural scene representation during training, guiding
the network to learn view-invariant features that encode persistent structural
information while suppressing complex environmental interference. Experiments
on TartanAir demonstrate that our approach outperforms existing pose regression
methods in challenging scenarios with dynamic objects, illumination changes,
and occlusions. Our findings show that integrating multi-scale processing with
semantic guidance offers a promising strategy for robust visual localization in
real-world dynamic environments.

</details>


### [33] [LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid Temporal Modeling with Only 4$\times$RTX 4090s](https://arxiv.org/abs/2506.08529)
*Xijun Wang,Xin Li,Bingchen Li,Zhibo Chen*

Main category: cs.CV

TL;DR: LiftVSR是一种高效的视频超分辨率框架，通过结合动态时间注意力和注意力记忆缓存，显著提升了时间一致性并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在时间一致性和计算成本方面存在不足，尤其是对长视频处理时。

Method: 提出混合时间建模机制，包括动态时间注意力（DTA）和注意力记忆缓存（AMC），并结合非对称采样策略。

Result: 在多个VSR基准测试中表现出色，计算成本显著降低。

Conclusion: LiftVSR在保持高效的同时，实现了时间一致性和性能的平衡。

Abstract: Diffusion models have significantly advanced video super-resolution (VSR) by
enhancing perceptual quality, largely through elaborately designed temporal
modeling to ensure inter-frame consistency. However, existing methods usually
suffer from limited temporal coherence and prohibitively high computational
costs (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for
long videos. In this work, we propose LiftVSR, an efficient VSR framework that
leverages and elevates the image-wise diffusion prior from PixArt-$\alpha$,
achieving state-of-the-art results using only 4$\times$RTX 4090 GPUs. To
balance long-term consistency and efficiency, we introduce a hybrid temporal
modeling mechanism that decomposes temporal learning into two complementary
components: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal
modeling within short frame segment ($\textit{i.e.}$, low complexity), and (ii)
Attention Memory Cache (AMC) for long-term temporal modeling across segments
($\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token
flows across frames within multi-head query and key tokens to warp inter-frame
contexts in the value tokens. AMC adaptively aggregates historical segment
information via a cache unit, ensuring long-term coherence with minimal
overhead. To further stabilize the cache interaction during inference, we
introduce an asymmetric sampling strategy that mitigates feature mismatches
arising from different diffusion sampling steps. Extensive experiments on
several typical VSR benchmarks have demonstrated that LiftVSR achieves
impressive performance with significantly lower computational costs.

</details>


### [34] [TrajFlow: Multi-modal Motion Prediction via Flow Matching](https://arxiv.org/abs/2506.08541)
*Qi Yan,Brian Zhang,Yutong Zhang,Daniel Yang,Joshua White,Di Chen,Jiachao Liu,Langechuan Liu,Binnan Zhuang,Shaoshuai Shi,Renjie Liao*

Main category: cs.CV

TL;DR: TrajFlow是一种基于流匹配的运动预测框架，通过单次推理预测多模态轨迹，显著降低计算开销，并在WOMD数据集上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中高效准确的运动预测对安全和决策至关重要，现有生成方法在多模态预测上存在效率和扩展性问题。

Method: 提出TrajFlow框架，采用流匹配技术单次预测多轨迹；引入Plackett-Luce分布的排序损失优化不确定性估计；设计自条件训练技术加速推理。

Result: 在Waymo Open Motion Dataset上表现优异，关键指标达到最先进水平。

Conclusion: TrajFlow为安全关键型自动驾驶应用提供了高效、准确的运动预测解决方案。

Abstract: Efficient and accurate motion prediction is crucial for ensuring safety and
informed decision-making in autonomous driving, particularly under dynamic
real-world conditions that necessitate multi-modal forecasts. We introduce
TrajFlow, a novel flow matching-based motion prediction framework that
addresses the scalability and efficiency challenges of existing generative
trajectory prediction methods. Unlike conventional generative approaches that
employ i.i.d. sampling and require multiple inference passes to capture diverse
outcomes, TrajFlow predicts multiple plausible future trajectories in a single
pass, significantly reducing computational overhead while maintaining coherence
across predictions. Moreover, we propose a ranking loss based on the
Plackett-Luce distribution to improve uncertainty estimation of predicted
trajectories. Additionally, we design a self-conditioning training technique
that reuses the model's own predictions to construct noisy inputs during a
second forward pass, thereby improving generalization and accelerating
inference. Extensive experiments on the large-scale Waymo Open Motion Dataset
(WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across
various key metrics, underscoring its effectiveness for safety-critical
autonomous driving applications. The code and other details are available on
the project website https://traj-flow.github.io/.

</details>


### [35] [Convergence of Spectral Principal Paths: How Deep Networks Distill Linear Representations from Noisy Inputs](https://arxiv.org/abs/2506.08543)
*Bowei Tian,Xuntao Lyu,Meng Liu,Hongyi Wang,Ang Li*

Main category: cs.CV

TL;DR: 论文提出了一种新的假设（ISLH）和框架（SPP），用于解释深度网络中概念对齐方向的线性表示形成，并通过实验验证了其在多模态模型中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 基于线性表示假设（LRH），研究旨在揭示深度网络中概念对齐方向的来源及其在输入空间中的线性特性，以增强AI的透明性和控制性。

Method: 提出了输入空间线性假设（ISLH）和谱主路径（SPP）框架，用于理论化和实证验证深度网络中线性表示的形成过程。

Result: 实验表明，SPP框架在多模态视觉语言模型（VLMs）中具有鲁棒性，支持理论假设。

Conclusion: 研究为深度网络中表示形成的结构化理论提供了基础，有助于提升AI的鲁棒性、公平性和透明性。

Abstract: High-level representations have become a central focus in enhancing AI
transparency and control, shifting attention from individual neurons or
circuits to structured semantic directions that align with human-interpretable
concepts. Motivated by the Linear Representation Hypothesis (LRH), we propose
the Input-Space Linearity Hypothesis (ISLH), which posits that concept-aligned
directions originate in the input space and are selectively amplified with
increasing depth. We then introduce the Spectral Principal Path (SPP)
framework, which formalizes how deep networks progressively distill linear
representations along a small set of dominant spectral directions. Building on
this framework, we further demonstrate the multimodal robustness of these
representations in Vision-Language Models (VLMs). By bridging theoretical
insights with empirical validation, this work advances a structured theory of
representation formation in deep networks, paving the way for improving AI
robustness, fairness, and transparency.

</details>


### [36] [From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge](https://arxiv.org/abs/2506.08553)
*Agnese Taluzzi,Davide Gesualdi,Riccardo Santambrogio,Chiara Plizzari,Francesca Palermo,Simone Mentasti,Matteo Matteucci*

Main category: cs.CV

TL;DR: SceneNet和KnowledgeNet是用于HD-EPIC VQA Challenge 2025的方法，分别利用场景图和外部知识提升视觉问答性能，组合后达到44.21%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决复杂自我中心视觉问答任务中细粒度对象交互和高级语义推理的需求。

Method: SceneNet使用多模态大语言模型生成场景图捕捉对象交互；KnowledgeNet引入ConceptNet的外部常识知识。

Result: 在HD-EPIC基准测试的七类任务中表现优异，组合方法准确率达44.21%。

Conclusion: SceneNet和KnowledgeNet的组合有效提升复杂视觉问答任务的性能。

Abstract: This report presents SceneNet and KnowledgeNet, our approaches developed for
the HD-EPIC VQA Challenge 2025. SceneNet leverages scene graphs generated with
a multi-modal large language model (MLLM) to capture fine-grained object
interactions, spatial relationships, and temporally grounded events. In
parallel, KnowledgeNet incorporates ConceptNet's external commonsense knowledge
to introduce high-level semantic connections between entities, enabling
reasoning beyond directly observable visual evidence. Each method demonstrates
distinct strengths across the seven categories of the HD-EPIC benchmark, and
their combination within our framework results in an overall accuracy of 44.21%
on the challenge, highlighting its effectiveness for complex egocentric VQA
tasks.

</details>


### [37] [Towards Cross-Subject EMG Pattern Recognition via Dual-Branch Adversarial Feature Disentanglement](https://arxiv.org/abs/2506.08555)
*Xinyue Niu,Akira Furui*

Main category: cs.CV

TL;DR: 提出了一种通过特征解耦消除校准需求的方法，用于跨受试者肌电图（EMG）模式识别，采用双分支对抗神经网络实现。


<details>
  <summary>Details</summary>
Motivation: 跨受试者EMG模式识别因个体差异面临挑战，传统方法依赖校准数据，耗时且不实用。

Method: 提出端到端双分支对抗神经网络，将EMG特征解耦为模式特定和受试者特定成分。

Result: 模型在未见用户数据上表现稳健，优于基线方法。

Conclusion: 该方法为无需校准的跨受试者EMG识别提供了新思路，并具有更广泛的应用潜力。

Abstract: Cross-subject electromyography (EMG) pattern recognition faces significant
challenges due to inter-subject variability in muscle anatomy, electrode
placement, and signal characteristics. Traditional methods rely on
subject-specific calibration data to adapt models to new users, an approach
that is both time-consuming and impractical for large-scale, real-world
deployment. This paper presents an approach to eliminate calibration
requirements through feature disentanglement, enabling effective cross-subject
generalization. We propose an end-to-end dual-branch adversarial neural network
that simultaneously performs pattern recognition and individual identification
by disentangling EMG features into pattern-specific and subject-specific
components. The pattern-specific components facilitate robust pattern
recognition for new users without model calibration, while the subject-specific
components enable downstream applications such as task-invariant biometric
identification. Experimental results demonstrate that the proposed model
achieves robust performance on data from unseen users, outperforming various
baseline methods in cross-subject scenarios. Overall, this study offers a new
perspective for cross-subject EMG pattern recognition without model calibration
and highlights the proposed model's potential for broader applications, such as
task-independent biometric systems.

</details>


### [38] [Hierarchical Neural Collapse Detection Transformer for Class Incremental Object Detection](https://arxiv.org/abs/2506.08562)
*Duc Thanh Pham,Hong Dang Nguyen,Nhat Minh Nguyen Quoc,Linh Ngo Van,Sang Dinh Viet,Duc Anh Nguyen*

Main category: cs.CV

TL;DR: Hier-DETR是一个新型的增量目标检测框架，通过利用神经崩溃和标签的层次关系，解决了现有IOD模型的性能不足和推理时间长的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中新物体不断出现，现有增量目标检测模型性能有限且推理时间长，无法满足实际需求。

Method: 提出Hier-DETR框架，结合神经崩溃处理不平衡数据集和标签的层次关系。

Result: 框架在效率和性能上均具有竞争力。

Conclusion: Hier-DETR为增量目标检测提供了一种高效且性能优越的解决方案。

Abstract: Recently, object detection models have witnessed notable performance
improvements, particularly with transformer-based models. However, new objects
frequently appear in the real world, requiring detection models to continually
learn without suffering from catastrophic forgetting. Although Incremental
Object Detection (IOD) has emerged to address this challenge, these existing
models are still not practical due to their limited performance and prolonged
inference time. In this paper, we introduce a novel framework for IOD, called
Hier-DETR: Hierarchical Neural Collapse Detection Transformer, ensuring both
efficiency and competitive performance by leveraging Neural Collapse for
imbalance dataset and Hierarchical relation of classes' labels.

</details>


### [39] [Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations](https://arxiv.org/abs/2506.08566)
*Yibo Cui,Liang Xie,Yu Zhao,Jiawei Sun,Erwei Yin*

Main category: cs.CV

TL;DR: 论文提出FCA-NIG框架，自动生成具有细粒度跨模态注释的导航指令，构建FCA-R2R数据集，显著提升VLN智能体的导航性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏细粒度的跨模态对齐注释，影响导航决策的准确性。

Method: 通过分割轨迹、地标检测、指令生成和实体选择，生成子指令-轨迹对，并聚合为完整指令-轨迹对。

Result: FCA-R2R数据集显著提升了多个VLN智能体的性能，增强了状态感知和导航准确性。

Conclusion: FCA-NIG框架无需人工标注即可生成高质量训练数据，推动了复杂导航任务中的细粒度跨模态学习。

Abstract: Vision-Language Navigation (VLN) enables intelligent agents to navigate
environments by integrating visual perception and natural language
instructions, yet faces significant challenges due to the scarcity of
fine-grained cross-modal alignment annotations. Existing datasets primarily
focus on global instruction-trajectory matching, neglecting
sub-instruction-level and entity-level alignments critical for accurate
navigation action decision-making. To address this limitation, we propose
FCA-NIG, a generative framework that automatically constructs navigation
instructions with dual-level fine-grained cross-modal annotations. In this
framework, an augmented trajectory is first divided into sub-trajectories,
which are then processed through GLIP-based landmark detection, crafted
instruction construction, OFA-Speaker based R2R-like instruction generation,
and CLIP-powered entity selection, generating sub-instruction-trajectory pairs
with entity-landmark annotations. Finally, these sub-pairs are aggregated to
form a complete instruction-trajectory pair. The framework generates the
FCA-R2R dataset, the first large-scale augmentation dataset featuring precise
sub-instruction-sub-trajectory and entity-landmark alignments. Extensive
experiments demonstrate that training with FCA-R2R significantly improves the
performance of multiple state-of-the-art VLN agents, including SF, EnvDrop,
RecBERT, and HAMT. Incorporating sub-instruction-trajectory alignment enhances
agents' state awareness and decision accuracy, while entity-landmark alignment
further boosts navigation performance and generalization. These results
highlight the effectiveness of FCA-NIG in generating high-quality, scalable
training data without manual annotation, advancing fine-grained cross-modal
learning in complex navigation tasks.

</details>


### [40] [Diversity-Guided MLP Reduction for Efficient Large Vision Transformers](https://arxiv.org/abs/2506.08591)
*Chengchao Shen,Hourun Zhu,Gongfan Fang,Jianxin Wang,Xinchao Wang*

Main category: cs.CV

TL;DR: 提出了一种多样性引导的MLP压缩方法（DGMR），显著减少大型视觉Transformer的参数和计算量，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大规模Transformer模型的参数和计算成本高昂，尤其是MLP模块占用了大部分参数。

Method: 采用Gram-Schmidt权重剪枝策略去除MLP隐藏层的冗余神经元，同时保留权重多样性以提升蒸馏性能。

Result: 在多个大型视觉Transformer上，参数和FLOPs减少超过57%，性能损失极小；EVA-CLIP-E（4.4B）参数和FLOPs减少71.5%且无性能下降。

Conclusion: DGMR方法高效压缩模型，显著降低成本，适用于大规模视觉Transformer。

Abstract: Transformer models achieve excellent scaling property, where the performance
is improved with the increment of model capacity. However, large-scale model
parameters lead to an unaffordable cost of computing and memory. We analyze
popular transformer architectures and find that multilayer perceptron (MLP)
modules take up the majority of model parameters. To this end, we focus on the
recoverability of the compressed models and propose a Diversity-Guided MLP
Reduction (DGMR) method to significantly reduce the parameters of large vision
transformers with only negligible performance degradation. Specifically, we
conduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons
of MLP hidden layer, while preserving weight diversity for better performance
recover during distillation. Compared to the model trained from scratch, our
pruned model only requires 0.06\% data of LAION-2B (for the training of large
vision transformers) without labels (ImageNet-1K) to recover the original
performance. Experimental results on several state-of-the-art large vision
transformers demonstrate that our method achieves a more than 57.0\% parameter
and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),
our method accomplishes a 71.5\% parameter and FLOPs reduction without
performance degradation. The source code and trained weights are available at
https://github.com/visresearch/DGMR.

</details>


### [41] [Transformers Meet Hyperspectral Imaging: A Comprehensive Study of Models, Challenges and Open Problems](https://arxiv.org/abs/2506.08596)
*Guyang Zhang,Waleed Abdulla*

Main category: cs.CV

TL;DR: 本文是第一篇专注于基于Transformer的高光谱图像（HSI）分类的端到端综述，总结了300多篇论文，分析了各阶段的设计选择，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: Transformer在长距离依赖学习中表现出色，但在HSI领域的应用仍处于起步阶段，需要系统性的综述和指导。

Method: 通过文献综述，分类分析了HSI分类流程中的各个环节（如预处理、特征提取、自注意力机制等），并对比了不同设计选择。

Result: 总结了HSI领域面临的挑战（如数据稀缺、计算开销大等），并提出了未来的研究议程。

Conclusion: 本文旨在帮助研究者选择或扩展适合HSI应用的Transformer组件，推动下一代HSI技术的发展。

Abstract: Transformers have become the architecture of choice for learning long-range
dependencies, yet their adoption in hyperspectral imaging (HSI) is still
emerging. We reviewed more than 300 papers published up to 2025 and present the
first end-to-end survey dedicated to Transformer-based HSI classification. The
study categorizes every stage of a typical pipeline-pre-processing, patch or
pixel tokenization, positional encoding, spatial-spectral feature extraction,
multi-head self-attention variants, skip connections, and loss design-and
contrasts alternative design choices with the unique spatial-spectral
properties of HSI. We map the field's progress against persistent obstacles:
scarce labeled data, extreme spectral dimensionality, computational overhead,
and limited model explainability. Finally, we outline a research agenda
prioritizing valuable public data sets, lightweight on-edge models,
illumination and sensor shifts robustness, and intrinsically interpretable
attention mechanisms. Our goal is to guide researchers in selecting, combining,
or extending Transformer components that are truly fit for purpose for
next-generation HSI applications.

</details>


### [42] [Towards Class-wise Fair Adversarial Training via Anti-Bias Soft Label Distillation](https://arxiv.org/abs/2506.08611)
*Shiji Zhao,Chi Chen,Ranjie Duan,Xizhe Wang,Xingxing Wei*

Main category: cs.CV

TL;DR: 本文探讨了对抗训练（AT）和对抗鲁棒性蒸馏（ARD）中的鲁棒公平性问题，并提出了一种名为ABSLD的方法，通过调整软标签的平滑度来提升公平性。


<details>
  <summary>Details</summary>
Motivation: AT和ARD在增强模型鲁棒性时存在公平性问题，即对不同类别的鲁棒性表现不均。本文旨在解决这一问题。

Method: 提出了ABSLD方法，通过为不同类别分配不同温度值来调整软标签的平滑度，从而减少学生模型在不同类别间的误差风险差距。

Result: 实验表明，ABSLD在鲁棒性和公平性方面优于现有方法。

Conclusion: ABSLD是一种有效提升对抗鲁棒公平性的方法，且易于与其他方法结合。

Abstract: Adversarial Training (AT) is widely recognized as an effective approach to
enhance the adversarial robustness of Deep Neural Networks. As a variant of AT,
Adversarial Robustness Distillation (ARD) has shown outstanding performance in
enhancing the robustness of small models. However, both AT and ARD face robust
fairness issue: these models tend to display strong adversarial robustness
against some classes (easy classes) while demonstrating weak adversarial
robustness against others (hard classes). This paper explores the underlying
factors of this problem and points out the smoothness degree of soft labels for
different classes significantly impacts the robust fairness from both empirical
observation and theoretical analysis. Based on the above exploration, we
propose Anti-Bias Soft Label Distillation (ABSLD) within the Knowledge
Distillation framework to enhance the adversarial robust fairness.
Specifically, ABSLD adaptively reduces the student's error risk gap between
different classes, which is accomplished by adjusting the class-wise smoothness
degree of teacher's soft labels during the training process, and the adjustment
is managed by assigning varying temperatures to different classes.
Additionally, as a label-based approach, ABSLD is highly adaptable and can be
integrated with the sample-based methods. Extensive experiments demonstrate
ABSLD outperforms state-of-the-art methods on the comprehensive performance of
robustness and fairness.

</details>


### [43] [Data-Efficient Challenges in Visual Inductive Priors: A Retrospective](https://arxiv.org/abs/2506.08612)
*Robert-Jan Bruintjes,Attila Lengyel,Osman Semih Kayhan,Davide Zambrano,Nergis Tömen,Hadi Jamali-Rad,Jan van Gemert*

Main category: cs.CV

TL;DR: 论文探讨了在数据不足情况下提升深度学习模型性能的方法，通过组织数据受限挑战赛，发现模型集成和数据增强是有效策略。


<details>
  <summary>Details</summary>
Motivation: 研究在数据不足情况下如何通过先验知识提升深度学习模型的性能，以解决计算机视觉任务中的数据效率问题。

Method: 组织数据受限挑战赛，限制参与者使用少量样本从头训练模型，禁止迁移学习，鼓励结合先验知识的方法。

Result: 成功参赛方案采用大型模型集成（Transformer与CNN结合）和强数据增强，部分方案通过先验知识提升了性能。

Conclusion: 在数据不足情况下，模型集成、数据增强和先验知识是提升深度学习性能的有效途径。

Abstract: Deep Learning requires large amounts of data to train models that work well.
In data-deficient settings, performance can be degraded. We investigate which
Deep Learning methods benefit training models in a data-deficient setting, by
organizing the "VIPriors: Visual Inductive Priors for Data-Efficient Deep
Learning" workshop series, featuring four editions of data-impaired challenges.
These challenges address the problem of training deep learning models for
computer vision tasks with limited data. Participants are limited to training
models from scratch using a low number of training samples and are not allowed
to use any form of transfer learning. We aim to stimulate the development of
novel approaches that incorporate prior knowledge to improve the data
efficiency of deep learning models. Successful challenge entries make use of
large model ensembles that mix Transformers and CNNs, as well as heavy data
augmentation. Novel prior knowledge-based methods contribute to success in some
entries.

</details>


### [44] [SAMSelect: A Spectral Index Search for Marine Debris Visualization using Segment Anything](https://arxiv.org/abs/2506.08613)
*Joost van Dalen,Yuki M. Asano,Marc Russwurm*

Main category: cs.CV

TL;DR: SAMSelect算法通过Segment Anything Model选择最佳三通道可视化组合，帮助海洋科学家更直观地解读Sentinel-2影像中的海洋垃圾。


<details>
  <summary>Details</summary>
Motivation: 海洋垃圾在中等分辨率影像中因成分异质性难以可视化，而专家通常依赖经验和启发式方法选择波段组合。

Method: SAMSelect利用小规模标注数据集，通过Segment Anything Model选择分类准确率最高的波段或指数组合。

Result: 在加纳阿克拉和南非德班的Sentinel-2影像中，SAMSelect发现了新的波段组合（如B8和B2的归一化差异指数），性能优于文献中的指数。

Conclusion: SAMSelect为海洋科学家的视觉解译提供了有效工具，并开源了代码库。

Abstract: This work proposes SAMSelect, an algorithm to obtain a salient three-channel
visualization for multispectral images. We develop SAMSelect and show its use
for marine scientists visually interpreting floating marine debris in
Sentinel-2 imagery. These debris are notoriously difficult to visualize due to
their compositional heterogeneity in medium-resolution imagery. Out of these
difficulties, a visual interpretation of imagery showing marine debris remains
a common practice by domain experts, who select bands and spectral indices on a
case-by-case basis informed by common practices and heuristics. SAMSelect
selects the band or index combination that achieves the best classification
accuracy on a small annotated dataset through the Segment Anything Model. Its
central assumption is that the three-channel visualization achieves the most
accurate segmentation results also provide good visual information for
photo-interpretation.
  We evaluate SAMSelect in three Sentinel-2 scenes containing generic marine
debris in Accra, Ghana, and Durban, South Africa, and deployed plastic targets
from the Plastic Litter Project. This reveals the potential of new previously
unused band combinations (e.g., a normalized difference index of B8, B2), which
demonstrate improved performance compared to literature-based indices. We
describe the algorithm in this paper and provide an open-source code repository
that will be helpful for domain scientists doing visual photo interpretation,
especially in the marine field.

</details>


### [45] [A Probability-guided Sampler for Neural Implicit Surface Rendering](https://arxiv.org/abs/2506.08619)
*Gonçalo Dias Pais,Valter Piedade,Moitreya Chatterjee,Marcus Greiff,Pedro Miraldo*

Main category: cs.CV

TL;DR: 该论文提出了一种改进的NeRF方法，通过隐式表面表示和3D图像投影空间的概率密度函数，实现更有针对性的采样，并结合新的表面重建损失函数，提升了3D重建和图像渲染的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法因可扩展性问题无法对所有可能的输入数据进行训练，导致采样效率低下。本文旨在通过改进采样策略和损失函数，提升渲染和重建的精度。

Method: 利用隐式表面表示和3D图像投影空间的概率密度函数，实现目标区域的针对性采样；提出新的表面重建损失函数，结合近表面和空空间信息。

Result: 通过集成新采样策略和损失函数，显著提升了3D重建和图像渲染的准确性，尤其是在目标区域。

Conclusion: 本文的方法通过优化采样和损失函数，显著提升了NeRF的性能，为3D场景重建和渲染提供了更高效的解决方案。

Abstract: Several variants of Neural Radiance Fields (NeRFs) have significantly
improved the accuracy of synthesized images and surface reconstruction of 3D
scenes/objects. In all of these methods, a key characteristic is that none can
train the neural network with every possible input data, specifically, every
pixel and potential 3D point along the projection rays due to scalability
issues. While vanilla NeRFs uniformly sample both the image pixels and 3D
points along the projection rays, some variants focus only on guiding the
sampling of the 3D points along the projection rays. In this paper, we leverage
the implicit surface representation of the foreground scene and model a
probability density function in a 3D image projection space to achieve a more
targeted sampling of the rays toward regions of interest, resulting in improved
rendering. Additionally, a new surface reconstruction loss is proposed for
improved performance. This new loss fully explores the proposed 3D image
projection space model and incorporates near-to-surface and empty space
components. By integrating our novel sampling strategy and novel loss into
current state-of-the-art neural implicit surface renderers, we achieve more
accurate and detailed 3D reconstructions and improved image rendering,
especially for the regions of interest in any given scene.

</details>


### [46] [ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network](https://arxiv.org/abs/2506.08629)
*Feixiang Du,Shengkun Wu*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级的CNN-Mamba网络（ECMNet）用于语义分割，通过结合CNN和Mamba的优势，并设计了多个模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管CNN和Transformer在语义分割中表现优异，但全局上下文建模仍不足。Mamba在视觉任务中展现出长距离依赖建模的潜力，因此结合两者以弥补各自的不足。

Method: ECMNet在胶囊框架中巧妙结合CNN和Mamba，设计了增强双注意力块（EDAB）、多尺度注意力单元（MSAU）和Mamba增强的特征融合模块（FFM）。

Result: 在Cityscapes和CamVid数据集上分别达到70.6%和73.6%的mIoU，参数仅0.87M，计算量为8.27G FLOPs。

Conclusion: ECMNet在准确性和效率上取得了平衡，验证了CNN与Mamba结合的有效性。

Abstract: In the past decade, Convolutional Neural Networks (CNNs) and Transformers
have achieved wide applicaiton in semantic segmentation tasks. Although CNNs
with Transformer models greatly improve performance, the global context
modeling remains inadequate. Recently, Mamba achieved great potential in vision
tasks, showing its advantages in modeling long-range dependency. In this paper,
we propose a lightweight Efficient CNN-Mamba Network for semantic segmentation,
dubbed as ECMNet. ECMNet combines CNN with Mamba skillfully in a capsule-based
framework to address their complementary weaknesses. Specifically, We design a
Enhanced Dual-Attention Block (EDAB) for lightweight bottleneck. In order to
improve the representations ability of feature, We devise a Multi-Scale
Attention Unit (MSAU) to integrate multi-scale feature aggregation, spatial
aggregation and channel aggregation. Moreover, a Mamba enhanced Feature Fusion
Module (FFM) merges diverse level feature, significantly enhancing segmented
accuracy. Extensive experiments on two representative datasets demonstrate that
the proposed model excels in accuracy and efficiency balance, achieving 70.6%
mIoU on Cityscapes and 73.6% mIoU on CamVid test datasets, with 0.87M
parameters and 8.27G FLOPs on a single RTX 3090 GPU platform.

</details>


### [47] [RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping](https://arxiv.org/abs/2506.08632)
*Yang Bai,Liudi Yang,George Eskandar,Fengyi Shen,Dong Chen,Mohammad Altillawi,Ziyuan Liu,Gitta Kutyniok*

Main category: cs.CV

TL;DR: RoboSwap是一种新型视频编辑框架，结合GAN和扩散模型，用于在未配对数据中交换机器人手臂，提升跨平台机器人学习的数据生成能力。


<details>
  <summary>Details</summary>
Motivation: 解决视频条件下机器人学习中数据稀缺和跨平台泛化问题，特别是机器人手臂交换的挑战。

Method: 通过分割机器人手臂，使用未配对的GAN模型进行翻译，再结合扩散模型增强视频的连贯性和运动真实性。

Result: 在三个基准测试中，RoboSwap在结构连贯性和运动一致性上优于现有视频和图像编辑模型。

Conclusion: RoboSwap为机器人学习提供了可靠的跨平台数据生成解决方案。

Abstract: Recent advancements in generative models have revolutionized video synthesis
and editing. However, the scarcity of diverse, high-quality datasets continues
to hinder video-conditioned robotic learning, limiting cross-platform
generalization. In this work, we address the challenge of swapping a robotic
arm in one video with another: a key step for crossembodiment learning. Unlike
previous methods that depend on paired video demonstrations in the same
environmental settings, our proposed framework, RoboSwap, operates on unpaired
data from diverse environments, alleviating the data collection needs. RoboSwap
introduces a novel video editing pipeline integrating both GANs and diffusion
models, combining their isolated advantages. Specifically, we segment robotic
arms from their backgrounds and train an unpaired GAN model to translate one
robotic arm to another. The translated arm is blended with the original video
background and refined with a diffusion model to enhance coherence, motion
realism and object interaction. The GAN and diffusion stages are trained
independently. Our experiments demonstrate that RoboSwap outperforms
state-of-the-art video and image editing models on three benchmarks in terms of
both structural coherence and motion consistency, thereby offering a robust
solution for generating reliable, cross-embodiment data in robotic learning.

</details>


### [48] [SurfR: Surface Reconstruction with Multi-scale Attention](https://arxiv.org/abs/2506.08635)
*Siddhant Ranade,Gonçalo Dias Pais,Ross Tyler Whitaker,Jacinto C. Nascimento,Pedro Miraldo,Srikumar Ramalingam*

Main category: cs.CV

TL;DR: 提出了一种基于隐式表示的快速准确的无组织点云表面重建算法，通过三种关键贡献实现了最佳精度与速度的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有学习方法要么需要针对单个对象训练的小模型（细节丰富但泛化性差），要么是泛化性强的大模型（细节不足且推理慢），需要一种兼顾速度与精度的新方法。

Method: 1. 延迟查询（lazy query）以加速重建；2. 并行多尺度网格表示以应对噪声和分辨率变化；3. 跨尺度注意力机制提升重建效果。

Result: 新方法在最佳分辨率下比基线更快，性能仅略低于最先进方法，实现了最佳精度与速度的权衡。

Conclusion: 提出的隐式表示方法在速度和精度上均优于现有方法，适用于通用3D形状重建。

Abstract: We propose a fast and accurate surface reconstruction algorithm for
unorganized point clouds using an implicit representation. Recent learning
methods are either single-object representations with small neural models that
allow for high surface details but require per-object training or generalized
representations that require larger models and generalize to newer shapes but
lack details, and inference is slow. We propose a new implicit representation
for general 3D shapes that is faster than all the baselines at their optimum
resolution, with only a marginal loss in performance compared to the
state-of-the-art. We achieve the best accuracy-speed trade-off using three key
contributions. Many implicit methods extract features from the point cloud to
classify whether a query point is inside or outside the object. First, to speed
up the reconstruction, we show that this feature extraction does not need to
use the query point at an early stage (lazy query). Second, we use a parallel
multi-scale grid representation to develop robust features for different noise
levels and input resolutions. Finally, we show that attention across scales can
provide improved reconstruction results.

</details>


### [49] [Orientation Matters: Making 3D Generative Models Orientation-Aligned](https://arxiv.org/abs/2506.08640)
*Yichong Lu,Yuzhuo Tian,Zijin Jiang,Yikun Zhao,Yuanbo Yang,Hao Ouyang,Haoji Hu,Huimin Yu,Yujun Shen,Yiyi Liao*

Main category: cs.CV

TL;DR: 论文提出了一种解决3D生成模型方向不一致问题的方法，通过构建方向对齐的数据集Objaverse-OA，并微调两种3D生成模型，实现了跨类别的一致性生成。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型因训练数据方向不一致导致结果不齐，限制了其在下游任务中的应用。

Method: 构建Objaverse-OA数据集（14,832个对齐3D模型），并基于多视角扩散和3D变分自编码器框架微调生成模型。

Result: 实验表明该方法优于后处理对齐方法，并展示了在零样本方向估计和高效旋转操作等下游任务中的应用。

Conclusion: 方向对齐的3D生成方法显著提升了模型的一致性和实用性。

Abstract: Humans intuitively perceive object shape and orientation from a single image,
guided by strong priors about canonical poses. However, existing 3D generative
models often produce misaligned results due to inconsistent training data,
limiting their usability in downstream tasks. To address this gap, we introduce
the task of orientation-aligned 3D object generation: producing 3D objects from
single images with consistent orientations across categories. To facilitate
this, we construct Objaverse-OA, a dataset of 14,832 orientation-aligned 3D
models spanning 1,008 categories. Leveraging Objaverse-OA, we fine-tune two
representative 3D generative models based on multi-view diffusion and 3D
variational autoencoder frameworks to produce aligned objects that generalize
well to unseen objects across various categories. Experimental results
demonstrate the superiority of our method over post-hoc alignment approaches.
Furthermore, we showcase downstream applications enabled by our aligned object
generation, including zero-shot object orientation estimation via
analysis-by-synthesis and efficient arrow-based object rotation manipulation.

</details>


### [50] [Enhancing Video Memorability Prediction with Text-Motion Cross-modal Contrastive Loss and Its Application in Video Summarization](https://arxiv.org/abs/2506.08649)
*Zhiyi Zhu,Xiaoyu Wu,Youwei Lu*

Main category: cs.CV

TL;DR: 论文提出了一种名为TMCCL的多模态视频记忆性预测模型，通过利用文本描述相似性改进运动特征表示，并在两个数据集上取得最佳性能。同时，提出MWCVS方法，将视频记忆性预测应用于视频摘要以减少主观性。


<details>
  <summary>Details</summary>
Motivation: 现有模型在预测视频记忆性时未能充分利用运动线索，且运动特征表示在微调阶段因缺乏标注数据而受损。

Method: 引入TMCCL模型，利用文本描述相似性构建正负运动样本集，改进运动特征表示；提出MWCVS方法，将记忆性预测应用于视频摘要。

Result: TMCCL在两个视频记忆性预测数据集上达到最佳性能；MWCVS在两个视频摘要数据集上验证了其有效性。

Conclusion: TMCCL显著提升了运动特征表示和记忆性预测准确性；MWCVS展示了视频记忆性预测的潜在应用价值。

Abstract: Video memorability refers to the ability of videos to be recalled after
viewing, playing a crucial role in creating content that remains memorable.
Existing models typically focus on extracting multimodal features to predict
video memorability scores but often fail to fully utilize motion cues. The
representation of motion features is compromised during the fine-tuning phase
of the motion feature extractor due to a lack of labeled data. In this paper,
we introduce the Text-Motion Cross-modal Contrastive Loss (TMCCL), a multimodal
video memorability prediction model designed to enhance the representation of
motion features. We tackle the challenge of improving motion feature
representation by leveraging text description similarities across videos to
establish positive and negative motion sample sets for a given target. This
enhancement allows the model to learn similar feature representations for
semantically related motion content, resulting in more accurate memorability
predictions. Our model achieves state-of-the-art performance on two video
memorability prediction datasets. Moreover, the potential applications of video
memorability prediction have been underexplored. To address this gap, we
present Memorability Weighted Correction for Video Summarization (MWCVS), using
video memorability prediction to reduce subjectivity in video summarization
labels. Experimental results on two video summarization datasets demonstrate
the effectiveness of MWCVS, showcasing the promising applications of video
memorability prediction.

</details>


### [51] [Beyond Calibration: Physically Informed Learning for Raw-to-Raw Mapping](https://arxiv.org/abs/2506.08650)
*Peter Grönquist,Stepan Tulyakov,Dengxin Dai*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级的神经物理模型（NPM），用于解决多相机间颜色一致性问题，适应性强且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 多相机系统中颜色一致性对图像融合和ISP兼容性至关重要，但现有方法在适应性和计算成本上存在不足。

Method: NPM通过模拟特定光照下的原始图像，估计设备间的转换关系，支持物理测量初始化，并可无配对数据训练。

Result: 在NUS和BeyondRGB数据集上，NPM优于现有方法，实现了跨传感器和光学系统的鲁棒颜色一致性。

Conclusion: NPM为多相机颜色一致性提供了一种高效且适应性强的解决方案。

Abstract: Achieving consistent color reproduction across multiple cameras is essential
for seamless image fusion and Image Processing Pipeline (ISP) compatibility in
modern devices, but it is a challenging task due to variations in sensors and
optics. Existing raw-to-raw conversion methods face limitations such as poor
adaptability to changing illumination, high computational costs, or impractical
requirements such as simultaneous camera operation and overlapping
fields-of-view. We introduce the Neural Physical Model (NPM), a lightweight,
physically-informed approach that simulates raw images under specified
illumination to estimate transformations between devices. The NPM effectively
adapts to varying illumination conditions, can be initialized with physical
measurements, and supports training with or without paired data. Experiments on
public datasets like NUS and BeyondRGB demonstrate that NPM outperforms recent
state-of-the-art methods, providing robust chromatic consistency across
different sensors and optical systems.

</details>


### [52] [LLaVA-c: Continual Improved Visual Instruction Tuning](https://arxiv.org/abs/2506.08666)
*Wenzhuo Liu,Fei Zhu,Haiyang Guo,Longhui Wei,Cheng-Lin Liu*

Main category: cs.CV

TL;DR: LLaVA-c通过光谱感知巩固和无监督查询正则化改进LLaVA-1.5，解决了多任务学习中的任务平衡和基础模型退化问题，在持续学习中表现优异。


<details>
  <summary>Details</summary>
Motivation: 多任务学习存在任务平衡和扩展成本问题，持续学习虽能增量获取知识但易导致基础模型退化。

Method: 在LLaVA-1.5基础上引入光谱感知巩固和无监督查询正则化。

Result: LLaVA-c在持续学习中表现优异，任务性能匹配或超越多任务联合学习。

Conclusion: LLaVA-c为持续学习提供了一种简单有效的方法，同时保持通用能力。

Abstract: Multimodal models like LLaVA-1.5 achieve state-of-the-art visual
understanding through visual instruction tuning on multitask datasets, enabling
strong instruction-following and multimodal performance. However, multitask
learning faces challenges such as task balancing, requiring careful adjustment
of data proportions, and expansion costs, where new tasks risk catastrophic
forgetting and need costly retraining. Continual learning provides a promising
alternative to acquiring new knowledge incrementally while preserving existing
capabilities. However, current methods prioritize task-specific performance,
neglecting base model degradation from overfitting to specific instructions,
which undermines general capabilities. In this work, we propose a simple but
effective method with two modifications on LLaVA-1.5: spectral-aware
consolidation for improved task balance and unsupervised inquiry regularization
to prevent base model degradation. We evaluate both general and task-specific
performance across continual pretraining and fine-tuning. Experiments
demonstrate that LLaVA-c consistently enhances standard benchmark performance
and preserves general capabilities. For the first time, we show that
task-by-task continual learning can achieve results that match or surpass
multitask joint learning. The code will be publicly released.

</details>


### [53] [ATAS: Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction](https://arxiv.org/abs/2506.08678)
*Juan Yeo,Soonwoo Cha,Jiwoo Song,Hyunbin Jin,Taesup Kim*

Main category: cs.CV

TL;DR: 论文提出了一种名为ATAS的新方法，通过自蒸馏技术提升CLIP模型在细粒度和语义一致性上的表现，无需额外模块或有监督微调。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在细粒度、区域级理解上表现不足，影响了其在密集预测任务中的效果。现有方法通常以牺牲语义一致性为代价提升细粒度对齐。

Method: 提出了Any-to-Any Self-Distillation (ATAS)，利用模型自身知识在所有表示层次上同时增强语义一致性和细粒度对齐，仅需无标注图像和内部自蒸馏过程。

Result: 在开放词汇目标检测和语义分割任务中，ATAS显著优于基线CLIP模型。

Conclusion: ATAS验证了同时保持语义一致性和细粒度对齐对提升开放词汇密集预测任务的重要性。

Abstract: Vision-language models such as CLIP have recently propelled open-vocabulary
dense prediction tasks by enabling recognition of a broad range of visual
concepts. However, CLIP still struggles with fine-grained, region-level
understanding, hindering its effectiveness on these dense prediction tasks. We
identify two pivotal factors required to address this limitation: semantic
coherence and fine-grained vision-language alignment. Current adaptation
methods often improve fine-grained alignment at the expense of semantic
coherence, and often rely on extra modules or supervised fine-tuning. To
overcome these issues, we propose Any-to-Any Self-Distillation (ATAS), a novel
approach that simultaneously enhances semantic coherence and fine-grained
alignment by leveraging own knowledge of a model across all representation
levels. Unlike prior methods, ATAS uses only unlabeled images and an internal
self-distillation process to refine representations of CLIP vision encoders,
preserving local semantic consistency while sharpening local detail
recognition. On open-vocabulary object detection and semantic segmentation
benchmarks, ATAS achieves substantial performance gains, outperforming baseline
CLIP models. These results validate the effectiveness of our approach and
underscore the importance of jointly maintaining semantic coherence and
fine-grained alignment for advanced open-vocabulary dense prediction.

</details>


### [54] [CanadaFireSat: Toward high-resolution wildfire forecasting with multiple modalities](https://arxiv.org/abs/2506.08690)
*Hugo Porta,Emanuele Dalsasso,Jessica L. McCarty,Devis Tuia*

Main category: cs.CV

TL;DR: 加拿大2023年经历了严重的野火季节，研究提出高分辨率野火预测模型CanadaFireSat，结合多模态数据提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致野火季节延长和加剧，需改进野火管理工具，高分辨率预测是关键。

Method: 利用Sentinel-2、MODIS和ERA5等多模态数据，开发深度学习模型进行100米分辨率野火预测。

Result: 多模态输入优于单模态，F1分数达60.3%，适用于未见过的2023年野火季节。

Conclusion: 多模态深度学习模型在高分辨率和大陆尺度野火预测中具有潜力。

Abstract: Canada experienced in 2023 one of the most severe wildfire seasons in recent
history, causing damage across ecosystems, destroying communities, and emitting
large quantities of CO2. This extreme wildfire season is symptomatic of a
climate-change-induced increase in the length and severity of the fire season
that affects the boreal ecosystem. Therefore, it is critical to empower
wildfire management in boreal communities with better mitigation solutions.
Wildfire probability maps represent an important tool for understanding the
likelihood of wildfire occurrence and the potential severity of future
wildfires. The massive increase in the availability of Earth observation data
has enabled the development of deep learning-based wildfire forecasting models,
aiming at providing precise wildfire probability maps at different spatial and
temporal scales. A main limitation of such methods is their reliance on
coarse-resolution environmental drivers and satellite products, leading to
wildfire occurrence prediction of reduced resolution, typically around $\sim
0.1${\deg}. This paper presents a benchmark dataset: CanadaFireSat, and
baseline methods for high-resolution: 100 m wildfire forecasting across Canada,
leveraging multi-modal data from high-resolution multi-spectral satellite
images (Sentinel-2 L1C), mid-resolution satellite products (MODIS), and
environmental factors (ERA5 reanalysis data). Our experiments consider two
major deep learning architectures. We observe that using multi-modal temporal
inputs outperforms single-modal temporal inputs across all metrics, achieving a
peak performance of 60.3% in F1 score for the 2023 wildfire season, a season
never seen during model training. This demonstrates the potential of
multi-modal deep learning models for wildfire forecasting at high-resolution
and continental scale.

</details>


### [55] [VReST: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism](https://arxiv.org/abs/2506.08691)
*Congzhi Zhang,Jiawei Peng,Zhenglin Wang,Yilong Lai,Haowen Sun,Heng Chang,Fei Ma,Weijiang Yu*

Main category: cs.CV

TL;DR: VReST是一种无需训练的新方法，通过蒙特卡洛树搜索和自奖励机制提升大型视觉语言模型（LVLM）的复杂视觉推理能力，并在多模态数学推理任务中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多模态任务中表现优异，但在复杂视觉推理中仍有局限，尤其是使用思维链提示技术时。

Method: 提出VReST方法，通过蒙特卡洛树搜索建立推理路径树，结合自奖励机制评估推理步骤质量，无需额外模型。

Result: VReST在三个多模态数学推理基准测试中超越现有提示方法，取得最优性能。

Conclusion: VReST验证了测试时扩展规律在多模态任务中的有效性，为未来研究提供了新方向。

Abstract: Large Vision-Language Models (LVLMs) have shown exceptional performance in
multimodal tasks, but their effectiveness in complex visual reasoning is still
constrained, especially when employing Chain-of-Thought prompting techniques.
In this paper, we propose VReST, a novel training-free approach that enhances
Reasoning in LVLMs through Monte Carlo Tree Search and Self-Reward mechanisms.
VReST meticulously traverses the reasoning landscape by establishing a search
tree, where each node encapsulates a reasoning step, and each path delineates a
comprehensive reasoning sequence. Our innovative multimodal Self-Reward
mechanism assesses the quality of reasoning steps by integrating the utility of
sub-questions, answer correctness, and the relevance of vision-language clues,
all without the need for additional models. VReST surpasses current prompting
methods and secures state-of-the-art performance across three multimodal
mathematical reasoning benchmarks. Furthermore, it substantiates the efficacy
of test-time scaling laws in multimodal tasks, offering a promising direction
for future research.

</details>


### [56] [MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning](https://arxiv.org/abs/2506.08694)
*Mohammadreza Salehi,Shashanka Venkataramanan,Ioana Simion,Efstratios Gavves,Cees G. M. Snoek,Yuki M Asano*

Main category: cs.CV

TL;DR: 提出了一种基于运动引导的自监督学习框架，通过聚类密集点轨迹学习时空一致的表示，提升了动态场景中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态增强，难以处理物体变形、遮挡和相机运动，导致时间上的特征学习不一致。

Method: 利用现成的点跟踪器提取长程运动轨迹，通过动量编码器的最优传输机制优化特征聚类，并沿跟踪点传播聚类分配以确保时间一致性。

Result: 在六个图像和视频数据集及四个评估基准上，性能提升了1%至6%。

Conclusion: 通过将运动作为隐式监督信号，该方法在动态场景和遮挡挑战中表现出更强的泛化能力。

Abstract: Dense self-supervised learning has shown great promise for learning pixel-
and patch-level representations, but extending it to videos remains challenging
due to the complexity of motion dynamics. Existing approaches struggle as they
rely on static augmentations that fail under object deformations, occlusions,
and camera movement, leading to inconsistent feature learning over time. We
propose a motion-guided self-supervised learning framework that clusters dense
point tracks to learn spatiotemporally consistent representations. By
leveraging an off-the-shelf point tracker, we extract long-range motion
trajectories and optimize feature clustering through a momentum-encoder-based
optimal transport mechanism. To ensure temporal coherence, we propagate cluster
assignments along tracked points, enforcing feature consistency across views
despite viewpoint changes. Integrating motion as an implicit supervisory
signal, our method learns representations that generalize across frames,
improving robustness in dynamic scenes and challenging occlusion scenarios. By
initializing from strong image-pretrained models and leveraging video data for
training, we improve state-of-the-art by 1% to 6% on six image and video
datasets and four evaluation benchmarks. The implementation is publicly
available at our GitHub repository: https://github.com/SMSD75/MoSiC/tree/main

</details>


### [57] [ArrowPose: Segmentation, Detection, and 5 DoF Pose Estimation Network for Colorless Point Clouds](https://arxiv.org/abs/2506.08699)
*Frederik Hagelskjaer*

Main category: cs.CV

TL;DR: 本文提出了一种用于无色点云的快速检测和5自由度姿态估计网络，通过神经网络预测物体的中心和顶部点来计算姿态。


<details>
  <summary>Details</summary>
Motivation: 无色点云的姿态估计在实时应用中具有挑战性，现有方法性能有限。

Method: 使用合成数据训练神经网络，预测物体的中心和顶部点以计算5自由度姿态。

Result: 在基准数据集上表现优于所有无色方法，推理时间仅250毫秒。

Conclusion: 该方法在无色点云姿态估计中实现了最先进的性能，适用于实时应用。

Abstract: This paper presents a fast detection and 5 DoF (Degrees of Freedom) pose
estimation network for colorless point clouds. The pose estimation is
calculated from center and top points of the object, predicted by the neural
network. The network is trained on synthetic data, and tested on a benchmark
dataset, where it demonstrates state-of-the-art performance and outperforms all
colorless methods. The network is able to run inference in only 250
milliseconds making it usable in many scenarios. Project page with code at
arrowpose.github.io

</details>


### [58] [TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering](https://arxiv.org/abs/2506.08704)
*Xiaohan Zhang,Sitong Wang,Yushen Yan,Yi Yang,Mingda Xu,Qi Liu*

Main category: cs.CV

TL;DR: TraGraph-GS提出了一种基于轨迹图的空间分区方法，解决了大规模场景中高质量新视角合成的挑战，显著提升了渲染精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在大规模场景中因刚性空间分区和高斯重叠问题导致渲染效果不佳，难以适应任意相机轨迹。

Method: 采用基于图的动态空间分区方法，结合正则化约束和渐进式渲染策略，减少高斯重叠和纹理失真。

Result: 在四个空中和四个地面数据集上，PSNR平均提升1.86 dB和1.62 dB，优于现有方法。

Conclusion: TraGraph-GS通过轨迹图和优化策略，显著提升大规模场景的渲染质量和效率。

Abstract: High-quality novel view synthesis for large-scale scenes presents a
challenging dilemma in 3D computer vision. Existing methods typically partition
large scenes into multiple regions, reconstruct a 3D representation using
Gaussian splatting for each region, and eventually merge them for novel view
rendering. They can accurately render specific scenes, yet they do not
generalize effectively for two reasons: (1) rigid spatial partition techniques
struggle with arbitrary camera trajectories, and (2) the merging of regions
results in Gaussian overlap to distort texture details. To address these
challenges, we propose TraGraph-GS, leveraging a trajectory graph to enable
high-precision rendering for arbitrarily large-scale scenes. We present a
spatial partitioning method for large-scale scenes based on graphs, which
incorporates a regularization constraint to enhance the rendering of textures
and distant objects, as well as a progressive rendering strategy to mitigate
artifacts caused by Gaussian overlap. Experimental results demonstrate its
superior performance both on four aerial and four ground datasets and highlight
its remarkable efficiency: our method achieves an average improvement of 1.86
dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to
state-of-the-art approaches.

</details>


### [59] [SceneSplat++: A Large Dataset and Comprehensive Benchmark for Language Gaussian Splatting](https://arxiv.org/abs/2506.08710)
*Mengjiao Ma,Qi Ma,Yue Li,Jiahuan Cheng,Runyi Yang,Bin Ren,Nikola Popovic,Mingqiang Wei,Nicu Sebe,Luc Van Gool,Theo Gevers,Martin R. Oswald,Danda Pani Paudel*

Main category: cs.CV

TL;DR: 论文提出首个大规模基准测试，评估三种3D高斯泼溅方法在3D空间中的表现，并引入新数据集GaussianWorld-49K，证明通用化方法的优势。


<details>
  <summary>Details</summary>
Motivation: 当前方法多基于2D视图评估，缺乏对3D场景的整体理解，需系统性评估3D空间中的表现。

Method: 提出大规模基准测试，评估三类方法（基于场景优化、无优化、通用化）在1060个场景中的表现，并引入49K场景数据集。

Result: 通用化方法在放松场景限制、快速推理和分割性能上表现最优。

Conclusion: 通用化方法具有潜力，数据集和基准将公开以推动研究。

Abstract: 3D Gaussian Splatting (3DGS) serves as a highly performant and efficient
encoding of scene geometry, appearance, and semantics. Moreover, grounding
language in 3D scenes has proven to be an effective strategy for 3D scene
understanding. Current Language Gaussian Splatting line of work fall into three
main groups: (i) per-scene optimization-based, (ii) per-scene
optimization-free, and (iii) generalizable approach. However, most of them are
evaluated only on rendered 2D views of a handful of scenes and viewpoints close
to the training views, limiting ability and insight into holistic 3D
understanding. To address this gap, we propose the first large-scale benchmark
that systematically assesses these three groups of methods directly in 3D
space, evaluating on 1060 scenes across three indoor datasets and one outdoor
dataset. Benchmark results demonstrate a clear advantage of the generalizable
paradigm, particularly in relaxing the scene-specific limitation, enabling fast
feed-forward inference on novel scenes, and achieving superior segmentation
performance. We further introduce GaussianWorld-49K a carefully curated 3DGS
dataset comprising around 49K diverse indoor and outdoor scenes obtained from
multiple sources, with which we demonstrate the generalizable approach could
harness strong data priors. Our codes, benchmark, and datasets will be made
public to accelerate research in generalizable 3DGS scene understanding.

</details>


### [60] [Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces](https://arxiv.org/abs/2506.08729)
*Dieuwertje Alblas,Patryk Rygiel,Julian Suk,Kaj O. Kappe,Marieke Hofman,Christoph Brune,Kak Khee Yeung,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 论文提出了一种基于SE(3)-对称变换器模型的方法，用于预测腹主动脉瘤（AAA）的生长，通过保留血管表面的解剖结构和几何保真度，实现个性化监测策略。


<details>
  <summary>Details</summary>
Motivation: 当前AAA监测依赖于最大直径，忽略了3D形状与生长的复杂关系，可能导致监测间隔不准确。个性化生长预测可优化监测策略。

Method: 使用SE(3)-对称变换器模型，直接在血管模型表面结合局部多物理特征预测AAA生长。训练数据为24名患者的113次CTA扫描。

Result: 模型预测AAA生长的中位直径误差为1.18 mm，并能以0.93的准确率预测患者是否在两年内需手术修复。外部验证集结果也显示良好泛化能力。

Conclusion: 局部定向AAA生长预测可行，可为个性化监测策略提供支持。

Abstract: Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the
abdominal aorta. AAAs may rupture, with a survival rate of only 20\%. Current
clinical guidelines recommend elective surgical repair when the maximum AAA
diameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet
these criteria are periodically monitored, with surveillance intervals based on
the maximum AAA diameter. However, this diameter does not take into account the
complex relation between the 3D AAA shape and its growth, making standardized
intervals potentially unfit. Personalized AAA growth predictions could improve
monitoring strategies. We propose to use an SE(3)-symmetric transformer model
to predict AAA growth directly on the vascular model surface enriched with
local, multi-physical features. In contrast to other works which have
parameterized the AAA shape, this representation preserves the vascular
surface's anatomical structure and geometric fidelity. We train our model using
a longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24
AAA patients at irregularly sampled intervals. After training, our model
predicts AAA growth to the next scan moment with a median diameter error of
1.18 mm. We further demonstrate our model's utility to identify whether a
patient will become eligible for elective repair within two years (acc = 0.93).
Finally, we evaluate our model's generalization on an external validation set
consisting of 25 CTAs from 7 AAA patients from a different hospital. Our
results show that local directional AAA growth prediction from the vascular
surface is feasible and may contribute to personalized surveillance strategies.

</details>


### [61] [InceptionMamba: An Efficient Hybrid Network with Large Band Convolution and Bottleneck Mamba](https://arxiv.org/abs/2506.08735)
*Yuhang Wang,Jun Li,Zhijian Wu,Jianhua Xu*

Main category: cs.CV

TL;DR: InceptionMamba是一种新型的卷积神经网络架构，通过正交带卷积和Mamba模块解决了InceptionNeXt在空间依赖性和全局上下文建模上的不足，实现了更优的分类和下游任务性能。


<details>
  <summary>Details</summary>
Motivation: InceptionNeXt在图像分类和下游任务中表现优异，但其一维条带卷积限制了空间依赖性的捕捉和局部邻域的空间建模能力，且卷积操作的局部性约束不利于全局上下文建模。

Method: 提出InceptionMamba架构，用正交带卷积替代传统一维条带卷积，并通过瓶颈Mamba模块实现全局上下文建模，增强跨通道信息融合和扩大感受野。

Result: 在分类和多种下游任务中，InceptionMamba实现了最先进的性能，同时具有优越的参数和计算效率。

Conclusion: InceptionMamba通过改进空间建模和全局上下文建模能力，显著提升了性能，是一种高效且竞争力强的架构。

Abstract: Within the family of convolutional neural networks, InceptionNeXt has shown
excellent competitiveness in image classification and a number of downstream
tasks. Built on parallel one-dimensional strip convolutions, however, it
suffers from limited ability of capturing spatial dependencies along different
dimensions and fails to fully explore spatial modeling in local neighborhood.
Besides, inherent locality constraints of convolution operations are
detrimental to effective global context modeling. To overcome these
limitations, we propose a novel backbone architecture termed InceptionMamba in
this study. More specifically, the traditional one-dimensional strip
convolutions are replaced by orthogonal band convolutions in our InceptionMamba
to achieve cohesive spatial modeling. Furthermore, global contextual modeling
can be achieved via a bottleneck Mamba module, facilitating enhanced
cross-channel information fusion and enlarged receptive field. Extensive
evaluations on classification and various downstream tasks demonstrate that the
proposed InceptionMamba achieves state-of-the-art performance with superior
parameter and computational efficiency. The source code will be available at
https://github.com/Wake1021/InceptionMamba.

</details>


### [62] [RS-MTDF: Multi-Teacher Distillation and Fusion for Remote Sensing Semi-Supervised Semantic Segmentation](https://arxiv.org/abs/2506.08772)
*Jiayi Song,Kaiyu Li,Xiangyong Cao,Deyu Meng*

Main category: cs.CV

TL;DR: 论文提出了一种名为RS-MTDF的半监督语义分割框架，利用预训练的视觉基础模型（VFMs）作为多教师，通过特征级蒸馏和知识融合提升遥感图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 遥感图像的语义分割需要大量标注数据，但标注成本高昂。现有半监督方法在标记与未标记数据分布不匹配时泛化能力不足。

Method: RS-MTDF框架利用多个冻结的VFMs（如DINOv2和CLIP）作为教师模型，通过特征级蒸馏对齐学生模型特征，并将知识融合到解码器中。

Result: 在三个遥感数据集（ISPRS Potsdam、LoveDA和DeepGlobe）上，RS-MTDF表现优于现有方法，尤其在LoveDA上不同标注比例下均取得最佳性能。

Conclusion: 多教师VFM指导显著提升了遥感分割的泛化能力和语义理解，各模块的有效性通过消融实验验证。

Abstract: Semantic segmentation in remote sensing images is crucial for various
applications, yet its performance is heavily reliant on large-scale,
high-quality pixel-wise annotations, which are notoriously expensive and
time-consuming to acquire. Semi-supervised semantic segmentation (SSS) offers a
promising alternative to mitigate this data dependency. However, existing SSS
methods often struggle with the inherent distribution mismatch between limited
labeled data and abundant unlabeled data, leading to suboptimal generalization.
We propose that Vision Foundation Models (VFMs), pre-trained on vast and
diverse datasets, possess robust generalization capabilities that can
effectively bridge this distribution gap and provide strong semantic priors for
SSS. Inspired by this, we introduce RS-MTDF (Multi-Teacher Distillation and
Fusion), a novel framework that leverages the powerful semantic knowledge
embedded in VFMs to guide semi-supervised learning in remote sensing.
Specifically, RS-MTDF employs multiple frozen VFMs (\textit{e.g.}, DINOv2 and
CLIP) as expert teachers, utilizing feature-level distillation to align student
features with their robust representations. To further enhance discriminative
power, the distilled knowledge is seamlessly fused into the student decoder.
Extensive experiments on three challenging remote sensing datasets (ISPRS
Potsdam, LoveDA, and DeepGlobe) demonstrate that RS-MTDF consistently achieves
state-of-the-art performance. Notably, our method outperforms existing
approaches across various label ratios on LoveDA and secures the highest IoU in
the majority of semantic categories. These results underscore the efficacy of
multi-teacher VFM guidance in significantly enhancing both generalization and
semantic understanding for remote sensing segmentation. Ablation studies
further validate the contribution of each proposed module.

</details>


### [63] [Gaussian2Scene: 3D Scene Representation Learning via Self-supervised Learning with 3D Gaussian Splatting](https://arxiv.org/abs/2506.08777)
*Keyi Liu,Weidong Yang,Ben Fei,Ying He*

Main category: cs.CV

TL;DR: 论文提出了一种名为Gaussian2Scene的新型自监督学习框架，利用3D高斯泼溅（3DGS）进行点云预训练，解决了现有方法在计算和几何理解上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法依赖隐式场景表示和高内存需求，且重建目标仅适用于2D空间，难以捕捉3D几何结构。

Method: 采用两阶段训练策略：第一阶段通过双分支掩码自编码器学习2D和3D场景表示；第二阶段利用重建点云和高斯基元的几何位置进行监督学习。

Result: 在多个下游3D物体检测任务中表现优于现有预训练方法。

Conclusion: Gaussian2Scene通过3DGS提升了计算效率和几何理解能力，为3D视觉任务提供了更高效的预训练框架。

Abstract: Self-supervised learning (SSL) for point cloud pre-training has become a
cornerstone for many 3D vision tasks, enabling effective learning from
large-scale unannotated data. At the scene level, existing SSL methods often
incorporate volume rendering into the pre-training framework, using RGB-D
images as reconstruction signals to facilitate cross-modal learning. This
strategy promotes alignment between 2D and 3D modalities and enables the model
to benefit from rich visual cues in the RGB-D inputs. However, these approaches
are limited by their reliance on implicit scene representations and high memory
demands. Furthermore, since their reconstruction objectives are applied only in
2D space, they often fail to capture underlying 3D geometric structures. To
address these challenges, we propose Gaussian2Scene, a novel scene-level SSL
framework that leverages the efficiency and explicit nature of 3D Gaussian
Splatting (3DGS) for pre-training. The use of 3DGS not only alleviates the
computational burden associated with volume rendering but also supports direct
3D scene reconstruction, thereby enhancing the geometric understanding of the
backbone network. Our approach follows a progressive two-stage training
strategy. In the first stage, a dual-branch masked autoencoder learns both 2D
and 3D scene representations. In the second stage, we initialize training with
reconstructed point clouds and further supervise learning using the geometric
locations of Gaussian primitives and rendered RGB images. This process
reinforces both geometric and cross-modal learning. We demonstrate the
effectiveness of Gaussian2Scene across several downstream 3D object detection
tasks, showing consistent improvements over existing pre-training methods.

</details>


### [64] [Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models](https://arxiv.org/abs/2506.08780)
*Isaac Corley,Lakshay Sharma,Ruth Crasto*

Main category: cs.CV

TL;DR: Landsat-Bench是一套基于Landsat影像的基准测试，包含EuroSAT-L、BigEarthNet-L和LC100-L，用于评估地理空间基础模型（GFM）的性能。


<details>
  <summary>Details</summary>
Motivation: Landsat数据缺乏标准化基准，限制了基于Landsat的地理空间基础模型的发展。

Method: 通过Landsat-Bench基准测试，评估了常见架构和基于SSL4EO-L预训练的GFM模型。

Result: SSL4EO-L预训练的GFM在EuroSAT-L和BigEarthNet-L上表现优于ImageNet，准确率分别提升4%和5.1%。

Conclusion: Landsat-Bench为Landsat数据提供了标准化评估工具，SSL4EO-L预训练的GFM在下游任务中表现更优。

Abstract: The Landsat program offers over 50 years of globally consistent Earth
imagery. However, the lack of benchmarks for this data constrains progress
towards Landsat-based Geospatial Foundation Models (GFM). In this paper, we
introduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that
adapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and
LC100-L. We establish baseline and standardized evaluation methods across both
common architectures and Landsat foundation models pretrained on the SSL4EO-L
dataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract
better representations for downstream tasks in comparison to ImageNet,
including performance gains of +4% OA and +5.1% mAP on EuroSAT-L and
BigEarthNet-L.

</details>


### [65] [HomographyAD: Deep Anomaly Detection Using Self Homography Learning](https://arxiv.org/abs/2506.08784)
*Jongyub Seok,Chanjin Kang*

Main category: cs.CV

TL;DR: 提出了一种基于ImageNet预训练网络的新方法HomographyAD，用于解决现有异常检测方法在真实工业环境中性能受限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法仅适用于完全对齐的数据集，而真实工业环境中的数据集往往未对齐，因此需要一种更适应实际场景的方法。

Method: 1. 使用深度单应性估计方法对齐输入前景；2. 通过自单应性学习微调模型以学习正常样本的额外形状信息；3. 基于测试样本特征与正常特征分布的距离进行异常检测。

Result: 实验表明，该方法能显著提升现有异常检测方法的性能。

Conclusion: HomographyAD是一种适用于真实工业环境的高效异常检测方法。

Abstract: Anomaly detection (AD) is a task that distinguishes normal and abnormal data,
which is important for applying automation technologies of the manufacturing
facilities. For MVTec dataset that is a representative AD dataset for
industrial environment, many recent works have shown remarkable performances.
However, the existing anomaly detection works have a limitation of showing good
performance for fully-aligned datasets only, unlike real-world industrial
environments. To solve this limitation, we propose HomographyAD, a novel deep
anomaly detection methodology based on the ImageNet-pretrained network, which
is specially designed for actual industrial dataset. Specifically, we first
suggest input foreground alignment using the deep homography estimation method.
In addition, we fine-tune the model by self homography learning to learn
additional shape information from normal samples. Finally, we conduct anomaly
detection based on the measure of how far the feature of test sample is from
the distribution of the extracted normal features. By applying our proposed
method to various existing AD approaches, we show performance enhancement
through extensive experiments.

</details>


### [66] [Flow Diverse and Efficient: Learning Momentum Flow Matching via Stochastic Velocity Field Sampling](https://arxiv.org/abs/2506.08796)
*Zhiyuan Ma,Ruixun Liu,Sixian Liu,Jianjun Li,Bowen Zhou*

Main category: cs.CV

TL;DR: Discretized-RF通过将直线路径离散化为多个可变速度子路径，提高了生成多样性和多尺度噪声建模能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统rectified flow在多样性和多尺度噪声建模上的局限性。

Method: 提出Discretized-RF，将直线路径离散化为动量场子路径，并在速度上引入噪声。

Result: 实验表明，该方法能生成多样且高效的轨迹，并产生高质量和多样化的结果。

Conclusion: Discretized-RF在多样性和噪声建模上优于传统rectified flow。

Abstract: Recently, the rectified flow (RF) has emerged as the new state-of-the-art
among flow-based diffusion models due to its high efficiency advantage in
straight path sampling, especially with the amazing images generated by a
series of RF models such as Flux 1.0 and SD 3.0. Although a straight-line
connection between the noisy and natural data distributions is intuitive, fast,
and easy to optimize, it still inevitably leads to: 1) Diversity concerns,
which arise since straight-line paths only cover a fairly restricted sampling
space. 2) Multi-scale noise modeling concerns, since the straight line flow
only needs to optimize the constant velocity field $\bm v$ between the two
distributions $\bm\pi_0$ and $\bm\pi_1$. In this work, we present
Discretized-RF, a new family of rectified flow (also called momentum flow
models since they refer to the previous velocity component and the random
velocity component in each diffusion step), which discretizes the straight path
into a series of variable velocity field sub-paths (namely ``momentum fields'')
to expand the search space, especially when close to the distribution
$p_\text{noise}$. Different from the previous case where noise is directly
superimposed on $\bm x$, we introduce noise on the velocity $\bm v$ of the
sub-path to change its direction in order to improve the diversity and
multi-scale noise modeling abilities. Experimental results on several
representative datasets demonstrate that learning momentum flow matching by
sampling random velocity fields will produce trajectories that are both diverse
and efficient, and can consistently generate high-quality and diverse results.
Code is available at https://github.com/liuruixun/momentum-fm.

</details>


### [67] [HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation](https://arxiv.org/abs/2506.08797)
*Ziyao Huang,Zixiang Zhou,Juan Cao,Yifeng Ma,Yi Chen,Zejing Rao,Zhiyong Xu,Hongmei Wang,Qin Lin,Yuan Zhou,Qinglin Lu,Fan Tang*

Main category: cs.CV

TL;DR: HunyuanVideo-HOMA是一个弱条件多模态驱动框架，通过稀疏解耦的运动指导提升可控性，减少对精确输入的依赖，并在弱监督下实现最先进的交互自然性和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决人-物交互（HOI）视频生成中的关键限制，包括对精选运动数据的依赖、对新物体/场景的泛化能力有限以及可访问性受限。

Method: 采用多模态扩散变换器（MMDiT）的双输入空间编码外观和运动信号，结合参数空间HOI适配器和面部交叉注意力适配器优化训练。

Result: 在交互自然性和泛化性能上达到最先进水平，支持文本条件生成和交互式物体操作。

Conclusion: HunyuanVideo-HOMA展示了在弱监督下的高效适应性和多功能性，提供用户友好的演示界面。

Abstract: To address key limitations in human-object interaction (HOI) video generation
-- specifically the reliance on curated motion data, limited generalization to
novel objects/scenarios, and restricted accessibility -- we introduce
HunyuanVideo-HOMA, a weakly conditioned multimodal-driven framework.
HunyuanVideo-HOMA enhances controllability and reduces dependency on precise
inputs through sparse, decoupled motion guidance. It encodes appearance and
motion signals into the dual input space of a multimodal diffusion transformer
(MMDiT), fusing them within a shared context space to synthesize temporally
consistent and physically plausible interactions. To optimize training, we
integrate a parameter-space HOI adapter initialized from pretrained MMDiT
weights, preserving prior knowledge while enabling efficient adaptation, and a
facial cross-attention adapter for anatomically accurate audio-driven lip
synchronization. Extensive experiments confirm state-of-the-art performance in
interaction naturalness and generalization under weak supervision. Finally,
HunyuanVideo-HOMA demonstrates versatility in text-conditioned generation and
interactive object manipulation, supported by a user-friendly demo interface.
The project page is at https://anonymous.4open.science/w/homa-page-0FBE/.

</details>


### [68] [Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought](https://arxiv.org/abs/2506.08817)
*Shuyi Zhang,Xiaoshuai Hao,Yingbo Tang,Lingfeng Zhang,Pengwei Wang,Zhongyuan Wang,Hongxuan Ma,Shanghang Zhang*

Main category: cs.CV

TL;DR: Video-CoT是一个新的数据集和基准测试，旨在提升视频内容理解的时空细节分析能力，包含大量问题和答案对，以及高质量的CoT注释样本。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模视觉语言模型（VLMs）在捕捉视频分析的时空细节方面表现不佳，因此需要新的数据集和方法来填补这一空白。

Method: 提出了Video-CoT数据集，包含192,000个细粒度时空问题-答案对和23,000个高质量的CoT注释样本，并设计了专门的基准测试。

Result: 实验表明，当前VLMs在时空理解任务上表现不佳，验证了该任务的挑战性。

Conclusion: Video-CoT为多媒体理解和智能系统的视频分析提供了新的研究方向和资源，推动了这一领域的发展。

Abstract: Video content comprehension is essential for various applications, ranging
from video analysis to interactive systems. Despite advancements in large-scale
vision-language models (VLMs), these models often struggle to capture the
nuanced, spatiotemporal details essential for thorough video analysis. To
address this gap, we introduce Video-CoT, a groundbreaking dataset designed to
enhance spatiotemporal understanding using Chain-of-Thought (CoT)
methodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal
question-answer pairs and 23,000 high-quality CoT-annotated samples, providing
a solid foundation for evaluating spatiotemporal understanding in video
comprehension. Additionally, we provide a comprehensive benchmark for assessing
these tasks, with each task featuring 750 images and tailored evaluation
metrics. Our extensive experiments reveal that current VLMs face significant
challenges in achieving satisfactory performance, high-lighting the
difficulties of effective spatiotemporal understanding. Overall, the Video-CoT
dataset and benchmark open new avenues for research in multimedia understanding
and support future innovations in intelligent systems requiring advanced video
analysis capabilities. By making these resources publicly available, we aim to
encourage further exploration in this critical area. Project
website:https://video-cot.github.io/ .

</details>


### [69] [CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics](https://arxiv.org/abs/2506.08835)
*Shravan Nayak,Mehar Bhatia,Xiaofeng Zhang,Verena Rieser,Lisa Anne Hendricks,Sjoerd van Steenkiste,Yash Goyal,Karolina Stańczak,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: 研究发现文本到图像（T2I）模型在表现多样文化背景时存在显著不足，无论是显性还是隐性文化期望，失败率分别为68%和49%。现有评估指标与人类文化对齐判断相关性低。


<details>
  <summary>Details</summary>
Motivation: 研究旨在量化T2I模型在文化表现上的准确性，揭示其在多样文化背景中的不足。

Method: 引入CulturalFrames基准，涵盖10个国家、5个社会文化领域，通过983个提示、3637张图像及10k+人工标注进行系统评估。

Result: T2I模型在显性和隐性文化期望上的失败率分别为68%和49%，现有评估指标与人类判断相关性差。

Conclusion: 研究揭示了T2I模型在文化表现上的关键缺陷，为开发更具文化敏感性的模型和评估方法提供了方向。

Abstract: The increasing ubiquity of text-to-image (T2I) models as tools for visual
content generation raises concerns about their ability to accurately represent
diverse cultural contexts. In this work, we present the first study to
systematically quantify the alignment of T2I models and evaluation metrics with
respect to both explicit as well as implicit cultural expectations. To this
end, we introduce CulturalFrames, a novel benchmark designed for rigorous human
evaluation of cultural representation in visual generations. Spanning 10
countries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,
3637 corresponding images generated by 4 state-of-the-art T2I models, and over
10k detailed human annotations. We find that T2I models not only fail to meet
the more challenging implicit expectations but also the less challenging
explicit expectations. Across models and countries, cultural expectations are
missed an average of 44% of the time. Among these failures, explicit
expectations are missed at a surprisingly high average rate of 68%, while
implicit expectation failures are also significant, averaging 49%. Furthermore,
we demonstrate that existing T2I evaluation metrics correlate poorly with human
judgments of cultural alignment, irrespective of their internal reasoning.
Collectively, our findings expose critical gaps, providing actionable
directions for developing more culturally informed T2I models and evaluation
methodologies.

</details>


### [70] [Adapting Vision-Language Foundation Model for Next Generation Medical Ultrasound Image Analysis](https://arxiv.org/abs/2506.08849)
*Jingguo Qu,Xinyang Han,Tonghuan Xiao,Jia Ai,Juan Wu,Tong Zhao,Jing Qin,Ann Dorothy King,Winnie Chiu-Wing Chu,Jing Cai,Michael Tin-Cheung Yingınst*

Main category: cs.CV

TL;DR: 该研究提出了一种基于视觉-语言基础模型的领域适应方法，用于提升超声图像分析的性能，通过微调管道和大语言模型作为文本细化器，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手动标注超声图像中的感兴趣区域耗时且易产生不一致性，而现有视觉-语言基础模型在医学图像领域表现不佳，需要领域适应方法。

Method: 研究设计了针对视觉-语言基础模型的微调管道，利用大语言模型作为文本细化器，并结合任务驱动的适应策略和头部结构。

Result: 在六个超声数据集和两项任务（分割和分类）上验证，该方法显著提升了性能，优于现有视觉-语言和纯基础模型。

Conclusion: 该方法有效解决了视觉-语言基础模型在医学图像领域的适应问题，为超声图像分析提供了新工具。

Abstract: Medical ultrasonography is an essential imaging technique for examining
superficial organs and tissues, including lymph nodes, breast, and thyroid. It
employs high-frequency ultrasound waves to generate detailed images of the
internal structures of the human body. However, manually contouring regions of
interest in these images is a labor-intensive task that demands expertise and
often results in inconsistent interpretations among individuals.
Vision-language foundation models, which have excelled in various computer
vision applications, present new opportunities for enhancing ultrasound image
analysis. Yet, their performance is hindered by the significant differences
between natural and medical imaging domains. This research seeks to overcome
these challenges by developing domain adaptation methods for vision-language
foundation models. In this study, we explore the fine-tuning pipeline for
vision-language foundation models by utilizing large language model as text
refiner with special-designed adaptation strategies and task-driven heads. Our
approach has been extensively evaluated on six ultrasound datasets and two
tasks: segmentation and classification. The experimental results show that our
method can effectively improve the performance of vision-language foundation
models for ultrasound image analysis, and outperform the existing
state-of-the-art vision-language and pure foundation models. The source code of
this study is available at
\href{https://github.com/jinggqu/NextGen-UIA}{GitHub}.

</details>


### [71] [Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning](https://arxiv.org/abs/2506.08854)
*Junzhuo Liu,Markus Eckstein,Zhixiang Wang,Friedrich Feuerhake,Dorit Merhof*

Main category: cs.CV

TL;DR: 该研究提出了一种基于对比学习的深度学习方法，用于从全切片图像预测空间分辨基因表达，显著提升了高表达基因、高变异基因和标记基因的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学数据获取成本高，大规模数据难以获得，因此需要一种高效的方法从现有图像数据中预测基因表达。

Method: 采用对比学习框架的深度学习方法，从全切片图像中预测空间分辨基因表达。

Result: 在六个疾病数据集上评估，方法在高表达基因、高变异基因和标记基因的预测中，Pearson相关系数分别提升了6.27%、6.11%和11.26%。

Conclusion: 该方法不仅保留了基因间相关性，还适用于小样本数据集，并在癌症组织定位中展现出潜力。

Abstract: Spatial transcriptomics is a technology that captures gene expression levels
at different spatial locations, widely used in tumor microenvironment analysis
and molecular profiling of histopathology, providing valuable insights into
resolving gene expression and clinical diagnosis of cancer. Due to the high
cost of data acquisition, large-scale spatial transcriptomics data remain
challenging to obtain. In this study, we develop a contrastive learning-based
deep learning method to predict spatially resolved gene expression from
whole-slide images. Evaluation across six different disease datasets
demonstrates that, compared to existing studies, our method improves Pearson
Correlation Coefficient (PCC) in the prediction of highly expressed genes,
highly variable genes, and marker genes by 6.27%, 6.11%, and 11.26%
respectively. Further analysis indicates that our method preserves gene-gene
correlations and applies to datasets with limited samples. Additionally, our
method exhibits potential in cancer tissue localization based on biomarker
expression.

</details>


### [72] [StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams](https://arxiv.org/abs/2506.08862)
*Zike Wu,Qi Yan,Xuanyu Yi,Lele Wang,Renjie Liao*

Main category: cs.CV

TL;DR: StreamSplat是一种实时动态3D场景重建框架，通过未校准视频流在线生成动态3D高斯泼溅表示，解决了实时处理、动态建模和长期稳定性的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时处理未校准输入、动态场景建模和长期稳定性，StreamSplat旨在解决这些问题。

Method: 提出静态编码器中的概率采样机制和动态解码器中的双向变形场，实现高效动态建模。

Result: 在静态和动态基准测试中表现优异，支持任意长度视频流的在线重建。

Conclusion: StreamSplat在重建质量和动态建模上优于现有方法，且具备在线处理能力。

Abstract: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams
is crucial for numerous real-world applications. However, existing methods
struggle to jointly address three key challenges: 1) processing uncalibrated
inputs in real time, 2) accurately modeling dynamic scene evolution, and 3)
maintaining long-term stability and computational efficiency. To this end, we
introduce StreamSplat, the first fully feed-forward framework that transforms
uncalibrated video streams of arbitrary length into dynamic 3D Gaussian
Splatting (3DGS) representations in an online manner, capable of recovering
scene dynamics from temporally local observations. We propose two key technical
innovations: a probabilistic sampling mechanism in the static encoder for 3DGS
position prediction, and a bidirectional deformation field in the dynamic
decoder that enables robust and efficient dynamic modeling. Extensive
experiments on static and dynamic benchmarks demonstrate that StreamSplat
consistently outperforms prior works in both reconstruction quality and dynamic
scene modeling, while uniquely supporting online reconstruction of arbitrarily
long video streams. Code and models are available at
https://github.com/nickwzk/StreamSplat.

</details>


### [73] [DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval](https://arxiv.org/abs/2506.08887)
*Leqi Shen,Guoqiang Gong,Tianxiang Hao,Tao He,Yifeng Zhang,Pengzhang Liu,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: 论文提出DiscoVLA方法，通过同时减少视觉、语言和对齐三方面的差异，提升视频-文本检索性能。


<details>
  <summary>Details</summary>
Motivation: CLIP模型专注于图像级视觉语言匹配，而视频-文本检索需要视频级的全面理解。现有方法主要关注视觉差异，忽视了语言和对齐差异。

Method: 提出Image-Video Features Fusion整合图像级和视频级特征，生成伪图像标题学习细粒度对齐，并通过Image-to-Video Alignment Distillation增强视频级对齐。

Result: 在MSRVTT数据集上，DiscoVLA以CLIP（ViT-B/16）为基础，R@1达到50.5%，优于之前方法1.5%。

Conclusion: DiscoVLA通过多维度差异减少，显著提升了视频-文本检索性能。

Abstract: The parameter-efficient adaptation of the image-text pretraining model CLIP
for video-text retrieval is a prominent area of research. While CLIP is focused
on image-level vision-language matching, video-text retrieval demands
comprehensive understanding at the video level. Three key discrepancies emerge
in the transfer from image-level to video-level: vision, language, and
alignment. However, existing methods mainly focus on vision while neglecting
language and alignment. In this paper, we propose Discrepancy Reduction in
Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all
three discrepancies. Specifically, we introduce Image-Video Features Fusion to
integrate image-level and video-level features, effectively tackling both
vision and language discrepancies. Additionally, we generate pseudo image
captions to learn fine-grained image-level alignment. To mitigate alignment
discrepancies, we propose Image-to-Video Alignment Distillation, which
leverages image-level alignment knowledge to enhance video-level alignment.
Extensive experiments demonstrate the superiority of our DiscoVLA. In
particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous
methods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is
available at https://github.com/LunarShen/DsicoVLA.

</details>


### [74] [Product of Experts for Visual Generation](https://arxiv.org/abs/2506.08894)
*Yunzhi Zhang,Carson Murtuza-Lanier,Zizhang Li,Yilun Du,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出了一种基于专家乘积（PoE）的框架，用于在推理时组合异构模型的知识，通过退火重要性采样（AIS）实现，提升图像和视频合成的可控性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 整合来自视觉生成模型、视觉语言模型以及人类知识源（如图形引擎和物理模拟器）的多样化知识，以提升生成任务的性能。

Method: 采用专家乘积（PoE）框架，通过退火重要性采样（AIS）从异构模型中组合知识，无需额外训练。

Result: 在图像和视频合成任务中表现出更好的可控性，并提供灵活的用户界面以指定生成目标。

Conclusion: 提出的框架为异构知识整合提供了一种高效且灵活的方法，适用于多种视觉生成任务。

Abstract: Modern neural models capture rich priors and have complementary knowledge
over shared data domains, e.g., images and videos. Integrating diverse
knowledge from multiple sources -- including visual generative models, visual
language models, and sources with human-crafted knowledge such as graphics
engines and physics simulators -- remains under-explored. We propose a Product
of Experts (PoE) framework that performs inference-time knowledge composition
from heterogeneous models. This training-free approach samples from the product
distribution across experts via Annealed Importance Sampling (AIS). Our
framework shows practical benefits in image and video synthesis tasks, yielding
better controllability than monolithic methods and additionally providing
flexible user interfaces for specifying visual generation goals.

</details>


### [75] [WetCat: Automating Skill Assessment in Wetlab Cataract Surgery Videos](https://arxiv.org/abs/2506.08896)
*Negin Ghamsarian,Raphael Sznitman,Klaus Schoeffmann,Jens Kowal*

Main category: cs.CV

TL;DR: WetCat是一个专门为白内障手术湿实验室环境设计的视频数据集，用于自动化技能评估，填补了现有数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 传统湿实验室培训依赖人工评估，效率低且主观性强，计算机视觉技术为自动化评估提供了可能。

Method: WetCat包含高分辨率视频，涵盖关键手术阶段的注释和语义分割，支持标准化技能评估。

Result: 数据集为开发可解释的AI评估工具奠定了基础，提升了手术教育的客观性和可扩展性。

Conclusion: WetCat为眼科培训中的自动化工作流分析和技能评估设定了新标准。

Abstract: To meet the growing demand for systematic surgical training, wetlab
environments have become indispensable platforms for hands-on practice in
ophthalmology. Yet, traditional wetlab training depends heavily on manual
performance evaluations, which are labor-intensive, time-consuming, and often
subject to variability. Recent advances in computer vision offer promising
avenues for automated skill assessment, enhancing both the efficiency and
objectivity of surgical education. Despite notable progress in ophthalmic
surgical datasets, existing resources predominantly focus on real surgeries or
isolated tasks, falling short of supporting comprehensive skill evaluation in
controlled wetlab settings. To address these limitations, we introduce WetCat,
the first dataset of wetlab cataract surgery videos specifically curated for
automated skill assessment. WetCat comprises high-resolution recordings of
surgeries performed by trainees on artificial eyes, featuring comprehensive
phase annotations and semantic segmentations of key anatomical structures.
These annotations are meticulously designed to facilitate skill assessment
during the critical capsulorhexis and phacoemulsification phases, adhering to
standardized surgical skill assessment frameworks. By focusing on these
essential phases, WetCat enables the development of interpretable, AI-driven
evaluation tools aligned with established clinical metrics. This dataset lays a
strong foundation for advancing objective, scalable surgical education and sets
a new benchmark for automated workflow analysis and skill assessment in
ophthalmology training. The dataset and annotations are publicly available in
Synapse https://www.synapse.org/Synapse:syn66401174/files.

</details>


### [76] [MIRAGE: Multimodal foundation model and benchmark for comprehensive retinal OCT image analysis](https://arxiv.org/abs/2506.08900)
*José Morano,Botond Fazekas,Emese Sükei,Ronald Fecso,Taha Emre,Markus Gumpinger,Georg Faustmann,Marzieh Oghbaie,Ursula Schmidt-Erfurth,Hrvoje Bogunović*

Main category: cs.CV

TL;DR: MIRAGE是一种新型多模态基础模型，用于分析OCT和SLO图像，并在分类和分割任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在眼科图像分析中依赖大量标注数据且泛化能力不足，基础模型（FMs）虽有潜力但缺乏验证。

Method: 提出MIRAGE多模态基础模型，并设计新的评估基准，涵盖OCT/SLO分类和分割任务。

Result: MIRAGE在分类和分割任务中优于通用和专业FMs及分割方法。

Conclusion: MIRAGE适合作为开发稳健眼科AI系统的基础，模型和评估基准已开源。

Abstract: Artificial intelligence (AI) has become a fundamental tool for assisting
clinicians in analyzing ophthalmic images, such as optical coherence tomography
(OCT). However, developing AI models often requires extensive annotation, and
existing models tend to underperform on independent, unseen data. Foundation
models (FMs), large AI models trained on vast unlabeled datasets, have shown
promise in overcoming these challenges. Nonetheless, available FMs for
ophthalmology lack extensive validation, especially for segmentation tasks, and
focus on a single imaging modality. In this context, we propose MIRAGE, a novel
multimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)
images. Additionally, we propose a new evaluation benchmark with OCT/SLO
classification and segmentation tasks. The comparison with general and
specialized FMs and segmentation methods shows the superiority of MIRAGE in
both types of tasks, highlighting its suitability as a basis for the
development of robust AI systems for retinal OCT image analysis. Both MIRAGE
and the evaluation benchmark are publicly available:
https://github.com/j-morano/MIRAGE.

</details>


### [77] [Hyperbolic Dual Feature Augmentation for Open-Environment](https://arxiv.org/abs/2506.08906)
*Peilin Yu,Yuwei Wu,Zhi Gao,Xiaomeng Fan,Shuo Yang,Yunde Jia*

Main category: cs.CV

TL;DR: 提出了一种双曲双特征增强方法，用于开放环境，增强已见和未见类别的特征。


<details>
  <summary>Details</summary>
Motivation: 现有双曲特征增强方法局限于封闭环境，无法处理开放环境中的未见类别。

Method: 1. 使用神经ODE模块估计特征分布；2. 引入正则化器保持双曲空间中的层次结构；3. 推导双曲双增强损失的上界。

Result: 在五个开放环境任务中显著提升双曲算法的性能。

Conclusion: 该方法有效增强了双曲算法在开放环境中的泛化能力。

Abstract: Feature augmentation generates novel samples in the feature space, providing
an effective way to enhance the generalization ability of learning algorithms
with hyperbolic geometry. Most hyperbolic feature augmentation is confined to
closed-environment, assuming the number of classes is fixed (\emph{i.e.}, seen
classes) and generating features only for these classes. In this paper, we
propose a hyperbolic dual feature augmentation method for open-environment,
which augments features for both seen and unseen classes in the hyperbolic
space. To obtain a more precise approximation of the real data distribution for
efficient training, (1) we adopt a neural ordinary differential equation
module, enhanced by meta-learning, estimating the feature distributions of both
seen and unseen classes; (2) we then introduce a regularizer to preserve the
latent hierarchical structures of data in the hyperbolic space; (3) we also
derive an upper bound for the hyperbolic dual augmentation loss, allowing us to
train a hyperbolic model using infinite augmentations for seen and unseen
classes. Extensive experiments on five open-environment tasks:
class-incremental learning, few-shot open-set recognition, few-shot learning,
zero-shot learning, and general image classification, demonstrate that our
method effectively enhances the performance of hyperbolic algorithms in
open-environment.

</details>


### [78] [SkipVAR: Accelerating Visual Autoregressive Modeling via Adaptive Frequency-Aware Skipping](https://arxiv.org/abs/2506.08908)
*Jiajun Li,Yue Ma,Xinyu Zhang,Qingyan Wei,Songhua Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: 论文分析了VAR模型中的计算冗余问题，提出了两种加速策略：自动跳步和无条件分支替换，并进一步提出了自适应框架SkipVAR，显著提升了生成效率。


<details>
  <summary>Details</summary>
Motivation: VAR模型的高频组件或后期步骤导致推理延迟，但计算冗余尚未深入研究。

Method: 提出自动跳步策略和无条件分支替换技术，并设计自适应框架SkipVAR动态选择加速策略。

Result: SkipVAR在保持模型质量的同时，实现了1.81倍整体加速和2.62倍速度提升。

Conclusion: 频率感知的自适应加速策略有效提升了VAR模型的生成效率。

Abstract: Recent studies on Visual Autoregressive (VAR) models have highlighted that
high-frequency components, or later steps, in the generation process contribute
disproportionately to inference latency. However, the underlying computational
redundancy involved in these steps has yet to be thoroughly investigated. In
this paper, we conduct an in-depth analysis of the VAR inference process and
identify two primary sources of inefficiency: step redundancy and unconditional
branch redundancy. To address step redundancy, we propose an automatic
step-skipping strategy that selectively omits unnecessary generation steps to
improve efficiency. For unconditional branch redundancy, we observe that the
information gap between the conditional and unconditional branches is minimal.
Leveraging this insight, we introduce unconditional branch replacement, a
technique that bypasses the unconditional branch to reduce computational cost.
Notably, we observe that the effectiveness of acceleration strategies varies
significantly across different samples. Motivated by this, we propose SkipVAR,
a sample-adaptive framework that leverages frequency information to dynamically
select the most suitable acceleration strategy for each instance. To evaluate
the role of high-frequency information, we introduce high-variation benchmark
datasets that test model sensitivity to fine details. Extensive experiments
show SkipVAR achieves over 0.88 average SSIM with up to 1.81x overall
acceleration and 2.62x speedup on the GenEval benchmark, maintaining model
quality. These results confirm the effectiveness of frequency-aware,
training-free adaptive acceleration for scalable autoregressive image
generation. Our code is available at https://github.com/fakerone-li/SkipVAR and
has been publicly released.

</details>


### [79] [Inherently Faithful Attention Maps for Vision Transformers](https://arxiv.org/abs/2506.08915)
*Ananthu Aniraj,Cassio F. Dantas,Dino Ienco,Diego Marcos*

Main category: cs.CV

TL;DR: 提出一种基于注意力的方法，通过学习的二进制注意力掩码确保只有关注的图像区域影响预测，显著提高了对虚假相关性和分布外背景的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 上下文可能强烈影响物体感知，尤其是在物体出现在分布外背景时可能导致有偏表示。同时，许多图像级物体中心任务需要识别相关区域，通常需要上下文。

Method: 提出两阶段框架：第一阶段处理完整图像以发现物体部分并识别任务相关区域；第二阶段利用输入注意力掩码限制其感受野到这些区域，进行聚焦分析并过滤潜在虚假信息。两阶段联合训练，第二阶段可优化第一阶段。

Result: 在多样化基准测试中，该方法显著提高了对虚假相关性和分布外背景的鲁棒性。

Conclusion: 该方法通过两阶段框架和注意力掩码，有效解决了上下文与物体感知的矛盾，提升了模型的鲁棒性。

Abstract: We introduce an attention-based method that uses learned binary attention
masks to ensure that only attended image regions influence the prediction.
Context can strongly affect object perception, sometimes leading to biased
representations, particularly when objects appear in out-of-distribution
backgrounds. At the same time, many image-level object-centric tasks require
identifying relevant regions, often requiring context. To address this
conundrum, we propose a two-stage framework: stage 1 processes the full image
to discover object parts and identify task-relevant regions, while stage 2
leverages input attention masking to restrict its receptive field to these
regions, enabling a focused analysis while filtering out potentially spurious
information. Both stages are trained jointly, allowing stage 2 to refine stage
1. Extensive experiments across diverse benchmarks demonstrate that our
approach significantly improves robustness against spurious correlations and
out-of-distribution backgrounds.

</details>


### [80] [Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions](https://arxiv.org/abs/2506.08927)
*David Acuna,Ximing Lu,Jaehun Jung,Hyunwoo Kim,Amlan Kar,Sanja Fidler,Yejin Choi*

Main category: cs.CV

TL;DR: 论文探讨了如何通过蒙特卡洛树搜索（MCTS）算法在不额外训练的情况下，激发非推理型视觉语言模型的隐藏知识，生成长推理链。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索是否可以通过搜索机制而非重新训练，激发已部署的非推理型视觉语言模型的潜在推理能力。

Method: 采用蒙特卡洛树搜索（MCTS）算法，通过注入子问题-子答案对，将推理过程视为搜索问题。

Result: 在三个基准测试中表现一致提升，MMMU-PRO上整体提升2%，其中文科领域显著提升9%。

Conclusion: 研究表明，通过搜索机制可以激发非推理型模型的隐藏知识，生成长推理链，无需额外训练。

Abstract: Recent research in vision-language models (VLMs) has centered around the
possibility of equipping them with implicit long-form chain-of-thought
reasoning -- akin to the success observed in language models -- via
distillation and reinforcement learning. But what about the non-reasoning
models already trained and deployed across the internet? Should we simply
abandon them, or is there hope for a search mechanism that can elicit hidden
knowledge and induce long reasoning traces -- without any additional training
or supervision? In this paper, we explore this possibility using a Monte Carlo
Tree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer
pairs into the model's output stream. We show that framing reasoning as a
search process -- where subquestions act as latent decisions within a broader
inference trajectory -- helps the model "connect the dots" between fragmented
knowledge and produce extended reasoning traces in non-reasoning models. We
evaluate our method across three benchmarks and observe consistent
improvements. Notably, our approach yields a 2% overall improvement on
MMMU-PRO, including a significant 9% gain in Liberal Arts.

</details>


### [81] [What Limits Virtual Agent Application? OmniBench: A Scalable Multi-Dimensional Benchmark for Essential Virtual Agent Capabilities](https://arxiv.org/abs/2506.08933)
*Wendong Bu,Yang Wu,Qifan Yu,Minghe Gao,Bingchen Miao,Zhenkui Zhang,Kaihang Pan,Yunfei Li,Mengze Li,Wei Ji,Juncheng Li,Siliang Tang,Yueting Zhuang*

Main category: cs.CV

TL;DR: OmniBench是一个自生成、跨平台、基于图的基准测试工具，通过子任务组合生成可控复杂度的任务，解决了现有基准测试的局限性。OmniEval是多维评估框架，评估虚拟代理的10种能力。数据集包含36k图结构任务，人工接受率达91%。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在任务复杂度不可控、手动标注场景有限和多维评估缺乏的问题。

Method: 提出OmniBench和OmniEval，前者通过子任务组合生成任务，后者提供多维评估框架。

Result: 数据集包含36k任务，人工接受率91%，图结构数据比手动标注更高效。

Conclusion: OmniBench和OmniEval为虚拟代理的多维评估和性能提升提供了新方向。

Abstract: As multimodal large language models (MLLMs) advance, MLLM-based virtual
agents have demonstrated remarkable performance. However, existing benchmarks
face significant limitations, including uncontrollable task complexity,
extensive manual annotation with limited scenarios, and a lack of
multidimensional evaluation. In response to these challenges, we introduce
OmniBench, a self-generating, cross-platform, graph-based benchmark with an
automated pipeline for synthesizing tasks of controllable complexity through
subtask composition. To evaluate the diverse capabilities of virtual agents on
the graph, we further present OmniEval, a multidimensional evaluation framework
that includes subtask-level evaluation, graph-based metrics, and comprehensive
tests across 10 capabilities. Our synthesized dataset contains 36k
graph-structured tasks across 20 scenarios, achieving a 91\% human acceptance
rate. Training on our graph-structured data shows that it can more efficiently
guide agents compared to manually annotated data. We conduct multidimensional
evaluations for various open-source and closed-source models, revealing their
performance across various capabilities and paving the way for future
advancements. Our project is available at https://omni-bench.github.io/.

</details>


### [82] [SSS: Semi-Supervised SAM-2 with Efficient Prompting for Medical Imaging Segmentation](https://arxiv.org/abs/2506.08949)
*Hongjie Zhu,Xiwei Liu,Rundong Xue,Zeyu Zhang,Yong Xu,Daji Ergu,Ying Cai,Yang Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种名为SSS的半监督学习方法，利用SAM-2的强大特征提取能力，结合多视图数据增强和特征增强机制，显著提升了医学图像分割的性能。


<details>
  <summary>Details</summary>
Motivation: 在医学图像分析中，有效利用大规模未标注数据并减少对高质量标注的依赖是一个关键挑战。半监督学习通过知识迁移提升模型性能，成为研究热点。

Method: 基于SAM-2的特征提取能力，提出SSS方法，引入判别性特征增强（DFE）机制和多尺度数据增强，结合物理约束滑动窗口（PCSW）生成提示。

Result: 在ACDC和BHSD数据集上，SSS表现优异，BHSD上的平均Dice分数达到53.15，比现有最优方法提升3.65。

Conclusion: SSS通过结合SAM-2和半监督学习，显著提升了医学图像分割性能，为未来研究提供了新思路。

Abstract: In the era of information explosion, efficiently leveraging large-scale
unlabeled data while minimizing the reliance on high-quality pixel-level
annotations remains a critical challenge in the field of medical imaging.
Semi-supervised learning (SSL) enhances the utilization of unlabeled data by
facilitating knowledge transfer, significantly improving the performance of
fully supervised models and emerging as a highly promising research direction
in medical image analysis. Inspired by the ability of Vision Foundation Models
(e.g., SAM-2) to provide rich prior knowledge, we propose SSS (Semi-Supervised
SAM-2), a novel approach that leverages SAM-2's robust feature extraction
capabilities to uncover latent knowledge in unlabeled medical images, thus
effectively enhancing feature support for fully supervised medical image
segmentation. Specifically, building upon the single-stream "weak-to-strong"
consistency regularization framework, this paper introduces a Discriminative
Feature Enhancement (DFE) mechanism to further explore the feature
discrepancies introduced by various data augmentation strategies across
multiple views. By leveraging feature similarity and dissimilarity across
multi-scale augmentation techniques, the method reconstructs and models the
features, thereby effectively optimizing the salient regions. Furthermore, a
prompt generator is developed that integrates Physical Constraints with a
Sliding Window (PCSW) mechanism to generate input prompts for unlabeled data,
fulfilling SAM-2's requirement for additional prompts. Extensive experiments
demonstrate the superiority of the proposed method for semi-supervised medical
image segmentation on two multi-label datasets, i.e., ACDC and BHSD. Notably,
SSS achieves an average Dice score of 53.15 on BHSD, surpassing the previous
state-of-the-art method by +3.65 Dice. Code will be available at
https://github.com/AIGeeksGroup/SSS.

</details>


### [83] [Cross-Spectral Body Recognition with Side Information Embedding: Benchmarks on LLCM and Analyzing Range-Induced Occlusions on IJB-MDF](https://arxiv.org/abs/2506.08953)
*Anirudh Nanduri,Siyuan Huang,Rama Chellappa*

Main category: cs.CV

TL;DR: 本文研究了如何将预训练的视觉Transformer（ViT）模型应用于跨光谱人体识别任务，通过引入侧信息嵌入（SIE）提升性能，并探讨了遮挡对可见-红外（VI）再识别的影响。


<details>
  <summary>Details</summary>
Motivation: 跨光谱人体识别（如可见光与红外图像匹配）是一个具有挑战性的任务，现有方法在遮挡处理方面研究不足。本文旨在通过ViT模型和侧信息嵌入提升性能，并填补遮挡研究的空白。

Method: 使用预训练的ViT模型，结合侧信息嵌入（SIE）编码相机和域信息，并在LLCM数据集上进行评估。同时，利用IJB-MDF数据集分析遮挡对VI再识别的影响。

Result: 实验表明，仅编码相机信息（不显式包含域信息）即可在LLCM数据集上达到最优性能。遮挡分析揭示了跨范围、跨光谱匹配的挑战。

Conclusion: ViT模型结合侧信息嵌入在跨光谱人体识别中表现优异，未来研究需进一步探索遮挡对VI再识别的影响。

Abstract: Vision Transformers (ViTs) have demonstrated impressive performance across a
wide range of biometric tasks, including face and body recognition. In this
work, we adapt a ViT model pretrained on visible (VIS) imagery to the
challenging problem of cross-spectral body recognition, which involves matching
images captured in the visible and infrared (IR) domains. Recent ViT
architectures have explored incorporating additional embeddings beyond
traditional positional embeddings. Building on this idea, we integrate Side
Information Embedding (SIE) and examine the impact of encoding domain and
camera information to enhance cross-spectral matching. Surprisingly, our
results show that encoding only camera information - without explicitly
incorporating domain information - achieves state-of-the-art performance on the
LLCM dataset. While occlusion handling has been extensively studied in
visible-spectrum person re-identification (Re-ID), occlusions in
visible-infrared (VI) Re-ID remain largely underexplored - primarily because
existing VI-ReID datasets, such as LLCM, SYSU-MM01, and RegDB, predominantly
feature full-body, unoccluded images. To address this gap, we analyze the
impact of range-induced occlusions using the IARPA Janus Benchmark Multi-Domain
Face (IJB-MDF) dataset, which provides a diverse set of visible and infrared
images captured at various distances, enabling cross-range, cross-spectral
evaluations.

</details>


### [84] [Segment Concealed Objects with Incomplete Supervision](https://arxiv.org/abs/2506.08955)
*Chunming He,Kai Li,Yachao Zhang,Ziyun Yang,Youwei Pang,Longxiang Tang,Chengyu Fang,Yulun Zhang,Linghe Kong,Xiu Li,Sina Farsiu*

Main category: cs.CV

TL;DR: 论文提出了一种统一方法SEE，用于不完全监督的隐蔽物体分割（ISCOS），通过结合SAM模型生成伪标签和混合粒度特征分组模块，解决了不完全监督和物体与背景相似性的挑战。


<details>
  <summary>Details</summary>
Motivation: 隐蔽物体分割任务因不完全标注数据和物体与背景的相似性而极具挑战性，需要一种统一的方法来解决这些问题。

Method: 提出了SEE框架，利用SAM模型生成伪标签，并通过伪标签生成、存储和监督策略优化训练；同时设计了混合粒度特征分组模块以提升分割一致性。

Result: 实验表明，SEE在多个ISCOS任务中达到最先进性能，并能作为即插即用的解决方案提升现有模型。

Conclusion: SEE通过结合伪标签生成和特征分组模块，有效解决了ISCOS任务中的不完全监督和相似性问题，具有广泛的应用潜力。

Abstract: Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves
segmenting objects that seamlessly blend into their surrounding environments,
utilizing incompletely annotated data, such as weak and semi-annotations, for
model training. This task remains highly challenging due to (1) the limited
supervision provided by the incompletely annotated training data, and (2) the
difficulty of distinguishing concealed objects from the background, which
arises from the intrinsic similarities in concealed scenarios. In this paper,
we introduce the first unified method for ISCOS to address these challenges. To
tackle the issue of incomplete supervision, we propose a unified mean-teacher
framework, SEE, that leverages the vision foundation model, ``\emph{Segment
Anything Model (SAM)}'', to generate pseudo-labels using coarse masks produced
by the teacher model as prompts. To mitigate the effect of low-quality
segmentation masks, we introduce a series of strategies for pseudo-label
generation, storage, and supervision. These strategies aim to produce
informative pseudo-labels, store the best pseudo-labels generated, and select
the most reliable components to guide the student model, thereby ensuring
robust network training. Additionally, to tackle the issue of intrinsic
similarity, we design a hybrid-granularity feature grouping module that groups
features at different granularities and aggregates these results. By clustering
similar features, this module promotes segmentation coherence, facilitating
more complete segmentation for both single-object and multiple-object images.
We validate the effectiveness of our approach across multiple ISCOS tasks, and
experimental results demonstrate that our method achieves state-of-the-art
performance. Furthermore, SEE can serve as a plug-and-play solution, enhancing
the performance of existing models.

</details>


### [85] [Data Augmentation For Small Object using Fast AutoAugment](https://arxiv.org/abs/2506.08956)
*DaeEun Yoon,Semin Kim,SangWook Yoo,Jongha Lee*

Main category: cs.CV

TL;DR: 提出了一种基于Fast AutoAugment的数据增强方法，显著提升了小目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管目标检测整体性能提升，但小目标检测性能仍显著落后，亟需改进。

Method: 采用Fast AutoAugment快速寻找最优数据增强策略，以克服小目标检测中的性能下降问题。

Result: 在DOTA数据集上实现了20%的性能提升。

Conclusion: 该方法有效解决了小目标检测的挑战，性能显著提升。

Abstract: In recent years, there has been tremendous progress in object detection
performance. However, despite these advances, the detection performance for
small objects is significantly inferior to that of large objects. Detecting
small objects is one of the most challenging and important problems in computer
vision. To improve the detection performance for small objects, we propose an
optimal data augmentation method using Fast AutoAugment. Through our proposed
method, we can quickly find optimal augmentation policies that can overcome
degradation when detecting small objects, and we achieve a 20% performance
improvement on the DOTA dataset.

</details>


### [86] [ORIDa: Object-centric Real-world Image Composition Dataset](https://arxiv.org/abs/2506.08964)
*Jinwoo Kim,Sangmin Han,Jinho Jeong,Jiwoo Choi,Dongyoung Kim,Seon Joo Kim*

Main category: cs.CV

TL;DR: ORIDa是一个大规模、真实捕获的数据集，用于对象合成任务，包含超过30,000张图像和200个独特对象，支持多样化的场景和位置。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏多样性和规模，无法全面探索真实世界的对象合成场景。

Method: ORIDa包含两种数据类型：事实-反事实集（每组5张图像）和仅事实场景（单张图像）。

Result: ORIDa是首个公开的具有如此规模和复杂性的真实世界图像合成数据集。

Conclusion: ORIDa为对象合成研究的进一步发展提供了宝贵资源。

Abstract: Object compositing, the task of placing and harmonizing objects in images of
diverse visual scenes, has become an important task in computer vision with the
rise of generative models. However, existing datasets lack the diversity and
scale required to comprehensively explore real-world scenarios. We introduce
ORIDa (Object-centric Real-world Image Composition Dataset), a large-scale,
real-captured dataset containing over 30,000 images featuring 200 unique
objects, each of which is presented across varied positions and scenes. ORIDa
has two types of data: factual-counterfactual sets and factual-only scenes. The
factual-counterfactual sets consist of four factual images showing an object in
different positions within a scene and a single counterfactual (or background)
image of the scene without the object, resulting in five images per scene. The
factual-only scenes include a single image containing an object in a specific
context, expanding the variety of environments. To our knowledge, ORIDa is the
first publicly available dataset with its scale and complexity for real-world
image composition. Extensive analysis and experiments highlight the value of
ORIDa as a resource for advancing further research in object compositing.

</details>


### [87] [ADAM: Autonomous Discovery and Annotation Model using LLMs for Context-Aware Annotations](https://arxiv.org/abs/2506.08968)
*Amirreza Rouhi,Solmaz Arezoomandan,Knut Peterson,Joseph T. Woods,David K. Han*

Main category: cs.CV

TL;DR: ADAM是一种无需训练的开放世界对象标注框架，利用LLM和CLIP生成标签并构建嵌入-标签库，通过检索和自优化实现对新对象的标注。


<details>
  <summary>Details</summary>
Motivation: 解决传统目标检测模型依赖预定义类别、无法识别开放世界中新对象的问题。

Method: 结合LLM生成候选标签和CLIP视觉嵌入构建ELR，通过检索、投票和跨模态重排序标注新对象，并引入自优化循环提升一致性。

Result: 在COCO和PASCAL数据集上，ADAM无需微调即可有效标注新类别。

Conclusion: ADAM为开放世界对象标注提供了一种无需训练的高效解决方案。

Abstract: Object detection models typically rely on predefined categories, limiting
their ability to identify novel objects in open-world scenarios. To overcome
this constraint, we introduce ADAM: Autonomous Discovery and Annotation Model,
a training-free, self-refining framework for open-world object labeling. ADAM
leverages large language models (LLMs) to generate candidate labels for unknown
objects based on contextual information from known entities within a scene.
These labels are paired with visual embeddings from CLIP to construct an
Embedding-Label Repository (ELR) that enables inference without category
supervision. For a newly encountered unknown object, ADAM retrieves visually
similar instances from the ELR and applies frequency-based voting and
cross-modal re-ranking to assign a robust label. To further enhance
consistency, we introduce a self-refinement loop that re-evaluates repository
labels using visual cohesion analysis and k-nearest-neighbor-based majority
re-labeling. Experimental results on the COCO and PASCAL datasets demonstrate
that ADAM effectively annotates novel categories using only visual and
contextual signals, without requiring any fine-tuning or retraining.

</details>


### [88] [Rethinking Range-View LiDAR Segmentation in Adverse Weather](https://arxiv.org/abs/2506.08979)
*Longyu Yang,Ping Hu,Lu Zhang,Jun Liu,Yap-Peng Tan,Heng Tao Shen,Xiaofeng Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级模块化框架，通过改进LiDAR分割的初始处理模块，增强了在恶劣天气条件下的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于范围视图的LiDAR分割方法在恶劣天气下的泛化性能不足，限制了其在实际环境中的可靠性。

Method: 提出了一种双分支框架，分别处理几何属性和反射强度，并引入GAS和RDC模块抑制天气引起的噪声和失真。

Result: 实验表明，该方法显著提升了模型在恶劣天气下的泛化能力，且计算开销极小。

Conclusion: 该框架为实际应用中的LiDAR分割提供了一种高效且实用的解决方案。

Abstract: LiDAR segmentation has emerged as an important task to enrich multimedia
experiences and analysis. Range-view-based methods have gained popularity due
to their high computational efficiency and compatibility with real-time
deployment. However, their generalized performance under adverse weather
conditions remains underexplored, limiting their reliability in real-world
environments. In this work, we identify and analyze the unique challenges that
affect the generalization of range-view LiDAR segmentation in severe weather.
To address these challenges, we propose a modular and lightweight framework
that enhances robustness without altering the core architecture of existing
models. Our method reformulates the initial stem block of standard range-view
networks into two branches to process geometric attributes and reflectance
intensity separately. Specifically, a Geometric Abnormality Suppression (GAS)
module reduces the influence of weather-induced spatial noise, and a
Reflectance Distortion Calibration (RDC) module corrects reflectance
distortions through memory-guided adaptive instance normalization. The
processed features are then fused and passed to the original segmentation
pipeline. Extensive experiments on different benchmarks and baseline models
demonstrate that our approach significantly improves generalization to adverse
weather with minimal inference overhead, offering a practical and effective
solution for real-world LiDAR segmentation.

</details>


### [89] [Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models](https://arxiv.org/abs/2506.08990)
*Chenyu Lian,Hong-Yu Zhou,Dongyun Liang,Jing Qin,Liansheng Wang*

Main category: cs.CV

TL;DR: ALTA是一种高效的医学视觉-语言对齐方法，通过适应预训练的视觉模型，显著提升了检索和零样本分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统跨模态对比学习方法在视觉表示能力上表现不佳，而多模态掩码建模方法虽然视觉表示能力强，但在直接跨模态匹配上表现不足。ALTA旨在解决这一矛盾。

Method: ALTA通过适应预训练的视觉模型（来自掩码记录建模），结合时间多视角放射图像输入，提升视觉-语言对齐效果。

Result: ALTA在文本到图像和图像到文本检索任务中分别比最佳对比方法高出4%和6%的绝对准确率。

Conclusion: ALTA不仅高效，还提升了视觉和语言理解能力，代码已开源。

Abstract: Medical vision-language alignment through cross-modal contrastive learning
shows promising performance in image-text matching tasks, such as retrieval and
zero-shot classification. However, conventional cross-modal contrastive
learning (CLIP-based) methods suffer from suboptimal visual representation
capabilities, which also limits their effectiveness in vision-language
alignment. In contrast, although the models pretrained via multimodal masked
modeling struggle with direct cross-modal matching, they excel in visual
representation. To address this contradiction, we propose ALTA (ALign Through
Adapting), an efficient medical vision-language alignment method that utilizes
only about 8% of the trainable parameters and less than 1/5 of the
computational consumption required for masked record modeling. ALTA achieves
superior performance in vision-language matching tasks like retrieval and
zero-shot classification by adapting the pretrained vision model from masked
record modeling. Additionally, we integrate temporal-multiview radiograph
inputs to enhance the information consistency between radiographs and their
corresponding descriptions in reports, further improving the vision-language
alignment. Experimental evaluations show that ALTA outperforms the
best-performing counterpart by over 4% absolute points in text-to-image
accuracy and approximately 6% absolute points in image-to-text retrieval
accuracy. The adaptation of vision-language models during efficient alignment
also promotes better vision and language understanding. Code is publicly
available at https://github.com/DopamineLcy/ALTA.

</details>


### [90] [Do Concept Replacement Techniques Really Erase Unacceptable Concepts?](https://arxiv.org/abs/2506.08991)
*Anudeep Das,Gurjot Singh,Prach Chantasantitam,N. Asokan*

Main category: cs.CV

TL;DR: 本文探讨了生成模型中的概念替换技术（CRTs）在文本到图像（T2I）和图像到图像（I2I）场景中的差异，指出现有CRTs在I2I中无效，并提出了一种兼顾效果和保真度的新方法AntiMirror。


<details>
  <summary>Details</summary>
Motivation: 生成模型在避免生成不可接受内容（如冒犯性或版权内容）方面存在挑战，现有CRTs在I2I场景中效果不佳，需要研究其差异并改进技术。

Method: 通过I2I模型验证现有CRTs的不足，提出以目标图像编辑技术实现概念替换的AntiMirror方法。

Result: 实验证明现有CRTs在I2I中无效，AntiMirror在替换不可接受概念的同时保持了输入内容的保真度。

Conclusion: AntiMirror为解决生成模型中的概念替换问题提供了有效且高保真的解决方案，强调了I2I场景中技术改进的必要性。

Abstract: Generative models, particularly diffusion-based text-to-image (T2I) models,
have demonstrated astounding success. However, aligning them to avoid
generating content with unacceptable concepts (e.g., offensive or copyrighted
content, or celebrity likenesses) remains a significant challenge. Concept
replacement techniques (CRTs) aim to address this challenge, often by trying to
"erase" unacceptable concepts from models. Recently, model providers have
started offering image editing services which accept an image and a text prompt
as input, to produce an image altered as specified by the prompt. These are
known as image-to-image (I2I) models. In this paper, we first use an I2I model
to empirically demonstrate that today's state-of-the-art CRTs do not in fact
erase unacceptable concepts. Existing CRTs are thus likely to be ineffective in
emerging I2I scenarios, despite their proven ability to remove unwanted
concepts in T2I pipelines, highlighting the need to understand this discrepancy
between T2I and I2I settings. Next, we argue that a good CRT, while replacing
unacceptable concepts, should preserve other concepts specified in the inputs
to generative models. We call this fidelity. Prior work on CRTs have neglected
fidelity in the case of unacceptable concepts. Finally, we propose the use of
targeted image-editing techniques to achieve both effectiveness and fidelity.
We present such a technique, AntiMirror, and demonstrate its viability.

</details>


### [91] [SDTagNet: Leveraging Text-Annotated Navigation Maps for Online HD Map Construction](https://arxiv.org/abs/2506.08997)
*Fabian Immel,Jan-Hendrik Pauls,Richard Fehler,Frank Bieder,Jonas Merkert,Christoph Stiller*

Main category: cs.CV

TL;DR: SDTagNet利用标准定义（SD）地图（如OpenStreetMap）增强在线高精地图构建，通过引入文本注释和点级编码器，显著提升远距离检测精度。


<details>
  <summary>Details</summary>
Motivation: 高精地图维护成本高，而在线构建方法受限于传感器感知范围短的问题。利用易维护的SD地图作为先验信息，可以提升性能。

Method: SDTagNet结合SD地图的折线数据和文本注释，引入NLP特征，并使用点级编码器和正交元素标识符统一整合地图元素。

Result: 在Argoverse 2和nuScenes数据集上，性能提升显著：相比无先验方法提升+5.9 mAP（+45%），相比现有SD先验方法提升+3.2 mAP（+20%）。

Conclusion: SDTagNet通过充分利用SD地图信息，显著提升了在线高精地图构建的性能，解决了传感器感知范围短的问题。

Abstract: Autonomous vehicles rely on detailed and accurate environmental information
to operate safely. High definition (HD) maps offer a promising solution, but
their high maintenance cost poses a significant barrier to scalable deployment.
This challenge is addressed by online HD map construction methods, which
generate local HD maps from live sensor data. However, these methods are
inherently limited by the short perception range of onboard sensors. To
overcome this limitation and improve general performance, recent approaches
have explored the use of standard definition (SD) maps as prior, which are
significantly easier to maintain. We propose SDTagNet, the first online HD map
construction method that fully utilizes the information of widely available SD
maps, like OpenStreetMap, to enhance far range detection accuracy. Our approach
introduces two key innovations. First, in contrast to previous work, we
incorporate not only polyline SD map data with manually selected classes, but
additional semantic information in the form of textual annotations. In this
way, we enrich SD vector map tokens with NLP-derived features, eliminating the
dependency on predefined specifications or exhaustive class taxonomies. Second,
we introduce a point-level SD map encoder together with orthogonal element
identifiers to uniformly integrate all types of map elements. Experiments on
Argoverse 2 and nuScenes show that this boosts map perception performance by up
to +5.9 mAP (+45%) w.r.t. map construction without priors and up to +3.2 mAP
(+20%) w.r.t. previous approaches that already use SD map priors. Code is
available at https://github.com/immel-f/SDTagNet

</details>


### [92] [Do MIL Models Transfer?](https://arxiv.org/abs/2506.09022)
*Daniel Shao,Richard J. Chen,Andrew H. Song,Joel Runevic,Ming Y. Lu,Tong Ding,Faisal Mahmood*

Main category: cs.CV

TL;DR: 该研究系统评估了预训练MIL模型在计算病理学中的迁移学习能力，发现其在跨器官和任务中表现优于从头训练的模型。


<details>
  <summary>Details</summary>
Motivation: 解决MIL模型在弱监督小数据集中的性能问题，并探索其在计算病理学中的迁移学习潜力。

Method: 评估11个预训练MIL模型在21个预训练任务中的表现，包括形态学和分子亚型预测。

Result: 预训练MIL模型在跨器官任务中表现优异，泛化能力强，且使用更少数据即可超越基础模型。

Conclusion: MIL模型具有强大的适应性，迁移学习可显著提升计算病理学任务性能，并提供了标准化资源和预训练权重。

Abstract: Multiple Instance Learning (MIL) is a cornerstone approach in computational
pathology (CPath) for generating clinically meaningful slide-level embeddings
from gigapixel tissue images. However, MIL often struggles with small, weakly
supervised clinical datasets. In contrast to fields such as NLP and
conventional computer vision, where transfer learning is widely used to address
data scarcity, the transferability of MIL models remains poorly understood. In
this study, we systematically evaluate the transfer learning capabilities of
pretrained MIL models by assessing 11 models across 21 pretraining tasks for
morphological and molecular subtype prediction. Our results show that
pretrained MIL models, even when trained on different organs than the target
task, consistently outperform models trained from scratch. Moreover,
pretraining on pancancer datasets enables strong generalization across organs
and tasks, outperforming slide foundation models while using substantially less
pretraining data. These findings highlight the robust adaptability of MIL
models and demonstrate the benefits of leveraging transfer learning to boost
performance in CPath. Lastly, we provide a resource which standardizes the
implementation of MIL models and collection of pretrained model weights on
popular CPath tasks, available at https://github.com/mahmoodlab/MIL-Lab

</details>


### [93] [DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging](https://arxiv.org/abs/2506.09024)
*Felix Wagner,Pramit Saha,Harry Anthony,J. Alison Noble,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: 论文提出了一种名为Decentralized Isolation Networks (DIsoN)的框架，用于在无法共享数据的情况下进行OOD检测，通过交换模型参数实现训练和测试数据的比较。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域（如医学影像）部署ML模型时，需要检测训练数据中未见的输入（OOD检测），但现有方法要么丢弃训练数据，要么假设测试数据和训练数据集中存储，这在现实中难以实现。

Method: 提出了Isolation Network框架，通过解决二元分类任务量化测试样本与训练数据的分离难度；进一步提出DIsoN，在无法共享数据时通过交换模型参数实现比较。

Result: 在四个医学影像数据集上的12个OOD检测任务中，DIsoN表现优于现有方法，同时尊重数据隐私。

Conclusion: DIsoN为ML开发者提供了一种新的服务模式，即远程、安全地利用训练数据进行OOD检测。

Abstract: Safe deployment of machine learning (ML) models in safety-critical domains
such as medical imaging requires detecting inputs with characteristics not seen
during training, known as out-of-distribution (OOD) detection, to prevent
unreliable predictions. Effective OOD detection after deployment could benefit
from access to the training data, enabling direct comparison between test
samples and the training data distribution to identify differences.
State-of-the-art OOD detection methods, however, either discard training data
after deployment or assume that test samples and training data are centrally
stored together, an assumption that rarely holds in real-world settings. This
is because shipping training data with the deployed model is usually impossible
due to the size of training databases, as well as proprietary or privacy
constraints. We introduce the Isolation Network, an OOD detection framework
that quantifies the difficulty of separating a target test sample from the
training data by solving a binary classification task. We then propose
Decentralized Isolation Networks (DIsoN), which enables the comparison of
training and test data when data-sharing is impossible, by exchanging only
model parameters between the remote computational nodes of training and
deployment. We further extend DIsoN with class-conditioning, comparing a target
sample solely with training data of its predicted class. We evaluate DIsoN on
four medical imaging datasets (dermatology, chest X-ray, breast ultrasound,
histopathology) across 12 OOD detection tasks. DIsoN performs favorably against
existing methods while respecting data-privacy. This decentralized OOD
detection framework opens the way for a new type of service that ML developers
could provide along with their models: providing remote, secure utilization of
their training data for OOD detection services. Code will be available upon
acceptance at: *****

</details>


### [94] [Diffuse and Disperse: Image Generation with Representation Regularization](https://arxiv.org/abs/2506.09027)
*Runqian Wang,Kaiming He*

Main category: cs.CV

TL;DR: 提出了一种名为Dispersive Loss的简单即插即用正则化方法，用于改进扩散生成模型，无需正样本对或额外参数。


<details>
  <summary>Details</summary>
Motivation: 扩散生成模型通常缺乏显式正则化，且与表示学习进展脱节。本文旨在通过Dispersive Loss弥合这一差距。

Method: 提出Dispersive Loss，鼓励隐藏空间中的表示分散，类似于对比自监督学习，但无需正样本对。

Result: 在ImageNet数据集上评估，Dispersive Loss在多种模型中均优于基线方法。

Conclusion: Dispersive Loss为生成模型与表示学习之间的桥梁提供了简单有效的解决方案。

Abstract: The development of diffusion-based generative models over the past decade has
largely proceeded independently of progress in representation learning. These
diffusion models typically rely on regression-based objectives and generally
lack explicit regularization. In this work, we propose \textit{Dispersive
Loss}, a simple plug-and-play regularizer that effectively improves
diffusion-based generative models. Our loss function encourages internal
representations to disperse in the hidden space, analogous to contrastive
self-supervised learning, with the key distinction that it requires no positive
sample pairs and therefore does not interfere with the sampling process used
for regression. Compared to the recent method of representation alignment
(REPA), our approach is self-contained and minimalist, requiring no
pre-training, no additional parameters, and no external data. We evaluate
Dispersive Loss on the ImageNet dataset across a range of models and report
consistent improvements over widely used and strong baselines. We hope our work
will help bridge the gap between generative modeling and representation
learning.

</details>


### [95] [Princeton365: A Diverse Dataset with Accurate Camera Pose](https://arxiv.org/abs/2506.09035)
*Karhan Kayan,Stamatis Alexandropoulos,Rishabh Jain,Yiming Zuo,Erich Liang,Jia Deng*

Main category: cs.CV

TL;DR: Princeton365是一个大规模多样化的视频数据集，包含365个带有精确相机姿态的视频，填补了当前SLAM基准测试中精度与数据多样性之间的空白。


<details>
  <summary>Details</summary>
Motivation: 解决当前SLAM基准测试中精度与数据多样性不足的问题，并提供更全面的评估指标。

Method: 通过校准板和360度相机收集室内、室外和物体扫描视频，包含单目、立体RGB视频和IMU数据，并提出基于光流的新评估指标。

Result: 数据集支持更全面的SLAM方法评估，并提出了新的场景尺度感知指标和挑战性的新视角合成基准。

Conclusion: Princeton365为SLAM研究提供了更丰富的数据和评估工具，有助于分析方法的失败模式。

Abstract: We introduce Princeton365, a large-scale diverse dataset of 365 videos with
accurate camera pose. Our dataset bridges the gap between accuracy and data
diversity in current SLAM benchmarks by introducing a novel ground truth
collection framework that leverages calibration boards and a 360-camera. We
collect indoor, outdoor, and object scanning videos with synchronized monocular
and stereo RGB video outputs as well as IMU. We further propose a new scene
scale-aware evaluation metric for SLAM based on the the optical flow induced by
the camera pose estimation error. In contrast to the current metrics, our new
metric allows for comparison between the performance of SLAM methods across
scenes as opposed to existing metrics such as Average Trajectory Error (ATE),
allowing researchers to analyze the failure modes of their methods. We also
propose a challenging Novel View Synthesis benchmark that covers cases not
covered by current NVS benchmarks, such as fully non-Lambertian scenes with
360-degree camera trajectories. Please visit
https://princeton365.cs.princeton.edu for the dataset, code, videos, and
submission.

</details>


### [96] [Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better](https://arxiv.org/abs/2506.09040)
*Dianyi Wang,Wei Song,Yikun Wang,Siyuan Wang,Kaicheng Yu,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: 论文提出ASVR方法，通过自回归语义视觉重建联合学习视觉和文本模态，显著提升多模态理解性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型仅对文本序列进行自回归监督，未能充分利用视觉模态，导致图像理解不足。

Method: 提出ASVR方法，在统一自回归框架中联合学习视觉和文本模态，通过重建图像的语义表示而非原始外观。

Result: ASVR在多种数据规模和LLM骨干上表现优异，如LLaVA-1.5在14个多模态基准上平均提升5%。

Conclusion: 自回归重建语义表示能稳定提升多模态理解，优于原始视觉外观重建。

Abstract: Typical large vision-language models (LVLMs) apply autoregressive supervision
solely to textual sequences, without fully incorporating the visual modality
into the learning process. This results in three key limitations: (1) an
inability to utilize images without accompanying captions, (2) the risk that
captions omit critical visual details, and (3) the challenge that certain
vision-centric content cannot be adequately conveyed through text. As a result,
current LVLMs often prioritize vision-to-language alignment while potentially
overlooking fine-grained visual information. While some prior works have
explored autoregressive image generation, effectively leveraging autoregressive
visual supervision to enhance image understanding remains an open challenge. In
this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),
which enables joint learning of visual and textual modalities within a unified
autoregressive framework. We show that autoregressively reconstructing the raw
visual appearance of images does not enhance and may even impair multimodal
understanding. In contrast, autoregressively reconstructing the semantic
representation of images consistently improves comprehension. Notably, we find
that even when models are given continuous image features as input, they can
effectively reconstruct discrete semantic tokens, resulting in stable and
consistent improvements across a wide range of multimodal understanding
benchmarks. Our approach delivers significant performance gains across varying
data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves
LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is
available at https://github.com/AlenjandroWang/ASVR.

</details>


### [97] [Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models](https://arxiv.org/abs/2506.09042)
*Xuanchi Ren,Yifan Lu,Tianshi Cao,Ruiyuan Gao,Shengyu Huang,Amirmojtaba Sabour,Tianchang Shen,Tobias Pfaff,Jay Zhangjie Wu,Runjian Chen,Seung Wook Kim,Jun Gao,Laura Leal-Taixe,Mike Chen,Sanja Fidler,Huan Ling*

Main category: cs.CV

TL;DR: 论文提出了一种名为Cosmos-Drive-Dreams的合成数据生成（SDG）管道，用于生成具有挑战性的驾驶场景，以解决真实数据收集和标注的高成本问题。


<details>
  <summary>Details</summary>
Motivation: 为自动驾驶系统（AV）收集和标注真实数据耗时且昂贵，尤其是难以捕捉对训练和测试至关重要的罕见边缘案例。

Method: 利用基于NVIDIA Cosmos世界基础模型的Cosmos-Drive套件，生成可控、高保真、多视角且时空一致的驾驶视频。

Result: 生成的合成数据有助于缓解长尾分布问题，并在3D车道检测、3D物体检测和驾驶策略学习等下游任务中提升泛化能力。

Conclusion: Cosmos-Drive-Dreams为自动驾驶领域提供了一种高效的数据生成工具，并开源了管道工具包、数据集和模型权重。

Abstract: Collecting and annotating real-world data for safety-critical physical AI
systems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is
especially challenging to capture rare edge cases, which play a critical role
in training and testing of an AV system. To address this challenge, we
introduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline
that aims to generate challenging scenarios to facilitate downstream tasks such
as perception and driving policy training. Powering this pipeline is
Cosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation
model for the driving domain and are capable of controllable, high-fidelity,
multi-view, and spatiotemporally consistent driving video generation. We
showcase the utility of these models by applying Cosmos-Drive-Dreams to scale
the quantity and diversity of driving datasets with high-fidelity and
challenging scenarios. Experimentally, we demonstrate that our generated data
helps in mitigating long-tail distribution problems and enhances generalization
in downstream tasks such as 3D lane detection, 3D object detection and driving
policy learning. We open source our pipeline toolkit, dataset and model weights
through the NVIDIA's Cosmos platform.
  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams

</details>


### [98] [MagCache: Fast Video Generation with Magnitude-Aware Cache](https://arxiv.org/abs/2506.09045)
*Zehong Ma,Longhui Wei,Feng Wang,Shiliang Zhang,Qi Tian*

Main category: cs.CV

TL;DR: 论文提出了一种基于统一幅度规律的视频扩散模型加速方法MagCache，无需大量校准样本即可实现高效加速。


<details>
  <summary>Details</summary>
Motivation: 现有加速技术依赖统一启发式方法或时间嵌入变体，需大量校准且易因提示特定过拟合导致输出不一致。

Method: 发现幅度比在多数时间步单调递减，提出MagCache，通过误差建模和自适应缓存策略跳过不重要时间步。

Result: MagCache在Open-Sora和Wan 2.1上分别实现2.1x和2.68x加速，视觉保真度优于现有方法。

Conclusion: MagCache显著提升性能，优于现有方法，且仅需单一样本校准。

Abstract: Existing acceleration techniques for video diffusion models often rely on
uniform heuristics or time-embedding variants to skip timesteps and reuse
cached features. These approaches typically require extensive calibration with
curated prompts and risk inconsistent outputs due to prompt-specific
overfitting. In this paper, we introduce a novel and robust discovery: a
unified magnitude law observed across different models and prompts.
Specifically, the magnitude ratio of successive residual outputs decreases
monotonically and steadily in most timesteps while rapidly in the last several
steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache)
that adaptively skips unimportant timesteps using an error modeling mechanism
and adaptive caching strategy. Unlike existing methods requiring dozens of
curated samples for calibration, MagCache only requires a single sample for
calibration. Experimental results show that MagCache achieves 2.1x and 2.68x
speedups on Open-Sora and Wan 2.1, respectively, while preserving superior
visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM,
and PSNR, under comparable computational budgets.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [99] [A System for Accurate Tracking and Video Recordings of Rodent Eye Movements using Convolutional Neural Networks for Biomedical Image Segmentation](https://arxiv.org/abs/2506.08183)
*Isha Puri,David Cox*

Main category: eess.IV

TL;DR: 提出了一种灵活、鲁棒且高精度的模型，用于啮齿类动物瞳孔和角膜反射识别，解决了现有技术未考虑啮齿类动物眼部独特特征的问题。


<details>
  <summary>Details</summary>
Motivation: 啮齿类动物是神经科学和视觉科学研究的常用对象，但现有眼动追踪技术主要针对人类眼睛，未考虑啮齿类动物眼部特征（如参数变异性、周围毛发多、尺寸小）。

Method: 采用基于卷积神经网络（CNN）的生物医学图像分割架构，支持增量训练以应对眼部参数变异性，并结合自动红外视频眼动记录系统。

Result: 该方法首次展示了高精度且实用的瞳孔和角膜反射识别技术，为啮齿类动物眼动追踪提供了先进工具。

Conclusion: 新方法与自动红外视频系统结合，为啮齿类动物的神经科学和视觉科学研究提供了最先进的眼动追踪技术。

Abstract: Research in neuroscience and vision science relies heavily on careful
measurements of animal subject's gaze direction. Rodents are the most widely
studied animal subjects for such research because of their economic advantage
and hardiness. Recently, video based eye trackers that use image processing
techniques have become a popular option for gaze tracking because they are easy
to use and are completely noninvasive. Although significant progress has been
made in improving the accuracy and robustness of eye tracking algorithms,
unfortunately, almost all of the techniques have focused on human eyes, which
does not account for the unique characteristics of the rodent eye images, e.g.,
variability in eye parameters, abundance of surrounding hair, and their small
size. To overcome these unique challenges, this work presents a flexible,
robust, and highly accurate model for pupil and corneal reflection
identification in rodent gaze determination that can be incrementally trained
to account for variability in eye parameters encountered in the field. To the
best of our knowledge, this is the first paper that demonstrates a highly
accurate and practical biomedical image segmentation based convolutional neural
network architecture for pupil and corneal reflection identification in eye
images. This new method, in conjunction with our automated infrared videobased
eye recording system, offers the state of the art technology in eye tracking
for neuroscience and vision science research for rodents.

</details>


### [100] [Snap-and-tune: combining deep learning and test-time optimization for high-fidelity cardiovascular volumetric meshing](https://arxiv.org/abs/2506.08280)
*Daniel H. Pak,Shubh Thaker,Kyle Baylous,Xiaoran Zhang,Danny Bluestein,James S. Duncan*

Main category: eess.IV

TL;DR: 提出了一种结合深度学习和测试时优化的“snap-and-tune”策略，用于医学图像的体积网格生成，显著提高了空间精度和网格质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的模板变形方法在高曲率区域灵活性不足且部分间距不真实，需要改进。

Method: 采用“snap-and-tune”策略，先通过深度学习快速拟合初始形状，再通过测试时优化进行细节修正。

Result: 方法显著提升了空间精度和网格质量，且无需额外训练标签。

Conclusion: 该方法在固体力学模拟中展示了多功能性和实用性，代码已开源。

Abstract: High-quality volumetric meshing from medical images is a key bottleneck for
physics-based simulations in personalized medicine. For volumetric meshing of
complex medical structures, recent studies have often utilized deep learning
(DL)-based template deformation approaches to enable fast test-time generation
with high spatial accuracy. However, these approaches still exhibit
limitations, such as limited flexibility at high-curvature areas and
unrealistic inter-part distances. In this study, we introduce a simple yet
effective snap-and-tune strategy that sequentially applies DL and test-time
optimization, which combines fast initial shape fitting with more detailed
sample-specific mesh corrections. Our method provides significant improvements
in both spatial accuracy and mesh quality, while being fully automated and
requiring no additional training labels. Finally, we demonstrate the
versatility and usefulness of our newly generated meshes via solid mechanics
simulations in two different software platforms. Our code is available at
https://github.com/danpak94/Deep-Cardiac-Volumetric-Mesh.

</details>


### [101] [Plug-and-Play Linear Attention for Pre-trained Image and Video Restoration Models](https://arxiv.org/abs/2506.08520)
*Srinivasan Kidambi,Pravin Nair*

Main category: eess.IV

TL;DR: PnP-Nystra是一种基于Nyström的线性自注意力近似方法，作为即插即用模块，无需重新训练即可集成到预训练模型中，显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 多头自注意力（MHSA）的二次复杂度在实时和资源受限环境中成为计算瓶颈，需高效替代方案。

Method: 提出PnP-Nystra，基于Nyström方法线性近似自注意力，作为即插即用模块直接替换MHSA。

Result: 在图像和视频修复任务中，PnP-Nystra在GPU和CPU上分别实现2-4倍和2-5倍加速，PSNR最大仅下降1.5 dB。

Conclusion: PnP-Nystra是首个无需训练的线性注意力替代方案，显著提升效率且性能损失极小。

Abstract: Multi-head self-attention (MHSA) has become a core component in modern
computer vision models. However, its quadratic complexity with respect to input
length poses a significant computational bottleneck in real-time and resource
constrained environments. We propose PnP-Nystra, a Nystr\"om based linear
approximation of self-attention, developed as a plug-and-play (PnP) module that
can be integrated into the pre-trained image and video restoration models
without retraining. As a drop-in replacement for MHSA, PnP-Nystra enables
efficient acceleration in various window-based transformer architectures,
including SwinIR, Uformer, and RVRT. Our experiments across diverse image and
video restoration tasks, including denoising, deblurring, and super-resolution,
demonstrate that PnP-Nystra achieves a 2-4x speed-up on an NVIDIA RTX 4090 GPU
and a 2-5x speed-up on CPU inference. Despite these significant gains, the
method incurs a maximum PSNR drop of only 1.5 dB across all evaluated tasks. To
the best of our knowledge, we are the first to demonstrate a linear attention
functioning as a training-free substitute for MHSA in restoration models.

</details>


### [102] [DCD: A Semantic Segmentation Model for Fetal Ultrasound Four-Chamber View](https://arxiv.org/abs/2506.08534)
*Donglian Li,Hui Guo,Minglang Chen,Huizhen Chen,Jialing Chen,Bocheng Liang,Pengchen Liang,Ying Tan*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的模型DCD，用于胎儿超声心动图A4C视图的自动分割，解决了超声伪影和边界模糊等问题。


<details>
  <summary>Details</summary>
Motivation: 胎儿超声心动图中A4C视图的精确分割对先天性心脏病早期诊断至关重要，但现有方法受限于伪影和噪声。

Method: DCD模型结合了Dense ASPP模块和CBAM模块，实现了多尺度特征提取和自适应特征增强。

Result: DCD模型实现了精确且鲁棒的分割，提升了产前心脏评估的准确性。

Conclusion: DCD模型为胎儿心脏结构的自动分割提供了高效解决方案，有助于减轻医生负担并提高诊断效率。

Abstract: Accurate segmentation of anatomical structures in the apical four-chamber
(A4C) view of fetal echocardiography is essential for early diagnosis and
prenatal evaluation of congenital heart disease (CHD). However, precise
segmentation remains challenging due to ultrasound artifacts, speckle noise,
anatomical variability, and boundary ambiguity across different gestational
stages. To reduce the workload of sonographers and enhance segmentation
accuracy, we propose DCD, an advanced deep learning-based model for automatic
segmentation of key anatomical structures in the fetal A4C view. Our model
incorporates a Dense Atrous Spatial Pyramid Pooling (Dense ASPP) module,
enabling superior multi-scale feature extraction, and a Convolutional Block
Attention Module (CBAM) to enhance adaptive feature representation. By
effectively capturing both local and global contextual information, DCD
achieves precise and robust segmentation, contributing to improved prenatal
cardiac assessment.

</details>


### [103] [Biologically Inspired Deep Learning Approaches for Fetal Ultrasound Image Classification](https://arxiv.org/abs/2506.08623)
*Rinat Prochii,Elizaveta Dakhova,Pavel Birulin,Maxim Sharaev*

Main category: eess.IV

TL;DR: 提出了一种基于生物视觉系统启发的深度学习集成框架，用于同时区分16种胎儿结构，在复杂临床数据中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决胎儿超声图像分类中的低质量、高类内变异性和类别不平衡问题。

Method: 采用双分支（浅层路径和详细路径）的模块化架构，结合EfficientNet-B0和EfficientNet-B6模型，使用LDAM-Focal损失函数。

Result: 在5298张临床图像上，90%的器官分类准确率>0.75，75%的器官>0.85，性能优于现有方法。

Conclusion: 生物启发的模块化堆叠方法在复杂临床环境中具有鲁棒性和可扩展性。

Abstract: Accurate classification of second-trimester fetal ultrasound images remains
challenging due to low image quality, high intra-class variability, and
significant class imbalance. In this work, we introduce a simple yet powerful,
biologically inspired deep learning ensemble framework that-unlike prior
studies focused on only a handful of anatomical targets-simultaneously
distinguishes 16 fetal structures. Drawing on the hierarchical, modular
organization of biological vision systems, our model stacks two complementary
branches (a "shallow" path for coarse, low-resolution cues and a "detailed"
path for fine, high-resolution features), concatenating their outputs for final
prediction. To our knowledge, no existing method has addressed such a large
number of classes with a comparably lightweight architecture. We trained and
evaluated on 5,298 routinely acquired clinical images (annotated by three
experts and reconciled via Dawid-Skene), reflecting real-world noise and
variability rather than a "cleaned" dataset. Despite this complexity, our
ensemble (EfficientNet-B0 + EfficientNet-B6 with LDAM-Focal loss) identifies
90% of organs with accuracy > 0.75 and 75% of organs with accuracy >
0.85-performance competitive with more elaborate models applied to far fewer
categories. These results demonstrate that biologically inspired modular
stacking can yield robust, scalable fetal anatomy recognition in challenging
clinical settings.

</details>


### [104] [MAMBO: High-Resolution Generative Approach for Mammography Images](https://arxiv.org/abs/2506.08677)
*Milica Škipina,Nikola Jovišić,Nicola Dall'Asen,Vanja Švenda,Anil Osman Tur,Slobodan Ilić,Elisa Ricci,Dubravko Ćulibrk*

Main category: eess.IV

TL;DR: 论文提出了一种名为MAMBO的基于扩散模型的乳腺X光图像生成方法，旨在解决数据隐私和多样性不足的问题，生成高分辨率图像以辅助乳腺癌诊断。


<details>
  <summary>Details</summary>
Motivation: 乳腺X光检查是乳腺癌检测的金标准，但AI训练需要大量多样化的数据，而隐私和伦理问题限制了数据获取。

Method: MAMBO采用基于块的扩散模型，结合局部和全局上下文信息，生成高达3840x3840像素的高分辨率乳腺X光图像。

Result: 实验表明，MAMBO能生成逼真的高分辨率图像，可用于分类模型训练和异常检测，提升诊断准确性。

Conclusion: MAMBO为乳腺X光分析提供了高效的数据增强工具，有望推动更准确的早期病变检测。

Abstract: Mammography is the gold standard for the detection and diagnosis of breast
cancer. This procedure can be significantly enhanced with Artificial
Intelligence (AI)-based software, which assists radiologists in identifying
abnormalities. However, training AI systems requires large and diverse
datasets, which are often difficult to obtain due to privacy and ethical
constraints. To address this issue, the paper introduces MAMmography ensemBle
mOdel (MAMBO), a novel patch-based diffusion approach designed to generate
full-resolution mammograms. Diffusion models have shown breakthrough results in
realistic image generation, yet few studies have focused on mammograms, and
none have successfully generated high-resolution outputs required to capture
fine-grained features of small lesions. To achieve this, MAMBO integrates
separate diffusion models to capture both local and global (image-level)
contexts. The contextual information is then fed into the final patch-based
model, significantly aiding the noise removal process. This thoughtful design
enables MAMBO to generate highly realistic mammograms of up to 3840x3840
pixels. Importantly, this approach can be used to enhance the training of
classification models and extended to anomaly detection. Experiments, both
numerical and radiologist validation, assess MAMBO's capabilities in image
generation, super-resolution, and anomaly detection, highlighting its potential
to enhance mammography analysis for more accurate diagnoses and earlier lesion
detection.

</details>


### [105] [Enhancing Synthetic CT from CBCT via Multimodal Fusion: A Study on the Impact of CBCT Quality and Alignment](https://arxiv.org/abs/2506.08716)
*Maximilian Tschuchnig,Lukas Lamminger,Philipp Steininger,Michael Gadermayr*

Main category: eess.IV

TL;DR: 通过多模态学习提升CBCT到CT的合成效果，显著改善了低质量CBCT-CT对齐情况下的图像质量。


<details>
  <summary>Details</summary>
Motivation: CBCT虽然快速且辐射低，但存在明显伪影，影响视觉质量。合成CT（sCT）是一种解决方案，但需进一步优化。

Method: 采用多模态学习方法，结合术中CBCT和术前CT数据，生成更高质量的sCT。

Result: 多模态sCT在低质量CBCT-CT对齐情况下表现最佳，且结果在真实临床数据中可复现。

Conclusion: 多模态学习能有效提升sCT生成质量，尤其在CBCT质量较差时效果显著。

Abstract: Cone-Beam Computed Tomography (CBCT) is widely used for real-time
intraoperative imaging due to its low radiation dose and high acquisition
speed. However, despite its high resolution, CBCT suffers from significant
artifacts and thereby lower visual quality, compared to conventional Computed
Tomography (CT). A recent approach to mitigate these artifacts is synthetic CT
(sCT) generation, translating CBCT volumes into the CT domain. In this work, we
enhance sCT generation through multimodal learning, integrating intraoperative
CBCT with preoperative CT. Beyond validation on two real-world datasets, we use
a versatile synthetic dataset, to analyze how CBCT-CT alignment and CBCT
quality affect sCT quality. The results demonstrate that multimodal sCT
consistently outperform unimodal baselines, with the most significant gains
observed in well-aligned, low-quality CBCT-CT cases. Finally, we demonstrate
that these findings are highly reproducible in real-world clinical datasets.

</details>


### [106] [Diver-Robot Communication Dataset for Underwater Hand Gesture Recognition](https://arxiv.org/abs/2506.08974)
*Igor Kvasić,Derek Orbaugh Antillon,Ðula Nađ,Christopher Walker,Iain Anderson,Nikola Mišković*

Main category: eess.IV

TL;DR: 本文介绍了一个用于水下人机交互的潜水手势数据集，旨在通过视觉检测潜水手势实现与潜水员的通信，并与智能手势识别手套的性能进行比较。


<details>
  <summary>Details</summary>
Motivation: 研究通过视觉检测潜水手势作为水下自主车辆（AUV）与潜水员通信的潜力，并比较其与智能手套方法的效率。

Method: 数据集包含30,000多帧水下图像，记录了近900个手势，同时使用智能手套（配备弹性体传感器和声学传输）进行并行记录。

Result: 数据集提供了手套手势识别的统计数据，包括反应时间、手势执行时间、识别成功率等，为不同能见度条件下的视觉手势识别技术提供了基准。

Conclusion: 该数据集为比较不同能见度条件下视觉潜水手势识别技术的性能提供了良好基础。

Abstract: In this paper, we present a dataset of diving gesture images used for
human-robot interaction underwater. By offering this open access dataset, the
paper aims at investigating the potential of using visual detection of diving
gestures from an autonomous underwater vehicle (AUV) as a form of communication
with a human diver. In addition to the image recording, the same dataset was
recorded using a smart gesture recognition glove. The glove uses elastomer
sensors and on-board processing to determine the selected gesture and transmit
the command associated with the gesture to the AUV via acoustics. Although this
method can be used under different visibility conditions and even without line
of sight, it introduces a communication delay required for the acoustic
transmission of the gesture command. To compare efficiency, the glove was
equipped with visual markers proposed in a gesture-based language called
CADDIAN and recorded with an underwater camera in parallel to the glove's
onboard recognition process. The dataset contains over 30,000 underwater frames
of nearly 900 individual gestures annotated in corresponding snippet folders.
The dataset was recorded in a balanced ratio with five different divers in sea
and five different divers in pool conditions, with gestures recorded at 1, 2
and 3 metres from the camera. The glove gesture recognition statistics are
reported in terms of average diver reaction time, average time taken to perform
a gesture, recognition success rate, transmission times and more. The dataset
presented should provide a good baseline for comparing the performance of state
of the art visual diving gesture recognition techniques under different
visibility conditions.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [107] [Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers](https://arxiv.org/abs/2506.08043)
*Ashkan Shahbazi,Kyvia Pereira,Jon S. Heiselman,Elaheh Akbari,Annie C. Benson,Sepehr Seifi,Xinyuan Liu,Garrison L. Johnston,Erwin Terpstra,Anne Draaisma,Jan-Jaap Severes,Jie Ying Wu,Nabil Simaan,Michael L. Miga,Soheil Kolouri*

Main category: cs.GR

TL;DR: 提出了一种基于物理信息神经模拟器的新方法，利用Kelvinlet先验改进软组织变形模拟，实现高精度和实时性能。


<details>
  <summary>Details</summary>
Motivation: 快速准确的软组织变形模拟对外科机器人和医学培训至关重要。

Method: 结合Kelvinlet先验和神经模拟器，利用大规模FEM模拟改进神经网络预测。

Result: 方法在多种架构中提高了预测精度和物理一致性，同时保持低延迟。

Conclusion: Kelvinlet增强学习是外科应用中实时、物理感知软组织模拟的高效策略。

Abstract: Fast and accurate simulation of soft tissue deformation is a critical factor
for surgical robotics and medical training. In this paper, we introduce a novel
physics-informed neural simulator that approximates soft tissue deformations in
a realistic and real-time manner. Our framework integrates Kelvinlet-based
priors into neural simulators, making it the first approach to leverage
Kelvinlets for residual learning and regularization in data-driven soft tissue
modeling. By incorporating large-scale Finite Element Method (FEM) simulations
of both linear and nonlinear soft tissue responses, our method improves neural
network predictions across diverse architectures, enhancing accuracy and
physical consistency while maintaining low latency for real-time performance.
We demonstrate the effectiveness of our approach by performing accurate
surgical maneuvers that simulate the use of standard laparoscopic tissue
grasping tools with high fidelity. These results establish Kelvinlet-augmented
learning as a powerful and efficient strategy for real-time, physics-aware soft
tissue simulation in surgical applications.

</details>


### [108] [A Real-time 3D Desktop Display](https://arxiv.org/abs/2506.08064)
*Livio Tenze,Enrique Canessa*

Main category: cs.GR

TL;DR: altiro3D C++ Library的扩展版本，支持从2D图像或视频流实时合成光场，用于3D体验。


<details>
  <summary>Details</summary>
Motivation: 旨在处理3D视频流，从2D图像或视频文件中生成光场，以提供更真实的3D体验。

Method: 使用MiDaS CNN从单张2D图像提取深度图，结合AI技术提升性能，支持多平台GUI。

Result: 扩展后的altiro3D能处理标准图像、视频流或桌面屏幕区域，并渲染为3D。

Conclusion: altiro3D 2.0.0版本成功实现了实时3D光场合成，支持多种输入源和输出设备。

Abstract: A new extended version of the altiro3D C++ Library -- initially developed to
get glass-free holographic displays starting from 2D images -- is here
introduced aiming to deal with 3D video streams from either 2D webcam images or
flat video files. These streams are processed in real-time to synthesize
light-fields (in Native format) and feed realistic 3D experiences. The core
function needed to recreate multiviews consists on the use of MiDaS
Convolutional Neural Network (CNN), which allows to extract a depth map from a
single 2D image. Artificial Intelligence (AI) computing techniques are applied
to improve the overall performance of the extended altiro3D Library. Thus,
altiro3D can now treat standard images, video streams or screen portions of a
Desktop where other apps may be also running (like web browsers, video chats,
etc) and render them into 3D. To achieve the latter, a screen region need to be
selected in order to feed the output directly into a light-field 3D device such
as Looking Glass (LG) Portrait. In order to simplify the acquisition of a
Desktop screen area by the user, a multi-platform Graphical User Interface has
been also implemented. Sources available at:
https://github.com/canessae/altiro3D/releases/tag/2.0.0

</details>


### [109] [GATE: Geometry-Aware Trained Encoding](https://arxiv.org/abs/2506.08161)
*Jakub Bokšanský,Daniel Meister,Carsten Benthin*

Main category: cs.GR

TL;DR: 提出了一种名为GATE的几何感知编码方法，将特征向量存储在三角网格表面，适用于神经渲染相关算法，并解决了哈希编码的局限性。


<details>
  <summary>Details</summary>
Motivation: 输入参数编码是神经网络算法的基本组成部分，其目标是将输入数据映射到高维空间，通常由训练的特征向量支持。这种映射对神经网络的效率和近似质量至关重要。

Method: 提出GATE编码方法，将特征向量存储在三角网格表面，利用网格颜色解耦特征向量密度与几何密度，并提供对神经网络训练和自适应细节级别的更精细控制。

Result: GATE适用于神经渲染相关算法（如神经辐射缓存），避免了哈希编码的局限性（如哈希冲突、分辨率与场景大小的选择、内存访问不一致）。

Conclusion: GATE是一种有效的几何感知编码方法，解决了哈希编码的局限性，适用于神经渲染相关算法。

Abstract: The encoding of input parameters is one of the fundamental building blocks of
neural network algorithms. Its goal is to map the input data to a
higher-dimensional space, typically supported by trained feature vectors. The
mapping is crucial for the efficiency and approximation quality of neural
networks. We propose a novel geometry-aware encoding called GATE that stores
feature vectors on the surface of triangular meshes. Our encoding is suitable
for neural rendering-related algorithms, for example, neural radiance caching.
It also avoids limitations of previous hash-based encoding schemes, such as
hash collisions, selection of resolution versus scene size, and divergent
memory access. Our approach decouples feature vector density from geometry
density using mesh colors, while allowing for finer control over neural network
training and adaptive level-of-detail.

</details>


### [110] [Solving partial differential equations in participating media](https://arxiv.org/abs/2506.08237)
*Bailey Miller,Rohan Sawhney,Keenan Crane,Ioannis Gkioulekas*

Main category: cs.GR

TL;DR: 提出两种新算法（volumetric walk on spheres和volumetric walk on stars），用于在复杂微粒子几何的参与介质中高效求解线性椭圆PDE（如拉普拉斯方程），优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 复杂微粒子几何的PDE求解难以显式建模，需通过统计特性（如粒子密度）进行随机建模。

Method: 基于指数介质的特性，提出两种蒙特卡洛算法，实现离散化自由的PDE模拟。

Result: 新算法在拉普拉斯边界值问题中比传统方法（如集合平均和均匀化）更准确高效。

Conclusion: 通过统计建模和蒙特卡洛方法，实现了复杂微粒子几何中PDE的高效求解。

Abstract: We consider the problem of solving partial differential equations (PDEs) in
domains with complex microparticle geometry that is impractical, or
intractable, to model explicitly. Drawing inspiration from volume rendering, we
propose tackling this problem by treating the domain as a participating medium
that models microparticle geometry stochastically, through aggregate
statistical properties (e.g., particle density). We first introduce the problem
setting of PDE simulation in participating media. We then specialize to
exponential media and describe the properties that make them an attractive
model of microparticle geometry for PDE simulation problems. We use these
properties to develop two new algorithms, volumetric walk on spheres and
volumetric walk on stars, that generalize previous Monte Carlo algorithms to
enable efficient and discretization-free simulation of linear elliptic PDEs
(e.g., Laplace) in participating media. We demonstrate experimentally that our
algorithms can solve Laplace boundary value problems with complex microparticle
geometry more accurately and more efficiently than previous approaches, such as
ensemble averaging and homogenization.

</details>


### [111] [Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos](https://arxiv.org/abs/2506.08334)
*Weikun Peng,Jun Lv,Cewu Lu,Manolis Savva*

Main category: cs.GR

TL;DR: 提出了一种从动态RGBD视频中重建关节物体的粗到细框架，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 关节物体在日常生活中普遍存在，但现有方法需要精心采集的数据，限制了其实际应用和扩展性。

Method: 采用粗到细框架，从动态RGBD视频中推断关节参数并分割可移动部分。

Result: 在合成和真实数据集上均显著优于现有方法，能够重建不同类别的关节物体。

Conclusion: 该方法为从动态视频中重建关节物体提供了一种实用且可扩展的解决方案。

Abstract: Articulated objects are prevalent in daily life. Understanding their
kinematic structure and reconstructing them have numerous applications in
embodied AI and robotics. However, current methods require carefully captured
data for training or inference, preventing practical, scalable, and
generalizable reconstruction of articulated objects. We focus on reconstruction
of an articulated object from a casually captured RGBD video shot with a
hand-held camera. A casually captured video of an interaction with an
articulated object is easy to acquire at scale using smartphones. However, this
setting is quite challenging, as the object and camera move simultaneously and
there are significant occlusions as the person interacts with the object. To
tackle these challenges, we introduce a coarse-to-fine framework that infers
joint parameters and segments movable parts of the object from a dynamic RGBD
video. To evaluate our method under this new setting, we build a 20$\times$
larger synthetic dataset of 784 videos containing 284 objects across 11
categories. We compare our approach with existing methods that also take video
as input. Experiments show that our method can reconstruct synthetic and real
articulated objects across different categories from dynamic RGBD videos,
outperforming existing methods significantly.

</details>


### [112] [Complex-Valued Holographic Radiance Fields](https://arxiv.org/abs/2506.08350)
*Yicheng Zhan,Dong-Ha Shin,Seung-Hwan Baek,Kaan Akşit*

Main category: cs.GR

TL;DR: 提出一种基于复值高斯基元的3D全息场景表示方法，显著提升渲染速度并保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 为全息显示提供物理上合理的渲染，需建模光的完整属性（振幅和相位），传统方法依赖强度中间表示且计算成本高。

Method: 利用RGBD多视图图像，直接优化复值高斯基元作为3D全息场景表示，避免昂贵的全息图重新优化。

Result: 相比现有方法，速度提升30x-10,000倍，图像质量相当。

Conclusion: 该方法首次实现几何对齐且物理合理的全息场景表示，为全息显示技术迈出重要一步。

Abstract: Modeling the full properties of light, including both amplitude and phase, in
3D representations is crucial for advancing physically plausible rendering,
particularly in holographic displays. To support these features, we propose a
novel representation that optimizes 3D scenes without relying on
intensity-based intermediaries. We reformulate 3D Gaussian splatting with
complex-valued Gaussian primitives, expanding support for rendering with light
waves. By leveraging RGBD multi-view images, our method directly optimizes
complex-valued Gaussians as a 3D holographic scene representation. This
eliminates the need for computationally expensive hologram re-optimization.
Compared with state-of-the-art methods, our method achieves 30x-10,000x speed
improvements while maintaining on-par image quality, representing a first step
towards geometrically aligned, physically plausible holographic scene
representations.

</details>


### [113] [Fine-Grained Spatially Varying Material Selection in Images](https://arxiv.org/abs/2506.09023)
*Julia Guerrero-Viu,Michael Fischer,Iliyan Georgiev,Elena Garces,Diego Gutierrez,Belen Masia,Valentin Deschaintre*

Main category: cs.GR

TL;DR: 提出了一种基于ViT的材料选择方法，对光照和反射变化鲁棒，支持纹理和子纹理两级选择。


<details>
  <summary>Details</summary>
Motivation: 图像编辑中材料选择是关键步骤，但现有方法对光照和反射变化敏感，且缺乏多级选择能力。

Method: 利用ViT模型特征，提出多分辨率处理策略，结合DuMaS数据集进行纹理和子纹理级选择。

Result: 方法在合成图像上表现优于现有方法，选择结果更精细稳定。

Conclusion: 该方法为图像编辑提供了更鲁棒和灵活的材料选择工具。

Abstract: Selection is the first step in many image editing processes, enabling faster
and simpler modifications of all pixels sharing a common modality. In this
work, we present a method for material selection in images, robust to lighting
and reflectance variations, which can be used for downstream editing tasks. We
rely on vision transformer (ViT) models and leverage their features for
selection, proposing a multi-resolution processing strategy that yields finer
and more stable selection results than prior methods. Furthermore, we enable
selection at two levels: texture and subtexture, leveraging a new two-level
material selection (DuMaS) dataset which includes dense annotations for over
800,000 synthetic images, both on the texture and subtexture levels.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [114] [Designing lensless imaging systems to maximize information capture](https://arxiv.org/abs/2506.08513)
*Leyla A. Kabuli,Henry Pinkard,Eric Markley,Clara S. Hung,Laura Waller*

Main category: physics.optics

TL;DR: 本文通过互信息估计评估和设计光学编码器，优化无透镜成像系统的信息捕获能力，揭示编码器复用与对象稀疏性的关系，并提出信息最优编码器设计。


<details>
  <summary>Details</summary>
Motivation: 研究无透镜成像中光学编码器的信息捕获能力，探索对象稀疏性、编码器复用和噪声之间的相互依赖关系。

Method: 使用互信息估计评估和设计编码器，分析对象稀疏性与编码器复用的关系，并通过实验验证优化设计的编码器性能。

Result: 发现最优编码器设计应根据对象稀疏性调整复用程度，信息最优编码器能显著提升重建性能。

Conclusion: 为无透镜成像系统提供了设计和工程原则，并为研究一般复用系统提供了模型。

Abstract: Mask-based lensless imaging uses an optical encoder (e.g. a phase or
amplitude mask) to capture measurements, then a computational decoding
algorithm to reconstruct images. In this work, we evaluate and design encoders
based on the information content of their measurements using mutual information
estimation. With this approach, we formalize the object-dependent nature of
lensless imaging and study the interdependence between object sparsity, encoder
multiplexing, and noise. Our analysis reveals that optimal encoder designs
should tailor encoder multiplexing to object sparsity for maximum information
capture, and that all optimally-encoded measurements share the same level of
sparsity. Using mutual information-based optimization, we design
information-optimal encoders with improved downstream reconstruction
performance. We validate the benefits of reduced multiplexing for dense,
natural images by evaluating experimental lensless imaging systems directly
from captured measurements, without the need for image formation models,
reconstruction algorithms, or ground truth images. Our comprehensive analysis
establishes design and engineering principles for improving lensless imaging
systems, and offers a model for the study of general multiplexing systems,
especially those with object-dependent performance.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [115] [Normalized Radon Cumulative Distribution Transforms for Invariance and Robustness in Optimal Transport Based Image Classification](https://arxiv.org/abs/2506.08761)
*Matthias Beckmann,Robert Beinert,Jonas Bresch*

Main category: math.NA

TL;DR: 论文介绍了max-normalized R-CDT和mean-normalized R-CDT两种特征提取方法，用于解决图像分类中非仿射变形和脉冲噪声的问题，并通过实验验证了其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在图像分类任务中，尤其是小数据情况下，R-CDT是一种易于计算的特征提取器，但其在非仿射变形和脉冲噪声下的鲁棒性不足。本文旨在研究max-normalized R-CDT的鲁棒性，并进一步提出mean-normalized R-CDT以应对更广泛的图像变形。

Method: 提出了max-normalized R-CDT和mean-normalized R-CDT两种方法，分别基于Wasserstein-infinity距离和Wasserstein-2距离，以增强对非仿射变形和脉冲噪声的鲁棒性。

Result: 理论分析和数值实验表明，这两种方法在非仿射变形和脉冲噪声下均能保持线性可分性，且具有较高的鲁棒性。

Conclusion: max-normalized R-CDT和mean-normalized R-CDT是有效的特征提取方法，适用于处理图像分类中的非仿射变形和噪声问题。

Abstract: The Radon cumulative distribution transform (R-CDT), is an easy-to-compute
feature extractor that facilitates image classification tasks especially in the
small data regime. It is closely related to the sliced Wasserstein distance and
provably guaranties the linear separability of image classes that emerge from
translations or scalings. In many real-world applications, like the recognition
of watermarks in filigranology, however, the data is subject to general affine
transformations originating from the measurement process. To overcome this
issue, we recently introduced the so-called max-normalized R-CDT that only
requires elementary operations and guaranties the separability under arbitrary
affine transformations. The aim of this paper is to continue our study of the
max-normalized R-CDT especially with respect to its robustness against
non-affine image deformations. Our sensitivity analysis shows that its
separability properties are stable provided the Wasserstein-infinity distance
between the samples can be controlled. Since the Wasserstein-infinity distance
only allows small local image deformations, we moreover introduce a
mean-normalized version of the R-CDT. In this case, robustness relates to the
Wasserstein-2 distance and also covers image deformations caused by impulsive
noise for instance. Our theoretical results are supported by numerical
experiments showing the effectiveness of our novel feature extractors as well
as their robustness against local non-affine deformations and impulsive noise.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [116] [Aligning Proteins and Language: A Foundation Model for Protein Retrieval](https://arxiv.org/abs/2506.08023)
*Qifeng Wu,Zhengzhe Liu,Han Zhu,Yizhou Zhao,Daisuke Kihara,Min Xu*

Main category: q-bio.BM

TL;DR: 提出了一种基于CLIP风格的多模态框架，用于对齐3D蛋白质结构与功能注释，通过对比学习实现蛋白质检索。


<details>
  <summary>Details</summary>
Motivation: 利用视觉语言模型（VLMs）的最新进展，促进蛋白质结构的功能解释，特别是针对cryo-EM等结构测定方法生成的蛋白质。

Method: 采用对比学习框架，构建了一个包含约20万蛋白质-描述对的大规模数据集，并在PDB和EMDB数据集上评估模型。

Result: 模型在域内和跨数据库检索中均表现出良好的零样本检索性能。

Conclusion: 多模态基础模型在蛋白质结构-功能理解方面具有潜力。

Abstract: This paper aims to retrieve proteins with similar structures and semantics
from large-scale protein dataset, facilitating the functional interpretation of
protein structures derived by structural determination methods like
cryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of
vision-language models (VLMs), we propose a CLIP-style framework for aligning
3D protein structures with functional annotations using contrastive learning.
For model training, we propose a large-scale dataset of approximately 200,000
protein-caption pairs with rich functional descriptors. We evaluate our model
in both in-domain and more challenging cross-database retrieval on Protein Data
Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In
both cases, our approach demonstrates promising zero-shot retrieval
performance, highlighting the potential of multimodal foundation models for
structure-function understanding in protein biology.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [117] [Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models](https://arxiv.org/abs/2506.08480)
*Huixuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 本文指出现有文本到图像生成评估框架的不足，提出改进建议。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要依赖人类判断，忽略了评估框架的其他关键特性。

Method: 识别可靠评估的两个关键方面，实证分析当前主流框架的不足。

Result: 当前框架未能满足多样化的评估需求。

Conclusion: 提出改进图像-文本对齐评估的建议。

Abstract: Text-to-image models often struggle to generate images that precisely match
textual prompts. Prior research has extensively studied the evaluation of
image-text alignment in text-to-image generation. However, existing evaluations
primarily focus on agreement with human assessments, neglecting other critical
properties of a trustworthy evaluation framework. In this work, we first
identify two key aspects that a reliable evaluation should address. We then
empirically demonstrate that current mainstream evaluation frameworks fail to
fully satisfy these properties across a diverse range of metrics and models.
Finally, we propose recommendations for improving image-text alignment
evaluation.

</details>


### [118] [ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts](https://arxiv.org/abs/2506.08700)
*Ruiran Su,Jiasheng Si,Zhijiang Guo,Janet B. Pierrehumbert*

Main category: cs.CL

TL;DR: ClimateViz是首个大规模科学图表事实核查基准，包含49,862个与2,896个图表相关的标注数据。当前多模态语言模型在图表推理上表现不佳，最高准确率仅77.8%，远低于人类水平（92.7%）。


<details>
  <summary>Details</summary>
Motivation: 科学事实核查主要关注文本和表格，忽略了科学图表的重要性。ClimateViz填补了这一空白，提供专家标注的图表数据集。

Method: 构建ClimateViz数据集，包含图表和标注的关联数据，并评估多模态语言模型在零样本和少样本设置下的表现。

Result: 当前模型在图表推理上表现较差，最高准确率为77.8%，人类表现达92.7%。部分模型通过解释增强输出有所提升。

Conclusion: ClimateViz为科学图表事实核查提供了基准，揭示了当前模型的局限性，并呼吁进一步研究提升图表推理能力。

Abstract: Scientific fact-checking has mostly focused on text and tables, overlooking
scientific charts, which are key for presenting quantitative evidence and
statistical reasoning. We introduce ClimateViz, the first large-scale benchmark
for scientific fact-checking using expert-curated scientific charts. ClimateViz
contains 49,862 claims linked to 2,896 visualizations, each labeled as support,
refute, or not enough information. To improve interpretability, each example
includes structured knowledge graph explanations covering trends, comparisons,
and causal relations. We evaluate state-of-the-art multimodal language models,
including both proprietary and open-source systems, in zero-shot and few-shot
settings. Results show that current models struggle with chart-based reasoning:
even the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to
77.8 percent accuracy in label-only settings, far below human performance (89.3
and 92.7 percent). Explanation-augmented outputs improve performance in some
models. We released our dataset and code alongside the paper.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [119] [Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain](https://arxiv.org/abs/2506.08277)
*Subba Reddy Oota,Khushbu Pahwa,Prachi Jindal,Satya Sai Srinath Namburi,Maneesh Singh,Tanmoy Chakraborty,Bapi S. Raju,Manish Gupta*

Main category: q-bio.NC

TL;DR: 多模态大语言模型（MLLMs）在任务特定指令下生成的表示与大脑活动对齐更强，显著优于非指令调优模型。


<details>
  <summary>Details</summary>
Motivation: 填补现有研究对指令调优多模态模型在大脑对齐评估中的空白。

Method: 利用指令调优的MLLMs生成表示，预测自然电影观看时的神经活动。

Result: 指令调优视频MLLMs比非指令调优模型性能提升15%-20%，且任务表示清晰分离。

Conclusion: 任务特定指令显著提升MLLMs与大脑活动的对齐，为研究多模态信息处理提供新途径。

Abstract: Recent voxel-wise multimodal brain encoding studies have shown that
multimodal large language models (MLLMs) exhibit a higher degree of brain
alignment compared to unimodal models in both unimodal and multimodal stimulus
settings. More recently, instruction-tuned multimodal models have shown to
generate task-specific representations that align strongly with brain activity.
However, prior work evaluating the brain alignment of MLLMs has primarily
focused on unimodal settings or relied on non-instruction-tuned multimodal
models for multimodal stimuli. To address this gap, we investigated brain
alignment, that is, measuring the degree of predictivity of neural activity
recorded while participants were watching naturalistic movies (video along with
audio) with representations derived from MLLMs. We utilized
instruction-specific embeddings from six video and two audio instruction-tuned
MLLMs. Experiments with 13 video task-specific instructions show that
instruction-tuned video MLLMs significantly outperform non-instruction-tuned
multimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for
both video and audio tasks using language-guided instructions shows clear
disentanglement in task-specific representations from MLLMs, leading to precise
differentiation of multimodal functional processing in the brain. We also find
that MLLM layers align hierarchically with the brain, with early sensory areas
showing strong alignment with early layers, while higher-level visual and
language regions align more with middle to late layers. These findings provide
clear evidence for the role of task-specific instructions in improving the
alignment between brain activity and MLLMs, and open new avenues for mapping
joint information processing in both the systems. We make the code publicly
available [https://github.com/subbareddy248/mllm_videos].

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [120] [FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching](https://arxiv.org/abs/2506.08518)
*Sunny Gupta,Nikita Jangid,Shounak Das,Amit Sethi*

Main category: cs.AI

TL;DR: FedTAIL是一个联邦领域泛化框架，通过梯度对齐和锐度感知优化解决长尾分布和优化冲突问题，实现了在领域偏移和标签不平衡下的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长尾类分布和冲突优化目标下表现不佳，需要一种更稳定的泛化方法。

Method: 结合梯度一致性正则化、类感知锐度最小化和曲率感知动态加权，同时通过锐度感知扰动增强条件分布对齐。

Result: 在标准领域泛化基准测试中表现优异，尤其在领域偏移和标签不平衡情况下。

Conclusion: FedTAIL在集中式和联邦设置下均有效，验证了其方法的优越性。

Abstract: Domain Generalization (DG) seeks to train models that perform reliably on
unseen target domains without access to target data during training. While
recent progress in smoothing the loss landscape has improved generalization,
existing methods often falter under long-tailed class distributions and
conflicting optimization objectives. We introduce FedTAIL, a federated domain
generalization framework that explicitly addresses these challenges through
sharpness-guided, gradient-aligned optimization. Our method incorporates a
gradient coherence regularizer to mitigate conflicts between classification and
adversarial objectives, leading to more stable convergence. To combat class
imbalance, we perform class-wise sharpness minimization and propose a
curvature-aware dynamic weighting scheme that adaptively emphasizes
underrepresented tail classes. Furthermore, we enhance conditional distribution
alignment by integrating sharpness-aware perturbations into entropy
regularization, improving robustness under domain shift. FedTAIL unifies
optimization harmonization, class-aware regularization, and conditional
alignment into a scalable, federated-compatible framework. Extensive
evaluations across standard domain generalization benchmarks demonstrate that
FedTAIL achieves state-of-the-art performance, particularly in the presence of
domain shifts and label imbalance, validating its effectiveness in both
centralized and federated settings. Code: https://github.com/sunnyinAI/FedTail

</details>


### [121] [VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning](https://arxiv.org/abs/2506.09049)
*Li Kang,Xiufeng Song,Heng Zhou,Yiran Qin,Jie Yang,Xiaohong Liu,Philip Torr,Lei Bai,Zhenfei Yin*

Main category: cs.AI

TL;DR: VIKI-Bench是首个针对具身多智能体协作的分层基准，包含三个结构化层级：智能体激活、任务规划和轨迹感知。VIKI-R框架通过两阶段方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 协调动态环境中的多智能体是AI核心挑战，现有视觉语言模型（VLM）方法对多样化具身支持有限。

Method: 提出VIKI-Bench基准和VIKI-R框架，后者结合预训练VLM微调和强化学习。

Result: VIKI-R在所有任务层级上显著优于基线方法，并促进异构智能体的组合协作模式。

Conclusion: VIKI-Bench和VIKI-R为具身AI系统中的多智能体视觉驱动协作提供了统一测试平台和方法。

Abstract: Coordinating multiple embodied agents in dynamic environments remains a core
challenge in artificial intelligence, requiring both perception-driven
reasoning and scalable cooperation strategies. While recent works have
leveraged large language models (LLMs) for multi-agent planning, a few have
begun to explore vision-language models (VLMs) for visual reasoning. However,
these VLM-based approaches remain limited in their support for diverse
embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical
benchmark tailored for embodied multi-agent cooperation, featuring three
structured levels: agent activation, task planning, and trajectory perception.
VIKI-Bench includes diverse robot embodiments, multi-view visual observations,
and structured supervision signals to evaluate reasoning grounded in visual
inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a
two-stage framework that fine-tunes a pretrained vision-language model (VLM)
using Chain-of-Thought annotated demonstrations, followed by reinforcement
learning under multi-level reward signals. Our extensive experiments show that
VIKI-R significantly outperforms baselines method across all task levels.
Furthermore, we show that reinforcement learning enables the emergence of
compositional cooperation patterns among heterogeneous agents. Together,
VIKI-Bench and VIKI-R offer a unified testbed and method for advancing
multi-agent, visual-driven cooperation in embodied AI systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [122] [POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration](https://arxiv.org/abs/2506.08785)
*Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: PARV-CE是一种支持多种精度格式的SIMD多精度MAC引擎，通过硬件-软件协同设计优化性能和能耗，适用于边缘AI加速。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型复杂度增加，边缘平台需要灵活支持多种精度格式的硬件以优化能耗。

Method: PARV-CE采用统一的4/8/16位定点、浮点和posit格式数据路径，结合层自适应精度策略和可重构SIMD流水线。

Result: 相比现有设计，PDP提升2倍，资源使用减少3倍，精度保持在FP32基线的1.8%以内。

Conclusion: PARV-CE是一种可扩展且高能效的边缘AI加速解决方案，适用于多种模型训练和推理。

Abstract: The increasing complexity of AI models requires flexible hardware capable of
supporting diverse precision formats, particularly for energy-constrained edge
platforms. This work presents PARV-CE, a SIMD-enabled, multi-precision MAC
engine that performs efficient multiply-accumulate operations using a unified
data-path for 4/8/16-bit fixed-point, floating point, and posit formats. The
architecture incorporates a layer adaptive precision strategy to align
computational accuracy with workload sensitivity, optimizing both performance
and energy usage. PARV-CE integrates quantization-aware execution with a
reconfigurable SIMD pipeline, enabling high-throughput processing with minimal
overhead through hardware-software co-design. The results demonstrate up to 2x
improvement in PDP and 3x reduction in resource usage compared to SoTA designs,
while retaining accuracy within 1.8% FP32 baseline. The architecture supports
both on-device training and inference across a range of workloads, including
DNNs, RNNs, RL, and Transformer models. The empirical analysis establish PARVCE
incorporated POLARON as a scalable and energy-efficient solution for
precision-adaptive AI acceleration at edge.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [123] [Gridding Forced Displacement using Semi-Supervised Learning](https://arxiv.org/abs/2506.08019)
*Andrew Wells,Geraldine Henningsen,Brice Bolane Tchinde Kengne*

Main category: cs.LG

TL;DR: 论文提出了一种半监督方法，将难民的统计数据从行政边界细化到0.5度的网格单元，覆盖25个撒哈拉以南非洲国家。


<details>
  <summary>Details</summary>
Motivation: 现有的难民统计数据通常局限于区域或国家层面，难以揭示局部的人口流动模式。

Method: 结合UNHCR的ProGres注册数据、Google Open Buildings的卫星建筑足迹和OpenStreetMap的地点坐标，通过标签传播算法生成高精度的空间难民统计数据。

Result: 方法平均准确率达92.9%，成功将1000多万难民观测数据分配到合适的网格单元，揭示了以往被掩盖的局部流动模式。

Conclusion: 高分辨率数据集为深入研究人口流动驱动因素提供了基础。

Abstract: We present a semi-supervised approach that disaggregates refugee statistics
from administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan
African countries. By integrating UNHCR's ProGres registration data with
satellite-derived building footprints from Google Open Buildings and location
coordinates from OpenStreetMap Populated Places, our label spreading algorithm
creates spatially explicit refugee statistics at high granularity.This
methodology achieves 92.9% average accuracy in placing over 10 million refugee
observations into appropriate grid cells, enabling the identification of
localized displacement patterns previously obscured in broader regional and
national statistics. The resulting high-resolution dataset provides a
foundation for a deeper understanding of displacement drivers.

</details>


### [124] [Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation](https://arxiv.org/abs/2506.08020)
*Zi-Ying Chen,Chuan-Xian Ren,Hong Yan*

Main category: cs.LG

TL;DR: 本文提出了一种双层次不平衡最优传输（BUOT）模型，用于解决部分域适应（PDA）问题，通过同时表征样本级和类级关系，提高知识转移的准确性。


<details>
  <summary>Details</summary>
Motivation: 部分域适应问题需要对齐跨域样本并区分异常类，现有加权框架仅能表征样本级关系，对异常类的识别不够准确。

Method: 提出BUOT模型，结合样本级和类级传输，通过合作机制提供结构信息和判别信息，并引入标签感知传输成本以提高效率。

Result: 在基准数据集上的实验验证了BUOT的竞争力。

Conclusion: BUOT模型通过双层次传输框架有效解决了部分域适应问题，提高了知识转移的准确性。

Abstract: Partial domain adaptation (PDA) problem requires aligning cross-domain
samples while distinguishing the outlier classes for accurate knowledge
transfer. The widely used weighting framework tries to address the outlier
classes by introducing the reweighed source domain with a similar label
distribution to the target domain. However, the empirical modeling of weights
can only characterize the sample-wise relations, which leads to insufficient
exploration of cluster structures, and the weights could be sensitive to the
inaccurate prediction and cause confusion on the outlier classes. To tackle
these issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model
to simultaneously characterize the sample-wise and class-wise relations in a
unified transport framework. Specifically, a cooperation mechanism between
sample-level and class-level transport is introduced, where the sample-level
transport provides essential structure information for the class-level
knowledge transfer, while the class-level transport supplies discriminative
information for the outlier identification. The bi-level transport plan
provides guidance for the alignment process. By incorporating the label-aware
transport cost, the local transport structure is ensured and a fast computation
formulation is derived to improve the efficiency. Extensive experiments on
benchmark datasets validate the competitiveness of BUOT.

</details>


### [125] [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/abs/2506.08022)
*Chenxi Liu,Tianyi Xiong,Ruibo Chen,Yihan Wu,Junfeng Guo,Tianyi Zhou,Heng Huang*

Main category: cs.LG

TL;DR: 论文提出了一种新的偏好学习框架MBPO，通过生成硬负样本和在线验证奖励，解决多模态模型中的模态不平衡问题，提升性能并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有LMMs在推理中存在模态不平衡问题，语言偏见压倒视觉输入，导致泛化能力差和幻觉现象。现有偏好优化方法未有效抑制LLM内部偏见，且依赖离线数据，无法适应动态分布变化。

Method: MBPO通过对抗性扰动生成硬负样本构建离线偏好数据集，并利用封闭任务生成在线验证奖励数据，结合GRPO进行训练。

Result: 实验表明MBPO能显著提升LMMs在视觉语言任务中的性能，并有效减少幻觉。

Conclusion: MBPO通过平衡模态偏好，解决了LMMs的模态不平衡问题，为多模态模型优化提供了新思路。

Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been
significantly advanced by instruction tuning and further strengthened by recent
preference optimization. Yet, most LMMs still suffer from severe modality
imbalance during reasoning, i.e., outweighing language prior biases over visual
inputs, which bottlenecks their generalization to downstream tasks and causes
hallucinations. However, existing preference optimization approaches for LMMs
do not focus on restraining the internal biases of their Large Language Model
(LLM) backbones when curating the training data. Moreover, they heavily rely on
offline data and lack the capacity to explore diverse responses adaptive to
dynamic distributional shifts during training. Meanwhile, Group Relative Policy
Optimization (GRPO), a recent method using online-generated data and verified
rewards to improve reasoning capabilities, remains largely underexplored in LMM
alignment. In this paper, we propose a novel preference learning framework,
Modality-Balancing Preference Optimization (MBPO), to address the modality
imbalance in LMMs. MBPO constructs a more effective offline preference dataset
by generating hard negatives, i.e., rejected responses misled by LLM biases due
to limited usage of visual information, through adversarial perturbation of
input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended
tasks to generate online responses with verified rewards. GRPO is then employed
to train the model with offline-online hybrid data. Extensive experiments
demonstrate that MBPO can enhance LMM performance on challenging
vision-language tasks and effectively reduce hallucinations.

</details>


### [126] [UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data](https://arxiv.org/abs/2506.08167)
*Sunny Gupta,Nikita Jangid,Amit Sethi*

Main category: cs.LG

TL;DR: UniVarFL是一个新的联邦学习框架，通过局部训练中的两种正则化策略解决非IID数据导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 非IID数据导致联邦学习性能下降，传统方法成本高或适应性差。

Method: UniVarFL采用分类器方差正则化和超球面均匀性正则化，模拟IID训练动态。

Result: 在多个基准数据集上，UniVarFL在准确性上优于现有方法。

Conclusion: UniVarFL是一种高效且可扩展的解决方案，适用于资源受限的实际联邦学习部署。

Abstract: Federated Learning (FL) often suffers from severe performance degradation
when faced with non-IID data, largely due to local classifier bias. Traditional
remedies such as global model regularization or layer freezing either incur
high computational costs or struggle to adapt to feature shifts. In this work,
we propose UniVarFL, a novel FL framework that emulates IID-like training
dynamics directly at the client level, eliminating the need for global model
dependency. UniVarFL leverages two complementary regularization strategies
during local training: Classifier Variance Regularization, which aligns
class-wise probability distributions with those expected under IID conditions,
effectively mitigating local classifier bias; and Hyperspherical Uniformity
Regularization, which encourages a uniform distribution of feature
representations across the hypersphere, thereby enhancing the model's ability
to generalize under diverse data distributions. Extensive experiments on
multiple benchmark datasets demonstrate that UniVarFL outperforms existing
methods in accuracy, highlighting its potential as a highly scalable and
efficient solution for real-world FL deployments, especially in
resource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL

</details>


### [127] [An Adaptive Method Stabilizing Activations for Enhanced Generalization](https://arxiv.org/abs/2506.08353)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: AdaAct是一种新颖的优化算法，通过根据激活方差调整学习率，提升神经元输出的稳定性，从而改善泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统激活正则化方法存在局限性，AdaAct通过神经元级别的自适应调整学习率，弥补了这一不足。

Method: AdaAct在训练过程中根据激活方差动态调整学习率，增强神经元输出的稳定性。

Result: 在CIFAR和ImageNet等标准图像分类基准测试中，AdaAct表现出色，兼具Adam的快速收敛和SGD的强泛化能力。

Conclusion: AdaAct是一种高效的优化算法，在保持执行时间竞争力的同时，显著提升了模型性能。

Abstract: We introduce AdaAct, a novel optimization algorithm that adjusts learning
rates according to activation variance. Our method enhances the stability of
neuron outputs by incorporating neuron-wise adaptivity during the training
process, which subsequently leads to better generalization -- a complementary
approach to conventional activation regularization methods. Experimental
results demonstrate AdaAct's competitive performance across standard image
classification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing
it with other state-of-the-art methods. Importantly, AdaAct effectively bridges
the gap between the convergence speed of Adam and the strong generalization
capabilities of SGD, all while maintaining competitive execution times. Code is
available at https://github.com/hseung88/adaact.

</details>


### [128] [Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings](https://arxiv.org/abs/2506.08435)
*Mingyuan Fan,Fuyi Wang,Cen Chen,Jianying Zhou*

Main category: cs.LG

TL;DR: 本文通过实证研究证明，即使在现实的联邦学习环境中，客户数据仍可被有效重构，并提出FedLeak方法以解决梯度匹配问题。


<details>
  <summary>Details</summary>
Motivation: 探讨联邦学习中的隐私风险，尤其是梯度泄漏攻击（GLAs）在实际环境中的有效性。

Method: 提出FedLeak方法，包含部分梯度匹配和梯度正则化两种新技术，并设计了一个基于文献和行业实践的实际评估协议。

Result: FedLeak在现实环境中仍能实现高保真数据重构，揭示了联邦学习系统的重大漏洞。

Conclusion: 联邦学习系统存在显著隐私风险，亟需更有效的防御方法。

Abstract: Federated learning (FL) enables collaborative model training among multiple
clients without the need to expose raw data. Its ability to safeguard privacy,
at the heart of FL, has recently been a hot-button debate topic. To elaborate,
several studies have introduced a type of attacks known as gradient leakage
attacks (GLAs), which exploit the gradients shared during training to
reconstruct clients' raw data. On the flip side, some literature, however,
contends no substantial privacy risk in practical FL environments due to the
effectiveness of such GLAs being limited to overly relaxed conditions, such as
small batch sizes and knowledge of clients' data distributions.
  This paper bridges this critical gap by empirically demonstrating that
clients' data can still be effectively reconstructed, even within realistic FL
environments. Upon revisiting GLAs, we recognize that their performance
failures stem from their inability to handle the gradient matching problem. To
alleviate the performance bottlenecks identified above, we develop FedLeak,
which introduces two novel techniques, partial gradient matching and gradient
regularization. Moreover, to evaluate the performance of FedLeak in real-world
FL environments, we formulate a practical evaluation protocol grounded in a
thorough review of extensive FL literature and industry practices. Under this
protocol, FedLeak can still achieve high-fidelity data reconstruction, thereby
underscoring the significant vulnerability in FL systems and the urgent need
for more effective defense methods.

</details>


### [129] [HSG-12M: A Large-Scale Spatial Multigraph Dataset](https://arxiv.org/abs/2506.08618)
*Xianquan Yan,Hakan Akgün,Kenji Kawaguchi,N. Duane Loh,Ching Hua Lee*

Main category: cs.LG

TL;DR: HSG-12M是首个大规模空间多重图数据集，保留了节点间多条几何路径，为几何感知图学习奠定基础。


<details>
  <summary>Details</summary>
Motivation: 现有图基准假设边是非空间且简单的，忽略了物理上不同的路径。HSG-12M旨在填补这一空白。

Method: 通过Poly2Graph管道，将一维晶体哈密顿量映射为谱图，生成静态和动态的Hamiltonian谱图。

Result: HSG-12M包含1160万静态和510万动态谱图，覆盖1401个特征多项式类。

Conclusion: HSG-12M为几何感知图学习提供了新工具，并在凝聚态物理等领域开辟了数据驱动科学发现的新机会。

Abstract: Existing graph benchmarks assume non-spatial, simple edges, collapsing
physically distinct paths into a single link. We introduce HSG-12M, the first
large-scale dataset of $\textbf{spatial multigraphs}-$graphs embedded in a
metric space where multiple geometrically distinct trajectories between two
nodes are retained as separate edges. HSG-12M contains 11.6 million static and
5.1 million dynamic $\textit{Hamiltonian spectral graphs}$ across 1401
characteristic-polynomial classes, derived from 177 TB of spectral potential
data. Each graph encodes the full geometry of a 1-D crystal's energy spectrum
on the complex plane, producing diverse, physics-grounded topologies that
transcend conventional node-coordinate datasets. To enable future extensions,
we release $\texttt{Poly2Graph}$: a high-performance, open-source pipeline that
maps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with
popular GNNs expose new challenges in learning from multi-edge geometry at
scale. Beyond its practical utility, we show that spectral graphs serve as
universal topological fingerprints of polynomials, vectors, and matrices,
forging a new algebra-to-graph link. HSG-12M lays the groundwork for
geometry-aware graph learning and new opportunities of data-driven scientific
discovery in condensed matter physics and beyond.

</details>


### [130] [Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers](https://arxiv.org/abs/2506.08641)
*Simon Roschmann,Quentin Bouniot,Vasilii Feofanov,Ievgen Redko,Zeynep Akata*

Main category: cs.LG

TL;DR: TiViT框架将时间序列转换为图像，利用预训练的视觉Transformer（ViT）提升时间序列分类性能，并在标准基准测试中达到最优表现。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类在医疗和工业领域很重要，但公开数据集稀缺限制了时间序列基础模型（TSFM）的发展。

Method: 提出TiViT框架，将时间序列转换为图像，利用预训练的ViT模型提取特征，并通过理论分析和实验验证其有效性。

Result: TiViT在标准时间序列分类任务中表现最优，且中间层的高维特征最有效。与TSFM结合可进一步提升性能。

Conclusion: TiViT展示了在非视觉领域复用视觉表示的新方向。

Abstract: Time series classification is a fundamental task in healthcare and industry,
yet the development of time series foundation models (TSFMs) remains limited by
the scarcity of publicly available time series datasets. In this work, we
propose Time Vision Transformer (TiViT), a framework that converts time series
into images to leverage the representational power of frozen Vision
Transformers (ViTs) pretrained on large-scale image datasets. First, we
theoretically motivate our approach by analyzing the 2D patching of ViTs for
time series, showing that it can increase the number of label-relevant tokens
and reduce the sample complexity. Second, we empirically demonstrate that TiViT
achieves state-of-the-art performance on standard time series classification
benchmarks by utilizing the hidden representations of large OpenCLIP models. We
explore the structure of TiViT representations and find that intermediate
layers with high intrinsic dimension are the most effective for time series
classification. Finally, we assess the alignment between TiViT and TSFM
representation spaces and identify a strong complementarity, with further
performance gains achieved by combining their features. Our findings reveal yet
another direction for reusing vision representations in a non-visual domain.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [131] [PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly](https://arxiv.org/abs/2506.08708)
*Liang Ma,Jiajun Wen,Min Lin,Rongtao Xu,Xiwen Liang,Bingqian Lin,Jun Ma,Yongxin Wang,Ziming Wei,Haokun Lin,Mingfei Han,Meng Cao,Bokui Chen,Ivan Laptev,Xiaodan Liang*

Main category: cs.RO

TL;DR: PhyBlock是一个用于评估视觉语言模型（VLMs）在物理理解和规划能力上的渐进式基准测试，通过3D积木组装任务和视觉问答（VQA）任务，揭示了VLMs在高级规划和空间推理上的显著局限性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在物理现象理解和结构化3D环境中的能力有限，需要一种系统化的评估方法来推动其在具身推理中的发展。

Method: PhyBench通过2600个任务（400个组装任务和2200个VQA任务）评估VLMs，涵盖部分完成、故障诊断和规划鲁棒性三个维度。

Result: 21个先进VLMs的测试结果显示其在高级规划和空间推理上表现不佳，尤其是任务复杂度增加时性能显著下降。

Conclusion: PhyBlock为具身推理提供了一个统一的测试平台，揭示了VLMs在物理问题解决中的局限性，并强调了进一步改进的必要性。

Abstract: While vision-language models (VLMs) have demonstrated promising capabilities
in reasoning and planning for embodied agents, their ability to comprehend
physical phenomena, particularly within structured 3D environments, remains
severely limited. To close this gap, we introduce PhyBlock, a progressive
benchmark designed to assess VLMs on physical understanding and planning
through robotic 3D block assembly tasks. PhyBlock integrates a novel four-level
cognitive hierarchy assembly task alongside targeted Visual Question Answering
(VQA) samples, collectively aimed at evaluating progressive spatial reasoning
and fundamental physical comprehension, including object properties, spatial
relationships, and holistic scene understanding. PhyBlock includes 2600 block
tasks (400 assembly tasks, 2200 VQA tasks) and evaluates models across three
key dimensions: partial completion, failure diagnosis, and planning robustness.
We benchmark 21 state-of-the-art VLMs, highlighting their strengths and
limitations in physically grounded, multi-step planning. Our empirical findings
indicate that the performance of VLMs exhibits pronounced limitations in
high-level planning and reasoning capabilities, leading to a notable decline in
performance for the growing complexity of the tasks. Error analysis reveals
persistent difficulties in spatial orientation and dependency reasoning.
Surprisingly, chain-of-thought prompting offers minimal improvements,
suggesting spatial tasks heavily rely on intuitive model comprehension. We
position PhyBlock as a unified testbed to advance embodied reasoning, bridging
vision-language understanding and real-world physical problem-solving.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [132] [SakugaFlow: A Stagewise Illustration Framework Emulating the Human Drawing Process and Providing Interactive Tutoring for Novice Drawing Skills](https://arxiv.org/abs/2506.08443)
*Kazuki Kawamura,Jun Rekimoto*

Main category: cs.HC

TL;DR: SakugaFlow是一个四阶段流程，结合扩散模型和大型语言模型，为新手提供实时反馈，支持非线性修改和多版本分支，将黑盒生成器转化为学习工具。


<details>
  <summary>Details</summary>
Motivation: 当前AI绘图工具虽能生成高质量图像，但缺乏人类艺术家逐步创作的过程，无法提供学习支持。

Method: 采用四阶段流程，结合扩散模型生成图像和语言模型提供实时反馈，支持非线性修改和多版本分支。

Result: SakugaFlow通过展示中间输出和嵌入教学对话，支持创意探索和技能学习。

Conclusion: SakugaFlow成功将黑盒生成器转化为支持学习和创作的工具。

Abstract: While current AI illustration tools can generate high-quality images from
text prompts, they rarely reveal the step-by-step procedure that human artists
follow. We present SakugaFlow, a four-stage pipeline that pairs diffusion-based
image generation with a large-language-model tutor. At each stage, novices
receive real-time feedback on anatomy, perspective, and composition, revise any
step non-linearly, and branch alternative versions. By exposing intermediate
outputs and embedding pedagogical dialogue, SakugaFlow turns a black-box
generator into a scaffolded learning environment that supports both creative
exploration and skills acquisition.

</details>


### [133] [MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback](https://arxiv.org/abs/2506.08634)
*Alvaro Becerra,Daniel Andres,Pablo Villegas,Roberto Daza,Ruth Cobos*

Main category: cs.HC

TL;DR: MOSAIC-F是一个多模态反馈框架，结合了多模态学习分析、观察、传感器、AI和协作评估，为学生提供个性化反馈。


<details>
  <summary>Details</summary>
Motivation: 旨在通过结合人类评估和数据驱动的多模态分析，提供更准确、个性化和可操作的反馈。

Method: 框架包括四个步骤：标准化评估、多模态数据收集、AI生成个性化反馈、学生自我评估与可视化。

Result: 在提升口头表达能力的测试中表现良好。

Conclusion: MOSAIC-F通过结合人类与数据驱动的评估，实现了更有效的反馈机制。

Abstract: In this article, we present a novel multimodal feedback framework called
MOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal
Learning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI),
and Collaborative assessments for generating personalized feedback on student
learning activities. This framework consists of four key steps. First, peers
and professors' assessments are conducted through standardized rubrics (that
include both quantitative and qualitative evaluations). Second, multimodal data
are collected during learning activities, including video recordings, audio
capture, gaze tracking, physiological signals (heart rate, motion data), and
behavioral interactions. Third, personalized feedback is generated using AI,
synthesizing human-based evaluations and data-based multimodal insights such as
posture, speech patterns, stress levels, and cognitive load, among others.
Finally, students review their own performance through video recordings and
engage in self-assessment and feedback visualization, comparing their own
evaluations with peers and professors' assessments, class averages, and
AI-generated recommendations. By combining human-based and data-based
evaluation techniques, this framework enables more accurate, personalized and
actionable feedback. We tested MOSAIC-F in the context of improving oral
presentation skills.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [134] [Surgeons Awareness, Expectations, and Involvement with Artificial Intelligence: a Survey Pre and Post the GPT Era](https://arxiv.org/abs/2506.08258)
*Lorenzo Arboit,Dennis N. Schneider,Toby Collins,Daniel A. Hashimoto,Silvana Perretta,Bernard Dallemagne,Jacques Marescaux,EAES Working Group,Nicolas Padoy,Pietro Mascagni*

Main category: cs.CY

TL;DR: 研究通过2021年和2024年的全球调查，分析了外科医生对AI的认知、期望和参与情况，发现AI课程意识和参与度提升，但基础知识仍有限。伦理问题和基础设施是主要障碍，但AI的积极影响和整合意愿较高。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在外科手术中的应用潜力及其对外科医生认知的影响，以推动AI在医疗领域的有效整合。

Method: 通过2021年和2024年的两次全球性横断面调查，评估外科医生的AI意识、期望、参与度和伦理观点。

Result: AI课程意识和参与度显著提升，但基础知识仍不足；伦理问题成为焦点，基础设施是主要障碍；多数受访者对AI持乐观态度并愿意整合。

Conclusion: 外科医生对AI的认知正在演变，但需通过教育、伦理框架和基础设施发展解决知识差距和实施障碍。

Abstract: Artificial Intelligence (AI) is transforming medicine, with generative AI
models like ChatGPT reshaping perceptions of its potential. This study examines
surgeons' awareness, expectations, and involvement with AI in surgery through
comparative surveys conducted in 2021 and 2024. Two cross-sectional surveys
were distributed globally in 2021 and 2024, the first before an IRCAD webinar
and the second during the annual EAES meeting. The surveys assessed
demographics, AI awareness, expectations, involvement, and ethics (2024 only).
The surveys collected a total of 671 responses from 98 countries, 522 in 2021
and 149 in 2024. Awareness of AI courses rose from 14.5% in 2021 to 44.6% in
2024, while course attendance increased from 12.9% to 23%. Despite this,
familiarity with foundational AI concepts remained limited. Expectations for
AI's role shifted in 2024, with hospital management gaining relevance. Ethical
concerns gained prominence, with 87.2% of 2024 participants emphasizing
accountability and transparency. Infrastructure limitations remained the
primary obstacle to implementation. Interdisciplinary collaboration and
structured training were identified as critical for successful AI adoption.
Optimism about AI's transformative potential remained high, with 79.9% of
respondents believing AI would positively impact surgery and 96.6% willing to
integrate AI into their clinical practice. Surgeons' perceptions of AI are
evolving, driven by the rise of generative AI and advancements in surgical data
science. While enthusiasm for integration is strong, knowledge gaps and
infrastructural challenges persist. Addressing these through education, ethical
frameworks, and infrastructure development is essential.

</details>
