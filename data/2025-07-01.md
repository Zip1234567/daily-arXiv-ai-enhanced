<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 222]
- [eess.IV](#eess.IV) [Total: 32]
- [cs.GR](#cs.GR) [Total: 12]
- [physics.geo-ph](#physics.geo-ph) [Total: 3]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 15]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [math.OC](#math.OC) [Total: 2]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.CL](#cs.CL) [Total: 2]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Robust Perspective Correction for Real-World Crack Evolution Tracking in Image-Based Structural Health Monitoring](https://arxiv.org/abs/2506.22437)
*Xinxin Sun,Peter Chang*

Main category: cs.CV

TL;DR: 提出了一种基于物理信息的图像对齐框架，针对结构健康监测中的裂纹定位问题，通过非线性各向异性扩散和RANSAC估计实现高精度对齐。


<details>
  <summary>Details</summary>
Motivation: 传统特征检测器（如SIFT、SURF）在高频边缘抑制和薄裂纹定位上表现不佳，而轻量级方法（如ORB、BRISK）在复杂场景中重复性差。

Method: 采用改进的KAZE架构，结合非线性各向异性扩散构建裂纹保留尺度空间，并集成RANSAC估计实现几何校正。

Result: 在多种实际条件下，裂纹面积和长度误差分别减少70%和90%，对齐误差低于5%。

Conclusion: 该方法无需训练或参数调优，适用于无人机和移动平台，为裂纹演化跟踪提供了鲁棒且物理基础的新方案。

Abstract: Accurate image alignment is essential for monitoring crack evolution in
structural health monitoring (SHM), particularly under real-world conditions
involving perspective distortion, occlusion, and low contrast. However,
traditional feature detectors such as SIFT and SURF, which rely on
Gaussian-based scale spaces, tend to suppress high-frequency edges, making them
unsuitable for thin crack localization. Lightweight binary alternatives like
ORB and BRISK, while computationally efficient, often suffer from poor keypoint
repeatability on textured or shadowed surfaces. This study presents a
physics-informed alignment framework that adapts the open KAZE architecture to
SHM-specific challenges. By utilizing nonlinear anisotropic diffusion to
construct a crack-preserving scale space, and integrating RANSAC-based
homography estimation, the framework enables accurate geometric correction
without the need for training, parameter tuning, or prior calibration. The
method is validated on time-lapse images of masonry and concrete acquired via
handheld smartphone under varied field conditions, including shadow
interference, cropping, oblique viewing angles, and surface clutter. Compared
to classical detectors, the proposed framework reduces crack area and spine
length errors by up to 70 percent and 90 percent, respectively, while
maintaining sub-5 percent alignment error in key metrics. Unsupervised,
interpretable, and computationally lightweight, this approach supports scalable
deployment via UAVs and mobile platforms. By tailoring nonlinear scale-space
modeling to SHM image alignment, this work offers a robust and physically
grounded alternative to conventional techniques for tracking real-world crack
evolution.

</details>


### [2] [Counting with Confidence: Accurate Pest Monitoring in Water Traps](https://arxiv.org/abs/2506.22438)
*Xumin Gao,Mark Stevens,Grzegorz Cielniak*

Main category: cs.CV

TL;DR: 本文提出了一种基于计数结果和外部环境信息的害虫计数置信度评估方法，通过多因素敏感性分析和自适应DBSCAN聚类算法，显著提升了评估准确性。


<details>
  <summary>Details</summary>
Motivation: 现有害虫计数研究缺乏对实际场景中计数结果可靠性的评估，本文旨在填补这一空白。

Method: 结合害虫检测网络、图像质量与复杂度评估、害虫分布均匀性分析，并通过回归模型预测计数置信度。

Result: 实验显示，该方法在害虫计数置信度测试集上MSE降低31.7%，R2提高15.2%。

Conclusion: 本研究首次全面评估害虫计数置信度，为精准农业决策提供了可靠工具。

Abstract: Accurate pest population monitoring and tracking their dynamic changes are
crucial for precision agriculture decision-making. A common limitation in
existing vision-based automatic pest counting research is that models are
typically evaluated on datasets with ground truth but deployed in real-world
scenarios without assessing the reliability of counting results due to the lack
of ground truth. To this end, this paper proposed a method for comprehensively
evaluating pest counting confidence in the image, based on information related
to counting results and external environmental conditions. First, a pest
detection network is used for pest detection and counting, extracting counting
result-related information. Then, the pest images undergo image quality
assessment, image complexity assessment, and pest distribution uniformity
assessment. And the changes in image clarity caused by stirring during image
acquisition are quantified by calculating the average gradient magnitude.
Notably, we designed a hypothesis-driven multi-factor sensitivity analysis
method to select the optimal image quality assessment and image complexity
assessment methods. And we proposed an adaptive DBSCAN clustering algorithm for
pest distribution uniformity assessment. Finally, the obtained information
related to counting results and external environmental conditions is input into
a regression model for prediction, resulting in the final pest counting
confidence. To the best of our knowledge, this is the first study dedicated to
comprehensively evaluating counting confidence in counting tasks, and
quantifying the relationship between influencing factors and counting
confidence through a model. Experimental results show our method reduces MSE by
31.7% and improves R2 by 15.2% on the pest counting confidence test set,
compared to the baseline built primarily on information related to counting
results.

</details>


### [3] [Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization](https://arxiv.org/abs/2506.22463)
*Weizhi Gao,Zhichao Hou,Junqi Yin,Feiyi Wang,Linyu Peng,Xiaorui Liu*

Main category: cs.CV

TL;DR: MoDiff是一种创新的扩散模型加速框架，通过调制量化和误差补偿提升生成效率，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的高计算成本是主要瓶颈，现有加速技术存在计算误差和生成质量的限制。

Method: 提出MoDiff框架，结合调制量化和误差补偿，适用于所有扩散模型。

Result: 在CIFAR-10和LSUN上，MoDiff将激活量化从8位降至3位且无性能损失。

Conclusion: MoDiff是一种高效且通用的扩散模型加速解决方案。

Abstract: Diffusion models have emerged as powerful generative models, but their high
computation cost in iterative sampling remains a significant bottleneck. In
this work, we present an in-depth and insightful study of state-of-the-art
acceleration techniques for diffusion models, including caching and
quantization, revealing their limitations in computation error and generation
quality. To break these limits, this work introduces Modulated Diffusion
(MoDiff), an innovative, rigorous, and principled framework that accelerates
generative modeling through modulated quantization and error compensation.
MoDiff not only inherents the advantages of existing caching and quantization
methods but also serves as a general framework to accelerate all diffusion
models. The advantages of MoDiff are supported by solid theoretical insight and
analysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate
that MoDiff significant reduces activation quantization from 8 bits to 3 bits
without performance degradation in post-training quantization (PTQ). Our code
implementation is available at https://github.com/WeizhiGao/MoDiff.

</details>


### [4] [ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction](https://arxiv.org/abs/2506.22498)
*Hao Liu,Yu Hu,Rakiba Rayhana,Ling Bai,Zheng Liu*

Main category: cs.CV

TL;DR: 论文提出了一种基于低成本负载传感器的床离开意图预测方法，通过图像融合和时间序列分类实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 医院和长期护理机构中，床相关跌倒是一个主要伤害来源，现有商业警报通常在患者已离开床后才触发，无法提前预防。

Method: 使用四个低成本负载传感器采集信号，转换为互补图像（RGB线图和三种纹理图），并设计双流Swin Transformer（ViFusionTST）进行并行处理和跨模态融合。

Result: 在真实数据集上，ViFusionTST达到0.885的准确率和0.794的F1分数，优于现有基线方法。

Conclusion: 基于图像融合的负载信号分类是一种实用且有效的实时隐私保护跌倒预防解决方案。

Abstract: Bed-related falls remain a leading source of injury in hospitals and
long-term-care facilities, yet many commercial alarms trigger only after a
patient has already left the bed. We show that early bed-exit intent can be
predicted using only four low-cost load cells mounted under the bed legs. The
resulting load signals are first converted into a compact set of complementary
images: an RGB line plot that preserves raw waveforms and three texture maps -
recurrence plot, Markov transition field, and Gramian angular field - that
expose higher-order dynamics. We introduce ViFusionTST, a dual-stream Swin
Transformer that processes the line plot and texture maps in parallel and fuses
them through cross-attention to learn data-driven modality weights.
  To provide a realistic benchmark, we collected six months of continuous data
from 95 beds in a long-term-care facility. On this real-world dataset
ViFusionTST reaches an accuracy of 0.885 and an F1 score of 0.794, surpassing
recent 1D and 2D time-series baselines across F1, recall, accuracy, and AUPRC.
The results demonstrate that image-based fusion of load-sensor signals for time
series classification is a practical and effective solution for real-time,
privacy-preserving fall prevention.

</details>


### [5] [Scalable Dynamic Origin-Destination Demand Estimation Enhanced by High-Resolution Satellite Imagery Data](https://arxiv.org/abs/2506.22499)
*Jiachao Liu,Pablo Guarda,Koichiro Niinuma,Sean Qian*

Main category: cs.CV

TL;DR: 提出了一种结合卫星影像与传统交通数据的动态起讫点需求估计框架，显著提升了估计性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统局部传感器数据稀疏性问题，利用卫星影像提供城市范围的交通信息。

Method: 设计计算机视觉流程提取车辆信息，构建计算图模型联合校准交通数据。

Result: 实验表明，结合卫星数据显著提升估计性能，尤其适用于无局部传感器的路段。

Conclusion: 框架适用于不同规模城市，具有实际部署潜力。

Abstract: This study presents a novel integrated framework for dynamic
origin-destination demand estimation (DODE) in multi-class mesoscopic network
models, leveraging high-resolution satellite imagery together with conventional
traffic data from local sensors. Unlike sparse local detectors, satellite
imagery offers consistent, city-wide road and traffic information of both
parking and moving vehicles, overcoming data availability limitations. To
extract information from imagery data, we design a computer vision pipeline for
class-specific vehicle detection and map matching, generating link-level
traffic density observations by vehicle class. Building upon this information,
we formulate a computational graph-based DODE model that calibrates dynamic
network states by jointly matching observed traffic counts and travel times
from local sensors with density measurements derived from satellite imagery. To
assess the accuracy and scalability of the proposed framework, we conduct a
series of numerical experiments using both synthetic and real-world data. The
results of out-of-sample tests demonstrate that supplementing traditional data
with satellite-derived density significantly improves estimation performance,
especially for links without local sensors. Real-world experiments also confirm
the framework's capability to handle large-scale networks, supporting its
potential for practical deployment in cities of varying sizes. Sensitivity
analysis further evaluates the impact of data quality related to satellite
imagery data.

</details>


### [6] [Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models](https://arxiv.org/abs/2506.22500)
*Weiyi Zhao,Xiaoyu Tan,Liang Liu,Sijia Li,Youwei Song,Xihe Qiu*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散模型生成的合成图像数据集OR-VSKC，用于解决多模态大语言模型在手术室风险检测中的视觉-语义知识冲突问题。


<details>
  <summary>Details</summary>
Motivation: 手术风险识别对患者安全至关重要，但现有模型在视觉安全违规检测上表现不佳，需解决视觉-语义知识冲突问题。

Method: 通过生成34,000多张合成图像和214张人工标注图像构建数据集，并微调模型以提升检测能力。

Result: 微调显著提升了模型对训练冲突实体的检测能力，但对未训练实体类型表现不佳。

Conclusion: 研究贡献包括数据生成方法、开源数据集及对MLLMs知识一致性的分析，为未来研究提供了资源。

Abstract: Surgical risk identification is critical for patient safety and reducing
preventable medical errors. While multimodal large language models (MLLMs) show
promise for automated operating room (OR) risk detection, they often exhibit
visual-semantic knowledge conflicts (VS-KC), failing to identify visual safety
violations despite understanding textual rules. To address this, we introduce a
dataset comprising over 34,000 synthetic images generated by diffusion models,
depicting operating room scenes containing entities that violate established
safety rules. These images were created to alleviate data scarcity and examine
MLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated
images that serve as a gold-standard reference for validation. This
comprehensive dataset, spanning diverse perspectives, stages, and
configurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC
significantly improves MLLMs' detection of trained conflict entities and
generalizes well to new viewpoints for these entities, but performance on
untrained entity types remains poor, highlighting learning specificity and the
need for comprehensive training. The main contributions of this work include:
(1) a data generation methodology tailored for rule-violation scenarios; (2)
the release of the OR-VSKC dataset and its associated benchmark as open-source
resources; and (3) an empirical analysis of violation-sensitive knowledge
consistency in representative MLLMs. The dataset and appendix are available at
https://github.com/zgg2577/VS-KC.

</details>


### [7] [How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?](https://arxiv.org/abs/2506.22501)
*Gautam Siddharth Kashyap,Manaswi Kulahara,Nipun Joshi,Usman Naseem*

Main category: cs.CV

TL;DR: 论文提出了一种名为SpatialNet-ViT的新模型，结合Vision Transformers和多任务学习，以提升遥感分类任务的泛化能力和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多局限于特定任务或数据集，难以泛化到多样化的遥感分类问题。

Method: 采用Vision Transformers和多任务学习，结合数据增强和迁移学习技术。

Result: 模型在分类准确性和可扩展性上均有提升。

Conclusion: SpatialNet-ViT为遥感分类任务提供了一种更通用且高效的解决方案。

Abstract: Remote sensing datasets offer significant promise for tackling key
classification tasks such as land-use categorization, object presence
detection, and rural/urban classification. However, many existing studies tend
to focus on narrow tasks or datasets, which limits their ability to generalize
across various remote sensing classification challenges. To overcome this, we
propose a novel model, SpatialNet-ViT, leveraging the power of Vision
Transformers (ViTs) and Multi-Task Learning (MTL). This integrated approach
combines spatial awareness with contextual understanding, improving both
classification accuracy and scalability. Additionally, techniques like data
augmentation, transfer learning, and multi-task learning are employed to
enhance model robustness and its ability to generalize across diverse datasets

</details>


### [8] [What Makes a Dribble Successful? Insights From 3D Pose Tracking Data](https://arxiv.org/abs/2506.22503)
*Michiel Schepers,Pieter Robberechts,Jan Van Haaren,Jesse Davis*

Main category: cs.CV

TL;DR: 研究利用三维姿态跟踪数据改进足球带球技能评估，发现姿态特征（如平衡和方向对齐）能显著提升模型预测带球成功率的性能。


<details>
  <summary>Details</summary>
Motivation: 传统二维位置数据无法全面捕捉带球中的平衡、方向和控球等关键因素，限制了分析的深度。

Method: 从2022/23赛季欧冠联赛的1,736次带球中提取姿态特征，并结合传统二维数据评估其对带球成功的影响。

Result: 姿态特征（如攻击者平衡和攻防方向对齐）对预测带球成功有显著作用，结合二维数据可提升模型性能。

Conclusion: 三维姿态数据为足球带球技能分析提供了更深入的视角，显著优于传统二维方法。

Abstract: Data analysis plays an increasingly important role in soccer, offering new
ways to evaluate individual and team performance. One specific application is
the evaluation of dribbles: one-on-one situations where an attacker attempts to
bypass a defender with the ball. While previous research has primarily relied
on 2D positional tracking data, this fails to capture aspects like balance,
orientation, and ball control, limiting the depth of current insights. This
study explores how pose tracking data (capturing players' posture and movement
in three dimensions) can improve our understanding of dribbling skills. We
extract novel pose-based features from 1,736 dribbles in the 2022/23 Champions
League season and evaluate their impact on dribble success. Our results
indicate that features capturing the attacker's balance and the alignment of
the orientation between the attacker and defender are informative for
predicting dribble success. Incorporating these pose-based features on top of
features derived from traditional 2D positional data leads to a measurable
improvement in model performance.

</details>


### [9] [Patch2Loc: Learning to Localize Patches for Unsupervised Brain Lesion Detection](https://arxiv.org/abs/2506.22504)
*Hassan Baker,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: 提出了一种名为Patch2Loc的无监督学习方法，用于从正常脑部MRI图像中学习，并通过预测异常斑块的空间位置来检测脑部病变。


<details>
  <summary>Details</summary>
Motivation: 脑部病变检测对诊断和治疗至关重要，但现有监督学习方法需要标注数据，限制了其应用。

Method: 通过训练神经网络模型从正常MRI图像中学习斑块的空间位置，异常斑块通过预测误差和方差检测。

Result: 在多个数据集上验证了模型的有效性，优于现有无监督分割方法。

Conclusion: Patch2Loc为无监督脑部病变检测提供了一种高效方法，代码已开源。

Abstract: Detecting brain lesions as abnormalities observed in magnetic resonance
imaging (MRI) is essential for diagnosis and treatment. In the search of
abnormalities, such as tumors and malformations, radiologists may benefit from
computer-aided diagnostics that use computer vision systems trained with
machine learning to segment normal tissue from abnormal brain tissue. While
supervised learning methods require annotated lesions, we propose a new
unsupervised approach (Patch2Loc) that learns from normal patches taken from
structural MRI. We train a neural network model to map a patch back to its
spatial location within a slice of the brain volume. During inference, abnormal
patches are detected by the relatively higher error and/or variance of the
location prediction. This generates a heatmap that can be integrated into
pixel-wise methods to achieve finer-grained segmentation. We demonstrate the
ability of our model to segment abnormal brain tissues by applying our approach
to the detection of tumor tissues in MRI on T2-weighted images from BraTS2021
and MSLUB datasets and T1-weighted images from ATLAS and WMH datasets. We show
that it outperforms the state-of-the art in unsupervised segmentation. The
codebase for this work can be found on our
\href{https://github.com/bakerhassan/Patch2Loc}{GitHub page}.

</details>


### [10] [Weakly Supervised Object Segmentation by Background Conditional Divergence](https://arxiv.org/abs/2506.22505)
*Hassan Baker,Matthew S. Emigh,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: 提出一种利用弱监督（图像级标签）训练掩码网络进行二元对象分割的方法，通过生成反事实背景图像提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决专业图像领域（如合成孔径声纳图像）中缺乏大量标注数据的问题，降低像素级标注成本。

Method: 利用图像级标签训练掩码网络，通过聚类背景图像生成反事实图像，结合对比损失和监督损失优化模型。

Result: 在声纳图像和自然图像上均优于现有无监督分割基线，且无需预训练网络或对抗性判别器。

Conclusion: 该方法在弱监督条件下有效，适用于多种图像领域，且计算成本较低。

Abstract: As a computer vision task, automatic object segmentation remains challenging
in specialized image domains without massive labeled data, such as synthetic
aperture sonar images, remote sensing, biomedical imaging, etc. In any domain,
obtaining pixel-wise segmentation masks is expensive. In this work, we propose
a method for training a masking network to perform binary object segmentation
using weak supervision in the form of image-wise presence or absence of an
object of interest, which provides less information but may be obtained more
quickly from manual or automatic labeling. A key step in our method is that the
segmented objects can be placed into background-only images to create
realistic, images of the objects with counterfactual backgrounds. To create a
contrast between the original and counterfactual background images, we propose
to first cluster the background-only images, and then during learning create
counterfactual images that blend objects segmented from their original source
backgrounds to backgrounds chosen from a targeted cluster. One term in the
training loss is the divergence between these counterfactual images and the
real object images with backgrounds of the target cluster. The other term is a
supervised loss for background-only images. While an adversarial critic could
provide the divergence, we use sample-based divergences. We conduct experiments
on side-scan and synthetic aperture sonar in which our approach succeeds
compared to previous unsupervised segmentation baselines that were only tested
on natural images. Furthermore, to show generality we extend our experiments to
natural images, obtaining reasonable performance with our method that avoids
pretrained networks, generative networks, and adversarial critics. The basecode
for this work can be found at
\href{GitHub}{https://github.com/bakerhassan/WSOS}.

</details>


### [11] [FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment](https://arxiv.org/abs/2506.22509)
*Hang Xu,Jie Huang,Linjiang Huang,Dong Li,Yidi Liu,Feng Zhao*

Main category: cs.CV

TL;DR: 提出了一种无需训练的域适应方法（DNA），通过调整扩散采样过程中的噪声统计量，提升扩散密集预测模型的跨域性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在建模域信息分布转换方面有效，但噪声统计偏差导致域偏移，需针对性解决。

Method: 提出Domain Noise Alignment (DNA)方法，通过对齐源域和目标域的噪声统计量实现域适应，源域不可用时利用高置信区域逐步调整噪声统计。

Result: 在四种密集预测任务中验证了DNA方法的有效性，提升了模型的域适应能力。

Conclusion: DNA是一种高效且无需训练的域适应方法，适用于扩散密集预测模型。

Abstract: Domain Adaptation(DA) for dense prediction tasks is an important topic, which
enhances the dense prediction model's performance when tested on its unseen
domain. Recently, with the development of Diffusion-based Dense Prediction
(DDP) models, the exploration of DA designs tailored to this framework is worth
exploring, since the diffusion model is effective in modeling the distribution
transformation that comprises domain information. In this work, we propose a
training-free mechanism for DDP frameworks, endowing them with DA capabilities.
Our motivation arises from the observation that the exposure bias (e.g., noise
statistics bias) in diffusion brings domain shift, and different domains in
conditions of DDP models can also be effectively captured by the noise
prediction statistics. Based on this, we propose a training-free Domain Noise
Alignment (DNA) approach, which alleviates the variations of noise statistics
to domain changes during the diffusion sampling process, thereby achieving
domain adaptation. Specifically, when the source domain is available, we
directly adopt the DNA method to achieve domain adaptation by aligning the
noise statistics of the target domain with those of the source domain. For the
more challenging source-free DA, inspired by the observation that regions
closer to the source domain exhibit higher confidence meeting variations of
sampling noise, we utilize the statistics from the high-confidence regions
progressively to guide the noise statistic adjustment during the sampling
process. Notably, our method demonstrates the effectiveness of enhancing the DA
capability of DDP models across four common dense prediction tasks. Code is
available at
\href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.

</details>


### [12] [Lightning the Night with Generative Artificial Intelligence](https://arxiv.org/abs/2506.22511)
*Tingting Zhou,Feng Zhang,Haoyang Fu,Baoxiang Pan,Renhe Zhang,Feng Lu,Zhixin Yang*

Main category: cs.CV

TL;DR: 该研究利用生成扩散模型，基于FY4B卫星的多波段热红外数据，开发了RefDiff模型，实现了夜间可见光反射率的高精度反演，显著提升了复杂云结构区域的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于夜间缺乏可见光，无法进行全天候天气观测，因此需要开发一种方法来解决这一限制。

Method: 基于FY4B卫星的AGRI多波段热红外数据，开发了生成扩散模型RefDiff，用于夜间可见光反射率反演。

Result: RefDiff的SSIM指数达到0.90，在复杂云结构和厚云区域表现尤为突出，夜间反演能力与白天相当。

Conclusion: 该研究显著提升了夜间可见光反射率的反演能力，拓展了夜间可见光数据的应用潜力。

Abstract: The visible light reflectance data from geostationary satellites is crucial
for meteorological observations and plays an important role in weather
monitoring and forecasting. However, due to the lack of visible light at night,
it is impossible to conduct continuous all-day weather observations using
visible light reflectance data. This study pioneers the use of generative
diffusion models to address this limitation. Based on the multi-band thermal
infrared brightness temperature data from the Advanced Geostationary Radiation
Imager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we
developed a high-precision visible light reflectance retrieval model, called
Reflectance Diffusion (RefDiff), which enables 0.47~\mu\mathrm{m},
0.65~\mu\mathrm{m}, and 0.825~\mu\mathrm{m} bands visible light reflectance
retrieval at night. Compared to the classical models, RefDiff not only
significantly improves accuracy through ensemble averaging but also provides
uncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90,
with particularly significant improvements in areas with complex cloud
structures and thick clouds. The model's nighttime retrieval capability was
validated using VIIRS nighttime product, demonstrating comparable performance
to its daytime counterpart. In summary, this research has made substantial
progress in the ability to retrieve visible light reflectance at night, with
the potential to expand the application of nighttime visible light data.

</details>


### [13] [Automated Defect Identification and Categorization in NDE 4.0 with the Application of Artificial Intelligence](https://arxiv.org/abs/2506.22513)
*Aditya Sharma*

Main category: cs.CV

TL;DR: 该研究开发了一种自动化框架，用于现代射线照相中的缺陷检测和组织，通过NDE 4.0技术验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 解决现有信息不足的问题，优化虚拟缺陷增强技术，并验证框架的实用性。

Method: 收集并分类223张飞机焊缝的CR照片，使用虚拟缺陷增强和标准增强扩展数据，训练改进的U-net模型进行语义缺陷分割。

Result: 模型在缺陷检测中表现出高灵敏度，尤其在a90/95特征下表现优异，扩展方法在焊缝区域效果显著。

Conclusion: 该框架具有快速推理能力，适用于大图像处理，并被专业评估者认为可作为测试周期的支持工具。

Abstract: This investigation attempts to create an automated framework for fault
detection and organization for usage in contemporary radiography, as per NDE
4.0. The review's goals are to address the lack of information that is
sufficiently explained, learn how to make the most of virtual defect increase,
and determine whether the framework is viable by using NDE measurements. As its
basic information source, the technique consists of compiling and categorizing
223 CR photographs of airplane welds. Information expansion systems, such as
virtual defect increase and standard increase, are used to work on the
preparation dataset. A modified U-net model is prepared using the improved data
to produce semantic fault division veils. To assess the effectiveness of the
model, NDE boundaries such as Case, estimating exactness, and misleading call
rate are used. Tiny a90/95 characteristics, which provide strong
differentiating evidence of flaws, reveal that the suggested approach achieves
exceptional awareness in defect detection. Considering a 90/95, size error, and
fake call rate in the weld area, the consolidated expansion approach clearly
wins. Due to the framework's fast derivation speed, large images can be broken
down efficiently and quickly. Professional controllers evaluate the transmitted
system in the field and believe that it has a guarantee as a support device in
the testing cycle, irrespective of particular equipment cut-off points and
programming resemblance.

</details>


### [14] [Container damage detection using advanced computer vision model Yolov12 vs Yolov11 vs RF-DETR A comparative analysis](https://arxiv.org/abs/2506.22517)
*Subhadip Kumar*

Main category: cs.CV

TL;DR: 比较了Yolov11、Yolov12和RF-DETR三种计算机视觉模型在集装箱损伤检测中的性能，发现RF-DETR在不常见损伤检测中表现更优。


<details>
  <summary>Details</summary>
Motivation: 集装箱损伤检测对延长使用寿命和避免安全隐患至关重要，需找到最适合的检测模型。

Method: 使用278张标注图像数据集训练和测试三种模型，比较mAP和精度。

Result: Yolov11和Yolov12的mAP@50为81.9%，RF-DETR为77.7%，但RF-DETR在不常见损伤检测中表现更优。

Conclusion: RF-DETR在不常见损伤检测中更具优势，适合实际应用。

Abstract: Containers are an integral part of the logistics industry and act as a
barrier for cargo. A typical service life for a container is more than 20
years. However, overtime containers suffer various types of damage due to the
mechanical as well as natural factors. A damaged container is a safety hazard
for the employees handling it and a liability for the logistic company.
Therefore, a timely inspection and detection of the damaged container is a key
for prolonging service life as well as avoiding safety hazards. In this paper,
we will compare the performance of the damage detection by three
state-of-the-art advanced computer vision models Yolov12, Yolov11 and RF-DETR.
We will use a dataset of 278 annotated images to train, validate and test the
model. We will compare the mAP and precision of the model. The objective of
this paper is to identify the model that is best suited for container damage
detection. The result is mixed. mAP@50 score of Yolov11 and 12 was 81.9%
compared to RF-DETR, which was 77.7%. However, while testing the model for
not-so-common damaged containers, the RF-DETR model outperformed the others
overall, exhibiting superiority to accurately detecting both damaged containers
as well as damage occurrences with high confidence.

</details>


### [15] [Neural Cellular Automata: From Cells to Pixels](https://arxiv.org/abs/2506.22899)
*Ehsan Pajouheshgar,Yitao Xu,Ali Abbasi,Alexander Mordvintsev,Wenzel Jakob,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: 论文提出了一种结合隐式解码器的神经细胞自动机（NCA）方法，解决了高分辨率输出的计算和内存问题，实现了实时生成高清图像的能力。


<details>
  <summary>Details</summary>
Motivation: NCA在低分辨率网格上表现优异，但在高分辨率下因计算和内存需求剧增、信息传播受限等问题难以扩展。

Method: 通过引入轻量级隐式解码器，在粗网格上运行NCA后解码为任意分辨率图像，并提出针对高分辨率任务的损失函数。

Result: 该方法显著提升了NCA在高分辨率下的质量和效率，实现了实时生成高清输出，并保持了自组织和涌现特性。

Conclusion: 结合隐式解码器的NCA框架能够高效扩展到高分辨率任务，适用于多种变体和应用场景。

Abstract: Neural Cellular Automata (NCAs) are bio-inspired systems in which identical
cells self-organize to form complex and coherent patterns by repeatedly
applying simple local rules. NCAs display striking emergent behaviors including
self-regeneration, generalization and robustness to unseen situations, and
spontaneous motion. Despite their success in texture synthesis and
morphogenesis, NCAs remain largely confined to low-resolution grids. This
limitation stems from (1) training time and memory requirements that grow
quadratically with grid size, (2) the strictly local propagation of information
which impedes long-range cell communication, and (3) the heavy compute demands
of real-time inference at high resolution. In this work, we overcome this
limitation by pairing NCA with a tiny, shared implicit decoder, inspired by
recent advances in implicit neural representations. Following NCA evolution on
a coarse grid, a lightweight decoder renders output images at arbitrary
resolution. We also propose novel loss functions for both morphogenesis and
texture synthesis tasks, specifically tailored for high-resolution output with
minimal memory and computation overhead. Combining our proposed architecture
and loss functions brings substantial improvement in quality, efficiency, and
performance. NCAs equipped with our implicit decoder can generate full-HD
outputs in real time while preserving their self-organizing, emergent
properties. Moreover, because each MLP processes cell states independently,
inference remains highly parallelizable and efficient. We demonstrate the
applicability of our approach across multiple NCA variants (on 2D, 3D grids,
and 3D meshes) and multiple tasks, including texture generation and
morphogenesis (growing patterns from a seed), showing that with our proposed
framework, NCAs seamlessly scale to high-resolution outputs with minimal
computational overhead.

</details>


### [16] [Preserve Anything: Controllable Image Synthesis with Object Preservation](https://arxiv.org/abs/2506.22531)
*Prasen Kumar Sharma,Neeraj Matiyali,Siddharth Srivastava,Gaurav Sharma*

Main category: cs.CV

TL;DR: 提出了一种名为“Preserve Anything”的新方法，用于解决文本到图像生成中的对象保留和语义一致性问题，通过多通道ControlNet实现高保真度和用户控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在保留多个对象、语义对齐和场景控制方面存在不足，需要一种更高效的方法。

Method: 采用N通道ControlNet，结合对象保留、背景引导和高频覆盖模块，确保细节保留和语义一致性。

Result: 在FID（15.26）和CLIP-S（32.85）指标上表现优异，用户研究显示显著提升。

Conclusion: 该方法在对象保留、语义一致性和用户控制方面优于现有技术。

Abstract: We introduce \textit{Preserve Anything}, a novel method for controlled image
synthesis that addresses key limitations in object preservation and semantic
consistency in text-to-image (T2I) generation. Existing approaches often fail
(i) to preserve multiple objects with fidelity, (ii) maintain semantic
alignment with prompts, or (iii) provide explicit control over scene
composition. To overcome these challenges, the proposed method employs an
N-channel ControlNet that integrates (i) object preservation with size and
placement agnosticism, color and detail retention, and artifact elimination,
(ii) high-resolution, semantically consistent backgrounds with accurate
shadows, lighting, and prompt adherence, and (iii) explicit user control over
background layouts and lighting conditions. Key components of our framework
include object preservation and background guidance modules, enforcing lighting
consistency and a high-frequency overlay module to retain fine details while
mitigating unwanted artifacts. We introduce a benchmark dataset consisting of
240K natural images filtered for aesthetic quality and 18K 3D-rendered
synthetic images with metadata such as lighting, camera angles, and object
relationships. This dataset addresses the deficiencies of existing benchmarks
and allows a complete evaluation. Empirical results demonstrate that our method
achieves state-of-the-art performance, significantly improving feature-space
fidelity (FID 15.26) and semantic alignment (CLIP-S 32.85) while maintaining
competitive aesthetic quality. We also conducted a user study to demonstrate
the efficacy of the proposed work on unseen benchmark and observed a remarkable
improvement of $\sim25\%$, $\sim19\%$, $\sim13\%$, and $\sim14\%$ in terms of
prompt alignment, photorealism, the presence of AI artifacts, and natural
aesthetics over existing works.

</details>


### [17] [MagShield: Towards Better Robustness in Sparse Inertial Motion Capture Under Magnetic Disturbances](https://arxiv.org/abs/2506.22907)
*Yunzhe Shao,Xinyu Yi,Lu Yin,Shihui Guo,Junhai Yong,Feng Xu*

Main category: cs.CV

TL;DR: MagShield是一种新方法，用于解决稀疏惯性动作捕捉系统中的磁干扰问题，通过检测和校正策略提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有IMU系统在磁干扰环境下易产生方向估计误差，限制了实际应用。

Method: 采用“检测-校正”策略，通过多IMU联合分析检测磁干扰，并利用人体运动先验校正方向误差。

Result: 实验表明，MagShield显著提高了磁干扰下的动作捕捉准确性，并与多种系统兼容。

Conclusion: MagShield有效解决了磁干扰问题，提升了稀疏惯性动作捕捉系统的实用性。

Abstract: This paper proposes a novel method called MagShield, designed to address the
issue of magnetic interference in sparse inertial motion capture (MoCap)
systems. Existing Inertial Measurement Unit (IMU) systems are prone to
orientation estimation errors in magnetically disturbed environments, limiting
their practical application in real-world scenarios. To address this problem,
MagShield employs a "detect-then-correct" strategy, first detecting magnetic
disturbances through multi-IMU joint analysis, and then correcting orientation
errors using human motion priors. MagShield can be integrated with most
existing sparse inertial MoCap systems, improving their performance in
magnetically disturbed environments. Experimental results demonstrate that
MagShield significantly enhances the accuracy of motion capture under magnetic
interference and exhibits good compatibility across different sparse inertial
MoCap systems.

</details>


### [18] [Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset](https://arxiv.org/abs/2506.22554)
*Vasu Agrawal,Akinniyi Akinyemi,Kathryn Alvero,Morteza Behrooz,Julia Buffalini,Fabio Maria Carlucci,Joy Chen,Junming Chen,Zhang Chen,Shiyang Cheng,Praveen Chowdary,Joe Chuang,Antony D'Avirro,Jon Daly,Ning Dong,Mark Duppenthaler,Cynthia Gao,Jeff Girard,Martin Gleize,Sahir Gomez,Hongyu Gong,Srivathsan Govindarajan,Brandon Han,Sen He,Denise Hernandez,Yordan Hristov,Rongjie Huang,Hirofumi Inaguma,Somya Jain,Raj Janardhan,Qingyao Jia,Christopher Klaiber,Dejan Kovachev,Moneish Kumar,Hang Li,Yilei Li,Pavel Litvin,Wei Liu,Guangyao Ma,Jing Ma,Martin Ma,Xutai Ma,Lucas Mantovani,Sagar Miglani,Sreyas Mohan,Louis-Philippe Morency,Evonne Ng,Kam-Woh Ng,Tu Anh Nguyen,Amia Oberai,Benjamin Peloquin,Juan Pino,Jovan Popovic,Omid Poursaeed,Fabian Prada,Alice Rakotoarison,Alexander Richard,Christophe Ropers,Safiyyah Saleem,Vasu Sharma,Alex Shcherbyna,Jia Shen,Jie Shen,Anastasis Stathopoulos,Anna Sun,Paden Tomasello,Tuan Tran,Arina Turkatenko,Bo Wan,Chao Wang,Jeff Wang,Mary Williamson,Carleigh Wood,Tao Xiang,Yilin Yang,Julien Yao,Chen Zhang,Jiemin Zhang,Xinyue Zhang,Jason Zheng,Pavlo Zhyzheria,Jan Zikes,Michael Zollhoefer*

Main category: cs.CV

TL;DR: 论文介绍了Seamless Interaction Dataset，用于开发能理解和生成双向行为动态的AI模型，并展示了相关模型及其应用。


<details>
  <summary>Details</summary>
Motivation: 开发社交智能AI技术，需理解并生成双向行为动态。

Method: 引入Seamless Interaction Dataset，开发模型生成与语音对齐的双向动作和表情。

Result: 模型能结合语音和视觉行为输入，生成动态响应，并支持情感和语义调整。

Conclusion: 该研究为更直观和响应式的人机交互提供了潜力。

Abstract: Human communication involves a complex interplay of verbal and nonverbal
signals, essential for conveying meaning and achieving interpersonal goals. To
develop socially intelligent AI technologies, it is crucial to develop models
that can both comprehend and generate dyadic behavioral dynamics. To this end,
we introduce the Seamless Interaction Dataset, a large-scale collection of over
4,000 hours of face-to-face interaction footage from over 4,000 participants in
diverse contexts. This dataset enables the development of AI technologies that
understand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,
telepresence experiences, and multimodal content analysis tools. We also
develop a suite of models that utilize the dataset to generate dyadic motion
gestures and facial expressions aligned with human speech. These models can
take as input both the speech and visual behavior of their interlocutors. We
present a variant with speech from an LLM model and integrations with 2D and 3D
rendering methods, bringing us closer to interactive virtual agents.
Additionally, we describe controllable variants of our motion models that can
adapt emotional responses and expressivity levels, as well as generating more
semantically-relevant gestures. Finally, we discuss methods for assessing the
quality of these dyadic motion models, which are demonstrating the potential
for more intuitive and responsive human-AI interactions.

</details>


### [19] [Recomposed realities: animating still images via patch clustering and randomness](https://arxiv.org/abs/2506.22556)
*Markus Juvonen,Samuli Siltanen*

Main category: cs.CV

TL;DR: 提出了一种基于图像块的重建与动画方法，利用现有图像数据为静态图像添加动态效果。


<details>
  <summary>Details</summary>
Motivation: 通过重新解释而非复制，使静态图像具有动态效果，同时允许源域和目标域在概念上不同但共享局部结构。

Method: 使用k-means聚类对图像块进行分组，并通过匹配和随机采样从这些聚类中重建新目标图像。

Result: 实现了静态图像的动态化，同时保留了局部结构的相似性。

Conclusion: 该方法为图像动画提供了一种新颖且灵活的重建方式。

Abstract: We present a patch-based image reconstruction and animation method that uses
existing image data to bring still images to life through motion. Image patches
from curated datasets are grouped using k-means clustering and a new target
image is reconstructed by matching and randomly sampling from these clusters.
This approach emphasizes reinterpretation over replication, allowing the source
and target domains to differ conceptually while sharing local structures.

</details>


### [20] [HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity](https://arxiv.org/abs/2506.23854)
*Yida Wang,Xueyang Zhang,Kun Zhan,Peng Jia,Xianpeng Lang*

Main category: cs.CV

TL;DR: HiNeuS是一个统一框架，解决了神经表面重建中的多视角辐射不一致、无纹理区域关键点缺失和Eikonal约束过强导致的结构退化问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂场景下难以平衡几何保真度和光度一致性，HiNeuS旨在通过统一管道解决这些核心限制。

Method: 引入差分可见性验证、平面共形正则化和物理基础的Eikonal松弛，实现外观与几何约束的协同优化。

Result: 在合成和真实数据集上表现优异，Chamfer距离减少21.4%，PSNR提升2.32 dB，能恢复高光物体和低纹理表面。

Conclusion: HiNeuS在神经表面重建和逆渲染任务中展现出卓越性能和通用性。

Abstract: Neural surface reconstruction faces persistent challenges in reconciling
geometric fidelity with photometric consistency under complex scene conditions.
We present HiNeuS, a unified framework that holistically addresses three core
limitations in existing approaches: multi-view radiance inconsistency, missing
keypoints in textureless regions, and structural degradation from over-enforced
Eikonal constraints during joint optimization. To resolve these issues through
a unified pipeline, we introduce: 1) Differential visibility verification
through SDF-guided ray tracing, resolving reflection ambiguities via continuous
occlusion modeling; 2) Planar-conformal regularization via ray-aligned geometry
patches that enforce local surface coherence while preserving sharp edges
through adaptive appearance weighting; and 3) Physically-grounded Eikonal
relaxation that dynamically modulates geometric constraints based on local
radiance gradients, enabling detail preservation without sacrificing global
regularity. Unlike prior methods that handle these aspects through sequential
optimizations or isolated modules, our approach achieves cohesive integration
where appearance-geometry constraints evolve synergistically throughout
training. Comprehensive evaluations across synthetic and real-world datasets
demonstrate state-of-the-art performance, including a 21.4% reduction in
Chamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement
against neural rendering counterparts. Qualitative analyses reveal superior
capability in recovering specular instruments, urban layouts with
centimeter-scale infrastructure, and low-textured surfaces without local patch
collapse. The method's generalizability is further validated through successful
application to inverse rendering tasks, including material decomposition and
view-consistent relighting.

</details>


### [21] [Improving Token-based Object Detection with Video](https://arxiv.org/abs/2506.22562)
*Abhineet Singh,Nilanjan Ray*

Main category: cs.CV

TL;DR: 论文改进了Pix2Seq目标检测器，将其扩展到视频领域，提出了一种新的端到端视频目标检测方法，解决了传统检测器的稀疏损失和启发式后处理问题。


<details>
  <summary>Details</summary>
Motivation: 传统视频目标检测方法需要采样所有可能的框，导致训练稀疏和推理后处理复杂，且通常将视频对象视为2D框的链接。本文旨在解决这些问题。

Method: 通过将对象表示为可变长度的离散标记序列，无需定位提示即可表示多样化的视频对象；将视频对象视为完整的3D框或轨迹，而非链接的2D框。

Result: 在多个数据集上优于静态Pix2Seq检测器，并在UA-DETRAC上与当前最优方法竞争。计算资源限制了性能提升。

Conclusion: 提出的方法在视频目标检测中表现优异，解决了传统方法的局限性，但计算资源是瓶颈。

Abstract: This paper improves upon the Pix2Seq object detector by extending it for
videos. In the process, it introduces a new way to perform end-to-end video
object detection that improves upon existing video detectors in two key ways.
First, by representing objects as variable-length sequences of discrete tokens,
we can succinctly represent widely varying numbers of video objects, with
diverse shapes and locations, without having to inject any localization cues in
the training process. This eliminates the need to sample the space of all
possible boxes that constrains conventional detectors and thus solves the dual
problems of loss sparsity during training and heuristics-based postprocessing
during inference. Second, it conceptualizes and outputs the video objects as
fully integrated and indivisible 3D boxes or tracklets instead of generating
image-specific 2D boxes and linking these boxes together to construct the video
object, as done in most conventional detectors. This allows it to scale
effortlessly with available computational resources by simply increasing the
length of the video subsequence that the network takes as input, even
generalizing to multi-object tracking if the subsequence can span the entire
video. We compare our video detector with the baseline Pix2Seq static detector
on several datasets and demonstrate consistent improvement, although with
strong signs of being bottlenecked by our limited computational resources. We
also compare it with several video detectors on UA-DETRAC to show that it is
competitive with the current state of the art even with the computational
bottleneck. We make our code and models publicly available.

</details>


### [22] [Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation](https://arxiv.org/abs/2506.22567)
*Shansong Wang,Zhecheng Jin,Mingzhe Hu,Mojtaba Safari,Feng Zhao,Chih-Wei Chang,Richard LJ Qiu,Justin Roper,David S. Yu,Xiaofeng Yang*

Main category: cs.CV

TL;DR: MMKD-CLIP通过多教师知识蒸馏构建高性能生物医学基础模型，解决了数据稀缺和异构性问题，在多种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 生物医学领域缺乏大规模图像-文本对数据，且数据模态和标准不统一，阻碍了通用生物医学基础模型的开发。

Method: 采用两阶段训练：首先在290万生物医学图像-文本对上进行CLIP式预训练，然后从9个教师模型中提取1920万特征对进行特征级蒸馏。

Result: 在58个生物医学数据集上评估，涵盖1080万图像和9种模态，MMKD-CLIP在6类任务中均优于教师模型，表现出强鲁棒性和泛化能力。

Conclusion: 多教师知识蒸馏是构建高性能生物医学基础模型的有效方法，适用于实际数据限制场景。

Abstract: CLIP models pretrained on natural images with billion-scale image-text pairs
have demonstrated impressive capabilities in zero-shot classification,
cross-modal retrieval, and open-ended visual answering. However, transferring
this success to biomedicine is hindered by the scarcity of large-scale
biomedical image-text corpora, the heterogeneity of image modalities, and
fragmented data standards across institutions. These limitations hinder the
development of a unified and generalizable biomedical foundation model trained
from scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical
foundation model developed via Multiple Medical CLIP Knowledge Distillation.
Rather than relying on billion-scale raw data, MMKD-CLIP distills knowledge
from nine state-of-the-art domain-specific or generalist biomedical CLIP
models, each pretrained on millions of biomedical image-text pairs. Our
two-stage training pipeline first performs CLIP-style pretraining on over 2.9
million biomedical image-text pairs from 26 image modalities, followed by
feature-level distillation using over 19.2 million feature pairs extracted from
teacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets,
encompassing over 10.8 million biomedical images across nine image modalities.
The evaluation spans six core task types: zero-shot classification, linear
probing, cross-modal retrieval, visual question answering, survival prediction,
and cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models
while demonstrating remarkable robustness and generalization across image
domains and task settings. These results underscore that multi-teacher
knowledge distillation is a scalable and effective paradigm for building
high-performing biomedical foundation models under the practical constraints of
real-world data availability.

</details>


### [23] [Dual Atrous Separable Convolution for Improving Agricultural Semantic Segmentation](https://arxiv.org/abs/2506.22570)
*Chee Mei Ling,Thangarajah Akilan,Aparna Ravinda Phalke*

Main category: cs.CV

TL;DR: 提出了一种基于DeepLabV3的高效农业图像语义分割方法，通过引入DAS Conv模块和优化跳跃连接，在保持低计算复杂度的同时，性能接近复杂Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 农业图像语义分割对精准农业至关重要，但现有方法在效率和性能之间难以平衡。

Method: 集成DAS Conv模块优化膨胀率和填充尺寸，并设计跳跃连接以捕捉细粒度空间特征。

Result: 在Agriculture Vision数据集上，模型性能接近SOTA Transformer模型，且效率提升66%。

Conclusion: 该方法为农业遥感提供了一种高效、轻量且高性能的语义分割解决方案。

Abstract: Agricultural image semantic segmentation is a pivotal component of modern
agriculture, facilitating accurate visual data analysis to improve crop
management, optimize resource utilization, and boost overall productivity. This
study proposes an efficient image segmentation method for precision
agriculture, focusing on accurately delineating farmland anomalies to support
informed decision-making and proactive interventions. A novel Dual Atrous
Separable Convolution (DAS Conv) module is integrated within the
DeepLabV3-based segmentation framework. The DAS Conv module is meticulously
designed to achieve an optimal balance between dilation rates and padding size,
thereby enhancing model performance without compromising efficiency. The study
also incorporates a strategic skip connection from an optimal stage in the
encoder to the decoder to bolster the model's capacity to capture fine-grained
spatial features. Despite its lower computational complexity, the proposed
model outperforms its baseline and achieves performance comparable to highly
complex transformer-based state-of-the-art (SOTA) models on the Agriculture
Vision benchmark dataset. It achieves more than 66% improvement in efficiency
when considering the trade-off between model complexity and performance,
compared to the SOTA model. This study highlights an efficient and effective
solution for improving semantic segmentation in remote sensing applications,
offering a computationally lightweight model capable of high-quality
performance in agricultural imagery.

</details>


### [24] [LIGHT: Multi-Modal Text Linking on Historical Maps](https://arxiv.org/abs/2506.22589)
*Yijun Lin,Rhett Olson,Junhan Wu,Yao-Yi Chiang,Jerod Weinman*

Main category: cs.CV

TL;DR: LIGHT是一种多模态方法，结合语言、图像和几何特征，用于链接历史地图上的文本，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 历史地图上的文本信息对多领域研究至关重要，但现有方法难以有效链接文本片段，尤其是多词地名。

Method: LIGHT通过几何感知嵌入模块编码文本区域的多边形坐标，结合LayoutLMv3的视觉和语言特征，预测文本实例的阅读顺序。

Result: LIGHT在ICDAR 2024/2025 MapText竞赛数据上表现优于现有方法。

Conclusion: 多模态学习在历史地图文本链接中具有显著优势。

Abstract: Text on historical maps provides valuable information for studies in history,
economics, geography, and other related fields. Unlike structured or
semi-structured documents, text on maps varies significantly in orientation,
reading order, shape, and placement. Many modern methods can detect and
transcribe text regions, but they struggle to effectively ``link'' the
recognized text fragments, e.g., determining a multi-word place name. Existing
layout analysis methods model word relationships to improve text understanding
in structured documents, but they primarily rely on linguistic features and
neglect geometric information, which is essential for handling map text. To
address these challenges, we propose LIGHT, a novel multi-modal approach that
integrates linguistic, image, and geometric features for linking text on
historical maps. In particular, LIGHT includes a geometry-aware embedding
module that encodes the polygonal coordinates of text regions to capture
polygon shapes and their relative spatial positions on an image. LIGHT unifies
this geometric information with the visual and linguistic token embeddings from
LayoutLMv3, a pretrained layout analysis model. LIGHT uses the cross-modal
information to predict the reading-order successor of each text instance
directly with a bi-directional learning strategy that enhances sequence
robustness. Experimental results show that LIGHT outperforms existing methods
on the ICDAR 2024/2025 MapText Competition data, demonstrating the
effectiveness of multi-modal learning for historical map text linking.

</details>


### [25] [BrainMT: A Hybrid Mamba-Transformer Architecture for Modeling Long-Range Dependencies in Functional MRI Data](https://arxiv.org/abs/2506.22591)
*Arunkumar Kannan,Martin A. Lindquist,Brian Caffo*

Main category: cs.CV

TL;DR: BrainMT是一种新型混合框架，通过双向Mamba块和Transformer块高效建模fMRI数据的时空依赖性，显著提升了分类和回归任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉fMRI数据中的长程时空依赖关系，限制了预测性能。

Method: BrainMT采用两阶段框架：双向Mamba块捕获全局时间交互，Transformer块建模空间关系。

Result: 在UKBioBank和Human Connectome Project数据集上，BrainMT在分类和回归任务中均达到最优性能。

Conclusion: BrainMT通过高效建模时空依赖，显著优于现有方法，代码将公开。

Abstract: Recent advances in deep learning have made it possible to predict phenotypic
measures directly from functional magnetic resonance imaging (fMRI) brain
volumes, sparking significant interest in the neuroimaging community. However,
existing approaches, primarily based on convolutional neural networks or
transformer architectures, often struggle to model the complex relationships
inherent in fMRI data, limited by their inability to capture long-range spatial
and temporal dependencies. To overcome these shortcomings, we introduce
BrainMT, a novel hybrid framework designed to efficiently learn and integrate
long-range spatiotemporal attributes in fMRI data. Our framework operates in
two stages: (1) a bidirectional Mamba block with a temporal-first scanning
mechanism to capture global temporal interactions in a computationally
efficient manner; and (2) a transformer block leveraging self-attention to
model global spatial relationships across the deep features processed by the
Mamba block. Extensive experiments on two large-scale public datasets,
UKBioBank and the Human Connectome Project, demonstrate that BrainMT achieves
state-of-the-art performance on both classification (sex prediction) and
regression (cognitive intelligence prediction) tasks, outperforming existing
methods by a significant margin. Our code and implementation details will be
made publicly available at this
https://github.com/arunkumar-kannan/BrainMT-fMRI

</details>


### [26] [Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning](https://arxiv.org/abs/2506.22624)
*Zuyao You,Zuxuan Wu*

Main category: cs.CV

TL;DR: Seg-R1使用强化学习增强大型多模态模型的像素级理解能力，通过GRPO策略在分割任务中表现优异，并展示了强大的开放世界泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用强化学习提升大型多模态模型在像素级任务（如前景分割）中的理解和推理能力。

Method: 采用Group Relative Policy Optimization (GRPO)策略，通过点提示和边界框提示引导SAM2生成分割掩码，仅使用RL训练。

Result: 在COD10K上达到0.873 S-measure，在RefCOCOg和ReasonSeg上分别实现71.4 cIoU和56.7 gIoU的零样本性能。

Conclusion: 纯RL训练在分割任务中表现优异，且具备强大的开放世界泛化能力，无需复杂模型修改或文本监督。

Abstract: We present Seg-R1, a preliminary exploration of using reinforcement learning
(RL) to enhance the pixel-level understanding and reasoning capabilities of
large multimodal models (LMMs). Starting with foreground segmentation tasks,
specifically camouflaged object detection (COD) and salient object detection
(SOD), our approach enables the LMM to generate point and bounding box prompts
in the next-token fashion, which are then used to guide SAM2 in producing
segmentation masks. We introduce Group Relative Policy Optimization (GRPO) into
the segmentation domain, equipping the LMM with pixel-level comprehension
through a carefully designed training strategy. Notably, Seg-R1 achieves
remarkable performance with purely RL-based training, achieving .873 S-measure
on COD10K without complex model modification. Moreover, we found that pure RL
training demonstrates strong open-world generalization. Despite being trained
solely on foreground segmentation image-mask pairs without text supervision,
Seg-R1 achieves impressive zero-shot performance on referring segmentation and
reasoning segmentation tasks, with 71.4 cIoU on RefCOCOg test and 56.7 gIoU on
ReasonSeg test, outperforming models fully supervised on these datasets.

</details>


### [27] [ReCo: Reminder Composition Mitigates Hallucinations in Vision-Language Models](https://arxiv.org/abs/2506.22636)
*Sotirios Panagiotis Chytas,Miso Choi,Hyunwoo J. Kim,Vikas Singh*

Main category: cs.CV

TL;DR: 论文提出了一种名为ReCo的轻量级模块，用于缓解视觉语言模型（VLMs）中的幻觉问题，通过几何代数和关系组合的方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在生成文本时容易产生幻觉（即生成与视觉输入无关或矛盾的文本），主要原因是模型对语言的过度依赖和视觉输入的‘记忆衰减效应’。

Method: 提出了一种基于几何代数和关系组合的小型可训练模块（ReCo），无需修改现有VLM结构，直接应用于模型顶部。

Result: 在InstructBLIP、LlaVA和MiniGPT4三种广泛使用的VLM上验证了ReCo模块的有效性，性能在多个基准测试中均有提升。此外，该模块还能与其他减少幻觉的方法结合使用，进一步改善结果。

Conclusion: ReCo模块是一种轻量且通用的解决方案，能够有效缓解VLMs中的幻觉问题，并与其他方法兼容，具有广泛的应用潜力。

Abstract: Vision Language Models (VLMs) show impressive capabilities in integrating and
reasoning with both visual and language data. But these models make mistakes. A
common finding -- similar to LLMs -- is their tendency to hallucinate, i.e.,
generate plausible sounding text which is not grounded in the visual input, or
at worst, is contradictory. A growing consensus attributes this behavior to an
over-reliance on language -- especially as the generation progresses, the model
suffers from a ``fading memory effect'' with respect to the provided visual
input. We study mechanisms by which this behavior can be controlled.
Specifically, using ideas from geometric algebra and relational compositions,
we propose the addition of a small, trainable module (named ReCo) on top of any
VLM -- no other modification is needed. We show that such a lightweight module
is able to mitigate the fading memory effect on three of the most widely used
VLMs (InstructBLIP, LlaVA, MiniGPT4), where we see performance improvements on
multiple benchmarks. Additionally, we show that our module can be combined with
many of the other approaches for reducing hallucination where we achieve
improved results for each one.

</details>


### [28] [CaO$_2$: Rectifying Inconsistencies in Diffusion-Based Dataset Distillation](https://arxiv.org/abs/2506.22637)
*Haoxuan Wang,Zhenghao Zhao,Junyi Wu,Yuzhang Shang,Gaowen Liu,Yan Yan*

Main category: cs.CV

TL;DR: 论文提出了一种名为CaO$_2$的两阶段扩散框架，解决了当前基于扩散的数据集蒸馏方法中的目标不一致和条件不一致问题，并在ImageNet及其子集上实现了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散的数据集蒸馏方法在评估过程中存在目标不一致和条件不一致的问题，影响了性能。

Method: 提出了CaO$_2$框架，包括概率信息样本选择管道和潜在表示细化两个阶段，以优化蒸馏过程。

Result: 在ImageNet及其子集上，CaO$_2$的平均准确率比最佳基线方法高出2.3%。

Conclusion: CaO$_2$通过解决目标不一致和条件不一致问题，显著提升了数据集蒸馏的性能。

Abstract: The recent introduction of diffusion models in dataset distillation has shown
promising potential in creating compact surrogate datasets for large,
high-resolution target datasets, offering improved efficiency and performance
over traditional bi-level/uni-level optimization methods. However, current
diffusion-based dataset distillation approaches overlook the evaluation process
and exhibit two critical inconsistencies in the distillation process: (1)
Objective Inconsistency, where the distillation process diverges from the
evaluation objective, and (2) Condition Inconsistency, leading to mismatches
between generated images and their corresponding conditions. To resolve these
issues, we introduce Condition-aware Optimization with Objective-guided
Sampling (CaO$_2$), a two-stage diffusion-based framework that aligns the
distillation process with the evaluation objective. The first stage employs a
probability-informed sample selection pipeline, while the second stage refines
the corresponding latent representations to improve conditional likelihood.
CaO$_2$ achieves state-of-the-art performance on ImageNet and its subsets,
surpassing the best-performing baselines by an average of 2.3% accuracy.

</details>


### [29] [3D Shape Generation: A Survey](https://arxiv.org/abs/2506.22678)
*Nicolas Caytuiro,Ivan Sipiran*

Main category: cs.CV

TL;DR: 本文综述了深度学习在3D形状生成领域的最新进展，围绕形状表示、生成方法和评估协议三大核心组件展开讨论。


<details>
  <summary>Details</summary>
Motivation: 深度学习在3D形状生成领域的快速发展催生了对现有技术进行系统性总结的需求，以帮助研究者和从业者更好地理解这一领域。

Method: 文章首先分类了3D形状的表示方法（显式、隐式和混合），然后回顾了生成方法（特别是前馈架构），并总结了常用数据集和评估指标。

Result: 综述提供了对3D形状生成领域的全面概述，包括技术分类、方法回顾和评估标准。

Conclusion: 文章指出了当前研究的开放挑战，并提出了未来研究方向，旨在推动可控、高效和高质量的3D形状生成技术的发展。

Abstract: Recent advances in deep learning have significantly transformed the field of
3D shape generation, enabling the synthesis of complex, diverse, and
semantically meaningful 3D objects. This survey provides a comprehensive
overview of the current state of the art in 3D shape generation, organizing the
discussion around three core components: shape representations, generative
modeling approaches, and evaluation protocols. We begin by categorizing 3D
representations into explicit, implicit, and hybrid setups, highlighting their
structural properties, advantages, and limitations. Next, we review a wide
range of generation methods, focusing on feedforward architectures. We further
summarize commonly used datasets and evaluation metrics that assess fidelity,
diversity, and realism of generated shapes. Finally, we identify open
challenges and outline future research directions that could drive progress in
controllable, efficient, and high-quality 3D shape generation. This survey aims
to serve as a valuable reference for researchers and practitioners seeking a
structured and in-depth understanding of this rapidly evolving field.

</details>


### [30] [LightBSR: Towards Lightweight Blind Super-Resolution via Discriminative Implicit Degradation Representation Learning](https://arxiv.org/abs/2506.22710)
*Jiang Yuan,JI Ma,Bo Wang,Guanzhou Ke,Weiming Hu*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的盲超分辨率模型LightBSR，通过优化隐式退化表示（IDR）的判别性，解决了现有方法中参数和计算量过大的问题。


<details>
  <summary>Details</summary>
Motivation: 现有IDE-BSR方法忽视了IDR判别性的重要性，导致模型复杂度过高，本文旨在优化IDR判别性并设计高效模型。

Method: 采用基于知识蒸馏的学习框架，包括退化先验约束的对比学习技术和特征对齐技术，以提升IDR判别性并简化模型。

Result: 实验表明，LightBSR在多种盲超分辨率任务中表现出色，且复杂度极低。

Conclusion: LightBSR通过优化IDR判别性，实现了高效且高性能的盲超分辨率，为相关领域提供了新思路。

Abstract: Implicit degradation estimation-based blind super-resolution (IDE-BSR) hinges
on extracting the implicit degradation representation (IDR) of the LR image and
adapting it to LR image features to guide HR detail restoration. Although
IDE-BSR has shown potential in dealing with noise interference and complex
degradations, existing methods ignore the importance of IDR discriminability
for BSR and instead over-complicate the adaptation process to improve effect,
resulting in a significant increase in the model's parameters and computations.
In this paper, we focus on the discriminability optimization of IDR and propose
a new powerful and lightweight BSR model termed LightBSR. Specifically, we
employ a knowledge distillation-based learning framework. We first introduce a
well-designed degradation-prior-constrained contrastive learning technique
during teacher stage to make the model more focused on distinguishing different
degradation types. Then we utilize a feature alignment technique to transfer
the degradation-related knowledge acquired by the teacher to the student for
practical inferencing. Extensive experiments demonstrate the effectiveness of
IDR discriminability-driven BSR model design. The proposed LightBSR can achieve
outstanding performance with minimal complexity across a range of blind SR
tasks. Our code is accessible at: https://github.com/MJ-NCEPU/LightBSR.

</details>


### [31] [Part Segmentation and Motion Estimation for Articulated Objects with Dynamic 3D Gaussians](https://arxiv.org/abs/2506.22718)
*Jun-Jee Chao,Qingyuan Jiang,Volkan Isler*

Main category: cs.CV

TL;DR: 提出一种联合解决部分分割和运动估计的方法，通过3D高斯表示处理点云序列，适用于动态采样场景。


<details>
  <summary>Details</summary>
Motivation: 解决动态点云序列中的部分分割和运动估计问题，尤其是在点云非固定采样或存在遮挡的情况下。

Method: 使用3D高斯表示物体，参数化时间相关的旋转、平移和缩放，通过点与高斯的对应关系实现分割和运动估计。

Result: 在遮挡场景下，部分分割性能优于现有方法13%，且对缺失点更鲁棒。

Conclusion: 该方法在动态点云处理中表现优异，尤其适用于遮挡和非固定采样场景。

Abstract: Part segmentation and motion estimation are two fundamental problems for
articulated object motion analysis. In this paper, we present a method to solve
these two problems jointly from a sequence of observed point clouds of a single
articulated object. The main challenge in our problem setting is that the point
clouds are not assumed to be generated by a fixed set of moving points.
Instead, each point cloud in the sequence could be an arbitrary sampling of the
object surface at that particular time step. Such scenarios occur when the
object undergoes major occlusions, or if the dataset is collected using
measurements from multiple sensors asynchronously. In these scenarios, methods
that rely on tracking point correspondences are not appropriate. We present an
alternative approach based on a compact but effective representation where we
represent the object as a collection of simple building blocks modeled as 3D
Gaussians. We parameterize the Gaussians with time-dependent rotations,
translations, and scales that are shared across all time steps. With our
representation, part segmentation can be achieved by building correspondences
between the observed points and the Gaussians. Moreover, the transformation of
each point across time can be obtained by following the poses of the assigned
Gaussian (even when the point is not observed). Experiments show that our
method outperforms existing methods that solely rely on finding point
correspondences. Additionally, we extend existing datasets to emulate
real-world scenarios by considering viewpoint occlusions. We further
demonstrate that our method is more robust to missing points as compared to
existing approaches on these challenging datasets, even when some parts are
completely occluded in some time-steps. Notably, our part segmentation
performance outperforms the state-of-the-art method by 13% on point clouds with
occlusions.

</details>


### [32] [Deterministic Object Pose Confidence Region Estimation](https://arxiv.org/abs/2506.22720)
*Jinghao Wang,Zhang Li,Zi Wang,Banglei Guan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出了一种确定性方法，用于高效估计6D位姿置信区域，解决了采样方法速度慢和区域过大的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于采样的6D位姿置信区域估计方法存在速度慢和置信区域过大的问题，限制了实际应用。

Method: 使用归纳共形预测校准高斯关键点分布，并通过隐函数定理直接传播到6D位姿置信区域。

Result: 在LineMOD Occlusion和SPEED数据集上，方法提高了位姿估计精度，减少了计算时间，置信区域体积显著缩小（旋转99.9%，平移99.8%）。

Conclusion: 该方法高效且准确，提供了紧凑的置信区域，适用于实际部署。

Abstract: 6D pose confidence region estimation has emerged as a critical direction,
aiming to perform uncertainty quantification for assessing the reliability of
estimated poses. However, current sampling-based approach suffers from critical
limitations that severely impede their practical deployment: 1) the sampling
speed significantly decreases as the number of samples increases. 2) the
derived confidence regions are often excessively large. To address these
challenges, we propose a deterministic and efficient method for estimating pose
confidence regions. Our approach uses inductive conformal prediction to
calibrate the deterministically regressed Gaussian keypoint distributions into
2D keypoint confidence regions. We then leverage the implicit function theorem
to propagate these keypoint confidence regions directly into 6D pose confidence
regions. This method avoids the inefficiency and inflated region sizes
associated with sampling and ensembling. It provides compact confidence regions
that cover the ground-truth poses with a user-defined confidence level.
Experimental results on the LineMOD Occlusion and SPEED datasets show that our
method achieves higher pose estimation accuracy with reduced computational
time. For the same coverage rate, our method yields significantly smaller
confidence region volumes, reducing them by up to 99.9\% for rotations and
99.8\% for translations. The code will be available soon.

</details>


### [33] [XTransfer: Cross-Modality Model Transfer for Human Sensing with Few Data at the Edge](https://arxiv.org/abs/2506.22726)
*Yu Zhang,Xi Zhang,Hualin zhou,Xinyuan Chen,Shang Gao,Hong Jia,Jianfei Yang,Yuankai Qi,Tao Gu*

Main category: cs.CV

TL;DR: XTransfer是一种资源高效、模态无关的模型迁移方法，通过模型修复和层重组解决边缘系统中深度学习模型的模态偏移和资源限制问题。


<details>
  <summary>Details</summary>
Motivation: 边缘系统中深度学习模型的训练和开发受限于传感器数据的稀缺和资源限制，现有方法存在模态偏移、资源需求高和适应性差的问题。

Method: XTransfer通过模型修复（修复模态偏移）和层重组（高效搜索和重组源模型层）实现资源高效的模型迁移。

Result: XTransfer在多种人类感知任务中达到最先进性能，显著降低了数据收集、模型训练和边缘部署的成本。

Conclusion: XTransfer为边缘系统中的人类感知任务提供了一种高效、适应性强的解决方案。

Abstract: Deep learning for human sensing on edge systems offers significant
opportunities for smart applications. However, its training and development are
hindered by the limited availability of sensor data and resource constraints of
edge systems. Current methods that rely on transferring pre-trained models
often encounter issues such as modality shift and high resource demands,
resulting in substantial accuracy loss, resource overhead, and poor
adaptability across different sensing applications. In this paper, we propose
XTransfer, a first-of-its-kind method for resource-efficient, modality-agnostic
model transfer. XTransfer freely leverages single or multiple pre-trained
models and transfers knowledge across different modalities by (i) model
repairing that safely repairs modality shift in pre-trained model layers with
only few sensor data, and (ii) layer recombining that efficiently searches and
recombines layers of interest from source models in a layer-wise manner to
create compact models. We benchmark various baselines across diverse human
sensing datasets spanning different modalities. Comprehensive results
demonstrate that XTransfer achieves state-of-the-art performance on human
sensing tasks while significantly reducing the costs of sensor data collection,
model training, and edge deployment.

</details>


### [34] [UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image Fusion Under Diverse Degradations and Misalignments](https://arxiv.org/abs/2506.22736)
*Dayong Su,Yafei Zhang,Huafeng Li,Jinxing Li,Yu Liu*

Main category: cs.CV

TL;DR: UniFuse是一个通用的多模态医学图像融合框架，通过降解感知提示学习模块和Omni统一特征表示方案，实现了对齐、恢复和融合的联合优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设源图像质量高且像素级对齐，但在处理未对齐或降质图像时效果不佳，因此需要一种更通用的解决方案。

Method: UniFuse结合降解感知提示学习模块、Omni统一特征表示方案和自适应LoRA协同网络（ALSN），实现单阶段的对齐、恢复和融合。

Result: 实验结果表明，UniFuse在多个数据集上优于现有方法，具有显著优势。

Conclusion: UniFuse通过统一框架解决了多模态医学图像融合中的对齐和降质问题，为实际应用提供了更高效的解决方案。

Abstract: Current multimodal medical image fusion typically assumes that source images
are of high quality and perfectly aligned at the pixel level. Its effectiveness
heavily relies on these conditions and often deteriorates when handling
misaligned or degraded medical images. To address this, we propose UniFuse, a
general fusion framework. By embedding a degradation-aware prompt learning
module, UniFuse seamlessly integrates multi-directional information from input
images and correlates cross-modal alignment with restoration, enabling joint
optimization of both tasks within a unified framework. Additionally, we design
an Omni Unified Feature Representation scheme, which leverages Spatial Mamba to
encode multi-directional features and mitigate modality differences in feature
alignment. To enable simultaneous restoration and fusion within an All-in-One
configuration, we propose a Universal Feature Restoration & Fusion module,
incorporating the Adaptive LoRA Synergistic Network (ALSN) based on LoRA
principles. By leveraging ALSN's adaptive feature representation along with
degradation-type guidance, we enable joint restoration and fusion within a
single-stage framework. Compared to staged approaches, UniFuse unifies
alignment, restoration, and fusion within a single framework. Experimental
results across multiple datasets demonstrate the method's effectiveness and
significant advantages over existing approaches.

</details>


### [35] [Deep Learning based Joint Geometry and Attribute Up-sampling for Large-Scale Colored Point Clouds](https://arxiv.org/abs/2506.22749)
*Yun Zhang,Feifan Chen,Na Li,Zhiwei Guo,Xu Wang,Fen Miao,Sam Kwong*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的联合几何和属性上采样方法（JGAU），用于生成高质量的大规模彩色点云，并通过实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 彩色点云是3D应用的主流表示，但现有方法难以同时处理几何和属性上采样，因此需要一种联合优化的方法。

Method: 提出JGAU框架，包含几何上采样网络和属性上采样网络，利用几何辅助信息建模属性相关性，并通过两种粗属性上采样方法和属性增强模块优化结果。

Result: JGAU在4倍、8倍、12倍和16倍上采样率下的PSNR分别为33.90、32.10、31.10和30.39分贝，优于现有方法。

Conclusion: JGAU通过联合优化几何和属性上采样，显著提升了彩色点云的质量，为3D应用提供了更高效的解决方案。

Abstract: Colored point cloud, which includes geometry and attribute components, is a
mainstream representation enabling realistic and immersive 3D applications. To
generate large-scale and denser colored point clouds, we propose a deep
learning-based Joint Geometry and Attribute Up-sampling (JGAU) method that
learns to model both geometry and attribute patterns while leveraging spatial
attribute correlations. First, we establish and release a large-scale dataset
for colored point cloud up-sampling called SYSU-PCUD, containing 121
large-scale colored point clouds with diverse geometry and attribute
complexities across six categories and four sampling rates. Second, to improve
the quality of up-sampled point clouds, we propose a deep learning-based JGAU
framework that jointly up-samples geometry and attributes. It consists of a
geometry up-sampling network and an attribute up-sampling network, where the
latter leverages the up-sampled auxiliary geometry to model neighborhood
correlations of the attributes. Third, we propose two coarse attribute
up-sampling methods, Geometric Distance Weighted Attribute Interpolation
(GDWAI) and Deep Learning-based Attribute Interpolation (DLAI), to generate
coarse up-sampled attributes for each point. Then, an attribute enhancement
module is introduced to refine these up-sampled attributes and produce
high-quality point clouds by further exploiting intrinsic attribute and
geometry patterns. Extensive experiments show that the Peak Signal-to-Noise
Ratio (PSNR) achieved by the proposed JGAU method is 33.90 decibels, 32.10
decibels, 31.10 decibels, and 30.39 decibels for up-sampling rates of 4 times,
8 times, 12 times, and 16 times, respectively. Compared to state-of-the-art
methods, JGAU achieves average PSNR gains of 2.32 decibels, 2.47 decibels, 2.28
decibels, and 2.11 decibels at these four up-sampling rates, demonstrating
significant improvement.

</details>


### [36] [Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography](https://arxiv.org/abs/2506.22753)
*Jianing Zhang,Jiayi Zhu,Feiyu Ji,Xiaokang Yang,Xiaoyun Yuan*

Main category: cs.CV

TL;DR: 论文提出了一种基于预训练模型的多路径扩散方法（Degradation-Modeled Multipath Diffusion），用于解决金属透镜计算成像中的光学退化和计算恢复问题，无需依赖大量配对数据或精确校准。


<details>
  <summary>Details</summary>
Motivation: 金属透镜在超紧凑计算成像中潜力巨大，但面临复杂光学退化和计算恢复的挑战。现有方法通常依赖精确校准或大量配对数据，且缺乏对推理过程的控制，容易产生伪影。

Method: 提出多路径扩散框架，利用预训练模型的自然图像先验，通过正、中、负提示路径平衡高频细节生成、结构保真度和退化抑制，并结合伪数据增强。引入可调解码器和空间变化退化感知注意力（SVDA）模块。

Result: 实验结果表明，该方法优于现有技术，实现了高保真和清晰的图像重建。

Conclusion: 该方法为金属透镜摄影提供了一种无需大量数据的高效解决方案，通过多路径扩散和自适应退化建模，显著提升了成像质量。

Abstract: Metalenses offer significant potential for ultra-compact computational
imaging but face challenges from complex optical degradation and computational
restoration difficulties. Existing methods typically rely on precise optical
calibration or massive paired datasets, which are non-trivial for real-world
imaging systems. Furthermore, a lack of control over the inference process
often results in undesirable hallucinated artifacts. We introduce
Degradation-Modeled Multipath Diffusion for tunable metalens photography,
leveraging powerful natural image priors from pretrained models instead of
large datasets. Our framework uses positive, neutral, and negative-prompt paths
to balance high-frequency detail generation, structural fidelity, and
suppression of metalens-specific degradation, alongside \textit{pseudo} data
augmentation. A tunable decoder enables controlled trade-offs between fidelity
and perceptual quality. Additionally, a spatially varying degradation-aware
attention (SVDA) module adaptively models complex optical and sensor-induced
degradation. Finally, we design and build a millimeter-scale MetaCamera for
real-world validation. Extensive results show that our approach outperforms
state-of-the-art methods, achieving high-fidelity and sharp image
reconstruction. More materials: https://dmdiff.github.io/.

</details>


### [37] [RoboPearls: Editable Video Simulation for Robot Manipulation](https://arxiv.org/abs/2506.22756)
*Tao Tang,Likui Zhang,Youpeng Wen,Kaidong Zhang,Jia-Wang Bian,xia zhou,Tianyi Yan,Kun Zhan,Peng Jia,Hefeng Wu,Liang Lin,Xiaodan Liang*

Main category: cs.CV

TL;DR: RoboPearls是一个基于3D高斯散射的可编辑视频仿真框架，用于机器人操作，通过结合大语言模型和视觉语言模型，实现高效仿真和性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界演示数据采集成本高、效率低的问题，以及缩小仿真与现实的差距。

Method: 利用3D高斯散射技术构建逼真仿真，结合增量语义蒸馏和3D正则化NNFM损失，并通过大语言模型和视觉语言模型自动化仿真过程。

Result: 在多个数据集和场景（如RLBench、COLOSSEUM等）上展示了满意的仿真性能。

Conclusion: RoboPearls为机器人操作提供了一种高效、可扩展的仿真解决方案。

Abstract: The development of generalist robot manipulation policies has seen
significant progress, driven by large-scale demonstration data across diverse
environments. However, the high cost and inefficiency of collecting real-world
demonstrations hinder the scalability of data acquisition. While existing
simulation platforms enable controlled environments for robotic learning, the
challenge of bridging the sim-to-real gap remains. To address these challenges,
we propose RoboPearls, an editable video simulation framework for robotic
manipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the
construction of photo-realistic, view-consistent simulations from demonstration
videos, and supports a wide range of simulation operators, including various
object manipulations, powered by advanced modules like Incremental Semantic
Distillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by
incorporating large language models (LLMs), RoboPearls automates the simulation
production process in a user-friendly manner through flexible command
interpretation and execution. Furthermore, RoboPearls employs a vision-language
model (VLM) to analyze robotic learning issues to close the simulation loop for
performance enhancement. To demonstrate the effectiveness of RoboPearls, we
conduct extensive experiments on multiple datasets and scenes, including
RLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which
demonstrate our satisfactory simulation performance.

</details>


### [38] [VSRM: A Robust Mamba-Based Framework for Video Super-Resolution](https://arxiv.org/abs/2506.22762)
*Dinh Phu Tran,Dao Duy Hung,Daeyoung Kim*

Main category: cs.CV

TL;DR: VSRM是一种基于Mamba的视频超分辨率框架，通过空间-时间和时间-空间Mamba块提取长范围时空特征，并提出可变形交叉Mamba对齐模块和频率损失函数，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 视频超分辨率任务中，CNN和Transformer方法存在局部感受野限制和二次复杂度问题，而Mamba因其长序列建模能力和线性复杂度成为潜在解决方案。

Method: VSRM引入空间-时间和时间-空间Mamba块提取特征，使用可变形交叉Mamba对齐模块动态对齐帧，并提出频率Charbonnier-like损失函数优化高频内容。

Result: VSRM在多个基准测试中取得了最先进的性能。

Conclusion: VSRM为视频超分辨率任务提供了高效且灵活的解决方案，为未来研究奠定了基础。

Abstract: Video super-resolution remains a major challenge in low-level vision tasks.
To date, CNN- and Transformer-based methods have delivered impressive results.
However, CNNs are limited by local receptive fields, while Transformers
struggle with quadratic complexity, posing challenges for processing long
sequences in VSR. Recently, Mamba has drawn attention for its long-sequence
modeling, linear complexity, and large receptive fields. In this work, we
propose VSRM, a novel \textbf{V}ideo \textbf{S}uper-\textbf{R}esolution
framework that leverages the power of \textbf{M}amba. VSRM introduces
Spatial-to-Temporal Mamba and Temporal-to-Spatial Mamba blocks to extract
long-range spatio-temporal features and enhance receptive fields efficiently.
To better align adjacent frames, we propose Deformable Cross-Mamba Alignment
module. This module utilizes a deformable cross-mamba mechanism to make the
compensation stage more dynamic and flexible, preventing feature distortions.
Finally, we minimize the frequency domain gaps between reconstructed and
ground-truth frames by proposing a simple yet effective Frequency
Charbonnier-like loss that better preserves high-frequency content and enhances
visual quality. Through extensive experiments, VSRM achieves state-of-the-art
results on diverse benchmarks, establishing itself as a solid foundation for
future research.

</details>


### [39] [Point Cloud Compression and Objective Quality Assessment: A Survey](https://arxiv.org/abs/2506.22902)
*Yiling Xu,Yujie Zhang,Shuting Xia,Kaifa Yang,He Huang,Ziyu Shan,Wenjie Huang,Qi Yang,Le Yang*

Main category: cs.CV

TL;DR: 本文综述了3D点云压缩（PCC）和质量评估（PCQA）的最新进展，分析了手工和基于学习的算法，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 3D点云数据的快速增长及其在自动驾驶、机器人等领域的应用需求，推动了对高效压缩和质量评估技术的需求。

Method: 分析了多种手工和基于学习的PCC算法及PCQA指标，并在新兴数据集上进行了基准测试。

Result: 通过对比代表性方法，总结了其优缺点，并指出当前在视觉保真度、延迟和多模态数据支持方面的挑战。

Conclusion: 未来研究方向包括混合压缩框架和高级特征提取策略，以实现更高效、沉浸式和智能的3D应用。

Abstract: The rapid growth of 3D point cloud data, driven by applications in autonomous
driving, robotics, and immersive environments, has led to criticals demand for
efficient compression and quality assessment techniques. Unlike traditional 2D
media, point clouds present unique challenges due to their irregular structure,
high data volume, and complex attributes. This paper provides a comprehensive
survey of recent advances in point cloud compression (PCC) and point cloud
quality assessment (PCQA), emphasizing their significance for real-time and
perceptually relevant applications. We analyze a wide range of handcrafted and
learning-based PCC algorithms, along with objective PCQA metrics. By
benchmarking representative methods on emerging datasets, we offer detailed
comparisons and practical insights into their strengths and limitations.
Despite notable progress, challenges such as enhancing visual fidelity,
reducing latency, and supporting multimodal data remain. This survey outlines
future directions, including hybrid compression frameworks and advanced feature
extraction strategies, to enable more efficient, immersive, and intelligent 3D
applications.

</details>


### [40] [PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection](https://arxiv.org/abs/2506.22783)
*Oguzhan Baser,Ahmet Ege Tanriverdi,Sriram Vishwanath,Sandeep P. Chinchali*

Main category: cs.CV

TL;DR: 论文提出PhonemeFake（PF）攻击方法，通过语言推理操纵关键语音片段，显著降低人类感知和检测准确率，并开源了数据集和检测模型。


<details>
  <summary>Details</summary>
Motivation: 现有Deepfake（DF）数据集未能真实反映攻击对人类感知的影响，需要更现实的攻击向量。

Method: 引入PF攻击方法，利用语言推理操纵关键语音片段，并开发了自适应优先计算的双层DF检测模型。

Result: PF攻击使人类感知降低42%，检测准确率降低94%；检测模型将EER降低91%，速度提升90%。

Conclusion: PF攻击和检测模型为DF研究提供了更现实的攻击向量和高效检测方案。

Abstract: Deepfake (DF) attacks pose a growing threat as generative models become
increasingly advanced. However, our study reveals that existing DF datasets
fail to deceive human perception, unlike real DF attacks that influence public
discourse. It highlights the need for more realistic DF attack vectors. We
introduce PhonemeFake (PF), a DF attack that manipulates critical speech
segments using language reasoning, significantly reducing human perception by
up to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF
dataset on HuggingFace and open-source bilevel DF segment detection model that
adaptively prioritizes compute on manipulated regions. Our extensive
experiments across three known DF datasets reveal that our detection model
reduces EER by 91% while achieving up to 90% speed-up, with minimal compute
overhead and precise localization beyond existing models as a scalable
solution.

</details>


### [41] [Single-Frame Point-Pixel Registration via Supervised Cross-Modal Feature Matching](https://arxiv.org/abs/2506.22784)
*Yu Han,Zhiwei Huang,Yanting Zhang,Fangjun Ding,Shen Cai,Rui Fan*

Main category: cs.CV

TL;DR: 提出一种基于无检测器的点-像素匹配框架，通过注意力网络直接匹配LiDAR点云和相机图像，无需多帧累积，显著提升稀疏单帧LiDAR下的配准性能。


<details>
  <summary>Details</summary>
Motivation: 解决LiDAR点云与相机图像之间的模态差异问题，特别是在稀疏单帧LiDAR条件下，现有方法因特征分离编码和依赖多帧累积而效果不佳。

Method: 将LiDAR强度图投影到2D视图，通过注意力网络进行跨模态匹配，并引入可重复性评分机制提升稀疏输入下的鲁棒性。

Result: 在KITTI、nuScenes和MIAS-LCEC-TF70基准测试中达到最优性能，优于依赖多帧累积的方法。

Conclusion: 该方法有效解决了稀疏单帧LiDAR下的点-像素配准问题，为自动驾驶和机器人感知提供了更可靠的解决方案。

Abstract: Point-pixel registration between LiDAR point clouds and camera images is a
fundamental yet challenging task in autonomous driving and robotic perception.
A key difficulty lies in the modality gap between unstructured point clouds and
structured images, especially under sparse single-frame LiDAR settings.
Existing methods typically extract features separately from point clouds and
images, then rely on hand-crafted or learned matching strategies. This separate
encoding fails to bridge the modality gap effectively, and more critically,
these methods struggle with the sparsity and noise of single-frame LiDAR, often
requiring point cloud accumulation or additional priors to improve reliability.
Inspired by recent progress in detector-free matching paradigms (e.g.
MatchAnything), we revisit the projection-based approach and introduce the
detector-free framework for direct point-pixel matching between LiDAR and
camera views. Specifically, we project the LiDAR intensity map into a 2D view
from the LiDAR perspective and feed it into an attention-based detector-free
matching network, enabling cross-modal correspondence estimation without
relying on multi-frame accumulation. To further enhance matching reliability,
we introduce a repeatability scoring mechanism that acts as a soft visibility
prior. This guides the network to suppress unreliable matches in regions with
low intensity variation, improving robustness under sparse input. Extensive
experiments on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks demonstrate that
our method achieves state-of-the-art performance, outperforming prior
approaches on nuScenes (even those relying on accumulated point clouds),
despite using only single-frame LiDAR.

</details>


### [42] [A Novel Frame Identification and Synchronization Technique for Smartphone Visible Light Communication Systems Based on Convolutional Neural Networks](https://arxiv.org/abs/2506.23004)
*Vaigai Nayaki Yokar,Hoa Le-Minh,Xicong Li,Wai Lok Woo,Luis Nero Alves,Stanislav Zvanovec,Tran The Son,Zabih Ghassemlooy*

Main category: cs.CV

TL;DR: 提出了一种基于CNN的轻量级监督学习方法，用于屏幕到相机（S2C）可见光通信中的帧识别与同步，实验准确率达98.74%。


<details>
  <summary>Details</summary>
Motivation: 解决S2C通信中因模糊、裁剪和旋转图像等实时挑战导致的性能问题。

Method: 使用Python和TensorFlow Keras框架构建CNN模型，通过三次实时实验训练，数据集针对S2C通信的挑战设计。

Result: 模型在实验中达到98.74%的准确率，显著提升了帧识别与同步性能。

Conclusion: 该方法在S2C VLC系统中表现出高效性和鲁棒性，适用于实际应用。

Abstract: This paper proposes a novel, robust, and lightweight supervised Convolutional
Neural Network (CNN)-based technique for frame identification and
synchronization, designed to enhance short-link communication performance in a
screen-to-camera (S2C) based visible light communication (VLC) system.
Developed using Python and the TensorFlow Keras framework, the proposed CNN
model was trained through three real-time experimental investigations conducted
in Jupyter Notebook. These experiments incorporated a dataset created from
scratch to address various real-time challenges in S2C communication, including
blurring, cropping, and rotated images in mobility scenarios. Overhead frames
were introduced for synchronization, which leads to enhanced system
performance. The experimental results demonstrate that the proposed model
achieves an overall accuracy of approximately 98.74%, highlighting its
effectiveness in identifying and synchronizing frames in S2C VLC systems.

</details>


### [43] [RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors](https://arxiv.org/abs/2506.22800)
*Sicong Du,Jiarun Liu,Qifeng Chen,Hao-Xiang Chen,Tai-Jiang Mu,Sheng Yang*

Main category: cs.CV

TL;DR: RGE-GS是一种新型扩展重建框架，结合扩散生成与奖励引导的高斯积分，解决了3D高斯泼溅技术中的物理不一致性和训练效率问题。


<details>
  <summary>Details</summary>
Motivation: 单次驾驶片段常导致道路结构扫描不完整，传感器模拟器需扩展重建场景以有效回归驾驶动作。现有3D高斯泼溅技术虽质量高，但直接结合扩散先验会引入物理不一致性和训练效率问题。

Method: RGE-GS框架包含两个创新：1）奖励网络学习识别并优先选择一致性生成模式；2）重建过程中采用差异化训练策略，根据场景收敛指标自动调整高斯优化进度。

Result: 公开数据集评估显示，RGE-GS在重建质量上达到最先进水平。

Conclusion: RGE-GS通过奖励引导和差异化训练，显著提升了重建质量和训练效率，代码将开源。

Abstract: A single-pass driving clip frequently results in incomplete scanning of the
road structure, making reconstructed scene expanding a critical requirement for
sensor simulators to effectively regress driving actions. Although contemporary
3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction
quality, their direct extension through the integration of diffusion priors
often introduces cumulative physical inconsistencies and compromises training
efficiency. To address these limitations, we present RGE-GS, a novel expansive
reconstruction framework that synergizes diffusion-based generation with
reward-guided Gaussian integration. The RGE-GS framework incorporates two key
innovations: First, we propose a reward network that learns to identify and
prioritize consistently generated patterns prior to reconstruction phases,
thereby enabling selective retention of diffusion outputs for spatial
stability. Second, during the reconstruction process, we devise a
differentiated training strategy that automatically adjust Gaussian
optimization progress according to scene converge metrics, which achieving
better convergence than baseline methods. Extensive evaluations of publicly
available datasets demonstrate that RGE-GS achieves state-of-the-art
performance in reconstruction quality. Our source-code will be made publicly
available at https://github.com/CN-ADLab/RGE-GS. (Camera-ready version
incorporating reviewer suggestions will be updated soon.)

</details>


### [44] [PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution](https://arxiv.org/abs/2506.23254)
*Aradhana Mishra,Bumshik Lee*

Main category: cs.CV

TL;DR: PixelBoost是一种新型扩散模型，通过引入布朗运动的随机性提升图像超分辨率质量，同时兼顾计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在图像超分辨率中因减少采样步骤导致的图像模糊和计算效率问题。

Method: 整合受控随机性到训练中，避免局部最优，采用sigmoidal噪声排序方法简化训练。

Result: 在LPIPS、LOE、PSNR、SSIM等指标上表现优异，边缘重建能力更强，推理速度更快。

Conclusion: PixelBoost通过随机性和自适应学习，显著提升了图像超分辨率的真实性和计算效率。

Abstract: Diffusion-model-based image super-resolution techniques often face a
trade-off between realistic image generation and computational efficiency. This
issue is exacerbated when inference times by decreasing sampling steps,
resulting in less realistic and hazy images. To overcome this challenge, we
introduce a novel diffusion model named PixelBoost that underscores the
significance of embracing the stochastic nature of Brownian motion in advancing
image super-resolution, resulting in a high degree of realism, particularly
focusing on texture and edge definitions. By integrating controlled
stochasticity into the training regimen, our proposed model avoids convergence
to local optima, effectively capturing and reproducing the inherent uncertainty
of image textures and patterns. Our proposed model demonstrates superior
objective results in terms of learned perceptual image patch similarity
(LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR),
structural similarity index measure (SSIM), as well as visual quality. To
determine the edge enhancement, we evaluated the gradient magnitude and pixel
value, and our proposed model exhibited a better edge reconstruction
capability. Additionally, our model demonstrates adaptive learning capabilities
by effectively adjusting to Brownian noise patterns and introduces a sigmoidal
noise sequencing method that simplifies training, resulting in faster inference
speeds.

</details>


### [45] [Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding](https://arxiv.org/abs/2506.22803)
*Nuoye Xiong,Anqi Dong,Ning Wang,Cong Hua,Guangming Zhu,Mei Lin,Peiyi Shen,Liang Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于概念瓶颈模型（CBM）的方法CBM-HNMU，通过自动识别和修正有害概念，提升黑盒模型的解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 深度学习的复杂模型缺乏解释性，现有方法难以有效干预或仅停留在样本层面。

Method: 利用CBM作为可解释框架，通过全局梯度贡献识别和修正有害概念，并将修正后的知识蒸馏回黑盒模型。

Result: 在多个数据集和模型上测试，最高准确率提升2.64%，平均准确率提升1.03%。

Conclusion: CBM-HNMU有效提升了模型的解释性和性能，为黑盒模型的理解和优化提供了新思路。

Abstract: Recent advances in deep learning have led to increasingly complex models with
deeper layers and more parameters, reducing interpretability and making their
decisions harder to understand. While many methods explain black-box reasoning,
most lack effective interventions or only operate at sample-level without
modifying the model itself. To address this, we propose the Concept Bottleneck
Model for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU).
CBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable
framework to approximate black-box reasoning and communicate conceptual
understanding. Detrimental concepts are automatically identified and refined
(removed/replaced) based on global gradient contributions. The modified CBM
then distills corrected knowledge back into the black-box model, enhancing both
interpretability and accuracy. We evaluate CBM-HNMU on various CNN and
transformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft,
and CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum
increase in average accuracy across 1.03%. Source code is available at:
https://github.com/XiGuaBo/CBM-HNMU.

</details>


### [46] [Layer Decomposition and Morphological Reconstruction for Task-Oriented Infrared Image Enhancement](https://arxiv.org/abs/2506.23353)
*Siyuan Chai,Xiaodong Guo,Tong Liu*

Main category: cs.CV

TL;DR: 提出了一种任务导向的红外图像增强方法，通过层分解和显著性信息提取，提升复杂天气下自动驾驶的感知能力。


<details>
  <summary>Details</summary>
Motivation: 红外图像在复杂天气条件下（如雾、雨、低光）对自动驾驶感知有帮助，但低对比度和噪声问题影响下游视觉任务性能。

Method: 方法包括层分解和基于形态学重建的显著性提取，增强细节并保留暗区特征，同时避免噪声放大。

Result: 实验表明，该方法在目标检测和语义分割任务中优于现有技术。

Conclusion: 该方法有效提升了红外图像质量，为自动驾驶感知任务提供了更好的输入。

Abstract: Infrared image helps improve the perception capabilities of autonomous
driving in complex weather conditions such as fog, rain, and low light.
However, infrared image often suffers from low contrast, especially in
non-heat-emitting targets like bicycles, which significantly affects the
performance of downstream high-level vision tasks. Furthermore, achieving
contrast enhancement without amplifying noise and losing important information
remains a challenge. To address these challenges, we propose a task-oriented
infrared image enhancement method. Our approach consists of two key components:
layer decomposition and saliency information extraction. First, we design an
layer decomposition method for infrared images, which enhances scene details
while preserving dark region features, providing more features for subsequent
saliency information extraction. Then, we propose a morphological
reconstruction-based saliency extraction method that effectively extracts and
enhances target information without amplifying noise. Our method improves the
image quality for object detection and semantic segmentation tasks. Extensive
experiments demonstrate that our approach outperforms state-of-the-art methods.

</details>


### [47] [Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate](https://arxiv.org/abs/2506.22806)
*Byung Hyun Lee,Sungjin Lim,Seunggyu Lee,Dong Un Kang,Se Young Chun*

Main category: cs.CV

TL;DR: 本文提出了一种名为概念精准擦除器（CPE）的新框架，通过非线性残差注意力门（ResAGs）选择性擦除目标概念，同时保护其他概念。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅通过微调交叉注意力层可能无法有效保护多样化的剩余概念，因此需要一种更有效的方法。

Method: CPE通过添加非线性ResAGs，并结合注意力锚定损失和对抗训练，选择性擦除目标概念。

Result: 实验表明，CPE在擦除名人、艺术风格和不当内容方面优于现有方法，且对攻击提示具有鲁棒性。

Conclusion: CPE通过非线性模块和对抗训练，实现了更高效和鲁棒的概念擦除。

Abstract: Remarkable progress in text-to-image diffusion models has brought a major
concern about potentially generating images on inappropriate or trademarked
concepts. Concept erasing has been investigated with the goals of deleting
target concepts in diffusion models while preserving other concepts with
minimal distortion. To achieve these goals, recent concept erasing methods
usually fine-tune the cross-attention layers of diffusion models. In this work,
we first show that merely updating the cross-attention layers in diffusion
models, which is mathematically equivalent to adding \emph{linear} modules to
weights, may not be able to preserve diverse remaining concepts. Then, we
propose a novel framework, dubbed Concept Pinpoint Eraser (CPE), by adding
\emph{nonlinear} Residual Attention Gates (ResAGs) that selectively erase (or
cut) target concepts while safeguarding remaining concepts from broad
distributions by employing an attention anchoring loss to prevent the
forgetting. Moreover, we adversarially train CPE with ResAG and learnable text
embeddings in an iterative manner to maximize erasing performance and enhance
robustness against adversarial attacks. Extensive experiments on the erasure of
celebrities, artistic styles, and explicit contents demonstrated that the
proposed CPE outperforms prior arts by keeping diverse remaining concepts while
deleting the target concepts with robustness against attack prompts. Code is
available at https://github.com/Hyun1A/CPE

</details>


### [48] [Evaluation of Geolocation Capabilities of Multimodal Large Language Models and Analysis of Associated Privacy Risks](https://arxiv.org/abs/2506.23481)
*Xian Zhang,Xiang Cheng*

Main category: cs.CV

TL;DR: 多模态大语言模型（MLLMs）在推理能力上的进步引发了隐私和伦理问题，尤其是通过视觉内容推断地理位置的能力。研究分析了现有技术，发现先进模型能在1公里半径内以49%准确率定位街景图像，并提出了隐私保护对策。


<details>
  <summary>Details</summary>
Motivation: 探讨MLLMs在地理定位任务中的能力及其对隐私的潜在威胁。

Method: 系统回顾文献并评估最先进的视觉推理模型在街景图像地理定位任务中的表现。

Result: 先进视觉大模型在1公里半径内定位准确率达49%，展示了其从视觉数据中提取地理线索的强大能力。

Conclusion: 研究识别了成功定位的关键视觉元素，并讨论了隐私风险及技术、政策对策。

Abstract: Objectives: The rapid advancement of Multimodal Large Language Models (MLLMs)
has significantly enhanced their reasoning capabilities, enabling a wide range
of intelligent applications. However, these advancements also raise critical
concerns regarding privacy and ethics. MLLMs are now capable of inferring the
geographic location of images -- such as those shared on social media or
captured from street views -- based solely on visual content, thereby posing
serious risks of privacy invasion, including doxxing, surveillance, and other
security threats.
  Methods: This study provides a comprehensive analysis of existing geolocation
techniques based on MLLMs. It systematically reviews relevant litera-ture and
evaluates the performance of state-of-the-art visual reasoning models on
geolocation tasks, particularly in identifying the origins of street view
imagery.
  Results: Empirical evaluation reveals that the most advanced visual large
models can successfully localize the origin of street-level imagery with up to
$49\%$ accuracy within a 1-kilometer radius. This performance underscores the
models' powerful capacity to extract and utilize fine-grained geographic cues
from visual data.
  Conclusions: Building on these findings, the study identifies key visual
elements that contribute to suc-cessful geolocation, such as text,
architectural styles, and environmental features. Furthermore, it discusses the
potential privacy implications associated with MLLM-enabled geolocation and
discuss several technical and policy-based coun-termeasures to mitigate
associated risks. Our code and dataset are available at
https://github.com/zxyl1003/MLLM-Geolocation-Evaluation.

</details>


### [49] [FreqDGT: Frequency-Adaptive Dynamic Graph Networks with Transformer for Cross-subject EEG Emotion Recognition](https://arxiv.org/abs/2506.22807)
*Yueyang Li,Shengyu Gong,Weiming Zeng,Nizhuan Wang,Wai Ting Siok*

Main category: cs.CV

TL;DR: FreqDGT是一种频率自适应动态图变换器，通过整合频率自适应处理、动态图学习和多尺度时间解缠网络，显著提高了跨被试情绪识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决EEG情绪识别中因个体差异导致的跨被试泛化问题。

Method: 结合频率自适应处理（FAP）、自适应动态图学习（ADGL）和多尺度时间解缠网络（MTDN）。

Result: 实验证明FreqDGT显著提高了跨被试情绪识别的准确性。

Conclusion: FreqDGT通过频率、空间和时间建模的整合，有效解决了跨被试情绪识别的挑战。

Abstract: Electroencephalography (EEG) serves as a reliable and objective signal for
emotion recognition in affective brain-computer interfaces, offering unique
advantages through its high temporal resolution and ability to capture
authentic emotional states that cannot be consciously controlled. However,
cross-subject generalization remains a fundamental challenge due to individual
variability, cognitive traits, and emotional responses. We propose FreqDGT, a
frequency-adaptive dynamic graph transformer that systematically addresses
these limitations through an integrated framework. FreqDGT introduces
frequency-adaptive processing (FAP) to dynamically weight emotion-relevant
frequency bands based on neuroscientific evidence, employs adaptive dynamic
graph learning (ADGL) to learn input-specific brain connectivity patterns, and
implements multi-scale temporal disentanglement network (MTDN) that combines
hierarchical temporal transformers with adversarial feature disentanglement to
capture both temporal dynamics and ensure cross-subject robustness.
Comprehensive experiments demonstrate that FreqDGT significantly improves
cross-subject emotion recognition accuracy, confirming the effectiveness of
integrating frequency-adaptive, spatial-dynamic, and temporal-hierarchical
modeling while ensuring robustness to individual differences. The code is
available at https://github.com/NZWANG/FreqDGT.

</details>


### [50] [Efficient Multi-Crop Saliency Partitioning for Automatic Image Cropping](https://arxiv.org/abs/2506.22814)
*Andrew Hamara,Andrew C. Freeman*

Main category: cs.CV

TL;DR: 本文提出了一种改进的自动图像裁剪方法，支持高效提取多个不重叠的裁剪区域。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅优化单个边界框，无法满足需要多个不连续裁剪的应用需求。

Method: 扩展了固定宽高比裁剪算法，动态调整注意力阈值并避免重复计算显著性图。

Result: 实现了线性时间内提取多个非重叠裁剪区域，并展示了定性结果。

Conclusion: 该方法为未来数据集和基准测试提供了潜在可能性。

Abstract: Automatic image cropping aims to extract the most visually salient regions
while preserving essential composition elements. Traditional saliency-aware
cropping methods optimize a single bounding box, making them ineffective for
applications requiring multiple disjoint crops. In this work, we extend the
Fixed Aspect Ratio Cropping algorithm to efficiently extract multiple
non-overlapping crops in linear time. Our approach dynamically adjusts
attention thresholds and removes selected crops from consideration without
recomputing the entire saliency map. We discuss qualitative results and
introduce the potential for future datasets and benchmarks.

</details>


### [51] [WaRA: Wavelet Low Rank Adaptation](https://arxiv.org/abs/2506.24092)
*Moein Heidari,Yasamin Medghalchi,Mahdi Khoursha,Reza Rezaeian,Ilker Hacihaliloglu*

Main category: cs.CV

TL;DR: WaRA是一种基于小波变换的参数高效微调方法，通过多分辨率分析改进LoRA，在视觉和语言任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法（如LoRA）依赖全局低秩分解，忽略了局部或多尺度结构，无法捕捉权重更新的复杂模式。

Method: WaRA利用小波变换将权重更新矩阵分解为多分辨率表示，在频域进行低秩分解并通过逆变换重构更新。

Result: WaRA在图像生成、分类和语义分割等任务中表现优越，显著提升生成质量并降低计算复杂度。

Conclusion: WaRA不仅适用于视觉任务，还能推广到语言任务，具有广泛适用性和通用性。

Abstract: Parameter-efficient fine-tuning (PEFT) has gained widespread adoption across
various applications. Among PEFT techniques, Low-Rank Adaptation (LoRA) and its
extensions have emerged as particularly effective, allowing efficient model
adaptation while significantly reducing computational overhead. However,
existing approaches typically rely on global low-rank factorizations, which
overlook local or multi-scale structure, failing to capture complex patterns in
the weight updates. To address this, we propose WaRA, a novel PEFT method that
leverages wavelet transforms to decompose the weight update matrix into a
multi-resolution representation. By performing low-rank factorization in the
wavelet domain and reconstructing updates through an inverse transform, WaRA
obtains compressed adaptation parameters that harness multi-resolution
analysis, enabling it to capture both coarse and fine-grained features while
providing greater flexibility and sparser representations than standard LoRA.
Through comprehensive experiments and analysis, we demonstrate that WaRA
performs superior on diverse vision tasks, including image generation,
classification, and semantic segmentation, significantly enhancing generated
image quality while reducing computational complexity. Although WaRA was
primarily designed for vision tasks, we further showcase its effectiveness in
language tasks, highlighting its broader applicability and generalizability.
The code is publicly available at
\href{GitHub}{https://github.com/moeinheidari7829/WaRA}.

</details>


### [52] [Unleashing the Multi-View Fusion Potential: Noise Correction in VLM for Open-Vocabulary 3D Scene Understanding](https://arxiv.org/abs/2506.22817)
*Xingyilang Yin,Jiale Wang,Xi Yang,Mutian Xu,Xu Gu,Nannan Wang*

Main category: cs.CV

TL;DR: MVOV3D提出了一种新方法，通过减少多视图融合中的固有噪声，提升开放词汇3D场景理解的性能，无需训练即可增强通用性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在有限词汇量的基准测试中表现良好，但难以处理多样化的对象类别，且3D数据量有限限制了开放词汇3D模型的训练。

Method: MVOV3D利用CLIP编码器提取精确的区域级图像和文本特征，并结合3D几何先验优化多视图融合。

Result: 在ScanNet200和Matterport160数据集上，MVOV3D分别以14.7%和16.2%的mIoU刷新了开放词汇语义分割的记录。

Conclusion: MVOV3D通过减少噪声和优化多视图融合，显著提升了开放词汇3D场景理解的性能。

Abstract: Recent open-vocabulary 3D scene understanding approaches mainly focus on
training 3D networks through contrastive learning with point-text pairs or by
distilling 2D features into 3D models via point-pixel alignment. While these
methods show considerable performance in benchmarks with limited vocabularies,
they struggle to handle diverse object categories as the limited amount of 3D
data upbound training strong open-vocabulary 3d models. We observe that 2D
multi-view fusion methods take precedence in understanding diverse concepts in
3D scenes. However, inherent noises in vision-language models lead multi-view
fusion to sub-optimal performance. To this end, we introduce MVOV3D, a novel
approach aimed at unleashing the potential of 2D multi-view fusion for
open-vocabulary 3D scene understanding. We focus on reducing the inherent
noises without training, thereby preserving the generalizability while
enhancing open-world capabilities. Specifically, MVOV3D improves multi-view 2D
features by leveraging precise region-level image features and text features
encoded by CLIP encoders and incorporates 3D geometric priors to optimize
multi-view fusion. Extensive experiments on various datasets demonstrate the
effectiveness of our method. Notably, our MVOV3D achieves a new record with
14.7% mIoU on ScanNet200 and 16.2% mIoU on Matterport160 for challenge
open-vocabulary semantic segmentation, outperforming current leading trained 3D
networks by a significant margin.

</details>


### [53] [Prompting without Panic: Attribute-aware, Zero-shot, Test-Time Calibration](https://arxiv.org/abs/2506.22819)
*Ramya Hebbalaguppe,Tamoghno Kandar,Abhinav Nagpal,Chetan Arora*

Main category: cs.CV

TL;DR: 本文提出了一种改进视觉语言模型（VLM）在测试时间提示调整（TPT）中的置信度校准问题的方法，通过初始化提示和正则化损失优化性能。


<details>
  <summary>Details</summary>
Motivation: TPT方法在提高准确性的同时忽略了置信度校准，限制了其在关键应用中的适用性。

Method: 1. 使用大型语言模型（LLM）初始化提示以减少过拟合；2. 提出正则化损失以减少类内距离并增加类间距离。

Result: 实验表明，该方法显著降低了预期校准误差（ECE），平均值为4.11，优于其他方法。

Conclusion: 提出的方法TCA有效解决了TPT中的校准问题，提升了模型的可靠性。

Abstract: Vision-language models (VLM) have demonstrated impressive performance in
image recognition by leveraging self-supervised training on large datasets.
Their performance can be further improved by adapting to the test sample using
test-time prompt tuning (TPT). Unfortunately, the singular focus of TPT
approaches on improving the accuracy suffers from tunnel vision, and leads to
degradation in confidence calibration. This limits the applicability of TPT in
critical applications.
  We make three contributions in this work. (1) We posit that random or naive
initialization of prompts leads to overfitting on a particular test sample, and
is the main reason for miscalibration of the VLM after TPT. To mitigate the
problem, we propose careful initialization of test time prompt using prior
knowledge about the target label attributes from a large language model (LLM);
(2) To further maintain the quality of prompts during \tpt, we propose a novel
regularization loss to reduce intraclass distance, and increase inter-class
distance between the learnt
  Through extensive experiments on different CLIP architectures and 15
datasets, we show that our approach can effectively improve the calibration
after TPT. We report an average expected calibration error (ECE) of 4.11 with
our method, TCA, compared to 11.7 for vanilla TPT, 6.12 for C-TPT (ICLR'24),
6.78 for DiffTPT (CVPR'23), and 8.43 for PromptAlign (NeurIPS'23). The code is
publicly accessible at:
https://github.com/rhebbalaguppe/TCA_PromptWithoutPanic.

</details>


### [54] [Listener-Rewarded Thinking in VLMs for Image Preferences](https://arxiv.org/abs/2506.22832)
*Alexander Gambashidze,Li Pengyi,Matvey Skripkin,Andrey Galichin,Anton Gusarov,Konstantin Sobolev,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.CV

TL;DR: 论文提出了一种基于听众增强的GRPO框架，通过重新评估推理链来校准奖励信号，显著提升了模型在人类视觉偏好任务中的泛化能力和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前奖励模型在泛化性和避免记忆化方面存在不足，需要复杂标注流程，而强化学习（如GRPO）虽能改善泛化性，但在推理准确性上仍有缺陷。

Method: 引入听众增强的GRPO框架，利用独立视觉语言模型（听众）重新评估推理链，生成密集校准的置信分数，优化奖励信号。

Result: 在ImageReward基准测试中达到67.4%的准确率，在大规模人类偏好数据集上OOD性能提升6%，并减少推理矛盾。

Conclusion: 听众增强的奖励机制为视觉语言模型与人类偏好对齐提供了高效、可扩展的解决方案。

Abstract: Training robust and generalizable reward models for human visual preferences
is essential for aligning text-to-image and text-to-video generative models
with human intent. However, current reward models often fail to generalize, and
supervised fine-tuning leads to memorization, demanding complex annotation
pipelines. While reinforcement learning (RL), specifically Group Relative
Policy Optimization (GRPO), improves generalization, we uncover a key failure
mode: a significant drop in reasoning accuracy occurs when a model's reasoning
trace contradicts that of an independent, frozen vision-language model
("listener") evaluating the same output. To address this, we introduce a
listener-augmented GRPO framework. Here, the listener re-evaluates the
reasoner's chain-of-thought to provide a dense, calibrated confidence score,
shaping the RL reward signal. This encourages the reasoner not only to answer
correctly, but to produce explanations that are persuasive to an independent
model. Our listener-shaped reward scheme achieves best accuracy on the
ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)
performance on a large-scale human preference dataset (1.2M votes, up to +6%
over naive reasoner), and reduces reasoning contradictions compared to strong
GRPO and SFT baselines. These results demonstrate that listener-based rewards
provide a scalable, data-efficient path to aligning vision-language models with
nuanced human preferences. We will release our reasoning model here:
https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.

</details>


### [55] [SemFaceEdit: Semantic Face Editing on Generative Radiance Manifolds](https://arxiv.org/abs/2506.22833)
*Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: SemFaceEdit是一种基于生成辐射流形的新方法，通过语义场实现面部图像的精确局部编辑，同时保持其他区域的完整性。


<details>
  <summary>Details</summary>
Motivation: 现有3D感知GAN技术虽能提供多视角一致性，但缺乏局部编辑能力，因此需要一种能高效学习细节并支持精确编辑的方法。

Method: SemFaceEdit通过几何模块和外观模块联合训练，生成语义辐射和占用场，并利用潜在代码解耦几何与外观。

Result: 实验表明，SemFaceEdit在语义场编辑和辐射场解耦方面表现优异，支持精确的面部语义编辑。

Conclusion: SemFaceEdit为面部图像的局部编辑提供了一种高效且精确的解决方案。

Abstract: Despite multiple view consistency offered by 3D-aware GAN techniques, the
resulting images often lack the capacity for localized editing. In response,
generative radiance manifolds emerge as an efficient approach for constrained
point sampling within volumes, effectively reducing computational demands and
enabling the learning of fine details. This work introduces SemFaceEdit, a
novel method that streamlines the appearance and geometric editing process by
generating semantic fields on generative radiance manifolds. Utilizing latent
codes, our method effectively disentangles the geometry and appearance
associated with different facial semantics within the generated image. In
contrast to existing methods that can change the appearance of the entire
radiance field, our method enables the precise editing of particular facial
semantics while preserving the integrity of other regions. Our network
comprises two key modules: the Geometry module, which generates semantic
radiance and occupancy fields, and the Appearance module, which is responsible
for predicting RGB radiance. We jointly train both modules in adversarial
settings to learn semantic-aware geometry and appearance descriptors. The
appearance descriptors are then conditioned on their respective semantic latent
codes by the Appearance Module, facilitating disentanglement and enhanced
control. Our experiments highlight SemFaceEdit's superior performance in
semantic field-based editing, particularly in achieving improved radiance field
disentanglement.

</details>


### [56] [FOCUS: Fine-grained Optimization with Semantic Guided Understanding for Pedestrian Attributes Recognition](https://arxiv.org/abs/2506.22836)
*Hongyan An,Kuan Zhu,Xin He,Haiyun Guo,Chaoyang Zhao,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: 论文提出FOCUS方法，通过多粒度混合令牌和属性引导视觉特征提取模块，自适应地提取细粒度属性级特征，解决了现有方法在行人属性识别中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常提取区域特征来预测固定属性集，但会牺牲某些属性的细粒度模式，且无法泛化到未见过的属性。

Method: 提出FOCUS方法，包括多粒度混合令牌（MGMT）、属性引导视觉特征提取模块（AVFE）和区域感知对比学习（RACL）。

Result: 在PA100K、PETA和RAPv1数据集上验证了方法的有效性和强泛化能力。

Conclusion: FOCUS方法能够自适应提取细粒度属性级特征，适用于未见过的属性，提升了行人属性识别的性能。

Abstract: Pedestrian attribute recognition (PAR) is a fundamental perception task in
intelligent transportation and security. To tackle this fine-grained task, most
existing methods focus on extracting regional features to enrich attribute
information. However, a regional feature is typically used to predict a fixed
set of pre-defined attributes in these methods, which limits the performance
and practicality in two aspects: 1) Regional features may compromise
fine-grained patterns unique to certain attributes in favor of capturing common
characteristics shared across attributes. 2) Regional features cannot
generalize to predict unseen attributes in the test time. In this paper, we
propose the \textbf{F}ine-grained \textbf{O}ptimization with semanti\textbf{C}
g\textbf{U}ided under\textbf{S}tanding (FOCUS) approach for PAR, which
adaptively extracts fine-grained attribute-level features for each attribute
individually, regardless of whether the attributes are seen or not during
training. Specifically, we propose the Multi-Granularity Mix Tokens (MGMT) to
capture latent features at varying levels of visual granularity, thereby
enriching the diversity of the extracted information. Next, we introduce the
Attribute-guided Visual Feature Extraction (AVFE) module, which leverages
textual attributes as queries to retrieve their corresponding visual attribute
features from the Mix Tokens using a cross-attention mechanism. To ensure that
textual attributes focus on the appropriate Mix Tokens, we further incorporate
a Region-Aware Contrastive Learning (RACL) method, encouraging attributes
within the same region to share consistent attention maps. Extensive
experiments on PA100K, PETA, and RAPv1 datasets demonstrate the effectiveness
and strong generalization ability of our method.

</details>


### [57] [AG-VPReID 2025: Aerial-Ground Video-based Person Re-identification Challenge Results](https://arxiv.org/abs/2506.22843)
*Kien Nguyen,Clinton Fookes,Sridha Sridharan,Huy Nguyen,Feng Liu,Xiaoming Liu,Arun Ross,Dana Michalski,Tamás Endrei,Ivan DeAndres-Tame,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez,Javier Ortega-Garcia,Zijing Gong,Yuhao Wang,Xuehu Liu,Pingping Zhang,Md Rashidunnabi,Hugo Proença,Kailash A. Hambarde,Saeid Rezaei*

Main category: cs.CV

TL;DR: 论文介绍了AG-VPReID 2025挑战赛，专注于高海拔（80-120米）的空中-地面视频行人重识别，提出了新的数据集和解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决空中与地面视角差异带来的行人重识别挑战，推动大规模监控和公共安全应用的发展。

Method: 基于AG-ReID 2023挑战赛成果，构建AG-VPReID数据集，组织国际团队开发多流架构、基于Transformer的时序推理和物理建模等方法。

Result: 领先方法X-TFCLIP在空对地和地对空设置中分别达到72.28%和70.77%的Rank-1准确率，超越现有基线。

Conclusion: AG-VPReID挑战赛展示了数据集复杂性，并为空中-地面行人重识别提供了新基准。

Abstract: Person re-identification (ReID) across aerial and ground vantage points has
become crucial for large-scale surveillance and public safety applications.
Although significant progress has been made in ground-only scenarios, bridging
the aerial-ground domain gap remains a formidable challenge due to extreme
viewpoint differences, scale variations, and occlusions. Building upon the
achievements of the AG-ReID 2023 Challenge, this paper introduces the AG-VPReID
2025 Challenge - the first large-scale video-based competition focused on
high-altitude (80-120m) aerial-ground ReID. Constructed on the new AG-VPReID
dataset with 3,027 identities, over 13,500 tracklets, and approximately 3.7
million frames captured from UAVs, CCTV, and wearable cameras, the challenge
featured four international teams. These teams developed solutions ranging from
multi-stream architectures to transformer-based temporal reasoning and
physics-informed modeling. The leading approach, X-TFCLIP from UAM, attained
72.28% Rank-1 accuracy in the aerial-to-ground ReID setting and 70.77% in the
ground-to-aerial ReID setting, surpassing existing baselines while highlighting
the dataset's complexity. For additional details, please refer to the official
website at https://agvpreid25.github.io.

</details>


### [58] [DMD-Net: Deep Mesh Denoising Network](https://arxiv.org/abs/2506.22850)
*Aalok Gangopadhyay,Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: DMD-Net是一种端到端的深度学习框架，用于解决网格去噪问题，通过图卷积神经网络和双流网络实现，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决网格去噪问题，提出一种高效且鲁棒的深度学习方法。

Method: 采用图卷积神经网络（GCN）和双流网络结构，结合特征引导变换器（FGT）范式，包括特征提取器、变换器和去噪器。

Result: 在大规模3D数据集上训练，性能优于现有方法，对高噪声具有鲁棒性。

Conclusion: DMD-Net在网格去噪任务中表现出色，各组件均对性能有重要贡献。

Abstract: We present Deep Mesh Denoising Network (DMD-Net), an end-to-end deep learning
framework, for solving the mesh denoising problem. DMD-Net consists of a Graph
Convolutional Neural Network in which aggregation is performed in both the
primal as well as the dual graph. This is realized in the form of an asymmetric
two-stream network, which contains a primal-dual fusion block that enables
communication between the primal-stream and the dual-stream. We develop a
Feature Guided Transformer (FGT) paradigm, which consists of a feature
extractor, a transformer, and a denoiser. The feature extractor estimates the
local features, that guide the transformer to compute a transformation, which
is applied to the noisy input mesh to obtain a useful intermediate
representation. This is further processed by the denoiser to obtain the
denoised mesh. Our network is trained on a large scale dataset of 3D objects.
We perform exhaustive ablation studies to demonstrate that each component in
our network is essential for obtaining the best performance. We show that our
method obtains competitive or better results when compared with the
state-of-the-art mesh denoising algorithms. We demonstrate that our method is
robust to various kinds of noise. We observe that even in the presence of
extremely high noise, our method achieves excellent performance.

</details>


### [59] [Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval](https://arxiv.org/abs/2506.22864)
*Li-Cheng Shen,Jih-Kang Hsieh,Wei-Hua Li,Chu-Song Chen*

Main category: cs.CV

TL;DR: 论文提出了一种名为MaTIR的新任务，结合了文本到图像检索（TIR）和参考表达式分割（RES），并设计了一个两阶段框架，利用SAM和Alpha-CLIP进行高效检索，再通过MLLM优化结果。


<details>
  <summary>Details</summary>
Motivation: 现有TIR方法缺乏解释性，而RES在大规模图像集上计算成本高，因此需要一种统一两者的高效且精确的方法。

Method: 提出两阶段框架：第一阶段使用SAM和Alpha-CLIP生成对象掩码和区域级嵌入；第二阶段通过MLLM重新排序和对象定位。

Result: 在COCO和D$^3$数据集上，方法在检索准确性和分割质量上均显著优于现有方法。

Conclusion: MaTIR任务和提出的框架有效结合了TIR和RES的优势，实现了高效且精确的图像检索与分割。

Abstract: Text-to-image retrieval (TIR) aims to find relevant images based on a textual
query, but existing approaches are primarily based on whole-image captions and
lack interpretability. Meanwhile, referring expression segmentation (RES)
enables precise object localization based on natural language descriptions but
is computationally expensive when applied across large image collections. To
bridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies
TIR and RES, requiring both efficient image search and accurate object
segmentation. To address this task, we propose a two-stage framework,
comprising a first stage for segmentation-aware image retrieval and a second
stage for reranking and object grounding with a multimodal large language model
(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract
region-level embeddings offline at first, enabling effective and scalable
online retrieval. Secondly, MLLM is used to refine retrieval rankings and
generate bounding boxes, which are matched to segmentation masks. We evaluate
our approach on COCO and D$^3$ datasets, demonstrating significant improvements
in both retrieval accuracy and segmentation quality over previous methods.

</details>


### [60] [Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception](https://arxiv.org/abs/2506.22866)
*Hang-Cheng Dong,Lu Zou,Bingguo Liu,Dong Ye,Guodong Liu*

Main category: cs.CV

TL;DR: 提出了一种弱监督语义分割框架，通过区域感知CAM和伪标签训练解决缺陷检测任务中标注数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 工业缺陷检测依赖大规模标注数据，但实际应用中标注资源有限，需弱监督方法。

Method: 引入过滤引导反向传播（FGBP）优化CAM，结合区域感知加权模块提升空间精度，并通过伪标签迭代优化模型。

Result: 在工业缺陷数据集上验证了方法的优越性，实现了高精度分割。

Conclusion: 该框架有效解决了弱监督学习与高精度缺陷分割之间的差距，适用于资源受限的工业场景。

Abstract: Surface defect detection plays a critical role in industrial quality
inspection. Recent advances in artificial intelligence have significantly
enhanced the automation level of detection processes. However, conventional
semantic segmentation and object detection models heavily rely on large-scale
annotated datasets, which conflicts with the practical requirements of defect
detection tasks. This paper proposes a novel weakly supervised semantic
segmentation framework comprising two key components: a region-aware class
activation map (CAM) and pseudo-label training. To address the limitations of
existing CAM methods, especially low-resolution thermal maps, and insufficient
detail preservation, we introduce filtering-guided backpropagation (FGBP),
which refines target regions by filtering gradient magnitudes to identify areas
with higher relevance to defects. Building upon this, we further develop a
region-aware weighted module to enhance spatial precision. Finally,
pseudo-label segmentation is implemented to refine the model's performance
iteratively. Comprehensive experiments on industrial defect datasets
demonstrate the superiority of our method. The proposed framework effectively
bridges the gap between weakly supervised learning and high-precision defect
segmentation, offering a practical solution for resource-constrained industrial
scenarios.

</details>


### [61] [STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing](https://arxiv.org/abs/2506.22868)
*Junsung Lee,Junoh Kang,Bohyung Han*

Main category: cs.CV

TL;DR: STR-Match是一种无需训练的视频编辑算法，通过新的STR评分优化潜在空间，解决了现有方法的时间不一致性和域转换限制。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导的视频编辑方法存在时间不一致、运动扭曲和域转换受限的问题，主要原因是时空像素相关性建模不足。

Method: 提出STR-Match算法，利用2D空间注意力和1D时间模块计算STR评分，指导潜在优化，避免昂贵的3D注意力机制。

Result: 实验表明，STR-Match在视觉质量和时空一致性上优于现有方法，尤其在显著域转换下仍保持性能。

Conclusion: STR-Match通过优化时空像素相关性建模，显著提升了视频编辑的时空一致性和视觉保真度。

Abstract: Previous text-guided video editing methods often suffer from temporal
inconsistency, motion distortion, and-most notably-limited domain
transformation. We attribute these limitations to insufficient modeling of
spatiotemporal pixel relevance during the editing process. To address this, we
propose STR-Match, a training-free video editing algorithm that produces
visually appealing and spatiotemporally coherent videos through latent
optimization guided by our novel STR score. The score captures spatiotemporal
pixel relevance across adjacent frames by leveraging 2D spatial attention and
1D temporal modules in text-to-video (T2V) diffusion models, without the
overhead of computationally expensive 3D attention mechanisms. Integrated into
a latent optimization framework with a latent mask, STR-Match generates
temporally consistent and visually faithful videos, maintaining strong
performance even under significant domain transformations while preserving key
visual attributes of the source. Extensive experiments demonstrate that
STR-Match consistently outperforms existing methods in both visual quality and
spatiotemporal consistency.

</details>


### [62] [Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and Grounder](https://arxiv.org/abs/2506.22880)
*Dang Jisheng,Wu Xudong,Wang Bimei,Lv Ning,Chen Jiayu,Jingwen Zhao,Yichu liu,Jizhao Liu,Juncheng Li,Teng Wang*

Main category: cs.CV

TL;DR: DeSa2VA提出了一种解耦增强的提示方案，通过文本预训练和线性解耦模块，解决了现有视频分割和接地方法中动态视觉信息与静态语义纠缠的问题，显著提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如Sa2VA）直接将特征融合到分割模型中，导致动态视觉信息与静态语义纠缠，降低了分割准确性。

Method: 1. 设计预训练范式，将文本标签转换为点级提示并生成文本掩码；2. 使用线性投影将大语言模型的隐藏状态解耦为文本和视觉特征子空间；3. 通过动态掩码融合策略结合解耦特征。

Result: 在图像分割、图像问答、视频分割和视频问答等任务中实现了最先进的性能。

Conclusion: DeSa2VA通过解耦和动态融合显著提升了分割和接地任务的性能，代码已开源。

Abstract: Existing video segmenter and grounder approaches, exemplified by Sa2VA,
directly fuse features within segmentation models. This often results in an
undesirable entanglement of dynamic visual information and static semantics,
thereby degrading segmentation accuracy. To systematically mitigate this issue,
we propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text
pre-training and a linear decoupling module to address the information
processing limitations inherent in SAM-2. Specifically, first, we devise a
pre-training paradigm that converts textual ground-truth labels into
point-level prompts while generating corresponding text masks. These masks are
refined through a hybrid loss function to strengthen the model's semantic
grounding capabilities. Next, we employ linear projection to disentangle hidden
states that generated by a large language model into distinct textual and
visual feature subspaces. Finally, a dynamic mask fusion strategy
synergistically combines these decoupled features through triple supervision
from predicted text/visual masks and ground-truth annotations. Extensive
experiments demonstrate state-of-the-art performance across diverse tasks,
including image segmentation, image question answering, video segmentation, and
video question answering. Our codes are available at
https://github.com/longmalongma/DeSa2VA.

</details>


### [63] [How Semantically Informative is an Image?: Measuring the Covariance-Weighted Norm of Contrastive Learning Embeddings](https://arxiv.org/abs/2506.22881)
*Fumiya Uchiyama,Rintaro Yanagi,Shohei Taniguchi,Shota Takashiro,Masahiro Suzuki,Hirokatsu Kataoka,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CV

TL;DR: 该论文提出了一种基于对比学习的语义信息度量方法，用于量化图像和文本之间的信息增益，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究对比学习是否能表示绝对语义信息量，而不仅仅是关系语义相似性。

Method: 通过对比学习模型计算图像和文本的语义信息量，重新定义信息增益概念，并应用于视觉和语言领域。

Result: 实验结果显示，信息增益得分低的图像通常是占位图标，且该方法与公开模型兼容，计算成本低。

Conclusion: 提出的方法能有效量化语义信息量，且具有高效性和广泛适用性。

Abstract: Contrastive learning has the capacity to model multimodal probability
distributions by embedding and aligning visual representations with semantics
from captions. This approach enables the estimation of relational semantic
similarity; however, it remains unclear whether it can also represent absolute
semantic informativeness. In this work, we introduce a semantic informativeness
metric for an image calculated from text samples via a contrastive learning
model; similarly, the informativeness of a text is calculated from image
samples. We propose a redefinition of the concept of Information Gain, a
concept previously explored in natural language processing, extending its
application to the domains of vision and language. Our metric quantifies how
conditioning on an image distorts the distribution of associated texts, and
vice versa for text conditioning on image distributions. In OpenCLIP's
empirical results, we observe that images with the lowest Information Gain
scores often correspond to placeholder icons such as "image not found."
Furthermore, we propose to measure a norm-based metric of the embedding to
estimate the Information Gain, following the theoretical results for Skip-Gram
with Negative Sampling (SGNS) word embedding. Information Gain can be measured
using either CLIP or SigLIP, and the results demonstrate a strong correlation
with a coefficient of determination ranging from 0.98 to 1.00. After obtaining
the mean and the covariance of the sample embedding, the computational cost of
this method is independent of the sample size, and it is compatible with
publicly available, open-weight models.

</details>


### [64] [CP-Guard: A Unified, Probability-Agnostic, and Adaptive Framework for Malicious Agent Detection and Defense in Multi-Agent Embodied Perception Systems](https://arxiv.org/abs/2506.22890)
*Senkang Hu,Yihang Tao,Guowen Xu,Xinyuan Qian,Yiqin Deng,Xianhao Chen,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.CV

TL;DR: CP-Guard是一个针对协作感知（CP）的统一防御框架，通过概率无关的样本共识和动态阈值调整，有效检测和消除恶意代理。


<details>
  <summary>Details</summary>
Motivation: 协作感知在多代理自动驾驶和机器人系统中表现优异，但易受恶意代理攻击，因此需要一种可靠的防御机制。

Method: 提出PASAC方法进行样本共识验证，定义CCLoss衡量协作差异，并通过动态阈值调整确保系统可靠性。

Result: 实验证明CP-Guard能有效检测和消除恶意代理，提升系统安全性。

Conclusion: CP-Guard为协作感知提供了一种概率无关且自适应的防御解决方案，适用于动态环境。

Abstract: Collaborative Perception (CP) has been shown to be a promising technique for
multi-agent autonomous driving and multi-agent robotic systems, where multiple
agents share their perception information to enhance the overall perception
performance and expand the perception range. However, in CP, an ego agent needs
to receive messages from its collaborators, which makes it vulnerable to
attacks from malicious agents. To address this critical issue, we propose a
unified, probability-agnostic, and adaptive framework, namely, CP-Guard, which
is a tailored defense mechanism for CP deployed by each agent to accurately
detect and eliminate malicious agents in its collaboration network. Our key
idea is to enable CP to reach a consensus rather than a conflict against an ego
agent's perception results. Based on this idea, we first develop a
probability-agnostic sample consensus (PASAC) method to effectively sample a
subset of the collaborators and verify the consensus without prior
probabilities of malicious agents. Furthermore, we define collaborative
consistency loss (CCLoss) for object detection task and bird's eye view (BEV)
segmentation task to capture the discrepancy between an ego agent and its
collaborators, which is used as a verification criterion for consensus. In
addition, we propose online adaptive threshold via dual sliding windows to
dynamically adjust the threshold for consensus verification and ensure the
reliability of the systems in dynamic environments. Finally, we conduct
extensive experiments and demonstrate the effectiveness of our framework. Code
will be released at https://github.com/CP-Security/CP-Guard

</details>


### [65] [MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering](https://arxiv.org/abs/2506.22900)
*Mai A. Shaaban,Tausifa Jan Saleem,Vijay Ram Papineni,Mohammad Yaqub*

Main category: cs.CV

TL;DR: MOTOR是一种新型多模态检索和重排序方法，通过结合文本和视觉信息提升医学视觉问答（MedVQA）的准确性，优于现有方法6.45%。


<details>
  <summary>Details</summary>
Motivation: 现有方法在医学视觉问答中常生成错误答案，且检索增强方法可能引入无关信息，忽略了多模态上下文的重要性。

Method: 提出MOTOR方法，利用基于文本和视觉信息的检索重排序，结合基础描述和最优传输技术。

Result: 在MedVQA数据集上，MOTOR平均准确率提升6.45%，优于现有方法。

Conclusion: MOTOR通过多模态上下文增强，显著提升了医学视觉问答的准确性和临床相关性。

Abstract: Medical visual question answering (MedVQA) plays a vital role in clinical
decision-making by providing contextually rich answers to image-based queries.
Although vision-language models (VLMs) are widely used for this task, they
often generate factually incorrect answers. Retrieval-augmented generation
addresses this challenge by providing information from external sources, but
risks retrieving irrelevant context, which can degrade the reasoning
capabilities of VLMs. Re-ranking retrievals, as introduced in existing
approaches, enhances retrieval relevance by focusing on query-text alignment.
However, these approaches neglect the visual or multimodal context, which is
particularly crucial for medical diagnosis. We propose MOTOR, a novel
multimodal retrieval and re-ranking approach that leverages grounded captions
and optimal transport. It captures the underlying relationships between the
query and the retrieved context based on textual and visual information.
Consequently, our approach identifies more clinically relevant contexts to
augment the VLM input. Empirical analysis and human expert evaluation
demonstrate that MOTOR achieves higher accuracy on MedVQA datasets,
outperforming state-of-the-art methods by an average of 6.45%. Code is
available at https://github.com/BioMedIA-MBZUAI/MOTOR.

</details>


### [66] [Attention to Burstiness: Low-Rank Bilinear Prompt Tuning](https://arxiv.org/abs/2506.22908)
*Yuzhu Wang,Manni Duan,Shu Kong*

Main category: cs.CV

TL;DR: 论文提出了一种名为Bilinear Prompt Tuning (BPT)的方法，通过数据白化和低秩分解优化视觉提示调谐（VPT），显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 视觉提示调谐（VPT）中，图像块嵌入与Transformer自注意力模块的交互导致非高斯分布，影响了提示学习的效果。

Method: 提出数据白化方法，对图像块嵌入和Transformer的关键投影器进行去相关和方差均衡，并引入低秩分解生成最终提示。

Result: BPT方法在CUB数据集上提升了超过25个准确率点，同时减少了参数数量和计算开销。

Conclusion: BPT方法在性能和效率上均优于现有VPT方法，为视觉Transformer的调谐提供了新思路。

Abstract: Visual Prompt Tuning (VPT) is a parameter-efficient fune-tuning technique
that adapts a pre-trained vision Transformer (ViT) by learning a small set of
parameters in the input space, known as prompts. In VPT, we uncover
``burstiness'' in the values arising from the interaction of image patch
embeddings, and the key and query projectors within Transformer's
self-attention module. Furthermore, the values of patch embeddings and the key
and query projectors exhibit Laplacian and hyper-Laplacian distribution,
respectively. Intuitively, these non-Gaussian distributions pose challenges for
learning prompts. To address this, we propose whitening these data,
de-correlating them and equalizing their variance towards more Gaussian before
learning prompts. We derive the whitening matrix over random image patch
embeddings and ViT's key and query projectors, and multiply it with the prompt
to be learned in a bilinear manner. Surprisingly, this method significantly
accelerates prompt tuning and boosts accuracy, e.g., $>$25 accuracy points on
the CUB dataset; interestingly, it learns ``bursty prompts''. Extending the
bilinear model which is known to introduce burstiness, we present a compact,
low-rank version by learning two smaller matrices whose multiplication yields
the final prompts. We call the proposed methods Bilinear Prompt Tuning (BPT).
Extensive experiments across multiple benchmark datasets demonstrate that BPT
methods not only outperform various VPT methods but also reduce parameter count
and computation overhead.

</details>


### [67] [Towards Explainable Bilingual Multimodal Misinformation Detection and Localization](https://arxiv.org/abs/2506.22930)
*Yiwei He,Xiangtai Li,Zhenglin Huang,Yi Dong,Hao Fei,Jiangning Zhang,Baoyuan Wu,Guangliang Cheng*

Main category: cs.CV

TL;DR: BiMi是一个双语多模态框架，用于检测新闻媒体中的虚假信息，通过区域级定位、跨模态和跨语言一致性检测以及自然语言解释，显著提升了分类和定位准确性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态内容的真实性增强，虚假信息变得更难检测，尤其是在双语字幕新闻中，局部图像编辑和跨语言不一致性共同扭曲了信息。

Method: BiMi框架结合了区域级定位、跨模态和跨语言一致性检测，并引入在线检索模块和GRPO优化解释质量。

Result: BiMi在分类准确性上提升8.9，定位准确性提升15.9，解释BERTScore提升2.5，优于现有基线。

Conclusion: BiMi在多语言虚假信息检测中取得了最先进的性能，并发布了BiMiBench基准测试和工具。

Abstract: The increasing realism of multimodal content has made misinformation more
subtle and harder to detect, especially in news media where images are
frequently paired with bilingual (e.g., Chinese-English) subtitles. Such
content often includes localized image edits and cross-lingual inconsistencies
that jointly distort meaning while remaining superficially plausible. We
introduce BiMi, a bilingual multimodal framework that jointly performs
region-level localization, cross-modal and cross-lingual consistency detection,
and natural language explanation for misinformation analysis. To support
generalization, BiMi integrates an online retrieval module that supplements
model reasoning with up-to-date external context. We further release BiMiBench,
a large-scale and comprehensive benchmark constructed by systematically editing
real news images and subtitles, comprising 104,000 samples with realistic
manipulations across visual and linguistic modalities. To enhance
interpretability, we apply Group Relative Policy Optimization (GRPO) to improve
explanation quality, marking the first use of GRPO in this domain. Extensive
experiments demonstrate that BiMi outperforms strong baselines by up to +8.9 in
classification accuracy, +15.9 in localization accuracy, and +2.5 in
explanation BERTScore, advancing state-of-the-art performance in realistic,
multilingual misinformation detection. Code, models, and datasets will be
released.

</details>


### [68] [Utilizing a Novel Deep Learning Method for Scene Categorization in Remote Sensing Data](https://arxiv.org/abs/2506.22939)
*Ghufran A. Omran,Wassan Saad Abduljabbar Hayale,Ahmad AbdulQadir AlRababah,Israa Ibraheem Al-Barazanchi,Ravi Sekhar,Pritesh Shah,Sushma Parihar,Harshavardhan Reddy Penubadi*

Main category: cs.CV

TL;DR: 论文提出了一种名为CO-BRNN的新方法，用于遥感数据的场景分类，其准确率高达97%，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 遥感图像场景分类在多个领域有广泛应用，但传统深度学习方法需要大量高噪声数据，难以实现高精度。

Method: 提出Cuttlefish优化的双向循环神经网络（CO-BRNN），并与多种现有方法（如MLP-CNN、CNN-LSTM等）进行比较。

Result: CO-BRNN达到97%的准确率，显著优于其他方法（LSTM-CRF 90%，MLP-CNN 85%，CNN-LSTM 80%）。

Conclusion: CO-BRNN在遥感场景分类中表现优异，同时强调了物理验证对卫星数据效率的重要性。

Abstract: Scene categorization (SC) in remotely acquired images is an important subject
with broad consequences in different fields, including catastrophe control,
ecological observation, architecture for cities, and more. Nevertheless, its
several apps, reaching a high degree of accuracy in SC from distant observation
data has demonstrated to be difficult. This is because traditional conventional
deep learning models require large databases with high variety and high levels
of noise to capture important visual features. To address these problems, this
investigation file introduces an innovative technique referred to as the
Cuttlefish Optimized Bidirectional Recurrent Neural Network (CO- BRNN) for type
of scenes in remote sensing data. The investigation compares the execution of
CO-BRNN with current techniques, including Multilayer Perceptron- Convolutional
Neural Network (MLP-CNN), Convolutional Neural Network-Long Short Term Memory
(CNN-LSTM), and Long Short Term Memory-Conditional Random Field (LSTM-CRF),
Graph-Based (GB), Multilabel Image Retrieval Model (MIRM-CF), Convolutional
Neural Networks Data Augmentation (CNN-DA). The results demonstrate that
CO-BRNN attained the maximum accuracy of 97%, followed by LSTM-CRF with 90%,
MLP-CNN with 85%, and CNN-LSTM with 80%. The study highlights the significance
of physical confirmation to ensure the efficiency of satellite data.

</details>


### [69] [YM-WML: A new Yolo-based segmentation Model with Weighted Multi-class Loss for medical imaging](https://arxiv.org/abs/2506.22955)
*Haniyeh Nikkhah,Jafar Tanha,Mahdi Zarrin,SeyedEhsan Roshan,Amin Kazempour*

Main category: cs.CV

TL;DR: YM-WML模型通过结合强大的特征提取、多尺度特征聚合和注意力机制，解决了医学图像分割中的类别不平衡和复杂结构问题，并在ACDC数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临类别不平衡和复杂结构的挑战，需要一种更有效的解决方案。

Method: 提出YM-WML模型，结合特征提取、YOLOv11多尺度特征聚合和注意力分割头，并引入WME损失函数处理类别不平衡。

Result: 在ACDC数据集上，Dice系数达到91.02，优于现有方法，表现出稳定的训练和强泛化能力。

Conclusion: YM-WML为心脏图像分割任务设定了新基准，具有高精度和鲁棒性。

Abstract: Medical image segmentation poses significant challenges due to class
imbalance and the complex structure of medical images. To address these
challenges, this study proposes YM-WML, a novel model for cardiac image
segmentation. The model integrates a robust backbone for effective feature
extraction, a YOLOv11 neck for multi-scale feature aggregation, and an
attention-based segmentation head for precise and accurate segmentation. To
address class imbalance, we introduce the Weighted Multi-class Exponential
(WME) loss function. On the ACDC dataset, YM-WML achieves a Dice Similarity
Coefficient of 91.02, outperforming state-of-the-art methods. The model
demonstrates stable training, accurate segmentation, and strong generalization,
setting a new benchmark in cardiac segmentation tasks.

</details>


### [70] [Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image Watermarking Technique for AI-Generated Images](https://arxiv.org/abs/2506.22960)
*Shreyas Dixit,Ashhar Aziz,Shashwat Bajpai,Vasu Sharma,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.CV

TL;DR: 欧盟预测到2026年90%的在线内容可能是AI生成的，引发对政治虚假信息的担忧。加州法案AB 3211要求对AI生成内容加水印，但现有技术易被篡改。本文提出PECCAVI，一种抗视觉转述攻击的无失真水印技术。


<details>
  <summary>Details</summary>
Motivation: 生成式AI可能加剧政治虚假信息传播，现有水印技术易被攻击，需要更安全的解决方案。

Method: PECCAVI通过在多通道频域嵌入水印，并利用噪声抛光技术保护非熔化点（NMPs），防止水印被移除。

Result: PECCAVI能有效抵抗视觉转述攻击，保持图像无失真，且模型无关。

Conclusion: PECCAVI为AI生成内容提供了一种安全、耐用的水印解决方案，代码将开源。

Abstract: A report by the European Union Law Enforcement Agency predicts that by 2026,
up to 90 percent of online content could be synthetically generated, raising
concerns among policymakers, who cautioned that "Generative AI could act as a
force multiplier for political disinformation. The combined effect of
generative text, images, videos, and audio may surpass the influence of any
single modality." In response, California's Bill AB 3211 mandates the
watermarking of AI-generated images, videos, and audio. However, concerns
remain regarding the vulnerability of invisible watermarking techniques to
tampering and the potential for malicious actors to bypass them entirely.
Generative AI-powered de-watermarking attacks, especially the newly introduced
visual paraphrase attack, have shown an ability to fully remove watermarks,
resulting in a paraphrase of the original image. This paper introduces PECCAVI,
the first visual paraphrase attack-safe and distortion-free image watermarking
technique. In visual paraphrase attacks, an image is altered while preserving
its core semantic regions, termed Non-Melting Points (NMPs). PECCAVI
strategically embeds watermarks within these NMPs and employs multi-channel
frequency domain watermarking. It also incorporates noisy burnishing to counter
reverse-engineering efforts aimed at locating NMPs to disrupt the embedded
watermark, thereby enhancing durability. PECCAVI is model-agnostic. All
relevant resources and codes will be open-sourced.

</details>


### [71] [ActAlign: Zero-Shot Fine-Grained Video Classification via Language-Guided Sequence Alignment](https://arxiv.org/abs/2506.22967)
*Amir Aghdam,Vincent Tao Hu*

Main category: cs.CV

TL;DR: ActAlign是一个零样本视频分类框架，通过序列对齐方法解决细粒度视频分类任务，无需视频示例或时间标注。


<details>
  <summary>Details</summary>
Motivation: 现有对比视觉语言模型（如SigLIP）在开放集识别中表现良好，但无法捕捉细粒度动作的时间结构。

Method: ActAlign利用大语言模型生成有序子动作序列，并通过动态时间规整（DTW）在共享嵌入空间中对齐视频帧。

Result: 在ActionAtlas基准测试中，ActAlign达到30.5%的准确率，优于十亿参数视频语言模型，且参数量减少8倍。

Conclusion: 结合结构化语言先验和经典对齐技术，ActAlign为细粒度视频理解提供了一种可扩展的通用方法。

Abstract: We address the task of zero-shot fine-grained video classification, where no
video examples or temporal annotations are available for unseen action classes.
While contrastive vision-language models such as SigLIP demonstrate strong
open-set recognition via mean-pooled image-text similarity, they fail to
capture the temporal structure critical for distinguishing fine-grained
activities. We introduce ActAlign, a zero-shot framework that formulates video
classification as sequence alignment. For each class, a large language model
generates an ordered sub-action sequence, which is aligned with video frames
using Dynamic Time Warping (DTW) in a shared embedding space. Without any
video-text supervision or fine-tuning, ActAlign achieves 30.5% accuracy on the
extremely challenging ActionAtlas benchmark, where human accuracy is only
61.6%. ActAlign outperforms billion-parameter video-language models while using
approximately 8x less parameters. These results demonstrate that structured
language priors, combined with classical alignment techniques, offer a scalable
and general approach to unlocking the open-set recognition potential of
vision-language models for fine-grained video understanding.

</details>


### [72] [Probabilistic Prototype Calibration of Vision-Language Models for Generalized Few-shot Semantic Segmentation](https://arxiv.org/abs/2506.22979)
*Jie Liu,Jiayi Shen,Pan Zhou,Jan-Jakob Sonke,Efstratios Gavves*

Main category: cs.CV

TL;DR: FewCLIP提出了一种概率原型校准框架，通过多模态原型学习改进广义少样本语义分割（GFSS）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于原型的方法在少样本情况下缺乏适应性，FewCLIP旨在通过概率原型校准提升模型对新类的泛化能力。

Method: FewCLIP结合冻结的文本原型和可学习的视觉校准原型，并通过分布正则化实现不确定性感知的原型学习。

Result: 在PASCAL-5i和COCO-20i数据集上，FewCLIP显著优于现有方法。

Conclusion: FewCLIP通过概率原型校准有效提升了GFSS的性能和泛化能力。

Abstract: Generalized Few-Shot Semantic Segmentation (GFSS) aims to extend a
segmentation model to novel classes with only a few annotated examples while
maintaining performance on base classes. Recently, pretrained vision-language
models (VLMs) such as CLIP have been leveraged in GFSS to improve
generalization on novel classes through multi-modal prototypes learning.
However, existing prototype-based methods are inherently deterministic,
limiting the adaptability of learned prototypes to diverse samples,
particularly for novel classes with scarce annotations. To address this, we
propose FewCLIP, a probabilistic prototype calibration framework over
multi-modal prototypes from the pretrained CLIP, thus providing more adaptive
prototype learning for GFSS. Specifically, FewCLIP first introduces a prototype
calibration mechanism, which refines frozen textual prototypes with learnable
visual calibration prototypes, leading to a more discriminative and adaptive
representation. Furthermore, unlike deterministic prototype learning
techniques, FewCLIP introduces distribution regularization over these
calibration prototypes. This probabilistic formulation ensures structured and
uncertainty-aware prototype learning, effectively mitigating overfitting to
limited novel class data while enhancing generalization. Extensive experimental
results on PASCAL-5$^i$ and COCO-20$^i$ datasets demonstrate that our proposed
FewCLIP significantly outperforms state-of-the-art approaches across both GFSS
and class-incremental setting. The code is available at
https://github.com/jliu4ai/FewCLIP.

</details>


### [73] [Revisiting CroPA: A Reproducibility Study and Enhancements for Cross-Prompt Adversarial Transferability in Vision-Language Models](https://arxiv.org/abs/2506.22982)
*Atharv Mittal,Agam Pandey,Amritanshu Tiwari,Sukrit Jindal,Swadesh Swain*

Main category: cs.CV

TL;DR: 论文研究了大型视觉语言模型（VLMs）在对抗攻击中的脆弱性，验证了CroPA方法的跨提示迁移性，并提出了改进策略以提高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在对抗攻击中表现脆弱，尤其是在多模态被操纵的情况下。研究旨在验证CroPA方法的有效性并提升其攻击性能。

Method: 1. 提出新的初始化策略提高攻击成功率；2. 研究跨图像迁移性；3. 设计针对视觉编码器注意力机制的损失函数。

Result: 改进方法在多个VLMs（如Flamingo、BLIP-2等）上验证了CroPA的优越性，并显著提升了对抗攻击效果。

Conclusion: 研究强调了VLMs对抗脆弱性的重要性，并为生成可迁移对抗样本提供了更鲁棒的框架。

Abstract: Large Vision-Language Models (VLMs) have revolutionized computer vision,
enabling tasks such as image classification, captioning, and visual question
answering. However, they remain highly vulnerable to adversarial attacks,
particularly in scenarios where both visual and textual modalities can be
manipulated. In this study, we conduct a comprehensive reproducibility study of
"An Image is Worth 1000 Lies: Adversarial Transferability Across Prompts on
Vision-Language Models" validating the Cross-Prompt Attack (CroPA) and
confirming its superior cross-prompt transferability compared to existing
baselines. Beyond replication we propose several key improvements: (1) A novel
initialization strategy that significantly improves Attack Success Rate (ASR).
(2) Investigate cross-image transferability by learning universal
perturbations. (3) A novel loss function targeting vision encoder attention
mechanisms to improve generalization. Our evaluation across prominent VLMs --
including Flamingo, BLIP-2, and InstructBLIP as well as extended experiments on
LLaVA validates the original results and demonstrates that our improvements
consistently boost adversarial effectiveness. Our work reinforces the
importance of studying adversarial vulnerabilities in VLMs and provides a more
robust framework for generating transferable adversarial examples, with
significant implications for understanding the security of VLMs in real-world
applications.

</details>


### [74] [MusiXQA: Advancing Visual Music Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2506.23009)
*Jian Chen,Wenye Ma,Penghang Liu,Wei Wang,Tengwei Song,Ming Li,Chenguang Wang,Ruiyi Zhang,Changyou Chen*

Main category: cs.CV

TL;DR: 该论文介绍了MusiXQA数据集，用于评估和改进多模态大语言模型（MLLMs）在乐谱理解方面的能力，并提出了Phi-3-MusiX模型，性能优于GPT方法。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在乐谱理解方面能力不足，缺乏相关数据集，因此需要填补这一空白。

Method: 使用MusiXTeX生成高质量合成乐谱，构建MusiXQA数据集，并开发Phi-3-MusiX模型进行微调。

Result: 当前MLLMs在乐谱理解方面表现有限，Phi-3-MusiX显著优于GPT方法。

Conclusion: MusiXQA和Phi-3-MusiX为未来MLLMs在乐谱理解领域的发展奠定了基础。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable visual
reasoning abilities in natural images, text-rich documents, and graphic
designs. However, their ability to interpret music sheets remains
underexplored. To bridge this gap, we introduce MusiXQA, the first
comprehensive dataset for evaluating and advancing MLLMs in music sheet
understanding. MusiXQA features high-quality synthetic music sheets generated
via MusiXTeX, with structured annotations covering note pitch and duration,
chords, clefs, key/time signatures, and text, enabling diverse visual QA tasks.
Through extensive evaluations, we reveal significant limitations of current
state-of-the-art MLLMs in this domain. Beyond benchmarking, we developed
Phi-3-MusiX, an MLLM fine-tuned on our dataset, achieving significant
performance gains over GPT-based methods. The proposed dataset and model
establish a foundation for future advances in MLLMs for music sheet
understanding. Code, data, and model will be released upon acceptance.

</details>


### [75] [VisionScores -- A system-segmented image score dataset for deep learning tasks](https://arxiv.org/abs/2506.23030)
*Alejandro Romero Amezcua,Mariano José Juan Rivera Meraz*

Main category: cs.CV

TL;DR: VisionScores是一个首个系统分割的图像乐谱数据集，专注于双钢琴曲目，提供高信息密度的图像，适用于机器和深度学习任务。


<details>
  <summary>Details</summary>
Motivation: 为机器和深度学习任务提供结构丰富、信息密集的图像乐谱数据，同时考虑图形相似性和作曲模式。

Method: 数据集包含24.8k样本，分为两种场景：同一作曲类型不同作者（14k样本）和同一作者不同作曲类型（10.8k样本）。样本为128×512像素的灰度图像，并提供元数据和未分割的完整乐谱。

Result: 提供了24.8k个格式化样本，以及系统顺序、元数据和未分割的完整乐谱。

Conclusion: VisionScores为研究乐谱分析和深度学习提供了丰富的数据资源。

Abstract: VisionScores presents a novel proposal being the first system-segmented image
score dataset, aiming to offer structure-rich, high information-density images
for machine and deep learning tasks. Delimited to two-handed piano pieces, it
was built to consider not only certain graphic similarity but also composition
patterns, as this creative process is highly instrument-dependent. It provides
two scenarios in relation to composer and composition type. The first, formed
by 14k samples, considers works from different authors but the same composition
type, specifically, Sonatinas. The latter, consisting of 10.8K samples,
presents the opposite case, various composition types from the same author,
being the one selected Franz Liszt. All of the 24.8k samples are formatted as
grayscale jpg images of $128 \times 512$ pixels. VisionScores supplies the
users not only the formatted samples but the systems' order and pieces'
metadata. Moreover, unsegmented full-page scores and the pre-formatted images
are included for further analysis.

</details>


### [76] [Inpainting is All You Need: A Diffusion-based Augmentation Method for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2506.23038)
*Xinrong Hu,Yiyu Shi*

Main category: cs.CV

TL;DR: AugPaint是一种数据增强框架，利用潜在扩散模型通过修复生成图像-标签对，解决医学图像分割中标注数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 医学数据集的像素级标注成本高且耗时，如何在标注数据稀缺的情况下提升分割性能是关键挑战。

Method: AugPaint利用潜在扩散模型进行修复，无需重新训练，通过反向去噪过程生成与标签掩码匹配的合成图像。

Result: 在四个公共医学图像分割数据集（CT、MRI和皮肤成像）上，AugPaint显著优于现有方法，提升了分割性能。

Conclusion: AugPaint通过高效生成图像-标签对，有效解决了标注数据稀缺问题，为下游分割模型提供了有价值的监督。

Abstract: Collecting pixel-level labels for medical datasets can be a laborious and
expensive process, and enhancing segmentation performance with a scarcity of
labeled data is a crucial challenge. This work introduces AugPaint, a data
augmentation framework that utilizes inpainting to generate image-label pairs
from limited labeled data. AugPaint leverages latent diffusion models, known
for their ability to generate high-quality in-domain images with low overhead,
and adapts the sampling process for the inpainting task without need for
retraining. Specifically, given a pair of image and label mask, we crop the
area labeled with the foreground and condition on it during reversed denoising
process for every noise level. Masked background area would gradually be filled
in, and all generated images are paired with the label mask. This approach
ensures the accuracy of match between synthetic images and label masks, setting
it apart from existing dataset generation methods. The generated images serve
as valuable supervision for training downstream segmentation models,
effectively addressing the challenge of limited annotations. We conducted
extensive evaluations of our data augmentation method on four public medical
image segmentation datasets, including CT, MRI, and skin imaging. Results
across all datasets demonstrate that AugPaint outperforms state-of-the-art
label-efficient methodologies, significantly improving segmentation
performance.

</details>


### [77] [From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient 3D Gaussian Splatting](https://arxiv.org/abs/2506.23042)
*Hung Nguyen,An Le,Runfa Li,Truong Nguyen*

Main category: cs.CV

TL;DR: AutoOpti3DGS通过小波变换控制高斯数量，提升3D高斯溅射的效率，减少内存占用。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯溅射中高斯数量增长导致的内存和带宽问题。

Method: 使用可学习的离散小波变换，固定低通滤波器，学习高通滤波器，并通过正交性损失逐步激活细节。

Result: AutoOpti3DGS生成更稀疏的场景表示，兼容内存受限硬件，且无需复杂调参。

Conclusion: AutoOpti3DGS有效平衡了视觉保真度和计算效率，适用于资源受限环境。

Abstract: 3D Gaussian Splatting has emerged as a powerful approach in novel view
synthesis, delivering rapid training and rendering but at the cost of an
ever-growing set of Gaussian primitives that strains memory and bandwidth. We
introduce AutoOpti3DGS, a training-time framework that automatically restrains
Gaussian proliferation without sacrificing visual fidelity. The key idea is to
feed the input images to a sequence of learnable Forward and Inverse Discrete
Wavelet Transforms, where low-pass filters are kept fixed, high-pass filters
are learnable and initialized to zero, and an auxiliary orthogonality loss
gradually activates fine frequencies. This wavelet-driven, coarse-to-fine
process delays the formation of redundant fine Gaussians, allowing 3DGS to
capture global structure first and refine detail only when necessary. Through
extensive experiments, AutoOpti3DGS requires just a single filter learning-rate
hyper-parameter, integrates seamlessly with existing efficient 3DGS frameworks,
and consistently produces sparser scene representations more compatible with
memory or storage-constrained hardware.

</details>


### [78] [Ovis-U1 Technical Report](https://arxiv.org/abs/2506.23044)
*Guo-Hua Wang,Shanshan Zhao,Xinjie Zhang,Liangfu Cao,Pengxin Zhan,Lunhao Duan,Shiyin Lu,Minghao Fu,Xiaohao Chen,Jianshan Zhao,Yang Li,Qing-Guo Chen*

Main category: cs.CV

TL;DR: Ovis-U1是一个30亿参数的多模态统一模型，集成了理解、生成和编辑功能，性能优于现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 通过统一训练方法整合多模态理解和生成任务，提升模型性能。

Method: 采用扩散式视觉解码器和双向令牌细化器，从语言模型开始进行统一训练。

Result: 在OpenCompass等多项基准测试中表现优异，超越Ristretto-3B等模型。

Conclusion: Ovis-U1推动了多模态理解、生成和编辑的边界，是系列的首个版本。

Abstract: In this report, we introduce Ovis-U1, a 3-billion-parameter unified model
that integrates multimodal understanding, text-to-image generation, and image
editing capabilities. Building on the foundation of the Ovis series, Ovis-U1
incorporates a diffusion-based visual decoder paired with a bidirectional token
refiner, enabling image generation tasks comparable to leading models like
GPT-4o. Unlike some previous models that use a frozen MLLM for generation
tasks, Ovis-U1 utilizes a new unified training approach starting from a
language model. Compared to training solely on understanding or generation
tasks, unified training yields better performance, demonstrating the
enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score
of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent
state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In
text-to-image generation, it excels with scores of 83.72 and 0.89 on the
DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves
4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the
initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries
of multimodal understanding, generation, and editing.

</details>


### [79] [Empowering Small VLMs to Think with Dynamic Memorization and Exploration](https://arxiv.org/abs/2506.23061)
*Jiazhen Liu,Yuchuan Deng,Long Chen*

Main category: cs.CV

TL;DR: DyME是一种动态选择记忆（SFT）和探索（RLVR）模式的训练范式，显著提升了小规模视觉语言模型（SVLMs）的思维可靠性和任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有训练范式（如SFT和RLVR）对SVLMs要求过高，导致伪思维痕迹和优势崩溃，限制了其性能和可靠性。

Method: 提出DyME，动态选择SFT和RLVR模式，确保每次优化步骤都能平衡记忆与探索。

Result: 实验表明DyME能有效平衡两者，显著提升SVLMs的性能。

Conclusion: DyME是一种实用且有效的解决方案，能够增强SVLMs的可靠思维能力。

Abstract: Empowering Small-scale Vision-Language Models (SVLMs) with reliable thinking
capabilities remains fundamentally challenging due to their limited parameter
capacity and weak instruction-following abilities. Existing training paradigms,
including Supervised Fine-Tuning (SFT) and Reinforcement Learning with
Verifiable Reward (RLVR), impose substantial demands on the base VLM, exceeding
the capabilities of SVLMs. Consequently, directly applying these paradigms to
SVLMs often suffers from severe pseudo thinking traces and advantage collapse,
ultimately undermining both thinking reliability and task performance. A
natural solution is to combine SFT and RLVR, leveraging their complementarity
to reduce the dependence on model capacity. However, the widely adopted
two-stage training paradigm still performs poorly on SVLMs, as their tendency
toward sub-optimal convergence hinders the trade-off and limits the benefits of
the combination. To address this, we propose DyME, a novel training paradigm
that Dynamically selects between Memorization (via SFT) and Exploration (via
RLVR) modes at each optimization step, ensuring that every update contributes
to the trade-off. Extensive experiments across diverse domains demonstrate that
DyME consistently achieves this balance, and thus delivers substantial
performance improvements. These results establish DyME as a practical and
effective solution for empowering SVLMs with reliable thinking capabilities.
GitHub: https://github.com/HKUST-LongGroup/DyME

</details>


### [80] [CoreMark: Toward Robust and Universal Text Watermarking Technique](https://arxiv.org/abs/2506.23066)
*Jiale Meng,Yiming Li,Zheming Lu,Zewei He,Hao Luo,Tianwei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为CORE的新嵌入范式，并基于此构建了文本水印框架CoreMark，通过动态提取和调整CORE的厚度嵌入数据，显著提升了鲁棒性和通用性。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本水印方案在鲁棒性、通用性和不可感知性方面难以兼顾的问题。

Method: 提出CORE嵌入范式，动态提取并选择鲁棒性强的字符，通过调整CORE厚度嵌入数据，并引入嵌入强度调节器。

Result: CoreMark在多语言和字体中表现优异，抗截图、打印扫描和打印相机攻击能力显著提升。

Conclusion: CoreMark在保持不可感知性的同时，显著提升了文本水印的鲁棒性和通用性。

Abstract: Text watermarking schemes have gained considerable attention in recent years,
yet still face critical challenges in achieving simultaneous robustness,
generalizability, and imperceptibility. This paper introduces a new embedding
paradigm,termed CORE, which comprises several consecutively aligned black pixel
segments. Its key innovation lies in its inherent noise resistance during
transmission and broad applicability across languages and fonts. Based on the
CORE, we present a text watermarking framework named CoreMark. Specifically,
CoreMark first dynamically extracts COREs from characters. Then, the characters
with stronger robustness are selected according to the lengths of COREs. By
modifying the thickness of the CORE, the hidden data is embedded into the
selected characters without causing significant visual distortions. Moreover, a
general plug-and-play embedding strength modulator is proposed, which can
adaptively enhance the robustness for small font sizes by adjusting the
embedding strength according to the font size. Experimental evaluation
indicates that CoreMark demonstrates outstanding generalizability across
multiple languages and fonts. Compared to existing methods, CoreMark achieves
significant improvements in resisting screenshot, print-scan, and print camera
attacks, while maintaining satisfactory imperceptibility.

</details>


### [81] [Unsupervised 3D Braided Hair Reconstruction from a Single-View Image](https://arxiv.org/abs/2506.23072)
*Jing Gao*

Main category: cs.CV

TL;DR: 提出了一种无监督的3D编织发型重建方法，优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 单视角图像中3D编织发型重建因复杂结构而具有挑战性，现有方法难以捕捉细节。

Method: 利用受编织理论启发的合成模型，捕捉编织发型的复杂交织结构。

Result: 实验表明，该方法在准确性、真实性和效率上优于现有技术。

Conclusion: 支持数字人类中更具表现力的发型建模。

Abstract: Reconstructing 3D braided hairstyles from single-view images remains a
challenging task due to the intricate interwoven structure and complex
topologies of braids. Existing strand-based hair reconstruction methods
typically focus on loose hairstyles and often struggle to capture the
fine-grained geometry of braided hair. In this paper, we propose a novel
unsupervised pipeline for efficiently reconstructing 3D braided hair from
single-view RGB images. Leveraging a synthetic braid model inspired by braid
theory, our approach effectively captures the complex intertwined structures of
braids. Extensive experiments demonstrate that our method outperforms
state-of-the-art approaches, providing superior accuracy, realism, and
efficiency in reconstructing 3D braided hairstyles, supporting expressive
hairstyle modeling in digital humans.

</details>


### [82] [Learning Counterfactually Decoupled Attention for Open-World Model Attribution](https://arxiv.org/abs/2506.23074)
*Yu Zheng,Boyang Gong,Fanye Kong,Yueqi Duan,Bingyao Yu,Wenzhao Zheng,Lei Chen,Jiwen Lu,Jie Zhou*

Main category: cs.CV

TL;DR: 提出了一种反事实解耦注意力学习（CDAL）方法，用于开放世界模型归属任务，通过建模注意力视觉痕迹与源模型归属的因果关系，提升对未见攻击的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手工设计的区域划分或特征空间，易受虚假统计相关性干扰，难以应对开放世界中的新型攻击。

Method: CDAL显式建模注意力视觉痕迹与源模型归属的因果关系，反事实解耦判别性模型特定伪影与混淆源偏差。

Result: 在开放世界模型归属基准测试中，CDAL显著提升现有最优模型性能，尤其对未见攻击效果突出。

Conclusion: CDAL通过最大化因果效应，鼓励网络捕捉泛化性强的生成模式，为开放世界模型归属提供了有效解决方案。

Abstract: In this paper, we propose a Counterfactually Decoupled Attention Learning
(CDAL) method for open-world model attribution. Existing methods rely on
handcrafted design of region partitioning or feature space, which could be
confounded by the spurious statistical correlations and struggle with novel
attacks in open-world scenarios. To address this, CDAL explicitly models the
causal relationships between the attentional visual traces and source model
attribution, and counterfactually decouples the discriminative model-specific
artifacts from confounding source biases for comparison. In this way, the
resulting causal effect provides a quantification on the quality of learned
attention maps, thus encouraging the network to capture essential generation
patterns that generalize to unseen source models by maximizing the effect.
Extensive experiments on existing open-world model attribution benchmarks show
that with minimal computational overhead, our method consistently improves
state-of-the-art models by large margins, particularly for unseen novel
attacks. Source code: https://github.com/yzheng97/CDAL.

</details>


### [83] [Dynamic Contrastive Learning for Hierarchical Retrieval: A Case Study of Distance-Aware Cross-View Geo-Localization](https://arxiv.org/abs/2506.23077)
*Suofei Zhang,Xinxin Wang,Xiaofu Wu,Quan Zhou,Haifeng Hu*

Main category: cs.CV

TL;DR: 论文提出了一种动态对比学习框架（DyCL），用于解决距离感知的跨视角地理定位问题，并通过构建DA-Campus基准数据集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注跨域图像匹配的准确性，而忽略了目标周围上下文信息的全面捕捉和定位误差的最小化。

Method: 提出动态对比学习（DyCL）框架，通过层次化空间边界的渐进对齐特征表示。

Result: DyCL在层次化检索性能和跨视角地理定位准确性上均有显著提升。

Conclusion: DyCL为解决复杂空间关系的地理定位问题提供了有效方法，并公开了代码和基准数据集。

Abstract: Existing deep learning-based cross-view geo-localization methods primarily
focus on improving the accuracy of cross-domain image matching, rather than
enabling models to comprehensively capture contextual information around the
target and minimize the cost of localization errors. To support systematic
research into this Distance-Aware Cross-View Geo-Localization (DACVGL) problem,
we construct Distance-Aware Campus (DA-Campus), the first benchmark that pairs
multi-view imagery with precise distance annotations across three spatial
resolutions. Based on DA-Campus, we formulate DACVGL as a hierarchical
retrieval problem across different domains. Our study further reveals that, due
to the inherent complexity of spatial relationships among buildings, this
problem can only be addressed via a contrastive learning paradigm, rather than
conventional metric learning. To tackle this challenge, we propose Dynamic
Contrastive Learning (DyCL), a novel framework that progressively aligns
feature representations according to hierarchical spatial margins. Extensive
experiments demonstrate that DyCL is highly complementary to existing
multi-scale metric learning methods and yields substantial improvements in both
hierarchical retrieval performance and overall cross-view geo-localization
accuracy. Our code and benchmark are publicly available at
https://github.com/anocodetest1/DyCL.

</details>


### [84] [Frequency-enhanced Multi-granularity Context Network for Efficient Vertebrae Segmentation](https://arxiv.org/abs/2506.23086)
*Jian Shi,Tianqi You,Pingping Zhang,Hongli Zhang,Rui Xu,Haojie Li*

Main category: cs.CV

TL;DR: 提出了一种频率增强的多粒度上下文网络（FMC-Net），用于提高3D CT和MRI图像中椎骨分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于当前成像技术的局限性和脊柱结构的复杂性，现有方法在减少图像模糊影响和区分相似椎骨方面仍存在困难。

Method: 使用小波变换进行无损下采样，分别处理高频和低频成分。高频部分采用高频特征细化（HFR）增强关键特征，低频部分使用多粒度状态空间模型（MG-SSM）提取多粒度上下文。

Result: 实验表明，该方法在CT和MRI椎骨分割数据集上优于现有技术。

Conclusion: FMC-Net通过频率增强和多粒度上下文提取，显著提高了椎骨分割的准确性。

Abstract: Automated and accurate segmentation of individual vertebra in 3D CT and MRI
images is essential for various clinical applications. Due to the limitations
of current imaging techniques and the complexity of spinal structures, existing
methods still struggle with reducing the impact of image blurring and
distinguishing similar vertebrae. To alleviate these issues, we introduce a
Frequency-enhanced Multi-granularity Context Network (FMC-Net) to improve the
accuracy of vertebrae segmentation. Specifically, we first apply wavelet
transform for lossless downsampling to reduce the feature distortion in blurred
images. The decomposed high and low-frequency components are then processed
separately. For the high-frequency components, we apply a High-frequency
Feature Refinement (HFR) to amplify the prominence of key features and filter
out noises, restoring fine-grained details in blurred images. For the
low-frequency components, we use a Multi-granularity State Space Model (MG-SSM)
to aggregate feature representations with different receptive fields,
extracting spatially-varying contexts while capturing long-range dependencies
with linear complexity. The utilization of multi-granularity contexts is
essential for distinguishing similar vertebrae and improving segmentation
accuracy. Extensive experiments demonstrate that our method outperforms
state-of-the-art approaches on both CT and MRI vertebrae segmentation datasets.
The source code is publicly available at https://github.com/anaanaa/FMCNet.

</details>


### [85] [Where, What, Why: Towards Explainable Driver Attention Prediction](https://arxiv.org/abs/2506.23088)
*Yuchen Zhou,Jiayu Tang,Xiaoyan Xiao,Yueyao Lin,Linkai Liu,Zipeng Guo,Hao Fei,Xiaobo Xia,Chao Gou*

Main category: cs.CV

TL;DR: 论文提出了一种可解释的驾驶员注意力预测任务范式（W3DA），结合空间注意力区域预测、语义解析和认知推理，并提出了基于大语言模型的框架LLada。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅预测驾驶员注视点的空间热图，未能捕捉特定情境下注意力分配的认知动机，限制了注意力机制的深入理解。

Method: 提出了W3DA数据集，包含详细语义和因果标注，并开发了LLada框架，统一像素建模、语义解析和认知推理。

Result: LLada在多种数据集和驾驶条件下表现出强大的泛化能力。

Conclusion: 该研究为深入理解驾驶员注意力机制提供了关键步骤，对自动驾驶、智能驾驶培训和HCI有重要意义。

Abstract: Modeling task-driven attention in driving is a fundamental challenge for both
autonomous vehicles and cognitive science. Existing methods primarily predict
where drivers look by generating spatial heatmaps, but fail to capture the
cognitive motivations behind attention allocation in specific contexts, which
limits deeper understanding of attention mechanisms. To bridge this gap, we
introduce Explainable Driver Attention Prediction, a novel task paradigm that
jointly predicts spatial attention regions (where), parses attended semantics
(what), and provides cognitive reasoning for attention allocation (why). To
support this, we present W3DA, the first large-scale explainable driver
attention dataset. It enriches existing benchmarks with detailed semantic and
causal annotations across diverse driving scenarios, including normal
conditions, safety-critical situations, and traffic accidents. We further
propose LLada, a Large Language model-driven framework for driver attention
prediction, which unifies pixel modeling, semantic parsing, and cognitive
reasoning within an end-to-end architecture. Extensive experiments demonstrate
the effectiveness of LLada, exhibiting robust generalization across datasets
and driving conditions. This work serves as a key step toward a deeper
understanding of driver attention mechanisms, with significant implications for
autonomous driving, intelligent driver training, and human-computer
interaction.

</details>


### [86] [DC-TTA: Divide-and-Conquer Framework for Test-Time Adaptation of Interactive Segmentation](https://arxiv.org/abs/2506.23104)
*Jihun Kim,Hoyong Kwon,Hyeokjun Kweon,Wooseong Jeong,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: DC-TTA是一种新的测试时适应框架，通过用户交互监督改进SAM在复杂场景中的表现。


<details>
  <summary>Details</summary>
Motivation: SAM在专业领域或复杂场景（如伪装或多部分对象）中表现不佳，需要改进。

Method: DC-TTA将用户点击分为更一致的子集，每个子集通过独立的TTA模型处理，减少冲突并实现局部更新。

Result: 实验表明，DC-TTA显著优于SAM的零样本结果和传统TTA方法，提高了复杂任务的准确性和交互效率。

Conclusion: DC-TTA通过分而治之策略，有效提升了SAM在复杂场景中的性能。

Abstract: Interactive segmentation (IS) allows users to iteratively refine object
boundaries with minimal cues, such as positive and negative clicks. While the
Segment Anything Model (SAM) has garnered attention in the IS community for its
promptable segmentation capabilities, it often struggles in specialized domains
or when handling complex scenarios (e.g., camouflaged or multi-part objects).
To overcome these challenges, we propose DC-TTA, a novel test-time adaptation
(TTA) framework that adapts SAM on a per-sample basis by leveraging user
interactions as supervision. Instead of forcing a single model to incorporate
all user clicks at once, DC-TTA partitions the clicks into more coherent
subsets, each processed independently via TTA with a separated model. This
Divide-and-Conquer strategy reduces conflicts among diverse cues and enables
more localized updates. Finally, we merge the adapted models to form a unified
predictor that integrates the specialized knowledge from each subset.
Experimental results across various benchmarks demonstrate that DC-TTA
significantly outperforms SAM's zero-shot results and conventional TTA methods,
effectively handling complex tasks such as camouflaged object segmentation with
fewer interactions and improved accuracy.

</details>


### [87] [Computer-Aided Multi-Stroke Character Simplification by Stroke Removal](https://arxiv.org/abs/2506.23106)
*Ryo Ishiyama,Shinnosuke Matsuo,Seiichi Uchida*

Main category: cs.CV

TL;DR: 提出一种通过选择性去除笔画来简化多笔画汉字的方法，以降低学习难度并提高字体设计的简洁性。


<details>
  <summary>Details</summary>
Motivation: 多笔画汉字对非母语学习者难度大，简化后能降低学习障碍、优化字体设计并提升字符通信效率。

Method: 利用高精度字符识别模型评估可读性，选择性去除对可读性影响最小的笔画。

Result: 实验表明，即使去除多笔画，许多汉字仍可区分，为简化策略提供了依据。

Conclusion: 该方法展示了多笔画汉字简化的潜力，为未来更系统的简化策略奠定了基础。

Abstract: Multi-stroke characters in scripts such as Chinese and Japanese can be highly
complex, posing significant challenges for both native speakers and,
especially, non-native learners. If these characters can be simplified without
degrading their legibility, it could reduce learning barriers for non-native
speakers, facilitate simpler and legible font designs, and contribute to
efficient character-based communication systems. In this paper, we propose a
framework to systematically simplify multi-stroke characters by selectively
removing strokes while preserving their overall legibility. More specifically,
we use a highly accurate character recognition model to assess legibility and
remove those strokes that minimally impact it. Experimental results on 1,256
character classes with 5, 10, 15, and 20 strokes reveal several key findings,
including the observation that even after removing multiple strokes, many
characters remain distinguishable. These findings suggest the potential for
more formalized simplification strategies.

</details>


### [88] [Hierarchical Corpus-View-Category Refinement for Carotid Plaque Risk Grading in Ultrasound](https://arxiv.org/abs/2506.23108)
*Zhiyuan Zhu,Jian Wang,Yong Jiang,Tong Han,Yuhao Huang,Ang Zhang,Kaiwen Yang,Mingyuan Luo,Zhe Liu,Yaofei Duan,Dong Ni,Tianhong Tang,Xin Yang*

Main category: cs.CV

TL;DR: 提出了一种名为CVC-RF的新框架，通过多级细化（语料库、视图和类别）提升颈动脉斑块分级（CPG）的性能，解决了现有方法忽略表示学习和类特征差异的问题。


<details>
  <summary>Details</summary>
Motivation: 颈动脉斑块分级对心血管和脑血管疾病风险评估至关重要，但现有深度学习方法多关注特征融合，忽略了表示学习和类特征差异。

Method: 提出CVC-RF框架，包括语料库级的中心记忆对比损失、视图级的级联下采样注意力模块和类别级的无参数专家混合加权策略。

Result: CVC-RF在CPG任务中表现优异，达到最新技术水平。

Conclusion: CVC-RF通过多级细化有效建模全局特征，为CPG任务提供了高效解决方案。

Abstract: Accurate carotid plaque grading (CPG) is vital to assess the risk of
cardiovascular and cerebrovascular diseases. Due to the small size and high
intra-class variability of plaque, CPG is commonly evaluated using a
combination of transverse and longitudinal ultrasound views in clinical
practice. However, most existing deep learning-based multi-view classification
methods focus on feature fusion across different views, neglecting the
importance of representation learning and the difference in class features. To
address these issues, we propose a novel Corpus-View-Category Refinement
Framework (CVC-RF) that processes information from Corpus-, View-, and
Category-levels, enhancing model performance. Our contribution is four-fold.
First, to the best of our knowledge, we are the foremost deep learning-based
method for CPG according to the latest Carotid Plaque-RADS guidelines. Second,
we propose a novel center-memory contrastive loss, which enhances the network's
global modeling capability by comparing with representative cluster centers and
diverse negative samples at the Corpus level. Third, we design a cascaded
down-sampling attention module to fuse multi-scale information and achieve
implicit feature interaction at the View level. Finally, a parameter-free
mixture-of-experts weighting strategy is introduced to leverage class
clustering knowledge to weight different experts, enabling feature decoupling
at the Category level. Experimental results indicate that CVC-RF effectively
models global features via multi-level refinement, achieving state-of-the-art
performance in the challenging CPG task.

</details>


### [89] [MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings](https://arxiv.org/abs/2506.23115)
*Haonan Chen,Hong Liu,Yuping Luo,Liang Wang,Nan Yang,Furu Wei,Zhicheng Dou*

Main category: cs.CV

TL;DR: MoCa是一个两阶段框架，通过改进预训练的因果视觉语言模型（VLMs）来解决当前多模态嵌入模型的三个关键限制，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前多模态嵌入模型存在三个主要问题：因果注意力不适用于嵌入任务、依赖高质量标注数据导致扩展性差、训练目标和数据多样性不足。

Method: MoCa分为两阶段：第一阶段通过联合重建目标增强双向上下文感知推理；第二阶段利用多样化多模态数据进行对比微调。

Result: MoCa在MMEB和ViDoRe-v2基准测试中表现优异，达到新的最优结果，并在模型规模和数据扩展性上表现强劲。

Conclusion: MoCa通过改进预训练和微调策略，有效解决了当前多模态嵌入模型的限制，提升了性能和扩展性。

Abstract: Multimodal embedding models, built upon causal Vision Language Models (VLMs),
have shown promise in various tasks. However, current approaches face three key
limitations: the use of causal attention in VLM backbones is suboptimal for
embedding tasks; scalability issues due to reliance on high-quality labeled
paired data for contrastive learning; and limited diversity in training
objectives and data. To address these issues, we propose MoCa, a two-stage
framework for transforming pre-trained VLMs into effective bidirectional
multimodal embedding models. The first stage, Modality-aware Continual
Pre-training, introduces a joint reconstruction objective that simultaneously
denoises interleaved text and image inputs, enhancing bidirectional
context-aware reasoning. The second stage, Heterogeneous Contrastive
Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple
image-caption pairs to enhance generalization and alignment. Our method
addresses the stated limitations by introducing bidirectional attention through
continual pre-training, scaling effectively with massive unlabeled datasets via
joint reconstruction objectives, and utilizing diverse multimodal data for
enhanced representation robustness. Experiments demonstrate that MoCa
consistently improves performance across MMEB and ViDoRe-v2 benchmarks,
achieving new state-of-the-art results, and exhibits strong scalability with
both model size and training data on MMEB.

</details>


### [90] [Enhancing Spatial Reasoning in Multimodal Large Language Models through Reasoning-based Segmentation](https://arxiv.org/abs/2506.23120)
*Zhenhua Ning,Zhuotao Tian,Shaoshuai Shi,Guangming Lu,Daojing He,Wenjie Pei,Li Jiang*

Main category: cs.CV

TL;DR: 论文提出了一种基于推理的分割框架R²S和数据集3D ReasonSeg，以增强3D点云感知的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理需要精确空间推理的复杂指令时存在挑战，尽管3D点云数据提供了详细的空间线索。

Method: R²S框架模拟人类认知过程，将空间推理分解为两个阶段：先识别相关元素，再根据视觉先验处理指令。

Result: 实验表明，R²S和3D ReasonSeg显著提升了3D点云感知的空间推理能力。

Conclusion: R²S和3D ReasonSeg可作为未来研究的新基准。

Abstract: Recent advances in point cloud perception have demonstrated remarkable
progress in scene understanding through vision-language alignment leveraging
large language models (LLMs). However, existing methods may still encounter
challenges in handling complex instructions that require accurate spatial
reasoning, even if the 3D point cloud data provides detailed spatial cues such
as size and position for identifying the targets. To tackle this issue, we
propose Relevant Reasoning Segmentation (R$^2$S), a reasoning-based
segmentation framework. The framework emulates human cognitive processes by
decomposing spatial reasoning into two sequential stages: first identifying
relevant elements, then processing instructions guided by their associated
visual priors. Furthermore, acknowledging the inadequacy of existing datasets
in complex reasoning tasks, we introduce 3D ReasonSeg, a reasoning-based
segmentation dataset comprising 25,185 training samples and 3,966 validation
samples with precise annotations. Both quantitative and qualitative experiments
demonstrate that the R$^2$S and 3D ReasonSeg effectively endow 3D point cloud
perception with stronger spatial reasoning capabilities, and we hope that they
can serve as a new baseline and benchmark for future work.

</details>


### [91] [Dare to Plagiarize? Plagiarized Painting Recognition and Retrieval](https://arxiv.org/abs/2506.23132)
*Sophie Zhou,Shu Kong*

Main category: cs.CV

TL;DR: 论文提出了一种基于DINOv2模型的绘画抄袭检测方法，通过检索视觉相似的原创作品来识别和解释抄袭行为。基线方法准确率高但检索精度低，微调后检索性能提升但准确率下降。


<details>
  <summary>Details</summary>
Motivation: 保护艺术家版权和知识产权，解决艺术抄袭检测中的挑战。

Method: 构建数据集，使用DINOv2模型提取特征并检索相似图像，通过微调提升检索性能。

Result: 基线方法准确率97.2%，检索精度29.0%；微调后检索性能提升12%，但准确率降至92.7%。

Conclusion: 研究为艺术抄袭检测提供了有效方法，但需进一步平衡检索性能和准确率。

Abstract: Art plagiarism detection plays a crucial role in protecting artists'
copyrights and intellectual property, yet it remains a challenging problem in
forensic analysis. In this paper, we address the task of recognizing
plagiarized paintings and explaining the detected plagarisms by retrieving
visually similar authentic artworks. To support this study, we construct a
dataset by collecting painting photos and synthesizing plagiarized versions
using generative AI, tailored to specific artists' styles. We first establish a
baseline approach using off-the-shelf features from the visual foundation model
DINOv2 to retrieve the most similar images in the database and classify
plagiarism based on a similarity threshold. Surprisingly, this non-learned
method achieves a high recognition accuracy of 97.2\% but suffers from low
retrieval precision 29.0\% average precision (AP). To improve retrieval
quality, we finetune DINOv2 with a metric learning loss using positive and
negative sample pairs sampled in the database. The finetuned model greatly
improves retrieval performance by 12\% AP over the baseline, though it
unexpectedly results in a lower recognition accuracy (92.7\%). We conclude with
insightful discussions and outline directions for future research.

</details>


### [92] [RoboScape: Physics-informed Embodied World Model](https://arxiv.org/abs/2506.23135)
*Yu Shang,Xin Zhang,Yinzhou Tang,Lei Jin,Chen Gao,Wei Wu,Yong Li*

Main category: cs.CV

TL;DR: RoboScape是一个统一的物理感知世界模型，通过联合学习RGB视频生成和物理知识，解决了当前模型在3D几何和运动动力学建模上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前的世界模型在物理感知方面表现不足，尤其是在3D几何和运动动力学建模上，导致接触丰富的机器人场景中视频生成不真实。

Method: RoboScape通过两个关键物理感知联合训练任务（时间深度预测和关键点动态学习）来增强视频生成的3D几何一致性和复杂运动建模。

Result: 实验表明，RoboScape生成的视频在视觉保真度和物理合理性上表现优异，并在下游应用中验证了其实用性。

Conclusion: RoboScape为构建高效的物理感知世界模型提供了新思路，推动了具身智能研究的发展。

Abstract: World models have become indispensable tools for embodied intelligence,
serving as powerful simulators capable of generating realistic robotic videos
while addressing critical data scarcity challenges. However, current embodied
world models exhibit limited physical awareness, particularly in modeling 3D
geometry and motion dynamics, resulting in unrealistic video generation for
contact-rich robotic scenarios. In this paper, we present RoboScape, a unified
physics-informed world model that jointly learns RGB video generation and
physics knowledge within an integrated framework. We introduce two key
physics-informed joint training tasks: temporal depth prediction that enhances
3D geometric consistency in video rendering, and keypoint dynamics learning
that implicitly encodes physical properties (e.g., object shape and material
characteristics) while improving complex motion modeling. Extensive experiments
demonstrate that RoboScape generates videos with superior visual fidelity and
physical plausibility across diverse robotic scenarios. We further validate its
practical utility through downstream applications including robotic policy
training with generated data and policy evaluation. Our work provides new
insights for building efficient physics-informed world models to advance
embodied intelligence research. The code is available at:
https://github.com/tsinghua-fib-lab/RoboScape.

</details>


### [93] [VisualPrompter: Prompt Optimization with Visual Feedback for Text-to-Image Synthesis](https://arxiv.org/abs/2506.23138)
*Shiyu Wu,Mingzhen Sun,Weining Wang,Yequan Wang,Jing Liu*

Main category: cs.CV

TL;DR: VisualPrompter是一个无需训练的新提示工程框架，通过自动自反思模块和细粒度提示优化机制，提升生成图像与用户描述的语义对齐。


<details>
  <summary>Details</summary>
Motivation: 现有提示工程方法虽能提升图像风格和美学，但常忽略语义对齐，导致生成图像内容不符合用户描述。

Method: 提出VisualPrompter，包含自动自反思模块识别缺失概念，以及目标特定提示优化机制细粒度修订提示。

Result: 在多个文本-图像对齐评估基准上达到新最优性能，且框架设计为即插即用。

Conclusion: VisualPrompter有效解决了语义对齐问题，并具有高度适应性。

Abstract: Since there exists a notable gap between user-provided and model-preferred
prompts, generating high-quality and satisfactory images using diffusion models
often requires prompt engineering to optimize user inputs. Current studies on
text-to-image prompt engineering can effectively enhance the style and
aesthetics of generated images. However, they often neglect the semantic
alignment between generated images and user descriptions, resulting in visually
appealing but content-wise unsatisfying outputs. In this work, we propose
VisualPrompter, a novel training-free prompt engineering framework that refines
user inputs to model-preferred sentences. In particular, VisualPrompter
utilizes an automatic self-reflection module to identify the missing concepts
in generated images and a target-specific prompt optimization mechanism to
revise the prompts in a fine-grained manner. Extensive experiments demonstrate
the effectiveness of our VisualPrompter, which achieves new state-of-the-art
performance on multiple benchmarks for text-image alignment evaluation.
Additionally, our framework features a plug-and-play design, making it highly
adaptable to various generative models.

</details>


### [94] [AlignCVC: Aligning Cross-View Consistency for Single-Image-to-3D Generation](https://arxiv.org/abs/2506.23150)
*Xinyue Liang,Zhiyuan Ma,Lingchen Sun,Yanjun Guo,Lei Zhang*

Main category: cs.CV

TL;DR: AlignCVC通过分布对齐而非严格回归损失改进单图像到3D生成中的跨视图一致性（CVC），显著提升生成质量和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过预训练生成模型合成的中间多视图图像缺乏跨视图一致性（CVC），导致3D重建性能下降。

Method: 提出AlignCVC框架，通过软硬对齐策略分别优化生成和重建模型的多视图分布，使其对齐真实多视图分布。

Result: 实验表明AlignCVC显著提升生成质量和推理速度（仅需4步）。

Conclusion: AlignCVC作为一种即插即用方法，有效整合多视图生成与3D重建模型，提升单图像到3D生成的性能。

Abstract: Single-image-to-3D models typically follow a sequential generation and
reconstruction workflow. However, intermediate multi-view images synthesized by
pre-trained generation models often lack cross-view consistency (CVC),
significantly degrading 3D reconstruction performance. While recent methods
attempt to refine CVC by feeding reconstruction results back into the
multi-view generator, these approaches struggle with noisy and unstable
reconstruction outputs that limit effective CVC improvement. We introduce
AlignCVC, a novel framework that fundamentally re-frames single-image-to-3D
generation through distribution alignment rather than relying on strict
regression losses. Our key insight is to align both generated and reconstructed
multi-view distributions toward the ground-truth multi-view distribution,
establishing a principled foundation for improved CVC. Observing that generated
images exhibit weak CVC while reconstructed images display strong CVC due to
explicit rendering, we propose a soft-hard alignment strategy with distinct
objectives for generation and reconstruction models. This approach not only
enhances generation quality but also dramatically accelerates inference to as
few as 4 steps. As a plug-and-play paradigm, our method, namely AlignCVC,
seamlessly integrates various multi-view generation models with 3D
reconstruction models. Extensive experiments demonstrate the effectiveness and
efficiency of AlignCVC for single-image-to-3D generation.

</details>


### [95] [MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation](https://arxiv.org/abs/2506.23151)
*Vladislav Bargatin,Egor Chistov,Alexander Yakovenko,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: MEMFOF是一种内存高效的多帧光流估计方法，显著降低了GPU内存消耗，同时在高分辨率输入下保持高精度。


<details>
  <summary>Details</summary>
Motivation: 当前光流估计方法在追求高精度的同时，GPU内存消耗急剧增加，限制了高分辨率（如1080p）输入的处理能力。

Method: 通过重新设计RAFT-like架构，减少相关体积并采用高分辨率训练协议，结合多帧估计，实现内存高效的光流估计。

Result: MEMFOF在多个基准测试中表现优异，内存消耗显著降低（运行时2.09GB，训练时28.5GB），并在Spring、Sintel和KITTI-2015等基准测试中排名第一。

Conclusion: MEMFOF在高分辨率光流估计中实现了内存效率与精度的平衡，为实际应用提供了更高效的解决方案。

Abstract: Recent advances in optical flow estimation have prioritized accuracy at the
cost of growing GPU memory consumption, particularly for high-resolution
(FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical
flow method that identifies a favorable trade-off between multi-frame
estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU
memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely
positions our method to be trained at native 1080p without the need for
cropping or downsampling. We systematically revisit design choices from
RAFT-like architectures, integrating reduced correlation volumes and
high-resolution training protocols alongside multi-frame estimation, to achieve
state-of-the-art performance across multiple benchmarks while substantially
reducing memory overhead. Our method outperforms more resource-intensive
alternatives in both accuracy and runtime efficiency, validating its robustness
for flow estimation at high resolutions. At the time of submission, our method
ranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289,
leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the
best Fl-all error on KITTI-2015 at 2.94%. The code is available at
https://github.com/msu-video-group/memfof.

</details>


### [96] [Dynamic View Synthesis from Small Camera Motion Videos](https://arxiv.org/abs/2506.23153)
*Huiqiang Sun,Xingyi Li,Juewen Peng,Liao Shen,Zhiguo Cao,Ke Xian,Guosheng Lin*

Main category: cs.CV

TL;DR: 论文提出了一种新的分布深度正则化（DDR）方法，解决了动态3D场景中新视角合成在小相机运动输入下的几何表示和相机参数估计问题。


<details>
  <summary>Details</summary>
Motivation: 动态3D场景的新视角合成在小相机运动输入下存在几何表示不准确和相机参数估计错误的问题，现有方法难以应对。

Method: 提出DDR方法，通过Gumbel-softmax采样点计算误差期望，并引入约束确保场景几何正确；同时结合相机参数学习提升模型鲁棒性。

Result: 实验表明，该方法在小相机运动输入下表现优于现有方法。

Conclusion: DDR方法有效解决了小相机运动输入下的新视角合成问题，提升了场景几何表示和相机参数估计的准确性。

Abstract: Novel view synthesis for dynamic $3$D scenes poses a significant challenge.
Many notable efforts use NeRF-based approaches to address this task and yield
impressive results. However, these methods rely heavily on sufficient motion
parallax in the input images or videos. When the camera motion range becomes
limited or even stationary (i.e., small camera motion), existing methods
encounter two primary challenges: incorrect representation of scene geometry
and inaccurate estimation of camera parameters. These challenges make prior
methods struggle to produce satisfactory results or even become invalid. To
address the first challenge, we propose a novel Distribution-based Depth
Regularization (DDR) that ensures the rendering weight distribution to align
with the true distribution. Specifically, unlike previous methods that use
depth loss to calculate the error of the expectation, we calculate the
expectation of the error by using Gumbel-softmax to differentiably sample
points from discrete rendering weight distribution. Additionally, we introduce
constraints that enforce the volume density of spatial points before the object
boundary along the ray to be near zero, ensuring that our model learns the
correct geometry of the scene. To demystify the DDR, we further propose a
visualization tool that enables observing the scene geometry representation at
the rendering weight level. For the second challenge, we incorporate camera
parameter learning during training to enhance the robustness of our model to
camera parameters. We conduct extensive experiments to demonstrate the
effectiveness of our approach in representing scenes with small camera motion
input, and our results compare favorably to state-of-the-art methods.

</details>


### [97] [Self-Supervised Contrastive Learning for Multi-Label Images](https://arxiv.org/abs/2506.23156)
*Jiale Chen*

Main category: cs.CV

TL;DR: 该论文提出了一种针对多标签图像的自监督学习方法，通过块级增强和图像感知对比损失，减少了预训练开销并提升了语义一致性。


<details>
  <summary>Details</summary>
Motivation: 主流自监督学习方法依赖单标签高体数据集（如ImageNet），预训练开销大且忽略了多标签图像的潜力。本文旨在利用多标签图像实现高效表示学习。

Method: 提出块级增强模块以提取更多潜在正视图对，并设计图像感知对比损失以建立视图间的联系，从而学习语义一致的表示。

Result: 通过线性微调和迁移学习验证了方法的竞争力，即使在样本质量和数量受限的情况下。

Conclusion: 该方法在多标签图像上实现了高效的自监督表示学习，为下游任务提供了更广泛的适用性。

Abstract: Self-supervised learning (SSL) has demonstrated its effectiveness in learning
representations through comparison methods that align with human intuition.
However, mainstream SSL methods heavily rely on high body datasets with single
label, such as ImageNet, resulting in intolerable pre-training overhead.
Besides, more general multi-label images are frequently overlooked in SSL,
despite their potential for richer semantic information and broader
applicability in downstream scenarios. Therefore, we tailor the mainstream SSL
approach to guarantee excellent representation learning capabilities using
fewer multi-label images. Firstly, we propose a block-wise augmentation module
aimed at extracting additional potential positive view pairs from multi-label
images. Subsequently, an image-aware contrastive loss is devised to establish
connections between these views, thereby facilitating the extraction of
semantically consistent representations. Comprehensive linear fine-tuning and
transfer learning validate the competitiveness of our approach despite
challenging sample quality and quantity.

</details>


### [98] [STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene](https://arxiv.org/abs/2506.23157)
*Hanyu Zhou,Haonan Wang,Haoyue Liu,Yuxing Duan,Luxin Yan,Gim Hee Lee*

Main category: cs.CV

TL;DR: 提出了一种时空解耦的高斯泼溅框架，用于高动态场景重建，通过事件相机补偿帧相机，解决背景与物体时空特征不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用统一表示模型（如高斯）直接匹配动态场景的时空特征，但无法处理帧成像导致的物体时空特征不连续及背景与物体空间特征的异质性。

Method: 引入事件相机补偿帧相机，提出时空解耦的高斯泼溅框架，通过聚类区分背景与物体的时空特征，并利用高斯表示与事件数据的一致性指导物体高斯解耦。

Result: 实验验证了所提方法的优越性，能够提升背景与物体的时空区分能力，渲染时间连续的动态场景。

Conclusion: 时空解耦的高斯泼溅框架有效解决了高动态场景重建中的时空特征不匹配问题。

Abstract: High-dynamic scene reconstruction aims to represent static background with
rigid spatial features and dynamic objects with deformed continuous
spatiotemporal features. Typically, existing methods adopt unified
representation model (e.g., Gaussian) to directly match the spatiotemporal
features of dynamic scene from frame camera. However, this unified paradigm
fails in the potential discontinuous temporal features of objects due to frame
imaging and the heterogeneous spatial features between background and objects.
To address this issue, we disentangle the spatiotemporal features into various
latent representations to alleviate the spatiotemporal mismatching between
background and objects. In this work, we introduce event camera to compensate
for frame camera, and propose a spatiotemporal-disentangled Gaussian splatting
framework for high-dynamic scene reconstruction. As for dynamic scene, we
figure out that background and objects have appearance discrepancy in
frame-based spatial features and motion discrepancy in event-based temporal
features, which motivates us to distinguish the spatiotemporal features between
background and objects via clustering. As for dynamic object, we discover that
Gaussian representations and event data share the consistent spatiotemporal
characteristic, which could serve as a prior to guide the spatiotemporal
disentanglement of object Gaussians. Within Gaussian splatting framework, the
cumulative scene-object disentanglement can improve the spatiotemporal
discrimination between background and objects to render the time-continuous
dynamic scene. Extensive experiments have been performed to verify the
superiority of the proposed method.

</details>


### [99] [Trident: Detecting Face Forgeries with Adversarial Triplet Learning](https://arxiv.org/abs/2506.23189)
*Mustafa Hakan Kara,Aysegul Dundar,Uğur Güdükbay*

Main category: cs.CV

TL;DR: 提出了一种名为Trident的人脸伪造检测框架，通过三元组学习和Siamese网络架构提升对不同伪造方法的适应性，并结合域对抗训练增强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络生成的人脸伪造技术日益复杂，检测数字媒体中的人脸伪造变得更具挑战性，维护数字媒体完整性和打击视觉虚假信息的重要性凸显。

Method: 采用三元组学习和Siamese网络架构，结合域对抗训练，避免过拟合特定伪造方法的特征。

Result: 在多个基准测试和消融研究中验证了框架的有效性。

Conclusion: Trident框架通过捕获细微伪造特征和增强泛化能力，显著提升了人脸伪造检测的性能。

Abstract: As face forgeries generated by deep neural networks become increasingly
sophisticated, detecting face manipulations in digital media has posed a
significant challenge, underscoring the importance of maintaining digital media
integrity and combating visual disinformation. Current detection models,
predominantly based on supervised training with domain-specific data, often
falter against forgeries generated by unencountered techniques. In response to
this challenge, we introduce \textit{Trident}, a face forgery detection
framework that employs triplet learning with a Siamese network architecture for
enhanced adaptability across diverse forgery methods. \textit{Trident} is
trained on curated triplets to isolate nuanced differences of forgeries,
capturing fine-grained features that distinguish pristine samples from
manipulated ones while controlling for other variables. To further enhance
generalizability, we incorporate domain-adversarial training with a forgery
discriminator. This adversarial component guides our embedding model towards
forgery-agnostic representations, improving its robustness to unseen
manipulations. In addition, we prevent gradient flow from the classifier head
to the embedding model, avoiding overfitting induced by artifacts peculiar to
certain forgeries. Comprehensive evaluations across multiple benchmarks and
ablation studies demonstrate the effectiveness of our framework. We will
release our code in a GitHub repository.

</details>


### [100] [DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding](https://arxiv.org/abs/2506.23196)
*Mona Ahmadian,Amir Shirian,Frank Guerin,Andrew Gilbert*

Main category: cs.CV

TL;DR: DEL框架通过多模态交互建模，在长视频中实现精细动作定位，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界视频中事件重叠和时间依赖复杂，需要更精确的多模态交互建模方法。

Method: DEL包含音频与视觉特征对齐模块和多模态交互细化模块，利用掩码自注意力增强一致性和跨模态依赖建模。

Result: 在多个TAL数据集上表现优异，平均mAP提升显著（最高+3.3%）。

Conclusion: DEL框架在多模态动作定位任务中具有显著优势，为复杂视频分析提供了有效解决方案。

Abstract: Real-world videos often contain overlapping events and complex temporal
dependencies, making multimodal interaction modeling particularly challenging.
We introduce DEL, a framework for dense semantic action localization, aiming to
accurately detect and classify multiple actions at fine-grained temporal
resolutions in long untrimmed videos. DEL consists of two key modules: the
alignment of audio and visual features that leverage masked self-attention to
enhance intra-mode consistency and a multimodal interaction refinement module
that models cross-modal dependencies across multiple scales, enabling
high-level semantics and fine-grained details. Our method achieves
state-of-the-art performance on multiple real-world Temporal Action
Localization (TAL) datasets, UnAV-100, THUMOS14, ActivityNet 1.3, and
EPIC-Kitchens-100, surpassing previous approaches with notable average mAP
gains of +3.3%, +2.6%, +1.2%, +1.7% (verb), and +1.4% (noun), respectively.

</details>


### [101] [Transformer-Based Person Search with High-Frequency Augmentation and Multi-Wave Mixing](https://arxiv.org/abs/2506.23202)
*Qilin Shu,Qixian Zhang,Qi Zhang,Hongyun Zhang,Duoqian Miao,Cairong Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种名为HAMW的新方法，通过高频增强和多波混合技术改进基于Transformer的人物搜索模型，解决了自注意力机制抑制高频特征和计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的人物搜索模型存在自注意力机制抑制高频特征和计算成本高的问题，影响了性能。

Method: 提出HAMW方法，采用三阶段框架优化检测和重识别性能，通过高频增强输入和多级Haar小波融合替代自注意力层，降低计算复杂度。

Result: 在CUHK-SYSU和PRW数据集上，HAMW实现了最先进的性能。

Conclusion: HAMW有效提升了人物搜索模型的性能，同时降低了计算成本。

Abstract: The person search task aims to locate a target person within a set of scene
images. In recent years, transformer-based models in this field have made some
progress. However, they still face three primary challenges: 1) the
self-attention mechanism tends to suppress high-frequency components in the
features, which severely impacts model performance; 2) the computational cost
of transformers is relatively high. To address these issues, we propose a novel
High-frequency Augmentation and Multi-Wave mixing (HAMW) method for person
search. HAMW is designed to enhance the discriminative feature extraction
capabilities of transformers while reducing computational overhead and
improving efficiency. Specifically, we develop a three-stage framework that
progressively optimizes both detection and re-identification performance. Our
model enhances the perception of high-frequency features by learning from
augmented inputs containing additional high-frequency components. Furthermore,
we replace the self-attention layers in the transformer with a strategy based
on multi-level Haar wavelet fusion to capture multi-scale features. This not
only lowers the computational complexity but also alleviates the suppression of
high-frequency features and enhances the ability to exploit multi-scale
information. Extensive experiments demonstrate that HAMW achieves
state-of-the-art performance on both the CUHK-SYSU and PRW datasets.

</details>


### [102] [BridgeShape: Latent Diffusion Schrödinger Bridge for 3D Shape Completion](https://arxiv.org/abs/2506.23205)
*Dequan Kong,Zhe Zhu,Honghua Chen,Mingqiang Wei*

Main category: cs.CV

TL;DR: BridgeShape提出了一种基于潜在扩散Schrödinger桥的3D形状补全框架，通过最优传输路径和深度增强的VQ-VAE编码，解决了现有方法在全局一致性和分辨率限制上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的3D形状补全方法未能显式建模最优全局传输路径，且受限于体素空间的分辨率约束，导致补全效果不佳。

Method: BridgeShape将形状补全建模为最优传输问题，并引入深度增强的VQ-VAE编码3D形状到紧凑潜在空间，结合多视角深度信息和DINOv2特征增强几何感知。

Result: BridgeShape在大规模3D形状补全基准测试中达到最先进性能，支持更高分辨率和未见物体类别的补全。

Conclusion: BridgeShape通过潜在扩散和最优传输路径的显式建模，显著提升了3D形状补全的全局一致性和细节生成能力。

Abstract: Existing diffusion-based 3D shape completion methods typically use a
conditional paradigm, injecting incomplete shape information into the denoising
network via deep feature interactions (e.g., concatenation, cross-attention) to
guide sampling toward complete shapes, often represented by voxel-based
distance functions. However, these approaches fail to explicitly model the
optimal global transport path, leading to suboptimal completions. Moreover,
performing diffusion directly in voxel space imposes resolution constraints,
limiting the generation of fine-grained geometric details. To address these
challenges, we propose BridgeShape, a novel framework for 3D shape completion
via latent diffusion Schr\"odinger bridge. The key innovations lie in two
aspects: (i) BridgeShape formulates shape completion as an optimal transport
problem, explicitly modeling the transition between incomplete and complete
shapes to ensure a globally coherent transformation. (ii) We introduce a
Depth-Enhanced Vector Quantized Variational Autoencoder (VQ-VAE) to encode 3D
shapes into a compact latent space, leveraging self-projected multi-view depth
information enriched with strong DINOv2 features to enhance geometric
structural perception. By operating in a compact yet structurally informative
latent space, BridgeShape effectively mitigates resolution constraints and
enables more efficient and high-fidelity 3D shape completion. BridgeShape
achieves state-of-the-art performance on large-scale 3D shape completion
benchmarks, demonstrating superior fidelity at higher resolutions and for
unseen object classes.

</details>


### [103] [TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints](https://arxiv.org/abs/2506.23207)
*Zhen Tan,Xieyuanli Chen,Lei Feng,Yangbing Ge,Shuaifeng Zhi,Jiaxiong Liu,Dewen Hu*

Main category: cs.CV

TL;DR: TVG-SLAM是一种基于3D高斯泼溅的RGB-only SLAM系统，通过三视图几何提升跟踪和建图的鲁棒性，在户外复杂环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-only SLAM系统依赖光度渲染损失，在户外无边界环境中因视角和光照变化导致跟踪不稳定。

Method: 提出三视图匹配模块和混合几何约束，结合光度损失提升跟踪；采用概率初始化策略和动态衰减渲染信任机制优化建图。

Result: 在多个户外数据集上表现优于现有方法，最大挑战数据集上平均轨迹误差降低69.0%，渲染质量达到最优。

Conclusion: TVG-SLAM通过几何约束和动态机制显著提升了RGB-only SLAM的鲁棒性和渲染质量，代码将开源。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM
systems to achieve high-fidelity scene representation. However, the heavy
reliance of existing systems on photometric rendering loss for camera tracking
undermines their robustness, especially in unbounded outdoor environments with
severe viewpoint and illumination changes. To address these challenges, we
propose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel
tri-view geometry paradigm to ensure consistent tracking and high-quality
mapping. We introduce a dense tri-view matching module that aggregates reliable
pairwise correspondences into consistent tri-view matches, forming robust
geometric constraints across frames. For tracking, we propose Hybrid Geometric
Constraints, which leverage tri-view matches to construct complementary
geometric cues alongside photometric loss, ensuring accurate and stable pose
estimation even under drastic viewpoint shifts and lighting variations. For
mapping, we propose a new probabilistic initialization strategy that encodes
geometric uncertainty from tri-view correspondences into newly initialized
Gaussians. Additionally, we design a Dynamic Attenuation of Rendering Trust
mechanism to mitigate tracking drift caused by mapping latency. Experiments on
multiple public outdoor datasets show that our TVG-SLAM outperforms prior
RGB-only 3DGS-based SLAM systems. Notably, in the most challenging dataset, our
method improves tracking robustness, reducing the average Absolute Trajectory
Error (ATE) by 69.0\% while achieving state-of-the-art rendering quality. The
implementation of our method will be released as open-source.

</details>


### [104] [A Hierarchical Slice Attention Network for Appendicitis Classification in 3D CT Scans](https://arxiv.org/abs/2506.23209)
*Chia-Wen Huang,Haw Hwai,Chien-Chang Lee,Pei-Yuan Wu*

Main category: cs.CV

TL;DR: 提出了一种基于3D CT扫描和切片注意力机制的深度学习模型，用于提高阑尾炎分类的准确性，并通过分层分类框架区分简单和复杂阑尾炎。


<details>
  <summary>Details</summary>
Motivation: CT影像虽为阑尾炎诊断标准工具，但病例增多可能导致放射科医生超负荷，引发诊断延迟。

Method: 结合3D CT扫描和切片注意力机制，利用外部2D数据集增强小病变检测；引入分层分类框架，使用预训练2D模型区分简单与复杂阑尾炎。

Result: 阑尾炎分类的AUC提升3%，复杂阑尾炎分类的AUC提升5.9%。

Conclusion: 该方法比现有工作更高效可靠，为阑尾炎诊断提供了更好的解决方案。

Abstract: Timely and accurate diagnosis of appendicitis is critical in clinical
settings to prevent serious complications. While CT imaging remains the
standard diagnostic tool, the growing number of cases can overwhelm
radiologists, potentially causing delays. In this paper, we propose a deep
learning model that leverages 3D CT scans for appendicitis classification,
incorporating Slice Attention mechanisms guided by external 2D datasets to
enhance small lesion detection. Additionally, we introduce a hierarchical
classification framework using pre-trained 2D models to differentiate between
simple and complicated appendicitis. Our approach improves AUC by 3% for
appendicitis and 5.9% for complicated appendicitis, offering a more efficient
and reliable diagnostic solution compared to previous work.

</details>


### [105] [UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding](https://arxiv.org/abs/2506.23219)
*Jie Feng,Shengyuan Wang,Tianhui Liu,Yanxin Xi,Yong Li*

Main category: cs.CV

TL;DR: 论文提出了一种多模态大语言模型UrbanLLaVA，用于处理城市研究中的多模态数据，并在多种任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前城市研究方法局限于特定数据类型，缺乏统一框架，多模态大语言模型（MLLMs）为解决这一问题提供了机会。

Method: 构建多样化的城市指令数据集，提出多阶段训练框架，分离空间推理增强与领域知识学习，并扩展城市研究基准。

Result: 实验表明UrbanLLaVA在单模态和跨模态任务中优于开源和专有MLLMs，并展现出跨城市的鲁棒泛化能力。

Conclusion: UrbanLLaVA为城市研究提供了一个高效的多模态处理框架，推动了该领域的发展。

Abstract: Urban research involves a wide range of scenarios and tasks that require the
understanding of multi-modal data. Current methods often focus on specific data
types and lack a unified framework in urban field for processing them
comprehensively. The recent success of multi-modal large language models
(MLLMs) presents a promising opportunity to overcome this limitation. In this
paper, we introduce $\textit{UrbanLLaVA}$, a multi-modal large language model
designed to process these four types of data simultaneously and achieve strong
performance across diverse urban tasks compared with general MLLMs. In
$\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset
encompassing both single-modal and cross-modal urban data, spanning from
location view to global view of urban environment. Additionally, we propose a
multi-stage training framework that decouples spatial reasoning enhancement
from domain knowledge learning, thereby improving the compatibility and
downstream performance of $\textit{UrbanLLaVA}$ across diverse urban tasks.
Finally, we also extend existing benchmark for urban research to assess the
performance of MLLMs across a wide range of urban tasks. Experimental results
from three cities demonstrate that $\textit{UrbanLLaVA}$ outperforms
open-source and proprietary MLLMs in both single-modal tasks and complex
cross-modal tasks and shows robust generalization abilities across cities.
Source codes and data are openly accessible to the research community via
https://github.com/tsinghua-fib-lab/UrbanLLaVA.

</details>


### [106] [High-quality Pseudo-labeling for Point Cloud Segmentation with Scene-level Annotation](https://arxiv.org/abs/2506.23227)
*Lunhao Duan,Shanshan Zhao,Xingxing Weng,Jing Zhang,Gui-Song Xia*

Main category: cs.CV

TL;DR: 本文提出了一种基于场景级标注的室内点云语义分割方法，通过多模态信息和区域-点语义一致性生成高质量伪标签，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在缺乏精确点级标注时，依赖伪标签训练分割模型，但伪标签的准确性不足影响了性能。本文旨在解决这一问题。

Method: 提出跨模态特征引导模块和区域-点语义一致性模块，利用2D-3D对应关系和区域投票策略生成高质量伪标签。

Result: 在ScanNet v2和S3DIS数据集上显著优于现有方法，消融实验验证了各模块的有效性。

Conclusion: 通过多模态信息和语义一致性，本文方法能够生成高质量伪标签，提升场景级标注下的点云语义分割性能。

Abstract: This paper investigates indoor point cloud semantic segmentation under
scene-level annotation, which is less explored compared to methods relying on
sparse point-level labels. In the absence of precise point-level labels,
current methods first generate point-level pseudo-labels, which are then used
to train segmentation models. However, generating accurate pseudo-labels for
each point solely based on scene-level annotations poses a considerable
challenge, substantially affecting segmentation performance. Consequently, to
enhance accuracy, this paper proposes a high-quality pseudo-label generation
framework by exploring contemporary multi-modal information and region-point
semantic consistency. Specifically, with a cross-modal feature guidance module,
our method utilizes 2D-3D correspondences to align point cloud features with
corresponding 2D image pixels, thereby assisting point cloud feature learning.
To further alleviate the challenge presented by the scene-level annotation, we
introduce a region-point semantic consistency module. It produces regional
semantics through a region-voting strategy derived from point-level semantics,
which are subsequently employed to guide the point-level semantic predictions.
Leveraging the aforementioned modules, our method can rectify inaccurate
point-level semantic predictions during training and obtain high-quality
pseudo-labels. Significant improvements over previous works on ScanNet v2 and
S3DIS datasets under scene-level annotation can demonstrate the effectiveness.
Additionally, comprehensive ablation studies validate the contributions of our
approach's individual components. The code is available at
https://github.com/LHDuan/WSegPC .

</details>


### [107] [VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions](https://arxiv.org/abs/2506.23236)
*Marko Mihajlovic,Siwei Zhang,Gen Li,Kaifeng Zhao,Lea Müller,Siyu Tang*

Main category: cs.CV

TL;DR: VolumetricSMPL是一种基于神经体积的人体模型，通过动态混合学习权重矩阵（NBW）提升计算效率和表达能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于表面网格的人体模型在处理几何实体交互时效率低下，现有神经体积模型在复杂人体动作或计算成本上存在不足。

Method: 提出VolumetricSMPL，利用NBW生成紧凑高效的MLP解码器，动态混合少量学习权重矩阵。

Result: 在推理速度、GPU内存使用、精度和SDF建模上显著优于COAP模型，并在多项任务中验证其优势。

Conclusion: VolumetricSMPL在广泛应用中表现出卓越性能和效率，为人体建模提供新方向。

Abstract: Parametric human body models play a crucial role in computer graphics and
vision, enabling applications ranging from human motion analysis to
understanding human-environment interactions. Traditionally, these models use
surface meshes, which pose challenges in efficiently handling interactions with
other geometric entities, such as objects and scenes, typically represented as
meshes or point clouds. To address this limitation, recent research has
explored volumetric neural implicit body models. However, existing works are
either insufficiently robust for complex human articulations or impose high
computational and memory costs, limiting their widespread use. To this end, we
introduce VolumetricSMPL, a neural volumetric body model that leverages Neural
Blend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike
prior approaches that rely on large MLPs, NBW dynamically blends a small set of
learned weight matrices using predicted shape- and pose-dependent coefficients,
significantly improving computational efficiency while preserving
expressiveness. VolumetricSMPL outperforms prior volumetric occupancy model
COAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy,
and a Signed Distance Function (SDF) for efficient and differentiable contact
modeling. We demonstrate VolumetricSMPL's strengths across four challenging
tasks: (1) reconstructing human-object interactions from in-the-wild images,
(2) recovering human meshes in 3D scenes from egocentric views, (3)
scene-constrained motion synthesis, and (4) resolving self-intersections. Our
results highlight its broad applicability and significant performance and
efficiency gains.

</details>


### [108] [Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification](https://arxiv.org/abs/2506.23247)
*James Hinns,David Martens*

Main category: cs.CV

TL;DR: 提出Segment Attribution Tables (SATs)，将局部显著性解释汇总为半全局见解，填补全局与局部解释之间的空白。


<details>
  <summary>Details</summary>
Motivation: 深度学习在图像分类中表现优异，但模型预测的解释仍具挑战性。现有方法要么过于局部（如显著性图），要么过于全局化，难以捕捉重要局部行为。

Method: SATs利用图像片段（如“眼睛”）和显著性图，量化其对模型预测的影响，揭示模型依赖的概念和虚假相关性。

Result: SATs能解释任何可生成显著性图的分类器，提供介于全局和局部之间的实用工具，帮助分析和调试模型。

Conclusion: SATs为图像分类器提供了一种平衡的解释方法，既不过于简化，也不过于详细，具有实际应用价值。

Abstract: Deep learning dominates image classification tasks, yet understanding how
models arrive at predictions remains a challenge. Much research focuses on
local explanations of individual predictions, such as saliency maps, which
visualise the influence of specific pixels on a model's prediction. However,
reviewing many of these explanations to identify recurring patterns is
infeasible, while global methods often oversimplify and miss important local
behaviours. To address this, we propose Segment Attribution Tables (SATs), a
method for summarising local saliency explanations into (semi-)global insights.
SATs take image segments (such as "eyes" in Chihuahuas) and leverage saliency
maps to quantify their influence. These segments highlight concepts the model
relies on across instances and reveal spurious correlations, such as reliance
on backgrounds or watermarks, even when out-of-distribution test performance
sees little change. SATs can explain any classifier for which a form of
saliency map can be produced, using segmentation maps that provide named
segments. SATs bridge the gap between oversimplified global summaries and
overly detailed local explanations, offering a practical tool for analysing and
debugging image classifiers.

</details>


### [109] [DGE-YOLO: Dual-Branch Gathering and Attention for Accurate UAV Object Detection](https://arxiv.org/abs/2506.23252)
*Kunwei Lv,Ping Lan*

Main category: cs.CV

TL;DR: DGE-YOLO是一种改进的YOLO框架，用于多模态无人机目标检测，通过双分支架构和EMA机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 无人机目标检测在复杂条件下对小物体的检测性能不足，现有方法在处理多模态输入时表现不佳。

Method: 提出双分支架构处理红外和可见光图像，引入EMA机制增强多尺度特征学习，并使用Gather-and-Distribute模块替代传统颈部结构。

Result: 在Drone Vehicle数据集上，DGE-YOLO优于现有方法。

Conclusion: DGE-YOLO在多模态无人机目标检测任务中表现优异。

Abstract: The rapid proliferation of unmanned aerial vehicles (UAVs) has highlighted
the importance of robust and efficient object detection in diverse aerial
scenarios. Detecting small objects under complex conditions, however, remains a
significant challenge. Existing approaches often prioritize inference speed,
leading to degraded performance when handling multi-modal inputs. To address
this, we present DGE-YOLO, an enhanced YOLO-based detection framework designed
to effectively fuse multi-modal information. Specifically, we introduce a
dual-branch architecture for modality-specific feature extraction, enabling the
model to process both infrared and visible images. To further enrich semantic
representation, we propose an Efficient Multi-scale Attention (EMA) mechanism
that enhances feature learning across spatial scales. Additionally, we replace
the conventional neck with a Gather-and-Distribute module to mitigate
information loss during feature aggregation. Extensive experiments on the Drone
Vehicle dataset demonstrate that DGE-YOLO achieves superior performance over
state-of-the-art methods, validating its effectiveness in multi-modal UAV
object detection tasks.

</details>


### [110] [PCLVis: Visual Analytics of Process Communication Latency in Large-Scale Simulation](https://arxiv.org/abs/2506.23257)
*Chongke Bi,Xin Gao,Baofeng Fu,Yuheng Zhao,Siming Chen,Ying Zhao,Yunhai Wang*

Main category: cs.CV

TL;DR: PCLVis框架通过MPI通信数据分析通信延迟事件，帮助用户优化大规模模拟。


<details>
  <summary>Details</summary>
Motivation: 解决超级计算机中大规模模拟的通信延迟问题，传统方法依赖管理员提供的物理链路信息，对普通用户不友好。

Method: 1. 空间定位PCL事件，构建进程相关性树聚类高相关进程；2. 构建通信依赖DAG分析事件传播路径，设计滑动窗口算法和CS-Glyph展示通信状态；3. 提出PCL事件归因策略。

Result: 在TH-1A超级计算机上验证了PCLVis的有效性，显著提升模拟效率。

Conclusion: PCLVis为普通用户提供了一种无需物理链路信息的通信延迟分析工具，优化了模拟性能。

Abstract: Large-scale simulations on supercomputers have become important tools for
users. However, their scalability remains a problem due to the huge
communication cost among parallel processes. Most of the existing communication
latency analysis methods rely on the physical link layer information, which is
only available to administrators. In this paper, a framework called PCLVis is
proposed to help general users analyze process communication latency (PCL)
events. Instead of the physical link layer information, the PCLVis uses the MPI
process communication data for the analysis. First, a spatial PCL event
locating method is developed. All processes with high correlation are
classified into a single cluster by constructing a process-correlation tree.
Second, the propagation path of PCL events is analyzed by constructing a
communication-dependency-based directed acyclic graph (DAG), which can help
users interactively explore a PCL event from the temporal evolution of a
located PCL event cluster. In this graph, a sliding window algorithm is
designed to generate the PCL events abstraction. Meanwhile, a new glyph called
the communication state glyph (CS-Glyph) is designed for each process to show
its communication states, including its in/out messages and load balance. Each
leaf node can be further unfolded to view additional information. Third, a PCL
event attribution strategy is formulated to help users optimize their
simulations. The effectiveness of the PCLVis framework is demonstrated by
analyzing the PCL events of several simulations running on the TH-1A
supercomputer. By using the proposed framework, users can greatly improve the
efficiency of their simulations.

</details>


### [111] [Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis](https://arxiv.org/abs/2506.23263)
*Lei-lei Li,Jianwu Fang,Junbin Xiao,Shanmin Pang,Hongkai Yu,Chen Lv,Jianru Xue,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出了一种名为Causal-VidSyn的扩散模型，用于合成以自我为中心的交通事故视频，通过结合因果关系和驾驶员注视数据提升视频质量。


<details>
  <summary>Details</summary>
Motivation: 为了提升自动驾驶汽车的安全性，需要合成能够反映因果关系的交通事故视频，以测试其对现实中难以承受事故的响应能力。

Method: 提出Causal-VidSyn模型，利用事故原因描述和驾驶员注视数据，通过事故原因回答和注视条件选择模块，识别事故参与者和行为。

Result: Causal-VidSyn在帧质量和因果敏感性方面优于现有视频扩散模型，支持多种任务如视频编辑、视频扩散和文本到视频生成。

Conclusion: Causal-VidSyn通过结合因果关系和驾驶员注视数据，显著提升了合成交通事故视频的质量和实用性。

Abstract: Egocentricly comprehending the causes and effects of car accidents is crucial
for the safety of self-driving cars, and synthesizing causal-entity reflected
accident videos can facilitate the capability test to respond to unaffordable
accidents in reality. However, incorporating causal relations as seen in
real-world videos into synthetic videos remains challenging. This work argues
that precisely identifying the accident participants and capturing their
related behaviors are of critical importance. In this regard, we propose a
novel diffusion model, Causal-VidSyn, for synthesizing egocentric traffic
accident videos. To enable causal entity grounding in video diffusion,
Causal-VidSyn leverages the cause descriptions and driver fixations to identify
the accident participants and behaviors, facilitated by accident reason
answering and gaze-conditioned selection modules. To support Causal-VidSyn, we
further construct Drive-Gaze, the largest driver gaze dataset (with 1.54M
frames of fixations) in driving accident scenarios. Extensive experiments show
that Causal-VidSyn surpasses state-of-the-art video diffusion models in terms
of frame quality and causal sensitivity in various tasks, including accident
video editing, normal-to-accident video diffusion, and text-to-video
generation.

</details>


### [112] [Token Activation Map to Visually Explain Multimodal LLMs](https://arxiv.org/abs/2506.23270)
*Yi Li,Hualiang Wang,Xinpeng Ding,Haonan Wang,Xiaomeng Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为Token Activation Map (TAM)的方法，用于解决多模态大语言模型(MLLMs)解释性不足的问题，通过估计因果推理和秩高斯滤波减少冗余激活干扰，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型(MLLMs)的解释性不足，影响了模型的可信度和可视化效果，现有方法忽视了上下文冗余激活对解释的干扰。

Method: 提出TAM方法，结合估计因果推理和秩高斯滤波，减少上下文冗余激活的干扰，提升解释质量。

Result: TAM在多种场景（如目标定位、失败案例分析、视频可视化等）中表现优异，显著优于现有方法。

Conclusion: TAM为MLLMs提供高质量的解释和可视化工具，增强了模型的可信度和理解深度。

Abstract: Multimodal large language models (MLLMs) are broadly empowering various
fields. Despite their advancements, the explainability of MLLMs remains less
explored, hindering deeper understanding, model credibility, and effective
visualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that
produce a single output, MLLMs generate sequences of tokens progressively,
where each generated token depends on the previous context. Therefore, earlier
context tokens can introduce redundant activations that interfere with the
explanation of later tokens beyond their original information. Existing studies
often overlook this issue, but our observations reveal that these redundant
correlations can significantly hurt the reliability of explanations. To address
this, we propose an estimated causal inference method to mitigate the
interference of context to achieve high-quality MLLM explanation, with a novel
rank Gaussian filter to further reduce activation noises. We term this method
Token Activation Map (TAM) to highlight the consideration of interactions
between tokens. TAM also indicates that it excels at explaining multiple tokens
of MLLM, which is different from the Class Activation Map (CAM) for a single
prediction. Our TAM method significantly outperforms existing SoTA methods,
showcasing high-quality visualization results that can be utilized for various
scenarios, such as object localization, failure case analysis, video
visualization, MLLMs visual comparison, and model understanding (e.g., color,
shape, action, location, visual reasoning, multi-turn conversation, etc). The
code is available atgithub.com/xmed-lab/TAM.

</details>


### [113] [Mettle: Meta-Token Learning for Memory-Efficient Audio-Visual Adaptation](https://arxiv.org/abs/2506.23271)
*Jinxing Zhou,Zhihui Li,Yongqiang Yu,Yanghao Zhou,Ruohao Guo,Guangyao Li,Yuxin Mao,Mingfei Han,Xiaojun Chang,Meng Wang*

Main category: cs.CV

TL;DR: Mettle是一种高效适应预训练Transformer模型的方法，通过并行蒸馏音频或视觉特征为元标记，支持分类和细粒度分割任务。


<details>
  <summary>Details</summary>
Motivation: 解决大规模预训练Transformer模型在下游音频-视觉任务中内存占用高、训练时间长的问题。

Method: 使用轻量级Layer-Centric Distillation (LCD)模块并行蒸馏特征为元标记，并引入Meta-Token Injection (MTI)模块支持细粒度分割。

Result: 在多个音频-视觉基准测试中显著减少内存使用和训练时间，同时保持参数效率和准确性。

Conclusion: Mettle是一种高效且内存友好的方法，适用于音频-视觉任务。

Abstract: We present \textbf{Met}a-\textbf{T}oken \textbf{Le}arning (Mettle), a simple
and memory-efficient method for adapting large-scale pretrained transformer
models to downstream audio-visual tasks. Instead of sequentially modifying the
output feature distribution of the transformer backbone, Mettle utilizes a
lightweight \textit{Layer-Centric Distillation (LCD)} module to distill in
parallel the intact audio or visual features embedded by each transformer layer
into compact meta-tokens. This distillation process considers both pretrained
knowledge preservation and task-specific adaptation. The obtained meta-tokens
can be directly applied to classification tasks, such as audio-visual event
localization and audio-visual video parsing. To further support fine-grained
segmentation tasks, such as audio-visual segmentation, we introduce a
\textit{Meta-Token Injection (MTI)} module, which utilizes the audio and visual
meta-tokens distilled from the top transformer layer to guide feature
adaptation in earlier layers. Extensive experiments on multiple audiovisual
benchmarks demonstrate that our method significantly reduces memory usage and
training time while maintaining parameter efficiency and competitive accuracy.

</details>


### [114] [Why Settle for One? Text-to-ImageSet Generation and Evaluation](https://arxiv.org/abs/2506.23275)
*Chengyou Jia,Xin Shen,Zhuohang Dang,Zhuohang Dang,Changliang Xia,Weijia Wu,Xinyu Zhang,Hangwei Qian,Ivor W. Tsang,Minnan Luo*

Main category: cs.CV

TL;DR: 本文提出了一种更具挑战性的问题——文本到图像集（T2IS）生成，旨在根据用户指令生成满足多种一致性要求的图像集。作者引入了T2IS-Bench基准和T2IS-Eval评估框架，并提出了AutoT2IS方法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的一致性方法通常局限于特定领域和一致性要求，限制了其泛化能力。因此，作者提出了更通用的T2IS问题。

Method: 作者提出了T2IS-Bench基准和T2IS-Eval评估框架，并开发了AutoT2IS方法，利用预训练的Diffusion Transformers的上下文能力来满足图像级提示对齐和集合级视觉一致性。

Result: 实验表明，AutoT2IS显著优于现有方法，并能支持多种实际应用。

Conclusion: AutoT2IS在T2IS问题上表现出色，具有重要的实际价值。

Abstract: Despite remarkable progress in Text-to-Image models, many real-world
applications require generating coherent image sets with diverse consistency
requirements. Existing consistent methods often focus on a specific domain with
specific aspects of consistency, which significantly constrains their
generalizability to broader applications. In this paper, we propose a more
challenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate
sets of images that meet various consistency requirements based on user
instructions. To systematically study this problem, we first introduce
$\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories,
providing comprehensive coverage for T2IS generation. Building on this, we
propose $\textbf{T2IS-Eval}$, an evaluation framework that transforms user
instructions into multifaceted assessment criteria and employs effective
evaluators to adaptively assess consistency fulfillment between criteria and
generated sets. Subsequently, we propose $\textbf{AutoT2IS}$, a training-free
framework that maximally leverages pretrained Diffusion Transformers'
in-context capabilities to harmonize visual elements to satisfy both
image-level prompt alignment and set-level visual consistency. Extensive
experiments on T2IS-Bench reveal that diverse consistency challenges all
existing methods, while our AutoT2IS significantly outperforms current
generalized and even specialized approaches. Our method also demonstrates the
ability to enable numerous underexplored real-world applications, confirming
its substantial practical value. Visit our project in
https://chengyou-jia.github.io/T2IS-Home.

</details>


### [115] [Autoregressive Denoising Score Matching is a Good Video Anomaly Detector](https://arxiv.org/abs/2506.23282)
*Hanwen Zhang,Congqi Cao,Qinyi Lv,Lingtong Min,Yanning Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于生成模型的视频异常检测方法，通过解决场景、运动和外观三个关键问题，提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于似然的异常检测方法无法处理局部模式中的异常，因此需要一种更全面的方法来检测这些“未见”异常。

Method: 作者构建了一个噪声条件化的分数变换器，引入了场景依赖和运动感知的分数函数，并通过自回归去噪分数匹配机制增强外观感知。

Result: 在三个流行的视频异常检测基准测试中，该方法表现出最先进的性能。

Conclusion: 通过综合考虑场景、运动和外观三个关键因素，该方法能够更全面地检测视频中的异常。

Abstract: Video anomaly detection (VAD) is an important computer vision problem. Thanks
to the mode coverage capabilities of generative models, the likelihood-based
paradigm is catching growing interest, as it can model normal distribution and
detect out-of-distribution anomalies. However, these likelihood-based methods
are blind to the anomalies located in local modes near the learned
distribution. To handle these ``unseen" anomalies, we dive into three gaps
uniquely existing in VAD regarding scene, motion and appearance. Specifically,
we first build a noise-conditioned score transformer for denoising score
matching. Then, we introduce a scene-dependent and motion-aware score function
by embedding the scene condition of input sequences into our model and
assigning motion weights based on the difference between key frames of input
sequences. Next, to solve the problem of blindness in principle, we integrate
unaffected visual information via a novel autoregressive denoising score
matching mechanism for inference. Through autoregressively injecting
intensifying Gaussian noise into the denoised data and estimating the
corresponding score function, we compare the denoised data with the original
data to get a difference and aggregate it with the score function for an
enhanced appearance perception and accumulate the abnormal context. With all
three gaps considered, we can compute a more comprehensive anomaly indicator.
Experiments on three popular VAD benchmarks demonstrate the state-of-the-art
performance of our method.

</details>


### [116] [MoMa: Modulating Mamba for Adapting Image Foundation Models to Video Recognition](https://arxiv.org/abs/2506.23283)
*Yuhuan Yang,Chaofan Ma,Zhenjie Mao,Jiangchao Yao,Ya Zhang,Yanfeng Wang*

Main category: cs.CV

TL;DR: MoMa是一个高效的适配器框架，通过将Mamba的选择性状态空间建模集成到图像基础模型（IFMs）中，实现了全时空建模，提升了视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理视频时空信息时往往分离处理，难以捕捉视频动态的复杂性。

Method: 提出SeqMod操作和Divide-and-Modulate架构，将时空信息注入预训练的IFMs中。

Result: 在多个视频基准测试中表现优异，计算成本更低。

Conclusion: MoMa通过高效的全时空建模显著提升了视频理解性能。

Abstract: Video understanding is a complex challenge that requires effective modeling
of spatial-temporal dynamics. With the success of image foundation models
(IFMs) in image understanding, recent approaches have explored
parameter-efficient fine-tuning (PEFT) to adapt IFMs for video. However, most
of these methods tend to process spatial and temporal information separately,
which may fail to capture the full intricacy of video dynamics. In this paper,
we propose MoMa, an efficient adapter framework that achieves full
spatial-temporal modeling by integrating Mamba's selective state space modeling
into IFMs. We propose a novel SeqMod operation to inject spatial-temporal
information into pre-trained IFMs, without disrupting their original features.
By incorporating SeqMod into a Divide-and-Modulate architecture, MoMa enhances
video understanding while maintaining computational efficiency. Extensive
experiments on multiple video benchmarks demonstrate the effectiveness of MoMa,
achieving superior performance with reduced computational cost.

</details>


### [117] [Competitive Distillation: A Simple Learning Strategy for Improving Visual Classification](https://arxiv.org/abs/2506.23285)
*Daqian Shi,Xiaolei Diao,Xu Chen,Cédric M. John*

Main category: cs.CV

TL;DR: 提出了一种新颖的竞争蒸馏策略，通过动态选择教师网络和引入竞争优化，提升多网络协同训练的性能。


<details>
  <summary>Details</summary>
Motivation: 现有蒸馏方法因对学习方向影响的理解不足而效果有限，需改进多网络协同训练策略。

Method: 提出竞争蒸馏策略，动态选择教师网络，引入竞争优化和随机扰动以提升学习性能。

Result: 实验表明，竞争蒸馏在多种任务和数据集上表现优异。

Conclusion: 竞争蒸馏通过动态竞争和扰动优化，显著提升了多网络协同训练的效果。

Abstract: Deep Neural Networks (DNNs) have significantly advanced the field of computer
vision. To improve DNN training process, knowledge distillation methods
demonstrate their effectiveness in accelerating network training by introducing
a fixed learning direction from the teacher network to student networks. In
this context, several distillation-based optimization strategies are proposed,
e.g., deep mutual learning and self-distillation, as an attempt to achieve
generic training performance enhancement through the cooperative training of
multiple networks. However, such strategies achieve limited improvements due to
the poor understanding of the impact of learning directions among networks
across different iterations. In this paper, we propose a novel competitive
distillation strategy that allows each network in a group to potentially act as
a teacher based on its performance, enhancing the overall learning performance.
Competitive distillation organizes a group of networks to perform a shared task
and engage in competition, where competitive optimization is proposed to
improve the parameter updating process. We further introduce stochastic
perturbation in competitive distillation, aiming to motivate networks to induce
mutations to achieve better visual representations and global optimum. The
experimental results show that competitive distillation achieves promising
performance in diverse tasks and datasets.

</details>


### [118] [DDL: A Dataset for Interpretable Deepfake Detection and Localization in Real-World Scenarios](https://arxiv.org/abs/2506.23292)
*Changtao Miao,Yi Zhang,Weize Gao,Man Luo,Weiwei Feng,Zhiya Tan,Jianshu Li,Ajian Liu,Yunfeng Diao,Qi Chu,Tao Gong,Zhe Li,Weibin Yao,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种新的大规模深度伪造检测与定位数据集（DDL），包含180万伪造样本和75种深度伪造方法，旨在解决现有数据集在多样性、规模和注释方面的不足，以支持更可靠的深度伪造检测与解释性研究。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的滥用日益严重，现有检测方法缺乏解释性，且现有数据集在多样性、规模和注释上不足，无法满足复杂现实场景的需求。

Method: 构建了DDL数据集，包含多样伪造场景、全面的深度伪造方法、多种操纵模式和细粒度伪造注释。

Result: DDL数据集提供了更具挑战性的基准，支持下一代深度伪造检测、定位和解释性方法的发展。

Conclusion: DDL数据集通过其多样性和规模，为深度伪造检测与解释性研究提供了重要支持。

Abstract: Recent advances in AIGC have exacerbated the misuse of malicious deepfake
content, making the development of reliable deepfake detection methods an
essential means to address this challenge. Although existing deepfake detection
models demonstrate outstanding performance in detection metrics, most methods
only provide simple binary classification results, lacking interpretability. In
critical domains such as law, interpretability is crucial for enhancing the
credibility and authority of decisions. Recent studies attempt to improve the
interpretability of classification results by providing spatial manipulation
masks or temporal forgery segments. However, the practical effectiveness of
these methods remains suboptimal due to limitations of the forgery data. Most
current deepfake datasets predominantly offer binary labels, only a few
datasets with localization annotations. However, they suffer from restricted
forgery scenarios, limited diversity in deepfake types, and insufficient data
scale, making them inadequate for complex real-world scenarios. To address this
predicament, we construct a novel large-scale deepfake detection and
localization ($\textbf{DDL}$) dataset containing over $\textbf{1.8M}$ forged
samples and encompassing up to $\textbf{75}$ distinct deepfake methods. The DDL
design incorporates four key innovations: (1) $\textbf{Diverse Forgery
Scenarios}$, (2) $\textbf{Comprehensive Deepfake Methods}$, (3) $\textbf{Varied
Manipulation Modes}$, and (4) $\textbf{Fine-grained Forgery Annotations}$.
Through these improvements, our DDL not only provides a more challenging
benchmark for complex real-world forgeries, but also offers crucial support for
building next-generation deepfake detection, localization, and interpretability
methods. The DDL dataset project page is on
https://deepfake-workshop-ijcai2025.github.io/main/index.html.

</details>


### [119] [DiffFit: Disentangled Garment Warping and Texture Refinement for Virtual Try-On](https://arxiv.org/abs/2506.23295)
*Xiang Xu*

Main category: cs.CV

TL;DR: DiffFit是一种新型的两阶段潜在扩散框架，用于高保真虚拟试穿，通过几何感知的服装变形和跨模态条件扩散模型解决现有方法在细节保留、对齐精度和效率上的问题。


<details>
  <summary>Details</summary>
Motivation: 虚拟试穿（VTON）在电子商务和数字时尚中有广泛应用，但现有方法在细节保留、对齐精度和效率上存在不足。

Method: DiffFit采用两阶段策略：第一阶段进行几何感知的服装变形和对齐，第二阶段通过跨模态条件扩散模型细化纹理保真度。

Result: 实验表明，DiffFit在定量指标和感知评估上均优于现有方法。

Conclusion: DiffFit通过解耦几何对齐和外观细化，显著提升了生成稳定性和视觉真实感。

Abstract: Virtual try-on (VTON) aims to synthesize realistic images of a person wearing
a target garment, with broad applications in e-commerce and digital fashion.
While recent advances in latent diffusion models have substantially improved
visual quality, existing approaches still struggle with preserving fine-grained
garment details, achieving precise garment-body alignment, maintaining
inference efficiency, and generalizing to diverse poses and clothing styles. To
address these challenges, we propose DiffFit, a novel two-stage latent
diffusion framework for high-fidelity virtual try-on. DiffFit adopts a
progressive generation strategy: the first stage performs geometry-aware
garment warping, aligning the garment with the target body through fine-grained
deformation and pose adaptation. The second stage refines texture fidelity via
a cross-modal conditional diffusion model that integrates the warped garment,
the original garment appearance, and the target person image for high-quality
rendering. By decoupling geometric alignment and appearance refinement, DiffFit
effectively reduces task complexity and enhances both generation stability and
visual realism. It excels in preserving garment-specific attributes such as
textures, wrinkles, and lighting, while ensuring accurate alignment with the
human body. Extensive experiments on large-scale VTON benchmarks demonstrate
that DiffFit achieves superior performance over existing state-of-the-art
methods in both quantitative metrics and perceptual evaluations.

</details>


### [120] [Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting](https://arxiv.org/abs/2506.23308)
*Yiming Huang,Long Bai,Beilei Cui,Yanheng Li,Tong Chen,Jie Wang,Jinlin Wu,Zhen Lei,Hongbin Liu,Hongliang Ren*

Main category: cs.CV

TL;DR: Endo-4DGX是一种针对内窥镜场景光照不均的新型重建方法，通过光照自适应高斯泼溅技术，解决了3D-GS在极端光照条件下的优化问题。


<details>
  <summary>Details</summary>
Motivation: 在图像引导机器人手术中，软组织的精确重建至关重要，但现有3D-GS方法在光照变化场景中表现不佳。

Method: 结合光照嵌入、区域感知增强模块和空间感知调整模块，实现光照自适应优化，并使用曝光控制损失恢复外观。

Result: Endo-4DGX在低光和过曝条件下均表现出色，显著优于现有重建和恢复方法。

Conclusion: Endo-4DGX在挑战性光照环境中具有优越性能，有望推动机器人辅助手术的发展。

Abstract: Accurate reconstruction of soft tissue is crucial for advancing automation in
image-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS)
techniques and their variants, 4DGS, achieve high-quality renderings of dynamic
surgical scenes in real-time. However, 3D-GS-based methods still struggle in
scenarios with varying illumination, such as low light and over-exposure.
Training 3D-GS in such extreme light conditions leads to severe optimization
problems and devastating rendering quality. To address these challenges, we
present Endo-4DGX, a novel reconstruction method with illumination-adaptive
Gaussian Splatting designed specifically for endoscopic scenes with uneven
lighting. By incorporating illumination embeddings, our method effectively
models view-dependent brightness variations. We introduce a region-aware
enhancement module to model the sub-area lightness at the Gaussian level and a
spatial-aware adjustment module to learn the view-consistent brightness
adjustment. With the illumination adaptive design, Endo-4DGX achieves superior
rendering performance under both low-light and over-exposure conditions while
maintaining geometric accuracy. Additionally, we employ an exposure control
loss to restore the appearance from adverse exposure to the normal level for
illumination-adaptive optimization. Experimental results demonstrate that
Endo-4DGX significantly outperforms combinations of state-of-the-art
reconstruction and restoration methods in challenging lighting environments,
underscoring its potential to advance robot-assisted surgical applications. Our
code is available at https://github.com/lastbasket/Endo-4DGX.

</details>


### [121] [FastSeg: Efficient Training-Free Open-Vocabulary Segmentation via Hierarchical Attention Refinement Method](https://arxiv.org/abs/2506.23323)
*Quang-Huy Che,Vinh-Tiep Nguyen*

Main category: cs.CV

TL;DR: FastSeg是一种高效的无训练框架，通过预训练扩散模型（如Stable Diffusion）的（1+1）步反向过程实现开放词汇语义分割，并引入三项关键技术提升分割质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有对比学习模型在像素级空间精度不足及扩散模型在迭代次数与分割质量平衡上的挑战。

Method: 提出FastSeg框架，包括双提示机制、分层注意力细化方法（HARD）和测试时翻转（TTF）方案。

Result: 在PASCAL VOC、PASCAL Context和COCO Object基准测试中达到43.8%的平均mIoU，保持高效推理。

Conclusion: FastSeg在分割质量和推理效率之间取得平衡，为扩展性提供坚实基础。

Abstract: Open-vocabulary semantic segmentation (OVSS) aims to segment objects from
arbitrary text categories without requiring densely annotated datasets.
Although contrastive learning based models enable zero-shot segmentation, they
often lose fine spatial precision at pixel level, due to global representation
bias. In contrast, diffusion-based models naturally encode fine-grained spatial
features via attention mechanisms that capture both global context and local
details. However, they often face challenges in balancing the number of
iterations with the quality of the segmentation. In this work, we propose
FastSeg, a novel and efficient training-free framework with only (1+1)-step of
reverse process of a pretrained diffusion model (e.g., Stable Diffusion).
Moreover, instead of running multiple times for different classes, FastSeg
performs segmentation for all classes at once. To further enhance the
segmentation quality, FastSeg introduces three key components: (i) a
dual-prompt mechanism for discriminative, class-aware attention extraction,
(ii) a Hierarchical Attention Refinement Method (HARD) that enhances fused
cross-attention using scale-aligned selfattention maps, and (iii) a Test-Time
Flipping (TTF) scheme designed to improve spatial consistency. Extensive
experiments show that FastSeg achieves state-of-the-art training-free
performance, obtaining 43.8% average mIoU across PASCAL VOC, PASCAL Context,
and COCO Object benchmarks while maintaining superior inference efficiency. Our
results demonstrate that FastSeg provides a strong foundation for
extendability, bridging the gap between segmentation quality and inference
efficiency.

</details>


### [122] [IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering](https://arxiv.org/abs/2506.23329)
*Parker Liu,Chenxin Li,Zhengxin Li,Yipeng Wu,Wuyang Li,Zhiqin Yang,Zhenyuan Zhang,Yunlong Lin,Sirui Han,Brandon Y. Feng*

Main category: cs.CV

TL;DR: IR3D-Bench是一个新基准，通过要求视觉语言模型（VLMs）主动使用工具重建输入图像的3D结构，测试其场景理解能力，而非仅依赖被动识别。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在描述任务上表现优异，但其是否真正理解场景仍不确定。本文旨在通过主动创建任务（而非被动识别）来验证其理解能力。

Method: 基于分析-合成范式，IR3D-Bench要求视觉语言代理（VLAs）使用编程和渲染工具重建输入图像的3D结构，实现代理逆向渲染。

Result: 初步实验显示，现有VLMs在视觉精度上存在局限，而非基本工具使用。

Conclusion: IR3D-Bench为系统研究和开发工具使用的VLAs提供了数据与评估协议，推动通过创建实现真正的场景理解。

Abstract: Vision-language models (VLMs) excel at descriptive tasks, but whether they
truly understand scenes from visual observations remains uncertain. We
introduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding
through active creation rather than passive recognition. Grounded in the
analysis-by-synthesis paradigm, IR3D-Bench tasks Vision-Language Agents (VLAs)
with actively using programming and rendering tools to recreate the underlying
3D structure of an input image, achieving agentic inverse rendering through
tool use. This "understanding-by-creating" approach probes the tool-using
generative capacity of VLAs, moving beyond the descriptive or conversational
capacity measured by traditional scene understanding benchmarks. We provide a
comprehensive suite of metrics to evaluate geometric accuracy, spatial
relations, appearance attributes, and overall plausibility. Initial experiments
on agentic inverse rendering powered by various state-of-the-art VLMs highlight
current limitations, particularly in visual precision rather than basic tool
usage. IR3D-Bench, including data and evaluation protocols, is released to
facilitate systematic study and development of tool-using VLAs towards genuine
scene understanding by creating.

</details>


### [123] [CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step Image Translation](https://arxiv.org/abs/2506.23347)
*Yi Liu,Shengqian Li,Zuzeng Lin,Feng Wang,Si Liu*

Main category: cs.CV

TL;DR: 提出了一种名为CycleVAR的新方法，通过Softmax Relaxed Quantization解决传统量化方法在图像翻译中的梯度问题，并在无监督图像翻译中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于向量量化的方法在无监督图像翻译中存在梯度中断问题，限制了端到端优化。

Method: 使用Softmax Relaxed Quantization保持梯度传播，并引入CycleVAR，将图像翻译转化为条件自回归生成任务。

Result: CycleVAR在无监督图像翻译中表现优于现有方法（如CycleGAN-Turbo），并行生成模式效果更佳。

Conclusion: CycleVAR通过改进量化方法和生成模式，显著提升了无监督图像翻译的性能和效率。

Abstract: The current conditional autoregressive image generation methods have shown
promising results, yet their potential remains largely unexplored in the
practical unsupervised image translation domain, which operates without
explicit cross-domain correspondences. A critical limitation stems from the
discrete quantization inherent in traditional Vector Quantization-based
frameworks, which disrupts gradient flow between the Variational Autoencoder
decoder and causal Transformer, impeding end-to-end optimization during
adversarial training in image space. To tackle this issue, we propose using
Softmax Relaxed Quantization, a novel approach that reformulates codebook
selection as a continuous probability mixing process via Softmax, thereby
preserving gradient propagation. Building upon this differentiable foundation,
we introduce CycleVAR, which reformulates image-to-image translation as
image-conditional visual autoregressive generation by injecting multi-scale
source image tokens as contextual prompts, analogous to prefix-based
conditioning in language models. CycleVAR exploits two modes to generate the
target image tokens, including (1) serial multi-step generation, enabling
iterative refinement across scales, and (2) parallel one-step generation
synthesizing all resolution outputs in a single forward pass. Experimental
findings indicate that the parallel one-step generation mode attains superior
translation quality with quicker inference speed than the serial multi-step
mode in unsupervised scenarios. Furthermore, both quantitative and qualitative
results indicate that CycleVAR surpasses previous state-of-the-art unsupervised
image translation models, \textit{e}.\textit{g}., CycleGAN-Turbo.

</details>


### [124] [GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language Fields](https://arxiv.org/abs/2506.23352)
*Shunsuke Yasuki,Taiki Miyanishi,Nakamasa Inoue,Shuhei Kurita,Koya Sakamoto,Daichi Azuma,Masato Taki,Yutaka Matsuo*

Main category: cs.CV

TL;DR: GeoProg3D是一个视觉编程框架，通过自然语言实现城市规模高保真3D场景的交互，解决了现有方法在小规模环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有3D语言方法在大规模复杂城市环境中缺乏可扩展性和组合推理能力，GeoProg3D旨在填补这一空白。

Method: 框架包含两个关键组件：地理感知的城市规模3D语言场（GCLF）和地理视觉API（GV-APIs），利用大型语言模型（LLMs）动态组合工具。

Result: GeoProg3D在GeoEval3D基准测试中显著优于现有方法，支持多种地理视觉任务。

Conclusion: GeoProg3D是首个通过自然语言实现城市规模高保真3D场景组合地理推理的框架。

Abstract: The advancement of 3D language fields has enabled intuitive interactions with
3D scenes via natural language. However, existing approaches are typically
limited to small-scale environments, lacking the scalability and compositional
reasoning capabilities necessary for large, complex urban settings. To overcome
these limitations, we propose GeoProg3D, a visual programming framework that
enables natural language-driven interactions with city-scale high-fidelity 3D
scenes. GeoProg3D consists of two key components: (i) a Geography-aware
City-scale 3D Language Field (GCLF) that leverages a memory-efficient
hierarchical 3D model to handle large-scale data, integrated with geographic
information for efficiently filtering vast urban spaces using directional cues,
distance measurements, elevation data, and landmark references; and (ii)
Geographical Vision APIs (GV-APIs), specialized geographic vision tools such as
area segmentation and object detection. Our framework employs large language
models (LLMs) as reasoning engines to dynamically combine GV-APIs and operate
GCLF, effectively supporting diverse geographic vision tasks. To assess
performance in city-scale reasoning, we introduce GeoEval3D, a comprehensive
benchmark dataset containing 952 query-answer pairs across five challenging
tasks: grounding, spatial reasoning, comparison, counting, and measurement.
Experiments demonstrate that GeoProg3D significantly outperforms existing 3D
language fields and vision-language models across multiple tasks. To our
knowledge, GeoProg3D is the first framework enabling compositional geographic
reasoning in high-fidelity city-scale 3D environments via natural language. The
code is available at https://snskysk.github.io/GeoProg3D/.

</details>


### [125] [OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions](https://arxiv.org/abs/2506.23361)
*Yuanhao Cai,He Zhang,Xi Chen,Jinbo Xing,Yiwei Hu,Yuqian Zhou,Kai Zhang,Zhifei Zhang,Soo Ye Kim,Tianyu Wang,Yulun Zhang,Xiaokang Yang,Zhe Lin,Alan Yuille*

Main category: cs.CV

TL;DR: 论文提出了一种多主题视频定制方法，通过数据构造管道和混合训练框架，结合扩散Transformer实现高效编辑和控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对单主题场景，且缺乏对多主题数据构造和信号控制的研究。

Method: 提出VideoCus-Factory数据构造管道和IVTM混合训练框架，结合扩散Transformer（OmniVCus）和两种嵌入机制（LE和TAE）。

Result: 实验表明，该方法在定量和定性评估中显著优于现有技术。

Conclusion: 论文成功解决了多主题视频定制和信号控制的挑战，为视频编辑提供了新工具。

Abstract: Existing feedforward subject-driven video customization methods mainly study
single-subject scenarios due to the difficulty of constructing multi-subject
training data pairs. Another challenging problem that how to use the signals
such as depth, mask, camera, and text prompts to control and edit the subject
in the customized video is still less explored. In this paper, we first propose
a data construction pipeline, VideoCus-Factory, to produce training data pairs
for multi-subject customization from raw videos without labels and control
signals such as depth-to-video and mask-to-video pairs. Based on our
constructed data, we develop an Image-Video Transfer Mixed (IVTM) training with
image editing data to enable instructive editing for the subject in the
customized video. Then we propose a diffusion Transformer framework, OmniVCus,
with two embedding mechanisms, Lottery Embedding (LE) and Temporally Aligned
Embedding (TAE). LE enables inference with more subjects by using the training
subjects to activate more frame embeddings. TAE encourages the generation
process to extract guidance from temporally aligned control signals by
assigning the same frame embeddings to the control and noise tokens.
Experiments demonstrate that our method significantly surpasses
state-of-the-art methods in both quantitative and qualitative evaluations.
Video demos are at our project page:
https://caiyuanhao1998.github.io/project/OmniVCus/. Our code will be released
at https://github.com/caiyuanhao1998/Open-OmniVCus

</details>


### [126] [SIEDD: Shared-Implicit Encoder with Discrete Decoders](https://arxiv.org/abs/2506.23382)
*Vikram Rangarajan,Shishira Maiya,Max Ehrlich,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: SIEDD是一种新型架构，通过共享隐式编码器和离散解码器，显著加速了隐式神经表示（INR）的编码速度，同时保持重建质量和压缩比。


<details>
  <summary>Details</summary>
Motivation: 现有的INR编码方法速度慢，且加速时往往牺牲重建质量或坐标级控制能力，限制了其实际应用。

Method: SIEDD采用共享的坐标编码器快速捕捉全局低频特征，并冻结编码器，并行训练轻量级离散解码器，结合坐标空间采样加速。

Result: 在HD和4K基准测试中，SIEDD实现了20-30倍的编码速度提升，同时保持竞争性的重建质量和压缩比。

Conclusion: SIEDD显著提升了高保真神经视频压缩的实用性，为实际部署提供了可扩展且高效的路径。

Abstract: Implicit Neural Representations (INRs) offer exceptional fidelity for video
compression by learning per-video optimized functions, but their adoption is
crippled by impractically slow encoding times. Existing attempts to accelerate
INR encoding often sacrifice reconstruction quality or crucial coordinate-level
control essential for adaptive streaming and transcoding. We introduce SIEDD
(Shared-Implicit Encoder with Discrete Decoders), a novel architecture that
fundamentally accelerates INR encoding without these compromises. SIEDD first
rapidly trains a shared, coordinate-based encoder on sparse anchor frames to
efficiently capture global, low-frequency video features. This encoder is then
frozen, enabling massively parallel training of lightweight, discrete decoders
for individual frame groups, further expedited by aggressive coordinate-space
sampling. This synergistic design delivers a remarkable 20-30X encoding
speed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while
maintaining competitive reconstruction quality and compression ratios.
Critically, SIEDD retains full coordinate-based control, enabling continuous
resolution decoding and eliminating costly transcoding. Our approach
significantly advances the practicality of high-fidelity neural video
compression, demonstrating a scalable and efficient path towards real-world
deployment. Our codebase is available at
https://github.com/VikramRangarajan/SIEDD .

</details>


### [127] [A High-Throughput Platform to Bench Test Smartphone-Based Heart Rate Measurements Derived From Video](https://arxiv.org/abs/2506.23414)
*Ming-Zher Poh,Jonathan Wang,Jonathan Hsu,Lawrence Cai,Eric Teasley,James A. Taylor,Jameson K. Rogers,Anupam Pathak,Shwetak Patel*

Main category: cs.CV

TL;DR: 本文提出了一种高通量测试平台，用于评估智能手机心率监测应用的性能，解决了设备差异和标准化测试的挑战。


<details>
  <summary>Details</summary>
Motivation: 智能手机心率监测应用因设备差异和缺乏标准化测试方法而面临性能评估和兼容性问题。

Method: 设计了一个包含12部智能手机并行测试的测试系统，生成可控心率和信号质量的合成PPG视频，并通过主机协调视频播放和数据记录。

Result: 系统在输入与测量心率之间的平均绝对百分比误差为0.11%，PPG信号相关性为0.92，20款智能手机均符合ANSI/CTA标准。

Conclusion: 该平台为智能手机心率应用提供了可扩展的预部署测试方案，提升了应用性能和设备兼容性。

Abstract: Smartphone-based heart rate (HR) monitoring apps using finger-over-camera
photoplethysmography (PPG) face significant challenges in performance
evaluation and device compatibility due to device variability and
fragmentation. Manual testing is impractical, and standardized methods are
lacking. This paper presents a novel, high-throughput bench-testing platform to
address this critical need. We designed a system comprising a test rig capable
of holding 12 smartphones for parallel testing, a method for generating
synthetic PPG test videos with controllable HR and signal quality, and a host
machine for coordinating video playback and data logging. The system achieved a
mean absolute percentage error (MAPE) of 0.11% +/- 0.001% between input and
measured HR, and a correlation coefficient of 0.92 +/- 0.008 between input and
measured PPG signals using a clinically-validated smartphone-based HR app.
Bench-testing results of 20 different smartphone models correctly classified
all the devices as meeting the ANSI/CTA accuracy standards for HR monitors
(MAPE <10%) when compared to a prospective clinical study with 80 participants,
demonstrating high positive predictive value. This platform offers a scalable
solution for pre-deployment testing of smartphone HR apps to improve app
performance, ensure device compatibility, and advance the field of mobile
health.

</details>


### [128] [Why Settle for Mid: A Probabilistic Viewpoint to Spatial Relationship Alignment in Text-to-image Models](https://arxiv.org/abs/2506.23418)
*Parham Rezaei,Arash Marioriyad,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: 论文提出了一种基于概率优势（PoS）的新框架，用于改进文本到图像模型在空间关系生成上的准确性，并引入了PSE评估指标和PSG生成方法。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在生成复杂空间关系时存在困难，无法准确反映输入提示中的空间配置。

Method: 提出PoS框架，设计PSE评估指标和PSG生成方法，后者通过梯度引导或搜索策略优化空间关系生成。

Result: PSE指标比传统方法更符合人类判断，PSG显著提升了模型在空间关系生成上的表现。

Conclusion: PoS框架有效解决了文本到图像模型的空间关系生成问题，PSE和PSG方法在评估和生成方面均优于现有技术。

Abstract: Despite the ability of text-to-image models to generate high-quality,
realistic, and diverse images, they face challenges in compositional
generation, often struggling to accurately represent details specified in the
input prompt. A prevalent issue in compositional generation is the misalignment
of spatial relationships, as models often fail to faithfully generate images
that reflect the spatial configurations specified between objects in the input
prompts. To address this challenge, we propose a novel probabilistic framework
for modeling the relative spatial positioning of objects in a scene, leveraging
the concept of Probability of Superiority (PoS). Building on this insight, we
make two key contributions. First, we introduce a novel evaluation metric,
PoS-based Evaluation (PSE), designed to assess the alignment of 2D and 3D
spatial relationships between text and image, with improved adherence to human
judgment. Second, we propose PoS-based Generation (PSG), an inference-time
method that improves the alignment of 2D and 3D spatial relationships in T2I
models without requiring fine-tuning. PSG employs a Part-of-Speech PoS-based
reward function that can be utilized in two distinct ways: (1) as a
gradient-based guidance mechanism applied to the cross-attention maps during
the denoising steps, or (2) as a search-based strategy that evaluates a set of
initial noise vectors to select the best one. Extensive experiments demonstrate
that the PSE metric exhibits stronger alignment with human judgment compared to
traditional center-based metrics, providing a more nuanced and reliable measure
of complex spatial relationship accuracy in text-image alignment. Furthermore,
PSG significantly enhances the ability of text-to-image models to generate
images with specified spatial configurations, outperforming state-of-the-art
methods across multiple evaluation metrics and benchmarks.

</details>


### [129] [Detecting What Matters: A Novel Approach for Out-of-Distribution 3D Object Detection in Autonomous Vehicles](https://arxiv.org/abs/2506.23426)
*Menna Taha,Aya Ahmed,Mohammed Karmoose,Yasser Gadallah*

Main category: cs.CV

TL;DR: 论文提出了一种新的目标检测方法，将重点从基于类别的分类转向对象危害性判断，以提升自动驾驶车辆对未知物体的检测能力。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测方法只能识别已知类别的物体，无法有效应对分布外（OOD）物体，存在安全隐患。

Method: 通过对象相对于自动驾驶车辆的位置和轨迹，将其分类为‘有害’或‘无害’。

Result: 模型能有效检测OOD物体并评估其危害性，提升了自动驾驶的实时决策能力。

Conclusion: 新方法显著增强了自动驾驶车辆在动态环境中的安全性和决策效果。

Abstract: Autonomous vehicles (AVs) use object detection models to recognize their
surroundings and make driving decisions accordingly. Conventional object
detection approaches classify objects into known classes, which limits the AV's
ability to detect and appropriately respond to Out-of-Distribution (OOD)
objects. This problem is a significant safety concern since the AV may fail to
detect objects or misclassify them, which can potentially lead to hazardous
situations such as accidents. Consequently, we propose a novel object detection
approach that shifts the emphasis from conventional class-based classification
to object harmfulness determination. Instead of object detection by their
specific class, our method identifies them as either 'harmful' or 'harmless'
based on whether they pose a danger to the AV. This is done based on the object
position relative to the AV and its trajectory. With this metric, our model can
effectively detect previously unseen objects to enable the AV to make safer
real-time decisions. Our results demonstrate that the proposed model
effectively detects OOD objects, evaluates their harmfulness, and classifies
them accordingly, thus enhancing the AV decision-making effectiveness in
dynamic environments.

</details>


### [130] [Towards foundational LiDAR world models with efficient latent flow matching](https://arxiv.org/abs/2506.23434)
*Tianran Liu,Shengwen Zhao,Nicholas Rhinehart*

Main category: cs.CV

TL;DR: LiDAR世界模型在多领域迁移中表现优异，显著减少对标注数据的依赖，并提出新框架提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR世界模型局限于单一领域，缺乏跨领域迁移能力，需解决其依赖标注数据多和效率低的问题。

Method: 提出基于潜在条件流匹配（CFM）的框架，优化数据压缩和训练目标，实现高效跨领域迁移。

Result: 单预训练模型在36次比较中30次优于从头训练，仅需5%标注数据即超越先前模型，计算效率提升23倍。

Conclusion: 新框架显著提升LiDAR世界模型的迁移性、效率和性能，为多领域应用提供可行方案。

Abstract: LiDAR-based world models offer more structured and geometry-aware
representations than their image-based counterparts. However, existing LiDAR
world models are narrowly trained; each model excels only in the domain for
which it was built. Can we develop LiDAR world models that exhibit strong
transferability across multiple domains? We conduct the first systematic domain
transfer study across three demanding scenarios: (i) outdoor to indoor
generalization, (ii) sparse-beam \& dense-beam adaptation, and (iii)
non-semantic to semantic transfer. Given different amounts of fine-tuning data,
our experiments show that a single pre-trained model can achieve up to 11%
absolute improvement (83\% relative) over training from scratch and outperforms
training from scratch in 30/36 of our comparisons. This transferability of
dynamic learning significantly reduces the reliance on manually annotated data
for semantic occupancy forecasting: our method exceed the previous semantic
occupancy forecasting models with only 5% of the labeled training data required
by prior models. We also observed inefficiencies of current LiDAR world models,
mainly through their under-compression of LiDAR data and inefficient training
objectives. To address this, we propose a latent conditional flow matching
(CFM)-based frameworks that achieves state-of-the-art reconstruction accuracy
using only half the training data and a compression ratio 6 times higher than
that of prior methods. Our model achieves SOTA performance on
future-trajectory-conditioned semantic occupancy forecasting while being 23x
more computationally efficient (a 28x FPS speedup); and achieves SOTA
performance on semantic occupancy forecasting while being 2x more
computationally efficient (a 1.1x FPS speedup).

</details>


### [131] [PathDiff: Histopathology Image Synthesis with Unpaired Text and Mask Conditions](https://arxiv.org/abs/2506.23440)
*Mahesh Bhosale,Abdul Wasi,Yuanhao Zhai,Yunjie Tian,Samuel Border,Nan Xi,Pinaki Sarder,Junsong Yuan,David Doermann,Xuan Gong*

Main category: cs.CV

TL;DR: PathDiff是一个扩散框架，利用未配对的文本和掩码数据生成高质量的病理图像，提升下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决病理图像数据稀缺问题，同时结合文本和掩码数据以增强生成图像的语义和空间控制。

Method: 提出PathDiff框架，将未配对的文本和掩码数据整合到统一的条件空间中，实现结构和上下文特征的精确控制。

Result: 生成的图像质量高、语义准确，提升了图像保真度、文本-图像对齐和下游任务（如核分割和分类）的性能。

Conclusion: PathDiff在病理图像生成中优于现有方法，为数据增强提供了有效解决方案。

Abstract: Diffusion-based generative models have shown promise in synthesizing
histopathology images to address data scarcity caused by privacy constraints.
Diagnostic text reports provide high-level semantic descriptions, and masks
offer fine-grained spatial structures essential for representing distinct
morphological regions. However, public datasets lack paired text and mask data
for the same histopathological images, limiting their joint use in image
generation. This constraint restricts the ability to fully exploit the benefits
of combining both modalities for enhanced control over semantics and spatial
details. To overcome this, we propose PathDiff, a diffusion framework that
effectively learns from unpaired mask-text data by integrating both modalities
into a unified conditioning space. PathDiff allows precise control over
structural and contextual features, generating high-quality, semantically
accurate images. PathDiff also improves image fidelity, text-image alignment,
and faithfulness, enhancing data augmentation for downstream tasks like nuclei
segmentation and classification. Extensive experiments demonstrate its
superiority over existing methods.

</details>


### [132] [Contrastive Learning with Diffusion Features for Weakly Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.23460)
*Dewen Zeng,Xinrong Hu,Yu-Jen Chen,Yawen Wu,Xiaowei Xu,Yiyu Shi*

Main category: cs.CV

TL;DR: 论文提出了一种名为CLDF的新方法，通过结合对比学习和扩散模型特征改进弱监督语义分割，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统基于CAM的方法在弱监督语义分割中存在部分激活和边界不精确的问题，而基于条件扩散模型的方法虽能生成分割掩码，但易受背景噪声干扰。

Method: 提出CLDF方法，利用对比学习训练像素解码器，将冻结CDM的扩散特征映射到低维嵌入空间，并结合梯度图和CAM以减少假阳性/阴性。

Result: 在两个公共医学数据集的四个分割任务上，CLDF显著优于现有基线。

Conclusion: CLDF通过结合对比学习和扩散特征，有效解决了弱监督语义分割中的噪声和边界问题，性能优越。

Abstract: Weakly supervised semantic segmentation (WSSS) methods using class labels
often rely on class activation maps (CAMs) to localize objects. However,
traditional CAM-based methods struggle with partial activations and imprecise
object boundaries due to optimization discrepancies between classification and
segmentation. Recently, the conditional diffusion model (CDM) has been used as
an alternative for generating segmentation masks in WSSS, leveraging its strong
image generation capabilities tailored to specific class distributions. By
modifying or perturbing the condition during diffusion sampling, the related
objects can be highlighted in the generated images. Yet, the saliency maps
generated by CDMs are prone to noise from background alterations during reverse
diffusion. To alleviate the problem, we introduce Contrastive Learning with
Diffusion Features (CLDF), a novel method that uses contrastive learning to
train a pixel decoder to map the diffusion features from a frozen CDM to a
low-dimensional embedding space for segmentation. Specifically, we integrate
gradient maps generated from CDM external classifier with CAMs to identify
foreground and background pixels with fewer false positives/negatives for
contrastive learning, enabling robust pixel embedding learning. Experimental
results on four segmentation tasks from two public medical datasets demonstrate
that our method significantly outperforms existing baselines.

</details>


### [133] [Time-variant Image Inpainting via Interactive Distribution Transition Estimation](https://arxiv.org/abs/2506.23461)
*Yun Xing,Qing Guo,Xiaoguang Li,Yihao Huang,Xiaofeng Cao,Di Lin,Ivor Tsang,Lei Ma*

Main category: cs.CV

TL;DR: 论文提出了一种新的时间变异图像修复任务（TAMP），并提出了InDiTE模块和InDiTE-Diff方法，通过交互式分布转移估计和扩散模型提升修复效果。实验表明该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决时间变异图像修复中因参考图像与目标图像内容差异大且可能受损而导致的修复困难问题。

Method: 提出InDiTE模块进行交互式语义补充，并结合扩散模型（InDiTE-Diff）进行潜在交叉参考。

Result: 在TAMP-Street数据集上的实验表明，该方法优于现有参考引导图像修复方法。

Conclusion: InDiTE-Diff方法有效解决了时间变异图像修复问题，并提供了新的基准数据集。

Abstract: In this work, we focus on a novel and practical task, i.e., Time-vAriant
iMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image
by leveraging the complementary information from a reference image, where both
images captured the same scene but with a significant time gap in between,
i.e., time-variant images. Different from conventional reference-guided image
inpainting, the reference image under TAMP setup presents significant content
distinction to the target image and potentially also suffers from damages. Such
an application frequently happens in our daily lives to restore a damaged image
by referring to another reference image, where there is no guarantee of the
reference image's source and quality. In particular, our study finds that even
state-of-the-art (SOTA) reference-guided image inpainting methods fail to
achieve plausible results due to the chaotic image complementation. To address
such an ill-posed problem, we propose a novel Interactive Distribution
Transition Estimation (InDiTE) module which interactively complements the
time-variant images with adaptive semantics thus facilitate the restoration of
damaged regions. To further boost the performance, we propose our TAMP
solution, namely Interactive Distribution Transition Estimation-driven
Diffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and
conducts latent cross-reference during sampling. Moreover, considering the lack
of benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street,
based on existing image and mask datasets. We conduct experiments on the
TAMP-Street datasets under two different time-variant image inpainting
settings, which show our method consistently outperform SOTA reference-guided
image inpainting methods for solving TAMP.

</details>


### [134] [Sanitizing Manufacturing Dataset Labels Using Vision-Language Models](https://arxiv.org/abs/2506.23465)
*Nazanin Mahjourian,Vinh Nguyen*

Main category: cs.CV

TL;DR: VLSR框架通过视觉-语言模型CLIP嵌入图像和文本标签到共享语义空间，利用余弦相似度进行标签清洗和聚类，提升工业数据集质量。


<details>
  <summary>Details</summary>
Motivation: 工业领域数据集常因众包和网络爬取而存在标签噪声和不一致，高质量标注成本高且耗时。

Method: 使用CLIP模型嵌入图像和标签到共享语义空间，通过余弦相似度清洗标签，并基于密度聚类合并相似标签。

Result: 在Factorynet数据集上，VLSR成功识别问题标签并提升一致性，显著减少标签词汇量。

Conclusion: VLSR框架能高效提升工业数据集质量，减少人工干预，适用于训练鲁棒机器学习模型。

Abstract: The success of machine learning models in industrial applications is heavily
dependent on the quality of the datasets used to train the models. However,
large-scale datasets, specially those constructed from crowd-sourcing and
web-scraping, often suffer from label noise, inconsistencies, and errors. This
problem is particularly pronounced in manufacturing domains, where obtaining
high-quality labels is costly and time-consuming. This paper introduces
Vision-Language Sanitization and Refinement (VLSR), which is a
vision-language-based framework for label sanitization and refinement in
multi-label manufacturing image datasets. This method embeds both images and
their associated textual labels into a shared semantic space leveraging the
CLIP vision-language model. Then two key tasks are addressed in this process by
computing the cosine similarity between embeddings. First, label sanitization
is performed to identify irrelevant, misspelled, or semantically weak labels,
and surface the most semantically aligned label for each image by comparing
image-label pairs using cosine similarity between image and label embeddings.
Second, the method applies density-based clustering on text embeddings,
followed by iterative cluster merging, to group semantically similar labels
into unified label groups. The Factorynet dataset, which includes noisy labels
from both human annotations and web-scraped sources, is employed to evaluate
the effectiveness of the proposed framework. Experimental results demonstrate
that the VLSR framework successfully identifies problematic labels and improves
label consistency. This method enables a significant reduction in label
vocabulary through clustering, which ultimately enhances the dataset's quality
for training robust machine learning models in industrial applications with
minimal human intervention.

</details>


### [135] [AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays](https://arxiv.org/abs/2506.23467)
*Chenlang Yi,Zizhan Xiong,Qi Qi,Xiyuan Wei,Girish Bathla,Ching-Long Lin,Bobak Jack Mortazavi,Tianbao Yang*

Main category: cs.CV

TL;DR: AdFair-CLIP通过对抗性特征干预减少CLIP模型在医学图像分类中的偏见，提升公平性和诊断准确性。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在医学图像分类中表现优异，但存在种族和性别偏见，导致诊断结果不公平。

Method: 提出AdFair-CLIP框架，利用对抗性特征干预抑制敏感属性，减少虚假相关性。

Result: 在胸部X光数据集上，AdFair-CLIP显著提高公平性和诊断准确性，同时保持零样本和小样本场景的泛化能力。

Conclusion: AdFair-CLIP为基于CLIP的医学诊断模型设定了公平性学习的新标准。

Abstract: Contrastive Language-Image Pre-training (CLIP) models have demonstrated
superior performance across various visual tasks including medical image
classification. However, fairness concerns, including demographic biases, have
received limited attention for CLIP models. This oversight leads to critical
issues, particularly those related to race and gender, resulting in disparities
in diagnostic outcomes and reduced reliability for underrepresented groups. To
address these challenges, we introduce AdFair-CLIP, a novel framework employing
adversarial feature intervention to suppress sensitive attributes, thereby
mitigating spurious correlations and improving prediction fairness. We conduct
comprehensive experiments on chest X-ray (CXR) datasets, and show that
AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while
maintaining robust generalization in zero-shot and few-shot scenarios. These
results establish new benchmarks for fairness-aware learning in CLIP-based
medical diagnostic models, particularly for CXR analysis.

</details>


### [136] [NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments](https://arxiv.org/abs/2506.23468)
*Xuan Yao,Junyu Gao,Changsheng Xu*

Main category: cs.CV

TL;DR: NavMorph是一种自演进的世界模型框架，用于提升视觉与语言导航（VLN-CE）任务中的环境理解和决策能力，通过紧凑的潜在表示和上下文演化记忆实现自适应规划。


<details>
  <summary>Details</summary>
Motivation: 现有方法在新环境中的泛化能力和导航过程中的动态适应能力不足，受人类认知启发，NavMorph旨在解决这些问题。

Method: NavMorph采用紧凑的潜在表示建模环境动态，并结合新颖的上下文演化记忆（Contextual Evolution Memory）来支持自适应规划和策略优化。

Result: 实验表明，NavMorph在VLN-CE基准测试中取得了显著的性能提升。

Conclusion: NavMorph通过自演进的世界模型和上下文记忆机制，有效提升了VLN-CE任务中的导航性能。

Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires
agents to execute sequential navigation actions in complex environments guided
by natural language instructions. Current approaches often struggle with
generalizing to novel environments and adapting to ongoing changes during
navigation. Inspired by human cognition, we present NavMorph, a self-evolving
world model framework that enhances environmental understanding and
decision-making in VLN-CE tasks. NavMorph employs compact latent
representations to model environmental dynamics, equipping agents with
foresight for adaptive planning and policy refinement. By integrating a novel
Contextual Evolution Memory, NavMorph leverages scene-contextual information to
support effective navigation while maintaining online adaptability. Extensive
experiments demonstrate that our method achieves notable performance
improvements on popular VLN-CE benchmarks. Code is available at
\href{https://github.com/Feliciaxyao/NavMorph}{this https URL}.

</details>


### [137] [Interactive Interface For Semantic Segmentation Dataset Synthesis](https://arxiv.org/abs/2506.23470)
*Ngoc-Do Tran,Minh-Tuan Huynh,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: SynthLab是一个模块化平台，用于合成视觉数据并提供用户友好界面，以解决高质量标注数据集创建的资源密集和隐私问题。


<details>
  <summary>Details</summary>
Motivation: AI和计算机视觉的快速发展对高质量标注数据集的需求增加，但创建这些数据集资源密集且涉及隐私问题。

Method: SynthLab采用模块化架构和用户友好界面，支持拖拽操作定制数据管道，模块化设计便于维护和扩展。

Result: 用户研究表明，SynthLab具有高度灵活性和可访问性，适用于不同背景的用户。

Conclusion: SynthLab为无深厚技术背景的用户提供了高效、灵活的解决方案，推动了AI在现实应用中的普及。

Abstract: The rapid advancement of AI and computer vision has significantly increased
the demand for high-quality annotated datasets, particularly for semantic
segmentation. However, creating such datasets is resource-intensive, requiring
substantial time, labor, and financial investment, and often raises privacy
concerns due to the use of real-world data. To mitigate these challenges, we
present SynthLab, consisting of a modular platform for visual data synthesis
and a user-friendly interface. The modular architecture of SynthLab enables
easy maintenance, scalability with centralized updates, and seamless
integration of new features. Each module handles distinct aspects of computer
vision tasks, enhancing flexibility and adaptability. Meanwhile, its
interactive, user-friendly interface allows users to quickly customize their
data pipelines through drag-and-drop actions. Extensive user studies involving
a diverse range of users across different ages, professions, and expertise
levels, have demonstrated flexible usage, and high accessibility of SynthLab,
enabling users without deep technical expertise to harness AI for real-world
applications.

</details>


### [138] [GeoCD: A Differential Local Approximation for Geodesic Chamfer Distance](https://arxiv.org/abs/2506.23478)
*Pedro Alonso,Tianrui Li,Chongshou Li*

Main category: cs.CV

TL;DR: 提出GeoCD，一种基于拓扑感知和可微分的测地距离近似方法，用于改进3D点云学习中的Chamfer Distance（CD）局限性。


<details>
  <summary>Details</summary>
Motivation: Chamfer Distance（CD）仅依赖欧氏距离，无法捕捉3D形状的内在几何特性，限制了其性能。

Method: 提出GeoCD，一种拓扑感知且完全可微分的测地距离近似方法，替代CD作为3D点云学习的度量标准。

Result: 实验表明，GeoCD在不同架构和数据集上均显著提升了重建质量，仅需单轮微调即可在多指标上取得显著改进。

Conclusion: GeoCD有效解决了CD的局限性，为3D点云学习提供了更优的度量标准。

Abstract: Chamfer Distance (CD) is a widely adopted metric in 3D point cloud learning
due to its simplicity and efficiency. However, it suffers from a fundamental
limitation: it relies solely on Euclidean distances, which often fail to
capture the intrinsic geometry of 3D shapes. To address this limitation, we
propose GeoCD, a topology-aware and fully differentiable approximation of
geodesic distance designed to serve as a metric for 3D point cloud learning.
Our experiments show that GeoCD consistently improves reconstruction quality
over standard CD across various architectures and datasets. We demonstrate this
by fine-tuning several models, initially trained with standard CD, using GeoCD.
Remarkably, fine-tuning for a single epoch with GeoCD yields significant gains
across multiple evaluation metrics.

</details>


### [139] [Instant GaussianImage: A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting](https://arxiv.org/abs/2506.23479)
*Zhaojie Zeng,Yuesong Wang,Chao Yang,Tao Guan,Lili Ju*

Main category: cs.CV

TL;DR: 提出了一种基于2D高斯泼溅的自适应图像表示框架，显著减少训练时间并动态调整高斯点数量。


<details>
  <summary>Details</summary>
Motivation: 解决Implicit Neural Representation (INR)的高GPU资源需求和GaussianImage训练慢、适应性差的问题。

Method: 使用网络快速生成粗略高斯表示，再通过少量微调步骤，动态调整高斯点数量以适应图像复杂度。

Result: 在DIV2K和Kodak数据集上，训练时间减少一个数量级，渲染性能优于GaussianImage。

Conclusion: 该方法在高效性和适应性上优于现有技术，具有实际应用潜力。

Abstract: Implicit Neural Representation (INR) has demonstrated remarkable advances in
the field of image representation but demands substantial GPU resources.
GaussianImage recently pioneered the use of Gaussian Splatting to mitigate this
cost, however, the slow training process limits its practicality, and the fixed
number of Gaussians per image limits its adaptability to varying information
entropy. To address these issues, we propose in this paper a generalizable and
self-adaptive image representation framework based on 2D Gaussian Splatting.
Our method employs a network to quickly generate a coarse Gaussian
representation, followed by minimal fine-tuning steps, achieving comparable
rendering quality of GaussianImage while significantly reducing training time.
Moreover, our approach dynamically adjusts the number of Gaussian points based
on image complexity to further enhance flexibility and efficiency in practice.
Experiments on DIV2K and Kodak datasets show that our method matches or exceeds
GaussianImage's rendering performance with far fewer iterations and shorter
training times. Specifically, our method reduces the training time by up to one
order of magnitude while achieving superior rendering performance with the same
number of Gaussians.

</details>


### [140] [MTADiffusion: Mask Text Alignment Diffusion Model for Object Inpainting](https://arxiv.org/abs/2506.23482)
*Jun Huang,Ting Liu,Yihang Wu,Xiaochao Qu,Luoqi Liu,Xiaolin Hu*

Main category: cs.CV

TL;DR: MTADiffusion是一种用于对象修复的Mask-Text Alignment扩散模型，通过MTAPipeline自动标注掩码描述，构建MTADataset，并采用多任务训练策略和风格一致性损失，显著提升了修复效果。


<details>
  <summary>Details</summary>
Motivation: 现有修复方法存在语义不对齐、结构扭曲和风格不一致的问题，MTADiffusion旨在解决这些问题。

Method: 提出MTAPipeline自动标注掩码描述，构建MTADataset；采用多任务训练策略（修复和边缘预测）；引入风格一致性损失。

Result: 在BrushBench和EditBench上，MTADiffusion表现优于其他方法。

Conclusion: MTADiffusion通过改进语义对齐、结构稳定性和风格一致性，实现了最先进的修复性能。

Abstract: Advancements in generative models have enabled image inpainting models to
generate content within specific regions of an image based on provided prompts
and masks. However, existing inpainting methods often suffer from problems such
as semantic misalignment, structural distortion, and style inconsistency. In
this work, we present MTADiffusion, a Mask-Text Alignment diffusion model
designed for object inpainting. To enhance the semantic capabilities of the
inpainting model, we introduce MTAPipeline, an automatic solution for
annotating masks with detailed descriptions. Based on the MTAPipeline, we
construct a new MTADataset comprising 5 million images and 25 million mask-text
pairs. Furthermore, we propose a multi-task training strategy that integrates
both inpainting and edge prediction tasks to improve structural stability. To
promote style consistency, we present a novel inpainting style-consistency loss
using a pre-trained VGG network and the Gram matrix. Comprehensive evaluations
on BrushBench and EditBench demonstrate that MTADiffusion achieves
state-of-the-art performance compared to other methods.

</details>


### [141] [Qwen-GUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding](https://arxiv.org/abs/2506.23491)
*ZongHan Hsieh,Tzer-Jen Wei*

Main category: cs.CV

TL;DR: Qwen-GUI-3B是一个轻量级视觉语言模型，专为图形用户界面（GUI）任务设计，性能接近更大模型，但可在单GPU上训练。


<details>
  <summary>Details</summary>
Motivation: 解决大规模视觉语言模型计算资源需求高的问题，同时提升GUI任务的性能。

Method: 结合跨平台多分辨率数据集、两阶段微调策略及数据去冗余技术。

Result: 在ScreenSpot和ScreenSpot-v2基准测试中分别达到84.9%和86.4%的准确率。

Conclusion: Qwen-GUI-3B通过高效数据策略和训练方法，实现了高性能且资源友好的GUI任务解决方案。

Abstract: This paper introduces Qwen-GUI-3B, a lightweight Vision-Language Model (VLM)
specifically designed for Graphical User Interface grounding tasks, achieving
performance competitive with significantly larger models. Unlike large-scale
VLMs (>7B parameters) that are computationally intensive and impractical for
consumer-grade hardware, Qwen-GUI-3B delivers strong grounding accuracy while
being fully trainable on a single GPU (RTX 4090). The model incorporates
several key innovations: (i) combine cross-platform, multi-resolution dataset
of 24K examples from diverse sources including mobile, desktop, and web GUI
screenshots to effectively address data scarcity in high-resolution desktop
environments; (ii) a two-stage fine-tuning strategy, where initial
cross-platform training establishes robust GUI understanding, followed by
specialized fine-tuning on high-resolution data to significantly enhance model
adaptability; and (iii) data curation and redundancy reduction strategies,
demonstrating that randomly sampling a smaller subset with reduced redundancy
achieves performance comparable to larger datasets, emphasizing data diversity
over sheer volume. Empirical evaluation on standard GUI grounding
benchmarks-including ScreenSpot, ScreenSpot-v2, and the challenging
ScreenSpot-Pro, highlights Qwen-GUI-3B's exceptional accuracy, achieving 84.9%
on ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B
parameters. Ablation studies validate the critical role of balanced sampling
and two-stage fine-tuning in enhancing robustness, particularly in
high-resolution desktop scenarios. The Qwen-GUI-3B is available at:
https://github.com/Han1018/Qwen-GUI-3B

</details>


### [142] [LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text Matching](https://arxiv.org/abs/2506.23502)
*Mengxiao Tian,Xinxiao Wu,Shuo Yang*

Main category: cs.CV

TL;DR: 本文提出了一种通过LLM增强的动作感知多模态提示调整方法，赋予CLIP细粒度动作级理解能力，以解决其在对象属性和空间关系理解上的不足。


<details>
  <summary>Details</summary>
Motivation: CLIP在图像-文本匹配任务中表现出色，但缺乏对细粒度细节（如对象属性和空间关系）的理解能力，尤其是动作感知能力。

Method: 引入LLM生成的动作相关知识，设计动作三元组提示和动作状态提示，并提出自适应交互模块以聚合视觉特征。

Result: 在两个基准数据集上的实验证明了该方法的有效性。

Conclusion: 通过LLM增强的提示调整方法，显著提升了CLIP在动作感知和细粒度理解方面的性能。

Abstract: Driven by large-scale contrastive vision-language pre-trained models such as
CLIP, recent advancements in the image-text matching task have achieved
remarkable success in representation learning. Due to image-level
visual-language alignment, CLIP falls short in understanding fine-grained
details such as object attributes and spatial relationships between objects.
Recent efforts have attempted to compel CLIP to acquire structured visual
representations by introducing prompt learning to achieve object-level
alignment. While achieving promising results, they still lack the capability to
perceive actions, which are crucial for describing the states or relationships
between objects. Therefore, we propose to endow CLIP with fine-grained
action-level understanding by introducing an LLM-enhanced action-aware
multi-modal prompt-tuning method, incorporating the action-related external
knowledge generated by large language models (LLMs). Specifically, we design an
action triplet prompt and an action state prompt to exploit compositional
semantic knowledge and state-related causal knowledge implicitly stored in
LLMs. Subsequently, we propose an adaptive interaction module to aggregate
attentive visual features conditioned on action-aware prompted knowledge for
establishing discriminative and action-aware visual representations, which
further improves the performance. Comprehensive experimental results on two
benchmark datasets demonstrate the effectiveness of our method.

</details>


### [143] [Improve Underwater Object Detection through YOLOv12 Architecture and Physics-informed Augmentation](https://arxiv.org/abs/2506.23505)
*Tinh Nguyen*

Main category: cs.CV

TL;DR: 该研究通过结合物理增强技术和YOLOv12架构，提升了水下目标检测的性能，特别是在低能见度条件下。


<details>
  <summary>Details</summary>
Motivation: 水下目标检测在自主导航和环境监测中至关重要，但受限于光线衰减和浑浊等问题，现有方法难以实时部署。

Method: 采用YOLOv12架构，结合Residual ELAN块和区域注意力机制，并引入领域特定的增强技术（如湍流自适应模糊和光谱HSV变换）。

Result: 在四个数据集上表现优异，Brackish数据集的mAP达98.30%，FPS为142，遮挡鲁棒性提升18.9%，小目标召回率提升22.4%。

Conclusion: 该研究为水下机器人和保护应用提供了高效且精确的解决方案，并通过消融实验验证了增强策略的关键作用。

Abstract: Underwater object detection is crucial for autonomous navigation,
environmental monitoring, and marine exploration, but it is severely hampered
by light attenuation, turbidity, and occlusion. Current methods balance
accuracy and computational efficiency, but they have trouble deploying in
real-time under low visibility conditions. Through the integration of
physics-informed augmentation techniques with the YOLOv12 architecture, this
study advances underwater detection. With Residual ELAN blocks to preserve
structural features in turbid waters and Area Attention to maintain large
receptive fields for occluded objects while reducing computational complexity.
Underwater optical properties are addressed by domain-specific augmentations
such as turbulence adaptive blurring, biologically grounded occlusion
simulation, and spectral HSV transformations for color distortion. Extensive
tests on four difficult datasets show state-of-the-art performance, with
Brackish data registering 98.30% mAP at 142 FPS. YOLOv12 improves occlusion
robustness by 18.9%, small-object recall by 22.4%, and detection precision by
up to 7.94% compared to previous models. The crucial role of augmentation
strategy is validated by ablation studies. This work offers a precise and
effective solution for conservation and underwater robotics applications.

</details>


### [144] [ViewPoint: Panoramic Video Generation with Pretrained Diffusion Models](https://arxiv.org/abs/2506.23513)
*Zixun Fang,Kai Zhu,Zhiheng Liu,Yu Liu,Wei Zhai,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出了一种利用预训练视角视频模型生成全景视频的新框架，通过ViewPoint map和Pano-Perspective注意力机制，解决了全景数据与视角数据之间的模态差距问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法因全景数据与视角数据之间的模态差距，难以生成高质量全景视频，而视角数据是现代扩散模型的主要训练数据。

Method: 设计了一种名为ViewPoint map的全景表示方法，结合Pano-Perspective注意力机制，利用预训练视角先验捕捉全景空间相关性。

Result: 实验表明，该方法能生成动态性强、空间一致的全景视频，性能优于现有方法。

Conclusion: 提出的框架成功解决了全景视频生成的模态差距问题，实现了最先进的性能。

Abstract: Panoramic video generation aims to synthesize 360-degree immersive videos,
holding significant importance in the fields of VR, world models, and spatial
intelligence. Existing works fail to synthesize high-quality panoramic videos
due to the inherent modality gap between panoramic data and perspective data,
which constitutes the majority of the training data for modern diffusion
models. In this paper, we propose a novel framework utilizing pretrained
perspective video models for generating panoramic videos. Specifically, we
design a novel panorama representation named ViewPoint map, which possesses
global spatial continuity and fine-grained visual details simultaneously. With
our proposed Pano-Perspective attention mechanism, the model benefits from
pretrained perspective priors and captures the panoramic spatial correlations
of the ViewPoint map effectively. Extensive experiments demonstrate that our
method can synthesize highly dynamic and spatially consistent panoramic videos,
achieving state-of-the-art performance and surpassing previous methods.

</details>


### [145] [WAVE: Warp-Based View Guidance for Consistent Novel View Synthesis Using a Single Image](https://arxiv.org/abs/2506.23518)
*Jiwoo Park,Tae Eun Choi,Youngjun Jun,Seong Jae Hwang*

Main category: cs.CV

TL;DR: 提出一种无需额外模块的扩散模型方法，通过自适应注意力操纵和噪声重新初始化提升视图一致性。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在多视图合成中空间连续性不足的问题，避免复杂多步流程的低效性。

Method: 利用视图引导的变形技术，实现训练自由的自适应注意力操纵和噪声重新初始化。

Result: 通过适用于新视图数据集的综合指标框架，验证了方法在多种扩散模型中的视图一致性提升。

Conclusion: 该方法具有广泛适用性，显著提升了扩散模型在视图一致性方面的表现。

Abstract: Generating high-quality novel views of a scene from a single image requires
maintaining structural coherence across different views, referred to as view
consistency. While diffusion models have driven advancements in novel view
synthesis, they still struggle to preserve spatial continuity across views.
Diffusion models have been combined with 3D models to address the issue, but
such approaches lack efficiency due to their complex multi-step pipelines. This
paper proposes a novel view-consistent image generation method which utilizes
diffusion models without additional modules. Our key idea is to enhance
diffusion models with a training-free method that enables adaptive attention
manipulation and noise reinitialization by leveraging view-guided warping to
ensure view consistency. Through our comprehensive metric framework suitable
for novel-view datasets, we show that our method improves view consistency
across various diffusion models, demonstrating its broader applicability.

</details>


### [146] [From Sight to Insight: Unleashing Eye-Tracking in Weakly Supervised Video Salient Object Detection](https://arxiv.org/abs/2506.23519)
*Qi Qin,Runmin Cong,Gen Zhan,Yiting Liao,Sam Kwong*

Main category: cs.CV

TL;DR: 论文提出了一种利用注视信息辅助弱监督下视频显著对象检测的方法，通过PSE模块和SLQ Competitor提升特征学习，并结合IIMC模型优化时空建模能力。


<details>
  <summary>Details</summary>
Motivation: 注视信息更易获取且更符合人眼真实视觉模式，因此论文旨在利用注视信息辅助弱监督下的视频显著对象检测。

Method: 提出PSE模块提供位置和语义指导，设计SLQ Competitor进行特征选择，结合IIMC模型实现时空建模。

Result: 在五个流行的VSOD基准测试中，模型在多种评估指标上优于其他竞争方法。

Conclusion: 通过注视信息的引入和弱监督下的时空建模优化，显著提升了视频显著对象检测的性能。

Abstract: The eye-tracking video saliency prediction (VSP) task and video salient
object detection (VSOD) task both focus on the most attractive objects in video
and show the result in the form of predictive heatmaps and pixel-level saliency
masks, respectively. In practical applications, eye tracker annotations are
more readily obtainable and align closely with the authentic visual patterns of
human eyes. Therefore, this paper aims to introduce fixation information to
assist the detection of video salient objects under weak supervision. On the
one hand, we ponder how to better explore and utilize the information provided
by fixation, and then propose a Position and Semantic Embedding (PSE) module to
provide location and semantic guidance during the feature learning process. On
the other hand, we achieve spatiotemporal feature modeling under weak
supervision from the aspects of feature selection and feature contrast. A
Semantics and Locality Query (SLQ) Competitor with semantic and locality
constraints is designed to effectively select the most matching and accurate
object query for spatiotemporal modeling. In addition, an Intra-Inter Mixed
Contrastive (IIMC) model improves the spatiotemporal modeling capabilities
under weak supervision by forming an intra-video and inter-video contrastive
learning paradigm. Experimental results on five popular VSOD benchmarks
indicate that our model outperforms other competitors on various evaluation
metrics.

</details>


### [147] [Lightweight Temporal Transformer Decomposition for Federated Autonomous Driving](https://arxiv.org/abs/2506.23523)
*Tuong Do,Binh X. Nguyen,Quang D. Tran,Erman Tjiputra,Te-Chuan Chiu,Anh Nguyen*

Main category: cs.CV

TL;DR: 提出轻量级时序Transformer分解方法，通过分解大注意力图为小矩阵，降低模型复杂度，提升自动驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于视觉的自动驾驶系统在复杂环境中依赖单帧图像输入时表现不佳，而现有高性能方法因资源密集难以实用化。

Method: 采用轻量级时序Transformer分解，处理连续图像帧和时序转向数据，分解大注意力图为小矩阵以降低复杂度。

Result: 在三个数据集上表现优于现有方法，实现实时性能，并通过真实机器人实验验证有效性。

Conclusion: 该方法显著提升自动驾驶性能，同时满足实时性和实用性需求。

Abstract: Traditional vision-based autonomous driving systems often face difficulties
in navigating complex environments when relying solely on single-image inputs.
To overcome this limitation, incorporating temporal data such as past image
frames or steering sequences, has proven effective in enhancing robustness and
adaptability in challenging scenarios. While previous high-performance methods
exist, they often rely on resource-intensive fusion networks, making them
impractical for training and unsuitable for federated learning. To address
these challenges, we propose lightweight temporal transformer decomposition, a
method that processes sequential image frames and temporal steering data by
breaking down large attention maps into smaller matrices. This approach reduces
model complexity, enabling efficient weight updates for convergence and
real-time predictions while leveraging temporal information to enhance
autonomous driving performance. Intensive experiments on three datasets
demonstrate that our method outperforms recent approaches by a clear margin
while achieving real-time performance. Additionally, real robot experiments
further confirm the effectiveness of our method.

</details>


### [148] [When Test-Time Adaptation Meets Self-Supervised Models](https://arxiv.org/abs/2506.23529)
*Jisu Han,Jihee Park,Dongyoon Han,Wonjun Hwang*

Main category: cs.CV

TL;DR: 论文提出了一种自监督测试时适应（TTA）协议，通过协作学习框架结合对比学习和知识蒸馏，提升自监督模型在测试时的表现，无需依赖源域预训练。


<details>
  <summary>Details</summary>
Motivation: 研究自监督学习（SSL）模型是否能在不依赖源域预训练的情况下，通过测试时适应（TTA）持续改进性能。

Method: 提出协作学习框架，结合对比学习和知识蒸馏，逐步优化表示。

Result: 在多种自监督模型（如DINO、MoCo、iBOT）上验证了方法的有效性，即使没有源预训练也能取得竞争性表现。

Conclusion: 自监督TTA协议和协作学习框架为动态环境中的模型适应提供了新思路。

Abstract: Training on test-time data enables deep learning models to adapt to dynamic
environmental changes, enhancing their practical applicability. Online
adaptation from source to target domains is promising but it remains highly
reliant on the performance of source pretrained model. In this paper, we
investigate whether test-time adaptation (TTA) methods can continuously improve
models trained via self-supervised learning (SSL) without relying on source
pretraining. We introduce a self-supervised TTA protocol after observing that
existing TTA approaches struggle when directly applied to self-supervised
models with low accuracy on the source domain. Furthermore, we propose a
collaborative learning framework that integrates SSL and TTA models, leveraging
contrastive learning and knowledge distillation for stepwise representation
refinement. We validate our method on diverse self-supervised models, including
DINO, MoCo, and iBOT, across TTA benchmarks. Extensive experiments validate the
effectiveness of our approach in SSL, showing that it achieves competitive
performance even without source pretraining.

</details>


### [149] [GViT: Representing Images as Gaussians for Visual Recognition](https://arxiv.org/abs/2506.23532)
*Jefferson Hernandez,Ruozhen He,Guha Balakrishnan,Alexander C. Berg,Vicente Ordonez*

Main category: cs.CV

TL;DR: GVIT是一种分类框架，用可学习的2D高斯集合替代传统像素或补丁网格输入表示，结合ViT分类器，性能接近传统ViT。


<details>
  <summary>Details</summary>
Motivation: 传统ViT使用像素或补丁网格输入表示，GVIT旨在探索更紧凑的高斯表示，同时保持分类性能。

Method: 图像编码为数百个高斯参数，位置、尺度、方向等与ViT分类器联合优化，利用分类器梯度指导高斯聚焦于类别显著区域。

Result: GVIT在Imagenet-1k上达到76.9% top-1准确率，性能接近传统ViT。

Conclusion: GVIT证明了高斯表示在ViT中的有效性，为输入表示提供了新思路。

Abstract: We introduce GVIT, a classification framework that abandons conventional
pixel or patch grid input representations in favor of a compact set of
learnable 2D Gaussians. Each image is encoded as a few hundred Gaussians whose
positions, scales, orientations, colors, and opacities are optimized jointly
with a ViT classifier trained on top of these representations. We reuse the
classifier gradients as constructive guidance, steering the Gaussians toward
class-salient regions while a differentiable renderer optimizes an image
reconstruction loss. We demonstrate that by 2D Gaussian input representations
coupled with our GVIT guidance, using a relatively standard ViT architecture,
closely matches the performance of a traditional patch-based ViT, reaching a
76.9% top-1 accuracy on Imagenet-1k using a ViT-B architecture.

</details>


### [150] [Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound](https://arxiv.org/abs/2506.23538)
*Yuhao Huang,Yueyue Xu,Haoran Dou,Jiaxiao Deng,Xin Yang,Hongyu Zheng,Dong Ni*

Main category: cs.CV

TL;DR: 提出了一种智能系统，用于同时实现自动平面定位和先天性子宫异常（CUA）诊断，结合去噪扩散模型和强化学习框架，显著提升了诊断效果。


<details>
  <summary>Details</summary>
Motivation: 先天性子宫异常（CUAs）可能导致不孕、流产、早产等并发症，传统2D超声难以准确评估，3D超声虽能提供更清晰的子宫形态，但需要智能系统辅助诊断。

Method: 1) 使用去噪扩散模型，结合局部（平面）和全局（体积/文本）指导；2) 引入基于强化学习的框架，通过无监督奖励提取关键切片；3) 提供文本驱动的不确定性建模，优化分类概率。

Result: 在大规模3D子宫超声数据集上的实验表明，该方法在平面定位和CUA诊断方面效果显著。

Conclusion: 所提出的智能系统通过多模态信息整合和自适应优化，显著提升了CUA诊断的准确性和效率。

Abstract: Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage,
preterm birth, and an increased risk of pregnancy complications. Compared to
traditional 2D ultrasound (US), 3D US can reconstruct the coronal plane,
providing a clear visualization of the uterine morphology for assessing CUAs
accurately. In this paper, we propose an intelligent system for simultaneous
automated plane localization and CUA diagnosis. Our highlights are: 1) we
develop a denoising diffusion model with local (plane) and global (volume/text)
guidance, using an adaptive weighting strategy to optimize attention allocation
to different conditions; 2) we introduce a reinforcement learning-based
framework with unsupervised rewards to extract the key slice summary from
redundant sequences, fully integrating information across multiple planes to
reduce learning difficulty; 3) we provide text-driven uncertainty modeling for
coarse prediction, and leverage it to adjust the classification probability for
overall performance improvement. Extensive experiments on a large 3D uterine US
dataset show the efficacy of our method, in terms of plane localization and CUA
diagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.

</details>


### [151] [Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention](https://arxiv.org/abs/2506.23542)
*Weida Wang,Changyong He,Jin Zeng,Di Qiu*

Main category: cs.CV

TL;DR: 提出了一种基于运动不变图融合的ToF深度去噪网络，通过跨帧几何注意力提升时空一致性，实验表明其在合成和真实数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: ToF传感器捕获的深度图像存在噪声，现有方法未充分考虑跨帧深度变化，导致时空不一致和模糊问题。

Method: 利用运动不变图融合，结合几何注意力和图像平滑先验，构建最大后验问题，并通过迭代滤波器实现去噪。

Result: 在合成DVToF和真实Kinectv2数据集上实现了最先进的准确性和一致性。

Conclusion: 该方法显著提升了ToF深度去噪的时空性能，具有高性能和可解释性。

Abstract: Depth images captured by Time-of-Flight (ToF) sensors are prone to noise,
requiring denoising for reliable downstream applications. Previous works either
focus on single-frame processing, or perform multi-frame processing without
considering depth variations at corresponding pixels across frames, leading to
undesirable temporal inconsistency and spatial ambiguity. In this paper, we
propose a novel ToF depth denoising network leveraging motion-invariant graph
fusion to simultaneously enhance temporal stability and spatial sharpness.
Specifically, despite depth shifts across frames, graph structures exhibit
temporal self-similarity, enabling cross-frame geometric attention for graph
fusion. Then, by incorporating an image smoothness prior on the fused graph and
data fidelity term derived from ToF noise distribution, we formulate a maximum
a posterior problem for ToF denoising. Finally, the solution is unrolled into
iterative filters whose weights are adaptively learned from the graph-informed
geometric attention, producing a high-performance yet interpretable network.
Experimental results demonstrate that the proposed scheme achieves
state-of-the-art performance in terms of accuracy and consistency on synthetic
DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset.
Source code will be released at
\href{https://github.com/davidweidawang/GIGA-ToF}{https://github.com/davidweidawang/GIGA-ToF}.

</details>


### [152] [Pyramidal Patchification Flow for Visual Generation](https://arxiv.org/abs/2506.23543)
*Hui Li,Baoyou Chen,Liwei Zhang,Jiaye Li,Jingdong Wang,Siyu Zhu*

Main category: cs.CV

TL;DR: PPFlow通过动态调整patch大小和线性投影，优化了Diffusion Transformers的计算成本，同时保持了图像生成性能。


<details>
  <summary>Details</summary>
Motivation: 传统DiTs在所有时间步使用固定patch大小，计算成本较高。PPFlow旨在通过动态调整patch大小来优化计算效率。

Method: 提出Pyramidal Patchification Flow (PPFlow)，在高噪声时间步使用大patch，低噪声时间步使用小patch，并学习每个patch大小的线性投影。

Result: PPFlow在训练和推理速度上显著提升（1.6×-2.0×），同时保持相似的图像生成性能。

Conclusion: PPFlow是一种高效且性能优越的DiTs优化方法，适用于从头训练或基于预训练模型的微调。

Abstract: Diffusion transformers (DiTs) adopt Patchify, mapping patch representations
to token representations through linear projections, to adjust the number of
tokens input to DiT blocks and thus the computation cost. Instead of a single
patch size for all the timesteps, we introduce a Pyramidal Patchification Flow
(PPFlow) approach: Large patch sizes are used for high noise timesteps and
small patch sizes for low noise timesteps; Linear projections are learned for
each patch size; and Unpatchify is accordingly modified. Unlike Pyramidal Flow,
our approach operates over full latent representations other than pyramid
representations, and adopts the normal denoising process without requiring the
renoising trick. We demonstrate the effectiveness of our approach through two
training manners. Training from scratch achieves a $1.6\times$ ($2.0\times$)
inference speed over SiT-B/2 for 2-level (3-level) pyramid patchification with
slightly lower training FLOPs and similar image generation performance.
Training from pretrained normal DiTs achieves even better performance with
small training time. The code and checkpoint are at
https://github.com/fudan-generative-vision/PPFlow.

</details>


### [153] [Oneta: Multi-Style Image Enhancement Using Eigentransformation Functions](https://arxiv.org/abs/2506.23547)
*Jiwon Kim,Soohyun Hwang,Dong-O Kim,Changsu Han,Min Kyu Park,Chang-Su Kim*

Main category: cs.CV

TL;DR: 提出了一种名为Oneta的多风格图像增强算法，通过两步操作（强度增强和色彩校正）实现高性能，支持多种风格任务。


<details>
  <summary>Details</summary>
Motivation: 解决多风格图像增强任务，通过简单但高效的两步模型实现多种图像处理需求。

Method: 使用Y-Net和C-Net分别预测eigenTF和CCM参数，通过K个可学习令牌支持K种风格。

Result: 实验表明，Oneta能有效完成六种图像增强任务，覆盖30个数据集。

Conclusion: Oneta是一种高效且通用的多风格图像增强方法。

Abstract: The first algorithm, called Oneta, for a novel task of multi-style image
enhancement is proposed in this work. Oneta uses two point operators
sequentially: intensity enhancement with a transformation function (TF) and
color correction with a color correction matrix (CCM). This two-step
enhancement model, though simple, achieves a high performance upper bound.
Also, we introduce eigentransformation function (eigenTF) to represent TF
compactly. The Oneta network comprises Y-Net and C-Net to predict eigenTF and
CCM parameters, respectively. To support $K$ styles, Oneta employs $K$
learnable tokens. During training, each style token is learned using image
pairs from the corresponding dataset. In testing, Oneta selects one of the $K$
style tokens to enhance an image accordingly. Extensive experiments show that
the single Oneta network can effectively undertake six enhancement tasks --
retouching, image signal processing, low-light image enhancement, dehazing,
underwater image enhancement, and white balancing -- across 30 datasets.

</details>


### [154] [JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching](https://arxiv.org/abs/2506.23552)
*Mingi Kwon,Joonghyuk Shin,Jaeseok Jung,Jaesik Park,Youngjung Uh*

Main category: cs.CV

TL;DR: JAM-Flow是一个统一框架，同时合成面部运动和语音，利用流匹配和多模态扩散Transformer架构，支持多种输入条件。


<details>
  <summary>Details</summary>
Motivation: 面部运动与语音的内在联系在生成建模中常被忽视，现有方法通常将它们作为独立任务处理。

Method: 采用流匹配和MM-DiT架构，结合Motion-DiT和Audio-DiT模块，通过选择性联合注意力层实现跨模态交互。

Result: JAM-Flow支持多种输入条件（如文本、参考音频和运动），实现同步的说话头部生成和音频驱动动画等任务。

Conclusion: JAM-Flow为多模态生成建模提供了实用解决方案，推动了音频-视觉合成的整体发展。

Abstract: The intrinsic link between facial motion and speech is often overlooked in
generative modeling, where talking head synthesis and text-to-speech (TTS) are
typically addressed as separate tasks. This paper introduces JAM-Flow, a
unified framework to simultaneously synthesize and condition on both facial
motion and speech. Our approach leverages flow matching and a novel Multi-Modal
Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT
and Audio-DiT modules. These are coupled via selective joint attention layers
and incorporate key architectural choices, such as temporally aligned
positional embeddings and localized joint attention masking, to enable
effective cross-modal interaction while preserving modality-specific strengths.
Trained with an inpainting-style objective, JAM-Flow supports a wide array of
conditioning inputs-including text, reference audio, and reference
motion-facilitating tasks such as synchronized talking head generation from
text, audio-driven animation, and much more, within a single, coherent model.
JAM-Flow significantly advances multi-modal generative modeling by providing a
practical solution for holistic audio-visual synthesis. project page:
https://joonghyuk.com/jamflow-web

</details>


### [155] [LH2Face: Loss function for Hard High-quality Face](https://arxiv.org/abs/2506.23555)
*Fan Xie,Pan Cao*

Main category: cs.CV

TL;DR: 提出了一种名为LH2Face的新型损失函数，通过自适应边缘和代理约束优化高质量硬样本的人脸识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于余弦相似度和softmax分类的人脸识别方法在处理硬样本时表现不佳，且未考虑人脸质量或识别难度。

Method: 结合vMF分布的相似性度量、自适应边缘的softmax分类、代理约束损失函数以及人脸重建渲染器。

Result: 在IJB-B数据集上达到49.39%的准确率，优于第二名2.37%。

Conclusion: LH2Face通过自适应策略和代理约束显著提升了高质量硬样本的人脸识别性能。

Abstract: In current practical face authentication systems, most face recognition (FR)
algorithms are based on cosine similarity with softmax classification. Despite
its reliable classification performance, this method struggles with hard
samples. A popular strategy to improve FR performance is incorporating angular
or cosine margins. However, it does not take face quality or recognition
hardness into account, simply increasing the margin value and thus causing an
overly uniform training strategy. To address this problem, a novel loss
function is proposed, named Loss function for Hard High-quality Face (LH2Face).
Firstly, a similarity measure based on the von Mises-Fisher (vMF) distribution
is stated, specifically focusing on the logarithm of the Probability Density
Function (PDF), which represents the distance between a probability
distribution and a vector. Then, an adaptive margin-based multi-classification
method using softmax, called the Uncertainty-Aware Margin Function, is
implemented in the article. Furthermore, proxy-based loss functions are used to
apply extra constraints between the proxy and sample to optimize their
representation space distribution. Finally, a renderer is constructed that
optimizes FR through face reconstruction and vice versa. Our LH2Face is
superior to similiar schemes on hard high-quality face datasets, achieving
49.39% accuracy on the IJB-B dataset, which surpasses the second-place method
by 2.37%.

</details>


### [156] [OcRFDet: Object-Centric Radiance Fields for Multi-View 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2506.23565)
*Mingqian Ji,Jian Yang,Shanshan Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种基于物体中心辐射场（OcRF）的多视角3D物体检测方法，通过聚焦前景物体并剔除背景噪声，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过深度估计或3D位置编码将2D特征隐式转换为3D空间，限制了检测性能。受辐射场在3D重建中的成功启发，作者尝试将其用于增强3D几何估计能力。

Method: 提出物体中心辐射场（OcRF），专注于前景物体建模，并通过高度感知不透明度注意力（HOA）增强2D BEV特征。

Result: 在nuScenes测试基准上，OcRFDet以57.2% mAP和64.8% NDS超越了现有最优方法。

Conclusion: OcRFDet通过结合辐射场和注意力机制，显著提升了多视角3D物体检测的性能。

Abstract: Current multi-view 3D object detection methods typically transfer 2D features
into 3D space using depth estimation or 3D position encoder, but in a fully
data-driven and implicit manner, which limits the detection performance.
Inspired by the success of radiance fields on 3D reconstruction, we assume they
can be used to enhance the detector's ability of 3D geometry estimation.
However, we observe a decline in detection performance, when we directly use
them for 3D rendering as an auxiliary task. From our analysis, we find the
performance drop is caused by the strong responses on the background when
rendering the whole scene. To address this problem, we propose object-centric
radiance fields, focusing on modeling foreground objects while discarding
background noises. Specifically, we employ Object-centric Radiance Fields
(OcRF) to enhance 3D voxel features via an auxiliary task of rendering
foreground objects. We further use opacity - the side-product of rendering- to
enhance the 2D foreground BEV features via Height-aware Opacity-based Attention
(HOA), where attention maps at different height levels are generated separately
via multiple networks in parallel. Extensive experiments on the nuScenes
validation and test datasets demonstrate that our OcRFDet achieves superior
performance, outperforming previous state-of-the-art methods with 57.2$\%$ mAP
and 64.8$\%$ NDS on the nuScenes test benchmark. Code will be available at
https://github.com/Mingqj/OcRFDet.

</details>


### [157] [Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image Super Resolution](https://arxiv.org/abs/2506.23566)
*Luigi Sigillo,Renato Giamba,Danilo Comminiello*

Main category: cs.CV

TL;DR: MWT-Diff是一种结合潜在扩散模型和小波变换的卫星图像超分辨率框架，通过MWT-Encoder生成嵌入特征，逐步重建高分辨率图像，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高分辨率卫星图像获取受限于传感器时空限制和高成本，影响环境监测等应用，需解决数据精细化和高分辨率需求。

Method: 提出MWT-Diff框架，结合潜在扩散模型和小波变换，利用MWT-Encoder生成嵌入特征，指导分层扩散动态重建高分辨率图像。

Result: 在多个数据集上表现优于现有方法，通过FID和LPIPS等感知质量指标验证。

Conclusion: MWT-Diff能有效重建高分辨率卫星图像，保留关键空间特征，适用于遥感分析。

Abstract: The acquisition of high-resolution satellite imagery is often constrained by
the spatial and temporal limitations of satellite sensors, as well as the high
costs associated with frequent observations. These challenges hinder
applications such as environmental monitoring, disaster response, and
agricultural management, which require fine-grained and high-resolution data.
In this paper, we propose MWT-Diff, an innovative framework for satellite image
super-resolution (SR) that combines latent diffusion models with wavelet
transforms to address these challenges. At the core of the framework is a novel
metadata-, wavelet-, and time-aware encoder (MWT-Encoder), which generates
embeddings that capture metadata attributes, multi-scale frequency information,
and temporal relationships. The embedded feature representations steer the
hierarchical diffusion dynamics, through which the model progressively
reconstructs high-resolution satellite imagery from low-resolution inputs. This
process preserves critical spatial characteristics including textural patterns,
boundary discontinuities, and high-frequency spectral components essential for
detailed remote sensing analysis. The comparative analysis of MWT-Diff across
multiple datasets demonstrated favorable performance compared to recent
approaches, as measured by standard perceptual quality metrics including FID
and LPIPS.

</details>


### [158] [Event-based Tiny Object Detection: A Benchmark Dataset and Baseline](https://arxiv.org/abs/2506.23575)
*Nuo Chen,Chao Xiao,Yimian Dai,Shiman He,Miao Li,Wei An*

Main category: cs.CV

TL;DR: 论文提出了首个大规模、多样化的基于事件的小目标检测数据集EV-UAV，并提出了EV-SpSegNet方法和STC损失函数，用于解决反无人机任务中的小目标检测问题。


<details>
  <summary>Details</summary>
Motivation: 传统帧式相机在复杂背景下检测小目标（如无人机）效果不佳，而事件相机的高动态范围和微秒级分辨率更适合此类任务，但现有数据集缺乏多样性和小目标标注。

Method: 提出EV-UAV数据集，包含147个序列和230万事件级标注；设计EV-SpSegNet网络和STC损失函数，利用时空事件点云中的运动连续性优化目标检测。

Result: 在EV-UAV数据集上的实验验证了方法的优越性，为未来研究提供了基准。

Conclusion: EV-UAV数据集和EV-SpSegNet方法为基于事件的小目标检测提供了有效解决方案和基准。

Abstract: Small object detection (SOD) in anti-UAV task is a challenging problem due to
the small size of UAVs and complex backgrounds. Traditional frame-based cameras
struggle to detect small objects in complex environments due to their low frame
rates, limited dynamic range, and data redundancy. Event cameras, with
microsecond temporal resolution and high dynamic range, provide a more
effective solution for SOD. However, existing event-based object detection
datasets are limited in scale, feature large targets size, and lack diverse
backgrounds, making them unsuitable for SOD benchmarks. In this paper, we
introduce a Event-based Small object detection (EVSOD) dataset (namely EV-UAV),
the first large-scale, highly diverse benchmark for anti-UAV tasks. It includes
147 sequences with over 2.3 million event-level annotations, featuring
extremely small targets (averaging 6.8 $\times$ 5.4 pixels) and diverse
scenarios such as urban clutter and extreme lighting conditions. Furthermore,
based on the observation that small moving targets form continuous curves in
spatiotemporal event point clouds, we propose Event based Sparse Segmentation
Network (EV-SpSegNet), a novel baseline for event segmentation in point cloud
space, along with a Spatiotemporal Correlation (STC) loss that leverages motion
continuity to guide the network in retaining target events. Extensive
experiments on the EV-UAV dataset demonstrate the superiority of our method and
provide a benchmark for future research in EVSOD. The dataset and code are at
https://github.com/ChenYichen9527/Ev-UAV.

</details>


### [159] [StackCLIP: Clustering-Driven Stacked Prompt in Zero-Shot Industrial Anomaly Detection](https://arxiv.org/abs/2506.23577)
*Yanning Hou,Yanran Ruan,Junfa Li,Shanshan Wang,Jianfeng Qiu,Ke Xu*

Main category: cs.CV

TL;DR: 论文提出StackCLIP模型，通过多类别名称堆叠生成堆叠提示，提升CLIP模型在零样本工业异常检测任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在预训练中使用特定类别提示，易导致过拟合和泛化能力受限。

Method: 提出Clustering-Driven Stacked Prompts (CSP)模块和Ensemble Feature Alignment (EFA)模块，结合Regulating Prompt Learning (RPL)模块优化提示学习。

Result: 在七个工业异常检测数据集上实现零样本异常检测和分割任务的最优性能。

Conclusion: StackCLIP模型显著提升训练速度、稳定性和收敛性，同时具备强大的分类任务泛化能力。

Abstract: Enhancing the alignment between text and image features in the CLIP model is
a critical challenge in zero-shot industrial anomaly detection tasks. Recent
studies predominantly utilize specific category prompts during pretraining,
which can cause overfitting to the training categories and limit model
generalization. To address this, we propose a method that transforms category
names through multicategory name stacking to create stacked prompts, forming
the basis of our StackCLIP model. Our approach introduces two key components.
The Clustering-Driven Stacked Prompts (CSP) module constructs generic prompts
by stacking semantically analogous categories, while utilizing multi-object
textual feature fusion to amplify discriminative anomalies among similar
objects. The Ensemble Feature Alignment (EFA) module trains knowledge-specific
linear layers tailored for each stack cluster and adaptively integrates them
based on the attributes of test categories. These modules work together to
deliver superior training speed, stability, and convergence, significantly
boosting anomaly segmentation performance. Additionally, our stacked prompt
framework offers robust generalization across classification tasks. To further
improve performance, we introduce the Regulating Prompt Learning (RPL) module,
which leverages the generalization power of stacked prompts to refine prompt
learning, elevating results in anomaly detection classification tasks.
Extensive testing on seven industrial anomaly detection datasets demonstrates
that our method achieves state-of-the-art performance in both zero-shot anomaly
detection and segmentation tasks.

</details>


### [160] [Dataset Distillation via Vision-Language Category Prototype](https://arxiv.org/abs/2506.23580)
*Yawen Zou,Guang Li,Duo Su,Zi Wang,Jun Yu,Chao Zhang*

Main category: cs.CV

TL;DR: 该研究提出了一种结合视觉语言方法的数据集蒸馏技术，通过引入文本原型来增强语义信息，提升蒸馏性能。


<details>
  <summary>Details</summary>
Motivation: 传统数据集蒸馏方法主要关注图像信息，忽略了语义信息，导致模型泛化能力不足。

Method: 结合视觉语言方法，利用开源大语言模型生成的文本原型与图像原型协同合成数据。

Result: 该方法生成的图像逻辑一致且包含目标对象，验证性能达到最优，并展现出强大的泛化能力。

Conclusion: 该框架扩展了数据集蒸馏的应用范围，适用于无文本描述的数据集，为传统图像方法提供了新思路。

Abstract: Dataset distillation (DD) condenses large datasets into compact yet
informative substitutes, preserving performance comparable to the original
dataset while reducing storage, transmission costs, and computational
consumption. However, previous DD methods mainly focus on distilling
information from images, often overlooking the semantic information inherent in
the data. The disregard for context hinders the model's generalization ability,
particularly in tasks involving complex datasets, which may result in illogical
outputs or the omission of critical objects. In this study, we integrate
vision-language methods into DD by introducing text prototypes to distill
language information and collaboratively synthesize data with image prototypes,
thereby enhancing dataset distillation performance. Notably, the text
prototypes utilized in this study are derived from descriptive text information
generated by an open-source large language model. This framework demonstrates
broad applicability across datasets without pre-existing text descriptions,
expanding the potential of dataset distillation beyond traditional image-based
approaches. Compared to other methods, the proposed approach generates
logically coherent images containing target objects, achieving state-of-the-art
validation performance and demonstrating robust generalization. Source code and
generated data are available in
https://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/

</details>


### [161] [PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection](https://arxiv.org/abs/2506.23581)
*Xiao Li,Yiming Zhu,Yifan Huang,Wei Zhang,Yingzhe He,Jie Shi,Xiaolin Hu*

Main category: cs.CV

TL;DR: 论文提出了一种针对物理可实现攻击的统一防御方法PBCAT，通过结合小区域梯度引导对抗补丁和全局对抗扰动，显著提升了目标检测器的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 物理可实现攻击（如对抗补丁和纹理）对目标检测器构成现实威胁，而现有对抗训练方法主要针对分类模型，对目标检测器的防御研究不足。

Method: 提出PBCAT策略，结合小区域对抗补丁和全局对抗扰动进行对抗训练。

Result: PBCAT显著提升了模型对多种物理可实现攻击的鲁棒性，检测准确率在对抗纹理攻击下提升了29.7%。

Conclusion: PBCAT是一种有效的统一防御方法，能够应对多种物理可实现攻击，为安全敏感应用提供了更强的保护。

Abstract: Object detection plays a crucial role in many security-sensitive
applications. However, several recent studies have shown that object detectors
can be easily fooled by physically realizable attacks, \eg, adversarial patches
and recent adversarial textures, which pose realistic and urgent threats.
Adversarial Training (AT) has been recognized as the most effective defense
against adversarial attacks. While AT has been extensively studied in the
$l_\infty$ attack settings on classification models, AT against physically
realizable attacks on object detectors has received limited exploration. Early
attempts are only performed to defend against adversarial patches, leaving AT
against a wider range of physically realizable attacks under-explored. In this
work, we consider defending against various physically realizable attacks with
a unified AT method. We propose PBCAT, a novel Patch-Based Composite
Adversarial Training strategy. PBCAT optimizes the model by incorporating the
combination of small-area gradient-guided adversarial patches and imperceptible
global adversarial perturbations covering the entire image. With these designs,
PBCAT has the potential to defend against not only adversarial patches but also
unseen physically realizable attacks such as adversarial textures. Extensive
experiments in multiple settings demonstrated that PBCAT significantly improved
robustness against various physically realizable attacks over state-of-the-art
defense methods. Notably, it improved the detection accuracy by 29.7\% over
previous defense methods under one recent adversarial texture attack.

</details>


### [162] [CAI: Caption-Sensitive Attention Intervention for Mitigating Object Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2506.23590)
*Qiming Li,Zekai Ye,Xiaocheng Feng,Weihong Zhong,Libo Qin,Ruihan Chen,Baohang Li,Kui Jiang,Yaowei Wang,Ting Liu,Bing Qin*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练、即插即用的方法CAI，通过利用大视觉语言模型在回答标题查询时的注意力模式，减少视觉信息偏差问题。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在解释视觉信息时经常产生与视觉内容不符的输出，即对象幻觉问题。现有方法依赖昂贵的人工标注和训练成本，或增加推理时间。

Method: 提出Caption-sensitive Attention Intervention (CAI)，利用模型在回答标题查询时的注意力激活模式，增强视觉感知能力。

Result: 在四个基准测试中，CAI在生成和判别任务上均达到最先进的幻觉缓解效果，且仅增加少量推理成本。

Conclusion: CAI是一种高效、低成本的方法，显著改善了大视觉语言模型的视觉感知能力。

Abstract: Although Large Vision-Language Models (LVLMs) have demonstrated powerful
capabilities in interpreting visual information, they frequently produce
content that deviates from visual information, leading to object hallucination.
To tackle this, recent works mostly depend on expensive manual annotations and
training cost, or significantly increase inference time. In this work, we
observe that LVLMs' attention to visual information is significantly stronger
when answering caption queries compared to non-caption queries. Inspired by
this phenomenon, we propose Caption-sensitive Attention Intervention (CAI), a
training-free, plug-and-play hallucination mitigation method that leverages the
attention activation pattern in response to caption queries to enhance LVLMs'
visual perception capability. Extensive experimental results across four
benchmarks covering both discriminative and generative tasks, demonstrate that
CAI achieves state-of-the-art (SOTA) hallucination mitigating performance only
with minimal additional inference cost.

</details>


### [163] [AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval](https://arxiv.org/abs/2506.23605)
*Suyash Maniyar,Vishvesh Trivedi,Ajoy Mondal,Anand Mishra,C. V. Jawahar*

Main category: cs.CV

TL;DR: 论文提出了一种基于大语言模型（LLM）的合成幻灯片生成方法SynLecSlideGen，用于解决幻灯片元素检测和检索任务中标注数据不足的问题。实验表明，合成数据能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决幻灯片理解任务中需要大量人工标注数据的痛点，降低标注成本。

Method: 提出SynLecSlideGen，利用LLM生成高质量合成幻灯片；创建RealSlide基准数据集进行验证。

Result: 实验显示，使用合成数据预训练的模型在少量真实数据上微调后性能显著提升。

Conclusion: 合成数据能有效弥补真实标注数据的不足，提升模型性能。

Abstract: Lecture slide element detection and retrieval are key problems in slide
understanding. Training effective models for these tasks often depends on
extensive manual annotation. However, annotating large volumes of lecture
slides for supervised training is labor intensive and requires domain
expertise. To address this, we propose a large language model (LLM)-guided
synthetic lecture slide generation pipeline, SynLecSlideGen, which produces
high-quality, coherent and realistic slides. We also create an evaluation
benchmark, namely RealSlide by manually annotating 1,050 real lecture slides.
To assess the utility of our synthetic slides, we perform few-shot transfer
learning on real data using models pre-trained on them. Experimental results
show that few-shot transfer learning with pretraining on synthetic slides
significantly improves performance compared to training only on real data. This
demonstrates that synthetic data can effectively compensate for limited labeled
lecture slides. The code and resources of our work are publicly available on
our project website: https://synslidegen.github.io/.

</details>


### [164] [SG-LDM: Semantic-Guided LiDAR Generation via Latent-Aligned Diffusion](https://arxiv.org/abs/2506.23606)
*Zhengkang Xiang,Zizhao Li,Amir Khodabandeh,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: SG-LDM是一种基于语义引导的激光雷达扩散模型，通过潜在对齐实现语义到激光雷达的合成，显著提升了激光雷达点云的生成质量，并提出了首个基于扩散的激光雷达翻译框架。


<details>
  <summary>Details</summary>
Motivation: 解决现有激光雷达点云生成方法缺乏语义引导的问题，提升生成点云的多样性和实用性。

Method: 提出SG-LDM模型，利用潜在对齐和显式语义条件，直接在激光雷达空间操作，实现高质量的语义到点云合成。

Result: SG-LDM在生成高保真激光雷达点云方面表现优异，其翻译框架进一步提升了下游任务的性能。

Conclusion: SG-LDM及其翻译框架为激光雷达点云生成和领域适应提供了高效解决方案。

Abstract: Lidar point cloud synthesis based on generative models offers a promising
solution to augment deep learning pipelines, particularly when real-world data
is scarce or lacks diversity. By enabling flexible object manipulation, this
synthesis approach can significantly enrich training datasets and enhance
discriminative models. However, existing methods focus on unconditional lidar
point cloud generation, overlooking their potential for real-world
applications. In this paper, we propose SG-LDM, a Semantic-Guided Lidar
Diffusion Model that employs latent alignment to enable robust
semantic-to-lidar synthesis. By directly operating in the native lidar space
and leveraging explicit semantic conditioning, SG-LDM achieves state-of-the-art
performance in generating high-fidelity lidar point clouds guided by semantic
labels. Moreover, we propose the first diffusion-based lidar translation
framework based on SG-LDM, which enables cross-domain translation as a domain
adaptation strategy to enhance downstream perception performance. Systematic
experiments demonstrate that SG-LDM significantly outperforms existing lidar
diffusion models and the proposed lidar translation framework further improves
data augmentation performance in the downstream lidar segmentation task.

</details>


### [165] [PGOV3D: Open-Vocabulary 3D Semantic Segmentation with Partial-to-Global Curriculum](https://arxiv.org/abs/2506.23607)
*Shiqi Zhang,Sha Zhang,Jiajun Deng,Yedong Shen,Mingxiao MA,Yanyong Zhang*

Main category: cs.CV

TL;DR: PGOV3D提出了一种新的两阶段训练框架，通过部分到全局的课程学习提升开放词汇3D语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅将多视角图像作为开放词汇信息传递的中介，忽略了其丰富的语义内容和跨视角对应关系，限制了模型效果。

Method: 采用两阶段训练：第一阶段在提供密集语义信息的局部场景上预训练，利用多模态大语言模型和2D分割基础模型生成开放词汇标签；第二阶段在完整场景点云上微调，通过伪标签桥接语义差距。

Result: 在ScanNet、ScanNet200和S3DIS基准测试中表现出色。

Conclusion: PGOV3D通过部分到全局的课程学习和跨视角一致性模块，显著提升了开放词汇3D语义分割的效果。

Abstract: Existing open-vocabulary 3D semantic segmentation methods typically supervise
3D segmentation models by merging text-aligned features (e.g., CLIP) extracted
from multi-view images onto 3D points. However, such approaches treat
multi-view images merely as intermediaries for transferring open-vocabulary
information, overlooking their rich semantic content and cross-view
correspondences, which limits model effectiveness. To address this, we propose
PGOV3D, a novel framework that introduces a Partial-to-Global curriculum for
improving open-vocabulary 3D semantic segmentation. The key innovation lies in
a two-stage training strategy. In the first stage, we pre-train the model on
partial scenes that provide dense semantic information but relatively simple
geometry. These partial point clouds are derived from multi-view RGB-D inputs
via pixel-wise depth projection. To enable open-vocabulary learning, we
leverage a multi-modal large language model (MLLM) and a 2D segmentation
foundation model to generate open-vocabulary labels for each viewpoint,
offering rich and aligned supervision. An auxiliary inter-frame consistency
module is introduced to enforce feature consistency across varying viewpoints
and enhance spatial understanding. In the second stage, we fine-tune the model
on complete scene-level point clouds, which are sparser and structurally more
complex. We aggregate the partial vocabularies associated with each scene and
generate pseudo labels using the pre-trained model, effectively bridging the
semantic gap between dense partial observations and large-scale 3D
environments. Extensive experiments on ScanNet, ScanNet200, and S3DIS
benchmarks demonstrate that PGOV3D achieves competitive performance in
open-vocabulary 3D semantic segmentation.

</details>


### [166] [AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention](https://arxiv.org/abs/2506.23611)
*Ziao Liu,Zhenjia Li,Yifeng Shi,Xiangang Li*

Main category: cs.CV

TL;DR: AttentionGS 提出了一种新框架，通过结构注意力直接从随机初始化进行 3D 重建，解决了 3DGS 依赖高质量点云的局限性。


<details>
  <summary>Details</summary>
Motivation: 3DGS 依赖 SfM 生成的高质量点云，在纹理缺失或视角受限的场景中表现不佳，限制了其应用范围。

Method: AttentionGS 结合几何注意力（早期训练）和纹理注意力（后期训练），并利用不透明度加权梯度指导高斯密度化。

Result: 实验表明，AttentionGS 在点云初始化不可靠的场景中显著优于现有方法。

Conclusion: AttentionGS 为 3DGS 在现实应用中提供了更鲁棒和灵活的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance
Fields (NeRF), excelling in complex scene reconstruction and efficient
rendering. However, it relies on high-quality point clouds from
Structure-from-Motion (SfM), limiting its applicability. SfM also fails in
texture-deficient or constrained-view scenarios, causing severe degradation in
3DGS reconstruction. To address this limitation, we propose AttentionGS, a
novel framework that eliminates the dependency on high-quality initial point
clouds by leveraging structural attention for direct 3D reconstruction from
randomly initialization. In the early training stage, we introduce geometric
attention to rapidly recover the global scene structure. As training
progresses, we incorporate texture attention to refine fine-grained details and
enhance rendering quality. Furthermore, we employ opacity-weighted gradients to
guide Gaussian densification, leading to improved surface reconstruction.
Extensive experiments on multiple benchmark datasets demonstrate that
AttentionGS significantly outperforms state-of-the-art methods, particularly in
scenarios where point cloud initialization is unreliable. Our approach paves
the way for more robust and flexible 3D Gaussian Splatting in real-world
applications.

</details>


### [167] [TurboVSR: Fantastic Video Upscalers and Where to Find Them](https://arxiv.org/abs/2506.23618)
*Zhongdao Wang,Guodongfang Zhao,Jingjing Ren,Bailan Feng,Shifeng Zhang,Wenbo Li*

Main category: cs.CV

TL;DR: TurboVSR是一种基于扩散模型的超高效视频超分辨率方法，通过高压缩比自动编码器、因子化条件和快捷模型设计，实现了100倍以上的加速，同时保持与现有方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的视频超分辨率方法在细节生成上表现优异，但计算效率低下，处理短视频耗时过长。

Method: 采用高压缩比自动编码器减少token数量；引入因子化条件降低学习复杂度；将预训练扩散模型转换为快捷模型以减少采样步骤。

Result: TurboVSR在性能上与现有方法相当，但速度快100倍以上，处理2秒1080p视频仅需7秒，并支持4K图像超分辨率。

Conclusion: TurboVSR的高效设计为视频和图像超分辨率提供了实用解决方案，尤其适用于高分辨率场景。

Abstract: Diffusion-based generative models have demonstrated exceptional promise in
the video super-resolution (VSR) task, achieving a substantial advancement in
detail generation relative to prior methods. However, these approaches face
significant computational efficiency challenges. For instance, current
techniques may require tens of minutes to super-resolve a mere 2-second, 1080p
video. In this paper, we present TurboVSR, an ultra-efficient diffusion-based
video super-resolution model. Our core design comprises three key aspects: (1)
We employ an autoencoder with a high compression ratio of 32$\times$32$\times$8
to reduce the number of tokens. (2) Highly compressed latents pose substantial
challenges for training. We introduce factorized conditioning to mitigate the
learning complexity: we first learn to super-resolve the initial frame;
subsequently, we condition the super-resolution of the remaining frames on the
high-resolution initial frame and the low-resolution subsequent frames. (3) We
convert the pre-trained diffusion model to a shortcut model to enable fewer
sampling steps, further accelerating inference. As a result, TurboVSR performs
on par with state-of-the-art VSR methods, while being 100+ times faster, taking
only 7 seconds to process a 2-second long 1080p video. TurboVSR also supports
image resolution by considering image as a one-frame video. Our efficient
design makes SR beyond 1080p possible, results on 4K (3648$\times$2048) image
SR show surprising fine details.

</details>


### [168] [Revisiting Audio-Visual Segmentation with Vision-Centric Transformer](https://arxiv.org/abs/2506.23623)
*Shaofei Huang,Rui Ling,Tianrui Hui,Hongyu Li,Xu Zhou,Shifeng Zhang,Si Liu,Richang Hong,Meng Wang*

Main category: cs.CV

TL;DR: 提出了一种新的视觉中心Transformer（VCT）框架，通过视觉驱动的查询解决音频中心方法的感知模糊和视觉细节丢失问题。


<details>
  <summary>Details</summary>
Motivation: 音频中心Transformer在音频视觉分割中存在感知模糊和视觉细节丢失的局限性。

Method: 采用视觉中心Transformer框架，结合原型提示查询生成模块（PPQG），通过视觉驱动的查询迭代获取音频和视觉信息。

Result: 在AVSBench数据集的三个子集上实现了新的最先进性能。

Conclusion: VCT框架有效解决了音频中心方法的局限性，提升了音频视觉分割的准确性。

Abstract: Audio-Visual Segmentation (AVS) aims to segment sound-producing objects in
video frames based on the associated audio signal. Prevailing AVS methods
typically adopt an audio-centric Transformer architecture, where object queries
are derived from audio features. However, audio-centric Transformers suffer
from two limitations: perception ambiguity caused by the mixed nature of audio,
and weakened dense prediction ability due to visual detail loss. To address
these limitations, we propose a new Vision-Centric Transformer (VCT) framework
that leverages vision-derived queries to iteratively fetch corresponding audio
and visual information, enabling queries to better distinguish between
different sounding objects from mixed audio and accurately delineate their
contours. Additionally, we also introduce a Prototype Prompted Query Generation
(PPQG) module within our VCT framework to generate vision-derived queries that
are both semantically aware and visually rich through audio prototype prompting
and pixel context grouping, facilitating audio-visual information aggregation.
Extensive experiments demonstrate that our VCT framework achieves new
state-of-the-art performances on three subsets of the AVSBench dataset. The
code is available at https://github.com/spyflying/VCT_AVS.

</details>


### [169] [Brain Tumor Detection through Thermal Imaging and MobileNET](https://arxiv.org/abs/2506.23627)
*Roham Maiti,Debasmita Bhoumik*

Main category: cs.CV

TL;DR: 该研究提出了一种基于MobileNET的高效脑肿瘤检测方法，通过减少计算资源和时间需求，实现了98.5%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤对人类健康构成重大威胁，传统检测方法成本高且依赖专业医疗知识，机器学习方法虽有效但存在计算量大、数据需求多等问题。

Method: 使用MobileNET模型结合图像处理技术，构建了一个计算资源需求低、运行时间短的脑肿瘤检测模型。

Result: 模型在脑肿瘤检测中达到了98.5%的平均准确率。

Conclusion: 该方法在减少资源消耗的同时保持了高准确性，为脑肿瘤的高效检测提供了可行方案。

Abstract: Brain plays a crucial role in regulating body functions and cognitive
processes, with brain tumors posing significant risks to human health. Precise
and prompt detection is a key factor in proper treatment and better patient
outcomes. Traditional methods for detecting brain tumors, that include
biopsies, MRI, and CT scans often face challenges due to their high costs and
the need for specialized medical expertise. Recent developments in machine
learning (ML) and deep learning (DL) has exhibited strong capabilities in
automating the identification and categorization of brain tumors from medical
images, especially MRI scans. However, these classical ML models have
limitations, such as high computational demands, the need for large datasets,
and long training times, which hinder their accessibility and efficiency. Our
research uses MobileNET model for efficient detection of these tumors. The
novelty of this project lies in building an accurate tumor detection model
which use less computing re-sources and runs in less time followed by efficient
decision making through the use of image processing technique for accurate
results. The suggested method attained an average accuracy of 98.5%.

</details>


### [170] [Blending Concepts with Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.23630)
*Lorenzo Olearo,Giorgio Longari,Alessandro Raganato,Rafael Peñaloza,Simone Melzi*

Main category: cs.CV

TL;DR: 扩散模型在零样本框架下能够将不同概念（从具体对象到抽象想法）融合为连贯的新视觉实体，展示了其创造性能力。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型是否能将多个概念的关键属性融合为单一新颖图像，而无需额外训练或微调。

Method: 研究了四种融合方法，涉及提示调度、嵌入插值和分层条件等扩散管道的不同方面。

Result: 实验表明扩散模型具有创造性融合能力，但不同方法在不同场景下表现各异，受提示顺序、概念距离和随机种子等因素影响。

Conclusion: 扩散模型展示了显著的组合潜力，但对输入微小变化敏感，需根据场景选择合适方法。

Abstract: Diffusion models have dramatically advanced text-to-image generation in
recent years, translating abstract concepts into high-fidelity images with
remarkable ease. In this work, we examine whether they can also blend distinct
concepts, ranging from concrete objects to intangible ideas, into coherent new
visual entities under a zero-shot framework. Specifically, concept blending
merges the key attributes of multiple concepts (expressed as textual prompts)
into a single, novel image that captures the essence of each concept. We
investigate four blending methods, each exploiting different aspects of the
diffusion pipeline (e.g., prompt scheduling, embedding interpolation, or
layer-wise conditioning). Through systematic experimentation across diverse
concept categories, such as merging concrete concepts, synthesizing compound
words, transferring artistic styles, and blending architectural landmarks, we
show that modern diffusion models indeed exhibit creative blending capabilities
without further training or fine-tuning. Our extensive user study, involving
100 participants, reveals that no single approach dominates in all scenarios:
each blending technique excels under certain conditions, with factors like
prompt ordering, conceptual distance, and random seed affecting the outcome.
These findings highlight the remarkable compositional potential of diffusion
models while exposing their sensitivity to seemingly minor input variations.

</details>


### [171] [Unified Multimodal Understanding via Byte-Pair Visual Encoding](https://arxiv.org/abs/2506.23639)
*Wanpeng Zhang,Yicheng Feng,Hao Luo,Yijiang Li,Zihao Yue,Sipeng Zheng,Zongqing Lu*

Main category: cs.CV

TL;DR: 提出了一种基于字节对编码的统一多模态理解框架，通过优先级引导的编码方案和多阶段训练，提升了跨模态关系捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视觉-语言理解方面取得进展，但不同模态的有效对齐仍是挑战。

Method: 采用字节对编码将结构信息融入视觉标记，结合优先级引导的编码和多阶段训练。

Result: 实验表明，该方法在多种视觉-语言任务中性能提升。

Conclusion: 通过弥合视觉与文本表示的差距，推动了更高效的多模态基础模型发展。

Abstract: Multimodal large language models (MLLMs) have made significant progress in
vision-language understanding, yet effectively aligning different modalities
remains a fundamental challenge. We present a framework that unifies multimodal
understanding by applying byte-pair encoding to visual tokens. Unlike
conventional approaches that rely on modality-specific encoders, our method
directly incorporates structural information into visual tokens, mirroring
successful tokenization strategies in text-only language models. We introduce a
priority-guided encoding scheme that considers both frequency and spatial
consistency, coupled with a multi-stage training procedure based on
curriculum-driven data composition. These enhancements enable the transformer
model to better capture cross-modal relationships and reason with visual
information. Comprehensive experiments demonstrate improved performance across
diverse vision-language tasks. By bridging the gap between visual and textual
representations, our approach contributes to the advancement of more capable
and efficient multimodal foundation models.

</details>


### [172] [VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation](https://arxiv.org/abs/2506.23641)
*Peng Huang,Junhu Fu,Bowen Guo,Zeju Li,Yuanyuan Wang,Yi Guo*

Main category: cs.CV

TL;DR: VAP-Diffusion利用多模态大语言模型（MLLMs）的外部知识，通过视觉属性提示（VAP）生成更真实多样的医学图像。


<details>
  <summary>Details</summary>
Motivation: 医学图像生成需要丰富的属性信息，但详细描述不易获取。

Method: 设计基于Chain-of-Thoughts的提示从MLLMs提取描述，提出原型条件机制增强生成器鲁棒性。

Result: 在四种数据集的三种医学图像上验证了VAP-Diffusion的有效性。

Conclusion: VAP-Diffusion通过外部知识和原型条件机制提升了医学图像生成的质量和多样性。

Abstract: As the appearance of medical images is influenced by multiple underlying
factors, generative models require rich attribute information beyond labels to
produce realistic and diverse images. For instance, generating an image of skin
lesion with specific patterns demands descriptions that go beyond diagnosis,
such as shape, size, texture, and color. However, such detailed descriptions
are not always accessible. To address this, we explore a framework, termed
Visual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from
pre-trained Multi-modal Large Language Models (MLLMs) to improve the quality
and diversity of medical image generation. First, to derive descriptions from
MLLMs without hallucination, we design a series of prompts following
Chain-of-Thoughts for common medical imaging tasks, including dermatologic,
colorectal, and chest X-ray images. Generated descriptions are utilized during
training and stored across different categories. During testing, descriptions
are randomly retrieved from the corresponding category for inference. Moreover,
to make the generator robust to unseen combination of descriptions at the test
time, we propose a Prototype Condition Mechanism that restricts test embeddings
to be similar to those from training. Experiments on three common types of
medical imaging across four datasets verify the effectiveness of VAP-Diffusion.

</details>


### [173] [MReg: A Novel Regression Model with MoE-based Video Feature Mining for Mitral Regurgitation Diagnosis](https://arxiv.org/abs/2506.23648)
*Zhe Liu,Yuhao Huang,Lian Liu,Chengrui Zhang,Haotian Lin,Tong Han,Zhiyuan Zhu,Yanlin Chen,Yuerui Chen,Dong Ni,Zhongshan Gou,Xin Yang*

Main category: cs.CV

TL;DR: 提出了一种自动化MR诊断模型MReg，基于4腔心彩色多普勒超声视频，通过回归任务和特征选择机制提升诊断准确性和临床适用性。


<details>
  <summary>Details</summary>
Motivation: 现有智能方法在MR诊断中依赖性强且准确性不足，需改进以符合临床需求。

Method: 采用回归任务、特征选择与放大机制，以及Mixture-of-Experts概念提取特征。

Result: 在1868例数据上表现优于其他方法，诊断性能显著提升。

Conclusion: MReg模型在MR诊断中具有高准确性和临床实用性。

Abstract: Color Doppler echocardiography is a crucial tool for diagnosing mitral
regurgitation (MR). Recent studies have explored intelligent methods for MR
diagnosis to minimize user dependence and improve accuracy. However, these
approaches often fail to align with clinical workflow and may lead to
suboptimal accuracy and interpretability. In this study, we introduce an
automated MR diagnosis model (MReg) developed on the 4-chamber cardiac color
Doppler echocardiography video (A4C-CDV). It follows comprehensive feature
mining strategies to detect MR and assess its severity, considering clinical
realities. Our contribution is threefold. First, we formulate the MR diagnosis
as a regression task to capture the continuity and ordinal relationships
between categories. Second, we design a feature selection and amplification
mechanism to imitate the sonographer's diagnostic logic for accurate MR
grading. Third, inspired by the Mixture-of-Experts concept, we introduce a
feature summary module to extract the category-level features, enhancing the
representational capacity for more accurate grading. We trained and evaluated
our proposed MReg on a large in-house A4C-CDV dataset comprising 1868 cases
with three graded regurgitation labels. Compared to other weakly supervised
video anomaly detection and supervised classification methods, MReg
demonstrated superior performance in MR diagnosis. Our code is available at:
https://github.com/cskdstz/MReg.

</details>


### [174] [Towards Markerless Intraoperative Tracking of Deformable Spine Tissue](https://arxiv.org/abs/2506.23657)
*Connor Daly,Elettra Marconi,Marco Riva,Jinendra Ekanayake,Daniel S. Elson,Ferdinando Rodriguez y Baena*

Main category: cs.CV

TL;DR: 论文介绍了首个用于脊柱手术的临床RGB-D数据集，开发了SpineAlign系统用于捕捉术前和术中脊柱状态的变形，并提出了一个术中分割网络和CorrespondNet多任务框架。


<details>
  <summary>Details</summary>
Motivation: 探索无标记跟踪技术在骨科手术中的应用，以减少手术时间和复杂性。

Method: 开发SpineAlign系统，提出术中分割网络和CorrespondNet多任务框架。

Result: 成功构建首个临床RGB-D数据集，并验证了系统的有效性。

Conclusion: 无标记跟踪技术在脊柱手术中具有潜力，未来可进一步推广。

Abstract: Consumer-grade RGB-D imaging for intraoperative orthopedic tissue tracking is
a promising method with high translational potential. Unlike bone-mounted
tracking devices, markerless tracking can reduce operating time and complexity.
However, its use has been limited to cadaveric studies. This paper introduces
the first real-world clinical RGB-D dataset for spine surgery and develops
SpineAlign, a system for capturing deformation between preoperative and
intraoperative spine states. We also present an intraoperative segmentation
network trained on this data and introduce CorrespondNet, a multi-task
framework for predicting key regions for registration in both intraoperative
and preoperative scenes.

</details>


### [175] [On the Domain Robustness of Contrastive Vision-Language Models](https://arxiv.org/abs/2506.23663)
*Mario Koddenbrock,Rudolf Hoffmann,David Brodmann,Erik Rodner*

Main category: cs.CV

TL;DR: Deepbench是一个框架，用于评估视觉语言模型（VLMs）在特定领域的鲁棒性，利用LLM生成领域相关的图像损坏，无需标注数据。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练基础模型在通用基准上表现优异，但在特定领域（如独特成像条件）下性能可能显著下降，需要领域感知的评估方法。

Method: Deepbench利用LLM生成领域特定的图像损坏，评估多种对比视觉语言架构在六个真实领域的表现。

Result: 不同架构在鲁棒性上表现出显著差异，强调了领域感知评估的必要性。

Conclusion: Deepbench作为开源工具，支持进一步研究领域感知的鲁棒性评估。

Abstract: In real-world vision-language applications, practitioners increasingly rely
on large, pretrained foundation models rather than custom-built solutions,
despite limited transparency regarding their training data and processes. While
these models achieve impressive performance on general benchmarks, their
effectiveness can decline notably under specialized domain shifts, such as
unique imaging conditions or environmental variations. In this work, we
introduce Deepbench, a framework designed to assess domain-specific robustness
of vision-language models (VLMs). Deepbench leverages a large language model
(LLM) to generate realistic, context-aware image corruptions tailored to
specific deployment domains without requiring labeled data. We evaluate a range
of contrastive vision-language architectures and architectural variants across
six real-world domains and observe substantial variability in robustness,
highlighting the need for targeted, domain-aware evaluation. Deepbench is
released as open-source software to support further research into domain-aware
robustness assessment.

</details>


### [176] [Partial Forward Blocking: A Novel Data Pruning Paradigm for Lossless Training Acceleration](https://arxiv.org/abs/2506.23674)
*Dongyue Wu,Zilin Guo,Jialong Zuo,Nong Sang,Changxin Gao*

Main category: cs.CV

TL;DR: PFB框架通过浅层特征评估样本重要性并动态剪枝，显著减少计算成本，同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据剪枝方法因依赖梯度或代理模型而带来的额外计算成本问题。

Method: 提出Partial Forward Blocking (PFB)，基于浅层特征评估样本重要性并动态剪枝，避免深层次前向传播和反向传播的计算开销。

Result: 在ImageNet上，PFB在剪除40%数据的情况下，准确率提升0.5%，训练时间减少33%。

Conclusion: PFB是一种高效且无损的训练加速框架，适用于大规模数据集。

Abstract: The ever-growing size of training datasets enhances the generalization
capability of modern machine learning models but also incurs exorbitant
computational costs. Existing data pruning approaches aim to accelerate
training by removing those less important samples. However, they often rely on
gradients or proxy models, leading to prohibitive additional costs of gradient
back-propagation and proxy model training. In this paper, we propose Partial
Forward Blocking (PFB), a novel framework for lossless training acceleration.
The efficiency of PFB stems from its unique adaptive pruning pipeline: sample
importance is assessed based on features extracted from the shallow layers of
the target model. Less important samples are then pruned, allowing only the
retained ones to proceed with the subsequent forward pass and loss
back-propagation. This mechanism significantly reduces the computational
overhead of deep-layer forward passes and back-propagation for pruned samples,
while also eliminating the need for auxiliary backward computations and proxy
model training. Moreover, PFB introduces probability density as an indicator of
sample importance. Combined with an adaptive distribution estimation module,
our method dynamically prioritizes relatively rare samples, aligning with the
constantly evolving training state. Extensive experiments demonstrate the
significant superiority of PFB in performance and speed. On ImageNet, PFB
achieves a 0.5% accuracy improvement and 33% training time reduction with 40%
data pruned.

</details>


### [177] [Pruning by Block Benefit: Exploring the Properties of Vision Transformer Blocks during Domain Adaptation](https://arxiv.org/abs/2506.23675)
*Patrick Glandorf,Bodo Rosenhahn*

Main category: cs.CV

TL;DR: 提出了一种名为P3B的剪枝方法，通过块级贡献全局分配参数资源，在保持性能的同时减少计算成本。


<details>
  <summary>Details</summary>
Motivation: Vision Transformer计算成本高，传统剪枝方法在未见数据域上表现不佳，导致资源分配不理想。

Method: P3B利用块级相对贡献全局分配参数，识别低影响组件并保留关键部分，通过层间保留比例确保性能。

Result: P3B在高稀疏性（70%参数减少）下仅损失0.64%准确率，在迁移学习中表现突出。

Conclusion: P3B是一种先进的剪枝方法，尤其适用于资源受限硬件和高稀疏性场景。

Abstract: Vision Transformer have set new benchmarks in several tasks, but these models
come with the lack of high computational costs which makes them impractical for
resource limited hardware. Network pruning reduces the computational complexity
by removing less important operations while maintaining performance. However,
pruning a model on an unseen data domain, leads to a misevaluation of weight
significance, resulting in suboptimal resource assignment. In this work, we
find that task-sensitive layers initially fail to improve the feature
representation on downstream tasks, leading to performance loss for early
pruning decisions. To address this problem, we introduce Pruning by Block
Benefit (P3B), a pruning method that utilizes the relative contribution on
block level to globally assign parameter resources. P3B identifies low-impact
components to reduce parameter allocation while preserving critical ones.
Classical pruning mask optimization struggles to reactivate zero-mask-elements.
In contrast, P3B sets a layerwise keep ratio based on global performance
metrics, ensuring the reactivation of late-converging blocks. We show in
extensive experiments that P3B is a state of the art pruning method with most
noticeable gains in transfer learning tasks. Notably, P3B is able to conserve
high performance, even in high sparsity regimes of 70% parameter reduction
while only losing 0.64% in accuracy.

</details>


### [178] [A Unified Framework for Stealthy Adversarial Generation via Latent Optimization and Transferability Enhancement](https://arxiv.org/abs/2506.23676)
*Gaozheng Pei,Ke Ma,Dongpeng Zhang,Chengzhi Sun,Qianqian Xu,Qingming Huang*

Main category: cs.CV

TL;DR: 提出了一种统一框架，将传统对抗样本增强策略融入基于扩散模型的图像编辑方法，提升其在更广泛下游任务中的适用性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成的对抗样本在传统图像分类任务中表现良好，但在Deepfake检测等任务中泛化能力不足，且传统增强策略难以适配。

Method: 设计了一个统一框架，将传统对抗样本增强策略与扩散模型结合，通过图像编辑生成对抗样本。

Result: 该方法在ACM MM25竞赛中获胜，验证了其有效性。

Conclusion: 该框架成功解决了扩散模型在对抗样本生成中的泛化问题，并展示了其在实际任务中的潜力。

Abstract: Due to their powerful image generation capabilities, diffusion-based
adversarial example generation methods through image editing are rapidly
gaining popularity. However, due to reliance on the discriminative capability
of the diffusion model, these diffusion-based methods often struggle to
generalize beyond conventional image classification tasks, such as in Deepfake
detection. Moreover, traditional strategies for enhancing adversarial example
transferability are challenging to adapt to these methods. To address these
challenges, we propose a unified framework that seamlessly incorporates
traditional transferability enhancement strategies into diffusion model-based
adversarial example generation via image editing, enabling their application
across a wider range of downstream tasks. Our method won first place in the
"1st Adversarial Attacks on Deepfake Detectors: A Challenge in the Era of
AI-Generated Media" competition at ACM MM25, which validates the effectiveness
of our approach.

</details>


### [179] [SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation](https://arxiv.org/abs/2506.23690)
*Shuai Tan,Biao Gong,Yujie Wei,Shiwei Zhang,Zhuoxin Liu,Dandan Zheng,Jingdong Chen,Yan Wang,Hao Ouyang,Kecheng Zheng,Yujun Shen*

Main category: cs.CV

TL;DR: SynMotion提出了一种结合语义指导和视觉适应的运动定制视频生成模型，通过双嵌入语义理解机制和参数高效运动适配器，解决了现有方法在语义和视觉层面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视频运动定制中仅关注语义对齐或视觉表示，导致忽视运动视觉复杂性或语义混淆。

Method: 提出双嵌入语义理解机制和参数高效运动适配器，交替优化主题和运动嵌入，并使用SPV数据集。

Result: 实验表明，SynMotion在T2V和I2V设置中优于现有基线。

Conclusion: SynMotion通过联合语义和视觉优化，提升了运动定制视频生成的质量和多样性。

Abstract: Diffusion-based video motion customization facilitates the acquisition of
human motion representations from a few video samples, while achieving
arbitrary subjects transfer through precise textual conditioning. Existing
approaches often rely on semantic-level alignment, expecting the model to learn
new motion concepts and combine them with other entities (e.g., ''cats'' or
''dogs'') to produce visually appealing results. However, video data involve
complex spatio-temporal patterns, and focusing solely on semantics cause the
model to overlook the visual complexity of motion. Conversely, tuning only the
visual representation leads to semantic confusion in representing the intended
action. To address these limitations, we propose SynMotion, a new
motion-customized video generation model that jointly leverages semantic
guidance and visual adaptation. At the semantic level, we introduce the
dual-embedding semantic comprehension mechanism which disentangles subject and
motion representations, allowing the model to learn customized motion features
while preserving its generative capabilities for diverse subjects. At the
visual level, we integrate parameter-efficient motion adapters into a
pre-trained video generation model to enhance motion fidelity and temporal
coherence. Furthermore, we introduce a new embedding-specific training strategy
which \textbf{alternately optimizes} subject and motion embeddings, supported
by the manually constructed Subject Prior Video (SPV) training dataset. This
strategy promotes motion specificity while preserving generalization across
diverse subjects. Lastly, we introduce MotionBench, a newly curated benchmark
with diverse motion patterns. Experimental results across both T2V and I2V
settings demonstrate that \method outperforms existing baselines. Project page:
https://lucaria-academy.github.io/SynMotion/

</details>


### [180] [Single Image Test-Time Adaptation via Multi-View Co-Training](https://arxiv.org/abs/2506.23705)
*Smriti Joshi,Richard Osuala,Lidia Garrucho,Kaisar Kushibar,Dimitri Kessler,Oliver Diaz,Karim Lekadir*

Main category: cs.CV

TL;DR: 提出一种基于补丁的多视图协同训练方法，用于单图像测试时适应，解决医学影像中实时推理的需求。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大数据集且忽略医学影像的立体信息，无法满足临床实时需求。

Method: 通过不确定性引导的自训练实现特征和预测一致性，仅需单张测试图像。

Result: 在三个乳腺癌MRI数据集上，性能接近监督基准，平均Dice系数提升3.75%。

Conclusion: 方法有效且实用，代码开源，易于集成到nnUNet框架。

Abstract: Test-time adaptation enables a trained model to adjust to a new domain during
inference, making it particularly valuable in clinical settings where such
on-the-fly adaptation is required. However, existing techniques depend on large
target domain datasets, which are often impractical and unavailable in medical
scenarios that demand per-patient, real-time inference. Moreover, current
methods commonly focus on two-dimensional images, failing to leverage the
volumetric richness of medical imaging data. Bridging this gap, we propose a
Patch-Based Multi-View Co-Training method for Single Image Test-Time
adaptation. Our method enforces feature and prediction consistency through
uncertainty-guided self-training, enabling effective volumetric segmentation in
the target domain with only a single test-time image. Validated on three
publicly available breast magnetic resonance imaging datasets for tumor
segmentation, our method achieves performance close to the upper bound
supervised benchmark while also outperforming all existing state-of-the-art
methods, on average by a Dice Similarity Coefficient of 3.75%. We publicly
share our accessible codebase, readily integrable with the popular nnUNet
framework, at https://github.com/smriti-joshi/muvi.git.

</details>


### [181] [Subjective Camera: Bridging Human Cognition and Visual Reconstruction through Sequence-Aware Sketch-Guided Diffusion](https://arxiv.org/abs/2506.23711)
*Haoyang Chen,Dongfang Sun,Caoyuan Ma,Shiqin Wang,Kewei Zhang,Zheng Wang,Zhixiang Wang*

Main category: cs.CV

TL;DR: 提出Subjective Camera方法，通过结合语言描述和渐进草图，将主观感知转化为逼真图像，解决了语言模糊性和草图抽象性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法面临用户主观输入偏差、草图与3D先验的模态差距及草图质量敏感性问题，需资源密集型调整或高精度草图要求。

Method: 采用概念顺序生成框架，通过文本奖励优化建立外观先验，顺序感知解耦生成处理草图顺序，潜在优化桥接模态差距，分层奖励框架支持粗糙草图。

Result: 在多样数据集上评估，方法在语义和空间一致性上达到最先进性能。

Conclusion: Subjective Camera框架有效解决了现有挑战，无需训练调整或高精度草图，实现了主观感知到逼真图像的转化。

Abstract: We propose Subjective Camera, a human-as-imaging-device paradigm that
reconstructs real-world scenes from mental impressions through synergistic use
of verbal descriptions and progressive rough sketches. This approach overcomes
dual limitations of language ambiguity and sketch abstraction by treating the
user's drawing sequence as priors, effectively translating subjective
perceptual expectations into photorealistic images.
  Existing approaches face three fundamental barriers: (1) user-specific
subjective input biases, (2) huge modality gap between planar sketch and 3D
priors in diffusion, and (3) sketch quality-sensitive performance degradation.
Current solutions either demand resource-intensive model adaptation or impose
impractical requirements on sketch precision.
  Our framework addresses these challenges through concept-sequential
generation. (1) We establish robust appearance priors through text-reward
optimization, and then implement sequence-aware disentangled generation that
processes concepts in sketching order; these steps accommodate user-specific
subjective expectation in a train-free way. (2) We employ latent optimization
that effectively bridges the modality gap between planar sketches and 3D priors
in diffusion. (3) Our hierarchical reward-guided framework enables the use of
rough sketches without demanding artistic expertise. Comprehensive evaluation
across diverse datasets demonstrates that our approach achieves
state-of-the-art performance in maintaining both semantic and spatial
coherence.

</details>


### [182] [Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization](https://arxiv.org/abs/2506.23714)
*Md Moinul Islam,Sofoklis Kakouros,Janne Heikkilä,Mourad Oussalah*

Main category: cs.CV

TL;DR: 本文提出了一种行为感知的多模态视频摘要框架，结合文本、音频和视觉线索生成时间戳对齐的摘要，显著提升了传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 随着视频内容在教育、职业和社交领域的增加，需要超越传统单模态方法的有效摘要技术。

Method: 通过提取韵律特征、文本线索和视觉指标，识别语义和情感重要的时刻，并利用跨模态强调的“奖励词”提升摘要的语义相关性和表达清晰度。

Result: 实验结果显示，与传统提取方法相比，ROUGE-1从0.4769提升至0.7929，BERTScore从0.9152提升至0.9536，视频评估中F1-Score提高了近23%。

Conclusion: 多模态整合在生成全面且行为感知的视频摘要中具有显著潜力。

Abstract: The increasing volume of video content in educational, professional, and
social domains necessitates effective summarization techniques that go beyond
traditional unimodal approaches. This paper proposes a behaviour-aware
multimodal video summarization framework that integrates textual, audio, and
visual cues to generate timestamp-aligned summaries. By extracting prosodic
features, textual cues and visual indicators, the framework identifies
semantically and emotionally important moments. A key contribution is the
identification of bonus words, which are terms emphasized across multiple
modalities and used to improve the semantic relevance and expressive clarity of
the summaries. The approach is evaluated against pseudo-ground truth (pGT)
summaries generated using LLM-based extractive method. Experimental results
demonstrate significant improvements over traditional extractive method, such
as the Edmundson method, in both text and video-based evaluation metrics.
Text-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore
from 0.9152 to 0.9536, while in video-based evaluation, our proposed framework
improves F1-Score by almost 23%. The findings underscore the potential of
multimodal integration in producing comprehensive and behaviourally informed
video summaries.

</details>


### [183] [When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation](https://arxiv.org/abs/2506.23724)
*Chang'an Yi,Xiaohui Deng,Guohao Chen,Yan Zhou,Qinghua Lu,Shuaicheng Niu*

Main category: cs.CV

TL;DR: 本文研究了跨模型知识对测试时适应（TTA）的影响，提出了COCA框架，通过跨模型协同学习提升TTA性能。


<details>
  <summary>Details</summary>
Motivation: 探索跨模型知识如何影响TTA过程，发现不同模型间可以互补，从而提升性能。

Method: 提出COCA框架，包含协同适应（整合其他模型知识）和自适应（增强模型自身优势）两种策略。

Result: COCA显著提升了现有SOTA方法的性能，例如将ViT-Base在ImageNet-C上的准确率从51.7%提高到64.5%。

Conclusion: 跨模型协同学习能有效提升TTA性能，COCA可作为即插即用模块应用于不同模型。

Abstract: Test-time Adaptation (TTA) adapts a given model to testing domain data with
potential domain shifts through online unsupervised learning, yielding
impressive performance. However, to date, existing TTA methods primarily focus
on single-model adaptation. In this work, we investigate an intriguing
question: how does cross-model knowledge influence the TTA process? Our
findings reveal that, in TTA's unsupervised online setting, each model can
provide complementary, confident knowledge to the others, even when there are
substantial differences in model size. For instance, a smaller model like
MobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base
(86.6M parameters). In light of this, we propose COCA, a Cross-Model
Co-Learning framework for TTA, which mainly consists of two main strategies. 1)
Co-adaptation adaptively integrates complementary knowledge from other models
throughout the TTA process, reducing individual model biases. 2)
Self-adaptation enhances each model's unique strengths via unsupervised
learning, enabling diverse adaptation to the target domain. Extensive
experiments show that COCA, which can also serve as a plug-and-play module,
significantly boosts existing SOTAs, on models with various sizes--including
ResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example,
with Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy
on ImageNet-C from 51.7% to 64.5%. The code is publicly available at
https://github.com/ycarobot/COCA.

</details>


### [184] [Proteus-ID: ID-Consistent and Motion-Coherent Video Customization](https://arxiv.org/abs/2506.23729)
*Guiyu Zhang,Chen Shi,Zijian Jiang,Xunzhi Xiang,Jingjing Qian,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: Proteus-ID是一个基于扩散模型的框架，用于视频身份定制，解决了身份一致性和运动连贯性的挑战。


<details>
  <summary>Details</summary>
Motivation: 视频身份定制需要从单张参考图像和文本提示生成连贯的视频，面临身份一致性和运动自然性的挑战。

Method: 提出多模态身份融合模块（MIF）、时间感知身份注入机制（TAII）和自适应运动学习（AML）策略。

Result: Proteus-ID在身份保留、文本对齐和运动质量上优于现有方法。

Conclusion: Proteus-ID为视频身份定制设立了新基准，代码和数据已公开。

Abstract: Video identity customization seeks to synthesize realistic, temporally
coherent videos of a specific subject, given a single reference image and a
text prompt. This task presents two core challenges: (1) maintaining identity
consistency while aligning with the described appearance and actions, and (2)
generating natural, fluid motion without unrealistic stiffness. To address
these challenges, we introduce Proteus-ID, a novel diffusion-based framework
for identity-consistent and motion-coherent video customization. First, we
propose a Multimodal Identity Fusion (MIF) module that unifies visual and
textual cues into a joint identity representation using a Q-Former, providing
coherent guidance to the diffusion model and eliminating modality imbalance.
Second, we present a Time-Aware Identity Injection (TAII) mechanism that
dynamically modulates identity conditioning across denoising steps, improving
fine-detail reconstruction. Third, we propose Adaptive Motion Learning (AML), a
self-supervised strategy that reweights the training loss based on
optical-flow-derived motion heatmaps, enhancing motion realism without
requiring additional inputs. To support this task, we construct Proteus-Bench,
a high-quality dataset comprising 200K curated clips for training and 150
individuals from diverse professions and ethnicities for evaluation. Extensive
experiments demonstrate that Proteus-ID outperforms prior methods in identity
preservation, text alignment, and motion quality, establishing a new benchmark
for video identity customization. Codes and data are publicly available at
https://grenoble-zhang.github.io/Proteus-ID/.

</details>


### [185] [Can We Challenge Open-Vocabulary Object Detectors with Generated Content in Street Scenes?](https://arxiv.org/abs/2506.23751)
*Annika Mütze,Sadia Ilyas,Christian Dörpelkus,Matthias Rottmann*

Main category: cs.CV

TL;DR: 论文探讨了如何通过合成数据挑战开放词汇目标检测器的局限性，并发现其系统性的失败模式。


<details>
  <summary>Details</summary>
Motivation: 开放词汇目标检测器在安全关键应用中存在局限性，但真实数据难以提供系统性评估。合成数据可以更系统地探索模型的边界。

Method: 设计了两个自动化流程，使用稳定扩散技术生成高多样性的语义内容，并在合成数据上评估多种开放词汇目标检测器。

Result: 合成数据能有效挑战检测器，发现其依赖对象位置而非语义的强依赖性。

Conclusion: 研究为挑战开放词汇模型提供了系统性方法，并揭示了改进数据采集的方向。

Abstract: Open-vocabulary object detectors such as Grounding DINO are trained on vast
and diverse data, achieving remarkable performance on challenging datasets. Due
to that, it is unclear where to find their limitations, which is of major
concern when using in safety-critical applications. Real-world data does not
provide sufficient control, required for a rigorous evaluation of model
generalization. In contrast, synthetically generated data allows to
systematically explore the boundaries of model competence/generalization. In
this work, we address two research questions: 1) Can we challenge
open-vocabulary object detectors with generated image content? 2) Can we find
systematic failure modes of those models? To address these questions, we design
two automated pipelines using stable diffusion to inpaint unusual objects with
high diversity in semantics, by sampling multiple substantives from WordNet and
ChatGPT. On the synthetically generated data, we evaluate and compare multiple
open-vocabulary object detectors as well as a classical object detector. The
synthetic data is derived from two real-world datasets, namely LostAndFound, a
challenging out-of-distribution (OOD) detection benchmark, and the NuImages
dataset. Our results indicate that inpainting can challenge open-vocabulary
object detectors in terms of overlooking objects. Additionally, we find a
strong dependence of open-vocabulary models on object location, rather than on
object semantics. This provides a systematic approach to challenge
open-vocabulary models and gives valuable insights on how data could be
acquired to effectively improve these models.

</details>


### [186] [Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking](https://arxiv.org/abs/2506.23783)
*Shiao Wang,Ju Huang,Qingchuan Ma,Jinfeng Gao,Chunyi Xu,Xiao Wang,Lan Chen,Bo Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于线性复杂度Vision Mamba网络的高效RGB-Event目标跟踪框架Mamba-FETrack V2，通过轻量级Prompt Generator和FEMamba主干实现跨模态交互与融合。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态跟踪算法依赖高复杂度Vision Transformer架构，导致计算开销大且跨模态交互效果有限。

Method: 设计轻量级Prompt Generator生成模态特定提示向量，结合Vision Mamba网络实现特征提取与融合。

Result: 在多个RGB-Event跟踪基准测试中表现出优越性能和效率。

Conclusion: Mamba-FETrack V2在计算效率和性能上均优于现有方法，代码和模型将开源。

Abstract: Combining traditional RGB cameras with bio-inspired event cameras for robust
object tracking has garnered increasing attention in recent years. However,
most existing multimodal tracking algorithms depend heavily on high-complexity
Vision Transformer architectures for feature extraction and fusion across
modalities. This not only leads to substantial computational overhead but also
limits the effectiveness of cross-modal interactions. In this paper, we propose
an efficient RGB-Event object tracking framework based on the linear-complexity
Vision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a
lightweight Prompt Generator that utilizes embedded features from each
modality, together with a shared prompt pool, to dynamically generate
modality-specific learnable prompt vectors. These prompts, along with the
modality-specific embedded features, are then fed into a Vision Mamba-based
FEMamba backbone, which facilitates prompt-guided feature extraction,
cross-modal interaction, and fusion in a unified manner. Finally, the fused
representations are passed to the tracking head for accurate target
localization. Extensive experimental evaluations on multiple RGB-Event tracking
benchmarks, including short-term COESOT dataset and long-term datasets, i.e.,
FE108 and FELT V2, demonstrate the superior performance and efficiency of the
proposed tracking framework. The source code and pre-trained models will be
released on https://github.com/Event-AHU/Mamba_FETrack

</details>


### [187] [Visual Textualization for Image Prompted Object Detection](https://arxiv.org/abs/2506.23785)
*Yongjian Wu,Yang Zhou,Jiya Saiyin,Bingzheng Wei,Yan Xu*

Main category: cs.CV

TL;DR: VisTex-OVLM是一种新颖的图像提示目标检测方法，通过视觉文本化增强对象级视觉语言模型（OVLM）对罕见类别的检测能力。


<details>
  <summary>Details</summary>
Motivation: 解决罕见类别因难以文本描述或预训练数据中缺失而导致的检测性能下降问题。

Method: 利用多尺度文本化块和多阶段融合策略，将视觉示例信息转化为文本特征空间中的视觉标记。

Result: 在开放集数据集和少样本基准（如PASCAL VOC和MSCOCO）上表现优异，达到最先进水平。

Conclusion: VisTex-OVLM在保持OVLM原有架构和泛化能力的同时，显著提升了少样本场景下的性能。

Abstract: We propose VisTex-OVLM, a novel image prompted object detection method that
introduces visual textualization -- a process that projects a few visual
exemplars into the text feature space to enhance Object-level Vision-Language
Models' (OVLMs) capability in detecting rare categories that are difficult to
describe textually and nearly absent from their pre-training data, while
preserving their pre-trained object-text alignment. Specifically, VisTex-OVLM
leverages multi-scale textualizing blocks and a multi-stage fusion strategy to
integrate visual information from visual exemplars, generating textualized
visual tokens that effectively guide OVLMs alongside text prompts. Unlike
previous methods, our method maintains the original architecture of OVLM,
maintaining its generalization capabilities while enhancing performance in
few-shot settings. VisTex-OVLM demonstrates superior performance across
open-set datasets which have minimal overlap with OVLM's pre-training data and
achieves state-of-the-art results on few-shot benchmarks PASCAL VOC and MSCOCO.
The code will be released at https://github.com/WitGotFlg/VisTex-OVLM.

</details>


### [188] [Controllable Reference-Based Real-World Remote Sensing Image Super-Resolution with Generative Diffusion Priors](https://arxiv.org/abs/2506.23801)
*Ce Wang,Wanjie Sun*

Main category: cs.CV

TL;DR: CRefDiff是一种基于扩散模型的可控参考超分辨率方法，通过双分支融合机制和预训练的Stable Diffusion模型，解决了现有RefSR方法在真实世界中的不足，并在新数据集Real-RefRSSRD上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有RefSR方法在真实世界应用中面临分辨率差异和地表覆盖变化的挑战，导致生成不足或过度依赖参考图像。

Method: 基于预训练的Stable Diffusion模型，引入双分支融合机制自适应整合参考图像的局部和全局信息，并提出Better Start策略加速推理。

Result: 在Real-RefRSSRD数据集上，CRefDiff在多项指标上达到最优，并提升了下游任务（如场景分类和语义分割）的性能。

Conclusion: CRefDiff通过可控性和高效性，为真实世界遥感图像超分辨率提供了有效解决方案。

Abstract: Super-resolution (SR) techniques can enhance the spatial resolution of remote
sensing images by utilizing low-resolution (LR) images to reconstruct
high-resolution (HR) images, enabling more efficient large-scale earth
observation applications. While single-image super-resolution (SISR) methods
have shown progress, reference-based super-resolution (RefSR) offers superior
performance by incorporating historical HR images alongside current LR
observations. However, existing RefSR methods struggle with real-world
complexities, such as cross-sensor resolution gap and significant land cover
changes, often leading to under-generation or over-reliance on reference image.
To address these challenges, we propose CRefDiff, a novel controllable
reference-based diffusion model for real-world remote sensing image SR. To
address the under-generation problem, CRefDiff is built upon the pretrained
Stable Diffusion model, leveraging its powerful generative prior to produce
accurate structures and textures. To mitigate over-reliance on the reference,
we introduce a dual-branch fusion mechanism that adaptively integrates both
local and global information from the reference image. Moreover, this novel
dual-branch design enables reference strength control during inference,
enhancing interactivity and flexibility of the model. Finally, a strategy named
Better Start is proposed to significantly reduce the number of denoising steps,
thereby accelerating the inference process. To support further research, we
introduce Real-RefRSSRD, a new real-world RefSR dataset for remote sensing
images, consisting of HR NAIP and LR Sentinel-2 image pairs with diverse land
cover changes and significant temporal gaps. Extensive experiments on
Real-RefRSSRD show that CRefDiff achieves state-of-the-art performance across
various metrics and improves downstream tasks such as scene classification and
semantic segmentation.

</details>


### [189] [Towards Initialization-free Calibrated Bundle Adjustment](https://arxiv.org/abs/2506.23808)
*Carl Olsson,Amanda Nilsson*

Main category: cs.CV

TL;DR: 提出了一种利用已知相机标定的方法，通过引入成对相对旋转估计，实现初始化自由的近度量重建。


<details>
  <summary>Details</summary>
Motivation: 传统初始化自由BA方法无法利用相机标定信息，导致重建结果仅能确定到投影变换，且需要更多数据。本文旨在解决这一问题。

Method: 引入成对相对旋转估计，将旋转平均集成到pOSE框架中，利用相机标定信息生成近度量解。

Result: 实验表明，该方法能可靠优化目标函数，从随机初始解高概率收敛到全局最小值，生成准确的近度量重建。

Conclusion: 该方法成功实现了初始化自由的标定SfM，生成更精确的重建结果。

Abstract: A recent series of works has shown that initialization-free BA can be
achieved using pseudo Object Space Error (pOSE) as a surrogate objective. The
initial reconstruction-step optimizes an objective where all terms are
projectively invariant and it cannot incorporate knowledge of the camera
calibration. As a result, the solution is only determined up to a projective
transformation of the scene and the process requires more data for successful
reconstruction.
  In contrast, we present a method that is able to use the known camera
calibration thereby producing near metric solutions, that is, reconstructions
that are accurate up to a similarity transformation. To achieve this we
introduce pairwise relative rotation estimates that carry information about
camera calibration. These are only invariant to similarity transformations,
thus encouraging solutions that preserve metric features of the real scene. Our
method can be seen as integrating rotation averaging into the pOSE framework
striving towards initialization-free calibrated SfM.
  Our experimental evaluation shows that we are able to reliably optimize our
objective, achieving convergence to the global minimum with high probability
from random starting solutions, resulting in accurate near metric
reconstructions.

</details>


### [190] [MadCLIP: Few-shot Medical Anomaly Detection with CLIP](https://arxiv.org/abs/2506.23810)
*Mahshid Shiri,Cigdem Beyan,Vittorio Murino*

Main category: cs.CV

TL;DR: 提出了一种基于预训练CLIP模型的少样本异常检测方法，用于医学数据的图像级和像素级异常检测，通过双分支设计和可学习文本提示提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决医学数据中少样本异常检测的挑战，避免依赖合成数据或内存库。

Method: 采用双分支设计捕捉正常和异常特征，使用可学习文本提示增强语义对齐，并应用SigLIP损失处理图像与文本提示的多对一关系。

Result: 在多种模态上验证了方法的优越性，性能优于现有方法，且不依赖合成数据或内存库。

Conclusion: 该方法在医学异常检测中表现出色，并通过消融实验验证了各组件的重要性。

Abstract: An innovative few-shot anomaly detection approach is presented, leveraging
the pre-trained CLIP model for medical data, and adapting it for both
image-level anomaly classification (AC) and pixel-level anomaly segmentation
(AS). A dual-branch design is proposed to separately capture normal and
abnormal features through learnable adapters in the CLIP vision encoder. To
improve semantic alignment, learnable text prompts are employed to link visual
features. Furthermore, SigLIP loss is applied to effectively handle the
many-to-one relationship between images and unpaired text prompts, showcasing
its adaptation in the medical field for the first time. Our approach is
validated on multiple modalities, demonstrating superior performance over
existing methods for AC and AS, in both same-dataset and cross-dataset
evaluations. Unlike prior work, it does not rely on synthetic data or memory
banks, and an ablation study confirms the contribution of each component. The
code is available at https://github.com/mahshid1998/MadCLIP.

</details>


### [191] [Interpretable Zero-Shot Learning with Locally-Aligned Vision-Language Model](https://arxiv.org/abs/2506.23822)
*Shiming Chen,Bowen Duan,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CV

TL;DR: LaZSL是一种基于局部视觉-语义对齐的可解释零样本学习方法，通过最优传输实现视觉区域与属性的交互，提升可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型（如CLIP）在零样本学习中表现优异，但缺乏可解释性，难以解释预测结果。

Method: 提出LaZSL模型，利用最优传输实现局部视觉特征与属性的对齐，无需额外训练即可提供可解释的相似性。

Result: 实验表明，LaZSL在可解释性、准确性和领域泛化性方面均有显著提升。

Conclusion: LaZSL为可解释零样本学习提供了一种有效方法，兼具性能和可解释性。

Abstract: Large-scale vision-language models (VLMs), such as CLIP, have achieved
remarkable success in zero-shot learning (ZSL) by leveraging large-scale
visual-text pair datasets. However, these methods often lack interpretability,
as they compute the similarity between an entire query image and the embedded
category words, making it difficult to explain their predictions. One approach
to address this issue is to develop interpretable models by integrating
language, where classifiers are built using discrete attributes, similar to
human perception. This introduces a new challenge: how to effectively align
local visual features with corresponding attributes based on pre-trained VLMs.
To tackle this, we propose LaZSL, a locally-aligned vision-language model for
interpretable ZSL. LaZSL employs local visual-semantic alignment via optimal
transport to perform interaction between visual regions and their associated
attributes, facilitating effective alignment and providing interpretable
similarity without the need for additional training. Extensive experiments
demonstrate that our method offers several advantages, including enhanced
interpretability, improved accuracy, and strong domain generalization. Codes
available at: https://github.com/shiming-chen/LaZSL.

</details>


### [192] [Flash-VStream: Efficient Real-Time Understanding for Long Video Streams](https://arxiv.org/abs/2506.23825)
*Haoji Zhang,Yiqin Wang,Yansong Tang,Yong Liu,Jiashi Feng,Xiaojie Jin*

Main category: cs.CV

TL;DR: Flash-VStream是一种高效的长视频语言模型，通过设计Flash Memory模块解决长视频处理的计算和内存开销问题，显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理长视频时效率低下，难以推广到更长的视频，Flash-VStream旨在解决这一问题。

Method: 提出Flash Memory模块，包含低容量上下文记忆和高容量增强记忆，分别聚合长上下文时间信息和检索详细空间信息。

Result: 在多个长视频基准测试中表现优异，推理延迟显著降低。

Conclusion: Flash-VStream在长视频理解中实现了高效和实时响应，性能领先。

Abstract: Benefiting from the advances in large language models and cross-modal
alignment, existing multimodal large language models have achieved prominent
performance in image and short video understanding. However, the understanding
of long videos is still challenging, as their long-context nature results in
significant computational and memory overhead. Most existing work treats long
videos in the same way as short videos, which is inefficient for real-world
applications and hard to generalize to even longer videos. To address these
issues, we propose Flash-VStream, an efficient video language model capable of
processing extremely long videos and responding to user queries in real time.
Particularly, we design a Flash Memory module, containing a low-capacity
context memory to aggregate long-context temporal information and model the
distribution of information density, and a high-capacity augmentation memory to
retrieve detailed spatial information based on this distribution. Compared to
existing models, Flash-VStream achieves significant reductions in inference
latency. Extensive experiments on long video benchmarks and comprehensive video
benchmarks, i.e., EgoSchema, MLVU, LVBench, MVBench and Video-MME, demonstrate
the state-of-the-art performance and outstanding efficiency of our method. Code
is available at https://github.com/IVGSZ/Flash-VStream.

</details>


### [193] [Spatially Gene Expression Prediction using Dual-Scale Contrastive Learning](https://arxiv.org/abs/2506.23827)
*Mingcheng Qu,Yuncong Wu,Donglin Di,Yue Gao,Tonghua Su,Yang Song,Lei Fan*

Main category: cs.CV

TL;DR: NH2ST框架通过整合空间上下文和多模态数据，显著提升了从病理图像预测基因表达的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了空间和分子交互，导致无法捕捉复杂跨模态关系，NH2ST旨在解决这一问题。

Method: 采用查询分支和邻居分支处理目标区域及其邻近信息，结合交叉注意力和对比学习。

Result: 在六个数据集上表现优于现有方法，PCC指标提升超过20%。

Conclusion: NH2ST为基因表达预测提供了更高效且准确的解决方案。

Abstract: Spatial transcriptomics (ST) provides crucial insights into tissue
micro-environments, but is limited to its high cost and complexity. As an
alternative, predicting gene expression from pathology whole slide images (WSI)
is gaining increasing attention. However, existing methods typically rely on
single patches or a single pathology modality, neglecting the complex spatial
and molecular interactions between target and neighboring information (e.g.,
gene co-expression). This leads to a failure in establishing connections among
adjacent regions and capturing intricate cross-modal relationships. To address
these issues, we propose NH2ST, a framework that integrates spatial context and
both pathology and gene modalities for gene expression prediction. Our model
comprises a query branch and a neighbor branch to process paired target patch
and gene data and their neighboring regions, where cross-attention and
contrastive learning are employed to capture intrinsic associations and ensure
alignments between pathology and gene expression. Extensive experiments on six
datasets demonstrate that our model consistently outperforms existing methods,
achieving over 20% in PCC metrics. Codes are available at
https://github.com/MCPathology/NH2ST

</details>


### [194] [Low-latency vision transformers via large-scale multi-head attention](https://arxiv.org/abs/2506.23832)
*Ronit D. Gross,Tal Halevi,Ella Koresh,Yarden Tzach,Ido Kanter*

Main category: cs.CV

TL;DR: 研究发现多头注意力机制中部分头在分类任务中自发对称性破缺，通过单头性能矩阵提高信噪比，提升分类精度，并优化了视觉Transformer架构。


<details>
  <summary>Details</summary>
Motivation: 探索多头注意力机制在分类任务中的学习机制，以及如何通过量化单头性能优化Transformer架构。

Method: 使用单头性能矩阵（SHP）量化多头注意力的性能，发现其由多个单元簇组成，每个标签由少数头明确识别。

Result: 提高了信噪比和分类精度，并设计出多种视觉Transformer架构，同时通过卷积层替换降低了延迟。

Conclusion: 该机制可扩展至自然语言处理任务，为深度学习提供新见解。

Abstract: The emergence of spontaneous symmetry breaking among a few heads of
multi-head attention (MHA) across transformer blocks in classification tasks
was recently demonstrated through the quantification of single-nodal
performance (SNP). This finding indicates that each head focuses its attention
on a subset of labels through cooperation among its SNPs. This underlying
learning mechanism is generalized to large-scale MHA (LS-MHA) using a single
matrix value representing single-head performance (SHP), analogous to
single-filter performance in convolutional neural networks (CNNs). The results
indicate that each SHP matrix comprises multiple unit clusters such that each
label being explicitly recognized by a few heads with negligible noise. This
leads to an increased signal-to-noise ratio (SNR) along the transformer blocks,
thereby improving classification accuracy. These features give rise to several
distinct vision transformer (ViT) architectures that achieve the same accuracy
but differ in their LS-MHA structures. As a result, their soft committee yields
superior accuracy, an outcome not typically observed in CNNs which rely on
hundreds of filters. In addition, a significant reduction in latency is
achieved without affecting the accuracy by replacing the initial transformer
blocks with convolutional layers. This substitution accelerates early-stage
learning, which is then improved by subsequent transformer layers. The
extension of this learning mechanism to natural language processing tasks,
based on quantitative differences between CNNs and ViT architectures, has the
potential to yield new insights in deep learning. The findings are demonstrated
using compact convolutional transformer architectures trained on the CIFAR-100
dataset.

</details>


### [195] [PointSSIM: A novel low dimensional resolution invariant image-to-image comparison metric](https://arxiv.org/abs/2506.23833)
*Oscar Ovanger,Ragnar Hauge,Jacob Skauvold,Michael J. Pyrcz,Jo Eidsvik*

Main category: cs.CV

TL;DR: PointSSIM是一种低维度的图像比较指标，具有分辨率不变性，适用于不同分辨率的二值图像比较。


<details>
  <summary>Details</summary>
Motivation: 现有图像比较方法在处理不同分辨率的二值图像时效果不佳，需要一种更鲁棒的比较方法。

Method: 通过将二值图像转换为标记点模式表示，提取关键特征（锚点），并使用包含强度、连通性、复杂性和结构属性的摘要向量进行比较。

Result: PointSSIM提供了一种高效可靠的图像比较方法，特别适用于跨分辨率的结构分析应用。

Conclusion: PointSSIM是一种有效的跨分辨率二值图像比较工具，具有广泛的应用潜力。

Abstract: This paper presents PointSSIM, a novel low-dimensional image-to-image
comparison metric that is resolution invariant. Drawing inspiration from the
structural similarity index measure and mathematical morphology, PointSSIM
enables robust comparison across binary images of varying resolutions by
transforming them into marked point pattern representations. The key features
of the image, referred to as anchor points, are extracted from binary images by
identifying locally adaptive maxima from the minimal distance transform. Image
comparisons are then performed using a summary vector, capturing intensity,
connectivity, complexity, and structural attributes. Results show that this
approach provides an efficient and reliable method for image comparison,
particularly suited to applications requiring structural analysis across
different resolutions.

</details>


### [196] [Refine Any Object in Any Scene](https://arxiv.org/abs/2506.23835)
*Ziwei Chen,Ziling Liu,Zitong Huang,Mingqi Gao,Feng Zheng*

Main category: cs.CV

TL;DR: RAISE是一种利用3D生成先验恢复缺失视角下物体几何和外观的3D增强框架，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决场景重建中因视角缺失导致物体建模不完整的问题，提升下游任务的性能。

Method: 采用两阶段细化：先通过3D生成模型替换退化物体，再通过7-DOF位姿对齐和注册约束增强恢复几何与纹理。

Result: 在多个基准测试中，RAISE在新视角合成和几何补全任务上表现优异。

Conclusion: RAISE能有效恢复缺失视角下的物体细节，同时保持场景一致性。

Abstract: Viewpoint missing of objects is common in scene reconstruction, as camera
paths typically prioritize capturing the overall scene structure rather than
individual objects. This makes it highly challenging to achieve high-fidelity
object-level modeling while maintaining accurate scene-level representation.
Addressing this issue is critical for advancing downstream tasks requiring
detailed object understanding and appearance modeling. In this paper, we
introduce Refine Any object In any ScenE (RAISE), a novel 3D enhancement
framework that leverages 3D generative priors to recover fine-grained object
geometry and appearance under missing views. Starting from substituting
degraded objects with proxies, via a 3D generative model with strong 3D
understanding, RAISE progressively refines geometry and texture by aligning
each proxy to its degraded counterpart in 7-DOF pose, followed by correcting
spatial and appearance inconsistencies via registration-constrained
enhancement. This two-stage refinement ensures the high-fidelity geometry and
appearance of the original object in unseen views while maintaining consistency
in spatial positioning, observed geometry, and appearance. Extensive
experiments on challenging benchmarks show that RAISE significantly outperforms
state-of-the-art methods in both novel view synthesis and geometry completion
tasks. RAISE is made publicly available at https://github.com/PolySummit/RAISE.

</details>


### [197] [RGC-VQA: An Exploration Database for Robotic-Generated Video Quality Assessment](https://arxiv.org/abs/2506.23852)
*Jianing Jin,Jiangyong Ying,Huiyu Duan,Liu Yang,Sijing Wu,Yunhao Li,Yushuo Zheng,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 论文提出了机器人生成内容（RGC）的概念，并建立了首个RGC视频数据库（RGCD），用于评估RGC视频的感知质量。通过主观实验和基准测试，发现现有视频质量评估（VQA）模型在RGC视频上表现不佳，呼吁开发针对RGC的VQA模型。


<details>
  <summary>Details</summary>
Motivation: 随着机器人平台融入日常生活，机器人生成的视频（RGC）在流媒体平台上出现，但其独特的失真和视觉需求尚未被研究。缺乏针对RGC视频的质量评估研究。

Method: 建立包含2,100个视频的RGCD数据库，涵盖三类机器人来源的视频。进行主观VQA实验评估人类感知，并对11种先进VQA模型进行基准测试。

Result: 实验显示现有VQA模型在复杂RGC视频上表现不佳，突显了对RGC专用VQA模型的需求。

Conclusion: RGCD数据库为RGC视频质量评估提供了基础，未来需开发针对RGC的VQA模型。

Abstract: As camera-equipped robotic platforms become increasingly integrated into
daily life, robotic-generated videos have begun to appear on streaming media
platforms, enabling us to envision a future where humans and robots coexist. We
innovatively propose the concept of Robotic-Generated Content (RGC) to term
these videos generated from egocentric perspective of robots. The perceptual
quality of RGC videos is critical in human-robot interaction scenarios, and RGC
videos exhibit unique distortions and visual requirements that differ markedly
from those of professionally-generated content (PGC) videos and user-generated
content (UGC) videos. However, dedicated research on quality assessment of RGC
videos is still lacking. To address this gap and to support broader robotic
applications, we establish the first Robotic-Generated Content Database (RGCD),
which contains a total of 2,100 videos drawn from three robot categories and
sourced from diverse platforms. A subjective VQA experiment is conducted
subsequently to assess human visual perception of robotic-generated videos.
Finally, we conduct a benchmark experiment to evaluate the performance of 11
state-of-the-art VQA models on our database. Experimental results reveal
significant limitations in existing VQA models when applied to complex,
robotic-generated content, highlighting a critical need for RGC-specific VQA
models. Our RGCD is publicly available at:
https://github.com/IntMeGroup/RGC-VQA.

</details>


### [198] [A Closer Look at Conditional Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2506.23856)
*Ji Zhang,Shihan Wu,Lianli Gao,Jingkuan Song,Nicu Sebe,Heng Tao Shen*

Main category: cs.CV

TL;DR: 论文提出Class-adaptive Prompt Tuning (CaPT)，通过基于文本类别信息（TCI）的动态提示解决Vision-Language Pretrained Models (VLPMs)中的Base-New Tradeoff (BNT)问题。CaPT可作为插件提升现有无条件提示调优方法的性能，实验表明其显著优于现有条件提示调优方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉图像信息（VII）的条件提示调优方法在解决BNT问题时表现不佳，甚至不如随机噪声条件提示。研究发现，基于文本类别信息（TCI）的动态提示是解决BNT问题的关键。

Method: 提出CaPT方法，通过学习基于TCI的动态提示，快速适应新类别。CaPT可作为插件与现有无条件提示调优方法结合，并进一步与DePT框架集成形成DeCaPT。

Result: 在11个数据集上的实验表明，CaPT显著提升了五种无条件提示调优基线的性能，且计算成本几乎不增加。DeCaPT在平均H ACC上优于现有最佳条件提示调优方法3.49%。

Conclusion: CaPT通过TCI-conditioned提示有效解决了BNT问题，且具有高效性和通用性，可作为插件提升现有方法。

Abstract: Despite the great promise of Prompt Tuning (PT) in adapting large
Vision-Language Pretrained Models (VLPMs) to downstream tasks, they often
struggle to overcome the Base-New Tradeoff (BNT) dilemma: as VLPMs are better
tuned to a base task, their ability to generalize to new tasks diminishes.
Recent work on conditional PT addresses this problem by replacing static
prompts with dynamic Visual Image Information (VII)-conditioned prompts,
improving the model's generalization to new tasks to some extent. In this work,
we first identify a critical issue with existing conditional PT methods: using
VII as the "condition" of prompts yields suboptimal performance, and even
random noise-conditioned prompts can outperform the VII-conditioned
counterparts. On further analysis, we find that learning dynamic prompts
conditioned on Textual Class Information (TCI) is the key to solving the BNT
problem. Motivated by this, we then propose Class-adaptive Prompt Tuning
(CaPT), which enables fast adaptation of tuned models to new classes by
learning TCI-conditioned prompts from base classes. Remarkably, CaPT can be
used as a plugin to mitigate the BNT problem for existing unconditional PT
schemes. Extensive experiments on 11 datasets show that CaPT consistently
improves the performance of five strong unconditional PT baselines with
negligible additional computational cost. Additionally, by integrating CaPT
with our recently proposed DePT framework, we devise a new conditional PT
approach, termed DeCaPT, which outperforms the H ACC of the state-of-the-art
conditional PT scheme by 3.49%, averaged over the 11 datasets. Code:
https://github.com/Koorye/CaPT.

</details>


### [199] [VMoBA: Mixture-of-Block Attention for Video Diffusion Models](https://arxiv.org/abs/2506.23858)
*Jianzong Wu,Liang Hou,Haotian Yang,Xin Tao,Ye Tian,Pengfei Wan,Di Zhang,Yunhai Tong*

Main category: cs.CV

TL;DR: VMoBA是一种专为视频扩散模型设计的稀疏注意力机制，通过动态块分区和全局块选择优化性能，显著提升训练和推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决视频扩散模型中全注意力机制的二次复杂度问题，同时优化稀疏注意力方法对视频时空特性的捕捉能力。

Method: 引入VMoBA，结合层递归块分区、全局块选择和基于阈值的块选择，动态适应时空注意力模式。

Result: VMoBA在训练和推理中分别实现2.92x和2.40x的FLOPs加速，生成质量与全注意力相当或更优。

Conclusion: VMoBA高效且性能优越，适用于长序列视频生成，显著提升计算效率。

Abstract: The quadratic complexity of full attention mechanisms poses a significant
bottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration,
high-resolution videos. While various sparse attention methods have been
proposed, many are designed as training-free inference accelerators or do not
optimally capture the unique spatio-temporal characteristics inherent in video
data when trained natively. This paper introduces Video Mixture of Block
Attention (VMoBA), a novel sparse attention mechanism specifically adapted for
VDMs. Motivated by an in-depth analysis of attention patterns within
pre-trained video transformers, which revealed strong spatio-temporal locality,
varying query importance, and head-specific concentration levels, VMoBA
enhances the original MoBA framework with three key modifications: (1) a
layer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to
diverse spatio-temporal attention patterns and improve efficiency; (2) global
block selection to prioritize the most salient query-key block interactions
across an entire attention head; and (3) threshold-based block selection to
dynamically determine the number of attended blocks based on their cumulative
similarity. Extensive experiments demonstrate that VMoBA significantly
accelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and
1.48x latency speedup, while attaining comparable or even superior generation
quality to full attention. Furthermore, VMoBA exhibits competitive performance
in training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for
high-res video generation.

</details>


### [200] [Puzzles: Unbounded Video-Depth Augmentation for Scalable End-to-End 3D Reconstruction](https://arxiv.org/abs/2506.23863)
*Jiahao Ma,Lei Wang,Miaomiao liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

TL;DR: 论文提出了一种名为Puzzles的数据增强策略，通过单张图像或视频片段合成高质量的姿态-深度数据，显著提升了3D重建模型的性能。


<details>
  <summary>Details</summary>
Motivation: 多视图3D重建在计算机视觉中仍具挑战性，现有方法（如DUST3R）受限于训练数据的多样性和规模。

Method: 通过模拟多样化的相机轨迹和场景几何，Puzzles从单张图像或视频片段合成大量高质量数据。

Result: 实验表明，Puzzles显著提升了3D重建模型的性能，仅需10%的原始数据即可达到与完整数据集相当的精度。

Conclusion: Puzzles是一种高效的数据增强方法，无需修改网络架构即可显著提升3D重建性能。

Abstract: Multi-view 3D reconstruction remains a core challenge in computer vision.
Recent methods, such as DUST3R and its successors, directly regress pointmaps
from image pairs without relying on known scene geometry or camera parameters.
However, the performance of these models is constrained by the diversity and
scale of available training data. In this work, we introduce Puzzles, a data
augmentation strategy that synthesizes an unbounded volume of high-quality
posed video-depth data from a single image or video clip. By simulating diverse
camera trajectories and realistic scene geometry through targeted image
transformations, Puzzles significantly enhances data variety. Extensive
experiments show that integrating Puzzles into existing video-based 3D
reconstruction pipelines consistently boosts performance without modifying the
underlying network architecture. Notably, models trained on only ten percent of
the original data augmented with Puzzles still achieve accuracy comparable to
those trained on the full dataset. Code is available at
https://jiahao-ma.github.io/puzzles/.

</details>


### [201] [Spurious-Aware Prototype Refinement for Reliable Out-of-Distribution Detection](https://arxiv.org/abs/2506.23881)
*Reihaneh Zohrabi,Hosein Hasani,Mahdieh Soleymani Baghshah,Anna Rohrbach,Marcus Rohrbach,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: SPROD是一种新型的原型OOD检测方法，通过优化类别原型减少虚假相关性影响，无需额外数据或调参，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法易受虚假相关性影响，导致模型鲁棒性不足，SPROD旨在解决这一问题。

Method: SPROD通过后处理优化类别原型，减少虚假特征的偏差，适用于多种主干网络和OOD检测场景。

Result: 在多个OOD数据集上，SPROD平均AUROC提升4.7%，FPR@95提升9.3%，表现优于现有方法。

Conclusion: SPROD有效解决了虚假相关性对OOD检测的影响，显著提升了模型的鲁棒性和性能。

Abstract: Out-of-distribution (OOD) detection is crucial for ensuring the reliability
and safety of machine learning models in real-world applications, where they
frequently face data distributions unseen during training. Despite progress,
existing methods are often vulnerable to spurious correlations that mislead
models and compromise robustness. To address this, we propose SPROD, a novel
prototype-based OOD detection approach that explicitly addresses the challenge
posed by unknown spurious correlations. Our post-hoc method refines class
prototypes to mitigate bias from spurious features without additional data or
hyperparameter tuning, and is broadly applicable across diverse backbones and
OOD detection settings. We conduct a comprehensive spurious correlation OOD
detection benchmarking, comparing our method against existing approaches and
demonstrating its superior performance across challenging OOD datasets, such as
CelebA, Waterbirds, UrbanCars, Spurious Imagenet, and the newly introduced
Animals MetaCoCo. On average, SPROD improves AUROC by 4.7% and FPR@95 by 9.3%
over the second best.

</details>


### [202] [PriOr-Flow: Enhancing Primitive Panoramic Optical Flow with Orthogonal View](https://arxiv.org/abs/2506.23897)
*Longliang Liu,Miaojie Feng,Junda Cheng,Jijun Xiang,Xuan Zhu,Xin Yang*

Main category: cs.CV

TL;DR: PriOr-Flow提出了一种双分支框架，利用正交视图的低失真特性提升全景光流估计性能，特别是在极地区域。


<details>
  <summary>Details</summary>
Motivation: 传统基于透视的光流方法在全景投影（如ERP）中因严重失真而性能下降，尤其在极地区域。

Method: 提出DCCL算子联合检索原始和正交成本体积信息，并设计ODDC模块迭代优化运动特征。

Result: PriOr-Flow在公开全景光流数据集上表现优异，成为宽视场运动估计的新标杆。

Conclusion: PriOr-Flow有效解决了全景光流中的失真问题，兼容多种迭代光流方法，性能领先。

Abstract: Panoramic optical flow enables a comprehensive understanding of temporal
dynamics across wide fields of view. However, severe distortions caused by
sphere-to-plane projections, such as the equirectangular projection (ERP),
significantly degrade the performance of conventional perspective-based optical
flow methods, especially in polar regions. To address this challenge, we
propose PriOr-Flow, a novel dual-branch framework that leverages the
low-distortion nature of the orthogonal view to enhance optical flow estimation
in these regions. Specifically, we introduce the Dual-Cost Collaborative Lookup
(DCCL) operator, which jointly retrieves correlation information from both the
primitive and orthogonal cost volumes, effectively mitigating distortion noise
during cost volume construction. Furthermore, our Ortho-Driven Distortion
Compensation (ODDC) module iteratively refines motion features from both
branches, further suppressing polar distortions. Extensive experiments
demonstrate that PriOr-Flow is compatible with various perspective-based
iterative optical flow methods and consistently achieves state-of-the-art
performance on publicly available panoramic optical flow datasets, setting a
new benchmark for wide-field motion estimation. The code is publicly available
at: https://github.com/longliangLiu/PriOr-Flow.

</details>


### [203] [GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models](https://arxiv.org/abs/2506.23903)
*Hamza Rasaee,Taha Koleilat,Hassan Rivaz*

Main category: cs.CV

TL;DR: 提出了一种基于提示驱动的视觉语言模型（VLM），结合Grounding DINO和SAM2，用于多器官超声图像分割，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 超声图像分割面临解剖变异性、成像协议多样性和标注数据有限等挑战，需要一种通用且准确的方法。

Method: 使用18个公共超声数据集，通过LoRA微调Grounding DINO，结合SAM2实现分割，并在未见数据集上测试。

Result: 在多数已见数据集上优于现有方法（如UniverSeg、MedSAM等），在未见数据集上表现稳定。

Conclusion: VLM在超声图像分析中具有潜力，减少对大规模标注数据的依赖，代码将公开。

Abstract: Accurate and generalizable object segmentation in ultrasound imaging remains
a significant challenge due to anatomical variability, diverse imaging
protocols, and limited annotated data. In this study, we propose a
prompt-driven vision-language model (VLM) that integrates Grounding DINO with
SAM2 to enable object segmentation across multiple ultrasound organs. A total
of 18 public ultrasound datasets, encompassing the breast, thyroid, liver,
prostate, kidney, and paraspinal muscle, were utilized. These datasets were
divided into 15 for fine-tuning and validation of Grounding DINO using Low Rank
Adaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for
testing to evaluate performance in unseen distributions. Comprehensive
experiments demonstrate that our approach outperforms state-of-the-art
segmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse,
and SAMUS on most seen datasets while maintaining strong performance on unseen
datasets without additional fine-tuning. These results underscore the promise
of VLMs in scalable and robust ultrasound image analysis, reducing dependence
on large, organ-specific annotated datasets. We will publish our code on
code.sonography.ai after acceptance.

</details>


### [204] [Three-dimensional end-to-end deep learning for brain MRI analysis](https://arxiv.org/abs/2506.23916)
*Radhika Juglan,Marta Ligero,Zunamys I. Carrero,Asier Rabasco,Tim Lenz,Leo Misera,Gregory Patrick Veldhuizen,Paul Kuntke,Hagen H. Kitzler,Sven Nebelung,Daniel Truhn,Jakob Nikolas Kather*

Main category: cs.CV

TL;DR: 研究发现，在脑影像分析中，简单的卷积网络（SFCN）比复杂的注意力架构（如Swin Transformer）表现更好，具有更强的跨数据集泛化能力。


<details>
  <summary>Details</summary>
Motivation: 评估深度学习模型在脑影像中预测年龄和性别的泛化能力，特别是针对不同人群的影像数据。

Method: 比较了三种3D架构（SFCN、DenseNet、Swin Transformer）在四个独立队列（UKB、DLBS、PPMI、IXI）中预测年龄和性别的性能。

Result: SFCN在性别分类和年龄预测任务中均表现最佳，泛化能力优于复杂架构。

Conclusion: 简单的卷积网络在脑影像分析中更具优势，尤其是在跨数据集泛化方面。

Abstract: Deep learning (DL) methods are increasingly outperforming classical
approaches in brain imaging, yet their generalizability across diverse imaging
cohorts remains inadequately assessed. As age and sex are key neurobiological
markers in clinical neuroscience, influencing brain structure and disease risk,
this study evaluates three of the existing three-dimensional architectures,
namely Simple Fully Connected Network (SFCN), DenseNet, and Shifted Window
(Swin) Transformers, for age and sex prediction using T1-weighted MRI from four
independent cohorts: UK Biobank (UKB, n=47,390), Dallas Lifespan Brain Study
(DLBS, n=132), Parkinson's Progression Markers Initiative (PPMI, n=108 healthy
controls), and Information eXtraction from Images (IXI, n=319). We found that
SFCN consistently outperformed more complex architectures with AUC of 1.00
[1.00-1.00] in UKB (internal test set) and 0.85-0.91 in external test sets for
sex classification. For the age prediction task, SFCN demonstrated a mean
absolute error (MAE) of 2.66 (r=0.89) in UKB and 4.98-5.81 (r=0.55-0.70) across
external datasets. Pairwise DeLong and Wilcoxon signed-rank tests with
Bonferroni corrections confirmed SFCN's superiority over Swin Transformer
across most cohorts (p<0.017, for three comparisons). Explainability analysis
further demonstrates the regional consistency of model attention across cohorts
and specific to each task. Our findings reveal that simpler convolutional
networks outperform the denser and more complex attention-based DL
architectures in brain image analysis by demonstrating better generalizability
across different datasets.

</details>


### [205] [Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers](https://arxiv.org/abs/2506.23918)
*Zhaochen Su,Peng Xia,Hangyu Guo,Zhenhua Liu,Yan Ma,Xiaoye Qu,Jiaqi Liu,Yanshu Li,Kaide Zeng,Zhengyuan Yang,Linjie Li,Yu Cheng,Heng Ji,Junxian He,Yi R.,Fung*

Main category: cs.CV

TL;DR: 论文探讨了多模态推理中的新范式——视觉动态思维，从静态视觉输入转向动态视觉认知，提出了三阶段框架，并总结了方法、评估和应用。


<details>
  <summary>Details</summary>
Motivation: 解决文本链式思维（CoT）在视觉推理中的语义鸿沟问题，推动AI从静态视觉处理转向动态视觉思维。

Method: 提出三阶段框架：外部工具探索、程序化操作和内在想象，并总结了各阶段的核心方法。

Result: 建立了视觉动态思维范式，提供了全面的方法综述、评估基准和应用分析。

Conclusion: 该研究为未来多模态AI的发展提供了清晰路线图，强调了视觉动态思维的重要性。

Abstract: Recent progress in multimodal reasoning has been significantly advanced by
textual Chain-of-Thought (CoT), a paradigm where models conduct reasoning
within language. This text-centric approach, however, treats vision as a
static, initial context, creating a fundamental "semantic gap" between rich
perceptual data and discrete symbolic thought. Human cognition often transcends
language, utilizing vision as a dynamic mental sketchpad. A similar evolution
is now unfolding in AI, marking a fundamental paradigm shift from models that
merely think about images to those that can truly think with images. This
emerging paradigm is characterized by models leveraging visual information as
intermediate steps in their thought process, transforming vision from a passive
input into a dynamic, manipulable cognitive workspace. In this survey, we chart
this evolution of intelligence along a trajectory of increasing cognitive
autonomy, which unfolds across three key stages: from external tool
exploration, through programmatic manipulation, to intrinsic imagination. To
structure this rapidly evolving field, our survey makes four key contributions.
(1) We establish the foundational principles of the think with image paradigm
and its three-stage framework. (2) We provide a comprehensive review of the
core methods that characterize each stage of this roadmap. (3) We analyze the
critical landscape of evaluation benchmarks and transformative applications.
(4) We identify significant challenges and outline promising future directions.
By providing this structured overview, we aim to offer a clear roadmap for
future research towards more powerful and human-aligned multimodal AI.

</details>


### [206] [Evaluating the Impact of Khmer Font Types on Text Recognition](https://arxiv.org/abs/2506.23963)
*Vannkinh Nom,Souhail Bakkali,Muhammad Muzzamil Luqman,Mickael Coustaty,Jean-Marc Ogier*

Main category: cs.CV

TL;DR: 研究了19种高棉字体对OCR识别准确率的影响，发现部分字体表现优异，而部分表现较差。


<details>
  <summary>Details</summary>
Motivation: 高棉字体多样性对OCR系统识别复杂脚本（如高棉语）的准确性构成挑战。

Method: 使用Pytesseract评估19种随机选择的高棉字体对文本识别准确率的影响。

Result: Khmer、Odor MeanChey等字体表现优异，而iSeth First等字体表现较差。

Conclusion: 字体选择对优化高棉文本识别至关重要，为开发更鲁棒的OCR系统提供了见解。

Abstract: Text recognition is significantly influenced by font types, especially for
complex scripts like Khmer. The variety of Khmer fonts, each with its unique
character structure, presents challenges for optical character recognition
(OCR) systems. In this study, we evaluate the impact of 19 randomly selected
Khmer font types on text recognition accuracy using Pytesseract. The fonts
include Angkor, Battambang, Bayon, Bokor, Chenla, Dangrek, Freehand, Kh Kompong
Chhnang, Kh SN Kampongsom, Khmer, Khmer CN Stueng Songke, Khmer Savuth Pen,
Metal, Moul, Odor MeanChey, Preah Vihear, Siemreap, Sithi Manuss, and iSeth
First. Our comparison of OCR performance across these fonts reveals that Khmer,
Odor MeanChey, Siemreap, Sithi Manuss, and Battambang achieve high accuracy,
while iSeth First, Bayon, and Dangrek perform poorly. This study underscores
the critical importance of font selection in optimizing Khmer text recognition
and provides valuable insights for developing more robust OCR systems.

</details>


### [207] [Visual and Memory Dual Adapter for Multi-Modal Object Tracking](https://arxiv.org/abs/2506.23972)
*Boyue Xu,Ruichao Hou,Tongwei Ren,Gangshan Wu*

Main category: cs.CV

TL;DR: 提出了一种新颖的视觉与记忆双重适配器（VMDA），通过联合建模频率、空间和通道特征，以及模拟人类记忆机制，显著提升了多模态跟踪的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在频率和时间域中未能充分利用关键线索，导致提示学习不可靠。

Method: 设计了视觉适配器和记忆适配器，前者自适应转移辅助模态的判别线索，后者存储全局时间线索并动态更新。

Result: 在RGB-热成像、RGB-深度和RGB-事件跟踪等多种任务中实现了最先进的性能。

Conclusion: VMDA通过双重适配器显著提升了多模态跟踪的鲁棒性和判别性。

Abstract: Prompt-learning-based multi-modal trackers have achieved promising progress
by employing lightweight visual adapters to incorporate auxiliary modality
features into frozen foundation models. However, existing approaches often
struggle to learn reliable prompts due to limited exploitation of critical cues
across frequency and temporal domains. In this paper, we propose a novel visual
and memory dual adapter (VMDA) to construct more robust and discriminative
representations for multi-modal tracking. Specifically, we develop a simple but
effective visual adapter that adaptively transfers discriminative cues from
auxiliary modality to dominant modality by jointly modeling the frequency,
spatial, and channel-wise features. Additionally, we design the memory adapter
inspired by the human memory mechanism, which stores global temporal cues and
performs dynamic update and retrieval operations to ensure the consistent
propagation of reliable temporal information across video sequences. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
on the various multi-modal tracking tasks, including RGB-Thermal, RGB-Depth,
and RGB-Event tracking. Code and models are available at
https://github.com/xuboyue1999/mmtrack.git.

</details>


### [208] [Toward Simple and Robust Contrastive Explanations for Image Classification by Leveraging Instance Similarity and Concept Relevance](https://arxiv.org/abs/2506.23975)
*Yuliia Kaidashova,Bettina Finzel,Ute Schmid*

Main category: cs.CV

TL;DR: 论文研究了基于概念的对比解释方法，用于图像分类模型，探讨了解释复杂度与概念相关性及图像增强的关系。


<details>
  <summary>Details</summary>
Motivation: 理解分类模型为何偏好某一类别是挑战，研究旨在通过概念相关性提供更可解释的对比解释。

Method: 利用实例嵌入相似性和人类可理解概念的相关性，提取概念并计算对比解释，评估解释复杂度及鲁棒性。

Result: 高相关性概念生成更简短的解释，低相关性则更复杂；解释在不同图像增强下表现不一。

Conclusion: 研究为构建更可解释和鲁棒的AI系统提供了潜在方向。

Abstract: Understanding why a classification model prefers one class over another for
an input instance is the challenge of contrastive explanation. This work
implements concept-based contrastive explanations for image classification by
leveraging the similarity of instance embeddings and relevance of
human-understandable concepts used by a fine-tuned deep learning model. Our
approach extracts concepts with their relevance score, computes contrasts for
similar instances, and evaluates the resulting contrastive explanations based
on explanation complexity. Robustness is tested for different image
augmentations. Two research questions are addressed: (1) whether explanation
complexity varies across different relevance ranges, and (2) whether
explanation complexity remains consistent under image augmentations such as
rotation and noise. The results confirm that for our experiments higher concept
relevance leads to shorter, less complex explanations, while lower relevance
results in longer, more diffuse explanations. Additionally, explanations show
varying degrees of robustness. The discussion of these findings offers insights
into the potential of building more interpretable and robust AI systems.

</details>


### [209] [StyleDrive: Towards Driving-Style Aware Benchmarking of End-To-End Autonomous Driving](https://arxiv.org/abs/2506.23982)
*Ruiyang Hao,Bowen Jing,Haibao Yu,Zaiqing Nie*

Main category: cs.CV

TL;DR: 论文提出了首个大规模真实世界数据集，用于端到端自动驾驶（E2EAD）的个性化研究，通过结合静态环境特征和动态上下文线索生成高质量偏好标注，并建立了首个个性化E2EAD评估基准。


<details>
  <summary>Details</summary>
Motivation: 传统自动驾驶系统已探索个性化，但在端到端自动驾驶（E2EAD）中仍被忽视，而用户对齐行为对信任和广泛采用至关重要。缺乏标注多样化驾驶偏好的大规模数据集是主要障碍。

Method: 从真实道路拓扑提取静态特征，使用微调视觉语言模型（VLM）推断动态上下文，结合行为分布分析和基于规则的启发式生成客观偏好标注，并通过VLM建模场景语义和驾驶员行为生成主观标注，最终通过人工验证融合两者。

Result: 实验表明，结合个性化偏好的模型能生成更符合人类驾驶的行为。

Conclusion: 该研究为个性化E2EAD奠定了基础，提供了标准化平台，推动以人为中心的自动驾驶研究。

Abstract: While personalization has been explored in traditional autonomous driving
systems, it remains largely overlooked in end-to-end autonomous driving
(E2EAD), despite its growing prominence. This gap is critical, as user-aligned
behavior is essential for trust, comfort, and widespread adoption of autonomous
vehicles. A core challenge is the lack of large-scale real-world datasets
annotated with diverse and fine-grained driving preferences, hindering the
development and evaluation of personalized E2EAD models. In this work, we
present the first large-scale real-world dataset enriched with annotations
capturing diverse driving preferences, establishing a foundation for
personalization in E2EAD. We extract static environmental features from
real-world road topology and infer dynamic contextual cues using a fine-tuned
visual language model (VLM), enabling consistent and fine-grained scenario
construction. Based on these scenarios, we derive objective preference
annotations through behavioral distribution analysis and rule-based heuristics.
To address the inherent subjectivity of driving style, we further employ the
VLM to generate subjective annotations by jointly modeling scene semantics and
driver behavior. Final high-quality labels are obtained through a
human-in-the-loop verification process that fuses both perspectives. Building
on this dataset, we propose the first benchmark for evaluating personalized
E2EAD models. We assess several state-of-the-art models with and without
preference conditioning, demonstrating that incorporating personalized
preferences results in behavior more aligned with human driving. Our work lays
the foundation for personalized E2EAD by providing a standardized platform to
systematically integrate human preferences into data-driven E2EAD systems,
catalyzing future research in human-centric autonomy.

</details>


### [210] [Ella: Embodied Social Agents with Lifelong Memory](https://arxiv.org/abs/2506.24019)
*Hongxin Zhang,Zheyuan Zhang,Zeyuan Wang,Zunzhe Zhang,Lixing Fang,Qinhong Zhou,Chuang Gan*

Main category: cs.CV

TL;DR: Ella是一个能够在3D开放世界中通过视觉观察和社交互动进行终身学习的社交智能体，其核心是多模态记忆系统与基础模型的结合。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过结构化记忆系统和基础模型提升具身智能体的终身学习能力，使其在动态环境中自主进化。

Method: Ella采用名称中心语义记忆和时空情景记忆的多模态记忆系统，结合基础模型进行信息检索和决策。

Result: 实验表明Ella能有效影响、领导和合作其他智能体完成任务，展示了其通过观察和社交互动的学习能力。

Conclusion: 结合结构化记忆系统与基础模型具有推动具身智能发展的潜力。

Abstract: We introduce Ella, an embodied social agent capable of lifelong learning
within a community in a 3D open world, where agents accumulate experiences and
acquire knowledge through everyday visual observations and social interactions.
At the core of Ella's capabilities is a structured, long-term multimodal memory
system that stores, updates, and retrieves information effectively. It consists
of a name-centric semantic memory for organizing acquired knowledge and a
spatiotemporal episodic memory for capturing multimodal experiences. By
integrating this lifelong memory system with foundation models, Ella retrieves
relevant information for decision-making, plans daily activities, builds social
relationships, and evolves autonomously while coexisting with other intelligent
beings in the open world. We conduct capability-oriented evaluations in a
dynamic 3D open world where 15 agents engage in social activities for days and
are assessed with a suite of unseen controlled evaluations. Experimental
results show that Ella can influence, lead, and cooperate with other agents
well to achieve goals, showcasing its ability to learn effectively through
observation and social interaction. Our findings highlight the transformative
potential of combining structured memory systems with foundation models for
advancing embodied intelligence. More videos can be found at
https://umass-embodied-agi.github.io/Ella/.

</details>


### [211] [Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data](https://arxiv.org/abs/2506.24039)
*Shubhabrata Mukherjee,Jack Lang,Obeen Kwon,Iryna Zenyuk,Valerie Brogden,Adam Weber,Daniela Ushizima*

Main category: cs.CV

TL;DR: Zenesis是一个无需代码的交互式平台，通过多模态适应技术和人机协作优化，显著提升了科学图像分析的准确性，尤其在稀缺数据场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统零样本和基于提示的技术在处理稀缺科学图像时表现不佳，因此需要一种能够降低数据准备障碍的解决方案。

Method: 开发了轻量级多模态适应技术，支持零样本操作，并结合人机协作和启发式时间增强选项。

Result: 在FIB-SEM数据上，Zenesis的平均准确率、IOU和Dice分数显著优于基线方法，如Otsu阈值和SAM模型。

Conclusion: Zenesis是科学图像分析的强大工具，特别适用于缺乏高质量标注数据的领域，能加速实验图像的准确分析。

Abstract: Zero-shot and prompt-based technologies capitalized on using frequently
occurring images to transform visual reasoning tasks, which explains why such
technologies struggle with valuable yet scarce scientific image sets. In this
work, we propose Zenesis, a comprehensive no-code interactive platform designed
to minimize barriers posed by data readiness for scientific images. We develop
lightweight multi-modal adaptation techniques that enable zero-shot operation
on raw scientific data, along with human-in-the-loop refinement and
heuristic-based temporal enhancement options. We demonstrate the performance of
our approach through comprehensive comparison and validation on challenging
Focused Ion Beam Scanning Electron Microscopy (FIB-SEM) data of catalyst-loaded
membranes. Zenesis significantly outperforms baseline methods, achieving an
average accuracy of 0.947, an Intersection over Union (IOU) of 0.858, and a
Dice score of 0.923 for amorphous catalyst samples and accuracy of 0.987, an
IOU of 0.857, and a Dice score of 0.923 for crystalline samples. These results
mark a substantial improvement over traditional methods like Otsu thresholding
and even advanced models like Segment Anything Model (SAM) when used in
isolation. Our results demonstrate that Zenesis is a powerful tool for
scientific applications, particularly in fields where high-quality annotated
datasets are unavailable, accelerating accurate analysis of experimental
imaging.

</details>


### [212] [A Survey on Vision-Language-Action Models for Autonomous Driving](https://arxiv.org/abs/2506.24044)
*Sicong Jiang,Zilin Huang,Kangan Qian,Ziang Luo,Tianze Zhu,Yang Zhong,Yihong Tang,Menglin Kong,Yunlong Wang,Siwen Jiao,Hao Ye,Zihao Sheng,Xin Zhao,Tuopu Wen,Zheng Fu,Sikai Chen,Kun Jiang,Diange Yang,Seongjin Choi,Lijun Sun*

Main category: cs.CV

TL;DR: 本文综述了视觉-语言-动作（VLA）模型在自动驾驶领域的应用，总结了架构演变、代表性模型比较、数据集与基准，并提出了未来挑战与方向。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶领域需要整合视觉感知、自然语言理解与控制的单一策略，但现有研究分散且快速扩展，亟需系统梳理。

Method: 通过综述方法，归纳VLA4AD的架构模块、模型演变，比较20多种代表性模型，并整理数据集与评测协议。

Result: 提供了VLA4AD领域的全面概述，包括模型比较、数据集整合及评测标准，突出了安全性、准确性和解释性。

Conclusion: VLA4AD面临鲁棒性、实时效率和形式验证等挑战，未来需推动可解释与社会对齐的自动驾驶发展。

Abstract: The rapid progress of multimodal large language models (MLLM) has paved the
way for Vision-Language-Action (VLA) paradigms, which integrate visual
perception, natural language understanding, and control within a single policy.
Researchers in autonomous driving are actively adapting these methods to the
vehicle domain. Such models promise autonomous vehicles that can interpret
high-level instructions, reason about complex traffic scenes, and make their
own decisions. However, the literature remains fragmented and is rapidly
expanding. This survey offers the first comprehensive overview of VLA for
Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks
shared across recent work, (ii) trace the evolution from early explainer to
reasoning-centric VLA models, and (iii) compare over 20 representative models
according to VLA's progress in the autonomous driving domain. We also
consolidate existing datasets and benchmarks, highlighting protocols that
jointly measure driving safety, accuracy, and explanation quality. Finally, we
detail open challenges - robustness, real-time efficiency, and formal
verification - and outline future directions of VLA4AD. This survey provides a
concise yet complete reference for advancing interpretable socially aligned
autonomous vehicles. Github repo is available at
\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.

</details>


### [213] [Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios](https://arxiv.org/abs/2506.24063)
*Deng Li,Aming Wu,Yang Li,Yaowei Wang,Yahong Han*

Main category: cs.CV

TL;DR: 论文提出了一种新的持续测试时适应方法，通过参数生成机制和双路径适配器提升目标检测器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实环境中数据分布随时间空间变化，传统闭集假设下的目标检测器难以适应，需改进持续测试时适应方法。

Method: 设计了双路径LoRA域感知适配器分离特征，结合条件扩散参数生成机制和类中心最优传输对齐方法。

Result: 在多种连续域自适应目标检测任务中表现优异，生成参数能捕捉更多目标相关信息。

Conclusion: 新方法通过参数生成和特征解耦有效提升了检测器的适应能力和泛化性能。

Abstract: In practice, environments constantly change over time and space, posing
significant challenges for object detectors trained based on a closed-set
assumption, i.e., training and test data share the same distribution. To this
end, continual test-time adaptation has attracted much attention, aiming to
improve detectors' generalization by fine-tuning a few specific parameters,
e.g., BatchNorm layers. However, based on a small number of test images,
fine-tuning certain parameters may affect the representation ability of other
fixed parameters, leading to performance degradation. Instead, we explore a new
mechanism, i.e., converting the fine-tuning process to a specific-parameter
generation. Particularly, we first design a dual-path LoRA-based domain-aware
adapter that disentangles features into domain-invariant and domain-specific
components, enabling efficient adaptation. Additionally, a conditional
diffusion-based parameter generation mechanism is presented to synthesize the
adapter's parameters based on the current environment, preventing the
optimization from getting stuck in local optima. Finally, we propose a
class-centered optimal transport alignment method to mitigate catastrophic
forgetting. Extensive experiments conducted on various continuous domain
adaptive object detection tasks demonstrate the effectiveness. Meanwhile,
visualization results show that the representation extracted by the generated
parameters can capture more object-related information and strengthen the
generalization ability.

</details>


### [214] [Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention](https://arxiv.org/abs/2506.24085)
*Wonwoong Cho,Yanxia Zhang,Yan-Ying Chen,David I. Inouye*

Main category: cs.CV

TL;DR: IT-Blender是一个基于T2I扩散适配器的工具，用于自动化视觉和文本概念的混合，以增强人类创造力。


<details>
  <summary>Details</summary>
Motivation: 人类在跨模态概念混合中存在认知偏差，如设计固定，导致设计空间局部最优。IT-Blender旨在解决这一问题。

Method: 利用预训练的扩散模型（SD和FLUX）混合干净参考图像和生成噪声图像的潜在表示，结合混合注意力机制。

Result: IT-Blender在视觉和文本概念混合方面大幅优于基线方法。

Conclusion: IT-Blender展示了图像生成模型在增强人类创造力方面的新应用潜力。

Abstract: Blending visual and textual concepts into a new visual concept is a unique
and powerful trait of human beings that can fuel creativity. However, in
practice, cross-modal conceptual blending for humans is prone to cognitive
biases, like design fixation, which leads to local minima in the design space.
In this paper, we propose a T2I diffusion adapter "IT-Blender" that can
automate the blending process to enhance human creativity. Prior works related
to cross-modal conceptual blending are limited in encoding a real image without
loss of details or in disentangling the image and text inputs. To address these
gaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend
the latent representations of a clean reference image with those of the noisy
generated image. Combined with our novel blended attention, IT-Blender encodes
the real reference image without loss of details and blends the visual concept
with the object specified by the text in a disentangled way. Our experiment
results show that IT-Blender outperforms the baselines by a large margin in
blending visual and textual concepts, shedding light on the new application of
image generative models to augment human creativity.

</details>


### [215] [MotionGPT3: Human Motion as a Second Modality](https://arxiv.org/abs/2506.24086)
*Bingfan Zhu,Biao Jiang,Sunyi Wang,Shixiang Tang,Tao Chen,Linjie Luo,Youyi Zheng,Xin Chen*

Main category: cs.CV

TL;DR: MotionGPT3是一个双模态运动-语言模型，通过分离参数处理运动建模，解决了运动模态与离散表示之间的重建差距和统一训练中语言智能退化的问题。


<details>
  <summary>Details</summary>
Motivation: 开发统一的运动-语言模型，解决运动模态与离散表示之间的重建差距以及语言智能退化问题。

Method: 采用混合专家方法，分离运动建模参数，通过共享注意力机制实现双模态交互，使用运动变分自编码器（VAE）和扩散头预测运动潜在表示。

Result: 在运动理解和生成任务中表现优异，同时保持了强大的语言能力。

Conclusion: MotionGPT3为自回归框架下的统一双模态运动扩散模型提供了有效解决方案。

Abstract: Though recent advances in multimodal models have demonstrated strong
capabilities and opportunities in unified understanding and generation, the
development of unified motion-language models remains underexplored. To enable
such models with high-fidelity human motion, two core challenges must be
addressed. The first is the reconstruction gap between the continuous motion
modality and discrete representation in an autoregressive manner, and the
second is the degradation of language intelligence during unified training.
Inspired by the mixture of experts, we propose MotionGPT3, a bimodal
motion-language model that treats human motion as a second modality, decoupling
motion modeling via separate model parameters and enabling both effective
cross-modal interaction and efficient multimodal scaling training. To preserve
language intelligence, the text branch retains the original structure and
parameters of the pretrained language model, while a new motion branch is
integrated via a shared attention mechanism, enabling bidirectional information
flow between two modalities. We first employ a motion Variational Autoencoder
(VAE) to encode raw human motion into latent representations. Based on this
continuous latent space, the motion branch predicts motion latents directly
from intermediate hidden states using a diffusion head, bypassing discrete
tokenization. Extensive experiments show that our approach achieves competitive
performance on both motion understanding and generation tasks while preserving
strong language capabilities, establishing a unified bimodal motion diffusion
framework within an autoregressive manner.

</details>


### [216] [MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction](https://arxiv.org/abs/2506.24096)
*Antoine Guédon,Diego Gomez,Nissim Maruani,Bingchen Gong,George Drettakis,Maks Ovsjanikov*

Main category: cs.CV

TL;DR: MILo是一种新颖的高斯泼溅框架，通过可微分地从3D高斯中提取网格，解决了从体积表示到表面表示的转换问题，同时保留了训练中捕获的几何细节。


<details>
  <summary>Details</summary>
Motivation: 当前方法在从高斯泼溅重建3D场景时，提取表面网格需要昂贵的后处理步骤，导致几何细节丢失或生成过于密集的网格。MILo旨在通过直接优化高斯参数来提取网格，避免这些问题。

Method: MILo设计了一种完全可微分的过程，直接从高斯参数构建网格（包括顶点位置和连接性），并引入了双向一致性框架、自适应网格提取过程和基于高斯的有符号距离计算方法。

Result: MILo能够以最先进的质量重建完整场景，同时比之前的方法减少了一个数量级的网格顶点数量，生成的网格轻量且适合下游应用。

Conclusion: MILo通过可微分网格提取，成功弥合了体积和表面表示之间的差距，为3D重建提供了高效且高质量的解决方案。

Abstract: While recent advances in Gaussian Splatting have enabled fast reconstruction
of high-quality 3D scenes from images, extracting accurate surface meshes
remains a challenge. Current approaches extract the surface through costly
post-processing steps, resulting in the loss of fine geometric details or
requiring significant time and leading to very dense meshes with millions of
vertices. More fundamentally, the a posteriori conversion from a volumetric to
a surface representation limits the ability of the final mesh to preserve all
geometric structures captured during training. We present MILo, a novel
Gaussian Splatting framework that bridges the gap between volumetric and
surface representations by differentiably extracting a mesh from the 3D
Gaussians. We design a fully differentiable procedure that constructs the
mesh-including both vertex locations and connectivity-at every iteration
directly from the parameters of the Gaussians, which are the only quantities
optimized during training. Our method introduces three key technical
contributions: a bidirectional consistency framework ensuring both
representations-Gaussians and the extracted mesh-capture the same underlying
geometry during training; an adaptive mesh extraction process performed at each
training iteration, which uses Gaussians as differentiable pivots for Delaunay
triangulation; a novel method for computing signed distance values from the 3D
Gaussians that enables precise surface extraction while avoiding geometric
erosion. Our approach can reconstruct complete scenes, including backgrounds,
with state-of-the-art quality while requiring an order of magnitude fewer mesh
vertices than previous methods. Due to their light weight and empty interior,
our meshes are well suited for downstream applications such as physics
simulations or animation.

</details>


### [217] [DenseWorld-1M: Towards Detailed Dense Grounded Caption in the Real World](https://arxiv.org/abs/2506.24102)
*Xiangtai Li,Tao Zhang,Yanwei Li,Haobo Yuan,Shihao Chen,Yikang Zhou,Jiahao Meng,Yueyi Sun,Shilin Xu,Lu Qi,Tianheng Cheng,Yi Lin,Zilong Huang,Wenhao Huang,Jiashi Feng,Guang Shi*

Main category: cs.CV

TL;DR: DenseWorld-1M是一个大规模、详细、密集的接地标注数据集，填补了现有数据集的不足，并通过三阶段标注流程和两个VLM模型提升了标注效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有标注数据集缺乏对视觉实体的详细描述和关系标注，尤其是在高分辨率图像上。

Method: 采用三阶段标注流程：开放世界感知、详细对象标注生成、密集标注合并，并引入两个VLM模型辅助标注。

Result: DenseWorld-1M在多种任务（如视觉语言理解、视觉接地、区域标注生成）中表现出有效性。

Conclusion: DenseWorld-1M为社区提供了一个高质量的数据集，解决了现有数据集的不足，并通过创新方法提升了标注效率。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate a complex understanding
of scenes, benefiting from large-scale and high-quality datasets. Most existing
caption datasets lack the ground locations and relations for visual entities.
Several grounded caption datasets face the problems of missing detailed
descriptions, relations, and massive object descriptions on high-resolution
images. To fill this gap for the community, we present DenseWorld-1M, the first
massive, detailed, dense grounded caption dataset in the real world. We design
a three-stage labeling pipeline, containing open-world perception, detailed
object caption generation, and dense caption merging. The first stage obtains
entity-level masks and labels. The second stage generates the object-level,
detailed captions with the guidance of masks and labels from the first stage.
The final stage merges object captions and masks into spatial and relational
dense captions. To accelerate the labeling process and improve caption quality,
we present two VLM models: the Detailed Region Caption model and the Spatial
Caption Merging model. Extensive experiments on various settings, including
vision-language understanding, visual grounding, and region caption generation,
demonstrate the effectiveness of our DenseWorld-1M dataset and labeling models.

</details>


### [218] [Epona: Autoregressive Diffusion World Model for Autonomous Driving](https://arxiv.org/abs/2506.24113)
*Kaiwen Zhang,Zhenyu Tang,Xiaotao Hu,Xingang Pan,Xiaoyang Guo,Yuan Liu,Jingwei Huang,Li Yuan,Qian Zhang,Xiao-Xiao Long,Xun Cao,Wei Yin*

Main category: cs.CV

TL;DR: Epona是一种自回归扩散世界模型，通过解耦时空因子化和模块化轨迹与视频预测，实现了高分辨率、长时长的视频生成，并在自动驾驶世界建模中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于视频扩散的世界模型在灵活长度、长时预测和轨迹规划整合方面存在不足，需要改进。

Method: 提出Epona模型，采用解耦时空因子化和模块化轨迹与视频预测方法，结合链式前向训练策略。

Result: 实验结果显示，Epona在FVD指标上提升7.4%，预测时长更长，并在NAVSIM基准测试中优于端到端规划器。

Conclusion: Epona在视频生成和运动规划方面表现出色，为自动驾驶世界建模提供了有效解决方案。

Abstract: Diffusion models have demonstrated exceptional visual quality in video
generation, making them promising for autonomous driving world modeling.
However, existing video diffusion-based world models struggle with
flexible-length, long-horizon predictions and integrating trajectory planning.
This is because conventional video diffusion models rely on global joint
distribution modeling of fixed-length frame sequences rather than sequentially
constructing localized distributions at each timestep. In this work, we propose
Epona, an autoregressive diffusion world model that enables localized
spatiotemporal distribution modeling through two key innovations: 1) Decoupled
spatiotemporal factorization that separates temporal dynamics modeling from
fine-grained future world generation, and 2) Modular trajectory and video
prediction that seamlessly integrate motion planning with visual modeling in an
end-to-end framework. Our architecture enables high-resolution, long-duration
generation while introducing a novel chain-of-forward training strategy to
address error accumulation in autoregressive loops. Experimental results
demonstrate state-of-the-art performance with 7.4\% FVD improvement and minutes
longer prediction duration compared to prior works. The learned world model
further serves as a real-time motion planner, outperforming strong end-to-end
planners on NAVSIM benchmarks. Code will be publicly available at
\href{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}.

</details>


### [219] [TextMesh4D: High-Quality Text-to-4D Mesh Generation](https://arxiv.org/abs/2506.24121)
*Sisi Dai,Xinxin Su,Boyan Wan,Ruizhen Hu,Kai Xu*

Main category: cs.CV

TL;DR: TextMesh4D是一个新颖的框架，用于高质量文本到4D生成，通过两阶段分解和正则化优化实现高效动态3D内容生成。


<details>
  <summary>Details</summary>
Motivation: 解决动态3D内容生成（文本到4D）的挑战性问题，填补扩散生成模型在这一领域的空白。

Method: 采用基于面Jacobian的可微分网格表示，将4D生成分为静态对象创建和动态运动合成两阶段，并引入灵活性-刚性正则化项优化。

Result: 实验显示TextMesh4D在时间一致性、结构保真度和视觉真实感方面达到最先进水平，且GPU内存需求低。

Conclusion: TextMesh4D为文本驱动的4D网格生成提供了高效且高质量的解决方案，代码将开源以促进未来研究。

Abstract: Recent advancements in diffusion generative models significantly advanced
image, video, and 3D content creation from user-provided text prompts. However,
the challenging problem of dynamic 3D content generation (text-to-4D) with
diffusion guidance remains largely unexplored. In this paper, we introduce
TextMesh4D, a novel framework for high-quality text-to-4D generation. Our
approach leverages per-face Jacobians as a differentiable mesh representation
and decomposes 4D generation into two stages: static object creation and
dynamic motion synthesis. We further propose a flexibility-rigidity
regularization term to stabilize Jacobian optimization under video diffusion
priors, ensuring robust geometric performance. Experiments demonstrate that
TextMesh4D achieves state-of-the-art results in terms of temporal consistency,
structural fidelity, and visual realism. Moreover, TextMesh4D operates with a
low GPU memory overhead-requiring only a single 24GB GPU-offering a
cost-effective yet high-quality solution for text-driven 4D mesh generation.
The code will be released to facilitate future research in text-to-4D
generation.

</details>


### [220] [Calligrapher: Freestyle Text Image Customization](https://arxiv.org/abs/2506.24123)
*Yue Ma,Qingyan Bai,Hao Ouyang,Ka Leong Cheng,Qiuyu Wang,Hongyu Liu,Zichen Liu,Haofan Wang,Jingye Chen,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: Calligrapher是一个基于扩散模型的框架，结合文本定制与艺术字体，解决了字体风格控制和数据依赖问题。


<details>
  <summary>Details</summary>
Motivation: 解决字体风格精确控制和数据依赖的挑战，为数字书法和设计提供高效工具。

Method: 1. 自蒸馏机制利用预训练模型构建风格基准；2. 可训练风格编码器提取特征；3. 上下文生成机制嵌入参考图像。

Result: 在多种字体和设计场景中，Calligrapher能准确复现风格细节和字形定位，超越传统模型。

Conclusion: Calligrapher自动化高质量字体设计，为数字艺术和品牌设计提供强大支持。

Abstract: We introduce Calligrapher, a novel diffusion-based framework that
innovatively integrates advanced text customization with artistic typography
for digital calligraphy and design applications. Addressing the challenges of
precise style control and data dependency in typographic customization, our
framework incorporates three key technical contributions. First, we develop a
self-distillation mechanism that leverages the pre-trained text-to-image
generative model itself alongside the large language model to automatically
construct a style-centric typography benchmark. Second, we introduce a
localized style injection framework via a trainable style encoder, which
comprises both Qformer and linear layers, to extract robust style features from
reference images. An in-context generation mechanism is also employed to
directly embed reference images into the denoising process, further enhancing
the refined alignment of target styles. Extensive quantitative and qualitative
evaluations across diverse fonts and design contexts confirm Calligrapher's
accurate reproduction of intricate stylistic details and precise glyph
positioning. By automating high-quality, visually consistent typography,
Calligrapher surpasses traditional models, empowering creative practitioners in
digital art, branding, and contextual typographic design.

</details>


### [221] [FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation](https://arxiv.org/abs/2506.24125)
*Jiacheng Cui,Xinyue Bi,Yaxin Luo,Xiaohan Zhao,Jiacheng Liu,Zhiqiang Shen*

Main category: cs.CV

TL;DR: 论文提出了一种名为FADRM的数据残差匹配方法，首次在数据层面引入跳跃连接，显著提升了数据集蒸馏任务的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 探索数据残差匹配在数据为中心的方法中的潜力，解决数据信息消失问题，平衡新知识与原始数据核心信息。

Method: 通过数据级跳跃连接和优化级改进，实现高效的数据生成和信息保留，减少训练时间和GPU内存使用。

Result: 在多个数据集基准测试中表现优异，例如在ImageNet-1K上，单模型和多模型蒸馏的测试准确率分别达到47.7%和50.0%。

Conclusion: FADRM方法在效率和效果上均优于现有方法，为数据集蒸馏任务设立了新的标杆。

Abstract: Residual connection has been extensively studied and widely applied at the
model architecture level. However, its potential in the more challenging
data-centric approaches remains unexplored. In this work, we introduce the
concept of Data Residual Matching for the first time, leveraging data-level
skip connections to facilitate data generation and mitigate data information
vanishing. This approach maintains a balance between newly acquired knowledge
through pixel space optimization and existing core local information
identification within raw data modalities, specifically for the dataset
distillation task. Furthermore, by incorporating optimization-level
refinements, our method significantly improves computational efficiency,
achieving superior performance while reducing training time and peak GPU memory
usage by 50%. Consequently, the proposed method Fast and Accurate Data Residual
Matching for Dataset Distillation (FADRM) establishes a new state-of-the-art,
demonstrating substantial improvements over existing methods across multiple
dataset benchmarks in both efficiency and effectiveness. For instance, with
ResNet-18 as the student model and a 0.8% compression ratio on ImageNet-1K, the
method achieves 47.7% test accuracy in single-model dataset distillation and
50.0% in multi-model dataset distillation, surpassing RDED by +5.7% and
outperforming state-of-the-art multi-model approaches, EDC and CV-DD, by +1.4%
and +4.0%. Code is available at: https://github.com/Jiacheng8/FADRM.

</details>


### [222] [How to Design and Train Your Implicit Neural Representation for Video Compression](https://arxiv.org/abs/2506.24127)
*Matthew Gwilliam,Roy Zhang,Namitha Padmanabhan,Hongyang Du,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 论文提出了一种改进的隐式神经表示（INR）视频压缩方法RNeRV，通过优化组件设计和引入超网络技术，显著提升了压缩质量和编码速度。


<details>
  <summary>Details</summary>
Motivation: 传统INR方法因需要逐样本训练网络导致编码速度过慢，限制了实际应用。

Method: 开发了一个库来解构NeRV家族方法，提出RNeRV配置，并探索超网络技术以分离训练和编码。

Result: RNeRV在相同训练时间下平均PSNR提升1.27%；超网络技术进一步提升了PSNR和MS-SSIM。

Conclusion: RNeRV和超网络技术显著提升了视频INR的性能和实用性，为实时编码提供了可能。

Abstract: Implicit neural representation (INR) methods for video compression have
recently achieved visual quality and compression ratios that are competitive
with traditional pipelines. However, due to the need for per-sample network
training, the encoding speeds of these methods are too slow for practical
adoption. We develop a library to allow us to disentangle and review the
components of methods from the NeRV family, reframing their performance in
terms of not only size-quality trade-offs, but also impacts on training time.
We uncover principles for effective video INR design and propose a
state-of-the-art configuration of these components, Rabbit NeRV (RNeRV). When
all methods are given equal training time (equivalent to 300 NeRV epochs) for 7
different UVG videos at 1080p, RNeRV achieves +1.27% PSNR on average compared
to the best-performing alternative for each video in our NeRV library. We then
tackle the encoding speed issue head-on by investigating the viability of
hyper-networks, which predict INR weights from video inputs, to disentangle
training from encoding to allow for real-time encoding. We propose masking the
weights of the predicted INR during training to allow for variable, higher
quality compression, resulting in 1.7% improvements to both PSNR and MS-SSIM at
0.037 bpp on the UCF-101 dataset, and we increase hyper-network parameters by
0.4% for 2.5%/2.7% improvements to PSNR/MS-SSIM with equal bpp and similar
speeds. Our project website is available at https://mgwillia.github.io/vinrb/
and our code is available at https://github.com/mgwillia/vinrb.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [223] [High Resolution Isotropic 3D Cine imaging with Automated Segmentation using Concatenated 2D Real-time Imaging and Deep Learning](https://arxiv.org/abs/2506.22532)
*Mark Wrobel,Michele Pascale,Tina Yao,Ruaraidh Campbell,Elena Milano,Michael Quail,Jennifer Steeden,Vivek Muthurangu*

Main category: eess.IV

TL;DR: 利用深度学习从2D实时电影图像生成3D电影数据集，验证了其在心血管磁共振中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统心血管磁共振（CMR）在儿科和先天性心脏病中使用2D和3D成像，但存在局限性。本研究旨在通过深度学习从2D图像生成3D数据集，提高效率和准确性。

Method: 训练了四种深度学习模型，分别用于对比校正、呼吸运动校正、超分辨率和分割。在10名患者中验证了该方法。

Result: 成功将实时数据转化为3D电影，处理时间短，与常规成像结果一致，但肺动脉直径略有高估。

Conclusion: 该方法展示了从2D图像生成3D数据的潜力，有望显著加快临床CMR检查速度。

Abstract: Background: Conventional cardiovascular magnetic resonance (CMR) in
paediatric and congenital heart disease uses 2D, breath-hold, balanced steady
state free precession (bSSFP) cine imaging for assessment of function and
cardiac-gated, respiratory-navigated, static 3D bSSFP whole-heart imaging for
anatomical assessment. Our aim is to concatenate a stack 2D free-breathing
real-time cines and use Deep Learning (DL) to create an isotropic a fully
segmented 3D cine dataset from these images. Methods: Four DL models were
trained on open-source data that performed: a) Interslice contrast correction;
b) Interslice respiratory motion correction; c) Super-resolution (slice
direction); and d) Segmentation of right and left atria and ventricles (RA, LA,
RV, and LV), thoracic aorta (Ao) and pulmonary arteries (PA). In 10 patients
undergoing routine cardiovascular examination, our method was validated on
prospectively acquired sagittal stacks of real-time cine images. Quantitative
metrics (ventricular volumes and vessel diameters) and image quality of the 3D
cines were compared to conventional breath hold cine and whole heart imaging.
Results: All real-time data were successfully transformed into 3D cines with a
total post-processing time of <1 min in all cases. There were no significant
biases in any LV or RV metrics with reasonable limits of agreement and
correlation. There is also reasonable agreement for all vessel diameters,
although there was a small but significant overestimation of RPA diameter.
Conclusion: We have demonstrated the potential of creating a 3D-cine data from
concatenated 2D real-time cine images using a series of DL models. Our method
has short acquisition and reconstruction times with fully segmented data being
available within 2 minutes. The good agreement with conventional imaging
suggests that our method could help to significantly speed up CMR in clinical
practice.

</details>


### [224] [FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for Federated Medical Image Segmentation](https://arxiv.org/abs/2506.22580)
*Vasilis Siomos,Jonathan Passerat-Palmbach,Giacomo Tarroni*

Main category: eess.IV

TL;DR: FedCLAM是一种联邦学习方法，通过客户端自适应动量和个性化阻尼因子解决医学影像中的特征差异问题，并在分割任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 医学影像中设备多样性和人群差异导致联邦学习效果下降，现有方法难以适应不同情况。

Method: 提出FedCLAM，结合客户端自适应动量、个性化阻尼因子和强度对齐损失。

Result: 在两个数据集上超越八种前沿方法，验证了其有效性。

Conclusion: FedCLAM在医学影像分割任务中表现出色，代码已开源。

Abstract: Federated learning is a decentralized training approach that keeps data under
stakeholder control while achieving superior performance over isolated
training. While inter-institutional feature discrepancies pose a challenge in
all federated settings, medical imaging is particularly affected due to diverse
imaging devices and population variances, which can diminish the global model's
effectiveness. Existing aggregation methods generally fail to adapt across
varied circumstances. To address this, we propose FedCLAM, which integrates
\textit{client-adaptive momentum} terms derived from each client's loss
reduction during local training, as well as a \textit{personalized dampening
factor} to curb overfitting. We further introduce a novel \textit{intensity
alignment} loss that matches predicted and ground-truth foreground
distributions to handle heterogeneous image intensity profiles across
institutions and devices. Extensive evaluations on two datasets show that
FedCLAM surpasses eight cutting-edge methods in medical segmentation tasks,
underscoring its efficacy. The code is available at
https://github.com/siomvas/FedCLAM.

</details>


### [225] [Multi-Domain FeFET-Based Pixel for In-Sensor Multiply-and-Accumulate Operations](https://arxiv.org/abs/2506.22596)
*Md Rahatul Islam Udoy,Wantong Li,Kai Ni,Ahmedullah Aziz*

Main category: eess.IV

TL;DR: 提出了一种基于FeFET的主动像素传感器，通过铁电层的多域极化状态实现传感器内乘加运算。


<details>
  <summary>Details</summary>
Motivation: 减少数据移动，提高能效，适用于实时边缘计算、神经形态视觉和安全传感应用。

Method: 将可编程FeFET集成到3晶体管像素电路中，利用FeFET的非易失性电导编码权重，光电二极管电压降编码输入，实现像素内模拟乘法。

Result: HSPICE仿真验证了设计的可行性和可扩展性。

Conclusion: 该紧凑且高效的架构为实时边缘计算等应用提供了理想解决方案。

Abstract: This paper presents an FeFET-based active pixel sensor that performs
in-sensor multiply-and-accumulate (MAC) operations by leveraging the
multi-domain polarization states of ferroelectric layers. The proposed design
integrates a programmable FeFET into a 3-transistor pixel circuit, where the
FeFET's non-volatile conductance encodes the weight, and the photodiode voltage
drop encodes the input. Their interaction generates an output current
proportional to the product, enabling in-pixel analog multiplication.
Accumulation is achieved by summing output currents along shared column lines,
realizing full MAC functionality within the image sensor array. Extensive
HSPICE simulations, using 45 nm CMOS models, validate the operation and confirm
the scalability of the design. This compact and power-efficient architecture
minimizes data movement, making it ideal for real-time edge computing,
neuromorphic vision, and secure sensing applications.

</details>


### [226] [ICME 2025 Generalizable HDR and SDR Video Quality Measurement Grand Challenge](https://arxiv.org/abs/2506.22790)
*Yixu Chen,Bowen Chen,Hai Wei,Alan C. Bovik,Baojun Li,Wei Sun,Linhan Cao,Kang Fu,Dandan Zhu,Jun Jia,Menghan Hu,Xiongkuo Min,Guangtao Zhai,Dounia Hammou,Fei Yin,Rafal Mantiuk,Amritha Premkumar,Prajit T Rajendran,Vignesh V Menon*

Main category: eess.IV

TL;DR: ICME 2025挑战赛聚焦于通用HDR和SDR视频质量评估，旨在推动跨动态范围和失真类型的VQA方法。五支团队提交了七种模型，其中四种优于VMAF基线，最优模型达到新标杆。


<details>
  <summary>Details</summary>
Motivation: 随着HDR和SDR内容的快速发展，现有VQA模型在跨动态范围和失真类型的一致性表现上存在不足，亟需通用性强的评估方法。

Method: 挑战赛通过Full Reference (FR)和No Reference (NR)两个赛道，评估团队提交的VQA模型在HDR和SDR内容上的表现。

Result: 七种模型中，四种优于VMAF基线，最优模型实现了最先进的性能。

Conclusion: 挑战赛成功推动了通用视频质量评估技术的发展，并为未来研究设定了新标准。

Abstract: This paper reports IEEE International Conference on Multimedia \& Expo (ICME)
2025 Grand Challenge on Generalizable HDR and SDR Video Quality Measurement.
With the rapid development of video technology, especially High Dynamic Range
(HDR) and Standard Dynamic Range (SDR) contents, the need for robust and
generalizable Video Quality Assessment (VQA) methods has become increasingly
demanded. Existing VQA models often struggle to deliver consistent performance
across varying dynamic ranges, distortion types, and diverse content. This
challenge was established to benchmark and promote VQA approaches capable of
jointly handling HDR and SDR content. In the final evaluation phase, five teams
submitted seven models along with technical reports to the Full Reference (FR)
and No Reference (NR) tracks. Among them, four methods outperformed VMAF
baseline, while the top-performing model achieved state-of-the-art performance,
setting a new benchmark for generalizable video quality assessment.

</details>


### [227] [CA-Diff: Collaborative Anatomy Diffusion for Brain Tissue Segmentation](https://arxiv.org/abs/2506.22882)
*Qilong Xing,Zikai Song,Yuteng Ye,Yuke Chen,Youjia Zhang,Na Feng,Junqing Yu,Wei Yang*

Main category: eess.IV

TL;DR: 提出了一种结合解剖学特征的扩散模型框架CA-Diff，用于提高脑MRI分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有CNN和基于Transformer的方法在复杂脑结构分割上表现不足，而扩散模型直接应用于脑MRI时忽略了解剖信息。

Method: 引入距离场作为辅助解剖条件，结合协作扩散过程建模其与解剖结构的联合分布，并设计一致性损失和时间适应通道注意力模块。

Result: CA-Diff在实验中优于现有最先进方法。

Conclusion: CA-Diff通过整合解剖特征和优化模型设计，显著提升了脑MRI分割的准确性。

Abstract: Segmentation of brain structures from MRI is crucial for evaluating brain
morphology, yet existing CNN and transformer-based methods struggle to
delineate complex structures accurately. While current diffusion models have
shown promise in image segmentation, they are inadequate when applied directly
to brain MRI due to neglecting anatomical information. To address this, we
propose Collaborative Anatomy Diffusion (CA-Diff), a framework integrating
spatial anatomical features to enhance segmentation accuracy of the diffusion
model. Specifically, we introduce distance field as an auxiliary anatomical
condition to provide global spatial context, alongside a collaborative
diffusion process to model its joint distribution with anatomical structures,
enabling effective utilization of anatomical features for segmentation.
Furthermore, we introduce a consistency loss to refine relationships between
the distance field and anatomical structures and design a time adapted channel
attention module to enhance the U-Net feature fusion procedure. Extensive
experiments show that CA-Diff outperforms state-of-the-art (SOTA) methods.

</details>


### [228] [Hierarchical Characterization of Brain Dynamics via State Space-based Vector Quantization](https://arxiv.org/abs/2506.22952)
*Yanwu Yang,Thomas Wolfers*

Main category: eess.IV

TL;DR: 提出了一种基于状态空间的层次化令牌化网络HST，用于量化大脑状态和动态，并通过改进的VQ-VAE提升表征能力。


<details>
  <summary>Details</summary>
Motivation: 理解大脑动态是神经科学的核心挑战，现有方法忽视了状态转换依赖性和稳定表征的量化。

Method: HST结合层次化状态空间模型和优化的VQ-VAE，引入量化误差反馈和聚类以提升性能。

Result: 在两个公开fMRI数据集上验证了HST的有效性，展示了其在疾病诊断和重建性能上的潜力。

Conclusion: HST为大脑动态表征提供了新框架，有助于分析大脑的亚稳态特性。

Abstract: Understanding brain dynamics through functional Magnetic Resonance Imaging
(fMRI) remains a fundamental challenge in neuroscience, particularly in
capturing how the brain transitions between various functional states.
Recently, metastability, which refers to temporarily stable brain states, has
offered a promising paradigm to quantify complex brain signals into
interpretable, discretized representations. In particular, compared to
cluster-based machine learning approaches, tokenization approaches leveraging
vector quantization have shown promise in representation learning with powerful
reconstruction and predictive capabilities. However, most existing methods
ignore brain transition dependencies and lack a quantification of brain
dynamics into representative and stable embeddings. In this study, we propose a
Hierarchical State space-based Tokenization network, termed HST, which
quantizes brain states and transitions in a hierarchical structure based on a
state space-based model. We introduce a refined clustered Vector-Quantization
Variational AutoEncoder (VQ-VAE) that incorporates quantization error feedback
and clustering to improve quantization performance while facilitating
metastability with representative and stable token representations. We validate
our HST on two public fMRI datasets, demonstrating its effectiveness in
quantifying the hierarchical dynamics of the brain and its potential in disease
diagnosis and reconstruction performance. Our method offers a promising
framework for the characterization of brain dynamics, facilitating the analysis
of metastability.

</details>


### [229] [An Image Processing Based Blur Reduction Technique in Smartphone-to-Smartphone Visible Light Communication System](https://arxiv.org/abs/2506.23002)
*Vaigai Nayaki Yokar,Hoa Le-Minh,Zabih Ghassemlooy,Wai Lok Woo*

Main category: eess.IV

TL;DR: 提出了一种智能手机间可见光通信（S2SVLC）的模糊减少技术，通过避免重复扫描和减少接收端数据丢弃，提高了识别效率和数据率。


<details>
  <summary>Details</summary>
Motivation: 解决S2SVLC系统中图像模糊问题，提高数据传输效率和接收端的恢复效率。

Method: 将RGB图像转换为灰度图，应用对比度增强、缩放和二值化处理以减少模糊。实验在不同距离、旋转、倾斜和光照条件下进行，使用ASCII和QR码传输数据。

Result: 实验结果表明，该技术在不同条件下将接收端的恢复效率提高到96%。

Conclusion: 提出的模糊减少技术显著提升了S2SVLC系统的性能，适用于多种环境条件。

Abstract: In this paper, we present a blur reduction technique for
smartphone-to-smartphone visible light communications (S2SVLC). The key
technique it to avoid the repeated scanning of the transmitted data and to
lower the amount of data discarded at the receiver end of the S2SVLC system.
This image processing method will improve the system recognition efficiency and
data rate. The proposed method includes converting the red-green-blue (RGB)
image into grayscale, applying contrast enhancement, scaling and binarizing the
image to reduce the blur levels in the image. The experiment includes practical
data acquisition and further processing and estimation in MATLAB. The
experiment is carried out in different conditions like distance, rotation, and
tilt also considering different surrounding illuminations like ambient light
and no light conditions to estimate the blur levels in S2SVLC. In this
experimental investigation two types of coding, American Standard code for
information interchange (ASCII), and quick response (QR) code are used for data
transmission in S2SVLC. The obtained results indicate that, the proposed
technique is proven to improve the recovery efficiency to 96% in the receiver
end at different conditions.

</details>


### [230] [Channel characterization in screen-to-camera based optical camera communication](https://arxiv.org/abs/2506.23005)
*Vaigai Nayaki Yokar,Hoa Le Minh,Zabih Ghassemlooy,Wai Lok Woo*

Main category: eess.IV

TL;DR: 论文提出了一种基于智能手机屏幕和摄像头的S2SVLC系统，实验验证了20厘米距离下的通信可行性，并分析了屏幕的Lambertian特性和信道特性。


<details>
  <summary>Details</summary>
Motivation: 随着光学相机通信（OCC）的发展，屏幕到摄像头的通信成为可能，推动了智能手机间可见光通信（S2SVLC）的研究。

Method: 通过智能手机屏幕和摄像头构建S2SVLC系统，实验测试20厘米距离下的通信性能，并分析屏幕的Lambertian特性和信道特性。

Result: 实验成功实现了20厘米距离下的S2SVLC通信，并获得了屏幕的Lambertian特性和信道特性的分析结果。

Conclusion: S2SVLC系统在短距离通信中具有潜力，为智能手机间可见光通信提供了实验基础。

Abstract: With the increase in optical camera communication (OCC), a screen to
camera-based communication can be established. This opens a new field of
visible light communication (VLC) known as smartphone to smartphone based
visible light communication (S2SVLC) system. In this paper, we experimentally
demonstrate a S2SVLC system based on VLC technology using a smartphone screen
and a smartphone camera over a link span of 20 cms. We analyze the Lambertian
order of the smartphone screen and carry out a channel characterization of a
screen to camera link-based VLC system under specific test conditions.

</details>


### [231] [MedRegion-CT: Region-Focused Multimodal LLM for Comprehensive 3D CT Report Generation](https://arxiv.org/abs/2506.23102)
*Sunggu Kyung,Jinyoung Seo,Hyunseok Lim,Dongyeong Kim,Hyungbin Park,Jimin Sung,Jihyun Kim,Wooyoung Jo,Yoojin Nam,Namkug Kim*

Main category: eess.IV

TL;DR: MedRegion-CT提出了一种区域聚焦的多模态大语言模型框架，通过区域代表性标记池化、通用分割模型和患者特定属性提取，显著提升了CT报告生成的临床相关性和自然语言质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注全局特征，难以捕捉区域特异性细节，可能导致异常被忽略。

Method: 1. 引入区域代表性标记池化（R² Token Pooling）提取3D CT特征；2. 使用通用分割模型生成伪掩码，提取区域中心特征；3. 利用分割结果提取患者特定属性并转换为文本提示。

Result: 在RadGenome-Chest CT上，MedRegion-CT在自然语言生成质量和临床相关性方面表现最优，同时保持可解释性。

Conclusion: MedRegion-CT通过区域聚焦方法显著提升了CT报告生成的性能，代码已公开。

Abstract: The recent release of RadGenome-Chest CT has significantly advanced CT-based
report generation. However, existing methods primarily focus on global
features, making it challenging to capture region-specific details, which may
cause certain abnormalities to go unnoticed. To address this, we propose
MedRegion-CT, a region-focused Multi-Modal Large Language Model (MLLM)
framework, featuring three key innovations. First, we introduce Region
Representative ($R^2$) Token Pooling, which utilizes a 2D-wise pretrained
vision model to efficiently extract 3D CT features. This approach generates
global tokens representing overall slice features and region tokens
highlighting target areas, enabling the MLLM to process comprehensive
information effectively. Second, a universal segmentation model generates
pseudo-masks, which are then processed by a mask encoder to extract
region-centric features. This allows the MLLM to focus on clinically relevant
regions, using six predefined region masks. Third, we leverage segmentation
results to extract patient-specific attributions, including organ size,
diameter, and locations. These are converted into text prompts, enriching the
MLLM's understanding of patient-specific contexts. To ensure rigorous
evaluation, we conducted benchmark experiments on report generation using the
RadGenome-Chest CT. MedRegion-CT achieved state-of-the-art performance,
outperforming existing methods in natural language generation quality and
clinical relevance while maintaining interpretability. The code for our
framework is publicly available.

</details>


### [232] [CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation](https://arxiv.org/abs/2506.23121)
*Xinlei Yu,Chanmiao Wang,Hui Jin,Ahmed Elazab,Gangyong Jia,Xiang Wan,Changqing Zou,Ruiquan Ge*

Main category: eess.IV

TL;DR: CRISP-SAM2是一种基于SAM2的多器官医学分割模型，通过跨模态交互和语义提示解决现有模型的细节不准确、依赖几何提示和空间信息丢失问题。


<details>
  <summary>Details</summary>
Motivation: 当前多器官分割模型存在细节不准确、依赖几何提示和空间信息丢失的问题，CRISP-SAM2旨在解决这些挑战。

Method: 模型通过跨模态交互机制将视觉和文本输入转换为上下文语义，并注入图像编码器；采用语义提示策略替代几何提示，结合记忆自更新和掩码细化策略。

Result: 在七个公开数据集上的实验表明，CRISP-SAM2优于现有模型，尤其在解决前述局限性方面表现突出。

Conclusion: CRISP-SAM2在医学图像分割中表现出色，解决了现有模型的不足，具有实际应用潜力。

Abstract: Multi-organ medical segmentation is a crucial component of medical image
processing, essential for doctors to make accurate diagnoses and develop
effective treatment plans. Despite significant progress in this field, current
multi-organ segmentation models often suffer from inaccurate details,
dependence on geometric prompts and loss of spatial information. Addressing
these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal
Interaction and Semantic Prompting based on SAM2. This model represents a
promising approach to multi-organ medical segmentation guided by textual
descriptions of organs. Our method begins by converting visual and textual
inputs into cross-modal contextualized semantics using a progressive
cross-attention interaction mechanism. These semantics are then injected into
the image encoder to enhance the detailed understanding of visual information.
To eliminate reliance on geometric prompts, we use a semantic prompting
strategy, replacing the original prompt encoder to sharpen the perception of
challenging targets. In addition, a similarity-sorting self-updating strategy
for memory and a mask-refining process is applied to further adapt to medical
imaging and enhance localized details. Comparative experiments conducted on
seven public datasets indicate that CRISP-SAM2 outperforms existing models.
Extensive analysis also demonstrates the effectiveness of our method, thereby
confirming its superior performance, especially in addressing the limitations
mentioned earlier. Our code is available at:
https://github.com/YU-deep/CRISP\_SAM2.git.

</details>


### [233] [Score-based Diffusion Model for Unpaired Virtual Histology Staining](https://arxiv.org/abs/2506.23184)
*Anran Liu,Xiaofei Wang,Jing Cai,Chao Li*

Main category: eess.IV

TL;DR: 提出了一种基于互信息引导的扩散模型，用于从H&E图像虚拟生成IHC图像，解决了现有方法在分解染色风格与组织结构、可控染色过程及结构一致性建模方面的挑战。


<details>
  <summary>Details</summary>
Motivation: H&E染色缺乏特异性标记，IHC染色受限于组织可用性和抗体特异性，虚拟染色技术有望高效生成IHC图像，但现有方法存在关键挑战。

Method: 设计了一个全局互信息引导的能量函数、时间步定制的反向扩散过程及局部互信息驱动的对比学习策略，以实现染色风格与组织结构的解耦、可控染色和结构一致性。

Result: 实验表明，该方法优于现有技术，展现了生物医学应用的潜力。

Conclusion: 该方法为虚拟染色提供了高效且可控的解决方案，代码将在接受后开源。

Abstract: Hematoxylin and eosin (H&E) staining visualizes histology but lacks
specificity for diagnostic markers. Immunohistochemistry (IHC) staining
provides protein-targeted staining but is restricted by tissue availability and
antibody specificity. Virtual staining, i.e., computationally translating the
H&E image to its IHC counterpart while preserving the tissue structure, is
promising for efficient IHC generation. Existing virtual staining methods still
face key challenges: 1) effective decomposition of staining style and tissue
structure, 2) controllable staining process adaptable to diverse tissue and
proteins, and 3) rigorous structural consistency modelling to handle the
non-pixel-aligned nature of paired H&E and IHC images. This study proposes a
mutual-information (MI)-guided score-based diffusion model for unpaired virtual
staining. Specifically, we design 1) a global MI-guided energy function that
disentangles the tissue structure and staining characteristics across
modalities, 2) a novel timestep-customized reverse diffusion process for
precise control of the staining intensity and structural reconstruction, and 3)
a local MI-driven contrastive learning strategy to ensure the cellular level
structural consistency between H&E-IHC images. Extensive experiments
demonstrate the our superiority over state-of-the-art approaches, highlighting
its biomedical potential. Codes will be open-sourced upon acceptance.

</details>


### [234] [Multi-Source COVID-19 Detection via Variance Risk Extrapolation](https://arxiv.org/abs/2506.23208)
*Runtian Yuan,Qingqiu Li,Junlin Hou,Jilan Xu,Yuejie Zhang,Rui Feng,Hao Chen*

Main category: eess.IV

TL;DR: 提出了一种结合VREx和Mixup的方法，用于多源COVID-19检测任务，显著提升了跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决不同医疗机构数据差异导致的域偏移问题，提升模型在跨域场景下的性能。

Method: 采用Variance Risk Extrapolation (VREx)减少域间风险差异，并结合Mixup数据增强提升泛化能力。

Result: 在验证集上平均宏F1得分为0.96，表现出强泛化能力。

Conclusion: VREx和Mixup的结合有效提升了模型在多源数据上的泛化性能。

Abstract: We present our solution for the Multi-Source COVID-19 Detection Challenge,
which aims to classify chest CT scans into COVID and Non-COVID categories
across data collected from four distinct hospitals and medical centers. A major
challenge in this task lies in the domain shift caused by variations in imaging
protocols, scanners, and patient populations across institutions. To enhance
the cross-domain generalization of our model, we incorporate Variance Risk
Extrapolation (VREx) into the training process. VREx encourages the model to
maintain consistent performance across multiple source domains by explicitly
minimizing the variance of empirical risks across environments. This
regularization strategy reduces overfitting to center-specific features and
promotes learning of domain-invariant representations. We further apply Mixup
data augmentation to improve generalization and robustness. Mixup interpolates
both the inputs and labels of randomly selected pairs of training samples,
encouraging the model to behave linearly between examples and enhancing its
resilience to noise and limited data. Our method achieves an average macro F1
score of 0.96 across the four sources on the validation set, demonstrating
strong generalization.

</details>


### [235] [Improving Myocardial Infarction Detection via Synthetic ECG Pretraining](https://arxiv.org/abs/2506.23259)
*Lachin Naghashyar*

Main category: eess.IV

TL;DR: 提出一种生理感知的流程，通过合成12导联心电图和自监督预训练模型，提升心肌梗死检测性能。


<details>
  <summary>Details</summary>
Motivation: 心肌梗死是全球主要死因，早期准确诊断至关重要，但深度学习模型需要大量标注数据，而实际中数据稀缺。

Method: 合成具有可调MI形态和真实噪声的12导联心电图，结合自监督掩码自编码和联合重建-分类目标预训练模型。

Result: 合成心电图保留了关键形态特征，预训练在低数据场景下显著提升分类性能，AUC提升高达4个百分点。

Conclusion: 合成心电图可在临床数据有限时有效改善心肌梗死检测。

Abstract: Myocardial infarction is a major cause of death globally, and accurate early
diagnosis from electrocardiograms (ECGs) remains a clinical priority. Deep
learning models have shown promise for automated ECG interpretation, but
require large amounts of labeled data, which are often scarce in practice. We
propose a physiology-aware pipeline that (i) synthesizes 12-lead ECGs with
tunable MI morphology and realistic noise, and (ii) pre-trains recurrent and
transformer classifiers with self-supervised masked-autoencoding plus a joint
reconstruction-classification objective. We validate the realism of synthetic
ECGs via statistical and visual analysis, confirming that key morphological
features are preserved. Pretraining on synthetic data consistently improved
classification performance, particularly in low-data settings, with AUC gains
of up to 4 percentage points. These results show that controlled synthetic ECGs
can help improve MI detection when real clinical data is limited.

</details>


### [236] [Exposing and Mitigating Calibration Biases and Demographic Unfairness in MLLM Few-Shot In-Context Learning for Medical Image Classification](https://arxiv.org/abs/2506.23298)
*Xing Shen,Justin Szeto,Mingyang Li,Hengguan Huang,Tal Arbel*

Main category: eess.IV

TL;DR: 该论文研究了多模态大语言模型（MLLMs）在医学图像分类中的校准偏差和人口统计学不公平性，并提出了一种名为CALIN的推理时校准方法，以减少这些偏差。


<details>
  <summary>Details</summary>
Motivation: MLLMs在医学图像分析中具有巨大潜力，但其预测准确性和校准误差在不同人口亚组中的表现需要深入分析，以确保临床实践中的安全部署。

Method: 论文提出了CALIN方法，通过双层程序（从群体水平到亚组水平）估计校准需求，并在推理时应用校准矩阵调整预测置信度。

Result: 在三个医学影像数据集上的实验表明，CALIN能有效确保预测的公平校准，同时提高整体预测准确性，并最小化公平性与实用性的权衡。

Conclusion: CALIN是一种有效的推理时校准方法，能够减少MLLMs在医学图像分类中的校准偏差和人口统计学不公平性。

Abstract: Multimodal large language models (MLLMs) have enormous potential to perform
few-shot in-context learning in the context of medical image analysis. However,
safe deployment of these models into real-world clinical practice requires an
in-depth analysis of the accuracies of their predictions, and their associated
calibration errors, particularly across different demographic subgroups. In
this work, we present the first investigation into the calibration biases and
demographic unfairness of MLLMs' predictions and confidence scores in few-shot
in-context learning for medical image classification. We introduce CALIN, an
inference-time calibration method designed to mitigate the associated biases.
Specifically, CALIN estimates the amount of calibration needed, represented by
calibration matrices, using a bi-level procedure: progressing from the
population level to the subgroup level prior to inference. It then applies this
estimation to calibrate the predicted confidence scores during inference.
Experimental results on three medical imaging datasets: PAPILA for fundus image
classification, HAM10000 for skin cancer classification, and MIMIC-CXR for
chest X-ray classification demonstrate CALIN's effectiveness at ensuring fair
confidence calibration in its prediction, while improving its overall
prediction accuracies and exhibiting minimum fairness-utility trade-off.

</details>


### [237] [BPD-Neo: An MRI Dataset for Lung-Trachea Segmentation with Clinical Data for Neonatal Bronchopulmonary Dysplasia](https://arxiv.org/abs/2506.23305)
*Rachit Saluja,Arzu Kovanlikaya,Candace Chien,Lauren Kathryn Blatt,Jeffrey M. Perlman,Stefan Worgall,Mert R. Sabuncu,Jonathan P. Dyke*

Main category: eess.IV

TL;DR: 该论文提出了一种基于高分辨率3D MRI数据的非侵入性方法，用于诊断支气管肺发育不良（BPD），并提供了40名新生儿的MRI扫描和语义分割数据，支持进一步研究。


<details>
  <summary>Details</summary>
Motivation: 传统BPD诊断依赖便携式X射线，存在辐射和需镇静的问题，而MRI提供了一种无辐射、非侵入性的替代方案。

Method: 利用高分辨率3D MRI数据，开发图像处理和语义分割算法，辅助临床诊断。数据集包含40名新生儿的StarVIBE系列MRI扫描及分割标注。

Result: 提供了已验证的基线分割模型和临床数据，支持BPD病因识别和研究。

Conclusion: MRI结合语义分割为BPD诊断提供了新工具，有望改善新生儿肺部成像的研究和临床应用。

Abstract: Bronchopulmonary dysplasia (BPD) is a common complication among preterm
neonates, with portable X-ray imaging serving as the standard diagnostic
modality in neonatal intensive care units (NICUs). However, lung magnetic
resonance imaging (MRI) offers a non-invasive alternative that avoids sedation
and radiation while providing detailed insights into the underlying mechanisms
of BPD. Leveraging high-resolution 3D MRI data, advanced image processing and
semantic segmentation algorithms can be developed to assist clinicians in
identifying the etiology of BPD. In this dataset, we present MRI scans paired
with corresponding semantic segmentations of the lungs and trachea for 40
neonates, the majority of whom are diagnosed with BPD. The imaging data consist
of free-breathing 3D stack-of-stars radial gradient echo acquisitions, known as
the StarVIBE series. Additionally, we provide comprehensive clinical data and
baseline segmentation models, validated against clinical assessments, to
support further research and development in neonatal lung imaging.

</details>


### [238] [SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting](https://arxiv.org/abs/2506.23309)
*Yiming Huang,Long Bai,Beilei Cui,Kun Yuan,Guankun Wang,Mobarakol Islam,Nicolas Padoy,Nassir Navab,Hongliang Ren*

Main category: eess.IV

TL;DR: SurgTPGS是一种新型的文本提示高斯泼溅方法，用于实时3D手术场景查询，结合了语义特征学习和变形跟踪，显著提升了手术场景的重建质量和语义理解。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对实时文本提示3D查询的支持，而准确理解3D手术场景对手术规划和实时指导至关重要。

Method: 结合Segment Anything模型和先进视觉语言模型，提出3D语义特征学习策略，语义感知变形跟踪和语义区域感知优化。

Result: 在两个真实手术数据集上的实验表明，SurgTPGS优于现有方法，提升了重建质量和语义平滑性。

Conclusion: SurgTPGS通过增强手术精度和安全性，为下一代智能手术系统的发展铺平了道路。

Abstract: In contemporary surgical research and practice, accurately comprehending 3D
surgical scenes with text-promptable capabilities is particularly crucial for
surgical planning and real-time intra-operative guidance, where precisely
identifying and interacting with surgical tools and anatomical structures is
paramount. However, existing works focus on surgical vision-language model
(VLM), 3D reconstruction, and segmentation separately, lacking support for
real-time text-promptable 3D queries. In this paper, we present SurgTPGS, a
novel text-promptable Gaussian Splatting method to fill this gap. We introduce
a 3D semantics feature learning strategy incorporating the Segment Anything
model and state-of-the-art vision-language models. We extract the segmented
language features for 3D surgical scene reconstruction, enabling a more
in-depth understanding of the complex surgical environment. We also propose
semantic-aware deformation tracking to capture the seamless deformation of
semantic features, providing a more precise reconstruction for both texture and
semantic features. Furthermore, we present semantic region-aware optimization,
which utilizes regional-based semantic information to supervise the training,
particularly promoting the reconstruction quality and semantic smoothness. We
conduct comprehensive experiments on two real-world surgical datasets to
demonstrate the superiority of SurgTPGS over state-of-the-art methods,
highlighting its potential to revolutionize surgical practices. SurgTPGS paves
the way for developing next-generation intelligent surgical systems by
enhancing surgical precision and safety. Our code is available at:
https://github.com/lastbasket/SurgTPGS.

</details>


### [239] [Physics informed guided diffusion for accelerated multi-parametric MRI reconstruction](https://arxiv.org/abs/2506.23311)
*Perla Mayo,Carolin M. Pirkl,Alin Achim,Bjoern Menze,Mohammad Golbabaee*

Main category: eess.IV

TL;DR: MRF-DiPh是一种基于物理约束的扩散去噪方法，用于从快速定量MRI中重建多参数组织图，优于现有深度学习和压缩感知方法。


<details>
  <summary>Details</summary>
Motivation: 解决快速定量MRI（如MRF）中的逆问题，提高参数图的准确性和物理一致性。

Method: 结合预训练的扩散去噪模型作为图像先验，同时强制k空间测量一致性和Bloch响应模型约束。

Result: 在体内脑扫描数据上表现优于基线方法，提供更准确的参数图并保持测量保真度。

Conclusion: MRF-DiPh为医学成像中的逆问题提供了可靠解决方案，具有更高的准确性和物理一致性。

Abstract: We introduce MRF-DiPh, a novel physics informed denoising diffusion approach
for multiparametric tissue mapping from highly accelerated, transient-state
quantitative MRI acquisitions like Magnetic Resonance Fingerprinting (MRF). Our
method is derived from a proximal splitting formulation, incorporating a
pretrained denoising diffusion model as an effective image prior to regularize
the MRF inverse problem. Further, during reconstruction it simultaneously
enforces two key physical constraints: (1) k-space measurement consistency and
(2) adherence to the Bloch response model. Numerical experiments on in-vivo
brain scans data show that MRF-DiPh outperforms deep learning and compressed
sensing MRF baselines, providing more accurate parameter maps while better
preserving measurement fidelity and physical model consistency-critical for
solving reliably inverse problems in medical imaging.

</details>


### [240] [Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation](https://arxiv.org/abs/2506.23334)
*Hongyi Pan,Ziliang Hong,Gorkem Durak,Ziyue Xu,Ulas Bagci*

Main category: eess.IV

TL;DR: 论文提出了一种基于生成AI的数据增强框架，通过合成图像共享提升联邦学习在乳腺癌超声图像诊断中的性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在医疗数据协作训练中面临数据不足和非独立同分布问题，影响模型性能和泛化能力。

Method: 使用两类特定深度卷积生成对抗网络生成合成图像，结合FedAvg和FedProx算法在三个公开数据集上进行实验。

Result: 适量合成图像将FedAvg的AUC从0.9206提升至0.9237，FedProx从0.9429提升至0.9538，但过量使用会降低性能。

Conclusion: 生成AI数据增强能有效提升联邦学习在乳腺癌超声分类任务中的表现，需平衡真实与合成数据比例。

Abstract: Federated learning (FL) has emerged as a promising paradigm for
collaboratively training deep learning models across institutions without
exchanging sensitive medical data. However, its effectiveness is often hindered
by limited data availability and non-independent, identically distributed data
across participating clients, which can degrade model performance and
generalization. To address these challenges, we propose a generative AI based
data augmentation framework that integrates synthetic image sharing into the
federated training process for breast cancer diagnosis via ultrasound images.
Specifically, we train two simple class-specific Deep Convolutional Generative
Adversarial Networks: one for benign and one for malignant lesions. We then
simulate a realistic FL setting using three publicly available breast
ultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are
adopted as baseline FL algorithms. Experimental results show that incorporating
a suitable number of synthetic images improved the average AUC from 0.9206 to
0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that
excessive use of synthetic data reduced performance, underscoring the
importance of maintaining a balanced ratio of real and synthetic samples. Our
findings highlight the potential of generative AI based data augmentation to
enhance FL results in the breast ultrasound image classification task.

</details>


### [241] [FD-DiT: Frequency Domain-Directed Diffusion Transformer for Low-Dose CT Reconstruction](https://arxiv.org/abs/2506.23466)
*Qiqing Liu,Guoquan Wei,Zekun Zhou,Yiyang Wen,Liu Shi,Qiegen Liu*

Main category: eess.IV

TL;DR: FD-DiT是一种基于频率域导向的扩散变换器方法，用于低剂量CT图像重建，通过噪声逐步引入和去噪处理，结合频率解耦技术，显著提升了图像细节保留和噪声抑制能力。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT（LDCT）虽减少辐射，但图像因噪声和伪影导致细节丢失，影响诊断准确性。现有方法在保留细节方面存在不足，需改进。

Method: 提出FD-DiT方法，通过扩散策略逐步引入噪声并去噪，结合频率解耦技术和高频噪声识别模块，优化重建过程。

Result: 实验表明，FD-DiT在相同剂量下，噪声和伪影抑制效果优于现有方法。

Conclusion: FD-DiT通过频率域导向和动态融合策略，显著提升了LDCT图像重建质量。

Abstract: Low-dose computed tomography (LDCT) reduces radiation exposure but suffers
from image artifacts and loss of detail due to quantum and electronic noise,
potentially impacting diagnostic accuracy. Transformer combined with diffusion
models has been a promising approach for image generation. Nevertheless,
existing methods exhibit limitations in preserving finegrained image details.
To address this issue, frequency domain-directed diffusion transformer (FD-DiT)
is proposed for LDCT reconstruction. FD-DiT centers on a diffusion strategy
that progressively introduces noise until the distribution statistically aligns
with that of LDCT data, followed by denoising processing. Furthermore, we
employ a frequency decoupling technique to concentrate noise primarily in
high-frequency domain, thereby facilitating effective capture of essential
anatomical structures and fine details. A hybrid denoising network is then
utilized to optimize the overall data reconstruction process. To enhance the
capability in recognizing high-frequency noise, we incorporate sliding sparse
local attention to leverage the sparsity and locality of shallow-layer
information, propagating them via skip connections for improving feature
representation. Finally, we propose a learnable dynamic fusion strategy for
optimal component integration. Experimental results demonstrate that at
identical dose levels, LDCT images reconstructed by FD-DiT exhibit superior
noise and artifact suppression compared to state-of-the-art methods.

</details>


### [242] [UltraTwin: Towards Cardiac Anatomical Twin Generation from Multi-view 2D Ultrasound](https://arxiv.org/abs/2506.23490)
*Junxuan Yu,Yaofei Duan,Yuhao Huang,Yu Wang,Rongbo Ling,Weihao Luo,Ang Zhang,Jingxian Xu,Qiongying Ni,Yongsong Zhou,Binghan Li,Haoran Dou,Liping Liu,Yanfen Chu,Feng Geng,Zhe Sheng,Zhifeng Ding,Dingxin Zhang,Rui Huang,Yuhang Zhang,Xiaowei Xu,Tao Tan,Dong Ni,Zhongshan Gou,Xin Yang*

Main category: eess.IV

TL;DR: 提出了一种名为UltraTwin的新型生成框架，用于从稀疏多视角2D超声图像构建心脏解剖双胞胎，解决了现有技术的局限性。


<details>
  <summary>Details</summary>
Motivation: 2D超声在心脏检查中常用，但难以精确计算指标和直接观察3D结构；3D超声则受限于分辨率低、视野小和实际可用性差。构建心脏解剖双胞胎可提供精确治疗规划和临床量化，但面临数据稀缺、结构复杂和超声噪声等挑战。

Method: 1. 构建了高质量的真实世界数据集，包含严格配对的多视角2D超声和CT数据及伪配对数据；2. 提出了一种从粗到细的分层重建优化方案；3. 引入了隐式自编码器以实现拓扑感知约束。

Result: 实验表明，UltraTwin能够重建高质量的心脏解剖双胞胎，优于其他强竞争方法。

Conclusion: UltraTwin推动了心脏解剖双胞胎建模的发展，有望在个性化心脏护理中发挥重要作用。

Abstract: Echocardiography is routine for cardiac examination. However, 2D ultrasound
(US) struggles with accurate metric calculation and direct observation of 3D
cardiac structures. Moreover, 3D US is limited by low resolution, small field
of view and scarce availability in practice. Constructing the cardiac
anatomical twin from 2D images is promising to provide precise treatment
planning and clinical quantification. However, it remains challenging due to
the rare paired data, complex structures, and US noises. In this study, we
introduce a novel generative framework UltraTwin, to obtain cardiac anatomical
twin from sparse multi-view 2D US. Our contribution is three-fold. First,
pioneered the construction of a real-world and high-quality dataset containing
strictly paired multi-view 2D US and CT, and pseudo-paired data. Second, we
propose a coarse-to-fine scheme to achieve hierarchical reconstruction
optimization. Last, we introduce an implicit autoencoder for topology-aware
constraints. Extensive experiments show that UltraTwin reconstructs
high-quality anatomical twins versus strong competitors. We believe it advances
anatomical twin modeling for potential applications in personalized cardiac
care.

</details>


### [243] [Artificial Intelligence-assisted Pixel-level Lung (APL) Scoring for Fast and Accurate Quantification in Ultra-short Echo-time MRI](https://arxiv.org/abs/2506.23506)
*Bowen Xin,Rohan Hickey,Tamara Blake,Jin Jin,Claire E Wainwright,Thomas Benkert,Alto Stemmer,Peter Sly,David Coman,Jason Dowling*

Main category: eess.IV

TL;DR: 研究提出了一种基于人工智能的像素级肺部评分（APL）方法，用于快速准确评估囊性纤维化（CF）患者的肺部结构损伤，结果显示其比传统方法更快且更准确。


<details>
  <summary>Details</summary>
Motivation: 由于MRI无电离辐射，适合儿科疾病如囊性纤维化的长期监测，但目前缺乏定量评分系统。

Method: APL评分包括图像加载、AI肺部分割、切片采样、像素级标注和量化报告五个步骤。

Result: APL评分耗时8.2分钟/例，比传统方法快两倍，且准确性更高（p=0.021），与网格评分强相关（R=0.973）。

Conclusion: APL评分有望优化临床工作流程，并可扩展至其他肺部疾病和MRI序列。

Abstract: Lung magnetic resonance imaging (MRI) with ultrashort echo-time (UTE)
represents a recent breakthrough in lung structure imaging, providing image
resolution and quality comparable to computed tomography (CT). Due to the
absence of ionising radiation, MRI is often preferred over CT in paediatric
diseases such as cystic fibrosis (CF), one of the most common genetic disorders
in Caucasians. To assess structural lung damage in CF imaging, CT scoring
systems provide valuable quantitative insights for disease diagnosis and
progression. However, few quantitative scoring systems are available in
structural lung MRI (e.g., UTE-MRI). To provide fast and accurate
quantification in lung MRI, we investigated the feasibility of novel Artificial
intelligence-assisted Pixel-level Lung (APL) scoring for CF. APL scoring
consists of 5 stages, including 1) image loading, 2) AI lung segmentation, 3)
lung-bounded slice sampling, 4) pixel-level annotation, and 5) quantification
and reporting. The results shows that our APL scoring took 8.2 minutes per
subject, which was more than twice as fast as the previous grid-level scoring.
Additionally, our pixel-level scoring was statistically more accurate
(p=0.021), while strongly correlating with grid-level scoring (R=0.973,
p=5.85e-9). This tool has great potential to streamline the workflow of UTE
lung MRI in clinical settings, and be extended to other structural lung MRI
sequences (e.g., BLADE MRI), and for other lung diseases (e.g.,
bronchopulmonary dysplasia).

</details>


### [244] [AFUNet: Cross-Iterative Alignment-Fusion Synergy for HDR Reconstruction via Deep Unfolding Paradigm](https://arxiv.org/abs/2506.23537)
*Xinyue Li,Zhangkai Ni,Wenhan Yang*

Main category: eess.IV

TL;DR: AFUNet通过交替优化的对齐与融合子任务，基于MAP估计理论，显著提升了HDR图像重建的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖经验设计，缺乏理论支持，影响可靠性。

Method: 提出AFUNet，将HDR重建分解为对齐与融合子任务，通过交替优化和展开迭代实现端到端训练。

Result: AFUNet在定性和定量评估中均优于现有方法。

Conclusion: AFUNet通过理论驱动的设计，显著提升了HDR重建的可靠性和性能。

Abstract: Existing learning-based methods effectively reconstruct HDR images from
multi-exposure LDR inputs with extended dynamic range and improved detail, but
they rely more on empirical design rather than theoretical foundation, which
can impact their reliability. To address these limitations, we propose the
cross-iterative Alignment and Fusion deep Unfolding Network (AFUNet), where HDR
reconstruction is systematically decoupled into two interleaved subtasks --
alignment and fusion -- optimized through alternating refinement, achieving
synergy between the two subtasks to enhance the overall performance. Our method
formulates multi-exposure HDR reconstruction from a Maximum A Posteriori (MAP)
estimation perspective, explicitly incorporating spatial correspondence priors
across LDR images and naturally bridging the alignment and fusion subproblems
through joint constraints. Building on the mathematical foundation, we
reimagine traditional iterative optimization through unfolding -- transforming
the conventional solution process into an end-to-end trainable AFUNet with
carefully designed modules that work progressively. Specifically, each
iteration of AFUNet incorporates an Alignment-Fusion Module (AFM) that
alternates between a Spatial Alignment Module (SAM) for alignment and a Channel
Fusion Module (CFM) for adaptive feature fusion, progressively bridging
misaligned content and exposure discrepancies. Extensive qualitative and
quantitative evaluations demonstrate AFUNet's superior performance,
consistently surpassing state-of-the-art methods. Our code is available at:
https://github.com/eezkni/AFUNet

</details>


### [245] [A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation](https://arxiv.org/abs/2506.23584)
*Renjie Liang,Zhengkang Fan,Jinqian Pan,Chenkun Sun,Russell Terry,Jie Xu*

Main category: eess.IV

TL;DR: 提出一种两阶段框架，从2D CT切片生成肾脏放射学报告，结合异常特征提取和视觉语言模型，生成与临床发现一致的自然语言报告。


<details>
  <summary>Details</summary>
Motivation: 由于医学影像的复杂性和临床文档的变异性，从CT扫描生成放射学报告是一项复杂任务。

Method: 使用多任务学习模型提取异常特征，结合CT图像输入微调的视觉语言模型生成报告。

Result: 模型在所有异常类型上优于随机基线，生成的报告能合理捕捉关键临床内容。

Conclusion: 模块化、基于特征的报告生成在肾脏影像中可行，未来将扩展到3D CT体积并提高临床保真度。

Abstract: Generating radiology reports from CT scans remains a complex task due to the
nuanced nature of medical imaging and the variability in clinical
documentation. In this study, we propose a two-stage framework for generating
renal radiology reports from 2D CT slices. First, we extract structured
abnormality features using a multi-task learning model trained to identify
lesion attributes such as location, size, enhancement, and attenuation. These
extracted features are subsequently combined with the corresponding CT image
and fed into a fine-tuned vision-language model to generate natural language
report sentences aligned with clinical findings. We conduct experiments on a
curated dataset of renal CT studies with manually annotated
sentence-slice-feature triplets and evaluate performance using both
classification metrics and natural language generation metrics. Our results
demonstrate that the proposed model outperforms random baselines across all
abnormality types, and the generated reports capture key clinical content with
reasonable textual accuracy. This exploratory work highlights the feasibility
of modular, feature-informed report generation for renal imaging. Future
efforts will focus on extending this pipeline to 3D CT volumes and further
improving clinical fidelity in multimodal medical AI systems.

</details>


### [246] [Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound Segmentation](https://arxiv.org/abs/2506.23664)
*Fangyijie Wang,Kevin Whelan,Félix Balado,Guénolé Silvestre,Kathleen M. Curran*

Main category: eess.IV

TL;DR: 提出一种基于扩散模型的掩码引导生成AI方法，用于生成合成胎儿头部超声图像及其分割掩码，以增强真实数据集，提升胎儿头部分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像数据因隐私和监管限制难以获取，且标注成本高。合成数据生成是解决这些挑战的有效途径。

Method: 采用扩散模型生成合成胎儿头部超声图像及其分割掩码，用于增强真实数据集并微调Segment Anything Model (SAM)。

Result: 合成数据能有效捕捉真实图像特征，在少量真实图像-掩码对的情况下达到胎儿头部分割的最先进水平（西班牙和非洲队列的Dice分数分别为94.66%和94.38%）。

Conclusion: 掩码引导的生成AI方法为医学图像分割提供了高效的数据增强方案，尤其在真实数据有限的情况下表现优异。

Abstract: Medical image data is less accessible than in other domains due to privacy
and regulatory constraints. In addition, labeling requires costly,
time-intensive manual image annotation by clinical experts. To overcome these
challenges, synthetic medical data generation offers a promising solution.
Generative AI (GenAI), employing generative deep learning models, has proven
effective at producing realistic synthetic images. This study proposes a novel
mask-guided GenAI approach using diffusion models to generate synthetic fetal
head ultrasound images paired with segmentation masks. These synthetic pairs
augment real datasets for supervised fine-tuning of the Segment Anything Model
(SAM). Our results show that the synthetic data captures real image features
effectively, and this approach reaches state-of-the-art fetal head
segmentation, especially when trained with a limited number of real image-mask
pairs. In particular, the segmentation reaches Dice Scores of 94.66\% and
94.38\% using a handful of ultrasound images from the Spanish and African
cohorts, respectively. Our code, models, and data are available on GitHub.

</details>


### [247] [GUSL: A Novel and Efficient Machine Learning Model for Prostate Segmentation on MRI](https://arxiv.org/abs/2506.23688)
*Jiaxin Yang,Vasileios Magoulianitis,Catherine Aurelia Christie Alexander,Jintang Xue,Masatomo Kaneko,Giovanni Cacciamani,Andre Abreu,Vinay Duddalwar,C. -C. Jay Kuo,Inderbir S. Gill,Chrysostomos Nikias*

Main category: eess.IV

TL;DR: 提出了一种名为GUSL的无反向传播机器学习模型，用于前列腺和区域分割，具有高能效和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在医学图像分割中的“黑盒”问题，提高临床部署的实用性。

Method: 采用多层回归方案进行粗到细分割，基于线性模型的特征提取，以及边界注意力机制和两步流水线解决类别不平衡。

Result: 在两个公开数据集和一个私有数据集上，GUSL在前列腺和区域分割任务中达到最先进性能，DSC大于0.9。

Conclusion: GUSL因其轻量级模型、高能效和特征提取的透明性，成为医学图像应用的实用解决方案。

Abstract: Prostate and zonal segmentation is a crucial step for clinical diagnosis of
prostate cancer (PCa). Computer-aided diagnosis tools for prostate segmentation
are based on the deep learning (DL) paradigm. However, deep neural networks are
perceived as "black-box" solutions by physicians, thus making them less
practical for deployment in the clinical setting. In this paper, we introduce a
feed-forward machine learning model, named Green U-shaped Learning (GUSL),
suitable for medical image segmentation without backpropagation. GUSL
introduces a multi-layer regression scheme for coarse-to-fine segmentation. Its
feature extraction is based on a linear model, which enables seamless
interpretability during feature extraction. Also, GUSL introduces a mechanism
for attention on the prostate boundaries, which is an error-prone region, by
employing regression to refine the predictions through residue correction. In
addition, a two-step pipeline approach is used to mitigate the class imbalance,
an issue inherent in medical imaging problems. After conducting experiments on
two publicly available datasets and one private dataset, in both prostate gland
and zonal segmentation tasks, GUSL achieves state-of-the-art performance among
other DL-based models. Notably, GUSL features a very energy-efficient pipeline,
since it has a model size several times smaller and less complexity than the
rest of the solutions. In all datasets, GUSL achieved a Dice Similarity
Coefficient (DSC) performance greater than $0.9$ for gland segmentation.
Considering also its lightweight model size and transparency in feature
extraction, it offers a competitive and practical package for medical imaging
applications.

</details>


### [248] [MedSAM-CA: A CNN-Augmented ViT with Attention-Enhanced Multi-Scale Fusion for Medical Image Segmentation](https://arxiv.org/abs/2506.23700)
*Peiting Tian,Xi Chen,Haixia Bi,Fan Li*

Main category: eess.IV

TL;DR: MedSAM-CA是一种基于预训练模型MedSAM的架构级微调方法，通过引入CBR-Net和Atte-FFB组件，减少对大规模标注数据的依赖，提升医学图像分割的边界精度。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割在临床诊断中至关重要，但现有深度学习方法依赖大规模标注数据且难以应对低对比度和模糊边界等挑战。

Method: 提出MedSAM-CA，结合CBR-Net（卷积注意力增强边界细化网络）和Atte-FFB（注意力增强特征融合块），优化预训练模型MedSAM。

Result: 在公开数据集上验证，MedSAM-CA仅用2%的训练数据即达到94.43% Dice分数，接近全数据训练性能的97.25%。

Conclusion: MedSAM-CA在低资源临床场景中表现优异，显著减少对标注数据的依赖并提升分割精度。

Abstract: Medical image segmentation plays a crucial role in clinical diagnosis and
treatment planning, where accurate boundary delineation is essential for
precise lesion localization, organ identification, and quantitative assessment.
In recent years, deep learning-based methods have significantly advanced
segmentation accuracy. However, two major challenges remain. First, the
performance of these methods heavily relies on large-scale annotated datasets,
which are often difficult to obtain in medical scenarios due to privacy
concerns and high annotation costs. Second, clinically challenging scenarios,
such as low contrast in certain imaging modalities and blurry lesion boundaries
caused by malignancy, still pose obstacles to precise segmentation. To address
these challenges, we propose MedSAM-CA, an architecture-level fine-tuning
approach that mitigates reliance on extensive manual annotations by adapting
the pretrained foundation model, Medical Segment Anything (MedSAM). MedSAM-CA
introduces two key components: the Convolutional Attention-Enhanced Boundary
Refinement Network (CBR-Net) and the Attention-Enhanced Feature Fusion Block
(Atte-FFB). CBR-Net operates in parallel with the MedSAM encoder to recover
boundary information potentially overlooked by long-range attention mechanisms,
leveraging hierarchical convolutional processing. Atte-FFB, embedded in the
MedSAM decoder, fuses multi-level fine-grained features from skip connections
in CBR-Net with global representations upsampled within the decoder to enhance
boundary delineation accuracy. Experiments on publicly available datasets
covering dermoscopy, CT, and MRI imaging modalities validate the effectiveness
of MedSAM-CA. On dermoscopy dataset, MedSAM-CA achieves 94.43% Dice with only
2% of full training data, reaching 97.25% of full-data training performance,
demonstrating strong effectiveness in low-resource clinical settings.

</details>


### [249] [MDPG: Multi-domain Diffusion Prior Guidance for MRI Reconstruction](https://arxiv.org/abs/2506.23701)
*Lingtong Zhang,Mengdie Song,Xiaohan Hao,Huayu Mai,Bensheng Qiu*

Main category: eess.IV

TL;DR: 提出了一种多域扩散先验引导（MDPG）方法，利用预训练的潜在扩散模型（LDMs）提升MRI重建任务的数据一致性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（DMs）在图像域中因随机性难以生成高保真图像，而潜在扩散模型（LDMs）在潜在域中提供了紧凑且详细的先验知识，可有效指导模型学习原始数据分布。

Method: 1. 构建基于Visual-Mamba的主干网络，高效编码和重建欠采样图像；2. 集成预训练的LDMs，提供潜在域和图像域的条件先验；3. 提出潜在引导注意力（LGA）实现多级潜在域高效融合；4. 设计双域融合分支（DFB）融合欠采样图像与生成的全采样图像；5. 提出基于非自动校准信号（NACS）的k空间正则化策略。

Result: 在两个公开MRI数据集上的实验证明了方法的有效性。

Conclusion: MDPG方法通过多域先验引导和k空间正则化，显著提升了MRI重建的数据一致性。

Abstract: Magnetic Resonance Imaging (MRI) reconstruction is essential in medical
diagnostics. As the latest generative models, diffusion models (DMs) have
struggled to produce high-fidelity images due to their stochastic nature in
image domains. Latent diffusion models (LDMs) yield both compact and detailed
prior knowledge in latent domains, which could effectively guide the model
towards more effective learning of the original data distribution. Inspired by
this, we propose Multi-domain Diffusion Prior Guidance (MDPG) provided by
pre-trained LDMs to enhance data consistency in MRI reconstruction tasks.
Specifically, we first construct a Visual-Mamba-based backbone, which enables
efficient encoding and reconstruction of under-sampled images. Then pre-trained
LDMs are integrated to provide conditional priors in both latent and image
domains. A novel Latent Guided Attention (LGA) is proposed for efficient fusion
in multi-level latent domains. Simultaneously, to effectively utilize a prior
in both the k-space and image domain, under-sampled images are fused with
generated full-sampled images by the Dual-domain Fusion Branch (DFB) for
self-adaption guidance. Lastly, to further enhance the data consistency, we
propose a k-space regularization strategy based on the non-auto-calibration
signal (NACS) set. Extensive experiments on two public MRI datasets fully
demonstrate the effectiveness of the proposed methodology. The code is
available at https://github.com/Zolento/MDPG.

</details>


### [250] [Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound](https://arxiv.org/abs/2506.23721)
*Gijs Luijten,Roberto Maria Scardigno,Lisle Faray de Paiva,Peter Hoyer,Jens Kleesiek,Domenico Buongiorno,Vitoantonio Bevilacqua,Jan Egger*

Main category: eess.IV

TL;DR: 论文提出了一种结合深度学习和增强现实的超声系统，用于实时自动化肾脏体积测量，以解决超声学习曲线陡峭和操作不便的问题。


<details>
  <summary>Details</summary>
Motivation: 超声技术虽然普及且无辐射，但学习曲线陡峭且操作不便，需要频繁切换视线。论文旨在通过自动化和增强现实技术提升超声的易用性和效率。

Method: 结合深度学习（DL）进行实时语义分割，实现自动化肾脏体积测量；同时利用增强现实（AR）将超声图像投影到医生视野中。提出了两种基于HoloLens-2的AR-DL辅助超声流程。

Result: 使用Open Kidney Dataset和开源分割模型（如nnU-Net、Segmenter等）验证了实时性和准确性。开源GitHub管道提供了模型实现和流媒体解决方案。

Conclusion: AR-DL辅助超声系统显著提升了超声操作的效率和易用性，特别适用于即时诊断场景。

Abstract: Ultrasound (US) is widely accessible and radiation-free but has a steep
learning curve due to its dynamic nature and non-standard imaging planes.
Additionally, the constant need to shift focus between the US screen and the
patient poses a challenge. To address these issues, we integrate deep learning
(DL)-based semantic segmentation for real-time (RT) automated kidney volumetric
measurements, which are essential for clinical assessment but are traditionally
time-consuming and prone to fatigue. This automation allows clinicians to
concentrate on image interpretation rather than manual measurements.
Complementing DL, augmented reality (AR) enhances the usability of US by
projecting the display directly into the clinician's field of view, improving
ergonomics and reducing the cognitive load associated with screen-to-patient
transitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one
streams directly via the application programming interface for a wireless
setup, while the other supports any US device with video output for broader
accessibility. We evaluate RT feasibility and accuracy using the Open Kidney
Dataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with
MedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model
implementations, measurement algorithms, and a Wi-Fi-based streaming solution,
enhancing US training and diagnostics, especially in point-of-care settings.

</details>


### [251] [Spatio-Temporal Representation Decoupling and Enhancement for Federated Instrument Segmentation in Surgical Videos](https://arxiv.org/abs/2506.23759)
*Zheng Fang,Xiaoming Qi,Chun-Mei Feng,Jialun Pei,Weixin Si,Yueming Jin*

Main category: eess.IV

TL;DR: 提出了一种个性化联邦学习方案FedST，通过解耦和增强时空表示，利用手术领域知识提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决手术器械分割在联邦学习中的挑战，如多样解剖背景与相似器械表示，以及合成数据的利用。

Method: 本地训练采用表示分离与合作机制（RSC），全局训练采用合成数据驱动的表示量化（SERQ）。

Result: 提升了模型的分割性能和泛化能力。

Conclusion: FedST有效结合手术领域知识，优化了联邦学习在手术数据科学中的应用。

Abstract: Surgical instrument segmentation under Federated Learning (FL) is a promising
direction, which enables multiple surgical sites to collaboratively train the
model without centralizing datasets. However, there exist very limited FL works
in surgical data science, and FL methods for other modalities do not consider
inherent characteristics in surgical domain: i) different scenarios show
diverse anatomical backgrounds while highly similar instrument representation;
ii) there exist surgical simulators which promote large-scale synthetic data
generation with minimal efforts. In this paper, we propose a novel Personalized
FL scheme, Spatio-Temporal Representation Decoupling and Enhancement (FedST),
which wisely leverages surgical domain knowledge during both local-site and
global-server training to boost segmentation. Concretely, our model embraces a
Representation Separation and Cooperation (RSC) mechanism in local-site
training, which decouples the query embedding layer to be trained privately, to
encode respective backgrounds. Meanwhile, other parameters are optimized
globally to capture the consistent representations of instruments, including
the temporal layer to capture similar motion patterns. A textual-guided channel
selection is further designed to highlight site-specific features, facilitating
model adapta tion to each site. Moreover, in global-server training, we propose
Synthesis-based Explicit Representation Quantification (SERQ), which defines an
explicit representation target based on synthetic data to synchronize the model
convergence during fusion for improving model generalization.

</details>


### [252] [ShapeKit](https://arxiv.org/abs/2506.24003)
*Junqi Liu,Dongli He,Wenxuan Li,Ningyu Wang,Alan L. Yuille,Zongwei Zhou*

Main category: eess.IV

TL;DR: ShapeKit工具包通过专注于形状优化，无需重新训练模型即可提升医学分割性能8%以上。


<details>
  <summary>Details</summary>
Motivation: 观察到形状优化工具在医学分割中的潜力，而传统模型架构改进效果有限（仅提升3%）。

Method: 开发ShapeKit工具包，灵活且易于集成，专注于优化解剖形状。

Result: ShapeKit显著提升分割性能（8%），优于模型架构改进（3%）。

Conclusion: 强调形状工具在医学分割中的重要性，呼吁社区关注其潜力。

Abstract: In this paper, we present a practical approach to improve anatomical shape
accuracy in whole-body medical segmentation. Our analysis shows that a
shape-focused toolkit can enhance segmentation performance by over 8%, without
the need for model re-training or fine-tuning. In comparison, modifications to
model architecture typically lead to marginal gains of less than 3%. Motivated
by this observation, we introduce ShapeKit, a flexible and easy-to-integrate
toolkit designed to refine anatomical shapes. This work highlights the
underappreciated value of shape-based tools and calls attention to their
potential impact within the medical segmentation community.

</details>


### [253] [Simultaneous Super-Resolution of Spatial and Spectral Imaging with a Camera Array and Notch Filters](https://arxiv.org/abs/2506.24014)
*Peng Lin,Xuesong Wang,Yating Chen,Xianyu Wu,Feng Huang,Shouqian Chen*

Main category: eess.IV

TL;DR: 提出了一种基于陷波滤波器相机阵列的算法，用于同时实现超分辨率成像和光谱重建，提升目标的空间分辨率和多光谱成像能力。


<details>
  <summary>Details</summary>
Motivation: 通过多孔径成像系统实现高时间、光谱和空间分辨率的需求。

Method: 结合多孔径超分辨率算法、全色锐化技术和光谱重建算法，利用9个不同成像孔径捕获的低分辨率图像的亚像素偏移信息和光谱差异。

Result: 成功重建了31幅超分辨率光谱图像，峰值信噪比达到35.6dB，比现有系统提升5dB，并减少处理时间。

Conclusion: 该研究为多孔径成像系统提供了一种高效的高分辨率解决方案。

Abstract: This study proposes an algorithm based on a notch filter camera array system
for simultaneous super-resolution imaging and spectral reconstruction,
enhancing the spatial resolution and multispectral imaging capabilities of
targets. In this study, multi-aperture super-resolution algorithms,
pan-sharpening techniques, and spectral reconstruction algorithms were
investigated and integrated. The sub-pixel level offset information and
spectral disparities among the 9 low-resolution images captured by the 9
distinct imaging apertures were utilized, leading to the successful
reconstruction of 31 super-resolution spectral images. By conducting
simulations with a publicly available dataset and performing qualitative and
quantitative comparisons with snapshot coded aperture spectral imaging systems,
the experimental results demonstrate that our system and algorithm attained a
peak signal-to-noise ratio of 35.6dB, representing a 5dB enhancement over the
most advanced snapshot coded aperture spectral imaging systems, while also
reducing processing time. This research offers an effective solution for
achieving high temporal, spectral, and spatial resolution through the
utilization of multi-aperture imaging systems.

</details>


### [254] [C3VDv2 -- Colonoscopy 3D video dataset with enhanced realism](https://arxiv.org/abs/2506.24074)
*Mayank V. Golhar,Lucas Sebastian Galeano Fretes,Loren Ayers,Venkata S. Akshintala,Taylor L. Bobrow,Nicholas J. Durr*

Main category: eess.IV

TL;DR: C3VDv2是一个高仿真结肠镜3D视频数据集，旨在支持3D结肠重建算法的定量评估，包含192个视频序列和多种真实场景模拟。


<details>
  <summary>Details</summary>
Motivation: 现有3D结肠镜数据集的缺乏限制了计算机视觉技术在结肠镜诊断中的发展。

Method: 通过60个高仿真硅胶结肠模型采集192个视频序列，并提供深度、表面法线、光流等真实数据。

Result: 数据集包含169个结肠镜视频的真实数据，以及8个模拟筛查视频和15个变形视频，模拟了多种挑战性场景。

Conclusion: C3VDv2的高仿真性将促进3D重建算法的开发和评估。

Abstract: Computer vision techniques have the potential to improve the diagnostic
performance of colonoscopy, but the lack of 3D colonoscopy datasets for
training and validation hinders their development. This paper introduces
C3VDv2, the second version (v2) of the high-definition Colonoscopy 3D Video
Dataset, featuring enhanced realism designed to facilitate the quantitative
evaluation of 3D colon reconstruction algorithms. 192 video sequences were
captured by imaging 60 unique, high-fidelity silicone colon phantom segments.
Ground truth depth, surface normals, optical flow, occlusion,
six-degree-of-freedom pose, coverage maps, and 3D models are provided for 169
colonoscopy videos. Eight simulated screening colonoscopy videos acquired by a
gastroenterologist are provided with ground truth poses. The dataset includes
15 videos featuring colon deformations for qualitative assessment. C3VDv2
emulates diverse and challenging scenarios for 3D reconstruction algorithms,
including fecal debris, mucous pools, blood, debris obscuring the colonoscope
lens, en-face views, and fast camera motion. The enhanced realism of C3VDv2
will allow for more robust and representative development and evaluation of 3D
reconstruction algorithms.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [255] [VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding](https://arxiv.org/abs/2506.22799)
*Minchao Jiang,Shunyu Jia,Jiaming Gu,Xiaoyuan Lu,Guangming Zhu,Anqi Dong,Liang Zhang*

Main category: cs.GR

TL;DR: VoteSplat结合Hough投票与3D高斯泼溅（3DGS），通过SAM实现实例分割和2D投票图，嵌入空间偏移向量以优化3D场景理解，降低训练成本并提升语义清晰度。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法主要关注几何和外观建模，缺乏深层场景理解且训练成本高，VoteSplat旨在解决这些问题。

Method: 利用SAM进行实例分割生成2D投票图，嵌入空间偏移向量到高斯基元，结合深度失真约束优化3D定位。

Result: 实验证明VoteSplat在开放词汇3D实例定位、点云理解、点击定位和分层分割等方面有效。

Conclusion: VoteSplat通过投票机制和语义映射，显著提升了3D场景理解的效率和准确性。

Abstract: 3D Gaussian Splatting (3DGS) has become horsepower in high-quality, real-time
rendering for novel view synthesis of 3D scenes. However, existing methods
focus primarily on geometric and appearance modeling, lacking deeper scene
understanding while also incurring high training costs that complicate the
originally streamlined differentiable rendering pipeline. To this end, we
propose VoteSplat, a novel 3D scene understanding framework that integrates
Hough voting with 3DGS. Specifically, Segment Anything Model (SAM) is utilized
for instance segmentation, extracting objects, and generating 2D vote maps. We
then embed spatial offset vectors into Gaussian primitives. These offsets
construct 3D spatial votes by associating them with 2D image votes, while depth
distortion constraints refine localization along the depth axis. For
open-vocabulary object localization, VoteSplat maps 2D image semantics to 3D
point clouds via voting points, reducing training costs associated with
high-dimensional CLIP features while preserving semantic unambiguity. Extensive
experiments demonstrate effectiveness of VoteSplat in open-vocabulary 3D
instance localization, 3D point cloud understanding, click-based 3D object
localization, hierarchical segmentation, and ablation studies. Our code is
available at https://sy-ja.github.io/votesplat/

</details>


### [256] [DOBB-BVH: Efficient Ray Traversal by Transforming Wide BVHs into Oriented Bounding Box Trees using Discrete Rotations](https://arxiv.org/abs/2506.22849)
*Michael A. Kern,Alain Galvan,David Oldcorn,Daniel Skinner,Rohan Mehalwal,Leo Reyes Lozano,Matthäus G. Chajdas*

Main category: cs.GR

TL;DR: 提出一种新型OBB构建技术，通过固定离散旋转集优化计算和内存效率，显著提升光线追踪性能。


<details>
  <summary>Details</summary>
Motivation: 解决OBB层次结构在计算和内存上的高成本问题，同时提升光线追踪中的相交测试性能。

Method: 采用固定离散旋转集的OBB变换，结合k-DOPs扩展多子节点层次结构，作为后处理步骤集成到现有流程。

Result: 构建时间增加12.6%，但主光线性能提升18.5%，次光线提升32.4%，最大光线相交性能提升65%。

Conclusion: 该方法在实时应用中显著提升光线追踪性能，且易于集成到现有系统。

Abstract: Oriented bounding box (OBB) bounding volume hierarchies offer a more precise
fit than axis-aligned bounding box hierarchies in scenarios with thin elongated
and arbitrarily rotated geometry, enhancing intersection test performance in
ray tracing. However, determining optimally oriented bounding boxes can be
computationally expensive and have high memory requirements. Recent research
has shown that pre-built hierarchies can be efficiently converted to OBB
hierarchies on the GPU in a bottom-up pass, yielding significant ray tracing
traversal improvements. In this paper, we introduce a novel OBB construction
technique where all internal node children share a consistent OBB transform,
chosen from a fixed set of discrete quantized rotations. This allows for
efficient encoding and reduces the computational complexity of OBB
transformations. We further extend our approach to hierarchies with multiple
children per node by leveraging Discrete Orientation Polytopes (k-DOPs),
demonstrating improvements in traversal performance while limiting the build
time impact for real-time applications. Our method is applied as a
post-processing step, integrating seamlessly into existing hierarchy
construction pipelines. Despite a 12.6% increase in build time, our
experimental results demonstrate an average improvement of 18.5% in primary,
32.4% in secondary rays, and maximum gain of 65% in ray intersection
performance, highlighting its potential for advancing real-time applications.

</details>


### [257] [Confident Splatting: Confidence-Based Compression of 3D Gaussian Splatting via Learnable Beta Distributions](https://arxiv.org/abs/2506.22973)
*AmirHossein Naghi Razlighi,Elaheh Badali Golezani,Shohreh Kasaei*

Main category: cs.GR

TL;DR: 提出了一种基于可学习置信度分数的3D高斯泼溅压缩方法，通过优化置信度分数实现高效压缩，同时保持视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅虽能实现高质量实时渲染，但通常生成数百万个泼溅点，导致存储和计算开销过大。

Method: 提出一种基于Beta分布的可学习置信度分数方法，通过重构感知损失优化每个泼溅点的置信度，从而剪枝低置信度泼溅点。

Result: 实验表明，该方法在压缩与保真度之间取得了优于先前工作的平衡。

Conclusion: 该方法架构无关，适用于任何高斯泼溅变体，且平均置信度可作为场景质量的新评估指标。

Abstract: 3D Gaussian Splatting enables high-quality real-time rendering but often
produces millions of splats, resulting in excessive storage and computational
overhead. We propose a novel lossy compression method based on learnable
confidence scores modeled as Beta distributions. Each splat's confidence is
optimized through reconstruction-aware losses, enabling pruning of
low-confidence splats while preserving visual fidelity. The proposed approach
is architecture-agnostic and can be applied to any Gaussian Splatting variant.
In addition, the average confidence values serve as a new metric to assess the
quality of the scene. Extensive experiments demonstrate favorable trade-offs
between compression and fidelity compared to prior work. Our code and data are
publicly available at
https://github.com/amirhossein-razlighi/Confident-Splatting

</details>


### [258] [The ultimate display: Where will all the pixels come from?](https://arxiv.org/abs/2506.23001)
*Benjamin Watson,David Luebke*

Main category: cs.GR

TL;DR: 通过减少像素计算，采用时间自适应采样的渲染器可能实现高分辨率显示器的快速更新。


<details>
  <summary>Details</summary>
Motivation: 传统渲染模式在高分辨率显示器上效率低下，需要寻找更高效的渲染方法。

Method: 采用时间自适应采样技术，减少像素计算量。

Result: 可能实现打印机分辨率级别的显示器，支持每秒数百次更新。

Conclusion: 时间自适应采样是解决高分辨率显示器快速渲染的有效途径。

Abstract: Could the answer be to compute fewer pixels? Renderers that break traditional
framed patterns and opt for temporally adaptive sampling might be the key to
printer-resolution wall displays that update hundreds of times per second.

</details>


### [259] [Glyph-Based Multiscale Visualization of Turbulent Multi-Physics Statistics](https://arxiv.org/abs/2506.23092)
*Arisa Cowe,Tyson Neuroth,Qi Wu,Martin Rieth,Jacqueline Chen,Myoungkyu Lee,Kwan-Liu Ma*

Main category: cs.GR

TL;DR: 提出了一种新的多尺度流场局部空间统计可视化方法，结合曲线波变换、水平集限制的质心Voronoi剖分和符号设计，支持多场和多尺度数据的交互式分析。


<details>
  <summary>Details</summary>
Motivation: 多物理问题涉及多尺度数据，现有方法难以直观展示跨空间、尺度和场的相关性。

Method: 使用曲线波变换分解尺度，水平集限制的质心Voronoi剖分划分空间区域，设计符号整合多尺度和多场信息。

Result: 实现了交互式可视化系统，应用于湍流燃烧和不可压缩通道流数据，提升了对多场和多尺度相互作用的理解。

Conclusion: 该方法为科学家提供了更直观的多尺度流场分析工具。

Abstract: Many scientific and engineering problems involving multi-physics span a wide
range of scales. Understanding the interactions across these scales is
essential for fully comprehending such complex problems. However, visualizing
multivariate, multiscale data within an integrated view where correlations
across space, scales, and fields are easily perceived remains challenging. To
address this, we introduce a novel local spatial statistical visualization of
flow fields across multiple fields and turbulence scales. Our method leverages
the curvelet transform for scale decomposition of fields of interest, a
level-set-restricted centroidal Voronoi tessellation to partition the spatial
domain into local regions for statistical aggregation, and a set of glyph
designs that combines information across scales and fields into a single, or
reduced set of perceivable visual representations. Each glyph represents data
aggregated within a Voronoi region and is positioned at the Voronoi site for
direct visualization in a 3D view centered around flow features of interest. We
implement and integrate our method into an interactive visualization system
where the glyph-based technique operates in tandem with linked 3D spatial views
and 2D statistical views, supporting a holistic analysis. We demonstrate with
case studies visualizing turbulent combustion data--multi-scalar compressible
flows--and turbulent incompressible channel flow data. This new capability
enables scientists to better understand the interactions between multiple
fields and length scales in turbulent flows.

</details>


### [260] [Data-Driven Compute Overlays for Interactive Geographic Simulation and Visualization](https://arxiv.org/abs/2506.23364)
*Patrick Komon,Gerald Kimmersdorfer,Adam Celarek,Manuela Waldner*

Main category: cs.GR

TL;DR: 提出了一种基于WebGPU的交互式数据驱动计算覆盖层，用于原生和基于Web的3D地理地图应用，显著提升了雪崩模拟的计算速度。


<details>
  <summary>Details</summary>
Motivation: 为3D地理地图应用提供高效、实时的数据驱动覆盖层，以支持复杂模拟（如雪崩）的交互式参数调整和即时可视化。

Method: 采用多步骤GPU计算工作流，从多个数据源生成数据驱动覆盖层。

Result: 大规模雪崩模拟的计算时间从毫秒到秒不等，比现有Python实现快多个数量级。

Conclusion: 该方法为实时地理模拟提供了高效解决方案，具有广泛的应用潜力。

Abstract: We present interactive data-driven compute overlays for native and web-based
3D geographic map applications based on WebGPU. Our data-driven overlays are
generated in a multi-step compute workflow from multiple data sources on the
GPU. We demonstrate their potential by showing results from snow cover and
avalanche simulations, where simulation parameters can be adjusted
interactively and results are visualized instantly. Benchmarks show that our
approach can compute large-scale avalanche simulations in milliseconds to
seconds, depending on the size of the terrain and the simulation parameters,
which is multiple orders of magnitude faster than a state-of-the-art Python
implementation.

</details>


### [261] [Escher Tile Deformation via Closed-Form Solution](https://arxiv.org/abs/2506.23388)
*Crane He Chen,Vladimir G. Kim*

Main category: cs.GR

TL;DR: 提出了一种实时变形方法，用于处理无缝拼接的Escher瓷砖，通过解析解生成周期性位移场，确保变形时无间隙或重叠。


<details>
  <summary>Details</summary>
Motivation: Escher瓷砖因其有机形态和对称性而独特，但现有方法难以在不破坏其无缝拼接特性的情况下进行变形。

Method: 将问题建模为周期性位移场的求解，通过解析解获得封闭形式的位移场，支持17种壁纸群，并处理图像和网格等多种表示。

Result: 方法成功实现了瓷砖边界和内部的同步变形，支持用户通过自适应衰减参数进行精细控制，适用于照片编辑和形状雕刻等应用。

Conclusion: 该方法为Escher瓷砖的实时变形提供了高效且艺术可控的解决方案，适用于制造和动画等领域。

Abstract: We present a real-time deformation method for Escher tiles -- interlocking
organic forms that seamlessly tessellate the plane following symmetry rules. We
formulate the problem as determining a periodic displacement field. The goal is
to deform Escher tiles without introducing gaps or overlaps. The resulting
displacement field is obtained in closed form by an analytical solution. Our
method processes tiles of 17 wallpaper groups across various representations
such as images and meshes. Rather than treating tiles as mere boundaries, we
consider them as textured shapes, ensuring that both the boundary and interior
deform simultaneously. To enable fine-grained artistic input, our interactive
tool features a user-controllable adaptive fall-off parameter, allowing precise
adjustment of locality and supporting deformations with meaningful semantic
control. We demonstrate the effectiveness of our method through various
examples, including photo editing and shape sculpting, showing its use in
applications such as fabrication and animation.

</details>


### [262] [Uncertain Mode Surfaces in 3D Symmetric Second-Order Tensor Field Ensembles](https://arxiv.org/abs/2506.23406)
*Tim Gerrits*

Main category: cs.GR

TL;DR: 本文提出了一种将不确定退化张量特征推广到任意模态值的不确定模态表面的方法，为张量场集合中的不确定模态拓扑特征提供了统一分析框架。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常通过派生标量属性或张量符号表示不确定性，但难以捕捉全局行为。模态表面（如中性表面和任意模态表面）对全面理解张量场拓扑至关重要，但现有研究主要关注退化张量位置。

Method: 提出了一种将不确定退化张量特征推广到任意模态值的不确定模态表面的方法，支持表面和线几何，形成统一框架。

Result: 在工程和材料科学的多个实际模拟数据集上验证了方法的有效性。

Conclusion: 该方法为张量场集合中的不确定模态拓扑特征提供了全面的分析工具，扩展了现有研究的范围。

Abstract: The analysis of 3D symmetric second-order tensor fields often relies on
topological features such as degenerate tensor lines, neutral surfaces, and
their generalization to mode surfaces, which reveal important structural
insights into the data. However, uncertainty in such fields is typically
visualized using derived scalar attributes or tensor glyph representations,
which often fail to capture the global behavior. Recent advances have
introduced uncertain topological features for tensor field ensembles by
focusing on degenerate tensor locations. Yet, mode surfaces, including neutral
surfaces and arbitrary mode surfaces are essential to a comprehensive
understanding of tensor field topology. In this work, we present a
generalization of uncertain degenerate tensor features to uncertain mode
surfaces of arbitrary mode values, encompassing uncertain degenerate tensor
lines as a special case. Our approach supports both surface and line
geometries, forming a unified framework for analyzing uncertain mode-based
topological features in tensor field ensembles. We demonstrate the
effectiveness of our method on several real-world simulation datasets from
engineering and materials science.

</details>


### [263] [Synthetically Expressive: Evaluating gesture and voice for emotion and empathy in VR and 2D scenarios](https://arxiv.org/abs/2506.23777)
*Haoyang Du,Kiran Chhatre,Christopher Peters,Brian Keegan,Rachel McDonnell,Cathy Ennis*

Main category: cs.GR

TL;DR: 研究评估了真实与合成的语音和手势在不同沉浸感（VR vs. 2D）和情感背景下对用户感知的影响，发现VR增强自然手势-语音配对感知，但对合成配对无类似效果。


<details>
  <summary>Details</summary>
Motivation: 探讨语音和手势生成技术在虚拟现实中的整合问题，以及它们如何传递情感细节。

Method: 通过比较真实与合成的语音和手势，结合不同沉浸感和情感背景，分析用户感知。

Result: VR提升自然手势-语音配对的感知，但对合成配对无效，凸显两者间的感知差距。

Conclusion: 需重新评估手势的适用性并优化AI驱动的合成技术以适应沉浸式环境。

Abstract: The creation of virtual humans increasingly leverages automated synthesis of
speech and gestures, enabling expressive, adaptable agents that effectively
engage users. However, the independent development of voice and gesture
generation technologies, alongside the growing popularity of virtual reality
(VR), presents significant questions about the integration of these signals and
their ability to convey emotional detail in immersive environments. In this
paper, we evaluate the influence of real and synthetic gestures and speech,
alongside varying levels of immersion (VR vs. 2D displays) and emotional
contexts (positive, neutral, negative) on user perceptions. We investigate how
immersion affects the perceived match between gestures and speech and the
impact on key aspects of user experience, including emotional and empathetic
responses and the sense of co-presence. Our findings indicate that while VR
enhances the perception of natural gesture-voice pairings, it does not
similarly improve synthetic ones - amplifying the perceptual gap between them.
These results highlight the need to reassess gesture appropriateness and refine
AI-driven synthesis for immersive environments. See video:
https://youtu.be/WMfjIB1X-dc

</details>


### [264] [GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering](https://arxiv.org/abs/2506.23957)
*Zinuo You,Stamatios Georgoulis,Anpei Chen,Siyu Tang,Dengxin Dai*

Main category: cs.GR

TL;DR: GaVS是一种基于3D的视频稳定方法，通过局部重建和渲染范式解决现有方法的几何失真、过度裁剪等问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频稳定方法存在几何失真、过度裁剪和泛化能力差等问题，影响用户体验。

Method: GaVS采用3D相机姿态，通过高斯散射原语预测和测试时微调，结合多视角动态感知光度监督和跨帧正则化，实现时间一致的局部重建和渲染。

Result: 在多样化的相机运动和场景动态数据集上，GaVS在传统任务指标和几何一致性上优于现有2D和2.5D方法，用户研究也验证了其优越性。

Conclusion: GaVS通过3D基础方法显著提升了视频稳定效果，解决了现有技术的局限性。

Abstract: Video stabilization is pivotal for video processing, as it removes unwanted
shakiness while preserving the original user motion intent. Existing
approaches, depending on the domain they operate, suffer from several issues
(e.g. geometric distortions, excessive cropping, poor generalization) that
degrade the user experience. To address these issues, we introduce
\textbf{GaVS}, a novel 3D-grounded approach that reformulates video
stabilization as a temporally-consistent `local reconstruction and rendering'
paradigm. Given 3D camera poses, we augment a reconstruction model to predict
Gaussian Splatting primitives, and finetune it at test-time, with multi-view
dynamics-aware photometric supervision and cross-frame regularization, to
produce temporally-consistent local reconstructions. The model are then used to
render each stabilized frame. We utilize a scene extrapolation module to avoid
frame cropping. Our method is evaluated on a repurposed dataset, instilled with
3D-grounded information, covering samples with diverse camera motions and scene
dynamics. Quantitatively, our method is competitive with or superior to
state-of-the-art 2D and 2.5D approaches in terms of conventional task metrics
and new geometry consistency. Qualitatively, our method produces noticeably
better results compared to alternatives, validated by the user study.

</details>


### [265] [Navigating with Annealing Guidance Scale in Diffusion Space](https://arxiv.org/abs/2506.24108)
*Shai Yehezkel,Omer Dahary,Andrey Voynov,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: 提出了一种动态调整引导尺度的调度器，显著提升文本到图像生成的质量和提示对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的无分类器引导（CFG）方法在生成图像时对引导尺度的选择敏感，影响图像质量和提示对齐。

Method: 通过基于条件噪声信号动态调整引导尺度的调度策略，优化CFG的表现。

Result: 实验表明，该方法显著提升了图像质量和提示对齐，且无需额外计算资源。

Conclusion: 提出的调度器改进了CFG，在图像生成中实现了更好的质量和提示对齐的平衡。

Abstract: Denoising diffusion models excel at generating high-quality images
conditioned on text prompts, yet their effectiveness heavily relies on careful
guidance during the sampling process. Classifier-Free Guidance (CFG) provides a
widely used mechanism for steering generation by setting the guidance scale,
which balances image quality and prompt alignment. However, the choice of the
guidance scale has a critical impact on the convergence toward a visually
appealing and prompt-adherent image. In this work, we propose an annealing
guidance scheduler which dynamically adjusts the guidance scale over time based
on the conditional noisy signal. By learning a scheduling policy, our method
addresses the temperamental behavior of CFG. Empirical results demonstrate that
our guidance scheduler significantly enhances image quality and alignment with
the text prompt, advancing the performance of text-to-image generation.
Notably, our novel scheduler requires no additional activations or memory
consumption, and can seamlessly replace the common classifier-free guidance,
offering an improved trade-off between prompt alignment and quality.

</details>


### [266] [ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes](https://arxiv.org/abs/2506.21629)
*Chenhao Zhang,Yezhi Shen,Fengqing Zhu*

Main category: cs.GR

TL;DR: 论文提出了一种结合ICP与优化细化方法，用于解决神经渲染中相机姿态估计问题，并引入体素场景密集化方法，提升大尺度场景重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法（如NeRFs和3DGS）依赖预处理相机姿态和3D结构先验，这在户外场景中难以获取。

Method: 结合ICP与优化细化进行相机姿态估计，并采用体素场景密集化方法指导大尺度场景重建。

Result: 实验表明，ICP-3DGS在相机姿态估计和新视角合成上优于现有方法。

Conclusion: 该方法有效解决了大尺度场景中相机姿态估计的挑战，提升了重建效果。

Abstract: In recent years, neural rendering methods such as NeRFs and 3D Gaussian
Splatting (3DGS) have made significant progress in scene reconstruction and
novel view synthesis. However, they heavily rely on preprocessed camera poses
and 3D structural priors from structure-from-motion (SfM), which are
challenging to obtain in outdoor scenarios. To address this challenge, we
propose to incorporate Iterative Closest Point (ICP) with optimization-based
refinement to achieve accurate camera pose estimation under large camera
movements. Additionally, we introduce a voxel-based scene densification
approach to guide the reconstruction in large-scale scenes. Experiments
demonstrate that our approach ICP-3DGS outperforms existing methods in both
camera pose estimation and novel view synthesis across indoor and outdoor
scenes of various scales. Source code is available at
https://github.com/Chenhao-Z/ICP-3DGS.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [267] [Cluster analysis of earthquake hypocenters in Azerbaijan and surrounding territories](https://arxiv.org/abs/2506.22829)
*Sergii Skurativskyi,Sergiy Mykulyak,Yuliya Semenova,Kateryna Skurativska*

Main category: physics.geo-ph

TL;DR: 研究分析了2010-2023年阿塞拜疆及周边地区的地震目录，通过统计方法和聚类算法划分震源簇，并评估分区与断层网络的相似性。


<details>
  <summary>Details</summary>
Motivation: 阿塞拜疆及周边地区地震活动频繁，研究旨在通过分析震源分布，揭示地震活动的空间异质性和潜在断层联系。

Method: 使用Morisita指数初步评估震源集的统计特性，结合DBSCAN和HDBSCAN算法识别空间簇，并通过Silhouette指数和Adjusted Rand指数优化分区。

Result: 发现震源空间分布不均匀，存在多个密集簇；分区结果与断层网络相似，并识别出双峰深度分布。

Conclusion: 研究提供了一种有效的震源簇划分方法，揭示了地震深度分布的双峰特征，为地震活动分析提供了新视角。

Abstract: The research focuses on seismic events that occurred in Azerbaijan and
adjacent territories, regions known for strong seismic activity. We analyze a
catalog of recorded earthquakes between 2010 and 2023, extracting the locations
of the earthquake hypocenters for study purposes. Using statistical methods and
cluster analysis tools, we developed a procedure for partitioning hypocenter
clusters. The procedure begins with estimates of the Morisita Index, which is
suitable for preliminary assessments of the statistical properties of
hypocenter sets. Analysis of the Morisita Index indicates that the spatial
distribution of hypocenters is heterogeneous, containing denser domains
referred to as clusters. The next stage involves identifying spatial clusters
using the DBSCAN and HDBSCAN algorithms. Due to the strong dependence of
results on the algorithm's parameters, we selected several partitions with 5-8
clusters that provided maximal or near-maximal Silhouette Index values. The
final stage assesses the similarity of the resulting partitions, using the
Adjusted Rand Index to identify partitions with a specified degree of
similarity. The final set of partitions was compared to the fault network of
the region. Based on the selected partition, the earthquake depth distributions
were studied. Specifically, approximate probability density functions were
constructed in the form of mixtures of normal distributions, leading to the
identification of several bimodal distributions.

</details>


### [268] [Physics-informed conditional diffusion model for generalizable elastic wave-mode separation](https://arxiv.org/abs/2506.23007)
*Shijun Cheng,Xinru Mu,Tariq Alkhalifah*

Main category: physics.geo-ph

TL;DR: 提出一种基于物理约束的条件扩散模型，用于弹性波场分离，结合物理方程提升效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统方法计算资源消耗大，纯数据驱动方法缺乏物理一致性，需一种高效且物理一致的方法。

Method: 结合物理方程的条件扩散模型，通过物理引导的损失和采样修正实现波场分离。

Result: 数值实验显示分离结果与传统方法接近，但成本更低，验证了方法的有效性和泛化性。

Conclusion: 该方法在保持物理一致性的同时，显著提升了计算效率，适用于大规模地质模型。

Abstract: Traditional elastic wavefield separation methods, while accurate, often
demand substantial computational resources, especially for large geological
models or 3D scenarios. Purely data-driven neural network approaches can be
more efficient, but may fail to generalize and maintain physical consistency
due to the absence of explicit physical constraints. Here, we propose a
physics-informed conditional diffusion model for elastic wavefield separation
that seamlessly integrates domain-specific physics equations into both the
training and inference stages of the reverse diffusion process. Conditioned on
full elastic wavefields and subsurface P- and S-wave velocity profiles, our
method directly predicts clean P-wave modes while enforcing Laplacian
separation constraints through physics-guided loss and sampling corrections.
Numerical experiments on diverse scenarios yield the separation results that
closely match conventional numerical solutions but at a reduced cost,
confirming the effectiveness and generalizability of our approach.

</details>


### [269] [Enhanced Ionospheric Ray-Tracing: Advanced Electron Collision and Horizontal Gradient Modeling in the IONORT-ISP-WC System](https://arxiv.org/abs/2506.24098)
*Alessandro Settimi*

Main category: physics.geo-ph

TL;DR: IONORT-ISP-WC是一种先进的电离层射线追踪工具，通过改进HF无线电波传播预测，显著提升了IONORT-ISP的性能。


<details>
  <summary>Details</summary>
Motivation: 改进电离层射线追踪工具的准确性，尤其是通过水平梯度建模和更高分辨率的数据网格来提升HF无线电波传播预测。

Method: 集成双指数碰撞频率模型、扩展3-D电子密度网格至65 km、提高空间分辨率至1° x 1°，并实现水平梯度的详细建模。

Result: 验证显示IONORT-ISP-WC在最大可用频率（MUF）预测上具有更高的准确性。

Conclusion: IONORT-ISP-WC是一种可靠且前沿的操作工具，未来需进一步验证水平梯度建模并增强全球数据同化网络。

Abstract: This manuscript analyzes IONORT-ISP-WC, an advanced ionospheric ray-tracing
tool improving HF radio wave propagation predictions. It significantly upgrades
IONORT-ISP by integrating a double-exponential collision frequency model for
the D-layer (primary HF absorption), extending the ISP 3-D electron density
grid to 65 km, and increasing spatial resolution from 2{\deg} x 2{\deg} to
1{\deg} x 1{\deg}. A central focus is the detailed examination and local_ionort
Fortran implementation of horizontal gradients in ionospheric electron density
profiles. This reveals a robust framework for incorporating these gradients.
Crucially, electx_grid now actively calculates horizontal gradients (previously
commented), though full Taylor optimization is a high-priority future
development to leverage the high-resolution grid for unparalleled accuracy.
IONORT-ISP-WC underwent rigorous validation against observed and synthetic
oblique ionograms (from IONORT-IRI-WC, based on the climatological IRI model).
Results demonstrate its superior Maximum Usable Frequency (MUF) prediction
accuracy. This underscores assimilative models' value in capturing dynamic
ionospheric conditions, especially with meticulous horizontal gradient
accounting. MUF prediction discrepancies are primarily attributed to real-time
assimilation data limitations (availability, geographical distribution). This
report positions IONORT-ISP-WC as a robust, reliable, cutting-edge operational
tool for diverse space weather applications. It outlines crucial future
developments: comprehensive validation of advanced horizontal gradient modeling
and strategic enhancement of global data assimilation networks for higher
accuracy and resilience.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [270] [Predicting Instabilities in Transient Landforms and Interconnected Ecosystems](https://arxiv.org/abs/2506.23946)
*Taylor Smith,Andreas Morr,Bodo Bookhagen,Niklas Boers*

Main category: nlin.CD

TL;DR: 提出一种基于特征值追踪和Floquet乘子的新方法，用于量化系统稳定性，无需预处理数据，适用于多种时空系统。


<details>
  <summary>Details</summary>
Motivation: 现有稳定性分析框架需要去除趋势和季节性，易引入误差，需一种更直接、稳健的方法。

Method: 基于特征值追踪和Floquet乘子，直接分析原始数据，无需去趋势或季节性，适用于合成数据、冰川运动和生态系统。

Result: 成功预测冰川涌动，分析亚马逊雨林植被生产力的空间模式，验证了方法的灵活性和有效性。

Conclusion: 新方法首次将临界减速理论应用于冰川动力学，为广泛时空系统的稳定性量化提供了灵活工具。

Abstract: Many parts of the Earth system are thought to have multiple stable
equilibrium states, with the potential for rapid and sometimes catastrophic
shifts between them. The most common frameworks for analyzing stability
changes, however, require stationary (trend- and seasonality-free) data, which
necessitates error-prone data pre-processing. Here we propose a novel method of
quantifying system stability based on eigenvalue tracking and Floquet
Multipliers, which can be applied directly to diverse data without first
removing trend and seasonality, and is robust to changing noise levels, as can
be caused by merging signals from different sensors. We first demonstrate this
approach with synthetic data and further show how glacier surge onset can be
predicted from observed surface velocity time series. We then show that our
method can be extended to analyze spatio-temporal data and illustrate this
flexibility with remotely sensed Amazon rainforest vegetation productivity,
highlighting the spatial patterns of whole-ecosystem destabilization. Our work
applies critical slowing down theory to glacier dynamics for the first time,
and provides a novel and flexible method to quantify the stability or
resilience of a wide range of spatiotemporal systems, including climate
subsystems, ecosystems, and transient landforms.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [271] [Wireless Home Automation Using Social Networking Websites](https://arxiv.org/abs/2506.22482)
*Divya Alok Gupta,Dwith Chenna,B. Aditya Vighnesh Ramakanth*

Main category: cs.NI

TL;DR: 提出一种基于社交网络认证的无线家庭自动化系统，解决安全性和用户友好性问题。


<details>
  <summary>Details</summary>
Motivation: 随着物联网的发展，无线家庭自动化系统面临安全性和多设备统一控制的挑战。

Method: 利用Twitter等社交网络的安全认证系统，跟踪用户活动并控制家用设备。

Result: 展示了系统的应用场景，并对比了传统系统的优势。

Conclusion: 提出的系统在安全性和用户体验上优于传统方案。

Abstract: With the advent of Internet of Things, Wireless Home Automation Systems WHAS
are gradually gaining popularity. These systems are faced with multiple
challenges such as security; controlling a variety of home appliances with a
single interface and user friendliness. In this paper we propose a system that
uses secure authentication systems of social networking websites such as
Twitter, tracks the end-users activities on the social network and then control
his or her domestic appliances. At the end, we highlight the applications of
the proposed WHAS and compare the advantages of our proposed system over
traditional home automation systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [272] [Multimodal Atmospheric Super-Resolution With Deep Generative Models](https://arxiv.org/abs/2506.22780)
*Dibyajyoti Chakraborty,Haiwen Guan,Jason Stock,Troy Arcomano,Guido Cervone,Romit Maulik*

Main category: cs.LG

TL;DR: 基于分数的扩散模型通过学习数据的对数概率密度梯度，实现复杂分布的采样，并支持零样本条件生成。本文将其应用于高维动力系统的超分辨率任务，结合多模态数据，展示了准确的恢复能力。


<details>
  <summary>Details</summary>
Motivation: 探索基于分数的扩散模型在数据与模型融合中的潜力，特别是在高维动力系统的超分辨率任务中，结合实时低分辨率和多模态观测数据。

Method: 利用预训练的分数扩散模型，通过贝叶斯框架更新隐式学习的分布，结合稀疏传感器测量和多模态数据，实现超分辨率重建。

Result: 实验表明，模型能够准确恢复高维状态，并在时空重建中平衡多模态数据的影响。

Conclusion: 基于分数的扩散模型为高维动力系统的超分辨率任务提供了一种有效方法，支持多模态数据融合和不确定性估计。

Abstract: Score-based diffusion modeling is a generative machine learning algorithm
that can be used to sample from complex distributions. They achieve this by
learning a score function, i.e., the gradient of the log-probability density of
the data, and reversing a noising process using the same. Once trained,
score-based diffusion models not only generate new samples but also enable
zero-shot conditioning of the generated samples on observed data. This promises
a novel paradigm for data and model fusion, wherein the implicitly learned
distributions of pretrained score-based diffusion models can be updated given
the availability of online data in a Bayesian formulation. In this article, we
apply such a concept to the super-resolution of a high-dimensional dynamical
system, given the real-time availability of low-resolution and experimentally
observed sparse sensor measurements from multimodal data. Additional analysis
on how score-based sampling can be used for uncertainty estimates is also
provided. Our experiments are performed for a super-resolution task that
generates the ERA5 atmospheric dataset given sparse observations from a
coarse-grained representation of the same and/or from unstructured experimental
observations of the IGRA radiosonde dataset. We demonstrate accurate recovery
of the high dimensional state given multiple sources of low-fidelity
measurements. We also discover that the generative model can balance the
influence of multiple dataset modalities during spatiotemporal reconstructions.

</details>


### [273] [Mitigating Semantic Collapse in Generative Personalization with a Surprisingly Simple Test-Time Embedding Adjustment](https://arxiv.org/abs/2506.22685)
*Anh Bui,Trang Vu,Trung Le,Junae Kim,Tamas Abraham,Rollin Omari,Amar Kaur,Dinh Phung*

Main category: cs.LG

TL;DR: 本文研究了生成个性化中的语义坍缩问题，提出了一种无需训练的方法来调整预训练嵌入的幅度和方向，有效解决了该问题。


<details>
  <summary>Details</summary>
Motivation: 生成个性化中，学习到的视觉概念（$V^*$）会逐渐偏离原始文本含义，导致多概念输入提示的语义丰富性降低，输出图像简化。

Method: 提出了一种无需训练的方法，在推理时调整预训练嵌入的幅度和方向。

Result: 该方法显著改善了文本与图像的匹配度，适用于多种个性化方法。

Conclusion: 通过调整嵌入的幅度和方向，有效缓解了语义坍缩问题，提升了生成图像的语义丰富性。

Abstract: In this paper, we investigate the semantic collapsing problem in generative
personalization, an under-explored topic where the learned visual concept
($V^*$) gradually shifts from its original textual meaning and comes to
dominate other concepts in multi-concept input prompts. This issue not only
reduces the semantic richness of complex input prompts like "a photo of $V^*$
wearing glasses and playing guitar" into simpler, less contextually rich forms
such as "a photo of $V^*$" but also leads to simplified output images that fail
to capture the intended concept.
  We identify the root cause as unconstrained optimisation, which allows the
learned embedding $V^*$ to drift arbitrarily in the embedding space, both in
direction and magnitude. To address this, we propose a simple yet effective
training-free method that adjusts the magnitude and direction of pre-trained
embedding at inference time, effectively mitigating the semantic collapsing
problem. Our method is broadly applicable across different personalization
methods and demonstrates significant improvements in text-image alignment in
diverse use cases. Our code is anonymously published at
https://anonymous.4open.science/r/Embedding-Adjustment.

</details>


### [274] [Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture](https://arxiv.org/abs/2506.22447)
*Fabio Merizzi,Harilaos Loukos*

Main category: cs.LG

TL;DR: 提出了一种多任务、多变量的Vision Transformer架构（1EMD），用于联合预测三个关键气候变量，优于单变量方法并提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 全球气候模型（GCMs）分辨率低，区域气候模型（RCMs）计算成本高且灵活性有限，现有深度学习方法多为单变量模型，存在上下文意识不足和计算冗余问题。

Method: 采用共享编码器和变量特定解码器的Vision Transformer架构，联合预测表面温度、风速和500 hPa位势高度。

Result: 多变量方法实现了跨变量知识转移，性能优于单变量基线，并提高了计算效率。

Conclusion: 多变量建模在高分辨率气候降尺度中具有显著优势。

Abstract: Global Climate Models (GCMs) are critical for simulating large-scale climate
dynamics, but their coarse spatial resolution limits their applicability in
regional studies. Regional Climate Models (RCMs) refine this through dynamic
downscaling, albeit at considerable computational cost and with limited
flexibility. While deep learning has emerged as an efficient data-driven
alternative, most existing studies have focused on single-variable models that
downscale one variable at a time. This approach can lead to limited contextual
awareness, redundant computation, and lack of cross-variable interaction. Our
study addresses these limitations by proposing a multi-task, multi-variable
Vision Transformer (ViT) architecture with a shared encoder and
variable-specific decoders (1EMD). The proposed architecture jointly predicts
three key climate variables: surface temperature (tas), wind speed (sfcWind),
and 500 hPa geopotential height (zg500), directly from GCM-resolution inputs,
emulating RCM-scale downscaling over Europe. We show that our multi-variable
approach achieves positive cross-variable knowledge transfer and consistently
outperforms single-variable baselines trained under identical conditions, while
also improving computational efficiency. These results demonstrate the
effectiveness of multi-variable modeling for high-resolution climate
downscaling.

</details>


### [275] [Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration](https://arxiv.org/abs/2506.22929)
*Chen Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于空间完备性的并行计算架构，用于处理高维数据，支持分布式处理和科学计算。


<details>
  <summary>Details</summary>
Motivation: 深度学习在高维数据应用中面临维度诅咒的计算挑战，现有工具缺乏对高级分析的数学统计支持。

Method: 通过将高维数据分解为维度无关的结构，实现分布式处理，并集成数据挖掘和并行优化的机器学习方法。

Result: 该框架支持跨数据类型（如医学和自然图像）的科学计算，并在统一系统中实现高效处理。

Conclusion: 提出的架构为高维数据的高效处理提供了可行的解决方案，支持多样化的科学计算需求。

Abstract: While deep learning excels in natural image and language processing, its
application to high-dimensional data faces computational challenges due to the
dimensionality curse. Current large-scale data tools focus on business-oriented
descriptive statistics, lacking mathematical statistics support for advanced
analysis. We propose a parallel computation architecture based on space
completeness, decomposing high-dimensional data into dimension-independent
structures for distributed processing. This framework enables seamless
integration of data mining and parallel-optimized machine learning methods,
supporting scientific computations across diverse data types like medical and
natural images within a unified system.

</details>


### [276] [Riemannian-Geometric Fingerprints of Generative Models](https://arxiv.org/abs/2506.22802)
*Hae Jin Song,Laurent Itti*

Main category: cs.LG

TL;DR: 论文提出了一种基于黎曼几何的生成模型指纹定义和计算方法，用于区分不同生成模型及其生成内容，解决了模型归属和数据真实性的问题。


<details>
  <summary>Details</summary>
Motivation: 生成模型的快速发展和广泛应用引发了对模型归属和指纹识别的需求，以保护知识产权、确保内容来源可信，并防止模型崩溃。

Method: 采用黎曼几何方法，定义了生成模型的指纹，并通过学习黎曼度量和计算测地距离来改进指纹识别算法。

Result: 该方法在多个数据集、模型架构和模态中表现优异，显著提升了模型归属的准确性和泛化能力。

Conclusion: 提出的黎曼几何框架为生成模型指纹的识别提供了理论基础和实践工具，具有广泛的应用潜力。

Abstract: Recent breakthroughs and rapid integration of generative models (GMs) have
sparked interest in the problem of model attribution and their fingerprints.
For instance, service providers need reliable methods of authenticating their
models to protect their IP, while users and law enforcement seek to verify the
source of generated content for accountability and trust. In addition, a
growing threat of model collapse is arising, as more model-generated data are
being fed back into sources (e.g., YouTube) that are often harvested for
training ("regurgitative training"), heightening the need to differentiate
synthetic from human data. Yet, a gap still exists in understanding generative
models' fingerprints, we believe, stemming from the lack of a formal framework
that can define, represent, and analyze the fingerprints in a principled way.
To address this gap, we take a geometric approach and propose a new definition
of artifact and fingerprint of GMs using Riemannian geometry, which allows us
to leverage the rich theory of differential geometry. Our new definition
generalizes previous work (Song et al., 2024) to non-Euclidean manifolds by
learning Riemannian metrics from data and replacing the Euclidean distances and
nearest-neighbor search with geodesic distances and kNN-based Riemannian center
of mass. We apply our theory to a new gradient-based algorithm for computing
the fingerprints in practice. Results show that it is more effective in
distinguishing a large array of GMs, spanning across 4 different datasets in 2
different resolutions (64 by 64, 256 by 256), 27 model architectures, and 2
modalities (Vision, Vision-Language). Using our proposed definition
significantly improves the performance on model attribution, as well as a
generalization to unseen datasets, model types, and modalities, suggesting its
practical efficacy.

</details>


### [277] [ReMem: Mutual Information-Aware Fine-tuning of Pretrained Vision Transformers for Effective Knowledge Distillation](https://arxiv.org/abs/2506.23041)
*Chengyu Dong,Huan Gui,Noveen Sachdeva,Long Jin,Ke Yin,Jingbo Shang,Lichan Hong,Ed H. Chi,Zhe Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种通过互信息感知优化和MLP块重加权的方法，以提高从大规模预训练视觉Transformer（ViT）中知识蒸馏的效果。


<details>
  <summary>Details</summary>
Motivation: 当从大规模预训练的强模型中进行知识蒸馏时，知识转移效果显著下降。本文旨在解决这一问题，尤其是针对预训练的视觉Transformer。

Method: 通过互信息感知优化进行微调，并在小规模或高度不平衡的下游数据集上采用MLP块重加权的启发式方法。

Result: 该方法使小型学生模型能够从最强的预训练模型中受益。

Conclusion: 提出的方法有效提升了知识蒸馏的效果，尤其是在大规模预训练模型的应用中。

Abstract: Knowledge distillation from pretrained visual representation models offers an
effective approach to improve small, task-specific production models. However,
the effectiveness of such knowledge transfer drops significantly when
distilling from strong models that are pretrained in a large scale. In this
paper, we address this challenge for pretrained Vision Transformers (ViTs) by
exploring methods to fine-tune them for more effective knowledge transfer.
Motivated by the connection between mutual information and distillation
effectiveness, we propose to employ mutual information-aware optimization
during finetuning. For small or highly-imbalanced downstream datasets where
such optimization becomes less effective, we introduce a simple yet effective
heuristic of reweighting MLP blocks. This approach is inspired by our
observation that top MLP blocks are primarily responsible for mutual
information loss. Our method enables small student models to benefit from those
pretrained models among the strongest.

</details>


### [278] [Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings](https://arxiv.org/abs/2506.23145)
*Shahad Hardan,Darya Taratynova,Abdelmajid Essofi,Karthik Nandakumar,Mohammad Yaqub*

Main category: cs.LG

TL;DR: 论文提出Forget-MI方法，用于多模态医疗数据的机器遗忘，通过损失函数和扰动技术实现，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医疗AI中隐私保护至关重要，但现有方法难以从多模态架构中移除敏感数据。

Method: 通过建立损失函数和扰动技术，遗忘单模态和联合表示，同时保留剩余数据知识。

Result: Forget-MI在遗忘数据集上表现优异，减少MIA攻击能力，测试集性能与原模型相当。

Conclusion: Forget-MI是一种有效的多模态医疗数据遗忘方法，平衡了隐私保护和模型性能。

Abstract: Privacy preservation in AI is crucial, especially in healthcare, where models
rely on sensitive patient data. In the emerging field of machine unlearning,
existing methodologies struggle to remove patient data from trained multimodal
architectures, which are widely used in healthcare. We propose Forget-MI, a
novel machine unlearning method for multimodal medical data, by establishing
loss functions and perturbation techniques. Our approach unlearns unimodal and
joint representations of the data requested to be forgotten while preserving
knowledge from the remaining data and maintaining comparable performance to the
original model. We evaluate our results using performance on the forget
dataset, performance on the test dataset, and Membership Inference Attack
(MIA), which measures the attacker's ability to distinguish the forget dataset
from the training dataset. Our model outperforms the existing approaches that
aim to reduce MIA and the performance on the forget dataset while keeping an
equivalent performance on the test set. Specifically, our approach reduces MIA
by 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305,
respectively. Additionally, our performance on the test set matches that of the
retrained model, while allowing forgetting. Code is available at
https://github.com/BioMedIA-MBZUAI/Forget-MI.git

</details>


### [279] [maneuverRecognition -- A Python package for Timeseries Classification in the domain of Vehicle Telematics](https://arxiv.org/abs/2506.23147)
*Jonathan Schuster,Fabian Transchel*

Main category: cs.LG

TL;DR: 论文介绍了一个名为maneuverRecognition的Python包，用于处理车辆远程信息数据，实现驾驶行为的分类和评估，以提高道路安全和环保驾驶。


<details>
  <summary>Details</summary>
Motivation: 驾驶行为的自动识别可用于个性化保险政策和提高道路安全，但目前缺乏快速处理数据和构建预测模型的工具。

Method: 开发了maneuverRecognition包，提供数据预处理、建模和评估功能，并包含一个可修改的LSTM网络结构。

Result: 使用智能手机传感器记录的三人真实驾驶数据验证了该包的有效性。

Conclusion: maneuverRecognition包为驾驶行为识别提供了实用工具，支持快速建模和评估。

Abstract: In the domain of vehicle telematics the automated recognition of driving
maneuvers is used to classify and evaluate driving behaviour. This not only
serves as a component to enhance the personalization of insurance policies, but
also to increase road safety, reduce accidents and the associated costs as well
as to reduce fuel consumption and support environmentally friendly driving. In
this context maneuver recognition technically requires a continuous application
of time series classification which poses special challenges to the transfer,
preprocessing and storage of telematic sensor data, the training of predictive
models, and the prediction itself. Although much research has been done in the
field of gathering relevant data or regarding the methods to build predictive
models for the task of maneuver recognition, there is a practical need for
python packages and functions that allow to quickly transform data into the
required structure as well as to build and evaluate such models. The
maneuverRecognition package was therefore developed to provide the necessary
functions for preprocessing, modelling and evaluation and also includes a ready
to use LSTM based network structure that can be modified. The implementation of
the package is demonstrated using real driving data of three different persons
recorded via smartphone sensors.

</details>


### [280] [Single Image Inpainting and Super-Resolution with Simultaneous Uncertainty Guarantees by Universal Reproducing Kernels](https://arxiv.org/abs/2506.23221)
*Bálint Horváth,Balázs Csanád Csáji*

Main category: cs.LG

TL;DR: 论文提出了一种统计学习方法用于图像缺失像素估计，同时提供不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 解决图像修复和超分辨率问题中的像素估计，并量化估计的不确定性。

Method: 基于再生核希尔伯特空间（RKHS）的假设，提出SGKI方法，扩展了现有核方法，并利用Schur补高效计算置信带。

Result: SGKI不仅能估计缺失像素，还能构建非渐近置信带，适用于所有缺失像素。

Conclusion: SGKI在合成和基准图像数据集上表现良好，适用于向量值函数。

Abstract: The paper proposes a statistical learning approach to the problem of
estimating missing pixels of images, crucial for image inpainting and
super-resolution problems. One of the main novelties of the method is that it
also provides uncertainty quantifications together with the estimated values.
Our core assumption is that the underlying data-generating function comes from
a Reproducing Kernel Hilbert Space (RKHS). A special emphasis is put on
band-limited functions, central to signal processing, which form Paley-Wiener
type RKHSs. The proposed method, which we call Simultaneously Guaranteed Kernel
Interpolation (SGKI), is an extension and refinement of a recently developed
kernel method. An advantage of SGKI is that it not only estimates the missing
pixels, but also builds non-asymptotic confidence bands for the unobserved
values, which are simultaneously guaranteed for all missing pixels. We also
show how to compute these bands efficiently using Schur complements, we discuss
a generalization to vector-valued functions, and we present a series of
numerical experiments on various datasets containing synthetically generated
and benchmark images, as well.

</details>


### [281] [Sample Margin-Aware Recalibration of Temperature Scaling](https://arxiv.org/abs/2506.23492)
*Haolan Guo,Linwei Tao,Haoyang Luo,Minjing Dong,Chang Xu*

Main category: cs.LG

TL;DR: 论文提出了一种名为SMART的轻量级、数据高效的后处理校准方法，通过基于logit间隙的精确调整，解决了现有校准方法在高偏差或高方差之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络在预测准确性上有所提升，但普遍存在过度自信的问题，尤其在安全关键场景中风险较高。现有后处理校准方法面临全局调整偏差大或局部调整方差高的困境。

Method: SMART利用logit间隙（前两个logit的差值）作为去噪的标量信号，结合软分箱的期望校准误差（SoftECE）目标，实现高效且稳定的参数更新。

Result: 在多种数据集和架构上的实验表明，SMART在参数更少的情况下达到了最先进的校准性能。

Conclusion: SMART为神经网络预测中的不确定性量化提供了一种高效、鲁棒的解决方案。

Abstract: Recent advances in deep learning have significantly improved predictive
accuracy. However, modern neural networks remain systematically overconfident,
posing risks for deployment in safety-critical scenarios. Current post-hoc
calibration methods face a fundamental dilemma: global approaches like
Temperature Scaling apply uniform adjustments across all samples, introducing
high bias despite computational efficiency, while more expressive methods that
operate on full logit distributions suffer from high variance due to noisy
high-dimensional inputs and insufficient validation data. To address these
challenges, we propose Sample Margin-Aware Recalibration of Temperature
(SMART), a lightweight, data-efficient recalibration method that precisely
scales logits based on the margin between the top two logits -- termed the
logit gap. Specifically, the logit gap serves as a denoised, scalar signal
directly tied to decision boundary uncertainty, providing a robust indicator
that avoids the noise inherent in high-dimensional logit spaces while
preserving model prediction invariance. Meanwhile, SMART employs a novel
soft-binned Expected Calibration Error (SoftECE) objective that balances model
bias and variance through adaptive binning, enabling stable parameter updates
even with extremely limited calibration data. Extensive evaluations across
diverse datasets and architectures demonstrate that SMART achieves
state-of-the-art calibration performance even with substantially fewer
parameters compared to existing parametric methods, offering a principled,
robust, and highly efficient solution for practical uncertainty quantification
in neural network predictions. The source code is available at:
https://anonymous.4open.science/r/SMART-8B11.

</details>


### [282] [FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization](https://arxiv.org/abs/2506.23516)
*Seung-Wook Kim,Seongyeol Kim,Jiah Kim,Seowon Ji,Se-Ho Lee*

Main category: cs.LG

TL;DR: FedWSQ框架通过权重标准化和非均匀量化解决联邦学习中的数据异构性和通信限制问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习常因数据异构性和通信限制导致性能下降，需改进。

Method: 结合权重标准化（WS）和分布感知非均匀量化（DANUQ），WS过滤本地更新中的偏差，DANUQ减少量化误差。

Result: FedWSQ在多种挑战性场景下优于现有方法，减少通信开销并保持高精度。

Conclusion: FedWSQ有效提升联邦学习的鲁棒性和效率。

Abstract: Federated learning (FL) often suffers from performance degradation due to key
challenges such as data heterogeneity and communication constraints. To address
these limitations, we present a novel FL framework called FedWSQ, which
integrates weight standardization (WS) and the proposed distribution-aware
non-uniform quantization (DANUQ). WS enhances FL performance by filtering out
biased components in local updates during training, thereby improving the
robustness of the model against data heterogeneity and unstable client
participation. In addition, DANUQ minimizes quantization errors by leveraging
the statistical properties of local model updates. As a result, FedWSQ
significantly reduces communication overhead while maintaining superior model
accuracy. Extensive experiments on FL benchmark datasets demonstrate that
FedWSQ consistently outperforms existing FL methods across various challenging
FL settings, including extreme data heterogeneity and ultra-low-bit
communication scenarios.

</details>


### [283] [Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models](https://arxiv.org/abs/2506.23731)
*Michel Meintz,Jan Dubiński,Franziska Boenisch,Adam Dziedzic*

Main category: cs.LG

TL;DR: 论文分析了扩散模型和自回归图像模型中水印的放射性问题，并提出了一种针对自回归模型的新型水印方法。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法在扩散模型中无法保持放射性，而自回归模型缺乏相关水印技术，需要解决这一问题以防止生成图像的未经授权使用。

Method: 提出了一种针对自回归模型的水印方法，借鉴了大型语言模型的技术。

Result: 实验证明该方法在自回归模型中能有效保持水印的放射性，实现强大的来源追踪。

Conclusion: 该方法为自回归模型提供了有效的水印解决方案，有助于防止生成图像的滥用。

Abstract: Image generative models have become increasingly popular, but training them
requires large datasets that are costly to collect and curate. To circumvent
these costs, some parties may exploit existing models by using the generated
images as training data for their own models. In general, watermarking is a
valuable tool for detecting unauthorized use of generated images. However, when
these images are used to train a new model, watermarking can only enable
detection if the watermark persists through training and remains identifiable
in the outputs of the newly trained model - a property known as radioactivity.
We analyze the radioactivity of watermarks in images generated by diffusion
models (DMs) and image autoregressive models (IARs). We find that existing
watermarking methods for DMs fail to retain radioactivity, as watermarks are
either erased during encoding into the latent space or lost in the
noising-denoising process (during the training in the latent space). Meanwhile,
despite IARs having recently surpassed DMs in image generation quality and
efficiency, no radioactive watermarking methods have been proposed for them. To
overcome this limitation, we propose the first watermarking method tailored for
IARs and with radioactivity in mind - drawing inspiration from techniques in
large language models (LLMs), which share IARs' autoregressive paradigm. Our
extensive experimental evaluation highlights our method's effectiveness in
preserving radioactivity within IARs, enabling robust provenance tracking, and
preventing unauthorized use of their generated images.

</details>


### [284] [Supercm: Revisiting Clustering for Semi-Supervised Learning](https://arxiv.org/abs/2506.23824)
*Durgesh Singh,Ahcene Boubekki,Robert Jenssen,Michael C. Kampffmeyer*

Main category: cs.LG

TL;DR: 本文提出了一种新的半监督学习方法，通过显式结合聚类假设，利用可微分聚类模块，简化训练策略并提升性能。


<details>
  <summary>Details</summary>
Motivation: 近年来半监督学习主要关注一致性正则化或熵最小化方法，导致模型训练复杂。本文旨在通过显式结合聚类假设简化方法。

Method: 扩展可微分聚类模块，利用标注数据指导聚类中心，实现端到端训练。

Result: 模型性能优于仅监督基线，并可与其他半监督学习方法结合进一步提升性能。

Conclusion: 提出的方法简化了半监督学习训练，同时提升了性能，具有与其他方法结合的潜力。

Abstract: The development of semi-supervised learning (SSL) has in recent years largely
focused on the development of new consistency regularization or entropy
minimization approaches, often resulting in models with complex training
strategies to obtain the desired results. In this work, we instead propose a
novel approach that explicitly incorporates the underlying clustering
assumption in SSL through extending a recently proposed differentiable
clustering module. Leveraging annotated data to guide the cluster centroids
results in a simple end-to-end trainable deep SSL approach. We demonstrate that
the proposed model improves the performance over the supervised-only baseline
and show that our framework can be used in conjunction with other SSL methods
to further boost their performance.

</details>


### [285] [The Illusion of Progress? A Critical Look at Test-Time Adaptation for Vision-Language Models](https://arxiv.org/abs/2506.24000)
*Lijun Sheng,Jian Liang,Ran He,Zilei Wang,Tieniu Tan*

Main category: cs.LG

TL;DR: TTA-VLM是一个用于评估视觉语言模型（VLMs）测试时适应（TTA）方法的综合基准，解决了现有研究的局限性，并提供了更全面的评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前TTA研究存在结果重复、评估指标有限、实验设置不一致和不足分析等问题，阻碍了公平比较和方法改进。

Method: TTA-VLM实现了8种情景TTA和7种在线TTA方法，并在15个数据集上评估，扩展了模型和评估范围。

Result: 实验发现现有TTA方法提升有限，与训练时微调方法协作不佳，且准确性提升常以模型可信度降低为代价。

Conclusion: TTA-VLM为TTA方法提供了公平比较和全面评估，鼓励开发更可靠和通用的策略。

Abstract: Test-time adaptation (TTA) methods have gained significant attention for
enhancing the performance of vision-language models (VLMs) such as CLIP during
inference, without requiring additional labeled data. However, current TTA
researches generally suffer from major limitations such as duplication of
baseline results, limited evaluation metrics, inconsistent experimental
settings, and insufficient analysis. These problems hinder fair comparisons
between TTA methods and obscure their practical strengths and weaknesses. To
address these challenges, we introduce TTA-VLM, a comprehensive benchmark for
evaluating TTA methods on VLMs. Our benchmark implements 8 episodic TTA and 7
online TTA methods within a unified and reproducible framework, and evaluates
them across 15 widely used datasets. Unlike prior studies focused solely on
CLIP, we extend the evaluation to SigLIP--a model trained with a Sigmoid
loss--and include training-time tuning methods such as CoOp, MaPLe, and TeCoA
to assess generality. Beyond classification accuracy, TTA-VLM incorporates
various evaluation metrics, including robustness, calibration,
out-of-distribution detection, and stability, enabling a more holistic
assessment of TTA methods. Through extensive experiments, we find that 1)
existing TTA methods produce limited gains compared to the previous pioneering
work; 2) current TTA methods exhibit poor collaboration with training-time
fine-tuning methods; 3) accuracy gains frequently come at the cost of reduced
model trustworthiness. We release TTA-VLM to provide fair comparison and
comprehensive evaluation of TTA methods for VLMs, and we hope it encourages the
community to develop more reliable and generalizable TTA strategies.

</details>


### [286] [Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives](https://arxiv.org/abs/2506.24124)
*Dong Sixun,Fan Wei,Teresa Wu,Fu Yanjie*

Main category: cs.LG

TL;DR: 提出了一种多模态对比学习框架，将时间序列转化为视觉和文本视角，并通过对比学习对齐，提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统单模态数值输入难以捕捉高级语义模式，现有基于文本的方法受限于离散性，缺乏人类视觉直觉。

Method: 构建视觉和文本视角，通过对比学习对齐，并引入变量选择模块。

Result: 在多个基准测试中优于单模态和跨模态基线。

Conclusion: 多模态对齐有效提升了时间序列预测性能。

Abstract: Time series forecasting traditionally relies on unimodal numerical inputs,
which often struggle to capture high-level semantic patterns due to their dense
and unstructured nature. While recent approaches have explored representing
time series as text using large language models (LLMs), these methods remain
limited by the discrete nature of token sequences and lack the perceptual
intuition humans typically apply, such as interpreting visual patterns. In this
paper, we propose a multimodal contrastive learning framework that transforms
raw time series into structured visual and textual perspectives. Rather than
using natural language or real-world images, we construct both modalities
directly from numerical sequences. We then align these views in a shared
semantic space via contrastive learning, enabling the model to capture richer
and more complementary representations. Furthermore, we introduce a variate
selection module that leverages the aligned representations to identify the
most informative variables for multivariate forecasting. Extensive experiments
on fifteen short-term and six long-term forecasting benchmarks demonstrate that
our approach consistently outperforms strong unimodal and cross-modal
baselines, highlighting the effectiveness of multimodal alignment in enhancing
time series forecasting. Code is available at:
https://github.com/Ironieser/TimesCLIP.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [287] [General Autonomous Cybersecurity Defense: Learning Robust Policies for Dynamic Topologies and Diverse Attackers](https://arxiv.org/abs/2506.22706)
*Arun Ramamurthy,Neil Dhir*

Main category: cs.CR

TL;DR: 论文提出了一种通用自主网络安全防御（GACD）方法，旨在解决现有系统在动态网络环境中适应性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有自主网络安全防御（ACD）系统假设网络动态是静态的，导致在真实动态环境中适应性差，无法泛化到不同拓扑的网络。

Method: 探索了在动态网络环境中学习泛化策略的方法，提出通用自主网络安全防御（GACD）。

Result: 未明确提及具体实验结果，但目标是提升防御系统在动态网络中的泛化能力。

Conclusion: GACD方法有望解决现有ACD系统在动态环境中的局限性，提升网络安全防御的适应性。

Abstract: In the face of evolving cyber threats such as malware, ransomware and
phishing, autonomous cybersecurity defense (ACD) systems have become essential
for real-time threat detection and response with optional human intervention.
However, existing ACD systems rely on limiting assumptions, particularly the
stationarity of the underlying network dynamics. In real-world scenarios,
network topologies can change due to actions taken by attackers or defenders,
system failures, or time evolution of networks, leading to failures in the
adaptive capabilities of current defense agents. Moreover, many agents are
trained on static environments, resulting in overfitting to specific
topologies, which hampers their ability to generalize to out-of-distribution
network topologies. This work addresses these challenges by exploring methods
for developing agents to learn generalizable policies across dynamic network
environments -- general ACD (GACD).

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [288] [TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity](https://arxiv.org/abs/2506.23484)
*Yuzhuo Chen,Zehua Ma,Han Fang,Weiming Zhang,Nenghai Yu*

Main category: cs.MM

TL;DR: 论文提出了一种名为TAG-WM的防篡改生成图像水印方法，解决了AIGC内容版权和真实性风险，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: AI生成内容（AIGC）带来高效视觉创作，但也引发版权和真实性风险。数字图像水印作为完整性验证和来源追踪的常见技术，被视为潜在解决方案。然而，现有水印方法在篡改鲁棒性和主动定位能力上存在不足。

Method: 提出TAG-WM方法，包含四个模块：双标记联合采样（DMJS）、水印潜在重建（WLR）、密集变化区域检测器（DVRD）和防篡改解码（TAD）。

Result: 实验表明，TAG-WM在保持无损生成质量和256位容量的同时，实现了SOTA的篡改鲁棒性和定位能力。

Conclusion: TAG-WM为AIGC内容的版权保护和真实性验证提供了有效解决方案。

Abstract: AI-generated content (AIGC) enables efficient visual creation but raises
copyright and authenticity risks. As a common technique for integrity
verification and source tracing, digital image watermarking is regarded as a
potential solution to above issues. Among these, watermarking methods capable
of preserving the generation quality are receiving increased attention.
However, the proliferation and high performance of generative image editing
applications have elevated the risks of malicious tampering, creating new
demands. 1) The tamper robustness of current lossless visual quality watermarks
remains constrained by the modification-sensitive diffusion inversion process,
necessitating enhanced robustness. 2) The improved tampering quality and rapid
iteration cycles render passive tampering detection methods inadequate, making
proactive tampering localization capability a desired feature for watermarks.
To address these requirements, this paper proposes a Tamper-Aware Generative
image WaterMarking method named TAG-WM. The proposed method comprises four key
modules: a dual-mark joint sampling (DMJS) algorithm for embedding copyright
and localization watermarks into the latent space while preserving generative
quality, the watermark latent reconstruction (WLR) utilizing reversed DMJS, a
dense variation region detector (DVRD) leveraging diffusion inversion
sensitivity to identify tampered areas via statistical deviation analysis, and
the tamper-aware decoding (TAD) guided by localization results. The
experimental results indicate that TAG-WM achieves SOTA tampering robustness
and tampering localization capability with distortions while maintaining
lossless generation quality and a considerable capacity of 256 bits.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [289] [MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning](https://arxiv.org/abs/2506.22992)
*Yulun Jiang,Yekun Chai,Maria Brbić,Michael Moor*

Main category: cs.AI

TL;DR: MARBLE是一个多模态推理基准测试，旨在评估多模态语言模型（MLLMs）在复杂多模态问题中的逐步推理能力。现有模型在MARBLE上表现不佳，表明复杂推理仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 现有推理基准主要关注文本或简单多模态问题，复杂多模态推理能力尚未被充分研究。

Method: MARBLE包含两个高难度任务（M-Portal和M-Cube），要求模型在空间、视觉和物理约束下进行多步推理。

Result: 12个先进模型在M-Portal上表现接近随机，M-Cube上准确率为0%，仅在某些简化子任务中表现略优。

Conclusion: MARBLE揭示了MLLMs在复杂推理和感知上的局限性，有望推动下一代多模态推理模型的发展。

Abstract: The ability to process information from multiple modalities and to reason
through it step-by-step remains a critical challenge in advancing artificial
intelligence. However, existing reasoning benchmarks focus on text-only
reasoning, or employ multimodal questions that can be answered by directly
retrieving information from a non-text modality. Thus, complex reasoning
remains poorly understood in multimodal domains. Here, we present MARBLE, a
challenging multimodal reasoning benchmark that is designed to scrutinize
multimodal language models (MLLMs) in their ability to carefully reason
step-by-step through complex multimodal problems and environments. MARBLE is
composed of two highly challenging tasks, M-Portal and M-Cube, that require the
crafting and understanding of multistep plans under spatial, visual, and
physical constraints. We find that current MLLMs perform poorly on MARBLE --
all the 12 advanced models obtain near-random performance on M-Portal and 0%
accuracy on M-Cube. Only in simplified subtasks some models outperform the
random baseline, indicating that complex reasoning is still a challenge for
existing MLLMs. Moreover, we show that perception remains a bottleneck, where
MLLMs occasionally fail to extract information from the visual inputs. By
shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the
development of the next generation of models with the ability to reason and
plan across many, multimodal reasoning steps.

</details>


### [290] [MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI](https://arxiv.org/abs/2506.23563)
*Huanjin Yao,Jiaxing Huang,Yawen Qiu,Michael K. Chen,Wenzheng Liu,Wei Zhang,Wenjie Zeng,Xikun Zhang,Jingyi Zhang,Yuxin Song,Wenhao Wu,Dacheng Tao*

Main category: cs.AI

TL;DR: MMReason是一个新的基准测试，旨在精确评估多模态大语言模型的长链推理能力，通过多样、开放和具有挑战性的问题填补现有测试的不足。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM基准测试在评估长链推理能力时存在不足，如缺乏难度多样性、易受猜测和记忆影响，以及对中间推理步骤评估不足。

Method: 1. 从多个学科和难度级别收集需要多步推理的问题；2. 将问题转化为开放形式，并通过多模型投票技术过滤以减少猜测和记忆的影响；3. 标注详细的分步解决方案，并设计基于参考的三元评分机制。

Result: MMReason对主流MLLM进行了基准测试，并深入分析了它们的推理能力。

Conclusion: MMReason有望成为推动MLLM推理研究的重要资源。

Abstract: Reasoning plays a crucial role in advancing Multimodal Large Language Models
(MLLMs) toward Artificial General Intelligence. However, existing MLLM
benchmarks often fall short in precisely and comprehensively evaluating
long-chain reasoning abilities from three key aspects: (1) lack of difficulty
and diversity, (2) susceptibility to guessability and memorization, (3)
inadequate assessment of intermediate reasoning steps. To fill this gap, we
introduce MMReason, a new benchmark designed to precisely and comprehensively
evaluate MLLM long-chain reasoning capability with diverse, open-ended,
challenging questions. First, we curate challenging questions requiring
multi-step reasoning from various fields (i.e., 6 disciplines) and multiple
difficulty levels (i.e., from pre-university to university, and from
foundational to competition tiers). Second, these questions are reformulated
into an open-ended format and filtered using a multi-model voting technique to
eliminate shortcut cases related to guessing and memorization, ensuring robust
reasoning evaluations. Third, we annotate the questions with detailed
step-by-step solutions, and design a reference-based ternary scoring mechanism
to reliably assess intermediate reasoning steps. With MMReason, we benchmark
popular leading MLLMs and provide an in-depth analysis of their reasoning
capabilities. We hope MMReason will serve as a valuable resource for advancing
MLLM reasoning research. Code will be available at
https://github.com/HJYao00/MMReason.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [291] [KiseKloset: Comprehensive System For Outfit Retrieval, Recommendation, And Try-On](https://arxiv.org/abs/2506.23471)
*Thanh-Tung Phan-Nguyen,Khoi-Nguyen Nguyen-Ngoc,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.IR

TL;DR: 论文提出了一种名为KiseKloset的综合性系统，用于服装检索、推荐和虚拟试穿，通过两种检索方法和新型Transformer架构提升用户体验，并引入轻量级虚拟试穿框架。用户研究表明84%的参与者认为系统非常有用。


<details>
  <summary>Details</summary>
Motivation: 提升全球时尚电商行业的用户体验，通过个性化推荐和虚拟试穿技术增强客户参与度和满意度。

Method: 1. 两种服装检索方法：相似物品检索和文本反馈引导检索。2. 新型Transformer架构用于多品类互补物品推荐。3. 近似算法优化搜索流程。4. 轻量级虚拟试穿框架，支持实时操作和高效内存使用。

Result: 用户研究中，84%的参与者认为系统显著提升了在线购物体验。

Conclusion: KiseKloset系统通过综合技术方案有效改善了时尚电商的用户体验，具有实际应用价值。

Abstract: The global fashion e-commerce industry has become integral to people's daily
lives, leveraging technological advancements to offer personalized shopping
experiences, primarily through recommendation systems that enhance customer
engagement through personalized suggestions. To improve customers' experience
in online shopping, we propose a novel comprehensive KiseKloset system for
outfit retrieval, recommendation, and try-on. We explore two approaches for
outfit retrieval: similar item retrieval and text feedback-guided item
retrieval. Notably, we introduce a novel transformer architecture designed to
recommend complementary items from diverse categories. Furthermore, we enhance
the overall performance of the search pipeline by integrating approximate
algorithms to optimize the search process. Additionally, addressing the crucial
needs of online shoppers, we employ a lightweight yet efficient virtual try-on
framework capable of real-time operation, memory efficiency, and maintaining
realistic outputs compared to its predecessors. This virtual try-on module
empowers users to visualize specific garments on themselves, enhancing the
customers' experience and reducing costs associated with damaged items for
retailers. We deployed our end-to-end system for online users to test and
provide feedback, enabling us to measure their satisfaction levels. The results
of our user study revealed that 84% of participants found our comprehensive
system highly useful, significantly improving their online shopping experience.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [292] [Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation](https://arxiv.org/abs/2506.23717)
*Xingting Yao,Qinghao Hu,Fei Zhou,Tielong Liu,Gang Li,Peisong Wang,Jian Cheng*

Main category: cs.NE

TL;DR: 本文提出了一种自适应比特分配策略，用于直接训练的脉冲神经网络（SNN），通过分层分配资源提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 多比特SNN在追求高能效和高精度时，内存和计算需求激增，导致性能提升不成比例。不同层的重要性不同，多余的比特可能浪费或干扰。

Method: 参数化权重和脉冲的时间长度与比特宽度，使其可通过梯度学习和控制；提出改进的脉冲神经元以应对可变比特宽度和时间长度；提出步长更新机制解决比特宽度学习中的步长不匹配问题。

Result: 在多个数据集上实验表明，该方法能降低内存和计算成本，同时提高准确性。例如，SEWResNet-34在ImageNet上实现了2.69%的精度提升和4.16倍的比特预算降低。

Conclusion: 自适应比特分配策略有效提升了SNN的效率和准确性，为高能效AI提供了新思路。

Abstract: Multi-bit spiking neural networks (SNNs) have recently become a heated
research spot, pursuing energy-efficient and high-accurate AI. However, with
more bits involved, the associated memory and computation demands escalate to
the point where the performance improvements become disproportionate. Based on
the insight that different layers demonstrate different importance and extra
bits could be wasted and interfering, this paper presents an adaptive bit
allocation strategy for direct-trained SNNs, achieving fine-grained layer-wise
allocation of memory and computation resources. Thus, SNN's efficiency and
accuracy can be improved. Specifically, we parametrize the temporal lengths and
the bit widths of weights and spikes, and make them learnable and controllable
through gradients. To address the challenges caused by changeable bit widths
and temporal lengths, we propose the refined spiking neuron, which can handle
different temporal lengths, enable the derivation of gradients for temporal
lengths, and suit spike quantization better. In addition, we theoretically
formulate the step-size mismatch problem of learnable bit widths, which may
incur severe quantization errors to SNN, and accordingly propose the step-size
renewal mechanism to alleviate this issue. Experiments on various datasets,
including the static CIFAR and ImageNet and the dynamic CIFAR-DVS and
DVS-GESTURE, demonstrate that our methods can reduce the overall memory and
computation cost while achieving higher accuracy. Particularly, our
SEWResNet-34 can achieve a 2.69\% accuracy gain and 4.16$\times$ lower bit
budgets over the advanced baseline work on ImageNet. This work will be fully
open-sourced.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [293] [Supra-threshold control of peripheral LOD](https://arxiv.org/abs/2506.22583)
*Benjamin Watson,Neff Walker,Larry F Hodges*

Main category: cs.HC

TL;DR: 论文探讨了超阈值LOD控制在视觉外围的应用，发现其应与基于阈值的LOD控制显著不同，并提出任务依赖的可靠感知性作为关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有LOD控制多基于阈值感知，而实际应用中LOD操作多发生在超阈值范围，且超阈值感知与阈值感知差异显著，因此需研究超阈值LOD控制是否也应不同。

Method: 通过两个实验研究视觉外围的超阈值LOD控制，分析感知性与任务需求的关系。

Result: 发现LOD需支持任务依赖的可靠感知性，超阈值时应最小化感知性，细节对比度比尺寸更能预测感知性；低于阈值时需最大化感知性，并随偏心距增加或对比度降低提升LOD。

Conclusion: 超阈值LOD控制应与基于阈值的方案显著不同，建议重新审视LOD控制方法，尤其是中心凹显示。

Abstract: Level of detail (LOD) is widely used to control visual feedback in
interactive applications. LOD control is typically based on perception at
threshold - the conditions in which a stimulus first becomes perceivable. Yet
most LOD manipulations are quite perceivable and occur well above threshold.
Moreover, research shows that supra-threshold perception differs drastically
from perception at threshold. In that case, should supra-threshold LOD control
also differ from LOD control at threshold?
  In two experiments, we examine supra-threshold LOD control in the visual
periphery and find that indeed, it should differ drastically from LOD control
at threshold. Specifically, we find that LOD must support a task-dependent
level of reliable perceptibility. Above that level, perceptibility of LOD
control manipulations should be minimized, and detail contrast is a better
predictor of perceptibility than detail size. Below that level, perceptibility
must be maximized, and LOD should be improved as eccentricity rises or contrast
drops. This directly contradicts prevailing threshold-based LOD control
schemes, and strongly suggests a reexamination of LOD control for foveal
display.

</details>


### [294] [Coordinated 2D-3D Visualization of Volumetric Medical Data in XR with Multimodal Interactions](https://arxiv.org/abs/2506.22926)
*Qixuan Liu,Shi Qiu,Yinqiao Wang,Xiwen Wu,Kenneth Siu Ho Chok,Chi-Wing Fu,Pheng-Ann Heng*

Main category: cs.HC

TL;DR: 论文提出了一种基于XR的医疗数据可视化系统，结合多层多平面重建与3D网格模型，并通过手势和LLM驱动的语音命令实现交互，初步评估显示其能提升空间理解并降低认知负荷。


<details>
  <summary>Details</summary>
Motivation: 医疗影像数据的3D可视化对非专业人士具有挑战性，需要一种更直观的交互方式。

Method: 开发了XR系统，整合多层多平面重建与3D网格模型，并采用手势和LLM语音命令的多模态交互框架。

Result: 用户研究和专家访谈显示，系统显著提升了任务完成时间、可用性和交互效果。

Conclusion: 该系统在医疗培训和临床实践中有潜力，未来需进一步优化。

Abstract: Volumetric medical imaging technologies produce detailed 3D representations
of anatomical structures. However, effective medical data visualization and
exploration pose significant challenges, especially for individuals with
limited medical expertise. We introduce a novel XR-based system with two key
innovations: (1) a coordinated visualization module integrating Multi-layered
Multi-planar Reconstruction with 3D mesh models and (2) a multimodal
interaction framework combining hand gestures with LLM-enabled voice commands.
We conduct preliminary evaluations, including a 15-participant user study and
expert interviews, to demonstrate the system's abilities to enhance spatial
understanding and reduce cognitive load. Experimental results show notable
improvements in task completion times, usability metrics, and interaction
effectiveness enhanced by LLM-driven voice control. While identifying areas for
future refinement, our findings highlight the potential of this immersive
visualization system to advance medical training and clinical practice. Our
demo application and supplemental materials are available for download at:
https://osf.io/bpjq5/.

</details>


### [295] [Deep Learning in Mild Cognitive Impairment Diagnosis using Eye Movements and Image Content in Visual Memory Tasks](https://arxiv.org/abs/2506.23016)
*Tomás Silva Santos Rocha,Anastasiia Mikhailova,Moreno I. Coco,José Santos-Victor*

Main category: cs.HC

TL;DR: 研究利用眼动追踪数据和深度学习模型VTNet区分健康人群与轻度认知障碍（MCI），模型性能与类似研究相当，为MCI自动化诊断工具开发提供支持。


<details>
  <summary>Details</summary>
Motivation: 全球痴呆症患病率预计到2050年翻倍，亟需可扩展的诊断工具。

Method: 使用44名参与者（24名MCI，20名健康对照）的眼动追踪数据训练VTNet模型，结合时间序列和空间数据，并整合扫描路径、热图和图像内容。

Result: 最佳模型在700×700像素热图分辨率下达到68%敏感性和76%特异性，性能与类似研究相当。

Conclusion: 研究支持MCI自动化诊断工具开发，未来需优化模型并采用标准化长期视觉记忆任务。

Abstract: The global prevalence of dementia is projected to double by 2050,
highlighting the urgent need for scalable diagnostic tools. This study utilizes
digital cognitive tasks with eye-tracking data correlated with memory processes
to distinguish between Healthy Controls (HC) and Mild Cognitive Impairment
(MCI), a precursor to dementia. A deep learning model based on VTNet was
trained using eye-tracking data from 44 participants (24 MCI, 20 HCs) who
performed a visual memory task. The model utilizes both time series and spatial
data derived from eye-tracking. It was modified to incorporate scan paths, heat
maps, and image content. These modifications also enabled testing parameters
such as image resolution and task performance, analyzing their impact on model
performance. The best model, utilizing $700\times700px$ resolution heatmaps,
achieved 68% sensitivity and 76% specificity. Despite operating under more
challenging conditions (e.g., smaller dataset size, shorter task duration, or a
less standardized task), the model's performance is comparable to an
Alzheimer's study using similar methods (70% sensitivity and 73% specificity).
These findings contribute to the development of automated diagnostic tools for
MCI. Future work should focus on refining the model and using a standardized
long-term visual memory task.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [296] [Maximum Dispersion, Maximum Concentration: Enhancing the Quality of MOP Solutions](https://arxiv.org/abs/2506.22568)
*Gladston Moreira,Ivan Meneghini,Elzabeth Wanner*

Main category: math.OC

TL;DR: 该研究提出了一种多目标优化方法，通过结合目标空间的收敛性和决策空间的分散性，提高解的质量，避免决策空间中的偏差。


<details>
  <summary>Details</summary>
Motivation: 多目标优化问题（MOPs）需要在冲突目标之间权衡，同时最大化目标空间的多样性和收敛性。传统方法可能因解在决策空间中的聚集而产生偏差，因此需要一种新方法来平衡分散性和收敛性。

Method: 研究定义了一个基于锥形的兴趣区域（ROI），反映决策者在目标空间的偏好，并使用均匀性度量增强决策空间中解的分散性。通过结合目标空间的收敛性和决策空间的分散性，优化Pareto最优解的搜索。

Result: 初步实验表明，该方法通过生成平衡分散性和收敛性的解，有效提高了多目标优化的质量，并减少了决策空间中的偏差。

Conclusion: 该方法通过同时优化目标空间的收敛性和决策空间的分散性，显著提升了多目标优化解的质量，避免了传统方法中的偏差问题。

Abstract: Multi-objective optimization problems (MOPs) often require a trade-off
between conflicting objectives, maximizing diversity and convergence in the
objective space. This study presents an approach to improve the quality of MOP
solutions by optimizing the dispersion in the decision space and the
convergence in a specific region of the objective space. Our approach defines a
Region of Interest (ROI) based on a cone representing the decision maker's
preferences in the objective space, while enhancing the dispersion of solutions
in the decision space using a uniformity measure. Combining solution
concentration in the objective space with dispersion in the decision space
intensifies the search for Pareto-optimal solutions while increasing solution
diversity. When combined, these characteristics improve the quality of
solutions and avoid the bias caused by clustering solutions in a specific
region of the decision space. Preliminary experiments suggest that this method
enhances multi-objective optimization by generating solutions that effectively
balance dispersion and concentration, thereby mitigating bias in the decision
space.

</details>


### [297] [Denoising Multi-Color QR Codes and Stiefel-Valued Data by Relaxed Regularizations](https://arxiv.org/abs/2506.22826)
*Robert Beinert,Jonas Bresch*

Main category: math.OC

TL;DR: 论文提出了一种新的高效去噪方法，适用于多二进制和Stiefel值数据，通过欧几里得嵌入和凸化技术实现。


<details>
  <summary>Details</summary>
Motivation: 处理流形值数据（如颜色恢复、旋转信息、高斯图像处理）时，现有方法需要扩展以支持多二进制和Stiefel值数据。

Method: 将数据嵌入欧几里得空间，通过半正定矩阵编码流形，并松弛秩约束以实现凸化，适用于TV和Tikhonov去噪模型。

Result: 在合成实验中验证了所提方法的有效性。

Conclusion: 该方法为多二进制和Stiefel值数据提供了一种高效的去噪解决方案。

Abstract: The handling of manifold-valued data, for instance, plays a central role in
color restoration tasks relying on circle- or sphere-valued color models, in
the study of rotational or directional information related to the special
orthogonal group, and in Gaussian image processing, where the pixel statistics
are interpreted as values on the hyperbolic sheet. Especially, to denoise these
kind of data, there have been proposed several generalizations of total
variation (TV) and Tikhonov-type denoising models incorporating the underlying
manifolds. Recently, a novel, numerically efficient denoising approach has been
introduced, where the data are embedded in an Euclidean ambient space, the
non-convex manifolds are encoded by a series of positive semi-definite,
fixed-rank matrices, and the rank constraint is relaxed to obtain a
convexification that can be solved using standard algorithms from convex
analysis. The aim of the present paper is to extent this approach to new kinds
of data like multi-binary and Stiefel-valued data. Multi-binary data can, for
instance, be used to model multi-color QR codes whereas Stiefel-valued data
occur in image and video-based recognition. For both new data types, we propose
TV- and Tikhonov-based denoising modelstogether with easy-to-solve
convexification. All derived methods are evaluated on proof-of-concept,
synthetic experiments.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [298] [DriveBLIP2: Attention-Guided Explanation Generation for Complex Driving Scenarios](https://arxiv.org/abs/2506.22494)
*Shihong Ling,Yue Wan,Xiaowei Jia,Na Du*

Main category: cs.RO

TL;DR: DriveBLIP2框架基于BLIP2-OPT架构，通过注意力图生成器提升自动驾驶场景中的解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在复杂多目标环境（如自动驾驶）中表现不佳，需要改进实时关键对象识别。

Method: 提出注意力图生成器，聚焦关键视频帧中的重要对象，生成清晰解释。

Result: 在DRAMA数据集上，BLEU、ROUGE、CIDEr和SPICE分数显著提升。

Conclusion: 目标注意力机制可增强自动驾驶中视觉语言模型的可解释性。

Abstract: This paper introduces a new framework, DriveBLIP2, built upon the BLIP2-OPT
architecture, to generate accurate and contextually relevant explanations for
emerging driving scenarios. While existing vision-language models perform well
in general tasks, they encounter difficulties in understanding complex,
multi-object environments, particularly in real-time applications such as
autonomous driving, where the rapid identification of key objects is crucial.
To address this limitation, an Attention Map Generator is proposed to highlight
significant objects relevant to driving decisions within critical video frames.
By directing the model's focus to these key regions, the generated attention
map helps produce clear and relevant explanations, enabling drivers to better
understand the vehicle's decision-making process in critical situations.
Evaluations on the DRAMA dataset reveal significant improvements in explanation
quality, as indicated by higher BLEU, ROUGE, CIDEr, and SPICE scores compared
to baseline models. These findings underscore the potential of targeted
attention mechanisms in vision-language models for enhancing explainability in
real-time autonomous driving.

</details>


### [299] [Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding](https://arxiv.org/abs/2506.22593)
*Antonello Longo,Chanyoung Chung,Matteo Palieri,Sung-Kyun Kim,Ali Agha,Cataldo Guaragnella,Shehryar Khattak*

Main category: cs.RO

TL;DR: Pix2G方法通过轻量级实时处理图像和LiDAR数据，生成场景图，支持机器人在资源受限平台上进行自主探索。


<details>
  <summary>Details</summary>
Motivation: 解决人类操作员与机器人之间的高效协作问题，通过3D场景图桥接人类可读的2D BIM与机器人3D地图。

Method: Pix2G方法从图像像素和LiDAR地图实时生成结构化场景图，仅使用CPU满足资源限制。

Result: 生成去噪的2D环境地图和结构分割的3D点云，通过多层图连接，成功在NASA JPL NeBula-Spot机器人上实时测试。

Conclusion: Pix2G方法在资源受限平台上实现了高效的实时环境探索与地图生成。

Abstract: Autonomous robots are increasingly playing key roles as support platforms for
human operators in high-risk, dangerous applications. To accomplish challenging
tasks, an efficient human-robot cooperation and understanding is required.
While typically robotic planning leverages 3D geometric information, human
operators are accustomed to a high-level compact representation of the
environment, like top-down 2D maps representing the Building Information Model
(BIM). 3D scene graphs have emerged as a powerful tool to bridge the gap
between human readable 2D BIM and the robot 3D maps. In this work, we introduce
Pixels-to-Graph (Pix2G), a novel lightweight method to generate structured
scene graphs from image pixels and LiDAR maps in real-time for the autonomous
exploration of unknown environments on resource-constrained robot platforms. To
satisfy onboard compute constraints, the framework is designed to perform all
operation on CPU only. The method output are a de-noised 2D top-down
environment map and a structure-segmented 3D pointcloud which are seamlessly
connected using a multi-layer graph abstracting information from object-level
up to the building-level. The proposed method is quantitatively and
qualitatively evaluated during real-world experiments performed using the NASA
JPL NeBula-Spot legged robot to autonomously explore and map cluttered garage
and urban office like environments in real-time.

</details>


### [300] [InfGen: Scenario Generation as Next Token Group Prediction](https://arxiv.org/abs/2506.23316)
*Zhenghao Peng,Yuxin Liu,Bolei Zhou*

Main category: cs.RO

TL;DR: InfGen是一个基于Transformer的交通场景生成框架，支持动态、长期场景模拟，并能持续插入新车辆，生成多样且真实的交通行为。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的交通模拟方法依赖静态初始化或日志回放数据，难以模拟动态、长期场景和变化的车辆群体。

Method: InfGen将整个场景表示为包含交通信号、车辆状态和运动向量的令牌序列，通过Transformer模型自回归生成交通轨迹。

Result: 实验表明InfGen能生成真实、多样且自适应的交通行为，且在其生成的场景中训练的强化学习策略具有更强的鲁棒性和泛化能力。

Conclusion: InfGen是一个高保真度的自动驾驶模拟环境，适用于训练和评估自动驾驶系统。

Abstract: Realistic and interactive traffic simulation is essential for training and
evaluating autonomous driving systems. However, most existing data-driven
simulation methods rely on static initialization or log-replay data, limiting
their ability to model dynamic, long-horizon scenarios with evolving agent
populations. We propose InfGen, a scenario generation framework that outputs
agent states and trajectories in an autoregressive manner. InfGen represents
the entire scene as a sequence of tokens, including traffic light signals,
agent states, and motion vectors, and uses a transformer model to simulate
traffic over time. This design enables InfGen to continuously insert new agents
into traffic, supporting infinite scene generation. Experiments demonstrate
that InfGen produces realistic, diverse, and adaptive traffic behaviors.
Furthermore, reinforcement learning policies trained in InfGen-generated
scenarios achieve superior robustness and generalization, validating its
utility as a high-fidelity simulation environment for autonomous driving. More
information is available at https://metadriverse.github.io/infgen/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [301] [SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions](https://arxiv.org/abs/2506.23046)
*Xianzhe Fan,Xuhui Zhou,Chuanyang Jin,Kolby Nottingham,Hao Zhu,Maarten Sap*

Main category: cs.CL

TL;DR: SoMi-ToM 是一个用于评估多智能体复杂社交互动中多视角心智理论（ToM）能力的基准，基于丰富的多模态交互数据，支持第一人称和第三人称视角的评估。实验表明，当前大型视觉语言模型（LVLMs）的表现远低于人类。


<details>
  <summary>Details</summary>
Motivation: 现有 ToM 基准多为静态文本场景，与真实动态社交互动存在显著差距。SoMi-ToM 旨在填补这一空白，提供更贴近现实的评估方法。

Method: 基于 SoMi 交互环境生成多模态数据，设计第一人称（实时状态推断）和第三人称（目标与行为推断）评估框架，并构建包含视频、图像和专家标注问题的数据集。

Result: 人类在 SoMi-ToM 上的表现显著优于 LVLMs，平均准确率差距为 40.1%（第一人称）和 26.4%（第三人称）。

Conclusion: 未来 LVLMs 需提升在复杂社交互动中的 ToM 能力，SoMi-ToM 为此提供了有效的评估工具。

Abstract: Humans continuously infer the states, goals, and behaviors of others by
perceiving their surroundings in dynamic, real-world social interactions.
However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based
scenarios, which have a significant gap compared to real interactions. We
propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in
embodied multi-agent complex social interactions. This benchmark is based on
rich multimodal interaction data generated by the interaction environment SoMi,
covering diverse crafting goals and social relationships. Our framework
supports multi-level evaluation: (1) first-person evaluation provides
multimodal (visual, dialogue, action, etc.) input from a first-person
perspective during a task for real-time state inference, (2) third-person
evaluation provides complete third-person perspective video and text records
after a task for goal and behavior inference. This evaluation method allows for
a more comprehensive examination of a model's ToM capabilities from both the
subjective immediate experience and the objective global observation. We
constructed a challenging dataset containing 35 third-person perspective
videos, 363 first-person perspective images, and 1225 expert-annotated
multiple-choice questions (three options). On this dataset, we systematically
evaluated the performance of human subjects and several state-of-the-art large
vision-language models (LVLMs). The results show that LVLMs perform
significantly worse than humans on SoMi-ToM: the average accuracy gap between
humans and models is 40.1% in first-person evaluation and 26.4% in third-person
evaluation. This indicates that future LVLMs need to further improve their ToM
capabilities in embodied, complex social interactions.

</details>


### [302] [EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations](https://arxiv.org/abs/2506.24016)
*Hyunjong Kim,Sangyeop Kim,Jongheon Jeong,Yeongjae Cho,Sungzoon Cho*

Main category: cs.CL

TL;DR: EXPERT是一种新的图像字幕评价指标，基于流畅性、相关性和描述性三个标准生成结构化解释，并通过大规模数据集和监督模型实现高质量解释。


<details>
  <summary>Details</summary>
Motivation: 现有图像字幕评价指标的解释缺乏标准化和验证，EXPERT旨在解决这一问题。

Method: 提出EXPERT指标，构建大规模结构化解释数据集，采用两阶段评估模板监督视觉语言模型。

Result: 在基准数据集上达到最优性能，生成解释的质量显著优于现有指标。

Conclusion: EXPERT为图像字幕评价提供了高质量的结构化解释，代码和数据集已开源。

Abstract: Recent advances in large language models and vision-language models have led
to growing interest in explainable evaluation metrics for image captioning.
However, these metrics generate explanations without standardized criteria, and
the overall quality of the generated explanations remains unverified. In this
paper, we propose EXPERT, a reference-free evaluation metric that provides
structured explanations based on three fundamental criteria: fluency,
relevance, and descriptiveness. By constructing large-scale datasets of
high-quality structured explanations, we develop a two-stage evaluation
template to effectively supervise a vision-language model for both scoring and
explanation generation. EXPERT achieves state-of-the-art results on benchmark
datasets while providing significantly higher-quality explanations than
existing metrics, as validated through comprehensive human evaluation. Our code
and datasets are available at https://github.com/hjkim811/EXPERT.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [303] [Supervised Diffusion-Model-Based PET Image Reconstruction](https://arxiv.org/abs/2506.24034)
*George Webber,Alexander Hammers,Andrew P King,Andrew J Reader*

Main category: physics.med-ph

TL;DR: 提出了一种基于监督扩散模型（DM）的PET图像重建方法，通过结合PET的泊松似然模型和强度范围，显著提升了重建精度和不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 现有基于DM的无监督方法未能显式建模DM先验与噪声测量数据的交互，限制了重建精度。

Method: 提出监督DM算法，强制PET泊松似然模型的非负性，并适应PET图像的宽强度范围。

Result: 在真实脑PET模型上，该方法在多个剂量水平上优于或匹配现有深度学习方法，且后验采样更准确。

Conclusion: 监督DM方法显著提升了PET重建性能，并扩展至全3D PET，展示了实际应用潜力。

Abstract: Diffusion models (DMs) have recently been introduced as a regularizing prior
for PET image reconstruction, integrating DMs trained on high-quality PET
images with unsupervised schemes that condition on measured data. While these
approaches have potential generalization advantages due to their independence
from the scanner geometry and the injected activity level, they forgo the
opportunity to explicitly model the interaction between the DM prior and noisy
measurement data, potentially limiting reconstruction accuracy. To address
this, we propose a supervised DM-based algorithm for PET reconstruction. Our
method enforces the non-negativity of PET's Poisson likelihood model and
accommodates the wide intensity range of PET images. Through experiments on
realistic brain PET phantoms, we demonstrate that our approach outperforms or
matches state-of-the-art deep learning-based methods quantitatively across a
range of dose levels. We further conduct ablation studies to demonstrate the
benefits of the proposed components in our model, as well as its dependence on
training data, parameter count, and number of diffusion steps. Additionally, we
show that our approach enables more accurate posterior sampling than
unsupervised DM-based methods, suggesting improved uncertainty estimation.
Finally, we extend our methodology to a practical approach for fully 3D PET and
present example results from real [$^{18}$F]FDG brain PET data.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [304] [WISVA: Generative AI for 5G Network Optimization in Smart Warehouses](https://arxiv.org/abs/2506.22456)
*Rahul Gulia,Amlan Ganguly,Andres Kwasinski,Michael E. Kuhl,Ehsan Rashedi,Clark Hochgraf*

Main category: eess.SP

TL;DR: 论文提出了一种基于变分自编码器（VAE）的框架WISVA，用于5G频段下智能仓库的室内无线电传播建模，展示了其在预测SINR热图和处理复杂环境中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 随着数据密集型应用需求的增长和新兴技术的快速采用，无线通信需要更先进的信号处理技术和网络架构，以支持工业数字化转型。

Method: 论文设计了WISVA框架，通过训练数据张量捕捉复杂电磁波行为，并详细描述了VAE模型的架构和训练方法。

Result: WISVA在预测SINR热图、去噪任务、验证数据集和未知仓库布局中表现出色，其重建误差热图显示其优于传统自编码器模型。

Conclusion: WISVA在优化工业4.0无线基础设施方面具有潜力，能够高效处理复杂智能仓库环境。

Abstract: The next decade will usher in a profound transformation of wireless
communication, driven by the ever-increasing demand for data-intensive
applications and the rapid adoption of emerging technologies. To fully unlock
the potential of 5G and beyond, substantial advancements are required in signal
processing techniques, innovative network architectures, and efficient spectrum
utilization strategies. These advancements facilitate seamless integration of
emerging technologies, driving industrial digital transformation and
connectivity. This paper introduces a novel Variational Autoencoder (VAE)-based
framework, Wireless Infrastructure for Smart Warehouses using VAE (WISVA),
designed for accurate indoor radio propagation modeling in automated Industry
4.0 environments such as warehouses and factory floors operating within 5G
wireless bands. The research delves into the meticulous creation of training
data tensors, capturing complex electromagnetic (EM) wave behaviors influenced
by diverse obstacles, and outlines the architecture and training methodology of
the proposed VAE model. The model's robustness and adaptability are showcased
through its ability to predict signal-to-interference-plus-noise ratio (SINR)
heatmaps across various scenarios, including denoising tasks, validation
datasets, extrapolation to unseen configurations, and previously unencountered
warehouse layouts. Compelling reconstruction error heatmaps are presented,
highlighting the superior accuracy of WISVA compared to traditional autoencoder
models. The paper also analyzes the model's performance in handling complex
smart warehouse environments, demonstrating its potential as a key enabler for
optimizing wireless infrastructure in Industry 4.0.

</details>


### [305] [SegmentAnyMuscle: A universal muscle segmentation model across different locations in MRI](https://arxiv.org/abs/2506.22467)
*Roy Colglazier,Jisoo Lee,Haoyu Dong,Hanxue Gu,Yaqian Chen,Joseph Cao,Zafer Yildiz,Zhonghao Liu,Nicholas Konz,Jichen Yang,Jikai Zhang,Yuwen Chen,Lin Li,Adrian Camarena,Maciej A. Mazurowski*

Main category: eess.SP

TL;DR: 开发了一个公开可用的深度学习模型，用于MRI中的肌肉分割，并在不同解剖位置和成像序列中验证了其适用性。


<details>
  <summary>Details</summary>
Motivation: 肌肉的数量和质量对健康结果有重要预测作用，但MRI中精确量化肌肉仍具挑战性。

Method: 使用362例MRI数据（160名患者）开发模型，并在不同序列和异常情况下测试其性能。

Result: 在常见序列和异常情况下，模型的Dice相似系数分别为88.45%和86.21%。

Conclusion: 该模型实现了全自动肌肉分割，为肌肉与健康关系的研究提供了可重复的工具。

Abstract: The quantity and quality of muscles are increasingly recognized as important
predictors of health outcomes. While MRI offers a valuable modality for such
assessments, obtaining precise quantitative measurements of musculature remains
challenging. This study aimed to develop a publicly available model for muscle
segmentation in MRIs and demonstrate its applicability across various
anatomical locations and imaging sequences. A total of 362 MRIs from 160
patients at a single tertiary center (Duke University Health System, 2016-2020)
were included, with 316 MRIs from 114 patients used for model development. The
model was tested on two separate sets: one with 28 MRIs representing common
sequence types, achieving an average Dice Similarity Coefficient (DSC) of
88.45%, and another with 18 MRIs featuring less frequent sequences and
abnormalities such as muscular atrophy, hardware, and significant noise,
achieving 86.21% DSC. These results demonstrate the feasibility of a fully
automated deep learning algorithm for segmenting muscles on MRI across diverse
settings. The public release of this model enables consistent, reproducible
research into the relationship between musculature and health.

</details>
