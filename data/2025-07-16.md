<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [eess.IV](#eess.IV) [Total: 10]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.RO](#cs.RO) [Total: 5]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 7]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [CWNet: Causal Wavelet Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.10689)
*Tongshun Zhang,Pingping Liu,Yubing Lu,Mengen Cai,Zijian Zhang,Zhe Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: CWNet是一种基于小波变换和因果推理的低光图像增强方法，通过全局和局部因果分析优化图像增强效果。


<details>
  <summary>Details</summary>
Motivation: 传统低光图像增强方法忽视实例级语义信息和特征特性，CWNet旨在解决这一问题。

Method: 结合因果推理和小波变换，采用全局度量学习和局部CLIP语义损失，设计小波变换骨干网络。

Result: CWNet在多个数据集上显著优于现有方法。

Conclusion: CWNet通过因果推理和小波变换实现了更精确的低光图像增强。

Abstract: Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on
uniform brightness adjustment, often neglecting instance-level semantic
information and the inherent characteristics of different features. To address
these limitations, we propose CWNet (Causal Wavelet Network), a novel
architecture that leverages wavelet transforms for causal reasoning.
Specifically, our approach comprises two key components: 1) Inspired by the
concept of intervention in causality, we adopt a causal reasoning perspective
to reveal the underlying causal relationships in low-light enhancement. From a
global perspective, we employ a metric learning strategy to ensure causal
embeddings adhere to causal principles, separating them from non-causal
confounding factors while focusing on the invariance of causal factors. At the
local level, we introduce an instance-level CLIP semantic loss to precisely
maintain causal factor consistency. 2) Based on our causal analysis, we present
a wavelet transform-based backbone network that effectively optimizes the
recovery of frequency information, ensuring precise enhancement tailored to the
specific attributes of wavelet transforms. Extensive experiments demonstrate
that CWNet significantly outperforms current state-of-the-art methods across
multiple datasets, showcasing its robust performance across diverse scenes.
Code is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.

</details>


### [2] [Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines](https://arxiv.org/abs/2507.10737)
*Jiayuan Chen,Thai-Hoang Pham,Yuanlong Wang,Ping Zhang*

Main category: cs.CV

TL;DR: 提出了一种整合外部生物知识的新框架，通过解耦扰动特异性和细胞系特异性表征，提升显微镜图像分析模型对新细胞系的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决新细胞系在扰动筛选中因形态和生物异质性带来的挑战。

Method: 结合蛋白质相互作用知识图谱和单细胞转录组特征，解耦扰动和细胞系特异性表征。

Result: 在RxRx数据库上验证，模型对新细胞系的图像分析能力显著提升。

Conclusion: 该方法在表型药物发现中具有实际应用价值。

Abstract: High-throughput screening techniques, such as microscopy imaging of cellular
responses to genetic and chemical perturbations, play a crucial role in drug
discovery and biomedical research. However, robust perturbation screening for
\textit{de novo} cell lines remains challenging due to the significant
morphological and biological heterogeneity across cell lines. To address this,
we propose a novel framework that integrates external biological knowledge into
existing pretraining strategies to enhance microscopy image profiling models.
Our approach explicitly disentangles perturbation-specific and cell
line-specific representations using external biological information.
Specifically, we construct a knowledge graph leveraging protein interaction
data from STRING and Hetionet databases to guide models toward
perturbation-specific features during pretraining. Additionally, we incorporate
transcriptomic features from single-cell foundation models to capture cell
line-specific representations. By learning these disentangled features, our
method improves the generalization of imaging models to \textit{de novo} cell
lines. We evaluate our framework on the RxRx database through one-shot
fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from
the RxRx19a dataset. Experimental results demonstrate that our method enhances
microscopy image profiling for \textit{de novo} cell lines, highlighting its
effectiveness in real-world phenotype-based drug discovery applications.

</details>


### [3] [Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias](https://arxiv.org/abs/2507.10755)
*Rina Khan,Catherine Stinson*

Main category: cs.CV

TL;DR: 该论文分析了面部表情识别（FER）算法在检测自发表情和不同种族肤色时的性能下降问题，并提出了一种识别自发或摆拍图像的方法。研究发现数据集中的摆拍图像较多，且模型对非白人或深肤色人群存在偏见。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决FER算法在检测自发表情和不同种族肤色时的性能下降问题，以及数据集收集方式对模型性能的影响。

Method: 方法包括对两个先进的FER数据集进行审计，随机抽样检查图像是自发还是摆拍，并测试三个模型在不同种族和肤色上的表现。

Result: 发现数据集中存在大量摆拍图像，且模型对非白人或深肤色人群更容易预测负面情绪，存在偏见。

Conclusion: 结论是这些偏见可能导致模型在真实应用中造成伤害，需改进数据集收集和模型训练方法。

Abstract: Facial expression recognition (FER) algorithms classify facial expressions
into emotions such as happy, sad, or angry. An evaluative challenge facing FER
algorithms is the fall in performance when detecting spontaneous expressions
compared to posed expressions. An ethical (and evaluative) challenge facing FER
algorithms is that they tend to perform poorly for people of some races and
skin colors. These challenges are linked to the data collection practices
employed in the creation of FER datasets. In this study, we audit two
state-of-the-art FER datasets. We take random samples from each dataset and
examine whether images are spontaneous or posed. In doing so, we propose a
methodology for identifying spontaneous or posed images. We discover a
significant number of images that were posed in the datasets purporting to
consist of in-the-wild images. Since performance of FER models vary between
spontaneous and posed images, the performance of models trained on these
datasets will not represent the true performance if such models were to be
deployed in in-the-wild applications. We also observe the skin color of
individuals in the samples, and test three models trained on each of the
datasets to predict facial expressions of people from various races and skin
tones. We find that the FER models audited were more likely to predict people
labeled as not white or determined to have dark skin as showing a negative
emotion such as anger or sadness even when they were smiling. This bias makes
such models prone to perpetuate harm in real life applications.

</details>


### [4] [FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching](https://arxiv.org/abs/2507.10770)
*Ionuţ Grigore,Călin-Adrian Popa,Claudiu Leoveanu-Condrei*

Main category: cs.CV

TL;DR: 提出一种无需描述符的兴趣点匹配方法，显著降低内存使用。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖描述符进行兴趣点匹配，计算和存储开销大。

Method: 在检测阶段直接关联兴趣点，省去描述符的计算和匹配。

Result: 匹配精度略低于传统方法，但内存使用大幅减少。

Conclusion: 该方法为几何计算机视觉任务提供了一种高效替代方案。

Abstract: The extraction and matching of interest points are fundamental to many
geometric computer vision tasks. Traditionally, matching is performed by
assigning descriptors to interest points and identifying correspondences based
on descriptor similarity. This work introduces a technique where interest
points are inherently associated during detection, eliminating the need for
computing, storing, transmitting, or matching descriptors. Although the
matching accuracy is marginally lower than that of conventional approaches, our
method completely eliminates the need for descriptors, leading to a drastic
reduction in memory usage for localization systems. We assess its effectiveness
by comparing it against both classical handcrafted methods and modern learned
approaches.

</details>


### [5] [A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers](https://arxiv.org/abs/2507.10775)
*Jeffrey Joan Sam,Janhavi Sathe,Nikhil Chigali,Naman Gupta,Radhey Ruparel,Yicheng Jiang,Janmajay Singh,James W. Berck,Arko Barman*

Main category: cs.CV

TL;DR: 论文提出了一种新的航天器图像数据集，用于训练和评估图像分割模型，以支持航天器的自主检测系统。


<details>
  <summary>Details</summary>
Motivation: 航天器在太空环境中易受损坏，人工或机器人维修成本高且风险大，因此需要开发可靠的自主检测系统。

Method: 使用真实航天器模型和合成背景创建了64k标注图像数据集，并添加噪声和失真模拟真实环境。基于YOLOv8和YOLOv11模型进行微调，并在硬件和时间约束下评估性能。

Result: 模型在测试中达到Dice分数0.92、Hausdorff距离0.69，推理时间约0.5秒。

Conclusion: 提出的数据集和模型为航天器实时图像分割提供了有效解决方案，支持太空自主检测系统的开发。

Abstract: Spacecraft deployed in outer space are routinely subjected to various forms
of damage due to exposure to hazardous environments. In addition, there are
significant risks to the subsequent process of in-space repairs through human
extravehicular activity or robotic manipulation, incurring substantial
operational costs. Recent developments in image segmentation could enable the
development of reliable and cost-effective autonomous inspection systems. While
these models often require large amounts of training data to achieve
satisfactory results, publicly available annotated spacecraft segmentation data
are very scarce. Here, we present a new dataset of nearly 64k annotated
spacecraft images that was created using real spacecraft models, superimposed
on a mixture of real and synthetic backgrounds generated using NASA's TTALOS
pipeline. To mimic camera distortions and noise in real-world image
acquisition, we also added different types of noise and distortion to the
images. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to
generate performance benchmarks for the dataset under well-defined hardware and
inference time constraints to mimic real-world image segmentation challenges
for real-time onboard applications in space on NASA's inspector spacecraft. The
resulting models, when tested under these constraints, achieved a Dice score of
0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.
The dataset and models for performance benchmark are available at
https://github.com/RiceD2KLab/SWiM.

</details>


### [6] [Warehouse Spatial Question Answering with LLM Agent](https://arxiv.org/abs/2507.10778)
*Hsiang-Wei Huang,Jen-Hao Cheng,Kuang-Ming Chen,Cheng-Yen Yang,Bahaa Alattar,Yi-Ru Lin,Pyongkun Kim,Sangwon Kim,Kwangju Kim,Chung-I Huang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: 提出了一种数据高效的方法，通过LLM代理系统增强空间推理能力，解决复杂室内仓库场景中的空间问答任务。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型（MLLMs）在空间理解任务上表现不佳，需要更高效的方法提升其能力。

Method: 设计了一个LLM代理系统，集成多种工具进行空间推理和API交互，以回答复杂空间问题。

Result: 在2025 AI City Challenge数据集上验证了系统在物体检索、计数和距离估计任务中的高准确性和效率。

Conclusion: 该方法为提升MLLMs的空间理解能力提供了一种高效且实用的解决方案。

Abstract: Spatial understanding has been a challenging task for existing Multi-modal
Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM
finetuning to enhance MLLM's spatial understanding ability. In this paper, we
present a data-efficient approach. We propose a LLM agent system with strong
and advanced spatial reasoning ability, which can be used to solve the
challenging spatial question answering task in complex indoor warehouse
scenarios. Our system integrates multiple tools that allow the LLM agent to
conduct spatial reasoning and API tools interaction to answer the given
complicated spatial question. Extensive evaluations on the 2025 AI City
Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that
our system achieves high accuracy and efficiency in tasks such as object
retrieval, counting, and distance estimation. The code is available at:
https://github.com/hsiangwei0903/SpatialAgent

</details>


### [7] [ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference](https://arxiv.org/abs/2507.10800)
*Ali Hojjat,Janek Haberer,Soren Pirk,Olaf Landsiedel*

Main category: cs.CV

TL;DR: ThinkingViT是一种动态调整计算资源的嵌套ViT架构，通过渐进式推理阶段提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决固定计算预算的Vision Transformers在处理不同复杂度输入时的效率问题。

Method: 采用渐进式思考阶段和Token Recycling机制，动态激活注意力头并终止简单输入的推理。

Result: 在ImageNet-1K上，相同吞吐量下准确率提升2.0 p.p.，相同计算量下提升2.9 p.p.。

Conclusion: ThinkingViT通过动态计算分配显著提升了效率和性能，同时兼容现有ViT架构。

Abstract: Vision Transformers deliver state-of-the-art performance, yet their fixed
computational budget prevents scalable deployment across heterogeneous
hardware. Recent nested Transformer architectures mitigate this by embedding
nested subnetworks within a single model to enable scalable inference. However,
these models allocate the same amount of compute to all inputs, regardless of
their complexity, which leads to inefficiencies. To address this, we introduce
ThinkingViT, a nested ViT architecture that employs progressive thinking stages
to dynamically adjust inference computation based on input difficulty.
ThinkingViT initiates inference by activating a small subset of the most
important attention heads and terminates early if predictions reach sufficient
certainty. Otherwise, it activates additional attention heads and re-evaluates
the input. At the core of ThinkingViT is our Token Recycling mechanism, which
conditions each subsequent inference stage on the embeddings from the previous
stage, enabling progressive improvement. Due to its backbone-preserving design,
ThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show
that ThinkingViT surpasses nested baselines by up to 2.0 percentage points
(p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs
on ImageNet-1K. The source code is available at
https://github.com/ds-kiel/ThinkingViT.

</details>


### [8] [LLM-Guided Agentic Object Detection for Open-World Understanding](https://arxiv.org/abs/2507.10844)
*Furkan Mumcu,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.CV

TL;DR: 提出了一种基于大语言模型（LLM）的自主目标检测框架（LAOD），通过动态生成场景特定对象名称，实现无需标签的零样本检测。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测依赖固定类别集，难以处理新对象；现有开放世界和开放词汇检测方法存在语义标签缺失或依赖用户提示的问题。

Method: 利用LLM生成场景特定对象名称，结合开放词汇检测器进行定位，动态调整目标。

Result: 在LVIS、COCO和COCO-OOD数据集上验证了方法的有效性，展示了检测和命名新对象的强大性能。

Conclusion: LAOD框架提高了开放世界理解的自主性和适应性。

Abstract: Object detection traditionally relies on fixed category sets, requiring
costly re-training to handle novel objects. While Open-World and
Open-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD
lacks semantic labels for unknowns, and OVOD depends on user prompts, limiting
autonomy. We propose an LLM-guided agentic object detection (LAOD) framework
that enables fully label-free, zero-shot detection by prompting a Large
Language Model (LLM) to generate scene-specific object names. These are passed
to an open-vocabulary detector for localization, allowing the system to adapt
its goals dynamically. We introduce two new metrics, Class-Agnostic Average
Precision (CAAP) and Semantic Naming Average Precision (SNAP), to separately
evaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD
validate our approach, showing strong performance in detecting and naming novel
objects. Our method offers enhanced autonomy and adaptability for open-world
understanding.

</details>


### [9] [Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization](https://arxiv.org/abs/2507.10846)
*Casey Wall,Longwei Wang,Rodrigue Rizk,KC Santosh*

Main category: cs.CV

TL;DR: Winsor-CAM是一种改进的Grad-CAM方法，通过跨卷积层聚合信息并应用Winsorization技术生成更鲁棒和连贯的显著性图。


<details>
  <summary>Details</summary>
Motivation: 解释卷积神经网络（CNN）的决策过程在高风险领域至关重要，而现有方法（如Grad-CAM）可能掩盖重要语义线索或放大噪声。

Method: 提出Winsor-CAM，结合Winsorization技术（基于百分位的异常值衰减）和用户可调阈值，跨所有卷积层生成显著性图。

Result: 在PASCAL VOC 2012数据集上，Winsor-CAM在可解释性和定位指标（如IoU和质心对齐）上优于Grad-CAM和均匀层平均基线。

Conclusion: Winsor-CAM通过提供可解释的多层洞察和人为控制，推动了可信AI的发展。

Abstract: Interpreting the decision-making process of Convolutional Neural Networks
(CNNs) is critical for deploying models in high-stakes domains.
Gradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method
for visual explanations, yet it typically focuses on the final convolutional
layer or na\"ively averages across layers, strategies that can obscure
important semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a
novel, human-tunable extension of Grad-CAM that generates robust and coherent
saliency maps by aggregating information across all convolutional layers. To
mitigate the influence of noisy or extreme attribution values, Winsor-CAM
applies Winsorization, a percentile-based outlier attenuation technique. A
user-controllable threshold allows for semantic-level tuning, enabling flexible
exploration of model behavior across representational hierarchies. Evaluations
on standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the
PASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable
heatmaps and achieves superior performance in localization metrics, including
intersection-over-union and center-of-mass alignment, when compared to Grad-CAM
and uniform layer-averaging baselines. Winsor-CAM advances the goal of
trustworthy AI by offering interpretable, multi-layer insights with
human-in-the-loop control.

</details>


### [10] [Sparse Fine-Tuning of Transformers for Generative Tasks](https://arxiv.org/abs/2507.10855)
*Wei Chen,Jingxi Yu,Zichen Miao,Qiang Qiu*

Main category: cs.CV

TL;DR: 论文提出了一种基于稀疏编码的微调框架，通过稀疏组合特征字典原子来提升模型的可解释性和任务适应性。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法难以解释参数更新的贡献，稀疏编码框架旨在解决这一问题。

Method: 利用稀疏编码表示微调特征，通过稀疏系数指示原子重要性，提升任务适应性。

Result: 在图像编辑和文本到图像概念定制任务中表现优于基线方法。

Conclusion: 稀疏编码框架有效提升了模型的可解释性和任务适应性。

Abstract: Large pre-trained transformers have revolutionized artificial intelligence
across various domains, and fine-tuning remains the dominant approach for
adapting these models to downstream tasks due to the cost of training from
scratch. However, in existing fine-tuning methods, the updated representations
are formed as a dense combination of modified parameters, making it challenging
to interpret their contributions and understand how the model adapts to new
tasks. In this work, we introduce a fine-tuning framework inspired by sparse
coding, where fine-tuned features are represented as a sparse combination of
basic elements, i.e., feature dictionary atoms. The feature dictionary atoms
function as fundamental building blocks of the representation, and tuning atoms
allows for seamless adaptation to downstream tasks. Sparse coefficients then
serve as indicators of atom importance, identifying the contribution of each
atom to the updated representation. Leveraging the atom selection capability of
sparse coefficients, we first demonstrate that our method enhances image
editing performance by improving text alignment through the removal of
unimportant feature dictionary atoms. Additionally, we validate the
effectiveness of our approach in the text-to-image concept customization task,
where our method efficiently constructs the target concept using a sparse
combination of feature dictionary atoms, outperforming various baseline
fine-tuning methods.

</details>


### [11] [MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection](https://arxiv.org/abs/2507.11252)
*Guanghao Wu,Chen Xu,Hai Song,Chong Wang,Qixing Zhang*

Main category: cs.CV

TL;DR: 提出了一种生成森林火灾烟雾图像的框架，通过改进修复模型和引入新损失函数，生成高质量烟雾图像，提升烟雾检测模型性能。


<details>
  <summary>Details</summary>
Motivation: 森林火灾烟雾图像数据稀缺，现有修复模型生成烟雾图像质量不足，需改进以支持烟雾检测任务。

Method: 结合预训练分割模型和多模态模型获取烟雾掩码和图像描述；提出基于掩码和掩码图像特征的网络架构；引入掩码随机差异损失函数；利用多模态大语言模型筛选合成图像。

Result: 生成的烟雾图像真实多样，有效提升了森林火灾烟雾检测模型的性能。

Conclusion: 提出的框架解决了烟雾图像生成的质量问题，为烟雾检测任务提供了高质量数据集。

Abstract: Smoke is the first visible indicator of a wildfire.With the advancement of
deep learning, image-based smoke detection has become a crucial method for
detecting and preventing forest fires. However, the scarcity of smoke image
data from forest fires is one of the significant factors hindering the
detection of forest fire smoke. Image generation models offer a promising
solution for synthesizing realistic smoke images. However, current inpainting
models exhibit limitations in generating high-quality smoke representations,
particularly manifesting as inconsistencies between synthesized smoke and
background contexts. To solve these problems, we proposed a comprehensive
framework for generating forest fire smoke images. Firstly, we employed the
pre-trained segmentation model and the multimodal model to obtain smoke masks
and image captions.Then, to address the insufficient utilization of masks and
masked images by inpainting models, we introduced a network architecture guided
by mask and masked image features. We also proposed a new loss function, the
mask random difference loss, which enhances the consistency of the generated
effects around the mask by randomly expanding and eroding the mask
edges.Finally, to generate a smoke image dataset using random masks for
subsequent detection tasks, we incorporated smoke characteristics and use a
multimodal large language model as a filtering tool to select diverse and
reasonable smoke images, thereby improving the quality of the synthetic
dataset. Experiments showed that our generated smoke images are realistic and
diverse, and effectively enhance the performance of forest fire smoke detection
models. Code is available at https://github.com/wghr123/MFGDiffusion.

</details>


### [12] [A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n](https://arxiv.org/abs/2507.10864)
*Saadat Behzadi,Danial Sharifrazi,Bita Mesbahzadeh,Javad Hassannataj Joloudarid,Roohallah Alizadehsani*

Main category: cs.CV

TL;DR: 该研究提出了一种结合LOF算法和YOLO-v11n的轻量级框架，用于结直肠息肉检测，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌是全球主要死因之一，及时准确的息肉检测对诊断和预防至关重要。

Method: 使用LOF算法过滤噪声数据，结合YOLO-v11n模型，通过5折交叉验证和多数据集测试。

Result: 模型表现优异，精度达95.83%，召回率91.85%，F1分数93.48%，mAP@0.5为96.48%。

Conclusion: 该方法适合临床实时应用，强调了数据预处理和模型效率在医学影像AI系统中的重要性。

Abstract: Objectives: Timely and accurate detection of colorectal polyps plays a
crucial role in diagnosing and preventing colorectal cancer, a major cause of
mortality worldwide. This study introduces a new, lightweight, and efficient
framework for polyp detection that combines the Local Outlier Factor (LOF)
algorithm for filtering noisy data with the YOLO-v11n deep learning model.
  Study design: An experimental study leveraging deep learning and outlier
removal techniques across multiple public datasets.
  Methods: The proposed approach was tested on five diverse and publicly
available datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.
Since these datasets originally lacked bounding box annotations, we converted
their segmentation masks into suitable detection labels. To enhance the
robustness and generalizability of our model, we apply 5-fold cross-validation
and remove anomalous samples using the LOF method configured with 30 neighbors
and a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a
fast and resource-efficient object detection architecture optimized for
real-time applications. We train the model using a combination of modern
augmentation strategies to improve detection accuracy under diverse conditions.
  Results: Our approach significantly improves polyp localization performance,
achieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5
of 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,
our model demonstrates enhanced accuracy and efficiency.
  Conclusions: These results suggest that the proposed method is well-suited
for real-time colonoscopy support in clinical settings. Overall, the study
underscores how crucial data preprocessing and model efficiency are when
designing effective AI systems for medical imaging.

</details>


### [13] [Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes](https://arxiv.org/abs/2507.10881)
*Roman Naeem,David Hagerman,Jennifer Alvén,Lennart Svensson,Fredrik Kahl*

Main category: cs.CV

TL;DR: Trexplorer Super是一种改进的3D医学图像中心线追踪模型，通过新方法解决了重复分支和过早终止的问题，并在多个数据集上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 准确追踪管状树结构（如血管和气道）并保持其拓扑结构对医学任务至关重要，但现有模型存在重复分支和过早终止的问题。

Method: 提出Trexplorer Super模型，通过新方法改进性能，并开发了三个难度递增的中心线数据集（一个合成，两个真实）用于评估。

Result: Trexplorer Super在所有数据集上均优于现有SOTA模型，但合成数据上的强表现不一定适用于真实数据。

Conclusion: Trexplorer Super在中心线追踪任务中表现优异，同时强调了真实数据评估的重要性。

Abstract: Tubular tree structures, such as blood vessels and airways, are essential in
human anatomy and accurately tracking them while preserving their topology is
crucial for various downstream tasks. Trexplorer is a recurrent model designed
for centerline tracking in 3D medical images but it struggles with predicting
duplicate branches and terminating tracking prematurely. To address these
issues, we present Trexplorer Super, an enhanced version that notably improves
performance through novel advancements. However, evaluating centerline tracking
models is challenging due to the lack of public datasets. To enable thorough
evaluation, we develop three centerline datasets, one synthetic and two real,
each with increasing difficulty. Using these datasets, we conduct a
comprehensive evaluation of existing state-of-the-art (SOTA) models and compare
them with our approach. Trexplorer Super outperforms previous SOTA models on
every dataset. Our results also highlight that strong performance on synthetic
data does not necessarily translate to real datasets. The code and datasets are
available at https://github.com/RomStriker/Trexplorer-Super.

</details>


### [14] [Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency](https://arxiv.org/abs/2507.10893)
*Minjong Cheon,Eunhan Goo,Su-Hyeon Shin,Muhammad Ahmed,Hyungjun Kim*

Main category: cs.CV

TL;DR: 本文介绍了一种基于CNN的轻量级全球天气预报模型KAI-a，其在保持高精度的同时显著降低了计算需求。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的AI天气预报模型取得了显著进展，但其高计算复杂性和资源需求限制了实用性。因此，研究旨在开发一种更高效的替代方案。

Method: KAI-a采用尺度不变架构和InceptionNeXt块，结合地球系统数据特性设计，训练于ERA5数据集，仅需7百万参数和12小时单GPU训练。

Result: KAI-a在中期天气预报中表现与最先进模型相当，同时显著减少计算负担，并能有效捕捉极端天气事件。

Conclusion: KAI-a为数据驱动的天气预报提供了一种高效、实用的解决方案，尤其适用于资源受限的场景。

Abstract: Recently, AI-based weather forecast models have achieved impressive advances.
These models have reached accuracy levels comparable to traditional NWP
systems, marking a significant milestone in data-driven weather prediction.
However, they mostly leverage Transformer-based architectures, which often
leads to high training complexity and resource demands due to the massive
parameter sizes. In this study, we introduce a modernized CNN-based model for
global weather forecasting that delivers competitive accuracy while
significantly reducing computational requirements. To present a systematic
modernization roadmap, we highlight key architectural enhancements across
multiple design scales from an earlier CNN-based approach. KAI-a incorporates a
scale-invariant architecture and InceptionNeXt-based blocks within a
geophysically-aware design, tailored to the structure of Earth system data.
Trained on the ERA5 daily dataset with 67 atmospheric variables, the model
contains about 7 million parameters and completes training in just 12 hours on
a single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the
performance of state-of-the-art models in medium-range weather forecasting,
while offering a significantly lightweight design. Furthermore, case studies on
the 2018 European heatwave and the East Asian summer monsoon demonstrate
KAI-a's robust skill in capturing extreme events, reinforcing its practical
utility.

</details>


### [15] [Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition](https://arxiv.org/abs/2507.10895)
*Xiaocong Zeng,Craig Michoski,Yan Pang,Dongyang Kuang*

Main category: cs.CV

TL;DR: 论文提出两种新的正则化策略（LVL和LGCL）解决EEG情感识别中的时间尺度依赖标签不一致问题，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决EEG情感识别中时间尺度依赖标签不一致（TsDLI）问题，提升模型泛化性和可解释性。

Method: 提出局部变异损失（LVL）和局部-全局一致性损失（LGCL），结合有界变异函数和交换时间距离的数学原理。

Result: 在DREAMER和DEAP数据集上，LVL和LGCL表现优于现有方法，LVL综合排名最佳。

Conclusion: 提出的方法在标签不一致情况下平衡了可解释性和预测能力，LVL表现最优。

Abstract: In this work, we address the often-overlooked issue of Timescale Dependent
Label Inconsistency (TsDLI) in training neural network models for EEG-based
human emotion recognition. To mitigate TsDLI and enhance model generalization
and explainability, we propose two novel regularization strategies: Local
Variation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods
incorporate classical mathematical principles--specifically, functions of
bounded variation and commute-time distances--within a graph theoretic
framework. Complementing our regularizers, we introduce a suite of new
evaluation metrics that better capture the alignment between temporally local
predictions and their associated global emotion labels. We validate our
approach through comprehensive experiments on two widely used EEG emotion
datasets, DREAMER and DEAP, across a range of neural architectures including
LSTM and transformer-based models. Performance is assessed using five distinct
metrics encompassing both quantitative accuracy and qualitative consistency.
Results consistently show that our proposed methods outperform state-of-the-art
baselines, delivering superior aggregate performance and offering a principled
trade-off between interpretability and predictive power under label
inconsistency. Notably, LVL achieves the best aggregate rank across all
benchmarked backbones and metrics, while LGCL frequently ranks the second,
highlighting the effectiveness of our framework.

</details>


### [16] [GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization](https://arxiv.org/abs/2507.10935)
*Shaowen Tong,Zimin Xia,Alexandre Alahi,Xuming He,Yujiao Shi*

Main category: cs.CV

TL;DR: GeoDistill提出了一种基于几何引导的弱监督自蒸馏框架，通过教师-学生学习和视场掩码提升跨视图定位的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有跨视图定位方法依赖昂贵的全监督学习，GeoDistill旨在通过弱监督减少对精确标注的需求。

Method: 使用教师模型定位全景图像，学生模型通过视场掩码学习局部特征，并通过对齐预测提升性能。

Result: 实验表明GeoDistill显著提升了定位精度，并减少了不确定性。

Conclusion: GeoDistill为跨视图定位提供了高效且可扩展的解决方案。

Abstract: Cross-view localization, the task of estimating a camera's
3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with
satellite images, is crucial for large-scale outdoor applications like
autonomous navigation and augmented reality. Existing methods often rely on
fully supervised learning, which requires costly ground-truth pose annotations.
In this work, we propose GeoDistill, a Geometry guided weakly supervised self
distillation framework that uses teacher-student learning with Field-of-View
(FoV)-based masking to enhance local feature learning for robust cross-view
localization. In GeoDistill, the teacher model localizes a panoramic image,
while the student model predicts locations from a limited FoV counterpart
created by FoV-based masking. By aligning the student's predictions with those
of the teacher, the student focuses on key features like lane lines and ignores
textureless regions, such as roads. This results in more accurate predictions
and reduced uncertainty, regardless of whether the query images are panoramas
or limited FoV images. Our experiments show that GeoDistill significantly
improves localization performance across different frameworks. Additionally, we
introduce a novel orientation estimation network that predicts relative
orientation without requiring precise planar position ground truth. GeoDistill
provides a scalable and efficient solution for real-world cross-view
localization challenges. Code and model can be found at
https://github.com/tongshw/GeoDistill.

</details>


### [17] [Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2507.10938)
*Zhengyi Xu,Haoran Wu,Wen Jiang,Jie Geng*

Main category: cs.CV

TL;DR: 论文提出了一种名为GAPL-SCD的图聚合原型学习方法，用于解决语义变化检测中的多任务优化问题，通过自适应权重分配和梯度旋转方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 语义变化检测（SCD）需要同时优化多个任务，容易因任务间冲突导致负迁移，因此需要一种方法来解决这一问题。

Method: 提出GAPL-SCD框架，结合多任务联合优化、图聚合原型学习、自适应权重分配和梯度旋转方法，增强多任务学习能力。

Result: 在SECOND和Landsat-SCD数据集上实现了最先进的性能，显著提升了SCD任务的准确性和鲁棒性。

Conclusion: GAPL-SCD通过多任务优化和特征交互模块，有效解决了语义变化检测中的任务冲突问题，提升了模型性能。

Abstract: Semantic change detection (SCD) extends the binary change detection task to
provide not only the change locations but also the detailed "from-to"
categories in multi-temporal remote sensing data. Such detailed semantic
insights into changes offer considerable advantages for a wide array of
applications. However, since SCD involves the simultaneous optimization of
multiple tasks, the model is prone to negative transfer due to task-specific
learning difficulties and conflicting gradient flows. To address this issue, we
propose Graph Aggregation Prototype Learning for Semantic Change Detection in
remote sensing(GAPL-SCD). In this framework, a multi-task joint optimization
method is designed to optimize the primary task of semantic segmentation and
change detection, along with the auxiliary task of graph aggregation prototype
learning. Adaptive weight allocation and gradient rotation methods are used to
alleviate the conflict between training tasks and improve multi-task learning
capabilities. Specifically, the graph aggregation prototype learning module
constructs an interaction graph using high-level features. Prototypes serve as
class proxies, enabling category-level domain alignment across time points and
reducing interference from irrelevant changes. Additionally, the proposed
self-query multi-level feature interaction and bi-temporal feature fusion
modules further enhance multi-scale feature representation, improving
performance in complex scenes. Experimental results on the SECOND and
Landsat-SCD datasets demonstrate that our method achieves state-of-the-art
performance, with significant improvements in accuracy and robustness for SCD
task.

</details>


### [18] [Robust ID-Specific Face Restoration via Alignment Learning](https://arxiv.org/abs/2507.10943)
*Yushun Fang,Lu Liu,Xiang Gao,Qiang Hu,Ning Cao,Jianghe Cui,Gang Chen,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: RIDFR是一种基于扩散模型的ID特定人脸修复框架，通过内容注入和身份注入模块，结合对齐学习，显著提升了身份保真度和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 当前人脸修复方法在身份保真度方面存在不足，尤其是面对身份模糊输入和随机生成过程时。

Method: RIDFR利用预训练扩散模型，结合内容注入和身份注入模块，并通过对齐学习抑制ID无关语义干扰。

Result: 实验表明，RIDFR优于现有方法，能够重建高质量且身份保真度高的结果。

Conclusion: RIDFR在身份特定人脸修复中表现出色，具有高保真度和强鲁棒性。

Abstract: The latest developments in Face Restoration have yielded significant
advancements in visual quality through the utilization of diverse diffusion
priors. Nevertheless, the uncertainty of face identity introduced by
identity-obscure inputs and stochastic generative processes remains unresolved.
To address this challenge, we present Robust ID-Specific Face Restoration
(RIDFR), a novel ID-specific face restoration framework based on diffusion
models. Specifically, RIDFR leverages a pre-trained diffusion model in
conjunction with two parallel conditioning modules. The Content Injection
Module inputs the severely degraded image, while the Identity Injection Module
integrates the specific identity from a given image. Subsequently, RIDFR
incorporates Alignment Learning, which aligns the restoration results from
multiple references with the same identity in order to suppress the
interference of ID-irrelevant face semantics (e.g. pose, expression, make-up,
hair style). Experiments demonstrate that our framework outperforms the
state-of-the-art methods, reconstructing high-quality ID-specific results with
high identity fidelity and demonstrating strong robustness.

</details>


### [19] [Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data](https://arxiv.org/abs/2507.10969)
*Palash Ray,Mahuya Sasmal,Asish Bera*

Main category: cs.CV

TL;DR: 该论文提出了一个名为WomenSports的新数据集，用于女性体育动作分类，并通过卷积神经网络（CNN）和通道注意力机制提升特征表示，取得了89.15%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏足够的女性体育动作图像，且缺乏类内和类间变化，限制了相关研究的发展。

Method: 提出一个包含多样化体育活动的数据集WomenSports，并设计了一种基于CNN和通道注意力机制的深度特征提取方法。

Result: 在多个数据集上测试，性能显著，特别是在WomenSports数据集上达到89.15%的Top-1分类准确率。

Conclusion: WomenSports数据集和提出的方法为女性体育动作分类提供了有效工具，数据集已公开供研究使用。

Abstract: Sports action classification representing complex body postures and
player-object interactions is an emerging area in image-based sports analysis.
Some works have contributed to automated sports action recognition using
machine learning techniques over the past decades. However, sufficient image
datasets representing women sports actions with enough intra- and inter-class
variations are not available to the researchers. To overcome this limitation,
this work presents a new dataset named WomenSports for women sports
classification using small-scale training data. This dataset includes a variety
of sports activities, covering wide variations in movements, environments, and
interactions among players. In addition, this study proposes a convolutional
neural network (CNN) for deep feature extraction. A channel attention scheme
upon local contextual regions is applied to refine and enhance feature
representation. The experiments are carried out on three different sports
datasets and one dance dataset for generalizing the proposed algorithm, and the
performances on these datasets are noteworthy. The deep learning method
achieves 89.15% top-1 classification accuracy using ResNet-50 on the proposed
WomenSports dataset, which is publicly available for research at Mendeley Data.

</details>


### [20] [Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection](https://arxiv.org/abs/2507.10977)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: 提出了一种基于小波注意力的主干网络和射线编码器架构，用于高效且可靠的人类-物体交互检测。


<details>
  <summary>Details</summary>
Motivation: 现有的人类-物体交互检测器在高效性和可靠性上存在不足，依赖资源密集型训练和低效架构。

Method: 设计了小波注意力主干网络以聚合低阶和高阶交互特征，并提出射线编码器优化多尺度注意力。

Result: 在ImageNet和HICO-DET等基准数据集上展示了优越性能。

Conclusion: 新型架构显著提升了人类-物体交互检测的效率和准确性。

Abstract: Human-object interaction (HOI) detection is essential for accurately
localizing and characterizing interactions between humans and objects,
providing a comprehensive understanding of complex visual scenes across various
domains. However, existing HOI detectors often struggle to deliver reliable
predictions efficiently, relying on resource-intensive training methods and
inefficient architectures. To address these challenges, we conceptualize a
wavelet attention-like backbone and a novel ray-based encoder architecture
tailored for HOI detection. Our wavelet backbone addresses the limitations of
expressing middle-order interactions by aggregating discriminative features
from the low- and high-order interactions extracted from diverse convolutional
filters. Concurrently, the ray-based encoder facilitates multi-scale attention
by optimizing the focus of the decoder on relevant regions of interest and
mitigating computational overhead. As a result of harnessing the attenuated
intensity of learnable ray origins, our decoder aligns query embeddings with
emphasized regions of interest for accurate predictions. Experimental results
on benchmark datasets, including ImageNet and HICO-DET, showcase the potential
of our proposed architecture. The code is publicly available at
[https://github.com/henry-pay/RayEncoder].

</details>


### [21] [Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction](https://arxiv.org/abs/2507.10978)
*Ayush Gupta,Siyuan Huang,Rama Chellappa*

Main category: cs.CV

TL;DR: RG-Gait提出了一种通过残差学习解决步态识别中遮挡问题的方法，同时保持对完整步态的识别性能。


<details>
  <summary>Details</summary>
Motivation: 当前步态识别方法大多未解决遮挡问题，且现有方法在遮挡和完整步态之间难以兼顾。

Method: 将遮挡步态建模为完整步态表示的残差偏差，通过自适应残差学习提升遮挡步态识别性能。

Result: 在Gait3D、GREW和BRIAR数据集上验证了方法的有效性，显著提升了遮挡步态的识别性能且不影响完整步态识别。

Conclusion: 残差学习是解决遮挡步态识别并保留完整步态性能的有效技术。

Abstract: Gait is becoming popular as a method of person re-identification because of
its ability to identify people at a distance. However, most current works in
gait recognition do not address the practical problem of occlusions. Among
those which do, some require paired tuples of occluded and holistic sequences,
which are impractical to collect in the real world. Further, these approaches
work on occlusions but fail to retain performance on holistic inputs. To
address these challenges, we propose RG-Gait, a method for residual correction
for occluded gait recognition with holistic retention. We model the problem as
a residual learning task, conceptualizing the occluded gait signature as a
residual deviation from the holistic gait representation. Our proposed network
adaptively integrates the learned residual, significantly improving performance
on occluded gait sequences without compromising the holistic recognition
accuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR
datasets and show that learning the residual can be an effective technique to
tackle occluded gait recognition with holistic retention.

</details>


### [22] [SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition](https://arxiv.org/abs/2507.10999)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: SpaRTAN是一种轻量级架构设计，通过多尺度空间特征和通道聚合模块提升性能，同时减少参数冗余，在ImageNet和COCO上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决CNN和Transformer在视觉任务中偏好简单特征和存在信息冗余的问题。

Method: 结合多尺度空间特征捕捉和波基通道聚合模块，动态整合特征。

Result: 在ImageNet-1k上达到77.7%准确率（3.8M参数），COCO上50.0% AP（21.5M参数）。

Conclusion: SpaRTAN通过高效设计实现了高性能和参数效率。

Abstract: The resurgence of convolutional neural networks (CNNs) in visual recognition
tasks, exemplified by ConvNeXt, has demonstrated their capability to rival
transformer-based architectures through advanced training methodologies and
ViT-inspired design principles. However, both CNNs and transformers exhibit a
simplicity bias, favoring straightforward features over complex structural
representations. Furthermore, modern CNNs often integrate MLP-like blocks akin
to those in transformers, but these blocks suffer from significant information
redundancies, necessitating high expansion ratios to sustain competitive
performance. To address these limitations, we propose SpaRTAN, a lightweight
architectural design that enhances spatial and channel-wise information
processing. SpaRTAN employs kernels with varying receptive fields, controlled
by kernel size and dilation factor, to capture discriminative multi-order
spatial features effectively. A wave-based channel aggregation module further
modulates and reinforces pixel interactions, mitigating channel-wise
redundancies. Combining the two modules, the proposed network can efficiently
gather and dynamically contextualize discriminative features. Experimental
results in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable
parameter efficiency while maintaining competitive performance. In particular,
on the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M
parameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver
strong performance through an efficient design. On the COCO benchmark, it
achieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M
parameters. The code is publicly available at
[https://github.com/henry-pay/SpaRTAN].

</details>


### [23] [Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection](https://arxiv.org/abs/2507.11003)
*Yuhu Bai,Jiangning Zhang,Yunkang Cao,Guangyuan Lu,Qingdong He,Xiangtai Li,Guanzhong Tian*

Main category: cs.CV

TL;DR: FiSeCLIP利用CLIP模型进行零样本异常检测，通过特征匹配和跨模态对齐，结合批次内图像作为参考，并利用文本信息过滤噪声，提升性能。


<details>
  <summary>Details</summary>
Motivation: 零样本异常检测在工业应用中需求广泛，但现有方法在批次测试和噪声过滤方面存在不足。

Method: FiSeCLIP结合特征匹配与跨模态对齐，利用批次内图像作为参考，并通过文本信息过滤噪声，同时恢复CLIP的局部语义相关性。

Result: 在MVTec-AD等基准测试中，FiSeCLIP在异常分类和分割任务上表现优异，显著超越现有方法。

Conclusion: FiSeCLIP为零样本异常检测提供了更强基线，展示了CLIP模型在细粒度任务中的潜力。

Abstract: With the advent of vision-language models (e.g., CLIP) in zero- and few-shot
settings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in
recent research, where the rare classes are essential and expected in many
applications. This study introduces \textbf{FiSeCLIP} for ZSAD with
training-free \textbf{CLIP}, combining the feature matching with the
cross-modal alignment. Testing with the entire dataset is impractical, while
batch-based testing better aligns with real industrial needs, and images within
a batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes
other images in the same batch as reference information for the current image.
However, the lack of labels for these references can introduce ambiguity, we
apply text information to \textbf{fi}lter out noisy features. In addition, we
further explore CLIP's inherent potential to restore its local
\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection
tasks to enable a more accurate filtering process. Our approach exhibits
superior performance for both anomaly classification and segmentation on
anomaly detection benchmarks, building a stronger baseline for the direction,
e.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by
+4.6\%$\uparrow$/+5.7\%$\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.

</details>


### [24] [Semantically Informed Salient Regions Guided Radiology Report Generation](https://arxiv.org/abs/2507.11015)
*Zeyi Hou,Zeqiang Wei,Ruixin Yan,Ning Lang,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: 提出了一种基于语义显著区域的放射学报告生成方法（SISRNet），通过聚焦医学关键区域生成更准确的报告。


<details>
  <summary>Details</summary>
Motivation: 现有方法因数据偏差导致报告流畅但不准确，限制了临床应用。

Method: 利用细粒度跨模态语义识别医学关键区域，并在图像建模和报告生成中聚焦这些区域。

Result: 在IU-Xray和MIMIC-CXR数据集上表现优于同类方法。

Conclusion: SISRNet能有效减少数据偏差影响，生成临床准确的报告。

Abstract: Recent advances in automated radiology report generation from chest X-rays
using deep learning algorithms have the potential to significantly reduce the
arduous workload of radiologists. However, due to the inherent massive data
bias in radiology images, where abnormalities are typically subtle and sparsely
distributed, existing methods often produce fluent yet medically inaccurate
reports, limiting their applicability in clinical practice. To address this
issue effectively, we propose a Semantically Informed Salient Regions-guided
(SISRNet) report generation method. Specifically, our approach explicitly
identifies salient regions with medically critical characteristics using
fine-grained cross-modal semantics. Then, SISRNet systematically focuses on
these high-information regions during both image modeling and report
generation, effectively capturing subtle abnormal findings, mitigating the
negative impact of data bias, and ultimately generating clinically accurate
reports. Compared to its peers, SISRNet demonstrates superior performance on
widely used IU-Xray and MIMIC-CXR datasets.

</details>


### [25] [Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schrödinger Bridge with Conditional Diffusion](https://arxiv.org/abs/2507.11025)
*Sung Ho Kang,Hyun-Cheol Park*

Main category: cs.CV

TL;DR: 提出了一种基于Schrodinger Bridge（SB）的CBCT到MDCT转换框架，结合GAN先验和人类引导的条件扩散，确保解剖保真度和感知可控性。


<details>
  <summary>Details</summary>
Motivation: 解决传统GAN或扩散模型在医学图像转换中边界一致性和临床偏好对齐的不足。

Method: 采用SB框架，结合GAN先验和人类反馈（通过CFG），通过迭代优化和锦标赛选择实现偏好学习。

Result: 在临床数据集上，RMSE、SSIM、LPIPS和Dice指标表现优异，仅需10步采样。

Conclusion: 该框架高效且有效，适用于实时、偏好对齐的医学图像转换。

Abstract: We present a novel framework for CBCT-to-MDCT translation, grounded in the
Schrodinger Bridge (SB) formulation, which integrates GAN-derived priors with
human-guided conditional diffusion. Unlike conventional GANs or diffusion
models, our approach explicitly enforces boundary consistency between CBCT
inputs and pseudo targets, ensuring both anatomical fidelity and perceptual
controllability. Binary human feedback is incorporated via classifier-free
guidance (CFG), effectively steering the generative process toward clinically
preferred outcomes. Through iterative refinement and tournament-based
preference selection, the model internalizes human preferences without relying
on a reward model. Subtraction image visualizations reveal that the proposed
method selectively attenuates shade artifacts in key anatomical regions while
preserving fine structural detail. Quantitative evaluations further demonstrate
superior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical
datasets -- outperforming prior GAN- and fine-tuning-based feedback methods --
while requiring only 10 sampling steps. These findings underscore the
effectiveness and efficiency of our framework for real-time, preference-aligned
medical image translation.

</details>


### [26] [Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2507.11030)
*Sunghyun Park,Jungsoo Lee,Shubhankar Borse,Munawar Hayat,Sungha Choi,Kyuwoong Hwang,Fatih Porikli*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本提示调优的插件方法，用于个性化开放词汇语义分割（OVSS），以识别用户特定的视觉概念，同时保持原始OVSS性能。


<details>
  <summary>Details</summary>
Motivation: 开放词汇语义分割（OVSS）无法理解用户个性化文本（如“我的马克杯”）以分割特定兴趣区域。本文旨在解决这一挑战。

Method: 提出了一种文本提示调优的插件方法，结合“负掩膜提议”以减少错误预测，并通过注入视觉嵌入丰富文本提示表示。

Result: 在FSS$^\text{per}$、CUB$^\text{per}$和ADE$^\text{per}$等新基准测试中表现优越。

Conclusion: 该方法在不影响原始OVSS性能的情况下，显著提升了个性化OVSS的能力。

Abstract: While open-vocabulary semantic segmentation (OVSS) can segment an image into
semantic regions based on arbitrarily given text descriptions even for classes
unseen during training, it fails to understand personal texts (e.g., `my mug
cup') for segmenting regions of specific interest to users. This paper
addresses challenges like recognizing `my mug cup' among `multiple mug cups'.
To overcome this challenge, we introduce a novel task termed
\textit{personalized open-vocabulary semantic segmentation} and propose a text
prompt tuning-based plug-in method designed to recognize personal visual
concepts using a few pairs of images and masks, while maintaining the
performance of the original OVSS. Based on the observation that reducing false
predictions is essential when applying text prompt tuning to this task, our
proposed method employs `negative mask proposal' that captures visual concepts
other than the personalized concept. We further improve the performance by
enriching the representation of text prompts by injecting visual embeddings of
the personal concept into them. This approach enhances personalized OVSS
without compromising the original OVSS performance. We demonstrate the
superiority of our method on our newly established benchmarks for this task,
including FSS$^\text{per}$, CUB$^\text{per}$, and ADE$^\text{per}$.

</details>


### [27] [Efficient Dual-domain Image Dehazing with Haze Prior Perception](https://arxiv.org/abs/2507.11035)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

TL;DR: 提出了一种双域去雾网络DGFDNet，结合空间和频率域特征，通过物理引导的退化对齐提升性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在单图像去雾中全局建模能力强，但计算成本高，现有方法依赖空间域特征，计算昂贵且在复杂雾况下效果不佳。

Method: DGFDNet包含HAFM模块（基于暗通道先验生成雾置信图，增强相关频率成分）和MGAM模块（多尺度特征融合）。PCGB分支通过闭环反馈机制迭代优化先验。

Result: 在四个基准数据集上表现优异，兼具鲁棒性和实时性。

Conclusion: DGFDNet通过双域协同和物理引导，显著提升了去雾效果和效率。

Abstract: Transformer-based models exhibit strong global modeling capabilities in
single-image dehazing, but their high computational cost limits real-time
applicability. Existing methods predominantly rely on spatial-domain features
to capture long-range dependencies, which are computationally expensive and
often inadequate under complex haze conditions. While some approaches introduce
frequency-domain cues, the weak coupling between spatial and frequency branches
limits the overall performance. To overcome these limitations, we propose the
Dark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel
dual-domain framework that performs physically guided degradation alignment
across spatial and frequency domains. At its core, the DGFDBlock comprises two
key modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a
pixel-level haze confidence map from dark channel priors to adaptively enhance
haze-relevant frequency components, thereby achieving global degradation-aware
spectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which
fuses multi-scale features through diverse convolutional kernels and hybrid
gating mechanisms to recover fine structural details. Additionally, a Prior
Correction Guidance Branch (PCGB) incorporates a closed-loop feedback
mechanism, enabling iterative refinement of the prior by intermediate dehazed
features and significantly improving haze localization accuracy, especially in
challenging outdoor scenes. Extensive experiments on four benchmark haze
datasets demonstrate that DGFDNet achieves state-of-the-art performance with
superior robustness and real-time efficiency. Code is available at:
https://github.com/Dilizlr/DGFDNet.

</details>


### [28] [A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion](https://arxiv.org/abs/2507.11037)
*Jie-Wen Li,Zi-Han Ye,Qingyuan Zhou,Jiayi Song,Ying He,Ben Fei,Wen-Ming Chen*

Main category: cs.CV

TL;DR: FootGait3D是一个专注于踝足区域的高分辨率点云数据集，用于研究步态中的3D形状补全任务。


<details>
  <summary>Details</summary>
Motivation: 解决动态步态中踝足表面几何数据采集的挑战，如遮挡和视角限制。

Method: 使用五摄像头深度传感系统采集46名受试者的8,403帧点云数据，提供完整和部分视图的对比。

Result: 数据集支持单模态和多模态3D点云补全方法的评估，推动生物力学和多段足部建模研究。

Conclusion: FootGait3D为临床步态分析、假肢设计和机器人应用提供了重要资源。

Abstract: The kinematics analysis of foot-ankle complex during gait is essential for
advancing biomechanical research and clinical assessment. Collecting accurate
surface geometry data from the foot and ankle during dynamic gait conditions is
inherently challenging due to swing foot occlusions and viewing limitations.
Thus, this paper introduces FootGait3D, a novel multi-view dataset of
high-resolution ankle-foot surface point clouds captured during natural gait.
Different from existing gait datasets that typically target whole-body or
lower-limb motion, FootGait3D focuses specifically on the detailed modeling of
the ankle-foot region, offering a finer granularity of motion data. To address
this, FootGait3D consists of 8,403 point cloud frames collected from 46
subjects using a custom five-camera depth sensing system. Each frame includes a
complete 5-view reconstruction of the foot and ankle (serving as ground truth)
along with partial point clouds obtained from only four, three, or two views.
This structured variation enables rigorous evaluation of 3D point cloud
completion methods under varying occlusion levels and viewpoints. Our dataset
is designed for shape completion tasks, facilitating the benchmarking of
state-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and
multi-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the
challenge of recovering the full foot geometry from occluded inputs. FootGait3D
has significant potential to advance research in biomechanics and multi-segment
foot modeling, offering a valuable testbed for clinical gait analysis,
prosthetic design, and robotics applications requiring detailed 3D models of
the foot during motion. The dataset is now available at
https://huggingface.co/datasets/ljw285/FootGait3D.

</details>


### [29] [Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery](https://arxiv.org/abs/2507.11040)
*Nicolas Drapier,Aladine Chetouani,Aurélien Chateigner*

Main category: cs.CV

TL;DR: GLOD是一种基于Transformer的架构，用于高分辨率卫星图像中的目标检测，采用Swin Transformer和新型UpConvMixer块，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决卫星图像中目标检测的挑战，如高分辨率和多尺度对象，同时保持计算效率。

Method: 使用Swin Transformer替代CNN骨干网络，结合UpConvMixer块进行上采样，以及Fusion Blocks进行多尺度特征融合。创新包括CBAM注意力机制和多路径头设计。

Result: 在xView数据集上达到32.95%的准确率，比现有最佳方法高出11.46%。

Conclusion: GLOD通过Transformer架构和创新的模块设计，显著提升了卫星图像目标检测的性能。

Abstract: We present GLOD, a transformer-first architecture for object detection in
high-resolution satellite imagery. GLOD replaces CNN backbones with a Swin
Transformer for end-to-end feature extraction, combined with novel UpConvMixer
blocks for robust upsampling and Fusion Blocks for multi-scale feature
integration. Our approach achieves 32.95\% on xView, outperforming SOTA methods
by 11.46\%. Key innovations include asymmetric fusion with CBAM attention and a
multi-path head design capturing objects across scales. The architecture is
optimized for satellite imagery challenges, leveraging spatial priors while
maintaining computational efficiency.

</details>


### [30] [Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation](https://arxiv.org/abs/2507.11055)
*Shuchang Ye,Usman Naseem,Mingyuan Meng,Jinman Kim*

Main category: cs.CV

TL;DR: ProLearn框架通过原型驱动学习减轻语言引导分割中的文本依赖，提升图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言引导分割方法依赖成对的图像-文本输入，限制了数据利用和临床应用。

Method: 引入原型驱动语义近似（PSA）模块，从文本中提取语义并近似无文本图像的语义指导。

Result: 在多个数据集上，ProLearn在文本有限时优于现有语言引导方法。

Conclusion: ProLearn有效解决了文本依赖问题，扩展了语言引导分割的适用性。

Abstract: Medical language-guided segmentation, integrating textual clinical reports as
auxiliary guidance to enhance image segmentation, has demonstrated significant
improvements over unimodal approaches. However, its inherent reliance on paired
image-text input, which we refer to as ``textual reliance", presents two
fundamental limitations: 1) many medical segmentation datasets lack paired
reports, leaving a substantial portion of image-only data underutilized for
training; and 2) inference is limited to retrospective analysis of cases with
paired reports, limiting its applicability in most clinical scenarios where
segmentation typically precedes reporting. To address these limitations, we
propose ProLearn, the first Prototype-driven Learning framework for
language-guided segmentation that fundamentally alleviates textual reliance. At
its core, in ProLearn, we introduce a novel Prototype-driven Semantic
Approximation (PSA) module to enable approximation of semantic guidance from
textual input. PSA initializes a discrete and compact prototype space by
distilling segmentation-relevant semantics from textual reports. Once
initialized, it supports a query-and-respond mechanism which approximates
semantic guidance for images without textual input, thereby alleviating textual
reliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG
demonstrate that ProLearn outperforms state-of-the-art language-guided methods
when limited text is available.

</details>


### [31] [Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling](https://arxiv.org/abs/2507.11061)
*Hayeon Kim,Ji Ha Jang,Se Young Chun*

Main category: cs.CV

TL;DR: RoMaP是一个新的局部3D高斯编辑框架，通过3D-GALP和正则化SDS损失实现精确的局部编辑。


<details>
  <summary>Details</summary>
Motivation: 解决高斯Splatting中局部3D编辑的挑战，如多视角分割不一致和SDS损失的模糊性。

Method: 提出3D-GALP模块生成鲁棒的3D掩码，并结合正则化SDS损失（如L1锚定损失和SLaMP编辑方法）。

Result: RoMaP在重建和生成的3D场景中实现了最先进的局部编辑效果。

Conclusion: RoMaP为3D高斯编辑提供了更鲁棒和灵活的方法。

Abstract: Recent advances in 3D neural representations and instance-level editing
models have enabled the efficient creation of high-quality 3D content. However,
achieving precise local 3D edits remains challenging, especially for Gaussian
Splatting, due to inconsistent multi-view 2D part segmentations and inherently
ambiguous nature of Score Distillation Sampling (SDS) loss. To address these
limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that
enables precise and drastic part-level modifications. First, we introduce a
robust 3D mask generation module with our 3D-Geometry Aware Label Prediction
(3D-GALP), which uses spherical harmonics (SH) coefficients to model
view-dependent label variations and soft-label property, yielding accurate and
consistent part segmentations across viewpoints. Second, we propose a
regularized SDS loss that combines the standard SDS loss with additional
regularizers. In particular, an L1 anchor loss is introduced via our Scheduled
Latent Mixing and Part (SLaMP) editing method, which generates high-quality
part-edited 2D images and confines modifications only to the target region
while preserving contextual coherence. Additional regularizers, such as
Gaussian prior removal, further improve flexibility by allowing changes beyond
the existing context, and robust 3D masking prevents unintended edits.
Experimental results demonstrate that our RoMaP achieves state-of-the-art local
3D editing on both reconstructed and generated Gaussian scenes and objects
qualitatively and quantitatively, making it possible for more robust and
flexible part-level 3D Gaussian editing.

</details>


### [32] [Joint angle model based learning to refine kinematic human pose estimation](https://arxiv.org/abs/2507.11075)
*Chang Peng,Yifei Zhou,Huifeng Xi,Shiqing Huang,Chuangye Chen,Jianming Yang,Bao Yang,Zhenyu Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于关节角度的新方法，用于改进无标记人体姿态估计（HPE）的关键点识别和轨迹平滑问题。


<details>
  <summary>Details</summary>
Motivation: 现有HPE方法在关键点识别和轨迹分析中存在误差和波动，且训练数据集标注不准确限制了深度学习模型的性能。

Method: 通过关节角度建模和高阶傅里叶级数逼近关节角度变化，设计双向循环网络作为后处理模块。

Result: 使用该方法构建的高质量数据集训练的模型在修正错误关节和平滑轨迹方面表现优异。

Conclusion: 基于关节角度的改进方法（JAR）在复杂场景（如花样滑冰和街舞）中优于现有HPE改进网络。

Abstract: Marker-free human pose estimation (HPE) has found increasing applications in
various fields. Current HPE suffers from occasional errors in keypoint
recognition and random fluctuation in keypoint trajectories when analyzing
kinematic human poses. The performance of existing deep learning-based models
for HPE refinement is considerably limited by inaccurate training datasets in
which the keypoints are manually annotated. This paper proposed a novel method
to overcome the difficulty through joint angle-based modeling. The key
techniques include: (i) A joint angle-based model of human pose, which is
robust to describe kinematic human poses; (ii) Approximating temporal variation
of joint angles through high order Fourier series to get reliable "ground
truth"; (iii) A bidirectional recurrent network is designed as a
post-processing module to refine the estimation of well-established HRNet.
Trained with the high-quality dataset constructed using our method, the network
demonstrates outstanding performance to correct wrongly recognized joints and
smooth their spatiotemporal trajectories. Tests show that joint angle-based
refinement (JAR) outperforms the state-of-the-art HPE refinement network in
challenging cases like figure skating and breaking.

</details>


### [33] [GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft](https://arxiv.org/abs/2507.11077)
*Weizhao Ma,Dong Zhou,Yuhui Hu,Zipeng He*

Main category: cs.CV

TL;DR: 提出了一种基于图的关键点网络（GKNet），用于非合作航天器的单目姿态估计，解决了结构对称性和部分遮挡问题，并发布了SKD数据集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 非合作航天器的单目姿态估计对在轨服务任务至关重要，但现有关键点检测器易受结构对称性和部分遮挡影响。

Method: 设计了基于图的关键点网络（GKNet），利用关键点图的几何约束，并创建了SKD数据集进行验证。

Result: 实验表明GKNet在精度和有效性上优于现有方法。

Conclusion: GKNet在非合作航天器姿态估计中表现出色，SKD数据集为未来研究提供了支持。

Abstract: Monocular pose estimation of non-cooperative spacecraft is significant for
on-orbit service (OOS) tasks, such as satellite maintenance, space debris
removal, and station assembly. Considering the high demands on pose estimation
accuracy, mainstream monocular pose estimation methods typically consist of
keypoint detectors and PnP solver. However, current keypoint detectors remain
vulnerable to structural symmetry and partial occlusion of non-cooperative
spacecraft. To this end, we propose a graph-based keypoints network for the
monocular pose estimation of non-cooperative spacecraft, GKNet, which leverages
the geometric constraint of keypoints graph. In order to better validate
keypoint detectors, we present a moderate-scale dataset for the spacecraft
keypoint detection, named SKD, which consists of 3 spacecraft targets, 90,000
simulated images, and corresponding high-precise keypoint annotations.
Extensive experiments and an ablation study have demonstrated the high accuracy
and effectiveness of our GKNet, compared to the state-of-the-art spacecraft
keypoint detectors. The code for GKNet and the SKD dataset is available at
https://github.com/Dongzhou-1996/GKNet.

</details>


### [34] [Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification](https://arxiv.org/abs/2507.11081)
*Chang Peng,Bao Yang,Meiqi Li,Ge Zhang,Hui Sun,Zhenyu Jiang*

Main category: cs.CV

TL;DR: 论文提出了一种基于交叉验证策略的深度学习方法，用于从GPR图像中自动识别道路地下病害（RSD），显著提高了识别准确率并减少了人工工作量。


<details>
  <summary>Details</summary>
Motivation: GPR图像中RSD识别依赖人工且效率低，现有深度学习方法受限于数据集质量不足和网络区分能力不足。

Method: 构建了一个包含2134个样本的3D GPR数据集，并提出了一种基于YOLO模型的交叉验证策略。

Result: 在实地测试中，该方法实现了超过98.6%的召回率，并将检测工作量减少了约90%。

Conclusion: 该方法为RSD自动识别提供了一种高效且准确的解决方案，具有实际应用价值。

Abstract: Ground penetrating radar (GPR) has become a rapid and non-destructive
solution for road subsurface distress (RSD) detection. However, RSD recognition
from GPR images is labor-intensive and heavily relies on inspectors' expertise.
Deep learning offers the possibility for automatic RSD recognition, but its
current performance is limited by two factors: Scarcity of high-quality dataset
for network training and insufficient capability of network to distinguish RSD.
In this study, a rigorously validated 3D GPR dataset containing 2134 samples of
diverse types was constructed through field scanning. Based on the finding that
the YOLO model trained with one of the three scans of GPR images exhibits
varying sensitivity to specific type of RSD, we proposed a novel
cross-verification strategy with outstanding accuracy in RSD recognition,
achieving recall over 98.6% in field tests. The approach, integrated into an
online RSD detection system, can reduce the labor of inspection by around 90%.

</details>


### [35] [Atmos-Bench: 3D Atmospheric Structures for Climate Insight](https://arxiv.org/abs/2507.11085)
*Tianchi Xu*

Main category: cs.CV

TL;DR: 论文提出了Atmos-Bench，首个3D大气基准数据集，以及FourCastX模型，用于从卫星LiDAR数据中恢复大气结构，无需辅助输入即可超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖辅助输入和简化物理近似，缺乏标准化3D基准，可能引入不确定性且无法充分捕捉真实辐射传输和大气散射-吸收效应。

Method: 提出FourCastX模型，结合WRF和增强的COSP模拟器生成高质量3D散射体积数据，并将物理约束嵌入模型架构。

Result: 在Atmos-Bench数据集上，模型在355 nm和532 nm波段均优于现有基线模型。

Conclusion: Atmos-Bench为卫星3D大气结构恢复设定了新标准，有助于更深入的气候研究。

Abstract: Atmospheric structure, represented by backscatter coefficients (BC) recovered
from satellite LiDAR attenuated backscatter (ATB), provides a volumetric view
of clouds, aerosols, and molecules, playing a critical role in human
activities, climate understanding, and extreme weather forecasting. Existing
methods often rely on auxiliary inputs and simplified physics-based
approximations, and lack a standardized 3D benchmark for fair evaluation.
However, such approaches may introduce additional uncertainties and
insufficiently capture realistic radiative transfer and atmospheric
scattering-absorption effects. To bridge these gaps, we present Atmos-Bench:
the first 3D atmospheric benchmark, along with a novel FourCastX:
Frequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a)
generates 921,600 image slices from 3D scattering volumes simulated at 532 nm
and 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean
time steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC
physical constraints into the model architecture, promoting energy consistency
during restoration; (c) achieves consistent improvements on the Atmos-Bench
dataset across both 355 nm and 532 nm bands, outperforming state-of-the-art
baseline models without relying on auxiliary inputs. Atmos-Bench establishes a
new standard for satellite-based 3D atmospheric structure recovery and paves
the way for deeper climate insight.

</details>


### [36] [A Survey on Interpretability in Visual Recognition](https://arxiv.org/abs/2507.11099)
*Qiyang Wan,Chengzhi Gao,Ruiping Wang,Xilin Chen*

Main category: cs.CV

TL;DR: 本文系统综述了视觉识别模型的可解释性研究，并提出了一种以人为中心的分类法，同时总结了评估指标的需求和新技术带来的机遇。


<details>
  <summary>Details</summary>
Motivation: 随着视觉识别模型在关键领域的应用，理解其机制和失败原因的需求推动了可解释性研究的发展。

Method: 提出了一种基于意图、对象、呈现和方法的分类法，系统整理了现有可解释性方法。

Result: 建立了一套系统的分类标准，并探讨了新技术（如多模态大模型）带来的新机遇。

Conclusion: 本文旨在组织现有研究并启发未来对视觉识别模型可解释性的探索。

Abstract: In recent years, visual recognition methods have advanced significantly,
finding applications across diverse fields. While researchers seek to
understand the mechanisms behind the success of these models, there is also a
growing impetus to deploy them in critical areas like autonomous driving and
medical diagnostics to better diagnose failures, which promotes the development
of interpretability research. This paper systematically reviews existing
research on the interpretability of visual recognition models and proposes a
taxonomy of methods from a human-centered perspective. The proposed taxonomy
categorizes interpretable recognition methods based on Intent, Object,
Presentation, and Methodology, thereby establishing a systematic and coherent
set of grouping criteria for these XAI methods. Additionally, we summarize the
requirements for evaluation metrics and explore new opportunities enabled by
recent technologies, such as large multimodal models. We aim to organize
existing research in this domain and inspire future investigations into the
interpretability of visual recognition models.

</details>


### [37] [KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model](https://arxiv.org/abs/2507.11102)
*Jie Yang,Wang Zeng,Sheng Jin,Lumin Xu,Wentao Liu,Chen Qian,Zhen Li,Ruimao Zhang*

Main category: cs.CV

TL;DR: KptLLM++是一种新型多模态大语言模型，专注于通用关键点理解，通过用户指令整合多种输入模态，显著提升细粒度图像分析的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在捕捉细粒度语义信息（如对象关键点）方面表现不足，而关键点对细粒度图像分析、对象检索和行为识别至关重要。

Method: 采用“识别-检测”范式，先解释关键点语义，再通过结构化思维链推理机制定位其精确位置，并扩展训练数据集至50万样本。

Result: 在多个关键点检测基准测试中表现优异，展示了卓越的准确性和泛化能力。

Conclusion: KptLLM++为细粒度图像理解提供了统一解决方案，对提升人机交互具有变革性意义。

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has revolutionized
image understanding by bridging textual and visual modalities. However, these
models often struggle with capturing fine-grained semantic information, such as
the precise identification and analysis of object keypoints. Keypoints, as
structure-aware, pixel-level, and compact representations of objects,
particularly articulated ones, play a crucial role in applications such as
fine-grained image analysis, object retrieval, and behavior recognition. In
this paper, we propose KptLLM++, a novel multimodal large language model that
specifically designed for generic keypoint comprehension through the
integration of diverse input modalities guided by user-defined instructions. By
unifying keypoint detection across varied contexts, KptLLM++ establishes itself
as an advanced interface, fostering more effective human-AI collaboration. The
model is built upon a novel identify-then-detect paradigm, which first
interprets keypoint semantics and subsequently localizes their precise
positions through a structured chain-of-thought reasoning mechanism. To push
the boundaries of performance, we have scaled up the training dataset to over
500K samples, encompassing diverse objects, keypoint categories, image styles,
and scenarios with complex occlusions. This extensive scaling enables KptLLM++
to unlock its potential, achieving remarkable accuracy and generalization.
Comprehensive experiments on multiple keypoint detection benchmarks demonstrate
its state-of-the-art performance, underscoring its potential as a unified
solution for fine-grained image understanding and its transformative
implications for human-AI interaction.

</details>


### [38] [Jellyfish Species Identification: A CNN Based Artificial Neural Network Approach](https://arxiv.org/abs/2507.11116)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha,Mostofa Kamal Nasir*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的框架，用于水母物种的检测和分类，结合多种特征提取技术和分类器，最高准确率达98%。


<details>
  <summary>Details</summary>
Motivation: 水母在海洋生态系统中扮演重要角色，但其快速增殖和生态影响对生物多样性和保护构成挑战，准确识别水母物种对生态监测和管理至关重要。

Method: 整合MobileNetV3、ResNet50等特征提取技术，结合传统机器学习分类器和前馈神经网络分类器，并使用softmax函数直接分类。

Result: 结合MobileNetV3的人工神经网络模型表现最佳，准确率达98%。

Conclusion: 深度学习和混合框架在解决生物多样性挑战和推进海洋物种检测方面具有显著效果。

Abstract: Jellyfish, a diverse group of gelatinous marine organisms, play a crucial
role in maintaining marine ecosystems but pose significant challenges for
biodiversity and conservation due to their rapid proliferation and ecological
impact. Accurate identification of jellyfish species is essential for
ecological monitoring and management. In this study, we proposed a deep
learning framework for jellyfish species detection and classification using an
underwater image dataset. The framework integrates advanced feature extraction
techniques, including MobileNetV3, ResNet50, EfficientNetV2-B0, and VGG16,
combined with seven traditional machine learning classifiers and three
Feedforward Neural Network classifiers for precise species identification.
Additionally, we activated the softmax function to directly classify jellyfish
species using the convolutional neural network models. The combination of the
Artificial Neural Network with MobileNetV3 is our best-performing model,
achieving an exceptional accuracy of 98%, significantly outperforming other
feature extractor-classifier combinations. This study demonstrates the efficacy
of deep learning and hybrid frameworks in addressing biodiversity challenges
and advancing species detection in marine environments.

</details>


### [39] [Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID](https://arxiv.org/abs/2507.11119)
*Hankun Liu,Yujian Zhao,Guanglin Niu*

Main category: cs.CV

TL;DR: 论文提出了一种多模态引导的难样本生成与学习框架（HSGL），通过结合文本和视觉模态，明确生成和优化难样本，提升了服装变化行人重识别（CC-ReID）的性能。


<details>
  <summary>Details</summary>
Motivation: 难样本在行人重识别任务中具有挑战性，尤其是在服装变化情况下。其模糊性和缺乏明确定义限制了模型的设计和鲁棒性。

Method: HSGL框架包括双粒度难样本生成（DGHSG）和难样本自适应学习（HSAL），通过多模态线索生成难样本并优化特征距离。

Result: 实验表明，HSGL在多个CC-ReID基准测试中表现优异，显著加速了学习过程，并在PRCC和LTCC数据集上达到最优性能。

Conclusion: 多模态引导的难样本生成与学习为CC-ReID提供了有效解决方案，提升了模型的判别能力和鲁棒性。

Abstract: Hard samples pose a significant challenge in person re-identification (ReID)
tasks, particularly in clothing-changing person Re-ID (CC-ReID). Their inherent
ambiguity or similarity, coupled with the lack of explicit definitions, makes
them a fundamental bottleneck. These issues not only limit the design of
targeted learning strategies but also diminish the model's robustness under
clothing or viewpoint changes. In this paper, we propose a novel
multimodal-guided Hard Sample Generation and Learning (HSGL) framework, which
is the first effort to unify textual and visual modalities to explicitly
define, generate, and optimize hard samples within a unified paradigm. HSGL
comprises two core components: (1) Dual-Granularity Hard Sample Generation
(DGHSG), which leverages multimodal cues to synthesize semantically consistent
samples, including both coarse- and fine-grained hard positives and negatives
for effectively increasing the hardness and diversity of the training data. (2)
Hard Sample Adaptive Learning (HSAL), which introduces a hardness-aware
optimization strategy that adjusts feature distances based on textual semantic
labels, encouraging the separation of hard positives and drawing hard negatives
closer in the embedding space to enhance the model's discriminative capability
and robustness to hard samples. Extensive experiments on multiple CC-ReID
benchmarks demonstrate the effectiveness of our approach and highlight the
potential of multimodal-guided hard sample generation and learning for robust
CC-ReID. Notably, HSAL significantly accelerates the convergence of the
targeted learning procedure and achieves state-of-the-art performance on both
PRCC and LTCC datasets. The code is available at
https://github.com/undooo/TryHarder-ACMMM25.

</details>


### [40] [MMOne: Representing Multiple Modalities in One Scene](https://arxiv.org/abs/2507.11129)
*Zhifeng Gu,Bing Wang*

Main category: cs.CV

TL;DR: MMOne框架通过模态建模模块和多模态分解机制解决模态冲突，提升多模态场景表示能力。


<details>
  <summary>Details</summary>
Motivation: 多模态感知有助于理解物理世界，但模态冲突（属性差异和粒度差异）是主要挑战。

Method: 提出MMOne框架，包括模态建模模块（带模态指示器）和多模态分解机制，分离共享和模态特定信息。

Result: 实验表明，MMOne提升了各模态表示能力，并支持扩展更多模态。

Conclusion: MMOne为多模态场景表示提供了一种紧凑高效的解决方案。

Abstract: Humans perceive the world through multimodal cues to understand and interact
with the environment. Learning a scene representation for multiple modalities
enhances comprehension of the physical world. However, modality conflicts,
arising from inherent distinctions among different modalities, present two
critical challenges: property disparity and granularity disparity. To address
these challenges, we propose a general framework, MMOne, to represent multiple
modalities in one scene, which can be readily extended to additional
modalities. Specifically, a modality modeling module with a novel modality
indicator is proposed to capture the unique properties of each modality.
Additionally, we design a multimodal decomposition mechanism to separate
multi-modal Gaussians into single-modal Gaussians based on modality
differences. We address the essential distinctions among modalities by
disentangling multimodal information into shared and modality-specific
components, resulting in a more compact and efficient multimodal scene
representation. Extensive experiments demonstrate that our method consistently
enhances the representation capability for each modality and is scalable to
additional modalities. The code is available at
https://github.com/Neal2020GitHub/MMOne.

</details>


### [41] [RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images](https://arxiv.org/abs/2507.11143)
*Lam Pham,Cam Le,Hieu Tang,Khang Truong,Truong Nguyen,Jasmin Lampert,Alexander Schindler,Martin Boyer,Son Phan*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的端到端模型，利用遥感图像自动观测滑坡事件，并在多个数据集上取得了高精度结果。


<details>
  <summary>Details</summary>
Motivation: 由于极端天气和人类活动导致滑坡灾害频发，但传统观测方法在大范围和复杂地形中难以实现自动化。

Method: 设计了一种新型神经网络架构，用于滑坡检测和分割任务，输入为遥感图像。

Result: 在LandSlide4Sense、Bijie和Nepal数据集上，检测任务的F1分数分别为98.23和93.83，分割任务的mIoU分数为63.74和76.88。

Conclusion: 实验结果证明了该模型在实际滑坡观测系统中的潜在应用价值。

Abstract: In recent years, landslide disasters have reported frequently due to the
extreme weather events of droughts, floods , storms, or the consequence of
human activities such as deforestation, excessive exploitation of natural
resources. However, automatically observing landslide is challenging due to the
extremely large observing area and the rugged topography such as mountain or
highland. This motivates us to propose an end-to-end deep-learning-based model
which explores the remote sensing images for automatically observing landslide
events. By considering remote sensing images as the input data, we can obtain
free resource, observe large and rough terrains by time. To explore the remote
sensing images, we proposed a novel neural network architecture which is for
two tasks of landslide detection and landslide segmentation. We evaluated our
proposed model on three different benchmark datasets of LandSlide4Sense, Bijie,
and Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,
93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU
scores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,
Nepal datasets. These experimental results prove potential to integrate our
proposed model into real-life landslide observation systems.

</details>


### [42] [Assessing Color Vision Test in Large Vision-language Models](https://arxiv.org/abs/2507.11153)
*Hongfei Ye,Bin Chen,Wenxi Liu,Yu Zhang,Zhao Li,Dandan Ni,Hongyang Chen*

Main category: cs.CV

TL;DR: 本文探讨了大型视觉语言模型的色彩视觉能力，提出了一个测试任务并构建了数据集，分析了错误类型并提出了优化策略。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型的色彩视觉能力尚未被充分研究，填补这一空白是本文的动机。

Method: 定义色彩视觉测试任务，构建多类别、多难度数据集，分析模型错误并提出微调策略。

Result: 研究发现模型在色彩视觉测试中存在特定错误，微调策略能提升其表现。

Conclusion: 本文为大型视觉语言模型的色彩视觉能力提供了测试方法和优化方向。

Abstract: With the widespread adoption of large vision-language models, the capacity
for color vision in these models is crucial. However, the color vision
abilities of large visual-language models have not yet been thoroughly
explored. To address this gap, we define a color vision testing task for large
vision-language models and construct a dataset \footnote{Anonymous Github
Showing some of the data
https://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers
multiple categories of test questions and tasks of varying difficulty levels.
Furthermore, we analyze the types of errors made by large vision-language
models and propose fine-tuning strategies to enhance their performance in color
vision tests.

</details>


### [43] [Clustering-Guided Multi-Layer Contrastive Representation Learning for Citrus Disease Classification](https://arxiv.org/abs/2507.11171)
*Jun Chen,Yonghua Yu,Weifu Li,Yaohui Chen,Hong Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于聚类引导的自监督多层对比表示学习（CMCRL）算法，用于柑橘病害检测与分类，显著提高了准确率并减少了标注数据的需求。


<details>
  <summary>Details</summary>
Motivation: 柑橘是全球重要的经济作物，病害严重影响产量。传统深度学习方法依赖大量标注数据，而CMCRL通过自监督学习减少标注需求，同时提升分类性能。

Method: 引入两种关键设计：聚类中心对比和多层对比训练（MCT），通过自监督学习优化未标注样本，适应病害症状相似性，并实现分层特征表示学习。

Result: 在公开数据集CDD上，CMCRL的准确率比现有方法高出4.5%-30.1%，并在F1分数、精确率和召回率等指标上表现优异。

Conclusion: CMCRL在减少标注需求的同时，实现了与全监督方法接近的性能，为柑橘病害检测提供了高效解决方案。

Abstract: Citrus, as one of the most economically important fruit crops globally,
suffers severe yield depressions due to various diseases. Accurate disease
detection and classification serve as critical prerequisites for implementing
targeted control measures. Recent advancements in artificial intelligence,
particularly deep learning-based computer vision algorithms, have substantially
decreased time and labor requirements while maintaining the accuracy of
detection and classification. Nevertheless, these methods predominantly rely on
massive, high-quality annotated training examples to attain promising
performance. By introducing two key designs: contrasting with cluster centroids
and a multi-layer contrastive training (MCT) paradigm, this paper proposes a
novel clustering-guided self-supervised multi-layer contrastive representation
learning (CMCRL) algorithm. The proposed method demonstrates several advantages
over existing counterparts: (1) optimizing with massive unannotated samples;
(2) effective adaptation to the symptom similarity across distinct citrus
diseases; (3) hierarchical feature representation learning. The proposed method
achieves state-of-the-art performance on the public citrus image set CDD,
outperforming existing methods by 4.5\%-30.1\% accuracy. Remarkably, our method
narrows the performance gap with fully supervised counterparts (all samples are
labeled). Beyond classification accuracy, our method shows great performance on
other evaluation metrics (F1 score, precision, and recall), highlighting the
robustness against the class imbalance challenge.

</details>


### [44] [How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study](https://arxiv.org/abs/2507.11200)
*Che Liu,Jiazhen Pan,Weixiang Shen,Wenjia Bai,Daniel Rueckert,Rossella Arcucci*

Main category: cs.CV

TL;DR: 评估开源通用和医学专用视觉语言模型在医疗任务中的表现，发现通用模型在某些任务上优于医学专用模型，但推理能力不足，临床可靠性未达标。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在医疗任务中的能力，填补其医学应用研究的空白。

Method: 评估3B至72B参数的模型在八个医疗基准测试中的表现，分理解和推理两部分分析。

Result: 通用模型在部分任务上表现优于医学专用模型；推理能力普遍较弱；不同基准测试表现差异大。

Conclusion: 需加强多模态对齐和更严格的评估协议，以提高模型的临床可靠性。

Abstract: Vision-Language Models (VLMs) trained on web-scale corpora excel at natural
image tasks and are increasingly repurposed for healthcare; however, their
competence in medical tasks remains underexplored. We present a comprehensive
evaluation of open-source general-purpose and medically specialised VLMs,
ranging from 3B to 72B parameters, across eight benchmarks: MedXpert,
OmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model
performance across different aspects, we first separate it into understanding
and reasoning components. Three salient findings emerge. First, large
general-purpose models already match or surpass medical-specific counterparts
on several benchmarks, demonstrating strong zero-shot transfer from natural to
medical images. Second, reasoning performance is consistently lower than
understanding, highlighting a critical barrier to safe decision support. Third,
performance varies widely across benchmarks, reflecting differences in task
design, annotation quality, and knowledge demands. No model yet reaches the
reliability threshold for clinical deployment, underscoring the need for
stronger multimodal alignment and more rigorous, fine-grained evaluation
protocols.

</details>


### [45] [A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition](https://arxiv.org/abs/2507.11202)
*Xinkui Zhao,Jinsong Shu,Yangyang Wu,Guanjie Cheng,Zihe Liu,Naibo Wang,Shuiguang Deng,Zhongle Xie,Jianwei Yin*

Main category: cs.CV

TL;DR: MCULoRA是一种针对不完整多模态学习的参数高效训练框架，通过解耦模态组合的共享信息和动态调整训练比例，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态情感识别中因传感器故障或隐私保护导致的不完整模态问题，避免现有方法中训练梯度冲突的局限性。

Method: 提出MCULoRA框架，包含模态组合感知低秩适应（MCLA）和动态参数微调（DPFT）模块，分别解耦模态信息和优化训练比例。

Result: 在多个基准数据集上的实验表明，MCULoRA在下游任务准确率上显著优于现有方法。

Conclusion: MCULoRA有效解决了不完整多模态学习中的梯度冲突问题，提升了模型性能。

Abstract: Multimodal Emotion Recognition (MER) often encounters incomplete
multimodality in practical applications due to sensor failures or privacy
protection requirements. While existing methods attempt to address various
incomplete multimodal scenarios by balancing the training of each modality
combination through additional gradients, these approaches face a critical
limitation: training gradients from different modality combinations conflict
with each other, ultimately degrading the performance of the final prediction
model. In this paper, we propose a unimodal decoupled dynamic low-rank
adaptation method based on modality combinations, named MCULoRA, which is a
novel framework for the parameter-efficient training of incomplete multimodal
learning models. MCULoRA consists of two key modules, modality combination
aware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The
MCLA module effectively decouples the shared information from the distinct
characteristics of individual modality combinations. The DPFT module adjusts
the training ratio of modality combinations based on the separability of each
modality's representation space, optimizing the learning efficiency across
different modality combinations. Our extensive experimental evaluation in
multiple benchmark datasets demonstrates that MCULoRA substantially outperforms
previous incomplete multimodal learning approaches in downstream task accuracy.

</details>


### [46] [NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models](https://arxiv.org/abs/2507.11245)
*X. Feng,H. Yu,M. Wu,S. Hu,J. Chen,C. Zhu,J. Wu,X. Chu,K. Huang*

Main category: cs.CV

TL;DR: 提出了首个针对长视频生成模型的叙事表达能力评估基准NarrLV，结合电影叙事理论设计自动提示生成和评估指标，实验表明其与人类判断一致。


<details>
  <summary>Details</summary>
Motivation: 现有长视频生成模型缺乏专门评估叙事表达能力的基准，导致评估不全面。

Method: 引入Temporal Narrative Atom（TNA）作为基本叙事单元，设计自动提示生成管道和基于MLLM的评估指标。

Result: 实验证明评估指标与人类判断一致，揭示了当前视频生成模型在叙事表达上的能力边界。

Conclusion: NarrLV为长视频生成模型的叙事表达能力提供了首个全面评估工具。

Abstract: With the rapid development of foundation video generation technologies, long
video generation models have exhibited promising research potential thanks to
expanded content creation space. Recent studies reveal that the goal of long
video generation tasks is not only to extend video duration but also to
accurately express richer narrative content within longer videos. However, due
to the lack of evaluation benchmarks specifically designed for long video
generation models, the current assessment of these models primarily relies on
benchmarks with simple narrative prompts (e.g., VBench). To the best of our
knowledge, our proposed NarrLV is the first benchmark to comprehensively
evaluate the Narrative expression capabilities of Long Video generation models.
Inspired by film narrative theory, (i) we first introduce the basic narrative
unit maintaining continuous visual presentation in videos as Temporal Narrative
Atom (TNA), and use its count to quantitatively measure narrative richness.
Guided by three key film narrative elements influencing TNA changes, we
construct an automatic prompt generation pipeline capable of producing
evaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based
on the three progressive levels of narrative content expression, we design an
effective evaluation metric using the MLLM-based question generation and
answering framework. (iii) Finally, we conduct extensive evaluations on
existing long video generation models and the foundation generation models.
Experimental results demonstrate that our metric aligns closely with human
judgments. The derived evaluation outcomes reveal the detailed capability
boundaries of current video generation models in narrative content expression.

</details>


### [47] [Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone](https://arxiv.org/abs/2507.11247)
*Veronika Shilova,Emmanuel Malherbe,Giovanni Palma,Laurent Risser,Jean-Michel Loubes*

Main category: cs.CV

TL;DR: 提出了一种基于公平性的连续敏感属性分组方法，通过最大化组间歧视差异的新标准，识别关键子群体，并在实验中验证了其有效性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统公平性评估方法在连续敏感属性（如肤色）下可能忽略少数群体的歧视问题，需改进分组方式以更细致地揭示歧视模式。

Method: 提出基于歧视水平的公平性分组方法，通过最大化组间歧视方差的分割标准识别关键子群体，并应用于去偏任务。

Result: 实验证明该方法能揭示更细致的歧视模式，且在去偏任务中保持准确性，适用于工业部署。

Conclusion: 该方法有效解决了连续敏感属性下的公平性问题，为工业应用提供了可行方案。

Abstract: Within a legal framework, fairness in datasets and models is typically
assessed by dividing observations into predefined groups and then computing
fairness measures (e.g., Disparate Impact or Equality of Odds with respect to
gender). However, when sensitive attributes such as skin color are continuous,
dividing into default groups may overlook or obscure the discrimination
experienced by certain minority subpopulations. To address this limitation, we
propose a fairness-based grouping approach for continuous (possibly
multidimensional) sensitive attributes. By grouping data according to observed
levels of discrimination, our method identifies the partition that maximizes a
novel criterion based on inter-group variance in discrimination, thereby
isolating the most critical subgroups.
  We validate the proposed approach using multiple synthetic datasets and
demonstrate its robustness under changing population distributions - revealing
how discrimination is manifested within the space of sensitive attributes.
Furthermore, we examine a specialized setting of monotonic fairness for the
case of skin color. Our empirical results on both CelebA and FFHQ, leveraging
the skin tone as predicted by an industrial proprietary algorithm, show that
the proposed segmentation uncovers more nuanced patterns of discrimination than
previously reported, and that these findings remain stable across datasets for
a given model. Finally, we leverage our grouping model for debiasing purpose,
aiming at predicting fair scores with group-by-group post-processing. The
results demonstrate that our approach improves fairness while having minimal
impact on accuracy, thus confirming our partition method and opening the door
for industrial deployment.

</details>


### [48] [ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition](https://arxiv.org/abs/2507.11261)
*Ronggang Huang,Haoxin Yang,Yan Cai,Xuemiao Xu,Huaidong Zhang,Shengfeng He*

Main category: cs.CV

TL;DR: ViewSRD框架通过结构化多视角分解解决3D视觉定位中的复杂查询和空间描述不一致问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理复杂多锚点查询和视角变化导致的空间描述不一致问题。

Method: 提出ViewSRD框架，包括Simple Relation Decoupling (SRD)模块、Multi-view Textual-Scene Interaction (Multi-TSI)模块和Textual-Scene Reasoning模块。

Result: 在3D视觉定位数据集上表现优异，尤其在需要精确空间区分的复杂查询中。

Conclusion: ViewSRD通过结构化多视角分解有效解决了3D视觉定位中的挑战。

Abstract: 3D visual grounding aims to identify and localize objects in a 3D space based
on textual descriptions. However, existing methods struggle with disentangling
targets from anchors in complex multi-anchor queries and resolving
inconsistencies in spatial descriptions caused by perspective variations. To
tackle these challenges, we propose ViewSRD, a framework that formulates 3D
visual grounding as a structured multi-view decomposition process. First, the
Simple Relation Decoupling (SRD) module restructures complex multi-anchor
queries into a set of targeted single-anchor statements, generating a
structured set of perspective-aware descriptions that clarify positional
relationships. These decomposed representations serve as the foundation for the
Multi-view Textual-Scene Interaction (Multi-TSI) module, which integrates
textual and scene features across multiple viewpoints using shared, Cross-modal
Consistent View Tokens (CCVTs) to preserve spatial correlations. Finally, a
Textual-Scene Reasoning module synthesizes multi-view predictions into a
unified and robust 3D visual grounding. Experiments on 3D visual grounding
datasets show that ViewSRD significantly outperforms state-of-the-art methods,
particularly in complex queries requiring precise spatial differentiation.

</details>


### [49] [YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery](https://arxiv.org/abs/2507.11267)
*Aon Safdar,Usman Akram,Waseem Anwar,Basit Malik,Mian Ibad Ali*

Main category: cs.CV

TL;DR: 论文提出了一种改进的单阶段检测器YOLOatr，用于热红外图像中的目标检测与识别，解决了现有深度学习模型在该领域的性能不足问题。


<details>
  <summary>Details</summary>
Motivation: 热红外图像在国防和监控领域的自动目标检测与识别任务面临诸多挑战，如数据集有限、硬件限制、天气影响等，导致现有先进模型表现不佳。

Method: 基于改进的YOLOv5s，优化了检测头、特征融合和自定义数据增强策略，提出了YOLOatr模型。

Result: 在DSIAC MWIR数据集上测试，YOLOatr实现了高达99.6%的识别准确率，优于现有方法。

Conclusion: YOLOatr在热红外图像目标识别任务中表现出色，为实时应用提供了高效解决方案。

Abstract: Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared
(TI) imagery in the defense and surveillance domain is a challenging computer
vision (CV) task in comparison to the commercial autonomous vehicle perception
domain. Limited datasets, peculiar domain-specific and TI modality-specific
challenges, i.e., limited hardware, scale invariance issues due to greater
distances, deliberate occlusion by tactical vehicles, lower sensor resolution
and resultant lack of structural information in targets, effects of weather,
temperature, and time of day variations, and varying target to clutter ratios
all result in increased intra-class variability and higher inter-class
similarity, making accurate real-time ATR a challenging CV task. Resultantly,
contemporary state-of-the-art (SOTA) deep learning architectures underperform
in the ATR domain. We propose a modified anchor-based single-stage detector,
called YOLOatr, based on a modified YOLOv5s, with optimal modifications to the
detection heads, feature fusion in the neck, and a custom augmentation profile.
We evaluate the performance of our proposed model on a comprehensive DSIAC MWIR
dataset for real-time ATR over both correlated and decorrelated testing
protocols. The results demonstrate that our proposed model achieves
state-of-the-art ATR performance of up to 99.6%.

</details>


### [50] [Tomato Multi-Angle Multi-Pose Dataset for Fine-Grained Phenotyping](https://arxiv.org/abs/2507.11279)
*Yujie Zhang,Sabine Struckmeyer,Andreas Kolb,Sven Reichardt*

Main category: cs.CV

TL;DR: TomatoMAP是一个基于物联网的番茄植物表型数据集，包含64,464张RGB图像和精细标注，通过深度学习框架验证其准确性。


<details>
  <summary>Details</summary>
Motivation: 传统植物表型分析方法存在观察者偏见和不一致性问题，影响准确性和可重复性。

Method: 开发TomatoMAP数据集，使用IoT成像系统和标准化协议采集数据，结合深度学习模型（MobileNetv3、YOLOv11、MaskRCNN）进行验证。

Result: 模型在精细表型分析中达到与专家相当的准确性和速度，Cohen's Kappa和热图验证了方法的可靠性。

Conclusion: TomatoMAP为植物表型研究提供了高精度、可重复的数据集和自动化分析框架。

Abstract: Observer bias and inconsistencies in traditional plant phenotyping methods
limit the accuracy and reproducibility of fine-grained plant analysis. To
overcome these challenges, we developed TomatoMAP, a comprehensive dataset for
Solanum lycopersicum using an Internet of Things (IoT) based imaging system
with standardized data acquisition protocols. Our dataset contains 64,464 RGB
images that capture 12 different plant poses from four camera elevation angles.
Each image includes manually annotated bounding boxes for seven regions of
interest (ROIs), including leaves, panicle, batch of flowers, batch of fruits,
axillary shoot, shoot and whole plant area, along with 50 fine-grained growth
stage classifications based on the BBCH scale. Additionally, we provide 3,616
high-resolution image subset with pixel-wise semantic and instance segmentation
annotations for fine-grained phenotyping. We validated our dataset using a
cascading model deep learning framework combining MobileNetv3 for
classification, YOLOv11 for object detection, and MaskRCNN for segmentation.
Through AI vs. Human analysis involving five domain experts, we demonstrate
that the models trained on our dataset achieve accuracy and speed comparable to
the experts. Cohen's Kappa and inter-rater agreement heatmap confirm the
reliability of automated fine-grained phenotyping using our approach.

</details>


### [51] [Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers](https://arxiv.org/abs/2507.11287)
*An-Lun Liu,Yu-Wei Chao,Yi-Ting Chen*

Main category: cs.CV

TL;DR: 论文提出了一种任务导向的人体抓取合成方法，通过任务感知接触图结合场景和任务信息，显著提升了抓取质量和任务表现。


<details>
  <summary>Details</summary>
Motivation: 传统抓取合成方法仅考虑物体与手的关系，缺乏对场景和任务的综合考量，限制了抓取的准确性和任务适应性。

Method: 采用两阶段流程：首先生成任务感知接触图，随后基于该图合成任务导向的抓取姿势。

Result: 实验验证了方法的有效性，在抓取质量和任务表现上优于现有方法。

Conclusion: 任务和场景信息的综合建模对提升抓取合成的准确性和任务适应性至关重要。

Abstract: In this paper, we study task-oriented human grasp synthesis, a new grasp
synthesis task that demands both task and context awareness. At the core of our
method is the task-aware contact maps. Unlike traditional contact maps that
only reason about the manipulated object and its relation with the hand, our
enhanced maps take into account scene and task information. This comprehensive
map is critical for hand-object interaction, enabling accurate grasping poses
that align with the task. We propose a two-stage pipeline that first constructs
a task-aware contact map informed by the scene and task. In the subsequent
stage, we use this contact map to synthesize task-oriented human grasps. We
introduce a new dataset and a metric for the proposed task to evaluate our
approach. Our experiments validate the importance of modeling both scene and
task, demonstrating significant improvements over existing methods in both
grasp quality and task performance. See our project page for more details:
https://hcis-lab.github.io/TOHGS/

</details>


### [52] [Detección y Cuantificación de Erosión Fluvial con Visión Artificial](https://arxiv.org/abs/2507.11301)
*Paúl Maji,Marlon Túquerres,Stalin Valencia,Marcela Valenzuela,Christian Mejia-Escobar*

Main category: cs.CV

TL;DR: 提出了一种基于AI的方法，用于自动识别侵蚀区域并估算其面积，通过YOLOv11模型和LiDAR图像实现高效检测，开发了交互式应用EROSCAN。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要专业知识且处理繁琐，希望通过AI技术优化侵蚀检测和量化。

Method: 使用YOLOv11模型，结合LiDAR图像和照片，通过Roboflow平台进行数据标注和训练。

Result: 模型准确率为70%，能精确识别侵蚀区域并计算面积，开发了EROSCAN应用。

Conclusion: 该方法优化了侵蚀检测和量化，为风险管理和规划提供了便利。

Abstract: Fluvial erosion is a natural process that can generate significant impacts on
soil stability and strategic infrastructures. The detection and monitoring of
this phenomenon is traditionally addressed by photogrammetric methods and
analysis in geographic information systems. These tasks require specific
knowledge and intensive manual processing. This study proposes an artificial
intelligence-based approach for automatic identification of eroded zones and
estimation of their area. The state-of-the-art computer vision model YOLOv11,
adjusted by fine-tuning and trained with photographs and LiDAR images, is used.
This combined dataset was segmented and labeled using the Roboflow platform.
Experimental results indicate efficient detection of erosion patterns with an
accuracy of 70%, precise identification of eroded areas and reliable
calculation of their extent in pixels and square meters. As a final product,
the EROSCAN system has been developed, an interactive web application that
allows users to upload images and obtain automatic segmentations of fluvial
erosion, together with the estimated area. This tool optimizes the detection
and quantification of the phenomenon, facilitating decision making in risk
management and territorial planning.

</details>


### [53] [A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction](https://arxiv.org/abs/2507.11321)
*Haoxuan Qu,Yujun Cai,Hossein Rahmani,Ajay Kumar,Junsong Yuan,Jun Liu*

Main category: cs.CV

TL;DR: 论文提出了一种新的高斯泼溅框架，首次在表面重建过程中引入多种几何基元，提高了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有高斯泼溅方法仅使用单一类型基元（椭圆或椭球）表示复杂多样的物体表面，限制了重建质量。

Method: 提出组合泼溅策略、混合基元初始化策略和顶点修剪机制，支持多种基元的高斯泼溅流程。

Result: 实验证明该框架能有效提升表面重建的准确性。

Conclusion: 通过引入多种几何基元，新框架显著改善了高斯泼溅的表面重建性能。

Abstract: Recently, Gaussian Splatting (GS) has received a lot of attention in surface
reconstruction. However, while 3D objects can be of complex and diverse shapes
in the real world, existing GS-based methods only limitedly use a single type
of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent
object surfaces during their reconstruction. In this paper, we highlight that
this can be insufficient for object surfaces to be represented in high quality.
Thus, we propose a novel framework that, for the first time, enables Gaussian
Splatting to incorporate multiple types of (geometrical) primitives during its
surface reconstruction process. Specifically, in our framework, we first
propose a compositional splatting strategy, enabling the splatting and
rendering of different types of primitives in the Gaussian Splatting pipeline.
In addition, we also design our framework with a mixed-primitive-based
initialization strategy and a vertex pruning mechanism to further promote its
surface representation learning process to be well executed leveraging
different types of primitives. Extensive experiments show the efficacy of our
framework and its accurate surface reconstruction performance.

</details>


### [54] [MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network](https://arxiv.org/abs/2507.11333)
*Jianfei Jiang,Qiankun Liu,Haochen Yu,Hongyuan Liu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: MonoMVSNet integrates monocular depth and feature priors into MVS to improve performance in challenging regions like textureless and reflective surfaces.


<details>
  <summary>Details</summary>
Motivation: Existing MVS methods struggle with feature matching in difficult regions, while monocular depth estimation is robust there.

Method: Combines monocular features and depth with MVS via attention and dynamic depth candidate updates, supervised by a relative consistency loss.

Result: Achieves state-of-the-art performance on DTU and Tanks-and-Temples datasets, ranking first on Intermediate and Advanced benchmarks.

Conclusion: MonoMVSNet effectively bridges the gap between monocular depth estimation and MVS, enhancing robustness in challenging scenarios.

Abstract: Learning-based Multi-View Stereo (MVS) methods aim to predict depth maps for
a sequence of calibrated images to recover dense point clouds. However,
existing MVS methods often struggle with challenging regions, such as
textureless regions and reflective surfaces, where feature matching fails. In
contrast, monocular depth estimation inherently does not require feature
matching, allowing it to achieve robust relative depth estimation in these
regions. To bridge this gap, we propose MonoMVSNet, a novel monocular feature
and depth guided MVS network that integrates powerful priors from a monocular
foundation model into multi-view geometry. Firstly, the monocular feature of
the reference view is integrated into source view features by the attention
mechanism with a newly designed cross-view position encoding. Then, the
monocular depth of the reference view is aligned to dynamically update the
depth candidates for edge regions during the sampling procedure. Finally, a
relative consistency loss is further designed based on the monocular depth to
supervise the depth prediction. Extensive experiments demonstrate that
MonoMVSNet achieves state-of-the-art performance on the DTU and
Tanks-and-Temples datasets, ranking first on the Tanks-and-Temples Intermediate
and Advanced benchmarks. The source code is available at
https://github.com/JianfeiJ/MonoMVSNet.

</details>


### [55] [UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks](https://arxiv.org/abs/2507.11336)
*Peiran Wu,Yunze Liu,Zhengdong Zhu,Enmin Zhou,Shawn Shen*

Main category: cs.CV

TL;DR: 论文提出了UGC-VideoCap，一个专注于音频和视觉模态平衡整合的新基准和模型框架，用于短格式用户生成视频的详细多模态字幕生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频字幕基准和模型主要依赖视觉信息，忽视了音频在传达场景动态、说话者意图和叙事背景中的关键作用。

Method: 提出了UGC-VideoCap基准，包含1000个TikTok视频和4000个QA对，并设计了UGC-VideoCaptioner(3B)模型，采用两阶段训练策略。

Result: 模型在有限数据下实现了高效适应，并保持了竞争力。

Conclusion: UGC-VideoCap为无约束真实场景下的多模态视频字幕生成提供了高质量基准和数据高效解决方案。

Abstract: Real-world user-generated videos, especially on platforms like TikTok, often
feature rich and intertwined audio visual content. However, existing video
captioning benchmarks and models remain predominantly visual centric,
overlooking the crucial role of audio in conveying scene dynamics, speaker
intent, and narrative context. This lack of omni datasets and lightweight,
capable models hampers progress in fine grained, multimodal video
understanding. To address these challenges, we introduce UGC-VideoCap, a new
benchmark and model framework specifically designed for detailed omnimodal
captioning of short form user-generated videos. Unlike prior datasets,
UGC-VideoCap emphasizes balanced integration of audio and visual modalities,
featuring 1000 TikTok videos annotated through a structured three stage
human-in-the-loop pipeline covering audio only, visual only, and joint audio
visual semantics. The benchmark also includes 4000 carefully crafted QA pairs
probing both unimodal and cross modal understanding. Alongside the dataset, we
propose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from
Gemini 2.5 Flash. Using a novel two-stage training strategy supervised fine
tuning followed by Group Relative Policy Optimization (GRPO), our approach
enables efficient adaptation from limited data while maintaining competitive
performance. Together, our benchmark and model offer a high-quality foundation
and a data-efficient solution for advancing omnimodal video captioning in
unconstrained real-world UGC settings.

</details>


### [56] [Attributes Shape the Embedding Space of Face Recognition Models](https://arxiv.org/abs/2507.11372)
*Pierrick Leroy,Antonio Mastropietro,Marco Nurisso,Francesco Vaccarino*

Main category: cs.CV

TL;DR: 论文提出了一种几何方法，用于分析人脸识别模型对不同面部和图像属性的依赖性或不变性，并引入了一种物理启发的对齐度量。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络在人脸识别任务中取得了显著进展，但现有方法主要关注身份信息，忽略了嵌入空间中的多尺度几何结构。

Method: 提出了一种几何方法，结合物理启发的对齐度量，评估模型对不同属性的依赖性或不变性。

Result: 研究发现，模型对不同属性表现出不同程度的鲁棒性，揭示了其优势和局限性。

Conclusion: 该方法为模型提供了更深的可解释性，有助于理解其行为。

Abstract: Face Recognition (FR) tasks have made significant progress with the advent of
Deep Neural Networks, particularly through margin-based triplet losses that
embed facial images into high-dimensional feature spaces. During training,
these contrastive losses focus exclusively on identity information as labels.
However, we observe a multiscale geometric structure emerging in the embedding
space, influenced by interpretable facial (e.g., hair color) and image
attributes (e.g., contrast). We propose a geometric approach to describe the
dependence or invariance of FR models to these attributes and introduce a
physics-inspired alignment metric. We evaluate the proposed metric on
controlled, simplified models and widely used FR models fine-tuned with
synthetic data for targeted attribute augmentation. Our findings reveal that
the models exhibit varying degrees of invariance across different attributes,
providing insight into their strengths and weaknesses and enabling deeper
interpretability. Code available here:
https://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs

</details>


### [57] [Implementing Adaptations for Vision AutoRegressive Model](https://arxiv.org/abs/2507.11441)
*Kaif Shaikh,Antoni Kowalczuk,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: VAR在图像生成领域作为Diffusion Models的替代方案，本文探讨其适应性和差分隐私（DP）适应性，发现VAR在非DP适应性上优于DMs，但在DP适应性上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究VAR模型的适应性，尤其是针对特定下游任务（如医疗数据生成）的微调，以及差分隐私适应性，填补VAR在这些领域的空白。

Method: 实现并比较多种VAR适应策略，与最先进的DM适应策略进行对比。

Result: VAR在非DP适应性上优于DMs，但在DP适应性上表现较差。

Conclusion: VAR在非DP任务中表现优异，但需进一步研究其DP适应性。代码已开源。

Abstract: Vision AutoRegressive model (VAR) was recently introduced as an alternative
to Diffusion Models (DMs) in image generation domain. In this work we focus on
its adaptations, which aim to fine-tune pre-trained models to perform specific
downstream tasks, like medical data generation. While for DMs there exist many
techniques, adaptations for VAR remain underexplored. Similarly, differentially
private (DP) adaptations-ones that aim to preserve privacy of the adaptation
data-have been extensively studied for DMs, while VAR lacks such solutions. In
our work, we implement and benchmark many strategies for VAR, and compare them
to state-of-the-art DM adaptation strategies. We observe that VAR outperforms
DMs for non-DP adaptations, however, the performance of DP suffers, which
necessitates further research in private adaptations for VAR. Code is available
at https://github.com/sprintml/finetuning_var_dp.

</details>


### [58] [COLI: A Hierarchical Efficient Compressor for Large Images](https://arxiv.org/abs/2507.11443)
*Haoran Wang,Hanyu Pei,Yang Lyu,Kai Zhang,Li Li,Feng-Lei Fan*

Main category: cs.CV

TL;DR: COLI框架通过NeRV加速INR压缩，结合预训练和超压缩技术，显著提升大图像压缩效率和比率。


<details>
  <summary>Details</summary>
Motivation: 高分辨率大视场图像的压缩需求增加，传统方法难以保留细节，数据驱动方法泛化性差。

Method: 采用NeRV加速INR压缩，引入预训练-微调范式、混合精度训练和并行化目标；提出超压缩技术提升压缩比。

Result: 在医学影像数据上，COLI在PSNR和SSIM上表现优异，压缩比显著提升，训练速度加快4倍。

Conclusion: COLI为高效压缩大图像提供了新思路，解决了INR压缩的速度和比率问题。

Abstract: The escalating adoption of high-resolution, large-field-of-view imagery
amplifies the need for efficient compression methodologies. Conventional
techniques frequently fail to preserve critical image details, while
data-driven approaches exhibit limited generalizability. Implicit Neural
Representations (INRs) present a promising alternative by learning continuous
mappings from spatial coordinates to pixel intensities for individual images,
thereby storing network weights rather than raw pixels and avoiding the
generalization problem. However, INR-based compression of large images faces
challenges including slow compression speed and suboptimal compression ratios.
To address these limitations, we introduce COLI (Compressor for Large Images),
a novel framework leveraging Neural Representations for Videos (NeRV). First,
recognizing that INR-based compression constitutes a training process, we
accelerate its convergence through a pretraining-finetuning paradigm,
mixed-precision training, and reformulation of the sequential loss into a
parallelizable objective. Second, capitalizing on INRs' transformation of image
storage constraints into weight storage, we implement Hyper-Compression, a
novel post-training technique to substantially enhance compression ratios while
maintaining minimal output distortion. Evaluations across two medical imaging
datasets demonstrate that COLI consistently achieves competitive or superior
PSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while
accelerating NeRV training by up to 4 times.

</details>


### [59] [HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing](https://arxiv.org/abs/2507.11474)
*Pan Du,Mingqi Xu,Xiaozhi Zhu,Jian-xun Wang*

Main category: cs.CV

TL;DR: HUG-VAS是一种基于NURBS和扩散模型的血管几何生成方法，能够合成真实的多分支主动脉结构，支持零样本条件生成。


<details>
  <summary>Details</summary>
Motivation: 传统统计形状建模方法受限于线性假设，难以处理复杂血管拓扑结构，因此需要更灵活的生成模型。

Method: HUG-VAS结合NURBS参数化和分层扩散模型，首先生成中心线，再根据中心线生成径向轮廓。

Result: 模型生成的主动脉几何与原始数据集高度一致，支持零样本条件生成，适用于多种实际应用。

Conclusion: HUG-VAS首次通过NURBS和分层扩散模型将图像先验与生成形状建模统一起来，具有重要应用价值。

Abstract: Accurate characterization of vascular geometry is essential for
cardiovascular diagnosis and treatment planning. Traditional statistical shape
modeling (SSM) methods rely on linear assumptions, limiting their expressivity
and scalability to complex topologies such as multi-branch vascular structures.
We introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular
geometry Synthesis, which integrates NURBS surface parameterization with
diffusion-based generative modeling to synthesize realistic, fine-grained
aortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates
anatomically faithful aortas with supra-aortic branches, yielding biomarker
distributions that closely match those of the original dataset. HUG-VAS adopts
a hierarchical architecture comprising a denoising diffusion model that
generates centerlines and a guided diffusion model that synthesizes radial
profiles conditioned on those centerlines, thereby capturing two layers of
anatomical variability. Critically, the framework supports zero-shot
conditional generation from image-derived priors, enabling practical
applications such as interactive semi-automatic segmentation, robust
reconstruction under degraded imaging conditions, and implantable device
optimization. To our knowledge, HUG-VAS is the first SSM framework to bridge
image-derived priors with generative shape modeling via a unified integration
of NURBS parameterization and hierarchical diffusion processes.

</details>


### [60] [C-FBI: A Combinatorial method using Convolutions for Circle Fitting in Blurry Images](https://arxiv.org/abs/2507.11476)
*Esteban Román Catafau,Torbjörn E. M. Nordling*

Main category: cs.CV

TL;DR: 3C-FBI算法通过组合边缘像素采样和卷积密度估计，在模糊图像中实现高精度圆检测和拟合，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决在退化成像条件下鲁棒圆检测和拟合的计算机视觉挑战。

Method: 结合高效的组合边缘像素采样和参数空间中的卷积密度估计。

Result: 在真实医学数据和合成数据中表现优异，Jaccard指数达0.896，实时性能40.3 fps，优于RCD等方法。

Conclusion: 3C-FBI在精度、速度和鲁棒性上表现突出，适用于医疗影像、机器人和工业检测。

Abstract: This paper addresses the fundamental computer vision challenge of robust
circle detection and fitting in degraded imaging conditions. We present
Combinatorial Convolution-based Circle Fitting for Blurry Images (3C-FBI), an
algorithm that bridges the gap between circle detection and precise parametric
fitting by combining (1) efficient combinatorial edge pixel (edgel) sampling
and (2) convolution-based density estimation in parameter space.
  We evaluate 3C-FBI across three experimental frameworks: (1) real-world
medical data from Parkinson's disease assessments (144 frames from 36 videos),
(2) controlled synthetic data following established circle-fitting benchmarks,
and (3) systematic analysis across varying spatial resolutions and outlier
contamination levels. Results show that 3C-FBI achieves state-of-the-art
accuracy (Jaccard index 0.896) while maintaining real-time performance (40.3
fps), significantly outperforming classical methods like RCD (6.8 fps) on a
standard CPU (i7-10875H). It maintains near-perfect accuracy (Jaccard almost
1.0) at high resolutions (480x480) and reliable performance (Jaccard higher
than 0.95) down to 160x160 with up to 20% outliers.
  In extensive synthetic testing, 3C-FBI achieves a mean Jaccard Index of 0.989
across contamination levels, comparable to modern methods like Qi et al. (2024,
0.991), and surpassing RHT (0.964). This combination of accuracy, speed, and
robustness makes 3C-FBI ideal for medical imaging, robotics, and industrial
inspection under challenging conditions.

</details>


### [61] [COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation](https://arxiv.org/abs/2507.11488)
*Pakizar Shamoi,Nuray Toganas,Muragul Muratbekova,Elnara Kadyrgali,Adilet Yerkin,Ayan Igali,Malika Ziyada,Ayana Adilova,Aron Karatayev,Yerdauit Torekhan*

Main category: cs.CV

TL;DR: 论文提出了一种基于人类感知的模糊颜色模型COLIBRI，通过模糊集和逻辑构建颜色分类框架，实验验证其优于传统颜色模型。


<details>
  <summary>Details</summary>
Motivation: 解决计算机难以模仿人类颜色感知的问题，填补计算颜色表示与人类视觉感知之间的鸿沟。

Method: 采用三阶段实验方法：初步实验确定可区分颜色刺激，大规模人类分类调查（1000+受试者），提取模糊分区并生成隶属函数。

Result: 模型与传统颜色模型（RGB、HSV、LAB）相比，更符合人类感知，且支持基于反馈和上下文的自适应。

Conclusion: COLIBRI模型在设计和人机交互等领域具有重要意义，是首个基于大规模人类样本构建的颜色属性规范模型。

Abstract: Colors are omnipresent in today's world and play a vital role in how humans
perceive and interact with their surroundings. However, it is challenging for
computers to imitate human color perception. This paper introduces the Human
Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based
Representation and Interpretation), designed to bridge the gap between
computational color representations and human visual perception. The proposed
model uses fuzzy sets and logic to create a framework for color categorization.
Using a three-phase experimental approach, the study first identifies
distinguishable color stimuli for hue, saturation, and intensity through
preliminary experiments, followed by a large-scale human categorization survey
involving more than 1000 human subjects. The resulting data are used to extract
fuzzy partitions and generate membership functions that reflect real-world
perceptual uncertainty. The model incorporates a mechanism for adaptation that
allows refinement based on feedback and contextual changes. Comparative
evaluations demonstrate the model's alignment with human perception compared to
traditional color models, such as RGB, HSV, and LAB. To the best of our
knowledge, no previous research has documented the construction of a model for
color attribute specification based on a sample of this size or a comparable
sample of the human population (n = 2496). Our findings are significant for
fields such as design, artificial intelligence, marketing, and human-computer
interaction, where perceptually relevant color representation is critical.

</details>


### [62] [CATVis: Context-Aware Thought Visualization](https://arxiv.org/abs/2507.11522)
*Tariq Mehmood,Hamza Ahmad,Muhammad Haroon Shakeel,Murtaza Taj*

Main category: cs.CV

TL;DR: 提出了一种新颖的5阶段框架，用于从EEG信号解码视觉表示，并通过实验验证了其优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: EEG信号解码视觉表示具有挑战性，因其复杂且噪声多，需要一种更有效的方法。

Method: 5阶段框架：EEG编码器、跨模态对齐、标题重排、加权插值和图像生成。

Result: 方法在分类准确率、生成准确率和图像质量上显著优于现有方法。

Conclusion: 该框架在EEG到图像的生成任务中表现出色，具有实际应用潜力。

Abstract: EEG-based brain-computer interfaces (BCIs) have shown promise in various
applications, such as motor imagery and cognitive state monitoring. However,
decoding visual representations from EEG signals remains a significant
challenge due to their complex and noisy nature. We thus propose a novel
5-stage framework for decoding visual representations from EEG signals: (1) an
EEG encoder for concept classification, (2) cross-modal alignment of EEG and
text embeddings in CLIP feature space, (3) caption refinement via re-ranking,
(4) weighted interpolation of concept and caption embeddings for richer
semantics, and (5) image generation using a pre-trained Stable Diffusion model.
We enable context-aware EEG-to-image generation through cross-modal alignment
and re-ranking. Experimental results demonstrate that our method generates
high-quality images aligned with visual stimuli, outperforming SOTA approaches
by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and
reducing Fr\'echet Inception Distance by 36.61%, indicating superior semantic
alignment and image quality.

</details>


### [63] [CharaConsist: Fine-Grained Consistent Character Generation](https://arxiv.org/abs/2507.11533)
*Mengyu Wang,Henghui Ding,Jianing Peng,Yao Zhao,Yunpeng Chen,Yunchao Wei*

Main category: cs.CV

TL;DR: CharaConsist提出了一种文本到图像生成方法，通过点跟踪注意力和自适应令牌合并，解决了现有方法在背景细节和角色一致性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成一致内容时无法保持背景细节和角色一致性，限制了实际应用。

Method: 使用点跟踪注意力和自适应令牌合并，结合前景和背景的解耦控制。

Result: CharaConsist能够生成高质量且一致的视觉输出，适用于多种场景。

Conclusion: CharaConsist是首个针对DiT模型的文本到图像一致生成方法，具有广泛的实际应用潜力。

Abstract: In text-to-image generation, producing a series of consistent contents that
preserve the same identity is highly valuable for real-world applications.
Although a few works have explored training-free methods to enhance the
consistency of generated subjects, we observe that they suffer from the
following problems. First, they fail to maintain consistent background details,
which limits their applicability. Furthermore, when the foreground character
undergoes large motion variations, inconsistencies in identity and clothing
details become evident. To address these problems, we propose CharaConsist,
which employs point-tracking attention and adaptive token merge along with
decoupled control of the foreground and background. CharaConsist enables
fine-grained consistency for both foreground and background, supporting the
generation of one character in continuous shots within a fixed scene or in
discrete shots across different scenes. Moreover, CharaConsist is the first
consistent generation method tailored for text-to-image DiT model. Its ability
to maintain fine-grained consistency, combined with the larger capacity of
latest base model, enables it to produce high-quality visual outputs,
broadening its applicability to a wider range of real-world scenarios. The
source code has been released at https://github.com/Murray-Wang/CharaConsist

</details>


### [64] [Streaming 4D Visual Geometry Transformer](https://arxiv.org/abs/2507.11539)
*Dong Zhuo,Wenzhao Zheng,Jiahe Guo,Yuqi Wu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 提出了一种流式4D视觉几何变换器，用于实时4D时空几何感知与重建，通过因果注意力和历史缓存实现高效在线处理。


<details>
  <summary>Details</summary>
Motivation: 解决视频中4D时空几何感知与重建的挑战，支持交互式和实时应用。

Method: 采用因果变换器架构，利用时间因果注意力和历史缓存作为隐式记忆，实现流式长期4D重建。训练时从双向视觉几何变换器（VGGT）蒸馏知识，推理时支持高效注意力操作（如FlashAttention）。

Result: 在多个4D几何感知基准测试中表现优异，提升了在线场景的推理速度，同时保持高质量空间一致性。

Conclusion: 该模型为可扩展和交互式4D视觉系统奠定了基础，代码已开源。

Abstract: Perceiving and reconstructing 4D spatial-temporal geometry from videos is a
fundamental yet challenging computer vision task. To facilitate interactive and
real-time applications, we propose a streaming 4D visual geometry transformer
that shares a similar philosophy with autoregressive large language models. We
explore a simple and efficient design and employ a causal transformer
architecture to process the input sequence in an online manner. We use temporal
causal attention and cache the historical keys and values as implicit memory to
enable efficient streaming long-term 4D reconstruction. This design can handle
real-time 4D reconstruction by incrementally integrating historical information
while maintaining high-quality spatial consistency. For efficient training, we
propose to distill knowledge from the dense bidirectional visual geometry
grounded transformer (VGGT) to our causal model. For inference, our model
supports the migration of optimized efficient attention operator (e.g.,
FlashAttention) from the field of large language models. Extensive experiments
on various 4D geometry perception benchmarks demonstrate that our model
increases the inference speed in online scenarios while maintaining competitive
performance, paving the way for scalable and interactive 4D vision systems.
Code is available at: https://github.com/wzzheng/StreamVGGT.

</details>


### [65] [Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation](https://arxiv.org/abs/2507.11540)
*Zhen Xu,Hongyu Zhou,Sida Peng,Haotong Lin,Haoyu Guo,Jiahao Shao,Peishan Yang,Qinglin Yang,Sheng Miao,Xingyi He,Yifan Wang,Yue Wang,Ruizhen Hu,Yiyi Liao,Xiaowei Zhou,Hujun Bao*

Main category: cs.CV

TL;DR: 综述了深度估计领域的发展，探讨了基于视觉的方法的潜力，并提出了构建稳健深度基础模型的路径。


<details>
  <summary>Details</summary>
Motivation: 传统硬件传感器方法成本高且受限，视觉方法虽前景广阔但面临泛化性和稳定性挑战。

Method: 调查了单目、立体、多视图和单目视频设置下的深度学习架构和范式。

Result: 提出了大规模数据集和关键架构，为深度基础模型的发展提供支持。

Conclusion: 深度基础模型有望解决现有挑战，未来研究和应用前景广阔。

Abstract: Depth estimation is a fundamental task in 3D computer vision, crucial for
applications such as 3D reconstruction, free-viewpoint rendering, robotics,
autonomous driving, and AR/VR technologies. Traditional methods relying on
hardware sensors like LiDAR are often limited by high costs, low resolution,
and environmental sensitivity, limiting their applicability in real-world
scenarios. Recent advances in vision-based methods offer a promising
alternative, yet they face challenges in generalization and stability due to
either the low-capacity model architectures or the reliance on domain-specific
and small-scale datasets. The emergence of scaling laws and foundation models
in other domains has inspired the development of "depth foundation models":
deep neural networks trained on large datasets with strong zero-shot
generalization capabilities. This paper surveys the evolution of deep learning
architectures and paradigms for depth estimation across the monocular, stereo,
multi-view, and monocular video settings. We explore the potential of these
models to address existing challenges and provide a comprehensive overview of
large-scale datasets that can facilitate their development. By identifying key
architectures and training strategies, we aim to highlight the path towards
robust depth foundation models, offering insights into their future research
and applications.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [66] [Comparative Analysis of Vision Transformers and Traditional Deep Learning Approaches for Automated Pneumonia Detection in Chest X-Rays](https://arxiv.org/abs/2507.10589)
*Gaurav Singh*

Main category: eess.IV

TL;DR: 比较传统机器学习和深度学习在肺炎检测中的表现，发现Vision Transformers（尤其是Cross-ViT）表现最佳，准确率达88.25%，召回率99.42%。


<details>
  <summary>Details</summary>
Motivation: 肺炎（如COVID-19引起的）是全球健康挑战，需要快速准确的诊断方法。

Method: 评估多种方法，包括传统机器学习（PCA聚类、逻辑回归、支持向量分类）和深度学习（CNN、DenseNet-121、多种ViT架构）。

Result: Cross-ViT表现最优，参数较少但性能超越更大模型。

Conclusion: Vision Transformers在肺炎检测中具有潜力，可提升诊断速度和准确性。

Abstract: Pneumonia, particularly when induced by diseases like COVID-19, remains a
critical global health challenge requiring rapid and accurate diagnosis. This
study presents a comprehensive comparison of traditional machine learning and
state-of-the-art deep learning approaches for automated pneumonia detection
using chest X-rays (CXRs). We evaluate multiple methodologies, ranging from
conventional machine learning techniques (PCA-based clustering, Logistic
Regression, and Support Vector Classification) to advanced deep learning
architectures including Convolutional Neural Networks (Modified LeNet,
DenseNet-121) and various Vision Transformer (ViT) implementations (Deep-ViT,
Compact Convolutional Transformer, and Cross-ViT). Using a dataset of 5,856
pediatric CXR images, we demonstrate that Vision Transformers, particularly the
Cross-ViT architecture, achieve superior performance with 88.25% accuracy and
99.42% recall, surpassing traditional CNN approaches. Our analysis reveals that
architectural choices impact performance more significantly than model size,
with Cross-ViT's 75M parameters outperforming larger models. The study also
addresses practical considerations including computational efficiency, training
requirements, and the critical balance between precision and recall in medical
diagnostics. Our findings suggest that Vision Transformers offer a promising
direction for automated pneumonia detection, potentially enabling more rapid
and accurate diagnosis during health crises.

</details>


### [67] [A Survey on Medical Image Compression: From Traditional to Learning-Based](https://arxiv.org/abs/2507.10615)
*Guofeng Tong,Sixuan Liu,Yang Lv,Hanyu Pei,Feng-Lei Fan*

Main category: eess.IV

TL;DR: 论文探讨了医学图像压缩的挑战，对比了传统与深度学习方法，并提出了基于数据结构和技术的分类。


<details>
  <summary>Details</summary>
Motivation: 医学图像数据激增带来存储和传输挑战，需高效压缩方法以保留诊断细节。

Method: 通过传统数学变换与深度学习方法的对比，建立基于数据结构和技术的分类体系。

Result: 传统方法提供理论支持和标准化，深度学习方法能捕捉复杂统计特征。

Conclusion: 未来医学图像压缩需结合两者优势，解决多维数据的技术挑战。

Abstract: The exponential growth of medical imaging has created significant challenges
in data storage, transmission, and management for healthcare systems. In this
vein, efficient compression becomes increasingly important. Unlike natural
image compression, medical image compression prioritizes preserving diagnostic
details and structural integrity, imposing stricter quality requirements and
demanding fast, memory-efficient algorithms that balance computational
complexity with clinically acceptable reconstruction quality. Meanwhile, the
medical imaging family includes a plethora of modalities, each possessing
different requirements. For example, 2D medical image (e.g., X-rays,
histopathological images) compression focuses on exploiting intra-slice spatial
redundancy, while volumetric medical image faces require handling intra-slice
and inter-slice spatial correlations, and 4D dynamic imaging (e.g., time-series
CT/MRI, 4D ultrasound) additionally demands processing temporal correlations
between consecutive time frames. Traditional compression methods, grounded in
mathematical transforms and information theory principles, provide solid
theoretical foundations, predictable performance, and high standardization
levels, with extensive validation in clinical environments. In contrast, deep
learning-based approaches demonstrate remarkable adaptive learning capabilities
and can capture complex statistical characteristics and semantic information
within medical images. This comprehensive survey establishes a two-facet
taxonomy based on data structure (2D vs 3D/4D) and technical approaches
(traditional vs learning-based), thereby systematically presenting the complete
technological evolution, analyzing the unique technical challenges, and
prospecting future directions in medical image compression.

</details>


### [68] [Focus on Texture: Rethinking Pre-training in Masked Autoencoders for Medical Image Classification](https://arxiv.org/abs/2507.10869)
*Chetan Madan,Aarjav Satia,Soumen Basu,Pankaj Gupta,Usha Dutta,Chetan Arora*

Main category: eess.IV

TL;DR: GLCM-MAE是一种基于GLCM的新型MAE预训练框架，用于医学图像的自监督表示学习，显著提升了多种下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统MAE在医学图像中因纹理信息重要性而表现不佳，GLCM-MAE通过匹配GLCM矩阵的损失函数保留形态特征。

Method: 提出GLCM-MAE框架，使用GLCM矩阵匹配作为重建损失，并设计可微分损失函数。

Result: 在胆囊癌、乳腺癌、肺炎和COVID检测任务中，GLCM-MAE分别提升2.1%、3.1%、0.5%和0.6%。

Conclusion: GLCM-MAE通过GLCM损失函数有效保留医学图像的形态特征，显著提升下游任务性能。

Abstract: Masked Autoencoders (MAEs) have emerged as a dominant strategy for
self-supervised representation learning in natural images, where models are
pre-trained to reconstruct masked patches with a pixel-wise mean squared error
(MSE) between original and reconstructed RGB values as the loss. We observe
that MSE encourages blurred image re-construction, but still works for natural
images as it preserves dominant edges. However, in medical imaging, when the
texture cues are more important for classification of a visual abnormality, the
strategy fails. Taking inspiration from Gray Level Co-occurrence Matrix (GLCM)
feature in Radiomics studies, we propose a novel MAE based pre-training
framework, GLCM-MAE, using reconstruction loss based on matching GLCM. GLCM
captures intensity and spatial relationships in an image, hence proposed loss
helps preserve morphological features. Further, we propose a novel formulation
to convert matching GLCM matrices into a differentiable loss function. We
demonstrate that unsupervised pre-training on medical images with the proposed
GLCM loss improves representations for downstream tasks. GLCM-MAE outperforms
the current state-of-the-art across four tasks - gallbladder cancer detection
from ultrasound images by 2.1%, breast cancer detection from ultrasound by
3.1%, pneumonia detection from x-rays by 0.5%, and COVID detection from CT by
0.6%. Source code and pre-trained models are available at:
https://github.com/ChetanMadan/GLCM-MAE.

</details>


### [69] [Real-Time Foreign Object Recognition Based on Improved Wavelet Scattering Deep Network and Edge Computing](https://arxiv.org/abs/2507.11043)
*He Zhichao,Shen Xiangyu,Zhang Yong,Xie Nan*

Main category: eess.IV

TL;DR: 提出了一种基于改进小波散射网络的轻量级模型，用于在无人机边缘设备上实时识别电力系统中的异物，准确率超过90%，推理时间小于7ms。


<details>
  <summary>Details</summary>
Motivation: 新能源在电力系统中的渗透率增加，对变电站和输电线路的运维提出了更高要求，需要实时识别异物以消除安全隐患。

Method: 采用改进的小波散射网络提取图像单通道的散射系数和模系数，替代传统CNN的卷积和池化层，结合简化的多层感知机进行分类。

Result: 模型在树莓派和Jetson Nano等边缘设备上实现90%以上的准确率，推理时间小于7ms，且识别准确率优于YOLOv5s和YOLOv8s。

Conclusion: 该轻量级模型有效解决了无人机边缘设备计算能力有限的问题，为电力系统异物识别提供了高效解决方案。

Abstract: The increasing penetration rate of new energy in the power system has put
forward higher requirements for the operation and maintenance of substations
and transmission lines. Using the Unmanned Aerial Vehicles (UAV) to identify
foreign object in real time can quickly and effectively eliminate potential
safety hazards. However, due to the limited computation power, the captured
image cannot be real-time processed on edge devices in UAV locally. To overcome
this problem, a lightweight model based on an improved wavelet scatter deep
network is proposed. This model contains improved wavelet scattering network
for extracting the scatter coefficients and modulus coefficients of image
single channel, replacing the role of convolutional layer and pooling layer in
convolutional neural network. The following 3 fully connected layers, also
constituted a simplified Multilayer Perceptron (MLP), are used to classify the
extracted features. Experiments prove that the model constructed with
biorthogonal wavelets basis is able to recognize and classify the foreign
object in edge devices such as Raspberry Pi and Jetson Nano, with accuracy
higher than 90% and inference time less than 7ms for 720P (1280*720) images.
Further experiments demonstrate that the recognition accuracy of our model is
1.1% higher than YOLOv5s and 0.3% higher than YOLOv8s.

</details>


### [70] [Using Continual Learning for Real-Time Detection of Vulnerable Road Users in Complex Traffic Scenarios](https://arxiv.org/abs/2507.11046)
*Faryal Aurooj Nasir,Salman Liaquat,Nor Muzlifah Mahyuddin*

Main category: eess.IV

TL;DR: 该研究提出了一种基于YOLOv8-Dynamic算法的智能自适应系统，用于检测和实时适应复杂交通场景中的弱势道路使用者（VRUs），以防止事故发生。YOLOv8x在性能上优于其他先进模型，并通过持续学习能力适应动态环境。


<details>
  <summary>Details</summary>
Motivation: 弱势道路使用者（如行人和骑行者）在复杂交通场景中面临较高风险，需要一种实时检测和适应系统以减少事故。

Method: 研究采用YOLOv8x作为检测器，并优化其梯度下降机制，结合持续学习能力，训练于不同统计特性的数据集。

Result: 相比YOLOv5x和YOLOv7x，YOLOv8x在F1分数和mAP上分别提升12.14%/45.61%和21.26%/128.44%。优化后的算法在新数据集上F1分数和mAP分别提升21.08%和31.86%。

Conclusion: YOLOv8-Dynamic算法通过持续学习和优化机制，显著提升了VRUs检测的准确性和适应性，解决了深度模型在统计差异数据集上的灾难性遗忘问题。

Abstract: Pedestrians and bicyclists are among the vulnerable road users (VRUs) that
are inherently exposed to intricate traffic scenarios, which puts them at
increased risk of sustaining injuries or facing fatal outcomes. This study
presents an intelligent adaptive system that uses the YOLOv8-Dynamic (YOLOv8-D)
algorithm that detects vulnerable road users and adapts in real time to prevent
accidents before they occur. We select YOLOv8x as the detector by comparing it
with other state-of-the-art object detection models, including Faster-RCNN,
YOLOv5, YOLOv7, and variants. Compared to YOLOv5x, YOLOv8x shows improvements
of 12.14% in F1 score and 45.61% in mean Average Precision (mAP). Against
YOLOv7x, the improvements are 21.26% in F1 score and 128.44% in mAP. Our
algorithm integrates continual learning ability in the architecture of the
YOLOv8 detector to adjust to evolving road conditions flexibly, ensuring
adaptability across multiple dataset domains and facilitating continuous
enhancement of detection and tracking accuracy for VRUs, embracing the dynamic
nature of real-world environments. In our proposed framework, we optimized the
gradient descent mechanism of YOLOv8 model and train our optimized algorithm on
two statistically different datasets in terms of image viewpoint and number of
classes to achieve a 21.08% improvement in F1 score and a 31.86% improvement in
mAP as compared to a custom YOLOv8 framework trained on a new dataset, thus
overcoming the issue of catastrophic forgetting, which occurs when deep models
are trained on statistically different types of datasets.

</details>


### [71] [U-RWKV: Lightweight medical image segmentation with direction-adaptive RWKV](https://arxiv.org/abs/2507.11415)
*Hongbo Ye,Fenghe Tang,Peiang Zhao,Zhen Huang,Dexin Zhao,Minghao Bian,S. Kevin Zhou*

Main category: eess.IV

TL;DR: U-RWKV是一种新型医疗图像分割框架，通过RWKV架构实现高效长距离建模，解决了现有方法全局感受野受限的问题，适用于资源有限环境。


<details>
  <summary>Details</summary>
Motivation: 解决现有医疗图像分割方法（如U-Net）全局感受野受限的问题，以提升资源有限环境下的医疗可及性。

Method: 提出U-RWKV框架，包含方向自适应RWKV模块（DARM）和阶段自适应挤压激励模块（SASE），实现高效长距离建模和动态特征提取。

Result: 实验表明U-RWKV在计算高效的同时实现了最先进的分割性能。

Conclusion: U-RWKV为资源受限环境提供了实用的高级医疗影像技术解决方案。

Abstract: Achieving equity in healthcare accessibility requires lightweight yet
high-performance solutions for medical image segmentation, particularly in
resource-limited settings. Existing methods like U-Net and its variants often
suffer from limited global Effective Receptive Fields (ERFs), hindering their
ability to capture long-range dependencies. To address this, we propose U-RWKV,
a novel framework leveraging the Recurrent Weighted Key-Value(RWKV)
architecture, which achieves efficient long-range modeling at O(N)
computational cost. The framework introduces two key innovations: the
Direction-Adaptive RWKV Module(DARM) and the Stage-Adaptive
Squeeze-and-Excitation Module(SASE). DARM employs Dual-RWKV and QuadScan
mechanisms to aggregate contextual cues across images, mitigating directional
bias while preserving global context and maintaining high computational
efficiency. SASE dynamically adapts its architecture to different feature
extraction stages, balancing high-resolution detail preservation and semantic
relationship capture. Experiments demonstrate that U-RWKV achieves
state-of-the-art segmentation performance with high computational efficiency,
offering a practical solution for democratizing advanced medical imaging
technologies in resource-constrained environments. The code is available at
https://github.com/hbyecoding/U-RWKV.

</details>


### [72] [Precision Spatio-Temporal Feature Fusion for Robust Remote Sensing Change Detection](https://arxiv.org/abs/2507.11523)
*Buddhi Wijenayake,Athulya Ratnayake,Praveen Sumanasekara,Nichula Wasalathilaka,Mathivathanan Piratheepan,Roshan Godaliyadda,Mervyn Ekanayake,Vijitha Herath*

Main category: eess.IV

TL;DR: 提出了一种基于ChangeMamba架构的遥感变化检测方法，通过精度融合块和增强的解码器管道提升性能，并在多个数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法和早期深度学习模型在遥感变化检测中难以捕捉长距离依赖和全局上下文的问题，同时降低计算复杂度。

Method: 采用ChangeMamba架构，引入精度融合块捕捉通道级时序变化和像素级差异，优化解码器管道和损失函数。

Result: 在SYSU-CD、LEVIR-CD+和WHU-CD数据集上表现出更高的精度、召回率、F1分数和IoU。

Conclusion: 该方法在遥感变化检测中表现出高效性和鲁棒性，代码和预训练模型已开源。

Abstract: Remote sensing change detection is vital for monitoring environmental and
urban transformations but faces challenges like manual feature extraction and
sensitivity to noise. Traditional methods and early deep learning models, such
as convolutional neural networks (CNNs), struggle to capture long-range
dependencies and global context essential for accurate change detection in
complex scenes. While Transformer-based models mitigate these issues, their
computational complexity limits their applicability in high-resolution remote
sensing. Building upon ChangeMamba architecture, which leverages state space
models for efficient global context modeling, this paper proposes precision
fusion blocks to capture channel-wise temporal variations and per-pixel
differences for fine-grained change detection. An enhanced decoder pipeline,
incorporating lightweight channel reduction mechanisms, preserves local details
with minimal computational cost. Additionally, an optimized loss function
combining Cross Entropy, Dice and Lovasz objectives addresses class imbalance
and boosts Intersection-over-Union (IoU). Evaluations on SYSU-CD, LEVIR-CD+,
and WHU-CD datasets demonstrate superior precision, recall, F1 score, IoU, and
overall accuracy compared to state-of-the-art methods, highlighting the
approach's robustness for remote sensing change detection. For complete
transparency, the codes and pretrained models are accessible at
https://github.com/Buddhi19/MambaCD.git

</details>


### [73] [Latent Space Consistency for Sparse-View CT Reconstruction](https://arxiv.org/abs/2507.11152)
*Duoyou Chen,Yunqing Chen,Can Zhang,Zhou Wang,Cheng Chen,Ruoxiu Xiao*

Main category: eess.IV

TL;DR: 提出了一种名为CLS-DM的模型，通过跨模态特征对比学习，解决2D X射线图像与3D CT图像在潜在空间对齐的问题，显著提升了稀疏视图CT重建的性能。


<details>
  <summary>Details</summary>
Motivation: 传统CT重建方法存在耗时和辐射风险问题，稀疏视图CT重建成为研究热点，但现有方法在潜在空间对齐上表现不佳。

Method: 提出CLS-DM模型，结合跨模态特征对比学习，从2D X射线图像中提取3D潜在信息，实现模态间潜在空间对齐。

Result: 在LIDC-IDRI和CTSpine1K数据集上，CLS-DM在PSNR和SSIM指标上优于经典和最新生成模型。

Conclusion: CLS-DM不仅提升了稀疏视图CT重建的效果和经济性，还可推广至其他跨模态转换任务，如文本到图像合成。

Abstract: Computed Tomography (CT) is a widely utilized imaging modality in clinical
settings. Using densely acquired rotational X-ray arrays, CT can capture 3D
spatial features. However, it is confronted with challenged such as significant
time consumption and high radiation exposure. CT reconstruction methods based
on sparse-view X-ray images have garnered substantial attention from
researchers as they present a means to mitigate costs and risks. In recent
years, diffusion models, particularly the Latent Diffusion Model (LDM), have
demonstrated promising potential in the domain of 3D CT reconstruction.
Nonetheless, due to the substantial differences between the 2D latent
representation of X-ray modalities and the 3D latent representation of CT
modalities, the vanilla LDM is incapable of achieving effective alignment
within the latent space. To address this issue, we propose the Consistent
Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature
contrastive learning to efficiently extract latent 3D information from 2D X-ray
images and achieve latent space alignment between modalities. Experimental
results indicate that CLS-DM outperforms classical and state-of-the-art
generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the
LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing
the effectiveness and economic viability of sparse X-ray reconstructed CT but
can also be generalized to other cross-modal transformation tasks, such as
text-to-image synthesis. We have made our code publicly available at
https://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research
and applications in other domains.

</details>


### [74] [3D Magnetic Inverse Routine for Single-Segment Magnetic Field Images](https://arxiv.org/abs/2507.11293)
*J. Senthilnath,Chen Hao,F. C. Wellstood*

Main category: eess.IV

TL;DR: 论文提出了一种名为3D MIR的新方法，通过结合深度学习和物理约束优化技术，从磁场图像中恢复半导体封装中的3D电流信息。


<details>
  <summary>Details</summary>
Motivation: 在半导体封装中，准确恢复3D信息对于非破坏性测试（NDT）定位电路缺陷至关重要。

Method: 3D MIR方法分为三个阶段：1) 使用CNN预测导线长度和深度；2) 利用空间物理约束提供初始参数估计；3) 通过优化器调整参数以最小化重建与实际磁场图像的差异。

Result: 结果表明，3D MIR方法能够高精度地恢复3D信息，为半导体封装中的磁场图像重建设定了新标准。

Conclusion: 该方法展示了深度学习和物理驱动优化在实际应用中的潜力。

Abstract: In semiconductor packaging, accurately recovering 3D information is crucial
for non-destructive testing (NDT) to localize circuit defects. This paper
presents a novel approach called the 3D Magnetic Inverse Routine (3D MIR),
which leverages Magnetic Field Images (MFI) to retrieve the parameters for the
3D current flow of a single-segment. The 3D MIR integrates a deep learning
(DL)-based Convolutional Neural Network (CNN), spatial-physics-based
constraints, and optimization techniques. The method operates in three stages:
i) The CNN model processes the MFI data to predict ($\ell/z_o$), where $\ell$
is the wire length and $z_o$ is the wire's vertical depth beneath the magnetic
sensors and classify segment type ($c$). ii) By leveraging
spatial-physics-based constraints, the routine provides initial estimates for
the position ($x_o$, $y_o$, $z_o$), length ($\ell$), current ($I$), and current
flow direction (positive or negative) of the current segment. iii) An optimizer
then adjusts these five parameters ($x_o$, $y_o$, $z_o$, $\ell$, $I$) to
minimize the difference between the reconstructed MFI and the actual MFI. The
results demonstrate that the 3D MIR method accurately recovers 3D information
with high precision, setting a new benchmark for magnetic image reconstruction
in semiconductor packaging. This method highlights the potential of combining
DL and physics-driven optimization in practical applications.

</details>


### [75] [HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging](https://arxiv.org/abs/2507.11325)
*Arefin Ittesafun Abian,Ripon Kumar Debnath,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Md Rafiqul Islam,Asif Karim,Reem E. Mohamed,Sami Azam*

Main category: eess.IV

TL;DR: HANS-Net是一种新型肝脏和肿瘤分割框架，结合多种先进技术，显著提升了分割精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 腹部CT图像中肝脏和肿瘤的准确分割对诊断和治疗至关重要，但复杂解剖结构和有限标注数据使其具有挑战性。

Method: HANS-Net结合双曲卷积、小波分解模块、突触可塑性机制和隐式神经表示，并引入不确定性感知和时间注意力。

Result: 在LiTS数据集上，Dice得分93.26%，IoU 88.09%，ASSD 0.72 mm，VOE 11.91%；跨数据集验证表现同样优异。

Conclusion: HANS-Net在肝脏和肿瘤分割中表现出高效性、鲁棒性和泛化能力。

Abstract: Accurate liver and tumor segmentation on abdominal CT images is critical for
reliable diagnosis and treatment planning, but remains challenging due to
complex anatomical structures, variability in tumor appearance, and limited
annotated data. To address these issues, we introduce Hyperbolic-convolutions
Adaptive-temporal-attention with Neural-representation and Synaptic-plasticity
Network (HANS-Net), a novel segmentation framework that synergistically
combines hyperbolic convolutions for hierarchical geometric representation, a
wavelet-inspired decomposition module for multi-scale texture learning, a
biologically motivated synaptic plasticity mechanism for adaptive feature
enhancement, and an implicit neural representation branch to model fine-grained
and continuous anatomical boundaries. Additionally, we incorporate
uncertainty-aware Monte Carlo dropout to quantify prediction confidence and
lightweight temporal attention to improve inter-slice consistency without
sacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate
that HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an
average symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap
error (VOE) of 11.91%. Furthermore, cross-dataset validation on the
3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of
1.525 mm, and VOE of 19.71%, indicating strong generalization across different
datasets. These results confirm the effectiveness and robustness of HANS-Net in
providing anatomically consistent, accurate, and confident liver and tumor
segmentation.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [76] [Developing and evaluating quilts for the depiction of large layered graphs](https://arxiv.org/abs/2507.10883)
*Juhee Bae,Benjamin Watson*

Main category: cs.GR

TL;DR: Quilts是一种基于矩阵的分层图表示方法，通过改进其设计（如颜色、文本或混合表示跳过链接），显著提高了路径查找的速度和准确性，优于传统的节点链接和矩阵表示。


<details>
  <summary>Details</summary>
Motivation: 传统分层图表示（如流程图）在复杂图中难以理解，Quilts旨在解决这一问题。

Method: 开发了三种跳过链接的设计（颜色、文本、混合），并通过实验比较其效果；随后将最佳设计与节点链接和矩阵表示进行对比。

Result: 混合表示在路径查找中表现最佳，Quilts整体速度显著快于节点链接和矩阵表示（46.6秒 vs. 58.3秒和71.2秒）。

Conclusion: Quilts在复杂图中表现优越，尤其是混合表示跳过链接的设计，为分层图的可视化提供了更高效的解决方案。

Abstract: Traditional layered graph depictions such as flow charts are in wide use. Yet
as graphs grow more complex, these depictions can become difficult to
understand. Quilts are matrix-based depictions for layered graphs designed to
address this problem. In this research, we first improve Quilts by developing
three design alternatives, and then compare the best of these alternatives to
better-known node-link and matrix depictions. A primary weakness in Quilts is
their depiction of skip links, links that do not simply connect to a succeeding
layer. Therefore in our first study, we compare Quilts using color-only,
text-only, and mixed (color and text) skip link depictions, finding that path
finding with the color-only depiction is significantly slower and less
accurate, and that in certain cases, the mixed depiction offers an advantage
over the text-only depiction. In our second study, we compare Quilts using the
mixed depiction to node-link diagrams and centered matrices. Overall results
show that users can find paths through graphs significantly faster with Quilts
(46.6 secs) than with node-link (58.3 secs) or matrix (71.2 secs) diagrams.
This speed advantage is still greater in large graphs (e.g. in 200 node graphs,
55.4 secs vs. 71.1 secs for node-link and 84.2 secs for matrix depictions).

</details>


### [77] [OffsetCrust: Variable-Radius Offset Approximation with Power Diagrams](https://arxiv.org/abs/2507.10924)
*Zihan Zhao,Pengfei Wang,Minfeng Xu,Shuangmin Chen,Shiqing Xin,Changhe Tu,Wenping Wang*

Main category: cs.GR

TL;DR: OffsetCrust是一个新颖的框架，通过计算功率图高效解决变半径偏移曲面问题，适用于几何处理中的多种应用。


<details>
  <summary>Details</summary>
Motivation: 变半径偏移曲面的计算是一个具有挑战性的问题，现有方法在常半径偏移曲面方面已有进展，但变半径偏移仍需解决。

Method: OffsetCrust通过构造功率图，基于基曲面上的采样点和对应的离曲面点，沿半径函数依赖的方向位移，解决变半径偏移问题。

Result: 实验验证了OffsetCrust的准确性和高效性，并展示了其在从MAT重建原始边界曲面等应用中的实用性。

Conclusion: OffsetCrust提供了一种轻量级且有效的方法，解决了变半径偏移曲面的计算问题，并改进了传统基于外壳的方法中的对齐问题。

Abstract: Offset surfaces, defined as the Minkowski sum of a base surface and a rolling
ball, play a crucial role in geometry processing, with applications ranging
from coverage motion planning to brush modeling. While considerable progress
has been made in computing constant-radius offset surfaces, computing
variable-radius offset surfaces remains a challenging problem. In this paper,
we present OffsetCrust, a novel framework that efficiently addresses the
variable-radius offsetting problem by computing a power diagram. Let $R$ denote
the radius function defined on the base surface $S$. The power diagram is
constructed from contributing sites, consisting of carefully sampled base
points on $S$ and their corresponding off-surface points, displaced along
$R$-dependent directions. In the constant-radius case only, these displacement
directions align exactly with the surface normals of $S$. Moreover, our method
mitigates the misalignment issues commonly seen in crust-based approaches
through a lightweight fine-tuning procedure. We validate the accuracy and
efficiency of OffsetCrust through extensive experiments, and demonstrate its
practical utility in applications such as reconstructing original boundary
surfaces from medial axis transform (MAT) representations.

</details>


### [78] [Elevating 3D Models: High-Quality Texture and Geometry Refinement from a Low-Quality Model](https://arxiv.org/abs/2507.11465)
*Nuri Ryu,Jiyun Won,Jooeun Son,Minsu Gong,Joo-Haeng Lee,Sunghyun Cho*

Main category: cs.GR

TL;DR: Elevate3D是一个新框架，将低质量3D资产提升为高质量，通过HFS-SDEdit增强纹理并结合几何优化，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高质量3D资产稀缺且获取成本高，Elevate3D旨在解决这一问题。

Method: 使用HFS-SDEdit增强纹理，并结合单目几何预测器优化几何，逐视图交替进行纹理和几何细化。

Result: Elevate3D在3D模型细化中实现了最先进的质量，显著提升了纹理和几何的准确性。

Conclusion: Elevate3D有效解决了高质量开源3D资产的稀缺问题，为计算机图形学和3D视觉应用提供了实用工具。

Abstract: High-quality 3D assets are essential for various applications in computer
graphics and 3D vision but remain scarce due to significant acquisition costs.
To address this shortage, we introduce Elevate3D, a novel framework that
transforms readily accessible low-quality 3D assets into higher quality. At the
core of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that
significantly improves texture quality while preserving the appearance and
geometry while fixing its degradations. Furthermore, Elevate3D operates in a
view-by-view manner, alternating between texture and geometry refinement.
Unlike previous methods that have largely overlooked geometry refinement, our
framework leverages geometric cues from images refined with HFS-SDEdit by
employing state-of-the-art monocular geometry predictors. This approach ensures
detailed and accurate geometry that aligns seamlessly with the enhanced
texture. Elevate3D outperforms recent competitors by achieving state-of-the-art
quality in 3D model refinement, effectively addressing the scarcity of
high-quality open-source 3D assets.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [79] [SFATTI: Spiking FPGA Accelerator for Temporal Task-driven Inference -- A Case Study on MNIST](https://arxiv.org/abs/2507.10561)
*Alessio Caviglia,Filippo Marostica,Alessio Carpegna,Alessandro Savino,Stefano Di Carlo*

Main category: cs.NE

TL;DR: 论文探讨了使用Spiker+框架为MNIST数据集生成优化的SNN加速器，分析了边缘计算中的权衡。


<details>
  <summary>Details</summary>
Motivation: 硬件加速器对边缘应用（如图像识别）的低延迟和高效能推理至关重要，而SNN因其事件驱动和稀疏特性适合FPGA部署。

Method: 使用开源的Spiker+框架，支持高级网络拓扑、神经元模型和量化规范，自动生成可部署的HDL。

Result: 评估了多种配置，分析了与边缘计算约束相关的权衡。

Conclusion: Spiker+框架为SNN在FPGA上的高效部署提供了可行方案。

Abstract: Hardware accelerators are essential for achieving low-latency,
energy-efficient inference in edge applications like image recognition. Spiking
Neural Networks (SNNs) are particularly promising due to their event-driven and
temporally sparse nature, making them well-suited for low-power Field
Programmable Gate Array (FPGA)-based deployment. This paper explores using the
open-source Spiker+ framework to generate optimized SNNs accelerators for
handwritten digit recognition on the MNIST dataset. Spiker+ enables high-level
specification of network topologies, neuron models, and quantization,
automatically generating deployable HDL. We evaluate multiple configurations
and analyze trade-offs relevant to edge computing constraints.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [80] [Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers](https://arxiv.org/abs/2507.10787)
*Yilun Zhao,Chengye Wang,Chuhan Li,Arman Cohan*

Main category: cs.CL

TL;DR: MISS-QA是首个评估模型解读科学文献中示意图能力的基准，包含1,500个专家标注示例，测试了18种前沿多模态模型，发现其与人类专家存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 科学文献中的示意图是重要信息载体，但现有模型对其理解能力不足，需专门基准评估和改进。

Method: 构建MISS-QA基准，包含1,500个标注示例，测试18种多模态模型，分析其表现及错误。

Result: 模型表现显著低于人类专家，尤其在不可回答问题上差距明显。

Conclusion: MISS-QA揭示了当前模型在理解多模态科学文献中的局限性，为改进提供了关键方向。

Abstract: This paper introduces MISS-QA, the first benchmark specifically designed to
evaluate the ability of models to interpret schematic diagrams within
scientific literature. MISS-QA comprises 1,500 expert-annotated examples over
465 scientific papers. In this benchmark, models are tasked with interpreting
schematic diagrams that illustrate research overviews and answering
corresponding information-seeking questions based on the broader context of the
paper. We assess the performance of 18 frontier multimodal foundation models,
including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant
performance gap between these models and human experts on MISS-QA. Our analysis
of model performance on unanswerable questions and our detailed error analysis
further highlight the strengths and limitations of current models, offering key
insights to enhance models in comprehending multimodal scientific literature.

</details>


### [81] [Teach Me Sign: Stepwise Prompting LLM for Sign Language Production](https://arxiv.org/abs/2507.10972)
*Zhaoyi An,Rei Kawakami*

Main category: cs.CL

TL;DR: TEAM-Sign通过微调大语言模型（LLM）处理手语生成任务，利用逐步提示策略提取LLM中的手语知识，实验证明其在How2Sign和Phoenix14T数据集上有效。


<details>
  <summary>Details</summary>
Motivation: 手语生成因复杂性和独特规则在大语言模型中的应用受限，研究旨在利用LLM的推理能力和知识解决这一问题。

Method: 将手语视为自然语言，通过微调LLM学习文本与手语的对应关系，采用逐步提示策略提取LLM中的手语知识。

Result: 在How2Sign和Phoenix14T数据集上，TEAM-Sign成功对齐手语与口语的分布和语法规则。

Conclusion: TEAM-Sign证明了LLM在手语生成任务中的潜力，为手语处理提供了新思路。

Abstract: Large language models, with their strong reasoning ability and rich
knowledge, have brought revolution to many tasks of AI, but their impact on
sign language generation remains limited due to its complexity and unique
rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign
language as another natural language. By fine-tuning an LLM, we enable it to
learn the correspondence between text and sign language, and facilitate
generation. Considering the differences between sign and spoken language, we
employ a stepwise prompting strategy to extract the inherent sign language
knowledge within the LLM, thereby supporting the learning and generation
process. Experimental results on How2Sign and Phoenix14T datasets demonstrate
that our approach effectively leverages both the sign language knowledge and
reasoning capabilities of LLM to align the different distribution and
grammatical rules between sign and spoken language.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [82] [Stochastic Entanglement Configuration for Constructive Entanglement Topologies in Quantum Machine Learning with Application to Cardiac MRI](https://arxiv.org/abs/2507.11401)
*Mehri Mehrnia,Mohammed S. M. Elbaz*

Main category: quant-ph

TL;DR: 提出了一种随机纠缠配置方法，用于量子机器学习中的变分量子电路，通过生成多样化的纠缠拓扑结构，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 当前固定纠缠拓扑结构无法适应任务需求，限制了量子模型超越经典模型的潜力。

Method: 采用随机二进制矩阵编码纠缠拓扑，通过无约束和约束采样模式生成并评估400种配置。

Result: 在心脏MRI疾病分类任务中，16%的配置优于经典基线，最高准确率提升20%。

Conclusion: 随机纠缠配置方法具有鲁棒性和通用性，显著提升了量子模型的性能。

Abstract: Efficient entanglement strategies are essential for advancing variational
quantum circuits (VQCs) for quantum machine learning (QML). However, most
current approaches use fixed entanglement topologies that are not adaptive to
task requirements, limiting potential gains over classical models. We introduce
a novel stochastic entanglement configuration method that systematically
generates diverse entanglement topologies to identify a subspace of
constructive entanglement configurations, defined as entanglement topologies
that boost hybrid model performance (e.g., classification accuracy) beyond
classical baselines. Each configuration is encoded as a stochastic binary
matrix, denoting directed entanglement between qubits. This enables scalable
exploration of the hyperspace of candidate entanglement topologies using
entanglement density and per-qubit constraints as key metrics. We define
unconstrained and constrained sampling modes, controlling entanglement per
qubit. Using our method, 400 stochastic configurations were generated and
evaluated in a hybrid QML for cardiac MRI disease classification. We identified
64 (16%) novel constructive entanglement configurations that consistently
outperformed the classical baseline. Ensemble aggregation of top-performing
configurations achieved ~0.92 classification accuracy, exceeding the classical
model (~0.87) by over 5%. Compared to four conventional topologies (ring,
nearest neighbor, no entanglement, fully entangled), none surpassed the
classical baseline (maximum accuracy ~0.82), while our configurations delivered
up to ~20% higher accuracy. Thus, highlighting the robustness and
generalizability of the identified constructive entanglements.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [83] [Deep Equilibrium models for Poisson Imaging Inverse problems via Mirror Descent](https://arxiv.org/abs/2507.11461)
*Christian Daniele,Silvia Villa,Samuel Vaiter,Luca Calatroni*

Main category: math.OC

TL;DR: 论文提出了一种基于Mirror Descent的Deep Equilibrium Models（DEQs）方法，用于解决Poisson逆问题，通过非欧几里得几何结构适应数据项，提升了性能并减少了超参数调优需求。


<details>
  <summary>Details</summary>
Motivation: 传统DEQs在Gaussian保真度问题中表现良好，但在Poisson逆问题中缺乏适应性。本文旨在扩展DEQs的应用范围，解决Poisson数据保真度问题。

Method: 提出了一种基于Mirror Descent的DEQ新框架，利用非欧几里得几何结构适应数据项，并提供了收敛性保证和高效的计算策略。

Result: 数值实验表明，该方法优于传统模型驱动方法，与Bregman Plug-and-Play方法性能相当，同时减少了初始化敏感性和超参数调优需求。

Conclusion: 该方法为Poisson逆问题提供了一种高效且鲁棒的解决方案，代码已开源。

Abstract: Deep Equilibrium Models (DEQs) are implicit neural networks with fixed
points, which have recently gained attention for learning image regularization
functionals, particularly in settings involving Gaussian fidelities, where
assumptions on the forward operator ensure contractiveness of standard
(proximal) Gradient Descent operators. In this work, we extend the application
of DEQs to Poisson inverse problems, where the data fidelity term is more
appropriately modeled by the Kullback-Leibler divergence. To this end, we
introduce a novel DEQ formulation based on Mirror Descent defined in terms of a
tailored non-Euclidean geometry that naturally adapts with the structure of the
data term. This enables the learning of neural regularizers within a principled
training framework. We derive sufficient conditions to guarantee the
convergence of the learned reconstruction scheme and propose computational
strategies that enable both efficient training and fully parameter-free
inference. Numerical experiments show that our method outperforms traditional
model-based approaches and it is comparable to the performance of Bregman
Plug-and-Play methods, while mitigating their typical drawbacks - namely,
sensitivity to initialization and careful tuning of hyperparameters. The code
is publicly available at https://github.com/christiandaniele/DEQ-MD.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [84] [AGFS-Tractometry: A Novel Atlas-Guided Fine-Scale Tractometry Approach for Enhanced Along-Tract Group Statistical Comparison Using Diffusion MRI Tractography](https://arxiv.org/abs/2507.10601)
*Ruixi Zheng,Wei Zhang,Yijie Li,Xi Zhu,Zhou Lan,Jarrett Rushmore,Yogesh Rathi,Nikos Makris,Lauren J. O'Donnell,Fan Zhang*

Main category: q-bio.QM

TL;DR: 提出了一种新型的基于图谱引导的精细尺度纤维束分析方法AGFS-Tractometry，通过利用纤维束空间信息和置换检验，增强了群体间沿纤维束的统计分析能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在改进现有的纤维束分析方法，以更敏感和特异地检测白质（WM）的局部差异。

Method: 提出了图谱引导的纤维束分析模板和一种非参数置换检验方法，用于群体间沿纤维束的统计分析。

Result: 在合成数据和真实数据实验中，AGFS-Tractometry表现出更高的敏感性和特异性，能够检测到更多解剖学上一致的显著差异区域。

Conclusion: AGFS-Tractometry能够有效检测细微或空间局部的白质差异，其模板和代码已开源。

Abstract: Diffusion MRI (dMRI) tractography is currently the only method for in vivo
mapping of the brain's white matter (WM) connections. Tractometry is an
advanced tractography analysis technique for along-tract profiling to
investigate the morphology and microstructural properties along the fiber
tracts. Tractometry has become an essential tool for studying local along-tract
differences between different populations (e.g., health vs disease). In this
study, we propose a novel atlas-guided fine-scale tractometry method, namely
AGFS-Tractometry, that leverages tract spatial information and permutation
testing to enhance the along-tract statistical analysis between populations.
There are two major contributions in AGFS-Tractometry. First, we create a novel
atlas-guided tract profiling template that enables consistent, fine-scale,
along-tract parcellation of subject-specific fiber tracts. Second, we propose a
novel nonparametric permutation testing group comparison method to enable
simultaneous analysis across all along-tract parcels while correcting for
multiple comparisons. We perform experimental evaluations on synthetic datasets
with known group differences and in vivo real data. We compare AGFS-Tractometry
with two state-of-the-art tractometry methods, including Automated Fiber-tract
Quantification (AFQ) and BUndle ANalytics (BUAN). Our results show that the
proposed AGFS-Tractometry obtains enhanced sensitivity and specificity in
detecting local WM differences. In the real data analysis experiments,
AGFS-Tractometry can identify more regions with significant differences, which
are anatomically consistent with the existing literature. Overall, these
demonstrate the ability of AGFS-Tractometry to detect subtle or spatially
localized WM group-level differences. The created tract profiling template and
related code are available at:
https://github.com/ZhengRuixi/AGFS-Tractometry.git.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [85] [rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active Instance-Level Object Understanding](https://arxiv.org/abs/2507.10776)
*Howard H. Qian,Yiting Chen,Gaotian Wang,Podshara Chanrungmaneekul,Kaiyu Hang*

Main category: cs.RO

TL;DR: 提出了一种实时交互感知框架rt-RISeg，通过机器人交互和设计的体帧不变特征（BFIF）连续分割未见物体，无需学习分割模型，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有未见物体实例分割（UOIS）方法依赖大规模数据集训练，容易过拟合静态视觉特征，泛化性能差。

Method: 基于视觉交互性原则，提出rt-RISeg框架，利用机器人交互和BFIF特征实时分割物体。

Result: 平均分割准确率比现有UOIS方法高27.5%，且生成的分割掩膜可作为视觉基础模型的提示。

Conclusion: rt-RISeg通过交互感知显著提升分割性能，且具有独立性和扩展性。

Abstract: Successful execution of dexterous robotic manipulation tasks in new
environments, such as grasping, depends on the ability to proficiently segment
unseen objects from the background and other objects. Previous works in unseen
object instance segmentation (UOIS) train models on large-scale datasets, which
often leads to overfitting on static visual features. This dependency results
in poor generalization performance when confronted with out-of-distribution
scenarios. To address this limitation, we rethink the task of UOIS based on the
principle that vision is inherently interactive and occurs over time. We
propose a novel real-time interactive perception framework, rt-RISeg, that
continuously segments unseen objects by robot interactions and analysis of a
designed body frame-invariant feature (BFIF). We demonstrate that the relative
rotational and linear velocities of randomly sampled body frames, resulting
from selected robot interactions, can be used to identify objects without any
learned segmentation model. This fully self-contained segmentation pipeline
generates and updates object segmentation masks throughout each robot
interaction without the need to wait for an action to finish. We showcase the
effectiveness of our proposed interactive perception method by achieving an
average object segmentation accuracy rate 27.5% greater than state-of-the-art
UOIS methods. Furthermore, although rt-RISeg is a standalone framework, we show
that the autonomously generated segmentation masks can be used as prompts to
vision foundation models for significantly improved performance.

</details>


### [86] [Whom to Respond To? A Transformer-Based Model for Multi-Party Social Robot Interaction](https://arxiv.org/abs/2507.10960)
*He Zhu,Ryo Miyoshi,Yuki Okafuji*

Main category: cs.RO

TL;DR: 提出了一种基于Transformer的多任务学习框架，用于改进社交机器人在多用户环境中的决策能力，通过两种新的损失函数和新的数据集实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 多用户环境中，社交机器人需要理解上下文并决定何时及向谁回应，而现有研究主要关注单用户交互。

Method: 采用Transformer多任务学习框架，提出两种新损失函数：一种约束活跃说话者以改进场景建模，另一种引导回应选择针对机器人的话语。构建了新的多用户HRI数据集。

Result: 实验表明，该模型在回应决策上优于现有的启发式和单任务方法，达到最先进性能。

Conclusion: 研究为开发能够进行自然且上下文感知的多方交互的社交智能机器人提供了贡献。

Abstract: Prior human-robot interaction (HRI) research has primarily focused on
single-user interactions, where robots do not need to consider the timing or
recipient of their responses. However, in multi-party interactions, such as at
malls and hospitals, social robots must understand the context and decide both
when and to whom they should respond. In this paper, we propose a
Transformer-based multi-task learning framework to improve the decision-making
process of social robots, particularly in multi-user environments. Considering
the characteristics of HRI, we propose two novel loss functions: one that
enforces constraints on active speakers to improve scene modeling, and another
that guides response selection towards utterances specifically directed at the
robot. Additionally, we construct a novel multi-party HRI dataset that captures
real-world complexities, such as gaze misalignment. Experimental results
demonstrate that our model achieves state-of-the-art performance in respond
decisions, outperforming existing heuristic-based and single-task approaches.
Our findings contribute to the development of socially intelligent social
robots capable of engaging in natural and context-aware multi-party
interactions.

</details>


### [87] [Learning to Tune Like an Expert: Interpretable and Scene-Aware Navigation via MLLM Reasoning and CVAE-Based Adaptation](https://arxiv.org/abs/2507.11001)
*Yanbo Wang,Zipeng Fang,Lei Zhao,Weidong Chen*

Main category: cs.RO

TL;DR: LE-Nav是一个基于多模态大语言模型和条件变分自编码器的导航框架，通过自适应调整规划器超参数，提升服务机器人在动态环境中的性能和社会接受度。


<details>
  <summary>Details</summary>
Motivation: 传统导航系统在动态和非结构化环境中表现不佳，强化学习方法因泛化能力差和仿真多样性不足而难以实际应用。

Method: 结合多模态大语言模型推理和条件变分自编码器，利用单次示例和思维链提示策略实现零样本场景理解，并自适应调整超参数。

Result: 实验表明LE-Nav能生成达到人类水平的超参数，在真实导航试验和用户研究中表现优于现有方法。

Conclusion: LE-Nav在成功率和主观评价上均优于现有方法，展示了其在动态环境中的实用性和社会接受度。

Abstract: Service robots are increasingly deployed in diverse and dynamic environments,
where both physical layouts and social contexts change over time and across
locations. In these unstructured settings, conventional navigation systems that
rely on fixed parameters often fail to generalize across scenarios, resulting
in degraded performance and reduced social acceptance. Although recent
approaches have leveraged reinforcement learning to enhance traditional
planners, these methods often fail in real-world deployments due to poor
generalization and limited simulation diversity, which hampers effective
sim-to-real transfer. To tackle these issues, we present LE-Nav, an
interpretable and scene-aware navigation framework that leverages multi-modal
large language model reasoning and conditional variational autoencoders to
adaptively tune planner hyperparameters. To achieve zero-shot scene
understanding, we utilize one-shot exemplars and chain-of-thought prompting
strategies. Additionally, a conditional variational autoencoder captures the
mapping between natural language instructions and navigation hyperparameters,
enabling expert-level tuning. Experiments show that LE-Nav can generate
hyperparameters achieving human-level tuning across diverse planners and
scenarios. Real-world navigation trials and a user study on a smart wheelchair
platform demonstrate that it outperforms state-of-the-art methods on
quantitative metrics such as success rate, efficiency, safety, and comfort,
while receiving higher subjective scores for perceived safety and social
acceptance. Code is available at https://github.com/Cavendish518/LE-Nav.

</details>


### [88] [TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update](https://arxiv.org/abs/2507.11069)
*Jeongyun Kim,Seunghoon Jeong,Giseop Kim,Myung-Hwan Jeon,Eunji Jun,Ayoung Kim*

Main category: cs.RO

TL;DR: TRAN-D是一种基于2D高斯泼溅的透明物体深度重建方法，通过分离透明物体与背景、优化高斯分布和物理模拟，显著提升了稀疏视图和动态环境下的3D重建精度。


<details>
  <summary>Details</summary>
Motivation: 透明物体的3D几何重建因反射和折射等物理特性而具有挑战性，尤其在稀疏视图和动态环境中。

Method: TRAN-D通过分离透明物体与背景，优化高斯分布，并使用对象感知损失和物理模拟减少伪影。

Result: 在合成和真实场景中，TRAN-D比现有方法平均绝对误差降低39%，单图像更新时精度提升1.5倍。

Conclusion: TRAN-D在透明物体重建中表现出色，适用于稀疏视图和动态环境，显著优于现有方法。

Abstract: Understanding the 3D geometry of transparent objects from RGB images is
challenging due to their inherent physical properties, such as reflection and
refraction. To address these difficulties, especially in scenarios with sparse
views and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian
Splatting-based depth reconstruction method for transparent objects. Our key
insight lies in separating transparent objects from the background, enabling
focused optimization of Gaussians corresponding to the object. We mitigate
artifacts with an object-aware loss that places Gaussians in obscured regions,
ensuring coverage of invisible surfaces while reducing overfitting.
Furthermore, we incorporate a physics-based simulation that refines the
reconstruction in just a few seconds, effectively handling object removal and
chain-reaction movement of remaining objects without the need for rescanning.
TRAN-D is evaluated on both synthetic and real-world sequences, and it
consistently demonstrated robust improvements over existing GS-based
state-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean
absolute error by over 39% for the synthetic TRansPose sequences. Furthermore,
despite being updated using only one image, TRAN-D reaches a {\delta} < 2.5 cm
accuracy of 48.46%, over 1.5 times that of baselines, which uses six images.
Code and more results are available at https://jeongyun0609.github.io/TRAN-D/.

</details>


### [89] [All Eyes, no IMU: Learning Flight Attitude from Vision Alone](https://arxiv.org/abs/2507.11302)
*Jesse J. Hagenaars,Stein Stroobants,Sander M. Bohte,Guido C. H. E. De Croon*

Main category: cs.RO

TL;DR: 首次提出仅依赖视觉的无人机飞行控制方法，使用事件相机和神经网络替代传统惯性传感器。


<details>
  <summary>Details</summary>
Motivation: 飞行机器人通常依赖惯性传感器，而许多飞行生物仅依赖视觉，研究旨在探索纯视觉控制的可行性。

Method: 使用下视事件相机和循环卷积神经网络，通过监督学习训练，估计姿态和旋转速率。

Result: 实验证明该方法可替代惯性测量单元，网络在不同环境中表现良好，窄视野版本泛化能力更强。

Conclusion: 纯视觉飞行控制为微型自主飞行机器人提供了有前景的解决方案。

Abstract: Vision is an essential part of attitude control for many flying animals, some
of which have no dedicated sense of gravity. Flying robots, on the other hand,
typically depend heavily on accelerometers and gyroscopes for attitude
stabilization. In this work, we present the first vision-only approach to
flight control for use in generic environments. We show that a quadrotor drone
equipped with a downward-facing event camera can estimate its attitude and
rotation rate from just the event stream, enabling flight control without
inertial sensors. Our approach uses a small recurrent convolutional neural
network trained through supervised learning. Real-world flight tests
demonstrate that our combination of event camera and low-latency neural network
is capable of replacing the inertial measurement unit in a traditional flight
control loop. Furthermore, we investigate the network's generalization across
different environments, and the impact of memory and different fields of view.
While networks with memory and access to horizon-like visual cues achieve best
performance, variants with a narrower field of view achieve better relative
generalization. Our work showcases vision-only flight control as a promising
candidate for enabling autonomous, insect-scale flying robots.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [90] [Perspective-Aware AI in Extended Reality](https://arxiv.org/abs/2507.11479)
*Daniel Platnick,Matti Gruener,Marjan Alirezaie,Kent Larson,Dava J. Newman,Hossein Rahnama*

Main category: cs.AI

TL;DR: PAiR框架通过整合Perspective-Aware AI（PAi）与XR，提供基于用户身份的上下文感知体验，解决了当前系统在用户建模和认知上下文方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前AI增强的XR系统因用户建模浅显和认知上下文有限而无法提供充分的沉浸式体验。

Method: PAiR利用Chronicles（多模态数字足迹学习到的推理就绪身份模型）构建闭环系统，将动态用户状态与沉浸式环境连接。

Result: 通过Unity-based OpenDome引擎实现的两个概念验证场景展示了PAiR的实用性。

Conclusion: PAiR通过将基于视角的身份模型嵌入沉浸式系统，为人机交互开辟了新方向。

Abstract: AI-enhanced Extended Reality (XR) aims to deliver adaptive, immersive
experiences-yet current systems fall short due to shallow user modeling and
limited cognitive context. We introduce Perspective-Aware AI in Extended
Reality (PAiR), a foundational framework for integrating Perspective-Aware AI
(PAi) with XR to enable interpretable, context-aware experiences grounded in
user identity. PAi is built on Chronicles: reasoning-ready identity models
learned from multimodal digital footprints that capture users' cognitive and
experiential evolution. PAiR employs these models in a closed-loop system
linking dynamic user states with immersive environments. We present PAiR's
architecture, detailing its modules and system flow, and demonstrate its
utility through two proof-of-concept scenarios implemented in the Unity-based
OpenDome engine. PAiR opens a new direction for human-AI interaction by
embedding perspective-based identity models into immersive systems.

</details>


### [91] [NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization](https://arxiv.org/abs/2507.10894)
*Zongtao He,Liuyi Wang,Lu Chen,Chengju Liu,Qijun Chen*

Main category: cs.AI

TL;DR: NavComposer是一个自动生成高质量导航指令的框架，通过分解和重组语义实体（如动作、场景和对象）来生成自然语言指令。NavInstrCritic是一个无需标注的评估系统，从对比匹配、语义一致性和语言多样性三个维度评估指令质量。


<details>
  <summary>Details</summary>
Motivation: 现有导航指令数量有限且质量不足，限制了大规模研究。

Method: NavComposer分解语义实体并重组为指令，支持数据无关的适应性；NavInstrCritic提供多维度的无标注评估。

Result: 实验证明该方法有效，支持更可扩展和通用的研究。

Conclusion: NavComposer和NavInstrCritic解决了导航指令生成和评估的瓶颈，推动了更广泛的研究。

Abstract: Language-guided navigation is a cornerstone of embodied AI, enabling agents
to interpret language instructions and navigate complex environments. However,
expert-provided instructions are limited in quantity, while synthesized
annotations often lack quality, making them insufficient for large-scale
research. To address this, we propose NavComposer, a novel framework for
automatically generating high-quality navigation instructions. NavComposer
explicitly decomposes semantic entities such as actions, scenes, and objects,
and recomposes them into natural language instructions. Its modular
architecture allows flexible integration of state-of-the-art techniques, while
the explicit use of semantic entities enhances both the richness and accuracy
of instructions. Moreover, it operates in a data-agnostic manner, supporting
adaptation to diverse navigation trajectories without domain-specific training.
Complementing NavComposer, we introduce NavInstrCritic, a comprehensive
annotation-free evaluation system that assesses navigation instructions on
three dimensions: contrastive matching, semantic consistency, and linguistic
diversity. NavInstrCritic provides a holistic evaluation of instruction
quality, addressing limitations of traditional metrics that rely heavily on
expert annotations. By decoupling instruction generation and evaluation from
specific navigation agents, our method enables more scalable and generalizable
research. Extensive experiments provide direct and practical evidence for the
effectiveness of our method.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [92] [LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis Assessment in Rectal Cancer](https://arxiv.org/abs/2507.11457)
*Yaoxian Dong,Yifan Gao,Haoyue Li,Yanfen Cui,Xin Gao*

Main category: cs.LG

TL;DR: LRMR框架通过两阶段LLM方法提升直肠癌淋巴结转移评估的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统MRI评估和现有AI模型在淋巴结转移诊断中存在性能不足和缺乏可解释性的问题。

Method: LRMR框架分为两阶段：多模态LLM生成结构化报告，文本LLM进行患者间风险排序。

Result: 在117名患者中，LRMR的AUC为0.7917，F1-score为0.7200，优于基线模型。

Conclusion: 两阶段LLM框架为淋巴结转移评估提供了高效、可解释的新方法。

Abstract: Accurate preoperative assessment of lymph node (LN) metastasis in rectal
cancer guides treatment decisions, yet conventional MRI evaluation based on
morphological criteria shows limited diagnostic performance. While some
artificial intelligence models have been developed, they often operate as black
boxes, lacking the interpretability needed for clinical trust. Moreover, these
models typically evaluate nodes in isolation, overlooking the patient-level
context. To address these limitations, we introduce LRMR, an LLM-Driven
Relational Multi-node Ranking framework. This approach reframes the diagnostic
task from a direct classification problem into a structured reasoning and
ranking process. The LRMR framework operates in two stages. First, a multimodal
large language model (LLM) analyzes a composite montage image of all LNs from a
patient, generating a structured report that details ten distinct radiological
features. Second, a text-based LLM performs pairwise comparisons of these
reports between different patients, establishing a relative risk ranking based
on the severity and number of adverse features. We evaluated our method on a
retrospective cohort of 117 rectal cancer patients. LRMR achieved an area under
the curve (AUC) of 0.7917 and an F1-score of 0.7200, outperforming a range of
deep learning baselines, including ResNet50 (AUC 0.7708). Ablation studies
confirmed the value of our two main contributions: removing the relational
ranking stage or the structured prompting stage led to a significant
performance drop, with AUCs falling to 0.6875 and 0.6458, respectively. Our
work demonstrates that decoupling visual perception from cognitive reasoning
through a two-stage LLM framework offers a powerful, interpretable, and
effective new paradigm for assessing lymph node metastasis in rectal cancer.

</details>


### [93] [FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise](https://arxiv.org/abs/2507.10611)
*Mengwen Ye,Yingzi Huangfu,Shujian Gao,Wei Ren,Weifan Liu,Zekuan Yu*

Main category: cs.LG

TL;DR: FedGSCA是一种新颖的联邦学习框架，通过全局样本选择器和客户端自适应调整机制，有效解决医学图像分类中的标签噪声问题，提升模型鲁棒性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 医学联邦学习中标签噪声和数据不平衡问题导致模型性能下降，现有方法难以应对噪声异质性和数据分布不均。

Method: 提出FedGSCA框架，包括全局样本选择器和客户端自适应调整机制，结合伪标签生成和鲁棒标签损失。

Result: 在真实和合成医学数据集上，FedGSCA在极端和异质噪声场景中表现优于现有方法，显著提升模型稳定性。

Conclusion: FedGSCA适用于真实医学联邦学习场景，能有效处理复杂噪声和数据不平衡问题。

Abstract: Federated Learning (FL) emerged as a solution for collaborative medical image
classification while preserving data privacy. However, label noise, which
arises from inter-institutional data variability, can cause training
instability and degrade model performance. Existing FL methods struggle with
noise heterogeneity and the imbalance in medical data. Motivated by these
challenges, we propose FedGSCA, a novel framework for enhancing robustness in
noisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates
noise knowledge from all clients, effectively addressing noise heterogeneity
and improving global model stability. Furthermore, we develop a Client Adaptive
Adjustment (CAA) mechanism that combines adaptive threshold pseudo-label
generation and Robust Credal Labeling Loss. CAA dynamically adjusts to class
distributions, ensuring the inclusion of minority samples and carefully
managing noisy labels by considering multiple plausible labels. This dual
approach mitigates the impact of noisy data and prevents overfitting during
local training, which improves the generalizability of the model. We evaluate
FedGSCA on one real-world colon slides dataset and two synthetic medical
datasets under various noise conditions, including symmetric, asymmetric,
extreme, and heterogeneous types. The results show that FedGSCA outperforms the
state-of-the-art methods, excelling in extreme and heterogeneous noise
scenarios. Moreover, FedGSCA demonstrates significant advantages in improving
model stability and handling complex noise, making it well-suited for
real-world medical federated learning scenarios.

</details>


### [94] [Flows and Diffusions on the Neural Manifold](https://arxiv.org/abs/2507.10623)
*Daniel Saragih,Deyu Cao,Tejas Balaji*

Main category: cs.LG

TL;DR: 该论文将扩散和流式生成模型扩展到权重空间学习，通过梯度流匹配框架统一轨迹推断技术，优化权重生成和初始化，并在安全关键系统中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 扩散和流式生成模型在图像合成等领域已取得显著成功，但尚未广泛应用于权重空间学习。本文旨在利用优化动态的结构先验，扩展这些模型的适用性。

Method: 提出梯度流匹配框架，将梯度下降轨迹建模为轨迹推断问题，结合奖励微调、自编码器、任务上下文条件化和Kaiming均匀分布等技术。

Result: 实验表明，该方法在生成权重、优化初始化和微调性能方面优于基线，并在检测有害协变量偏移的实际应用中表现优异。

Conclusion: 梯度流匹配框架为权重空间学习提供了理论支持，并在生成和优化任务中展现出显著优势。

Abstract: Diffusion and flow-based generative models have achieved remarkable success
in domains such as image synthesis, video generation, and natural language
modeling. In this work, we extend these advances to weight space learning by
leveraging recent techniques to incorporate structural priors derived from
optimization dynamics. Central to our approach is modeling the trajectory
induced by gradient descent as a trajectory inference problem. We unify several
trajectory inference techniques under the framework of gradient flow matching,
providing a theoretical framework for treating optimization paths as inductive
bias. We further explore architectural and algorithmic choices, including
reward fine-tuning by adjoint matching, the use of autoencoders for latent
weight representation, conditioning on task-specific context data, and adopting
informative source distributions such as Kaiming uniform. Experiments
demonstrate that our method matches or surpasses baselines in generating
in-distribution weights, improves initialization for downstream training, and
supports fine-tuning to enhance performance. Finally, we illustrate a practical
application in safety-critical systems: detecting harmful covariate shifts,
where our method outperforms the closest comparable baseline.

</details>


### [95] [A Simple Baseline for Stable and Plastic Neural Networks](https://arxiv.org/abs/2507.10637)
*É. Künzel,A. Jaziri,V. Ramesh*

Main category: cs.LG

TL;DR: RDBP是一种新的持续学习方法，结合ReLUDown和Decreasing Backpropagation机制，平衡可塑性和稳定性，在Continual ImageNet上表现优异且计算成本低。


<details>
  <summary>Details</summary>
Motivation: 解决现有持续学习方法在可塑性和稳定性之间难以平衡的问题。

Method: 结合ReLUDown（轻量级激活修改）和Decreasing Backpropagation（梯度调度方案）。

Result: 在Continual ImageNet上表现优于或匹配现有方法，同时降低计算成本。

Conclusion: RDBP为持续学习提供了实用解决方案和清晰的基准。

Abstract: Continual learning in computer vision requires that models adapt to a
continuous stream of tasks without forgetting prior knowledge, yet existing
approaches often tip the balance heavily toward either plasticity or stability.
We introduce RDBP, a simple, low-overhead baseline that unites two
complementary mechanisms: ReLUDown, a lightweight activation modification that
preserves feature sensitivity while preventing neuron dormancy, and Decreasing
Backpropagation, a biologically inspired gradient-scheduling scheme that
progressively shields early layers from catastrophic updates. Evaluated on the
Continual ImageNet benchmark, RDBP matches or exceeds the plasticity and
stability of state-of-the-art methods while reducing computational cost. RDBP
thus provides both a practical solution for real-world continual learning and a
clear benchmark against which future continual learning strategies can be
measured.

</details>


### [96] [Spatial Reasoners for Continuous Variables in Any Domain](https://arxiv.org/abs/2507.10768)
*Bart Pogodzinski,Christopher Wewer,Bernt Schiele,Jan Eric Lenssen*

Main category: cs.LG

TL;DR: Spatial Reasoners是一个软件框架，用于通过生成去噪模型对连续变量进行空间推理。


<details>
  <summary>Details</summary>
Motivation: 生成去噪模型在图像生成中已成为标准，但在多连续变量推理中的应用尚需基础设施支持。

Method: 提供易于使用的接口，支持变量映射、生成模型范式和推理策略的灵活控制。

Result: 框架开源，旨在促进该领域的研究。

Conclusion: Spatial Reasoners为生成推理提供了高效工具，降低了研究门槛。

Abstract: We present Spatial Reasoners, a software framework to perform spatial
reasoning over continuous variables with generative denoising models. Denoising
generative models have become the de-facto standard for image generation, due
to their effectiveness in sampling from complex, high-dimensional
distributions. Recently, they have started being explored in the context of
reasoning over multiple continuous variables. Providing infrastructure for
generative reasoning with such models requires a high effort, due to a wide
range of different denoising formulations, samplers, and inference strategies.
Our presented framework aims to facilitate research in this area, providing
easy-to-use interfaces to control variable mapping from arbitrary data domains,
generative model paradigms, and inference strategies. Spatial Reasoners are
openly available at https://spatialreasoners.github.io/

</details>


### [97] [First-Order Error Matters: Accurate Compensation for Quantized Large Language Models](https://arxiv.org/abs/2507.11017)
*Xingyu Zheng,Haotong Qin,Yuye Li,Jiakai Wang,Jinyang Guo,Michele Magno,Xianglong Liu*

Main category: cs.LG

TL;DR: FOEM是一种新的后训练量化方法，通过显式引入一阶梯度项改进量化误差补偿，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有补偿式权重校准方法假设一阶项可忽略，但实际中渐进补偿过程会引入累积的一阶偏差，导致假设不成立。

Method: FOEM通过直接计算潜在权重与全精度权重的差值近似梯度，避免反向传播的高成本，并利用预计算的Cholesky因子实时恢复Hessian子矩阵的逆。

Result: FOEM在多种模型和基准测试中表现优异，3位量化下Llama3-8B困惑度降低89.6%，Llama3-70B的5-shot MMLU准确率从51.7%提升至74.9%。

Conclusion: FOEM显著缩小与全精度基线的差距，并能与先进技术无缝集成，进一步优化性能。

Abstract: Post-training quantization (PTQ) offers an efficient approach to compressing
large language models (LLMs), significantly reducing memory access and
computational costs. Existing compensation-based weight calibration methods
often rely on a second-order Taylor expansion to model quantization error,
under the assumption that the first-order term is negligible in well-trained
full-precision models. However, we reveal that the progressive compensation
process introduces accumulated first-order deviations between latent weights
and their full-precision counterparts, making this assumption fundamentally
flawed. To address this, we propose FOEM, a novel PTQ method that explicitly
incorporates first-order gradient terms to improve quantization error
compensation. FOEM approximates gradients by directly computing the difference
between latent and full-precision weights, avoiding the high cost and limited
generalization of backpropagation-based gradient computation. This approach
introduces minimal additional computational overhead. Moreover, FOEM leverages
precomputed Cholesky factors to efficiently recover the inverse of Hessian
submatrices in real time. Extensive experiments across a wide range of models
and benchmarks demonstrate that FOEM consistently outperforms the classical
GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of
Llama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from
51.7% to 74.9%, approaching the full-precision performance of 78.6%.
Furthermore, FOEM can be seamlessly integrated with advanced techniques such as
GPTAQ and SpinQuant, yielding additional improvements under the challenging
W4A4KV4 setting, and further narrowing the accuracy gap with full-precision
baselines beyond what current state-of-the-art methods achieve. The code is
available at https://github.com/Xingyu-Zheng/FOEM.

</details>


### [98] [LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection](https://arxiv.org/abs/2507.11071)
*Isaiah Thompson Ocansey,Ritwik Bhattacharya,Tanmay Sen*

Main category: cs.LG

TL;DR: 论文提出了一种基于LoRA和适配器的高效微调方法，用于检测大规模日志数据中的异常序列，相比传统方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则或深度学习的日志异常检测方法在处理大规模复杂日志序列时效果有限，需要更高效的方法。

Method: 采用参数高效微调技术（如LoRA和适配器），在Thunderbird数据集上比较不同小型大语言模型（LLMs）。

Result: LoRA微调方法比LogBert全微调方法性能提升18-19%，准确率达到97.76%-98.83%，而后者仅为79.37%。

Conclusion: LoRA微调方法在日志异常检测中表现出色，为系统维护和开发提供了更高效的解决方案。

Abstract: Log anomaly detection using traditional rule based or deep learning based
methods is often challenging due to the large volume and highly complex nature
of log sequence. So effective way of detection of anomalous sequence of logs is
crucial for system maintenance and development. This paper proposes parameter
efficient finetuning specifically low rank adaptation (LoRA) and adapter based
approaches for finding contextual anomalies in sequence of logs in large log
data set. It compares different tiny large language models (LLMs) on the
Thunderbird dataset. The results show that LoRA based finetuning provides
substantial performance improvements of 18 to 19 percentage over LogBert based
full finetuning approach, achieving accuracy scores between 97.76% and 98.83%
compared to 79.37%.

</details>
