<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 73]
- [eess.IV](#eess.IV) [Total: 12]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.LG](#cs.LG) [Total: 5]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer](https://arxiv.org/abs/2508.14187)
*Md Ashiqur Rahman,Chiao-An Yang,Michael N. Cheng,Lim Jun Hao,Jeremiah Jiang,Teck-Yian Lim,Raymond A. Yeh*

Main category: cs.CV

TL;DR: 提出深度均衡规范化器(DEC)来解决计算机视觉中的尺度变化问题，提高模型的局部尺度等变性，可轻松集成到现有网络架构中并在ImageNet基准测试中提升性能


<details>
  <summary>Details</summary>
Motivation: 解决计算机视觉中物体尺度变化的基本挑战，同一类别的物体可能有不同大小，且感知大小还受相机距离影响，这些变化是局部的

Method: 提出深度均衡规范化器(DEC)，可以改善模型的局部尺度等变性，能够轻松集成到现有网络架构中，并可适配预训练模型

Result: 在竞争性的ImageNet基准测试中，DEC提高了四个流行预训练深度网络(ViT、DeiT、Swin、BEiT)的模型性能和局部尺度一致性

Conclusion: DEC是处理尺度变化的有效方法，能够提升现有模型的性能和尺度一致性，代码已开源

Abstract: Scale variation is a fundamental challenge in computer vision. Objects of the
same class can have different sizes, and their perceived size is further
affected by the distance from the camera. These variations are local to the
objects, i.e., different object sizes may change differently within the same
image. To effectively handle scale variations, we present a deep equilibrium
canonicalizer (DEC) to improve the local scale equivariance of a model. DEC can
be easily incorporated into existing network architectures and can be adapted
to a pre-trained model. Notably, we show that on the competitive ImageNet
benchmark, DEC improves both model performance and local scale consistency
across four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT. Our
code is available at https://github.com/ashiq24/local-scale-equivariance.

</details>


### [2] [A comparative study of some wavelet and sampling operators on various features of an image](https://arxiv.org/abs/2508.14043)
*Digvijay Singh,Rahul Shukla,Karunesh Kumar Singh*

Main category: cs.CV

TL;DR: 本文研究了多种正采样Kantorovich算子（SK算子）及其收敛性质，包括SK、高斯、双边和阈值小波算子，分析了局部和全局逼近特性，并通过数值实验验证了基本逼近定理。


<details>
  <summary>Details</summary>
Motivation: 研究不同采样Kantorovich算子在图像处理中的逼近性能，特别是在非理想条件下对各种图像特征的处理效果，为选择合适的算子提供理论依据。

Method: 采用多种算子（SK、高斯、双边、阈值小波）进行理论分析和数值实验，通过MSE、SI、SSI、SMPI、ENL等指标评估性能，并使用2D Shepp-Logan Phantom图像进行验证。

Result: 不同算子在处理图像不同特征时表现各异，某些算子对特定特征效果好，而其他算子可能不适用，验证了基本逼近定理的正确性。

Conclusion: 由于图像的非均匀特性，没有单一最优算子，需要根据具体图像特征选择适当的采样Kantorovich算子，各种算子各有其适用场景。

Abstract: This research includes the study of some positive sampling Kantorovich
operators (SK operators) and their convergence properties. A comprehensive
analysis of both local and global approximation properties is presented using
sampling Kantorovich (SK), Gaussian, Bilateral and the thresholding
wavelet-based operators in the framework of SK-operators. Explicitly, we start
the article by introducing the basic terminology and state the fundamental
theorem of approximation (FTA) by imposing the various required conditions
corresponding to the various defined operators. We measure the error and study
the other mathematical parameters such as the mean square error (MSE), the
speckle index (SI), the speckle suppression index (SSI), the speckle mean
preservation index (SMPI), and the equivalent number of looks (ENL) at various
levels of resolution parameters. The nature of these operators are demonstrated
via an example under ideal conditions in tabulated form at a certain level of
samples. Eventually, another numerical example is illustrated to discuss the
region of interest (ROI) via SI, SSI and SMPI of 2D Shepp-Logan Phantom taken
slice from the 3D image, which gives the justification of the fundamental
theorem of approximation (FTA). At the end of the derivation and illustrations
we observe that the various operators have their own significance while
studying the various features of the image because of the uneven nature of an
image (non-ideal condition). Therefore, to some extent, some operators work
well and some do not for some specific features of the image.

</details>


### [3] [Federated Action Recognition for Smart Worker Assistance Using FastPose](https://arxiv.org/abs/2508.14113)
*Vinit Hegiste,Vidit Goyal,Tatjana Legler,Martin Ruskowski*

Main category: cs.CV

TL;DR: 提出基于联邦学习的姿态动作识别框架，在隐私保护的工业场景中显著提升跨用户泛化能力，FedEnsemble方法比集中式训练准确率提升16.3个百分点


<details>
  <summary>Details</summary>
Motivation: 智能制造环境中需要实时准确识别工人动作，但传统集中式方法在隐私敏感场景中不实用，需要隐私保护的解决方案

Method: 使用自定义骨骼数据集和修改的FastPose模型，比较LSTM和Transformer两种时序骨干网络，在四种训练范式（集中式、本地、FedAvg、FedEnsemble）下进行评估

Result: 联邦学习Transformer在全局测试集上比集中式训练提升12.4个百分点，FedEnsemble提升16.3个百分点；在未见外部客户端上，联邦学习和FedEnsemble分别比集中式准确率提升52.6和58.3个百分点

Conclusion: 联邦学习不仅能保护隐私，还能显著增强跨用户泛化能力，是异构工业环境中可扩展、隐私感知的人体活动识别实用解决方案

Abstract: In smart manufacturing environments, accurate and real-time recognition of
worker actions is essential for productivity, safety, and human-machine
collaboration. While skeleton-based human activity recognition (HAR) offers
robustness to lighting, viewpoint, and background variations, most existing
approaches rely on centralized datasets, which are impractical in
privacy-sensitive industrial scenarios. This paper presents a federated
learning (FL) framework for pose-based HAR using a custom skeletal dataset of
eight industrially relevant upper-body gestures, captured from five
participants and processed using a modified FastPose model. Two temporal
backbones, an LSTM and a Transformer encoder, are trained and evaluated under
four paradigms: centralized, local (per-client), FL with weighted federated
averaging (FedAvg), and federated ensemble learning (FedEnsemble). On the
global test set, the FL Transformer improves over centralized training by +12.4
percentage points, with FedEnsemble delivering a +16.3 percentage points gain.
On an unseen external client, FL and FedEnsemble exceed centralized accuracy by
+52.6 and +58.3 percentage points, respectively. These results demonstrate that
FL not only preserves privacy but also substantially enhances cross-user
generalization, establishing it as a practical solution for scalable,
privacy-aware HAR in heterogeneous industrial settings.

</details>


### [4] [LENS: Learning to Segment Anything with Unified Reinforced Reasoning](https://arxiv.org/abs/2508.14153)
*Lianghui Zhu,Bin Ouyang,Yuxuan Zhang,Tianheng Cheng,Rui Hu,Haocheng Shen,Longjin Ran,Xiaoxin Chen,Li Yu,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: LENS是一个基于强化学习的端到端框架，通过联合优化推理过程和分割任务，在文本提示图像分割中实现了更好的泛化性能


<details>
  <summary>Details</summary>
Motivation: 现有的监督微调方法在测试时忽略了显式的思维链推理，限制了模型对未见提示和领域的泛化能力

Method: 提出统一的强化学习奖励机制，涵盖句子级、边界框级和分割级线索，鼓励模型生成信息丰富的思维链推理同时提升掩码质量

Result: 在RefCOCO、RefCOCO+和RefCOCOg基准测试上达到81.2%的平均cIoU，比GLaMM方法提升高达5.6%

Conclusion: 强化学习驱动的思维链推理为文本提示分割提供了强大的先验，是实现更通用分割模型的有效路径

Abstract: Text-prompted image segmentation enables fine-grained visual understanding
and is critical for applications such as human-computer interaction and
robotics. However, existing supervised fine-tuning methods typically ignore
explicit chain-of-thought (CoT) reasoning at test time, which limits their
ability to generalize to unseen prompts and domains. To address this issue, we
introduce LENS, a scalable reinforcement-learning framework that jointly
optimizes the reasoning process and segmentation in an end-to-end manner. We
propose unified reinforcement-learning rewards that span sentence-, box-, and
segment-level cues, encouraging the model to generate informative CoT
rationales while refining mask quality. Using a publicly available
3-billion-parameter vision-language model, i.e., Qwen2.5-VL-3B-Instruct, LENS
achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg
benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to
5.6%. These results demonstrate that RL-driven CoT reasoning serves as a robust
prior for text-prompted segmentation and offers a practical path toward more
generalizable Segment Anything models. Code is available at
https://github.com/hustvl/LENS.

</details>


### [5] [RynnEC: Bringing MLLMs into Embodied World](https://arxiv.org/abs/2508.14160)
*Ronghao Dang,Yuqian Yuan,Yunxuan Mao,Kehan Li,Jiangpin Liu,Zhikai Wang,Xin Li,Fan Wang,Deli Zhao*

Main category: cs.CV

TL;DR: RynnEC是一个用于具身认知的视频多模态大语言模型，通过区域编码器和掩码解码器实现灵活的区域级视频交互，在物体属性理解、分割和空间推理方面达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决具身智能体需要精细感知物理世界并进行精确交互的需求，以及标注3D数据稀缺的问题。

Method: 基于通用视觉语言基础模型，加入区域编码器和掩码解码器，提出基于第一人称视频的具身认知数据生成流程。

Result: 在紧凑架构下实现了最先进的性能，并创建了RynnEC-Bench评估基准。

Conclusion: RynnEC为具身智能体提供了区域中心的视频范式，将推动通用认知核心的发展并促进跨任务的泛化能力。

Abstract: We introduce RynnEC, a video multimodal large language model designed for
embodied cognition. Built upon a general-purpose vision-language foundation
model, RynnEC incorporates a region encoder and a mask decoder, enabling
flexible region-level video interaction. Despite its compact architecture,
RynnEC achieves state-of-the-art performance in object property understanding,
object segmentation, and spatial reasoning. Conceptually, it offers a
region-centric video paradigm for the brain of embodied agents, providing
fine-grained perception of the physical world and enabling more precise
interactions. To mitigate the scarcity of annotated 3D datasets, we propose an
egocentric video based pipeline for generating embodied cognition data.
Furthermore, we introduce RynnEC-Bench, a region-centered benchmark for
evaluating embodied cognitive capabilities. We anticipate that RynnEC will
advance the development of general-purpose cognitive cores for embodied agents
and facilitate generalization across diverse embodied tasks. The code, model
checkpoints, and benchmark are available at:
https://github.com/alibaba-damo-academy/RynnEC

</details>


### [6] [CLIPSym: Delving into Symmetry Detection with CLIP](https://arxiv.org/abs/2508.14197)
*Tinghan Yang,Md Ashiqur Rahman,Raymond A. Yeh*

Main category: cs.CV

TL;DR: CLIPSym利用CLIP视觉语言模型的预训练能力，通过语义感知提示分组和旋转等变解码器，在对称性检测任务上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 对称性是计算机视觉中最基本的几何线索之一，但检测对称性一直是一个挑战。研究探索预训练的CLIP模型是否能利用自然图像描述中的对称线索来辅助对称性检测。

Method: 提出CLIPSym方法，结合CLIP的图像和语言编码器，以及基于Transformer和G-卷积混合的旋转等变解码器来检测旋转和反射对称性。开发了语义感知提示分组(SAPG)技术来聚合多样化的基于对象的提示。

Result: 在三个标准对称性检测数据集(DENDI、SDRW和LDRS)上超越了当前最先进的方法。消融实验验证了CLIP预训练、等变解码器和SAPG技术的有效性。

Conclusion: CLIPSym成功证明了预训练视觉语言模型在对称性检测任务中的价值，通过创新的提示技术和等变架构设计，显著提升了对称性检测的性能。

Abstract: Symmetry is one of the most fundamental geometric cues in computer vision,
and detecting it has been an ongoing challenge. With the recent advances in
vision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP
model can aid symmetry detection by leveraging the additional symmetry cues
found in the natural image descriptions. We propose CLIPSym, which leverages
CLIP's image and language encoders and a rotation-equivariant decoder based on
a hybrid of Transformer and $G$-Convolution to detect rotation and reflection
symmetries. To fully utilize CLIP's language encoder, we have developed a novel
prompting technique called Semantic-Aware Prompt Grouping (SAPG), which
aggregates a diverse set of frequent object-based prompts to better integrate
the semantic cues for symmetry detection. Empirically, we show that CLIPSym
outperforms the current state-of-the-art on three standard symmetry detection
datasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations
verifying the benefits of CLIP's pre-training, the proposed equivariant
decoder, and the SAPG technique. The code is available at
https://github.com/timyoung2333/CLIPSym.

</details>


### [7] [A Survey on Video Anomaly Detection via Deep Learning: Human, Vehicle, and Environment](https://arxiv.org/abs/2508.14203)
*Ghazal Alinezhad Noghre,Armin Danesh Pazho,Hamed Tabkhi*

Main category: cs.CV

TL;DR: 这是一篇关于视频异常检测(VAD)的综述论文，系统性地整理了该领域在不同监督级别、学习范式和应用场景下的研究现状，旨在为该领域提供结构化基础并推动理论和实际应用的发展。


<details>
  <summary>Details</summary>
Motivation: 视频异常检测在计算机视觉中具有重要意义，但该领域在不同领域和学习范式之间存在碎片化现象，需要系统性的整理和综述来为研究者提供参考并推动领域发展。

Method: 采用系统性文献综述方法，从监督级别(包括不同监督水平)、自适应学习方法(在线学习、主动学习、持续学习)以及三大应用类别(人本中心、车辆中心、环境中心)三个维度对VAD文献进行组织和分析。

Result: 识别了当前方法的基本贡献和局限性，为社区提供了结构化的基础，有助于推动VAD系统的理论理解和实际应用。

Conclusion: 该综述为研究者提供了有用的参考，同时指出了异常检测领域更广泛的开放挑战，包括基础研究问题和实际部署障碍，旨在促进VAD领域的进一步发展。

Abstract: Video Anomaly Detection (VAD) has emerged as a pivotal task in computer
vision, with broad relevance across multiple fields. Recent advances in deep
learning have driven significant progress in this area, yet the field remains
fragmented across domains and learning paradigms. This survey offers a
comprehensive perspective on VAD, systematically organizing the literature
across various supervision levels, as well as adaptive learning methods such as
online, active, and continual learning. We examine the state of VAD across
three major application categories: human-centric, vehicle-centric, and
environment-centric scenarios, each with distinct challenges and design
considerations. In doing so, we identify fundamental contributions and
limitations of current methodologies. By consolidating insights from subfields,
we aim to provide the community with a structured foundation for advancing both
theoretical understanding and real-world applicability of VAD systems. This
survey aims to support researchers by providing a useful reference, while also
drawing attention to the broader set of open challenges in anomaly detection,
including both fundamental research questions and practical obstacles to
real-world deployment.

</details>


### [8] [Accelerating Image Classification with Graph Convolutional Neural Networks using Voronoi Diagrams](https://arxiv.org/abs/2508.14218)
*Mustafa Mohammadi Gharasuie,Luis Rueda*

Main category: cs.CV

TL;DR: 该研究提出了一种结合Voronoi图和图卷积网络(GCN)的创新图像分类框架NVGCN，通过图结构表示图像并利用Delaunay三角剖分简化处理，在多个基准数据集上实现了预处理时间和分类准确率的显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络在处理复杂数据结构时存在局限，需要探索新的范式来处理图像中的关系数据，特别是复杂场景和细粒度分类任务。

Method: 使用基于图的图像表示方法，将像素或区域视为图顶点，通过Delaunay三角剖分进行简化，并提出了归一化Voronoi图卷积网络(NVGCN)新架构。

Result: 在多个基准数据集上实验验证显示，该方法在预处理时间和分类准确率方面显著优于现有最先进模型，特别是在复杂场景和细粒度分类任务中表现突出。

Conclusion: 该研究证明了GCN与Voronoi图结合在图像分类中的潜力，为计算机视觉和非结构化数据领域的图基学习范式开辟了新途径，提出的NVGCN比常规GCN更快速高效。

Abstract: Recent advances in image classification have been significantly propelled by
the integration of Graph Convolutional Networks (GCNs), offering a novel
paradigm for handling complex data structures. This study introduces an
innovative framework that employs GCNs in conjunction with Voronoi diagrams to
peform image classification, leveraging their exceptional capability to model
relational data. Unlike conventional convolutional neural networks, our
approach utilizes a graph-based representation of images, where pixels or
regions are treated as vertices of a graph, which are then simplified in the
form of the corresponding Delaunay triangulations. Our model yields significant
improvement in pre-processing time and classification accuracy on several
benchmark datasets, surpassing existing state-of-the-art models, especially in
scenarios that involve complex scenes and fine-grained categories. The
experimental results, validated via cross-validation, underscore the potential
of integrating GCNs with Voronoi diagrams in advancing image classification
tasks. This research contributes to the field by introducing a novel approach
to image classification, while opening new avenues for developing graph-based
learning paradigms in other domains of computer vision and non-structured data.
In particular, we have proposed a new version of the GCN in this paper, namely
normalized Voronoi Graph Convolution Network (NVGCN), which is faster than the
regular GCN.

</details>


### [9] [Directed-Tokens: A Robust Multi-Modality Alignment Approach to Large Language-Vision Models](https://arxiv.org/abs/2508.14264)
*Thanh-Dat Truong,Huu-Thien Tran,Tran Thai Son,Bhiksha Raj,Khoa Luu*

Main category: cs.CV

TL;DR: 提出了一种通过解决视觉-文本特征排序问题来提升大型多模态模型鲁棒性和泛化能力的新方法，包括图像和文本顺序重构任务以及定向标记技术


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型在视觉和文本特征对齐方面存在鲁棒性和泛化性不足的问题，需要改进跨模态对齐能力

Method: 引入两个新任务：图像顺序重构和文本顺序重构；提出定向标记方法捕获视觉和文本知识；使用图像到响应引导损失提升视觉理解

Result: 在学术任务导向和指令跟随基准测试中 consistently 达到最先进性能

Conclusion: 该方法通过解决排序问题有效提升了多模态模型的推理能力、视觉理解和跨模态对齐性能

Abstract: Large multimodal models (LMMs) have gained impressive performance due to
their outstanding capability in various understanding tasks. However, these
models still suffer from some fundamental limitations related to robustness and
generalization due to the alignment and correlation between visual and textual
features. In this paper, we introduce a simple but efficient learning mechanism
for improving the robust alignment between visual and textual modalities by
solving shuffling problems. In particular, the proposed approach can improve
reasoning capability, visual understanding, and cross-modality alignment by
introducing two new tasks: reconstructing the image order and the text order
into the LMM's pre-training and fine-tuning phases. In addition, we propose a
new directed-token approach to capture visual and textual knowledge, enabling
the capability to reconstruct the correct order of visual inputs. Then, we
introduce a new Image-to-Response Guided loss to further improve the visual
understanding of the LMM in its responses. The proposed approach consistently
achieves state-of-the-art (SoTA) performance compared with prior LMMs on
academic task-oriented and instruction-following LMM benchmarks.

</details>


### [10] [Effect of Data Augmentation on Conformal Prediction for Diabetic Retinopathy](https://arxiv.org/abs/2508.14266)
*Rizwan Ahamed,Annahita Amireskandari,Joel Palko,Carol Laxson,Binod Bhattarai,Prashnna Gyawali*

Main category: cs.CV

TL;DR: 本研究系统分析了不同数据增强策略对糖尿病视网膜病变分级中保形预测器性能的影响，发现Mixup和CutMix等样本混合策略不仅能提高预测准确性，还能产生更可靠和高效的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在高风险医疗任务（如糖尿病视网膜病变分级）中需要可靠的不确定性量化。虽然保形预测能提供统计保证的预测集，但标准训练实践（如数据增强）与这些保证有效性之间的相互作用尚不明确。

Method: 使用DDR数据集，评估ResNet-50和Co-Scale Conv-Attentional Transformer两种架构在五种增强策略下的表现：无增强、标准几何变换、CLAHE、Mixup和CutMix，分析对保形预测指标的影响。

Result: 样本混合策略（Mixup和CutMix）不仅提高了预测准确性，还产生了更可靠和高效的不确定性估计；而CLAHE等方法可能对模型确定性产生负面影响。

Conclusion: 需要将增强策略与下游不确定性量化协同设计，以构建真正值得信赖的医学影像AI系统。

Abstract: The clinical deployment of deep learning models for high-stakes tasks such as
diabetic retinopathy (DR) grading requires demonstrable reliability. While
models achieve high accuracy, their clinical utility is limited by a lack of
robust uncertainty quantification. Conformal prediction (CP) offers a
distribution-free framework to generate prediction sets with statistical
guarantees of coverage. However, the interaction between standard training
practices like data augmentation and the validity of these guarantees is not
well understood. In this study, we systematically investigate how different
data augmentation strategies affect the performance of conformal predictors for
DR grading. Using the DDR dataset, we evaluate two backbone architectures --
ResNet-50 and a Co-Scale Conv-Attentional Transformer (CoaT) -- trained under
five augmentation regimes: no augmentation, standard geometric transforms,
CLAHE, Mixup, and CutMix. We analyze the downstream effects on conformal
metrics, including empirical coverage, average prediction set size, and correct
efficiency. Our results demonstrate that sample-mixing strategies like Mixup
and CutMix not only improve predictive accuracy but also yield more reliable
and efficient uncertainty estimates. Conversely, methods like CLAHE can
negatively impact model certainty. These findings highlight the need to
co-design augmentation strategies with downstream uncertainty quantification in
mind to build genuinely trustworthy AI systems for medical imaging.

</details>


### [11] [Tooth-Diffusion: Guided 3D CBCT Synthesis with Fine-Grained Tooth Conditioning](https://arxiv.org/abs/2508.14276)
*Said Djafar Said,Torkan Gholamalizadeh,Mostafa Mehdipour Ghazi*

Main category: cs.CV

TL;DR: 提出了一种基于条件扩散框架的3D牙齿CBCT扫描生成方法，通过牙齿级二元属性实现精确控制，支持牙齿添加、移除和全牙列合成等任务。


<details>
  <summary>Details</summary>
Motivation: 牙科CBCT扫描在诊断和治疗规划中日益重要，但生成具有精细控制的解剖学真实扫描仍然是一个挑战。需要一种能够实现局部牙齿修改而不需要重新扫描的方法。

Method: 采用基于小波的去噪扩散模型，结合FiLM条件调节和掩码损失函数，专注于相关解剖结构的学习，通过牙齿级二元属性进行精确控制。

Result: 模型在多种任务上表现出色，FID分数低，修复性能强，SSIM值超过0.91（即使在未见过的扫描上），显示出强大的保真度和泛化能力。

Conclusion: 该方法能够实现真实、局部的牙列修改，为外科规划、患者沟通和牙科AI工作流程中的针对性数据增强开辟了新机会。

Abstract: Despite the growing importance of dental CBCT scans for diagnosis and
treatment planning, generating anatomically realistic scans with fine-grained
control remains a challenge in medical image synthesis. In this work, we
propose a novel conditional diffusion framework for 3D dental volume
generation, guided by tooth-level binary attributes that allow precise control
over tooth presence and configuration. Our approach integrates wavelet-based
denoising diffusion, FiLM conditioning, and masked loss functions to focus
learning on relevant anatomical structures. We evaluate the model across
diverse tasks, such as tooth addition, removal, and full dentition synthesis,
using both paired and distributional similarity metrics. Results show strong
fidelity and generalization with low FID scores, robust inpainting performance,
and SSIM values above 0.91 even on unseen scans. By enabling realistic,
localized modification of dentition without rescanning, this work opens
opportunities for surgical planning, patient communication, and targeted data
augmentation in dental AI workflows. The codes are available at:
https://github.com/djafar1/tooth-diffusion.

</details>


### [12] [GALA: Guided Attention with Language Alignment for Open Vocabulary Gaussian Splatting](https://arxiv.org/abs/2508.14278)
*Elena Alegret Regalado,Kunyi Li,Sen Wang,Siyun Liang,Michael Niemeyer,Stefano Gasperini,Nassir Navab,Federico Tombari*

Main category: cs.CV

TL;DR: GALA是一个基于3D高斯泼溅的新型开放词汇3D场景理解框架，通过自监督对比学习和交叉注意力模块实现语言感知的3D表示，在2D和3D开放词汇查询方面表现优异


<details>
  <summary>Details</summary>
Motivation: 现有方法难以从2D图像中捕获细粒度、语言感知的3D表示，需要开发能够支持开放词汇查询的高效3D场景理解方法

Method: 使用3D高斯泼溅技术，通过自监督对比学习提炼场景特定的3D实例特征场，引入带有两个可学习码本的交叉注意力模块来编码视图无关的语义嵌入

Result: 在真实世界数据集上的广泛实验表明，GALA在2D和3D开放词汇查询方面都表现出色，同时通过避免每个高斯的高维特征学习来减少内存消耗

Conclusion: GALA框架成功实现了开放词汇的3D场景理解，在保持特征相似性的同时支持无缝的2D和3D查询，为3D场景重建和理解提供了有效的解决方案

Abstract: 3D scene reconstruction and understanding have gained increasing popularity,
yet existing methods still struggle to capture fine-grained, language-aware 3D
representations from 2D images. In this paper, we present GALA, a novel
framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting
(3DGS). GALA distills a scene-specific 3D instance feature field via
self-supervised contrastive learning. To extend to generalized language feature
fields, we introduce the core contribution of GALA, a cross-attention module
with two learnable codebooks that encode view-independent semantic embeddings.
This design not only ensures intra-instance feature similarity but also
supports seamless 2D and 3D open-vocabulary queries. It reduces memory
consumption by avoiding per-Gaussian high-dimensional feature learning.
Extensive experiments on real-world datasets demonstrate GALA's remarkable
open-vocabulary performance on both 2D and 3D.

</details>


### [13] [Multi-Rationale Explainable Object Recognition via Contrastive Conditional Inference](https://arxiv.org/abs/2508.14280)
*Ali Rasekh,Sepehr Kazemi Ranjbar,Simon Gottschalk*

Main category: cs.CV

TL;DR: 提出了一个多理由可解释物体识别基准和对比条件推理框架，通过建模图像嵌入、类别标签和理由之间的概率关系，在无需训练的情况下实现更有效的条件推理，在分类准确性和理由质量方面达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖基于提示的条件化，受限于CLIP文本编码器的局限性，且提供的解释结构条件化较弱。先前数据集通常只有单一且嘈杂的理由，无法捕捉判别性图像特征的完整多样性。

Method: 提出了对比条件推理（CCI）框架，显式建模图像嵌入、类别标签和理由之间的概率关系。该框架无需训练即可实现更有效的理由条件化来预测准确的物体类别。

Result: 在多理由可解释物体识别基准上取得了最先进的结果，包括强大的零样本性能，在分类准确性和理由质量方面都设立了新标准。

Conclusion: 这项工作与基准一起为评估未来可解释物体识别模型提供了更完整的框架，代码将在线提供。

Abstract: Explainable object recognition using vision-language models such as CLIP
involves predicting accurate category labels supported by rationales that
justify the decision-making process. Existing methods typically rely on
prompt-based conditioning, which suffers from limitations in CLIP's text
encoder and provides weak conditioning on explanatory structures. Additionally,
prior datasets are often restricted to single, and frequently noisy, rationales
that fail to capture the full diversity of discriminative image features. In
this work, we introduce a multi-rationale explainable object recognition
benchmark comprising datasets in which each image is annotated with multiple
ground-truth rationales, along with evaluation metrics designed to offer a more
comprehensive representation of the task. To overcome the limitations of
previous approaches, we propose a contrastive conditional inference (CCI)
framework that explicitly models the probabilistic relationships among image
embeddings, category labels, and rationales. Without requiring any training,
our framework enables more effective conditioning on rationales to predict
accurate object categories. Our approach achieves state-of-the-art results on
the multi-rationale explainable object recognition benchmark, including strong
zero-shot performance, and sets a new standard for both classification accuracy
and rationale quality. Together with the benchmark, this work provides a more
complete framework for evaluating future models in explainable object
recognition. The code will be made available online.

</details>


### [14] [OccluNet: Spatio-Temporal Deep Learning for Occlusion Detection on DSA](https://arxiv.org/abs/2508.14286)
*Anushka A. Kore,Frank G. te Nijenhuis,Matthijs van der Sluijs,Wim van Zwam,Charles Majoie,Geert Lycklama à Nijeholt,Danny Ruijters,Frans Vos,Sandra Cornelissen,Ruisheng Su,Theo van Walsum*

Main category: cs.CV

TL;DR: OccluNet是一个用于自动检测急性缺血性卒中DSA序列中血管闭塞的深度学习模型，结合YOLOX目标检测器和transformer时间注意力机制，在精度和召回率上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 急性缺血性卒中血管内取栓术中准确检测血管闭塞至关重要，但由于解剖结构复杂和时间紧迫，DSA序列的解读具有挑战性，需要自动化解决方案。

Method: 提出OccluNet模型，集成YOLOX单阶段目标检测器和基于transformer的时间注意力机制，探索纯时间注意力和分时空间注意力两种变体，并与基于单帧或最小强度投影的YOLOv11基线进行比较。

Result: 在MR CLEAN Registry的DSA图像上评估，模型能够捕捉时间一致性特征，达到89.02%的精度和74.87%的召回率，显著优于基线模型，两种注意力变体性能相近。

Conclusion: OccluNet成功实现了DSA序列中血管闭塞的自动化检测，为急性缺血性卒中的快速诊断提供了有效的深度学习解决方案。

Abstract: Accurate detection of vascular occlusions during endovascular thrombectomy
(EVT) is critical in acute ischemic stroke (AIS). Interpretation of digital
subtraction angiography (DSA) sequences poses challenges due to anatomical
complexity and time constraints. This work proposes OccluNet, a spatio-temporal
deep learning model that integrates YOLOX, a single-stage object detector, with
transformer-based temporal attention mechanisms to automate occlusion detection
in DSA sequences. We compared OccluNet with a YOLOv11 baseline trained on
either individual DSA frames or minimum intensity projections. Two
spatio-temporal variants were explored for OccluNet: pure temporal attention
and divided space-time attention. Evaluation on DSA images from the MR CLEAN
Registry revealed the model's capability to capture temporally consistent
features, achieving precision and recall of 89.02% and 74.87%, respectively.
OccluNet significantly outperformed the baseline models, and both attention
variants attained similar performance. Source code is available at
https://github.com/anushka-kore/OccluNet.git

</details>


### [15] [Improving OCR using internal document redundancy](https://arxiv.org/abs/2508.14557)
*Diego Belzarena,Seginus Mowlavi,Aitor Artola,Camilo Mariño,Marina Gardella,Ignacio Ramírez,Antoine Tadros,Roy He,Natalia Bottaioli,Boshra Rajaei,Gregory Randall,Jean-Michel Morel*

Main category: cs.CV

TL;DR: 提出一种利用文档内字符形状冗余的无监督OCR后处理方法，通过扩展高斯混合模型和统计测试来纠正OCR输出错误


<details>
  <summary>Details</summary>
Motivation: 当前OCR系统在处理低质量文档时表现不佳，特别是印刷文档的域间变异性高而域内变异性低，未能充分利用文档内部的冗余信息

Method: 扩展高斯混合模型(GMM)，交替使用期望最大化(EM)算法和簇内重新对齐过程，结合正态性统计测试来优化字符聚类

Result: 在多种退化程度的文档上验证有效，包括乌拉圭军事档案和17-20世纪欧洲报纸，展示了改进效果

Conclusion: 该方法能够有效利用文档内部字符形状的冗余性来提升OCR识别精度，特别适用于历史文档和低质量印刷材料

Abstract: Current OCR systems are based on deep learning models trained on large
amounts of data. Although they have shown some ability to generalize to unseen
data, especially in detection tasks, they can struggle with recognizing
low-quality data. This is particularly evident for printed documents, where
intra-domain data variability is typically low, but inter-domain data
variability is high. In that context, current OCR methods do not fully exploit
each document's redundancy. We propose an unsupervised method by leveraging the
redundancy of character shapes within a document to correct imperfect outputs
of a given OCR system and suggest better clustering. To this aim, we introduce
an extended Gaussian Mixture Model (GMM) by alternating an
Expectation-Maximization (EM) algorithm with an intra-cluster realignment
process and normality statistical testing. We demonstrate improvements in
documents with various levels of degradation, including recovered Uruguayan
military archives and 17th to mid-20th century European newspapers.

</details>


### [16] [Pixels to Play: A Foundation Model for 3D Gameplay](https://arxiv.org/abs/2508.14295)
*Yuguang Yue,Chris Green,Samuel Hunt,Irakli Salia,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.CV

TL;DR: Pixels2Play-0.1是一个通过像素输入学习玩3D游戏的基础模型，使用行为克隆训练，结合人工演示和未标注视频数据，能够在消费级GPU上实现低延迟游戏操作


<details>
  <summary>Details</summary>
Motivation: 满足AI队友、可控NPC、个性化直播和辅助测试等新兴需求，需要代理能够基于与玩家相同的像素流进行游戏，并以最小游戏特定工程泛化到新游戏

Method: 端到端行为克隆训练，结合人工标注演示数据和通过逆向动力学模型推断动作的未标注公共视频数据，使用解码器transformer架构实现自回归动作输出

Result: 在简单Roblox和经典MS-DOS游戏中展现出有竞争力的游戏表现，进行了未标注数据消融实验，并规划了达到专家级文本条件控制的扩展和评估步骤

Conclusion: P2P0.1展示了基于像素输入的游戏AI的可行性，为构建通用游戏AI基础模型奠定了基础，未来需要进一步扩展规模和实现文本条件控制

Abstract: We introduce Pixels2Play-0.1 (P2P0.1), a foundation model that learns to play
a wide range of 3D video games with recognizable human-like behavior. Motivated
by emerging consumer and developer use cases - AI teammates, controllable NPCs,
personalized live-streamers, assistive testers - we argue that an agent must
rely on the same pixel stream available to players and generalize to new titles
with minimal game-specific engineering. P2P0.1 is trained end-to-end with
behavior cloning: labeled demonstrations collected from instrumented human
game-play are complemented by unlabeled public videos, to which we impute
actions via an inverse-dynamics model. A decoder-only transformer with
auto-regressive action output handles the large action space while remaining
latency-friendly on a single consumer GPU. We report qualitative results
showing competent play across simple Roblox and classic MS-DOS titles,
ablations on unlabeled data, and outline the scaling and evaluation steps
required to reach expert-level, text-conditioned control.

</details>


### [17] [A Comprehensive Review of Agricultural Parcel and Boundary Delineation from Remote Sensing Images: Recent Progress and Future Perspectives](https://arxiv.org/abs/2508.14558)
*Juepeng Zheng,Zi Ye,Yibin Wen,Jianxi Huang,Zhiwei Zhang,Qingmei Li,Qiong Hu,Baodong Xu,Lingyuan Zhao,Haohuan Fu*

Main category: cs.CV

TL;DR: 这是一篇关于农业田块边界划分(APBD)的综述性论文，系统总结了利用遥感图像进行农业田块检测和划界的方法，包括传统图像处理、传统机器学习和深度学习方法，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 高分辨率遥感图像的发展为自动化农业资产调查提供了潜力，需要系统总结APBD领域的研究进展和发展趋势，为研空人员提供清晰的知识地图。

Method: 进行了全面的文献综述和元数据分析，包括算法、研究地点、作物类型、传感器类型、评估方法等。将APBD方法分为三类：传统图像处理方法、传统机器学习方法和深度学习方法。

Result: 系统分析了APBD领域的各种方法，特别是深度学习方法（语义分割、目标检测、Transformer等）。讨论了多传感器数据、单任务与多任务学习对比、不同算法对比等关键问题。

Conclusion: 本综述为APBD领域研穵人员提供了全面的发展概况，并提出了未来研究方向和潜在热门话题，有助于跟踪该领域的发展趋势。

Abstract: Powered by advances in multiple remote sensing sensors, the production of
high spatial resolution images provides great potential to achieve
cost-efficient and high-accuracy agricultural inventory and analysis in an
automated way. Lots of studies that aim at providing an inventory of the level
of each agricultural parcel have generated many methods for Agricultural Parcel
and Boundary Delineation (APBD). This review covers APBD methods for detecting
and delineating agricultural parcels and systematically reviews the past and
present of APBD-related research applied to remote sensing images. With the
goal to provide a clear knowledge map of existing APBD efforts, we conduct a
comprehensive review of recent APBD papers to build a meta-data analysis,
including the algorithm, the study site, the crop type, the sensor type, the
evaluation method, etc. We categorize the methods into three classes: (1)
traditional image processing methods (including pixel-based, edge-based and
region-based); (2) traditional machine learning methods (such as random forest,
decision tree); and (3) deep learning-based methods. With deep
learning-oriented approaches contributing to a majority, we further discuss
deep learning-based methods like semantic segmentation-based, object
detection-based and Transformer-based methods. In addition, we discuss five
APBD-related issues to further comprehend the APBD domain using remote sensing
data, such as multi-sensor data in APBD task, comparisons between single-task
learning and multi-task learning in the APBD domain, comparisons among
different algorithms and different APBD tasks, etc. Finally, this review
proposes some APBD-related applications and a few exciting prospects and
potential hot topics in future APBD research. We hope this review help
researchers who involved in APBD domain to keep track of its development and
tendency.

</details>


### [18] [MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation](https://arxiv.org/abs/2508.14327)
*Guile Wu,David Huang,Dongfeng Bai,Bingbing Liu*

Main category: cs.CV

TL;DR: 本文提出了一种统一的多模态多视角视频生成方法，能够同时生成RGB、深度地图和语义分割图等多模态数据，提升自动驾驶场景生成的完整性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法主要集中于RGB视频生成，缺乏多模态生成能力，而深度地图和语义分割图等多模态数据对自动驾驶场景理解至关重要。使用多个模型分别生成不同模态会增加部署难度且无法利用模态间的互补信息。

Method: 构建了一个统一的正向散布Transformer模型，包含模态共享组件和模态特定组件。利用多样化的条件输入来编码可控制的场景结构和内容线索，在统一框架下实现多模态多视角视频生成。

Result: 在nuScenes自动驾驶数据集上的实验结果显示，该方法能够生成具有高保真度和高可控性的多模态多视角城市场景视频，性能超越了当前最先进的方法。

Conclusion: 该研究提出的统一多模态多视角视频生成框架，有效解决了自动驾驶领域多模态数据生成的难题，为整体场景理解提供了更全面的数据支持。

Abstract: Video generation has recently shown superiority in urban scene synthesis for
autonomous driving. Existing video generation approaches to autonomous driving
primarily focus on RGB video generation and lack the ability to support
multi-modal video generation. However, multi-modal data, such as depth maps and
semantic maps, are crucial for holistic urban scene understanding in autonomous
driving. Although it is feasible to use multiple models to generate different
modalities, this increases the difficulty of model deployment and does not
leverage complementary cues for multi-modal data generation. To address this
problem, in this work, we propose a novel multi-modal multi-view video
generation approach to autonomous driving. Specifically, we construct a unified
diffusion transformer model composed of modal-shared components and
modal-specific components. Then, we leverage diverse conditioning inputs to
encode controllable scene structure and content cues into the unified diffusion
model for multi-modal multi-view video generation. In this way, our approach is
capable of generating multi-modal multi-view driving scene videos in a unified
framework. Our experiments on the challenging real-world autonomous driving
dataset, nuScenes, show that our approach can generate multi-modal multi-view
urban scene videos with high fidelity and controllability, surpassing the
state-of-the-art methods.

</details>


### [19] [Inter-Class Relational Loss for Small Object Detection: A Case Study on License Plates](https://arxiv.org/abs/2508.14343)
*Dian Ning,Dong Seog Han*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的类间关系损失函数(ICR loss)，通过利用对象间的空间关系来改善小目标检测的梯度更新效率，并发布了一个新的车牌检测数据集SVMLP。


<details>
  <summary>Details</summary>
Motivation: 传统IoU基损失在小目标检测中存在梯度更新不足的问题，导致小目标学习效率低下。论文通过利用对象间的空间关系（如车牌与汽车的位置关系）来解决这一问题。

Method: 提出类间关系损失函数(ICR loss)，当预测的小目标（如车牌）不在其相关大目标（如汽车）的边界框内时，添加损失惩罚。该损失与车边界框和预测车牌边界框的重叠面积成反比。

Result: 在YOLOv12-T上实现了mAP$^{\text{test}}_{50}$指标10.3%的提升，在UAV-DETR上实现了1.6%的提升，无需额外调参。发布了SVMLP车牌检测数据集。

Conclusion: ICR损失可以简单地添加到现有IoU基损失中，通过利用类间空间关系有效改善小目标检测性能，特别是在车牌检测任务中显示出显著效果。

Abstract: In one-stage multi-object detection tasks, various intersection over union
(IoU)-based solutions aim at smooth and stable convergence near the targets
during training. However, IoU-based losses fail to correctly update the
gradient of small objects due to an extremely flat gradient. During the update
of multiple objects, the learning of small objects' gradients suffers more
because of insufficient gradient updates. Therefore, we propose an inter-class
relational loss to efficiently update the gradient of small objects while not
sacrificing the learning efficiency of other objects based on the simple fact
that an object has a spatial relationship to another object (e.g., a car plate
is attached to a car in a similar position). When the predicted car plate's
bounding box is not within its car, a loss punishment is added to guide the
learning, which is inversely proportional to the overlapped area of the car's
and predicted car plate's bounding box. By leveraging the spatial relationship
at the inter-class level, the loss guides small object predictions using larger
objects and enhances latent information in deeper feature maps. In this paper,
we present twofold contributions using license plate detection as a case study:
(1) a new small vehicle multi-license plate dataset (SVMLP), featuring diverse
real-world scenarios with high-quality annotations; and (2) a novel inter-class
relational loss function designed to promote effective detection performance.
We highlight the proposed ICR loss penalty can be easily added to existing
IoU-based losses and enhance the performance. These contributions improve the
standard mean Average Precision (mAP) metric, achieving gains of 10.3% and 1.6%
in mAP$^{\text{test}}_{50}$ for YOLOv12-T and UAV-DETR, respectively, without
any additional hyperparameter tuning. Code and dataset will be available soon.

</details>


### [20] [Adversarial Hospital-Invariant Feature Learning for WSI Patch Classification](https://arxiv.org/abs/2508.14779)
*Mengliang Zhang,Jacob M. Luber*

Main category: cs.CV

TL;DR: 本文系统研究了病理学基础模型中的医院域偏差问题，提出了一个轻量级对抗框架来消除医院特异性特征，同时保持疾病分类性能。


<details>
  <summary>Details</summary>
Motivation: 不同医院的病理图像因扫描硬件和预处理方式差异存在域偏差，导致病理学基础模型可能学习到医院特异性特征，影响临床部署的可靠性。

Method: 构建了量化PFMs域偏差的流程，提出了包含可训练适配器和域分类器的轻量级对抗框架，通过梯度反转层(GRL)从冻结表示中移除医院特异性特征。

Result: 在多中心组织病理学数据集上的实验表明，该方法显著降低了域可预测性，同时保持甚至提升了疾病分类性能，特别是在未见医院场景中效果显著。

Conclusion: 提出的对抗框架有效缓解了医院偏差，通过医院检测和特征空间可视化分析验证了方法的有效性，为病理学基础模型的临床部署提供了重要解决方案。

Abstract: Pathology foundation models (PFMs) have demonstrated remarkable potential in
whole-slide image (WSI) diagnosis. However, pathology images from different
hospitals often vary due to differences in scanning hardware and preprocessing
styles, which may lead PFMs to inadvertently learn hospital-specific features,
posing risks for clinical deployment. In this work, we present the first
systematic study of domain bias in PFMs arising from hospital source
characteristics. Specifically, we (1) construct a pipeline for quantifying
domain bias in PFMs, (2) evaluate and compare the performance of multiple
models, and (3) propose a lightweight adversarial framework that removes latent
hospital-specific features from frozen representations without modifying the
encoder itself. By introducing a trainable adapter and a domain classifier
connected through a gradient reversal layer (GRL), our method learns
task-discriminative yet domain-invariant representations. Experiments on
multi-center histopathology datasets demonstrate that our approach
substantially reduces domain predictability while maintaining or even improving
disease classification performance, particularly in out-of-domain (unseen
hospital) scenarios. Further analyses, including hospital detection and feature
space visualization, confirm the effectiveness of our method in mitigating
hospital bias. We will provide our code based on acceptance.

</details>


### [21] [HandCraft: Dynamic Sign Generation for Synthetic Data Augmentation](https://arxiv.org/abs/2508.14345)
*Gaston Gustavo Rios*

Main category: cs.CV

TL;DR: 通过CMLPe基础的轻量签语生成模型和合成数据预训练方法，解决了签语识别中训练数据不足的问题，在LSFB和DiSPLaY数据集上创造了新的最高水准。


<details>
  <summary>Details</summary>
Motivation: 签语识别模型因训练数据不足而遇到性能限制，需要找到有效的数据增强方法来提升识别准确度。

Method: 提出了基于CMLPe的轻量签语生成模型，结合合成数据预训练方法，使用Mamba-SL和Transformer-SL分类器进行识别。

Result: 在LSFB和DiSPLaY数据集上达到了新的state-of-the-art结果，合成数据预训练在某些情况下超过传统数据增强方法，并且与其他方法组合时产生互补效果。

Conclusion: 该方法通过提供计算效率高的方法，实现了签语生成和合成数据预训练的普及化，在多样化数据集上都能实现显著的性能提升。

Abstract: Sign Language Recognition (SLR) models face significant performance
limitations due to insufficient training data availability. In this article, we
address the challenge of limited data in SLR by introducing a novel and
lightweight sign generation model based on CMLPe. This model, coupled with a
synthetic data pretraining approach, consistently improves recognition
accuracy, establishing new state-of-the-art results for the LSFB and DiSPLaY
datasets using our Mamba-SL and Transformer-SL classifiers. Our findings reveal
that synthetic data pretraining outperforms traditional augmentation methods in
some cases and yields complementary benefits when implemented alongside them.
Our approach democratizes sign generation and synthetic data pretraining for
SLR by providing computationally efficient methods that achieve significant
performance improvements across diverse datasets.

</details>


### [22] [Deep Learning for Taxol Exposure Analysis: A New Cell Image Dataset and Attention-Based Baseline Model](https://arxiv.org/abs/2508.14349)
*Sean Fletcher,Gabby Scott,Douglas Currie,Xin Zhang,Yuqi Song,Bruce MacLeod*

Main category: cs.CV

TL;DR: 这篇论文提出了一个新的微观镜图像数据集，用于自动化分析细胞对化疗药Taxol的响应，并设计了基线模型ResAttention-KNN来进行浓度分类。


<details>
  <summary>Details</summary>
Motivation: 现有检测Taxol效果的方法需要专业设备和训练有素的人员，成本高且不适合高速通量分析，而且缺乏公开的数据集来支持自动化的细胞形态分析。

Method: 收集了C6脑研细胞经过不同浓度Taxol处理后的微视镜图像数据集，并提出ResAttention-KNN模型，结合ResNet-50、卷积注意力模块和k近邻分类器来提高精度和可解释性。

Result: 构建了公开的Taxol处理细胞图像数据集，并为该数据集提供了基线模型，为未来研究设立了标准。数据集和模型实现都已公开发布。

Conclusion: 该研究填补了Taxol浓度自动分析领域的数据空白，提供的数据集和基线模型将有助于推动视觉基图医学分析的发展，支持可复现性研究。

Abstract: Monitoring the effects of the chemotherapeutic agent Taxol at the cellular
level is critical for both clinical evaluation and biomedical research.
However, existing detection methods require specialized equipment, skilled
personnel, and extensive sample preparation, making them expensive,
labor-intensive, and unsuitable for high-throughput or real-time analysis. Deep
learning approaches have shown great promise in medical and biological image
analysis, enabling automated, high-throughput assessment of cellular
morphology. Yet, no publicly available dataset currently exists for automated
morphological analysis of cellular responses to Taxol exposure. To address this
gap, we introduce a new microscopy image dataset capturing C6 glioma cells
treated with varying concentrations of Taxol. To provide an effective solution
for Taxol concentration classification and establish a benchmark for future
studies on this dataset, we propose a baseline model named ResAttention-KNN,
which combines a ResNet-50 with Convolutional Block Attention Modules and uses
a k-Nearest Neighbors classifier in the learned embedding space. This model
integrates attention-based refinement and non-parametric classification to
enhance robustness and interpretability. Both the dataset and implementation
are publicly released to support reproducibility and facilitate future research
in vision-based biomedical analysis.

</details>


### [23] [Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation](https://arxiv.org/abs/2508.14358)
*Zhujun Li,Shuo Zhang,Ioannis Stamos*

Main category: cs.CV

TL;DR: HRC-Pose是一个基于深度学习的类别级物体位姿估计框架，通过对比学习保持6D位姿连续性，在旋转和平移分量上分别处理，在REAL275和CAMERA25基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅使用6D位姿作为监督信号，没有显式捕捉位姿的内在连续性，导致预测不一致和对未见位姿的泛化能力降低。

Method: 提出HRC-Pose框架，解耦物体位姿为旋转和平移分量分别编码；引入基于6D位感知层次排序方案的对比学习策略；设计分别处理旋转感知和平移感知嵌入的位姿估计模块。

Result: 在REAL275和CAMERA25基准测试中一致优于现有的深度学习方法，能够实时运行，成功学习了连续特征空间。

Conclusion: HRC-Pose通过对比学习有效解决了类别级物体位姿估计中的连续性保持问题，具有实际应用的潜力和有效性。

Abstract: Category-level object pose estimation aims to predict the 6D pose and 3D size
of objects within given categories. Existing approaches for this task rely
solely on 6D poses as supervisory signals without explicitly capturing the
intrinsic continuity of poses, leading to inconsistencies in predictions and
reduced generalization to unseen poses. To address this limitation, we propose
HRC-Pose, a novel depth-only framework for category-level object pose
estimation, which leverages contrastive learning to learn point cloud
representations that preserve the continuity of 6D poses. HRC-Pose decouples
object pose into rotation and translation components, which are separately
encoded and leveraged throughout the network. Specifically, we introduce a
contrastive learning strategy for multi-task, multi-category scenarios based on
our 6D pose-aware hierarchical ranking scheme, which contrasts point clouds
from multiple categories by considering rotational and translational
differences as well as categorical information. We further design pose
estimation modules that separately process the learned rotation-aware and
translation-aware embeddings. Our experiments demonstrate that HRC-Pose
successfully learns continuous feature spaces. Results on REAL275 and CAMERA25
benchmarks show that our method consistently outperforms existing depth-only
state-of-the-art methods and runs in real-time, demonstrating its effectiveness
and potential for real-world applications. Our code is at
https://github.com/zhujunli1993/HRC-Pose.

</details>


### [24] [Taming Transformer for Emotion-Controllable Talking Face Generation](https://arxiv.org/abs/2508.14359)
*Ziqi Zhang,Cheng Deng*

Main category: cs.CV

TL;DR: 提出了一种离散化的情感可控说话人脸生成方法，通过预训练策略分离音频组件、量化视频为视觉标记，并引入情感锚表示来整合情感信息，使用自回归变换器生成情感化视频


<details>
  <summary>Details</summary>
Motivation: 解决情感可控说话人脸生成的两个关键挑战：如何有效建模与特定情感相关的多模态关系，以及如何利用这种关系合成保持身份特征的情感化视频

Method: 采用两种预训练策略将音频分离为独立组件并将视频量化为视觉标记的组合，提出情感锚表示将情感信息整合到视觉标记中，使用自回归变换器建模视觉标记的全局分布并预测索引序列

Result: 在MEAD数据集上进行实验，控制多种情感音频条件下的视频情感，大量实验证明该方法在定性和定量评估上都具有优越性

Conclusion: 该方法能够有效实现情感可控的说话人脸生成，通过离散化处理和情感锚表示成功解决了多模态关系建模和身份保持的情感视频合成问题

Abstract: Talking face generation is a novel and challenging generation task, aiming at
synthesizing a vivid speaking-face video given a specific audio. To fulfill
emotion-controllable talking face generation, current methods need to overcome
two challenges: One is how to effectively model the multimodal relationship
related to the specific emotion, and the other is how to leverage this
relationship to synthesize identity preserving emotional videos. In this paper,
we propose a novel method to tackle the emotion-controllable talking face
generation task discretely. Specifically, we employ two pre-training strategies
to disentangle audio into independent components and quantize videos into
combinations of visual tokens. Subsequently, we propose the emotion-anchor (EA)
representation that integrates the emotional information into visual tokens.
Finally, we introduce an autoregressive transformer to model the global
distribution of the visual tokens under the given conditions and further
predict the index sequence for synthesizing the manipulated videos. We conduct
experiments on the MEAD dataset that controls the emotion of videos conditioned
on multiple emotional audios. Extensive experiments demonstrate the
superiorities of our method both qualitatively and quantitatively.

</details>


### [25] [FastTracker: Real-Time and Accurate Visual Tracking](https://arxiv.org/abs/2508.14370)
*Hamidreza Hashempoor,Yu Dong Hwang*

Main category: cs.CV

TL;DR: 提出了一种通用的多目标跟踪框架，特别针对复杂交通场景中的车辆跟踪，包含遮挡感知重识别和道路结构感知轨迹优化，并在新基准和公共数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统的多目标跟踪系统主要针对行人跟踪，对其他物体类别的泛化能力有限，需要开发能够处理多种物体类型的通用跟踪框架。

Method: 采用两个关键组件：1) 遮挡感知重识别机制，增强被严重遮挡物体的身份保持；2) 道路结构感知轨迹优化策略，利用车道方向、人行横道、道路边界等语义场景先验来提高轨迹连续性和准确性。

Result: 在新引入的数据集和多个公共基准测试中均取得稳健性能，在MOT17和MOT20测试集上分别获得66.4和65.7的HOTA分数。

Conclusion: 该框架不仅适用于通用的多类跟踪，在传统基准测试中也表现出强大性能，证明了其在通用目标跟踪中的有效性。

Abstract: Conventional multi-object tracking (MOT) systems are predominantly designed
for pedestrian tracking and often exhibit limited generalization to other
object categories. This paper presents a generalized tracking framework capable
of handling multiple object types, with a particular emphasis on vehicle
tracking in complex traffic scenes. The proposed method incorporates two key
components: (1) an occlusion-aware re-identification mechanism that enhances
identity preservation for heavily occluded objects, and (2) a
road-structure-aware tracklet refinement strategy that utilizes semantic scene
priors such as lane directions, crosswalks, and road boundaries to improve
trajectory continuity and accuracy. In addition, we introduce a new benchmark
dataset comprising diverse vehicle classes with frame-level tracking
annotations, specifically curated to support evaluation of vehicle-focused
tracking methods. Extensive experimental results demonstrate that the proposed
approach achieves robust performance on both the newly introduced dataset and
several public benchmarks, highlighting its effectiveness in general-purpose
object tracking. While our framework is designed for generalized multi-class
tracking, it also achieves strong performance on conventional benchmarks, with
HOTA scores of 66.4 on MOT17 and 65.7 on MOT20 test sets. Code and Benchmark
are available: github.com/Hamidreza-Hashempoor/FastTracker,
huggingface.co/datasets/Hamidreza-Hashemp/FastTracker-Benchmark.

</details>


### [26] [TCFNet: Bidirectional face-bone transformation via a Transformer-based coarse-to-fine point movement network](https://arxiv.org/abs/2508.14373)
*Runshi Zhang,Bimeng Jie,Yang He,Junchen Wang*

Main category: cs.CV

TL;DR: 提出基于Transformer的粗到细点移动网络(TCFNet)，用于面部-骨骼点云变换，通过两阶段网络结构解决传统方法和现有深度学习方法在计算效率、精度和可扩展性方面的限制。


<details>
  <summary>Details</summary>
Motivation: 传统生物力学模拟方法计算耗时、数据处理繁琐且精度低，现有深度学习方法无法处理大规模点云、感受野有限导致噪声点，且需要复杂的预处理和后处理操作，限制了性能和广泛应用。

Method: 提出TCFNet端到端框架：第一阶段使用Transformer网络，第二阶段使用局部信息聚合网络(LIA-Net)，两者相互增强生成精确点移动路径。LIA-Net通过建模局部几何结构补偿Transformer的邻域精度损失，并使用门控循环单元引导局部位移。引入基于可变形医学图像配准的辅助损失来利用专家知识重建关键器官。

Result: 在收集的数据集上与现有最先进方法相比，TCFNet在评估指标和可视化结果方面表现优异。

Conclusion: TCFNet通过Transformer和局部信息聚合网络的结合，有效解决了面部-骨骼点云变换中的精度和效率问题，为计算机辅助外科手术模拟提供了更优的解决方案。

Abstract: Computer-aided surgical simulation is a critical component of orthognathic
surgical planning, where accurately simulating face-bone shape transformations
is significant. The traditional biomechanical simulation methods are limited by
their computational time consumption levels, labor-intensive data processing
strategies and low accuracy. Recently, deep learning-based simulation methods
have been proposed to view this problem as a point-to-point transformation
between skeletal and facial point clouds. However, these approaches cannot
process large-scale points, have limited receptive fields that lead to noisy
points, and employ complex preprocessing and postprocessing operations based on
registration. These shortcomings limit the performance and widespread
applicability of such methods. Therefore, we propose a Transformer-based
coarse-to-fine point movement network (TCFNet) to learn unique, complicated
correspondences at the patch and point levels for dense face-bone point cloud
transformations. This end-to-end framework adopts a Transformer-based network
and a local information aggregation network (LIA-Net) in the first and second
stages, respectively, which reinforce each other to generate precise point
movement paths. LIA-Net can effectively compensate for the neighborhood
precision loss of the Transformer-based network by modeling local geometric
structures (edges, orientations and relative position features). The previous
global features are employed to guide the local displacement using a gated
recurrent unit. Inspired by deformable medical image registration, we propose
an auxiliary loss that can utilize expert knowledge for reconstructing critical
organs.Compared with the existing state-of-the-art (SOTA) methods on gathered
datasets, TCFNet achieves outstanding evaluation metrics and visualization
results. The code is available at https://github.com/Runshi-Zhang/TCFNet.

</details>


### [27] [QuadINR: Hardware-Efficient Implicit Neural Representations Through Quadratic Activation](https://arxiv.org/abs/2508.14374)
*Wenyong Zhou,Boyu Li,Jiachen Ren,Taiqiang Wu,Zhilin Ai,Zhengwu Liu,Ngai Wong*

Main category: cs.CV

TL;DR: QuadINR是一种硬件高效的隐式神经表示方法，使用分段二次激活函数来减少硬件开销，同时提高高频信号表达能力，在FPGA和ASIC实现中显著降低了资源消耗和功耗。


<details>
  <summary>Details</summary>
Motivation: 现有的隐式神经表示方法通过复杂激活函数来缓解频谱偏差，但带来了显著的硬件开销问题。

Method: 提出QuadINR方法，使用分段二次激活函数来丰富谐波内容，并通过NTK分析验证其表达能力，开发了统一的N级流水线框架来实现高效硬件实现。

Result: 在图像和视频实验中，QuadINR相比现有方法PSNR提升达2.06dB，面积仅1914μm²，动态功耗6.14mW，资源消耗减少97%，功耗降低97%，延迟改善93%。

Conclusion: QuadINR通过硬件友好的分段二次激活函数，在保持高性能的同时大幅降低了硬件资源消耗和功耗，为隐式神经表示的硬件实现提供了高效解决方案。

Abstract: Implicit Neural Representations (INRs) encode discrete signals continuously
while addressing spectral bias through activation functions (AFs). Previous
approaches mitigate this bias by employing complex AFs, which often incur
significant hardware overhead. To tackle this challenge, we introduce QuadINR,
a hardware-efficient INR that utilizes piecewise quadratic AFs to achieve
superior performance with dramatic reductions in hardware consumption. The
quadratic functions encompass rich harmonic content in their Fourier series,
delivering enhanced expressivity for high-frequency signals, as verified
through Neural Tangent Kernel (NTK) analysis. We develop a unified $N$-stage
pipeline framework that facilitates efficient hardware implementation of
various AFs in INRs. We demonstrate FPGA implementations on the VCU128 platform
and an ASIC implementation in a 28nm process. Experiments across images and
videos show that QuadINR achieves up to 2.06dB PSNR improvement over prior
work, with an area of only 1914$\mu$m$^2$ and a dynamic power of 6.14mW,
reducing resource and power consumption by up to 97\% and improving latency by
up to 93\% vs existing baselines.

</details>


### [28] [Img2ST-Net: Efficient High-Resolution Spatial Omics Prediction from Whole Slide Histology Images via Fully Convolutional Image-to-Image Learning](https://arxiv.org/abs/2508.14393)
*Junchao Zhu,Ruining Deng,Junlin Guo,Tianyuan Yao,Juming Xiong,Chongyu Qu,Mengmeng Yin,Yu Wang,Shilin Zhao,Haichun Yang,Daguang Xu,Yucheng Tang,Yuankai Huo*

Main category: cs.CV

TL;DR: Img2ST-Net是一个新颖的组织学图像到空间转录组学生成框架，通过全卷积架构并行生成高分辨率基因表达图谱，解决了传统方法在高分辨率下的计算效率和稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 随着空间转录组学分辨率提升到8um或更高，传统逐点回归方法变得低效且不稳定，同时高分辨率ST数据的极端稀疏性和低表达水平给预测和评估带来了挑战。

Method: 采用全卷积架构将HD ST数据建模为超像素表示，将任务重新定义为具有数百或数千输出通道的超内容图像生成问题，并引入SSIM-ST评估指标来增强稀疏表达模式下的鲁棒性。

Result: 提出了一个可扩展、生物学一致的高分辨率ST预测框架，提供了高效准确的大规模ST推断的解决方案。

Conclusion: Img2ST-Net为下一代稳健且分辨率感知的ST建模奠定了基础，代码已公开。

Abstract: Recent advances in multi-modal AI have demonstrated promising potential for
generating the currently expensive spatial transcriptomics (ST) data directly
from routine histology images, offering a means to reduce the high cost and
time-intensive nature of ST data acquisition. However, the increasing
resolution of ST, particularly with platforms such as Visium HD achieving 8um
or finer, introduces significant computational and modeling challenges.
Conventional spot-by-spot sequential regression frameworks become inefficient
and unstable at this scale, while the inherent extreme sparsity and low
expression levels of high-resolution ST further complicate both prediction and
evaluation. To address these limitations, we propose Img2ST-Net, a novel
histology-to-ST generation framework for efficient and parallel high-resolution
ST prediction. Unlike conventional spot-by-spot inference methods, Img2ST-Net
employs a fully convolutional architecture to generate dense, HD gene
expression maps in a parallelized manner. By modeling HD ST data as super-pixel
representations, the task is reformulated from image-to-omics inference into a
super-content image generation problem with hundreds or thousands of output
channels. This design not only improves computational efficiency but also
better preserves the spatial organization intrinsic to spatial omics data. To
enhance robustness under sparse expression patterns, we further introduce
SSIM-ST, a structural-similarity-based evaluation metric tailored for
high-resolution ST analysis. We present a scalable, biologically coherent
framework for high-resolution ST prediction. Img2ST-Net offers a principled
solution for efficient and accurate ST inference at scale. Our contributions
lay the groundwork for next-generation ST modeling that is robust and
resolution-aware. The source code has been made publicly available at
https://github.com/hrlblab/Img2ST-Net.

</details>


### [29] [CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality English Text-to-Image Communities](https://arxiv.org/abs/2508.14405)
*Yue Gong,Shanyuan Liu,Liuzhuozheng Li,Jian Zhu,Bo Cheng,Liebucha Wu,Xiaoyu Wu,Yuhang Ma,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: 中文文本适配器CTA-Flux，通过MMDiT技术直接控制Flux模型核心，在保持英语模型兼容性的同时提升中文语义理解能力和图像生成质量


<details>
  <summary>Details</summary>
Motivation: 解决Flux等英语训练的文本生图模型在处理中文等非英语提示时的语言和文化偏见问题，现有翻译或双语练练方法无法保证文化特定语义的真实性

Method: 继承ControlNet类架构思想，利用多模态双向Transformer(MMDiT)直接控制Flux核心模型，大幅减少参数量的同时提升中文语义理解能力

Result: 实验评估显示CTA-Flux同时支持中英文提示，在图像生成质量、视觉真实性和中文语义准确性方面达到优异表现

Conclusion: 该方法在不需大规模重新训练整体模型的情况下，保持了与现有插件的兼容性，同时显著提升了中文文本的图像生成质量和文化真实性

Abstract: We proposed the Chinese Text Adapter-Flux (CTA-Flux). An adaptation method
fits the Chinese text inputs to Flux, a powerful text-to-image (TTI) generative
model initially trained on the English corpus. Despite the notable image
generation ability conditioned on English text inputs, Flux performs poorly
when processing non-English prompts, particularly due to linguistic and
cultural biases inherent in predominantly English-centric training datasets.
Existing approaches, such as translating non-English prompts into English or
finetuning models for bilingual mappings, inadequately address culturally
specific semantics, compromising image authenticity and quality. To address
this issue, we introduce a novel method to bridge Chinese semantic
understanding with compatibility in English-centric TTI model communities.
Existing approaches relying on ControlNet-like architectures typically require
a massive parameter scale and lack direct control over Chinese semantics. In
comparison, CTA-flux leverages MultiModal Diffusion Transformer (MMDiT) to
control the Flux backbone directly, significantly reducing the number of
parameters while enhancing the model's understanding of Chinese semantics. This
integration significantly improves the generation quality and cultural
authenticity without extensive retraining of the entire model, thus maintaining
compatibility with existing text-to-image plugins such as LoRA, IP-Adapter, and
ControlNet. Empirical evaluations demonstrate that CTA-flux supports Chinese
and English prompts and achieves superior image generation quality, visual
realism, and faithful depiction of Chinese semantics.

</details>


### [30] [MoCHA-former: Moiré-Conditioned Hybrid Adaptive Transformer for Video Demoiréing](https://arxiv.org/abs/2508.14423)
*Jeahun Sung,Changhyun Roh,Chanho Eom,Jihyong Oh*

Main category: cs.CV

TL;DR: MoCHA-former是一种新型的去摩尔纹方法，通过解耦摩尔纹和内容特征，结合空间-时间自适应处理，有效解决了相机拍摄屏幕时产生的摩尔纹问题，在RAW和sRGB域都取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 相机CFA与显示器子像素之间的频率混叠会产生严重的摩尔纹图案，现有方法在处理空间变化强度、大尺度结构、通道依赖统计和时序波动等方面存在局限。

Method: 提出MoCHA-former，包含两个核心组件：解耦摩尔纹自适应去摩尔纹（DMAD）和时空自适应去摩尔纹（STAD）。DMAD通过MDB和DDB分离摩尔纹与内容，MCB生成摩尔纹自适应特征；STAD使用SFB捕获大尺度结构，FCA建模RAW帧通道依赖性，并通过隐式帧对齐确保时序一致性。

Result: 在两个视频数据集（RAW和sRGB域）上的评估显示，MoCHA-former在PSNR、SSIM和LPIPS指标上均优于现有方法。

Conclusion: MoCHA-former通过创新的解耦和自适应机制，有效解决了屏幕拍摄中的摩尔纹问题，为便携式成像提供了高质量的解决方案。

Abstract: Recent advances in portable imaging have made camera-based screen capture
ubiquitous. Unfortunately, frequency aliasing between the camera's color filter
array (CFA) and the display's sub-pixels induces moir\'e patterns that severely
degrade captured photos and videos. Although various demoir\'eing models have
been proposed to remove such moir\'e patterns, these approaches still suffer
from several limitations: (i) spatially varying artifact strength within a
frame, (ii) large-scale and globally spreading structures, (iii)
channel-dependent statistics and (iv) rapid temporal fluctuations across
frames. We address these issues with the Moir\'e Conditioned Hybrid Adaptive
Transformer (MoCHA-former), which comprises two key components: Decoupled
Moir\'e Adaptive Demoir\'eing (DMAD) and Spatio-Temporal Adaptive Demoir\'eing
(STAD). DMAD separates moir\'e and content via a Moir\'e Decoupling Block (MDB)
and a Detail Decoupling Block (DDB), then produces moir\'e-adaptive features
using a Moir\'e Conditioning Block (MCB) for targeted restoration. STAD
introduces a Spatial Fusion Block (SFB) with window attention to capture
large-scale structures, and a Feature Channel Attention (FCA) to model channel
dependence in RAW frames. To ensure temporal consistency, MoCHA-former performs
implicit frame alignment without any explicit alignment module. We analyze
moir\'e characteristics through qualitative and quantitative studies, and
evaluate on two video datasets covering RAW and sRGB domains. MoCHA-former
consistently surpasses prior methods across PSNR, SSIM, and LPIPS.

</details>


### [31] [HyperDiff: Hypergraph Guided Diffusion Model for 3D Human Pose Estimation](https://arxiv.org/abs/2508.14431)
*Bing Han,Yuhua Huang,Pan Gao*

Main category: cs.CV

TL;DR: HyperDiff结合扩散模型和HyperGCN，通过多粒度结构建模关节间高阶相关性，有效解决单目3D人体姿态估计中的深度模糊和遮挡问题，在Human3.6M和MPI-INF-3DHP数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决单目3D人体姿态估计中的深度模糊和遮挡问题，以及传统方法在利用骨架结构信息时忽略多尺度特征导致精度下降的挑战。

Method: 提出HyperDiff方法，集成扩散模型和HyperGCN。扩散模型捕捉数据不确定性，HyperGCN作为去噪器使用多粒度结构精确建模关节间的高阶相关性。

Result: 在Human3.6M和MPI-INF-3DHP数据集上实现最先进性能，并能灵活适应不同计算资源以平衡性能与效率。

Conclusion: HyperDiff通过结合扩散模型的不确定性建模能力和HyperGCN的多粒度结构建模，有效提升了复杂姿态下的3D人体姿态估计精度和鲁棒性。

Abstract: Monocular 3D human pose estimation (HPE) often encounters challenges such as
depth ambiguity and occlusion during the 2D-to-3D lifting process.
Additionally, traditional methods may overlook multi-scale skeleton features
when utilizing skeleton structure information, which can negatively impact the
accuracy of pose estimation. To address these challenges, this paper introduces
a novel 3D pose estimation method, HyperDiff, which integrates diffusion models
with HyperGCN. The diffusion model effectively captures data uncertainty,
alleviating depth ambiguity and occlusion. Meanwhile, HyperGCN, serving as a
denoiser, employs multi-granularity structures to accurately model high-order
correlations between joints. This improves the model's denoising capability
especially for complex poses. Experimental results demonstrate that HyperDiff
achieves state-of-the-art performance on the Human3.6M and MPI-INF-3DHP
datasets and can flexibly adapt to varying computational resources to balance
performance and efficiency.

</details>


### [32] [FOCUS: Frequency-Optimized Conditioning of DiffUSion Models for mitigating catastrophic forgetting during Test-Time Adaptation](https://arxiv.org/abs/2508.14437)
*Gabriel Tjio,Jie Zhang,Xulei Yang,Yun Xing,Nhat Chung,Xiaofeng Cao,Ivor W. Tsang,Chee Keong Kwoh,Qing Guo*

Main category: cs.CV

TL;DR: FOCUS是一种基于频率调节的扩散驱动输入适应框架，通过频率先验保持任务相关语义信息，解决测试时适应中知识保持与域适应平衡的挑战


<details>
  <summary>Details</summary>
Motivation: 解决测试时适应中平衡知识保持和域适应迁移的挑战，避免适应域偏移时导致任务相关知识的遗忘

Method: 提出FOCUS方法，使用训练好的轻量级Y形频率预测网络(Y-FPN)从噪声图像中分离高低频信息，结合FrequencyMix数据增强方法，在扩散驱动去噪过程中通过空间自适应频率先验进行条件调节

Result: 在15种损坏类型和三个数据集上的语义分割和单目深度估计任务中达到最先进的平均性能，并能通过FOCUS去噪图像生成伪标签来补充现有模型适应方法

Conclusion: FOCUS有效缓解了灾难性遗忘问题，在有限间歇监督下仍能保持性能，为测试时适应提供了有效的频率基础解决方案

Abstract: Test-time adaptation enables models to adapt to evolving domains. However,
balancing the tradeoff between preserving knowledge and adapting to domain
shifts remains challenging for model adaptation methods, since adapting to
domain shifts can induce forgetting of task-relevant knowledge. To address this
problem, we propose FOCUS, a novel frequency-based conditioning approach within
a diffusion-driven input-adaptation framework. Utilising learned, spatially
adaptive frequency priors, our approach conditions the reverse steps during
diffusion-driven denoising to preserve task-relevant semantic information for
dense prediction.
  FOCUS leverages a trained, lightweight, Y-shaped Frequency Prediction Network
(Y-FPN) that disentangles high and low frequency information from noisy images.
This minimizes the computational costs involved in implementing our approach in
a diffusion-driven framework. We train Y-FPN with FrequencyMix, a novel data
augmentation method that perturbs the images across diverse frequency bands,
which improves the robustness of our approach to diverse corruptions.
  We demonstrate the effectiveness of FOCUS for semantic segmentation and
monocular depth estimation across 15 corruption types and three datasets,
achieving state-of-the-art averaged performance. In addition to improving
standalone performance, FOCUS complements existing model adaptation methods
since we can derive pseudo labels from FOCUS-denoised images for additional
supervision. Even under limited, intermittent supervision with the pseudo
labels derived from the FOCUS denoised images, we show that FOCUS mitigates
catastrophic forgetting for recent model adaptation methods.

</details>


### [33] [MUSE: Multi-Subject Unified Synthesis via Explicit Layout Semantic Expansion](https://arxiv.org/abs/2508.14440)
*Fei Peng,Junqiang Wu,Yan Li,Tingting Gao,Di Zhang,Huiyuan Fu*

Main category: cs.CV

TL;DR: MUSE是一个统一的文本到图像合成框架，通过串联交叉注意力机制实现布局可控的多主体合成，在保持身份一致性的同时实现精确的空间控制。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在多主体组合合成方面存在挑战，难以同时满足空间精度和身份保持的双重要求。

Method: 提出串联交叉注意力(CCA)机制，通过显式语义空间扩展将布局规范与文本指导无缝集成，并采用渐进式两阶段训练策略。

Result: 实验表明MUSE在零样本端到端生成中实现了优越的空间准确性和身份一致性，超越了现有解决方案。

Conclusion: MUSE通过创新的CCA机制和训练策略，推进了可控图像合成的前沿，能够同时实现精确的空间控制和身份保持。

Abstract: Existing text-to-image diffusion models have demonstrated remarkable
capabilities in generating high-quality images guided by textual prompts.
However, achieving multi-subject compositional synthesis with precise spatial
control remains a significant challenge. In this work, we address the task of
layout-controllable multi-subject synthesis (LMS), which requires both faithful
reconstruction of reference subjects and their accurate placement in specified
regions within a unified image. While recent advancements have separately
improved layout control and subject synthesis, existing approaches struggle to
simultaneously satisfy the dual requirements of spatial precision and identity
preservation in this composite task. To bridge this gap, we propose MUSE, a
unified synthesis framework that employs concatenated cross-attention (CCA) to
seamlessly integrate layout specifications with textual guidance through
explicit semantic space expansion. The proposed CCA mechanism enables
bidirectional modality alignment between spatial constraints and textual
descriptions without interference. Furthermore, we design a progressive
two-stage training strategy that decomposes the LMS task into learnable
sub-objectives for effective optimization. Extensive experiments demonstrate
that MUSE achieves zero-shot end-to-end generation with superior spatial
accuracy and identity consistency compared to existing solutions, advancing the
frontier of controllable image synthesis. Our code and model are available at
https://github.com/pf0607/MUSE.

</details>


### [34] [Reconstruction Using the Invisible: Intuition from NIR and Metadata for Enhanced 3D Gaussian Splatting](https://arxiv.org/abs/2508.14443)
*Gyusam Chang,Tuan-Anh Vu,Vivek Alumootil,Harris Song,Deanna Pham,Sangpil Kim,M. Khalid Jawed*

Main category: cs.CV

TL;DR: NIRPlant是一个包含近红外、RGB、深度和LiDAR数据的多模态农业数据集，NIRSplat是基于3D高斯泼溅的多模态架构，在农业场景重建中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 农业场景的3D重建面临光照不均、遮挡和视野受限等独特挑战，现有3D高斯泼溅方法在农业领域应用不足

Method: 提出NIRPlant多模态数据集，包含NIR、RGB、深度和LiDAR数据；开发NIRSplat架构，使用交叉注意力机制和3D点位置编码

Result: NIRSplat在挑战性农业场景中优于3DGS、CoR-GS和InstantSplat等现有方法

Conclusion: 多模态数据融合和NIR信息显著提升了农业场景3D重建的鲁棒性和植物学洞察力

Abstract: While 3D Gaussian Splatting (3DGS) has rapidly advanced, its application in
agriculture remains underexplored. Agricultural scenes present unique
challenges for 3D reconstruction methods, particularly due to uneven
illumination, occlusions, and a limited field of view. To address these
limitations, we introduce \textbf{NIRPlant}, a novel multimodal dataset
encompassing Near-Infrared (NIR) imagery, RGB imagery, textual metadata, Depth,
and LiDAR data collected under varied indoor and outdoor lighting conditions.
By integrating NIR data, our approach enhances robustness and provides crucial
botanical insights that extend beyond the visible spectrum. Additionally, we
leverage text-based metadata derived from vegetation indices, such as NDVI,
NDWI, and the chlorophyll index, which significantly enriches the contextual
understanding of complex agricultural environments. To fully exploit these
modalities, we propose \textbf{NIRSplat}, an effective multimodal Gaussian
splatting architecture employing a cross-attention mechanism combined with 3D
point-based positional encoding, providing robust geometric priors.
Comprehensive experiments demonstrate that \textbf{NIRSplat} outperforms
existing landmark methods, including 3DGS, CoR-GS, and InstantSplat,
highlighting its effectiveness in challenging agricultural scenarios. The code
and dataset are publicly available at:
https://github.com/StructuresComp/3D-Reconstruction-NIR

</details>


### [35] [Generalizable Engagement Estimation in Conversation via Domain Prompting and Parallel Attention](https://arxiv.org/abs/2508.14448)
*Yangche Yu,Yin Chen,Jia Li,Peng Jia,Yu Zhang,Li Dai,Zhenzhen Hu,Meng Wang,Richang Hong*

Main category: cs.CV

TL;DR: DAPA是一个用于通用对话参与度建模的新框架，通过域提示机制和平行交叉注意力模块，在跨文化和跨语言基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 准确的参与度估计对于自适应人机交互系统至关重要，但现有方法在跨域泛化性和复杂交互动态建模方面存在挑战。

Method: 提出DAPA框架，包含域提示机制（可学习的域特定向量）和平行交叉注意力模块（对齐前向和后向BiLSTM状态），显式建模交互同步性。

Result: 在多个跨文化跨语言基准测试中达到最先进性能，在NoXi-J测试集上CCC指标绝对提升0.45，并在MultiMediate'25挑战赛中获第一名。

Conclusion: DAPA通过显式的域适应和交互同步建模，有效解决了对话参与度估计中的泛化性问题，为跨域参与度建模提供了有效解决方案。

Abstract: Accurate engagement estimation is essential for adaptive human-computer
interaction systems, yet robust deployment is hindered by poor generalizability
across diverse domains and challenges in modeling complex interaction
dynamics.To tackle these issues, we propose DAPA (Domain-Adaptive Parallel
Attention), a novel framework for generalizable conversational engagement
modeling. DAPA introduces a Domain Prompting mechanism by prepending learnable
domain-specific vectors to the input, explicitly conditioning the model on the
data's origin to facilitate domain-aware adaptation while preserving
generalizable engagement representations. To capture interactional synchrony,
the framework also incorporates a Parallel Cross-Attention module that
explicitly aligns reactive (forward BiLSTM) and anticipatory (backward BiLSTM)
states between participants.Extensive experiments demonstrate that DAPA
establishes a new state-of-the-art performance on several cross-cultural and
cross-linguistic benchmarks, notably achieving an absolute improvement of 0.45
in Concordance Correlation Coefficient (CCC) over a strong baseline on the
NoXi-J test set. The superiority of our method was also confirmed by winning
the first place in the Multi-Domain Engagement Estimation Challenge at
MultiMediate'25.

</details>


### [36] [D^3-Talker: Dual-Branch Decoupled Deformation Fields for Few-Shot 3D Talking Head Synthesis](https://arxiv.org/abs/2508.14449)
*Yuhang Guo,Kaijun Deng,Siyang Song,Jindong Xie,Wenhui Ma,Linlin Shen*

Main category: cs.CV

TL;DR: D^3-Talker提出了一种新颖的3D说话头合成方法，通过构建静态3D高斯属性场，使用音频和面部运动信号分别控制两个不同的高斯属性变形场，有效解耦通用和个性化变形预测，在有限训练数据下实现高保真渲染和准确的音频-唇部同步。


<details>
  <summary>Details</summary>
Motivation: 现有3D说话头合成方法需要大量视频数据训练每个目标身份，且音频中包含与唇部运动无关的信息，导致在少量训练帧下难以实现准确的唇部同步和高质量的说话头图像。

Method: 构建静态3D高斯属性场，使用音频和面部运动信号分别控制两个独立的高斯属性变形场；设计相似性对比损失函数实现更彻底解耦；集成粗到细模块细化渲染图像。

Result: 大量实验表明，D^3-Talker在有限训练数据下，在高保真渲染和准确音频-唇部同步方面优于现有最先进方法。

Conclusion: 该方法通过有效解耦变形预测和图像细化模块，成功解决了少样本3D说话头合成的挑战，实现了高质量的渲染效果和准确的唇部同步。

Abstract: A key challenge in 3D talking head synthesis lies in the reliance on a
long-duration talking head video to train a new model for each target identity
from scratch. Recent methods have attempted to address this issue by extracting
general features from audio through pre-training models. However, since audio
contains information irrelevant to lip motion, existing approaches typically
struggle to map the given audio to realistic lip behaviors in the target face
when trained on only a few frames, causing poor lip synchronization and talking
head image quality. This paper proposes D^3-Talker, a novel approach that
constructs a static 3D Gaussian attribute field and employs audio and Facial
Motion signals to independently control two distinct Gaussian attribute
deformation fields, effectively decoupling the predictions of general and
personalized deformations. We design a novel similarity contrastive loss
function during pre-training to achieve more thorough decoupling. Furthermore,
we integrate a Coarse-to-Fine module to refine the rendered images, alleviating
blurriness caused by head movements and enhancing overall image quality.
Extensive experiments demonstrate that D^3-Talker outperforms state-of-the-art
methods in both high-fidelity rendering and accurate audio-lip synchronization
with limited training data. Our code will be provided upon acceptance.

</details>


### [37] [Ouroboros: Single-step Diffusion Models for Cycle-consistent Forward and Inverse Rendering](https://arxiv.org/abs/2508.14461)
*Shanlin Sun,Yifan Wang,Hanwen Zhang,Yifeng Xiong,Qin Ren,Ruogu Fang,Xiaohui Xie,Chenyu You*

Main category: cs.CV

TL;DR: Ouroboros是一个双向单步扩散模型框架，通过前向和逆向渲染的相互增强，解决了现有方法中的循环不一致性和推理速度慢的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多步扩散模型往往将前向和逆向渲染作为独立问题处理，导致循环不一致和推理速度缓慢，需要一种能够同时处理这两个问题并确保一致性的方法。

Method: 使用两个单步扩散模型分别处理前向和逆向渲染，引入循环一致性机制确保输出的一致性，并将本征分解扩展到室内外场景。

Result: 在多样化场景中实现了最先进的性能，相比其他基于扩散的方法显著提高了推理速度，并能以无需训练的方式迁移到视频分解任务。

Conclusion: Ouroboros框架通过双向单步扩散模型的相互增强，有效解决了前向和逆向渲染的一致性问题，同时大幅提升了推理效率，具有良好的泛化能力。

Abstract: While multi-step diffusion models have advanced both forward and inverse
rendering, existing approaches often treat these problems independently,
leading to cycle inconsistency and slow inference speed. In this work, we
present Ouroboros, a framework composed of two single-step diffusion models
that handle forward and inverse rendering with mutual reinforcement. Our
approach extends intrinsic decomposition to both indoor and outdoor scenes and
introduces a cycle consistency mechanism that ensures coherence between forward
and inverse rendering outputs. Experimental results demonstrate
state-of-the-art performance across diverse scenes while achieving
substantially faster inference speed compared to other diffusion-based methods.
We also demonstrate that Ouroboros can transfer to video decomposition in a
training-free manner, reducing temporal inconsistency in video sequences while
maintaining high-quality per-frame inverse rendering.

</details>


### [38] [DreamSwapV: Mask-guided Subject Swapping for Any Customized Video Editing](https://arxiv.org/abs/2508.14465)
*Weitao Wang,Zichen Wang,Hongdeng Shen,Yulei Lu,Xirui Fan,Suhui Wu,Jun Zhang,Haoqian Wang,Hao Zhang*

Main category: cs.CV

TL;DR: DreamSwapV是一个端到端的视频主体替换框架，通过掩码引导和参考图像实现任意视频中任意主体的定制化替换，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着视频生成技术的快速发展，定制化视频编辑需求激增，但主体替换这一关键组件仍未被充分探索。现有方法要么局限于特定领域，要么依赖间接编辑范式或模糊文本提示，影响最终保真度。

Method: 提出掩码引导、主体无关的端到端框架DreamSwapV，使用用户指定的掩码和参考图像进行主体替换。引入多条件输入和专用条件融合模块，设计自适应掩码策略以适应不同尺度和属性的主体，采用精心设计的两阶段数据集构建和训练方案。

Result: 在VBench指标和新提出的DreamSwapV-Benchmark上进行的综合实验验证了该方法优于现有方法。

Conclusion: DreamSwapV提供了一个高效、通用的视频主体替换解决方案，能够实现高质量的主体定制化替换，并改善了替换主体与周围环境的交互效果。

Abstract: With the rapid progress of video generation, demand for customized video
editing is surging, where subject swapping constitutes a key component yet
remains under-explored. Prevailing swapping approaches either specialize in
narrow domains--such as human-body animation or hand-object interaction--or
rely on some indirect editing paradigm or ambiguous text prompts that
compromise final fidelity. In this paper, we propose DreamSwapV, a mask-guided,
subject-agnostic, end-to-end framework that swaps any subject in any video for
customization with a user-specified mask and reference image. To inject
fine-grained guidance, we introduce multiple conditions and a dedicated
condition fusion module that integrates them efficiently. In addition, an
adaptive mask strategy is designed to accommodate subjects of varying scales
and attributes, further improving interactions between the swapped subject and
its surrounding context. Through our elaborate two-phase dataset construction
and training scheme, our DreamSwapV outperforms existing methods, as validated
by comprehensive experiments on VBench indicators and our first introduced
DreamSwapV-Benchmark.

</details>


### [39] [LookOut: Real-World Humanoid Egocentric Navigation](https://arxiv.org/abs/2508.14466)
*Boxiao Pan,Adam W. Harley,C. Karen Liu,Leonidas J. Guibas*

Main category: cs.CV

TL;DR: 提出了从第一人称视角视频预测未来6D头部姿态序列的挑战性问题，包括头部平移和旋转，以学习主动信息收集行为。


<details>
  <summary>Details</summary>
Motivation: 预测无碰撞的未来轨迹在人形机器人、VR/AR和辅助导航等应用中至关重要，需要理解人类主动信息收集的头部转动行为。

Method: 提出了一个基于时间聚合3D潜在特征的框架，建模静态和动态环境的几何和语义约束，并使用Project Aria眼镜收集数据构建数据集。

Result: 创建了Aria Navigation Dataset (AND)数据集，包含4小时真实世界导航记录，模型能够学习人类导航行为并在未见环境中泛化。

Conclusion: 该工作成功解决了从第一人称视角预测6D头部姿态的问题，展示了人类类似导航行为的泛化能力，为学习真实世界导航策略提供了宝贵资源。

Abstract: The ability to predict collision-free future trajectories from egocentric
observations is crucial in applications such as humanoid robotics, VR / AR, and
assistive navigation. In this work, we introduce the challenging problem of
predicting a sequence of future 6D head poses from an egocentric video. In
particular, we predict both head translations and rotations to learn the active
information-gathering behavior expressed through head-turning events. To solve
this task, we propose a framework that reasons over temporally aggregated 3D
latent features, which models the geometric and semantic constraints for both
the static and dynamic parts of the environment. Motivated by the lack of
training data in this space, we further contribute a data collection pipeline
using the Project Aria glasses, and present a dataset collected through this
approach. Our dataset, dubbed Aria Navigation Dataset (AND), consists of 4
hours of recording of users navigating in real-world scenarios. It includes
diverse situations and navigation behaviors, providing a valuable resource for
learning real-world egocentric navigation policies. Extensive experiments show
that our model learns human-like navigation behaviors such as waiting / slowing
down, rerouting, and looking around for traffic while generalizing to unseen
environments. Check out our project webpage at
https://sites.google.com/stanford.edu/lookout.

</details>


### [40] [Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration](https://arxiv.org/abs/2508.14483)
*Haoran Bai,Xiaoxu Chen,Canqian Yang,Zongyao He,Sibin Deng,Ying Chen*

Main category: cs.CV

TL;DR: Vivid-VR是基于DiT的生成式视频修复方法，通过ControlNet控制生成过程确保内容一致性，采用概念蒸馏训练策略和重新设计的控制架构，在纹理真实性和时间一致性方面表现优异


<details>
  <summary>Details</summary>
Motivation: 传统可控管道的微调常因多模态对齐不完美而导致分布漂移，影响纹理真实性和时间连贯性，需要解决这些挑战

Method: 提出概念蒸馏训练策略，利用预训练T2V模型合成带文本概念的训练样本；重新设计控制架构，包括控制特征投影器和双分支ControlNet连接器

Result: 在合成和真实世界基准测试以及AIGC视频上表现优于现有方法，实现了出色的纹理真实性、视觉生动性和时间一致性

Conclusion: Vivid-VR通过创新的训练策略和架构设计，有效解决了视频修复中的纹理和时间一致性问题，为生成式视频恢复提供了有效解决方案

Abstract: We present Vivid-VR, a DiT-based generative video restoration method built
upon an advanced T2V foundation model, where ControlNet is leveraged to control
the generation process, ensuring content consistency. However, conventional
fine-tuning of such controllable pipelines frequently suffers from distribution
drift due to limitations in imperfect multimodal alignment, resulting in
compromised texture realism and temporal coherence. To tackle this challenge,
we propose a concept distillation training strategy that utilizes the
pretrained T2V model to synthesize training samples with embedded textual
concepts, thereby distilling its conceptual understanding to preserve texture
and temporal quality. To enhance generation controllability, we redesign the
control architecture with two key components: 1) a control feature projector
that filters degradation artifacts from input video latents to minimize their
propagation through the generation pipeline, and 2) a new ControlNet connector
employing a dual-branch design. This connector synergistically combines
MLP-based feature mapping with cross-attention mechanism for dynamic control
feature retrieval, enabling both content preservation and adaptive control
signal modulation. Extensive experiments show that Vivid-VR performs favorably
against existing approaches on both synthetic and real-world benchmarks, as
well as AIGC videos, achieving impressive texture realism, visual vividness,
and temporal consistency. The codes and checkpoints are publicly available at
https://github.com/csbhr/Vivid-VR.

</details>


### [41] [WeedSense: Multi-Task Learning for Weed Segmentation, Height Estimation, and Growth Stage Classification](https://arxiv.org/abs/2508.14486)
*Toqi Tahamid Sarker,Khaled R Ahmed,Taminul Islam,Cristiana Bernardi Rankrape,Karla Gage*

Main category: cs.CV

TL;DR: WeedSense是一个多任务学习架构，能够同时进行杂草语义分割、高度估计和生长阶段分类，在保持实时推理速度的同时显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 杂草管理是农业中的关键挑战，严重影响作物产量且需要大量控制资源。需要有效的杂草监测和分析策略来实现可持续农业和精准管理。

Method: 提出WeedSense多任务学习架构，采用双路径编码器结合Universal Inverted Bottleneck块和基于transformer的多任务分叉解码器，生成多尺度特征并实现多任务同时预测。

Result: 在包含16种杂草11周生长周期的数据集上，WeedSense达到89.78%的mIoU分割精度、1.67cm高度估计MAE、99.99%生长阶段分类准确率，推理速度160 FPS，比单任务序列执行快3倍，参数减少32.4%。

Conclusion: WeedSense提供了一个全面、高效的杂草分析解决方案，在保持实时性能的同时显著提升了多任务处理的准确性和效率，为精准农业杂草管理提供了有力工具。

Abstract: Weed management represents a critical challenge in agriculture, significantly
impacting crop yields and requiring substantial resources for control.
Effective weed monitoring and analysis strategies are crucial for implementing
sustainable agricultural practices and site-specific management approaches. We
introduce WeedSense, a novel multi-task learning architecture for comprehensive
weed analysis that jointly performs semantic segmentation, height estimation,
and growth stage classification. We present a unique dataset capturing 16 weed
species over an 11-week growth cycle with pixel-level annotations, height
measurements, and temporal labels. WeedSense leverages a dual-path encoder
incorporating Universal Inverted Bottleneck blocks and a Multi-Task Bifurcated
Decoder with transformer-based feature fusion to generate multi-scale features
and enable simultaneous prediction across multiple tasks. WeedSense outperforms
other state-of-the-art models on our comprehensive evaluation. On our
multi-task dataset, WeedSense achieves mIoU of 89.78% for segmentation, 1.67cm
MAE for height estimation, and 99.99% accuracy for growth stage classification
while maintaining real-time inference at 160 FPS. Our multitask approach
achieves 3$\times$ faster inference than sequential single-task execution and
uses 32.4% fewer parameters. Please see our project page at
weedsense.github.io.

</details>


### [42] [SATURN: Autoregressive Image Generation Guided by Scene Graphs](https://arxiv.org/abs/2508.14502)
*Thanh-Nhan Vo,Trong-Thuan Nguyen,Tam V. Nguyen,Minh-Triet Tran*

Main category: cs.CV

TL;DR: SATURN是一个轻量级的文本到图像生成方法，通过将场景图转换为显著性排序的token序列，在保持VAR-CLIP主干网络冻结的情况下，仅微调VAR变换器，显著提升了复杂提示下的布局和对象关系准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型在照片级渲染方面表现出色，但在处理复杂提示时难以准确捕捉布局和对象关系。虽然场景图提供了结构先验，但之前的图引导方法依赖于繁重的GAN或扩散管道，在速度和保真度上落后于现代自回归架构。

Method: SATURN是VAR-CLIP的轻量级扩展，将场景图转换为显著性排序的token序列，使冻结的CLIP-VQ-VAE主干能够解释图结构，同时仅微调VAR变换器。

Result: 在Visual Genome数据集上，SATURN将FID从56.45%降低到21.62%，将Inception Score从16.03提高到24.78，优于SG2IM和SGDiff等先前方法，且无需额外模块或多阶段训练。

Conclusion: 定性结果进一步证实了在对象计数保真度和空间关系准确性方面的改进，表明SATURN有效地将结构感知与最先进的自回归保真度相结合。

Abstract: State-of-the-art text-to-image models excel at photorealistic rendering but
often struggle to capture the layout and object relationships implied by
complex prompts. Scene graphs provide a natural structural prior, yet previous
graph-guided approaches have typically relied on heavy GAN or diffusion
pipelines, which lag behind modern autoregressive architectures in both speed
and fidelity. We introduce SATURN (Structured Arrangement of Triplets for
Unified Rendering Networks), a lightweight extension to VAR-CLIP that
translates a scene graph into a salience-ordered token sequence, enabling a
frozen CLIP-VQ-VAE backbone to interpret graph structure while fine-tuning only
the VAR transformer. On the Visual Genome dataset, SATURN reduces FID from
56.45% to 21.62% and increases the Inception Score from 16.03 to 24.78,
outperforming prior methods such as SG2IM and SGDiff without requiring extra
modules or multi-stage training. Qualitative results further confirm
improvements in object count fidelity and spatial relation accuracy, showing
that SATURN effectively combines structural awareness with state-of-the-art
autoregressive fidelity.

</details>


### [43] [PB-IAD: Utilizing multimodal foundation models for semantic industrial anomaly detection in dynamic manufacturing environments](https://arxiv.org/abs/2508.14504)
*Bernd Hofmann,Albert Scheck,Joerg Franke,Patrick Bruendl*

Main category: cs.CV

TL;DR: PB-IAD是一个基于基础模型的工业异常检测框架，利用多模态和推理能力，通过语义指令在数据稀疏场景中实现优异性能


<details>
  <summary>Details</summary>
Motivation: 传统统计和数据驱动方法依赖大量标注数据且灵活性有限，基础模型的感知能力为工业异常检测提供了新机遇

Method: 提出PB-IAD框架，包含专门设计的提示模板和预处理模块，利用GPT-4.1进行多模态异常检测，支持领域专家无需数据科学知识即可定制系统

Result: 在三种制造场景、两种数据模态下评估，在数据稀疏和少样本设置中表现优于PatchCore等最先进方法

Conclusion: PB-IAD框架通过语义指令实现了卓越的工业异常检测性能，特别适用于动态生产环境中的数据稀疏场景

Abstract: The detection of anomalies in manufacturing processes is crucial to ensure
product quality and identify process deviations. Statistical and data-driven
approaches remain the standard in industrial anomaly detection, yet their
adaptability and usability are constrained by the dependence on extensive
annotated datasets and limited flexibility under dynamic production conditions.
Recent advances in the perception capabilities of foundation models provide
promising opportunities for their adaptation to this downstream task. This
paper presents PB-IAD (Prompt-based Industrial Anomaly Detection), a novel
framework that leverages the multimodal and reasoning capabilities of
foundation models for industrial anomaly detection. Specifically, PB-IAD
addresses three key requirements of dynamic production environments: data
sparsity, agile adaptability, and domain user centricity. In addition to the
anomaly detection, the framework includes a prompt template that is
specifically designed for iteratively implementing domain-specific process
knowledge, as well as a pre-processing module that translates domain user
inputs into effective system prompts. This user-centric design allows domain
experts to customise the system flexibly without requiring data science
expertise. The proposed framework is evaluated by utilizing GPT-4.1 across
three distinct manufacturing scenarios, two data modalities, and an ablation
study to systematically assess the contribution of semantic instructions.
Furthermore, PB-IAD is benchmarked to state-of-the-art methods for anomaly
detection such as PatchCore. The results demonstrate superior performance,
particularly in data-sparse scenarios and low-shot settings, achieved solely
through semantic instructions.

</details>


### [44] [Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles](https://arxiv.org/abs/2508.14527)
*Jiangfan Liu,Yongkang Guo,Fangzhi Zhong,Tianyuan Zhang,Zonglei Jing,Siyuan Liang,Jiakai Wang,Mingchuan Zhang,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TL;DR: ScenGE是一个通过大语言模型生成安全关键场景的框架，能够推理新颖的对抗案例并通过复杂交通流放大威胁，相比现有方法能发现更多严重碰撞案例(+31.96%)


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶安全评估主要依赖预定义的威胁模式或基于规则的策略，无法暴露多样化和未预见的故障模式，需要更有效的安全关键场景生成方法

Method: 采用两阶段方法：1) Meta-Scenario Generation - 使用基于结构化驾驶知识的大语言模型推理对抗代理行为；2) Complex Scenario Evolution - 构建对抗协作图优化关键代理轨迹，同时减少ego车辆操纵空间并创建关键遮挡

Result: 在多个基于强化学习的AV模型上实验显示，ScenGE比最先进基线平均多发现31.96%的严重碰撞案例，生成的场景既合理又关键，且能通过对抗训练提高模型鲁棒性

Conclusion: 该框架为建立公众信任和确保自动驾驶安全部署迈出了关键一步，可应用于大模型AV系统并在不同模拟器上部署，已通过实车测试和人工评估验证

Abstract: The generation of safety-critical scenarios in simulation has become
increasingly crucial for safety evaluation in autonomous vehicles prior to road
deployment in society. However, current approaches largely rely on predefined
threat patterns or rule-based strategies, which limit their ability to expose
diverse and unforeseen failure modes. To overcome these, we propose ScenGE, a
framework that can generate plentiful safety-critical scenarios by reasoning
novel adversarial cases and then amplifying them with complex traffic flows.
Given a simple prompt of a benign scene, it first performs Meta-Scenario
Generation, where a large language model, grounded in structured driving
knowledge, infers an adversarial agent whose behavior poses a threat that is
both plausible and deliberately challenging. This meta-scenario is then
specified in executable code for precise in-simulator control. Subsequently,
Complex Scenario Evolution uses background vehicles to amplify the core threat
introduced by Meta-Scenario. It builds an adversarial collaborator graph to
identify key agent trajectories for optimization. These perturbations are
designed to simultaneously reduce the ego vehicle's maneuvering space and
create critical occlusions. Extensive experiments conducted on multiple
reinforcement learning based AV models show that ScenGE uncovers more severe
collision cases (+31.96%) on average than SoTA baselines. Additionally, our
ScenGE can be applied to large model based AV systems and deployed on different
simulators; we further observe that adversarial training on our scenarios
improves the model robustness. Finally, we validate our framework through
real-world vehicle tests and human evaluation, confirming that the generated
scenarios are both plausible and critical. We hope our paper can build up a
critical step towards building public trust and ensuring their safe deployment.

</details>


### [45] [WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch Selection with VLM and LLM Knowledge Fusion](https://arxiv.org/abs/2508.14537)
*Yonghan Shin,SeungKyu Kim,Won-Ki Jeong*

Main category: cs.CV

TL;DR: WISE-FUSE是一个自适应全切片图像编码框架，通过病理领域视觉语言模型和大型语言模型选择性处理诊断相关区域，将编码时间减少三倍以上，同时保持或超越详尽处理的诊断性能。


<details>
  <summary>Details</summary>
Motivation: 计算病理学中的全切片图像(WSI)由于千兆像素级的规模，需要处理数万到数十万个高分辨率图像块，导致预处理和训练时间长达数天甚至数周，成为实际部署中的主要瓶颈。

Method: WISE-FUSE首先使用知识蒸馏机制计算低分辨率图像块与类别特定文本描述之间的相似度得分，基于这些得分选择信息丰富的区域子集，然后选择性地编码对应的高分辨率图像块并与文本嵌入融合以增强诊断上下文。

Result: 广泛实验表明，WISE-FUSE将WSI编码时间减少了三倍以上，同时实现了与详尽图像块处理相当或更优的诊断性能。

Conclusion: WISE-FUSE为计算病理学提供了一个可扩展且实用的解决方案，通过选择性处理诊断相关区域有效解决了全切片图像编码的瓶颈问题。

Abstract: Whole slide images (WSIs) in computational pathology (CPath) pose a major
computational challenge due to their gigapixel scale, often requiring the
processing of tens to hundreds of thousands of high-resolution patches per
slide. This results in prohibitive encoding costs, with preprocessing and
training times extending to days or even weeks-making WSI encoding the most
significant bottleneck in real-world deployment. In this work, we propose
WISE-FUSE, an adaptive WSI encoding framework that leverages pathology-domain
vision-language models and large language models to address this challenge by
selectively processing diagnostically relevant regions. WISE-FUSE first
computes similarity scores between low-resolution patches and class-specific
textual descriptions using a knowledge distillation mechanism that preserves
fine-grained diagnostic features. Based on these similarity scores, we select a
small subset of informative regions for the target task, which quickly
eliminates irrelevant patches at the coarse level. The corresponding
high-resolution patches are then selectively encoded and fused with textual
embeddings to reinforce diagnostic context. Extensive experiments demonstrate
that WISE-FUSE reduces WSI encoding time by over threefold while achieving
diagnostic performance comparable to or surpassing that of exhaustive patch
processing, offering a scalable and practical solution for CPath.

</details>


### [46] [Making Pose Representations More Expressive and Disentangled via Residual Vector Quantization](https://arxiv.org/abs/2508.14561)
*Sukhyun Jeong,Hong-Gi Shin,Yong-Hoon Choi*

Main category: cs.CV

TL;DR: 通过残差向量量化(RVQ)技术在姿势码基础上增强连续运动特征表达，提升了可控制运动生成的细节表现力和控制性


<details>
  <summary>Details</summary>
Motivation: 现有的可控制运动生成方法主要依靠离散的姿势码表示，无法抓取细雅的运动细节，限制了表达力

Method: 提出使用残差向量量化(RVQ)技术，在姿势码基础的潜在表示中增强连续运动特征，保持解释性和可操控性的同时抓取细微运动特征

Result: 在HumanML3D数据集上，模型将FID从0.041降低到0.015，Top-1 R-Precision从0.508提升到0.510，定性分析证实了运动编辑的可控制性

Conclusion: 该方法有效地结合了离散姿势码的解释性和连续特征的细节表达能力，显著提升了可控制运动生成的质量

Abstract: Recent progress in text-to-motion has advanced both 3D human motion
generation and text-based motion control. Controllable motion generation
(CoMo), which enables intuitive control, typically relies on pose code
representations, but discrete pose codes alone cannot capture fine-grained
motion details, limiting expressiveness. To overcome this, we propose a method
that augments pose code-based latent representations with continuous motion
features using residual vector quantization (RVQ). This design preserves the
interpretability and manipulability of pose codes while effectively capturing
subtle motion characteristics such as high-frequency details. Experiments on
the HumanML3D dataset show that our model reduces Frechet inception distance
(FID) from 0.041 to 0.015 and improves Top-1 R-Precision from 0.508 to 0.510.
Qualitative analysis of pairwise direction similarity between pose codes
further confirms the model's controllability for motion editing.

</details>


### [47] [Locality-aware Concept Bottleneck Model](https://arxiv.org/abs/2508.14562)
*Sujin Jeon,Hyundo Lee,Eungseo Kim,Sanghack Lee,Byoung-Tak Zhang,Inwoo Hwang*

Main category: cs.CV

TL;DR: 提出LCBM框架，通过原型学习确保概念的空间定位准确性，利用基础模型信息改善概念定位问题，同时保持分类性能


<details>
  <summary>Details</summary>
Motivation: 现有的无标签概念瓶颈模型往往无法准确定位概念的相关区域，在预测概念存在时会关注视觉上不相关的区域

Method: 为每个概念分配一个原型，通过鼓励原型编码相似局部区域来学习概念的原型图像特征，利用基础模型确保原型与相关概念的相关性，然后使用原型促进学习识别每个概念应该从哪个局部区域预测

Result: 实验结果表明LCBM能有效识别图像中的现有概念，在保持可比分类性能的同时展现出改进的定位能力

Conclusion: LCBM框架成功解决了概念定位问题，通过原型学习和基础模型的结合实现了准确的空间概念定位

Abstract: Concept bottleneck models (CBMs) are inherently interpretable models that
make predictions based on human-understandable visual cues, referred to as
concepts. As obtaining dense concept annotations with human labeling is
demanding and costly, recent approaches utilize foundation models to determine
the concepts existing in the images. However, such label-free CBMs often fail
to localize concepts in relevant regions, attending to visually unrelated
regions when predicting concept presence. To this end, we propose a framework,
coined Locality-aware Concept Bottleneck Model (LCBM), which utilizes rich
information from foundation models and adopts prototype learning to ensure
accurate spatial localization of the concepts. Specifically, we assign one
prototype to each concept, promoted to represent a prototypical image feature
of that concept. These prototypes are learned by encouraging them to encode
similar local regions, leveraging foundation models to assure the relevance of
each prototype to its associated concept. Then we use the prototypes to
facilitate the learning process of identifying the proper local region from
which each concept should be predicted. Experimental results demonstrate that
LCBM effectively identifies present concepts in the images and exhibits
improved localization while maintaining comparable classification performance.

</details>


### [48] [GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via Gaussian Surfels](https://arxiv.org/abs/2508.14563)
*Xingyuan Yang,Min Wei*

Main category: cs.CV

TL;DR: GOGS是一个基于2D高斯面元的两阶段框架，通过物理渲染和材料分解解决光泽物体逆渲染中的模糊性问题，在几何重建、材质分离和新光照下的真实感重照明方面达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 解决光泽物体逆渲染中的固有模糊性问题。基于NeRF的方法计算成本过高，而3D高斯溅射在镜面反射下存在局限性，多视角不一致性导致高频表面噪声和结构伪影，简化渲染方程掩盖了材质属性

Method: 两阶段框架：1) 通过基于物理的渲染和分割求和近似建立稳健的表面重建，利用基础模型的几何先验增强；2) 通过蒙特卡洛重要性采样进行材料分解，使用可微分2D高斯光线追踪建模间接照明，通过基于球面mipmap的方向编码细化高频镜面细节

Result: 在几何重建、材质分离和新光照下的真实感重照明方面表现出最先进的性能，优于现有的逆渲染方法

Conclusion: GOGS框架有效解决了光泽物体逆渲染中的关键挑战，通过创新的两阶段方法实现了高质量的表面重建和材料分解，为复杂光照条件下的真实感渲染提供了有效解决方案

Abstract: Inverse rendering of glossy objects from RGB imagery remains fundamentally
limited by inherent ambiguity. Although NeRF-based methods achieve
high-fidelity reconstruction via dense-ray sampling, their computational cost
is prohibitive. Recent 3D Gaussian Splatting achieves high reconstruction
efficiency but exhibits limitations under specular reflections. Multi-view
inconsistencies introduce high-frequency surface noise and structural
artifacts, while simplified rendering equations obscure material properties,
leading to implausible relighting results. To address these issues, we propose
GOGS, a novel two-stage framework based on 2D Gaussian surfels. First, we
establish robust surface reconstruction through physics-based rendering with
split-sum approximation, enhanced by geometric priors from foundation models.
Second, we perform material decomposition by leveraging Monte Carlo importance
sampling of the full rendering equation, modeling indirect illumination via
differentiable 2D Gaussian ray tracing and refining high-frequency specular
details through spherical mipmap-based directional encoding that captures
anisotropic highlights. Extensive experiments demonstrate state-of-the-art
performance in geometry reconstruction, material separation, and photorealistic
relighting under novel illuminations, outperforming existing inverse rendering
approaches.

</details>


### [49] [Safety-Critical Learning for Long-Tail Events: The TUM Traffic Accident Dataset](https://arxiv.org/abs/2508.14567)
*Walter Zimmer,Ross Greer,Xingcheng Zhou,Rui Song,Marc Pavel,Daniel Lehmberg,Ahmed Ghita,Akshay Gopalkrishnan,Mohan Trivedi,Alois Knoll*

Main category: cs.CV

TL;DR: TUMTraf-A数据集是一个包含10个高速公路事故序列的真实世界数据集，提供2D/3D标注框和轨迹ID，并提出了结合规则和学习的Accid3nD事故检测模型。


<details>
  <summary>Details</summary>
Motivation: 尽管交通安全研究已有大量工作，但事故仍频繁发生，需要被理解为交通网络中不可避免的偶发结果，因此需要高质量的事故数据集和检测方法。

Method: 提出了TUMTraf-A数据集（包含路边摄像头和LiDAR数据），并开发了Accid3nD模型，结合规则方法和学习方法进行事故检测。

Result: 数据集包含294,924个2D标注框和93,012个3D标注框，48,144帧标注图像，10个物体类别。实验和消融研究证明了所提方法的鲁棒性。

Conclusion: 该研究提供了高质量的高速公路事故数据集和有效的事故检测方法，为交通安全研究提供了重要资源。

Abstract: Even though a significant amount of work has been done to increase the safety
of transportation networks, accidents still occur regularly. They must be
understood as an unavoidable and sporadic outcome of traffic networks. We
present the TUM Traffic Accident (TUMTraf-A) dataset, a collection of
real-world highway accidents. It contains ten sequences of vehicle crashes at
high-speed driving with 294,924 labeled 2D and 93,012 labeled 3D boxes and
track IDs within 48,144 labeled frames recorded from four roadside cameras and
LiDARs at 10 Hz. The dataset contains ten object classes and is provided in the
OpenLABEL format. We propose Accid3nD, an accident detection model that
combines a rule-based approach with a learning-based one. Experiments and
ablation studies on our dataset show the robustness of our proposed method. The
dataset, model, and code are available on our project website:
https://tum-traffic-dataset.github.io/tumtraf-a.

</details>


### [50] [Controllable Latent Space Augmentation for Digital Pathology](https://arxiv.org/abs/2508.14588)
*Sofiène Boutaj,Marin Scalbert,Pierre Marza,Florent Couzinie-Devy,Maria Vakalopoulou,Stergios Christodoulidis*

Main category: cs.CV

TL;DR: HistAug是一种用于数字病理学的高效生成模型，通过在潜在空间中进行可控增强来改善多实例学习性能，特别是在数据稀缺情况下表现优异


<details>
  <summary>Details</summary>
Motivation: 数字病理学中的全切片图像分析面临巨大分辨率挑战和密集监督信号稀缺问题，现有增强方法要么计算成本过高，要么缺乏对变换语义的控制

Method: 开发基于生成模型的HistAug方法，通过在潜在空间中条件化显式补丁级变换（如色调、侵蚀等）来生成真实的增强嵌入，同时保持初始语义信息

Result: 在多个切片级任务和不同器官上的实验表明，HistAug优于现有方法，特别是在低数据情况下表现突出，消融研究证实了学习变换相对于基于噪声的扰动的优势

Conclusion: HistAug提供了一种快速高效的增强方法，能够处理大量补丁并持续改善MIL模型性能，强调了均匀WSI级增强的重要性

Abstract: Whole slide image (WSI) analysis in digital pathology presents unique
challenges due to the gigapixel resolution of WSIs and the scarcity of dense
supervision signals. While Multiple Instance Learning (MIL) is a natural fit
for slide-level tasks, training robust models requires large and diverse
datasets. Even though image augmentation techniques could be utilized to
increase data variability and reduce overfitting, implementing them effectively
is not a trivial task. Traditional patch-level augmentation is prohibitively
expensive due to the large number of patches extracted from each WSI, and
existing feature-level augmentation methods lack control over transformation
semantics. We introduce HistAug, a fast and efficient generative model for
controllable augmentations in the latent space for digital pathology. By
conditioning on explicit patch-level transformations (e.g., hue, erosion),
HistAug generates realistic augmented embeddings while preserving initial
semantic information. Our method allows the processing of a large number of
patches in a single forward pass efficiently, while at the same time
consistently improving MIL model performance. Experiments across multiple
slide-level tasks and diverse organs show that HistAug outperforms existing
methods, particularly in low-data regimes. Ablation studies confirm the
benefits of learned transformations over noise-based perturbations and
highlight the importance of uniform WSI-wise augmentation. Code is available at
https://github.com/MICS-Lab/HistAug.

</details>


### [51] [Reliable Smoke Detection via Optical Flow-Guided Feature Fusion and Transformer-Based Uncertainty Modeling](https://arxiv.org/abs/2508.14597)
*Nitish Kumar Mahala,Muzammil Khan,Pushpendra Kumar*

Main category: cs.CV

TL;DR: 提出了一种基于单目视觉的两阶段不确定性感知移位窗口Transformer框架，用于烟雾检测，通过光流运动编码和不确定性估计实现高精度、可靠的早期火灾预警


<details>
  <summary>Details</summary>
Motivation: 火灾对生命和基础设施构成严重威胁，传统烟雾检测器受光照变化、流动动力学和环境噪声影响，可靠性不足，需要高保真度的早期预警系统

Method: 使用四色定理启发的双相水平集分数阶变分模型进行光流估计，生成颜色编码光流图；通过高斯混合模型融合外观线索生成烟雾区域分割掩码；采用增强多尺度不确定性估计头的移位窗口Transformer，在两阶段学习机制下训练

Result: 通过多评估指标和与最先进方法的对比分析，证明了该方法具有优异的泛化能力和鲁棒性

Conclusion: 该方法为监控、工业安全和自主监控应用中的早期火灾检测提供了可靠的解决方案

Abstract: Fire outbreaks pose critical threats to human life and infrastructure,
necessitating high-fidelity early-warning systems that detect combustion
precursors such as smoke. However, smoke plumes exhibit complex spatiotemporal
dynamics influenced by illumination variability, flow kinematics, and
environmental noise, undermining the reliability of traditional detectors. To
address these challenges without the logistical complexity of multi-sensor
arrays, we propose an information-fusion framework by integrating smoke feature
representations extracted from monocular imagery. Specifically, a Two-Phase
Uncertainty-Aware Shifted Windows Transformer for robust and reliable smoke
detection, leveraging a novel smoke segmentation dataset, constructed via
optical flow-based motion encoding, is proposed. The optical flow estimation is
performed with a four-color-theorem-inspired dual-phase level-set
fractional-order variational model, which preserves motion discontinuities. The
resulting color-encoded optical flow maps are fused with appearance cues via a
Gaussian Mixture Model to generate binary segmentation masks of the smoke
regions. These fused representations are fed into the novel Shifted-Windows
Transformer, which is augmented with a multi-scale uncertainty estimation head
and trained under a two-phase learning regimen. First learning phase optimizes
smoke detection accuracy, while during the second phase, the model learns to
estimate plausibility confidence in its predictions by jointly modeling
aleatoric and epistemic uncertainties. Extensive experiments using multiple
evaluation metrics and comparative analysis with state-of-the-art approaches
demonstrate superior generalization and robustness, offering a reliable
solution for early fire detection in surveillance, industrial safety, and
autonomous monitoring applications.

</details>


### [52] [Incremental Object Detection with Prompt-based Methods](https://arxiv.org/abs/2508.14599)
*Matthias Neuwirth-Trapp,Maarten Bieshaar,Danda Pani Paudel,Luc Van Gool*

Main category: cs.CV

TL;DR: 本文分析了视觉提示方法在增量目标检测(IOD)中的应用，发现在复杂域增量学习设置下，单纯的提示方法表现不佳，但结合少量数据回放的方法能取得最佳效果。


<details>
  <summary>Details</summary>
Motivation: 视觉提示方法在增量图像分类中表现出色，但其在增量目标检测中的通用性和有效性尚未得到验证，需要系统性地评估这些方法在更复杂场景下的表现。

Method: 在复杂域增量学习设置下分析三种不同的提示方法，提供广泛的基线对比，并测试提示长度和初始化策略的影响。

Result: 实验表明，单纯的提示方法在IOD任务中表现不佳，但将视觉提示与少量历史数据回放相结合的方法能够取得最优性能。

Conclusion: 研究为推进提示方法在增量目标检测中的应用提供了宝贵见解，强调了结合数据回放策略的重要性。

Abstract: Visual prompt-based methods have seen growing interest in incremental
learning (IL) for image classification. These approaches learn additional
embedding vectors while keeping the model frozen, making them efficient to
train. However, no prior work has applied such methods to incremental object
detection (IOD), leaving their generalizability unclear. In this paper, we
analyze three different prompt-based methods under a complex domain-incremental
learning setting. We additionally provide a wide range of reference baselines
for comparison. Empirically, we show that the prompt-based approaches we tested
underperform in this setting. However, a strong yet practical method, combining
visual prompts with replaying a small portion of previous data, achieves the
best results. Together with additional experiments on prompt length and
initialization, our findings offer valuable insights for advancing prompt-based
IL in IOD.

</details>


### [53] [UST-SSM: Unified Spatio-Temporal State Space Models for Point Cloud Video Modeling](https://arxiv.org/abs/2508.14604)
*Peiming Li,Ziyi Wang,Yulin Yuan,Hong Liu,Xiangming Meng,Junsong Yuan,Mengyuan Liu*

Main category: cs.CV

TL;DR: 提出UST-SSM模型，通过空间-时间选择扫描、结构聚合和时间交互采样，解决了点云视频在选择性状态空间模型中的时空无序问题，在多个数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 点云视频能捕捉动态3D运动并减少光照和视角变化的影响，但时空无序性阻碍了选择性状态空间模型对点云视频的单向建模。

Method: 提出统一时空状态空间模型(UST-SSM)，包含：1)空间-时间选择扫描(STSS)通过提示引导聚类重组无序点；2)空间-时间结构聚合(STSA)聚合时空特征；3)时间交互采样(TIS)增强时间依赖性。

Result: 在MSR-Action3D、NTU RGB+D和Synthia 4D数据集上的实验结果验证了方法的有效性。

Conclusion: UST-SSM成功将SSM最新进展扩展到点云视频，有效解决了时空无序问题，提升了点云视频动作识别性能。

Abstract: Point cloud videos capture dynamic 3D motion while reducing the effects of
lighting and viewpoint variations, making them highly effective for recognizing
subtle and continuous human actions. Although Selective State Space Models
(SSMs) have shown good performance in sequence modeling with linear complexity,
the spatio-temporal disorder of point cloud videos hinders their unidirectional
modeling when directly unfolding the point cloud video into a 1D sequence
through temporally sequential scanning. To address this challenge, we propose
the Unified Spatio-Temporal State Space Model (UST-SSM), which extends the
latest advancements in SSMs to point cloud videos. Specifically, we introduce
Spatial-Temporal Selection Scanning (STSS), which reorganizes unordered points
into semantic-aware sequences through prompt-guided clustering, thereby
enabling the effective utilization of points that are spatially and temporally
distant yet similar within the sequence. For missing 4D geometric and motion
details, Spatio-Temporal Structure Aggregation (STSA) aggregates
spatio-temporal features and compensates. To improve temporal interaction
within the sampled sequence, Temporal Interaction Sampling (TIS) enhances
fine-grained temporal dependencies through non-anchor frame utilization and
expanded receptive fields. Experimental results on the MSR-Action3D, NTU RGB+D,
and Synthia 4D datasets validate the effectiveness of our method. Our code is
available at https://github.com/wangzy01/UST-SSM.

</details>


### [54] [SMTrack: End-to-End Trained Spiking Neural Networks for Multi-Object Tracking in RGB Videos](https://arxiv.org/abs/2508.14607)
*Pengzhi Zhong,Xinzhe Wang,Dan Zeng,Qihua Zhou,Feixiang He,Shuiwang Li*

Main category: cs.CV

TL;DR: 提出了SMTrack——首个直接在标准RGB视频上进行端到端多目标跟踪的深度脉冲神经网络框架，通过自适应尺度感知归一化Wasserstein距离损失和TrackTrack身份模块，在多个数据集上达到与ANN方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前脉冲神经网络在视觉任务中主要局限于图像分类、目标检测和事件跟踪，而在传统RGB视频流中直接训练SNN进行复杂时序任务（如多目标跟踪）的研究仍不足。

Method: 提出SMTrack框架，包含自适应尺度感知归一化Wasserstein距离损失（Asa-NWDLoss）来提升不同尺度和密度下的检测定位性能，以及TrackTrack身份模块来维持稳定的目标轨迹。

Result: 在BEE24、MOT17、MOT20和DanceTrack数据集上的广泛评估表明，SMTrack达到了与领先的基于ANN的MOT方法相当的性能。

Conclusion: 该工作推动了SNN在复杂场景下鲁棒准确跟踪的发展，展示了直接训练SNN在RGB视频多目标跟踪任务中的潜力。

Abstract: Brain-inspired Spiking Neural Networks (SNNs) exhibit significant potential
for low-power computation, yet their application in visual tasks remains
largely confined to image classification, object detection, and event-based
tracking. In contrast, real-world vision systems still widely use conventional
RGB video streams, where the potential of directly-trained SNNs for complex
temporal tasks such as multi-object tracking (MOT) remains underexplored. To
address this challenge, we propose SMTrack-the first directly trained deep SNN
framework for end-to-end multi-object tracking on standard RGB videos. SMTrack
introduces an adaptive and scale-aware Normalized Wasserstein Distance loss
(Asa-NWDLoss) to improve detection and localization performance under varying
object scales and densities. Specifically, the method computes the average
object size within each training batch and dynamically adjusts the
normalization factor, thereby enhancing sensitivity to small objects. For the
association stage, we incorporate the TrackTrack identity module to maintain
robust and consistent object trajectories. Extensive evaluations on BEE24,
MOT17, MOT20, and DanceTrack show that SMTrack achieves performance on par with
leading ANN-based MOT methods, advancing robust and accurate SNN-based tracking
in complex scenarios.

</details>


### [55] [AnchorSync: Global Consistency Optimization for Long Video Editing](https://arxiv.org/abs/2508.14609)
*Zichi Liu,Yinggui Wang,Tao Wei,Chao Ma*

Main category: cs.CV

TL;DR: AnchorSync是一个基于扩散模型的视频编辑框架，通过分离锚点帧编辑和中间帧插值来解决长视频编辑中的全局一致性和时间连贯性问题


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理分钟级长视频时容易出现结构漂移和时间伪影，难以维持数千帧的全局一致性和时间连贯性

Method: 将长视频编辑任务解耦为稀疏锚点帧编辑和平滑中间帧插值，通过渐进去噪过程强制结构一致性，并利用多模态指导保持时间动态

Result: 大量实验表明AnchorSync能够产生连贯、高保真的编辑效果，在视觉质量和时间稳定性方面超越现有方法

Conclusion: AnchorSync通过创新的解耦框架有效解决了长视频编辑中的关键挑战，为高质量长视频编辑提供了可行方案

Abstract: Editing long videos remains a challenging task due to the need for
maintaining both global consistency and temporal coherence across thousands of
frames. Existing methods often suffer from structural drift or temporal
artifacts, particularly in minute-long sequences. We introduce AnchorSync, a
novel diffusion-based framework that enables high-quality, long-term video
editing by decoupling the task into sparse anchor frame editing and smooth
intermediate frame interpolation. Our approach enforces structural consistency
through a progressive denoising process and preserves temporal dynamics via
multimodal guidance. Extensive experiments show that AnchorSync produces
coherent, high-fidelity edits, surpassing prior methods in visual quality and
temporal stability.

</details>


### [56] [Towards PerSense++: Advancing Training-Free Personalized Instance Segmentation in Dense Images](https://arxiv.org/abs/2508.14660)
*Muhammad Ibraheem Siddiqui,Muhammad Umer Sheikh,Hassan Abid,Kevin Henry,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: PerSense是一个无需训练、模型无关的单样本密集图像个性化实例分割框架，通过密度图生成实例候选点提示并过滤假阳性。PerSense++增强版本增加了多样性感知范例选择、混合提示生成和无关掩码拒绝模块，在密集场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 密集视觉场景中的分割面临遮挡、背景杂乱和尺度变化的挑战，需要开发专门的方法来处理这些复杂情况。

Method: 使用Instance Detection Module通过密度图生成实例级候选点提示，Point Prompt Selection Module通过自适应阈值和空间门控过滤假阳性，反馈机制自动选择有效范例提升密度图质量。PerSense++增加了多样性感知范例选择、混合IDM（结合轮廓和峰值提示生成）和无关掩码拒绝模块。

Result: 在多个基准测试上的广泛实验表明，PerSense++在密集设置中优于现有方法。

Conclusion: 提出的PerSense框架及其增强版本PerSense++能够有效处理密集图像中的个性化实例分割任务，并为此任务提供了专用基准数据集PerSense-D。

Abstract: Segmentation in dense visual scenes poses significant challenges due to
occlusions, background clutter, and scale variations. To address this, we
introduce PerSense, an end-to-end, training-free, and model-agnostic one-shot
framework for Personalized instance Segmentation in dense images. PerSense
employs a novel Instance Detection Module (IDM) that leverages density maps
(DMs) to generate instance-level candidate point prompts, followed by a Point
Prompt Selection Module (PPSM) that filters false positives via adaptive
thresholding and spatial gating. A feedback mechanism further enhances
segmentation by automatically selecting effective exemplars to improve DM
quality. We additionally present PerSense++, an enhanced variant that
incorporates three additional components to improve robustness in cluttered
scenes: (i) a diversity-aware exemplar selection strategy that leverages
feature and scale diversity for better DM generation; (ii) a hybrid IDM
combining contour and peak-based prompt generation for improved instance
separation within complex density patterns; and (iii) an Irrelevant Mask
Rejection Module (IMRM) that discards spatially inconsistent masks using
outlier analysis. Finally, to support this underexplored task, we introduce
PerSense-D, a dedicated benchmark for personalized segmentation in dense
images. Extensive experiments across multiple benchmarks demonstrate that
PerSense++ outperforms existing methods in dense settings.

</details>


### [57] [GeMS: Efficient Gaussian Splatting for Extreme Motion Blur](https://arxiv.org/abs/2508.14682)
*Gopi Raju Matta,Trisha Reddypalli,Vemunuri Divya Madhuri,Kaushik Mitra*

Main category: cs.CV

TL;DR: GeMS是一个处理严重运动模糊图像的3D高斯泼溅框架，无需依赖清晰图像，通过深度学习SfM和概率分布建模实现从极端模糊输入的直接场景重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要清晰图像进行相机姿态估计和点云生成，这在严重模糊场景中不现实。基于COLMAP初始化的方法也因特征对应不可靠而失败，需要直接从极端模糊图像重建场景的解决方案。

Method: 整合VGGSfM深度学习SfM管道进行姿态估计和点云生成；使用3DGS-MCMC将高斯分布作为概率分布样本进行稳健初始化；联合优化相机轨迹和高斯参数；GeMS-E版本还集成了事件驱动的双重积分去模糊进行渐进式细化。

Result: 在合成和真实数据集上实现了最先进的性能，是首个直接从严重模糊输入处理极端运动模糊的3DGS框架。

Conclusion: GeMS框架成功解决了从极端运动模糊图像直接进行3D重建的挑战，通过深度学习SfM和概率建模方法实现了无需清晰参考图像的稳健重建，GeMS-E版本进一步通过事件数据提升了重建质量。

Abstract: We introduce GeMS, a framework for 3D Gaussian Splatting (3DGS) designed to
handle severely motion-blurred images. State-of-the-art deblurring methods for
extreme blur, such as ExBluRF, as well as Gaussian Splatting-based approaches
like Deblur-GS, typically assume access to sharp images for camera pose
estimation and point cloud generation, an unrealistic assumption. Methods
relying on COLMAP initialization, such as BAD-Gaussians, also fail due to
unreliable feature correspondences under severe blur. To address these
challenges, we propose GeMS, a 3DGS framework that reconstructs scenes directly
from extremely blurred images. GeMS integrates: (1) VGGSfM, a deep
learning-based Structure-from-Motion pipeline that estimates poses and
generates point clouds directly from blurred inputs; (2) 3DGS-MCMC, which
enables robust scene initialization by treating Gaussians as samples from a
probability distribution, eliminating heuristic densification and pruning; and
(3) joint optimization of camera trajectories and Gaussian parameters for
stable reconstruction. While this pipeline produces strong results,
inaccuracies may remain when all inputs are severely blurred. To mitigate this,
we propose GeMS-E, which integrates a progressive refinement step using events:
(4) Event-based Double Integral (EDI) deblurring restores sharper images that
are then fed into GeMS, improving pose estimation, point cloud generation, and
overall reconstruction. Both GeMS and GeMS-E achieve state-of-the-art
performance on synthetic and real-world datasets. To our knowledge, this is the
first framework to address extreme motion blur within 3DGS directly from
severely blurred inputs.

</details>


### [58] [Seeing Further on the Shoulders of Giants: Knowledge Inheritance for Vision Foundation Models](https://arxiv.org/abs/2508.14707)
*Jiabo Huang,Chen Chen,Lingjuan Lyu*

Main category: cs.CV

TL;DR: 这篇论文提出了一种模型驱动的视觉基础模型训练方法，通过联合知识转移和保持技术，统一多个预训练模型来构建功能强大的通用视觉模型，避免了对大规模标签数据和高端GPU的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决数据中心方法对大规模高质量标签数据和高端硬件的依赖问题，充分利用现有开源预训练模型的知识，提供一种更接近实际应用场景的视觉基础模型建模方法。

Method: 通过在共享隐科空间统一多个预训练教师模型来缓解分布差异导致的矩阵转移不均表问题，使用适配器模块以通用教师模型作为知识库来整合其他专门模型的知识。

Result: 实验结果显示，该方法在四个基础视觉任务（图像分类、目标检测、语义分割和实例分割）上都超过了现有的数据中心模型。

Conclusion: 该研究提供了一种有效的模型驱动方案，能够通过联合知识转移和保持技术，构建功能强大的通用视觉基础模型，无需大规模标签数据训练，为视觉AI领域提供了新的建模思路。

Abstract: Vision foundation models (VFMs) are predominantly developed using
data-centric methods. These methods require training on vast amounts of data
usually with high-quality labels, which poses a bottleneck for most
institutions that lack both large-scale data and high-end GPUs. On the other
hand, many open-source vision models have been pretrained on domain-specific
data, enabling them to distill and represent core knowledge in a form that is
transferable across diverse applications. Even though these models are highly
valuable assets, they remain largely under-explored in empowering the
development of a general-purpose VFM. In this paper, we presents a new
model-driven approach for training VFMs through joint knowledge transfer and
preservation. Our method unifies multiple pre-trained teacher models in a
shared latent space to mitigate the ``imbalanced transfer'' issue caused by
their distributional gaps. Besides, we introduce a knowledge preservation
strategy to take a general-purpose teacher as a knowledge base for integrating
knowledge from the remaining purpose-specific teachers using an adapter module.
By unifying and aggregating existing models, we build a powerful VFM to inherit
teachers' expertise without needing to train on a large amount of labeled data.
Our model not only provides generalizable visual features, but also inherently
supports multiple downstream tasks. Extensive experiments demonstrate that our
VFM outperforms existing data-centric models across four fundamental vision
tasks, including image classification, object detection, semantic and instance
segmentation.

</details>


### [59] [GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting](https://arxiv.org/abs/2508.14717)
*Jiaxin Wei,Stefan Leutenegger,Simon Schaefer*

Main category: cs.CV

TL;DR: GSFix3D是一个新框架，通过将扩散模型的先验知识蒸馏到3D表示中，改善3D高斯泼溅在极端视角和部分观察区域的渲染质量，同时保持与观测场景细节的一致性。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅在新视角合成方面有显著进展，但在极端新视角或部分观察区域生成高质量渲染仍具挑战性。扩散模型具有强大的生成能力，但依赖文本提示且缺乏特定场景信息，阻碍了准确的3D重建任务。

Method: 提出GSFix3D框架，核心是GSFixer（通过定制微调协议获得的潜在扩散模型），利用网格和3D高斯将预训练生成模型适配到不同环境和重建方法的伪影类型。还提出随机掩码增强策略，使GSFixer能够合理修复缺失区域。

Result: 在具有挑战性的基准测试中，GSFix3D和GSFixer实现了最先进的性能，仅需在捕获数据上进行最少的场景特定微调。真实世界测试进一步证实其对潜在姿态错误的鲁棒性。

Conclusion: GSFix3D通过将扩散模型先验知识与3D表示相结合，有效解决了3D高斯泼溅在约束不足区域的视觉保真度问题，为未见相机姿态提供了鲁棒的新视角修复能力。

Abstract: Recent developments in 3D Gaussian Splatting have significantly enhanced
novel view synthesis, yet generating high-quality renderings from extreme novel
viewpoints or partially observed regions remains challenging. Meanwhile,
diffusion models exhibit strong generative capabilities, but their reliance on
text prompts and lack of awareness of specific scene information hinder
accurate 3D reconstruction tasks. To address these limitations, we introduce
GSFix3D, a novel framework that improves the visual fidelity in
under-constrained regions by distilling prior knowledge from diffusion models
into 3D representations, while preserving consistency with observed scene
details. At its core is GSFixer, a latent diffusion model obtained via our
customized fine-tuning protocol that can leverage both mesh and 3D Gaussians to
adapt pretrained generative models to a variety of environments and artifact
types from different reconstruction methods, enabling robust novel view repair
for unseen camera poses. Moreover, we propose a random mask augmentation
strategy that empowers GSFixer to plausibly inpaint missing regions.
Experiments on challenging benchmarks demonstrate that our GSFix3D and GSFixer
achieve state-of-the-art performance, requiring only minimal scene-specific
fine-tuning on captured data. Real-world test further confirms its resilience
to potential pose errors. Our code and data will be made publicly available.
Project page: https://gsfix3d.github.io.

</details>


### [60] [Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving](https://arxiv.org/abs/2508.14729)
*Leila Cheshmi,Mennatullah Siam*

Main category: cs.CV

TL;DR: 提出了一种基于多尺度视频transformer的高效类无关分割方法，仅使用运动线索检测未知物体，无需光流计算，在保持高分辨率信息的同时实现实时性能


<details>
  <summary>Details</summary>
Motivation: 自动驾驶安全需要处理未知物体和意外场景，现有方法依赖已知类别训练或计算成本高昂的视觉基础模型，需要更高效的像素级未知物体检测方案

Method: 使用多阶段多尺度查询-记忆解码架构，采用尺度特定的随机drop-token技术，通过共享可学习记忆模块保持详细的时空特征，避免传统解码器的特征压缩问题

Result: 在DAVIS'16、KITTI和Cityscapes数据集上持续超越多尺度基线方法，同时在GPU内存和运行时间方面保持高效

Conclusion: 该方法为安全关键机器人的实时鲁棒密集预测提供了有前景的方向，能够有效处理未知物体检测问题

Abstract: Ensuring safety in autonomous driving is a complex challenge requiring
handling unknown objects and unforeseen driving scenarios. We develop
multiscale video transformers capable of detecting unknown objects using only
motion cues. Video semantic and panoptic segmentation often relies on known
classes seen during training, overlooking novel categories. Recent visual
grounding with large language models is computationally expensive, especially
for pixel-level output. We propose an efficient video transformer trained
end-to-end for class-agnostic segmentation without optical flow. Our method
uses multi-stage multiscale query-memory decoding and a scale-specific random
drop-token to ensure efficiency and accuracy, maintaining detailed
spatiotemporal features with a shared, learnable memory module. Unlike
conventional decoders that compress features, our memory-centric design
preserves high-resolution information at multiple scales. We evaluate on
DAVIS'16, KITTI, and Cityscapes. Our method consistently outperforms multiscale
baselines while being efficient in GPU memory and run-time, demonstrating a
promising direction for real-time, robust dense prediction in safety-critical
robotics.

</details>


### [61] [Improved Mapping Between Illuminations and Sensors for RAW Images](https://arxiv.org/abs/2508.14730)
*Abhijith Punnappurath,Luxi Zhao,Hoang Le,Abdelrahman Abdelhamed,SaiKiran Kumar Tedla,Michael S. Brown*

Main category: cs.CV

TL;DR: 这篇论文提出了一种轻量级神经网络方法，用于RAW图像的照明来源增强和传感器映射，解决不同传感器和照明条件下数据集采集的挑战。


<details>
  <summary>Details</summary>
Motivation: RAW图像具有传感器特定性和照明依赖性，导致不同传感器和照明条件下需要重复采集数据，这为深度学习方法的数据准备带来巨大负担。

Method: 构建了首个包含广泛照明条件和多种相机的数据集，包含390种照明、4台相机和18个场景。提出了轻量级神经网络方法来实现照明来源增强和传感器映射。

Result: 方法在照明和传感器映射任务上超越了竞争方法，并在神经ISP训练下游任务中展示了其应用价值。

Conclusion: 该研究为解决RAW图像数据集挑战提供了有效方法，通过照明增强和传感器映射技术减少了数据采集的负担。

Abstract: RAW images are unprocessed camera sensor output with sensor-specific RGB
values based on the sensor's color filter spectral sensitivities. RAW images
also incur strong color casts due to the sensor's response to the spectral
properties of scene illumination. The sensor- and illumination-specific nature
of RAW images makes it challenging to capture RAW datasets for deep learning
methods, as scenes need to be captured for each sensor and under a wide range
of illumination. Methods for illumination augmentation for a given sensor and
the ability to map RAW images between sensors are important for reducing the
burden of data capture. To explore this problem, we introduce the
first-of-its-kind dataset comprising carefully captured scenes under a wide
range of illumination. Specifically, we use a customized lightbox with tunable
illumination spectra to capture several scenes with different cameras. Our
illumination and sensor mapping dataset has 390 illuminations, four cameras,
and 18 scenes. Using this dataset, we introduce a lightweight neural network
approach for illumination and sensor mapping that outperforms competing
methods. We demonstrate the utility of our approach on the downstream task of
training a neural ISP. Link to project page:
https://github.com/SamsungLabs/illum-sensor-mapping.

</details>


### [62] [Fusing Monocular RGB Images with AIS Data to Create a 6D Pose Estimation Dataset for Marine Vessels](https://arxiv.org/abs/2508.14767)
*Fabian Holst,Emre Gülsoylu,Simone Frintrop*

Main category: cs.CV

TL;DR: 通过融合单目RGB图像与AIS数据，提出了一种无需手动标注的海上船舶6D姿势估计数据集创建方法，并发布了BONK-pose数据集。


<details>
  <summary>Details</summary>
Motivation: 解决依赖单纯AIS系统存在的设备可靠性、数据操控和传输延迟等问题，实现更准确的船舶6D姿势估计。

Method: 使用YOLOX-X目标检测网络从RGB图像中检测船舶，然后结合AIS数据，通过传统变换方法（单应映和PnP）将数据对齐到图像坐标系中，生成3D边界框表示6D姿势。

Result: PnP方法比单应映方法投影误差显著更低；YOLOX-X模型在IoU阈值0.5时达到相关船舶类别的0.80 mAP；创建了包含3753张图像的BONK-pose公开数据集。

Conclusion: 该方法能够自动创建高质量的6D姿势估计数据集，免去手动标注的需求，为海上船舶监控和安全提供了有效的数据支持。

Abstract: The paper presents a novel technique for creating a 6D pose estimation
dataset for marine vessels by fusing monocular RGB images with Automatic
Identification System (AIS) data. The proposed technique addresses the
limitations of relying purely on AIS for location information, caused by issues
like equipment reliability, data manipulation, and transmission delays. By
combining vessel detections from monocular RGB images, obtained using an object
detection network (YOLOX-X), with AIS messages, the technique generates 3D
bounding boxes that represent the vessels' 6D poses, i.e. spatial and
rotational dimensions. The paper evaluates different object detection models to
locate vessels in image space. We also compare two transformation methods
(homography and Perspective-n-Point) for aligning AIS data with image
coordinates. The results of our work demonstrate that the Perspective-n-Point
(PnP) method achieves a significantly lower projection error compared to
homography-based approaches used before, and the YOLOX-X model achieves a mean
Average Precision (mAP) of 0.80 at an Intersection over Union (IoU) threshold
of 0.5 for relevant vessel classes. We show indication that our approach allows
the creation of a 6D pose estimation dataset without needing manual annotation.
Additionally, we introduce the Boats on Nordelbe Kehrwieder (BONK-pose), a
publicly available dataset comprising 3753 images with 3D bounding box
annotations for pose estimation, created by our data fusion approach. This
dataset can be used for training and evaluating 6D pose estimation networks. In
addition we introduce a set of 1000 images with 2D bounding box annotations for
ship detection from the same scene.

</details>


### [63] [6-DoF Object Tracking with Event-based Optical Flow and Frames](https://arxiv.org/abs/2508.14776)
*Zhichao Li,Arren Glover,Chiara Bartolozzi,Lorenzo Natale*

Main category: cs.CV

TL;DR: 通过结合事件相机光流测量和RGB相机全局位姿估计的方法，实现高速运动物体的6自由度位姿跟踪


<details>
  <summary>Details</summary>
Motivation: 解决高速运动物体位姿跟踪问题，充分利用事件相机高时间分辨率优势和RGB相机丰富视觉信息的特点

Method: 使用事件基光流算法测量物体运动，实现6-DoF速度跟踪，然后与低频全局位姿估计器集成

Result: 在合成和真实数据上验证有效性，特别在高速运动场景中表现优异

Conclusion: 结合事件相机和RGB相机优势的方法能够有效解决高速运动物体6-DoF位姿跟踪挑战

Abstract: Tracking the position and orientation of objects in space (i.e., in 6-DoF) in
real time is a fundamental problem in robotics for environment interaction. It
becomes more challenging when objects move at high-speed due to frame rate
limitations in conventional cameras and motion blur. Event cameras are
characterized by high temporal resolution, low latency and high dynamic range,
that can potentially overcome the impacts of motion blur. Traditional RGB
cameras provide rich visual information that is more suitable for the
challenging task of single-shot object pose estimation. In this work, we
propose using event-based optical flow combined with an RGB based global object
pose estimator for 6-DoF pose tracking of objects at high-speed, exploiting the
core advantages of both types of vision sensors. Specifically, we propose an
event-based optical flow algorithm for object motion measurement to implement
an object 6-DoF velocity tracker. By integrating the tracked object 6-DoF
velocity with low frequency estimated pose from the global pose estimator, the
method can track pose when objects move at high-speed. The proposed algorithm
is tested and validated on both synthetic and real world data, demonstrating
its effectiveness, especially in high-speed motion scenarios.

</details>


### [64] [MF-LPR$^2$: Multi-Frame License Plate Image Restoration and Recognition using Optical Flow](https://arxiv.org/abs/2508.14797)
*Kihyun Na,Junseok Oh,Youngkwan Cho,Bumjin Kim,Sungmin Cho,Jinyoung Choi,Injung Kim*

Main category: cs.CV

TL;DR: 提出MF-LPR²框架，通过多帧对齐和聚合解决低质量车牌图像恢复问题，在恢复质量和识别准确率上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 车载摄像头拍摄的车牌图像存在低分辨率、运动模糊和眩光等问题，现有基于预训练先验的生成模型无法可靠恢复这类低质量图像，经常产生严重伪影和失真

Method: 使用先进的光流估计器进行帧对齐，设计算法检测和校正错误的光流估计，利用车牌图像序列的时空一致性进行多帧聚合

Result: 在PSNR、SSIM和LPIPS指标上显著优于8个最新恢复模型，识别准确率达到86.44%，远超最佳单帧LPR(14.04%)和多帧LPR(82.55%)

Conclusion: MF-LPR²框架通过多帧对齐和聚合有效解决了低质量车牌图像恢复问题，提出的过滤和精炼算法对性能提升有显著贡献

Abstract: License plate recognition (LPR) is important for traffic law enforcement,
crime investigation, and surveillance. However, license plate areas in dash cam
images often suffer from low resolution, motion blur, and glare, which make
accurate recognition challenging. Existing generative models that rely on
pretrained priors cannot reliably restore such poor-quality images, frequently
introducing severe artifacts and distortions. To address this issue, we propose
a novel multi-frame license plate restoration and recognition framework,
MF-LPR$^2$, which addresses ambiguities in poor-quality images by aligning and
aggregating neighboring frames instead of relying on pretrained knowledge. To
achieve accurate frame alignment, we employ a state-of-the-art optical flow
estimator in conjunction with carefully designed algorithms that detect and
correct erroneous optical flow estimations by leveraging the spatio-temporal
consistency inherent in license plate image sequences. Our approach enhances
both image quality and recognition accuracy while preserving the evidential
content of the input images. In addition, we constructed a novel Realistic LPR
(RLPR) dataset to evaluate MF-LPR$^2$. The RLPR dataset contains 200 pairs of
low-quality license plate image sequences and high-quality pseudo ground-truth
images, reflecting the complexities of real-world scenarios. In experiments,
MF-LPR$^2$ outperformed eight recent restoration models in terms of PSNR, SSIM,
and LPIPS by significant margins. In recognition, MF-LPR$^2$ achieved an
accuracy of 86.44%, outperforming both the best single-frame LPR (14.04%) and
the multi-frame LPR (82.55%) among the eleven baseline models. The results of
ablation studies confirm that our filtering and refinement algorithms
significantly contribute to these improvements.

</details>


### [65] [DINOv3 with Test-Time Training for Medical Image Registration](https://arxiv.org/abs/2508.14809)
*Shansong Wang,Mojtaba Safari,Mingzhe Hu,Qiang Li,Chih-Wei Chang,Richard LJ Qiu,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 提出无需训练的医学图像配准方法，使用冻结的DINOv3编码器和测试时特征空间变形场优化，在两个基准测试中表现优异


<details>
  <summary>Details</summary>
Motivation: 传统学习方法需要大量训练数据，限制了临床采用，需要开发无需训练数据的解决方案

Method: 使用冻结的DINOv3编码器提取特征，在测试时通过优化特征空间的变形场来实现图像配准

Result: 在腹部MR-CT配准中取得最佳Dice分数0.790，最低HD95距离4.9±5.0，最低SDLogJ 0.08±0.02；在心脏MRI配准中DSC提升至0.769，SDLogJ降至0.11，HD95降至4.8

Conclusion: 在紧凑的基础特征空间中进行测试时优化，为无需额外训练的临床配准提供了实用且通用的解决方案

Abstract: Prior medical image registration approaches, particularly learning-based
methods, often require large amounts of training data, which constrains
clinical adoption. To overcome this limitation, we propose a training-free
pipeline that relies on a frozen DINOv3 encoder and test-time optimization of
the deformation field in feature space. Across two representative benchmarks,
the method is accurate and yields regular deformations. On Abdomen MR-CT, it
attained the best mean Dice score (DSC) of 0.790 together with the lowest 95th
percentile Hausdorff Distance (HD95) of 4.9+-5.0 and the lowest standard
deviation of Log-Jacobian (SDLogJ) of 0.08+-0.02. On ACDC cardiac MRI, it
improves mean DSC to 0.769 and reduces SDLogJ to 0.11 and HD95 to 4.8, a marked
gain over the initial alignment. The results indicate that operating in a
compact foundation feature space at test time offers a practical and general
solution for clinical registration without additional training.

</details>


### [66] [Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization](https://arxiv.org/abs/2508.14811)
*Canyu Zhao,Xiaoman Li,Tianjian Feng,Zhiyue Zhao,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: Tinker是一个无需逐场景微调的高保真3D编辑框架，支持单样本和少样本编辑，通过重新利用预训练扩散模型实现多视角一致性编辑


<details>
  <summary>Details</summary>
Motivation: 解决现有3D编辑技术需要大量逐场景优化和多视角一致输入的问题，降低通用3D内容创作的门槛

Method: 包含两个核心组件：1)参考多视角编辑器实现精确的参考驱动编辑；2)任意视角到视频合成器利用视频扩散的空间-时间先验进行场景补全和新视角生成

Result: 在编辑、新视角合成和渲染增强任务上达到最先进性能，显著减少3D内容创作的障碍

Conclusion: Tinker代表了向真正可扩展的零样本3D编辑迈出的关键一步

Abstract: We introduce Tinker, a versatile framework for high-fidelity 3D editing that
operates in both one-shot and few-shot regimes without any per-scene
finetuning. Unlike prior techniques that demand extensive per-scene
optimization to ensure multi-view consistency or to produce dozens of
consistent edited input views, Tinker delivers robust, multi-view consistent
edits from as few as one or two images. This capability stems from repurposing
pretrained diffusion models, which unlocks their latent 3D awareness. To drive
research in this space, we curate the first large-scale multi-view editing
dataset and data pipeline, spanning diverse scenes and styles. Building on this
dataset, we develop our framework capable of generating multi-view consistent
edited views without per-scene training, which consists of two novel
components: (1) Referring multi-view editor: Enables precise, reference-driven
edits that remain coherent across all viewpoints. (2) Any-view-to-video
synthesizer: Leverages spatial-temporal priors from video diffusion to perform
high-quality scene completion and novel-view generation even from sparse
inputs. Through extensive experiments, Tinker significantly reduces the barrier
to generalizable 3D content creation, achieving state-of-the-art performance on
editing, novel-view synthesis, and rendering enhancement tasks. We believe that
Tinker represents a key step towards truly scalable, zero-shot 3D editing.
Project webpage: https://aim-uofa.github.io/Tinker

</details>


### [67] [Repeating Words for Video-Language Retrieval with Coarse-to-Fine Objectives](https://arxiv.org/abs/2508.14812)
*Haoyu Zhao,Jiaxi Gu,Shicong Wang,Xing Zhang,Hang Xu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 这篇论文提出了一种新的视频-语言检索框架，通过细粒度对齐和推理流程改进，在不增加训练成本的情况下提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 解决视频流媒体检索中的高准确性与低训练成本问题，并深入挖掘视频和文本的细粒度信息。

Method: 使用粗细粒度目标函数进行语义理解，通过粒度感知表征模块获取细粒度数据，并提出包含投票机制和匹配熵的新推理流程。

Result: 在4个标准数据集上超过之前方法，MSR-VTT数据集Recall@1提升2.1%，DiDeMo数据集提升1.6%。

Conclusion: 该方法通过细粒度对齐和创新的推理流程，在不需额外训练的情况下显著提升了视频-语言检索性能。

Abstract: The explosive growth of video streaming presents challenges in achieving high
accuracy and low training costs for video-language retrieval. However, existing
methods rely on large-scale pre-training to improve video retrieval
performance, resulting in significant computational demands. Additionally, the
fine-grained information in videos and texts remains underexplored. To
alleviate these problems, we propose a novel framework to learn fine-grained
features for better alignment and introduce an inference pipeline to improve
performance without additional training. Specifically, we employ coarse-to-fine
objectives to understand the semantic information of video-text pairs,
including contrastive and matching learning. The fine-grained data used for
training is obtained through the Granularity-Aware Representation module, which
is designed based on similarity analysis between video frames and words in
captions. Furthermore, we observe that the repetition of keywords in the
original captions, referred to as "Repetition", can enhance retrieval
performance and improve alignment between video and text. Based on this
insight, we propose a novel and effective inference pipeline that incorporates
a voting mechanism and a new Matching Entropy metric to achieve better
retrieval performance without requiring additional pre-training. Experimental
results on four benchmarks demonstrate that the proposed method outperforms
previous approaches. Additionally, our inference pipeline achieves significant
performance improvements, with a 2.1% increase in Recall@1 on the MSR-VTT
dataset and a 1.6% increase on the DiDeMo dataset.

</details>


### [68] [TransLight: Image-Guided Customized Lighting Control with Generative Decoupling](https://arxiv.org/abs/2508.14814)
*Zongming Li,Lianghui Zhu,Haocheng Shen,Longjin Ran,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: TransLight是一个创新的光照效果迁移框架，通过生成式解耦技术分离图像内容和光照效果，实现跨图像的高保真、高自由度光照传输。


<details>
  <summary>Details</summary>
Motivation: 现有光照编辑方法无法同时提供定制化的光照控制并保持内容完整性，特别是在从参考图像向目标图像传输复杂光照效果方面存在不足。

Method: 使用两个微调的扩散模型进行生成式解耦，准确分离图像内容和光照效果，构建百万级图像-内容-光照三元组数据集，并采用IC-Light作为生成模型进行训练。

Result: TransLight成功实现了跨不同图像的光照效果迁移，提供了比现有技术更定制化的光照控制，在光照和谐化和编辑方面开辟了新方向。

Conclusion: 该框架通过彻底解耦参考图像中的光照效果，赋予了TransLight高度灵活的光照控制能力，为光照编辑研究提供了新的解决方案。

Abstract: Most existing illumination-editing approaches fail to simultaneously provide
customized control of light effects and preserve content integrity. This makes
them less effective for practical lighting stylization requirements, especially
in the challenging task of transferring complex light effects from a reference
image to a user-specified target image. To address this problem, we propose
TransLight, a novel framework that enables high-fidelity and high-freedom
transfer of light effects. Extracting the light effect from the reference image
is the most critical and challenging step in our method. The difficulty lies in
the complex geometric structure features embedded in light effects that are
highly coupled with content in real-world scenarios. To achieve this, we first
present Generative Decoupling, where two fine-tuned diffusion models are used
to accurately separate image content and light effects, generating a newly
curated, million-scale dataset of image-content-light triplets. Then, we employ
IC-Light as the generative model and train our model with our triplets,
injecting the reference lighting image as an additional conditioning signal.
The resulting TransLight model enables customized and natural transfer of
diverse light effects. Notably, by thoroughly disentangling light effects from
reference images, our generative decoupling strategy endows TransLight with
highly flexible illumination control. Experimental results establish TransLight
as the first method to successfully transfer light effects across disparate
images, delivering more customized illumination control than existing
techniques and charting new directions for research in illumination
harmonization and editing.

</details>


### [69] [EventSSEG: Event-driven Self-Supervised Segmentation with Probabilistic Attention](https://arxiv.org/abs/2508.14856)
*Lakshmi Annamalai,Chetan Singh Thakur*

Main category: cs.CV

TL;DR: EventSSEG是一种基于事件相机的道路分割方法，采用事件专用计算和概率注意力机制，通过事件自监督学习解决标注数据稀缺问题，在DSEC-Semantic和DDD17数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统帧相机在道路分割中存在延迟高和计算量大的问题，事件相机提供了低功耗感知的替代方案，但面临预训练权重迁移困难和标注数据稀缺的挑战。

Method: 提出EventSSEG方法，使用事件专用计算和概率注意力机制，采用事件自监督学习来避免对大量标注数据的依赖。

Result: 在DSEC-Semantic和DDD17数据集上的实验表明，EventSSEG仅需极少量标注事件就能达到最先进的性能水平。

Conclusion: 该方法充分发挥了事件相机的优势，有效解决了标注事件数据缺乏的问题，为自动驾驶车辆提供了低延迟、低计算量的道路分割解决方案。

Abstract: Road segmentation is pivotal for autonomous vehicles, yet achieving low
latency and low compute solutions using frame based cameras remains a
challenge. Event cameras offer a promising alternative. To leverage their low
power sensing, we introduce EventSSEG, a method for road segmentation that uses
event only computing and a probabilistic attention mechanism. Event only
computing poses a challenge in transferring pretrained weights from the
conventional camera domain, requiring abundant labeled data, which is scarce.
To overcome this, EventSSEG employs event-based self supervised learning,
eliminating the need for extensive labeled data. Experiments on DSEC-Semantic
and DDD17 show that EventSSEG achieves state of the art performance with
minimal labeled events. This approach maximizes event cameras capabilities and
addresses the lack of labeled events.

</details>


### [70] [Lifespan Pancreas Morphology for Control vs Type 2 Diabetes using AI on Largescale Clinical Imaging](https://arxiv.org/abs/2508.14878)
*Lucas W. Remedios,Chloe Cho,Trent M. Schwartz,Dingjie Su,Gaurav Rudravaram,Chenyu Gao,Aravind R. Krishnan,Adam M. Saunders,Michael E. Kim,Shunxing Bao,Thomas A. Lasko,Alvin C. Powers,Bennett A. Landman,John Virostko*

Main category: cs.CV

TL;DR: 本研究通过AI分析2533名患者的CT/MRI影像，建立了0-90岁胰腺形态的年龄变化趋势参考标准，发现2型糖尿病患者在10/13个形态特征上与正常人群存在显著差异，证实了2型糖尿病患者的胰腺更小。


<details>
  <summary>Details</summary>
Motivation: 了解胰腺随年龄变化的正常模式对于检测2型糖尿病和其他胰腺疾病的异常变化至关重要，需要建立可靠的临床影像测量方法和规范的年龄变化趋势。

Method: 分析2533例腹部CT/MRI影像，采用3mm等向分辨率重采样，自动化分割胰腺并提取13个形态特征。使用GAMLSS回归模型比较1350名年龄、性别匹配的2型糖尿病患者与对照组的形态差异。

Result: 调整混杂因素后，13个形态特征中有10个在2型糖尿病患者与对照组之间存在显著差异(p<0.05)。MRI与CT的测量结果存在差异。

Conclusion: 建立了大规模非糖尿病对照组的胰腺形态寿命参考标准，证实2型糖尿病患者的胰腺尺寸和形态发生改变，特别是胰腺更小，为临床诊断提供了重要参考。

Abstract: Purpose: Understanding how the pancreas changes is critical for detecting
deviations in type 2 diabetes and other pancreatic disease. We measure pancreas
size and shape using morphological measurements from ages 0 to 90. Our goals
are to 1) identify reliable clinical imaging modalities for AI-based pancreas
measurement, 2) establish normative morphological aging trends, and 3) detect
potential deviations in type 2 diabetes.
  Approach: We analyzed a clinically acquired dataset of 2533 patients imaged
with abdominal CT or MRI. We resampled the scans to 3mm isotropic resolution,
segmented the pancreas using automated methods, and extracted 13 morphological
pancreas features across the lifespan. First, we assessed CT and MRI
measurements to determine which modalities provide consistent lifespan trends.
Second, we characterized distributions of normative morphological patterns
stratified by age group and sex. Third, we used GAMLSS regression to model
pancreas morphology trends in 1350 patients matched for age, sex, and type 2
diabetes status to identify any deviations from normative aging associated with
type 2 diabetes.
  Results: When adjusting for confounders, the aging trends for 10 of 13
morphological features were significantly different between patients with type
2 diabetes and non-diabetic controls (p < 0.05 after multiple comparisons
corrections). Additionally, MRI appeared to yield different pancreas
measurements than CT using our AI-based method.
  Conclusions: We provide lifespan trends demonstrating that the size and shape
of the pancreas is altered in type 2 diabetes using 675 control patients and
675 diabetes patients. Moreover, our findings reinforce that the pancreas is
smaller in type 2 diabetes. Additionally, we contribute a reference of lifespan
pancreas morphology from a large cohort of non-diabetic control patients in a
clinical setting.

</details>


### [71] [MS-CLR: Multi-Skeleton Contrastive Learning for Human Action Recognition](https://arxiv.org/abs/2508.14889)
*Mert Kiray,Alvaro Ritter,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: 提出了多骨架对比学习框架MS-CLR，通过跨骨架对齐学习结构不变性，在NTU RGB+D数据集上取得了新的SOTA结果


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法依赖单一骨架结构，限制了在不同关节结构和解剖覆盖数据集上的泛化能力

Method: MS-CLR框架从同一序列中提取多个骨架结构，通过对比学习对齐不同骨架表示，并改进ST-GCN架构以处理不同关节布局和尺度

Result: 在NTU RGB+D 60和120数据集上持续超越单骨架对比学习方法，多骨架集成进一步提升了性能

Conclusion: 多骨架对比学习能够学习更表达性和泛化性强的特征，为骨架动作识别提供了有效的自监督学习框架

Abstract: Contrastive learning has gained significant attention in skeleton-based
action recognition for its ability to learn robust representations from
unlabeled data. However, existing methods rely on a single skeleton convention,
which limits their ability to generalize across datasets with diverse joint
structures and anatomical coverage. We propose Multi-Skeleton Contrastive
Learning (MS-CLR), a general self-supervised framework that aligns pose
representations across multiple skeleton conventions extracted from the same
sequence. This encourages the model to learn structural invariances and capture
diverse anatomical cues, resulting in more expressive and generalizable
features. To support this, we adapt the ST-GCN architecture to handle skeletons
with varying joint layouts and scales through a unified representation scheme.
Experiments on the NTU RGB+D 60 and 120 datasets demonstrate that MS-CLR
consistently improves performance over strong single-skeleton contrastive
learning baselines. A multi-skeleton ensemble further boosts performance,
setting new state-of-the-art results on both datasets.

</details>


### [72] [GaussianArt: Unified Modeling of Geometry and Motion for Articulated Objects](https://arxiv.org/abs/2508.14891)
*Licheng Shen,Saining Zhang,Honghan Li,Peilin Yang,Zihao Huang,Zongzheng Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: 提出了一种使用铰接3D高斯联合建模几何和运动的统一表示方法，显著提升了多部件铰接物体的重建能力，支持多达20个部件的复杂物体，并建立了包含90个铰接物体的新基准MPArt-90。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将几何和运动解耦，先重建不同状态下的物体形状，再通过后处理对齐估计铰接关系，这种分离使重建流程复杂且限制了可扩展性，特别是对于具有复杂多部件铰接的物体。

Method: 引入铰接3D高斯统一表示，联合建模几何和运动，改进了运动分解的鲁棒性，支持多达20个部件的铰接物体。

Result: 在广泛的物体类型上始终实现部件级几何重建和运动估计的优越准确性，显著优于先前方法（先前方法通常只能处理2-3个部件）。

Conclusion: 该方法展示了统一铰接表示在可扩展物理建模中的潜力，适用于机器人仿真和人机交互建模等下游任务。

Abstract: Reconstructing articulated objects is essential for building digital twins of
interactive environments. However, prior methods typically decouple geometry
and motion by first reconstructing object shape in distinct states and then
estimating articulation through post-hoc alignment. This separation complicates
the reconstruction pipeline and restricts scalability, especially for objects
with complex, multi-part articulation. We introduce a unified representation
that jointly models geometry and motion using articulated 3D Gaussians. This
formulation improves robustness in motion decomposition and supports
articulated objects with up to 20 parts, significantly outperforming prior
approaches that often struggle beyond 2--3 parts due to brittle initialization.
To systematically assess scalability and generalization, we propose MPArt-90, a
new benchmark consisting of 90 articulated objects across 20 categories, each
with diverse part counts and motion configurations. Extensive experiments show
that our method consistently achieves superior accuracy in part-level geometry
reconstruction and motion estimation across a broad range of object types. We
further demonstrate applicability to downstream tasks such as robotic
simulation and human-scene interaction modeling, highlighting the potential of
unified articulated representations in scalable physical modeling.

</details>


### [73] [Virtual Community: An Open World for Humans, Robots, and Society](https://arxiv.org/abs/2508.14893)
*Qinhong Zhou,Hongxin Zhang,Xiangye Lin,Zheyuan Zhang,Yutian Chen,Wenjun Liu,Zunzhe Zhang,Sunli Chen,Lixing Fang,Qiushi Lyu,Xinyu Sun,Jincheng Yang,Zeyuan Wang,Bao Chi Dang,Zhehuan Chen,Daksha Ladia,Jiageng Liu,Chuang Gan*

Main category: cs.CV

TL;DR: Virtual Community是一个基于物理引擎的开放世界平台，用于研究人类与机器人在真实3D场景中的社会共存，提出了社区规划和社区机器人两大挑战任务。


<details>
  <summary>Details</summary>
Motivation: 随着AI和机器人技术的快速发展，人类与机器人将在共享社区中共存，这既带来机遇也带来挑战。需要研究具身社会智能、人机协作与竞争、以及开放世界中的人机共存问题。

Method: 构建了开源的多人物理模拟器，支持机器人和人类在社会中的交互；开发了大规模真实世界对齐的社区生成流水线，包括户外空间、室内场景和具有丰富特征的智能体；提出了社区规划挑战和社区机器人挑战两大任务。

Result: 评估了各种基线方法，展示了在高层开放世界任务规划和低层协作控制方面存在的挑战，验证了平台的有效性。

Conclusion: Virtual Community平台为开放世界环境中人机共存的研究提供了新的可能性，有望推动该领域的进一步发展。

Abstract: The rapid progress in AI and Robotics may lead to a profound societal
transformation, as humans and robots begin to coexist within shared
communities, introducing both opportunities and challenges. To explore this
future, we present Virtual Community-an open-world platform for humans, robots,
and society-built on a universal physics engine and grounded in real-world 3D
scenes. With Virtual Community, we aim to study embodied social intelligence at
scale: 1) How robots can intelligently cooperate or compete; 2) How humans
develop social relations and build community; 3) More importantly, how
intelligent robots and humans can co-exist in an open world. To support these,
Virtual Community features: 1) An open-source multi-agent physics simulator
that supports robots, humans, and their interactions within a society; 2) A
large-scale, real-world aligned community generation pipeline, including vast
outdoor space, diverse indoor scenes, and a community of grounded agents with
rich characters and appearances. Leveraging Virtual Community, we propose two
novel challenges. The Community Planning Challenge evaluates multi-agent
reasoning and planning ability in open-world settings, such as cooperating to
help agents with daily activities and efficiently connecting other agents. The
Community Robot Challenge requires multiple heterogeneous robots to collaborate
in solving complex open-world tasks. We evaluate various baselines on these
tasks and demonstrate the challenges in both high-level open-world task
planning and low-level cooperation controls. We hope that Virtual Community
will unlock further study of human-robot coexistence within open-world
environments.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [74] [Hallucinations in medical devices](https://arxiv.org/abs/2508.14118)
*Jason Granstedt,Prabhat Kc,Rucha Deshpande,Victor Garcia,Aldo Badano*

Main category: eess.IV

TL;DR: 本文提出了医疗设备中AI幻觉的实用定义：指那些看似合理但可能对医疗任务产生影响的错误，旨在促进跨产品领域的医疗设备评估。


<details>
  <summary>Details</summary>
Motivation: 当前医疗AI设备经常产生错误输出，但业界常将其统称为"幻觉"而缺乏明确定义，这阻碍了准确的设备评估和改进。

Method: 通过理论发展和多个医疗设备领域的实证研究，提出了一个通用的幻觉定义，并使用影像和非影像应用案例进行验证。

Result: 建立了医疗AI幻觉的明确定义框架，将幻觉分类为看似合理且可能对任务产生影响或无害的错误类型。

Conclusion: 提出的定义有助于标准化医疗设备幻觉评估，并讨论了现有减少幻觉发生率的方法，为改进医疗AI系统提供了理论基础。

Abstract: Computer methods in medical devices are frequently imperfect and are known to
produce errors in clinical or diagnostic tasks. However, when deep learning and
data-based approaches yield output that exhibit errors, the devices are
frequently said to hallucinate. Drawing from theoretical developments and
empirical studies in multiple medical device areas, we introduce a practical
and universal definition that denotes hallucinations as a type of error that is
plausible and can be either impactful or benign to the task at hand. The
definition aims at facilitating the evaluation of medical devices that suffer
from hallucinations across product areas. Using examples from imaging and
non-imaging applications, we explore how the proposed definition relates to
evaluation methodologies and discuss existing approaches for minimizing the
prevalence of hallucinations.

</details>


### [75] [3D Cardiac Anatomy Generation Using Mesh Latent Diffusion Models](https://arxiv.org/abs/2508.14122)
*Jolanta Mozyrska,Marcel Beetz,Luke Melas-Kyriazi,Vicente Grau,Abhirup Banerjee,Alfonso Bueno-Orovio*

Main category: eess.IV

TL;DR: 本文提出MeshLDM，一种新颖的潜在扩散模型架构，用于生成3D心脏解剖网格，特别针对急性心肌梗死患者的左心室形状，在临床和重建指标上表现优异。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在3D医学成像特别是心脏病学中的应用仍然稀缺，而生成多样化真实的心脏解剖结构对于计算机模拟试验、电机械模拟和机器学习数据增强等应用至关重要。

Method: 提出MeshLDM架构，这是一种基于潜在扩散模型的方法，专门设计用于生成3D心脏解剖网格，并在急性心肌梗死患者的左心室网格数据集上进行训练和评估。

Result: MeshLDM成功捕捉了舒张末期和收缩末期心脏相位的形状特征，生成的网格与金标准相比，在群体均值上仅有2.4%的差异，在定性和定量临床指标上都表现出色。

Conclusion: MeshLDM证明了扩散模型在3D心脏解剖生成中的有效性，为心脏病学领域的计算机模拟和数据增强应用提供了有前景的工具。

Abstract: Diffusion models have recently gained immense interest for their generative
capabilities, specifically the high quality and diversity of the synthesized
data. However, examples of their applications in 3D medical imaging are still
scarce, especially in cardiology. Generating diverse realistic cardiac
anatomies is crucial for applications such as in silico trials,
electromechanical computer simulations, or data augmentations for machine
learning models. In this work, we investigate the application of Latent
Diffusion Models (LDMs) for generating 3D meshes of human cardiac anatomies. To
this end, we propose a novel LDM architecture -- MeshLDM. We apply the proposed
model on a dataset of 3D meshes of left ventricular cardiac anatomies from
patients with acute myocardial infarction and evaluate its performance in terms
of both qualitative and quantitative clinical and 3D mesh reconstruction
metrics. The proposed MeshLDM successfully captures characteristics of the
cardiac shapes at end-diastolic (relaxation) and end-systolic (contraction)
cardiac phases, generating meshes with a 2.4% difference in population mean
compared to the gold standard.

</details>


### [76] [Fracture Detection and Localisation in Wrist and Hand Radiographs using Detection Transformer Variants](https://arxiv.org/abs/2508.14129)
*Aditya Bagri,Vasanthakumar Venugopal,Anandakumar D,Revathi Ezhumalai,Kalyan Sivasailam,Bargava Subramanian,VarshiniPriya,Meenakumari K S,Abi M,Renita S*

Main category: eess.IV

TL;DR: 本研究应用Co-DETR目标检测变换器模型，在26,000多张腕部和手部X光片上实现骨折检测，达到83.1%准确率和96.4%召回率，展示了在急诊放射学中的临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 腕部和手部骨折的X光片诊断在急诊护理中至关重要，但人工判读速度慢且易出错。基于变换器的模型在医学图像分析中显示出潜力，但在肢体骨折应用方面有限，本研究旨在填补这一空白。

Method: 使用在COCO上预训练的RT-DETR和Co-DETR模型，在26,000多张标注X光片上进行微调。采用ResNet-50分类器对裁剪区域进行异常分类精炼，使用监督对比学习提升嵌入质量。使用AP@50、精确率和召回率等指标评估性能。

Result: RT-DETR表现中等（AP@50=0.39），而Co-DETR表现更优，AP@50达到0.615且收敛更快。集成管道在真实X光片上达到83.1%准确率、85.1%精确率和96.4%召回率，在13种骨折类型上展现出强大的泛化能力。

Conclusion: 基于Co-DETR的管道在腕部和手部骨折检测中表现出高准确性和临床相关性，提供可靠的骨折定位和类型区分。该系统可扩展、高效，适合医院工作流程的实时部署，可提高肌肉骨骼放射学的诊断速度和可靠性。

Abstract: Background: Accurate diagnosis of wrist and hand fractures using radiographs
is essential in emergency care, but manual interpretation is slow and prone to
errors. Transformer-based models show promise in improving medical image
analysis, but their application to extremity fractures is limited. This study
addresses this gap by applying object detection transformers to wrist and hand
X-rays.
  Methods: We fine-tuned the RT-DETR and Co-DETR models, pre-trained on COCO,
using over 26,000 annotated X-rays from a proprietary clinical dataset. Each
image was labeled for fracture presence with bounding boxes. A ResNet-50
classifier was trained on cropped regions to refine abnormality classification.
Supervised contrastive learning was used to enhance embedding quality.
Performance was evaluated using AP@50, precision, and recall metrics, with
additional testing on real-world X-rays.
  Results: RT-DETR showed moderate results (AP@50 = 0.39), while Co-DETR
outperformed it with an AP@50 of 0.615 and faster convergence. The integrated
pipeline achieved 83.1% accuracy, 85.1% precision, and 96.4% recall on
real-world X-rays, demonstrating strong generalization across 13 fracture
types. Visual inspection confirmed accurate localization.
  Conclusion: Our Co-DETR-based pipeline demonstrated high accuracy and
clinical relevance in wrist and hand fracture detection, offering reliable
localization and differentiation of fracture types. It is scalable, efficient,
and suitable for real-time deployment in hospital workflows, improving
diagnostic speed and reliability in musculoskeletal radiology.

</details>


### [77] [Automated surgical planning with nnU-Net: delineation of the anatomy in hepatobiliary phase MRI](https://arxiv.org/abs/2508.14133)
*Karin A. Olthof,Matteo Fusagli,Bianca Güttner,Tiziano Natali,Bram Westerink,Stefanie Speidel,Theo J. M. Ruers,Koert F. D. Kuhlmann,Andrey Zhylka*

Main category: eess.IV

TL;DR: 基于nnU-Net的深度学习模型实现了肝脏解剖结构的自动分割，包括肝实质、肿瘤、门静脉、肝静脉和胆道系统，在术前规划中表现出良好的准确性和临床实用性。


<details>
  <summary>Details</summary>
Motivation: 开发自动化肝脏解剖分割方法以简化术前规划临床工作流程，提高肝脏手术的精准性和效率。

Method: 使用90例患者的肝胆期MRI数据进行手动分割，采用nnU-Net v1网络在72例患者数据上训练，特别关注薄壁结构和拓扑结构保持，在18例测试集上评估性能。

Result: 测试集DSC评分：肝实质0.97、肝静脉0.80、胆道0.79、肿瘤0.77、门静脉0.74；肿瘤检测率76.6%，临床评估显示3D模型仅需微小调整即可使用，并在前瞻性使用中发现3个放射科医师漏诊的肿瘤。

Conclusion: 该方法能够准确自动地描绘肝脏解剖结构，使3D规划能够高效地作为肝脏手术患者的标准护理方案应用。

Abstract: Background: The aim of this study was to develop and evaluate a deep
learning-based automated segmentation method for hepatic anatomy (i.e.,
parenchyma, tumors, portal vein, hepatic vein and biliary tree) from the
hepatobiliary phase of gadoxetic acid-enhanced MRI. This method should ease the
clinical workflow of preoperative planning.
  Methods: Manual segmentation was performed on hepatobiliary phase MRI scans
from 90 consecutive patients who underwent liver surgery between January 2020
and October 2023. A deep learning network (nnU-Net v1) was trained on 72
patients with an extra focus on thin structures and topography preservation.
Performance was evaluated on an 18-patient test set by comparing automated and
manual segmentations using Dice similarity coefficient (DSC). Following
clinical integration, 10 segmentations (assessment dataset) were generated
using the network and manually refined for clinical use to quantify required
adjustments using DSC.
  Results: In the test set, DSCs were 0.97+/-0.01 for liver parenchyma,
0.80+/-0.04 for hepatic vein, 0.79+/-0.07 for biliary tree, 0.77+/-0.17 for
tumors, and 0.74+/-0.06 for portal vein. Average tumor detection rate was
76.6+/-24.1%, with a median of one false-positive per patient. The assessment
dataset showed minor adjustments were required for clinical use of the 3D
models, with high DSCs for parenchyma (1.00+/-0.00), portal vein (0.98+/-0.01)
and hepatic vein (0.95+/-0.07). Tumor segmentation exhibited greater
variability (DSC 0.80+/-0.27). During prospective clinical use, the model
detected three additional tumors initially missed by radiologists.
  Conclusions: The proposed nnU-Net-based segmentation method enables accurate
and automated delineation of hepatic anatomy. This enables 3D planning to be
applied efficiently as a standard-of-care for every patient undergoing liver
surgery.

</details>


### [78] [A Systematic Study of Deep Learning Models and xAI Methods for Region-of-Interest Detection in MRI Scans](https://arxiv.org/abs/2508.14151)
*Justin Yiu,Kushank Arora,Daniel Steinberg,Rohit Ghiya*

Main category: eess.IV

TL;DR: 这篇论文系统评估了多种深度学习模型在膝盖MRI自动ROI检测中的性能，发现ResNet50在分类和ROI识别中表现最佳，超越了变换器模型，Grad-CAM提供了最临床可解释的结果。


<details>
  <summary>Details</summary>
Motivation: 手工解释MRI分析耗时且存在观察者间差异，需要自动化检测方法来提高效率和一致性。

Method: 采用ResNet50、InceptionV3、Vision Transformers和U-Net变种等深度学习模型，结合Grad-CAM和Saliency Maps等xAI技术，使用AUC、PSNR/SSIM指标评估性能。

Result: ResNet50在分类和ROI识别中表现最优，超过变换器模型；Grad-CAM提供最临床可解释的结果；U-Net+MLP混合方法在重建质量和可解释性方面显示潜力。

Conclusion: CNN基于过传学习是该数据集上最有效的方法，未来需要更大规模预训练来充分发挥变换器模型的潜力。

Abstract: Magnetic Resonance Imaging (MRI) is an essential diagnostic tool for
assessing knee injuries. However, manual interpretation of MRI slices remains
time-consuming and prone to inter-observer variability. This study presents a
systematic evaluation of various deep learning architectures combined with
explainable AI (xAI) techniques for automated region of interest (ROI)
detection in knee MRI scans. We investigate both supervised and self-supervised
approaches, including ResNet50, InceptionV3, Vision Transformers (ViT), and
multiple U-Net variants augmented with multi-layer perceptron (MLP)
classifiers. To enhance interpretability and clinical relevance, we integrate
xAI methods such as Grad-CAM and Saliency Maps. Model performance is assessed
using AUC for classification and PSNR/SSIM for reconstruction quality, along
with qualitative ROI visualizations. Our results demonstrate that ResNet50
consistently excels in classification and ROI identification, outperforming
transformer-based models under the constraints of the MRNet dataset. While
hybrid U-Net + MLP approaches show potential for leveraging spatial features in
reconstruction and interpretability, their classification performance remains
lower. Grad-CAM consistently provided the most clinically meaningful
explanations across architectures. Overall, CNN-based transfer learning emerges
as the most effective approach for this dataset, while future work with
larger-scale pretraining may better unlock the potential of transformer models.

</details>


### [79] [Fine-grained Image Quality Assessment for Perceptual Image Restoration](https://arxiv.org/abs/2508.14475)
*Xiangfei Sheng,Xiaofeng Pan,Zhichao Yang,Pengfei Chen,Leida Li*

Main category: eess.IV

TL;DR: 该论文提出了首个针对图像恢复任务的细粒度图像质量评估数据集FGRestore，并基于此开发了FGResQ模型，该模型在粗粒度评分回归和细粒度质量排序方面表现优异，显著超越了现有IQA指标。


<details>
  <summary>Details</summary>
Motivation: 现有图像质量评估(IQA)指标在图像恢复(IR)任务中存在固有弱点，特别是在区分恢复图像之间的细粒度质量差异方面表现不佳，这促使研究者开发更精确的评估方法。

Method: 构建了包含18,408张恢复图像和30,886对细粒度偏好标注的FGRestore数据集；提出了FGResQ模型，结合粗粒度评分回归和细粒度质量排序；在六个常见IR任务上进行了全面基准测试。

Result: 实验表明现有IQA指标在评分评估与细粒度恢复质量之间存在显著不一致；FGResQ模型在广泛实验和比较中显著优于最先进的IQA指标。

Conclusion: FGRestore数据集和FGResQ模型为图像恢复任务提供了更准确的细粒度质量评估解决方案，解决了现有IQA指标在此领域的局限性。

Abstract: Recent years have witnessed remarkable achievements in perceptual image
restoration (IR), creating an urgent demand for accurate image quality
assessment (IQA), which is essential for both performance comparison and
algorithm optimization. Unfortunately, the existing IQA metrics exhibit
inherent weakness for IR task, particularly when distinguishing fine-grained
quality differences among restored images. To address this dilemma, we
contribute the first-of-its-kind fine-grained image quality assessment dataset
for image restoration, termed FGRestore, comprising 18,408 restored images
across six common IR tasks. Beyond conventional scalar quality scores,
FGRestore was also annotated with 30,886 fine-grained pairwise preferences.
Based on FGRestore, a comprehensive benchmark was conducted on the existing IQA
metrics, which reveal significant inconsistencies between score-based IQA
evaluations and the fine-grained restoration quality. Motivated by these
findings, we further propose FGResQ, a new IQA model specifically designed for
image restoration, which features both coarse-grained score regression and
fine-grained quality ranking. Extensive experiments and comparisons demonstrate
that FGResQ significantly outperforms state-of-the-art IQA metrics. Codes and
model weights have been released in https://pxf0429.github.io/FGResQ/

</details>


### [80] [Deep Skin Lesion Segmentation with Transformer-CNN Fusion: Toward Intelligent Skin Cancer Analysis](https://arxiv.org/abs/2508.14509)
*Xin Wang,Xiaopei Zhang,Xingang Wang*

Main category: eess.IV

TL;DR: 提出基于改进TransUNet的高精度皮肤病变语义分割方法，通过结合Transformer和CNN模块，增强对复杂病变结构和模糊边界的处理能力，在多个指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决皮肤病变图像中复杂病变结构、模糊边界和显著尺度变化等挑战，提高自动化分割任务的准确性和鲁棒性。

Method: 在传统编码器-解码器框架中集成Transformer模块建模全局语义信息，保留卷积分支保持局部纹理特征，设计边界引导注意力机制和多尺度上采样路径。

Result: 在mIoU、mDice和mAcc指标上优于现有代表性方法，在复杂场景下实现更好的边界重建和结构恢复。

Conclusion: 该方法适用于皮肤病变分析中的自动化分割任务关键需求，具有较强的病变识别准确性和鲁棒性。

Abstract: This paper proposes a high-precision semantic segmentation method based on an
improved TransUNet architecture to address the challenges of complex lesion
structures, blurred boundaries, and significant scale variations in skin lesion
images. The method integrates a transformer module into the traditional
encoder-decoder framework to model global semantic information, while retaining
a convolutional branch to preserve local texture and edge features. This
enhances the model's ability to perceive fine-grained structures. A
boundary-guided attention mechanism and multi-scale upsampling path are also
designed to improve lesion boundary localization and segmentation consistency.
To verify the effectiveness of the approach, a series of experiments were
conducted, including comparative studies, hyperparameter sensitivity analysis,
data augmentation effects, input resolution variation, and training data split
ratio tests. Experimental results show that the proposed model outperforms
existing representative methods in mIoU, mDice, and mAcc, demonstrating
stronger lesion recognition accuracy and robustness. In particular, the model
achieves better boundary reconstruction and structural recovery in complex
scenarios, making it well-suited for the key demands of automated segmentation
tasks in skin lesion analysis.

</details>


### [81] [From Slices to Structures: Unsupervised 3D Reconstruction of Female Pelvic Anatomy from Freehand Transvaginal Ultrasound](https://arxiv.org/abs/2508.14552)
*Max Krähenmann,Sergio Tascon-Morales,Fabian Laumer,Julia E. Vogt,Ece Ozkan*

Main category: eess.IV

TL;DR: 无监督方法从自由手2D式阴道超声扫描重建3D解剖结构，不需外部跟踪或位置估计器


<details>
  <summary>Details</summary>
Motivation: 体积超声能提高诊断准确性，但应用受限于专门硬件和严格的采集协议，需要一种可扩展的方案

Method: 将高斯拉斑原理调整到超声领域，使用切片感知的可微分柱化器，将解剖结构模型化为各异性高斯分布，从图像级监督优化参数

Result: 实现了紧凑、灵活、内存效率高的体积表示，以高空间保真度捕获解剖细节

Conclusion: 仅通过计算方法即可从2D超声图像实现准确的3D重建，为传统3D系统提供可扩展替代方案，开启AI辅助分析和诊断的新机遇

Abstract: Volumetric ultrasound has the potential to significantly improve diagnostic
accuracy and clinical decision-making, yet its widespread adoption remains
limited by dependence on specialized hardware and restrictive acquisition
protocols. In this work, we present a novel unsupervised framework for
reconstructing 3D anatomical structures from freehand 2D transvaginal
ultrasound (TVS) sweeps, without requiring external tracking or learned pose
estimators. Our method adapts the principles of Gaussian Splatting to the
domain of ultrasound, introducing a slice-aware, differentiable rasterizer
tailored to the unique physics and geometry of ultrasound imaging. We model
anatomy as a collection of anisotropic 3D Gaussians and optimize their
parameters directly from image-level supervision, leveraging sensorless probe
motion estimation and domain-specific geometric priors. The result is a
compact, flexible, and memory-efficient volumetric representation that captures
anatomical detail with high spatial fidelity. This work demonstrates that
accurate 3D reconstruction from 2D ultrasound images can be achieved through
purely computational means, offering a scalable alternative to conventional 3D
systems and enabling new opportunities for AI-assisted analysis and diagnosis.

</details>


### [82] [Broadband Near-Infrared Compressive Spectral Imaging System with Reflective Structure](https://arxiv.org/abs/2508.14573)
*Yutong Li,Zhenming Yu,Liming Cheng,Jiayu Di,Liang Lin,Jingyue Ma,Tongshuo Zhang,Yue Zhou,Haiying Zhao,Kun Xu*

Main category: eess.IV

TL;DR: 提出了一种宽带近红外压缩光谱成像系统，能够在700-1600nm范围内捕获高光谱数据，解决了传统系统成本高、体积大和数据采集效率低的问题


<details>
  <summary>Details</summary>
Motivation: 传统近红外高光谱成像系统面临成本高、仪器笨重和数据采集效率低的挑战，需要开发更紧凑、高效的解决方案

Method: 通过波长分段和设计专用光学元件来克服硬件光谱限制，采用反射式光学结构使系统更加紧凑

Result: 成功开发出能够捕获700-1600nm宽带光谱数据的压缩光谱成像系统

Conclusion: 该方法为近红外高光谱成像提供了一种新颖的技术解决方案，具有宽带覆盖和紧凑结构的优势

Abstract: Near-infrared (NIR) hyperspectral imaging has become a critical tool in
modern analytical science. However, conventional NIR hyperspectral imaging
systems face challenges including high cost, bulky instrumentation, and
inefficient data collection. In this work, we demonstrate a broadband NIR
compressive spectral imaging system that is capable of capturing hyperspectral
data covering a broad spectral bandwidth ranging from 700 to 1600 nm. By
segmenting wavelengths and designing specialized optical components, our design
overcomes hardware spectral limitations to capture broadband data, while the
reflective optical structure makes the system compact. This approach provides a
novel technical solution for NIR hyperspectral imaging.

</details>


### [83] [Integrated Snapshot Near-infrared Hypersepctral Imaging Framework with Diffractive Optics](https://arxiv.org/abs/2508.14585)
*Jingyue Ma,Zhenming Yu,Zhengyang Li,Liang Lin,Liming Cheng,Kun Xu*

Main category: eess.IV

TL;DR: 提出了一种结合设计衍射光学元件(DOE)和NIRSA-Net的集成式快照近红外高光谱成像框架，在700-1000nm波段实现10nm分辨率，PSNR提升1.47dB，SSIM提升0.006


<details>
  <summary>Details</summary>
Motivation: 开发一种高效的近红外高光谱成像系统，通过集成光学设计和深度学习网络来提升成像质量和光谱分辨率

Method: 结合设计的衍射光学元件(DOE)与NIRSA-Net深度学习网络，构建集成式快照近红外高光谱成像框架

Result: 在700-1000nm波段实现了10nm的光谱分辨率，PSNR指标提升1.47dB，SSIM指标提升0.006

Conclusion: 该方法成功实现了高质量的快照近红外高光谱成像，在保持高光谱分辨率的同时显著提升了图像质量指标

Abstract: We propose an integrated snapshot near-infrared hyperspectral imaging
framework that combines designed DOE with NIRSA-Net. The results demonstrate
near-infrared spectral imaging at 700-1000nm with 10nm resolution while
achieving improvement of PSNR 1.47dB and SSIM 0.006.

</details>


### [84] [Virtual Multiplex Staining for Histological Images using a Marker-wise Conditioned Diffusion Model](https://arxiv.org/abs/2508.14681)
*Hyun-Jic Oh,Junsik Kim,Zhiyi Shi,Yichen Wu,Yu-An Chen,Peter K. Sorger,Hanspeter Pfister,Won-Ki Jeong*

Main category: eess.IV

TL;DR: 基于潜在液化模型的虚拟多重染色框架，能够从H&E图像生成至18种标记物的多重图像，提高了生成精度和速度


<details>
  <summary>Details</summary>
Motivation: 多重成像技术虽能提供分子级视觉化，但复杂性和成本高，而现有大量H&E图像库缺乏对应的多重图像，限制了多模态分析的发展

Method: 利用预训练潜在液化模型参数，构建条件液化模型，通过标记物条件化实现标记物逐个生成，采用单步金石采样提高速度，通过像素级损失函数改善颜色对比保真度

Result: 在两个公开数据集上验证，能够生成至18种不同标记物类型，精度显著提高，远超之前方法的2-3种标记物生成能力

Conclusion: 该框架成功桥接了H&E和多重成像之间的差距，为对现有H&E图像库进行回顾性研究和大规模分析提供了可能，开启了虚拟多重染色的新方向

Abstract: Multiplex imaging is revolutionizing pathology by enabling the simultaneous
visualization of multiple biomarkers within tissue samples, providing
molecular-level insights that traditional hematoxylin and eosin (H&E) staining
cannot provide. However, the complexity and cost of multiplex data acquisition
have hindered its widespread adoption. Additionally, most existing large
repositories of H&E images lack corresponding multiplex images, limiting
opportunities for multimodal analysis. To address these challenges, we leverage
recent advances in latent diffusion models (LDMs), which excel at modeling
complex data distributions utilizing their powerful priors for fine-tuning to a
target domain. In this paper, we introduce a novel framework for virtual
multiplex staining that utilizes pretrained LDM parameters to generate
multiplex images from H&E images using a conditional diffusion model. Our
approach enables marker-by-marker generation by conditioning the diffusion
model on each marker, while sharing the same architecture across all markers.
To tackle the challenge of varying pixel value distributions across different
marker stains and to improve inference speed, we fine-tune the model for
single-step sampling, enhancing both color contrast fidelity and inference
efficiency through pixel-level loss functions. We validate our framework on two
publicly available datasets, notably demonstrating its effectiveness in
generating up to 18 different marker types with improved accuracy, a
substantial increase over the 2-3 marker types achieved in previous approaches.
This validation highlights the potential of our framework, pioneering virtual
multiplex staining. Finally, this paper bridges the gap between H&E and
multiplex imaging, potentially enabling retrospective studies and large-scale
analyses of existing H&E image repositories.

</details>


### [85] [Rule-based Key-Point Extraction for MR-Guided Biomechanical Digital Twins of the Spine](https://arxiv.org/abs/2508.14708)
*Robert Graf,Tanja Lerchl,Kati Nispel,Hendrik Möller,Matan Atad,Julian McGinnis,Julius Maria Watrinet,Johannes Paetzold,Daniel Rueckert,Jan S. Kirschke*

Main category: eess.IV

TL;DR: 提出基于规则的MRI亚像素级关键点提取方法，用于构建脊柱生物力学数字孪生模型，实现无辐射的个性化解剖建模和临床决策支持


<details>
  <summary>Details</summary>
Motivation: 数字孪生需要精确的个体化解剖建模，现有方法主要基于CT，存在辐射问题且不适合大规模研究和特定人群使用

Method: 采用基于规则的方法，结合稳健的图像配准和椎骨特异性方向估计，从MRI中提取亚像素级精度的解剖标志点

Result: 能够生成解剖学意义明确的地标点，作为生物力学模型的边界条件和力作用点，支持考虑个体解剖结构的脊柱力学模拟

Conclusion: 该方法填补了精确医学图像分析与生物力学模拟之间的空白，为个性化医疗建模提供了无辐射的解决方案，适合大规模临床研究和特殊人群应用

Abstract: Digital twins offer a powerful framework for subject-specific simulation and
clinical decision support, yet their development often hinges on accurate,
individualized anatomical modeling. In this work, we present a rule-based
approach for subpixel-accurate key-point extraction from MRI, adapted from
prior CT-based methods. Our approach incorporates robust image alignment and
vertebra-specific orientation estimation to generate anatomically meaningful
landmarks that serve as boundary conditions and force application points, like
muscle and ligament insertions in biomechanical models. These models enable the
simulation of spinal mechanics considering the subject's individual anatomy,
and thus support the development of tailored approaches in clinical diagnostics
and treatment planning. By leveraging MR imaging, our method is radiation-free
and well-suited for large-scale studies and use in underrepresented
populations. This work contributes to the digital twin ecosystem by bridging
the gap between precise medical image analysis with biomechanical simulation,
and aligns with key themes in personalized modeling for healthcare.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [86] [A Real-world Display Inverse Rendering Dataset](https://arxiv.org/abs/2508.14411)
*Seokjun Choi,Hoon-Gyu Chung,Yujin Jeon,Giljoo Nam,Seung-Hwan Baek*

Main category: cs.GR

TL;DR: 首个基于显示器-相机系统的实际逆向渲染数据集，包含OLAT照明模式和高质量真实地形数据


<details>
  <summary>Details</summary>
Motivation: 显示器-相机系统具有编程化点光源和偏振光分离优势，但缺乏公开实际数据集阻碍了相关方法的发展和评估

Method: 构建和标定LCD显示器与双目偏振相机系统，采集多样化对象的OLAT图像，提供高质量地形真值

Result: 数据集支持任意显示模式和噪声水平的图像合成，建立的基准方法超越了现有最佳逆向渲染方法

Conclusion: 该数据集为显示基础逆向渲染研究提供了重要支撑，提出的简单有效基准方法显示了良好性能

Abstract: Inverse rendering aims to reconstruct geometry and reflectance from captured
images. Display-camera imaging systems offer unique advantages for this task:
each pixel can easily function as a programmable point light source, and the
polarized light emitted by LCD displays facilitates diffuse-specular
separation. Despite these benefits, there is currently no public real-world
dataset captured using display-camera systems, unlike other setups such as
light stages. This absence hinders the development and evaluation of
display-based inverse rendering methods. In this paper, we introduce the first
real-world dataset for display-based inverse rendering. To achieve this, we
construct and calibrate an imaging system comprising an LCD display and stereo
polarization cameras. We then capture a diverse set of objects with diverse
geometry and reflectance under one-light-at-a-time (OLAT) display patterns. We
also provide high-quality ground-truth geometry. Our dataset enables the
synthesis of captured images under arbitrary display patterns and different
noise levels. Using this dataset, we evaluate the performance of existing
photometric stereo and inverse rendering methods, and provide a simple, yet
effective baseline for display inverse rendering, outperforming
state-of-the-art inverse rendering methods. Code and dataset are available on
our project page at https://michaelcsj.github.io/DIR/

</details>


### [87] [MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds](https://arxiv.org/abs/2508.14879)
*Bingquan Dai,Li Ray Luo,Qihong Tang,Jie Wang,Xinyu Lian,Hao Xu,Minghan Qin,Xudong Xu,Bo Dai,Haoqian Wang,Zhaoyang Lyu,Jiangmiao Pang*

Main category: cs.GR

TL;DR: MeshCoder是一个将3D点云重建为可编辑Blender Python脚本的新框架，通过大规模数据集和多模态大语言模型实现复杂几何结构的程序化重建和编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖有限的领域特定语言和小规模数据集，无法有效建模复杂几何结构和拓扑关系，需要一种更灵活的程序化重建解决方案。

Method: 开发了全面的Blender Python API集合，构建大规模配对的物体-代码数据集，训练多模态大语言模型将3D点云转换为可执行的Blender脚本。

Result: 在形状到代码重建任务中取得优异性能，支持通过代码修改进行直观的几何和拓扑编辑，增强LLM在3D形状理解任务中的推理能力。

Conclusion: MeshCoder为程序化3D形状重建和理解提供了一个强大而灵活的解决方案，在逆向工程和形状编辑应用中具有重要价值。

Abstract: Reconstructing 3D objects into editable programs is pivotal for applications
like reverse engineering and shape editing. However, existing methods often
rely on limited domain-specific languages (DSLs) and small-scale datasets,
restricting their ability to model complex geometries and structures. To
address these challenges, we introduce MeshCoder, a novel framework that
reconstructs complex 3D objects from point clouds into editable Blender Python
scripts. We develop a comprehensive set of expressive Blender Python APIs
capable of synthesizing intricate geometries. Leveraging these APIs, we
construct a large-scale paired object-code dataset, where the code for each
object is decomposed into distinct semantic parts. Subsequently, we train a
multimodal large language model (LLM) that translates 3D point cloud into
executable Blender Python scripts. Our approach not only achieves superior
performance in shape-to-code reconstruction tasks but also facilitates
intuitive geometric and topological editing through convenient code
modifications. Furthermore, our code-based representation enhances the
reasoning capabilities of LLMs in 3D shape understanding tasks. Together, these
contributions establish MeshCoder as a powerful and flexible solution for
programmatic 3D shape reconstruction and understanding.

</details>


### [88] [Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds](https://arxiv.org/abs/2508.14892)
*Jia Lu,Taoran Yi,Jiemin Fang,Chen Yang,Chuiyun Wu,Wei Shen,Wenyu Liu,Qi Tian,Xinggang Wang*

Main category: cs.GR

TL;DR: 提出了一种从仅两张图像（正面和背面）重建3D人体的方法，通过几何重建模型和增强算法实现快速高质量重建


<details>
  <summary>Details</summary>
Motivation: 从稀疏视图重建3D人体是一个重要课题，本文旨在降低用户创建3D数字人的门槛，通过仅需两张图像即可完成重建

Method: 重新设计基于基础重建模型的几何重建模型来预测一致的点云，应用增强算法补充缺失的颜色信息，并将点云转换为3D高斯以实现更好的渲染质量

Result: 在单张NVIDIA RTX 4090上仅需190ms即可重建整个人体，在THuman2.0和跨域数据集上达到最先进性能，且支持低成本移动设备拍摄的图像

Conclusion: 该方法能够以极低的硬件要求和时间成本实现高质量的3D人体重建，大大降低了数据采集的门槛

Abstract: Reconstructing 3D human bodies from sparse views has been an appealing topic,
which is crucial to broader the related applications. In this paper, we propose
a quite challenging but valuable task to reconstruct the human body from only
two images, i.e., the front and back view, which can largely lower the barrier
for users to create their own 3D digital humans. The main challenges lie in the
difficulty of building 3D consistency and recovering missing information from
the highly sparse input. We redesign a geometry reconstruction model based on
foundation reconstruction models to predict consistent point clouds even input
images have scarce overlaps with extensive human data training. Furthermore, an
enhancement algorithm is applied to supplement the missing color information,
and then the complete human point clouds with colors can be obtained, which are
directly transformed into 3D Gaussians for better rendering quality.
Experiments show that our method can reconstruct the entire human in 190 ms on
a single NVIDIA RTX 4090, with two images at a resolution of 1024x1024,
demonstrating state-of-the-art performance on the THuman2.0 and cross-domain
datasets. Additionally, our method can complete human reconstruction even with
images captured by low-cost mobile devices, reducing the requirements for data
collection. Demos and code are available at
https://hustvl.github.io/Snap-Snap/.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [89] [STAS: Spatio-Temporal Adaptive Computation Time for Spiking Transformers](https://arxiv.org/abs/2508.14138)
*Donghwa Kang,Doohyun Kim,Sang-Ki Ko,Jinkyu Lee,Brent ByungHoon Kang,Hyeongboo Baek*

Main category: cs.LG

TL;DR: STAS框架通过时空自适应计算时间方法，解决了脉冲神经网络在视觉Transformer中的高延迟和计算开销问题，显著降低能耗同时提升准确率


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络(SNNs)虽然比人工神经网络(ANNs)更节能，但由于多时间步操作特性导致高延迟和计算开销。现有动态计算方法分散且不统一，自适应计算时间(ACT)原则虽然提供了统一方法的基础，但在SNN-based视觉Transformer中的应用受到时间相似性前提违反和静态架构不适配两个核心问题的阻碍

Method: 提出STAS框架，通过集成脉冲补丁分割(I-SPS)模块建立时间稳定性，创建统一输入表示来解决时间不相似性的架构问题；开发自适应脉冲自注意力(A-SSA)模块，在空间和时间两个维度上进行令牌剪枝

Result: 在CIFAR-10、CIFAR-100和ImageNet数据集上验证，STAS分别降低能耗45.9%、43.8%和30.1%，同时准确率超过最先进模型

Conclusion: STAS通过协同设计静态架构和动态计算策略，成功解决了SNN-based视觉Transformer中的关键挑战，实现了显著的能耗降低和性能提升，为脉冲神经网络的高效应用提供了统一框架

Abstract: Spiking neural networks (SNNs) offer energy efficiency over artificial neural
networks (ANNs) but suffer from high latency and computational overhead due to
their multi-timestep operational nature. While various dynamic computation
methods have been developed to mitigate this by targeting spatial, temporal, or
architecture-specific redundancies, they remain fragmented. While the
principles of adaptive computation time (ACT) offer a robust foundation for a
unified approach, its application to SNN-based vision Transformers (ViTs) is
hindered by two core issues: the violation of its temporal similarity
prerequisite and a static architecture fundamentally unsuited for its
principles. To address these challenges, we propose STAS (Spatio-Temporal
Adaptive computation time for Spiking transformers), a framework that
co-designs the static architecture and dynamic computation policy. STAS
introduces an integrated spike patch splitting (I-SPS) module to establish
temporal stability by creating a unified input representation, thereby solving
the architectural problem of temporal dissimilarity. This stability, in turn,
allows our adaptive spiking self-attention (A-SSA) module to perform
two-dimensional token pruning across both spatial and temporal axes.
Implemented on spiking Transformer architectures and validated on CIFAR-10,
CIFAR-100, and ImageNet, STAS reduces energy consumption by up to 45.9%, 43.8%,
and 30.1%, respectively, while simultaneously improving accuracy over SOTA
models.

</details>


### [90] [Organ-Agents: Virtual Human Physiology Simulator via LLMs](https://arxiv.org/abs/2508.14357)
*Rihao Chang,He Jiao,Weizhi Nie,Honglin Guo,Keliang Xie,Zhenhua Wu,Lina Zhao,Yunpeng Bai,Yongtao Ma,Lanjun Wang,Yuting Su,Xi Gao,Weijie Wang,Nicu Sebe,Bruno Lepri,Bingwei Sun*

Main category: cs.LG

TL;DR: Organ-Agents是一个基于大语言模型的多代理框架，通过9个系统代理模拟人体生理过程，在脓毒症患者数据上表现出高仿真精度和临床实用性。


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型的新进展来模拟复杂生理系统，为重症监护提供精确诊断、治疗模拟和假设测试的数字孪生工具。

Method: 采用监督微调系统特异性时间序列数据，结合强化学习的动态参考选择和错误校正进行协调训练，使用7,134名脓毒症患者和7,895名对照组数据。

Result: 在4,509名保留患者上达到高仿真精度（每系统MSE<0.16），外部验证显示分布偏移下性能稳定，15名重症医师确认真实性和生理合理性（平均评分3.9和3.7）。

Conclusion: Organ-Agents是一个可信、可解释且可推广的数字孪生系统，能够准确重现关键多系统事件，支持反事实治疗策略模拟和早期预警任务。

Abstract: Recent advances in large language models (LLMs) have enabled new
possibilities in simulating complex physiological systems. We introduce
Organ-Agents, a multi-agent framework that simulates human physiology via
LLM-driven agents. Each Simulator models a specific system (e.g.,
cardiovascular, renal, immune). Training consists of supervised fine-tuning on
system-specific time-series data, followed by reinforcement-guided coordination
using dynamic reference selection and error correction. We curated data from
7,134 sepsis patients and 7,895 controls, generating high-resolution
trajectories across 9 systems and 125 variables. Organ-Agents achieved high
simulation accuracy on 4,509 held-out patients, with per-system MSEs <0.16 and
robustness across SOFA-based severity strata. External validation on 22,689 ICU
patients from two hospitals showed moderate degradation under distribution
shifts with stable simulation. Organ-Agents faithfully reproduces critical
multi-system events (e.g., hypotension, hyperlactatemia, hypoxemia) with
coherent timing and phase progression. Evaluation by 15 critical care
physicians confirmed realism and physiological plausibility (mean Likert
ratings 3.9 and 3.7). Organ-Agents also enables counterfactual simulations
under alternative sepsis treatment strategies, generating trajectories and
APACHE II scores aligned with matched real-world patients. In downstream early
warning tasks, classifiers trained on synthetic data showed minimal AUROC drops
(<0.04), indicating preserved decision-relevant patterns. These results
position Organ-Agents as a credible, interpretable, and generalizable digital
twin for precision diagnosis, treatment simulation, and hypothesis testing in
critical care.

</details>


### [91] [Disentanglement in T-space for Faster and Distributed Training of Diffusion Models with Fewer Latent-states](https://arxiv.org/abs/2508.14413)
*Samarth Gupta,Raghudeep Gadde,Rui Chen,Aleix M. Martinez*

Main category: cs.LG

TL;DR: 该论文挑战了扩散模型需要大量潜在状态/时间步数的假设，证明通过精心选择噪声调度，只需少量甚至单个潜在状态就能达到与传统方法相当的性能，并提出解耦模型实现4-6倍的加速收敛。


<details>
  <summary>Details</summary>
Motivation: 挑战扩散模型需要大量时间步数的传统假设，探索在少量潜在状态下实现高质量生成的可能性，以提高训练效率和收敛速度。

Method: 1. 精心选择噪声调度策略；2. 在少量潜在状态（T~32）下训练扩散模型；3. 进一步推进到单个潜在状态的完全解耦；4. 组合多个独立训练的单潜在状态模型生成高质量样本。

Result: 1. 少量潜在状态（32步）模型性能与大量潜在状态（1000步）模型相当；2. 单潜在状态解耦模型能生成高质量样本；3. 解耦模型在多个指标上实现4-6倍的加速收敛；4. 在两个不同数据集上验证了方法的有效性。

Conclusion: 扩散模型并不需要大量时间步数，通过合理的噪声调度和模型解耦，可以在保持生成质量的同时显著提高训练效率，为扩散模型的实用化提供了新的思路。

Abstract: We challenge a fundamental assumption of diffusion models, namely, that a
large number of latent-states or time-steps is required for training so that
the reverse generative process is close to a Gaussian. We first show that with
careful selection of a noise schedule, diffusion models trained over a small
number of latent states (i.e. $T \sim 32$) match the performance of models
trained over a much large number of latent states ($T \sim 1,000$). Second, we
push this limit (on the minimum number of latent states required) to a single
latent-state, which we refer to as complete disentanglement in T-space. We show
that high quality samples can be easily generated by the disentangled model
obtained by combining several independently trained single latent-state models.
We provide extensive experiments to show that the proposed disentangled model
provides 4-6$\times$ faster convergence measured across a variety of metrics on
two different datasets.

</details>


### [92] [Understanding Data Influence with Differential Approximation](https://arxiv.org/abs/2508.14648)
*Haoru Tan,Sitong Wu,Xiuzhe Wu,Wang Wang,Bo Zhao,Zeke Xie,Gui-Song Xia,Xiaojuan Qi*

Main category: cs.LG

TL;DR: Diff-In是一种新的样本影响力近似方法，通过累积连续训练步骤中的影响力差异来估计样本影响力，无需模型凸性假设，计算效率高且精度优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据影响力分析工具精度不足，往往需要假设神经网络损失函数是凸的，这限制了方法的实际应用效果。

Method: 提出Diff-In方法，将样本影响力定义为连续训练迭代中影响力变化的累积和，使用二阶近似高精度估计这些差异项，同时通过计算Hessian矩阵和梯度的乘积来保持计算效率。

Result: 理论分析和大量实验证明，Diff-In相比现有影响力估计器具有显著更低的近似误差，在数据清洗、数据删除和核心集选择等任务中表现优异，能够扩展到百万级数据点。

Conclusion: Diff-In方法有效解决了现有影响力分析工具的局限性，在保持计算效率的同时提供了更高的精度，为数据中心的AI任务提供了强有力的工具。

Abstract: Data plays a pivotal role in the groundbreaking advancements in artificial
intelligence. The quantitative analysis of data significantly contributes to
model training, enhancing both the efficiency and quality of data utilization.
However, existing data analysis tools often lag in accuracy. For instance, many
of these tools even assume that the loss function of neural networks is convex.
These limitations make it challenging to implement current methods effectively.
In this paper, we introduce a new formulation to approximate a sample's
influence by accumulating the differences in influence between consecutive
learning steps, which we term Diff-In. Specifically, we formulate the
sample-wise influence as the cumulative sum of its changes/differences across
successive training iterations. By employing second-order approximations, we
approximate these difference terms with high accuracy while eliminating the
need for model convexity required by existing methods. Despite being a
second-order method, Diff-In maintains computational complexity comparable to
that of first-order methods and remains scalable. This efficiency is achieved
by computing the product of the Hessian and gradient, which can be efficiently
approximated using finite differences of first-order gradients. We assess the
approximation accuracy of Diff-In both theoretically and empirically. Our
theoretical analysis demonstrates that Diff-In achieves significantly lower
approximation error compared to existing influence estimators. Extensive
experiments further confirm its superior performance across multiple benchmark
datasets in three data-centric tasks: data cleaning, data deletion, and coreset
selection. Notably, our experiments on data pruning for large-scale
vision-language pre-training show that Diff-In can scale to millions of data
points and outperforms strong baselines.

</details>


### [93] [Squeezed Diffusion Models](https://arxiv.org/abs/2508.14871)
*Jyotirmai Singh,Samar Khanna,James Burgess*

Main category: cs.LG

TL;DR: 本文提出了挤压扩散模型(SDM)，通过数据感知的各向异性噪声缩放来改进扩散模型性能，在多个数据集上实现了FID提升15%的效果


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型使用各向同性高斯噪声，忽略了数据的结构信息。受量子压缩态根据海森堡不确定性原理重新分布不确定性的启发，希望通过对主成分方向进行各向异性噪声缩放来更好地学习重要数据特征

Method: 提出了两种配置：(1)海森堡扩散模型，在主轴上补偿缩放并在正交方向进行逆缩放；(2)标准SDM变体，仅缩放主轴。在CIFAR-10/100和CelebA-64数据集上进行实验

Result: 反直觉地发现，适度的反压缩（即增加主轴方差）能持续改善FID达15%，并将精度-召回边界推向更高召回率

Conclusion: 简单的数据感知噪声整形可以在不改变架构的情况下带来稳健的生成性能提升，证明了数据相关噪声缩放的有效性

Abstract: Diffusion models typically inject isotropic Gaussian noise, disregarding
structure in the data. Motivated by the way quantum squeezed states
redistribute uncertainty according to the Heisenberg uncertainty principle, we
introduce Squeezed Diffusion Models (SDM), which scale noise anisotropically
along the principal component of the training distribution. As squeezing
enhances the signal-to-noise ratio in physics, we hypothesize that scaling
noise in a data-dependent manner can better assist diffusion models in learning
important data features. We study two configurations: (i) a Heisenberg
diffusion model that compensates the scaling on the principal axis with inverse
scaling on orthogonal directions and (ii) a standard SDM variant that scales
only the principal axis. Counterintuitively, on CIFAR-10/100 and CelebA-64,
mild antisqueezing - i.e. increasing variance on the principal axis -
consistently improves FID by up to 15% and shifts the precision-recall frontier
toward higher recall. Our results demonstrate that simple, data-aware noise
shaping can deliver robust generative gains without architectural changes.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [94] [Activity Coefficient-based Channel Selection for Electroencephalogram: A Task-Independent Approach](https://arxiv.org/abs/2508.14060)
*Kartik Pandey,Arun Balasubramanian,Debasis Samanta*

Main category: q-bio.NC

TL;DR: 提出了一种任务无关的脑电信号通道选择方法ACCS，通过通道活动系数(CAC)量化通道效用，选择前16个通道可在多分类任务中提升34.97%的准确率。


<details>
  <summary>Details</summary>
Motivation: 高密度电极阵列带来通道间干扰和计算开销问题，现有通道选择方法通常针对特定任务，需要为每个新应用重新优化。

Method: 基于通道活动系数(CAC)的任务无关通道选择方法(ACCS)，通过量化通道活动水平来评估通道效用，选择排名前16的通道。

Result: ACCS方法在多分类任务中实现了高达34.97%的准确率提升，且选择的通道集可跨任务和模型重用。

Conclusion: ACCS提供了一种任务无关、可重用的脑电通道选择方案，适用于多样化的脑机接口应用场景。

Abstract: Electroencephalogram (EEG) signals have gained widespread adoption in
brain-computer interface (BCI) applications due to their non-invasive,
low-cost, and relatively simple acquisition process. The demand for higher
spatial resolution, particularly in clinical settings, has led to the
development of high-density electrode arrays. However, increasing the number of
channels introduces challenges such as cross-channel interference and
computational overhead. To address these issues, modern BCI systems often
employ channel selection algorithms. Existing methods, however, are typically
task-specific and require re-optimization for each new application. This work
proposes a task-agnostic channel selection method, Activity Coefficient-based
Channel Selection (ACCS), which uses a novel metric called the Channel Activity
Coefficient (CAC) to quantify channel utility based on activity levels. By
selecting the top 16 channels ranked by CAC, ACCS achieves up to 34.97%
improvement in multi-class classification accuracy. Unlike traditional
approaches, ACCS identifies a reusable set of informative channels independent
of the downstream task or model, making it highly adaptable for diverse
EEG-based applications.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [95] [From Image Captioning to Visual Storytelling](https://arxiv.org/abs/2508.14045)
*Admitos Passadakis,Yingjin Song,Albert Gatt*

Main category: cs.CL

TL;DR: 该论文提出将视觉叙事视为图像描述的超集，通过先使用视觉-语言模型生成图像描述，再用语言-语言方法将其转换为连贯故事，在质量和效率上都有提升


<details>
  <summary>Details</summary>
Motivation: 解决视觉叙事任务中既要基于图像序列又要保持叙事连贯性的挑战，平衡这两个方面

Method: 采用两阶段方法：首先使用视觉-语言模型为输入图像生成描述，然后通过语言-语言方法将这些描述转换为连贯的叙事故事

Result: 评估显示整合描述和叙事到统一框架对故事质量有积极影响，加速训练时间，提高框架的可重用性和可复现性

Conclusion: 提出的方法有效平衡了视觉叙事的两个关键方面，并引入了新的ideality度量标准来模拟结果与理想模型的接近程度

Abstract: Visual Storytelling is a challenging multimodal task between Vision &
Language, where the purpose is to generate a story for a stream of images. Its
difficulty lies on the fact that the story should be both grounded to the image
sequence but also narrative and coherent. The aim of this work is to balance
between these aspects, by treating Visual Storytelling as a superset of Image
Captioning, an approach quite different compared to most of prior relevant
studies. This means that we firstly employ a vision-to-language model for
obtaining captions of the input images, and then, these captions are
transformed into coherent narratives using language-to-language methods. Our
multifarious evaluation shows that integrating captioning and storytelling
under a unified framework, has a positive impact on the quality of the produced
stories. In addition, compared to numerous previous studies, this approach
accelerates training time and makes our framework readily reusable and
reproducible by anyone interested. Lastly, we propose a new metric/tool, named
ideality, that can be used to simulate how far some results are from an oracle
model, and we apply it to emulate human-likeness in visual storytelling.

</details>


### [96] [ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine](https://arxiv.org/abs/2508.14706)
*Junying Chen,Zhenyang Cai,Zhiheng Liu,Yunjin Yang,Rongsheng Wang,Qingying Xiao,Xiangyi Feng,Zhan Su,Jing Guo,Xiang Wan,Guangjun Yu,Haizhou Li,Benyou Wang*

Main category: cs.CL

TL;DR: ShizhenGPT是首个针对中医的多模态大语言模型，通过构建大规模多模态数据集（100GB+文本和200GB+多模态数据），解决了中医数据稀缺和多模态诊断的挑战，在中医资格考试和视觉诊断任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型在中医领域应用受限，主要因为高质量中医数据稀缺以及中医诊断固有的多模态特性（望闻问切），这些感官丰富的模态超出了传统LLMs的处理范围。

Method: 构建了迄今最大的中医数据集，包含文本和多模态数据（图像、音频、生理信号）；对ShizhenGPT进行预训练和指令微调，使其具备深厚的中医知识和多模态推理能力；建立了中医资格考试和视觉诊断评估基准。

Result: ShizhenGPT在可比规模的LLMs中表现优异，甚至能与更大的专有模型竞争；在中医视觉理解方面领先于现有多模态LLMs；在声音、脉搏、气味和视觉等多种模态上展现出统一的感知能力。

Conclusion: 该研究为中医领域的多模态感知和诊断开辟了新途径，公开的数据集、模型和代码将促进该领域的进一步探索，展示了多模态LLMs在传统医学中的巨大潜力。

Abstract: Despite the success of large language models (LLMs) in various domains, their
potential in Traditional Chinese Medicine (TCM) remains largely underexplored
due to two critical barriers: (1) the scarcity of high-quality TCM data and (2)
the inherently multimodal nature of TCM diagnostics, which involve looking,
listening, smelling, and pulse-taking. These sensory-rich modalities are beyond
the scope of conventional LLMs. To address these challenges, we present
ShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data
scarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text
and 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and
physiological signals. ShizhenGPT is pretrained and instruction-tuned to
achieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect
recent national TCM qualification exams and build a visual benchmark for
Medicinal Recognition and Visual Diagnosis. Experiments demonstrate that
ShizhenGPT outperforms comparable-scale LLMs and competes with larger
proprietary models. Moreover, it leads in TCM visual understanding among
existing multimodal LLMs and demonstrates unified perception across modalities
like sound, pulse, smell, and vision, paving the way toward holistic multimodal
perception and diagnosis in TCM. Datasets, models, and code are publicly
available. We hope this work will inspire further exploration in this field.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [97] [OmniSense: Towards Edge-Assisted Online Analytics for 360-Degree Videos](https://arxiv.org/abs/2508.14237)
*Miao Zhang,Yifei Zhu,Linfeng Shen,Fangxin Wang,Jiangchuan Liu*

Main category: cs.NI

TL;DR: OmniSense是一个边缘辅助的360度视频分析框架，通过球形感兴趣区域预测和智能模型缩放，在保持低延迟的同时显著提高分析精度


<details>
  <summary>Details</summary>
Motivation: 随着全向相机硬件成本降低和扩展现实应用普及，360度视频分析需求增长，但面临计算和网络资源挑战，需要既能降低延迟又能保证高精度的解决方案

Method: 提出轻量级球形感兴趣区域(SRoI)预测算法来修剪冗余信息，结合视频内容和网络动态智能缩放视觉模型，优化资源利用率

Result: 相比资源无关基线方法，精度提高19.8%-114.6%，在相似端到端延迟下实现2.0-2.4倍加速，同时保持与基线最高精度相当

Conclusion: OmniSense框架成功解决了360度视频分析的计算和网络资源挑战，实现了低延迟和高精度的平衡，为沉浸式视频分析提供了有效解决方案

Abstract: With the reduced hardware costs of omnidirectional cameras and the
proliferation of various extended reality applications, more and more
$360^\circ$ videos are being captured. To fully unleash their potential,
advanced video analytics is expected to extract actionable insights and
situational knowledge without blind spots from the videos. In this paper, we
present OmniSense, a novel edge-assisted framework for online immersive video
analytics. OmniSense achieves both low latency and high accuracy, combating the
significant computation and network resource challenges of analyzing
$360^\circ$ videos. Motivated by our measurement insights into $360^\circ$
videos, OmniSense introduces a lightweight spherical region of interest (SRoI)
prediction algorithm to prune redundant information in $360^\circ$ frames.
Incorporating the video content and network dynamics, it then smartly scales
vision models to analyze the predicted SRoIs with optimized resource
utilization. We implement a prototype of OmniSense with commodity devices and
evaluate it on diverse real-world collected $360^\circ$ videos. Extensive
evaluation results show that compared to resource-agnostic baselines, it
improves the accuracy by $19.8\%$ -- $114.6\%$ with similar end-to-end
latencies. Meanwhile, it hits $2.0\times$ -- $2.4\times$ speedups while keeping
the accuracy on par with the highest accuracy of baselines.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [98] [High-Throughput Low-Cost Segmentation of Brightfield Microscopy Live Cell Images](https://arxiv.org/abs/2508.14106)
*Surajit Das,Gourav Roy,Pavel Zun*

Main category: q-bio.QM

TL;DR: 这是一个基于CNN的细胞分割模型，专门处理光广领基微镜下的无染色活细胞图像，在低对比度、噪声和模糊图像上达到93%测试准确率和89%平均F1分数。


<details>
  <summary>Details</summary>
Motivation: 解决光广领微镜下活细胞图像分割的挑战，包括时间性表型变化、低对比度、噪声、细胞运动导致的模糊等问题，并提供一个高速通量的解决方案。

Method: 使用U-Net架构结合注意力机制、实例识别系统、适配损失函数、难样本重训、动态学习率、渐进机制和集成技术的CNN流水线。

Result: 在公开数据集上验证，在低对比度、噪声和模糊图像上达到93%测试准确率和89%平均F1分数，并能够良好地沿用到相位差微镜数据集。

Conclusion: 该模型在光广领微镜分割中表现出优异的稳健性和精度，体秱小且可通过Google Colab等基础深度学习环境进行调整，具有实际实验室部署的潜力。

Abstract: Live cell culture is crucial in biomedical studies for analyzing cell
properties and dynamics in vitro. This study focuses on segmenting unstained
live cells imaged with bright-field microscopy. While many segmentation
approaches exist for microscopic images, none consistently address the
challenges of bright-field live-cell imaging with high throughput, where
temporal phenotype changes, low contrast, noise, and motion-induced blur from
cellular movement remain major obstacles. We developed a low-cost CNN-based
pipeline incorporating comparative analysis of frozen encoders within a unified
U-Net architecture enhanced with attention mechanisms, instance-aware systems,
adaptive loss functions, hard instance retraining, dynamic learning rates,
progressive mechanisms to mitigate overfitting, and an ensemble technique. The
model was validated on a public dataset featuring diverse live cell variants,
showing consistent competitiveness with state-of-the-art methods, achieving 93%
test accuracy and an average F1-score of 89% (std. 0.07) on low-contrast,
noisy, and blurry images. Notably, the model was trained primarily on
bright-field images with limited exposure to phase-contrast microscopy (<10%),
yet it generalized effectively to the phase-contrast LIVECell dataset,
demonstrating modality, robustness and strong performance. This highlights its
potential for real-world laboratory deployment across imaging conditions. The
model requires minimal compute power and is adaptable using basic deep learning
setups such as Google Colab, making it practical for training on other cell
variants. Our pipeline outperforms existing methods in robustness and precision
for bright-field microscopy segmentation. The code and dataset are available
for reproducibility

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [99] [Physics-Constrained Diffusion Reconstruction with Posterior Correction for Quantitative and Fast PET Imaging](https://arxiv.org/abs/2508.14364)
*Yucun Hou,Fenglin Zhan,Chenxi Li,Ziquan Yuan,Haoyu Lu,Yue Chen,Yihao Chen,Kexin Wang,Runze Liao,Haoqi Wen,Ganxi Du,Jiaru Ni,Taoran Chen,Jinyue Zhang,Jigang Yang,Jianyong Jiang*

Main category: physics.med-ph

TL;DR: 提出PET-DPC方法，结合条件扩散模型和物理后校正，实现快速且定量准确的PET图像重建，相比传统方法重建时间减少50-85%。


<details>
  <summary>Details</summary>
Motivation: 深度学习PET重建方法存在定量准确性不足、伪影问题，以及模型可解释性差、数据依赖性强和过拟合风险，阻碍了临床采用。

Method: 使用条件扩散模型，通过创新的归一化程序生成GTP-image输入，在扩散采样过程中融入物理信息进行后散射、衰减和随机校正。

Result: 在300个脑部和50个全身PET数据集上验证，PET-DPC重建结果与完全校正的OSEM图像高度一致，定量指标优于端到端深度学习模型，泛化性能好，重建时间大幅减少。

Conclusion: PET-DPC是一种快速、定量准确的PET重建方法，具有改善临床成像工作流程的强大潜力。

Abstract: Deep learning-based reconstruction of positron emission tomography(PET) data
has gained increasing attention in recent years. While these methods achieve
fast reconstruction,concerns remain regarding quantitative accuracy and the
presence of artifacts,stemming from limited model interpretability,data driven
dependence, and overfitting risks.These challenges have hindered clinical
adoption.To address them,we propose a conditional diffusion model with
posterior physical correction (PET-DPC) for PET image reconstruction. An
innovative normalization procedure generates the input Geometric TOF
Probabilistic Image (GTP-image),while physical information is incorporated
during the diffusion sampling process to perform posterior
scatter,attenuation,and random corrections. The model was trained and validated
on 300 brain and 50 whole-body PET datasets,a physical phantom,and 20 simulated
brain datasets. PET-DPC produced reconstructions closely aligned with fully
corrected OSEM images,outperforming end-to-end deep learning models in
quantitative metrics and,in some cases, surpassing traditional iterative
methods. The model also generalized well to out-of-distribution(OOD) data.
Compared to iterative methods,PET-DPC reduced reconstruction time by 50% for
brain scans and 85% for whole-body scans. Ablation studies confirmed the
critical role of posterior correction in implementing scatter and attenuation
corrections,enhancing reconstruction accuracy. Experiments with physical
phantoms further demonstrated PET-DPC's ability to preserve background
uniformity and accurately reproduce tumor-to-background intensity ratios.
Overall,these results highlight PET-DPC as a promising approach for rapid,
quantitatively accurate PET reconstruction,with strong potential to improve
clinical imaging workflows.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [100] [FakeHunter: Multimodal Step-by-Step Reasoning for Explainable Video Forensics](https://arxiv.org/abs/2508.14581)
*Chen Chen,Runze Li,Zejun Zhang,Pukun Zhao,Fanqing Zhou,Longxiang Wang,Haojian Huang*

Main category: cs.MM

TL;DR: FakeHunter是一个多模态深度伪造检测框架，结合记忆检索、思维链推理和工具增强验证，在X-AVFake基准上达到34.75%的准确率，比基础模型提升16.87个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造检测方法往往缺乏可解释性，难以提供详细的篡改定位和解释。需要开发一个既能准确检测又能提供结构化解释的多模态检测系统。

Method: 使用CLIP编码视觉内容，CLAP编码音频，生成联合音视频嵌入，从FAISS索引的记忆库中检索相似真实样本进行上下文锚定。通过观察-思考-行动的思维链推理，在置信度低时自动调用图像取证和频谱分析等工具进行细粒度验证。

Result: 在X-AVFake基准（5.7k+视频）上达到34.75%的准确率，比Qwen2.5-Omni-7B提升16.87个百分点。记忆检索贡献7.75个百分点的增益，工具检查将低置信度案例提升至46.50%。处理10分钟视频需8分钟（单GPU）或2分钟（4GPU）。

Conclusion: FakeHunter通过多模态融合、记忆检索和工具增强验证，实现了准确且可解释的深度伪造检测，在保持实用部署性的同时显著提升了检测性能。

Abstract: FakeHunter is a multimodal deepfake detection framework that combines
memory-guided retrieval, chain-of-thought (Observation-Thought-Action)
reasoning, and tool-augmented verification to provide accurate and
interpretable video forensics. FakeHunter encodes visual content using CLIP and
audio using CLAP, generating joint audio-visual embeddings that retrieve
semantically similar real exemplars from a FAISS-indexed memory bank for
contextual grounding. Guided by the retrieved context, the system iteratively
reasons over evidence to localize manipulations and explain them. When
confidence is low, it automatically invokes specialized tools-such as zoom-in
image forensics or mel-spectrogram inspection-for fine-grained verification.
Built on Qwen2.5-Omni-7B, FakeHunter produces structured JSON verdicts that
specify what was modified, where it occurs, and why it is judged fake. We also
introduce X-AVFake, a benchmark comprising 5.7k+ manipulated and real videos
(950+ min) annotated with manipulation type, region/entity, violated reasoning
category, and free-form justification. On X-AVFake, FakeHunter achieves an
accuracy of 34.75%, outperforming the vanilla Qwen2.5-Omni-7B by 16.87
percentage points and MiniCPM-2.6 by 25.56 percentage points. Ablation studies
reveal that memory retrieval contributes a 7.75 percentage point gain, and
tool-based inspection improves low-confidence cases to 46.50%. Despite its
multi-stage design, the pipeline processes a 10-minute clip in 8 minutes on a
single NVIDIA A800 (0.8x real-time) or 2 minutes on four GPUs (0.2x),
demonstrating practical deployability.

</details>
