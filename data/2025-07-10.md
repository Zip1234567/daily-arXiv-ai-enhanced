<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 84]
- [eess.IV](#eess.IV) [Total: 15]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.LG](#cs.LG) [Total: 4]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Unveiling the Underwater World: CLIP Perception Model-Guided Underwater Image Enhancement](https://arxiv.org/abs/2507.06234)
*Jiangzhong Cao,Zekai Zeng,Xu Zhang,Huan Zhang,Chunling Fan,Gangyi Jiang,Weisi Lin*

Main category: cs.CV

TL;DR: 提出了一种结合CLIP感知损失模块和课程对比正则化的水下图像增强方法，以提升感知质量和内容恢复。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习水下图像增强方法忽视人类感知且解空间约束不足，导致增强图像感知质量下降或内容恢复不佳。

Method: 利用CLIP模型的视觉语义特征提取能力设计感知损失模块，并结合课程对比正则化优化增强效果。

Result: 实验表明，该方法在视觉质量和泛化能力上优于现有方法。

Conclusion: 结合CLIP感知和课程对比正则化能有效提升水下图像增强的感知质量和内容恢复效果。

Abstract: High-quality underwater images are essential for both machine vision tasks
and viewers with their aesthetic appeal.However, the quality of underwater
images is severely affected by light absorption and scattering. Deep
learning-based methods for Underwater Image Enhancement (UIE) have achieved
good performance. However, these methods often overlook considering human
perception and lack sufficient constraints within the solution space.
Consequently, the enhanced images often suffer from diminished perceptual
quality or poor content restoration.To address these issues, we propose a UIE
method with a Contrastive Language-Image Pre-Training (CLIP) perception loss
module and curriculum contrastive regularization. Above all, to develop a
perception model for underwater images that more aligns with human visual
perception, the visual semantic feature extraction capability of the CLIP model
is leveraged to learn an appropriate prompt pair to map and evaluate the
quality of underwater images. This CLIP perception model is then incorporated
as a perception loss module into the enhancement network to improve the
perceptual quality of enhanced images. Furthermore, the CLIP perception model
is integrated with the curriculum contrastive regularization to enhance the
constraints imposed on the enhanced images within the CLIP perceptual space,
mitigating the risk of both under-enhancement and over-enhancement.
Specifically, the CLIP perception model is employed to assess and categorize
the learning difficulty level of negatives in the regularization process,
ensuring comprehensive and nuanced utilization of distorted images and
negatives with varied quality levels. Extensive experiments demonstrate that
our method outperforms state-of-the-art methods in terms of visual quality and
generalization ability.

</details>


### [2] [FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation](https://arxiv.org/abs/2507.06523)
*Liqiang Jing,Viet Lai,Seunghyun Yoon,Trung Bui,Xinya Du*

Main category: cs.CV

TL;DR: FIFA是一个统一的视频多模态大语言模型（VideoMLLMs）忠实性评估框架，通过提取描述性事实、建模语义依赖关系并验证，解决了现有评估方法在开放自由响应中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法局限于单一任务且无法评估开放自由响应中的幻觉，导致VideoMLLMs生成的文本或视频内容与视觉输入不符。

Method: 提出FIFA框架，提取描述性事实，构建时空语义依赖图，并通过VideoQA模型验证；进一步引入基于工具的Post-Correction修正框架。

Result: FIFA比现有评估方法更接近人类判断，Post-Correction有效提升了文本和视频生成的事实一致性。

Conclusion: FIFA为VideoMLLMs的忠实性评估提供了统一且有效的解决方案，Post-Correction进一步优化了生成内容的事实一致性。

Abstract: Video Multimodal Large Language Models (VideoMLLMs) have achieved remarkable
progress in both Video-to-Text and Text-to-Video tasks. However, they often
suffer fro hallucinations, generating content that contradicts the visual
input. Existing evaluation methods are limited to one task (e.g., V2T) and also
fail to assess hallucinations in open-ended, free-form responses. To address
this gap, we propose FIFA, a unified FaIthFulness evAluation framework that
extracts comprehensive descriptive facts, models their semantic dependencies
via a Spatio-Temporal Semantic Dependency Graph, and verifies them using
VideoQA models. We further introduce Post-Correction, a tool-based correction
framework that revises hallucinated content. Extensive experiments demonstrate
that FIFA aligns more closely with human judgment than existing evaluation
methods, and that Post-Correction effectively improves factual consistency in
both text and video generation.

</details>


### [3] [SPARC: Concept-Aligned Sparse Autoencoders for Cross-Model and Cross-Modal Interpretability](https://arxiv.org/abs/2507.06265)
*Ali Nasiri-Sarvi,Hassan Rivaz,Mahdi S. Hosseini*

Main category: cs.CV

TL;DR: SPARC框架通过全局TopK稀疏机制和跨重建损失，为不同AI模型学习统一的概念空间，显著提升概念对齐效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因模型独立表示导致的概念空间不兼容问题，实现跨模型和跨模态的通用概念对齐。

Method: 采用全局TopK稀疏机制和跨重建损失，强制不同模型在相同概念上激活相同的潜在维度。

Result: 在Open Images数据集上，SPARC的Jaccard相似度达到0.80，比之前方法提升三倍以上。

Conclusion: SPARC通过统一稀疏潜在空间，实现了跨模型和跨模态的概念对齐，支持直接比较和实际应用。

Abstract: Understanding how different AI models encode the same high-level concepts,
such as objects or attributes, remains challenging because each model typically
produces its own isolated representation. Existing interpretability methods
like Sparse Autoencoders (SAEs) produce latent concepts individually for each
model, resulting in incompatible concept spaces and limiting cross-model
interpretability. To address this, we introduce SPARC (Sparse Autoencoders for
Aligned Representation of Concepts), a new framework that learns a single,
unified latent space shared across diverse architectures and modalities (e.g.,
vision models like DINO, and multimodal models like CLIP). SPARC's alignment is
enforced through two key innovations: (1) a Global TopK sparsity mechanism,
ensuring all input streams activate identical latent dimensions for a given
concept; and (2) a Cross-Reconstruction Loss, which explicitly encourages
semantic consistency between models. On Open Images, SPARC dramatically
improves concept alignment, achieving a Jaccard similarity of 0.80, more than
tripling the alignment compared to previous methods. SPARC creates a shared
sparse latent space where individual dimensions often correspond to similar
high-level concepts across models and modalities, enabling direct comparison of
how different architectures represent identical concepts without requiring
manual alignment or model-specific analysis. As a consequence of this aligned
representation, SPARC also enables practical applications such as text-guided
spatial localization in vision-only models and cross-model/cross-modal
retrieval. Code and models are available at
https://github.com/AtlasAnalyticsLab/SPARC.

</details>


### [4] [A Probabilistic Approach to Uncertainty Quantification Leveraging 3D Geometry](https://arxiv.org/abs/2507.06269)
*Rushil Desai,Frederik Warburg,Trevor Darrell,Marissa Ramirez de Chanlatte*

Main category: cs.CV

TL;DR: BayesSDF提出了一种新的概率框架，用于神经隐式SDF模型中的不确定性量化，解决了现有方法的计算效率低、几何不一致等问题。


<details>
  <summary>Details</summary>
Motivation: 科学模拟应用（如森林环境中的流体模拟）需要精确的表面几何和不确定性量化，但现有方法缺乏直接几何集成，导致不确定性图校准不佳。

Method: BayesSDF利用拉普拉斯近似，通过基于Hessian的度量量化局部表面不稳定性，实现高效、表面感知的不确定性估计。

Result: 实验表明，BayesSDF在合成和真实数据集上优于现有方法，提供了与重建几何紧密相关的不确定性预测。

Conclusion: BayesSDF为不确定性感知的3D场景重建、模拟和机器人决策奠定了坚实基础。

Abstract: Quantifying uncertainty in neural implicit 3D representations, particularly
those utilizing Signed Distance Functions (SDFs), remains a substantial
challenge due to computational inefficiencies, scalability issues, and
geometric inconsistencies. Existing methods typically neglect direct geometric
integration, leading to poorly calibrated uncertainty maps. We introduce
BayesSDF, a novel probabilistic framework for uncertainty quantification in
neural implicit SDF models, motivated by scientific simulation applications
with 3D environments (e.g., forests) such as modeling fluid flow through
forests, where precise surface geometry and awareness of fidelity surface
geometric uncertainty are essential. Unlike radiance-based models such as NeRF
or 3D Gaussian splatting, which lack explicit surface formulations, SDFs define
continuous and differentiable geometry, making them better suited for physical
modeling and analysis. BayesSDF leverages a Laplace approximation to quantify
local surface instability via Hessian-based metrics, enabling computationally
efficient, surface-aware uncertainty estimation. Our method shows that
uncertainty predictions correspond closely with poorly reconstructed geometry,
providing actionable confidence measures for downstream use. Extensive
evaluations on synthetic and real-world datasets demonstrate that BayesSDF
outperforms existing methods in both calibration and geometric consistency,
establishing a strong foundation for uncertainty-aware 3D scene reconstruction,
simulation, and robotic decision-making.

</details>


### [5] [LIRA: Inferring Segmentation in Large Multi-modal Models with Local Interleaved Region Assistance](https://arxiv.org/abs/2507.06272)
*Zhang Li,Biao Yang,Qiang Liu,Shuo Zhang,Zhiyin Ma,Shuo Zhang,Liang Yin,Linger Deng,Yabo Sun,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: LIRA框架通过结合语义增强特征提取器和交错局部视觉耦合，解决了大型多模态模型在分割和视觉理解中的不准确和幻觉问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型在分割和理解任务中存在不准确分割和幻觉理解的问题，主要源于视觉理解能力弱和缺乏细粒度感知。

Method: 提出LIRA框架，包含语义增强特征提取器（SEFE）和交错局部视觉耦合（ILVC），分别提升分割准确性和减少幻觉。

Result: LIRA在分割和理解任务中达到最先进性能，并发现分割精度与语义相关性正相关。

Conclusion: LIRA通过结合语义和局部特征，显著提升了多模态模型的分割和理解能力。

Abstract: While large multi-modal models (LMMs) demonstrate promising capabilities in
segmentation and comprehension, they still struggle with two limitations:
inaccurate segmentation and hallucinated comprehension. These challenges stem
primarily from constraints in weak visual comprehension and a lack of
fine-grained perception. To alleviate these limitations, we propose LIRA, a
framework that capitalizes on the complementary relationship between visual
comprehension and segmentation via two key components: (1) Semantic-Enhanced
Feature Extractor (SEFE) improves object attribute inference by fusing semantic
and pixel-level features, leading to more accurate segmentation; (2)
Interleaved Local Visual Coupling (ILVC) autoregressively generates local
descriptions after extracting local features based on segmentation masks,
offering fine-grained supervision to mitigate hallucinations. Furthermore, we
find that the precision of object segmentation is positively correlated with
the latent related semantics of the <seg> token. To quantify this relationship
and the model's potential semantic inferring ability, we introduce the
Attributes Evaluation (AttrEval) dataset. Our experiments show that LIRA
achieves state-of-the-art performance in both segmentation and comprehension
tasks. Code will be available at https://github.com/echo840/LIRA.

</details>


### [6] [Advancing Offline Handwritten Text Recognition: A Systematic Review of Data Augmentation and Generation Techniques](https://arxiv.org/abs/2507.06275)
*Yassin Hussein Rassul,Aram M. Ahmed,Polla Fattah,Bryar A. Hassan,Arwaa W. Abdulkareem,Tarik A. Rashid,Joan Lu*

Main category: cs.CV

TL;DR: 本文综述了离线手写文本识别（HTR）中的数据增强与生成技术，探讨了传统方法与深度学习方法（如GANs、扩散模型和基于Transformer的方法）的优缺点，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 离线HTR系统在历史文档数字化等领域至关重要，但标注数据稀缺，尤其是低资源语言和复杂脚本，限制了其性能。本文旨在通过数据增强和生成技术提升HTR系统的准确性和鲁棒性。

Method: 采用PRISMA方法，系统筛选了1,302篇文献，最终纳入848篇，涵盖IEEE、Springer等学术资源，分析了传统与深度学习方法。

Result: 研究发现生成多样且真实的手写样本仍具挑战，尤其是在保持脚本真实性和解决数据稀缺方面。

Conclusion: 本文总结了现有技术的不足，并提出了未来研究方向，以推动手写文本生成领域的发展。

Abstract: Offline Handwritten Text Recognition (HTR) systems play a crucial role in
applications such as historical document digitization, automatic form
processing, and biometric authentication. However, their performance is often
hindered by the limited availability of annotated training data, particularly
for low-resource languages and complex scripts. This paper presents a
comprehensive survey of offline handwritten data augmentation and generation
techniques designed to improve the accuracy and robustness of HTR systems. We
systematically examine traditional augmentation methods alongside recent
advances in deep learning, including Generative Adversarial Networks (GANs),
diffusion models, and transformer-based approaches. Furthermore, we explore the
challenges associated with generating diverse and realistic handwriting
samples, particularly in preserving script authenticity and addressing data
scarcity. This survey follows the PRISMA methodology, ensuring a structured and
rigorous selection process. Our analysis began with 1,302 primary studies,
which were filtered down to 848 after removing duplicates, drawing from key
academic sources such as IEEE Digital Library, Springer Link, Science Direct,
and ACM Digital Library. By evaluating existing datasets, assessment metrics,
and state-of-the-art methodologies, this survey identifies key research gaps
and proposes future directions to advance the field of handwritten text
generation across diverse linguistic and stylistic landscapes.

</details>


### [7] [Centralized Copy-Paste: Enhanced Data Augmentation Strategy for Wildland Fire Semantic Segmentation](https://arxiv.org/abs/2507.06321)
*Joon Tai Kim,Tianle Chen,Ziyu Dong,Nishanth Kunchala,Alexander Guller,Daniel Ospina Acero,Roger Williams,Mrinal Kumar*

Main category: cs.CV

TL;DR: 提出了一种名为CCPDA的数据增强方法，专注于提升野火分割模型的性能，通过集中复制粘贴技术增加数据集多样性。


<details>
  <summary>Details</summary>
Motivation: 由于标注图像成本高且公共数据集稀缺，特别是在野火科学领域，需要一种有效的数据增强方法来提升分割模型的训练效果。

Method: CCPDA方法包括三步：(i)识别源图像中的火簇，(ii)通过集中技术聚焦火区核心，(iii)将处理后的火簇粘贴到目标图像上。

Result: CCPDA显著提升了火类分割性能，优于其他增强方法，并通过多目标优化验证了其有效性。

Conclusion: CCPDA能有效缓解小规模标注数据集的训练难题，尤其在火类分割任务中表现突出。

Abstract: Collecting and annotating images for the purpose of training segmentation
models is often cost prohibitive. In the domain of wildland fire science, this
challenge is further compounded by the scarcity of reliable public datasets
with labeled ground truth. This paper presents the Centralized Copy-Paste Data
Augmentation (CCPDA) method, for the purpose of assisting with the training of
deep-learning multiclass segmentation models, with special focus on improving
segmentation outcomes for the fire-class. CCPDA has three main steps: (i)
identify fire clusters in the source image, (ii) apply a centralization
technique to focus on the core of the fire area, and (iii) paste the refined
fire clusters onto a target image. This method increases dataset diversity
while preserving the essential characteristics of the fire class. The
effectiveness of this augmentation technique is demonstrated via numerical
analysis and comparison against various other augmentation methods using a
weighted sum-based multi-objective optimization approach. This approach helps
elevate segmentation performance metrics specific to the fire class, which
carries significantly more operational significance than other classes (fuel,
ash, or background). Numerical performance assessment validates the efficacy of
the presented CCPDA method in alleviating the difficulties associated with
small, manually labeled training datasets. It also illustrates that CCPDA
outperforms other augmentation strategies in the application scenario
considered, particularly in improving fire-class segmentation performance.

</details>


### [8] [AR2: Attention-Guided Repair for the Robustness of CNNs Against Common Corruptions](https://arxiv.org/abs/2507.06332)
*Fuyuan Zhang,Qichen Wang,Jianjun Zhao*

Main category: cs.CV

TL;DR: AR2是一种通过对齐干净和损坏图像的类激活图（CAMs）来增强预训练CNN鲁棒性的方法，无需改变架构。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在常见损坏（如噪声、模糊等）下性能显著下降，限制了其在实际应用中的可靠性。

Method: AR2通过CAM对齐和迭代修复策略（CAM引导的细化和标准微调交替）提升鲁棒性。

Result: AR2在标准损坏基准测试中表现优于现有方法，同时保持了干净数据的准确性。

Conclusion: AR2为增强模型在多样化损坏环境中的可靠性提供了可扩展的解决方案。

Abstract: Deep neural networks suffer from significant performance degradation when
exposed to common corruptions such as noise, blur, weather, and digital
distortions, limiting their reliability in real-world applications. In this
paper, we propose AR2 (Attention-Guided Repair for Robustness), a simple yet
effective method to enhance the corruption robustness of pretrained CNNs. AR2
operates by explicitly aligning the class activation maps (CAMs) between clean
and corrupted images, encouraging the model to maintain consistent attention
even under input perturbations. Our approach follows an iterative repair
strategy that alternates between CAM-guided refinement and standard
fine-tuning, without requiring architectural changes. Extensive experiments
show that AR2 consistently outperforms existing state-of-the-art methods in
restoring robustness on standard corruption benchmarks (CIFAR-10-C, CIFAR-100-C
and ImageNet-C), achieving a favorable balance between accuracy on clean data
and corruption robustness. These results demonstrate that AR2 provides a robust
and scalable solution for enhancing model reliability in real-world
environments with diverse corruptions.

</details>


### [9] [When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking](https://arxiv.org/abs/2507.06400)
*Weiran Li,Yeqiang Liu,Qiannan Guo,Yijie Wei,Hwa Liang Leo,Zhenbo Li*

Main category: cs.CV

TL;DR: 论文提出了首个水下多鱼跟踪数据集MFT25，并开发了专门用于非线性鱼类运动的SU-T跟踪框架，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 水下多目标跟踪在海洋生态和水产养殖中具有重要意义，但相关研究较少。

Method: 提出了MFT25数据集和SU-T跟踪框架，包括优化的Unscented Kalman Filter和FishIoU匹配方法。

Result: SU-T在MFT25上表现优异，HOTA为34.1，IDF1为44.6。

Conclusion: MFT25和SU-T为水下跟踪研究提供了坚实基础，对海洋生物学和水产养殖有重要应用价值。

Abstract: Multiple object tracking (MOT) technology has made significant progress in
terrestrial applications, but underwater tracking scenarios remain
underexplored despite their importance to marine ecology and aquaculture. We
present Multiple Fish Tracking Dataset 2025 (MFT25), the first comprehensive
dataset specifically designed for underwater multiple fish tracking, featuring
15 diverse video sequences with 408,578 meticulously annotated bounding boxes
across 48,066 frames. Our dataset captures various underwater environments,
fish species, and challenging conditions including occlusions, similar
appearances, and erratic motion patterns. Additionally, we introduce
Scale-aware and Unscented Tracker (SU-T), a specialized tracking framework
featuring an Unscented Kalman Filter (UKF) optimized for non-linear fish
swimming patterns and a novel Fish-Intersection-over-Union (FishIoU) matching
that accounts for the unique morphological characteristics of aquatic species.
Extensive experiments demonstrate that our SU-T baseline achieves
state-of-the-art performance on MFT25, with 34.1 HOTA and 44.6 IDF1, while
revealing fundamental differences between fish tracking and terrestrial object
tracking scenarios. MFT25 establishes a robust foundation for advancing
research in underwater tracking systems with important applications in marine
biology, aquaculture monitoring, and ecological conservation. The dataset and
codes are released at https://vranlee.github.io/SU-T/.

</details>


### [10] [SImpHAR: Advancing impedance-based human activity recognition using 3D simulation and text-to-motion models](https://arxiv.org/abs/2507.06405)
*Lala Shakti Swarup Ray,Mengxi Liu,Deepika Gurung,Bo Zhou,Sungho Suh,Paul Lukowicz*

Main category: cs.CV

TL;DR: SImpHAR框架通过模拟生物阻抗信号和两阶段训练策略，显著提升了基于阻抗的人体活动识别性能。


<details>
  <summary>Details</summary>
Motivation: 生物阻抗传感在精细运动捕捉中具有优势，但缺乏标记数据限制了其应用。

Method: 提出模拟管道生成生物阻抗信号，并采用两阶段训练策略。

Result: 在多个数据集上性能提升，最高达22.3%准确率和21.8%宏F1分数。

Conclusion: 模拟驱动增强和模块化训练对阻抗基HAR具有潜力。

Abstract: Human Activity Recognition (HAR) with wearable sensors is essential for
applications in healthcare, fitness, and human-computer interaction.
Bio-impedance sensing offers unique advantages for fine-grained motion capture
but remains underutilized due to the scarcity of labeled data. We introduce
SImpHAR, a novel framework addressing this limitation through two core
contributions. First, we propose a simulation pipeline that generates realistic
bio-impedance signals from 3D human meshes using shortest-path estimation,
soft-body physics, and text-to-motion generation serving as a digital twin for
data augmentation. Second, we design a two-stage training strategy with
decoupled approach that enables broader activity coverage without requiring
label-aligned synthetic data. We evaluate SImpHAR on our collected ImpAct
dataset and two public benchmarks, showing consistent improvements over
state-of-the-art methods, with gains of up to 22.3% and 21.8%, in terms of
accuracy and macro F1 score, respectively. Our results highlight the promise of
simulation-driven augmentation and modular training for impedance-based HAR.

</details>


### [11] [Hierarchical Multi-Stage Transformer Architecture for Context-Aware Temporal Action Localization](https://arxiv.org/abs/2507.06411)
*Hayat Ullah,Arslan Munir,Oliver Nina*

Main category: cs.CV

TL;DR: 论文提出了一种名为PCL-Former的分层多阶段Transformer架构，用于时序动作定位任务，通过三个专用模块分别处理候选段识别、动作分类和边界预测，在多个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 受Transformer和多阶段架构在视频识别和目标检测领域的成功启发，探索其在时序动作定位任务中的潜力。

Method: 提出PCL-Former架构，包含Proposal-Former、Classification-Former和Localization-Former三个模块，每个模块配备专用损失函数。

Result: 在THUMOS14、ActivityNet-1.3和HACS数据集上分别超越现有方法2.8%、1.2%和4.8%。

Conclusion: PCL-Former通过模块化设计显著提升了时序动作定位的性能，验证了多阶段Transformer架构的有效性。

Abstract: Inspired by the recent success of transformers and multi-stage architectures
in video recognition and object detection domains. We thoroughly explore the
rich spatio-temporal properties of transformers within a multi-stage
architecture paradigm for the temporal action localization (TAL) task. This
exploration led to the development of a hierarchical multi-stage transformer
architecture called PCL-Former, where each subtask is handled by a dedicated
transformer module with a specialized loss function. Specifically, the
Proposal-Former identifies candidate segments in an untrimmed video that may
contain actions, the Classification-Former classifies the action categories
within those segments, and the Localization-Former precisely predicts the
temporal boundaries (i.e., start and end) of the action instances. To evaluate
the performance of our method, we have conducted extensive experiments on three
challenging benchmark datasets: THUMOS-14, ActivityNet-1.3, and HACS Segments.
We also conducted detailed ablation experiments to assess the impact of each
individual module of our PCL-Former. The obtained quantitative results validate
the effectiveness of the proposed PCL-Former, outperforming state-of-the-art
TAL approaches by 2.8%, 1.2%, and 4.8% on THUMOS14, ActivityNet-1.3, and HACS
datasets, respectively.

</details>


### [12] [THOR: Thermal-guided Hand-Object Reasoning via Adaptive Vision Sampling](https://arxiv.org/abs/2507.06442)
*Soroush Shahi,Farzad Shahabi,Rama Nabulsi,Glenn Fernandes,Aggelos Katsaggelos,Nabil Alshurafa*

Main category: cs.CV

TL;DR: THOR是一种实时自适应时空RGB帧采样方法，利用热感技术捕捉手部活动，显著减少数据处理量和能耗，同时保持高识别准确率。


<details>
  <summary>Details</summary>
Motivation: 解决可穿戴相机在连续处理RGB图像时的高能耗、大数据量、隐私问题和计算资源需求。

Method: 结合低分辨率热感数据识别手部活动切换，动态调整RGB采样率，并利用热感线索定位感兴趣区域以减少图像处理量。

Result: 仅使用3%的原始RGB数据，实现了与完整视频相当的识别准确率（F1-score 95% vs 94%）。

Conclusion: THOR为实时监测手部活动和健康风险行为提供了更实用的解决方案。

Abstract: Wearable cameras are increasingly used as an observational and interventional
tool for human behaviors by providing detailed visual data of hand-related
activities. This data can be leveraged to facilitate memory recall for logging
of behavior or timely interventions aimed at improving health. However,
continuous processing of RGB images from these cameras consumes significant
power impacting battery lifetime, generates a large volume of unnecessary video
data for post-processing, raises privacy concerns, and requires substantial
computational resources for real-time analysis. We introduce THOR, a real-time
adaptive spatio-temporal RGB frame sampling method that leverages thermal
sensing to capture hand-object patches and classify them in real-time. We use
low-resolution thermal camera data to identify moments when a person switches
from one hand-related activity to another, and adjust the RGB frame sampling
rate by increasing it during activity transitions and reducing it during
periods of sustained activity. Additionally, we use the thermal cues from the
hand to localize the region of interest (i.e., the hand-object interaction) in
each RGB frame, allowing the system to crop and process only the necessary part
of the image for activity recognition. We develop a wearable device to validate
our method through an in-the-wild study with 14 participants and over 30
activities, and further evaluate it on Ego4D (923 participants across 9
countries, totaling 3,670 hours of video). Our results show that using only 3%
of the original RGB video data, our method captures all the activity segments,
and achieves hand-related activity recognition F1-score (95%) comparable to
using the entire RGB video (94%). Our work provides a more practical path for
the longitudinal use of wearable cameras to monitor hand-related activities and
health-risk behaviors in real time.

</details>


### [13] [EA: An Event Autoencoder for High-Speed Vision Sensing](https://arxiv.org/abs/2507.06459)
*Riadul Islam,Joey Mulé,Dhandeep Challagundla,Shahmir Rizvi,Sean Carson*

Main category: cs.CV

TL;DR: 提出了一种事件自动编码器架构，用于高效压缩和重建事件数据，提升事件相机的物体检测性能，同时减少计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统帧式视觉系统存在运动模糊、高延迟和数据冗余问题，事件相机虽能解决这些问题，但其稀疏和噪声的事件流对物体检测提出了挑战。

Method: 采用卷积编码的事件自动编码器架构，结合自适应阈值选择和轻量级分类器，以提升识别精度并降低计算复杂度。

Result: 在SEFD数据集上，模型精度与YOLO-v4相当，但参数减少35.5倍；在嵌入式平台上实现8-44.8 FPS的高帧率，比现有方法快87.84倍。

Conclusion: 该模型显著提升了事件相机的实时性能，适用于低功耗、高速的边缘计算应用。

Abstract: High-speed vision sensing is essential for real-time perception in
applications such as robotics, autonomous vehicles, and industrial automation.
Traditional frame-based vision systems suffer from motion blur, high latency,
and redundant data processing, limiting their performance in dynamic
environments. Event cameras, which capture asynchronous brightness changes at
the pixel level, offer a promising alternative but pose challenges in object
detection due to sparse and noisy event streams. To address this, we propose an
event autoencoder architecture that efficiently compresses and reconstructs
event data while preserving critical spatial and temporal features. The
proposed model employs convolutional encoding and incorporates adaptive
threshold selection and a lightweight classifier to enhance recognition
accuracy while reducing computational complexity. Experimental results on the
existing Smart Event Face Dataset (SEFD) demonstrate that our approach achieves
comparable accuracy to the YOLO-v4 model while utilizing up to $35.5\times$
fewer parameters. Implementations on embedded platforms, including Raspberry Pi
4B and NVIDIA Jetson Nano, show high frame rates ranging from 8 FPS up to 44.8
FPS. The proposed classifier exhibits up to 87.84x better FPS than the
state-of-the-art and significantly improves event-based vision performance,
making it ideal for low-power, high-speed applications in real-time edge
computing.

</details>


### [14] [Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning](https://arxiv.org/abs/2507.06485)
*Ziyang Wang,Jaehong Yoon,Shoubin Yu,Md Mohaiminul Islam,Gedas Bertasius,Mohit Bansal*

Main category: cs.CV

TL;DR: Video-RTS提出了一种数据高效的视频推理方法，结合纯强化学习和自适应测试时间缩放策略，显著减少了数据需求和计算资源。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习和大型语言模型的视频推理方法需要大量数据和标注，成本高且难以扩展。

Method: 跳过监督微调步骤，采用纯强化学习训练和稀疏到密集的视频测试时间缩放策略。

Result: 在多个视频推理基准测试中，Video-RTS平均准确率提升2.4%，仅需3.6%的训练样本。

Conclusion: Video-RTS通过数据高效的方法和自适应策略，显著提升了视频推理性能。

Abstract: Despite advances in reinforcement learning (RL)-based video reasoning with
large language models (LLMs), data collection and finetuning remain significant
challenges. These methods often rely on large-scale supervised fine-tuning
(SFT) with extensive video data and long Chain-of-Thought (CoT) annotations,
making them costly and hard to scale. To address this, we present Video-RTS, a
new approach to improve video reasoning capability with drastically improved
data efficiency by combining data-efficient RL with a video-adaptive test-time
scaling (TTS) strategy. Based on observations about the data scaling of RL
samples, we skip the resource-intensive SFT step and employ efficient pure-RL
training with output-based rewards, requiring no additional annotations or
extensive fine-tuning. Furthermore, to utilize computational resources more
efficiently, we introduce a sparse-to-dense video TTS strategy that improves
inference by iteratively adding frames based on output consistency. We validate
our approach on multiple video reasoning benchmarks, showing that Video-RTS
surpasses existing video reasoning models by an average of 2.4% in accuracy
using only 3.6% training samples. For example, Video-RTS achieves a 4.2%
improvement on Video-Holmes, a recent and challenging video reasoning
benchmark, and a 2.6% improvement on MMVU. Notably, our pure RL training and
adaptive video TTS offer complementary strengths, enabling Video-RTS's strong
reasoning performance.

</details>


### [15] [Mask6D: Masked Pose Priors For 6D Object Pose Estimation](https://arxiv.org/abs/2507.06486)
*Yuechen Xie,Haobo Jiang,Jin Xie*

Main category: cs.CV

TL;DR: 提出了一种名为Mask6D的新预训练策略，通过结合2D-3D对应图和可见掩码图，提升了单目RGB图像在复杂或遮挡条件下的6D物体姿态估计性能。


<details>
  <summary>Details</summary>
Motivation: 当前姿态估计网络在复杂场景中难以提取具有区分性的姿态感知特征，尤其是在目标被遮挡时RGB信息有限。

Method: 提出Mask6D预训练策略，结合2D-3D对应图和可见掩码图，设计对象聚焦的预训练损失函数，并通过传统姿态训练策略微调网络。

Result: 实验表明，该方法优于现有的端到端姿态估计方法。

Conclusion: Mask6D通过引入姿态感知的多模态信息，有效提升了复杂场景下的姿态估计性能。

Abstract: Robust 6D object pose estimation in cluttered or occluded conditions using
monocular RGB images remains a challenging task. One reason is that current
pose estimation networks struggle to extract discriminative, pose-aware
features using 2D feature backbones, especially when the available RGB
information is limited due to target occlusion in cluttered scenes. To mitigate
this, we propose a novel pose estimation-specific pre-training strategy named
Mask6D. Our approach incorporates pose-aware 2D-3D correspondence maps and
visible mask maps as additional modal information, which is combined with RGB
images for the reconstruction-based model pre-training. Essentially, this 2D-3D
correspondence maps a transformed 3D object model to 2D pixels, reflecting the
pose information of the target in camera coordinate system. Meanwhile, the
integrated visible mask map can effectively guide our model to disregard
cluttered background information. In addition, an object-focused pre-training
loss function is designed to further facilitate our network to remove the
background interference. Finally, we fine-tune our pre-trained pose prior-aware
network via conventional pose training strategy to realize the reliable pose
prediction. Extensive experiments verify that our method outperforms previous
end-to-end pose estimation methods.

</details>


### [16] [Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection](https://arxiv.org/abs/2507.06510)
*Yupeng Hu,Changxing Ding,Chang Sun,Shaoli Huang,Xiangmin Xu*

Main category: cs.CV

TL;DR: 提出了一种双边协作框架（BC-HOI），用于开放词汇的人-物交互（HOI）检测，通过注意力偏置引导（ABG）和基于大语言模型（LLM）的监督引导（LSG）提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖视觉语言模型（VLM）生成的整体粗粒度特征，与检测任务的细粒度需求矛盾。

Method: BC-HOI框架包含ABG（引导VLM生成细粒度特征）和LSG（利用LLM提供细粒度监督）。

Result: 在HICO-DET和V-COCO基准测试中表现优异，开放和封闭词汇设置均优于现有方法。

Conclusion: BC-HOI通过双边协作解决了开放词汇HOI检测的细粒度特征问题，性能显著提升。

Abstract: Open vocabulary Human-Object Interaction (HOI) detection is a challenging
task that detects all <human, verb, object> triplets of interest in an image,
even those that are not pre-defined in the training set. Existing approaches
typically rely on output features generated by large Vision-Language Models
(VLMs) to enhance the generalization ability of interaction representations.
However, the visual features produced by VLMs are holistic and coarse-grained,
which contradicts the nature of detection tasks. To address this issue, we
propose a novel Bilateral Collaboration framework for open vocabulary HOI
detection (BC-HOI). This framework includes an Attention Bias Guidance (ABG)
component, which guides the VLM to produce fine-grained instance-level
interaction features according to the attention bias provided by the HOI
detector. It also includes a Large Language Model (LLM)-based Supervision
Guidance (LSG) component, which provides fine-grained token-level supervision
for the HOI detector by the LLM component of the VLM. LSG enhances the ability
of ABG to generate high-quality attention bias. We conduct extensive
experiments on two popular benchmarks: HICO-DET and V-COCO, consistently
achieving superior performance in the open vocabulary and closed settings. The
code will be released in Github.

</details>


### [17] [Capturing Stable HDR Videos Using a Dual-Camera System](https://arxiv.org/abs/2507.06593)
*Qianyu Zhang,Bolun Zheng,Hangjia Pan,Lingyu Zhu,Zunjie Zhu,Zongpeng Li,Shiqi Wang*

Main category: cs.CV

TL;DR: 论文提出了一种双摄像头系统（DCS）用于HDR视频重建，通过曝光自适应融合网络（EAFNet）解决参考图像曝光波动导致的闪烁问题，取得了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决交替曝光方法中参考图像曝光波动导致的HDR视频闪烁问题。

Method: 采用双摄像头系统（DCS），一个摄像头捕获稳定参考序列，另一个补充信息；提出EAFNet网络，包含预对齐子网络、非对称跨特征融合子网络和重建子网络。

Result: 在多个数据集上实现了最先进的性能，验证了DCS在HDR视频重建中的潜力。

Conclusion: DCS和EAFNet的组合有效解决了HDR视频重建中的闪烁问题，具有广泛应用前景。

Abstract: In HDR video reconstruction, exposure fluctuations in reference images from
alternating exposure methods often result in flickering. To address this issue,
we propose a dual-camera system (DCS) for HDR video acquisition, where one
camera is assigned to capture consistent reference sequences, while the other
is assigned to capture non-reference sequences for information supplementation.
To tackle the challenges posed by video data, we introduce an exposure-adaptive
fusion network (EAFNet) to achieve more robust results. EAFNet introduced a
pre-alignment subnetwork to explore the influence of exposure, selectively
emphasizing the valuable features across different exposure levels. Then, the
enhanced features are fused by the asymmetric cross-feature fusion subnetwork,
which explores reference-dominated attention maps to improve image fusion by
aligning cross-scale features and performing cross-feature fusion. Finally, the
reconstruction subnetwork adopts a DWT-based multiscale architecture to reduce
ghosting artifacts and refine features at different resolutions. Extensive
experimental evaluations demonstrate that the proposed method achieves
state-of-the-art performance on different datasets, validating the great
potential of the DCS in HDR video reconstruction. The codes and data captured
by DCS will be available at https://github.com/zqqqyu/DCS.

</details>


### [18] [What Demands Attention in Urban Street Scenes? From Scene Understanding towards Road Safety: A Survey of Vision-driven Datasets and Studies](https://arxiv.org/abs/2507.06513)
*Yaoqi Huang,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.CV

TL;DR: 该论文提出了一种系统分类法，将交通场景中的关键元素分为异常和正常但关键的实体，分析了35个视觉驱动任务和73个数据集，并提供了统一的分析框架。


<details>
  <summary>Details</summary>
Motivation: 利用视觉传感器和计算机视觉算法的进步，提升道路安全，填补现有调查中孤立领域的不足。

Method: 通过分类法将交通实体分为异常和正常但关键的两大类，整合10个类别和20个子类，分析任务和数据集。

Result: 建立了跨领域的统一框架，分析了35个任务和73个数据集，总结了优缺点和潜在解决方案。

Conclusion: 该分类法和综合分析为研究者提供了全面视角，指导资源选择并指出研究空白。

Abstract: Advances in vision-based sensors and computer vision algorithms have
significantly improved the analysis and understanding of traffic scenarios. To
facilitate the use of these improvements for road safety, this survey
systematically categorizes the critical elements that demand attention in
traffic scenarios and comprehensively analyzes available vision-driven tasks
and datasets. Compared to existing surveys that focus on isolated domains, our
taxonomy categorizes attention-worthy traffic entities into two main groups
that are anomalies and normal but critical entities, integrating ten categories
and twenty subclasses. It establishes connections between inherently related
fields and provides a unified analytical framework. Our survey highlights the
analysis of 35 vision-driven tasks and comprehensive examinations and
visualizations of 73 available datasets based on the proposed taxonomy. The
cross-domain investigation covers the pros and cons of each benchmark with the
aim of providing information on standards unification and resource
optimization. Our article concludes with a systematic discussion of the
existing weaknesses, underlining the potential effects and promising solutions
from various perspectives. The integrated taxonomy, comprehensive analysis, and
recapitulatory tables serve as valuable contributions to this rapidly evolving
field by providing researchers with a holistic overview, guiding strategic
resource selection, and highlighting critical research gaps.

</details>


### [19] [GreenHyperSpectra: A multi-source hyperspectral dataset for global vegetation trait prediction](https://arxiv.org/abs/2507.06806)
*Eya Cherif,Arthur Ouaknine,Luke A. Brown,Phuong D. Dao,Kyle R. Kovach,Bing Lu,Daniel Mederer,Hannes Feilhauer,Teja Kattenborn,David Rolnick*

Main category: cs.CV

TL;DR: GreenHyperSpectra数据集通过半监督和自监督方法，解决了植物性状预测中标签稀缺和领域转移问题，提升了多输出回归模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统野外采样无法覆盖生态尺度上的植物性状变异，而机器学习结合遥感高光谱数据提供了解决方案，但面临标签稀缺和领域转移的挑战。

Method: 提出GreenHyperSpectra数据集，采用半监督和自监督方法，评估框架包括分布内和分布外场景。

Result: 预训练的标签高效多输出回归模型优于现有监督基线，显著提升了光谱表征学习能力。

Conclusion: GreenHyperSpectra为植物功能性状评估与表征学习的交叉研究提供了方法论框架和数据支持。

Abstract: Plant traits such as leaf carbon content and leaf mass are essential
variables in the study of biodiversity and climate change. However,
conventional field sampling cannot feasibly cover trait variation at
ecologically meaningful spatial scales. Machine learning represents a valuable
solution for plant trait prediction across ecosystems, leveraging hyperspectral
data from remote sensing. Nevertheless, trait prediction from hyperspectral
data is challenged by label scarcity and substantial domain shifts (\eg across
sensors, ecological distributions), requiring robust cross-domain methods.
Here, we present GreenHyperSpectra, a pretraining dataset encompassing
real-world cross-sensor and cross-ecosystem samples designed to benchmark trait
prediction with semi- and self-supervised methods. We adopt an evaluation
framework encompassing in-distribution and out-of-distribution scenarios. We
successfully leverage GreenHyperSpectra to pretrain label-efficient
multi-output regression models that outperform the state-of-the-art supervised
baseline. Our empirical analyses demonstrate substantial improvements in
learning spectral representations for trait prediction, establishing a
comprehensive methodological framework to catalyze research at the intersection
of representation learning and plant functional traits assessment. All code and
data are available at: https://github.com/echerif18/HyspectraSSL.

</details>


### [20] [Hallucinating 360°: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting](https://arxiv.org/abs/2507.06971)
*Fei Teng,Kai Luo,Sheng Wu,Siyu Li,Pujun Guo,Jiale Wei,Kunyu Peng,Jiaming Zhang,Kailun Yang*

Main category: cs.CV

TL;DR: 论文提出了一种名为Percep360的全景生成方法，用于自动驾驶，解决了现有方法在数据分布固定和可控性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要全景感知，但数据采集和标注复杂耗时。现有生成模型无法实现高质量、可控的全景生成。

Method: 提出Local Scenes Diffusion Method (LSDM)解决信息丢失问题，以及Probabilistic Prompting Method (PPM)实现可控生成。

Result: 生成图像在无参考质量指标上优于原始拼接图像，并提升了下游感知模型性能。

Conclusion: Percep360在连贯性和可控性上表现优异，为自动驾驶全景数据生成提供了有效解决方案。

Abstract: Panoramic perception holds significant potential for autonomous driving,
enabling vehicles to acquire a comprehensive 360{\deg} surround view in a
single shot. However, autonomous driving is a data-driven task. Complete
panoramic data acquisition requires complex sampling systems and annotation
pipelines, which are time-consuming and labor-intensive. Although existing
street view generation models have demonstrated strong data regeneration
capabilities, they can only learn from the fixed data distribution of existing
datasets and cannot achieve high-quality, controllable panoramic generation. In
this paper, we propose the first panoramic generation method Percep360 for
autonomous driving. Percep360 enables coherent generation of panoramic data
with control signals based on the stitched panoramic data. Percep360 focuses on
two key aspects: coherence and controllability. Specifically, to overcome the
inherent information loss caused by the pinhole sampling process, we propose
the Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama
generation as a spatially continuous diffusion process, bridging the gaps
between different data distributions. Additionally, to achieve the controllable
generation of panoramic images, we propose a Probabilistic Prompting Method
(PPM). PPM dynamically selects the most relevant control cues, enabling
controllable panoramic image generation. We evaluate the effectiveness of the
generated images from three perspectives: image quality assessment (i.e.,
no-reference and with reference), controllability, and their utility in
real-world Bird's Eye View (BEV) segmentation. Notably, the generated data
consistently outperforms the original stitched images in no-reference quality
metrics and enhances downstream perception models. The source code will be
publicly available at https://github.com/Bryant-Teng/Percep360.

</details>


### [21] [Concept Unlearning by Modeling Key Steps of Diffusion Process](https://arxiv.org/abs/2507.06526)
*Chaoshuo Zhang,Chenhao Lin,Zhengyu Zhao,Le Yang,Qian Wang,Chao Shen*

Main category: cs.CV

TL;DR: 提出了一种名为KSCU的新方法，通过针对扩散模型的关键步骤进行概念遗忘，平衡了遗忘效果与生成能力的保留。


<details>
  <summary>Details</summary>
Motivation: 现有的概念遗忘方法难以平衡遗忘效果与生成能力的保留，导致文本到图像扩散模型的安全风险。

Method: KSCU方法利用扩散模型的逐步采样特性，专注于对最终结果影响最大的关键步骤，仅在这些步骤上微调模型。

Result: 实验证明，KSCU能有效防止生成不良图像，同时更好地保留模型的生成能力。

Conclusion: KSCU为文本到图像扩散模型的安全使用提供了一种高效且有针对性的解决方案。

Abstract: Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion,
which generate highly realistic images based on textual input, have been widely
used. However, their misuse poses serious security risks. While existing
concept unlearning methods aim to mitigate these risks, they struggle to
balance unlearning effectiveness with generative retainability.To overcome this
limitation, we innovatively propose the Key Step Concept Unlearning (KSCU)
method, which ingeniously capitalizes on the unique stepwise sampling
characteristic inherent in diffusion models during the image generation
process. Unlike conventional approaches that treat all denoising steps equally,
KSCU strategically focuses on pivotal steps with the most influence over the
final outcome by dividing key steps for different concept unlearning tasks and
fine-tuning the model only at those steps. This targeted approach reduces the
number of parameter updates needed for effective unlearning, while maximizing
the retention of the model's generative capabilities.Through extensive
benchmark experiments, we demonstrate that KSCU effectively prevents T2I DMs
from generating undesirable images while better retaining the model's
generative capabilities.Our code will be released.

</details>


### [22] [4KAgent: Agentic Any Image to 4K Super-Resolution](https://arxiv.org/abs/2507.07105)
*Yushen Zuo,Qi Zheng,Mingyang Wu,Xinrui Jiang,Renjie Li,Jian Wang,Yide Zhang,Gengchen Mai,Lihong V. Wang,James Zou,Xiaoyu Wang,Ming-Hsuan Yang,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 4KAgent是一个统一的超分辨率通用系统，能将任何图像提升至4K分辨率，甚至更高。它通过三个核心组件实现：定制化流程的Profiling模块、分析输入图像的Perception Agent，以及执行修复的Restoration Agent。系统在多个领域表现优异，并开源代码和模型。


<details>
  <summary>Details</summary>
Motivation: 解决低分辨率图像修复的通用性问题，尤其是极端低分辨率（如256x256）图像的清晰化需求。

Method: 系统包含Profiling、Perception Agent和Restoration Agent三个模块，采用递归执行-反思范式和质量驱动的专家混合策略。

Result: 在11个任务类别和26个基准测试中表现优异，覆盖自然图像、医学影像等多个领域，并在感知和保真度指标上领先。

Conclusion: 4KAgent为低层次视觉任务建立了新的代理范式，推动了视觉自主代理的广泛研究和创新。

Abstract: We present 4KAgent, a unified agentic super-resolution generalist system
designed to universally upscale any image to 4K resolution (and even higher, if
applied iteratively). Our system can transform images from extremely low
resolutions with severe degradations, for example, highly distorted inputs at
256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three
core components: (1) Profiling, a module that customizes the 4KAgent pipeline
based on bespoke use cases; (2) A Perception Agent, which leverages
vision-language models alongside image quality assessment experts to analyze
the input image and make a tailored restoration plan; and (3) A Restoration
Agent, which executes the plan, following a recursive execution-reflection
paradigm, guided by a quality-driven mixture-of-expert policy to select the
optimal output for each step. Additionally, 4KAgent embeds a specialized face
restoration pipeline, significantly enhancing facial details in portrait and
selfie photos. We rigorously evaluate our 4KAgent across 11 distinct task
categories encompassing a total of 26 diverse benchmarks, setting new
state-of-the-art on a broad spectrum of imaging domains. Our evaluations cover
natural images, portrait photos, AI-generated content, satellite imagery,
fluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and
X-ray, demonstrating superior performance in terms of both perceptual (e.g.,
NIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic
paradigm for low-level vision tasks, we aim to catalyze broader interest and
innovation within vision-centric autonomous agents across diverse research
communities. We will release all the code, models, and results at:
https://4kagent.github.io.

</details>


### [23] [Speak2Sign3D: A Multi-modal Pipeline for English Speech to American Sign Language Animation](https://arxiv.org/abs/2507.06530)
*Kazi Mahathir Rahman,Naveed Imtiaz Nafis,Md. Farhan Sadik,Mohammad Al Rafi,Mehedi Hasan Shahed*

Main category: cs.CV

TL;DR: 本文提出了一种完整的流程，将英语语音转化为流畅的3D手语动画，结合语音识别、文本翻译和动作生成技术。


<details>
  <summary>Details</summary>
Motivation: 帮助聋哑和听力障碍者更轻松地沟通，填补了从英语语音到手语动画转换的研究空白。

Method: 使用Whisper将语音转为文本，MarianMT模型将文本翻译为ASL gloss，结合Word2Vec和FastText优化翻译，并通过3D关键点系统生成动画。

Result: 系统表现优异，BLEU分数达0.7714和0.8923，并创建了新数据集BookGlossCorpus-CG和Sign3D-WLASL。

Conclusion: 该框架整合了音频、文本和动作，实现了从英语语音到逼真3D手语动画的全流程转换。

Abstract: Helping deaf and hard-of-hearing people communicate more easily is the main
goal of Automatic Sign Language Translation. Although most past research has
focused on turning sign language into text, doing the reverse, turning spoken
English into sign language animations, has been largely overlooked. That's
because it involves multiple steps, such as understanding speech, translating
it into sign-friendly grammar, and generating natural human motion. In this
work, we introduce a complete pipeline that converts English speech into
smooth, realistic 3D sign language animations. Our system starts with Whisper
to translate spoken English into text. Then, we use a MarianMT machine
translation model to translate that text into American Sign Language (ASL)
gloss, a simplified version of sign language that captures meaning without
grammar. This model performs well, reaching BLEU scores of 0.7714 and 0.8923.
To make the gloss translation more accurate, we also use word embeddings such
as Word2Vec and FastText to understand word meanings. Finally, we animate the
translated gloss using a 3D keypoint-based motion system trained on
Sign3D-WLASL, a dataset we created by extracting body, hand, and face key
points from real ASL videos in the WLASL dataset. To support the gloss
translation stage, we also built a new dataset called BookGlossCorpus-CG, which
turns everyday English sentences from the BookCorpus dataset into ASL gloss
using grammar rules. Our system stitches everything together by smoothly
interpolating between signs to create natural, continuous animations. Unlike
previous works like How2Sign and Phoenix-2014T that focus on recognition or use
only one type of data, our pipeline brings together audio, text, and motion in
a single framework that goes all the way from spoken English to lifelike 3D
sign language animation.

</details>


### [24] [ILNet: Trajectory Prediction with Inverse Learning Attention for Enhancing Intention Capture](https://arxiv.org/abs/2507.06531)
*Mingjin Zeng,Nan Ouyang,Wenkang Wan,Lei Ao,Qing Cai,Kai Sheng*

Main category: cs.CV

TL;DR: ILNet提出了一种基于逆向学习注意力机制和动态锚点选择的多智能体轨迹预测方法，显著提升了复杂交互场景下的预测性能。


<details>
  <summary>Details</summary>
Motivation: 受人类驾驶行为的启发，旨在解决现有方法在时空协调和适应性方面的不足。

Method: 结合逆向学习注意力机制（IL Attention）和动态锚点选择模块（DAS），动态建模交互意图并优化轨迹预测。

Result: 在INTERACTION和Argoverse数据集上达到最优性能，尤其在复杂交互场景中表现突出。

Conclusion: ILNet通过动态建模和优化策略，显著提升了轨迹预测的准确性和多模态分布能力。

Abstract: Trajectory prediction for multi-agent interaction scenarios is a crucial
challenge. Most advanced methods model agent interactions by efficiently
factorized attention based on the temporal and agent axes. However, this static
and foward modeling lacks explicit interactive spatio-temporal coordination,
capturing only obvious and immediate behavioral intentions. Alternatively, the
modern trajectory prediction framework refines the successive predictions by a
fixed-anchor selection strategy, which is difficult to adapt in different
future environments. It is acknowledged that human drivers dynamically adjust
initial driving decisions based on further assumptions about the intentions of
surrounding vehicles. Motivated by human driving behaviors, this paper proposes
ILNet, a multi-agent trajectory prediction method with Inverse Learning (IL)
attention and Dynamic Anchor Selection (DAS) module. IL Attention employs an
inverse learning paradigm to model interactions at neighboring moments,
introducing proposed intentions to dynamically encode the spatio-temporal
coordination of interactions, thereby enhancing the model's ability to capture
complex interaction patterns. Then, the learnable DAS module is proposed to
extract multiple trajectory change keypoints as anchors in parallel with almost
no increase in parameters. Experimental results show that the ILNet achieves
state-of-the-art performance on the INTERACTION and Argoverse motion
forecasting datasets. Particularly, in challenged interaction scenarios, ILNet
achieves higher accuracy and more multimodal distributions of trajectories over
fewer parameters. Our codes are available at https://github.com/mjZeng11/ILNet.

</details>


### [25] [A model-agnostic active learning approach for animal detection from camera traps](https://arxiv.org/abs/2507.06537)
*Thi Thu Thuy Nguyen,Duc Thanh Nguyen*

Main category: cs.CV

TL;DR: 提出了一种模型无关的主动学习方法，用于优化野生动物相机陷阱数据的标注，通过结合样本的不确定性和多样性，仅需30%的训练数据即可达到或超越完整数据集的检测性能。


<details>
  <summary>Details</summary>
Motivation: 野生动物相机陷阱数据量大且标注成本高，现有主动学习方法需要完全访问模型，限制了其应用。

Method: 提出了一种模型无关的主动学习方法，结合对象和图像层面的不确定性和多样性进行样本选择。

Result: 实验表明，仅使用30%的训练数据，动物检测器的性能即可达到或超过完整数据集的效果。

Conclusion: 该方法为野生动物监测和保护提供了一种高效的数据标注和模型训练解决方案。

Abstract: Smart data selection is becoming increasingly important in data-driven
machine learning. Active learning offers a promising solution by allowing
machine learning models to be effectively trained with optimal data including
the most informative samples from large datasets. Wildlife data captured by
camera traps are excessive in volume, requiring tremendous effort in data
labelling and animal detection models training. Therefore, applying active
learning to optimise the amount of labelled data would be a great aid in
enabling automated wildlife monitoring and conservation. However, existing
active learning techniques require that a machine learning model (i.e., an
object detector) be fully accessible, limiting the applicability of the
techniques. In this paper, we propose a model-agnostic active learning approach
for detection of animals captured by camera traps. Our approach integrates
uncertainty and diversity quantities of samples at both the object-based and
image-based levels into the active learning sample selection process. We
validate our approach in a benchmark animal dataset. Experimental results
demonstrate that, using only 30% of the training data selected by our approach,
a state-of-the-art animal detector can achieve a performance of equal or
greater than that with the use of the complete training dataset.

</details>


### [26] [Token Bottleneck: One Token to Remember Dynamics](https://arxiv.org/abs/2507.06543)
*Taekyung Kim,Dongyoon Han,Byeongho Heo,Jeongeun Park,Sangdoo Yun*

Main category: cs.CV

TL;DR: ToBo是一种自监督学习框架，通过压缩场景为瓶颈令牌并预测后续场景，学习动态场景的紧凑表示。


<details>
  <summary>Details</summary>
Motivation: 动态场景的紧凑和时间感知表示对视觉跟踪和机器人操作等任务至关重要。

Method: ToBo通过压缩步骤将参考场景编码为瓶颈令牌，在扩展步骤中利用少量目标补丁预测目标场景。

Result: 在视频标签传播和机器人操作等任务中，ToBo优于基线方法，并在真实环境中验证了其鲁棒性。

Conclusion: ToBo能够有效捕捉时间动态，适用于不同规模的模型，具有实际应用的潜力。

Abstract: Deriving compact and temporally aware visual representations from dynamic
scenes is essential for successful execution of sequential scene understanding
tasks such as visual tracking and robotic manipulation. In this paper, we
introduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised
learning pipeline that squeezes a scene into a bottleneck token and predicts
the subsequent scene using minimal patches as hints. The ToBo pipeline
facilitates the learning of sequential scene representations by conservatively
encoding the reference scene into a compact bottleneck token during the squeeze
step. In the expansion step, we guide the model to capture temporal dynamics by
predicting the target scene using the bottleneck token along with few target
patches as hints. This design encourages the vision backbone to embed temporal
dependencies, thereby enabling understanding of dynamic transitions across
scenes. Extensive experiments in diverse sequential tasks, including video
label propagation and robot manipulation in simulated environments demonstrate
the superiority of ToBo over baselines. Moreover, deploying our pre-trained
model on physical robots confirms its robustness and effectiveness in
real-world environments. We further validate the scalability of ToBo across
different model scales.

</details>


### [27] [Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution](https://arxiv.org/abs/2507.06547)
*Yonghyun Park,Chieh-Hsin Lai,Satoshi Hayakawa,Yuhta Takida,Naoki Murata,Wei-Hsiang Liao,Woosung Choi,Kin Wai Cheuk,Junghyun Koo,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: 论文提出了一种名为Concept-TRAK的新方法，用于解决扩散模型在图像生成中的版权和透明度问题，通过概念级归因提供更细粒度的分析。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中表现优异，但其广泛使用引发了版权和模型透明度的担忧。现有方法无法针对图像中的特定元素（如风格或对象）进行归因分析。

Method: 提出了Concept-TRAK方法，扩展了影响函数，包括基于扩散后验采样的训练损失重构和强调语义相关性的概念感知奖励函数。

Result: 在AbC基准测试中，Concept-TRAK显著优于现有方法，并通过案例研究展示了其在保护知识产权和内容安全等方面的实际应用。

Conclusion: Concept-TRAK为生成式AI的负责任开发和治理提供了可操作的见解。

Abstract: While diffusion models excel at image generation, their growing adoption
raises critical concerns around copyright issues and model transparency.
Existing attribution methods identify training examples influencing an entire
image, but fall short in isolating contributions to specific elements, such as
styles or objects, that matter most to stakeholders. To bridge this gap, we
introduce \emph{concept-level attribution} via a novel method called
\emph{Concept-TRAK}. Concept-TRAK extends influence functions with two key
innovations: (1) a reformulated diffusion training loss based on diffusion
posterior sampling, enabling robust, sample-specific attribution; and (2) a
concept-aware reward function that emphasizes semantic relevance. We evaluate
Concept-TRAK on the AbC benchmark, showing substantial improvements over prior
methods. Through diverse case studies--ranging from identifying IP-protected
and unsafe content to analyzing prompt engineering and compositional
learning--we demonstrate how concept-level attribution yields actionable
insights for responsible generative AI development and governance.

</details>


### [28] [Divergence-Based Similarity Function for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06560)
*Jae Hyoung Jeon,Cheolsu Lim,Myungjoo Kang*

Main category: cs.CV

TL;DR: 提出了一种基于分布散度的相似性函数（DSF），通过将多视图表示为分布并测量分布间的散度来显式捕捉联合结构，提升了性能并提高了效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要捕捉成对关系，未能建模所有视图的联合结构，因此需要更有效的方法。

Method: 提出DSF，将多视图表示为分布，通过测量分布间的散度来建模联合结构。

Result: DSF在kNN分类和线性评估等任务中表现优异，效率更高且无需温度超参数。

Conclusion: DSF通过显式建模联合结构，显著提升了多视图对比学习的性能与效率。

Abstract: Recent success in contrastive learning has sparked growing interest in more
effectively leveraging multiple augmented views of an instance. While prior
methods incorporate multiple views at the loss or feature level, they primarily
capture pairwise relationships and fail to model the joint structure across all
views. In this work, we propose a divergence-based similarity function (DSF)
that explicitly captures the joint structure by representing each set of
augmented views as a distribution and measuring similarity as the divergence
between distributions. Extensive experiments demonstrate that DSF consistently
improves performance across various tasks, including kNN classification and
linear evaluation, while also offering greater efficiency compared to other
multi-view methods. Furthermore, we establish a theoretical connection between
DSF and cosine similarity, and show that, unlike cosine similarity, DSF
operates effectively without requiring a temperature hyperparameter.

</details>


### [29] [Edge-Boundary-Texture Loss: A Tri-Class Generalization of Weighted Binary Cross-Entropy for Enhanced Edge Detection](https://arxiv.org/abs/2507.06569)
*Hao Shu*

Main category: cs.CV

TL;DR: 提出了一种新的损失函数EBT，通过将像素分为边缘、边界和纹理三类，优化边缘检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统WBCE损失函数对所有非边缘像素一视同仁，忽略了边界附近的结构差异，导致预测模糊。

Method: 提出EBT损失函数，将像素分为边缘、边界和纹理三类，并为每类分配不同的监督权重。

Result: 在多个基准测试中，EBT损失函数在定量和感知上均优于WBCE，且超参数鲁棒性强。

Conclusion: EBT损失函数易于部署，性能优越，是边缘检测任务的有效改进。

Abstract: Edge detection (ED) remains a fundamental task in computer vision, yet its
performance is often hindered by the ambiguous nature of non-edge pixels near
object boundaries. The widely adopted Weighted Binary Cross-Entropy (WBCE) loss
treats all non-edge pixels uniformly, overlooking the structural nuances around
edges and often resulting in blurred predictions. In this paper, we propose the
Edge-Boundary-Texture (EBT) loss, a novel objective that explicitly divides
pixels into three categories, edge, boundary, and texture, and assigns each a
distinct supervisory weight. This tri-class formulation enables more structured
learning by guiding the model to focus on both edge precision and contextual
boundary localization. We theoretically show that the EBT loss generalizes the
WBCE loss, with the latter becoming a limit case. Extensive experiments across
multiple benchmarks demonstrate the superiority of the EBT loss both
quantitatively and perceptually. Furthermore, the consistent use of unified
hyperparameters across all models and datasets, along with robustness to their
moderate variations, indicates that the EBT loss requires minimal fine-tuning
and is easily deployable in practice.

</details>


### [30] [MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction](https://arxiv.org/abs/2507.06590)
*Yin Wang,Mu li,Zhiying Leng,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.CV

TL;DR: MOST是一种新颖的运动扩散模型，通过时间片段Banzhaf交互解决从罕见语言提示生成人类运动的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在粗粒度匹配和忽略语义线索上的不足，利用细粒度片段关系改进文本到运动的生成。

Method: 采用时间片段Banzhaf交互量化文本-运动一致性，结合运动提示模块生成语义一致的运动。

Result: MOST在文本到运动检索和生成上达到最先进水平，尤其在罕见提示上表现突出。

Conclusion: MOST通过细粒度片段关系和创新的交互方法，显著提升了文本到运动生成的性能。

Abstract: We introduce MOST, a novel motion diffusion model via temporal clip Banzhaf
interaction, aimed at addressing the persistent challenge of generating human
motion from rare language prompts. While previous approaches struggle with
coarse-grained matching and overlook important semantic cues due to motion
redundancy, our key insight lies in leveraging fine-grained clip relationships
to mitigate these issues. MOST's retrieval stage presents the first formulation
of its kind - temporal clip Banzhaf interaction - which precisely quantifies
textual-motion coherence at the clip level. This facilitates direct,
fine-grained text-to-motion clip matching and eliminates prevalent redundancy.
In the generation stage, a motion prompt module effectively utilizes retrieved
motion clips to produce semantically consistent movements. Extensive
evaluations confirm that MOST achieves state-of-the-art text-to-motion
retrieval and generation performance by comprehensively addressing previous
challenges, as demonstrated through quantitative and qualitative results
highlighting its effectiveness, especially for rare prompts.

</details>


### [31] [Ambiguity-aware Point Cloud Segmentation by Adaptive Margin Contrastive Learning](https://arxiv.org/abs/2507.06592)
*Yang Chen,Yueqi Duan,Haowen Sun,Jiwen Lu,Yap-Peng Tan*

Main category: cs.CV

TL;DR: 提出了一种自适应边界对比学习方法（AMContrast3D和AMContrast3D++），用于点云的3D语义分割，通过模糊度估计优化模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法对模糊区域的点采用均等惩罚目标，导致模型性能不佳。人类标注的模糊点标签不可靠，硬约束会降低模型效果。

Method: 设计AMContrast3D，将对比学习融入模糊度估计框架，为不同模糊度的点分配自适应目标；进一步提出AMContrast3D++，通过并行分支和模糊预测模块优化嵌入。

Result: 在S3DIS和ScanNet数据集上验证了方法的有效性，提升了分割性能和鲁棒性。

Conclusion: 自适应模糊度估计和对比学习结合的方法显著改善了3D语义分割效果。

Abstract: This paper proposes an adaptive margin contrastive learning method for 3D
semantic segmentation on point clouds. Most existing methods use equally
penalized objectives, which ignore the per-point ambiguities and less
discriminated features stemming from transition regions. However, as highly
ambiguous points may be indistinguishable even for humans, their manually
annotated labels are less reliable, and hard constraints over these points
would lead to sub-optimal models. To address this, we first design
AMContrast3D, a method comprising contrastive learning into an ambiguity
estimation framework, tailored to adaptive objectives for individual points
based on ambiguity levels. As a result, our method promotes model training,
which ensures the correctness of low-ambiguity points while allowing mistakes
for high-ambiguity points. As ambiguities are formulated based on position
discrepancies across labels, optimization during inference is constrained by
the assumption that all unlabeled points are uniformly unambiguous, lacking
ambiguity awareness. Inspired by the insight of joint training, we further
propose AMContrast3D++ integrating with two branches trained in parallel, where
a novel ambiguity prediction module concurrently learns point ambiguities from
generated embeddings. To this end, we design a masked refinement mechanism that
leverages predicted ambiguities to enable the ambiguous embeddings to be more
reliable, thereby boosting segmentation performance and enhancing robustness.
Experimental results on 3D indoor scene datasets, S3DIS and ScanNet,
demonstrate the effectiveness of the proposed method. Code is available at
https://github.com/YangChenApril/AMContrast3D.

</details>


### [32] [Cross-Modal Dual-Causal Learning for Long-Term Action Recognition](https://arxiv.org/abs/2507.06603)
*Xu Shaowu,Jia Xibin,Gao Junyu,Sun Qianmei,Chang Jing,Fan Chao*

Main category: cs.CV

TL;DR: 本文提出了一种名为CMDCL的跨模态双因果学习方法，用于解决长时动作识别中的跨模态偏差和视觉混淆问题。


<details>
  <summary>Details</summary>
Motivation: 长时动作识别（LTAR）因时间跨度长、动作关联复杂以及视觉混淆问题而具有挑战性。现有方法多依赖统计相关性而非因果机制，且缺乏跨模态因果建模。

Method: CMDCL通过结构因果模型揭示视频与标签文本间的因果关系，并通过文本因果干预和视觉因果干预消除跨模态偏差和视觉混淆。

Result: 在Charades、Breakfast和COIN三个基准测试中验证了CMDCL的有效性。

Conclusion: CMDCL通过双因果干预实现了鲁棒的动作表示，为长时动作识别提供了有效解决方案。

Abstract: Long-term action recognition (LTAR) is challenging due to extended temporal
spans with complex atomic action correlations and visual confounders. Although
vision-language models (VLMs) have shown promise, they often rely on
statistical correlations instead of causal mechanisms. Moreover, existing
causality-based methods address modal-specific biases but lack cross-modal
causal modeling, limiting their utility in VLM-based LTAR. This paper proposes
\textbf{C}ross-\textbf{M}odal \textbf{D}ual-\textbf{C}ausal \textbf{L}earning
(CMDCL), which introduces a structural causal model to uncover causal
relationships between videos and label texts.
  CMDCL addresses cross-modal biases in text embeddings via textual causal
intervention and removes confounders inherent in the visual modality through
visual causal intervention guided by the debiased text.
  These dual-causal interventions enable robust action representations to
address LTAR challenges. Experimental results on three benchmarks including
Charades, Breakfast and COIN, demonstrate the effectiveness of the proposed
model. Our code is available at https://github.com/xushaowu/CMDCL.

</details>


### [33] [Omni-Fusion of Spatial and Spectral for Hyperspectral Image Segmentation](https://arxiv.org/abs/2507.06606)
*Qing Zhang,Guoquan Pei,Yan Wang*

Main category: cs.CV

TL;DR: 提出了一种名为Omni-Fuse的新型空间-光谱全融合网络，用于高光谱图像分割，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学高光谱成像（MHSI）在疾病诊断中潜力巨大，但高维度和光谱冗余特性使其空间和光谱信息融合具有挑战性。

Method: 设计了跨维度特征融合操作，包括双向注意力机制、光谱引导的空间查询选择和两阶段跨维度解码器。

Result: 在两个高光谱图像数据集上的实验表明，该方法在DSC指标上比现有方法提升了5.73%。

Conclusion: Omni-Fuse通过高效的跨维度特征融合，显著提升了高光谱图像的分割性能。

Abstract: Medical Hyperspectral Imaging (MHSI) has emerged as a promising tool for
enhanced disease diagnosis, particularly in computational pathology, offering
rich spectral information that aids in identifying subtle biochemical
properties of tissues. Despite these advantages, effectively fusing both
spatial-dimensional and spectral-dimensional information from MHSIs remains
challenging due to its high dimensionality and spectral redundancy inherent
characteristics. To solve the above challenges, we propose a novel
spatial-spectral omni-fusion network for hyperspectral image segmentation,
named as Omni-Fuse. Here, we introduce abundant cross-dimensional feature
fusion operations, including a cross-dimensional enhancement module that
refines both spatial and spectral features through bidirectional attention
mechanisms, a spectral-guided spatial query selection to select the most
spectral-related spatial feature as the query, and a two-stage
cross-dimensional decoder which dynamically guide the model to focus on the
selected spatial query. Despite of numerous attention blocks, Omni-Fuse remains
efficient in execution. Experiments on two microscopic hyperspectral image
datasets show that our approach can significantly improve the segmentation
performance compared with the state-of-the-art methods, with over 5.73 percent
improvement in DSC. Code available at:
https://github.com/DeepMed-Lab-ECNU/Omni-Fuse.

</details>


### [34] [PointVDP: Learning View-Dependent Projection by Fireworks Rays for 3D Point Cloud Segmentation](https://arxiv.org/abs/2507.06618)
*Yang Chen,Yueqi Duan,Haowen Sun,Ziwei Wang,Jiwen Lu,Yap-Peng Tan*

Main category: cs.CV

TL;DR: 提出了一种基于视点依赖投影（VDP）的点云分割方法，通过动态适应视点变化生成高效3D到2D映射，解决了传统方法中投影多样性和计算效率不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统投影方法依赖预定义参数，无法适应不同视点变化，导致投影多样性不足且计算冗余。

Method: 设计了VDP框架，通过数据驱动生成自适应投影，并结合颜色正则化优化特征提取。

Result: 在S3DIS和ScanNet基准测试中表现优异，计算成本低。

Conclusion: PointVDP提供了一种资源高效的语义理解解决方案。

Abstract: In this paper, we propose view-dependent projection (VDP) to facilitate point
cloud segmentation, designing efficient 3D-to-2D mapping that dynamically
adapts to the spatial geometry from view variations. Existing projection-based
methods leverage view-independent projection in complex scenes, relying on
straight lines to generate direct rays or upward curves to reduce occlusions.
However, their view independence provides projection rays that are limited to
pre-defined parameters by human settings, restricting point awareness and
failing to capture sufficient projection diversity across different view
planes. Although multiple projections per view plane are commonly used to
enhance spatial variety, the projected redundancy leads to excessive
computational overhead and inefficiency in image processing. To address these
limitations, we design a framework of VDP to generate data-driven projections
from 3D point distributions, producing highly informative single-image inputs
by predicting rays inspired by the adaptive behavior of fireworks. In addition,
we construct color regularization to optimize the framework, which emphasizes
essential features within semantic pixels and suppresses the non-semantic
features within black pixels, thereby maximizing 2D space utilization in a
projected image. As a result, our approach, PointVDP, develops lightweight
projections in marginal computation costs. Experiments on S3DIS and ScanNet
benchmarks show that our approach achieves competitive results, offering a
resource-efficient solution for semantic understanding.

</details>


### [35] [EXAONE Path 2.0: Pathology Foundation Model with End-to-End Supervision](https://arxiv.org/abs/2507.06639)
*Myungjang Pyeon,Janghyeon Lee,Minsoo Lee,Juseung Yun,Hwanil Choi,Jonghyun Kim,Jiwon Kim,Yi Hu,Jongseong Jang,Soonyoung Lee*

Main category: cs.CV

TL;DR: EXAONE Path 2.0提出了一种病理学基础模型，通过直接利用切片级监督学习补丁级表示，解决了现有自监督学习方法在生物标志物预测中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法在数字病理学中可能忽略复杂的领域特定特征，且数据效率低，需要大量计算资源。

Method: EXAONE Path 2.0采用直接切片级监督学习补丁级表示，仅需37k全切片图像进行训练。

Result: 在10个生物标志物预测任务中，EXAONE Path 2.0实现了最先进的平均性能，表现出显著的数据效率。

Conclusion: EXAONE Path 2.0通过直接切片级监督学习，显著提升了生物标志物预测的性能和数据效率。

Abstract: In digital pathology, whole-slide images (WSIs) are often difficult to handle
due to their gigapixel scale, so most approaches train patch encoders via
self-supervised learning (SSL) and then aggregate the patch-level embeddings
via multiple instance learning (MIL) or slide encoders for downstream tasks.
However, patch-level SSL may overlook complex domain-specific features that are
essential for biomarker prediction, such as mutation status and molecular
characteristics, as SSL methods rely only on basic augmentations selected for
natural image domains on small patch-level area. Moreover, SSL methods remain
less data efficient than fully supervised approaches, requiring extensive
computational resources and datasets to achieve competitive performance. To
address these limitations, we present EXAONE Path 2.0, a pathology foundation
model that learns patch-level representations under direct slide-level
supervision. Using only 37k WSIs for training, EXAONE Path 2.0 achieves
state-of-the-art average performance across 10 biomarker prediction tasks,
demonstrating remarkable data efficiency.

</details>


### [36] [Learning from Sparse Point Labels for Dense Carcinosis Localization in Advanced Ovarian Cancer Assessment](https://arxiv.org/abs/2507.06643)
*Farahdiba Zarin,Riccardo Oliva,Vinkle Srivastav,Armine Vardazaryan,Andrea Rosati,Alice Zampolini Faustini,Giovanni Scambia,Anna Fagotti,Pietro Mascagni,Nicolas Padoy*

Main category: cs.CV

TL;DR: 论文提出了一种名为Crag and Tail的损失函数，用于从稀疏点标注中学习密集预测任务，特别是在医学图像中关键点定位的应用。


<details>
  <summary>Details</summary>
Motivation: 医学领域中标注成本高，尤其是密集像素级标注难以获取，因此需要从少量标注中学习的方法。

Method: 将问题建模为稀疏热图回归，并提出Crag and Tail损失函数，有效利用稀疏标注并减少假阴性影响。

Result: 通过消融实验验证了方法的有效性，能够准确实现关键点的密集定位。

Conclusion: 该方法在标注难以获取的场景中具有潜力，推动了相关研究进展。

Abstract: Learning from sparse labels is a challenge commonplace in the medical domain.
This is due to numerous factors, such as annotation cost, and is especially
true for newly introduced tasks. When dense pixel-level annotations are needed,
this becomes even more unfeasible. However, being able to learn from just a few
annotations at the pixel-level, while extremely difficult and underutilized,
can drive progress in studies where perfect annotations are not immediately
available. This work tackles the challenge of learning the dense prediction
task of keypoint localization from a few point annotations in the context of 2d
carcinosis keypoint localization from laparoscopic video frames for diagnostic
planning of advanced ovarian cancer patients. To enable this, we formulate the
problem as a sparse heatmap regression from a few point annotations per image
and propose a new loss function, called Crag and Tail loss, for efficient
learning. Our proposed loss function effectively leverages positive sparse
labels while minimizing the impact of false negatives or missed annotations.
Through an extensive ablation study, we demonstrate the effectiveness of our
approach in achieving accurate dense localization of carcinosis keypoints,
highlighting its potential to advance research in scenarios where dense
annotations are challenging to obtain.

</details>


### [37] [ClipGS: Clippable Gaussian Splatting for Interactive Cinematic Visualization of Volumetric Medical Data](https://arxiv.org/abs/2507.06647)
*Chengkun Li,Yuqi Tong,Kai Chen,Zhenya Yang,Ruiyang Li,Shi Qiu,Jason Ying-Kuen Chan,Pheng-Ann Heng,Qi Dou*

Main category: cs.CV

TL;DR: ClipGS是一个支持裁剪平面的高斯样条框架，用于医学体积数据的交互式电影渲染，解决了高计算成本和低渲染速度的问题。


<details>
  <summary>Details</summary>
Motivation: 提升医学体积数据的可视化质量，以增强诊断准确性和手术规划，同时解决现有方法的高计算成本和低交互性问题。

Method: 提出ClipGS框架，包括可学习的截断方案和自适应调整模型，动态调整高斯基元的可见性和变形。

Result: 在五种医学体积数据上验证，平均PSNR为36.635，帧率为156 FPS，模型大小为16.1 MB，优于现有方法。

Conclusion: ClipGS在渲染质量和效率上表现优异，适用于医学数据的交互式可视化。

Abstract: The visualization of volumetric medical data is crucial for enhancing
diagnostic accuracy and improving surgical planning and education. Cinematic
rendering techniques significantly enrich this process by providing
high-quality visualizations that convey intricate anatomical details, thereby
facilitating better understanding and decision-making in medical contexts.
However, the high computing cost and low rendering speed limit the requirement
of interactive visualization in practical applications. In this paper, we
introduce ClipGS, an innovative Gaussian splatting framework with the clipping
plane supported, for interactive cinematic visualization of volumetric medical
data. To address the challenges posed by dynamic interactions, we propose a
learnable truncation scheme that automatically adjusts the visibility of
Gaussian primitives in response to the clipping plane. Besides, we also design
an adaptive adjustment model to dynamically adjust the deformation of Gaussians
and refine the rendering performance. We validate our method on five volumetric
medical data (including CT and anatomical slice data), and reach an average
36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size,
outperforming state-of-the-art methods in rendering quality and efficiency.

</details>


### [38] [Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with Diffusion Prior](https://arxiv.org/abs/2507.06651)
*Juncheng Mu,Chengwei Ren,Weixiang Zhang,Liang Pan,Xiao-Ping Zhang,Yue Gao*

Main category: cs.CV

TL;DR: Diff$^2$I2P 是一种利用扩散模型先验的完全可微分图像到点云（I2P）配准框架，通过 Control-Side Score Distillation (CSD) 和 Deformable Correspondence Tuning (DCT) 模块，显著提升了跨模态特征学习和配准效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过度量学习实现跨模态特征对齐，但忽略了图像和点云数据之间的固有模态差距，导致跨模态对应关系不准确。

Method: 提出 Diff$^2$I2P 框架，结合 CSD 技术从扩散模型蒸馏知识优化变换预测，并引入 DCT 模块实现可微分的对应关系估计和变换求解。

Result: 在 7-Scenes 基准测试中，Diff$^2$I2P 显著优于现有方法，配准召回率提升超过 7%。

Conclusion: Diff$^2$I2P 通过扩散模型先验和可微分设计，有效解决了跨模态配准问题，提升了性能。

Abstract: Learning cross-modal correspondences is essential for image-to-point cloud
(I2P) registration. Existing methods achieve this mostly by utilizing metric
learning to enforce feature alignment across modalities, disregarding the
inherent modality gap between image and point data. Consequently, this paradigm
struggles to ensure accurate cross-modal correspondences. To this end, inspired
by the cross-modal generation success of recent large diffusion models, we
propose Diff$^2$I2P, a fully Differentiable I2P registration framework,
leveraging a novel and effective Diffusion prior for bridging the modality gap.
Specifically, we propose a Control-Side Score Distillation (CSD) technique to
distill knowledge from a depth-conditioned diffusion model to directly optimize
the predicted transformation. However, the gradients on the transformation fail
to backpropagate onto the cross-modal features due to the non-differentiability
of correspondence retrieval and PnP solver. To this end, we further propose a
Deformable Correspondence Tuning (DCT) module to estimate the correspondences
in a differentiable way, followed by the transformation estimation using a
differentiable PnP solver. With these two designs, the Diffusion model serves
as a strong prior to guide the cross-modal feature learning of image and point
cloud for forming robust correspondences, which significantly improves the
registration. Extensive experimental results demonstrate that Diff$^2$I2P
consistently outperforms SoTA I2P registration methods, achieving over 7%
improvement in registration recall on the 7-Scenes benchmark.

</details>


### [39] [MS-DPPs: Multi-Source Determinantal Point Processes for Contextual Diversity Refinement of Composite Attributes in Text to Image Retrieval](https://arxiv.org/abs/2507.06654)
*Naoya Sogi,Takashi Shibata,Makoto Terao,Masanori Suganuma,Takayuki Okatani*

Main category: cs.CV

TL;DR: 本文提出了一种名为CDR-CA的新任务，旨在根据应用上下文优化多属性的多样性，并提出了Multi-Source DPPs作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅关注图像外观的多样性指标，但其期望值因应用而异，限制了结果多样化的应用范围。

Method: 提出Multi-Source DPPs，扩展了DPP到多源场景，基于流形表示的统一相似性矩阵建模，并引入Tangent Normalization以反映上下文。

Result: 实验证明所提方法的有效性。

Conclusion: CDR-CA和Multi-Source DPPs为结果多样化提供了更灵活的解决方案。

Abstract: Result diversification (RD) is a crucial technique in Text-to-Image Retrieval
for enhancing the efficiency of a practical application. Conventional methods
focus solely on increasing the diversity metric of image appearances. However,
the diversity metric and its desired value vary depending on the application,
which limits the applications of RD. This paper proposes a novel task called
CDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims
to refine the diversities of multiple attributes, according to the
application's context. To address this task, we propose Multi-Source DPPs, a
simple yet strong baseline that extends the Determinantal Point Process (DPP)
to multi-sources. We model MS-DPP as a single DPP model with a unified
similarity matrix based on a manifold representation. We also introduce Tangent
Normalization to reflect contexts. Extensive experiments demonstrate the
effectiveness of the proposed method. Our code is publicly available at
https://github.com/NEC-N-SOGI/msdpp.

</details>


### [40] [Enhancing Diffusion Model Stability for Image Restoration via Gradient Management](https://arxiv.org/abs/2507.06656)
*Hongjie Wu,Mingqin Zhang,Linchao He,Ji-Zhe Zhou,Jiancheng Lv*

Main category: cs.CV

TL;DR: 论文提出了一种名为SPGD的新方法，通过梯度管理技术解决扩散模型中先验和似然梯度冲突及不稳定性问题，显著提升了图像恢复性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像恢复中表现出色，但先验和似然梯度之间的冲突及其不稳定性影响了生成过程。本文旨在分析并解决这些问题。

Method: 提出SPGD方法，包括渐进式似然预热策略和自适应方向动量平滑技术，以管理梯度冲突和不稳定性。

Result: 实验表明，SPGD显著提高了生成稳定性，在多种恢复任务中取得了最先进的定量和视觉结果。

Conclusion: SPGD通过梯度管理有效解决了扩散模型中的不稳定性问题，为图像恢复提供了更优的解决方案。

Abstract: Diffusion models have shown remarkable promise for image restoration by
leveraging powerful priors. Prominent methods typically frame the restoration
problem within a Bayesian inference framework, which iteratively combines a
denoising step with a likelihood guidance step. However, the interactions
between these two components in the generation process remain underexplored. In
this paper, we analyze the underlying gradient dynamics of these components and
identify significant instabilities. Specifically, we demonstrate conflicts
between the prior and likelihood gradient directions, alongside temporal
fluctuations in the likelihood gradient itself. We show that these
instabilities disrupt the generative process and compromise restoration
performance. To address these issues, we propose Stabilized Progressive
Gradient Diffusion (SPGD), a novel gradient management technique. SPGD
integrates two synergistic components: (1) a progressive likelihood warm-up
strategy to mitigate gradient conflicts; and (2) adaptive directional momentum
(ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive
experiments across diverse restoration tasks demonstrate that SPGD
significantly enhances generation stability, leading to state-of-the-art
performance in quantitative metrics and visually superior results. Code is
available at \href{https://github.com/74587887/SPGD}{here}.

</details>


### [41] [MK-Pose: Category-Level Object Pose Estimation via Multimodal-Based Keypoint Learning](https://arxiv.org/abs/2507.06662)
*Yifan Yang,Peili Song,Enfan Lan,Dong Liu,Jingtai Liu*

Main category: cs.CV

TL;DR: MK-Pose是一种基于多模态的关键点学习框架，结合RGB图像、点云数据和类别文本描述，用于类别级物体姿态估计，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在物体遮挡和跨实例、跨类别泛化能力上的不足。

Method: 提出自监督关键点检测模块，结合注意力查询生成、软热图匹配和图关系建模；设计图增强特征融合模块。

Result: 在CAMERA25和REAL275数据集上表现优异，IoU和平均精度均优于现有方法。

Conclusion: MK-Pose在类别级物体姿态估计任务中表现出色，无需形状先验即可实现高性能。

Abstract: Category-level object pose estimation, which predicts the pose of objects
within a known category without prior knowledge of individual instances, is
essential in applications like warehouse automation and manufacturing. Existing
methods relying on RGB images or point cloud data often struggle with object
occlusion and generalization across different instances and categories. This
paper proposes a multimodal-based keypoint learning framework (MK-Pose) that
integrates RGB images, point clouds, and category-level textual descriptions.
The model uses a self-supervised keypoint detection module enhanced with
attention-based query generation, soft heatmap matching and graph-based
relational modeling. Additionally, a graph-enhanced feature fusion module is
designed to integrate local geometric information and global context. MK-Pose
is evaluated on CAMERA25 and REAL275 dataset, and is further tested for
cross-dataset capability on HouseCat6D dataset. The results demonstrate that
MK-Pose outperforms existing state-of-the-art methods in both IoU and average
precision without shape priors. Codes will be released at
\href{https://github.com/yangyifanYYF/MK-Pose}{https://github.com/yangyifanYYF/MK-Pose}.

</details>


### [42] [FlexGaussian: Flexible and Cost-Effective Training-Free Compression for 3D Gaussian Splatting](https://arxiv.org/abs/2507.06671)
*Boyuan Tian,Qizhe Gao,Siran Xianyu,Xiaotong Cui,Minjia Zhang*

Main category: cs.CV

TL;DR: FlexGaussian是一种无需训练的3D高斯压缩方法，结合混合精度量化和属性判别剪枝，实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 大规模3D模型需要高效压缩以减少内存和计算成本，现有方法缺乏灵活性且需要重新训练。

Method: 采用混合精度量化和属性判别剪枝，无需重新训练，适应多种压缩目标。

Result: 压缩率高达96.4%，渲染质量损失小（PSNR下降<1 dB），速度比现有方法快1.7-2.1倍。

Conclusion: FlexGaussian是一种灵活、高效的3D高斯压缩方法，适用于移动设备。

Abstract: 3D Gaussian splatting has become a prominent technique for representing and
rendering complex 3D scenes, due to its high fidelity and speed advantages.
However, the growing demand for large-scale models calls for effective
compression to reduce memory and computation costs, especially on mobile and
edge devices with limited resources. Existing compression methods effectively
reduce 3D Gaussian parameters but often require extensive retraining or
fine-tuning, lacking flexibility under varying compression constraints.
  In this paper, we introduce FlexGaussian, a flexible and cost-effective
method that combines mixed-precision quantization with attribute-discriminative
pruning for training-free 3D Gaussian compression. FlexGaussian eliminates the
need for retraining and adapts easily to diverse compression targets.
Evaluation results show that FlexGaussian achieves up to 96.4% compression
while maintaining high rendering quality (<1 dB drop in PSNR), and is
deployable on mobile devices. FlexGaussian delivers high compression ratios
within seconds, being 1.7-2.1x faster than state-of-the-art training-free
methods and 10-100x faster than training-involved approaches. The code is being
prepared and will be released soon at:
https://github.com/Supercomputing-System-AI-Lab/FlexGaussian

</details>


### [43] [Text-promptable Object Counting via Quantity Awareness Enhancement](https://arxiv.org/abs/2507.06679)
*Miaojing Shi,Xiaowen Zhang,Zijie Yue,Yong Luo,Cairong Zhao,Li Li*

Main category: cs.CV

TL;DR: QUANet提出了一种新的数量导向文本提示和视觉-文本数量对齐损失，以增强模型的数量感知能力，并通过双流自适应计数解码器提升密度图预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本提示中仅包含对象类别信息，不足以训练模型准确区分计数任务中的对象数量。

Method: 引入数量导向文本提示和视觉-文本数量对齐损失；设计双流自适应计数解码器（Transformer流、CNN流和T2C适配器）；提出跨流数量排序损失。

Result: 在FSC-147、CARPK、PUCPR+和ShanghaiTech等标准基准测试中表现出强大的零样本类无关计数泛化能力。

Conclusion: QUANet通过数量感知和双流解码器设计，显著提升了文本可提示对象计数任务的性能。

Abstract: Recent advances in large vision-language models (VLMs) have shown remarkable
progress in solving the text-promptable object counting problem. Representative
methods typically specify text prompts with object category information in
images. This however is insufficient for training the model to accurately
distinguish the number of objects in the counting task. To this end, we propose
QUANet, which introduces novel quantity-oriented text prompts with a
vision-text quantity alignment loss to enhance the model's quantity awareness.
Moreover, we propose a dual-stream adaptive counting decoder consisting of a
Transformer stream, a CNN stream, and a number of Transformer-to-CNN
enhancement adapters (T2C-adapters) for density map prediction. The
T2C-adapters facilitate the effective knowledge communication and aggregation
between the Transformer and CNN streams. A cross-stream quantity ranking loss
is proposed in the end to optimize the ranking orders of predictions from the
two streams. Extensive experiments on standard benchmarks such as FSC-147,
CARPK, PUCPR+, and ShanghaiTech demonstrate our model's strong generalizability
for zero-shot class-agnostic counting. Code is available at
https://github.com/viscom-tongji/QUANet

</details>


### [44] [StixelNExT++: Lightweight Monocular Scene Segmentation and Representation for Collective Perception](https://arxiv.org/abs/2507.06687)
*Marcel Vosshans,Omar Ait-Aider,Youcef Mezouar,Markus Enzweiler*

Main category: cs.CV

TL;DR: StixelNExT++是一种用于单目感知系统的新型场景表示方法，通过聚类3D Stixel单元增强对象分割，实现高压缩场景信息，并支持点云和鸟瞰图表示。


<details>
  <summary>Details</summary>
Motivation: 改进现有的Stixel表示方法，提升单目感知系统的场景理解和对象分割能力。

Method: 使用轻量级神经网络，基于LiDAR生成的自动标注数据进行训练，实时处理每帧仅需10毫秒。

Result: 在Waymo数据集上30米范围内表现优异，适用于自动驾驶系统的集体感知。

Conclusion: StixelNExT++在实时性和性能上具有潜力，适用于自动驾驶场景。

Abstract: This paper presents StixelNExT++, a novel approach to scene representation
for monocular perception systems. Building on the established Stixel
representation, our method infers 3D Stixels and enhances object segmentation
by clustering smaller 3D Stixel units. The approach achieves high compression
of scene information while remaining adaptable to point cloud and
bird's-eye-view representations. Our lightweight neural network, trained on
automatically generated LiDAR-based ground truth, achieves real-time
performance with computation times as low as 10 ms per frame. Experimental
results on the Waymo dataset demonstrate competitive performance within a
30-meter range, highlighting the potential of StixelNExT++ for collective
perception in autonomous systems.

</details>


### [45] [Spatial-Temporal Graph Mamba for Music-Guided Dance Video Synthesis](https://arxiv.org/abs/2507.06689)
*Hao Tang,Ling Shao,Zhenyu Zhang,Luc Van Gool,Nicu Sebe*

Main category: cs.CV

TL;DR: STG-Mamba是一种新颖的空间-时间图模型，用于音乐引导的舞蹈视频合成任务，通过音乐到骨架和骨架到视频的两步映射实现。


<details>
  <summary>Details</summary>
Motivation: 解决音乐到舞蹈视频的合成问题，捕捉空间和时间维度的依赖关系。

Method: 1. 音乐到骨架翻译：使用STGM块构建骨架序列；2. 骨架到视频翻译：提出自监督正则化网络。

Result: STG-Mamba在实验中表现优于现有方法。

Conclusion: STG-Mamba在音乐引导舞蹈视频合成任务中表现出色。

Abstract: We propose a novel spatial-temporal graph Mamba (STG-Mamba) for the
music-guided dance video synthesis task, i.e., to translate the input music to
a dance video. STG-Mamba consists of two translation mappings:
music-to-skeleton translation and skeleton-to-video translation. In the
music-to-skeleton translation, we introduce a novel spatial-temporal graph
Mamba (STGM) block to effectively construct skeleton sequences from the input
music, capturing dependencies between joints in both the spatial and temporal
dimensions. For the skeleton-to-video translation, we propose a novel
self-supervised regularization network to translate the generated skeletons,
along with a conditional image, into a dance video. Lastly, we collect a new
skeleton-to-video translation dataset from the Internet, containing 54,944
video clips. Extensive experiments demonstrate that STG-Mamba achieves
significantly better results than existing methods.

</details>


### [46] [A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding](https://arxiv.org/abs/2507.06719)
*Zhenyang Liu,Sixiao Zheng,Siyu Chen,Cairong Zhao,Longfei Liang,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: 提出SpatialReasoner框架，通过LLM驱动的空间推理和层次化特征场，提升开放词汇3D视觉定位的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有语言场方法在空间关系推理上的不足，如“椅子上的书”这类查询的定位问题。

Method: 结合LLM进行空间关系推理，构建视觉属性增强的层次化特征场，利用CLIP和SAM提取特征。

Result: 实验表明，该框架能无缝集成不同神经表示，显著提升3D视觉定位性能。

Conclusion: SpatialReasoner有效增强了空间推理能力，为开放词汇3D视觉定位提供了新思路。

Abstract: Open-vocabulary 3D visual grounding aims to localize target objects based on
free-form language queries, which is crucial for embodied AI applications such
as autonomous navigation, robotics, and augmented reality. Learning 3D language
fields through neural representations enables accurate understanding of 3D
scenes from limited viewpoints and facilitates the localization of target
objects in complex environments. However, existing language field methods
struggle to accurately localize instances using spatial relations in language
queries, such as ``the book on the chair.'' This limitation mainly arises from
inadequate reasoning about spatial relations in both language queries and 3D
scenes. In this work, we propose SpatialReasoner, a novel neural
representation-based framework with large language model (LLM)-driven spatial
reasoning that constructs a visual properties-enhanced hierarchical feature
field for open-vocabulary 3D visual grounding. To enable spatial reasoning in
language queries, SpatialReasoner fine-tunes an LLM to capture spatial
relations and explicitly infer instructions for the target, anchor, and spatial
relation. To enable spatial reasoning in 3D scenes, SpatialReasoner
incorporates visual properties (opacity and color) to construct a hierarchical
feature field. This field represents language and instance features using
distilled CLIP features and masks extracted via the Segment Anything Model
(SAM). The field is then queried using the inferred instructions in a
hierarchical manner to localize the target 3D instance based on the spatial
relation in the language query. Extensive experiments show that our framework
can be seamlessly integrated into different neural representations,
outperforming baseline models in 3D visual grounding while empowering their
spatial reasoning capability.

</details>


### [47] [Hierarchical Feature Alignment for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2507.06732)
*Sobhan Asasi,Mohamed Ilyes Lakhal,Richard Bowden*

Main category: cs.CV

TL;DR: 本文提出了一种基于伪注释和对比视频-语言对齐的分层预训练策略，用于提升手语翻译（SLT）的质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在端到端学习中难以处理视觉与文本表示之间的差异，而基于注释的方法需要大量标注工作。本文旨在通过伪注释和分层特征提取解决这一问题。

Method: 采用分层预训练策略，在帧、片段和视频级别提取特征，并通过伪注释和对比对齐增强翻译质量。

Result: 实验表明，该方法在BLEU-4和ROUGE分数上有所提升，同时保持高效性。

Conclusion: 通过分层特征提取和对齐策略，本文方法在手语翻译任务中取得了更好的性能。

Abstract: Sign Language Translation (SLT) attempts to convert sign language videos into
spoken sentences. However, many existing methods struggle with the disparity
between visual and textual representations during end-to-end learning.
Gloss-based approaches help to bridge this gap by leveraging structured
linguistic information. While, gloss-free methods offer greater flexibility and
remove the burden of annotation, they require effective alignment strategies.
Recent advances in Large Language Models (LLMs) have enabled gloss-free SLT by
generating text-like representations from sign videos. In this work, we
introduce a novel hierarchical pre-training strategy inspired by the structure
of sign language, incorporating pseudo-glosses and contrastive video-language
alignment. Our method hierarchically extracts features at frame, segment, and
video levels, aligning them with pseudo-glosses and the spoken sentence to
enhance translation quality. Experiments demonstrate that our approach improves
BLEU-4 and ROUGE scores while maintaining efficiency.

</details>


### [48] [MADPOT: Medical Anomaly Detection with CLIP Adaptation and Partial Optimal Transport](https://arxiv.org/abs/2507.06733)
*Mahshid Shiri,Cigdem Beyan,Vittorio Murino*

Main category: cs.CV

TL;DR: 提出了一种结合视觉适配器、提示学习、部分最优传输和对比学习的新方法，用于提升CLIP在医学图像异常检测中的适应性。


<details>
  <summary>Details</summary>
Motivation: 医学异常检测面临成像模态多样、解剖变异大和标记数据有限等挑战，需要更灵活的方法。

Method: 采用多提示学习与部分最优传输（POT）结合，通过对比学习增强类内凝聚和类间分离。

Result: 在少样本、零样本和跨数据集场景中取得最优结果，无需合成数据或记忆库。

Conclusion: 该方法显著提升了医学图像异常检测的性能和适应性。

Abstract: Medical anomaly detection (AD) is challenging due to diverse imaging
modalities, anatomical variations, and limited labeled data. We propose a novel
approach combining visual adapters and prompt learning with Partial Optimal
Transport (POT) and contrastive learning (CL) to improve CLIP's adaptability to
medical images, particularly for AD. Unlike standard prompt learning, which
often yields a single representation, our method employs multiple prompts
aligned with local features via POT to capture subtle abnormalities. CL further
enforces intra-class cohesion and inter-class separation. Our method achieves
state-of-the-art results in few-shot, zero-shot, and cross-dataset scenarios
without synthetic data or memory banks. The code is available at
https://github.com/mahshid1998/MADPOT.

</details>


### [49] [Residual Prior-driven Frequency-aware Network for Image Fusion](https://arxiv.org/abs/2507.06735)
*Guan Zheng,Xue Wang,Wenhua Qian,Peng Liu,Runzhuo Ma*

Main category: cs.CV

TL;DR: RPFNet通过双分支框架（残差先验模块和频域融合模块）解决图像融合中的计算成本和互补特征捕获问题，结合辅助解码器和对比损失提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决图像融合中长程特征依赖的高计算成本和缺乏真实数据的问题。

Method: 采用双分支特征提取框架（RPM和FDFM），结合CPM模块和辅助解码器，使用频域对比损失和SSIM损失优化。

Result: 实验证明RPFNet能有效整合特征，增强纹理细节和显著目标，支持高级视觉任务。

Conclusion: RPFNet通过频域建模和残差先验，显著提升了图像融合的性能和效率。

Abstract: Image fusion aims to integrate complementary information across modalities to
generate high-quality fused images, thereby enhancing the performance of
high-level vision tasks. While global spatial modeling mechanisms show
promising results, constructing long-range feature dependencies in the spatial
domain incurs substantial computational costs. Additionally, the absence of
ground-truth exacerbates the difficulty of capturing complementary features
effectively. To tackle these challenges, we propose a Residual Prior-driven
Frequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a
dual-branch feature extraction framework: the Residual Prior Module (RPM)
extracts modality-specific difference information from residual maps, thereby
providing complementary priors for fusion; the Frequency Domain Fusion Module
(FDFM) achieves efficient global feature modeling and integration through
frequency-domain convolution. Additionally, the Cross Promotion Module (CPM)
enhances the synergistic perception of local details and global structures
through bidirectional feature interaction. During training, we incorporate an
auxiliary decoder and saliency structure loss to strengthen the model's
sensitivity to modality-specific differences. Furthermore, a combination of
adaptive weight-based frequency contrastive loss and SSIM loss effectively
constrains the solution space, facilitating the joint capture of local details
and global features while ensuring the retention of complementary information.
Extensive experiments validate the fusion performance of RPFNet, which
effectively integrates discriminative features, enhances texture details and
salient objects, and can effectively facilitate the deployment of the
high-level vision task.

</details>


### [50] [DIFFUMA: High-Fidelity Spatio-Temporal Video Prediction via Dual-Path Mamba and Diffusion Enhancement](https://arxiv.org/abs/2507.06738)
*Xinyu Xie,Weifeng Cao,Jun Shi,Yangyang Hu,Hui Liang,Wanyong Liang,Xiaoliang Qian*

Main category: cs.CV

TL;DR: 论文提出了一种名为DIFFUMA的双路径预测架构，并发布了首个半导体晶圆切割过程的公开数据集CHDL，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 高精度工业场景（如半导体制造）缺乏专用数据集，阻碍了复杂过程建模和预测的研究。

Method: 构建CHDL数据集，并提出DIFFUMA模型，结合Mamba模块和扩散模块以捕捉全局时序和增强空间细节。

Result: DIFFUMA在CHDL数据集上MSE降低39%，SSIM从0.926提升至0.988，并在自然现象数据集上表现优异。

Conclusion: 研究不仅提供了新的SOTA模型，还为工业AI领域贡献了宝贵的数据资源。

Abstract: Spatio-temporal video prediction plays a pivotal role in critical domains,
ranging from weather forecasting to industrial automation. However, in
high-precision industrial scenarios such as semiconductor manufacturing, the
absence of specialized benchmark datasets severely hampers research on modeling
and predicting complex processes. To address this challenge, we make a twofold
contribution.First, we construct and release the Chip Dicing Lane Dataset
(CHDL), the first public temporal image dataset dedicated to the semiconductor
wafer dicing process. Captured via an industrial-grade vision system, CHDL
provides a much-needed and challenging benchmark for high-fidelity process
modeling, defect detection, and digital twin development.Second, we propose
DIFFUMA, an innovative dual-path prediction architecture specifically designed
for such fine-grained dynamics. The model captures global long-range temporal
context through a parallel Mamba module, while simultaneously leveraging a
diffusion module, guided by temporal features, to restore and enhance
fine-grained spatial details, effectively combating feature degradation.
Experiments demonstrate that on our CHDL benchmark, DIFFUMA significantly
outperforms existing methods, reducing the Mean Squared Error (MSE) by 39% and
improving the Structural Similarity (SSIM) from 0.926 to a near-perfect 0.988.
This superior performance also generalizes to natural phenomena datasets. Our
work not only delivers a new state-of-the-art (SOTA) model but, more
importantly, provides the community with an invaluable data resource to drive
future research in industrial AI.

</details>


### [51] [PromptTea: Let Prompts Tell TeaCache the Optimal Threshold](https://arxiv.org/abs/2507.06739)
*Zishen Huang,Chunyu Yang,Mengyuan Ren*

Main category: cs.CV

TL;DR: 论文提出了一种基于提示复杂度的自适应缓存方法（PCA缓存），通过动态调整重用阈值和优化输入输出建模，显著提升了视频生成的推理速度，同时保持高质量输出。


<details>
  <summary>Details</summary>
Motivation: 视频生成中的推理速度是主要瓶颈，固定频率的重用机制在复杂场景中效果不佳，手动调整阈值效率低且缺乏鲁棒性。

Method: 提出PCA缓存，根据输入提示估计场景复杂度动态调整重用阈值；改进TeaCache的输入输出建模；引入动态CFGCache机制。

Result: 实验表明，该方法在Wan2.1模型上实现了2.79倍的加速，同时保持了高视觉保真度。

Conclusion: PCA缓存和动态CFGCache的结合有效解决了视频生成中的速度与质量权衡问题。

Abstract: Despite recent progress in video generation, inference speed remains a major
bottleneck. A common acceleration strategy involves reusing model outputs via
caching mechanisms at fixed intervals. However, we find that such
fixed-frequency reuse significantly degrades quality in complex scenes, while
manually tuning reuse thresholds is inefficient and lacks robustness. To
address this, we propose Prompt-Complexity-Aware (PCA) caching, a method that
automatically adjusts reuse thresholds based on scene complexity estimated
directly from the input prompt. By incorporating prompt-derived semantic cues,
PCA enables more adaptive and informed reuse decisions than conventional
caching methods. We also revisit the assumptions behind TeaCache and identify a
key limitation: it suffers from poor input-output relationship modeling due to
an oversimplified prior. To overcome this, we decouple the noisy input, enhance
the contribution of meaningful textual information, and improve the model's
predictive accuracy through multivariate polynomial feature expansion. To
further reduce computational cost, we replace the static CFGCache with
DynCFGCache, a dynamic mechanism that selectively reuses classifier-free
guidance (CFG) outputs based on estimated output variations. This allows for
more flexible reuse without compromising output quality. Extensive experiments
demonstrate that our approach achieves significant acceleration-for example,
2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across
a range of scenes.

</details>


### [52] [Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching](https://arxiv.org/abs/2507.06744)
*Yafei Zhang,Yongle Shang,Huafeng Li*

Main category: cs.CV

TL;DR: 提出了一种局部与全局双粒度身份关联机制，用于弱监督文本到人物图像匹配，显著提升了跨模态匹配精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理复杂的一对多身份关系，限制了性能提升，因此提出新机制以解决这一问题。

Method: 采用局部与全局双粒度身份关联机制，局部层面显式建立跨模态身份关系，全局层面构建动态跨模态身份关联网络，并结合信息不对称样本对构建与一致性学习。

Result: 实验结果表明，该方法显著提升了跨模态匹配精度。

Conclusion: 该方法为文本到人物图像匹配提供了一种高效实用的解决方案。

Abstract: Weakly supervised text-to-person image matching, as a crucial approach to
reducing models' reliance on large-scale manually labeled samples, holds
significant research value. However, existing methods struggle to predict
complex one-to-many identity relationships, severely limiting performance
improvements. To address this challenge, we propose a local-and-global
dual-granularity identity association mechanism. Specifically, at the local
level, we explicitly establish cross-modal identity relationships within a
batch, reinforcing identity constraints across different modalities and
enabling the model to better capture subtle differences and correlations. At
the global level, we construct a dynamic cross-modal identity association
network with the visual modality as the anchor and introduce a confidence-based
dynamic adjustment mechanism, effectively enhancing the model's ability to
identify weakly associated samples while improving overall sensitivity.
Additionally, we propose an information-asymmetric sample pair construction
method combined with consistency learning to tackle hard sample mining and
enhance model robustness. Experimental results demonstrate that the proposed
method substantially boosts cross-modal matching accuracy, providing an
efficient and practical solution for text-to-person image matching.

</details>


### [53] [Finetuning Vision-Language Models as OCR Systems for Low-Resource Languages: A Case Study of Manchu](https://arxiv.org/abs/2507.06761)
*Yan Hon Michael Chung,Donghyeok Choi*

Main category: cs.CV

TL;DR: 该研究开发了高性能的OCR系统，用于识别濒危语言满文的历史文档，通过微调开源视觉语言模型，实现了高准确率。


<details>
  <summary>Details</summary>
Motivation: 满文是一种濒危语言，对理解早期现代东亚历史至关重要，但目前缺乏有效的OCR系统处理真实历史文档。

Method: 研究通过参数高效训练，在60,000张合成满文单词图像上微调了三种开源视觉语言模型（LLaMA-3.2-11B、Qwen2.5-VL-7B、Qwen2.5-VL-3B）。

Result: LLaMA-3.2-11B在合成数据上表现优异（98.3%单词准确率），在真实手写文档上保持93.1%准确率，显著优于传统方法。

Conclusion: 该研究为濒危语言OCR提供了可迁移的框架，降低了技术和财务门槛，助力历史学家和语言学家处理历史档案。

Abstract: Manchu, a critically endangered language essential for understanding early
modern Eastern Eurasian history, lacks effective OCR systems that can handle
real-world historical documents. This study develops high-performing OCR
systems by fine-tuning three open-source vision-language models (LLaMA-3.2-11B,
Qwen2.5-VL-7B, Qwen2.5-VL-3B) on 60,000 synthetic Manchu word images using
parameter-efficient training. LLaMA-3.2-11B achieved exceptional performance
with 98.3\% word accuracy and 0.0024 character error rate on synthetic data,
while crucially maintaining 93.1\% accuracy on real-world handwritten
documents. Comparative evaluation reveals substantial advantages over
traditional approaches: while a CRNN baseline achieved 99.8\% synthetic
accuracy, it suffered severe degradation to 72.5\% on real documents. Our
approach demonstrates effective synthetic-to-real domain transfer, providing a
cost-effective solution deployable on accessible infrastructure. This work
establishes a transferable framework for endangered language OCR that removes
technical and financial barriers in digital humanities, enabling historians and
linguists to process historical archives without specialized computing
resources. Code and model weights are available at
https://github.com/mic7ch1/ManchuAI-OCR.

</details>


### [54] [FOLC-Net: A Federated-Optimized Lightweight Architecture for Enhanced MRI Disease Diagnosis across Axial, Coronal, and Sagittal Views](https://arxiv.org/abs/2507.06763)
*Saif Ur Rehman Khan,Muhammad Nabeel Asim,Sebastian Vollmer,Andreas Dengel*

Main category: cs.CV

TL;DR: FOLC-Net框架通过轻量级架构和优化机制，显著提升了MRI多视角和单视角分析的性能，尤其在矢状面表现突出。


<details>
  <summary>Details</summary>
Motivation: 解决现有SOTA模型在处理MRI多视角（轴向、冠状、矢状面）时性能下降的问题。

Method: 提出FOLC-Net，结合MRFO优化、全局模型克隆和ConvNeXt，参数仅1.217百万，存储需求0.9MB。

Result: FOLC-Net在矢状面准确率达92.44%，优于其他模型，并在多视角和单视角中均表现优异。

Conclusion: FOLC-Net为分散式环境中的医学图像分析提供了更可靠和鲁棒的解决方案。

Abstract: The framework is designed to improve performance in the analysis of combined
as well as single anatomical perspectives for MRI disease diagnosis. It
specifically addresses the performance degradation observed in state-of-the-art
(SOTA) models, particularly when processing axial, coronal, and sagittal
anatomical planes. The paper introduces the FOLC-Net framework, which
incorporates a novel federated-optimized lightweight architecture with
approximately 1.217 million parameters and a storage requirement of only 0.9
MB. FOLC-Net integrates Manta-ray foraging optimization (MRFO) mechanisms for
efficient model structure generation, global model cloning for scalable
training, and ConvNeXt for enhanced client adaptability. The model was
evaluated on combined multi-view data as well as individual views, such as
axial, coronal, and sagittal, to assess its robustness in various medical
imaging scenarios. Moreover, FOLC-Net tests a ShallowFed model on different
data to evaluate its ability to generalize beyond the training dataset. The
results show that FOLC-Net outperforms existing models, particularly in the
challenging sagittal view. For instance, FOLC-Net achieved an accuracy of
92.44% on the sagittal view, significantly higher than the 88.37% accuracy of
study method (DL + Residual Learning) and 88.95% of DL models. Additionally,
FOLC-Net demonstrated improved accuracy across all individual views, providing
a more reliable and robust solution for medical image analysis in decentralized
environments. FOLC-Net addresses the limitations of existing SOTA models by
providing a framework that ensures better adaptability to individual views
while maintaining strong performance in multi-view settings. The incorporation
of MRFO, global model cloning, and ConvNeXt ensures that FOLC-Net performs
better in real-world medical applications.

</details>


### [55] [Unlocking Thermal Aerial Imaging: Synthetic Enhancement of UAV Datasets](https://arxiv.org/abs/2507.06797)
*Antonella Barisic Kulas,Andreja Jurasovic,Stjepan Bogdan*

Main category: cs.CV

TL;DR: 提出了一种生成合成热成像数据的流程，用于扩展无人机热成像数据集，提升深度学习模型性能。


<details>
  <summary>Details</summary>
Motivation: 无人机热成像在搜索救援等领域潜力大，但数据稀缺限制了模型发展。

Method: 通过合成热成像数据，将新物体类别整合到现有热背景中，控制位置、大小和视角。

Result: 在HIT-UAV和MONET数据集中新增类别，验证了模型在新应用中的强性能。

Conclusion: 合成热成像数据有效扩展了应用范围，热成像检测器优于可见光模型，视角模拟很重要。

Abstract: Thermal imaging from unmanned aerial vehicles (UAVs) holds significant
potential for applications in search and rescue, wildlife monitoring, and
emergency response, especially under low-light or obscured conditions. However,
the scarcity of large-scale, diverse thermal aerial datasets limits the
advancement of deep learning models in this domain, primarily due to the high
cost and logistical challenges of collecting thermal data. In this work, we
introduce a novel procedural pipeline for generating synthetic thermal images
from an aerial perspective. Our method integrates arbitrary object classes into
existing thermal backgrounds by providing control over the position, scale, and
orientation of the new objects, while aligning them with the viewpoints of the
background. We enhance existing thermal datasets by introducing new object
categories, specifically adding a drone class in urban environments to the
HIT-UAV dataset and an animal category to the MONET dataset. In evaluating
these datasets for object detection task, we showcase strong performance across
both new and existing classes, validating the successful expansion into new
applications. Through comparative analysis, we show that thermal detectors
outperform their visible-light-trained counterparts and highlight the
importance of replicating aerial viewing angles. Project page:
https://github.com/larics/thermal_aerial_synthetic.

</details>


### [56] [Democratizing High-Fidelity Co-Speech Gesture Video Generation](https://arxiv.org/abs/2507.06812)
*Xu Yang,Shaoli Huang,Shenbo Xie,Xuelin Chen,Yifei Liu,Changxing Ding*

Main category: cs.CV

TL;DR: 该论文提出了一种轻量级框架，利用2D全身骨架作为辅助条件，通过扩散模型生成与音频同步的说话者视频，并发布了首个公开数据集CSG-405。


<details>
  <summary>Details</summary>
Motivation: 解决语音到视频生成任务中音频与视觉内容的一对多映射问题，以及数据稀缺和高计算需求的挑战。

Method: 使用2D骨架作为中间条件，结合扩散模型和骨架-音频特征融合，生成与音频严格同步的骨架运动，再通过现有人体视频生成模型合成高保真视频。

Result: 实验表明，该方法在视觉质量和同步性上优于现有技术，并能泛化到不同说话者和场景。

Conclusion: 提出的框架和数据集为语音到视频生成任务提供了高效且可扩展的解决方案。

Abstract: Co-speech gesture video generation aims to synthesize realistic,
audio-aligned videos of speakers, complete with synchronized facial expressions
and body gestures. This task presents challenges due to the significant
one-to-many mapping between audio and visual content, further complicated by
the scarcity of large-scale public datasets and high computational demands. We
propose a lightweight framework that utilizes 2D full-body skeletons as an
efficient auxiliary condition to bridge audio signals with visual outputs. Our
approach introduces a diffusion model conditioned on fine-grained audio
segments and a skeleton extracted from the speaker's reference image,
predicting skeletal motions through skeleton-audio feature fusion to ensure
strict audio coordination and body shape consistency. The generated skeletons
are then fed into an off-the-shelf human video generation model with the
speaker's reference image to synthesize high-fidelity videos. To democratize
research, we present CSG-405-the first public dataset with 405 hours of
high-resolution videos across 71 speech types, annotated with 2D skeletons and
diverse speaker demographics. Experiments show that our method exceeds
state-of-the-art approaches in visual quality and synchronization while
generalizing across speakers and contexts.

</details>


### [57] [HVI-CIDNet+: Beyond Extreme Darkness for Low-Light Image Enhancement](https://arxiv.org/abs/2507.06814)
*Qingsen Yan,Kangbiao Shi,Yixu Feng,Tao Hu,Peng Wu,Guansong Pang,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的颜色空间HVI和基于此的HVI-CIDNet+网络，用于低光图像增强，解决了现有方法的颜色偏差和噪声问题，并在10个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于sRGB和HSV颜色空间的低光图像增强方法存在颜色偏差和噪声问题，需要一种更有效的解决方案。

Method: 提出HVI颜色空间，结合HV颜色图和可学习强度，设计HVI-CIDNet+网络，利用预训练视觉语言模型提取上下文知识，并通过Prior-guided Attention Block和Region Refinement Block优化内容恢复和颜色校正。

Result: 在10个基准数据集上，HVI-CIDNet+优于现有方法。

Conclusion: HVI颜色空间和HVI-CIDNet+网络有效解决了低光图像增强中的颜色偏差和噪声问题，提升了性能。

Abstract: Low-Light Image Enhancement (LLIE) aims to restore vivid content and details
from corrupted low-light images. However, existing standard RGB (sRGB) color
space-based LLIE methods often produce color bias and brightness artifacts due
to the inherent high color sensitivity. While Hue, Saturation, and Value (HSV)
color space can decouple brightness and color, it introduces significant red
and black noise artifacts. To address this problem, we propose a new color
space for LLIE, namely Horizontal/Vertical-Intensity (HVI), defined by the HV
color map and learnable intensity. The HV color map enforces small distances
for the red coordinates to remove red noise artifacts, while the learnable
intensity compresses the low-light regions to remove black noise artifacts.
Additionally, we introduce the Color and Intensity Decoupling Network+
(HVI-CIDNet+), built upon the HVI color space, to restore damaged content and
mitigate color distortion in extremely dark regions. Specifically, HVI-CIDNet+
leverages abundant contextual and degraded knowledge extracted from low-light
images using pre-trained vision-language models, integrated via a novel
Prior-guided Attention Block (PAB). Within the PAB, latent semantic priors can
promote content restoration, while degraded representations guide precise color
correction, both particularly in extremely dark regions through the
meticulously designed cross-attention fusion mechanism. Furthermore, we
construct a Region Refinement Block that employs convolution for
information-rich regions and self-attention for information-scarce regions,
ensuring accurate brightness adjustments. Comprehensive results from benchmark
experiments demonstrate that the proposed HVI-CIDNet+ outperforms the
state-of-the-art methods on 10 datasets.

</details>


### [58] [Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation](https://arxiv.org/abs/2507.06830)
*Tao Feng,Xianbing Zhao,Zhenhua Chen,Tien Tsin Wong,Hamid Rezatofighi,Gholamreza Haffari,Lizhen Qu*

Main category: cs.CV

TL;DR: 提出了一种结合符号回归和轨迹引导图像到视频生成的新框架，以解决现有视频生成模型物理对齐不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散和自回归视频生成模型虽视觉逼真，但缺乏物理准确性，无法模拟真实世界物体运动。

Method: 通过提取输入视频的运动轨迹，利用检索式预训练增强符号回归，发现运动方程以预测物理准确的未来轨迹，并引导视频生成。

Result: 在经典力学场景（如弹簧-质量系统、摆锤、抛体运动）中，成功恢复真实解析方程，并提高了生成视频的物理对齐性。

Conclusion: 该框架无需微调现有模型，即可提升视频生成的物理准确性。

Abstract: Recent advances in diffusion-based and autoregressive video generation models
have achieved remarkable visual realism. However, these models typically lack
accurate physical alignment, failing to replicate real-world dynamics in object
motion. This limitation arises primarily from their reliance on learned
statistical correlations rather than capturing mechanisms adhering to physical
laws. To address this issue, we introduce a novel framework that integrates
symbolic regression (SR) and trajectory-guided image-to-video (I2V) models for
physics-grounded video forecasting. Our approach extracts motion trajectories
from input videos, uses a retrieval-based pre-training mechanism to enhance
symbolic regression, and discovers equations of motion to forecast physically
accurate future trajectories. These trajectories then guide video generation
without requiring fine-tuning of existing models. Evaluated on scenarios in
Classical Mechanics, including spring-mass, pendulums, and projectile motions,
our method successfully recovers ground-truth analytical equations and improves
the physical alignment of generated videos over baseline methods.

</details>


### [59] [Know Your Attention Maps: Class-specific Token Masking for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2507.06848)
*Joelle Hanna,Damian Borth*

Main category: cs.CV

TL;DR: 提出一种基于Vision Transformer的端到端弱监督语义分割方法，利用多[CLS]令牌和随机掩码策略生成伪分割掩码，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统弱监督语义分割方法依赖外部模块（如CAM），本文旨在直接利用ViT的注意力图，提高伪掩码生成效率和准确性。

Method: 训练稀疏ViT，每个类分配一个[CLS]令牌，采用随机掩码策略促进令牌-类别分配。推理时聚合各[CLS]令牌的自注意力图生成伪掩码。

Result: 在多个标准数据集上实验表明，该方法生成的伪掩码更准确，分割模型性能接近全监督模型。

Conclusion: 该方法显著减少对细粒度标注数据的依赖，同时保持高性能，为弱监督语义分割提供了新思路。

Abstract: Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that
has been extensively studied in recent years. Traditional approaches often rely
on external modules like Class Activation Maps to highlight regions of interest
and generate pseudo segmentation masks. In this work, we propose an end-to-end
method that directly utilizes the attention maps learned by a Vision
Transformer (ViT) for WSSS. We propose training a sparse ViT with multiple
[CLS] tokens (one for each class), using a random masking strategy to promote
[CLS] token - class assignment. At inference time, we aggregate the different
self-attention maps of each [CLS] token corresponding to the predicted labels
to generate pseudo segmentation masks. Our proposed approach enhances the
interpretability of self-attention maps and ensures accurate class assignments.
Extensive experiments on two standard benchmarks and three specialized datasets
demonstrate that our method generates accurate pseudo-masks, outperforming
related works. Those pseudo-masks can be used to train a segmentation model
which achieves results comparable to fully-supervised models, significantly
reducing the need for fine-grained labeled data.

</details>


### [60] [IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware Localization and Perturbation Optimization](https://arxiv.org/abs/2507.06856)
*Subrat Kishore Dutta,Xiao Zhang*

Main category: cs.CV

TL;DR: IAP是一种新型对抗补丁攻击框架，通过感知感知定位和扰动优化方案生成高度不可见的对抗补丁，显著提升了隐蔽性和攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在针对性攻击场景中表现不佳或生成的补丁不够隐蔽，容易被人类或自动防御系统发现。

Method: IAP结合类感知定位和敏感度图选择补丁位置，并采用感知感知正则化对抗损失和颜色恒常性梯度更新规则优化扰动。

Result: 在多种图像基准和模型架构上，IAP在针对性攻击中表现出高成功率，同时显著提升补丁的不可见性。

Conclusion: IAP不仅对人类高度隐蔽，还能有效绕过多种先进补丁防御系统。

Abstract: Despite modifying only a small localized input region, adversarial patches
can drastically change the prediction of computer vision models. However, prior
methods either cannot perform satisfactorily under targeted attack scenarios or
fail to produce contextually coherent adversarial patches, causing them to be
easily noticeable by human examiners and insufficiently stealthy against
automatic patch defenses. In this paper, we introduce IAP, a novel attack
framework that generates highly invisible adversarial patches based on
perceptibility-aware localization and perturbation optimization schemes.
Specifically, IAP first searches for a proper location to place the patch by
leveraging classwise localization and sensitivity maps, balancing the
susceptibility of patch location to both victim model prediction and human
visual system, then employs a perceptibility-regularized adversarial loss and a
gradient update rule that prioritizes color constancy for optimizing invisible
perturbations. Comprehensive experiments across various image benchmarks and
model architectures demonstrate that IAP consistently achieves competitive
attack success rates in targeted settings with significantly improved patch
invisibility compared to existing baselines. In addition to being highly
imperceptible to humans, IAP is shown to be stealthy enough to render several
state-of-the-art patch defenses ineffective.

</details>


### [61] [Longitudinal Study of Facial Biometrics at the BEZ: Temporal Variance Analysis](https://arxiv.org/abs/2507.06858)
*Mathias Schulz,Alexander Spenke,Pia Funk,Florian Blümel,Markus Rohde,Ralph Breithaupt,Gerd Nolden,Norbert Jung,Robert Lange*

Main category: cs.CV

TL;DR: 长期生物特征评估显示，个体间的日间分数波动比整个测量期间的波动更显著，强调了长期测试的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估长期生物特征数据的稳定性，为生物识别技术的改进提供基础。

Method: 使用先进的人脸识别算法分析长期比较分数，基于400多名参与者的多样化数据。

Result: 发现个体日间分数波动显著，而整个测量期间波动较小。

Conclusion: 长期控制在生物特征测试中至关重要，为未来生物数据分析奠定了基础。

Abstract: This study presents findings from long-term biometric evaluations conducted
at the Biometric Evaluation Center (bez). Over the course of two and a half
years, our ongoing research with over 400 participants representing diverse
ethnicities, genders, and age groups were regularly assessed using a variety of
biometric tools and techniques at the controlled testing facilities. Our
findings are based on the General Data Protection Regulation-compliant local
bez database with more than 238.000 biometric data sets categorized into
multiple biometric modalities such as face and finger. We used state-of-the-art
face recognition algorithms to analyze long-term comparison scores. Our results
show that these scores fluctuate more significantly between individual days
than over the entire measurement period. These findings highlight the
importance of testing biometric characteristics of the same individuals over a
longer period of time in a controlled measurement environment and lays the
groundwork for future advancements in biometric data analysis.

</details>


### [62] [SemRaFiner: Panoptic Segmentation in Sparse and Noisy Radar Point Clouds](https://arxiv.org/abs/2507.06906)
*Matthias Zeller,Daniel Casado Herraez,Bengisu Ayan,Jens Behley,Michael Heidingsfeld,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 论文提出了一种名为SemRaFiner的方法，用于改进稀疏雷达点云的泛光分割，以增强自动驾驶的场景理解。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要准确的场景理解，但现有传感器（如相机和LiDAR）在恶劣天气下表现不佳且无法提供运动信息。雷达虽然能克服这些限制，但其测量数据稀疏且噪声较多。

Method: SemRaFiner方法通过优化特征提取和处理稀疏雷达点云中的密度变化，改进了泛光分割。此外，提出了一种优化的训练过程，通过数据增强细化实例分配。

Result: 实验表明，SemRaFiner在基于雷达的泛光分割任务中优于现有方法。

Conclusion: SemRaFiner通过改进特征提取和训练过程，显著提升了雷达点云的场景理解能力。

Abstract: Semantic scene understanding, including the perception and classification of
moving agents, is essential to enabling safe and robust driving behaviours of
autonomous vehicles. Cameras and LiDARs are commonly used for semantic scene
understanding. However, both sensor modalities face limitations in adverse
weather and usually do not provide motion information. Radar sensors overcome
these limitations and directly offer information about moving agents by
measuring the Doppler velocity, but the measurements are comparably sparse and
noisy. In this paper, we address the problem of panoptic segmentation in sparse
radar point clouds to enhance scene understanding. Our approach, called
SemRaFiner, accounts for changing density in sparse radar point clouds and
optimizes the feature extraction to improve accuracy. Furthermore, we propose
an optimized training procedure to refine instance assignments by incorporating
a dedicated data augmentation. Our experiments suggest that our approach
outperforms state-of-the-art methods for radar-based panoptic segmentation.

</details>


### [63] [Adaptive Part Learning for Fine-Grained Generalized Category Discovery: A Plug-and-Play Enhancement](https://arxiv.org/abs/2507.06928)
*Qiyuan Dai,Hanzhuo Huang,Yu Wu,Sibei Yang*

Main category: cs.CV

TL;DR: 论文提出了一种自适应部分发现和学习方法（APL），通过共享可学习部分查询和DINO部分先验，生成一致的对象部分及其对应关系，无需额外标注。APL通过新颖的all-min对比损失学习区分性强且可泛化的部分表示，显著提升了GCD框架在细粒度数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法依赖DINO的全局表示，导致区分性和泛化性之间存在固有权衡。APL旨在通过自适应部分发现和学习解决这一问题。

Method: APL利用共享可学习部分查询和DINO部分先验生成一致的对象部分及其对应关系，并提出all-min对比损失学习区分性强且可泛化的部分表示。

Result: APL显著提升了GCD框架在细粒度数据集上的性能。

Conclusion: APL通过自适应部分发现和学习，有效解决了GCD中区分性与泛化性的权衡问题，提升了性能。

Abstract: Generalized Category Discovery (GCD) aims to recognize unlabeled images from
known and novel classes by distinguishing novel classes from known ones, while
also transferring knowledge from another set of labeled images with known
classes. Existing GCD methods rely on self-supervised vision transformers such
as DINO for representation learning. However, focusing solely on the global
representation of the DINO CLS token introduces an inherent trade-off between
discriminability and generalization. In this paper, we introduce an adaptive
part discovery and learning method, called APL, which generates consistent
object parts and their correspondences across different similar images using a
set of shared learnable part queries and DINO part priors, without requiring
any additional annotations. More importantly, we propose a novel all-min
contrastive loss to learn discriminative yet generalizable part representation,
which adaptively highlights discriminative object parts to distinguish similar
categories for enhanced discriminability while simultaneously sharing other
parts to facilitate knowledge transfer for improved generalization. Our APL can
easily be incorporated into different GCD frameworks by replacing their CLS
token feature with our part representations, showing significant enhancements
on fine-grained datasets.

</details>


### [64] [MCCD: A Multi-Attribute Chinese Calligraphy Character Dataset Annotated with Script Styles, Dynasties, and Calligraphers](https://arxiv.org/abs/2507.06948)
*Yixin Zhao,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: 论文提出了一个多属性中国书法字符数据集（MCCD），填补了现有书法数据集稀缺且缺乏多属性标注的空白，为书法字符识别、书法家识别等研究提供了丰富资源。


<details>
  <summary>Details</summary>
Motivation: 研究书法字符的属性信息（如风格、朝代、书法家）具有重要文化价值，但现有数据集稀缺且缺乏多属性标注，限制了深入研究。

Method: 构建了包含7,765类329,715个书法字符图像的MCCD数据集，并基于脚本风格、朝代和书法家属性提取了三个子集。

Result: 实验表明，书法字符的笔画结构复杂性和多属性交互显著增加了准确识别的难度。

Conclusion: MCCD填补了详细书法数据集的空白，为书法研究和多领域发展提供了宝贵资源。

Abstract: Research on the attribute information of calligraphy, such as styles,
dynasties, and calligraphers, holds significant cultural and historical value.
However, the styles of Chinese calligraphy characters have evolved dramatically
through different dynasties and the unique touches of calligraphers, making it
highly challenging to accurately recognize these different characters and their
attributes. Furthermore, existing calligraphic datasets are extremely scarce,
and most provide only character-level annotations without additional attribute
information. This limitation has significantly hindered the in-depth study of
Chinese calligraphy. To fill this gap, we present a novel Multi-Attribute
Chinese Calligraphy Character Dataset (MCCD). The dataset encompasses 7,765
categories with a total of 329,715 isolated image samples of Chinese
calligraphy characters, and three additional subsets were extracted based on
the attribute labeling of the three types of script styles (10 types),
dynasties (15 periods) and calligraphers (142 individuals). The rich
multi-attribute annotations render MCCD well-suited diverse research tasks,
including calligraphic character recognition, writer identification, and
evolutionary studies of Chinese characters. We establish benchmark performance
through single-task and multi-task recognition experiments across MCCD and all
of its subsets. The experimental results demonstrate that the complexity of the
stroke structure of the calligraphic characters, and the interplay between
their different attributes, leading to a substantial increase in the difficulty
of accurate recognition. MCCD not only fills a void in the availability of
detailed calligraphy datasets but also provides valuable resources for
advancing research in Chinese calligraphy and fostering advancements in
multiple fields. The dataset is available at
https://github.com/SCUT-DLVCLab/MCCD.

</details>


### [65] [Pre-Columbian Settlements Shaped Palm Clusters in the Sierra Nevada de Santa Marta, Colombia](https://arxiv.org/abs/2507.06949)
*Sebastian Fajardo,Sina Mohammadi,Jonas Gregorio de Souza,César Ardila,Alan Tapscott Baltar,Shaddai Heidgen,Maria Isabel Mayorga Hernández,Sylvia Mota de Oliveira,Fernando Montejo,Marco Moderato,Vinicius Peripato,Katy Puche,Carlos Reina,Juan Carlos Vargas,Frank W. Takes,Marco Madella*

Main category: cs.CV

TL;DR: 利用深度学习模型和聚类算法，通过卫星图像识别棕榈树分布，揭示古代人类管理对植被的影响。


<details>
  <summary>Details</summary>
Motivation: 研究古代人类对尼奥特罗皮克森林的长期影响，特别是在高分辨率尺度下的管理效果。

Method: 结合深度学习模型（识别棕榈树）和聚类算法（识别棕榈树集群），估计古代人类管理区域。

Result: 棕榈树在考古遗址附近显著更多，最大集群显示古代人类管理区域可能比考古证据显示的大两个数量级。

Conclusion: 前哥伦布时期人类通过影响植被促进了棕榈树的增殖，留下了持久的生态足迹，展示了人工智能与生态考古数据结合揭示人类环境互动的潜力。

Abstract: Ancient populations markedly transformed Neotropical forests, yet
understanding the long-term effects of ancient human management, particularly
at high-resolution scales, remains challenging. In this work we propose a new
approach to investigate archaeological areas of influence based on vegetation
signatures. It consists of a deep learning model trained on satellite imagery
to identify palm trees, followed by a clustering algorithm to identify palm
clusters, which are then used to estimate ancient management areas. To assess
the palm distribution in relation to past human activity, we applied the
proposed approach to unique high-resolution satellite imagery data covering 765
km2 of the Sierra Nevada de Santa Marta, Colombia. With this work, we also
release a manually annotated palm tree dataset along with estimated locations
of archaeological sites from ground-surveys and legacy records. Results
demonstrate how palms were significantly more abundant near archaeological
sites showing large infrastructure investment. The extent of the largest palm
cluster indicates that ancient human-managed areas linked to major
infrastructure sites may be up to two orders of magnitude bigger than indicated
by archaeological evidence alone. Our findings suggest that pre-Columbian
populations influenced local vegetation fostering conditions conducive to palm
proliferation, leaving a lasting ecological footprint. This may have lowered
the logistical costs of establishing infrastructure-heavy settlements in
otherwise less accessible locations. Overall, this study demonstrates the
potential of integrating artificial intelligence approaches with new ecological
and archaeological data to identify archaeological areas of interest through
vegetation patterns, revealing fine-scale human-environment interactions.

</details>


### [66] [CheXPO: Preference Optimization for Chest X-ray VLMs with Counterfactual Rationale](https://arxiv.org/abs/2507.06959)
*Xiao Liang,Jiawei Hu,Di Wang,Zhi Ma,Lin Zhao,Ronghan Li,Bo Wan,Quan Wang*

Main category: cs.CV

TL;DR: CheXPO通过结合置信度-相似性联合挖掘与反事实推理，优化胸部X光视觉语言模型的偏好，减少幻觉问题，提升性能。


<details>
  <summary>Details</summary>
Motivation: 医疗应用中视觉语言模型（VLMs）的幻觉问题严重影响了可靠性，而传统偏好优化方法面临数据分布不均、专家标注成本高等挑战。

Method: 提出CheXPO策略，通过多任务胸部X光视觉指令数据集进行监督微调，利用置信度分析和相似性检索扩展困难样本，并结合反事实推理提供细粒度偏好。

Result: 实验表明，CheXPO仅使用5%的监督微调样本即实现8.93%的相对性能提升，达到多种临床任务的最优性能。

Conclusion: CheXPO为放射学应用提供了一种可扩展、可解释的解决方案，有效减少了幻觉问题。

Abstract: Vision-language models (VLMs) are prone to hallucinations that critically
compromise reliability in medical applications. While preference optimization
can mitigate these hallucinations through clinical feedback, its implementation
faces challenges such as clinically irrelevant training samples, imbalanced
data distributions, and prohibitive expert annotation costs. To address these
challenges, we introduce CheXPO, a Chest X-ray Preference Optimization strategy
that combines confidence-similarity joint mining with counterfactual rationale.
Our approach begins by synthesizing a unified, fine-grained multi-task chest
X-ray visual instruction dataset across different question types for supervised
fine-tuning (SFT). We then identify hard examples through token-level
confidence analysis of SFT failures and use similarity-based retrieval to
expand hard examples for balancing preference sample distributions, while
synthetic counterfactual rationales provide fine-grained clinical preferences,
eliminating the need for additional expert input. Experiments show that CheXPO
achieves 8.93% relative performance gain using only 5% of SFT samples, reaching
state-of-the-art performance across diverse clinical tasks and providing a
scalable, interpretable solution for real-world radiology applications.

</details>


### [67] [Segmentation Regularized Training for Multi-Domain Deep Learning Registration applied to MR-Guided Prostate Cancer Radiotherapy](https://arxiv.org/abs/2507.06966)
*Sudharsan Madhavan,Chengcheng Gui,Lando Bosma,Josiah Simeth,Jue Jiang,Nicolas Cote,Nima Hassan Rezaeian,Himanshu Nagar,Victoria Brennan,Neelam Tyagi,Harini Veeraraghavan*

Main category: cs.CV

TL;DR: 该研究提出了一种深度学习的可变形图像配准方法（ProRSeg），用于多领域MR-MR配准，并在前列腺癌患者的MR引导自适应放疗中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 在MR引导的自适应放疗中，准确的图像配准对于轮廓传播和剂量累积至关重要。本研究旨在开发一种能够适应不同MR领域的配准方法。

Method: 采用逐步优化的配准和分割方法（ProRSeg），使用加权分割一致性损失进行训练，并在不同领域的MR数据集上测试其性能。

Result: ProRSeg在膀胱配准中表现出跨领域的泛化能力（DSC 0.88-0.86），而在直肠和CTV中性能依赖于领域。剂量累积结果显示83.3%的患者满足CTV覆盖和膀胱保护约束。

Conclusion: ProRSeg在多领域MR-MR配准中表现合理，初步验证了其在评估治疗合规性中的可行性。

Abstract: Background: Accurate deformable image registration (DIR) is required for
contour propagation and dose accumulation in MR-guided adaptive radiotherapy
(MRgART). This study trained and evaluated a deep learning DIR method for
domain invariant MR-MR registration. Methods: A progressively refined
registration and segmentation (ProRSeg) method was trained with 262 pairs of 3T
MR simulation scans from prostate cancer patients using weighted segmentation
consistency loss. ProRSeg was tested on same- (58 pairs), cross- (72 1.5T MR
Linac pairs), and mixed-domain (42 MRSim-MRL pairs) datasets for contour
propagation accuracy of clinical target volume (CTV), bladder, and rectum. Dose
accumulation was performed for 42 patients undergoing 5-fraction MRgART.
Results: ProRSeg demonstrated generalization for bladder with similar Dice
Similarity Coefficients across domains (0.88, 0.87, 0.86). For rectum and CTV,
performance was domain-dependent with higher accuracy on cross-domain MRL
dataset (DSCs 0.89) versus same-domain data. The model's strong cross-domain
performance prompted us to study the feasibility of using it for dose
accumulation. Dose accumulation showed 83.3% of patients met CTV coverage (D95
>= 40.0 Gy) and bladder sparing (D50 <= 20.0 Gy) constraints. All patients
achieved minimum mean target dose (>40.4 Gy), but only 9.5% remained under
upper limit (<42.0 Gy). Conclusions: ProRSeg showed reasonable multi-domain
MR-MR registration performance for prostate cancer patients with preliminary
feasibility for evaluating treatment compliance to clinical constraints.

</details>


### [68] [A multi-modal dataset for insect biodiversity with imagery and DNA at the trap and individual level](https://arxiv.org/abs/2507.06972)
*Johanna Orsholm,John Quinto,Hannu Autto,Gaia Banelyte,Nicolas Chazot,Jeremy deWaard,Stephanie deWaard,Arielle Farrell,Brendan Furneaux,Bess Hardwick,Nao Ito,Amlan Kar,Oula Kalttopää,Deirdre Kerdraon,Erik Kristensen,Jaclyn McKeown,Tommi Mononen,Ellen Nein,Hanna Rogers,Tomas Roslin,Paula Schmitz,Jayme Sones,Maija Sujala,Amy Thompson,Evgeny V. Zakharov,Iuliia Zarubiieva,Akshita Gupta,Scott C. Lowe,Graham W. Taylor*

Main category: cs.CV

TL;DR: 论文介绍了MassID45数据集，结合分子和成像数据，用于训练批量昆虫样本的自动分类器，推动生态和机器学习研究。


<details>
  <summary>Details</summary>
Motivation: 昆虫多样性研究需要高效方法，现有图像方法依赖单个标本，而生态调查多为批量样本。

Method: 结合DNA条形码和高分辨率成像，创建MassID45数据集，包含批量样本和个体标本数据，AI辅助标注。

Result: 标注了17,000多个标本的分割掩膜和分类标签，结合DNA条形码实现高分辨率分类。

Conclusion: MassID45数据集为快速大规模昆虫群落表征提供了潜力，推动了小物体检测和实例分割的创新。

Abstract: Insects comprise millions of species, many experiencing severe population
declines under environmental and habitat changes. High-throughput approaches
are crucial for accelerating our understanding of insect diversity, with DNA
barcoding and high-resolution imaging showing strong potential for automatic
taxonomic classification. However, most image-based approaches rely on
individual specimen data, unlike the unsorted bulk samples collected in
large-scale ecological surveys. We present the Mixed Arthropod Sample
Segmentation and Identification (MassID45) dataset for training automatic
classifiers of bulk insect samples. It uniquely combines molecular and imaging
data at both the unsorted sample level and the full set of individual
specimens. Human annotators, supported by an AI-assisted tool, performed two
tasks on bulk images: creating segmentation masks around each individual
arthropod and assigning taxonomic labels to over 17 000 specimens. Combining
the taxonomic resolution of DNA barcodes with precise abundance estimates of
bulk images holds great potential for rapid, large-scale characterization of
insect communities. This dataset pushes the boundaries of tiny object detection
and instance segmentation, fostering innovation in both ecological and machine
learning research.

</details>


### [69] [Free on the Fly: Enhancing Flexibility in Test-Time Adaptation with Online EM](https://arxiv.org/abs/2507.06973)
*Qiyuan Dai,Sibei Yang*

Main category: cs.CV

TL;DR: FreeTTA是一种无需训练且通用的测试时适应方法，通过显式建模测试数据分布，利用在线EM算法提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时适应方法依赖昂贵训练或不现实假设，FreeTTA旨在解决这些问题，提升灵活性。

Method: 引入在线EM算法，利用VLM的零样本预测作为先验，迭代更新测试样本的后验概率和参数。

Result: 在15个数据集上，FreeTTA在跨域和分布外场景中均优于现有方法。

Conclusion: FreeTTA为测试时适应提供了高效且无需假设的解决方案，显著提升了性能。

Abstract: Vision-Language Models (VLMs) have become prominent in open-world image
recognition for their strong generalization abilities. Yet, their effectiveness
in practical applications is compromised by domain shifts and distributional
changes, especially when test data distributions diverge from training data.
Therefore, the paradigm of test-time adaptation (TTA) has emerged, enabling the
use of online off-the-shelf data at test time, supporting independent sample
predictions, and eliminating reliance on test annotations. Traditional TTA
methods, however, often rely on costly training or optimization processes, or
make unrealistic assumptions about accessing or storing historical training and
test data. Instead, this study proposes FreeTTA, a training-free and
universally available method that makes no assumptions, to enhance the
flexibility of TTA. More importantly, FreeTTA is the first to explicitly model
the test data distribution, enabling the use of intrinsic relationships among
test samples to enhance predictions of individual samples without simultaneous
access--a direction not previously explored. FreeTTA achieves these advantages
by introducing an online EM algorithm that utilizes zero-shot predictions from
VLMs as priors to iteratively compute the posterior probabilities of each
online test sample and update parameters. Experiments demonstrate that FreeTTA
achieves stable and significant improvements compared to state-of-the-art
methods across 15 datasets in both cross-domain and out-of-distribution
settings.

</details>


### [70] [DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via Joint LiDAR-Based 3D Object Detection and Denoising](https://arxiv.org/abs/2507.06976)
*Sven Teufel,Dominique Mayer,Jörg Gamerdinger,Oliver Bringmann*

Main category: cs.CV

TL;DR: 论文提出了一种名为DenoiseCP-Net的多任务架构，用于恶劣天气下的LiDAR集体感知，通过噪声过滤和对象检测的集成，减少了通信开销和计算成本。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆的感知系统在恶劣天气下容易受到传感器性能下降的影响，集体感知虽能解决这一问题，但相关研究较少。

Method: 提出DenoiseCP-Net，集成体素级噪声过滤和对象检测，采用稀疏卷积骨干网络，避免冗余计算。

Result: 在模拟的恶劣天气条件下，DenoiseCP-Net实现了近乎完美的去噪效果，带宽需求降低23.6%，同时保持检测精度和减少延迟。

Conclusion: DenoiseCP-Net为恶劣天气下的集体感知提供了一种高效解决方案，显著提升了系统性能。

Abstract: While automated vehicles hold the potential to significantly reduce traffic
accidents, their perception systems remain vulnerable to sensor degradation
caused by adverse weather and environmental occlusions. Collective perception,
which enables vehicles to share information, offers a promising approach to
overcoming these limitations. However, to this date collective perception in
adverse weather is mostly unstudied. Therefore, we conduct the first study of
LiDAR-based collective perception under diverse weather conditions and present
a novel multi-task architecture for LiDAR-based collective perception under
adverse weather. Adverse weather conditions can not only degrade perception
capabilities, but also negatively affect bandwidth requirements and latency due
to the introduced noise that is also transmitted and processed. Denoising prior
to communication can effectively mitigate these issues. Therefore, we propose
DenoiseCP-Net, a novel multi-task architecture for LiDAR-based collective
perception under adverse weather conditions. DenoiseCP-Net integrates
voxel-level noise filtering and object detection into a unified sparse
convolution backbone, eliminating redundant computations associated with
two-stage pipelines. This design not only reduces inference latency and
computational cost but also minimizes communication overhead by removing
non-informative noise. We extended the well-known OPV2V dataset by simulating
rain, snow, and fog using our realistic weather simulation models. We
demonstrate that DenoiseCP-Net achieves near-perfect denoising accuracy in
adverse weather, reduces the bandwidth requirements by up to 23.6% while
maintaining the same detection accuracy and reducing the inference latency for
cooperative vehicles.

</details>


### [71] [MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology Report Generation](https://arxiv.org/abs/2507.06992)
*Qilong Xing,Zikai Song,Youjia Zhang,Na Feng,Junqing Yu,Wei Yang*

Main category: cs.CV

TL;DR: 论文提出MCA-RG框架，通过将视觉特征与医学概念对齐，提升放射学报告生成的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在放射学报告生成中难以准确映射病理和解剖特征到文本描述，且特征提取缺乏语义关联。

Method: MCA-RG利用病理和解剖概念库对齐视觉特征，采用对比学习和匹配损失优化特征，并通过特征门控机制过滤低质量特征。

Result: 在MIMIC-CXR和CheXpert Plus基准测试中表现优异。

Conclusion: MCA-RG通过知识驱动的方法显著提升了放射学报告生成的准确性和临床适用性。

Abstract: Despite significant advancements in adapting Large Language Models (LLMs) for
radiology report generation (RRG), clinical adoption remains challenging due to
difficulties in accurately mapping pathological and anatomical features to
their corresponding text descriptions. Additionally, semantic agnostic feature
extraction further hampers the generation of accurate diagnostic reports. To
address these challenges, we introduce Medical Concept Aligned Radiology Report
Generation (MCA-RG), a knowledge-driven framework that explicitly aligns visual
features with distinct medical concepts to enhance the report generation
process. MCA-RG utilizes two curated concept banks: a pathology bank containing
lesion-related knowledge, and an anatomy bank with anatomical descriptions. The
visual features are aligned with these medical concepts and undergo tailored
enhancement. We further propose an anatomy-based contrastive learning procedure
to improve the generalization of anatomical features, coupled with a matching
loss for pathological features to prioritize clinically relevant regions.
Additionally, a feature gating mechanism is employed to filter out low-quality
concept features. Finally, the visual features are corresponding to individual
medical concepts, and are leveraged to guide the report generation process.
Experiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate
that MCA-RG achieves superior performance, highlighting its effectiveness in
radiology report generation.

</details>


### [72] [Cross-Modality Masked Learning for Survival Prediction in ICI Treated NSCLC Patients](https://arxiv.org/abs/2507.06994)
*Qilong Xing,Zikai Song,Bingxin Gong,Lian Yang,Junqing Yu,Wei Yang*

Main category: cs.CV

TL;DR: 提出了一种用于非小细胞肺癌（NSCLC）免疫治疗患者生存预测的多模态特征融合框架，结合3D CT图像和临床数据，通过跨模态掩码学习策略提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 精准预测NSCLC患者的预后对个性化治疗至关重要，但缺乏大规模数据集和有效的多模态特征融合方法。

Method: 构建了一个包含3D CT图像和临床记录的数据集，并提出了跨模态掩码学习框架，包括Slice-Depth Transformer和基于图的Transformer。

Result: 该方法在多模态集成中表现优异，超越了现有方法，为预后模型设定了新基准。

Conclusion: 提出的框架显著提升了NSCLC生存预测的准确性，为个性化治疗提供了有力工具。

Abstract: Accurate prognosis of non-small cell lung cancer (NSCLC) patients undergoing
immunotherapy is essential for personalized treatment planning, enabling
informed patient decisions, and improving both treatment outcomes and quality
of life. However, the lack of large, relevant datasets and effective
multi-modal feature fusion strategies pose significant challenges in this
domain. To address these challenges, we present a large-scale dataset and
introduce a novel framework for multi-modal feature fusion aimed at enhancing
the accuracy of survival prediction. The dataset comprises 3D CT images and
corresponding clinical records from NSCLC patients treated with immune
checkpoint inhibitors (ICI), along with progression-free survival (PFS) and
overall survival (OS) data. We further propose a cross-modality masked learning
approach for medical feature fusion, consisting of two distinct branches, each
tailored to its respective modality: a Slice-Depth Transformer for extracting
3D features from CT images and a graph-based Transformer for learning node
features and relationships among clinical variables in tabular data. The fusion
process is guided by a masked modality learning strategy, wherein the model
utilizes the intact modality to reconstruct missing components. This mechanism
improves the integration of modality-specific features, fostering more
effective inter-modality relationships and feature interactions. Our approach
demonstrates superior performance in multi-modal integration for NSCLC survival
prediction, surpassing existing methods and setting a new benchmark for
prognostic models in this context.

</details>


### [73] [Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs](https://arxiv.org/abs/2507.06999)
*Yahan Yu,Yuyang Dong,Masafumi Oyamada*

Main category: cs.CV

TL;DR: 论文提出了一种名为D2I的框架，通过规则奖励提升多模态大语言模型的推理能力，无需额外标注。


<details>
  <summary>Details</summary>
Motivation: 解决多模态推理研究中模态对齐和训练成本高的问题。

Method: 采用D2I框架，训练时使用规则奖励增强模态对齐，评估时转为直觉推理。

Result: D2I在领域内外基准测试中优于基线模型。

Conclusion: D2I展示了规则奖励在提升模型推理能力中的作用，为训练与测试推理深度解耦提供了方向。

Abstract: Reasoning is a key capability for large language models (LLMs), particularly
when applied to complex tasks such as mathematical problem solving. However,
multimodal reasoning research still requires further exploration of modality
alignment and training costs. Many of these approaches rely on additional data
annotation and relevant rule-based rewards to enhance the understanding and
reasoning ability, which significantly increases training costs and limits
scalability. To address these challenges, we propose the
Deliberate-to-Intuitive reasoning framework (D2I) that improves the
understanding and reasoning ability of multimodal LLMs (MLLMs) without extra
annotations and complex rewards. Specifically, our method sets deliberate
reasoning strategies to enhance modality alignment only through the rule-based
format reward during training. While evaluating, the reasoning style shifts to
intuitive, which removes deliberate reasoning strategies during training and
implicitly reflects the model's acquired abilities in the response. D2I
outperforms baselines across both in-domain and out-of-domain benchmarks. Our
findings highlight the role of format reward in fostering transferable
reasoning skills in MLLMs, and inspire directions for decoupling training-time
reasoning depth from test-time response flexibility.

</details>


### [74] [GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision Transformers for Whole Slide Image Classification and Captioning](https://arxiv.org/abs/2507.07006)
*S M Taslim Uddin Raju,Md. Milon Islam,Md Rezwanul Haque,Hamdi Altaheri,Fakhri Karray*

Main category: cs.CV

TL;DR: GNN-ViTCap框架通过动态聚类和注意力机制处理冗余病理图像，结合图神经网络和语言模型，显著提升分类和描述生成性能。


<details>
  <summary>Details</summary>
Motivation: 解决病理图像分类和描述生成中的冗余补丁和未知位置问题，提高诊断准确性。

Method: 使用视觉特征提取器生成补丁嵌入，动态聚类去除冗余，图神经网络捕获上下文，结合语言模型生成描述。

Result: 在BreakHis和PatchGastric数据集上，分类F1分数0.934，AUC 0.963；描述BLEU-4 0.811，METEOR 0.569。

Conclusion: GNN-ViTCap优于现有方法，为病理诊断提供高效可靠解决方案。

Abstract: Microscopic assessment of histopathology images is vital for accurate cancer
diagnosis and treatment. Whole Slide Image (WSI) classification and captioning
have become crucial tasks in computer-aided pathology. However, microscopic WSI
face challenges such as redundant patches and unknown patch positions due to
subjective pathologist captures. Moreover, generating automatic pathology
captions remains a significant challenge. To address these issues, we introduce
a novel GNN-ViTCap framework for classification and caption generation from
histopathological microscopic images. First, a visual feature extractor
generates patch embeddings. Redundant patches are then removed by dynamically
clustering these embeddings using deep embedded clustering and selecting
representative patches via a scalar dot attention mechanism. We build a graph
by connecting each node to its nearest neighbors in the similarity matrix and
apply a graph neural network to capture both local and global context. The
aggregated image embeddings are projected into the language model's input space
through a linear layer and combined with caption tokens to fine-tune a large
language model. We validate our method on the BreakHis and PatchGastric
datasets. GNN-ViTCap achieves an F1 score of 0.934 and an AUC of 0.963 for
classification, along with a BLEU-4 score of 0.811 and a METEOR score of 0.569
for captioning. Experimental results demonstrate that GNN-ViTCap outperforms
state of the art approaches, offering a reliable and efficient solution for
microscopy based patient diagnosis.

</details>


### [75] [Integrating Pathology Foundation Models and Spatial Transcriptomics for Cellular Decomposition from Histology Images](https://arxiv.org/abs/2507.07013)
*Yutong Sun,Sichen Zhu,Peng Qiu*

Main category: cs.CV

TL;DR: 提出了一种轻量级方法，利用预训练的病理学基础模型从H&E染色组织学图像预测细胞组成，避免了昂贵的空间转录组学实验。


<details>
  <summary>Details</summary>
Motivation: 数字病理学和深度学习的快速发展催生了病理学基础模型，结合空间转录组学技术，为从组织学图像中更精细地分析细胞组成提供了可能。

Method: 通过预训练的病理学基础模型提取特征嵌入，训练轻量级多层感知机（MLP）回归器预测细胞类型丰度。

Result: 方法在预测细胞组成方面表现优异，计算复杂度显著低于现有方法（如Hist2Cell）。

Conclusion: 该方法高效且准确，为从组织学图像中分析细胞组成提供了低成本解决方案。

Abstract: The rapid development of digital pathology and modern deep learning has
facilitated the emergence of pathology foundation models that are expected to
solve general pathology problems under various disease conditions in one
unified model, with or without fine-tuning. In parallel, spatial
transcriptomics has emerged as a transformative technology that enables the
profiling of gene expression on hematoxylin and eosin (H&E) stained histology
images. Spatial transcriptomics unlocks the unprecedented opportunity to dive
into existing histology images at a more granular, cellular level. In this
work, we propose a lightweight and training-efficient approach to predict
cellular composition directly from H&E-stained histology images by leveraging
information-enriched feature embeddings extracted from pre-trained pathology
foundation models. By training a lightweight multi-layer perceptron (MLP)
regressor on cell-type abundances derived via cell2location, our method
efficiently distills knowledge from pathology foundation models and
demonstrates the ability to accurately predict cell-type compositions from
histology images, without physically performing the costly spatial
transcriptomics. Our method demonstrates competitive performance compared to
existing methods such as Hist2Cell, while significantly reducing computational
complexity.

</details>


### [76] [MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation](https://arxiv.org/abs/2507.07015)
*Hui Li,Pengfei Yang,Juanyang Chen,Le Dong,Yanxin Chen,Quan Wang*

Main category: cs.CV

TL;DR: MST-Distill是一种新型跨模态知识蒸馏框架，通过混合专业教师模型和动态路由网络解决传统方法的局限性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法在跨模态场景中因数据和统计异质性难以利用跨模态教师模型的互补知识。

Method: 提出MST-Distill框架，结合跨模态和多模态教师模型，采用动态路由网络和掩码模块抑制模态差异。

Result: 在五个多模态数据集上的实验表明，MST-Distill显著优于现有方法。

Conclusion: MST-Distill有效解决了跨模态知识蒸馏中的路径选择和知识漂移问题，提升了知识转移效果。

Abstract: Knowledge distillation as an efficient knowledge transfer technique, has
achieved remarkable success in unimodal scenarios. However, in cross-modal
settings, conventional distillation methods encounter significant challenges
due to data and statistical heterogeneities, failing to leverage the
complementary prior knowledge embedded in cross-modal teacher models. This
paper empirically reveals two critical issues in existing approaches:
distillation path selection and knowledge drift. To address these limitations,
we propose MST-Distill, a novel cross-modal knowledge distillation framework
featuring a mixture of specialized teachers. Our approach employs a diverse
ensemble of teacher models across both cross-modal and multimodal
configurations, integrated with an instance-level routing network that
facilitates adaptive and dynamic distillation. This architecture effectively
transcends the constraints of traditional methods that rely on monotonous and
static teacher models. Additionally, we introduce a plug-in masking module,
independently trained to suppress modality-specific discrepancies and
reconstruct teacher representations, thereby mitigating knowledge drift and
enhancing transfer effectiveness. Extensive experiments across five diverse
multimodal datasets, spanning visual, audio, and text, demonstrate that our
method significantly outperforms existing state-of-the-art knowledge
distillation methods in cross-modal distillation tasks. The source code is
available at https://github.com/Gray-OREO/MST-Distill.

</details>


### [77] [Design and Implementation of an OCR-Powered Pipeline for Table Extraction from Invoices](https://arxiv.org/abs/2507.07029)
*Parshva Dhilankumar Patel*

Main category: cs.CV

TL;DR: 论文提出了一种基于OCR的流水线，用于高效从发票中提取表格数据，结合Tesseract OCR和自定义后处理逻辑，显著提升了数据提取的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决从非标准和噪声较多的发票文档中提取结构化表格数据的挑战，支持自动化财务流程和数字归档等实际应用。

Method: 采用动态预处理、表格边界检测和行列映射等技术，结合Tesseract OCR进行文本识别和自定义后处理逻辑。

Result: 开发的流水线显著提高了数据提取的准确性和一致性。

Conclusion: 该方法为发票表格提取提供了一种高效且可靠的解决方案，适用于实际应用场景。

Abstract: This paper presents the design and development of an OCR-powered pipeline for
efficient table extraction from invoices. The system leverages Tesseract OCR
for text recognition and custom post-processing logic to detect, align, and
extract structured tabular data from scanned invoice documents. Our approach
includes dynamic preprocessing, table boundary detection, and row-column
mapping, optimized for noisy and non-standard invoice formats. The resulting
pipeline significantly improves data extraction accuracy and consistency,
supporting real-world use cases such as automated financial workflows and
digital archiving.

</details>


### [78] [Evaluating Large Multimodal Models for Nutrition Analysis: A Benchmark Enriched with Contextual Metadata](https://arxiv.org/abs/2507.07048)
*Bruce Coburn,Jiangpeng He,Megan E. Rollo,Satvinder S. Dhaliwal,Deborah A. Kerr,Fengqing Zhu*

Main category: cs.CV

TL;DR: 研究探讨了如何通过整合上下文元数据（如GPS、时间戳和食物信息）提升大型多模态模型（LMMs）在营养分析中的表现，并引入新数据集ACETADA。实验表明，元数据整合显著降低了预测误差。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要评估专有模型（如GPT-4），忽略了其他LLMs的潜力，且上下文元数据对模型性能的影响尚未充分探索。

Method: 通过整合GPS、时间戳和食物信息等元数据，结合多种推理修饰符（如Chain-of-Thought），评估八种LMMs在营养分析中的表现。

Result: 元数据整合显著降低了预测营养值的平均绝对误差（MAE）和平均绝对百分比误差（MAPE）。

Conclusion: 上下文感知的LMMs在营养分析中具有显著潜力，未来可进一步优化和扩展。

Abstract: Large Multimodal Models (LMMs) are increasingly applied to meal images for
nutrition analysis. However, existing work primarily evaluates proprietary
models, such as GPT-4. This leaves the broad range of LLMs underexplored.
Additionally, the influence of integrating contextual metadata and its
interaction with various reasoning modifiers remains largely uncharted. This
work investigates how interpreting contextual metadata derived from GPS
coordinates (converted to location/venue type), timestamps (transformed into
meal/day type), and the food items present can enhance LMM performance in
estimating key nutritional values. These values include calories,
macronutrients (protein, carbohydrates, fat), and portion sizes. We also
introduce ACETADA, a new food-image dataset slated for public release. This
open dataset provides nutrition information verified by the dietitian and
serves as the foundation for our analysis. Our evaluation across eight LMMs
(four open-weight and four closed-weight) first establishes the benefit of
contextual metadata integration over straightforward prompting with images
alone. We then demonstrate how this incorporation of contextual information
enhances the efficacy of reasoning modifiers, such as Chain-of-Thought,
Multimodal Chain-of-Thought, Scale Hint, Few-Shot, and Expert Persona.
Empirical results show that integrating metadata intelligently, when applied
through straightforward prompting strategies, can significantly reduce the Mean
Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicted
nutritional values. This work highlights the potential of context-aware LMMs
for improved nutrition analysis.

</details>


### [79] [An AI Approach for Learning the Spectrum of the Laplace-Beltrami Operator](https://arxiv.org/abs/2507.07073)
*Yulin An,Enrique del Castillo*

Main category: cs.CV

TL;DR: 提出了一种基于几何深度学习的框架，用于高效预测CAD网格的Laplace-Beltrami谱，显著节省计算时间且保持准确性。


<details>
  <summary>Details</summary>
Motivation: 传统有限元方法（FEM）计算Laplace-Beltrami谱效率低，尤其在需要频繁处理大型网格的CAD机械零件数据库或质量控制应用中。

Method: 采用图神经网络（GNN）架构，结合高斯曲率、平均曲率和主曲率等丰富特征，预测Laplace-Beltrami谱。

Result: 实验表明，该方法将计算时间减少约5倍，同时保持与FEM相当的准确性。

Conclusion: Laplace-Beltrami谱是可学习的，提出的深度学习框架为高效计算提供了可行方案。

Abstract: The spectrum of the Laplace-Beltrami (LB) operator is central in geometric
deep learning tasks, capturing intrinsic properties of the shape of the object
under consideration. The best established method for its estimation, from a
triangulated mesh of the object, is based on the Finite Element Method (FEM),
and computes the top k LB eigenvalues with a complexity of O(Nk), where N is
the number of points. This can render the FEM method inefficient when
repeatedly applied to databases of CAD mechanical parts, or in quality control
applications where part metrology is acquired as large meshes and decisions
about the quality of each part are needed quickly and frequently. As a solution
to this problem, we present a geometric deep learning framework to predict the
LB spectrum efficiently given the CAD mesh of a part, achieving significant
computational savings without sacrificing accuracy, demonstrating that the LB
spectrum is learnable. The proposed Graph Neural Network architecture uses a
rich set of part mesh features - including Gaussian curvature, mean curvature,
and principal curvatures. In addition to our trained network, we make
available, for repeatability, a large curated dataset of real-world mechanical
CAD models derived from the publicly available ABC dataset used for training
and testing. Experimental results show that our method reduces computation time
of the LB spectrum by approximately 5 times over linear FEM while delivering
competitive accuracy.

</details>


### [80] [Reading a Ruler in the Wild](https://arxiv.org/abs/2507.07077)
*Yimu Pan,Manas Mehta,Gwen Sincerbeaux,Jeffery A. Goldstein,Alison D. Gernand,James Z. Wang*

Main category: cs.CV

TL;DR: RulerNet是一个深度学习框架，通过将尺子刻度检测统一为关键点检测问题，并利用几何级数参数表示尺子，实现了在复杂环境下的高精度尺度估计。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉中像素测量到实际尺寸的转换是一个基础性挑战，限制了生物医学、法医学、营养分析和电子商务等关键应用的发展。

Method: RulerNet通过关键点检测和几何级数参数表示尺子，结合合成数据增强训练多样性，并提出了轻量级网络DeepGP以实现实时估计。

Result: 实验表明，RulerNet在复杂现实条件下提供了准确、一致且高效的尺度估计。

Conclusion: RulerNet作为一种通用测量工具，具有广泛的应用潜力，并可与其他视觉组件集成以实现自动化分析。

Abstract: Accurately converting pixel measurements into absolute real-world dimensions
remains a fundamental challenge in computer vision and limits progress in key
applications such as biomedicine, forensics, nutritional analysis, and
e-commerce. We introduce RulerNet, a deep learning framework that robustly
infers scale "in the wild" by reformulating ruler reading as a unified
keypoint-detection problem and by representing the ruler with
geometric-progression parameters that are invariant to perspective
transformations. Unlike traditional methods that rely on handcrafted thresholds
or rigid, ruler-specific pipelines, RulerNet directly localizes centimeter
marks using a distortion-invariant annotation and training strategy, enabling
strong generalization across diverse ruler types and imaging conditions while
mitigating data scarcity. We also present a scalable synthetic-data pipeline
that combines graphics-based ruler generation with ControlNet to add
photorealistic context, greatly increasing training diversity and improving
performance. To further enhance robustness and efficiency, we propose DeepGP, a
lightweight feed-forward network that regresses geometric-progression
parameters from noisy marks and eliminates iterative optimization, enabling
real-time scale estimation on mobile or edge devices. Experiments show that
RulerNet delivers accurate, consistent, and efficient scale estimates under
challenging real-world conditions. These results underscore its utility as a
generalizable measurement tool and its potential for integration with other
vision components for automated, scale-aware analysis in high-impact domains. A
live demo is available at https://huggingface.co/spaces/ymp5078/RulerNet-Demo.

</details>


### [81] [Evaluating Attribute Confusion in Fashion Text-to-Image Generation](https://arxiv.org/abs/2507.07079)
*Ziyue Liu,Federico Girella,Yiming Wang,Davide Talon*

Main category: cs.CV

TL;DR: 论文提出了一种新的自动评估指标L-VQAScore，用于解决现有T2I生成模型在复杂组合生成（如时尚领域）中评估的局限性，特别是在实体-属性关联方面。


<details>
  <summary>Details</summary>
Motivation: 现有T2I评估方法在评估复杂实体-属性语义时存在局限性，尤其是属性混淆问题。

Method: 基于视觉问答（VQA）定位策略，提出局部化评估协议和L-VQAScore指标，结合视觉定位和VQA探测。

Result: 在新数据集上，L-VQAScore在与人评估相关性上优于现有方法，能有效捕捉细粒度实体-属性关联。

Conclusion: L-VQAScore是一种可靠且可扩展的替代主观评估的方法。

Abstract: Despite the rapid advances in Text-to-Image (T2I) generation models, their
evaluation remains challenging in domains like fashion, involving complex
compositional generation. Recent automated T2I evaluation methods leverage
pre-trained vision-language models to measure cross-modal alignment. However,
our preliminary study reveals that they are still limited in assessing rich
entity-attribute semantics, facing challenges in attribute confusion, i.e.,
when attributes are correctly depicted but associated to the wrong entities. To
address this, we build on a Visual Question Answering (VQA) localization
strategy targeting one single entity at a time across both visual and textual
modalities. We propose a localized human evaluation protocol and introduce a
novel automatic metric, Localized VQAScore (L-VQAScore), that combines visual
localization with VQA probing both correct (reflection) and miss-localized
(leakage) attribute generation. On a newly curated dataset featuring
challenging compositional alignment scenarios, L-VQAScore outperforms
state-of-the-art T2I evaluation methods in terms of correlation with human
judgments, demonstrating its strength in capturing fine-grained
entity-attribute associations. We believe L-VQAScore can be a reliable and
scalable alternative to subjective evaluations.

</details>


### [82] [Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data](https://arxiv.org/abs/2507.07095)
*Ke Fan,Shunlin Lu,Minyue Dai,Runyi Yu,Lixing Xiao,Zhiyang Dou,Junting Dong,Lizhuang Ma,Jingbo Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于文本描述生成多样且自然的人体运动序列的方法，解决了零样本泛化能力不足的问题，并引入了最大的运动数据集和评估基准。


<details>
  <summary>Details</summary>
Motivation: 当前方法在零样本泛化能力上存在不足，主要由于训练数据集规模有限，且缺乏全面的评估框架。

Method: 开发了高效的标注流程并构建了MotionMillion数据集（2000小时、200万条高质量运动序列），提出了MotionMillion-Eval评估基准，并采用可扩展的7B参数模型。

Result: 模型在MotionMillion-Eval上表现出色，能够泛化到域外和复杂组合运动。

Conclusion: 该研究为零样本人体运动生成迈出了重要一步，代码已开源。

Abstract: Generating diverse and natural human motion sequences based on textual
descriptions constitutes a fundamental and challenging research area within the
domains of computer vision, graphics, and robotics. Despite significant
advancements in this field, current methodologies often face challenges
regarding zero-shot generalization capabilities, largely attributable to the
limited size of training datasets. Moreover, the lack of a comprehensive
evaluation framework impedes the advancement of this task by failing to
identify directions for improvement. In this work, we aim to push
text-to-motion into a new era, that is, to achieve the generalization ability
of zero-shot. To this end, firstly, we develop an efficient annotation pipeline
and introduce MotionMillion-the largest human motion dataset to date, featuring
over 2,000 hours and 2 million high-quality motion sequences. Additionally, we
propose MotionMillion-Eval, the most comprehensive benchmark for evaluating
zero-shot motion generation. Leveraging a scalable architecture, we scale our
model to 7B parameters and validate its performance on MotionMillion-Eval. Our
results demonstrate strong generalization to out-of-domain and complex
compositional motions, marking a significant step toward zero-shot human motion
generation. The code is available at
https://github.com/VankouF/MotionMillion-Codes.

</details>


### [83] [Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models](https://arxiv.org/abs/2507.07104)
*Tiezheng Zhang,Yitong Li,Yu-cheng Chou,Jieneng Chen,Alan Yuille,Chen Wei,Junfei Xiao*

Main category: cs.CV

TL;DR: 提出了一种名为VLV的自动编码框架，通过利用预训练组件（视觉编码器、T2I扩散模型解码器和LLM）显著降低训练成本和数据需求，实现高效视觉语言模型构建。


<details>
  <summary>Details</summary>
Motivation: 传统视觉语言模型需要大量高质量图像-文本对和计算资源，成本高昂。VLV框架旨在通过预训练组件和知识蒸馏减少这些需求。

Method: 利用预训练视觉编码器、T2I扩散模型解码器和LLM，通过信息瓶颈和语言表示空间正则化，构建VLV自动编码框架。

Result: VLV框架在低成本（<1000美元）下实现了与GPT-4o和Gemini 2.0 Flash相当的SoTA性能。

Conclusion: VLV框架为视觉语言模型提供了一种高效、低成本的替代方案，显著减少了对大规模数据的需求。

Abstract: Building state-of-the-art Vision-Language Models (VLMs) with strong
captioning capabilities typically necessitates training on billions of
high-quality image-text pairs, requiring millions of GPU hours. This paper
introduces the Vision-Language-Vision (VLV) auto-encoder framework, which
strategically leverages key pretrained components: a vision encoder, the
decoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large
Language Model (LLM). Specifically, we establish an information bottleneck by
regularizing the language representation space, achieved through freezing the
pretrained T2I diffusion decoder. Our VLV pipeline effectively distills
knowledge from the text-conditioned diffusion model using continuous
embeddings, demonstrating comprehensive semantic understanding via high-quality
reconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the
intermediate language representations into detailed descriptions, we construct
a state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o
and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and
significantly reduces data requirements; by primarily utilizing single-modal
images for training and maximizing the utility of existing pretrained models
(image encoder, T2I diffusion model, and LLM), it circumvents the need for
massive paired image-text datasets, keeping the total training expenditure
under $1,000 USD.

</details>


### [84] [Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor](https://arxiv.org/abs/2507.07106)
*Vatsal Agarwal,Matthew Gwilliam,Gefen Kohavi,Eshan Verma,Daniel Ulbricht,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 该论文探讨了使用预训练的文本到图像扩散模型作为视觉编码器的潜力，以弥补CLIP在捕捉细粒度细节上的不足，并提出了一种融合策略以提升视觉问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs依赖CLIP作为视觉编码器，但其无法捕捉细粒度细节，限制了视觉问答能力。本文旨在探索扩散模型是否能作为更优的视觉编码器。

Method: 分析扩散模型的内部表示，发现其语义丰富且能对齐图像-文本。提出融合CLIP和扩散特征的策略，并解决LLM信息泄漏问题。

Result: 在通用VQA和专用MLLM基准测试中，融合策略表现出色，尤其在需要空间和组合推理的任务中。

Conclusion: 扩散模型在视觉理解任务中具有潜力，特别是在需要细粒度细节的任务中。融合策略有效提升了性能。

Abstract: Recent advances in multimodal large language models (MLLMs) have enabled
image-based question-answering capabilities. However, a key limitation is the
use of CLIP as the visual encoder; while it can capture coarse global
information, it often can miss fine-grained details that are relevant to the
input query. To address these shortcomings, this work studies whether
pre-trained text-to-image diffusion models can serve as instruction-aware
visual encoders. Through an analysis of their internal representations, we find
diffusion features are both rich in semantics and can encode strong image-text
alignment. Moreover, we find that we can leverage text conditioning to focus
the model on regions relevant to the input question. We then investigate how to
align these features with large language models and uncover a leakage
phenomenon, where the LLM can inadvertently recover information from the
original diffusion prompt. We analyze the causes of this leakage and propose a
mitigation strategy. Based on these insights, we explore a simple fusion
strategy that utilizes both CLIP and conditional diffusion features. We
evaluate our approach on both general VQA and specialized MLLM benchmarks,
demonstrating the promise of diffusion models for visual understanding,
particularly in vision-centric tasks that require spatial and compositional
reasoning. Our project page can be found
https://vatsalag99.github.io/mustafar/.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [85] [X-ray transferable polyrepresentation learning](https://arxiv.org/abs/2507.06264)
*Weronika Hryniewska-Guzik,Przemyslaw Biecek*

Main category: eess.IV

TL;DR: 论文提出了一种名为“多表征”（polyrepresentation）的新概念，通过整合同一模态的多种表征（如Siamese Network的向量嵌入、自监督模型和可解释的放射组学特征）来提升机器学习算法的性能。该方法在X射线图像上验证了其有效性和可迁移性。


<details>
  <summary>Details</summary>
Motivation: 机器学习算法的成功依赖于有意义的特征提取，而数据表示的质量和泛化能力是关键挑战。

Method: 提出多表征概念，整合同一模态的多种表征（如Siamese Network、自监督模型和放射组学特征）。

Result: 多表征方法在性能指标上优于单一表征，且在X射线图像上展示了可迁移性。

Conclusion: 多表征方法具有实用性和资源效率，适用于医学数据及其他领域，展示了广泛的潜在影响。

Abstract: The success of machine learning algorithms is inherently related to the
extraction of meaningful features, as they play a pivotal role in the
performance of these algorithms. Central to this challenge is the quality of
data representation. However, the ability to generalize and extract these
features effectively from unseen datasets is also crucial. In light of this, we
introduce a novel concept: the polyrepresentation. Polyrepresentation
integrates multiple representations of the same modality extracted from
distinct sources, for example, vector embeddings from the Siamese Network,
self-supervised models, and interpretable radiomic features. This approach
yields better performance metrics compared to relying on a single
representation. Additionally, in the context of X-ray images, we demonstrate
the transferability of the created polyrepresentation to a smaller dataset,
underscoring its potential as a pragmatic and resource-efficient approach in
various image-related solutions. It is worth noting that the concept of
polyprepresentation on the example of medical data can also be applied to other
domains, showcasing its versatility and broad potential impact.

</details>


### [86] [Optimization of Fractal Image Compression](https://arxiv.org/abs/2507.06325)
*Nastaran Pourshab*

Main category: eess.IV

TL;DR: 本文研究了优化分形图像压缩（FIC）的方法，提出了一种名为“盒计数法”的新方法，以提高压缩比并减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 分形图像压缩虽然能实现高压缩比，但计算成本高昂。本文旨在优化FIC的效率。

Method: 采用盒计数法估计分形维度，该方法简单易集成到FIC中。

Result: 优化技术显著提升了压缩比和压缩时间。

Conclusion: 盒计数法是一种高效且易于实现的FIC优化方法。

Abstract: Fractal Image Compression (FIC) is a lossy image compression technique that
leverages self-similarity within an image to achieve high compression ratios.
However, the process of compressing the image is computationally expensive.
This paper investigates optimization techniques to improve the efficiency of
FIC, focusing on increasing compression ratio and reducing computational time.
The paper explores a novel approach named the Box Counting Method for
estimating fractal dimensions, which is very simple to integrate into FIC
compared to other algorithms. The results show that implementing these
optimization techniques enhances both the compression ratio and the compression
time.

</details>


### [87] [Mamba Goes HoME: Hierarchical Soft Mixture-of-Experts for 3D Medical Image Segmentation](https://arxiv.org/abs/2507.06363)
*Szymon Płotka,Maciej Chrabaszcz,Gizem Mert,Ewa Szczurek,Arkadiusz Sitek*

Main category: eess.IV

TL;DR: 提出了一种名为HoME的分层软专家混合方法，用于高效处理3D医学图像分割，通过局部和全局专家路由提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D医学图像处理中的模态多样性和数据变异性问题。

Method: 基于Mamba状态空间模型，采用两级令牌路由层（局部和全局SMoE）进行特征提取和上下文建模。

Result: 在多种3D医学成像模态和数据质量下，性能优于现有方法。

Conclusion: HoME通过分层设计显著提升了分割性能和泛化能力。

Abstract: In recent years, artificial intelligence has significantly advanced medical
image segmentation. However, challenges remain, including efficient 3D medical
image processing across diverse modalities and handling data variability. In
this work, we introduce Hierarchical Soft Mixture-of-Experts (HoME), a
two-level token-routing layer for efficient long-context modeling, specifically
designed for 3D medical image segmentation. Built on the Mamba state-space
model (SSM) backbone, HoME enhances sequential modeling through sparse,
adaptive expert routing. The first stage employs a Soft Mixture-of-Experts
(SMoE) layer to partition input sequences into local groups, routing tokens to
specialized per-group experts for localized feature extraction. The second
stage aggregates these outputs via a global SMoE layer, enabling cross-group
information fusion and global context refinement. This hierarchical design,
combining local expert routing with global expert refinement improves
generalizability and segmentation performance, surpassing state-of-the-art
results across datasets from the three most commonly used 3D medical imaging
modalities and data quality.

</details>


### [88] [Mitigating Multi-Sequence 3D Prostate MRI Data Scarcity through Domain Adaptation using Locally-Trained Latent Diffusion Models for Prostate Cancer Detection](https://arxiv.org/abs/2507.06384)
*Emerson P. Grabke,Babak Taati,Masoom A. Haider*

Main category: eess.IV

TL;DR: CCELLA++扩展了CCELLA，通过生成双参数前列腺MRI（bpMRI）并研究域适应，提高了分类器性能。


<details>
  <summary>Details</summary>
Motivation: 解决CCELLA在仅支持AxT2序列、未研究域转移及偏重放射学结果的问题，提升临床实用性。

Method: 扩展CCELLA以生成bpMRI（包括AxT2、HighB和ADC），研究域适应，通过预训练和微调分类器。

Result: CCELLA++在HighB和ADC序列上表现优于CCELLA，分类器预训练性能优于真实数据。

Conclusion: CCELLA++生成的合成bpMRI优于真实数据，未来需优化图像质量和多序列训练。

Abstract: Objective: Latent diffusion models (LDMs) could mitigate data scarcity
challenges affecting machine learning development for medical image
interpretation. The recent CCELLA LDM improved prostate cancer detection
performance using synthetic MRI for classifier training but was limited to the
axial T2-weighted (AxT2) sequence, did not investigate inter-institutional
domain shift, and prioritized radiology over histopathology outcomes. We
propose CCELLA++ to address these limitations and improve clinical utility.
Methods: CCELLA++ expands CCELLA for simultaneous biparametric prostate MRI
(bpMRI) generation, including the AxT2, high b-value diffusion series (HighB)
and apparent diffusion coefficient map (ADC). Domain adaptation was
investigated by pretraining classifiers on real or LDM-generated synthetic data
from an internal institution, followed with fine-tuning on progressively
smaller fractions of an out-of-distribution, external dataset. Results:
CCELLA++ improved 3D FID for HighB and ADC but not AxT2 (0.013, 0.012, 0.063
respectively) sequences compared to CCELLA (0.060). Classifier pretraining with
CCELLA++ bpMRI outperformed real bpMRI in AP and AUC for all domain adaptation
scenarios. CCELLA++ pretraining achieved highest classifier performance below
50% (n=665) external dataset volume. Conclusion: Synthetic bpMRI generated by
our method can improve downstream classifier generalization and performance
beyond real bpMRI or CCELLA-generated AxT2-only images. Future work should seek
to quantify medical image sample quality, balance multi-sequence LDM training,
and condition the LDM with additional information. Significance: The proposed
CCELLA++ LDM can generate synthetic bpMRI that outperforms real data for domain
adaptation with a limited target institution dataset. Our code is available at
https://github.com/grabkeem/CCELLA-plus-plus

</details>


### [89] [Attention-Enhanced Deep Learning Ensemble for Breast Density Classification in Mammography](https://arxiv.org/abs/2507.06410)
*Peyman Sharifian,Xiaotong Hong,Alireza Karimian,Mehdi Amini,Hossein Arabi*

Main category: eess.IV

TL;DR: 该研究提出了一种基于深度学习的自动化系统，用于乳腺密度的二元分类（低密度A/B vs.高密度C/D），通过优化集成多个卷积神经网络和新型损失函数，取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺密度评估对乳腺癌筛查至关重要，但高密度乳腺（BI-RADS C/D类）既是癌症风险因素，又增加了肿瘤检测的技术难度。

Method: 研究比较了四种改进的卷积神经网络（ResNet18、ResNet50、EfficientNet-B0和DenseNet121），结合通道注意力机制，并开发了新型损失函数（Combined Focal Label Smoothing Loss）。预处理包括CLAHE和数据增强，最终通过集成投票优化模型性能。

Result: 系统性能优异（AUC: 0.963, F1-score: 0.952），优于单一模型。

Conclusion: 该系统有望标准化临床乳腺密度评估，提高筛查效率和早期癌症检出率，减少放射科医生间的差异。

Abstract: Breast density assessment is a crucial component of mammographic
interpretation, with high breast density (BI-RADS categories C and D)
representing both a significant risk factor for developing breast cancer and a
technical challenge for tumor detection. This study proposes an automated deep
learning system for robust binary classification of breast density (low: A/B
vs. high: C/D) using the VinDr-Mammo dataset. We implemented and compared four
advanced convolutional neural networks: ResNet18, ResNet50, EfficientNet-B0,
and DenseNet121, each enhanced with channel attention mechanisms. To address
the inherent class imbalance, we developed a novel Combined Focal Label
Smoothing Loss function that integrates focal loss, label smoothing, and
class-balanced weighting. Our preprocessing pipeline incorporated advanced
techniques, including contrast-limited adaptive histogram equalization (CLAHE)
and comprehensive data augmentation. The individual models were combined
through an optimized ensemble voting approach, achieving superior performance
(AUC: 0.963, F1-score: 0.952) compared to any single model. This system
demonstrates significant potential to standardize density assessments in
clinical practice, potentially improving screening efficiency and early cancer
detection rates while reducing inter-observer variability among radiologists.

</details>


### [90] [Capsule-ConvKAN: A Hybrid Neural Approach to Medical Image Classification](https://arxiv.org/abs/2507.06417)
*Laura Pituková,Peter Sinčák,László József Kovács*

Main category: eess.IV

TL;DR: 比较四种神经网络架构，提出新的Capsule-ConvKAN模型，在生物医学图像分类中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 改进特征表示和分类准确性，特别是在具有挑战性的生物医学图像数据中。

Method: 结合Capsule Network的动态路由和空间层次能力与ConvKAN的灵活可解释函数逼近，提出Capsule-ConvKAN。

Result: 在组织病理学图像数据集上，Capsule-ConvKAN以91.21%的准确率表现最佳。

Conclusion: Capsule-ConvKAN在捕捉空间模式和管理复杂特征方面具有潜力，优于传统卷积模型。

Abstract: This study conducts a comprehensive comparison of four neural network
architectures: Convolutional Neural Network, Capsule Network, Convolutional
Kolmogorov--Arnold Network, and the newly proposed Capsule--Convolutional
Kolmogorov--Arnold Network. The proposed Capsule-ConvKAN architecture combines
the dynamic routing and spatial hierarchy capabilities of Capsule Network with
the flexible and interpretable function approximation of Convolutional
Kolmogorov--Arnold Networks. This novel hybrid model was developed to improve
feature representation and classification accuracy, particularly in challenging
real-world biomedical image data. The architectures were evaluated on a
histopathological image dataset, where Capsule-ConvKAN achieved the highest
classification performance with an accuracy of 91.21\%. The results demonstrate
the potential of the newly introduced Capsule-ConvKAN in capturing spatial
patterns, managing complex features, and addressing the limitations of
traditional convolutional models in medical image classification.

</details>


### [91] [COS2A: Conversion from Sentinel-2 to AVIRIS Hyperspectral Data Using Interpretable Algorithm With Spectral-Spatial Duality](https://arxiv.org/abs/2507.06575)
*Chia-Hsiang Lin,Jui-Ting Chen,Zi-Chao Leng,Jhao-Ting Lin*

Main category: eess.IV

TL;DR: 该论文提出了一种新算法（COS2A），将Sentinel-2的多分辨率12波段图像转换为高分辨率（10米）的AVIRIS级高光谱图像（172波段），解决了现有方法未处理的多分辨率问题。


<details>
  <summary>Details</summary>
Motivation: Sentinel-2卫星的12波段图像分辨率不均且波段有限，限制了其在遥感识别任务中的效果。现有超分辨率方法未解决多分辨率问题且仅适用于31波段的CAVE级图像，无法满足实际需求。

Method: 通过定制新算法，结合深度展开正则化和Q-二次范数正则化，提出COS2A算法，利用光谱-空间对偶性实现超分辨率。

Result: 实验验证表明，COS2A算法在不同土地覆盖类型上表现出卓越的光谱超分辨率效果。

Conclusion: 该研究首次解决了Sentinel-2数据到AVIRIS级高光谱图像的转换问题，为历史数据提供了高质量的高光谱对应版本。

Abstract: The Sentinel-2 satellite, launched by the European Space Agency (ESA), offers
extensive spatial coverage and has become indispensable in a wide range of
remote sensing applications. However, it just has 12 spectral bands, making
substances/objects identification less effective, not mentioning the varying
spatial resolutions (10/20/60 m) across the 12 bands. If such a
multi-resolution 12-band image can be computationally converted into a
hyperspectral image with uniformly high resolution (i.e., 10 m), it
significantly facilitates remote identification tasks. Though there are some
spectral super-resolution methods, they did not address the multi-resolution
issue on one hand, and, more seriously, they mostly focused on the CAVE-level
hyperspectral image reconstruction (involving only 31 visible bands) on the
other hand, greatly limiting their applicability in real-world remote sensing
scenarios. We ambitiously aim to convert Sentinel-2 data directly into NASA's
AVIRIS-level hyperspectral image (encompassing up to 172 visible and
near-infrared (NIR) bands, after ignoring those absorption/corruption ones).
For the first time, this paper solves this specific super-resolution problem
(highly ill-posed), allowing all historical Sentinel-2 data to have their
corresponding high-standard AVIRIS counterparts. We achieve so by customizing a
novel algorithm that introduces deep unfolding regularization and
Q-quadratic-norm regularization into the so-called convex/deep (CODE)
small-data learning criterion. Based on the derived spectral-spatial duality,
the proposed interpretable COS2A algorithm demonstrates superior spectral
super-resolution results across diverse land cover types, as validated through
extensive experiments.

</details>


### [92] [Airway Segmentation Network for Enhanced Tubular Feature Extraction](https://arxiv.org/abs/2507.06581)
*Qibiao Wu,Yagang Wang,Qian Zhang*

Main category: eess.IV

TL;DR: 提出了一种名为TfeNet的新型管状特征提取网络，用于解决传统卷积神经网络在气道分割中难以捕捉细微结构的问题。


<details>
  <summary>Details</summary>
Motivation: 手动标注气道区域耗时且依赖专业知识，自动分割是快速支气管镜导航和机器人系统临床部署的前提。

Method: TfeNet引入方向感知卷积操作和管状特征融合模块（TFFM），通过空间旋转变换和不对称卷积增强对细微气道结构的关注。

Result: 在多个数据集上表现优异，特别是在ATM22数据集上达到94.95%的最高分。

Conclusion: TfeNet在气道分割任务中实现了更准确和连续的结构预测，优于现有方法。

Abstract: Manual annotation of airway regions in computed tomography images is a
time-consuming and expertise-dependent task. Automatic airway segmentation is
therefore a prerequisite for enabling rapid bronchoscopic navigation and the
clinical deployment of bronchoscopic robotic systems. Although convolutional
neural network methods have gained considerable attention in airway
segmentation, the unique tree-like structure of airways poses challenges for
conventional and deformable convolutions, which often fail to focus on fine
airway structures, leading to missed segments and discontinuities. To address
this issue, this study proposes a novel tubular feature extraction network,
named TfeNet. TfeNet introduces a novel direction-aware convolution operation
that first applies spatial rotation transformations to adjust the sampling
positions of linear convolution kernels. The deformed kernels are then
represented as line segments or polylines in 3D space. Furthermore, a tubular
feature fusion module (TFFM) is designed based on asymmetric convolution and
residual connection strategies, enhancing the network's focus on subtle airway
structures. Extensive experiments conducted on one public dataset and two
datasets used in airway segmentation challenges demonstrate that the proposed
TfeNet achieves more accuracy and continuous airway structure predictions
compared with existing methods. In particular, TfeNet achieves the highest
overall score of 94.95% on the current largest airway segmentation dataset,
Airway Tree Modeling(ATM22), and demonstrates advanced performance on the lung
fibrosis dataset(AIIB23). The code is available at
https://github.com/QibiaoWu/TfeNet.

</details>


### [93] [Photometric Stereo using Gaussian Splatting and inverse rendering](https://arxiv.org/abs/2507.06684)
*Matéo Ducastel,David Tschumperlé,Yvain Quéau*

Main category: eess.IV

TL;DR: 该论文提出了一种基于高斯溅射形式的新方法，用于解决校准光度立体问题，通过简化的光表示模型展示了其潜力。


<details>
  <summary>Details</summary>
Motivation: 现有光度立体算法依赖神经网络，通过先验学习或逆渲染优化，但缺乏可解释性。本文旨在利用高斯溅射形式改进3D逆渲染，以更可解释的方式优化场景重建。

Method: 采用高斯溅射形式参数化3D场景，并结合简化的光表示模型进行优化。

Result: 展示了高斯溅射渲染引擎在光度立体问题中的潜力。

Conclusion: 该方法为光度立体问题提供了一种更可解释且高效的解决方案。

Abstract: Recent state-of-the-art algorithms in photometric stereo rely on neural
networks and operate either through prior learning or inverse rendering
optimization. Here, we revisit the problem of calibrated photometric stereo by
leveraging recent advances in 3D inverse rendering using the Gaussian Splatting
formalism. This allows us to parameterize the 3D scene to be reconstructed and
optimize it in a more interpretable manner. Our approach incorporates a
simplified model for light representation and demonstrates the potential of the
Gaussian Splatting rendering engine for the photometric stereo problem.

</details>


### [94] [QoE Optimization for Semantic Self-Correcting Video Transmission in Multi-UAV Networks](https://arxiv.org/abs/2507.06717)
*Xuyang Chen,Chong Huang,Daquan Feng,Lei Luo,Yao Sun,Xiang-Gen Xia*

Main category: eess.IV

TL;DR: 提出了一种新型语义自校正视频传输框架（SSCV-G），通过超细粒度比特率控制和多帧联合解码，显著提升了无人机视频流的带宽效率和抗丢包能力。


<details>
  <summary>Details</summary>
Motivation: 实时无人机视频流在远程监控、应急响应等时间敏感应用中至关重要，但面临带宽限制、延迟波动和高丢包率等挑战。

Method: SSCV-G将视频帧编码为紧凑的语义码本空间，自适应发送语义索引子集；接收端使用时空视觉变换器（ST-ViT）进行多帧联合解码；结合多用户近端策略优化（MUPPO）动态优化资源分配和语义比特率选择。

Result: 实验表明，SSCV-G在编码效率、带宽适应性和抗丢包能力上显著优于现有视频编解码器；MUPPO方案在用户体验质量（QoE）优化上表现优异。

Conclusion: SSCV-G和MUPPO方案为实时无人机视频流提供了高效、鲁棒的解决方案，适用于动态网络环境。

Abstract: Real-time unmanned aerial vehicle (UAV) video streaming is essential for
time-sensitive applications, including remote surveillance, emergency response,
and environmental monitoring. However, it faces challenges such as limited
bandwidth, latency fluctuations, and high packet loss. To address these issues,
we propose a novel semantic self-correcting video transmission framework with
ultra-fine bitrate granularity (SSCV-G). In SSCV-G, video frames are encoded
into a compact semantic codebook space, and the transmitter adaptively sends a
subset of semantic indices based on bandwidth availability, enabling
fine-grained bitrate control for improved bandwidth efficiency. At the
receiver, a spatio-temporal vision transformer (ST-ViT) performs multi-frame
joint decoding to reconstruct dropped semantic indices by modeling intra- and
inter-frame dependencies. To further improve performance under dynamic network
conditions, we integrate a multi-user proximal policy optimization (MUPPO)
reinforcement learning scheme that jointly optimizes communication resource
allocation and semantic bitrate selection to maximize user Quality of
Experience (QoE). Extensive experiments demonstrate that the proposed SSCV-G
significantly outperforms state-of-the-art video codecs in coding efficiency,
bandwidth adaptability, and packet loss robustness. Moreover, the proposed
MUPPO-based QoE optimization consistently surpasses existing benchmarks.

</details>


### [95] [Fast Equivariant Imaging: Acceleration for Unsupervised Learning via Augmented Lagrangian and Auxiliary PnP Denoisers](https://arxiv.org/abs/2507.06764)
*Guixian Xu,Jinglai Li,Junqi Tang*

Main category: eess.IV

TL;DR: 提出Fast Equivariant Imaging (FEI)，一种无需真实数据的无监督学习框架，通过拉格朗日乘数法和即插即用去噪器优化问题，显著提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统Equivariant Imaging (EI)在训练深度成像网络时效率低下的问题，提出更高效的替代方案。

Method: 基于拉格朗日乘数法重新构建EI优化问题，并利用即插即用去噪器（PnP）实现快速训练。

Result: 在CT100数据集上训练U-Net时，PnP-FEI比标准EI快10倍，且泛化性能更优。

Conclusion: FEI框架在无监督学习场景下表现出显著的高效性和性能优势。

Abstract: We propose Fast Equivariant Imaging (FEI), a novel unsupervised learning
framework to efficiently train deep imaging networks without ground-truth data.
From the perspective of reformulating the Equivariant Imaging based
optimization problem via the method of Lagrange multipliers and utilizing
plug-and-play denoisers, this novel unsupervised scheme shows superior
efficiency and performance compared to vanilla Equivariant Imaging paradigm. In
particular, our PnP-FEI scheme achieves an order-of-magnitude (10x)
acceleration over standard EI on training U-Net with CT100 dataset for X-ray CT
reconstruction, with improved generalization performance.

</details>


### [96] [Speckle2Self: Self-Supervised Ultrasound Speckle Reduction Without Clean Data](https://arxiv.org/abs/2507.06828)
*Xuesong Li,Nassir Navab,Zhongliang Jiang*

Main category: eess.IV

TL;DR: Speckle2Self是一种新型自监督算法，用于仅使用单次噪声观测减少医学超声图像中的斑点噪声。


<details>
  <summary>Details</summary>
Motivation: 医学超声图像中的斑点噪声具有组织依赖性，传统去噪方法无法直接应用，因此需要一种新的自监督方法。

Method: 通过多尺度扰动（MSP）操作引入组织依赖的斑点模式变化，同时保留共享的解剖结构，将干净图像建模为低秩信号并分离稀疏噪声。

Result: Speckle2Self在模拟和真实超声图像上表现优于传统滤波器和最先进的基于学习的方法，并展示了良好的泛化能力。

Conclusion: Speckle2Self为医学超声图像斑点噪声去除提供了一种有效的自监督解决方案，具有广泛的应用潜力。

Abstract: Image denoising is a fundamental task in computer vision, particularly in
medical ultrasound (US) imaging, where speckle noise significantly degrades
image quality. Although recent advancements in deep neural networks have led to
substantial improvements in denoising for natural images, these methods cannot
be directly applied to US speckle noise, as it is not purely random. Instead,
US speckle arises from complex wave interference within the body
microstructure, making it tissue-dependent. This dependency means that
obtaining two independent noisy observations of the same scene, as required by
pioneering Noise2Noise, is not feasible. Additionally, blind-spot networks also
cannot handle US speckle noise due to its high spatial dependency. To address
this challenge, we introduce Speckle2Self, a novel self-supervised algorithm
for speckle reduction using only single noisy observations. The key insight is
that applying a multi-scale perturbation (MSP) operation introduces
tissue-dependent variations in the speckle pattern across different scales,
while preserving the shared anatomical structure. This enables effective
speckle suppression by modeling the clean image as a low-rank signal and
isolating the sparse noise component. To demonstrate its effectiveness,
Speckle2Self is comprehensively compared with conventional filter-based
denoising algorithms and SOTA learning-based methods, using both realistic
simulated US images and human carotid US images. Additionally, data from
multiple US machines are employed to evaluate model generalization and
adaptability to images from unseen domains. \textit{Code and datasets will be
released upon acceptance.

</details>


### [97] [Dataset and Benchmark for Enhancing Critical Retained Foreign Object Detection](https://arxiv.org/abs/2507.06937)
*Yuli Wang,Victoria R. Shi,Liwei Zhou,Richard Chin,Yuwei Dai,Yuanyun Hu,Cheng-Yi Li,Haoyue Guan,Jiashu Cheng,Yu Sun,Cheng Ting Lin,Ihab Kamel,Premal Trivedi,Pamela Johnson,John Eng,Harrison Bai*

Main category: eess.IV

TL;DR: 论文介绍了首个针对关键遗留异物（RFOs）的胸部X光数据集Hopkins RFOs Bench，并评估了多种目标检测模型和合成图像方法，以提升关键RFOs的检测能力。


<details>
  <summary>Details</summary>
Motivation: 关键RFOs对患者安全和医疗机构造成严重风险，但现有数据集仅包含非关键RFOs，限制了AI检测算法的开发。

Method: 引入Hopkins RFOs Bench数据集，评估多种目标检测模型，并探索两种合成图像方法（DeepDRR-RFO和RoentGen-RFO）以解决数据稀缺问题。

Result: 研究提供了对合成图像方法优缺点的全面分析，并展示了Hopkins RFOs Bench在提升关键RFOs检测中的潜力。

Conclusion: Hopkins RFOs Bench和研究成果显著推动了AI在临床胸部X光中检测关键RFOs的可靠性和通用性。

Abstract: Critical retained foreign objects (RFOs), including surgical instruments like
sponges and needles, pose serious patient safety risks and carry significant
financial and legal implications for healthcare institutions. Detecting
critical RFOs using artificial intelligence remains challenging due to their
rarity and the limited availability of chest X-ray datasets that specifically
feature critical RFOs cases. Existing datasets only contain non-critical RFOs,
like necklace or zipper, further limiting their utility for developing
clinically impactful detection algorithms. To address these limitations, we
introduce "Hopkins RFOs Bench", the first and largest dataset of its kind,
containing 144 chest X-ray images of critical RFO cases collected over 18 years
from the Johns Hopkins Health System. Using this dataset, we benchmark several
state-of-the-art object detection models, highlighting the need for enhanced
detection methodologies for critical RFO cases. Recognizing data scarcity
challenges, we further explore image synthetic methods to bridge this gap. We
evaluate two advanced synthetic image methods, DeepDRR-RFO, a physics-based
method, and RoentGen-RFO, a diffusion-based method, for creating realistic
radiographs featuring critical RFOs. Our comprehensive analysis identifies the
strengths and limitations of each synthetic method, providing insights into
effectively utilizing synthetic data to enhance model training. The Hopkins
RFOs Bench and our findings significantly advance the development of reliable,
generalizable AI-driven solutions for detecting critical RFOs in clinical chest
X-rays.

</details>


### [98] [SimCortex: Collision-free Simultaneous Cortical Surfaces Reconstruction](https://arxiv.org/abs/2507.06955)
*Kaveh Moradkhani,R Jarrett Rushmore,Sylvain Bouix*

Main category: eess.IV

TL;DR: SimCortex是一个深度学习框架，用于从T1加权MRI数据中重建大脑皮层表面，解决了现有方法的拓扑缺陷和表面重叠问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在重建复杂皮层几何时存在拓扑缺陷和表面重叠问题，SimCortex旨在解决这些问题。

Method: 通过深度学习生成九类组织标签图，创建无碰撞初始表面网格，并利用多尺度微分同胚变形和SVFs实现平滑、拓扑保持的变换。

Result: SimCortex显著减少了表面重叠和自交，同时保持了最先进的几何精度。

Conclusion: SimCortex在皮层表面重建中表现出色，优于现有方法。

Abstract: Accurate cortical surface reconstruction from magnetic resonance imaging
(MRI) data is crucial for reliable neuroanatomical analyses. Current methods
have to contend with complex cortical geometries, strict topological
requirements, and often produce surfaces with overlaps, self-intersections, and
topological defects. To overcome these shortcomings, we introduce SimCortex, a
deep learning framework that simultaneously reconstructs all brain surfaces
(left/right white-matter and pial) from T1-weighted(T1w) MRI volumes while
preserving topological properties. Our method first segments the T1w image into
a nine-class tissue label map. From these segmentations, we generate
subject-specific, collision-free initial surface meshes. These surfaces serve
as precise initializations for subsequent multiscale diffeomorphic
deformations. Employing stationary velocity fields (SVFs) integrated via
scaling-and-squaring, our approach ensures smooth, topology-preserving
transformations with significantly reduced surface collisions and
self-intersections. Evaluations on standard datasets demonstrate that SimCortex
dramatically reduces surface overlaps and self-intersections, surpassing
current methods while maintaining state-of-the-art geometric accuracy.

</details>


### [99] [Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning](https://arxiv.org/abs/2507.07011)
*Daniel Onah,Ravish Desai*

Main category: eess.IV

TL;DR: Deep Brain Net 是一种结合 EfficientNetB0 和 ResNet50 的深度学习系统，用于高效准确地检测和分类脑肿瘤。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习在脑肿瘤检测中面临高精度和计算效率的挑战。

Method: 结合 EfficientNetB0 和 ResNet50，利用迁移学习提升泛化能力和减少训练时间。

Result: 在公开 MRI 数据集上表现优异，准确率 88%，加权 F1 分数 88.75%，AUC ROC 分数 98.17%。

Conclusion: Deep Brain Net 在临床辅助诊断中具有潜力。

Abstract: In recent years, deep learning has shown great promise in the automated
detection and classification of brain tumors from MRI images. However,
achieving high accuracy and computational efficiency remains a challenge. In
this research, we propose Deep Brain Net, a novel deep learning system designed
to optimize performance in the detection of brain tumors. The model integrates
the strengths of two advanced neural network architectures which are
EfficientNetB0 and ResNet50, combined with transfer learning to improve
generalization and reduce training time. The EfficientNetB0 architecture
enhances model efficiency by utilizing mobile inverted bottleneck blocks, which
incorporate depth wise separable convolutions. This design significantly
reduces the number of parameters and computational cost while preserving the
ability of models to learn complex feature representations. The ResNet50
architecture, pre trained on large scale datasets like ImageNet, is fine tuned
for brain tumor classification. Its use of residual connections allows for
training deeper networks by mitigating the vanishing gradient problem and
avoiding performance degradation. The integration of these components ensures
that the proposed system is both computationally efficient and highly accurate.
Extensive experiments performed on publicly available MRI datasets demonstrate
that Deep Brain Net consistently outperforms existing state of the art methods
in terms of classification accuracy, precision, recall, and computational
efficiency. The result is an accuracy of 88 percent, a weighted F1 score of
88.75 percent, and a macro AUC ROC score of 98.17 percent which demonstrates
the robustness and clinical potential of Deep Brain Net in assisting
radiologists with brain tumor diagnosis.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [100] [3D-Generalist: Self-Improving Vision-Language-Action Models for Crafting 3D Worlds](https://arxiv.org/abs/2507.06484)
*Fan-Yun Sun,Shengguang Wu,Christian Jacobsen,Thomas Yim,Haoming Zou,Alex Zook,Shangru Li,Yu-Hsin Chou,Ethem Can,Xunlei Wu,Clemens Eppner,Valts Blukis,Jonathan Tremblay,Jiajun Wu,Stan Birchfield,Nick Haber*

Main category: cs.GR

TL;DR: 提出了一种可扩展的方法3D-Generalist，利用VLMs生成高质量3D环境作为基础模型的训练数据，通过自改进微调提升生成质量，并在下游任务中表现优于人工合成数据。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练模型在空间推理能力上仍面临挑战，因缺乏3D世界数据。人工创建3D环境耗时费力。

Method: 将3D环境构建视为序列决策问题，使用VLMs作为策略生成布局、材质、光照和资产，并通过自改进微调优化。

Result: 3D-Generalist能生成仿真就绪的3D环境，其生成的数据预训练视觉基础模型后，在下游任务中表现优于人工合成数据。

Conclusion: 3D-Generalist提供了一种高效生成3D环境的方法，为空间推理任务提供了高质量数据支持。

Abstract: Despite large-scale pretraining endowing models with language and vision
reasoning capabilities, improving their spatial reasoning capability remains
challenging due to the lack of data grounded in the 3D world. While it is
possible for humans to manually create immersive and interactive worlds through
3D graphics, as seen in applications such as VR, gaming, and robotics, this
process remains highly labor-intensive. In this paper, we propose a scalable
method for generating high-quality 3D environments that can serve as training
data for foundation models. We recast 3D environment building as a sequential
decision-making problem, employing Vision-Language-Models (VLMs) as policies
that output actions to jointly craft a 3D environment's layout, materials,
lighting, and assets. Our proposed framework, 3D-Generalist, trains VLMs to
generate more prompt-aligned 3D environments via self-improvement fine-tuning.
We demonstrate the effectiveness of 3D-Generalist and the proposed training
strategy in generating simulation-ready 3D environments. Furthermore, we
demonstrate its quality and scalability in synthetic data generation by
pretraining a vision foundation model on the generated data. After fine-tuning
the pre-trained model on downstream tasks, we show that it surpasses models
pre-trained on meticulously human-crafted synthetic data and approaches results
achieved with real data orders of magnitude larger.

</details>


### [101] [Assessing Learned Models for Phase-only Hologram Compression](https://arxiv.org/abs/2507.06646)
*Zicong Peng,Yicheng Zhan,Josef Spjut,Kaan Akşit*

Main category: cs.GR

TL;DR: 评估了四种基于INR和VAE结构的模型在压缩相位全息图时的性能，发现SIREN表现最佳，而预训练的VAE模型TAESD效果不佳。


<details>
  <summary>Details</summary>
Motivation: 研究不同模型在压缩相位全息图时的表现，以优化全息显示中的图像重建质量。

Method: 使用四种模型（MLP、SIREN、FilmSIREN和TAESD）进行实验，比较其在压缩和重建3D图像时的性能。

Result: SIREN以4.9k参数实现40%压缩，PSNR为34.54 dB；TAESD表现不佳。

Conclusion: INR模型（如SIREN）在相位全息图压缩中更有效，预训练VAE需任务适配。

Abstract: We evaluate the performance of four common learned models utilizing INR and
VAE structures for compressing phase-only holograms in holographic displays.
The evaluated models include a vanilla MLP, SIREN, and FilmSIREN, with TAESD as
the representative VAE model. Our experiments reveal that a pretrained image
VAE, TAESD, with 2.2M parameters struggles with phase-only hologram
compression, revealing the need for task-specific adaptations. Among the INRs,
SIREN with 4.9k parameters achieves %40 compression with high quality in the
reconstructed 3D images (PSNR = 34.54 dB). These results emphasize the
effectiveness of INRs and identify the limitations of pretrained image
compression VAEs for hologram compression task.

</details>


### [102] [Better frame rates or better visuals? An early report of Esports player practice in Dota 2](https://arxiv.org/abs/2507.06790)
*Arjun Madhusudan,Benjamin Watson*

Main category: cs.GR

TL;DR: 研究探讨了Dota 2玩家在游戏中通过降低视觉质量以提高性能和延迟的实践，发现玩家普遍关闭VSYNC以减少延迟，但牺牲了视觉质量。


<details>
  <summary>Details</summary>
Motivation: 了解玩家如何在实践中管理视觉质量与性能的权衡，填补相关研究的空白。

Method: 通过调查收集Dota 2玩家的游戏配置数据，并询问其主观意图。

Result: 玩家确实会限制视觉细节（如关闭VSYNC），且其意图与配置行为一致。

Conclusion: 研究为未来更严格的调查奠定了基础，有助于新玩家更快适应游戏，并为开发者设计低视觉配置提供依据。

Abstract: Esports athletes often reduce visual quality to improve latency and frame
rate, and increase their in-game performance. Little research has examined the
effects of this visuo-spatial tradeoff on performance, but we could find no
work studying how players manage this tradeoff in practice. This paper is an
initial examination of this question in the game Dota 2. First, we gather the
game configuration data of Dota 2 players in a small survey. We learn that
players do limit visual detail, particularly by turning off VSYNC, which
removes rendering/display synchronization delay but permits visual "tearing".
Second, we survey the intent of those same players with a few subjective
questions. Player intent matches configuration practice. While our sampling of
Dota 2 players may not be representative, our survey does reveal suggestive
trends that lay the groundwork for future, more rigorous and larger surveys.
Such surveys can help new players adapt to the game more quickly, encourage
researchers to investigate the relative importance of temporal and visual
detail, and justify design effort by developers in "low visual" game
configurations.

</details>


### [103] [Enhancing non-Rigid 3D Model Deformations Using Mesh-based Gaussian Splatting](https://arxiv.org/abs/2507.07000)
*Wijayathunga W. M. R. D. B*

Main category: cs.GR

TL;DR: 提出了一种新框架，通过将网格表示与3D高斯泼溅结合，增强非刚性3D模型变形能力。


<details>
  <summary>Details</summary>
Motivation: 传统高斯泼溅虽然能实现快速实时辐射场渲染，但其后编辑功能和对大规模非刚性变形的支持有限。

Method: 将高斯核直接嵌入显式网格表面，利用网格的拓扑和几何先验指导直观编辑操作，如移动、缩放和旋转3D组件。

Result: 支持复杂变形（如弯曲和拉伸），为3D内容创作提供更灵活的工作流。

Conclusion: 该框架为虚拟现实、角色动画和交互设计等应用提供了更灵活的3D内容创作工具。

Abstract: We propose a novel framework that enhances non-rigid 3D model deformations by
bridging mesh representations with 3D Gaussian splatting. While traditional
Gaussian splatting delivers fast, real-time radiance-field rendering, its
post-editing capabilities and support for large-scale, non-rigid deformations
remain limited. Our method addresses these challenges by embedding Gaussian
kernels directly onto explicit mesh surfaces. This allows the mesh's inherent
topological and geometric priors to guide intuitive editing operations -- such
as moving, scaling, and rotating individual 3D components -- and enables
complex deformations like bending and stretching. This work paves the way for
more flexible 3D content-creation workflows in applications spanning virtual
reality, character animation, and interactive design.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [104] [Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation](https://arxiv.org/abs/2507.06380)
*Habibur Rahaman,Atri Chatterjee,Swarup Bhunia*

Main category: cs.LG

TL;DR: WINGs框架通过动态生成全连接层权重和压缩卷积层权重，显著减少内存需求，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂神经网络存储大量突触权重的高内存需求问题。

Method: 使用PCA降维和轻量级SVR模型预测权重，结合敏感性分析压缩CNN权重。

Result: 实现FC层53倍压缩，AlexNet在MNIST上28倍压缩，CIFAR-10上18倍压缩，准确率损失1-2%。

Conclusion: WINGs显著降低内存需求，提升推理效率，适用于资源受限的边缘应用。

Abstract: Complex neural networks require substantial memory to store a large number of
synaptic weights. This work introduces WINGs (Automatic Weight Generator for
Secure and Storage-Efficient Deep Learning Models), a novel framework that
dynamically generates layer weights in a fully connected neural network (FC)
and compresses the weights in convolutional neural networks (CNNs) during
inference, significantly reducing memory requirements without sacrificing
accuracy. WINGs framework uses principal component analysis (PCA) for
dimensionality reduction and lightweight support vector regression (SVR) models
to predict layer weights in the FC networks, removing the need for storing
full-weight matrices and achieving substantial memory savings. It also
preferentially compresses the weights in low-sensitivity layers of CNNs using
PCA and SVR with sensitivity analysis. The sensitivity-aware design also offers
an added level of security, as any bit-flip attack with weights in compressed
layers has an amplified and readily detectable effect on accuracy. WINGs
achieves 53x compression for the FC layers and 28x for AlexNet with MNIST
dataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss.
This significant reduction in memory results in higher throughput and lower
energy for DNN inference, making it attractive for resource-constrained edge
applications.

</details>


### [105] [Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation](https://arxiv.org/abs/2507.06613)
*Anshuk Uppal,Yuhta Takida,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 论文提出了一种新的生成模型框架，通过结合不同β值的VAE和扩散模型，平衡解耦表示与生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型中解耦表示与重建质量之间的权衡问题。

Method: 使用多β值的VAE训练，结合非线性扩散模型平滑过渡不同β值的潜在表示。

Result: 实现了高质量的解耦表示和生成，支持无输入图像的样本生成。

Conclusion: 该框架在解耦和生成质量上表现优异，支持潜在空间的平滑过渡和输出一致性操作。

Abstract: Disentangled and interpretable latent representations in generative models
typically come at the cost of generation quality. The $\beta$-VAE framework
introduces a hyperparameter $\beta$ to balance disentanglement and
reconstruction quality, where setting $\beta > 1$ introduces an information
bottleneck that favors disentanglement over sharp, accurate reconstructions. To
address this trade-off, we propose a novel generative modeling framework that
leverages a range of $\beta$ values to learn multiple corresponding latent
representations. First, we obtain a slew of representations by training a
single variational autoencoder (VAE), with a new loss function that controls
the information retained in each latent representation such that the higher
$\beta$ value prioritize disentanglement over reconstruction fidelity. We then,
introduce a non-linear diffusion model that smoothly transitions latent
representations corresponding to different $\beta$ values. This model denoises
towards less disentangled and more informative representations, ultimately
leading to (almost) lossless representations, enabling sharp reconstructions.
Furthermore, our model supports sample generation without input images,
functioning as a standalone generative model. We evaluate our framework in
terms of both disentanglement and generation quality. Additionally, we observe
smooth transitions in the latent spaces with respect to changes in $\beta$,
facilitating consistent manipulation of generated outputs.

</details>


### [106] [A Principled Framework for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06979)
*Panagiotis Koromilas,Efthymios Georgiou,Giorgos Bouritsas,Theodoros Giannakopoulos,Mihalis A. Nicolaou,Yannis Panagakis*

Main category: cs.LG

TL;DR: 论文提出两种新的损失函数MV-InfoNCE和MV-DHEL，解决了多视图对比学习中存在的四个关键问题，并在实验中验证了其优越性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前对比学习方法在处理多视图时存在优化冲突、交互建模不足、对齐-均匀性耦合等问题，限制了多视图学习的潜力。

Method: 提出MV-InfoNCE（扩展InfoNCE以同时建模所有视图交互）和MV-DHEL（解耦对齐与均匀性并随视图数量扩展交互复杂度）。

Result: 在ImageNet1K等数据集上，新方法显著优于现有多视图方法，并能扩展到多模态数据。MV-DHEL在五视图以上时有效缓解维度坍缩。

Conclusion: 新方法为多视图对比学习提供了理论支持和实际改进，实现了监督学习中观察到的多视图优势。

Abstract: Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning
(SSL), typically relies on pairs of data views generated through augmentation.
While multiple augmentations per instance (more than two) improve
generalization in supervised learning, current CL methods handle additional
views suboptimally by simply aggregating different pairwise objectives. This
approach suffers from four critical limitations: (L1) it utilizes multiple
optimization terms per data point resulting to conflicting objectives, (L2) it
fails to model all interactions across views and data points, (L3) it inherits
fundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL
losses, and (L4) it prevents fully realizing the benefits of increased view
multiplicity observed in supervised settings. We address these limitations
through two novel loss functions: MV-InfoNCE, which extends InfoNCE to
incorporate all possible view interactions simultaneously in one term per data
point, and MV-DHEL, which decouples alignment from uniformity across views
while scaling interaction complexity with view multiplicity. Both approaches
are theoretically grounded - we prove they asymptotically optimize for
alignment of all views and uniformity, providing principled extensions to
multi-view contrastive learning. Our empirical results on ImageNet1K and three
other datasets demonstrate that our methods consistently outperform existing
multi-view approaches and effectively scale with increasing view multiplicity.
We also apply our objectives to multimodal data and show that, in contrast to
other contrastive objectives, they can scale beyond just two modalities. Most
significantly, ablation studies reveal that MV-DHEL with five or more views
effectively mitigates dimensionality collapse by fully utilizing the embedding
space, thereby delivering multi-view benefits observed in supervised learning.

</details>


### [107] [Addressing Imbalanced Domain-Incremental Learning through Dual-Balance Collaborative Experts](https://arxiv.org/abs/2507.07100)
*Lan Li,Da-Wei Zhou,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.LG

TL;DR: 论文提出DCE框架，解决领域增量学习中类不平衡和跨域分布偏移问题，通过频率感知专家组和动态选择器实现优异性能。


<details>
  <summary>Details</summary>
Motivation: 领域增量学习（DIL）在非平稳环境中面临类内不平衡和跨域分布偏移的挑战，影响模型性能。

Method: 提出DCE框架，包括频率感知专家组和动态专家选择器，分别解决类内不平衡和跨域知识迁移问题。

Result: 在四个基准数据集上，DCE表现出最先进的性能。

Conclusion: DCE有效解决了DIL中的关键挑战，显著提升了模型性能。

Abstract: Domain-Incremental Learning (DIL) focuses on continual learning in
non-stationary environments, requiring models to adjust to evolving domains
while preserving historical knowledge. DIL faces two critical challenges in the
context of imbalanced data: intra-domain class imbalance and cross-domain class
distribution shifts. These challenges significantly hinder model performance,
as intra-domain imbalance leads to underfitting of few-shot classes, while
cross-domain shifts require maintaining well-learned many-shot classes and
transferring knowledge to improve few-shot class performance in old domains. To
overcome these challenges, we introduce the Dual-Balance Collaborative Experts
(DCE) framework. DCE employs a frequency-aware expert group, where each expert
is guided by specialized loss functions to learn features for specific
frequency groups, effectively addressing intra-domain class imbalance.
Subsequently, a dynamic expert selector is learned by synthesizing
pseudo-features through balanced Gaussian sampling from historical class
statistics. This mechanism navigates the trade-off between preserving many-shot
knowledge of previous domains and leveraging new data to improve few-shot class
performance in earlier tasks. Extensive experimental results on four benchmark
datasets demonstrate DCE's state-of-the-art performance.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [108] [Downscaling with AI reveals the large role of internal variability in fine-scale projections of climate extremes](https://arxiv.org/abs/2507.06527)
*Neelesh Rampal,Peter B. Gibson,Steven C. Sherwood,Laura E. Queen,Hamish Lewis,Gab Abramowitz*

Main category: physics.ao-ph

TL;DR: 生成式AI方法用于区域降尺度，显著扩展了降尺度范围，能捕捉传统方法无法处理的罕见极端事件。测试显示，未来降水和温度极端事件的预测性成分更平滑，但局部变化较小。


<details>
  <summary>Details</summary>
Motivation: 传统动力降尺度的计算成本限制了集合规模，难以捕捉罕见极端事件。

Method: 开发了一种生成式AI方法，应用于新西兰区域降尺度，分析未来（2080-2099）降水和温度极端事件。

Result: 未来罕见降水极端事件变化更剧烈，内部变异性更大；精细尺度下内部变异性主导模型不确定性。

Conclusion: 精细尺度未来降水变化预测性低于普遍假设，需更大集合规模以可靠评估。

Abstract: The computational cost of dynamical downscaling limits ensemble sizes in
regional downscaling efforts. We present a newly developed generative-AI
approach to greatly expand the scope of such downscaling, enabling fine-scale
future changes to be characterised including rare extremes that cannot be
addressed by traditional approaches. We test this approach for New Zealand,
where strong regional effects are anticipated. At fine scales, the forced
(predictable) component of precipitation and temperature extremes for future
periods (2080--2099) is spatially smoother than changes in individual
simulations, and locally smaller. Future changes in rarer (10-year and 20-year)
precipitation extremes are more severe and have larger internal variability
spread than annual extremes. Internal variability spread is larger at fine
scales that at the coarser scales simulated in climate models. Unpredictability
from internal variability dominates model uncertainty and, for precipitation,
its variance increases with warming, exceeding the variance across emission
scenarios by fourfold for annual and tenfold for decadal extremes. These
results indicate that fine-scale changes in future precipitation are less
predictable than widely assumed and require much larger ensembles to assess
reliably than changes at coarser scales.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [109] [Learning to Evaluate Autonomous Behaviour in Human-Robot Interaction](https://arxiv.org/abs/2507.06404)
*Matteo Tiezzi,Tommaso Apicella,Carlos Cardenas-Perez,Giovanni Fregonese,Stefano Dafarra,Pietro Morerio,Daniele Pucci,Alessio Del Bue*

Main category: cs.RO

TL;DR: 提出了一种基于轨迹性能的通用评估框架NeME，用于比较模仿学习方法的性能，无需人类参与。


<details>
  <summary>Details</summary>
Motivation: 评估人形机器人性能具有挑战性，传统成功率指标难以复现且无法捕捉复杂运动轨迹。

Method: 设计NeME（神经元评估器），通过深度学习模型分类机器人关节轨迹动作，作为元评估器比较控制策略性能。

Result: 在ergoCub人形机器人上验证，NeME比基线方法更符合实际成功率，提供可复现的系统性评估。

Conclusion: NeME为复杂人机交互任务中的多模态模仿学习方法提供了可复现且系统的性能比较手段。

Abstract: Evaluating and comparing the performance of autonomous Humanoid Robots is
challenging, as success rate metrics are difficult to reproduce and fail to
capture the complexity of robot movement trajectories, critical in Human-Robot
Interaction and Collaboration (HRIC). To address these challenges, we propose a
general evaluation framework that measures the quality of Imitation Learning
(IL) methods by focusing on trajectory performance. We devise the Neural Meta
Evaluator (NeME), a deep learning model trained to classify actions from robot
joint trajectories. NeME serves as a meta-evaluator to compare the performance
of robot control policies, enabling policy evaluation without requiring human
involvement in the loop. We validate our framework on ergoCub, a humanoid
robot, using teleoperation data and comparing IL methods tailored to the
available platform. The experimental results indicate that our method is more
aligned with the success rate obtained on the robot than baselines, offering a
reproducible, systematic, and insightful means for comparing the performance of
multimodal imitation learning approaches in complex HRI tasks.

</details>


### [110] [LOVON: Legged Open-Vocabulary Object Navigator](https://arxiv.org/abs/2507.06747)
*Daojie Peng,Jiahang Cao,Qiang Zhang,Jun Ma*

Main category: cs.RO

TL;DR: LOVON框架结合大型语言模型和开放词汇视觉检测模型，解决开放环境中长程物体导航问题，通过专用解决方案（如拉普拉斯方差滤波）应对现实挑战，并在多种机器人上验证其兼容性。


<details>
  <summary>Details</summary>
Motivation: 开放环境中的长程物体导航对机器人系统极具挑战，传统方法难以有效整合开放世界物体检测与高级任务规划。

Method: LOVON框架整合大型语言模型（LLMs）用于分层任务规划，结合开放词汇视觉检测模型，并设计专用解决方案（如拉普拉斯方差滤波）应对视觉抖动等问题。

Result: 实验表明LOVON能成功完成涉及实时检测、搜索和导航的长序列任务，并在多种机器人上验证其兼容性。

Conclusion: LOVON为动态、非结构化环境中的长程物体导航提供了一种有效解决方案，具有广泛的适用性和鲁棒性。

Abstract: Object navigation in open-world environments remains a formidable and
pervasive challenge for robotic systems, particularly when it comes to
executing long-horizon tasks that require both open-world object detection and
high-level task planning. Traditional methods often struggle to integrate these
components effectively, and this limits their capability to deal with complex,
long-range navigation missions. In this paper, we propose LOVON, a novel
framework that integrates large language models (LLMs) for hierarchical task
planning with open-vocabulary visual detection models, tailored for effective
long-range object navigation in dynamic, unstructured environments. To tackle
real-world challenges including visual jittering, blind zones, and temporary
target loss, we design dedicated solutions such as Laplacian Variance Filtering
for visual stabilization. We also develop a functional execution logic for the
robot that guarantees LOVON's capabilities in autonomous navigation, task
adaptation, and robust task completion. Extensive evaluations demonstrate the
successful completion of long-sequence tasks involving real-time detection,
search, and navigation toward open-vocabulary dynamic targets. Furthermore,
real-world experiments across different legged robots (Unitree Go2, B2, and
H1-2) showcase the compatibility and appealing plug-and-play feature of LOVON.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [111] [Compressibility of Confined Fluids from Volume Fluctuations](https://arxiv.org/abs/2507.07020)
*Jason Ogbebor,Santiago A. Flores Roman,Geordy Jomon,Gennady Y. Gor*

Main category: cond-mat.soft

TL;DR: 提出了一种新的分子模拟方法，用于计算受限流体的压缩性，应用于甲烷在碳纳米孔中的情况，比蒙特卡洛方法快一个数量级。


<details>
  <summary>Details</summary>
Motivation: 研究纳米孔中流体性质与体相流体的差异，特别是在煤层气和页岩气勘探与开采中的重要性。

Method: 基于等温等压系综中的体积波动，利用积分势能开发新方法。

Result: 预测3纳米狭缝孔中流体体积模量增加4倍，随孔径增大逐渐减小，100纳米时与体相差异小于5%。

Conclusion: 新方法高效且适用于大孔径（达100纳米），揭示了纳米孔中流体压缩性的变化规律。

Abstract: When fluids are confined in nanopores, many of their properties deviate from
bulk. These include bulk modulus, or compressibility, which determines the
mechanical properties of fluid-saturated porous solids. Such properties are of
importance for exploration and recovery of coal-bed methane and shale gas. We
developed a new molecular simulation method for calculating compressibility of
confined fluids, and applied it to methane in carbon nanopores. The method is
based on volume fluctuations in the isothermal-isobaric ensemble, made possible
through integrated potentials. Our method is one order of magnitude faster than
the Monte Carlo approach, and allows calculations for pore sizes up to 100 nm.
Our simulations predicted an increase in the fluid bulk modulus by a factor of
4 in 3 nm slit pores, and showed a gradual decrease with the increase of the
pore size, so that at 100 nm, the deviation from the bulk is less than 5%.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [112] [The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced Planning, Navigation, and Dynamic Adaptation](https://arxiv.org/abs/2507.06993)
*Jieren Deng,Aleksandar Cvetkovic,Pak Kiu Chung,Dragomir Yankov,Chiqun Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种动态旅行规划系统，通过三个协作代理解决传统系统的不足，显著提升了查询解析、导航准确性和抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 传统旅行规划系统静态且碎片化，无法应对现实世界的复杂性和突发变化，导致用户体验不佳。

Method: 提出三个协作代理：旅行规划代理（基于网格空间分析和地图）、目的地助手代理（精细化导航）和本地发现代理（利用图像嵌入和RAG应对干扰）。

Result: 系统在查询解析、导航准确性和抗干扰能力方面表现出显著改进。

Conclusion: 该系统在从城市探索到应急响应等多个领域具有应用潜力。

Abstract: Traditional travel-planning systems are often static and fragmented, leaving
them ill-equipped to handle real-world complexities such as evolving
environmental conditions and unexpected itinerary disruptions. In this paper,
we identify three gaps between existing service providers causing frustrating
user experience: intelligent trip planning, precision "last-100-meter"
navigation, and dynamic itinerary adaptation. We propose three cooperative
agents: a Travel Planning Agent that employs grid-based spatial grounding and
map analysis to help resolve complex multi-modal user queries; a Destination
Assistant Agent that provides fine-grained guidance for the final navigation
leg of each journey; and a Local Discovery Agent that leverages image
embeddings and Retrieval-Augmented Generation (RAG) to detect and respond to
trip plan disruptions. With evaluations and experiments, our system
demonstrates substantial improvements in query interpretation, navigation
accuracy, and disruption resilience, underscoring its promise for applications
from urban exploration to emergency response.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [113] [Conformal Prediction for Long-Tailed Classification](https://arxiv.org/abs/2507.06867)
*Tiffany Ding,Jean-Baptiste Fermanian,Joseph Salmon*

Main category: stat.ML

TL;DR: 论文提出两种方法，用于解决长尾分布分类问题中预测集覆盖率和大小之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 长尾分布的分类问题中，现有方法无法同时保证预测集的小规模和良好的类别条件覆盖率。

Method: 提出两种方法：1) 针对宏覆盖率的prevalence-adjusted softmax评分函数；2) 标签加权共形预测方法。

Result: 在Pl@ntNet和iNaturalist数据集上验证了方法的有效性。

Conclusion: 新方法在保证边际覆盖率的同时，灵活权衡预测集大小和类别条件覆盖率。

Abstract: Many real-world classification problems, such as plant identification, have
extremely long-tailed class distributions. In order for prediction sets to be
useful in such settings, they should (i) provide good class-conditional
coverage, ensuring that rare classes are not systematically omitted from the
prediction sets, and (ii) be a reasonable size, allowing users to easily verify
candidate labels. Unfortunately, existing conformal prediction methods, when
applied to the long-tailed setting, force practitioners to make a binary choice
between small sets with poor class-conditional coverage or sets with very good
class-conditional coverage but that are extremely large. We propose methods
with guaranteed marginal coverage that smoothly trade off between set size and
class-conditional coverage. First, we propose a conformal score function,
prevalence-adjusted softmax, that targets a relaxed notion of class-conditional
coverage called macro-coverage. Second, we propose a label-weighted conformal
prediction method that allows us to interpolate between marginal and
class-conditional conformal prediction. We demonstrate our methods on Pl@ntNet
and iNaturalist, two long-tailed image datasets with 1,081 and 8,142 classes,
respectively.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [114] [PAST: A multimodal single-cell foundation model for histopathology and spatial transcriptomics in cancer](https://arxiv.org/abs/2507.06418)
*Changchun Yang,Haoyang Li,Yushuai Wu,Yilan Zhang,Yifeng Jiao,Yu Zhang,Rihan Huang,Yuan Cheng,Yuan Qi,Xin Guo,Xin Gao*

Main category: q-bio.QM

TL;DR: PAST是一个基于20万对病理图像和单细胞转录组的跨模态基础模型，整合了细胞形态和基因表达，提升了癌症精准医学的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有病理基础模型缺乏与单细胞分子数据的整合，限制了其在精准肿瘤学中的应用。

Method: PAST通过联合编码细胞形态和基因表达，学习跨模态的统一表征，支持单细胞基因表达预测、虚拟分子染色和多模态生存分析。

Result: PAST在多种癌症和任务中表现优于现有方法，展示了强大的泛化能力和可扩展性。

Conclusion: PAST为病理基础模型提供了新范式，支持高分辨率空间组学、机制发现和精准癌症研究。

Abstract: While pathology foundation models have transformed cancer image analysis,
they often lack integration with molecular data at single-cell resolution,
limiting their utility for precision oncology. Here, we present PAST, a
pan-cancer single-cell foundation model trained on 20 million paired
histopathology images and single-cell transcriptomes spanning multiple tumor
types and tissue contexts. By jointly encoding cellular morphology and gene
expression, PAST learns unified cross-modal representations that capture both
spatial and molecular heterogeneity at the cellular level. This approach
enables accurate prediction of single-cell gene expression, virtual molecular
staining, and multimodal survival analysis directly from routine pathology
slides. Across diverse cancers and downstream tasks, PAST consistently exceeds
the performance of existing approaches, demonstrating robust generalizability
and scalability. Our work establishes a new paradigm for pathology foundation
models, providing a versatile tool for high-resolution spatial omics,
mechanistic discovery, and precision cancer research.

</details>
