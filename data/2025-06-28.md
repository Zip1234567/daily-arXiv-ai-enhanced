<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 9]
- [eess.IV](#eess.IV) [Total: 9]
- [cs.GR](#cs.GR) [Total: 6]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [OTSurv: A Novel Multiple Instance Learning Framework for Survival Prediction with Heterogeneity-aware Optimal Transport](https://arxiv.org/abs/2506.20741)
*Qin Ren,Yifan Wang,Ruogu Fang,Haibin Ling,Chenyu You*

Main category: cs.CV

TL;DR: OTSurv是一种基于最优传输（OT）的多实例学习（MIL）框架，用于从全切片图像（WSIs）中预测生存率，通过全局和局部约束解决病理异质性问题，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有MIL方法未能明确捕捉WSIs中的病理异质性，包括全局的长尾形态分布和局部的瓦片级预测不确定性。

Method: OTSurv将生存预测建模为具有全局长尾约束和局部不确定性约束的OT问题，并通过不平衡OT公式高效求解。

Result: 在六个基准测试中，OTSurv实现了平均C-index绝对提升3.6%，并在对数秩检验中达到统计显著性。

Conclusion: OTSurv是一种高效、可解释的生存预测工具，适用于数字病理学。

Abstract: Survival prediction using whole slide images (WSIs) can be formulated as a
multiple instance learning (MIL) problem. However, existing MIL methods often
fail to explicitly capture pathological heterogeneity within WSIs, both
globally -- through long-tailed morphological distributions, and locally
through -- tile-level prediction uncertainty. Optimal transport (OT) provides a
principled way of modeling such heterogeneity by incorporating marginal
distribution constraints. Building on this insight, we propose OTSurv, a novel
MIL framework from an optimal transport perspective. Specifically, OTSurv
formulates survival predictions as a heterogeneity-aware OT problem with two
constraints: (1) global long-tail constraint that models prior morphological
distributions to avert both mode collapse and excessive uniformity by
regulating transport mass allocation, and (2) local uncertainty-aware
constraint that prioritizes high-confidence patches while suppressing noise by
progressively raising the total transport mass. We then recast the initial OT
problem, augmented by these constraints, into an unbalanced OT formulation that
can be solved with an efficient, hardware-friendly matrix scaling algorithm.
Empirically, OTSurv sets new state-of-the-art results across six popular
benchmarks, achieving an absolute 3.6% improvement in average C-index. In
addition, OTSurv achieves statistical significance in log-rank tests and offers
high interpretability, making it a powerful tool for survival prediction in
digital pathology. Our codes are available at
https://github.com/Y-Research-SBU/OTSurv.

</details>


### [2] [StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation](https://arxiv.org/abs/2506.20756)
*Haodong Li,Chen Wang,Jiahui Lei,Kostas Daniilidis,Lingjie Liu*

Main category: cs.CV

TL;DR: StereoDiff提出了一种两阶段视频深度估计方法，结合立体匹配和视频深度扩散，分别处理静态和动态区域，实现了更高的时空一致性。


<details>
  <summary>Details</summary>
Motivation: 视频深度估计不仅是图像深度估计的简单扩展，静态和动态区域的时空一致性需求不同。静态区域可通过立体匹配获得全局3D线索，而动态区域需依赖大规模视频数据学习平滑过渡。

Method: StereoDiff采用两阶段方法：静态区域通过立体匹配实现一致性，动态区域通过视频深度扩散模型学习平滑过渡。通过频域分析证明两者的互补性。

Result: 在零样本、真实世界动态视频深度基准测试中，StereoDiff实现了最先进的性能，展示了其在一致性和准确性上的优势。

Conclusion: StereoDiff通过结合立体匹配和视频深度扩散，为视频深度估计提供了一种高效且一致的解决方案。

Abstract: Recent video depth estimation methods achieve great performance by following
the paradigm of image depth estimation, i.e., typically fine-tuning pre-trained
video diffusion models with massive data. However, we argue that video depth
estimation is not a naive extension of image depth estimation. The temporal
consistency requirements for dynamic and static regions in videos are
fundamentally different. Consistent video depth in static regions, typically
backgrounds, can be more effectively achieved via stereo matching across all
frames, which provides much stronger global 3D cues. While the consistency for
dynamic regions still should be learned from large-scale video depth data to
ensure smooth transitions, due to the violation of triangulation constraints.
Based on these insights, we introduce StereoDiff, a two-stage video depth
estimator that synergizes stereo matching for mainly the static areas with
video depth diffusion for maintaining consistent depth transitions in dynamic
areas. We mathematically demonstrate how stereo matching and video depth
diffusion offer complementary strengths through frequency domain analysis,
highlighting the effectiveness of their synergy in capturing the advantages of
both. Experimental results on zero-shot, real-world, dynamic video depth
benchmarks, both indoor and outdoor, demonstrate StereoDiff's SoTA performance,
showcasing its superior consistency and accuracy in video depth estimation.

</details>


### [3] [ConViTac: Aligning Visual-Tactile Fusion with Contrastive Representations](https://arxiv.org/abs/2506.20757)
*Zhiyuan Wu,Yongqiang Zhao,Shan Luo*

Main category: cs.CV

TL;DR: 提出了一种名为ConViTac的视觉-触觉表征学习网络，通过对比表征增强特征融合的对齐性，显著提升了材料分类和抓取预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉和触觉是机器人感知和操作任务中的两种基本感官模态，但现有方法在模态融合时特征整合效果不佳。

Method: 提出ConViTac网络，采用对比嵌入条件（CEC）机制，通过自监督对比学习预训练的编码器将视觉和触觉输入投影到统一的潜在嵌入中，并通过跨模态注意力进行特征融合。

Result: 实验表明，ConViTac在材料分类和抓取预测任务中比现有方法提升了12.0%的准确率。

Conclusion: ConViTac通过对比表征学习有效提升了视觉-触觉特征融合的性能，为机器人感知任务提供了更优的解决方案。

Abstract: Vision and touch are two fundamental sensory modalities for robots, offering
complementary information that enhances perception and manipulation tasks.
Previous research has attempted to jointly learn visual-tactile representations
to extract more meaningful information. However, these approaches often rely on
direct combination, such as feature addition and concatenation, for modality
fusion, which tend to result in poor feature integration. In this paper, we
propose ConViTac, a visual-tactile representation learning network designed to
enhance the alignment of features during fusion using contrastive
representations. Our key contribution is a Contrastive Embedding Conditioning
(CEC) mechanism that leverages a contrastive encoder pretrained through
self-supervised contrastive learning to project visual and tactile inputs into
unified latent embeddings. These embeddings are used to couple visual-tactile
feature fusion through cross-modal attention, aiming at aligning the unified
representations and enhancing performance on downstream tasks. We conduct
extensive experiments to demonstrate the superiority of ConViTac in real world
over current state-of-the-art methods and the effectiveness of our proposed CEC
mechanism, which improves accuracy by up to 12.0% in material classification
and grasping prediction tasks.

</details>


### [4] [AI-Driven MRI-based Brain Tumour Segmentation Benchmarking](https://arxiv.org/abs/2506.20786)
*Connor Ludwig,Khashayar Namdar,Farzad Khalvati*

Main category: cs.CV

TL;DR: 研究评估和比较了多种可提示模型（如SAM、SAM 2、MedSAM等）在医学图像分割中的表现，发现高精度提示下SAM和SAM 2表现优于nnU-Net，但nnU-Net仍是主流。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对多种可提示模型在不同提示质量下的评估和比较，尤其是在医学数据集上。

Method: 使用SAM、SAM 2、MedSAM、SAM-Med-3D和nnU-Net在BraTS 2023数据集上进行零样本推理，并对部分模型进行微调。

Result: SAM和SAM 2在高精度边界框提示下Dice分数达0.894和0.893，优于nnU-Net，但nnU-Net仍更实用。微调后点提示性能显著提升，但仍不及边界框或nnU-Net。

Conclusion: 高精度提示下可提示模型表现优异，但nnU-Net仍是医学图像分割的主流选择。微调显示未来潜力，但需进一步研究。

Abstract: Medical image segmentation has greatly aided medical diagnosis, with U-Net
based architectures and nnU-Net providing state-of-the-art performance. There
have been numerous general promptable models and medical variations introduced
in recent years, but there is currently a lack of evaluation and comparison of
these models across a variety of prompt qualities on a common medical dataset.
This research uses Segment Anything Model (SAM), Segment Anything Model 2 (SAM
2), MedSAM, SAM-Med-3D, and nnU-Net to obtain zero-shot inference on the BraTS
2023 adult glioma and pediatrics dataset across multiple prompt qualities for
both points and bounding boxes. Several of these models exhibit promising Dice
scores, particularly SAM and SAM 2 achieving scores of up to 0.894 and 0.893,
respectively when given extremely accurate bounding box prompts which exceeds
nnU-Net's segmentation performance. However, nnU-Net remains the dominant
medical image segmentation network due to the impracticality of providing
highly accurate prompts to the models. The model and prompt evaluation, as well
as the comparison, are extended through fine-tuning SAM, SAM 2, MedSAM, and
SAM-Med-3D on the pediatrics dataset. The improvements in point prompt
performance after fine-tuning are substantial and show promise for future
investigation, but are unable to achieve better segmentation than bounding
boxes or nnU-Net.

</details>


### [5] [How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?](https://arxiv.org/abs/2506.20795)
*Stephanie Käs,Anton Burenko,Louis Markert,Onur Alp Culha,Dennis Mack,Timm Linder,Bastian Leibe*

Main category: cs.CV

TL;DR: 研究探讨了基于视觉基础模型（VFMs）和视觉语言模型（VLMs）的动态全身手势识别，比较了V-JEPA、Gemini Flash 2.0和HD-GCN的性能，发现HD-GCN表现最佳，但V-JEPA接近，为简化系统复杂度提供了可能。


<details>
  <summary>Details</summary>
Motivation: 在嘈杂环境中，手势是非人机交互的重要方式，传统深度学习方法依赖任务特定架构，而VFMs和VLMs的泛化能力可能降低系统复杂度。

Method: 研究比较了V-JEPA、Gemini Flash 2.0和HD-GCN在动态全身手势识别中的表现，并引入了NUGGET数据集进行评估。

Result: HD-GCN表现最佳，V-JEPA接近，Gemini在零样本设置中表现不佳。

Conclusion: V-JEPA作为共享多任务模型潜力大，但需进一步研究手势的输入表示方式。

Abstract: Gestures enable non-verbal human-robot communication, especially in noisy
environments like agile production. Traditional deep learning-based gesture
recognition relies on task-specific architectures using images, videos, or
skeletal pose estimates as input. Meanwhile, Vision Foundation Models (VFMs)
and Vision Language Models (VLMs) with their strong generalization abilities
offer potential to reduce system complexity by replacing dedicated
task-specific modules. This study investigates adapting such models for
dynamic, full-body gesture recognition, comparing V-JEPA (a state-of-the-art
VFM), Gemini Flash 2.0 (a multimodal VLM), and HD-GCN (a top-performing
skeleton-based approach). We introduce NUGGET, a dataset tailored for
human-robot communication in intralogistics environments, to evaluate the
different gesture recognition approaches. In our experiments, HD-GCN achieves
best performance, but V-JEPA comes close with a simple, task-specific
classification head - thus paving a possible way towards reducing system
complexity, by using it as a shared multi-task model. In contrast, Gemini
struggles to differentiate gestures based solely on textual descriptions in the
zero-shot setting, highlighting the need of further research on suitable input
representations for gestures.

</details>


### [6] [Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models](https://arxiv.org/abs/2506.20832)
*Cansu Korkmaz,Ahmet Murat Tekalp,Zafer Dogan*

Main category: cs.CV

TL;DR: 论文提出了一种利用视觉语言模型（VLM）从扩散模型生成的超分辨率（SR）图像中选择最可信样本的自动化框架，并通过混合指标（TWS）量化可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决传统SR模型在信息关键应用中因平衡保真度和感知质量而引入的模糊性问题，以及扩散模型生成多样SR图像后选择可信样本的挑战。

Method: 利用BLIP-2、GPT-4o等VLM进行语义正确性、视觉质量和伪影评估，并通过TWS（结合CLIP嵌入、SSIM和小波分解）量化可靠性。

Result: TWS与人类偏好高度相关，VLM引导的选择能持续获得高TWS值，优于传统指标（如PSNR、LPIPS）。

Conclusion: 该框架为生成SR的不确定性提供了可扩展、通用的解决方案，设定了可信SR的新基准。

Abstract: Super-resolution (SR) is an ill-posed inverse problem with many feasible
solutions consistent with a given low-resolution image. On one hand, regressive
SR models aim to balance fidelity and perceptual quality to yield a single
solution, but this trade-off often introduces artifacts that create ambiguity
in information-critical applications such as recognizing digits or letters. On
the other hand, diffusion models generate a diverse set of SR images, but
selecting the most trustworthy solution from this set remains a challenge. This
paper introduces a robust, automated framework for identifying the most
trustworthy SR sample from a diffusion-generated set by leveraging the semantic
reasoning capabilities of vision-language models (VLMs). Specifically, VLMs
such as BLIP-2, GPT-4o, and their variants are prompted with structured queries
to assess semantic correctness, visual quality, and artifact presence. The
top-ranked SR candidates are then ensembled to yield a single trustworthy
output in a cost-effective manner. To rigorously assess the validity of
VLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid
metric that quantifies SR reliability based on three complementary components:
semantic similarity via CLIP embeddings, structural integrity using SSIM on
edge maps, and artifact sensitivity through multi-level wavelet decomposition.
We empirically show that TWS correlates strongly with human preference in both
ambiguous and natural images, and that VLM-guided selections consistently yield
high TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail
to reflect information fidelity, our approach offers a principled, scalable,
and generalizable solution for navigating the uncertainty of the diffusion SR
space. By aligning outputs with human expectations and semantic correctness,
this work sets a new benchmark for trustworthiness in generative SR.

</details>


### [7] [FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization](https://arxiv.org/abs/2506.20841)
*Ha Min Son,Shahbaz Rezaei,Xin Liu*

Main category: cs.CV

TL;DR: FixCLR是一种新的半监督域泛化方法，通过对比学习显式正则化域不变表示，解决了标签稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 由于标签稀缺，现有半监督域泛化方法未能显式正则化域不变表示，导致性能不足。

Method: FixCLR结合伪标签的类别信息和排斥项，将对比学习应用于域不变表示的正则化。

Result: 实验表明，FixCLR有效提升了性能，尤其是与其他半监督方法结合时。

Conclusion: FixCLR是一种高效的半监督域泛化方法，适用于多领域数据集。

Abstract: Semi-supervised domain generalization (SSDG) aims to solve the problem of
generalizing to out-of-distribution data when only a few labels are available.
Due to label scarcity, applying domain generalization methods often
underperform. Consequently, existing SSDG methods combine semi-supervised
learning methods with various regularization terms. However, these methods do
not explicitly regularize to learn domains invariant representations across all
domains, which is a key goal for domain generalization. To address this, we
introduce FixCLR. Inspired by success in self-supervised learning, we change
two crucial components to adapt contrastive learning for explicit domain
invariance regularization: utilization of class information from pseudo-labels
and using only a repelling term. FixCLR can also be added on top of most
existing SSDG and semi-supervised methods for complementary performance
improvements. Our research includes extensive experiments that have not been
previously explored in SSDG studies. These experiments include benchmarking
different improvements to semi-supervised methods, evaluating the performance
of pretrained versus non-pretrained models, and testing on datasets with many
domains. Overall, FixCLR proves to be an effective SSDG method, especially when
combined with other semi-supervised methods.

</details>


### [8] [Vector Contrastive Learning For Pixel-Wise Pretraining In Medical Vision](https://arxiv.org/abs/2506.20850)
*Yuting He,Shuo Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为COVER的向量对比学习框架，解决了传统对比学习在像素级表示中的过分散问题，提升了医学视觉基础模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统对比学习在像素级表示中存在过分散问题，破坏了像素级特征相关性，影响了医学视觉任务的性能。

Method: 提出向量对比学习（vector CL），将对比学习重新定义为向量回归问题，通过COVER框架实现向量自学习、优化流程一致性和粒度适应。

Result: 在8个任务、2个维度和4种模态的实验中，COVER显著提升了像素级自监督预训练的性能。

Conclusion: COVER框架有效解决了像素级表示中的过分散问题，为医学视觉基础模型提供了更好的泛化能力。

Abstract: Contrastive learning (CL) has become a cornerstone of self-supervised
pretraining (SSP) in foundation models, however, extending CL to pixel-wise
representation, crucial for medical vision, remains an open problem. Standard
CL formulates SSP as a binary optimization problem (binary CL) where the
excessive pursuit of feature dispersion leads to an over-dispersion problem,
breaking pixel-wise feature correlation thus disrupting the intra-class
distribution. Our vector CL reformulates CL as a vector regression problem,
enabling dispersion quantification in pixel-wise pretraining via modeling
feature distances in regressing displacement vectors. To implement this novel
paradigm, we propose the COntrast in VEctor Regression (COVER) framework. COVER
establishes an extendable vector-based self-learning, enforces a consistent
optimization flow from vector regression to distance modeling, and leverages a
vector pyramid architecture for granularity adaptation, thus preserving
pixel-wise feature correlations in SSP. Extensive experiments across 8 tasks,
spanning 2 dimensions and 4 modalities, show that COVER significantly improves
pixel-wise SSP, advancing generalizable medical visual foundation models.

</details>


### [9] [Enhancing Ambiguous Dynamic Facial Expression Recognition with Soft Label-based Data Augmentation](https://arxiv.org/abs/2506.20867)
*Ryosuke Kawamura,Hideaki Hayashi,Shunsuke Otake,Noriko Takemura,Hajime Nagahara*

Main category: cs.CV

TL;DR: 论文提出了一种名为MIDAS的数据增强方法，用于提升动态面部表情识别（DFER）在模糊表情数据上的性能，通过软标签和视频帧的凸组合实现。


<details>
  <summary>Details</summary>
Motivation: 解决实际应用中模糊面部表情识别的挑战，尤其是在自然场景数据中。

Method: 提出MIDAS方法，通过软标签和视频帧的凸组合进行数据增强，扩展了mixup方法到视频数据。

Result: 在DFEW和FERV39k-Plus数据集上，MIDAS显著提升了模型性能，优于现有方法。

Conclusion: MIDAS是一种简单而高效的数据增强方法，能有效处理DFER中的模糊表情问题。

Abstract: Dynamic facial expression recognition (DFER) is a task that estimates
emotions from facial expression video sequences. For practical applications,
accurately recognizing ambiguous facial expressions -- frequently encountered
in in-the-wild data -- is essential. In this study, we propose MIDAS, a data
augmentation method designed to enhance DFER performance for ambiguous facial
expression data using soft labels representing probabilities of multiple
emotion classes. MIDAS augments training data by convexly combining pairs of
video frames and their corresponding emotion class labels. This approach
extends mixup to soft-labeled video data, offering a simple yet highly
effective method for handling ambiguity in DFER. To evaluate MIDAS, we
conducted experiments on both the DFEW dataset and FERV39k-Plus, a newly
constructed dataset that assigns soft labels to an existing DFER dataset. The
results demonstrate that models trained with MIDAS-augmented data achieve
superior performance compared to the state-of-the-art method trained on the
original dataset.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [10] [Global and Local Contrastive Learning for Joint Representations from Cardiac MRI and ECG](https://arxiv.org/abs/2506.20683)
*Alexander Selivanov,Philip Müller,Özgün Turgut,Nil Stolt-Ansó,Daniel Rückert*

Main category: eess.IV

TL;DR: PTACL是一种多模态对比学习框架，通过整合CMR的时空信息增强ECG表征，提升心脏功能评估能力。


<details>
  <summary>Details</summary>
Motivation: ECG无法直接测量心脏功能参数，而CMR虽精确但昂贵且不易获取，PTACL旨在弥合这一差距。

Method: PTACL结合全局患者级和局部时间级对比损失，对齐ECG与CMR表征，无需新增可学习参数。

Result: 在UK Biobank数据上，PTACL在患者检索和心脏功能参数预测任务中表现优于基线方法。

Conclusion: PTACL展示了通过ECG增强非侵入性心脏诊断的潜力。

Abstract: An electrocardiogram (ECG) is a widely used, cost-effective tool for
detecting electrical abnormalities in the heart. However, it cannot directly
measure functional parameters, such as ventricular volumes and ejection
fraction, which are crucial for assessing cardiac function. Cardiac magnetic
resonance (CMR) is the gold standard for these measurements, providing detailed
structural and functional insights, but is expensive and less accessible. To
bridge this gap, we propose PTACL (Patient and Temporal Alignment Contrastive
Learning), a multimodal contrastive learning framework that enhances ECG
representations by integrating spatio-temporal information from CMR. PTACL uses
global patient-level contrastive loss and local temporal-level contrastive
loss. The global loss aligns patient-level representations by pulling ECG and
CMR embeddings from the same patient closer together, while pushing apart
embeddings from different patients. Local loss enforces fine-grained temporal
alignment within each patient by contrasting encoded ECG segments with
corresponding encoded CMR frames. This approach enriches ECG representations
with diagnostic information beyond electrical activity and transfers more
insights between modalities than global alignment alone, all without
introducing new learnable weights. We evaluate PTACL on paired ECG-CMR data
from 27,951 subjects in the UK Biobank. Compared to baseline approaches, PTACL
achieves better performance in two clinically relevant tasks: (1) retrieving
patients with similar cardiac phenotypes and (2) predicting CMR-derived cardiac
function parameters, such as ventricular volumes and ejection fraction. Our
results highlight the potential of PTACL to enhance non-invasive cardiac
diagnostics using ECG. The code is available at:
https://github.com/alsalivan/ecgcmr

</details>


### [11] [Building Lightweight Semantic Segmentation Models for Aerial Images Using Dual Relation Distillation](https://arxiv.org/abs/2506.20688)
*Minglong Li,Lianlei Shan,Weiqiang Wang,Ke Lv,Bin Luo,Si-Bao Chen*

Main category: eess.IV

TL;DR: 提出了一种新颖的双重关系蒸馏（DRD）技术，通过传递空间和通道关系，提升轻量模型的语义分割精度。


<details>
  <summary>Details</summary>
Motivation: 解决CNN模型在语义分割中因模型过重和推理速度慢而限制实际应用的问题。

Method: 通过计算教师模型和学生模型的空间和通道关系图，并最小化其距离，传递特征分布信息。

Result: 在三个数据集（Vaihingen、Potsdam和Cityscapes）上显著提升了学生模型的性能，且未增加计算开销。

Conclusion: DRD技术有效平衡了分割精度与效率，具有实际应用潜力。

Abstract: Recently, there have been significant improvements in the accuracy of CNN
models for semantic segmentation. However, these models are often heavy and
suffer from low inference speed, which limits their practical application. To
address this issue, knowledge distillation has emerged as a promising approach
to achieve a good trade-off between segmentation accuracy and efficiency. In
this paper, we propose a novel dual relation distillation (DRD) technique that
transfers both spatial and channel relations in feature maps from a cumbersome
model (teacher) to a compact model (student). Specifically, we compute spatial
and channel relation maps separately for the teacher and student models, and
then align corresponding relation maps by minimizing their distance. Since the
teacher model usually learns more information and collects richer spatial and
channel correlations than the student model, transferring these correlations
from the teacher to the student can help the student mimic the teacher better
in terms of feature distribution, thus improving the segmentation accuracy of
the student model. We conduct comprehensive experiments on three segmentation
datasets, including two widely adopted benchmarks in the remote sensing field
(Vaihingen and Potsdam datasets) and one popular benchmark in general scene
(Cityscapes dataset). The experimental results demonstrate that our novel
distillation framework can significantly boost the performance of the student
network without incurring extra computational overhead.

</details>


### [12] [U-R-VEDA: Integrating UNET, Residual Links, Edge and Dual Attention, and Vision Transformer for Accurate Semantic Segmentation of CMRs](https://arxiv.org/abs/2506.20689)
*Racheal Mukisa,Arvind K. Bansal*

Main category: eess.IV

TL;DR: 提出了一种名为U-R-Veda的深度学习模型，用于心脏磁共振图像的自动语义分割，结合卷积变换、视觉变换器、注意力机制和边缘检测，显著提高了分割精度。


<details>
  <summary>Details</summary>
Motivation: 心脏图像自动准确分割是心脏疾病诊断和管理的关键步骤，现有方法在精度和信息保留方面存在不足。

Method: U-R-Veda模型整合了卷积变换、视觉变换器、残差连接、通道和空间注意力机制，以及边缘检测的跳跃连接，以减少信息损失并提升特征提取能力。

Result: 模型在DSC指标上达到95.2%的平均准确率，优于其他模型，尤其在右心室和左心室心肌的分割上表现突出。

Conclusion: U-R-Veda模型通过多模块集成显著提升了心脏图像分割的精度，为医学图像分析提供了更可靠的自动化工具。

Abstract: Artificial intelligence, including deep learning models, will play a
transformative role in automated medical image analysis for the diagnosis of
cardiac disorders and their management. Automated accurate delineation of
cardiac images is the first necessary initial step for the quantification and
automated diagnosis of cardiac disorders. In this paper, we propose a deep
learning based enhanced UNet model, U-R-Veda, which integrates convolution
transformations, vision transformer, residual links, channel-attention, and
spatial attention, together with edge-detection based skip-connections for an
accurate fully-automated semantic segmentation of cardiac magnetic resonance
(CMR) images. The model extracts local-features and their interrelationships
using a stack of combination convolution blocks, with embedded channel and
spatial attention in the convolution block, and vision transformers. Deep
embedding of channel and spatial attention in the convolution block identifies
important features and their spatial localization. The combined edge
information with channel and spatial attention as skip connection reduces
information-loss during convolution transformations. The overall model
significantly improves the semantic segmentation of CMR images necessary for
improved medical image analysis. An algorithm for the dual attention module
(channel and spatial attention) has been presented. Performance results show
that U-R-Veda achieves an average accuracy of 95.2%, based on DSC metrics. The
model outperforms the accuracy attained by other models, based on DSC and HD
metrics, especially for the delineation of right-ventricle and
left-ventricle-myocardium.

</details>


### [13] [Development of MR spectral analysis method robust against static magnetic field inhomogeneity](https://arxiv.org/abs/2506.20897)
*Shuki Maruyama,Hidenori Takeshima*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的谱分析方法，通过模拟B0不均匀性诱导的谱变化，提高了谱分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决静态磁场B0不均匀性对谱分析准确性的影响。

Method: 利用深度学习模型，训练基于B0图和健康人脑代谢物比生成的模拟谱，评估模型性能。

Result: 模拟谱与实测谱接近，模型性能显著优于传统方法，MSE和MAPE均有所改善。

Conclusion: 新方法通过增加训练样本，有望提高谱分析的准确性。

Abstract: Purpose:To develop a method that enhances the accuracy of spectral analysis
in the presence of static magnetic field B0 inhomogeneity. Methods:The authors
proposed a new spectral analysis method utilizing a deep learning model trained
on modeled spectra that consistently represent the spectral variations induced
by B0 inhomogeneity. These modeled spectra were generated from the B0 map and
metabolite ratios of the healthy human brain. The B0 map was divided into a
patch size of subregions, and the separately estimated metabolites and baseline
components were averaged and then integrated. The quality of the modeled
spectra was visually and quantitatively evaluated against the measured spectra.
The analysis models were trained using measured, simulated, and modeled
spectra. The performance of the proposed method was assessed using mean squared
errors (MSEs) of metabolite ratios. The mean absolute percentage errors (MAPEs)
of the metabolite ratios were also compared to LCModel when analyzing the
phantom spectra acquired under two types of B0 inhomogeneity. Results:The
modeled spectra exhibited broadened and narrowed spectral peaks depending on
the B0 inhomogeneity and were quantitatively close to the measured spectra. The
analysis model trained using measured spectra with modeled spectra improved
MSEs by 49.89% compared to that trained using measured spectra alone, and by
26.66% compared to that trained using measured spectra with simulated spectra.
The performance improved as the number of modeled spectra increased from 0 to
1,000. This model showed significantly lower MAPEs than LCModel under both
types of B0 inhomogeneity. Conclusion:A new spectral analysis-trained deep
learning model using the modeled spectra was developed. The results suggest
that the proposed method has the potential to improve the accuracy of spectral
analysis by increasing the training samples of spectra.

</details>


### [14] [A Novel Framework for Integrating 3D Ultrasound into Percutaneous Liver Tumour Ablation](https://arxiv.org/abs/2506.21162)
*Shuwei Xing,Derek W. Cool,David Tessier,Elvis C. S. Chen,Terry M. Peters,Aaron Fenster*

Main category: eess.IV

TL;DR: 本文提出了一种将3D超声（US）整合到标准肿瘤消融工作流程的新框架，包括一种临床可行的2D US-CT/MRI配准方法，并通过多模态图像可视化技术验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 3D超声在肝肿瘤消融中具有潜力，但肿瘤识别问题阻碍了其广泛应用。本文旨在解决这一问题，推动3D超声在治疗领域的应用。

Method: 提出了一种2D US-CT/MRI配准方法，利用3D超声作为中介降低配准复杂度，并开发了多模态图像可视化技术用于验证。

Result: 2D US-CT/MRI配准的标记点距离误差为2-4 mm，运行时间为0.22秒/图像对；非刚性配准比刚性配准减少了40%的对齐误差。

Conclusion: 该框架提升了3D超声在肿瘤消融中的应用能力，展示了其在临床治疗中的扩展潜力。

Abstract: 3D ultrasound (US) imaging has shown significant benefits in enhancing the
outcomes of percutaneous liver tumour ablation. Its clinical integration is
crucial for transitioning 3D US into the therapeutic domain. However,
challenges of tumour identification in US images continue to hinder its broader
adoption. In this work, we propose a novel framework for integrating 3D US into
the standard ablation workflow. We present a key component, a clinically viable
2D US-CT/MRI registration approach, leveraging 3D US as an intermediary to
reduce registration complexity. To facilitate efficient verification of the
registration workflow, we also propose an intuitive multimodal image
visualization technique. In our study, 2D US-CT/MRI registration achieved a
landmark distance error of approximately 2-4 mm with a runtime of 0.22s per
image pair. Additionally, non-rigid registration reduced the mean alignment
error by approximately 40% compared to rigid registration. Results demonstrated
the efficacy of the proposed 2D US-CT/MRI registration workflow. Our
integration framework advanced the capabilities of 3D US imaging in improving
percutaneous tumour ablation, demonstrating the potential to expand the
therapeutic role of 3D US in clinical interventions.

</details>


### [15] [Uncover Treasures in DCT: Advancing JPEG Quality Enhancement by Exploiting Latent Correlations](https://arxiv.org/abs/2506.21171)
*Jing Yang,Qunliang Xing,Mai Xu,Minglang Qiao*

Main category: eess.IV

TL;DR: 提出了一种基于DCT域的JPEG质量增强方法（AJQE），通过利用DCT系数中的两种关键相关性，显著提升了性能并降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有JPEG质量增强方法多在像素域操作，计算成本高；而DCT域方法性能有限。

Method: 识别JPEG图像DCT系数中的两种关键相关性，提出AJQE方法，将像素域模型适配到DCT域。

Result: 与像素域方法相比，PSNR提升0.35 dB，增强吞吐量提高60.5%。

Conclusion: AJQE方法在DCT域实现了高性能和低计算复杂度的JPEG质量增强。

Abstract: Joint Photographic Experts Group (JPEG) achieves data compression by
quantizing Discrete Cosine Transform (DCT) coefficients, which inevitably
introduces compression artifacts. Most existing JPEG quality enhancement
methods operate in the pixel domain, suffering from the high computational
costs of decoding. Consequently, direct enhancement of JPEG images in the DCT
domain has gained increasing attention. However, current DCT-domain methods
often exhibit limited performance. To address this challenge, we identify two
critical types of correlations within the DCT coefficients of JPEG images.
Building on this insight, we propose an Advanced DCT-domain JPEG Quality
Enhancement (AJQE) method that fully exploits these correlations. The AJQE
method enables the adaptation of numerous well-established pixel-domain models
to the DCT domain, achieving superior performance with reduced computational
complexity. Compared to the pixel-domain counterparts, the DCT-domain models
derived by our method demonstrate a 0.35 dB improvement in PSNR and a 60.5%
increase in enhancement throughput on average.

</details>


### [16] [GANet-Seg: Adversarial Learning for Brain Tumor Segmentation with Hybrid Generative Models](https://arxiv.org/abs/2506.21245)
*Qifei Cui,Xinyu Lu*

Main category: eess.IV

TL;DR: 提出了一种结合预训练GAN和Unet的脑肿瘤分割框架，通过全局异常检测和精细化掩模生成网络提高分割精度，利用多模态MRI数据和合成图像增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决脑肿瘤分割中标注数据有限的问题，提高分割精度和鲁棒性。

Method: 结合预训练GAN和Unet，引入全局异常检测模块和精细化掩模生成网络，利用对抗损失约束和多模态MRI数据增强。

Result: 在BraTS数据集上表现优异，Dice和HD95指标优于基线方法。

Conclusion: 该方法减少了对全标注数据的依赖，具有实际临床应用潜力。

Abstract: This work introduces a novel framework for brain tumor segmentation
leveraging pre-trained GANs and Unet architectures. By combining a global
anomaly detection module with a refined mask generation network, the proposed
model accurately identifies tumor-sensitive regions and iteratively enhances
segmentation precision using adversarial loss constraints. Multi-modal MRI data
and synthetic image augmentation are employed to improve robustness and address
the challenge of limited annotated datasets. Experimental results on the BraTS
dataset demonstrate the effectiveness of the approach, achieving high
sensitivity and accuracy in both lesion-wise Dice and HD95 metrics than the
baseline. This scalable method minimizes the dependency on fully annotated
data, paving the way for practical real-world applications in clinical
settings.

</details>


### [17] [Lightweight Physics-Informed Zero-Shot Ultrasound Plane Wave Denoising](https://arxiv.org/abs/2506.21499)
*Hojat Asgariandehkordi,Mostafa Sharifzadeh,Hassan Rivaz*

Main category: eess.IV

TL;DR: 提出一种零样本去噪框架，用于低角度CPWC成像，通过自监督学习提升图像对比度，无需额外训练数据。


<details>
  <summary>Details</summary>
Motivation: 传统CPWC成像在增加角度数量时虽能提升图像质量，但会降低帧率并引入模糊伪影，且对噪声敏感。

Method: 将传输角度分为两个子集，生成含噪声的复合图像，通过自监督残差学习训练轻量级模型，分离噪声与组织信号。

Result: 在仿真、体模和活体数据上验证，对比度和结构保持优于传统及深度学习方法。

Conclusion: 该方法无需领域特定微调或配对数据，适应性强，计算成本低。

Abstract: Ultrasound Coherent Plane Wave Compounding (CPWC) enhances image contrast by
combining echoes from multiple steered transmissions. While increasing the
number of angles generally improves image quality, it drastically reduces the
frame rate and can introduce blurring artifacts in fast-moving targets.
Moreover, compounded images remain susceptible to noise, particularly when
acquired with a limited number of transmissions. We propose a zero-shot
denoising framework tailored for low-angle CPWC acquisitions, which enhances
contrast without relying on a separate training dataset. The method divides the
available transmission angles into two disjoint subsets, each used to form
compound images that include higher noise levels. The new compounded images are
then used to train a deep model via a self-supervised residual learning scheme,
enabling it to suppress incoherent noise while preserving anatomical
structures. Because angle-dependent artifacts vary between the subsets while
the underlying tissue response is similar, this physics-informed pairing allows
the network to learn to disentangle the inconsistent artifacts from the
consistent tissue signal. Unlike supervised methods, our model requires no
domain-specific fine-tuning or paired data, making it adaptable across
anatomical regions and acquisition setups. The entire pipeline supports
efficient training with low computational cost due to the use of a lightweight
architecture, which comprises only two convolutional layers. Evaluations on
simulation, phantom, and in vivo data demonstrate superior contrast enhancement
and structure preservation compared to both classical and deep learning-based
denoising methods.

</details>


### [18] [Exploring the Design Space of 3D MLLMs for CT Report Generation](https://arxiv.org/abs/2506.21535)
*Mohammed Baharoon,Jun Ma,Congyu Fang,Augustin Toma,Bo Wang*

Main category: eess.IV

TL;DR: 本文系统研究了3D多模态大语言模型（MLLMs）在放射学报告生成（RRG）中的设计空间，包括视觉输入表示、投影器、大语言模型（LLMs）和微调技术，并提出了两种基于知识的报告增强方法。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用3D MLLMs自动化生成放射学报告，并优化其性能。

Method: 研究了3D MLLMs的设计空间，包括视觉输入表示、投影器、LLMs和微调技术，并引入了两种知识驱动的报告增强方法。

Result: 在AMOS-MM数据集上，性能提升了10%，在MICCAI 2024挑战赛中排名第二；发现RRG性能与LLM大小无关，且分割掩码的使用能提升性能。

Conclusion: 3D MLLMs在RRG中具有潜力，但需注意预训练数据与模型设计的匹配，分割掩码的使用可进一步优化性能。

Abstract: Multimodal Large Language Models (MLLMs) have emerged as a promising way to
automate Radiology Report Generation (RRG). In this work, we systematically
investigate the design space of 3D MLLMs, including visual input
representation, projectors, Large Language Models (LLMs), and fine-tuning
techniques for 3D CT report generation. We also introduce two knowledge-based
report augmentation methods that improve performance on the GREEN score by up
to 10\%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our
results on the 1,687 cases from the AMOS-MM dataset show that RRG is largely
independent of the size of LLM under the same training protocol. We also show
that larger volume size does not always improve performance if the original ViT
was pre-trained on a smaller volume size. Lastly, we show that using a
segmentation mask along with the CT volume improves performance. The code is
publicly available at https://github.com/bowang-lab/AMOS-MM-Solution

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [19] [Generative Blocks World: Moving Things Around in Pictures](https://arxiv.org/abs/2506.20703)
*Vaibhav Vavilala,Seemandhar Jain,Rahul Vasanth,D. A. Forsyth,Anand Bhattad*

Main category: cs.GR

TL;DR: 提出了一种通过操纵简单几何抽象来编辑生成图像场景的方法，使用3D基元表示场景，并通过流式方法生成图像。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在纹理一致性和编辑灵活性上的不足。

Method: 将场景表示为3D基元的组合，通过流式方法生成图像，利用纹理提示保持一致性。

Result: 在视觉保真度、可编辑性和组合泛化性上优于现有方法。

Conclusion: 该方法在生成图像的编辑和一致性上表现优异，具有实际应用潜力。

Abstract: We describe Generative Blocks World to interact with the scene of a generated
image by manipulating simple geometric abstractions. Our method represents
scenes as assemblies of convex 3D primitives, and the same scene can be
represented by different numbers of primitives, allowing an editor to move
either whole structures or small details. Once the scene geometry has been
edited, the image is generated by a flow-based method which is conditioned on
depth and a texture hint. Our texture hint takes into account the modified 3D
primitives, exceeding texture-consistency provided by existing key-value
caching techniques. These texture hints (a) allow accurate object and camera
moves and (b) largely preserve the identity of objects depicted. Quantitative
and qualitative experiments demonstrate that our approach outperforms prior
works in visual fidelity, editability, and compositional generalization.

</details>


### [20] [3DGH: 3D Head Generation with Composable Hair and Face](https://arxiv.org/abs/2506.20875)
*Chengan He,Junxuan Li,Tobias Kirschstein,Artem Sevastopolsky,Shunsuke Saito,Qingyang Tan,Javier Romero,Chen Cao,Holly Rushmeier,Giljoo Nam*

Main category: cs.GR

TL;DR: 3DGH是一个无条件生成3D人头模型，能够组合头发和面部组件，通过分离建模和新型数据表示实现高效合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法将头发和面部建模混为一谈，限制了灵活性和编辑能力，3DGH旨在解决这一问题。

Method: 使用基于模板的3D高斯点云表示，引入可变形头发几何体，设计双生成器GAN架构和交叉注意力机制。

Result: 实验验证了3DGH的有效性，在无条件全头图像合成和可组合3D发型编辑上优于现有方法。

Conclusion: 3DGH通过分离头发和面部建模，实现了高效的生成和编辑能力，为3D人头合成提供了新思路。

Abstract: We present 3DGH, an unconditional generative model for 3D human heads with
composable hair and face components. Unlike previous work that entangles the
modeling of hair and face, we propose to separate them using a novel data
representation with template-based 3D Gaussian Splatting, in which deformable
hair geometry is introduced to capture the geometric variations across
different hairstyles. Based on this data representation, we design a 3D
GAN-based architecture with dual generators and employ a cross-attention
mechanism to model the inherent correlation between hair and face. The model is
trained on synthetic renderings using carefully designed objectives to
stabilize training and facilitate hair-face separation. We conduct extensive
experiments to validate the design choice of 3DGH, and evaluate it both
qualitatively and quantitatively by comparing with several state-of-the-art 3D
GAN methods, demonstrating its effectiveness in unconditional full-head image
synthesis and composable 3D hairstyle editing. More details will be available
on our project page: https://c-he.github.io/projects/3dgh/.

</details>


### [21] [Data Visualization for Improving Financial Literacy: A Systematic Review](https://arxiv.org/abs/2506.20901)
*Meng Du,Robert Amor,Kwan-Liu Ma,Burkhard C. Wünsche*

Main category: cs.GR

TL;DR: 本文通过系统综述37篇研究论文，探讨了数据可视化和视觉分析在金融教育和素养提升中的应用，并分类为五个关键领域，同时指出了研究空白和未来方向。


<details>
  <summary>Details</summary>
Motivation: 金融素养对个人财务决策至关重要，但许多人对此感到困难。数据可视化可以简化金融概念，提高学习效果。

Method: 系统综述37篇研究论文，分类为五个关键领域：可视化使用的演变、动机、金融主题与教学方法、工具与技术、教学干预效果评估。

Result: 研究发现数据可视化在金融教育中具有显著效果，并提供了实用见解以优化视觉工具的设计和使用。

Conclusion: 数据可视化是提升金融素养的有效工具，未来研究可进一步探索其应用潜力。

Abstract: Financial literacy empowers individuals to make informed and effective
financial decisions, improving their overall financial well-being and security.
However, for many people understanding financial concepts can be daunting and
only half of US adults are considered financially literate. Data visualization
simplifies these concepts, making them accessible and engaging for learners of
all ages. This systematic review analyzes 37 research papers exploring the use
of data visualization and visual analytics in financial education and literacy
enhancement. We classify these studies into five key areas: (1) the evolution
of visualization use across time and space, (2) motivations for using
visualization tools, (3) the financial topics addressed and instructional
approaches used, (4) the types of tools and technologies applied, and (5) how
the effectiveness of teaching interventions was evaluated. Furthermore, we
identify research gaps and highlight opportunities for advancing financial
literacy. Our findings offer practical insights for educators and professionals
to effectively utilize or design visual tools for financial literacy.

</details>


### [22] [Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models](https://arxiv.org/abs/2506.20946)
*Donggoo Kang,Jangyeong Kim,Dasol Jeong,Junyoung Choi,Jeonga Wi,Hyunmin Lee,Joonho Gwon,Joonki Paik*

Main category: cs.GR

TL;DR: VideoTex利用视频生成模型解决3D纹理合成中的时空不一致问题，通过几何感知条件和结构UV扩散策略，生成更平滑、一致的纹理。


<details>
  <summary>Details</summary>
Motivation: 现有纹理合成方法因缺乏全局上下文和几何理解导致不一致，而视频生成模型在时间一致性上表现优异，因此结合两者解决纹理合成问题。

Method: 提出VideoTex框架，结合几何感知条件和结构UV扩散策略，利用3D网格结构和语义信息生成纹理。

Result: 实验表明VideoTex在纹理保真度、接缝融合和稳定性上优于现有方法。

Conclusion: VideoTex为动态实时应用提供了高质量、时间稳定的纹理合成方案。

Abstract: Current texture synthesis methods, which generate textures from fixed
viewpoints, suffer from inconsistencies due to the lack of global context and
geometric understanding. Meanwhile, recent advancements in video generation
models have demonstrated remarkable success in achieving temporally consistent
videos. In this paper, we introduce VideoTex, a novel framework for seamless
texture synthesis that leverages video generation models to address both
spatial and temporal inconsistencies in 3D textures. Our approach incorporates
geometry-aware conditions, enabling precise utilization of 3D mesh structures.
Additionally, we propose a structure-wise UV diffusion strategy, which enhances
the generation of occluded areas by preserving semantic information, resulting
in smoother and more coherent textures. VideoTex not only achieves smoother
transitions across UV boundaries but also ensures high-quality, temporally
stable textures across video frames. Extensive experiments demonstrate that
VideoTex outperforms existing methods in texture fidelity, seam blending, and
stability, paving the way for dynamic real-time applications that demand both
visual quality and temporal coherence.

</details>


### [23] [FairyGen: Storied Cartoon Video from a Single Child-Drawn Character](https://arxiv.org/abs/2506.21272)
*Jiayi Zheng,Xiaodong Cun*

Main category: cs.GR

TL;DR: FairyGen是一个自动系统，能从单张儿童绘画生成故事驱动的卡通视频，并保留其独特的艺术风格。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注角色一致性和基本动作，而FairyGen旨在通过分离角色建模与风格化背景生成，并结合电影镜头设计，实现更具表现力和连贯性的故事讲述。

Method: 系统使用MLLM生成结构化故事板，引入风格传播适配器确保视觉一致性，并通过镜头设计模块增强视觉多样性和电影质量。角色动画通过3D代理重建和MMDiT模型实现，运动定制适配器分两阶段学习外观特征和时序动态。

Result: 实验表明，FairyGen生成的动画风格忠实、叙事结构自然，具有个性化故事动画的潜力。

Conclusion: FairyGen为个性化故事动画提供了一种高效且风格一致的解决方案。

Abstract: We propose FairyGen, an automatic system for generating story-driven cartoon
videos from a single child's drawing, while faithfully preserving its unique
artistic style. Unlike previous storytelling methods that primarily focus on
character consistency and basic motion, FairyGen explicitly disentangles
character modeling from stylized background generation and incorporates
cinematic shot design to support expressive and coherent storytelling. Given a
single character sketch, we first employ an MLLM to generate a structured
storyboard with shot-level descriptions that specify environment settings,
character actions, and camera perspectives. To ensure visual consistency, we
introduce a style propagation adapter that captures the character's visual
style and applies it to the background, faithfully retaining the character's
full visual identity while synthesizing style-consistent scenes. A shot design
module further enhances visual diversity and cinematic quality through frame
cropping and multi-view synthesis based on the storyboard. To animate the
story, we reconstruct a 3D proxy of the character to derive physically
plausible motion sequences, which are then used to fine-tune an MMDiT-based
image-to-video diffusion model. We further propose a two-stage motion
customization adapter: the first stage learns appearance features from
temporally unordered frames, disentangling identity from motion; the second
stage models temporal dynamics using a timestep-shift strategy with frozen
identity weights. Once trained, FairyGen directly renders diverse and coherent
video scenes aligned with the storyboard. Extensive experiments demonstrate
that our system produces animations that are stylistically faithful,
narratively structured natural motion, highlighting its potential for
personalized and engaging story animation. The code will be available at
https://github.com/GVCLab/FairyGen

</details>


### [24] [IDGraphs: Intrusion Detection and Analysis Using Stream Compositing](https://arxiv.org/abs/2506.21425)
*Pin Ren,Yan Gao,Zhichun Li,Yan Chen,Benjamin Watson*

Main category: cs.GR

TL;DR: IDGraphs是一个交互式可视化系统，用于解决现有入侵检测系统在交互式检查、分析蠕虫传播模式和发现相关攻击方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前网络流量异常和攻击频发，现有入侵检测系统在交互式分析和检测分布式攻击方面存在局限，亟需改进。

Method: IDGraphs通过流级跟踪和时间轴可视化，结合Histographs技术汇总数据，用户可通过交互式查询和分析子集来检测攻击。

Result: 在包含1.79亿流级记录的真实数据集上，IDGraphs成功检测到端口扫描、蠕虫爆发、TCP SYN洪泛等多种攻击。

Conclusion: IDGraphs提供了一种高效且直观的方法，显著提升了入侵检测的交互性和准确性。

Abstract: Traffic anomalies and attacks are commonplace in today's networks and
identifying them rapidly and accurately is critical for large network
operators. For a statistical intrusion detection system (IDS), it is crucial to
detect at the flow-level for accurate detection and mitigation. However,
existing IDS systems offer only limited support for 1) interactively examining
detected intrusions and anomalies, 2) analyzing worm propagation patterns, 3)
and discovering correlated attacks. These problems are becoming even more acute
as the traffic on today's high-speed routers continues to grow.
  IDGraphs is an interactive visualization system for intrusion detection that
addresses these challenges. The central visualization in the system is a
flow-level trace plotted with time on the horizontal axis and aggregated number
of unsuccessful connections on the vertical axis. We then summarize a stack of
tens or hundreds of thousands of these traces using the Histographs [RW05]
technique, which maps data frequency at each pixel to brightness. Users may
then interactively query the summary view, performing analysis by highlighting
subsets of the traces. For example, brushing a linked correlation matrix view
highlights traces with similar patterns, revealing distributed attacks that are
difficult to detect using standard statistical analysis.
  We apply IDGraphs system to a real network router data-set with 179M
flow-level records representing a total traffic of 1.16TB. The system
successfully detects and analyzes a variety of attacks and anomalies, including
port scanning, worm outbreaks, stealthy TCP SYN floodings, and some distributed
attacks.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [25] [Efficacy of Temporal Fusion Transformers for Runoff Simulation](https://arxiv.org/abs/2506.20831)
*Sinan Rasiya Koya,Tirthankar Roy*

Main category: physics.geo-ph

TL;DR: TFT在降雨径流建模中略优于LSTM，尤其在模拟水文图中段和峰值时表现更好，并能处理更长序列。


<details>
  <summary>Details</summary>
Motivation: 探索TFT在降雨径流建模中的优势，并与LSTM进行比较，以改进水文模型。

Method: 训练10个随机初始化的TFT和LSTM模型，应用于531个美国CAMELS流域，并在Caravan数据集的5个子集上重复实验。

Result: TFT略优于LSTM，能处理更长序列，但两者在Caravan数据集上性能显著下降。

Conclusion: TFT在水文建模中具有潜力，但数据质量可能影响模型性能。

Abstract: Combining attention with recurrence has shown to be valuable in sequence
modeling, including hydrological predictions. Here, we explore the strength of
Temporal Fusion Transformers (TFTs) over Long Short-Term Memory (LSTM) networks
in rainfall-runoff modeling. We train ten randomly initialized models, TFT and
LSTM, for 531 CAMELS catchments in the US. We repeat the experiment with five
subsets of the Caravan dataset, each representing catchments in the US,
Australia, Brazil, Great Britain, and Chile. Then, the performance of the
models, their variability regarding the catchment attributes, and the
difference according to the datasets are assessed. Our findings show that TFT
slightly outperforms LSTM, especially in simulating the midsection and peak of
hydrographs. Furthermore, we show the ability of TFT to handle longer sequences
and why it can be a better candidate for higher or larger catchments. Being an
explainable AI technique, TFT identifies the key dynamic and static variables,
providing valuable scientific insights. However, both TFT and LSTM exhibit a
considerable drop in performance with the Caravan dataset, indicating possible
data quality issues. Overall, the study highlights the potential of TFT in
improving hydrological modeling and understanding.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [26] [An evaluation of level of detail degradation in head-mounted display peripheries](https://arxiv.org/abs/2506.21441)
*Benjamin Watson,Neff Walker,Larry F Hodges,Martin Reddy*

Main category: cs.HC

TL;DR: 论文提出了一种管理虚拟环境中细节级别的系统设计范式，并通过用户研究评估了高细节插图的显示效果。结果显示，无插图的高细节显示与有插图的显示在搜索时间和准确性上无显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索如何在虚拟环境中有效管理细节级别，尤其是通过高细节插图提升用户体验。

Method: 方法包括用户研究，10名受试者在不同显示条件下完成搜索任务，控制变量包括帧率、目标位置等。

Result: 结果显示，无插图的高细节显示与有插图的显示在性能上无显著差异，仅无插图的低细节显示表现显著不同。

Conclusion: 结论表明高细节插图可能并非必要，未来研究将进一步探讨任务复杂性、插图大小和细节级别的影响。

Abstract: A paradigm for the design of systems that manage level of detail in virtual
environments is proposed. As an example of the prototyping step in this
paradigm, a user study was performed to evaluate the effectiveness of high
detail insets used with head-mounted displays. Ten subjects were given a simple
search task that required the location and identification of a single target
object. All subjects used seven different displays (the independent variable),
varying in inset size and peripheral detail, to perform this task. Frame rate,
target location, subject input method, and order of display use were all
controlled. Primary dependent measures were search time on trials with correct
identification, and the percentage of all trials correctly identified. ANOVAs
of the results showed that insetless, high detail displays did not lead to
significantly different search times or accuracies than displays with insets.
In fact, only the insetless, low detail display returned significantly
different results. Further research is being performed to examine the effect of
varying task complexity, inset size, and level of detail.

</details>


### [27] [Managing level of detail through head-tracked peripheral degradation: a model and resulting design principles](https://arxiv.org/abs/2506.21456)
*Benjamin Watson,Neff Walker,Larry F Hodges*

Main category: cs.HC

TL;DR: 研究提出了一种基于心理物理学的模型，解释了外围细节降低（LOD）在头戴式大视场显示器中的有效性，并通过实验验证了中心高细节区域（inset）的形状和面积对搜索性能的影响。


<details>
  <summary>Details</summary>
Motivation: 探索外围细节降低（LOD）在头戴式大视场显示器中的有效性，并为其设计提供理论依据。

Method: 提出了一种基于眼/头运动权衡的心理物理学模型，并通过实验评估了中心高细节区域的形状和面积对搜索性能的影响。

Result: 实验表明，中心区域的形状对性能无显著影响，但面积（至少30度的水平和垂直视角）是关键因素，与未降低细节的显示性能相当。

Conclusion: 研究结果支持了提出的模型，为外围细节降低显示器的设计提供了指导。

Abstract: Previous work has demonstrated the utility of reductions in the level of
detail (LOD) in the periphery of head-tracked, large field of view displays.
This paper provides a psychophysically based model, centered around an eye/head
movement tradeoff, that explains the effectiveness of peripheral degradation
and suggests how peripherally degraded displays should be designed. An
experiment evaluating the effect on search performance of the shape and area of
the high detail central area (inset) in peripherally degraded displays was
performed, results indicated that inset shape is not a significant factor in
performance. Inset area, however, was significant: performance with displays
subtending at least 30 degrees of horizontal and vertical angle was not
significantly different from performance with an undegraded display. These
results agreed with the proposed model.

</details>
