<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 322]
- [eess.IV](#eess.IV) [Total: 46]
- [cs.RO](#cs.RO) [Total: 52]
- [cs.GR](#cs.GR) [Total: 8]
- [physics.geo-ph](#physics.geo-ph) [Total: 2]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.CL](#cs.CL) [Total: 12]
- [stat.ML](#stat.ML) [Total: 2]
- [physics.optics](#physics.optics) [Total: 4]
- [cs.HC](#cs.HC) [Total: 2]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [cs.AI](#cs.AI) [Total: 4]
- [math.NA](#math.NA) [Total: 2]
- [astro-ph.EP](#astro-ph.EP) [Total: 2]
- [cs.LG](#cs.LG) [Total: 12]
- [cs.AR](#cs.AR) [Total: 2]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.CR](#cs.CR) [Total: 2]
- [stat.AP](#stat.AP) [Total: 2]
- [stat.CO](#stat.CO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Enhancing Vision Transformer Explainability Using Artificial Astrocytes](https://arxiv.org/abs/2505.21513)
*Nicolas Echevarrieta-Catalan,Ana Ribas-Rodriguez,Francisco Cedron,Odelia Schwartz,Vanessa Aguiar-Pulido*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练的ViTA方法，通过模拟神经科学中的星形胶质细胞，提升预训练模型的解释性，使其更符合人类感知。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型虽精度高，但解释性不足，且随着模型复杂度增加，解释性进一步降低。现有方法（如XAI技术或训练约束）适用性有限。

Method: 提出ViTA方法，基于神经科学启发，通过人工星形胶质细胞增强预训练模型的推理能力，无需额外训练。使用Grad-CAM和Grad-CAM++技术评估。

Result: 实验表明，ViTA显著提升了模型解释与人类感知的对齐度，在所有XAI技术和指标上均有统计学意义的改进。

Conclusion: ViTA为提升模型解释性提供了有效且无需训练的新思路，尤其在复杂模型中表现突出。

Abstract: Machine learning models achieve high precision, but their decision-making
processes often lack explainability. Furthermore, as model complexity
increases, explainability typically decreases. Existing efforts to improve
explainability primarily involve developing new eXplainable artificial
intelligence (XAI) techniques or incorporating explainability constraints
during training. While these approaches yield specific improvements, their
applicability remains limited. In this work, we propose the Vision Transformer
with artificial Astrocytes (ViTA). This training-free approach is inspired by
neuroscience and enhances the reasoning of a pretrained deep neural network to
generate more human-aligned explanations. We evaluated our approach employing
two well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a
standard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the
similarity between the heatmaps produced by the XAI techniques and a
(human-aligned) ground truth. Our results consistently demonstrate that
incorporating artificial astrocytes enhances the alignment of model
explanations with human perception, leading to statistically significant
improvements across all XAI techniques and metrics utilized.

</details>


### [2] [Do DeepFake Attribution Models Generalize?](https://arxiv.org/abs/2505.21520)
*Spiros Baxavanakis,Manos Schinas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 论文探讨了DeepFake检测的局限性，提出多分类和归因模型的重要性，并通过实验验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 由于DeepFake技术的普及，检测模型的局限性（如将所有篡改方法等同对待）威胁了信息的真实性，因此需要更精细的检测和归因模型。

Method: 利用五种骨干模型，在六个数据集上比较了二分类和多分类模型的泛化能力，并评估了归因模型和对比方法的性能。

Result: 二分类模型泛化能力更强，但更大的模型、对比方法和高质量数据可提升归因模型的性能。

Conclusion: 归因模型在提升检测可信度和可解释性方面具有潜力，未来需进一步优化。

Abstract: Recent advancements in DeepFake generation, along with the proliferation of
open-source tools, have significantly lowered the barrier for creating
synthetic media. This trend poses a serious threat to the integrity and
authenticity of online information, undermining public trust in institutions
and media. State-of-the-art research on DeepFake detection has primarily
focused on binary detection models. A key limitation of these models is that
they treat all manipulation techniques as equivalent, despite the fact that
different methods introduce distinct artifacts and visual cues. Only a limited
number of studies explore DeepFake attribution models, although such models are
crucial in practical settings. By providing the specific manipulation method
employed, these models could enhance both the perceived trustworthiness and
explainability for end users. In this work, we leverage five state-of-the-art
backbone models and conduct extensive experiments across six DeepFake datasets.
First, we compare binary and multi-class models in terms of cross-dataset
generalization. Second, we examine the accuracy of attribution models in
detecting seen manipulation methods in unknown datasets, hence uncovering data
distribution shifts on the same DeepFake manipulations. Last, we assess the
effectiveness of contrastive methods in improving cross-dataset generalization
performance. Our findings indicate that while binary models demonstrate better
generalization abilities, larger models, contrastive methods, and higher data
quality can lead to performance improvements in attribution models. The code of
this work is available on GitHub.

</details>


### [3] [CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures](https://arxiv.org/abs/2505.21522)
*Shan Gao,Zhiqiang Wu,Yawen Niu,Xiaotao Li,Qingqing Xu*

Main category: cs.CV

TL;DR: 论文提出了一种硬件-算法协同设计框架CIM-NET，通过优化架构和引入伪卷积算子CIM-CONV，显著减少了矩阵-向量乘法操作，提升了在内存计算芯片上的推理速度，同时保持了去噪性能。


<details>
  <summary>Details</summary>
Motivation: 现有DNN模型未考虑内存计算（CIM）芯片的架构限制，限制了其加速潜力，因此需要一种硬件友好的设计框架。

Method: 提出了CIM-NET架构和伪卷积算子CIM-CONV，结合滑动窗口处理和全连接变换，优化了大规模感受野操作和矩阵-向量乘法加速。

Result: 相比FastDVDnet，CIM-NET将MVM操作减少至1/77，同时PSNR仅略有下降（35.11 dB vs. 35.56 dB）。

Conclusion: CIM-NET框架在保持性能的同时显著提升了推理效率，为边缘设备上的实时视频去噪提供了可行方案。

Abstract: While deep neural network (DNN)-based video denoising has demonstrated
significant performance, deploying state-of-the-art models on edge devices
remains challenging due to stringent real-time and energy efficiency
requirements. Computing-in-Memory (CIM) chips offer a promising solution by
integrating computation within memory cells, enabling rapid matrix-vector
multiplication (MVM). However, existing DNN models are often designed without
considering CIM architectural constraints, thus limiting their acceleration
potential during inference. To address this, we propose a hardware-algorithm
co-design framework incorporating two innovations: (1) a CIM-Aware
Architecture, CIM-NET, optimized for large receptive field operation and CIM's
crossbar-based MVM acceleration; and (2) a pseudo-convolutional operator,
CIM-CONV, used within CIM-NET to integrate slide-based processing with fully
connected transformations for high-quality feature extraction and
reconstruction. This framework significantly reduces the number of MVM
operations, improving inference speed on CIM chips while maintaining
competitive performance. Experimental results indicate that, compared to the
conventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM
operations with a slight decrease in denoising performance. With a stride value
of 8, CIM-NET reduces MVM operations to 1/77th of the original, while
maintaining competitive PSNR (35.11 dB vs. 35.56 dB

</details>


### [4] [Learning Shared Representations from Unpaired Data](https://arxiv.org/abs/2505.21524)
*Amitai Yacobi,Nir Ben-Ari,Ronen Talmon,Uri Shaham*

Main category: cs.CV

TL;DR: 该论文提出了一种仅需非配对数据即可学习共享表示的方法，通过谱嵌入技术在多模态任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前多模态表示学习方法依赖配对样本，但配对数据难以获取，因此探索如何利用非配对数据学习共享表示。

Method: 基于随机游走矩阵的谱嵌入技术，独立构建单模态表示，再学习共享嵌入空间。

Result: 在计算机视觉和自然语言处理任务中表现优异，支持检索、生成、算术、零样本和跨域分类。

Conclusion: 首次证明非配对数据可学习通用跨模态嵌入，为多模态表示学习提供了新方向。

Abstract: Learning shared representations is a primary area of multimodal
representation learning. The current approaches to achieve a shared embedding
space rely heavily on paired samples from each modality, which are
significantly harder to obtain than unpaired ones. In this work, we demonstrate
that shared representations can be learned almost exclusively from unpaired
data. Our arguments are grounded in the spectral embeddings of the random walk
matrices constructed independently from each unimodal representation. Empirical
results in computer vision and natural language processing domains support its
potential, revealing the effectiveness of unpaired data in capturing meaningful
cross-modal relations, demonstrating high capabilities in retrieval tasks,
generation, arithmetics, zero-shot, and cross-domain classification. This work,
to the best of our knowledge, is the first to demonstrate these capabilities
almost exclusively from unpaired samples, giving rise to a cross-modal
embedding that could be viewed as universal, i.e., independent of the specific
modalities of the data. Our code IS publicly available at
https://github.com/shaham-lab/SUE.

</details>


### [5] [UniDB++: Fast Sampling of Unified Diffusion Bridge](https://arxiv.org/abs/2505.21528)
*Mokai Pan,Kaizhen Zhu,Yuexin Ma,Yanwei Fu,Jingyi Yu,Jingya Wang,Ye Shi*

Main category: cs.CV

TL;DR: UniDB++是一种无需训练的采样算法，通过精确求解反向时间SDE，显著提升了UniDB框架的效率和生成质量，减少了计算成本。


<details>
  <summary>Details</summary>
Motivation: UniDB框架在图像生成中依赖迭代采样方法，导致推理速度慢且计算成本高，现有加速技术无法解决其独特挑战。

Method: UniDB++通过推导反向时间SDE的闭式解，减少误差积累，并引入数据预测模型和SDE-Corrector机制提升稳定性。

Result: 实验表明，UniDB++在图像修复任务中表现优异，生成质量高且推理速度快，采样步骤减少20倍。

Conclusion: UniDB++在理论通用性和实际效率之间架起桥梁，为SOC驱动的扩散桥模型提供了高效解决方案。

Abstract: Diffusion Bridges enable transitions between arbitrary distributions, with
the Unified Diffusion Bridge (UniDB) framework achieving high-fidelity image
generation via a Stochastic Optimal Control (SOC) formulation. However, UniDB's
reliance on iterative Euler sampling methods results in slow, computationally
expensive inference, while existing acceleration techniques for diffusion or
diffusion bridge models fail to address its unique challenges: missing terminal
mean constraints and SOC-specific penalty coefficients in its SDEs. We present
UniDB++, a training-free sampling algorithm that significantly improves upon
these limitations. The method's key advancement comes from deriving exact
closed-form solutions for UniDB's reverse-time SDEs, effectively reducing the
error accumulation inherent in Euler approximations and enabling high-quality
generation with up to 20$\times$ fewer sampling steps. This method is further
complemented by replacing conventional noise prediction with a more stable data
prediction model, along with an SDE-Corrector mechanism that maintains
perceptual quality for low-step regimes (5-10 steps). Additionally, we
demonstrate that UniDB++ aligns with existing diffusion bridge acceleration
methods by evaluating their update rules, and UniDB++ can recover DBIMs as
special cases under some theoretical conditions. Experiments demonstrate
UniDB++'s state-of-the-art performance in image restoration tasks,
outperforming Euler-based methods in fidelity and speed while reducing
inference time significantly. This work bridges the gap between theoretical
generality and practical efficiency in SOC-driven diffusion bridge models. Our
code is available at https://github.com/2769433owo/UniDB-plusplus.

</details>


### [6] [How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control](https://arxiv.org/abs/2505.21531)
*Kunhang Li,Jason Naradowsky,Yansong Feng,Yusuke Miyao*

Main category: cs.CV

TL;DR: LLMs在3D虚拟角色控制中展示了对高级运动规划的理解能力，但在精确身体部位定位和多步骤运动规划上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在人类运动知识方面的能力，通过3D虚拟角色控制验证其运动规划和执行效果。

Method: 通过高级运动规划和低级身体部位定位两步法，生成虚拟角色动画，并进行人类评估和自动对比。

Result: LLMs擅长高级运动规划，但在精确定位和多步骤运动上表现较差；在创意和文化特定运动上表现良好。

Conclusion: LLMs在运动规划中有潜力，但需改进精确空间定位和多步骤运动处理能力。

Abstract: We explore Large Language Models (LLMs)' human motion knowledge through 3D
avatar control. Given a motion instruction, we prompt LLMs to first generate a
high-level movement plan with consecutive steps (High-level Planning), then
specify body part positions in each step (Low-level Planning), which we
linearly interpolate into avatar animations as a clear verification lens for
human evaluators. Through carefully designed 20 representative motion
instructions with full coverage of basic movement primitives and balanced body
part usage, we conduct comprehensive evaluations including human assessment of
both generated animations and high-level movement plans, as well as automatic
comparison with oracle positions in low-level planning. We find that LLMs are
strong at interpreting the high-level body movements but struggle with precise
body part positioning. While breaking down motion queries into atomic
components improves planning performance, LLMs have difficulty with multi-step
movements involving high-degree-of-freedom body parts. Furthermore, LLMs
provide reasonable approximation for general spatial descriptions, but fail to
handle precise spatial specifications in text, and the precise spatial-temporal
parameters needed for avatar control. Notably, LLMs show promise in
conceptualizing creative motions and distinguishing culturally-specific motion
patterns.

</details>


### [7] [EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media](https://arxiv.org/abs/2505.21532)
*Ismail Erbas,Ferhat Demirkiran,Karthik Swaminathan,Naigang Wang,Navid Ibtehaj Nizam,Stefan T. Radev,Kaoutar El Maghraoui,Xavier Intes,Vikas Pandey*

Main category: cs.CV

TL;DR: 论文提出了一种物理引导的混合专家（MoE）框架，用于解决荧光LiDAR在散射介质中深度和荧光寿命估计的计算难题。通过结合物理模型和证据驱动的专家评估，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 荧光LiDAR在散射介质中面临信号复杂、难以分离光子飞行时间和荧光寿命的问题，现有方法效果有限。

Method: 采用物理引导的MoE框架，结合证据驱动的专家评估（EDCs）和决策网络，自适应融合专家预测。

Result: 在模拟的荧光LiDAR数据中，深度估计的NRMSE为0.030，荧光寿命的NRMSE为0.074。

Conclusion: 该方法显著提升了荧光LiDAR在复杂环境中的性能，为医学和非侵入性检测提供了新工具。

Abstract: Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology
employed for distance and depth estimation across medical, automotive, and
other fields, encounters significant computational challenges in scattering
media. The complex nature of the acquired FLiDAR signal, particularly in such
environments, makes isolating photon time-of-flight (related to target depth)
and intrinsic fluorescence lifetime exceptionally difficult, thus limiting the
effectiveness of current analytical and computational methodologies. To
overcome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE)
framework tailored for specialized modeling of diverse temporal components. In
contrast to the conventional MoE approaches our expert models are informed by
underlying physics, such as the radiative transport equation governing photon
propagation in scattering media. Central to our approach is EvidenceMoE, which
integrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess
the reliability of each expert's output by providing per-expert quality scores
and corrective feedback. A Decider Network then leverages this information to
fuse expert predictions into a robust final estimate adaptively. We validate
our method using realistically simulated Fluorescence LiDAR (FLiDAR) data for
non-invasive cancer cell depth detection generated from photon transport models
in tissue. Our framework demonstrates strong performance, achieving a
normalized root mean squared error (NRMSE) of 0.030 for depth estimation and
0.074 for fluorescence lifetime.

</details>


### [8] [Self-Organizing Visual Prototypes for Non-Parametric Representation Learning](https://arxiv.org/abs/2505.21533)
*Thalles Silva,Helio Pedrini,Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: 提出了一种名为SOP的无监督视觉特征学习新方法，通过多语义相似表示（SEs）增强原型特征，并引入非参数损失函数和SOP-MIM任务，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法依赖单一原型编码隐藏簇特征，限制了性能。SOP通过多SEs互补特征提升表征能力。

Method: 提出SOP策略，使用多SEs表征原型，引入非参数损失函数和SOP-MIM任务（基于多SEs重建掩码表示）。

Result: 在检索、线性评估、微调和目标检测等任务中表现优异，预训练编码器在多个检索基准上达到SOTA。

Conclusion: SOP通过多SEs和非参数方法显著提升了无监督视觉特征学习的性能，适用于复杂编码器。

Abstract: We present Self-Organizing Visual Prototypes (SOP), a new training technique
for unsupervised visual feature learning. Unlike existing prototypical
self-supervised learning (SSL) methods that rely on a single prototype to
encode all relevant features of a hidden cluster in the data, we propose the
SOP strategy. In this strategy, a prototype is represented by many semantically
similar representations, or support embeddings (SEs), each containing a
complementary set of features that together better characterize their region in
space and maximize training performance. We reaffirm the feasibility of
non-parametric SSL by introducing novel non-parametric adaptations of two loss
functions that implement the SOP strategy. Notably, we introduce the SOP Masked
Image Modeling (SOP-MIM) task, where masked representations are reconstructed
from the perspective of multiple non-parametric local SEs. We comprehensively
evaluate the representations learned using the SOP strategy on a range of
benchmarks, including retrieval, linear evaluation, fine-tuning, and object
detection. Our pre-trained encoders achieve state-of-the-art performance on
many retrieval benchmarks and demonstrate increasing performance gains with
more complex encoders.

</details>


### [9] [Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement](https://arxiv.org/abs/2505.21535)
*Yuxin Ren,Maxwell D Collins,Miao Hu,Huanrui Yang*

Main category: cs.CV

TL;DR: FAR框架用LSTM替换预训练Transformer中的注意力机制，提升推理效率，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: Transformer的注意力机制在推理时效率低，尤其是在资源受限的设备上。研究发现推理时的序列映射较简单，可用更廉价函数替代。

Method: 提出FAR框架，用LSTM替换所有注意力模块，通过块级蒸馏和全局结构剪枝优化LSTM架构。

Result: 在DeiT视觉Transformer上验证，FAR在ImageNet和下游任务中保持原模型精度，同时减少参数和延迟。

Conclusion: FAR成功保留了注意力模块的语义关系，为高效推理提供了可行方案。

Abstract: While transformers excel across vision and language pretraining tasks, their
reliance on attention mechanisms poses challenges for inference efficiency,
especially on edge and embedded accelerators with limited parallelism and
memory bandwidth. Hinted by the observed redundancy of attention at inference
time, we hypothesize that though the model learns complicated token dependency
through pretraining, the inference-time sequence-to-sequence mapping in each
attention layer is actually ''simple'' enough to be represented with a much
cheaper function. In this work, we explore FAR, a Function-preserving Attention
Replacement framework that replaces all attention blocks in pretrained
transformers with learnable sequence-to-sequence modules, exemplified by an
LSTM. FAR optimize a multi-head LSTM architecture with a block-wise
distillation objective and a global structural pruning framework to achieve a
family of efficient LSTM-based models from pretrained transformers. We validate
FAR on the DeiT vision transformer family and demonstrate that it matches the
accuracy of the original models on ImageNet and multiple downstream tasks with
reduced parameters and latency. Further analysis shows that FAR preserves the
semantic token relationships and the token-to-token correlation learned in the
transformer's attention module.

</details>


### [10] [Caption This, Reason That: VLMs Caught in the Middle](https://arxiv.org/abs/2505.21538)
*Zihan Weng,Lucas Gomez,Taylor Whittington Webb,Pouya Bashivan*

Main category: cs.CV

TL;DR: 论文分析了视觉语言模型（VLMs）在认知能力上的局限性，提出通过认知科学方法评估其表现，并发现改进方向。


<details>
  <summary>Details</summary>
Motivation: 研究VLMs在视觉任务（如计数或关系推理）中与人类能力的差距，探索其认知瓶颈。

Method: 采用认知科学方法，评估VLMs在感知、注意力和记忆等核心认知轴上的表现，并进行视觉-文本解耦分析。

Result: 发现VLMs在某些任务（如类别识别）表现接近上限，但在空间理解或选择性注意力任务上仍有显著差距。通过微调可提升核心认知能力。

Conclusion: 研究揭示了VLMs的认知瓶颈，并提出改进思路，如增强链式推理能力（CoT）和针对性微调。

Abstract: Vision-Language Models (VLMs) have shown remarkable progress in visual
understanding in recent years. Yet, they still lag behind human capabilities in
specific visual tasks such as counting or relational reasoning. To understand
the underlying limitations, we adopt methodologies from cognitive science,
analyzing VLM performance along core cognitive axes: Perception, Attention, and
Memory. Using a suite of tasks targeting these abilities, we evaluate
state-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct
cognitive profiles: while advanced models approach ceiling performance on some
tasks (e.g. category identification), a significant gap persists, particularly
in tasks requiring spatial understanding or selective attention. Investigating
the source of these failures and potential methods for improvement, we employ a
vision-text decoupling analysis, finding that models struggling with direct
visual reasoning show marked improvement when reasoning over their own
generated text captions. These experiments reveal a strong need for improved
VLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed
human performance. Furthermore, we demonstrate the potential of targeted
fine-tuning on composite visual reasoning tasks and show that fine-tuning
smaller VLMs substantially improves core cognitive abilities. While this
improvement does not translate to large enhancements on challenging,
out-of-distribution benchmarks, we show broadly that VLM performance on our
datasets strongly correlates with performance on these other benchmarks. Our
work provides a detailed analysis of VLM cognitive strengths and weaknesses and
identifies key bottlenecks in simultaneous perception and reasoning while also
providing an effective and simple solution.

</details>


### [11] [Equivariant Flow Matching for Point Cloud Assembly](https://arxiv.org/abs/2505.21539)
*Ziming Wang,Nan Xue,Rebecka Jörnsten*

Main category: cs.CV

TL;DR: 提出了一种基于流匹配模型的等变求解器Eda，用于点云组装任务，能够高效处理非重叠输入。


<details>
  <summary>Details</summary>
Motivation: 点云组装任务的目标是通过对齐多个点云片段重建完整的3D形状，现有方法在非重叠情况下表现不佳。

Method: 提出Eda模型，通过流匹配学习条件向量场，并构建等变路径以提高训练效率。

Result: 数值实验表明Eda在实际数据集中表现优异，能处理非重叠输入。

Conclusion: Eda是一种高效的点云组装方法，尤其在非重叠情况下具有优势。

Abstract: The goal of point cloud assembly is to reconstruct a complete 3D shape by
aligning multiple point cloud pieces. This work presents a novel equivariant
solver for assembly tasks based on flow matching models. We first theoretically
show that the key to learning equivariant distributions via flow matching is to
learn related vector fields. Based on this result, we propose an assembly
model, called equivariant diffusion assembly (Eda), which learns related vector
fields conditioned on the input pieces. We further construct an equivariant
path for Eda, which guarantees high data efficiency of the training process.
Our numerical results show that Eda is highly competitive on practical
datasets, and it can even handle the challenging situation where the input
pieces are non-overlapped.

</details>


### [12] [DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers](https://arxiv.org/abs/2505.21541)
*Zitong Wang,Hang Zhao,Qianyu Zhou,Xuequan Lu,Xiangtai Li,Yiren Song*

Main category: cs.CV

TL;DR: 论文提出了一种新的任务：Alpha合成图像的逐层分解，并开发了DiffDecompose框架和AlphaBlend数据集来解决透明/半透明层分解的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有图像分解方法在处理透明/半透明层时存在局限性，如依赖掩码先验、静态对象假设和数据缺乏。

Method: 提出了DiffDecompose框架，基于扩散Transformer，通过上下文分解和层位置编码克隆技术实现无监督层分解。

Result: 在AlphaBlend和LOGO数据集上的实验验证了DiffDecompose的有效性。

Conclusion: DiffDecompose和AlphaBlend数据集为透明/半透明层分解任务提供了新的解决方案。

Abstract: Diffusion models have recently motivated great success in many generation
tasks like object removal. Nevertheless, existing image decomposition methods
struggle to disentangle semi-transparent or transparent layer occlusions due to
mask prior dependencies, static object assumptions, and the lack of datasets.
In this paper, we delve into a novel task: Layer-Wise Decomposition of
Alpha-Composited Images, aiming to recover constituent layers from single
overlapped images under the condition of semi-transparent/transparent alpha
layer non-linear occlusion. To address challenges in layer ambiguity,
generalization, and data scarcity, we first introduce AlphaBlend, the first
large-scale and high-quality dataset for transparent and semi-transparent layer
decomposition, supporting six real-world subtasks (e.g., translucent flare
removal, semi-transparent cell decomposition, glassware decomposition).
Building on this dataset, we present DiffDecompose, a diffusion
Transformer-based framework that learns the posterior over possible layer
decompositions conditioned on the input image, semantic prompts, and blending
type. Rather than regressing alpha mattes directly, DiffDecompose performs
In-Context Decomposition, enabling the model to predict one or multiple layers
without per-layer supervision, and introduces Layer Position Encoding Cloning
to maintain pixel-level correspondence across layers. Extensive experiments on
the proposed AlphaBlend dataset and public LOGO dataset verify the
effectiveness of DiffDecompose. The code and dataset will be available upon
paper acceptance. Our code will be available at:
https://github.com/Wangzt1121/DiffDecompose.

</details>


### [13] [Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance](https://arxiv.org/abs/2505.21544)
*Semanto Mondal*

Main category: cs.CV

TL;DR: 论文提出了一种结合目标检测、大语言模型（LLM）和检索增强生成（RAG）的混合方法，用于咖啡叶病害检测与治理，旨在提高农业效率并减少环境影响。


<details>
  <summary>Details</summary>
Motivation: 传统农业资源利用效率低且对环境造成挑战，需通过精准农业技术优化农业过程。

Method: 结合YOLOv8进行病害检测，利用RAG提供上下文感知诊断，并通过NLP生成治理建议。

Result: 系统实现了实时病害检测和自适应治理方案，减少农药使用并支持环保方法。

Conclusion: 该框架具有可扩展性和用户友好性，未来可广泛应用于农业领域。

Abstract: As a social being, we have an intimate bond with the environment. A plethora
of things in human life, such as lifestyle, health, and food are dependent on
the environment and agriculture. It comes under our responsibility to support
the environment as well as agriculture. However, traditional farming practices
often result in inefficient resource use and environmental challenges. To
address these issues, precision agriculture has emerged as a promising approach
that leverages advanced technologies to optimise agricultural processes. In
this work, a hybrid approach is proposed that combines the three different
potential fields of model AI: object detection, large language model (LLM), and
Retrieval-Augmented Generation (RAG). In this novel framework, we have tried to
combine the vision and language models to work together to identify potential
diseases in the tree leaf. This study introduces a novel AI-based precision
agriculture system that uses Retrieval Augmented Generation (RAG) to provide
context-aware diagnoses and natural language processing (NLP) and YOLOv8 for
crop disease detection. The system aims to tackle major issues with large
language models (LLMs), especially hallucinations and allows for adaptive
treatment plans and real-time disease detection. The system provides an
easy-to-use interface to the farmers, which they can use to detect the
different diseases related to coffee leaves by just submitting the image of the
affected leaf the model will detect the diseases as well as suggest potential
remediation methodologies which aim to lower the use of pesticides, preserving
livelihoods, and encouraging environmentally friendly methods. With an emphasis
on scalability, dependability, and user-friendliness, the project intends to
improve RAG-integrated object detection systems for wider agricultural
applications in the future.

</details>


### [14] [Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation](https://arxiv.org/abs/2505.21545)
*Chika Maduabuchi,Hao Chen,Yujin Han,Jindong Wang*

Main category: cs.CV

TL;DR: CAT-LVDM是一种针对LVDMs的鲁棒性训练框架，通过数据对齐的噪声注入提升模型性能，包括BCNI和SACN两种方法。


<details>
  <summary>Details</summary>
Motivation: 解决LVDMs在噪声视频-文本数据集上因不完美条件导致的语义漂移和时间不一致问题。

Method: 提出BCNI（批内语义方向噪声注入）和SACN（频谱感知噪声注入）两种噪声注入方法。

Result: BCNI在WebVid-2M等数据集上平均降低FVD 31.9%，SACN在UCF-101上提升12.3%。

Conclusion: CAT-LVDM为多模态噪声下的鲁棒视频扩散提供了一种可扩展的训练方法。

Abstract: Latent Video Diffusion Models (LVDMs) achieve high-quality generation but are
sensitive to imperfect conditioning, which causes semantic drift and temporal
incoherence on noisy, web-scale video-text datasets. We introduce CAT-LVDM, the
first corruption-aware training framework for LVDMs that improves robustness
through structured, data-aligned noise injection. Our method includes
Batch-Centered Noise Injection (BCNI), which perturbs embeddings along
intra-batch semantic directions to preserve temporal consistency. BCNI is
especially effective on caption-rich datasets like WebVid-2M, MSR-VTT, and
MSVD. We also propose Spectrum-Aware Contextual Noise (SACN), which injects
noise along dominant spectral directions to improve low-frequency smoothness,
showing strong results on UCF-101. On average, BCNI reduces FVD by 31.9% across
WebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101.
Ablation studies confirm the benefit of low-rank, data-aligned noise. Our
theoretical analysis further explains how such perturbations tighten entropy,
Wasserstein, score-drift, mixing-time, and generalization bounds. CAT-LVDM
establishes a principled, scalable training approach for robust video diffusion
under multimodal noise. Code and models: https://github.com/chikap421/catlvdm

</details>


### [15] [Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing](https://arxiv.org/abs/2505.21547)
*Weixing Wang,Zifeng Ding,Jindong Gu,Rui Cao,Christoph Meinel,Gerard de Melo,Haojin Yang*

Main category: cs.CV

TL;DR: 论文研究了大型视觉语言模型（LVLMs）中幻觉非存在对象的问题，并提出了一种基于图像令牌共现图的缓解方法。


<details>
  <summary>Details</summary>
Motivation: 发现LVLMs因训练中的视觉先验导致幻觉，即模型可能因令牌共现关联而生成不存在的对象。

Method: 构建图像令牌共现图，使用GNN和对比学习聚类高频共现令牌，提出通过修改潜在图像嵌入来抑制幻觉。

Result: 实验表明该方法能有效减少幻觉，同时保持模型的表达能力。

Conclusion: 通过分析令牌共现关系，提出了一种缓解LVLMs幻觉的有效方法。

Abstract: Large Vision-Language Models (LVLMs) with discrete image tokenizers unify
multimodal representations by encoding visual inputs into a finite set of
tokens. Despite their effectiveness, we find that these models still
hallucinate non-existent objects. We hypothesize that this may be due to visual
priors induced during training: When certain image tokens frequently co-occur
in the same spatial regions and represent shared objects, they become strongly
associated with the verbalizations of those objects. As a result, the model may
hallucinate by evoking visually absent tokens that often co-occur with present
ones. To test this assumption, we construct a co-occurrence graph of image
tokens using a segmentation dataset and employ a Graph Neural Network (GNN)
with contrastive learning followed by a clustering method to group tokens that
frequently co-occur in similar visual contexts. We find that hallucinations
predominantly correspond to clusters whose tokens dominate the input, and more
specifically, that the visually absent tokens in those clusters show much
higher correlation with hallucinated objects compared to tokens present in the
image. Based on this observation, we propose a hallucination mitigation method
that suppresses the influence of visually absent tokens by modifying latent
image embeddings during generation. Experiments show our method reduces
hallucinations while preserving expressivity. Code is available at
https://github.com/weixingW/CGC-VTD/tree/main

</details>


### [16] [Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation](https://arxiv.org/abs/2505.21549)
*Daniel Csizmadia,Andrei Codreanu,Victor Sim,Vighnesh Prabeau,Michael Lu,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.CV

TL;DR: DCLIP通过元教师-学生蒸馏框架增强CLIP模型的多模态检索能力，同时保留零样本分类性能。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在图像-文本检索任务中受限于固定分辨率和有限上下文，DCLIP旨在解决这些问题。

Method: 使用双向跨模态注意力机制和混合损失函数训练轻量级学生模型。

Result: 在少量数据上训练后，DCLIP显著提升检索指标，同时保留94%的零样本分类性能。

Conclusion: DCLIP在任务专业化和泛化之间取得平衡，为视觉-语言任务提供高效解决方案。

Abstract: We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that
enhances multimodal image-text retrieval while preserving the original model's
strong zero-shot classification capabilities. CLIP models are typically
constrained by fixed image resolutions and limited context, which can hinder
their effectiveness in retrieval tasks that require fine-grained cross-modal
understanding. DCLIP addresses these challenges through a meta teacher-student
distillation framework, where a cross-modal transformer teacher is fine-tuned
to produce enriched embeddings via bidirectional cross-attention between
YOLO-extracted image regions and corresponding textual spans. These
semantically and spatially aligned global representations guide the training of
a lightweight student model using a hybrid loss that combines contrastive
learning and cosine similarity objectives. Despite being trained on only
~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a
fraction of CLIP's original dataset-DCLIP significantly improves image-text
retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's
zero-shot classification performance. These results demonstrate that DCLIP
effectively mitigates the trade-off between task specialization and
generalization, offering a resource-efficient, domain-adaptive, and
detail-sensitive solution for advanced vision-language tasks. Code available at
https://anonymous.4open.science/r/DCLIP-B772/README.md.

</details>


### [17] [Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts](https://arxiv.org/abs/2505.21556)
*Hee-Seon Kim,Minbeom Kim,Wonjun Lee,Kihyun Kim,Changick Kim*

Main category: cs.CV

TL;DR: 论文提出了一种新的Benign-to-Toxic (B2T) jailbreak方法，优于传统的Toxic-Continuation范式，能在无显性毒性信号的情况下诱导模型生成有害输出。


<details>
  <summary>Details</summary>
Motivation: 传统Toxic-Continuation方法在缺乏显性毒性信号时效果有限，因此需要一种新方法以更隐蔽的方式突破模型的安全机制。

Method: 通过优化对抗性图像，使其在良性条件下诱导模型生成有害输出，从而绕过安全机制。

Result: B2T方法优于现有方法，适用于黑盒场景，并能与基于文本的jailbreak方法互补。

Conclusion: 该研究揭示了多模态对齐中的潜在漏洞，并为jailbreak方法提供了新方向。

Abstract: Optimization-based jailbreaks typically adopt the Toxic-Continuation setting
in large vision-language models (LVLMs), following the standard next-token
prediction objective. In this setting, an adversarial image is optimized to
make the model predict the next token of a toxic prompt. However, we find that
the Toxic-Continuation paradigm is effective at continuing already-toxic
inputs, but struggles to induce safety misalignment when explicit toxic signals
are absent. We propose a new paradigm: Benign-to-Toxic (B2T) jailbreak. Unlike
prior work, we optimize adversarial images to induce toxic outputs from benign
conditioning. Since benign conditioning contains no safety violations, the
image alone must break the model's safety mechanisms. Our method outperforms
prior approaches, transfers in black-box settings, and complements text-based
jailbreaks. These results reveal an underexplored vulnerability in multimodal
alignment and introduce a fundamentally new direction for jailbreak approaches.

</details>


### [18] [Analytical Calculation of Weights Convolutional Neural Network](https://arxiv.org/abs/2505.21557)
*Polad Geidarov*

Main category: cs.CV

TL;DR: 提出一种无需标准训练过程的CNN权重和阈值分析方法，仅需10张MNIST图像即可确定参数，实验表明其能快速识别手写数字。


<details>
  <summary>Details</summary>
Motivation: 探索无需训练即可直接构建和应用CNN的方法，简化分类任务流程。

Method: 通过解析计算确定CNN权重、阈值和通道数，使用C++ Builder实现模块，并在MNIST数据集上实验。

Result: 未经训练的CNN能识别半数以上1000张手写数字图像，推理速度极快。

Conclusion: CNN可通过纯解析计算构建，无需训练即可用于分类任务。

Abstract: This paper presents an algorithm for analytically calculating the weights and
thresholds of convolutional neural networks (CNNs) without using standard
training procedures. The algorithm enables the determination of CNN parameters
based on just 10 selected images from the MNIST dataset, each representing a
digit from 0 to 9. As part of the method, the number of channels in CNN layers
is also derived analytically. A software module was implemented in C++ Builder,
and a series of experiments were conducted using the MNIST dataset. Results
demonstrate that the analytically computed CNN can recognize over half of 1000
handwritten digit images without any training, achieving inference in fractions
of a second. These findings suggest that CNNs can be constructed and applied
directly for classification tasks without training, using purely analytical
computation of weights.

</details>


### [19] [A Novel Convolutional Neural Network-Based Framework for Complex Multiclass Brassica Seed Classification](https://arxiv.org/abs/2505.21558)
*Elhoucine Elfatimia,Recep Eryigitb,Lahcen Elfatimi*

Main category: cs.CV

TL;DR: 提出了一种基于CNN的新框架，用于高效分类十种常见的Brassica种子类型，解决了种子图像纹理相似性的挑战，准确率达93%。


<details>
  <summary>Details</summary>
Motivation: 农民缺乏时间和资源进行农场研究，种子分类对质量控制、生产效率和杂质检测至关重要。早期识别种子类型可降低成本和风险。

Method: 采用自定义设计的CNN架构，针对种子图像纹理相似性问题优化，并与几种预训练的最先进架构进行性能对比。

Result: 在收集的Brassica种子数据集上，模型准确率达到93%。

Conclusion: 该CNN框架为种子分类提供了高效解决方案，有助于提升种子质量管理和产量估算。

Abstract: Agricultural research has accelerated in recent years, yet farmers often lack
the time and resources for on-farm research due to the demands of crop
production and farm operations. Seed classification offers valuable insights
into quality control, production efficiency, and impurity detection. Early
identification of seed types is critical to reducing the cost and risk
associated with field emergence, which can lead to yield losses or disruptions
in downstream processes like harvesting. Seed sampling supports growers in
monitoring and managing seed quality, improving precision in determining seed
purity levels, guiding management adjustments, and enhancing yield estimations.
This study proposes a novel convolutional neural network (CNN)-based framework
for the efficient classification of ten common Brassica seed types. The
approach addresses the inherent challenge of texture similarity in seed images
using a custom-designed CNN architecture. The model's performance was evaluated
against several pre-trained state-of-the-art architectures, with adjustments to
layer configurations for optimized classification. Experimental results using
our collected Brassica seed dataset demonstrate that the proposed model
achieved a high accuracy rate of 93 percent.

</details>


### [20] [Knowledge Distillation Approach for SOS Fusion Staging: Towards Fully Automated Skeletal Maturity Assessment](https://arxiv.org/abs/2505.21561)
*Omid Halimi Milani,Amanda Nikho,Marouane Tliba,Lauren Mills,Ahmet Enis Cetin,Mohammed H Elnagar*

Main category: cs.CV

TL;DR: 提出了一种用于自动评估蝶枕软骨结合（SOS）融合的深度学习框架，采用师生模型架构和新型损失函数，提升诊断效率和一致性。


<details>
  <summary>Details</summary>
Motivation: 蝶枕软骨结合（SOS）融合是正畸和法医人类学的重要诊断标志，现有方法依赖外部裁剪或分割工具，效率低且不统一。

Method: 采用师生模型架构，教师模型基于裁剪图像训练，通过新型损失函数（空间逻辑对齐和梯度注意力映射）指导学生模型在全图像上学习。

Result: 框架实现了稳健的诊断准确性，形成临床可行的端到端流程，无需额外预处理工具。

Conclusion: 该方法提升了骨骼成熟评估的效率和一致性，适用于多样化临床场景。

Abstract: We introduce a novel deep learning framework for the automated staging of
spheno-occipital synchondrosis (SOS) fusion, a critical diagnostic marker in
both orthodontics and forensic anthropology. Our approach leverages a
dual-model architecture wherein a teacher model, trained on manually cropped
images, transfers its precise spatial understanding to a student model that
operates on full, uncropped images. This knowledge distillation is facilitated
by a newly formulated loss function that aligns spatial logits as well as
incorporates gradient-based attention spatial mapping, ensuring that the
student model internalizes the anatomically relevant features without relying
on external cropping or YOLO-based segmentation. By leveraging expert-curated
data and feedback at each step, our framework attains robust diagnostic
accuracy, culminating in a clinically viable end-to-end pipeline. This
streamlined approach obviates the need for additional pre-processing tools and
accelerates deployment, thereby enhancing both the efficiency and consistency
of skeletal maturation assessment in diverse clinical settings.

</details>


### [21] [Multi-instance Learning as Downstream Task of Self-Supervised Learning-based Pre-trained Model](https://arxiv.org/abs/2505.21564)
*Koki Matsuishi,Tsuyoshi Okita*

Main category: cs.CV

TL;DR: 本文提出了一种使用自监督预训练模型改进多实例学习的方法，解决了脑血肿CT中实例数量增加导致的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 在脑血肿CT中，当每个包中的实例数量增加到256时，传统的多实例学习方法难以有效学习。

Method: 采用自监督预训练模型作为多实例学习器的下游任务。

Result: 在脑血肿CT的低密度标记分类任务中，准确率提高了5%至13%，F1分数提高了40%至55%。

Conclusion: 自监督预训练模型能显著提升多实例学习在实例数量较多时的性能。

Abstract: In deep multi-instance learning, the number of applicable instances depends
on the data set. In histopathology images, deep learning multi-instance
learners usually assume there are hundreds to thousands instances in a bag.
However, when the number of instances in a bag increases to 256 in brain
hematoma CT, learning becomes extremely difficult. In this paper, we address
this drawback. To overcome this problem, we propose using a pre-trained model
with self-supervised learning for the multi-instance learner as a downstream
task. With this method, even when the original target task suffers from the
spurious correlation problem, we show improvements of 5% to 13% in accuracy and
40% to 55% in the F1 measure for the hypodensity marker classification of brain
hematoma CT.

</details>


### [22] [Diffusion Model-based Activity Completion for AI Motion Capture from Videos](https://arxiv.org/abs/2505.21566)
*Gao Huayu,Huang Tengjiu,Ye Xiaolong,Tsuyoshi Okita*

Main category: cs.CV

TL;DR: AI运动捕捉技术通过扩散模型生成补充动作序列，解决了传统方法依赖预定义动作的局限性，并在Human3.6M数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前AI运动捕捉技术依赖预定义视频序列，限制了动作的灵活性，无法处理未观察到的动作。

Method: 提出基于扩散模型的动作补全技术，结合门模块和位置-时间嵌入模块，生成平滑连续的动作序列。

Result: MDC-Net在Human3.6M数据集上表现优于现有方法，模型更小（16.84M），生成的动作更自然连贯。

Conclusion: 该方法为虚拟人类提供了更灵活的动作生成能力，并展示了在传感器数据提取上的潜力。

Abstract: AI-based motion capture is an emerging technology that offers a
cost-effective alternative to traditional motion capture systems. However,
current AI motion capture methods rely entirely on observed video sequences,
similar to conventional motion capture. This means that all human actions must
be predefined, and movements outside the observed sequences are not possible.
To address this limitation, we aim to apply AI motion capture to virtual
humans, where flexible actions beyond the observed sequences are required. We
assume that while many action fragments exist in the training data, the
transitions between them may be missing. To bridge these gaps, we propose a
diffusion-model-based action completion technique that generates complementary
human motion sequences, ensuring smooth and continuous movements. By
introducing a gate module and a position-time embedding module, our approach
achieves competitive results on the Human3.6M dataset. Our experimental results
show that (1) MDC-Net outperforms existing methods in ADE, FDE, and MMADE but
is slightly less accurate in MMFDE, (2) MDC-Net has a smaller model size
(16.84M) compared to HumanMAC (28.40M), and (3) MDC-Net generates more natural
and coherent motion sequences. Additionally, we propose a method for extracting
sensor data, including acceleration and angular velocity, from human motion
sequences.

</details>


### [23] [EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models](https://arxiv.org/abs/2505.21567)
*Feng Jiang,Zihao Zheng,Xiuping Cui,Maoliang Li,JIayu Chen,Xiang Chen*

Main category: cs.CV

TL;DR: 提出了一种名为EaqVLA的优化框架，通过编码对齐量化解决VLA模型的量化问题，显著降低了计算和存储成本。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型的计算和存储成本高昂，且现有量化方法因token对齐问题无法直接应用。

Method: 提出了一种分析方法来识别多粒度对齐问题，并基于此设计了编码对齐感知的混合精度量化方法。

Result: EaqVLA在量化性能上优于现有方法，实现了最小量化损失和显著的计算加速。

Conclusion: EaqVLA为VLA模型提供了一种高效的量化解决方案，具有实际应用价值。

Abstract: With the development of Embodied Artificial intelligence, the end-to-end
control policy such as Vision-Language-Action (VLA) model has become the
mainstream. Existing VLA models faces expensive computing/storage cost, which
need to be optimized. Quantization is considered as the most effective method
which can not only reduce the memory cost but also achieve computation
acceleration. However, we find the token alignment of VLA models hinders the
application of existing quantization methods. To address this, we proposed an
optimized framework called EaqVLA, which apply encoding-aligned quantization to
VLA models. Specifically, we propose an complete analysis method to find the
misalignment in various granularity. Based on the analysis results, we propose
a mixed precision quantization with the awareness of encoding alignment.
Experiments shows that the porposed EaqVLA achieves better quantization
performance (with the minimal quantization loss for end-to-end action control
and xxx times acceleration) than existing quantization methods.

</details>


### [24] [Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks](https://arxiv.org/abs/2505.21572)
*Sungwon Kim,Namkyeong Lee,Yunyoung Doh,Seungmin Shin,Guimok Cho,Seung-Won Jeon,Sangkook Kim,Chanyoung Park*

Main category: cs.CV

TL;DR: 提出了一种厚度感知的3D网格神经网络（T-EMNN），解决了现有方法忽略物体厚度的问题，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有3D网格分析方法主要关注表面拓扑和几何，忽略了物体厚度及其对行为的影响。

Method: 提出T-EMNN框架，结合厚度信息并保持E(3)-等变性，引入数据驱动的坐标编码空间信息。

Result: 在工业数据集上验证，T-EMNN能准确预测3D变形，有效捕捉厚度效应且计算高效。

Conclusion: T-EMNN为3D静态分析提供了更准确且高效的解决方案。

Abstract: Mesh-based 3D static analysis methods have recently emerged as efficient
alternatives to traditional computational numerical solvers, significantly
reducing computational costs and runtime for various physics-based analyses.
However, these methods primarily focus on surface topology and geometry, often
overlooking the inherent thickness of real-world 3D objects, which exhibits
high correlations and similar behavior between opposing surfaces. This
limitation arises from the disconnected nature of these surfaces and the
absence of internal edge connections within the mesh. In this work, we propose
a novel framework, the Thickness-aware E(3)-Equivariant 3D Mesh Neural Network
(T-EMNN), that effectively integrates the thickness of 3D objects while
maintaining the computational efficiency of surface meshes. Additionally, we
introduce data-driven coordinates that encode spatial information while
preserving E(3)-equivariance or invariance properties, ensuring consistent and
robust analysis. Evaluations on a real-world industrial dataset demonstrate the
superior performance of T-EMNN in accurately predicting node-level 3D
deformations, effectively capturing thickness effects while maintaining
computational efficiency.

</details>


### [25] [Do We Need All the Synthetic Data? Towards Targeted Synthetic Image Augmentation via Diffusion Models](https://arxiv.org/abs/2505.21574)
*Dang Nguyen,Jiping Li,Jinghao Zheng,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: 通过仅增强训练数据中未被早期学习部分，提升图像分类器泛化能力，优于全数据增强方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型增强方法在数据多样性和规模扩展上存在不足，需改进以提升性能。

Method: 分析双层CNN，证明部分数据增强策略能促进特征学习速度均匀化，减少噪声放大。

Result: 仅增强30%-40%数据，性能提升达2.8%，在多种模型和数据集上表现优异，甚至超越SOTA优化器。

Conclusion: 部分数据增强策略高效且兼容性强，可与其他增强方法结合进一步提升性能。

Abstract: Synthetically augmenting training datasets with diffusion models has been an
effective strategy for improving generalization of image classifiers. However,
existing techniques struggle to ensure the diversity of generation and increase
the size of the data by up to 10-30x to improve the in-distribution
performance. In this work, we show that synthetically augmenting part of the
data that is not learned early in training outperforms augmenting the entire
dataset. By analyzing a two-layer CNN, we prove that this strategy improves
generalization by promoting homogeneity in feature learning speed without
amplifying noise. Our extensive experiments show that by augmenting only
30%-40% of the data, our method boosts the performance by up to 2.8% in a
variety of scenarios, including training ResNet, ViT and DenseNet on CIFAR-10,
CIFAR-100, and TinyImageNet, with a range of optimizers including SGD and SAM.
Notably, our method applied with SGD outperforms the SOTA optimizer, SAM, on
CIFAR-100 and TinyImageNet. It can also easily stack with existing weak and
strong augmentation strategies to further boost the performance.

</details>


### [26] [Do you see what I see? An Ambiguous Optical Illusion Dataset exposing limitations of Explainable AI](https://arxiv.org/abs/2505.21589)
*Carina Newen,Luca Hinkamp,Maria Ntonti,Emmanuel Müller*

Main category: cs.CV

TL;DR: 论文介绍了一个新颖的光学幻觉数据集，旨在研究机器学习和人类感知中的模糊性问题，并探讨了视觉概念对模型准确性的影响。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域（如自动驾驶和医疗诊断）中，模糊数据的重要性日益凸显，而光学幻觉数据集稀缺，因此需要构建此类数据集以研究机器和人类感知的局限性。

Method: 通过系统生成包含交织动物对的光学幻觉数据集，重点关注视线方向和眼睛线索等视觉概念，并分析这些特征对模型的影响。

Result: 研究发现，视觉概念（如视线方向）对模型准确性有显著影响，为研究人类与机器视觉的偏差和对齐提供了基础。

Conclusion: 该数据集为研究视觉学习中的概念重要性及偏差缓解提供了资源，代码和数据集已公开。

Abstract: From uncertainty quantification to real-world object detection, we recognize
the importance of machine learning algorithms, particularly in safety-critical
domains such as autonomous driving or medical diagnostics. In machine learning,
ambiguous data plays an important role in various machine learning domains.
Optical illusions present a compelling area of study in this context, as they
offer insight into the limitations of both human and machine perception.
Despite this relevance, optical illusion datasets remain scarce. In this work,
we introduce a novel dataset of optical illusions featuring intermingled animal
pairs designed to evoke perceptual ambiguity. We identify generalizable visual
concepts, particularly gaze direction and eye cues, as subtle yet impactful
features that significantly influence model accuracy. By confronting models
with perceptual ambiguity, our findings underscore the importance of concepts
in visual learning and provide a foundation for studying bias and alignment
between human and machine vision. To make this dataset useful for general
purposes, we generate optical illusions systematically with different concepts
discussed in our bias mitigation section. The dataset is accessible in Kaggle
via
https://kaggle.com/datasets/693bf7c6dd2cb45c8a863f9177350c8f9849a9508e9d50526e2ffcc5559a8333.
Our source code can be found at
https://github.com/KDD-OpenSource/Ambivision.git.

</details>


### [27] [Any-to-Bokeh: One-Step Video Bokeh via Multi-Plane Image Guided Diffusion](https://arxiv.org/abs/2505.21593)
*Yang Yang,Siming Zheng,Jinwei Chen,Boxi Wu,Xiaofei He,Deng Cai,Bo Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 提出了一种新颖的一步视频虚化框架，通过多平面图像表示和预训练模型实现高质量、可控的虚化效果。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑模型无法明确控制焦点平面或调整虚化强度，且图像虚化方法扩展到视频时会出现时间闪烁和边缘模糊问题。

Method: 利用多平面图像表示和逐步扩大的深度采样函数，结合单步视频扩散模型和预训练模型（如Stable Video Diffusion），实现深度感知的虚化效果。

Result: 实验表明，该方法能生成高质量、可控的虚化效果，并在多个评估基准上达到最先进性能。

Conclusion: 提出的框架解决了视频虚化的时间一致性和深度感知问题，为可控光学效果提供了有效解决方案。

Abstract: Recent advances in diffusion based editing models have enabled realistic
camera simulation and image-based bokeh, but video bokeh remains largely
unexplored. Existing video editing models cannot explicitly control focus
planes or adjust bokeh intensity, limiting their applicability for controllable
optical effects. Moreover, naively extending image-based bokeh methods to video
often results in temporal flickering and unsatisfactory edge blur transitions
due to the lack of temporal modeling and generalization capability. To address
these challenges, we propose a novel one-step video bokeh framework that
converts arbitrary input videos into temporally coherent, depth-aware bokeh
effects. Our method leverages a multi-plane image (MPI) representation
constructed through a progressively widening depth sampling function, providing
explicit geometric guidance for depth-dependent blur synthesis. By conditioning
a single-step video diffusion model on MPI layers and utilizing the strong 3D
priors from pre-trained models such as Stable Video Diffusion, our approach
achieves realistic and consistent bokeh effects across diverse scenes.
Additionally, we introduce a progressive training strategy to enhance temporal
consistency, depth robustness, and detail preservation. Extensive experiments
demonstrate that our method produces high-quality, controllable bokeh effects
and achieves state-of-the-art performance on multiple evaluation benchmarks.

</details>


### [28] [Object Concepts Emerge from Motion](https://arxiv.org/abs/2505.21635)
*Haoqian Liang,Xiaohui Wang,Zhichao Li,Ya Yang,Naiyan Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于生物启发的无监督学习框架，利用运动边界信号生成伪实例监督，训练视觉编码器，并在多个视觉任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 受婴儿通过观察运动获取物体理解的启发，研究旨在开发一种无监督学习框架，从原始视频中学习物体中心视觉表示。

Method: 利用现成的光流和聚类算法生成基于运动的实例掩码，通过对比学习训练视觉编码器。

Result: 在单目深度估计、3D物体检测和占用预测任务中，模型表现优于现有监督和自监督基线，并展现出强泛化能力。

Conclusion: 运动诱导的物体表示是现有视觉基础模型的有力替代，捕捉了视觉实例这一关键但被忽视的抽象层次。

Abstract: Object concepts play a foundational role in human visual cognition, enabling
perception, memory, and interaction in the physical world. Inspired by findings
in developmental neuroscience - where infants are shown to acquire object
understanding through observation of motion - we propose a biologically
inspired framework for learning object-centric visual representations in an
unsupervised manner. Our key insight is that motion boundary serves as a strong
signal for object-level grouping, which can be used to derive pseudo instance
supervision from raw videos. Concretely, we generate motion-based instance
masks using off-the-shelf optical flow and clustering algorithms, and use them
to train visual encoders via contrastive learning. Our framework is fully
label-free and does not rely on camera calibration, making it scalable to
large-scale unstructured video data. We evaluate our approach on three
downstream tasks spanning both low-level (monocular depth estimation) and
high-level (3D object detection and occupancy prediction) vision. Our models
outperform previous supervised and self-supervised baselines and demonstrate
strong generalization to unseen scenes. These results suggest that
motion-induced object representations offer a compelling alternative to
existing vision foundation models, capturing a crucial but overlooked level of
abstraction: the visual instance. The corresponding code will be released upon
paper acceptance.

</details>


### [29] [BaryIR: Learning Multi-Source Unified Representation in Continuous Barycenter Space for Generalizable All-in-One Image Restoration](https://arxiv.org/abs/2505.21637)
*Xiaole Tang,Xiaoyi He,Xiang Gu,Jian Sun*

Main category: cs.CV

TL;DR: BaryIR提出了一种多源表示学习框架，通过分解多源退化图像的潜在空间为连续重心空间和源特定子空间，提升了全功能图像修复（AIR）的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有全功能图像修复方法对分布外退化和图像适应性不足，限制了实际应用。

Method: 引入多源潜在最优传输重心问题，学习连续重心映射，将潜在表示传输到重心空间，同时保持源特定子空间的正交性。

Result: BaryIR在实验中表现优异，尤其在真实数据和未见退化上展现出更强的泛化能力。

Conclusion: BaryIR通过统一和特定语义编码，显著提升了AIR的泛化性和实用性。

Abstract: Despite remarkable advances made in all-in-one image restoration (AIR) for
handling different types of degradations simultaneously, existing methods
remain vulnerable to out-of-distribution degradations and images, limiting
their real-world applicability. In this paper, we propose a multi-source
representation learning framework BaryIR, which decomposes the latent space of
multi-source degraded images into a continuous barycenter space for unified
feature encoding and source-specific subspaces for specific semantic encoding.
Specifically, we seek the multi-source unified representation by introducing a
multi-source latent optimal transport barycenter problem, in which a continuous
barycenter map is learned to transport the latent representations to the
barycenter space. The transport cost is designed such that the representations
from source-specific subspaces are contrasted with each other while maintaining
orthogonality to those from the barycenter space. This enables BaryIR to learn
compact representations with unified degradation-agnostic information from the
barycenter space, as well as degradation-specific semantics from
source-specific subspaces, capturing the inherent geometry of multi-source data
manifold for generalizable AIR. Extensive experiments demonstrate that BaryIR
achieves competitive performance compared to state-of-the-art all-in-one
methods. Particularly, BaryIR exhibits superior generalization ability to
real-world data and unseen degradations. The code will be publicly available at
https://github.com/xl-tang3/BaryIR.

</details>


### [30] [Learnable Burst-Encodable Time-of-Flight Imaging for High-Fidelity Long-Distance Depth Sensing](https://arxiv.org/abs/2505.22025)
*Manchao Bao,Shengjiang Fang,Tao Yue,Xuemei Hu*

Main category: cs.CV

TL;DR: 提出了一种新型的BE-ToF成像范式，通过突发模式发射光脉冲和端到端学习框架，解决了传统iToF系统的相位缠绕和低信噪比问题，实现了高保真、长距离的深度成像。


<details>
  <summary>Details</summary>
Motivation: 长距离深度成像在自动驾驶和机器人等领域有重要应用，但现有的dToF和iToF技术分别存在硬件要求高和相位缠绕、低信噪比的问题。

Method: 提出BE-ToF系统，采用突发模式发射光脉冲并估计整个突发周期的相位延迟；设计端到端学习框架，联合优化编码函数和深度重建网络，并引入双阱函数和一阶差分项以确保硬件可实现性。

Result: 通过仿真和原型实验验证了BE-ToF的有效性和实用性，解决了相位缠绕和低信噪比问题。

Conclusion: BE-ToF为长距离深度成像提供了一种高效且实用的解决方案，适用于实际应用场景。

Abstract: Long-distance depth imaging holds great promise for applications such as
autonomous driving and robotics. Direct time-of-flight (dToF) imaging offers
high-precision, long-distance depth sensing, yet demands ultra-short pulse
light sources and high-resolution time-to-digital converters. In contrast,
indirect time-of-flight (iToF) imaging often suffers from phase wrapping and
low signal-to-noise ratio (SNR) as the sensing distance increases. In this
paper, we introduce a novel ToF imaging paradigm, termed Burst-Encodable
Time-of-Flight (BE-ToF), which facilitates high-fidelity, long-distance depth
imaging. Specifically, the BE-ToF system emits light pulses in burst mode and
estimates the phase delay of the reflected signal over the entire burst period,
thereby effectively avoiding the phase wrapping inherent to conventional iToF
systems. Moreover, to address the low SNR caused by light attenuation over
increasing distances, we propose an end-to-end learnable framework that jointly
optimizes the coding functions and the depth reconstruction network. A
specialized double well function and first-order difference term are
incorporated into the framework to ensure the hardware implementability of the
coding functions. The proposed approach is rigorously validated through
comprehensive simulations and real-world prototype experiments, demonstrating
its effectiveness and practical applicability.

</details>


### [31] [Geometric Feature Prompting of Image Segmentation Models](https://arxiv.org/abs/2505.21644)
*Kenneth Ball,Erin Taylor,Nirav Patel,Andrew Bartels,Gary Koplik,James Polly,Jay Hineman*

Main category: cs.CV

TL;DR: 论文提出了一种几何驱动的提示生成器（GeomPrompt），用于在科学图像分析任务中自动生成敏感且特异的提示点，从而优化SAM模型在植物根系分割中的应用。


<details>
  <summary>Details</summary>
Motivation: 植物根系在根管图像中的分割任务传统上难以自动化，手工标注耗时且主观。利用SAM模型结合几何提示生成器，可以显著提高分割效率和准确性。

Method: 使用几何驱动的提示生成器（GeomPrompt）自动生成与感兴趣特征共定位的提示点，结合SAM模型进行分割。

Result: GeomPrompt生成的提示点能够显著减少所需提示数量，同时提高分割的敏感性和特异性，适用于植物根系分割任务。

Conclusion: GeomPrompt与SAM的结合为科学图像分析提供了一种高效、自动化的解决方案，特别是在植物根系分割领域具有重要应用价值。

Abstract: Advances in machine learning, especially the introduction of transformer
architectures and vision transformers, have led to the development of highly
capable computer vision foundation models. The segment anything model (known
colloquially as SAM and more recently SAM 2), is a highly capable foundation
model for segmentation of natural images and has been further applied to
medical and scientific image segmentation tasks. SAM relies on prompts --
points or regions of interest in an image -- to generate associated
segmentations.
  In this manuscript we propose the use of a geometrically motivated prompt
generator to produce prompt points that are colocated with particular features
of interest. Focused prompting enables the automatic generation of sensitive
and specific segmentations in a scientific image analysis task using SAM with
relatively few point prompts. The image analysis task examined is the
segmentation of plant roots in rhizotron or minirhizotron images, which has
historically been a difficult task to automate. Hand annotation of rhizotron
images is laborious and often subjective; SAM, initialized with GeomPrompt
local ridge prompts has the potential to dramatically improve rhizotron image
processing.
  The authors have concurrently released an open source software suite called
geomprompt https://pypi.org/project/geomprompt/ that can produce point prompts
in a format that enables direct integration with the segment-anything package.

</details>


### [32] [QuARI: Query Adaptive Retrieval Improvement](https://arxiv.org/abs/2505.21647)
*Eric Xing,Abby Stylianou,Robert Pless,Nathan Jacobs*

Main category: cs.CV

TL;DR: 通过学习查询特定的特征空间变换，提升大规模图像检索性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在挑战性检索任务（如大规模图像检索）中表现不佳的问题。

Method: 学习将查询映射到查询特定的线性特征空间变换，以低成本应用于大规模图像嵌入。

Result: 该方法在性能上优于现有最先进方法，且计算成本更低。

Conclusion: 查询特定的特征空间变换是一种高效且有效的大规模图像检索解决方案。

Abstract: Massive-scale pretraining has made vision-language models increasingly
popular for image-to-image and text-to-image retrieval across a broad
collection of domains. However, these models do not perform well when used for
challenging retrieval tasks, such as instance retrieval in very large-scale
image collections. Recent work has shown that linear transformations of VLM
features trained for instance retrieval can improve performance by emphasizing
subspaces that relate to the domain of interest. In this paper, we explore a
more extreme version of this specialization by learning to map a given query to
a query-specific feature space transformation. Because this transformation is
linear, it can be applied with minimal computational cost to millions of image
embeddings, making it effective for large-scale retrieval or re-ranking.
Results show that this method consistently outperforms state-of-the-art
alternatives, including those that require many orders of magnitude more
computation at query time.

</details>


### [33] [Visual Loop Closure Detection Through Deep Graph Consensus](https://arxiv.org/abs/2505.21754)
*Martin Büchner,Liza Dahiya,Simon Dorer,Vipul Ramtekkar,Kenji Nishimiya,Daniele Cattaneo,Abhinav Valada*

Main category: cs.CV

TL;DR: LoopGNN是一种图神经网络架构，通过利用视觉相似关键帧的团来估计闭环共识，提高了闭环检测的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统闭环检测依赖计算昂贵的几何验证，而现有深度学习方法仅处理关键帧对，限制了性能。

Method: 提出LoopGNN，通过图神经网络传播深度特征编码，利用多关键帧邻域检测闭环。

Result: 在TartanDrive 2.0和NCLT数据集上表现优于传统基线，计算效率更高。

Conclusion: LoopGNN在保持高召回率的同时提高了精度，且对深度特征编码类型鲁棒。

Abstract: Visual loop closure detection traditionally relies on place recognition
methods to retrieve candidate loops that are validated using computationally
expensive RANSAC-based geometric verification. As false positive loop closures
significantly degrade downstream pose graph estimates, verifying a large number
of candidates in online simultaneous localization and mapping scenarios is
constrained by limited time and compute resources. While most deep loop closure
detection approaches only operate on pairs of keyframes, we relax this
constraint by considering neighborhoods of multiple keyframes when detecting
loops. In this work, we introduce LoopGNN, a graph neural network architecture
that estimates loop closure consensus by leveraging cliques of visually similar
keyframes retrieved through place recognition. By propagating deep feature
encodings among nodes of the clique, our method yields high-precision estimates
while maintaining high recall. Extensive experimental evaluations on the
TartanDrive 2.0 and NCLT datasets demonstrate that LoopGNN outperforms
traditional baselines. Additionally, an ablation study across various keypoint
extractors demonstrates that our method is robust, regardless of the type of
deep feature encodings used, and exhibits higher computational efficiency
compared to classical geometric verification baselines. We release our code,
supplementary material, and keyframe data at
https://loopgnn.cs.uni-freiburg.de.

</details>


### [34] [PS4PRO: Pixel-to-pixel Supervision for Photorealistic Rendering and Optimization](https://arxiv.org/abs/2505.22616)
*Yezhi Shen,Qiuchen Zhai,Fengqing Zhu*

Main category: cs.CV

TL;DR: 提出了一种基于视频帧插值的数据增强方法PS4PRO，用于提升神经渲染的3D重建质量。


<details>
  <summary>Details</summary>
Motivation: 神经渲染方法在复杂动态场景中因输入视角有限导致重建质量受限。

Method: 设计轻量级高质量视频帧插值模型PS4PRO，通过丰富数据增强提升重建效果。

Result: 实验表明，该方法在静态和动态场景中均提升了重建性能。

Conclusion: PS4PRO作为隐式世界先验，有效增强了神经渲染的数据监督能力。

Abstract: Neural rendering methods have gained significant attention for their ability
to reconstruct 3D scenes from 2D images. The core idea is to take multiple
views as input and optimize the reconstructed scene by minimizing the
uncertainty in geometry and appearance across the views. However, the
reconstruction quality is limited by the number of input views. This limitation
is further pronounced in complex and dynamic scenes, where certain angles of
objects are never seen. In this paper, we propose to use video frame
interpolation as the data augmentation method for neural rendering.
Furthermore, we design a lightweight yet high-quality video frame interpolation
model, PS4PRO (Pixel-to-pixel Supervision for Photorealistic Rendering and
Optimization). PS4PRO is trained on diverse video datasets, implicitly modeling
camera movement as well as real-world 3D geometry. Our model performs as an
implicit world prior, enriching the photo supervision for 3D reconstruction. By
leveraging the proposed method, we effectively augment existing datasets for
neural rendering methods. Our experimental results indicate that our method
improves the reconstruction performance on both static and dynamic scenes.

</details>


### [35] [Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks](https://arxiv.org/abs/2505.21649)
*Keanu Nichols,Nazia Tasnim,Yan Yuting,Nicholas Ikechukwu,Elva Zou,Deepti Ghadiyaram,Bryan Plummer*

Main category: cs.CV

TL;DR: DORI是一个专注于物体方向理解的基准测试，揭示了当前多模态模型在方向感知上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言基准测试未能单独评估物体方向理解能力，DORI旨在填补这一空白。

Method: DORI通过11个数据集的67个对象类别，评估四个方向理解维度。

Result: 最佳模型在粗粒度任务中准确率为54.2%，细粒度任务中为33.0%，表现不佳。

Conclusion: DORI揭示了模型在3D空间表示上的不足，为改进机器人控制和3D场景重建提供了方向。

Abstract: Object orientation understanding represents a fundamental challenge in visual
perception critical for applications like robotic manipulation and augmented
reality. Current vision-language benchmarks fail to isolate this capability,
often conflating it with positional relationships and general scene
understanding. We introduce DORI (Discriminative Orientation Reasoning
Intelligence), a comprehensive benchmark establishing object orientation
perception as a primary evaluation target. DORI assesses four dimensions of
orientation comprehension: frontal alignment, rotational transformations,
relative directional relationships, and canonical orientation understanding.
Through carefully curated tasks from 11 datasets spanning 67 object categories
across synthetic and real-world scenarios, DORI provides insights on how
multi-modal systems understand object orientations. Our evaluation of 15
state-of-the-art vision-language models reveals critical limitations: even the
best models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular
orientation judgments, with performance deteriorating for tasks requiring
reference frame shifts or compound rotations. These findings demonstrate the
need for dedicated orientation representation mechanisms, as models show
systematic inability to perform precise angular estimations, track orientation
changes across viewpoints, and understand compound rotations - suggesting
limitations in their internal 3D spatial representations. As the first
diagnostic framework specifically designed for orientation awareness in
multimodal systems, DORI offers implications for improving robotic control, 3D
scene reconstruction, and human-AI interaction in physical environments. DORI
data: https://huggingface.co/datasets/appledora/DORI-Benchmark

</details>


### [36] [Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation](https://arxiv.org/abs/2505.21653)
*Ke Zhang,Cihan Xiao,Yiqun Mei,Jiacong Xu,Vishal M. Patel*

Main category: cs.CV

TL;DR: DiffPhy是一个通过微调预训练视频扩散模型来生成物理正确且逼真视频的框架，利用大语言模型（LLM）和多模态大语言模型（MLLM）指导生成过程。


<details>
  <summary>Details</summary>
Motivation: 当前视频扩散模型在生成视觉上吸引人的结果方面表现优异，但在合成正确的物理效果方面仍面临挑战。

Method: DiffPhy通过LLM从文本提示中推理物理上下文，并利用MLLM作为监督信号，引入新的训练目标以确保物理正确性和语义一致性。

Result: 在公共基准测试中，DiffPhy在多样化的物理相关场景中取得了最先进的结果。

Conclusion: DiffPhy为生成物理正确且逼真的视频提供了一种有效方法，并建立了高质量物理视频数据集以支持研究。

Abstract: Recent video diffusion models have demonstrated their great capability in
generating visually-pleasing results, while synthesizing the correct physical
effects in generated videos remains challenging. The complexity of real-world
motions, interactions, and dynamics introduce great difficulties when learning
physics from data. In this work, we propose DiffPhy, a generic framework that
enables physically-correct and photo-realistic video generation by fine-tuning
a pre-trained video diffusion model. Our method leverages large language models
(LLMs) to explicitly reason a comprehensive physical context from the text
prompt and use it to guide the generation. To incorporate physical context into
the diffusion model, we leverage a Multimodal large language model (MLLM) as a
supervisory signal and introduce a set of novel training objectives that
jointly enforce physical correctness and semantic consistency with the input
text. We also establish a high-quality physical video dataset containing
diverse phyiscal actions and events to facilitate effective finetuning.
Extensive experiments on public benchmarks demonstrate that DiffPhy is able to
produce state-of-the-art results across diverse physics-related scenarios. Our
project page is available at https://bwgzk-keke.github.io/DiffPhy/

</details>


### [37] [From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving Autonomous Driving](https://arxiv.org/abs/2505.22067)
*Xinyu Xia,Xingjun Ma,Yunfeng Hu,Ting Qu,Hong Chen,Xun Gong*

Main category: cs.CV

TL;DR: SERA是一个基于LLM的框架，通过针对性场景推荐修复自动驾驶系统的失败案例，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有场景生成和选择方法缺乏适应性和语义相关性，限制了性能提升。

Method: SERA分析性能日志，识别失败模式，动态检索语义对齐场景，并通过LLM优化推荐，用于少样本微调。

Result: 实验表明，SERA在安全关键条件下显著提升多个自动驾驶基线的关键指标。

Conclusion: SERA通过自适应场景推荐和少样本学习，有效提升自动驾驶系统的鲁棒性和泛化能力。

Abstract: Ensuring robust and generalizable autonomous driving requires not only broad
scenario coverage but also efficient repair of failure cases, particularly
those related to challenging and safety-critical scenarios. However, existing
scenario generation and selection methods often lack adaptivity and semantic
relevance, limiting their impact on performance improvement. In this paper, we
propose \textbf{SERA}, an LLM-powered framework that enables autonomous driving
systems to self-evolve by repairing failure cases through targeted scenario
recommendation. By analyzing performance logs, SERA identifies failure patterns
and dynamically retrieves semantically aligned scenarios from a structured
bank. An LLM-based reflection mechanism further refines these recommendations
to maximize relevance and diversity. The selected scenarios are used for
few-shot fine-tuning, enabling targeted adaptation with minimal data.
Experiments on the benchmark show that SERA consistently improves key metrics
across multiple autonomous driving baselines, demonstrating its effectiveness
and generalizability under safety-critical conditions.

</details>


### [38] [Scalable Segmentation for Ultra-High-Resolution Brain MR Images](https://arxiv.org/abs/2505.21697)
*Xiaoling Hu,Peirong Liu,Dina Zemlyanker,Jonathan Williams Ramirez,Oula Puonti,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 提出了一种利用低分辨率粗标签作为空间参考的新框架，通过回归有符号距离变换图实现高效、边界感知的3D脑MRI分割。


<details>
  <summary>Details</summary>
Motivation: 超高分辨率脑图像分割面临标注数据不足和计算需求高的挑战。

Method: 采用回归有符号距离变换图的方法，结合可扩展的类条件分割策略，逐类分割。

Result: 在合成和真实数据集上验证了方法的优越性能和可扩展性。

Conclusion: 该方法在减少内存消耗的同时，能够泛化到未见过的解剖类别。

Abstract: Although deep learning has shown great success in 3D brain MRI segmentation,
achieving accurate and efficient segmentation of ultra-high-resolution brain
images remains challenging due to the lack of labeled training data for
fine-scale anatomical structures and high computational demands. In this work,
we propose a novel framework that leverages easily accessible, low-resolution
coarse labels as spatial references and guidance, without incurring additional
annotation cost. Instead of directly predicting discrete segmentation maps, our
approach regresses per-class signed distance transform maps, enabling smooth,
boundary-aware supervision. Furthermore, to enhance scalability,
generalizability, and efficiency, we introduce a scalable class-conditional
segmentation strategy, where the model learns to segment one class at a time
conditioned on a class-specific input. This novel design not only reduces
memory consumption during both training and testing, but also allows the model
to generalize to unseen anatomical classes. We validate our method through
comprehensive experiments on both synthetic and real-world datasets,
demonstrating its superior performance and scalability compared to conventional
segmentation approaches.

</details>


### [39] [MedBridge: Bridging Foundation Vision-Language Models to Medical Image Diagnosis](https://arxiv.org/abs/2505.21698)
*Yitong Li,Morteza Ghahremani,Christian Wachinger*

Main category: cs.CV

TL;DR: MedBridge是一个轻量级多模态适应框架，通过重新利用预训练的视觉语言模型（VLM）提升医学图像诊断的准确性，无需大量资源。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言基础模型在自然图像分类上表现优异，但在医学图像上因领域差异表现不佳，而训练医学基础模型又需要大量资源。

Method: MedBridge包含三个关键组件：Focal Sampling模块提取高分辨率局部区域；Query Encoder通过可学习查询对齐医学语义；Mixture of Experts机制利用多种VLM的互补优势。

Result: 在五个医学影像基准测试中，MedBridge在跨领域和领域内适应任务中表现优异，AUC提升6-15%。

Conclusion: MedBridge有效利用基础模型，实现了数据高效的医学诊断，性能显著优于现有方法。

Abstract: Recent vision-language foundation models deliver state-of-the-art results on
natural image classification but falter on medical images due to pronounced
domain shifts. At the same time, training a medical foundation model requires
substantial resources, including extensive annotated data and high
computational capacity. To bridge this gap with minimal overhead, we introduce
MedBridge, a lightweight multimodal adaptation framework that re-purposes
pretrained VLMs for accurate medical image diagnosis. MedBridge comprises three
key components. First, a Focal Sampling module that extracts high-resolution
local regions to capture subtle pathological features and compensate for the
limited input resolution of general-purpose VLMs. Second, a Query Encoder
(QEncoder) injects a small set of learnable queries that attend to the frozen
feature maps of VLM, aligning them with medical semantics without retraining
the entire backbone. Third, a Mixture of Experts mechanism, driven by learnable
queries, harnesses the complementary strength of diverse VLMs to maximize
diagnostic performance. We evaluate MedBridge on five medical imaging
benchmarks across three key adaptation tasks, demonstrating its superior
performance in both cross-domain and in-domain adaptation settings, even under
varying levels of training data availability. Notably, MedBridge achieved over
6-15% improvement in AUC compared to state-of-the-art VLM adaptation methods in
multi-label thoracic disease diagnosis, underscoring its effectiveness in
leveraging foundation models for accurate and data-efficient medical diagnosis.
Our code is available at https://github.com/ai-med/MedBridge.

</details>


### [40] [Task-Driven Implicit Representations for Automated Design of LiDAR Systems](https://arxiv.org/abs/2505.22344)
*Nikhil Behari,Aaron Young,Akshat Dave,Ramesh Raskar*

Main category: cs.CV

TL;DR: 提出了一种基于任务驱动的自动化LiDAR系统设计框架，通过流生成模型学习设计空间中的隐式密度，并利用期望最大化方法合成满足约束的LiDAR系统。


<details>
  <summary>Details</summary>
Motivation: LiDAR设计复杂且耗时，传统方法依赖人工，难以满足多样化的空间和时间采样需求。

Method: 在六维设计空间中表示LiDAR配置，通过流生成模型学习任务特定的隐式密度，利用期望最大化方法拟合参数化分布。

Result: 在3D视觉任务中验证了方法的有效性，实现了人脸扫描、机器人跟踪和物体检测等应用的自动化设计。

Conclusion: 该框架为复杂约束下的LiDAR系统设计提供了高效、自动化的解决方案。

Abstract: Imaging system design is a complex, time-consuming, and largely manual
process; LiDAR design, ubiquitous in mobile devices, autonomous vehicles, and
aerial imaging platforms, adds further complexity through unique spatial and
temporal sampling requirements. In this work, we propose a framework for
automated, task-driven LiDAR system design under arbitrary constraints. To
achieve this, we represent LiDAR configurations in a continuous six-dimensional
design space and learn task-specific implicit densities in this space via
flow-based generative modeling. We then synthesize new LiDAR systems by
modeling sensors as parametric distributions in 6D space and fitting these
distributions to our learned implicit density using expectation-maximization,
enabling efficient, constraint-aware LiDAR system design. We validate our
method on diverse tasks in 3D vision, enabling automated LiDAR system design
across real-world-inspired applications in face scanning, robotic tracking, and
object detection.

</details>


### [41] [OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions](https://arxiv.org/abs/2505.21724)
*Cheng Luo,Jianghui Wang,Bing Li,Siyang Song,Bernard Ghanem*

Main category: cs.CV

TL;DR: 论文提出OMCRG任务，通过多模态输入生成同步的听者反馈，并介绍OmniResponse模型和ResponseNet数据集，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 研究自然对话中听者的多模态反馈同步问题，提出OMCRG任务以解决音频与面部反应的同步挑战。

Method: 利用文本作为中间模态，提出OmniResponse模型（基于MLLM），包含Chrono-Text和TempoVoice模块，生成高质量多模态反馈。

Result: 在ResponseNet数据集上，OmniResponse在语义内容、视听同步和生成质量上显著优于基线模型。

Conclusion: OmniResponse和ResponseNet为OMCRG研究提供了有效工具，未来可进一步优化多模态同步生成。

Abstract: In this paper, we introduce Online Multimodal Conversational Response
Generation (OMCRG), a novel task that aims to online generate synchronized
verbal and non-verbal listener feedback, conditioned on the speaker's
multimodal input. OMCRG reflects natural dyadic interactions and poses new
challenges in achieving synchronization between the generated audio and facial
responses of the listener. To address these challenges, we innovatively
introduce text as an intermediate modality to bridge the audio and facial
responses. We hence propose OmniResponse, a Multimodal Large Language Model
(MLLM) that autoregressively generates high-quality multi-modal listener
responses. OmniResponse leverages a pretrained LLM enhanced with two novel
components: Chrono-Text, which temporally anchors generated text tokens, and
TempoVoice, a controllable online TTS module that produces speech synchronized
with facial reactions. To support further OMCRG research, we present
ResponseNet, a new dataset comprising 696 high-quality dyadic interactions
featuring synchronized split-screen videos, multichannel audio, transcripts,
and facial behavior annotations. Comprehensive evaluations conducted on
ResponseNet demonstrate that OmniResponse significantly outperforms baseline
models in terms of semantic speech content, audio-visual synchronization, and
generation quality.

</details>


### [42] [Moment kernels: a simple and scalable approach for equivariance to rotations and reflections in deep convolutional networks](https://arxiv.org/abs/2505.21736)
*Zachary Schlamowitz,Andrew Bennecke,Daniel J. Tward*

Main category: cs.CV

TL;DR: 论文提出了一种名为“矩核”的简单卷积核形式，用于实现旋转和反射等对称性的等变性，解决了传统方法因数学复杂性而难以广泛应用的问题。


<details>
  <summary>Details</summary>
Motivation: 旋转和反射等对称性在生物医学图像分析中至关重要，但传统方法依赖复杂的数学理论（如表示理论），限制了其广泛应用。

Method: 通过设计一种称为“矩核”的简单卷积核形式，证明所有等变核必须采用这种形式，并基于标准卷积模块实现等变神经网络。

Result: 提出的方法成功应用于多个生物医学图像分析任务，包括分类、3D图像配准和细胞分割。

Conclusion: 矩核提供了一种简单且通用的方式来实现对称性等变性，为生物医学图像分析提供了新的工具。

Abstract: The principle of translation equivariance (if an input image is translated an
output image should be translated by the same amount), led to the development
of convolutional neural networks that revolutionized machine vision. Other
symmetries, like rotations and reflections, play a similarly critical role,
especially in biomedical image analysis, but exploiting these symmetries has
not seen wide adoption. We hypothesize that this is partially due to the
mathematical complexity of methods used to exploit these symmetries, which
often rely on representation theory, a bespoke concept in differential geometry
and group theory. In this work, we show that the same equivariance can be
achieved using a simple form of convolution kernels that we call ``moment
kernels,'' and prove that all equivariant kernels must take this form. These
are a set of radially symmetric functions of a spatial position $x$, multiplied
by powers of the components of $x$ or the identity matrix. We implement
equivariant neural networks using standard convolution modules, and provide
architectures to execute several biomedical image analysis tasks that depend on
equivariance principles: classification (outputs are invariant under orthogonal
transforms), 3D image registration (outputs transform like a vector), and cell
segmentation (quadratic forms defining ellipses transform like a matrix).

</details>


### [43] [GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control](https://arxiv.org/abs/2505.22421)
*Anthony Chen,Wenzhao Zheng,Yida Wang,Xueyang Zhang,Kun Zhan,Peng Jia,Kurt Keutzer,Shangbang Zhang*

Main category: cs.CV

TL;DR: GeoDrive通过将3D几何条件显式整合到驾驶世界模型中，提升了空间理解和动作可控性，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前方法在保持3D几何一致性或处理遮挡时存在缺陷，影响自动驾驶的安全评估可靠性。

Method: 从输入帧提取3D表示，基于用户指定轨迹生成2D渲染，并通过动态编辑模块增强渲染效果。

Result: 在动作准确性和3D空间感知上显著优于现有模型，支持新轨迹泛化和交互式场景编辑。

Conclusion: GeoDrive为自动驾驶提供了更真实、适应性强且可靠的场景建模，提升了安全性。

Abstract: Recent advancements in world models have revolutionized dynamic environment
simulation, allowing systems to foresee future states and assess potential
actions. In autonomous driving, these capabilities help vehicles anticipate the
behavior of other road users, perform risk-aware planning, accelerate training
in simulation, and adapt to novel scenarios, thereby enhancing safety and
reliability. Current approaches exhibit deficiencies in maintaining robust 3D
geometric consistency or accumulating artifacts during occlusion handling, both
critical for reliable safety assessment in autonomous navigation tasks. To
address this, we introduce GeoDrive, which explicitly integrates robust 3D
geometry conditions into driving world models to enhance spatial understanding
and action controllability. Specifically, we first extract a 3D representation
from the input frame and then obtain its 2D rendering based on the
user-specified ego-car trajectory. To enable dynamic modeling, we propose a
dynamic editing module during training to enhance the renderings by editing the
positions of the vehicles. Extensive experiments demonstrate that our method
significantly outperforms existing models in both action accuracy and 3D
spatial awareness, leading to more realistic, adaptable, and reliable scene
modeling for safer autonomous driving. Additionally, our model can generalize
to novel trajectories and offers interactive scene editing capabilities, such
as object editing and object trajectory control.

</details>


### [44] [What is Adversarial Training for Diffusion Models?](https://arxiv.org/abs/2505.21742)
*Briglia Maria Rosaria,Mujtaba Hussain Mirza,Giuseppe Lisanti,Iacopo Masi*

Main category: cs.CV

TL;DR: 对抗训练（AT）在扩散模型（DMs）中与分类器中不同，要求等变性以保持扩散过程与数据分布一致，从而提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索对抗训练在扩散模型中的独特作用，区别于分类器中的输出不变性，以提升模型对噪声和异常数据的处理能力。

Method: 在扩散训练中无缝集成随机噪声或对抗噪声，无需假设噪声模型，类似随机平滑或对抗训练。

Result: 在低维和高维数据集及标准基准（如CIFAR-10、CelebA和LSUN Bedroom）上表现优异，尤其在噪声、数据损坏和对抗攻击下。

Conclusion: 对抗训练在扩散模型中通过等变性提升鲁棒性，适用于噪声数据、异常值和对抗攻击场景。

Abstract: We answer the question in the title, showing that adversarial training (AT)
for diffusion models (DMs) fundamentally differs from classifiers: while AT in
classifiers enforces output invariance, AT in DMs requires equivariance to keep
the diffusion process aligned with the data distribution. AT is a way to
enforce smoothness in the diffusion flow, improving robustness to outliers and
corrupted data. Unlike prior art, our method makes no assumptions about the
noise model and integrates seamlessly into diffusion training by adding random
noise, similar to randomized smoothing, or adversarial noise, akin to AT. This
enables intrinsic capabilities such as handling noisy data, dealing with
extreme variability such as outliers, preventing memorization, and improving
robustness. We rigorously evaluate our approach with proof-of-concept datasets
with known distributions in low- and high-dimensional space, thereby taking a
perfect measure of errors; we further evaluate on standard benchmarks such as
CIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe
noise, data corruption, and iterative adversarial attacks.

</details>


### [45] [Zero-Shot 3D Visual Grounding from Vision-Language Models](https://arxiv.org/abs/2505.22429)
*Rong Li,Shijie Li,Lingdong Kong,Xulei Yang,Junwei Liang*

Main category: cs.CV

TL;DR: SeeGround是一个零样本3D视觉定位框架，利用2D视觉语言模型无需3D训练即可定位3D场景中的目标对象，性能显著优于现有零样本基线。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位方法依赖标注数据和预定义类别，难以扩展到开放世界场景。SeeGround旨在通过2D视觉语言模型实现零样本3D定位。

Method: 提出混合输入格式，结合查询对齐的渲染视图和空间增强文本描述；包含视角适应模块和融合对齐模块以优化定位。

Result: 在ScanRefer和Nr3D数据集上，SeeGround分别超越零样本基线7.7%和7.1%，性能接近全监督方法。

Conclusion: SeeGround展示了在零样本条件下强大的泛化能力，为3D视觉定位提供了高效且可扩展的解决方案。

Abstract: 3D Visual Grounding (3DVG) seeks to locate target objects in 3D scenes using
natural language descriptions, enabling downstream applications such as
augmented reality and robotics. Existing approaches typically rely on labeled
3D data and predefined categories, limiting scalability to open-world settings.
We present SeeGround, a zero-shot 3DVG framework that leverages 2D
Vision-Language Models (VLMs) to bypass the need for 3D-specific training. To
bridge the modality gap, we introduce a hybrid input format that pairs
query-aligned rendered views with spatially enriched textual descriptions. Our
framework incorporates two core components: a Perspective Adaptation Module
that dynamically selects optimal viewpoints based on the query, and a Fusion
Alignment Module that integrates visual and spatial signals to enhance
localization precision. Extensive evaluations on ScanRefer and Nr3D confirm
that SeeGround achieves substantial improvements over existing zero-shot
baselines -- outperforming them by 7.7% and 7.1%, respectively -- and even
rivals fully supervised alternatives, demonstrating strong generalization under
challenging conditions.

</details>


### [46] [Learning to See More: UAS-Guided Super-Resolution of Satellite Imagery for Precision Agriculture](https://arxiv.org/abs/2505.21746)
*Arif Masrur,Peder A. Olsen,Paul R. Adler,Carlan Jackson,Matthew W. Myers,Nathan Sedghi,Ray R. Weil*

Main category: cs.CV

TL;DR: 提出了一种融合卫星和无人机（UAS）影像的超分辨率框架，用于精准农业，显著提高了生物量和氮的估算精度。


<details>
  <summary>Details</summary>
Motivation: 卫星和无人机在精准农业中各具优缺点，卫星覆盖广但分辨率低，无人机分辨率高但成本高且覆盖有限。

Method: 通过超分辨率方法融合卫星和UAS影像，利用光谱扩展和空间扩展模型提升数据质量。

Result: 生物量和氮的估算精度分别提高了18%和31%，且减少了无人机数据采集的需求。

Conclusion: 该框架轻量且可扩展，为农场提供了经济高效的解决方案，适用于其他作物系统。

Abstract: Unmanned Aircraft Systems (UAS) and satellites are key data sources for
precision agriculture, yet each presents trade-offs. Satellite data offer broad
spatial, temporal, and spectral coverage but lack the resolution needed for
many precision farming applications, while UAS provide high spatial detail but
are limited by coverage and cost, especially for hyperspectral data. This study
presents a novel framework that fuses satellite and UAS imagery using
super-resolution methods. By integrating data across spatial, spectral, and
temporal domains, we leverage the strengths of both platforms cost-effectively.
We use estimation of cover crop biomass and nitrogen (N) as a case study to
evaluate our approach. By spectrally extending UAS RGB data to the vegetation
red edge and near-infrared regions, we generate high-resolution Sentinel-2
imagery and improve biomass and N estimation accuracy by 18% and 31%,
respectively. Our results show that UAS data need only be collected from a
subset of fields and time points. Farmers can then 1) enhance the spectral
detail of UAS RGB imagery; 2) increase the spatial resolution by using
satellite data; and 3) extend these enhancements spatially and across the
growing season at the frequency of the satellite flights. Our SRCNN-based
spectral extension model shows considerable promise for model transferability
over other cropping systems in the Upper and Lower Chesapeake Bay regions.
Additionally, it remains effective even when cloud-free satellite data are
unavailable, relying solely on the UAS RGB input. The spatial extension model
produces better biomass and N predictions than models built on raw UAS RGB
images. Once trained with targeted UAS RGB data, the spatial extension model
allows farmers to stop repeated UAS flights. While we introduce
super-resolution advances, the core contribution is a lightweight and scalable
system for affordable on-farm use.

</details>


### [47] [FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering](https://arxiv.org/abs/2505.21755)
*Chengyue Huang,Brisa Maneechotesuwan,Shivang Chopra,Zsolt Kira*

Main category: cs.CV

TL;DR: 提出了一个名为FRAMES-VQA的新基准，用于评估视觉问答（VQA）任务中的鲁棒微调方法，涵盖多模态分布偏移。


<details>
  <summary>Details</summary>
Motivation: 当前VQA系统在适应真实世界数据偏移时面临挑战，尤其是多模态场景下，现有评估方法多为单模态或特定OOD类型，难以全面反映多模态复杂性。

Method: 利用十个现有VQA基准（如VQAv2、IV-VQA等），将其分类为ID、近OOD和远OOD数据集，并通过计算Mahalanobis距离量化分布偏移。

Result: 通过分析单模态与多模态偏移的交互以及模态重要性，为开发更鲁棒的微调方法提供了指导。

Conclusion: FRAMES-VQA为多模态分布偏移下的VQA任务提供了新的评估基准和分析工具，有助于提升鲁棒性。

Abstract: Visual question answering (VQA) systems face significant challenges when
adapting to real-world data shifts, especially in multi-modal contexts. While
robust fine-tuning strategies are essential for maintaining performance across
in-distribution (ID) and out-of-distribution (OOD) scenarios, current
evaluation settings are primarily unimodal or particular to some types of OOD,
offering limited insight into the complexities of multi-modal contexts. In this
work, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across
Multi-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We
utilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA
and others, and categorize them into ID, near and far OOD datasets covering
uni-modal, multi-modal and adversarial distribution shifts. We first conduct a
comprehensive comparison of existing robust fine-tuning methods. We then
quantify the distribution shifts by calculating the Mahalanobis distance using
uni-modal and multi-modal embeddings extracted from various models. Further, we
perform an extensive analysis to explore the interactions between uni- and
multi-modal shifts as well as modality importance for ID and OOD samples. These
analyses offer valuable guidance on developing more robust fine-tuning methods
to handle multi-modal distribution shifts. The code is available at
https://github.com/chengyuehuang511/FRAMES-VQA .

</details>


### [48] [MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning](https://arxiv.org/abs/2505.21771)
*Prasham Yatinkumar Titiya,Jainil Trivedi,Chitta Baral,Vivek Gupta*

Main category: cs.CV

TL;DR: MMTBENCH是一个针对多模态表格（结合半结构化数据和视觉元素）的基准测试，包含500个真实表格和4021个问题对。评估显示现有模型在视觉推理和多步推理上表现不佳，需改进架构。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）在处理复杂多模态表格时表现不足，缺乏相关基准测试。MMTBENCH旨在填补这一空白。

Method: 构建包含500个真实多模态表格和4021个问题对的基准测试（MMTBENCH），覆盖多种问题类型、推理类型和表格类型。

Result: 现有模型在视觉推理和多步推理任务上表现显著不足。

Conclusion: MMTBENCH为未来研究提供了高质量资源，凸显了改进多模态表格处理的紧迫性。

Abstract: Multimodal tables those that integrate semi structured data with visual
elements such as charts and maps are ubiquitous across real world domains, yet
they pose a formidable challenge to current vision language models (VLMs).
While Large Language models (LLMs) and VLMs have demonstrated strong
capabilities in text and image understanding, their performance on complex,
real world multimodal table reasoning remains unexplored. To bridge this gap,
we introduce MMTBENCH (Multimodal Table Benchmark), a benchmark consisting of
500 real world multimodal tables drawn from diverse real world sources, with a
total of 4021 question answer pairs. MMTBENCH questions cover four question
types (Explicit, Implicit, Answer Mention, and Visual Based), five reasoning
types (Mathematical, Extrema Identification, Fact Verification, Vision Based,
and Others), and eight table types (Single/Multiple Entity, Maps and Charts
with Entities, Single/Multiple Charts, Maps, and Visualizations). Extensive
evaluation of state of the art models on all types reveals substantial
performance gaps, particularly on questions requiring visual-based reasoning
and multi-step inference. These findings show the urgent need for improved
architectures that more tightly integrate vision and language processing. By
providing a challenging, high-quality resource that mirrors the complexity of
real-world tasks, MMTBENCH underscores its value as a resource for future
research on multimodal tables.

</details>


### [49] [Compositional Scene Understanding through Inverse Generative Modeling](https://arxiv.org/abs/2505.21780)
*Yanbo Wang,Justin Dauwels,Yilun Du*

Main category: cs.CV

TL;DR: 论文探讨了生成模型如何用于场景理解，通过逆向生成建模推断场景结构，并展示了其在新场景中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究生成模型不仅能生成视觉内容，还能用于理解自然图像中的场景属性。

Method: 将场景理解建模为逆向生成问题，通过组合式视觉生成模型推断场景结构。

Result: 方法能够推断场景中的对象和全局因素，并在新场景中表现出鲁棒性。

Conclusion: 该方法可直接应用于预训练的文本到图像生成模型，实现零样本多对象感知。

Abstract: Generative models have demonstrated remarkable abilities in generating
high-fidelity visual content. In this work, we explore how generative models
can further be used not only to synthesize visual content but also to
understand the properties of a scene given a natural image. We formulate scene
understanding as an inverse generative modeling problem, where we seek to find
conditional parameters of a visual generative model to best fit a given natural
image. To enable this procedure to infer scene structure from images
substantially different than those seen during training, we further propose to
build this visual generative model compositionally from smaller models over
pieces of a scene. We illustrate how this procedure enables us to infer the set
of objects in a scene, enabling robust generalization to new test scenes with
an increased number of objects of new shapes. We further illustrate how this
enables us to infer global scene factors, likewise enabling robust
generalization to new scenes. Finally, we illustrate how this approach can be
directly applied to existing pretrained text-to-image generative models for
zero-shot multi-object perception. Code and visualizations are at
\href{https://energy-based-model.github.io/compositional-inference}{https://energy-based-model.github.io/compositional-inference}.

</details>


### [50] [SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation](https://arxiv.org/abs/2505.21795)
*Claudia Cuttano,Gabriele Trivigno,Giuseppe Averta,Carlo Masone*

Main category: cs.CV

TL;DR: SANSA改进了SAM2，使其适用于少样本分割任务，通过显式利用其潜在语义结构，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: SAM2虽然具备强大的分割能力，但其特征与任务特定线索纠缠，限制了其在需要高级语义理解的任务中的应用。

Method: 提出SANSA框架，通过最小任务特定修改，显式利用SAM2的潜在语义结构。

Result: 在少样本分割基准测试中表现优异，支持多种交互方式，且速度更快、模型更紧凑。

Conclusion: SANSA成功将SAM2重新用于少样本分割，展现了其潜在语义结构的有效性。

Abstract: Few-shot segmentation aims to segment unseen object categories from just a
handful of annotated examples. This requires mechanisms that can both identify
semantically related objects across images and accurately produce segmentation
masks. We note that Segment Anything 2 (SAM2), with its prompt-and-propagate
mechanism, offers both strong segmentation capabilities and a built-in feature
matching process. However, we show that its representations are entangled with
task-specific cues optimized for object tracking, which impairs its use for
tasks requiring higher level semantic understanding. Our key insight is that,
despite its class-agnostic pretraining, SAM2 already encodes rich semantic
structure in its features. We propose SANSA (Semantically AligNed Segment
Anything 2), a framework that makes this latent structure explicit, and
repurposes SAM2 for few-shot segmentation through minimal task-specific
modifications. SANSA achieves state-of-the-art performance on few-shot
segmentation benchmarks specifically designed to assess generalization,
outperforms generalist methods in the popular in-context setting, supports
various prompts flexible interaction via points, boxes, or scribbles, and
remains significantly faster and more compact than prior approaches. Code is
available at https://github.com/ClaudiaCuttano/SANSA.

</details>


### [51] [ALTER: All-in-One Layer Pruning and Temporal Expert Routing for Efficient Diffusion Generation](https://arxiv.org/abs/2505.21817)
*Xiaomeng Yang,Lei Lu,Qihui Fan,Changdi Yang,Juyi Lin,Yanzhi Wang,Xuan Zhang,Shangqian Gao*

Main category: cs.CV

TL;DR: ALTER框架通过统一层剪枝、专家路由和微调，显著提升了扩散模型的推理效率，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高保真图像方面表现出色，但迭代去噪过程导致计算开销大，现有加速方法无法有效捕捉时间动态。

Method: ALTER采用超网络动态生成层剪枝决策和时间步路由，统一优化剪枝、路由和微调。

Result: ALTER在20步推理中仅使用25.9%的MACs，实现3.64倍加速，同时保持与原始模型相同的视觉保真度。

Conclusion: ALTER为资源受限环境下的扩散模型部署提供了一种高效且高质量的解决方案。

Abstract: Diffusion models have demonstrated exceptional capabilities in generating
high-fidelity images. However, their iterative denoising process results in
significant computational overhead during inference, limiting their practical
deployment in resource-constrained environments. Existing acceleration methods
often adopt uniform strategies that fail to capture the temporal variations
during diffusion generation, while the commonly adopted sequential
pruning-then-fine-tuning strategy suffers from sub-optimality due to the
misalignment between pruning decisions made on pretrained weights and the
model's final parameters. To address these limitations, we introduce ALTER:
All-in-One Layer Pruning and Temporal Expert Routing, a unified framework that
transforms diffusion models into a mixture of efficient temporal experts. ALTER
achieves a single-stage optimization that unifies layer pruning, expert
routing, and model fine-tuning by employing a trainable hypernetwork, which
dynamically generates layer pruning decisions and manages timestep routing to
specialized, pruned expert sub-networks throughout the ongoing fine-tuning of
the UNet. This unified co-optimization strategy enables significant efficiency
gains while preserving high generative quality. Specifically, ALTER achieves
same-level visual fidelity to the original 50-step Stable Diffusion v2.1 model
while utilizing only 25.9% of its total MACs with just 20 inference steps and
delivering a 3.64x speedup through 35% sparsity.

</details>


### [52] [HDRSDR-VQA: A Subjective Video Quality Dataset for HDR and SDR Comparative Evaluation](https://arxiv.org/abs/2505.21831)
*Bowen Chen,Cheng-han Lee,Yixu Chen,Zaixi Shang,Hai Wei,Alan C. Bovik*

Main category: cs.CV

TL;DR: HDRSDR-VQA是一个大规模视频质量评估数据集，支持HDR和SDR内容的直接比较，包含960个视频和22,000对主观比较数据。


<details>
  <summary>Details</summary>
Motivation: 为HDR和SDR内容在真实观看条件下的质量比较提供数据支持，填补现有数据集仅关注单一动态范围或评估协议有限的空白。

Method: 生成54个源序列的HDR和SDR版本视频（共960个），通过145名参与者的主观研究收集22,000对比较数据，并转换为JOD分数。

Result: 数据集支持HDR和SDR内容的直接比较，揭示了格式偏好的具体场景和原因。

Conclusion: HDRSDR-VQA为视频质量评估、自适应流媒体和感知模型研究提供了重要资源。

Abstract: We introduce HDRSDR-VQA, a large-scale video quality assessment dataset
designed to facilitate comparative analysis between High Dynamic Range (HDR)
and Standard Dynamic Range (SDR) content under realistic viewing conditions.
The dataset comprises 960 videos generated from 54 diverse source sequences,
each presented in both HDR and SDR formats across nine distortion levels. To
obtain reliable perceptual quality scores, we conducted a comprehensive
subjective study involving 145 participants and six consumer-grade HDR-capable
televisions. A total of over 22,000 pairwise comparisons were collected and
scaled into Just-Objectionable-Difference (JOD) scores. Unlike prior datasets
that focus on a single dynamic range format or use limited evaluation
protocols, HDRSDR-VQA enables direct content-level comparison between HDR and
SDR versions, supporting detailed investigations into when and why one format
is preferred over the other. The open-sourced part of the dataset is publicly
available to support further research in video quality assessment,
content-adaptive streaming, and perceptual model development.

</details>


### [53] [UniMoGen: Universal Motion Generation](https://arxiv.org/abs/2505.21837)
*Aliasghar Khani,Arianna Rampini,Evan Atherton,Bruno Roy*

Main category: cs.CV

TL;DR: UniMoGen是一种基于UNet的扩散模型，用于骨架无关的运动生成，支持多样角色（如人类和动物）的运动数据训练，无需预定义关节数，具有高效性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有运动生成方法依赖特定骨架结构，限制了其通用性。UniMoGen旨在解决这一问题，实现骨架无关的运动生成。

Method: 采用UNet-based扩散模型，动态处理必要关节，支持风格和轨迹输入控制，并能延续历史帧运动。

Result: 在100style数据集上表现优于现有方法，同时在100style和LAFAN1数据集上均实现高性能和高效性。

Conclusion: UniMoGen为角色动画提供了灵活、高效且可控的解决方案，具有广泛应用潜力。

Abstract: Motion generation is a cornerstone of computer graphics, animation, gaming,
and robotics, enabling the creation of realistic and varied character
movements. A significant limitation of existing methods is their reliance on
specific skeletal structures, which restricts their versatility across
different characters. To overcome this, we introduce UniMoGen, a novel
UNet-based diffusion model designed for skeleton-agnostic motion generation.
UniMoGen can be trained on motion data from diverse characters, such as humans
and animals, without the need for a predefined maximum number of joints. By
dynamically processing only the necessary joints for each character, our model
achieves both skeleton agnosticism and computational efficiency. Key features
of UniMoGen include controllability via style and trajectory inputs, and the
ability to continue motions from past frames. We demonstrate UniMoGen's
effectiveness on the 100style dataset, where it outperforms state-of-the-art
methods in diverse character motion generation. Furthermore, when trained on
both the 100style and LAFAN1 datasets, which use different skeletons, UniMoGen
achieves high performance and improved efficiency across both skeletons. These
results highlight UniMoGen's potential to advance motion generation by
providing a flexible, efficient, and controllable solution for a wide range of
character animations.

</details>


### [54] [Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2505.21844)
*Mehrdad Noori,David Osowiechi,Gustavo Adolfo Vargas Hakim,Ali Bahri,Moslem Yazdanpanah,Sahar Dastani,Farzad Beizaee,Ismail Ben Ayed,Christian Desrosiers*

Main category: cs.CV

TL;DR: 提出了一种针对开放词汇语义分割（OVSS）的测试时适应（TTA）方法MLMP，通过多级多提示熵最小化优化视觉语言模型，无需额外数据或标签。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法主要关注图像分类，而密集预测任务如OVSS被忽视，因此需要专门的方法。

Method: MLMP方法结合中间视觉编码器层特征，使用不同文本提示模板在全局和局部级别进行熵最小化。

Result: 实验表明，MLMP在82种测试场景中显著优于直接采用分类TTA基线。

Conclusion: MLMP为OVSS提供了一种有效的TTA解决方案，并建立了标准化评估基准。

Abstract: Recently, test-time adaptation has attracted wide interest in the context of
vision-language models for image classification. However, to the best of our
knowledge, the problem is completely overlooked in dense prediction tasks such
as Open-Vocabulary Semantic Segmentation (OVSS). In response, we propose a
novel TTA method tailored to adapting VLMs for segmentation during test time.
Unlike TTA methods for image classification, our Multi-Level and Multi-Prompt
(MLMP) entropy minimization integrates features from intermediate
vision-encoder layers and is performed with different text-prompt templates at
both the global CLS token and local pixel-wise levels. Our approach could be
used as plug-and-play for any segmentation network, does not require additional
training data or labels, and remains effective even with a single test sample.
Furthermore, we introduce a comprehensive OVSS TTA benchmark suite, which
integrates a rigorous evaluation protocol, seven segmentation datasets, and 15
common corruptions, with a total of 82 distinct test scenarios, establishing a
standardized and comprehensive testbed for future TTA research in
open-vocabulary segmentation. Our experiments on this suite demonstrate that
our segmentation-tailored method consistently delivers significant gains over
direct adoption of TTA classification baselines.

</details>


### [55] [RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers](https://arxiv.org/abs/2505.21847)
*Xuwei Xu,Yang Li,Yudong Chen,Jiajun Liu,Sen Wang*

Main category: cs.CV

TL;DR: 研究发现FFN层是ViT推理延迟的主要来源，提出了一种通道空闲机制，通过结构重参数化优化FFN层，显著降低了延迟并保持（或提升）精度。


<details>
  <summary>Details</summary>
Motivation: 揭示FFN层对ViT推理延迟的关键影响，探索优化大规模ViT效率的方法。

Method: 提出通道空闲机制，允许部分特征通道绕过非线性激活，形成线性通路以实现结构重参数化。

Result: RePaViT系列模型在延迟上显著降低（如RePa-ViT-Large提速66.8%），同时精度保持或提升（如+1.7% top-1准确率）。

Conclusion: RePaViT通过FFN层结构重参数化，为高效ViT提供了新方向。

Abstract: We reveal that feedforward network (FFN) layers, rather than attention
layers, are the primary contributors to Vision Transformer (ViT) inference
latency, with their impact signifying as model size increases. This finding
highlights a critical opportunity for optimizing the efficiency of large-scale
ViTs by focusing on FFN layers. In this work, we propose a novel channel idle
mechanism that facilitates post-training structural reparameterization for
efficient FFN layers during testing. Specifically, a set of feature channels
remains idle and bypasses the nonlinear activation function in each FFN layer,
thereby forming a linear pathway that enables structural reparameterization
during inference. This mechanism results in a family of ReParameterizable
Vision Transformers (RePaViTs), which achieve remarkable latency reductions
with acceptable sacrifices (sometimes gains) in accuracy across various ViTs.
The benefits of our method scale consistently with model sizes, demonstrating
greater speed improvements and progressively narrowing accuracy gaps or even
higher accuracies on larger models. In particular, RePa-ViT-Large and
RePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1
accuracies under the same training strategy, respectively. RePaViT is the first
to employ structural reparameterization on FFN layers to expedite ViTs to our
best knowledge, and we believe that it represents an auspicious direction for
efficient ViTs. Source code is available at
https://github.com/Ackesnal/RePaViT.

</details>


### [56] [FPAN: Mitigating Replication in Diffusion Models through the Fine-Grained Probabilistic Addition of Noise to Token Embeddings](https://arxiv.org/abs/2505.21848)
*Jingqi Xu,Chenghao Li,Yuke Zhang,Peter A. Beerel*

Main category: cs.CV

TL;DR: 论文提出了一种细粒度噪声注入技术（FPAN），通过概率性地向标记嵌入添加较大噪声，显著减少扩散模型对训练数据的复制，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高质量图像方面表现出色，但其复制训练数据的倾向引发了隐私问题，尤其是涉及敏感数据时。现有方法效果有限，需进一步改进。

Method: 提出FPAN技术，通过分析不同噪声量的影响，概率性地向标记嵌入添加较大噪声，以减少数据复制。

Result: FPAN平均减少复制28.78%，优于基线模型和现有噪声添加方法26.51%，结合其他方法可进一步减少复制16.82%。

Conclusion: FPAN是一种有效的隐私保护方法，显著减少数据复制且不影响图像质量，可与其他方法结合使用。

Abstract: Diffusion models have demonstrated remarkable potential in generating
high-quality images. However, their tendency to replicate training data raises
serious privacy concerns, particularly when the training datasets contain
sensitive or private information. Existing mitigation strategies primarily
focus on reducing image duplication, modifying the cross-attention mechanism,
and altering the denoising backbone architecture of diffusion models. Moreover,
recent work has shown that adding a consistent small amount of noise to text
embeddings can reduce replication to some degree. In this work, we begin by
analyzing the impact of adding varying amounts of noise. Based on our analysis,
we propose a fine-grained noise injection technique that probabilistically adds
a larger amount of noise to token embeddings. We refer to our method as
Fine-grained Probabilistic Addition of Noise (FPAN). Through our extensive
experiments, we show that our proposed FPAN can reduce replication by an
average of 28.78% compared to the baseline diffusion model without
significantly impacting image quality, and outperforms the prior
consistent-magnitude-noise-addition approach by 26.51%. Moreover, when combined
with other existing mitigation methods, our FPAN approach can further reduce
replication by up to 16.82% with similar, if not improved, image quality.

</details>


### [57] [Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task](https://arxiv.org/abs/2505.21850)
*Yanbei Jiang,Yihao Ding,Chao Lei,Jiayang Ao,Jey Han Lau,Krista A. Ehinger*

Main category: cs.CV

TL;DR: 论文提出MultiStAR基准和MSEval指标，用于评估多模态大语言模型在抽象视觉推理中的多阶段表现，发现现有模型在复杂规则检测阶段表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有抽象视觉推理基准仅关注单步推理和最终结果，忽略了多阶段推理过程，且现有模型在这些基准上的失败原因不明。

Method: 基于RAVEN设计MultiStAR基准，提出MSEval指标评估中间步骤和最终结果的正确性，并在17种代表性MLLMs上进行实验。

Result: 实验显示现有MLLMs在基础感知任务表现良好，但在复杂规则检测阶段仍存在挑战。

Conclusion: MultiStAR和MSEval填补了现有评估方法的不足，揭示了MLLMs在抽象视觉推理中的局限性。

Abstract: Current Multimodal Large Language Models (MLLMs) excel in general visual
reasoning but remain underexplored in Abstract Visual Reasoning (AVR), which
demands higher-order reasoning to identify abstract rules beyond simple
perception. Existing AVR benchmarks focus on single-step reasoning, emphasizing
the end result but neglecting the multi-stage nature of reasoning process. Past
studies found MLLMs struggle with these benchmarks, but it doesn't explain how
they fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR
benchmark, based on RAVEN, designed to assess reasoning across varying levels
of complexity. Additionally, existing metrics like accuracy only focus on the
final outcomes while do not account for the correctness of intermediate steps.
Therefore, we propose a novel metric, MSEval, which considers the correctness
of intermediate steps in addition to the final outcomes. We conduct
comprehensive experiments on MultiStAR using 17 representative close-source and
open-source MLLMs. The results reveal that while existing MLLMs perform
adequately on basic perception tasks, they continue to face challenges in more
complex rule detection stages.

</details>


### [58] [Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification](https://arxiv.org/abs/2505.21854)
*Jun Chen,Xinke Li,Mingyue Xu,Tianrui Li,Chongshou Li*

Main category: cs.CV

TL;DR: 论文提出两种新策略改进基于梯度的对抗攻击，通过加权梯度和自适应步长策略（WAAttack）及子集攻击（SubAttack），提升攻击效果和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未考虑点云的异质性，导致扰动过大且易察觉，需改进梯度更新机制。

Method: 提出WAAttack（加权梯度和自适应步长）和SubAttack（聚焦关键子集），优化扰动策略。

Result: 实验表明，新方法在生成隐蔽对抗样本上优于现有基线。

Conclusion: 通过重新设计梯度攻击策略，显著提升了攻击效果和隐蔽性。

Abstract: Gradient-based adversarial attacks have become a dominant approach for
evaluating the robustness of point cloud classification models. However,
existing methods often rely on uniform update rules that fail to consider the
heterogeneous nature of point clouds, resulting in excessive and perceptible
perturbations. In this paper, we rethink the design of gradient-based attacks
by analyzing the limitations of conventional gradient update mechanisms and
propose two new strategies to improve both attack effectiveness and
imperceptibility. First, we introduce WAAttack, a novel framework that
incorporates weighted gradients and an adaptive step-size strategy to account
for the non-uniform contribution of points during optimization. This approach
enables more targeted and subtle perturbations by dynamically adjusting updates
according to the local structure and sensitivity of each point. Second, we
propose SubAttack, a complementary strategy that decomposes the point cloud
into subsets and focuses perturbation efforts on structurally critical regions.
Together, these methods represent a principled rethinking of gradient-based
adversarial attacks for 3D point cloud classification. Extensive experiments
demonstrate that our approach outperforms state-of-the-art baselines in
generating highly imperceptible adversarial examples. Code will be released
upon paper acceptance.

</details>


### [59] [Towards Scalable Language-Image Pre-training for 3D Medical Imaging](https://arxiv.org/abs/2505.21862)
*Chenhui Zhao,Yiwei Lyu,Asadur Chowdury,Edward Harake,Akhil Kondepudi,Akshay Rao,Xinhai Hou,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: HLIP是一种用于3D医学影像的可扩展预训练框架，通过分层注意力机制解决了计算效率问题，并在多个基准测试中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 3D医学影像（如CT和MRI）的高计算需求限制了语言-图像预训练的成功，HLIP旨在解决这一问题。

Method: 采用轻量级分层注意力机制（切片、扫描、研究层次），直接在未整理的临床数据集上进行训练。

Result: 在多个基准测试中表现优异，如Rad-ChestCT（+4.3% AUC）、Pub-Brain-5（+32.4% ACC）等。

Conclusion: HLIP证明直接在未整理的临床数据集上进行预训练是3D医学影像语言-图像预训练的有效方向。

Abstract: Language-image pre-training has demonstrated strong performance in 2D medical
imaging, but its success in 3D modalities such as CT and MRI remains limited
due to the high computational demands of volumetric data, which pose a
significant barrier to training on large-scale, uncurated clinical studies. In
this study, we introduce Hierarchical attention for Language-Image Pre-training
(HLIP), a scalable pre-training framework for 3D medical imaging. HLIP adopts a
lightweight hierarchical attention mechanism inspired by the natural hierarchy
of radiology data: slice, scan, and study. This mechanism exhibits strong
generalizability, e.g., +4.3% macro AUC on the Rad-ChestCT benchmark when
pre-trained on CT-RATE. Moreover, the computational efficiency of HLIP enables
direct training on uncurated datasets. Trained on 220K patients with 3.13
million scans for brain MRI and 240K patients with 1.44 million scans for head
CT, HLIP achieves state-of-the-art performance, e.g., +32.4% balanced ACC on
the proposed publicly available brain MRI benchmark Pub-Brain-5; +1.4% and
+6.9% macro AUC on head CT benchmarks RSNA and CQ500, respectively. These
results demonstrate that, with HLIP, directly pre-training on uncurated
clinical datasets is a scalable and effective direction for language-image
pre-training in 3D medical imaging. The code is available at
https://github.com/Zch0414/hlip

</details>


### [60] [GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning](https://arxiv.org/abs/2505.21863)
*Shikhhar Siingh,Abhinav Rawat,Vivek Gupta,Chitta Baral*

Main category: cs.CV

TL;DR: GETReason框架通过提取全局事件、时间和地理空间信息，提升图像上下文理解的深度，并引入GREAT评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确提取图像中的上下文信息，而公开事件图像对新闻和教育至关重要。

Method: 提出GETReason框架和GREAT评估指标，采用分层多智能体方法。

Result: 实验表明，该方法能有效推断图像与事件背景的联系。

Conclusion: GETReason和GREAT能显著提升图像上下文理解的准确性和深度。

Abstract: Publicly significant images from events hold valuable contextual information,
crucial for journalism and education. However, existing methods often struggle
to extract this relevance accurately. To address this, we introduce GETReason
(Geospatial Event Temporal Reasoning), a framework that moves beyond
surface-level image descriptions to infer deeper contextual meaning. We propose
that extracting global event, temporal, and geospatial information enhances
understanding of an image's significance. Additionally, we introduce GREAT
(Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric
for evaluating reasoning-based image understanding. Our layered multi-agent
approach, assessed using a reasoning-weighted metric, demonstrates that
meaningful insights can be inferred, effectively linking images to their
broader event context.

</details>


### [61] [Cross-DINO: Cross the Deep MLP and Transformer for Small Object Detection](https://arxiv.org/abs/2505.21868)
*Guiping Cao,Wenjian Huang,Xiangyuan Lan,Jianguo Zhang,Dongmei Jiang,Yaowei Wang*

Main category: cs.CV

TL;DR: 论文提出Cross-DINO方法，通过结合深度MLP网络和新的Cross Coding Twice Module（CCTM）提升小目标检测性能，并引入Category-Size（CS）软标签和Boost Loss函数。实验表明其在多个数据集上优于DINO。


<details>
  <summary>Details</summary>
Motivation: 小目标检测（SOD）因信息有限和模型预测分数低而具有挑战性，现有Transformer检测器在SOD中表现不足。

Method: 结合深度MLP网络聚合特征，引入CCTM模块增强小目标细节，提出CS软标签和Boost Loss函数。

Result: 在COCO等数据集上，Cross-DINO以36.4% APs优于DINO（+4.4%），且参数和计算量更少。

Conclusion: Cross-DINO有效提升了DETR类模型的小目标检测性能，具有高效性和优越性。

Abstract: Small Object Detection (SOD) poses significant challenges due to limited
information and the model's low class prediction score. While Transformer-based
detectors have shown promising performance, their potential for SOD remains
largely unexplored. In typical DETR-like frameworks, the CNN backbone network,
specialized in aggregating local information, struggles to capture the
necessary contextual information for SOD. The multiple attention layers in the
Transformer Encoder face difficulties in effectively attending to small objects
and can also lead to blurring of features. Furthermore, the model's lower class
prediction score of small objects compared to large objects further increases
the difficulty of SOD. To address these challenges, we introduce a novel
approach called Cross-DINO. This approach incorporates the deep MLP network to
aggregate initial feature representations with both short and long range
information for SOD. Then, a new Cross Coding Twice Module (CCTM) is applied to
integrate these initial representations to the Transformer Encoder feature,
enhancing the details of small objects. Additionally, we introduce a new kind
of soft label named Category-Size (CS), integrating the Category and Size of
objects. By treating CS as new ground truth, we propose a new loss function
called Boost Loss to improve the class prediction score of the model. Extensive
experimental results on COCO, WiderPerson, VisDrone, AI-TOD, and SODA-D
datasets demonstrate that Cross-DINO efficiently improves the performance of
DETR-like models on SOD. Specifically, our model achieves 36.4% APs on COCO for
SOD with only 45M parameters, outperforming the DINO by +4.4% APS (36.4% vs.
32.0%) with fewer parameters and FLOPs, under 12 epochs training setting. The
source codes will be available at https://github.com/Med-Process/Cross-DINO.

</details>


### [62] [EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance](https://arxiv.org/abs/2505.21876)
*Zun Wang,Jaemin Cho,Jialu Li,Han Lin,Jaehong Yoon,Yue Zhang,Mohit Bansal*

Main category: cs.CV

TL;DR: EPiC框架通过自动构建高质量锚点视频和轻量级Anchor-ControlNet模块，实现了无需昂贵相机轨迹标注的高效精确3D相机控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖点云估计和相机轨迹标注，导致锚点视频不准确且资源消耗大。

Method: 基于首帧可见性掩码生成高质量锚点视频，并引入Anchor-ControlNet模块集成到预训练VDMs中。

Result: 在RealEstate10K和MiraData上达到SOTA性能，支持零样本泛化到视频到视频场景。

Conclusion: EPiC在减少参数、训练步骤和数据需求的同时，实现了精确且鲁棒的3D相机控制。

Abstract: Recent approaches on 3D camera control in video diffusion models (VDMs) often
create anchor videos to guide diffusion models as a structured prior by
rendering from estimated point clouds following annotated camera trajectories.
However, errors inherent in point cloud estimation often lead to inaccurate
anchor videos. Moreover, the requirement for extensive camera trajectory
annotations further increases resource demands. To address these limitations,
we introduce EPiC, an efficient and precise camera control learning framework
that automatically constructs high-quality anchor videos without expensive
camera trajectory annotations. Concretely, we create highly precise anchor
videos for training by masking source videos based on first-frame visibility.
This approach ensures high alignment, eliminates the need for camera trajectory
annotations, and thus can be readily applied to any in-the-wild video to
generate image-to-video (I2V) training pairs. Furthermore, we introduce
Anchor-ControlNet, a lightweight conditioning module that integrates anchor
video guidance in visible regions to pretrained VDMs, with less than 1% of
backbone model parameters. By combining the proposed anchor video data and
ControlNet module, EPiC achieves efficient training with substantially fewer
parameters, training steps, and less data, without requiring modifications to
the diffusion model backbone typically needed to mitigate rendering
misalignments. Although being trained on masking-based anchor videos, our
method generalizes robustly to anchor videos made with point clouds during
inference, enabling precise 3D-informed camera control. EPiC achieves SOTA
performance on RealEstate10K and MiraData for I2V camera control task,
demonstrating precise and robust camera control ability both quantitatively and
qualitatively. Notably, EPiC also exhibits strong zero-shot generalization to
video-to-video scenarios.

</details>


### [63] [Hyperspectral Gaussian Splatting](https://arxiv.org/abs/2505.21890)
*Sunil Kumar Narayanan,Lingjun Zhao,Lu Gan,Yongsheng Chen*

Main category: cs.CV

TL;DR: 论文提出了一种结合3D高斯溅射和扩散模型的方法（HS-GS），用于高光谱场景的3D显式重建和新视角合成，解决了NeRF在训练时间和渲染速度上的限制。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像（HSI）在农业中用于无损估计植物营养成分，但现有方法如NeRF在训练和渲染效率上存在不足。

Method: HS-GS结合3D高斯溅射和扩散模型，引入波长编码器和KL散度损失，优化光谱重建和去噪。

Result: 在Hyper-NeRF数据集上的实验表明，HS-GS性能优于现有方法。

Conclusion: HS-GS为高光谱场景重建提供了高效且高质量的解决方案，代码将公开。

Abstract: Hyperspectral imaging (HSI) has been widely used in agricultural applications
for non-destructive estimation of plant nutrient composition and precise
determination of nutritional elements in samples. Recently, 3D reconstruction
methods have been used to create implicit neural representations of HSI scenes,
which can help localize the target object's nutrient composition spatially and
spectrally. Neural Radiance Field (NeRF) is a cutting-edge implicit
representation that can render hyperspectral channel compositions of each
spatial location from any viewing direction. However, it faces limitations in
training time and rendering speed. In this paper, we propose Hyperspectral
Gaussian Splatting (HS-GS), which combines the state-of-the-art 3D Gaussian
Splatting (3DGS) with a diffusion model to enable 3D explicit reconstruction of
the hyperspectral scenes and novel view synthesis for the entire spectral
range. To enhance the model's ability to capture fine-grained reflectance
variations across the light spectrum and leverage correlations between adjacent
wavelengths for denoising, we introduce a wavelength encoder to generate
wavelength-specific spherical harmonics offsets. We also introduce a novel
Kullback--Leibler divergence-based loss to mitigate the spectral distribution
gap between the rendered image and the ground truth. A diffusion model is
further applied for denoising the rendered images and generating photorealistic
hyperspectral images. We present extensive evaluations on five diverse
hyperspectral scenes from the Hyper-NeRF dataset to show the effectiveness of
our proposed HS-GS framework. The results demonstrate that HS-GS achieves new
state-of-the-art performance among all previously published methods. Code will
be released upon publication.

</details>


### [64] [Concentrate on Weakness: Mining Hard Prototypes for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2505.21897)
*Jianchao Jiang,Haofeng Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种改进的少样本医学图像分割方法，通过关注弱特征和边界优化，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有原型生成方法因随机采样或局部平均导致边界模糊，需关注弱特征以改善分割边界。

Method: 设计了支持自预测（SSP）模块识别弱特征，硬原型生成（HPG）模块生成硬原型，多相似性图融合（MSMF）模块优化分割，并引入边界损失。

Result: 在三个公开医学图像数据集上实现了最先进的性能。

Conclusion: 该方法通过优化弱特征和边界处理，显著提升了少样本医学图像分割的精度。

Abstract: Few-Shot Medical Image Segmentation (FSMIS) has been widely used to train a
model that can perform segmentation from only a few annotated images. However,
most existing prototype-based FSMIS methods generate multiple prototypes from
the support image solely by random sampling or local averaging, which can cause
particularly severe boundary blurring due to the tendency for normal features
accounting for the majority of features of a specific category. Consequently,
we propose to focus more attention to those weaker features that are crucial
for clear segmentation boundary. Specifically, we design a Support
Self-Prediction (SSP) module to identify such weak features by comparing true
support mask with one predicted by global support prototype. Then, a Hard
Prototypes Generation (HPG) module is employed to generate multiple hard
prototypes based on these weak features. Subsequently, a Multiple Similarity
Maps Fusion (MSMF) module is devised to generate final segmenting mask in a
dual-path fashion to mitigate the imbalance between foreground and background
in medical images. Furthermore, we introduce a boundary loss to further
constraint the edge of segmentation. Extensive experiments on three publicly
available medical image datasets demonstrate that our method achieves
state-of-the-art performance. Code is available at
https://github.com/jcjiang99/CoW.

</details>


### [65] [CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation](https://arxiv.org/abs/2505.21904)
*Pardis Taghavi,Tian Liu,Renjie Li,Reza Langari,Zhengzhong Tu*

Main category: cs.CV

TL;DR: CAST是一个半监督知识蒸馏框架，通过压缩预训练的视觉基础模型（VFM）为紧凑专家模型，利用有限标注和大量未标注数据实现高效实例分割。


<details>
  <summary>Details</summary>
Motivation: 实例分割需要昂贵的像素级标注和大模型，CAST旨在通过半监督学习减少标注需求并提升模型效率。

Method: CAST分为三个阶段：1）通过自训练和对比像素校准进行VFM教师的领域适应；2）通过多目标损失（结合监督学习和伪标签）蒸馏到紧凑学生模型；3）在标注数据上微调以消除伪标签偏差。核心是实例感知的像素级对比损失。

Result: 在Cityscapes和ADE20K数据集上，CAST的学生模型比其VFM教师模型分别提升了+3.4 AP和+1.5 AP，并优于现有半监督方法。

Conclusion: CAST通过结合对比学习和知识蒸馏，显著提升了实例分割的性能和效率，同时减少了对标注数据的依赖。

Abstract: Instance segmentation demands costly per-pixel annotations and large models.
We introduce CAST, a semi-supervised knowledge distillation (SSKD) framework
that compresses pretrained vision foundation models (VFM) into compact experts
using limited labeled and abundant unlabeled data. CAST unfolds in three
stages: (1) domain adaptation of the VFM teacher(s) via self-training with
contrastive pixel calibration, (2) distillation into a compact student via a
unified multi-objective loss that couples standard supervision and
pseudo-labels with our instance-aware pixel-wise contrastive term, and (3)
fine-tuning on labeled data to remove residual pseudo-label bias. Central to
CAST is an \emph{instance-aware pixel-wise contrastive loss} that fuses mask
and class scores to mine informative negatives and enforce clear inter-instance
margins. By maintaining this contrastive signal across both adaptation and
distillation, we align teacher and student embeddings and fully leverage
unlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses
its adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs.
15.2) and outperforms state-of-the-art semi-supervised approaches.

</details>


### [66] [Reference-Guided Identity Preserving Face Restoration](https://arxiv.org/abs/2505.21905)
*Mo Zhou,Keren Ye,Viraj Shah,Kangfu Mei,Mauricio Delbracio,Peyman Milanfar,Vishal M. Patel,Hossein Talebi*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过最大化参考人脸的效用，提升人脸修复和身份保留效果。


<details>
  <summary>Details</summary>
Motivation: 扩散基图像修复中，保留人脸身份是一个关键但持续的挑战，现有基于参考的方法未能充分利用参考人脸的潜力。

Method: 1) 提出复合上下文表示，融合参考人脸的多层次信息；2) 设计硬样本身份损失函数，改进身份学习效率；3) 提出无需训练的推理时多参考输入适应方法。

Result: 在FFHQ-Ref和CelebA-Ref-Test等基准测试中，该方法实现了高质量人脸修复和最优的身份保留效果。

Conclusion: 该方法显著提升了人脸修复和身份保留性能，优于现有工作。

Abstract: Preserving face identity is a critical yet persistent challenge in
diffusion-based image restoration. While reference faces offer a path forward,
existing reference-based methods often fail to fully exploit their potential.
This paper introduces a novel approach that maximizes reference face utility
for improved face restoration and identity preservation. Our method makes three
key contributions: 1) Composite Context, a comprehensive representation that
fuses multi-level (high- and low-level) information from the reference face,
offering richer guidance than prior singular representations. 2) Hard Example
Identity Loss, a novel loss function that leverages the reference face to
address the identity learning inefficiencies found in the existing identity
loss. 3) A training-free method to adapt the model to multi-reference inputs
during inference. The proposed method demonstrably restores high-quality faces
and achieves state-of-the-art identity preserving restoration on benchmarks
such as FFHQ-Ref and CelebA-Ref-Test, consistently outperforming previous work.

</details>


### [67] [AlignGen: Boosting Personalized Image Generation with Cross-Modality Prior Alignment](https://arxiv.org/abs/2505.21911)
*Yiheng Lin,Shifang Zhao,Ting Liu,Xiaochao Qu,Luoqi Liu,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: AlignGen提出了一种跨模态先验对齐机制，通过可学习令牌、鲁棒训练策略和选择性跨模态注意力掩码，解决了文本和参考图像不对齐时生成结果偏向文本先验的问题。


<details>
  <summary>Details</summary>
Motivation: 解决文本提示与参考图像不对齐时，生成结果偏向文本先验而丢失参考内容的问题。

Method: 1) 引入可学习令牌桥接文本和视觉先验；2) 采用鲁棒训练策略确保先验对齐；3) 使用选择性跨模态注意力掩码。

Result: AlignGen在零样本方法中表现优异，甚至超过流行的测试时优化方法。

Conclusion: AlignGen通过跨模态先验对齐机制，显著提升了个性化图像生成的质量和一致性。

Abstract: Personalized image generation aims to integrate user-provided concepts into
text-to-image models, enabling the generation of customized content based on a
given prompt. Recent zero-shot approaches, particularly those leveraging
diffusion transformers, incorporate reference image information through
multi-modal attention mechanism. This integration allows the generated output
to be influenced by both the textual prior from the prompt and the visual prior
from the reference image. However, we observe that when the prompt and
reference image are misaligned, the generated results exhibit a stronger bias
toward the textual prior, leading to a significant loss of reference content.
To address this issue, we propose AlignGen, a Cross-Modality Prior Alignment
mechanism that enhances personalized image generation by: 1) introducing a
learnable token to bridge the gap between the textual and visual priors, 2)
incorporating a robust training strategy to ensure proper prior alignment, and
3) employing a selective cross-modal attention mask within the multi-modal
attention mechanism to further align the priors. Experimental results
demonstrate that AlignGen outperforms existing zero-shot methods and even
surpasses popular test-time optimization approaches.

</details>


### [68] [LiDARDustX: A LiDAR Dataset for Dusty Unstructured Road Environments](https://arxiv.org/abs/2505.21914)
*Chenfeng Wei,Qi Wu,Si Zuo,Jiahua Xu,Boyang Zhao,Zeyu Yang,Guotao Xie,Shenhong Wang*

Main category: cs.CV

TL;DR: LiDARDustX数据集填补了高粉尘环境下感知任务的空白，包含30,000帧LiDAR数据，用于评估3D检测与分割算法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多集中于结构化城市环境，缺乏对高粉尘等特殊场景的覆盖。

Method: 使用六种LiDAR传感器采集数据，包含3D边界框标注和点云语义分割，80%以上为粉尘场景。

Result: 建立了高粉尘环境下算法的基准，并分析了粉尘对感知精度的影响。

Conclusion: LiDARDustX数据集为高粉尘环境下的感知研究提供了重要资源。

Abstract: Autonomous driving datasets are essential for validating the progress of
intelligent vehicle algorithms, which include localization, perception, and
prediction. However, existing datasets are predominantly focused on structured
urban environments, which limits the exploration of unstructured and
specialized scenarios, particularly those characterized by significant dust
levels. This paper introduces the LiDARDustX dataset, which is specifically
designed for perception tasks under high-dust conditions, such as those
encountered in mining areas. The LiDARDustX dataset consists of 30,000 LiDAR
frames captured by six different LiDAR sensors, each accompanied by 3D bounding
box annotations and point cloud semantic segmentation. Notably, over 80% of the
dataset comprises dust-affected scenes. By utilizing this dataset, we have
established a benchmark for evaluating the performance of state-of-the-art 3D
detection and segmentation algorithms. Additionally, we have analyzed the
impact of dust on perception accuracy and delved into the causes of these
effects. The data and further information can be accessed at:
https://github.com/vincentweikey/LiDARDustX.

</details>


### [69] [BD Open LULC Map: High-resolution land use land cover mapping & benchmarking for urban development in Dhaka, Bangladesh](https://arxiv.org/abs/2505.21915)
*Mir Sazzat Hossain,Ovi Paul,Md Akil Raihan Iftee,Rakibul Hasan Rajib,Abu Bakar Siddik Nayem,Anis Sarker,Arshad Momen,Md. Ashraful Amin,Amin Ahsan Ali,AKM Mahbubur Rahman*

Main category: cs.CV

TL;DR: BD Open LULC Map (BOLM) 是一个针对南亚/东亚地区的高分辨率土地利用土地覆盖数据集，用于支持深度学习模型和领域适应任务。


<details>
  <summary>Details</summary>
Motivation: 南亚/东亚发展中国家缺乏标注卫星数据，限制了土地利用土地覆盖分类的可靠性。

Method: 使用高分辨率Bing卫星影像（2.22米/像素）和DeepLab V3+模型进行LULC分割，并通过GIS专家验证地面真实数据。

Result: BOLM覆盖4,392平方公里，包含11个类别，并在Bing和Sentinel-2A影像上进行了性能比较。

Conclusion: BOLM填补了南亚/东亚地区LULC数据集的空白，支持可靠的深度学习和领域适应研究。

Abstract: Land Use Land Cover (LULC) mapping using deep learning significantly enhances
the reliability of LULC classification, aiding in understanding geography,
socioeconomic conditions, poverty levels, and urban sprawl. However, the
scarcity of annotated satellite data, especially in South/East Asian developing
countries, poses a major challenge due to limited funding, diverse
infrastructures, and dense populations. In this work, we introduce the BD Open
LULC Map (BOLM), providing pixel-wise LULC annotations across eleven classes
(e.g., Farmland, Water, Forest, Urban Structure, Rural Built-Up) for Dhaka
metropolitan city and its surroundings using high-resolution Bing satellite
imagery (2.22 m/pixel). BOLM spans 4,392 sq km (891 million pixels), with
ground truth validated through a three-stage process involving GIS experts. We
benchmark LULC segmentation using DeepLab V3+ across five major classes and
compare performance on Bing and Sentinel-2A imagery. BOLM aims to support
reliable deep models and domain adaptation tasks, addressing critical LULC
dataset gaps in South/East Asia.

</details>


### [70] [InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective](https://arxiv.org/abs/2505.21920)
*Yuanhong Zhang,Muyao Yuan,Weizhan Zhang,Tieliang Gong,Wen Wen,Jiangyong Ying,Weijie Shi*

Main category: cs.CV

TL;DR: InfoSAM提出了一种基于信息论的方法，通过保留预训练SAM的领域不变关系，提升其在专业领域的性能。


<details>
  <summary>Details</summary>
Motivation: SAM在通用任务中表现优异，但在专业领域表现不佳。现有PEFT方法忽略了预训练模型中的领域不变关系。

Method: 提出InfoSAM，通过两个互信息目标：压缩预训练SAM的领域不变关系，并最大化师生模型间的互信息。

Result: 实验证明InfoSAM能显著提升SAM在专业任务中的性能。

Conclusion: InfoSAM为SAM的PEFT提供了有效的知识蒸馏框架，适用于专业场景。

Abstract: The Segment Anything Model (SAM), a vision foundation model, exhibits
impressive zero-shot capabilities in general tasks but struggles in specialized
domains. Parameter-efficient fine-tuning (PEFT) is a promising approach to
unleash the potential of SAM in novel scenarios. However, existing PEFT methods
for SAM neglect the domain-invariant relations encoded in the pre-trained
model. To bridge this gap, we propose InfoSAM, an information-theoretic
approach that enhances SAM fine-tuning by distilling and preserving its
pre-trained segmentation knowledge. Specifically, we formulate the knowledge
transfer process as two novel mutual information-based objectives: (i) to
compress the domain-invariant relation extracted from pre-trained SAM,
excluding pseudo-invariant information as possible, and (ii) to maximize mutual
information between the relational knowledge learned by the teacher
(pre-trained SAM) and the student (fine-tuned model). The proposed InfoSAM
establishes a robust distillation framework for PEFT of SAM. Extensive
experiments across diverse benchmarks validate InfoSAM's effectiveness in
improving SAM family's performance on real-world tasks, demonstrating its
adaptability and superiority in handling specialized scenarios.

</details>


### [71] [Point-to-Region Loss for Semi-Supervised Point-Based Crowd Counting](https://arxiv.org/abs/2505.21943)
*Wei Lin,Chenyang Zhao,Antoni B. Chan*

Main category: cs.CV

TL;DR: 该论文提出了一种基于伪标签的半监督计数框架，通过点对区域（P2R）方案替代点对点（P2P）监督，解决了伪标签训练中的置信度传播问题。


<details>
  <summary>Details</summary>
Motivation: 点检测方法在密集人群定位和计数中表现优异，但标注成本高。论文旨在通过半监督学习减少标注需求。

Method: 提出点特定激活图（PSAM）分析训练问题，并设计P2R方案，将点监督扩展为区域监督。

Result: 实验表明P2R在半监督计数和无监督域适应中有效解决了PSAM揭示的问题。

Conclusion: P2R方案通过区域共享置信度，显著提升了伪标签训练的稳定性和性能。

Abstract: Point detection has been developed to locate pedestrians in crowded scenes by
training a counter through a point-to-point (P2P) supervision scheme. Despite
its excellent localization and counting performance, training a point-based
counter still faces challenges concerning annotation labor: hundreds to
thousands of points are required to annotate a single sample capturing a dense
crowd. In this paper, we integrate point-based methods into a semi-supervised
counting framework based on pseudo-labeling, enabling the training of a counter
with only a few annotated samples supplemented by a large volume of
pseudo-labeled data. However, during implementation, the training encounters
issues as the confidence for pseudo-labels fails to be propagated to background
pixels via the P2P. To tackle this challenge, we devise a point-specific
activation map (PSAM) to visually interpret the phenomena occurring during the
ill-posed training. Observations from the PSAM suggest that the feature map is
excessively activated by the loss for unlabeled data, causing the decoder to
misinterpret these over-activations as pedestrians. To mitigate this issue, we
propose a point-to-region (P2R) scheme to substitute P2P, which segments out
local regions rather than detects a point corresponding to a pedestrian for
supervision. Consequently, pixels in the local region can share the same
confidence with the corresponding pseudo points. Experimental results in both
semi-supervised counting and unsupervised domain adaptation highlight the
advantages of our method, illustrating P2R can resolve issues identified in
PSAM. The code is available at https://github.com/Elin24/P2RLoss.

</details>


### [72] [UniTalk: Towards Universal Active Speaker Detection in Real World Scenarios](https://arxiv.org/abs/2505.21954)
*Le Thien Phuc Nguyen,Zhuoran Yu,Khoa Quang Nhat Cao,Yuwei Guo,Tu Ho Manh Pham,Tuan Tai Nguyen,Toan Ngo Duc Vo,Lucas Poon,Soochahn Lee,Yong Jae Lee*

Main category: cs.CV

TL;DR: UniTalk是一个专为主动说话人检测任务设计的新数据集，强调挑战性场景以提升模型泛化能力。相比传统数据集如AVA，UniTalk专注于多样且困难的真实世界条件，包括低资源语言、嘈杂背景和拥挤场景。实验表明，现有模型在AVA上表现优异但在UniTalk上表现不佳，说明任务尚未解决。UniTalk为研究提供了新基准。


<details>
  <summary>Details</summary>
Motivation: 现有数据集（如AVA）主要基于老电影，存在显著的领域差距，无法反映真实世界的多样性。UniTalk旨在填补这一空白，提供更具挑战性的数据以推动模型泛化能力。

Method: UniTalk包含44.5小时视频，涵盖48,693个说话人身份，覆盖多样视频类型和复杂场景（如多说话人重叠）。数据集通过帧级标注提供高质量训练数据。

Result: 在UniTalk上，现有模型表现远未饱和，表明任务在真实条件下仍具挑战性。但UniTalk训练的模型在Talkies、ASW和AVA等数据集上表现出更强的泛化能力。

Conclusion: UniTalk为主动说话人检测提供了新基准，支持开发更具适应性和鲁棒性的模型，推动任务在真实场景中的进展。

Abstract: We present UniTalk, a novel dataset specifically designed for the task of
active speaker detection, emphasizing challenging scenarios to enhance model
generalization. Unlike previously established benchmarks such as AVA, which
predominantly features old movies and thus exhibits significant domain gaps,
UniTalk focuses explicitly on diverse and difficult real-world conditions.
These include underrepresented languages, noisy backgrounds, and crowded scenes
- such as multiple visible speakers speaking concurrently or in overlapping
turns. It contains over 44.5 hours of video with frame-level active speaker
annotations across 48,693 speaking identities, and spans a broad range of video
types that reflect real-world conditions. Through rigorous evaluation, we show
that state-of-the-art models, while achieving nearly perfect scores on AVA,
fail to reach saturation on UniTalk, suggesting that the ASD task remains far
from solved under realistic conditions. Nevertheless, models trained on UniTalk
demonstrate stronger generalization to modern "in-the-wild" datasets like
Talkies and ASW, as well as to AVA. UniTalk thus establishes a new benchmark
for active speaker detection, providing researchers with a valuable resource
for developing and evaluating versatile and resilient models.
  Dataset: https://huggingface.co/datasets/plnguyen2908/UniTalk-ASD
  Code: https://github.com/plnguyen2908/UniTalk-ASD-code

</details>


### [73] [Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs](https://arxiv.org/abs/2505.21955)
*Insu Lee,Wooje Park,Jaeyun Jang,Minyoung Noh,Kyuhong Shim,Byonghyo Shim*

Main category: cs.CV

TL;DR: 论文提出了一种结合第一人称（egocentric）和第三人称（exocentric）视角的框架，以增强大型视觉语言模型（LVLMs）在多视角推理中的表现，并提出了E3VQA基准和M3CoT提示技术。


<details>
  <summary>Details</summary>
Motivation: 第一人称视角的局限性（如视野狭窄和缺乏全局上下文）导致LVLMs在空间或上下文复杂的查询中表现不佳，因此需要结合第三人称视角提供补充信息。

Method: 提出了E3VQA基准（包含4K高质量问答对）和M3CoT提示技术（通过整合多视角场景图构建统一场景表示）。

Result: M3CoT显著提升了LVLMs的性能（GPT-4o提升4.84%，Gemini 2.0 Flash提升5.94%）。

Conclusion: 结合第一人称和第三人称视角能有效提升LVLMs的多视角推理能力，同时揭示了LVLMs的优缺点。

Abstract: Large vision-language models (LVLMs) are increasingly deployed in interactive
applications such as virtual and augmented reality, where first-person
(egocentric) view captured by head-mounted cameras serves as key input. While
this view offers fine-grained cues about user attention and hand-object
interactions, their narrow field of view and lack of global context often lead
to failures on spatially or contextually demanding queries. To address this, we
introduce a framework that augments egocentric inputs with third-person
(exocentric) views, providing complementary information such as global scene
layout and object visibility to LVLMs. We present E3VQA, the first benchmark
for multi-view question answering with 4K high-quality question-answer pairs
grounded in synchronized ego-exo image pairs. Additionally, we propose M3CoT, a
training-free prompting technique that constructs a unified scene
representation by integrating scene graphs from three complementary
perspectives. M3CoT enables LVLMs to reason more effectively across views,
yielding consistent performance gains (4.84% for GPT-4o and 5.94% for Gemini
2.0 Flash) over a recent CoT baseline. Our extensive evaluation reveals key
strengths and limitations of LVLMs in multi-view reasoning and highlights the
value of leveraging both egocentric and exocentric inputs.

</details>


### [74] [Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation](https://arxiv.org/abs/2505.21956)
*Mengdan Zhu,Senhao Cheng,Guangji Bai,Yifei Zhang,Liang Zhao*

Main category: cs.CV

TL;DR: 提出Cross-modal RAG框架，通过分解查询和图像为子维度组件，实现子查询感知的检索与生成，显著提升检索和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法无法处理复杂查询中多元素需求的问题，需要更细粒度的解决方案。

Method: 结合稀疏和密集检索器，分解查询和图像为子维度，利用多模态大语言模型选择性生成图像。

Result: 在多个数据集上显著优于现有基线，同时保持高效性。

Conclusion: Cross-modal RAG为复杂查询提供了更有效的解决方案，提升了生成和检索性能。

Abstract: Text-to-image generation increasingly demands access to domain-specific,
fine-grained, and rapidly evolving knowledge that pretrained models cannot
fully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to
address this by retrieving globally relevant images, but they fail when no
single image contains all desired elements from a complex user query. We
propose Cross-modal RAG, a novel framework that decomposes both queries and
images into sub-dimensional components, enabling subquery-aware retrieval and
generation. Our method introduces a hybrid retrieval strategy - combining a
sub-dimensional sparse retriever with a dense retriever - to identify a
Pareto-optimal set of images, each contributing complementary aspects of the
query. During generation, a multimodal large language model is guided to
selectively condition on relevant visual features aligned to specific
subqueries, ensuring subquery-aware image synthesis. Extensive experiments on
MS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal
RAG significantly outperforms existing baselines in both retrieval and
generation quality, while maintaining high efficiency.

</details>


### [75] [One-Way Ticket:Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion Models](https://arxiv.org/abs/2505.21960)
*Senmao Li,Lei Wang,Kai Wang,Tao Liu,Jiehang Xie,Joost van de Weijer,Fahad Shahbaz Khan,Shiqi Yang,Yaxing Wang,Jian Yang*

Main category: cs.CV

TL;DR: TiUE是一种新型的T2I扩散模型蒸馏方法，通过共享编码器特征和并行采样，显著提升推理速度，同时保持图像质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有蒸馏T2I模型在减少采样步数时牺牲了多样性和质量，且UNet编码器存在冗余计算。TiUE旨在解决这些问题。

Method: 提出时间无关的统一编码器TiUE，共享编码器特征，结合KL散度正则化噪声预测，实现并行采样和高效推理。

Result: TiUE在生成多样性和真实性上优于LCM、SD-Turbo和SwiftBrushv2，同时保持计算效率。

Conclusion: TiUE为T2I扩散模型提供了一种高效且高质量的蒸馏方案，显著提升了推理速度和生成效果。

Abstract: Text-to-Image (T2I) diffusion models have made remarkable advancements in
generative modeling; however, they face a trade-off between inference speed and
image quality, posing challenges for efficient deployment. Existing distilled
T2I models can generate high-fidelity images with fewer sampling steps, but
often struggle with diversity and quality, especially in one-step models. From
our analysis, we observe redundant computations in the UNet encoders. Our
findings suggest that, for T2I diffusion models, decoders are more adept at
capturing richer and more explicit semantic information, while encoders can be
effectively shared across decoders from diverse time steps. Based on these
observations, we introduce the first Time-independent Unified Encoder TiUE for
the student model UNet architecture, which is a loop-free image generation
approach for distilling T2I diffusion models. Using a one-pass scheme, TiUE
shares encoder features across multiple decoder time steps, enabling parallel
sampling and significantly reducing inference time complexity. In addition, we
incorporate a KL divergence term to regularize noise prediction, which enhances
the perceptual realism and diversity of the generated images. Experimental
results demonstrate that TiUE outperforms state-of-the-art methods, including
LCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results
while maintaining the computational efficiency.

</details>


### [76] [A2Seek: Towards Reasoning-Centric Benchmark for Aerial Anomaly Understanding](https://arxiv.org/abs/2505.21962)
*Mengjingcheng Mo,Xinyang Tong,Jiaxu Leng,Mingpi Tan,Jiankang Zheng,Yiran Liu,Haosheng Chen,Ji Gan,Weisheng Li,Xinbo Gao*

Main category: cs.CV

TL;DR: 论文提出A2Seek数据集和A2Seek-R1框架，用于无人机视角下的异常检测，显著提升了预测和定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和方法难以适应无人机视角的动态变化和复杂场景，导致性能下降。

Method: 提出A2Seek数据集，并设计A2Seek-R1框架，包括GoT引导的微调和A-GRPO奖励函数。

Result: A2Seek-R1在预测准确率和异常定位上分别提升22.04%和13.9%。

Conclusion: A2Seek-R1在复杂环境中表现优异，数据集和代码将开源。

Abstract: While unmanned aerial vehicles (UAVs) offer wide-area, high-altitude coverage
for anomaly detection, they face challenges such as dynamic viewpoints, scale
variations, and complex scenes. Existing datasets and methods, mainly designed
for fixed ground-level views, struggle to adapt to these conditions, leading to
significant performance drops in drone-view scenarios. To bridge this gap, we
introduce A2Seek (Aerial Anomaly Seek), a large-scale, reasoning-centric
benchmark dataset for aerial anomaly understanding. This dataset covers various
scenarios and environmental conditions, providing high-resolution real-world
aerial videos with detailed annotations, including anomaly categories,
frame-level timestamps, region-level bounding boxes, and natural language
explanations for causal reasoning. Building on this dataset, we propose
A2Seek-R1, a novel reasoning framework that generalizes R1-style strategies to
aerial anomaly understanding, enabling a deeper understanding of "Where"
anomalies occur and "Why" they happen in aerial frames. To this end, A2Seek-R1
first employs a graph-of-thought (GoT)-guided supervised fine-tuning approach
to activate the model's latent reasoning capabilities on A2Seek. Then, we
introduce Aerial Group Relative Policy Optimization (A-GRPO) to design
rule-based reward functions tailored to aerial scenarios. Furthermore, we
propose a novel "seeking" mechanism that simulates UAV flight behavior by
directing the model's attention to informative regions. Extensive experiments
demonstrate that A2Seek-R1 achieves up to a 22.04% improvement in AP for
prediction accuracy and a 13.9% gain in mIoU for anomaly localization,
exhibiting strong generalization across complex environments and
out-of-distribution scenarios. Our dataset and code will be released at
https://hayneyday.github.io/A2Seek/.

</details>


### [77] [DvD: Unleashing a Generative Paradigm for Document Dewarping via Coordinates-based Diffusion Model](https://arxiv.org/abs/2505.21975)
*Weiguang Zhang,Huangcheng Lu,Maizhen Ning,Xiaowei Huang,Wei Wang,Kaizhu Huang,Qiufeng Wang*

Main category: cs.CV

TL;DR: 论文提出了DvD，一种基于扩散模型的文档去扭曲方法，通过坐标级去噪和时间变体条件细化机制，显著提升了文档结构的保留效果，并提出了新的基准AnyPhotoDoc6300。


<details>
  <summary>Details</summary>
Motivation: 文档去扭曲在提升文本可读性方面取得了进展，但保留文档结构仍具挑战性。扩散模型在图像生成中的成功启发了其在文档去扭曲中的应用。

Method: DvD采用坐标级去噪而非像素级去噪，生成变形校正映射，并提出时间变体条件细化机制以增强文档结构保留。

Result: DvD在多个基准测试（如DocUNet、DIR300和AnyPhotoDoc6300）上实现了最先进的性能，并保持了计算效率。

Conclusion: DvD为文档去扭曲提供了新的生成模型解决方案，同时提出的AnyPhotoDoc6300基准为未来研究提供了更全面的评估标准。

Abstract: Document dewarping aims to rectify deformations in photographic document
images, thus improving text readability, which has attracted much attention and
made great progress, but it is still challenging to preserve document
structures. Given recent advances in diffusion models, it is natural for us to
consider their potential applicability to document dewarping. However, it is
far from straightforward to adopt diffusion models in document dewarping due to
their unfaithful control on highly complex document images (e.g.,
2000$\times$3000 resolution). In this paper, we propose DvD, the first
generative model to tackle document \textbf{D}ewarping \textbf{v}ia a
\textbf{D}iffusion framework. To be specific, DvD introduces a coordinate-level
denoising instead of typical pixel-level denoising, generating a mapping for
deformation rectification. In addition, we further propose a time-variant
condition refinement mechanism to enhance the preservation of document
structures. In experiments, we find that current document dewarping benchmarks
can not evaluate dewarping models comprehensively. To this end, we present
AnyPhotoDoc6300, a rigorously designed large-scale document dewarping benchmark
comprising 6,300 real image pairs across three distinct domains, enabling
fine-grained evaluation of dewarping models. Comprehensive experiments
demonstrate that our proposed DvD can achieve state-of-the-art performance with
acceptable computational efficiency on multiple metrics across various
benchmarks including DocUNet, DIR300, and AnyPhotoDoc6300. The new benchmark
and code will be publicly available.

</details>


### [78] [Learning World Models for Interactive Video Generation](https://arxiv.org/abs/2505.21996)
*Taiye Chen,Xun Hu,Zihan Ding,Chi Jin*

Main category: cs.CV

TL;DR: 论文提出视频检索增强生成（VRAG）方法，通过显式全局状态条件减少长期累积误差，提升世界模型的时空一致性。


<details>
  <summary>Details</summary>
Motivation: 现有长视频生成模型因累积误差和内存机制不足，缺乏有效的世界建模能力，需改进以实现交互性和时空一致性。

Method: 采用动作条件和自回归框架增强图像到视频模型，并提出VRAG方法，结合显式全局状态条件。

Result: VRAG显著减少长期累积误差，提高世界模型的时空一致性，而传统自回归生成和检索增强生成效果较差。

Conclusion: 研究揭示了视频世界模型的基本挑战，并为改进视频生成模型的世界建模能力提供了基准。

Abstract: Foundational world models must be both interactive and preserve
spatiotemporal coherence for effective future planning with action choices.
However, present models for long video generation have limited inherent world
modeling capabilities due to two main challenges: compounding errors and
insufficient memory mechanisms. We enhance image-to-video models with
interactive capabilities through additional action conditioning and
autoregressive framework, and reveal that compounding error is inherently
irreducible in autoregressive video generation, while insufficient memory
mechanism leads to incoherence of world models. We propose video retrieval
augmented generation (VRAG) with explicit global state conditioning, which
significantly reduces long-term compounding errors and increases spatiotemporal
consistency of world models. In contrast, naive autoregressive generation with
extended context windows and retrieval-augmented generation prove less
effective for video generation, primarily due to the limited in-context
learning capabilities of current video models. Our work illuminates the
fundamental challenges in video world models and establishes a comprehensive
benchmark for improving video generation models with internal world modeling
capabilities.

</details>


### [79] [D-Fusion: Direct Preference Optimization for Aligning Diffusion Models with Visually Consistent Samples](https://arxiv.org/abs/2505.22002)
*Zijing Hu,Fengda Zhang,Kun Kuang*

Main category: cs.CV

TL;DR: 论文提出D-Fusion方法，通过视觉一致性样本优化扩散模型的提示-图像对齐问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成图像与文本提示对齐方面存在局限性，且直接偏好优化（DPO）因视觉不一致问题效果受限。

Method: D-Fusion通过掩码引导的自注意力融合生成视觉一致性样本，并保留去噪轨迹以支持DPO训练。

Result: 实验表明D-Fusion能有效提升不同强化学习算法中的提示-图像对齐效果。

Conclusion: D-Fusion解决了视觉不一致问题，显著提升了扩散模型的对齐能力。

Abstract: The practical applications of diffusion models have been limited by the
misalignment between generated images and corresponding text prompts. Recent
studies have introduced direct preference optimization (DPO) to enhance the
alignment of these models. However, the effectiveness of DPO is constrained by
the issue of visual inconsistency, where the significant visual disparity
between well-aligned and poorly-aligned images prevents diffusion models from
identifying which factors contribute positively to alignment during
fine-tuning. To address this issue, this paper introduces D-Fusion, a method to
construct DPO-trainable visually consistent samples. On one hand, by performing
mask-guided self-attention fusion, the resulting images are not only
well-aligned, but also visually consistent with given poorly-aligned images. On
the other hand, D-Fusion can retain the denoising trajectories of the resulting
images, which are essential for DPO training. Extensive experiments demonstrate
the effectiveness of D-Fusion in improving prompt-image alignment when applied
to different reinforcement learning algorithms.

</details>


### [80] [Event-based Egocentric Human Pose Estimation in Dynamic Environment](https://arxiv.org/abs/2505.22007)
*Wataru Ikeda,Masashi Hatano,Ryosei Hara,Mariko Isogawa*

Main category: cs.CV

TL;DR: 论文提出了一种基于事件相机的前向视角人体姿态估计方法D-EventEgo，通过头部姿态估计和动态对象分割提升动态环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖RGB相机，无法应对低光环境和运动模糊，事件相机有望解决这些问题。

Method: 提出D-EventEgo框架，先估计头部姿态，再生成身体姿态，并引入动态对象分割模块提升头部姿态估计精度。

Result: 在合成事件数据集上，该方法在动态环境中五项指标中的四项优于基线。

Conclusion: D-EventEgo为事件相机的人体姿态估计提供了有效解决方案，尤其在动态环境中表现优异。

Abstract: Estimating human pose using a front-facing egocentric camera is essential for
applications such as sports motion analysis, VR/AR, and AI for wearable
devices. However, many existing methods rely on RGB cameras and do not account
for low-light environments or motion blur. Event-based cameras have the
potential to address these challenges. In this work, we introduce a novel task
of human pose estimation using a front-facing event-based camera mounted on the
head and propose D-EventEgo, the first framework for this task. The proposed
method first estimates the head poses, and then these are used as conditions to
generate body poses. However, when estimating head poses, the presence of
dynamic objects mixed with background events may reduce head pose estimation
accuracy. Therefore, we introduce the Motion Segmentation Module to remove
dynamic objects and extract background information. Extensive experiments on
our synthetic event-based dataset derived from EgoBody, demonstrate that our
approach outperforms our baseline in four out of five evaluation metrics in
dynamic environments.

</details>


### [81] [Prototype Embedding Optimization for Human-Object Interaction Detection in Livestreaming](https://arxiv.org/abs/2505.22011)
*Menghui Zhang,Jing Zhang,Lin Chen,Li Zhuo*

Main category: cs.CV

TL;DR: 提出了一种原型嵌入优化方法（PeO-HOI）来解决直播中人-物交互检测中的对象偏差问题，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 直播中的人-物交互检测存在对象偏差问题，即过度关注物体而忽略其与主播的交互。

Method: 通过对象检测与跟踪预处理直播视频，采用原型嵌入优化减少对象偏差，建模时空上下文后通过预测头输出检测结果。

Result: 在VidHOI和BJUT-HOI数据集上，PeO-HOI的检测准确率显著提升（如VidHOI上37.19%@full）。

Conclusion: PeO-HOI有效解决了直播中人-物交互检测中的对象偏差问题，提升了检测性能。

Abstract: Livestreaming often involves interactions between streamers and objects,
which is critical for understanding and regulating web content. While
human-object interaction (HOI) detection has made some progress in
general-purpose video downstream tasks, when applied to recognize the
interaction behaviors between a streamer and different objects in
livestreaming, it tends to focuses too much on the objects and neglects their
interactions with the streamer, which leads to object bias. To solve this
issue, we propose a prototype embedding optimization for human-object
interaction detection (PeO-HOI). First, the livestreaming is preprocessed using
object detection and tracking techniques to extract features of the
human-object (HO) pairs. Then, prototype embedding optimization is adopted to
mitigate the effect of object bias on HOI. Finally, after modelling the
spatio-temporal context between HO pairs, the HOI detection results are
obtained by the prediction head. The experimental results show that the
detection accuracy of the proposed PeO-HOI method has detection accuracies of
37.19%@full, 51.42%@non-rare, 26.20%@rare on the publicly available dataset
VidHOI, 45.13%@full, 62.78%@non-rare and 30.37%@rare on the self-built dataset
BJUT-HOI, which effectively improves the HOI detection performance in
livestreaming.

</details>


### [82] [PanoWan: Lifting Diffusion Video Generation Models to 360° with Latitude/Longitude-aware Mechanisms](https://arxiv.org/abs/2505.22016)
*Yifei Xia,Shuchen Weng,Siqi Yang,Jingqi Liu,Chengxuan Zhu,Minggui Teng,Zijian Jia,Han Jiang,Boxin Shi*

Main category: cs.CV

TL;DR: PanoWan利用预训练的文本到视频模型生成高质量全景视频，通过纬度感知采样和旋转语义去噪等技术解决现有模型的局限性，并贡献了一个高质量全景视频数据集PanoVid。


<details>
  <summary>Details</summary>
Motivation: 现有全景视频生成模型难以利用预训练的文本到视频模型，主要受限于数据集规模和空间特征表示的差异。

Method: PanoWan采用纬度感知采样避免纬度失真，通过旋转语义去噪和填充像素解码确保经度边界的无缝过渡。

Result: PanoWan在全景视频生成中达到最先进性能，并在零样本下游任务中表现出鲁棒性。

Conclusion: PanoWan通过创新技术和高质量数据集PanoVid，成功提升了预训练模型在全景视频生成中的应用效果。

Abstract: Panoramic video generation enables immersive 360{\deg} content creation,
valuable in applications that demand scene-consistent world exploration.
However, existing panoramic video generation models struggle to leverage
pre-trained generative priors from conventional text-to-video models for
high-quality and diverse panoramic videos generation, due to limited dataset
scale and the gap in spatial feature representations. In this paper, we
introduce PanoWan to effectively lift pre-trained text-to-video models to the
panoramic domain, equipped with minimal modules. PanoWan employs latitude-aware
sampling to avoid latitudinal distortion, while its rotated semantic denoising
and padded pixel-wise decoding ensure seamless transitions at longitude
boundaries. To provide sufficient panoramic videos for learning these lifted
representations, we contribute PanoVid, a high-quality panoramic video dataset
with captions and diverse scenarios. Consequently, PanoWan achieves
state-of-the-art performance in panoramic video generation and demonstrates
robustness for zero-shot downstream tasks.

</details>


### [83] [GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement](https://arxiv.org/abs/2505.22021)
*Zhihong Tang,Yang Li*

Main category: cs.CV

TL;DR: GL-PGENet是一种用于多退化彩色文档图像增强的新架构，通过全局与局部参数生成结合的方法，显著提升了文档图像增强的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于单退化恢复或灰度图像处理，无法满足多退化彩色文档图像的需求。

Method: 提出GL-PGENet，包含分层增强框架、双分支局部细化网络和修改的NestUNet架构，采用两阶段训练策略。

Result: 在DocUNet和RealDAE上分别达到0.7721和0.9480的SSIM分数，表现出卓越的跨域适应性和计算效率。

Conclusion: GL-PGENet在多退化彩色文档图像增强任务中表现出色，具有实际应用价值。

Abstract: Document Image Enhancement (DIE) serves as a critical component in Document
AI systems, where its performance substantially determines the effectiveness of
downstream tasks. To address the limitations of existing methods confined to
single-degradation restoration or grayscale image processing, we present Global
with Local Parametric Generation Enhancement Network (GL-PGENet), a novel
architecture designed for multi-degraded color document images, ensuring both
efficiency and robustness in real-world scenarios. Our solution incorporates
three key innovations: First, a hierarchical enhancement framework that
integrates global appearance correction with local refinement, enabling
coarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network
with parametric generation mechanisms that replaces conventional direct
prediction, producing enhanced outputs through learned intermediate parametric
representations rather than pixel-wise mapping. This approach enhances local
consistency while improving model generalization. Finally, a modified NestUNet
architecture incorporating dense block to effectively fuse low-level pixel
features and high-level semantic features, specifically adapted for document
image characteristics. In addition, to enhance generalization performance, we
adopt a two-stage training strategy: large-scale pretraining on a synthetic
dataset of 500,000+ samples followed by task-specific fine-tuning. Extensive
experiments demonstrate the superiority of GL-PGENet, achieving
state-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The
model also exhibits remarkable cross-domain adaptability and maintains
computational efficiency for high-resolution images without performance
degradation, confirming its practical utility in real-world scenarios.

</details>


### [84] [Guess the Age of Photos: An Interactive Web Platform for Historical Image Age Estimation](https://arxiv.org/abs/2505.22031)
*Hasan Yucedag,Adam Jatowt*

Main category: cs.CV

TL;DR: 论文介绍了一个名为“Guess the Age of Photos”的网页平台，通过两种游戏化模式让用户估计历史照片的年代。平台使用Python等技术构建，基于10,150张图片的数据集，用户评价良好，相对比较比绝对猜测更准确。


<details>
  <summary>Details</summary>
Motivation: 旨在通过互动方式提升用户对历史照片年代的理解，同时为研究人类对图像时间线索的感知提供资源。

Method: 平台提供两种游戏模式：Guess the Year（单图预测年份）和Timeline Challenge（比较两图年代）。使用Python、Flask等技术开发，基于10,150张图片的数据集。

Result: 用户满意度4.25/5，相对比较准确率65.9%，绝对猜测准确率25.6%。老年代更容易识别。

Conclusion: 平台作为教育工具，提升历史意识和分析能力，同时为计算机视觉模型提供标注数据。

Abstract: This paper introduces Guess the Age of Photos, a web platform engaging users
in estimating the years of historical photographs through two gamified modes:
Guess the Year (predicting a single image's year) and Timeline Challenge
(comparing two images to identify the older). Built with Python, Flask,
Bootstrap, and PostgreSQL, it uses a 10,150-image subset of the Date Estimation
in the Wild dataset (1930-1999). Features like dynamic scoring and leaderboards
boost engagement. Evaluated with 113 users and 15,473 gameplays, the platform
earned a 4.25/5 satisfaction rating. Users excelled in relative comparisons
(65.9% accuracy) over absolute year guesses (25.6% accuracy), with older
decades easier to identify. The platform serves as an educational tool,
fostering historical awareness and analytical skills via interactive
exploration of visual heritage. Furthermore, the platform provides a valuable
resource for studying human perception of temporal cues in images and could be
used to generate annotated data for training and evaluating computer vision
models.

</details>


### [85] [Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization](https://arxiv.org/abs/2505.22038)
*Kaiyuan Li,Xiaoyue Chen,Chen Gao,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: 论文提出了一种名为平衡令牌剪枝（BTP）的插件式方法，用于减少大型视觉语言模型（LVLM）中的图像令牌数量，从而降低计算开销。该方法通过分阶段剪枝，权衡局部和全局影响，显著提升了剪枝效果。


<details>
  <summary>Details</summary>
Motivation: 现有的令牌剪枝方法通常忽略了剪枝对当前层和后续层的联合影响，导致剪枝决策不理想。

Method: 提出BTP方法，利用校准集将剪枝过程分为多个阶段，早期阶段关注后续层影响，后期阶段注重局部输出一致性。

Result: 在多种LVLM上验证，平均压缩率达78%，同时保留96.7%的原始模型性能。

Conclusion: BTP方法在减少计算开销的同时，有效保持了模型性能，具有广泛适用性。

Abstract: Large Vision-Language Models (LVLMs) have shown impressive performance across
multi-modal tasks by encoding images into thousands of tokens. However, the
large number of image tokens results in significant computational overhead, and
the use of dynamic high-resolution inputs further increases this burden.
Previous approaches have attempted to reduce the number of image tokens through
token pruning, typically by selecting tokens based on attention scores or image
token diversity. Through empirical studies, we observe that existing methods
often overlook the joint impact of pruning on both the current layer's output
(local) and the outputs of subsequent layers (global), leading to suboptimal
pruning decisions. To address this challenge, we propose Balanced Token Pruning
(BTP), a plug-and-play method for pruning vision tokens. Specifically, our
method utilizes a small calibration set to divide the pruning process into
multiple stages. In the early stages, our method emphasizes the impact of
pruning on subsequent layers, whereas in the deeper stages, the focus shifts
toward preserving the consistency of local outputs. Extensive experiments
across various LVLMs demonstrate the broad effectiveness of our approach on
multiple benchmarks. Our method achieves a 78% compression rate while
preserving 96.7% of the original models' performance on average.

</details>


### [86] [OmniAD: Detect and Understand Industrial Anomaly via Multimodal Reasoning](https://arxiv.org/abs/2505.22039)
*Shifang Zhao,Yiheng Lin,Lu Han,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: OmniAD是一个结合视觉和文本推理的多模态框架，用于细粒度异常检测与分析，通过集成训练策略在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 工业知识在异常检测中的详细分析仍具挑战性，OmniAD旨在填补这一空白。

Method: OmniAD结合视觉推理（Text-as-Mask Encoding）和文本推理（Visual Guided Textual Reasoning），采用SFT与GRPO的集成训练策略。

Result: 在MMAD基准测试中达到79.1分，优于Qwen2.5-VL-7B和GPT-4o，并在多个异常检测基准中表现优异。

Conclusion: 增强视觉感知对异常理解的有效推理至关重要，OmniAD的代码和模型将公开。

Abstract: While anomaly detection has made significant progress, generating detailed
analyses that incorporate industrial knowledge remains a challenge. To address
this gap, we introduce OmniAD, a novel framework that unifies anomaly detection
and understanding for fine-grained analysis. OmniAD is a multimodal reasoner
that combines visual and textual reasoning processes. The visual reasoning
provides detailed inspection by leveraging Text-as-Mask Encoding to perform
anomaly detection through text generation without manually selected thresholds.
Following this, Visual Guided Textual Reasoning conducts comprehensive analysis
by integrating visual perception. To enhance few-shot generalization, we employ
an integrated training strategy that combines supervised fine-tuning (SFT) with
reinforcement learning (GRPO), incorporating three sophisticated reward
functions. Experimental results demonstrate that OmniAD achieves a performance
of 79.1 on the MMAD benchmark, surpassing models such as Qwen2.5-VL-7B and
GPT-4o. It also shows strong results across multiple anomaly detection
benchmarks. These results highlight the importance of enhancing visual
perception for effective reasoning in anomaly understanding. All codes and
models will be publicly available.

</details>


### [87] [LatentMove: Towards Complex Human Movement Video Generation](https://arxiv.org/abs/2505.22046)
*Ashkan Taghipour,Morteza Ghahremani,Mohammed Bennamoun,Farid Boussaid,Aref Miri Rekavandi,Zinuo Li,Qiuhong Ke,Hamid Laga*

Main category: cs.CV

TL;DR: LatentMove是一种基于DiT的框架，专注于生成高度动态的人体动画，通过条件控制分支和可学习的面部/身体标记提升帧间一致性和细节保留。


<details>
  <summary>Details</summary>
Motivation: 解决现有图像到视频生成方法在处理复杂、非重复性人体动作时的不自然变形问题。

Method: 采用DiT框架，结合条件控制分支和可学习的面部/身体标记，并引入Complex-Human-Videos数据集和两种评估指标。

Result: 实验表明，LatentMove显著提升了人体动画质量，尤其在处理快速复杂动作时表现优异。

Conclusion: LatentMove推动了图像到视频生成技术的边界，代码、数据集和评估指标将开源。

Abstract: Image-to-video (I2V) generation seeks to produce realistic motion sequences
from a single reference image. Although recent methods exhibit strong temporal
consistency, they often struggle when dealing with complex, non-repetitive
human movements, leading to unnatural deformations. To tackle this issue, we
present LatentMove, a DiT-based framework specifically tailored for highly
dynamic human animation. Our architecture incorporates a conditional control
branch and learnable face/body tokens to preserve consistency as well as
fine-grained details across frames. We introduce Complex-Human-Videos (CHV), a
dataset featuring diverse, challenging human motions designed to benchmark the
robustness of I2V systems. We also introduce two metrics to assess the flow and
silhouette consistency of generated videos with their ground truth.
Experimental results indicate that LatentMove substantially improves human
animation quality--particularly when handling rapid, intricate
movements--thereby pushing the boundaries of I2V generation. The code, the CHV
dataset, and the evaluation metrics will be available at https://github.com/
--.

</details>


### [88] [AquaMonitor: A multimodal multi-view image sequence dataset for real-life aquatic invertebrate biodiversity monitoring](https://arxiv.org/abs/2505.22065)
*Mikko Impiö,Philipp M. Rehsen,Tiina Laamanen,Arne J. Beermann,Florian Leese,Jenni Raitoharju*

Main category: cs.CV

TL;DR: AquaMonitor是首个大型水生无脊椎动物计算机视觉数据集，用于评估自动识别方法，包含2.7M图像和多模态数据，定义了三个基准任务。


<details>
  <summary>Details</summary>
Motivation: 现有物种识别数据集缺乏标准化采集协议，且未聚焦水生无脊椎动物，AquaMonitor填补了这一空白。

Method: 通过两年环境监测采集数据，包括图像、DNA序列、干质量和尺寸测量，定义了三个基准任务。

Result: 数据集包含2.7M图像、43,189样本，并提供了多模态数据，为生物多样性监测提供了实用工具。

Conclusion: AquaMonitor为水生生物监测提供了重要资源，其基准任务可直接推动监测技术的进步。

Abstract: This paper presents the AquaMonitor dataset, the first large computer vision
dataset of aquatic invertebrates collected during routine environmental
monitoring. While several large species identification datasets exist, they are
rarely collected using standardized collection protocols, and none focus on
aquatic invertebrates, which are particularly laborious to collect. For
AquaMonitor, we imaged all specimens from two years of monitoring whenever
imaging was possible given practical limitations. The dataset enables the
evaluation of automated identification methods for real-life monitoring
purposes using a realistically challenging and unbiased setup. The dataset has
2.7M images from 43,189 specimens, DNA sequences for 1358 specimens, and dry
mass and size measurements for 1494 specimens, making it also one of the
largest biological multi-view and multimodal datasets to date. We define three
benchmark tasks and provide strong baselines for these: 1) Monitoring
benchmark, reflecting real-life deployment challenges such as open-set
recognition, distribution shift, and extreme class imbalance, 2) Classification
benchmark, which follows a standard fine-grained visual categorization setup,
and 3) Few-shot benchmark, which targets classes with only few training
examples from very fine-grained categories. Advancements on the Monitoring
benchmark can directly translate to improvement of aquatic biodiversity
monitoring, which is an important component of regular legislative water
quality assessment in many countries.

</details>


### [89] [Bringing CLIP to the Clinic: Dynamic Soft Labels and Negation-Aware Learning for Medical Analysis](https://arxiv.org/abs/2505.22079)
*Hanbin Ko,Chang-Min Park*

Main category: cs.CV

TL;DR: 论文提出了一种改进医学视觉语言处理的方法，通过动态软标签和医学图形对齐提升模型对临床语言的理解，并引入否定性硬负样本优化性能。


<details>
  <summary>Details</summary>
Motivation: 通用领域架构（如CLIP）直接应用于医学数据时存在挑战，包括否定处理和数据不平衡问题。

Method: 结合临床增强的动态软标签和医学图形对齐，并引入否定性硬负样本。

Result: 方法在零样本、微调分类和报告检索等任务中达到最优性能，并通过新基准CXR-Align验证了其有效性。

Conclusion: 该方法易于实现且泛化性强，显著提升了医学视觉语言处理能力和临床语言理解。

Abstract: The development of large-scale image-text pair datasets has significantly
advanced self-supervised learning in Vision-Language Processing (VLP). However,
directly applying general-domain architectures such as CLIP to medical data
presents challenges, particularly in handling negations and addressing the
inherent data imbalance of medical datasets. To address these issues, we
propose a novel approach that integrates clinically-enhanced dynamic soft
labels and medical graphical alignment, thereby improving clinical
comprehension and the applicability of contrastive loss in medical contexts.
Furthermore, we introduce negation-based hard negatives to deepen the model's
understanding of the complexities of clinical language. Our approach is easily
integrated into the medical CLIP training pipeline and achieves
state-of-the-art performance across multiple tasks, including zero-shot,
fine-tuned classification, and report retrieval. To comprehensively evaluate
our model's capacity for understanding clinical language, we introduce
CXR-Align, a benchmark uniquely designed to evaluate the understanding of
negation and clinical information within chest X-ray (CXR) datasets.
Experimental results demonstrate that our proposed methods are straightforward
to implement and generalize effectively across contrastive learning frameworks,
enhancing medical VLP capabilities and advancing clinical language
understanding in medical imaging.

</details>


### [90] [MObyGaze: a film dataset of multimodal objectification densely annotated by experts](https://arxiv.org/abs/2505.22084)
*Julie Tores,Elisa Ancarani,Lucile Sassatelli,Hui-Yin Wu,Clement Bergman,Lea Andolfi,Victor Ecrement,Remy Sun,Frederic Precioso,Thierry Devars,Magali Guaresi,Virginie Julliard,Sarah Lecossais*

Main category: cs.CV

TL;DR: 论文提出了一种新的AI任务，通过多模态（视觉、语音、音频）时间模式来量化和表征电影中的物化现象，并发布了MObyGaze数据集。


<details>
  <summary>Details</summary>
Motivation: 研究电影中性别表征差异及其物化现象，以理解刻板印象如何在屏幕上延续。

Method: 结合电影研究和心理学，定义了物化的结构化分类，包含5个子结构和11个概念，覆盖3种模态。构建了MObyGaze数据集，包含20部电影的6072个片段，并标注了物化水平和概念。

Result: 提出了多种学习任务，评估了不同模型在多模态数据上的表现，证明了任务的可行性。

Conclusion: 该研究为多模态物化分析提供了新方法和数据集，推动了相关领域的发展。

Abstract: Characterizing and quantifying gender representation disparities in
audiovisual storytelling contents is necessary to grasp how stereotypes may
perpetuate on screen. In this article, we consider the high-level construct of
objectification and introduce a new AI task to the ML community: characterize
and quantify complex multimodal (visual, speech, audio) temporal patterns
producing objectification in films. Building on film studies and psychology, we
define the construct of objectification in a structured thesaurus involving 5
sub-constructs manifesting through 11 concepts spanning 3 modalities. We
introduce the Multimodal Objectifying Gaze (MObyGaze) dataset, made of 20
movies annotated densely by experts for objectification levels and concepts
over freely delimited segments: it amounts to 6072 segments over 43 hours of
video with fine-grained localization and categorization. We formulate different
learning tasks, propose and investigate best ways to learn from the diversity
of labels among a low number of annotators, and benchmark recent vision, text
and audio models, showing the feasibility of the task. We make our code and our
dataset available to the community and described in the Croissant format:
https://anonymous.4open.science/r/MObyGaze-F600/.

</details>


### [91] [Fast Feature Matching of UAV Images via Matrix Band Reduction-based GPU Data Schedule](https://arxiv.org/abs/2505.22089)
*San Jiang,Kan You,Wanshou Jiang,Qingquan Li*

Main category: cs.CV

TL;DR: 提出了一种基于GPU数据调度算法的高效无人机图像特征匹配方法，通过矩阵带约简（MBR）和GPU加速级联哈希实现。


<details>
  <summary>Details</summary>
Motivation: 特征匹配在运动恢复结构（SfM）中占主导时间成本，需提升无人机图像匹配效率。

Method: 1. 使用图像检索技术选择匹配对；2. 基于MBR生成紧凑图像块；3. GPU加速级联哈希执行特征匹配，结合局部几何约束和RANSAC验证。

Result: 在大型无人机数据集上测试，速度提升77.0至100.0倍，精度与KD-Tree方法相当。

Conclusion: 该算法为无人机图像特征匹配提供了一种高效解决方案。

Abstract: Feature matching dominats the time costs in structure from motion (SfM). The
primary contribution of this study is a GPU data schedule algorithm for
efficient feature matching of Unmanned aerial vehicle (UAV) images. The core
idea is to divide the whole dataset into blocks based on the matrix band
reduction (MBR) and achieve efficient feature matching via GPU-accelerated
cascade hashing. First, match pairs are selected by using an image retrieval
technique, which converts images into global descriptors and searches
high-dimension nearest neighbors with graph indexing. Second, compact image
blocks are iteratively generated from a MBR-based data schedule strategy, which
exploits image connections to avoid redundant data IO (input/output) burden and
increases the usage of GPU computing power. Third, guided by the generated
image blocks, feature matching is executed sequentially within the framework of
GPU-accelerated cascade hashing, and initial candidate matches are refined by
combining a local geometric constraint and RANSAC-based global verification.
For further performance improvement, these two seps are designed to execute
parallelly in GPU and CPU. Finally, the performance of the proposed solution is
evaluated by using large-scale UAV datasets. The results demonstrate that it
increases the efficiency of feature matching with speedup ratios ranging from
77.0 to 100.0 compared with KD-Tree based matching methods, and achieves
comparable accuracy in relative and absolute bundle adjustment (BA). The
proposed algorithm is an efficient solution for feature matching of UAV images.

</details>


### [92] [UAVPairs: A Challenging Benchmark for Match Pair Retrieval of Large-scale UAV Images](https://arxiv.org/abs/2505.22098)
*Junhuan Liu,San Jiang,Wei Ge,Wei Huang,Bingxuan Guo,Qingquan Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为UAVPairs的无人机图像匹配基准数据集和训练流程，通过几何相似性和多场景结构生成训练样本，并设计了排名列表损失以提高检索模型的区分度，实验证明其显著提升了检索精度和3D重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决大规模无人机图像匹配对检索中的训练样本生成成本高和模型区分度不足的问题。

Method: 构建UAVPairs数据集，提出批量非平凡样本挖掘策略和排名列表损失，优化训练流程。

Result: 在三个大规模无人机数据集上的实验表明，该方法显著提高了检索精度和3D模型重建质量。

Conclusion: UAVPairs数据集和训练流程为大规模无人机图像匹配对检索提供了高效解决方案。

Abstract: The primary contribution of this paper is a challenging benchmark dataset,
UAVPairs, and a training pipeline designed for match pair retrieval of
large-scale UAV images. First, the UAVPairs dataset, comprising 21,622
high-resolution images across 30 diverse scenes, is constructed; the 3D points
and tracks generated by SfM-based 3D reconstruction are employed to define the
geometric similarity of image pairs, ensuring genuinely matchable image pairs
are used for training. Second, to solve the problem of expensive mining cost
for global hard negative mining, a batched nontrivial sample mining strategy is
proposed, leveraging the geometric similarity and multi-scene structure of the
UAVPairs to generate training samples as to accelerate training. Third,
recognizing the limitation of pair-based losses, the ranked list loss is
designed to improve the discrimination of image retrieval models, which
optimizes the global similarity structure constructed from the positive set and
negative set. Finally, the effectiveness of the UAVPairs dataset and training
pipeline is validated through comprehensive experiments on three distinct
large-scale UAV datasets. The experiment results demonstrate that models
trained with the UAVPairs dataset and the ranked list loss achieve
significantly improved retrieval accuracy compared to models trained on
existing datasets or with conventional losses. Furthermore, these improvements
translate to enhanced view graph connectivity and higher quality of
reconstructed 3D models. The models trained by the proposed approach perform
more robustly compared with hand-crafted global features, particularly in
challenging repetitively textured scenes and weakly textured scenes. For match
pair retrieval of large-scale UAV images, the trained image retrieval models
offer an effective solution. The dataset would be made publicly available at
https://github.com/json87/UAVPairs.

</details>


### [93] [On the Transferability and Discriminability of Repersentation Learning in Unsupervised Domain Adaptation](https://arxiv.org/abs/2505.22099)
*Wenwen Qiang,Ziyin Gu,Lingyu Si,Jiangmeng Li,Changwen Zheng,Fuchun Sun,Hui Xiong*

Main category: cs.CV

TL;DR: 论文提出了一种新的对抗性无监督域适应（UDA）框架RLGLC，通过结合域对齐和目标域可区分性增强约束，解决了传统方法忽视目标域特征可区分性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统对抗性UDA框架仅依赖分布对齐和源域经验风险最小化，忽视了目标域特征的可区分性，导致性能不佳。

Method: 提出RLGLC框架，结合域对齐目标和可区分性增强约束，利用AR-WWD处理类别不平衡和语义维度加权，并通过局部一致性机制保留目标域细粒度信息。

Result: 在多个基准数据集上的实验表明，RLGLC显著优于现有方法。

Conclusion: 理论分析和实验结果验证了在对抗性UDA中同时保证可迁移性和可区分性的必要性。

Abstract: In this paper, we addressed the limitation of relying solely on distribution
alignment and source-domain empirical risk minimization in Unsupervised Domain
Adaptation (UDA). Our information-theoretic analysis showed that this standard
adversarial-based framework neglects the discriminability of target-domain
features, leading to suboptimal performance. To bridge this
theoretical-practical gap, we defined "good representation learning" as
guaranteeing both transferability and discriminability, and proved that an
additional loss term targeting target-domain discriminability is necessary.
Building on these insights, we proposed a novel adversarial-based UDA framework
that explicitly integrates a domain alignment objective with a
discriminability-enhancing constraint. Instantiated as Domain-Invariant
Representation Learning with Global and Local Consistency (RLGLC), our method
leverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD)
to address class imbalance and semantic dimension weighting, and employs a
local consistency mechanism to preserve fine-grained target-domain
discriminative information. Extensive experiments across multiple benchmark
datasets demonstrate that RLGLC consistently surpasses state-of-the-art
methods, confirming the value of our theoretical perspective and underscoring
the necessity of enforcing both transferability and discriminability in
adversarial-based UDA.

</details>


### [94] [Adapting Segment Anything Model for Power Transmission Corridor Hazard Segmentation](https://arxiv.org/abs/2505.22105)
*Hang Chen,Maoyuan Ye,Peng Yang,Haibin He,Juhua Liu,Bo Du*

Main category: cs.CV

TL;DR: ELE-SAM改进SAM模型，用于电力传输走廊危险分割任务，通过上下文感知提示适配器和高保真掩码解码器提升性能，并构建了ELE-40K数据集。


<details>
  <summary>Details</summary>
Motivation: 解决SAM模型在复杂电力传输走廊场景中难以分割精细结构目标的问题。

Method: 提出上下文感知提示适配器和高保真掩码解码器，结合多粒度掩码特征。

Result: 在ELE-40K数据集上，ELE-SAM比基线模型平均提升16.8% mIoU和20.6% mBIoU。

Conclusion: ELE-SAM在复杂场景中表现优异，为高质量通用目标分割提供了有效方法。

Abstract: Power transmission corridor hazard segmentation (PTCHS) aims to separate
transmission equipment and surrounding hazards from complex background,
conveying great significance to maintaining electric power transmission safety.
Recently, the Segment Anything Model (SAM) has emerged as a foundational vision
model and pushed the boundaries of segmentation tasks. However, SAM struggles
to deal with the target objects in complex transmission corridor scenario,
especially those with fine structure. In this paper, we propose ELE-SAM,
adapting SAM for the PTCHS task. Technically, we develop a Context-Aware Prompt
Adapter to achieve better prompt tokens via incorporating global-local features
and focusing more on key regions. Subsequently, to tackle the hazard objects
with fine structure in complex background, we design a High-Fidelity Mask
Decoder by leveraging multi-granularity mask features and then scaling them to
a higher resolution. Moreover, to train ELE-SAM and advance this field, we
construct the ELE-40K benchmark, the first large-scale and real-world dataset
for PTCHS including 44,094 image-mask pairs. Experimental results for ELE-40K
demonstrate the superior performance that ELE-SAM outperforms the baseline
model with the average 16.8% mIoU and 20.6% mBIoU performance improvement.
Moreover, compared with the state-of-the-art method on HQSeg-44K, the average
2.9% mIoU and 3.8% mBIoU absolute improvements further validate the
effectiveness of our method on high-quality generic object segmentation. The
source code and dataset are available at https://github.com/Hhaizee/ELE-SAM.

</details>


### [95] [Autoregression-free video prediction using diffusion model for mitigating error propagation](https://arxiv.org/abs/2505.22111)
*Woonho Ko,Jin Bok Park,Il Yong Chun*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的无自回归（ARFree）视频预测框架，解决了传统自回归方法在远距离帧预测中的误差传播问题。


<details>
  <summary>Details</summary>
Motivation: 传统自回归视频预测方法在远距离帧预测中存在误差传播问题，限制了预测性能。

Method: ARFree框架直接预测未来帧元组，包含运动预测模块和训练方法，以提升运动连续性和上下文一致性。

Result: 在两个基准数据集上的实验表明，ARFree优于现有最先进的视频预测方法。

Conclusion: ARFree框架通过无自回归机制和扩散模型，显著提升了视频预测的准确性和连续性。

Abstract: Existing long-term video prediction methods often rely on an autoregressive
video prediction mechanism. However, this approach suffers from error
propagation, particularly in distant future frames. To address this limitation,
this paper proposes the first AutoRegression-Free (ARFree) video prediction
framework using diffusion models. Different from an autoregressive video
prediction mechanism, ARFree directly predicts any future frame tuples from the
context frame tuple. The proposed ARFree consists of two key components: 1) a
motion prediction module that predicts a future motion using motion feature
extracted from the context frame tuple; 2) a training method that improves
motion continuity and contextual consistency between adjacent future frame
tuples. Our experiments with two benchmark datasets show that the proposed
ARFree video prediction framework outperforms several state-of-the-art video
prediction methods.

</details>


### [96] [SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model](https://arxiv.org/abs/2505.22126)
*Yifan Chang,Yukang Feng,Jianwen Sun,Jiaxin Ai,Chuanhao Li,S. Kevin Zhou,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 论文介绍了首个科学图表生成基准SridBench，用于评估AI在科学插图生成中的表现，发现现有顶级模型仍落后于人类。


<details>
  <summary>Details</summary>
Motivation: 科学插图生成需要高精度和专业知识，但目前缺乏评估AI在此任务上的基准。

Method: 构建了包含1,120个实例的SridBench基准，涵盖13个学科，每个实例从六个维度评估。

Result: 实验显示，即使是GPT-4o-image等顶级模型在语义保真度和结构准确性上仍不及人类。

Conclusion: 研究强调了需要更先进的推理驱动视觉生成能力。

Abstract: Recent years have seen rapid advances in AI-driven image generation. Early
diffusion models emphasized perceptual quality, while newer multimodal models
like GPT-4o-image integrate high-level reasoning, improving semantic
understanding and structural composition. Scientific illustration generation
exemplifies this evolution: unlike general image synthesis, it demands accurate
interpretation of technical content and transformation of abstract ideas into
clear, standardized visuals. This task is significantly more
knowledge-intensive and laborious, often requiring hours of manual work and
specialized tools. Automating it in a controllable, intelligent manner would
provide substantial practical value. Yet, no benchmark currently exists to
evaluate AI on this front. To fill this gap, we introduce SridBench, the first
benchmark for scientific figure generation. It comprises 1,120 instances
curated from leading scientific papers across 13 natural and computer science
disciplines, collected via human experts and MLLMs. Each sample is evaluated
along six dimensions, including semantic fidelity and structural accuracy.
Experimental results reveal that even top-tier models like GPT-4o-image lag
behind human performance, with common issues in text/visual clarity and
scientific correctness. These findings highlight the need for more advanced
reasoning-driven visual generation capabilities.

</details>


### [97] [Real-Time Blind Defocus Deblurring for Earth Observation: The IMAGIN-e Mission Approach](https://arxiv.org/abs/2505.22128)
*Alejandro D. Mousist*

Main category: cs.CV

TL;DR: 论文提出了一种适用于太空边缘计算的盲去模糊方法，用于修复IMAGIN-e任务中的地球观测图像机械散焦问题。


<details>
  <summary>Details</summary>
Motivation: 解决IMAGIN-e任务中地球观测图像的机械散焦问题，适应太空边缘计算的资源限制。

Method: 利用Sentinel-2数据估计散焦核，并在GAN框架中训练恢复模型，无需参考图像。

Result: 在合成退化的Sentinel-2图像上，SSIM提升72.47%，PSNR提升25.00%；在IMAGIN-e图像上，NIQE和BRISQUE分别提升60.66%和48.38%。

Conclusion: 该方法在IMAGIN-e任务中成功部署，展示了在资源受限的太空环境中高效处理高分辨率图像的潜力。

Abstract: This work addresses mechanical defocus in Earth observation images from the
IMAGIN-e mission aboard the ISS, proposing a blind deblurring approach adapted
to space-based edge computing constraints. Leveraging Sentinel-2 data, our
method estimates the defocus kernel and trains a restoration model within a GAN
framework, effectively operating without reference images.
  On Sentinel-2 images with synthetic degradation, SSIM improved by 72.47% and
PSNR by 25.00%, confirming the model's ability to recover lost details when the
original clean image is known. On IMAGIN-e, where no reference images exist,
perceptual quality metrics indicate a substantial enhancement, with NIQE
improving by 60.66% and BRISQUE by 48.38%, validating real-world onboard
restoration. The approach is currently deployed aboard the IMAGIN-e mission,
demonstrating its practical application in an operational space environment.
  By efficiently handling high-resolution images under edge computing
constraints, the method enables applications such as water body segmentation
and contour detection while maintaining processing viability despite resource
limitations.

</details>


### [98] [What Makes for Text to 360-degree Panorama Generation with Stable Diffusion?](https://arxiv.org/abs/2505.22129)
*Jinhong Ni,Chang-Bin Zhang,Qiang Zhang,Jing Zhang*

Main category: cs.CV

TL;DR: 论文探讨了如何通过微调预训练的扩散模型生成360度全景图像，发现注意力模块中的特定矩阵在适应全景域时起关键作用，并提出了一种高效框架UniPano。


<details>
  <summary>Details</summary>
Motivation: 研究预训练扩散模型在生成全景图像时的适应机制，以解决视角与全景图像之间的领域差距问题。

Method: 分析了注意力模块中不同矩阵的作用，提出UniPano框架，优化了内存和训练时间。

Result: UniPano在性能和效率上优于现有方法，支持更高分辨率的全景生成。

Conclusion: 研究揭示了预训练模型适应全景域的机制，为未来研究提供了简洁的基线。

Abstract: Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion,
has stimulated research to adapt them to 360-degree panorama generation. Prior
work has demonstrated the feasibility of using conventional low-rank adaptation
techniques on pre-trained diffusion models to generate panoramic images.
However, the substantial domain gap between perspective and panoramic images
raises questions about the underlying mechanisms enabling this empirical
success. We hypothesize and examine that the trainable counterparts exhibit
distinct behaviors when fine-tuned on panoramic data, and such an adaptation
conceals some intrinsic mechanism to leverage the prior knowledge within the
pre-trained diffusion models. Our analysis reveals the following: 1) the query
and key matrices in the attention modules are responsible for common
information that can be shared between the panoramic and perspective domains,
thus are less relevant to panorama generation; and 2) the value and output
weight matrices specialize in adapting pre-trained knowledge to the panoramic
domain, playing a more critical role during fine-tuning for panorama
generation. We empirically verify these insights by introducing a simple
framework called UniPano, with the objective of establishing an elegant
baseline for future research. UniPano not only outperforms existing methods but
also significantly reduces memory usage and training time compared to prior
dual-branch approaches, making it scalable for end-to-end panorama generation
with higher resolution. The code will be released.

</details>


### [99] [FaceEditTalker: Interactive Talking Head Generation with Facial Attribute Editing](https://arxiv.org/abs/2505.22141)
*Guanwen Feng,Zhiyuan Ma,Yunan Li,Junwei Jing,Jiahao Yang,Qiguang Miao*

Main category: cs.CV

TL;DR: FaceEditTalker是一个统一的框架，能够在生成高质量音频同步的说话头部视频时，实现可控的面部属性编辑。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动的说话头部生成方法忽视了面部属性编辑的重要性，而这一能力对个性化、品牌适配等应用至关重要。

Method: 方法包括图像特征空间编辑模块（提取语义和细节特征）和音频驱动视频生成模块（融合编辑特征与音频引导的面部标志）。

Result: 实验表明，该方法在唇同步精度、视频质量和属性可控性上优于现有技术。

Conclusion: FaceEditTalker通过统一框架实现了高质量、可控的面部属性编辑和视频生成。

Abstract: Recent advances in audio-driven talking head generation have achieved
impressive results in lip synchronization and emotional expression. However,
they largely overlook the crucial task of facial attribute editing. This
capability is crucial for achieving deep personalization and expanding the
range of practical applications, including user-tailored digital avatars,
engaging online education content, and brand-specific digital customer service.
In these key domains, the flexible adjustment of visual attributes-such as
hairstyle, accessories, and subtle facial features is essential for aligning
with user preferences, reflecting diverse brand identities, and adapting to
varying contextual demands. In this paper, we present FaceEditTalker, a unified
framework that enables controllable facial attribute manipulation while
generating high-quality, audio-synchronized talking head videos. Our method
consists of two key components: an image feature space editing module, which
extracts semantic and detail features and allows flexible control over
attributes like expression, hairstyle, and accessories; and an audio-driven
video generation module, which fuses these edited features with audio-guided
facial landmarks to drive a diffusion-based generator. This design ensures
temporal coherence, visual fidelity, and identity preservation across frames.
Extensive experiments on public datasets demonstrate that our method
outperforms state-of-the-art approaches in lip-sync accuracy, video quality,
and attribute controllability. Project page:
https://peterfanfan.github.io/FaceEditTalker/

</details>


### [100] [3D Question Answering via only 2D Vision-Language Models](https://arxiv.org/abs/2505.22143)
*Fengyun Wang,Sicheng Yu,Jiawei Wu,Jinhui Tang,Hanwang Zhang,Qianru Sun*

Main category: cs.CV

TL;DR: 论文提出cdViews方法，通过自动选择关键且多样的2D视图，利用2D视觉语言模型（LVLMs）在零样本条件下完成3D问答任务，无需训练3D模型。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用大型视觉语言模型（LVLMs）解决3D场景理解任务，尤其是3D问答（3D-QA），避免资源密集型的3D模型训练。

Method: 提出cdViews方法，包括viewSelector（选择关键视图）和viewNMS（增强视图多样性），通过采样2D视图并输入2D模型完成3D-QA。

Result: 在ScanQA和SQA基准测试中达到最先进性能，证明2D LVLMs是解决3D任务的有效替代方案。

Conclusion: 2D LVLMs是目前解决3D任务的最有效替代方案，无需训练3D模型。

Abstract: Large vision-language models (LVLMs) have significantly advanced numerous
fields. In this work, we explore how to harness their potential to address 3D
scene understanding tasks, using 3D question answering (3D-QA) as a
representative example. Due to the limited training data in 3D, we do not train
LVLMs but infer in a zero-shot manner. Specifically, we sample 2D views from a
3D point cloud and feed them into 2D models to answer a given question. When
the 2D model is chosen, e.g., LLAVA-OV, the quality of sampled views matters
the most. We propose cdViews, a novel approach to automatically selecting
critical and diverse Views for 3D-QA. cdViews consists of two key components:
viewSelector prioritizing critical views based on their potential to provide
answer-specific information, and viewNMS enhancing diversity by removing
redundant views based on spatial overlap. We evaluate cdViews on the
widely-used ScanQA and SQA benchmarks, demonstrating that it achieves
state-of-the-art performance in 3D-QA while relying solely on 2D models without
fine-tuning. These findings support our belief that 2D LVLMs are currently the
most effective alternative (of the resource-intensive 3D LVLMs) for addressing
3D tasks.

</details>


### [101] [Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language](https://arxiv.org/abs/2505.22146)
*Guangfu Hao,Haojie Wen,Liangxuna Guo,Yang Chen,Yanchao Bi,Shan Yu*

Main category: cs.CV

TL;DR: 论文提出了一种基于低维属性表示的框架，用于连接视觉工具感知和语言任务理解，显著提升了工具选择任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 人类在灵活工具选择方面具有独特的认知能力，但现有计算模型对此能力的研究不足。

Method: 使用视觉编码器（ResNet或ViT）从工具图像中提取属性，并结合微调的语言模型（如GPT-2、LLaMA、DeepSeek）从任务描述中推导所需属性。

Result: 该方法在工具选择任务中达到74%的准确率，显著优于直接工具匹配（20%）和其他小型多模态模型（21%-58%），且接近更大模型（如GPT-4o）的性能。

Conclusion: 研究提供了一种参数高效且可解释的解决方案，模拟了人类工具认知，推动了认知科学和实际应用的发展。

Abstract: Flexible tool selection reflects a complex cognitive ability that
distinguishes humans from other species, yet computational models that capture
this ability remain underdeveloped. We developed a framework using
low-dimensional attribute representations to bridge visual tool perception and
linguistic task understanding. We constructed a comprehensive dataset (ToolNet)
containing 115 common tools labeled with 13 carefully designed attributes
spanning physical, functional, and psychological properties, paired with
natural language scenarios describing tool usage. Visual encoders (ResNet or
ViT) extract attributes from tool images while fine-tuned language models
(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our
approach achieves 74% accuracy in tool selection tasks-significantly
outperforming direct tool matching (20%) and smaller multimodal models
(21%-58%), while approaching performance of much larger models like GPT-4o
(73%) with substantially fewer parameters. Ablation studies revealed that
manipulation-related attributes (graspability, hand-relatedness, elongation)
consistently prove most critical across modalities. This work provides a
parameter-efficient, interpretable solution that mimics human-like tool
cognition, advancing both cognitive science understanding and practical
applications in tool selection tasks.

</details>


### [102] [Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging](https://arxiv.org/abs/2505.22150)
*Runze Xia,Shuo Feng,Renzhi Wang,Congchi Yin,Xuyun Wen,Piji Li*

Main category: cs.CV

TL;DR: FgB2I方法通过细粒度文本作为桥梁，提升脑到图像重建的细节和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 现有脑到图像重建方法常因语义信息不足导致细节缺失和语义不一致。

Method: FgB2I包含三个阶段：细节增强、解码细粒度文本描述、基于文本的脑到图像重建，并引入三种奖励指标指导语言模型。

Result: 细粒度文本描述可整合到现有重建方法中，实现更精细的脑到图像重建。

Conclusion: FgB2I通过细粒度文本显著提升了重建图像的细节和语义一致性。

Abstract: Brain-to-Image reconstruction aims to recover visual stimuli perceived by
humans from brain activity. However, the reconstructed visual stimuli often
missing details and semantic inconsistencies, which may be attributed to
insufficient semantic information. To address this issue, we propose an
approach named Fine-grained Brain-to-Image reconstruction (FgB2I), which
employs fine-grained text as bridge to improve image reconstruction. FgB2I
comprises three key stages: detail enhancement, decoding fine-grained text
descriptions, and text-bridged brain-to-image reconstruction. In the
detail-enhancement stage, we leverage large vision-language models to generate
fine-grained captions for visual stimuli and experimentally validate its
importance. We propose three reward metrics (object accuracy, text-image
semantic similarity, and image-image semantic similarity) to guide the language
model in decoding fine-grained text descriptions from fMRI signals. The
fine-grained text descriptions can be integrated into existing reconstruction
methods to achieve fine-grained Brain-to-Image reconstruction.

</details>


### [103] [Learning A Robust RGB-Thermal Detector for Extreme Modality Imbalance](https://arxiv.org/abs/2505.22154)
*Chao Tian,Chao Yang,Guoqing Zhu,Qiang Wang,Zhenyu He*

Main category: cs.CV

TL;DR: 提出了一种新的RGB-T目标检测架构，通过模态交互模块和伪退化训练解决极端模态不平衡问题，显著降低了缺失率。


<details>
  <summary>Details</summary>
Motivation: 现实场景中RGB-T数据可能因环境或技术问题出现模态退化，导致训练和测试时的分布不一致，传统方法难以应对。

Method: 设计了基础-辅助检测器架构，引入模态交互模块动态加权模态，并通过伪退化模拟真实不平衡数据。

Result: 实验显示方法有效降低缺失率55%，并在多种基线检测器上提升性能。

Conclusion: 该方法显著提升了RGB-T检测在极端模态不平衡下的鲁棒性和性能。

Abstract: RGB-Thermal (RGB-T) object detection utilizes thermal infrared (TIR) images
to complement RGB data, improving robustness in challenging conditions.
Traditional RGB-T detectors assume balanced training data, where both
modalities contribute equally. However, in real-world scenarios, modality
degradation-due to environmental factors or technical issues-can lead to
extreme modality imbalance, causing out-of-distribution (OOD) issues during
testing and disrupting model convergence during training. This paper addresses
these challenges by proposing a novel base-and-auxiliary detector architecture.
We introduce a modality interaction module to adaptively weigh modalities based
on their quality and handle imbalanced samples effectively. Additionally, we
leverage modality pseudo-degradation to simulate real-world imbalances in
training data. The base detector, trained on high-quality pairs, provides a
consistency constraint for the auxiliary detector, which receives degraded
samples. This framework enhances model robustness, ensuring reliable
performance even under severe modality degradation. Experimental results
demonstrate the effectiveness of our method in handling extreme modality
imbalances~(decreasing the Missing Rate by 55%) and improving performance
across various baseline detectors.

</details>


### [104] [Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers](https://arxiv.org/abs/2505.22167)
*Weilun Feng,Chuanguang Yang,Haotong Qin,Xiangqi Li,Yu Wang,Zhulin An,Libo Huang,Boyu Diao,Zixiang Zhao,Yongjun Xu,Michele Magno*

Main category: cs.CV

TL;DR: Q-VDiT是一种专为视频DiT模型设计的量化框架，通过Token-aware Quantization Estimator（TQE）和Temporal Maintenance Distillation（TMD）解决量化中的信息丢失和优化目标与视频生成需求不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法在视频生成任务中表现不佳，主要由于量化过程中的信息丢失和优化目标与视频生成需求的脱节。

Method: 提出TQE补偿量化误差，TMD保持帧间时空相关性并优化整体视频上下文。

Result: W3A6 Q-VDiT在场景一致性上达到23.40，比现有量化方法性能提升1.9倍。

Conclusion: Q-VDiT为视频DiT模型的量化提供了高效解决方案，显著提升了性能。

Abstract: Diffusion transformers (DiT) have demonstrated exceptional performance in
video generation. However, their large number of parameters and high
computational complexity limit their deployment on edge devices. Quantization
can reduce storage requirements and accelerate inference by lowering the
bit-width of model parameters. Yet, existing quantization methods for image
generation models do not generalize well to video generation tasks. We identify
two primary challenges: the loss of information during quantization and the
misalignment between optimization objectives and the unique requirements of
video generation. To address these challenges, we present Q-VDiT, a
quantization framework specifically designed for video DiT models. From the
quantization perspective, we propose the Token-aware Quantization Estimator
(TQE), which compensates for quantization errors in both the token and feature
dimensions. From the optimization perspective, we introduce Temporal
Maintenance Distillation (TMD), which preserves the spatiotemporal correlations
between frames and enables the optimization of each frame with respect to the
overall video context. Our W3A6 Q-VDiT achieves a scene consistency of 23.40,
setting a new benchmark and outperforming current state-of-the-art quantization
methods by 1.9$\times$. Code will be available at
https://github.com/cantbebetter2/Q-VDiT.

</details>


### [105] [S2AFormer: Strip Self-Attention for Efficient Vision Transformer](https://arxiv.org/abs/2505.22195)
*Guoan Xu,Wenfeng Huang,Wenjing Jia,Jiamao Li,Guangwei Gao,Guo-Jun Qi*

Main category: cs.CV

TL;DR: S2AFormer是一种高效的Vision Transformer架构，通过Strip Self-Attention（SSA）和Hybrid Perception Blocks（HPBs）结合CNN的局部感知与Transformer的全局建模能力，显著降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: Vision Transformer（ViT）在处理全局依赖时计算需求呈二次增长，限制了其实用效率。现有方法虽结合卷积与自注意力，但仍存在计算瓶颈。

Method: 提出S2AFormer，采用SSA减少空间和通道维度计算，同时设计HPBs整合CNN与Transformer的优势。

Result: 在ImageNet-1k、ADE20k和COCO等基准测试中，S2AFormer在保持精度的同时显著提升了效率。

Conclusion: S2AFormer在效率和效果间取得了最佳平衡，是高效视觉Transformer的有力候选。

Abstract: Vision Transformer (ViT) has made significant advancements in computer
vision, thanks to its token mixer's sophisticated ability to capture global
dependencies between all tokens. However, the quadratic growth in computational
demands as the number of tokens increases limits its practical efficiency.
Although recent methods have combined the strengths of convolutions and
self-attention to achieve better trade-offs, the expensive pairwise token
affinity and complex matrix operations inherent in self-attention remain a
bottleneck. To address this challenge, we propose S2AFormer, an efficient
Vision Transformer architecture featuring novel Strip Self-Attention (SSA). We
design simple yet effective Hybrid Perception Blocks (HPBs) to effectively
integrate the local perception capabilities of CNNs with the global context
modeling of Transformer's attention mechanisms. A key innovation of SSA lies in
its reducing the spatial dimensions of $K$ and $V$ while compressing the
channel dimensions of $Q$ and $K$. This design significantly reduces
computational overhead while preserving accuracy, striking an optimal balance
between efficiency and effectiveness. We evaluate the robustness and efficiency
of S2AFormer through extensive experiments on multiple vision benchmarks,
including ImageNet-1k for image classification, ADE20k for semantic
segmentation, and COCO for object detection and instance segmentation. Results
demonstrate that S2AFormer achieves significant accuracy gains with superior
efficiency in both GPU and non-GPU environments, making it a strong candidate
for efficient vision Transformers.

</details>


### [106] [Investigating Mechanisms for In-Context Vision Language Binding](https://arxiv.org/abs/2505.22200)
*Darshana Saravanan,Makarand Tapaswi,Vineet Gandhi*

Main category: cs.CV

TL;DR: 研究探讨了视觉语言模型（VLMs）中图像与文本绑定的机制，通过合成数据集验证了模型为图像和文本分配独特的Binding ID以实现关联。


<details>
  <summary>Details</summary>
Motivation: 理解VLMs如何通过Binding ID机制在图像和文本之间建立关联，以提升模型的多模态理解能力。

Method: 使用合成数据集和任务，要求模型将图像中的3D对象与文本描述关联，分析模型激活中的Binding ID机制。

Result: 实验表明，VLMs为图像和文本分配独特的Binding ID，实现了上下文内的关联。

Conclusion: Binding ID机制在VLMs中有效支持图像与文本的绑定，为多模态理解提供了新视角。

Abstract: To understand a prompt, Vision-Language models (VLMs) must perceive the
image, comprehend the text, and build associations within and across both
modalities. For instance, given an 'image of a red toy car', the model should
associate this image to phrases like 'car', 'red toy', 'red object', etc. Feng
and Steinhardt propose the Binding ID mechanism in LLMs, suggesting that the
entity and its corresponding attribute tokens share a Binding ID in the model
activations. We investigate this for image-text binding in VLMs using a
synthetic dataset and task that requires models to associate 3D objects in an
image with their descriptions in the text. Our experiments demonstrate that
VLMs assign a distinct Binding ID to an object's image tokens and its textual
references, enabling in-context association.

</details>


### [107] [A Survey on Training-free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2505.22209)
*Naomi Kombol,Ivan Martinović,Siniša Šegvić*

Main category: cs.CV

TL;DR: 该论文综述了无需训练的开放词汇语义分割方法，利用现有多模态分类模型，覆盖了历史、方法、最新进展及未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统语义分割方法需要大量计算资源和标注数据，而开放词汇分割要求模型分类超出学习类别，数据标注成本高，因此转向无需训练的方法。

Method: 综述了30多种方法，分为纯CLIP、辅助视觉基础模型和生成方法三类，并讨论了任务定义和模型类型。

Result: 总结了当前研究的局限性和潜在问题，并提出了未来研究方向。

Conclusion: 该综述为新研究者提供了入门指南，并有望激发对该领域的进一步兴趣。

Abstract: Semantic segmentation is one of the most fundamental tasks in image
understanding with a long history of research, and subsequently a myriad of
different approaches. Traditional methods strive to train models up from
scratch, requiring vast amounts of computational resources and training data.
In the advent of moving to open-vocabulary semantic segmentation, which asks
models to classify beyond learned categories, large quantities of finely
annotated data would be prohibitively expensive. Researchers have instead
turned to training-free methods where they leverage existing models made for
tasks where data is more easily acquired. Specifically, this survey will cover
the history, nuance, idea development and the state-of-the-art in training-free
open-vocabulary semantic segmentation that leverages existing multi-modal
classification models. We will first give a preliminary on the task definition
followed by an overview of popular model archetypes and then spotlight over 30
approaches split into broader research branches: purely CLIP-based, those
leveraging auxiliary visual foundation models and ones relying on generative
methods. Subsequently, we will discuss the limitations and potential problems
of current research, as well as provide some underexplored ideas for future
study. We believe this survey will serve as a good onboarding read to new
researchers and spark increased interest in the area.

</details>


### [108] [Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation](https://arxiv.org/abs/2505.22222)
*Yunsoo Kim,Jinge Wu,Su-Hwan Kim,Pardeep Vasudev,Jiashu Shen,Honghan Wu*

Main category: cs.CV

TL;DR: 提出了一种名为Look & Mark（L&M）的新方法，通过结合放射科医生的注视点和标注框，显著提升了多模态大语言模型在医学图像分析中的性能，减少了临床错误。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在医学图像分析中存在幻觉和临床错误，限制了其在实际应用中的可靠性。

Method: 提出L&M方法，结合放射科医生的注视点（Look）和标注框（Mark），通过上下文学习提升模型性能，无需重新训练。

Result: L&M显著提升了模型性能，例如CXR-LLaVA的A.AVG指标提升1.2%，LLaVA-Med提升9.2%，并减少了临床错误（每报告平均减少0.43个错误）。

Conclusion: L&M是一种可扩展且高效的解决方案，有望改善低资源临床环境中的诊断流程。

Abstract: Recent advancements in multimodal Large Language Models (LLMs) have
significantly enhanced the automation of medical image analysis, particularly
in generating radiology reports from chest X-rays (CXR). However, these models
still suffer from hallucinations and clinically significant errors, limiting
their reliability in real-world applications. In this study, we propose Look &
Mark (L&M), a novel grounding fixation strategy that integrates radiologist eye
fixations (Look) and bounding box annotations (Mark) into the LLM prompting
framework. Unlike conventional fine-tuning, L&M leverages in-context learning
to achieve substantial performance gains without retraining. When evaluated
across multiple domain-specific and general-purpose models, L&M demonstrates
significant gains, including a 1.2% improvement in overall metrics (A.AVG) for
CXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for
LLaVA-Med. General-purpose models also benefit from L&M combined with
in-context learning, with LLaVA-OV achieving an 87.3% clinical average
performance (C.AVG)-the highest among all models, even surpassing those
explicitly trained for CXR report generation. Expert evaluations further
confirm that L&M reduces clinically significant errors (by 0.43 average errors
per report), such as false predictions and omissions, enhancing both accuracy
and reliability. These findings highlight L&M's potential as a scalable and
efficient solution for AI-assisted radiology, paving the way for improved
diagnostic workflows in low-resource clinical settings.

</details>


### [109] [Hadaptive-Net: Efficient Vision Models via Adaptive Cross-Hadamard Synergy](https://arxiv.org/abs/2505.22226)
*Xuyang Zhang,Xi Zhang,Liang Chen,Hao Shi,Qingshan Guo*

Main category: cs.CV

TL;DR: 本文提出了一种基于Hadamard乘积的自适应模块（ACH）和轻量级网络Hadaptive-Net，显著提升了视觉任务中推理速度与精度的平衡。


<details>
  <summary>Details</summary>
Motivation: 尽管Hadamard乘积在增强网络表示能力和维度压缩方面具有潜力，但其实际应用尚未充分开发。本文旨在系统探索并有效应用这一技术。

Method: 分析了Hadamard乘积在跨通道交互和通道扩展中的优势，提出自适应跨Hadamard模块（ACH），并构建轻量级网络Hadaptive-Net。

Result: 实验表明，Hadaptive-Net通过ACH模块实现了推理速度和精度的前所未有的平衡。

Conclusion: 本文成功将Hadamard乘积技术应用于实际网络设计，为视觉任务提供了高效解决方案。

Abstract: Recent studies have revealed the immense potential of Hadamard product in
enhancing network representational capacity and dimensional compression.
However, despite its theoretical promise, this technique has not been
systematically explored or effectively applied in practice, leaving its full
capabilities underdeveloped. In this work, we first analyze and identify the
advantages of Hadamard product over standard convolutional operations in
cross-channel interaction and channel expansion. Building upon these insights,
we propose a computationally efficient module: Adaptive Cross-Hadamard (ACH),
which leverages adaptive cross-channel Hadamard products for high-dimensional
channel expansion. Furthermore, we introduce Hadaptive-Net (Hadamard Adaptive
Network), a lightweight network backbone for visual tasks, which is
demonstrated through experiments that it achieves an unprecedented balance
between inference speed and accuracy through our proposed module.

</details>


### [110] [GoMatching++: Parameter- and Data-Efficient Arbitrary-Shaped Video Text Spotting and Benchmarking](https://arxiv.org/abs/2505.22228)
*Haibin He,Jing Zhang,Maoyuan Ye,Juhua Liu,Bo Du,Dacheng Tao*

Main category: cs.CV

TL;DR: GoMatching++ 是一种参数和数据高效的方法，将现成的图像文本检测器转化为视频文本检测器，通过冻结图像检测器并引入轻量级跟踪器，显著提升了视频文本检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频文本检测方法在识别能力上存在局限，即使经过端到端训练仍表现不佳。

Method: 提出 GoMatching++，包括重新评分机制和 LST-Matcher，优化图像检测器在视频中的表现。

Result: 在多个基准测试中创下新记录，并显著降低训练成本。同时引入 ArTVideo 新数据集。

Conclusion: GoMatching++ 和 ArTVideo 数据集将推动视频文本检测的未来发展。

Abstract: Video text spotting (VTS) extends image text spotting (ITS) by adding text
tracking, significantly increasing task complexity. Despite progress in VTS,
existing methods still fall short of the performance seen in ITS. This paper
identifies a key limitation in current video text spotters: limited recognition
capability, even after extensive end-to-end training. To address this, we
propose GoMatching++, a parameter- and data-efficient method that transforms an
off-the-shelf image text spotter into a video specialist. The core idea lies in
freezing the image text spotter and introducing a lightweight, trainable
tracker, which can be optimized efficiently with minimal training data. Our
approach includes two key components: (1) a rescoring mechanism to bridge the
domain gap between image and video data, and (2) the LST-Matcher, which
enhances the frozen image text spotter's ability to handle video text. We
explore various architectures for LST-Matcher to ensure efficiency in both
parameters and training data. As a result, GoMatching++ sets new performance
records on challenging benchmarks such as ICDAR15-video, DSText, and BOVText,
while significantly reducing training costs. To address the lack of curved text
datasets in VTS, we introduce ArTVideo, a new benchmark featuring over 30%
curved text with detailed annotations. We also provide a comprehensive
statistical analysis and experimental results for ArTVideo. We believe that
GoMatching++ and the ArTVideo benchmark will drive future advancements in video
text spotting. The source code, models and dataset are publicly available at
https://github.com/Hxyz-123/GoMatching.

</details>


### [111] [Enjoying Information Dividend: Gaze Track-based Medical Weakly Supervised Segmentation](https://arxiv.org/abs/2505.22230)
*Zhisong Wang,Yiwen Ye,Ziyang Chen,Yong Xia*

Main category: cs.CV

TL;DR: GradTrack利用医生的注视轨迹（包括注视点、持续时间和时间顺序）提升弱监督语义分割（WSSS）性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像中弱监督语义分割难以有效利用稀疏标注，现有基于注视的方法未充分利用注视数据中的丰富信息。

Method: GradTrack包含注视轨迹图生成和轨迹注意力两个组件，通过多级注视监督在解码过程中逐步优化特征。

Result: 在Kvasir-SEG和NCI-ISBI数据集上，GradTrack的Dice分数分别提高了3.21%和2.61%，显著缩小了与全监督模型的差距。

Conclusion: GradTrack通过充分利用注视数据，显著提升了弱监督语义分割的性能。

Abstract: Weakly supervised semantic segmentation (WSSS) in medical imaging struggles
with effectively using sparse annotations. One promising direction for WSSS
leverages gaze annotations, captured via eye trackers that record regions of
interest during diagnostic procedures. However, existing gaze-based methods,
such as GazeMedSeg, do not fully exploit the rich information embedded in gaze
data. In this paper, we propose GradTrack, a framework that utilizes
physicians' gaze track, including fixation points, durations, and temporal
order, to enhance WSSS performance. GradTrack comprises two key components:
Gaze Track Map Generation and Track Attention, which collaboratively enable
progressive feature refinement through multi-level gaze supervision during the
decoding process. Experiments on the Kvasir-SEG and NCI-ISBI datasets
demonstrate that GradTrack consistently outperforms existing gaze-based
methods, achieving Dice score improvements of 3.21\% and 2.61\%, respectively.
Moreover, GradTrack significantly narrows the performance gap with fully
supervised models such as nnUNet.

</details>


### [112] [StateSpaceDiffuser: Bringing Long Context to Diffusion World Models](https://arxiv.org/abs/2505.22246)
*Nedko Savov,Naser Kazemi,Deheng Zhang,Danda Pani Paudel,Xi Wang,Luc Van Gool*

Main category: cs.CV

TL;DR: StateSpaceDiffuser结合状态空间模型（Mamba）与扩散模型，解决了现有世界模型因缺乏长期环境状态而导致的视觉一致性崩溃问题，显著提升了长上下文任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型因依赖短序列观测而无法保持长期视觉一致性，导致场景生成失去上下文信息。

Method: 通过整合状态空间模型的序列表示（Mamba）到扩散模型中，设计StateSpaceDiffuser，以恢复长期记忆并保持高保真合成。

Result: StateSpaceDiffuser在2D迷宫导航和复杂3D环境中均显著优于纯扩散模型，能保持视觉一致性达更多步数。

Conclusion: 将状态空间表示引入扩散模型能有效兼顾视觉细节与长期记忆，为世界模型提供新方向。

Abstract: World models have recently become promising tools for predicting realistic
visuals based on actions in complex environments. However, their reliance on a
short sequence of observations causes them to quickly lose track of context. As
a result, visual consistency breaks down after just a few steps, and generated
scenes no longer reflect information seen earlier. This limitation of the
state-of-the-art diffusion-based world models comes from their lack of a
lasting environment state. To address this problem, we introduce
StateSpaceDiffuser, where a diffusion model is enabled to perform on
long-context tasks by integrating a sequence representation from a state-space
model (Mamba), representing the entire interaction history. This design
restores long-term memory without sacrificing the high-fidelity synthesis of
diffusion models. To rigorously measure temporal consistency, we develop an
evaluation protocol that probes a model's ability to reinstantiate seen content
in extended rollouts. Comprehensive experiments show that StateSpaceDiffuser
significantly outperforms a strong diffusion-only baseline, maintaining a
coherent visual context for an order of magnitude more steps. It delivers
consistent views in both a 2D maze navigation and a complex 3D environment.
These results establish that bringing state-space representations into
diffusion models is highly effective in demonstrating both visual details and
long-term memory.

</details>


### [113] [YH-MINER: Multimodal Intelligent System for Natural Ecological Reef Metric Extraction](https://arxiv.org/abs/2505.22250)
*Mingzhuang Wang,Yvyang Li,Xiyang Zhang,Fei Tan,Qi Shi,Guotao Zhang,Siqi Chen,Yufei Liu,Lei Lei,Ming Zhou,Qiang Lin,Hongqiang Yang*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态大模型（MLLM）的YH-OSI系统，用于珊瑚礁生态监测，结合目标检测、语义分割和先验输入，提高了分类准确性和生态指标提取效率。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁生态监测面临人工分析效率低和复杂水下场景分割精度不足的挑战，亟需智能化解决方案。

Method: 开发了YH-OSI系统，利用目标检测模块生成空间先验框，驱动分割模块完成像素级分割，并通过多模态模型实现分类和生态指标提取。

Result: 系统在低光和密集遮挡场景下表现出色，分类准确率达88%，并能提取核心生态指标。

Conclusion: YH-OSI系统为珊瑚礁生态监测提供了高效自动化解决方案，并具备未来集成到水下机器人中的潜力。

Abstract: Coral reefs, crucial for sustaining marine biodiversity and ecological
processes (e.g., nutrient cycling, habitat provision), face escalating threats,
underscoring the need for efficient monitoring. Coral reef ecological
monitoring faces dual challenges of low efficiency in manual analysis and
insufficient segmentation accuracy in complex underwater scenarios. This study
develops the YH-OSI system, establishing an intelligent framework centered on
the Multimodal Large Model (MLLM) for "object detection-semantic
segmentation-prior input". The system uses the object detection module
(mAP@0.5=0.78) to generate spatial prior boxes for coral instances, driving the
segment module to complete pixel-level segmentation in low-light and densely
occluded scenarios. The segmentation masks and finetuned classification
instructions are fed into the Qwen2-VL-based multimodal model as prior inputs,
achieving a genus-level classification accuracy of 88% and simultaneously
extracting core ecological metrics. Meanwhile, the system retains the
scalability of the multimodal model through standardized interfaces, laying a
foundation for future integration into multimodal agent-based underwater robots
and supporting the full-process automation of "image acquisition-prior
generation-real-time analysis."

</details>


### [114] [Domain Adaptation of Attention Heads for Zero-shot Anomaly Detection](https://arxiv.org/abs/2505.22259)
*Kiyoon Jeong,Jaehyuk Heo,Junyeong Son,Pilsung Kang*

Main category: cs.CV

TL;DR: HeadCLIP是一种零样本异常检测方法，通过自适应文本和图像编码器，结合可学习提示和动态头权重，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本异常检测方法在领域适应方面存在不足，HeadCLIP旨在通过全面适应文本和图像编码器来解决这一问题。

Method: HeadCLIP通过可学习提示调整文本编码器，引入动态头权重优化图像编码器，并结合像素级信息计算联合异常分数。

Result: 在工业和医疗领域的数据集上，HeadCLIP在像素和图像级别的异常检测得分均优于现有方法，最高提升4.9%p。

Conclusion: HeadCLIP通过全面领域适应，显著提升了零样本异常检测的性能，适用于多种实际场景。

Abstract: Zero-shot anomaly detection (ZSAD) in images is an approach that can detect
anomalies without access to normal samples, which can be beneficial in various
realistic scenarios where model training is not possible. However, existing
ZSAD research has shown limitations by either not considering domain adaptation
of general-purpose backbone models to anomaly detection domains or by
implementing only partial adaptation to some model components. In this paper,
we propose HeadCLIP to overcome these limitations by effectively adapting both
text and image encoders to the domain. HeadCLIP generalizes the concepts of
normality and abnormality through learnable prompts in the text encoder, and
introduces learnable head weights to the image encoder to dynamically adjust
the features held by each attention head according to domain characteristics.
Additionally, we maximize the effect of domain adaptation by introducing a
joint anomaly score that utilizes domain-adapted pixel-level information for
image-level anomaly detection. Experimental results using multiple real
datasets in both industrial and medical domains show that HeadCLIP outperforms
existing ZSAD techniques at both pixel and image levels. In the industrial
domain, improvements of up to 4.9%p in pixel-level mean anomaly detection score
(mAD) and up to 3.0%p in image-level mAD were achieved, with similar
improvements (3.2%p, 3.1%p) in the medical domain.

</details>


### [115] [Learning Fine-Grained Geometry for Sparse-View Splatting via Cascade Depth Loss](https://arxiv.org/abs/2505.22279)
*Wenjun Lu,Haodong Chen,Anqi Yi,Yuk Ying Chung,Zhiyong Wang,Kun Hu*

Main category: cs.CV

TL;DR: 论文提出了一种名为HDGS的深度监督框架，通过多尺度深度一致性提升稀疏视图下的新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图条件下，现有方法（如NeRF和3DGS）因几何线索不足导致重建质量下降，模糊细节和结构伪影问题显著。

Method: 引入HDGS框架，采用渐进式几何细化策略，并提出Cascade Pearson Correlation Loss（CPCL）实现多尺度深度对齐。

Result: 在LLFF和DTU基准测试中，HDGS在稀疏视图下实现了最先进的性能，同时保持高效和高渲染质量。

Conclusion: HDGS通过多尺度深度一致性显著提升了稀疏视图下的结构保真度，为新视角合成任务提供了有效解决方案。

Abstract: Novel view synthesis is a fundamental task in 3D computer vision that aims to
reconstruct realistic images from a set of posed input views. However,
reconstruction quality degrades significantly under sparse-view conditions due
to limited geometric cues. Existing methods, such as Neural Radiance Fields
(NeRF) and the more recent 3D Gaussian Splatting (3DGS), often suffer from
blurred details and structural artifacts when trained with insufficient views.
Recent works have identified the quality of rendered depth as a key factor in
mitigating these artifacts, as it directly affects geometric accuracy and view
consistency. In this paper, we address these challenges by introducing
Hierarchical Depth-Guided Splatting (HDGS), a depth supervision framework that
progressively refines geometry from coarse to fine levels. Central to HDGS is a
novel Cascade Pearson Correlation Loss (CPCL), which aligns rendered and
estimated monocular depths across multiple spatial scales. By enforcing
multi-scale depth consistency, our method substantially improves structural
fidelity in sparse-view scenarios. Extensive experiments on the LLFF and DTU
benchmarks demonstrate that HDGS achieves state-of-the-art performance under
sparse-view settings while maintaining efficient and high-quality rendering

</details>


### [116] [From Controlled Scenarios to Real-World: Cross-Domain Degradation Pattern Matching for All-in-One Image Restoration](https://arxiv.org/abs/2505.22284)
*Junyu Fan,Chuanlin Liao,Yi Lin*

Main category: cs.CV

TL;DR: 论文提出了一种统一的域自适应图像修复（UDAIR）框架，通过从源域到目标域的知识迁移，解决了多退化模式图像修复中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在封闭场景下表现良好，但在真实场景中因数据分布差异导致性能下降，需提升退化识别能力。

Method: 设计了代码本学习离散嵌入表示退化模式，引入跨样本对比学习机制，并提出域适应策略和测试时适应机制。

Result: 在10个开源数据集上验证了UDAIR的优越性，展示了在未知条件下的退化识别和真实场景的鲁棒泛化能力。

Conclusion: UDAIR框架在多退化图像修复任务中实现了最先进性能，并验证了其在真实场景中的有效性。

Abstract: As a fundamental imaging task, All-in-One Image Restoration (AiOIR) aims to
achieve image restoration caused by multiple degradation patterns via a single
model with unified parameters. Although existing AiOIR approaches obtain
promising performance in closed and controlled scenarios, they still suffered
from considerable performance reduction in real-world scenarios since the gap
of data distributions between the training samples (source domain) and
real-world test samples (target domain) can lead inferior degradation awareness
ability. To address this issue, a Unified Domain-Adaptive Image Restoration
(UDAIR) framework is proposed to effectively achieve AiOIR by leveraging the
learned knowledge from source domain to target domain. To improve the
degradation identification, a codebook is designed to learn a group of discrete
embeddings to denote the degradation patterns, and the cross-sample contrastive
learning mechanism is further proposed to capture shared features from
different samples of certain degradation. To bridge the data gap, a domain
adaptation strategy is proposed to build the feature projection between the
source and target domains by dynamically aligning their codebook embeddings,
and a correlation alignment-based test-time adaptation mechanism is designed to
fine-tune the alignment discrepancies by tightening the degradation embeddings
to the corresponding cluster center in the source domain. Experimental results
on 10 open-source datasets demonstrate that UDAIR achieves new state-of-the-art
performance for the AiOIR task. Most importantly, the feature cluster validate
the degradation identification under unknown conditions, and qualitative
comparisons showcase robust generalization to real-world scenarios.

</details>


### [117] [Neural Restoration of Greening Defects in Historical Autochrome Photographs Based on Purely Synthetic Data](https://arxiv.org/abs/2505.22291)
*Saptarshi Neil Sinha,P. Julius Kuehn,Johannes Koppe,Arjan Kuijper,Michael Weinmann*

Main category: cs.CV

TL;DR: 提出了一种基于合成数据集生成和生成式AI的方法，用于自动修复数字化奥托克罗姆照片中的绿色缺陷，减少了手动修复的时间和精力。


<details>
  <summary>Details</summary>
Motivation: 早期视觉艺术（尤其是彩色照片）因老化和不当存储导致模糊、划痕、颜色渗色和褪色等问题，亟需自动修复方法。

Method: 通过合成数据集生成模拟绿色缺陷，并采用改进的加权损失函数（ChaIR方法）进行修复。

Result: 方法能够高效修复照片，减少时间需求，并更准确地还原原始颜色。

Conclusion: 该方法为视觉艺术的自动修复提供了新思路，解决了现有方法在颜色还原和手动干预方面的不足。

Abstract: The preservation of early visual arts, particularly color photographs, is
challenged by deterioration caused by aging and improper storage, leading to
issues like blurring, scratches, color bleeding, and fading defects. In this
paper, we present the first approach for the automatic removal of greening
color defects in digitized autochrome photographs. Our main contributions
include a method based on synthetic dataset generation and the use of
generative AI with a carefully designed loss function for the restoration of
visual arts. To address the lack of suitable training datasets for analyzing
greening defects in damaged autochromes, we introduce a novel approach for
accurately simulating such defects in synthetic data. We also propose a
modified weighted loss function for the ChaIR method to account for color
imbalances between defected and non-defected areas. While existing methods
struggle with accurately reproducing original colors and may require
significant manual effort, our method allows for efficient restoration with
reduced time requirements.

</details>


### [118] [CADReview: Automatically Reviewing CAD Programs with Error Detection and Correction](https://arxiv.org/abs/2505.22304)
*Jiali Chen,Xusen Hei,HongFei Liu,Yuancheng Wei,Zikun Deng,Jiayuan Xie,Yi Cai,Li Qing*

Main category: cs.CV

TL;DR: 论文提出ReCAD框架，用于自动检测和修正CAD程序中的错误，确保3D对象与参考图像一致，并创建了CADReview数据集验证其效果。


<details>
  <summary>Details</summary>
Motivation: 设计师在CAD设计流程中需耗费大量时间检查和修正原型，现有多模态大语言模型（MLLMs）在识别几何组件和空间操作方面表现不佳。

Method: 提出ReCAD框架，通过检测程序错误并提供修正反馈，同时构建包含20K程序-图像对的CADReview数据集。

Result: 实验表明ReCAD显著优于现有MLLMs，展示了在设计应用中的潜力。

Conclusion: ReCAD框架有效解决了CAD程序与参考图像一致性问题，提升了设计效率。

Abstract: Computer-aided design (CAD) is crucial in prototyping 3D objects through
geometric instructions (i.e., CAD programs). In practical design workflows,
designers often engage in time-consuming reviews and refinements of these
prototypes by comparing them with reference images. To bridge this gap, we
introduce the CAD review task to automatically detect and correct potential
errors, ensuring consistency between the constructed 3D objects and reference
images. However, recent advanced multimodal large language models (MLLMs)
struggle to recognize multiple geometric components and perform spatial
geometric operations within the CAD program, leading to inaccurate reviews. In
this paper, we propose the CAD program repairer (ReCAD) framework to
effectively detect program errors and provide helpful feedback on error
correction. Additionally, we create a dataset, CADReview, consisting of over
20K program-image pairs, with diverse errors for the CAD review task. Extensive
experiments demonstrate that our ReCAD significantly outperforms existing
MLLMs, which shows great potential in design applications.

</details>


### [119] [IKIWISI: An Interactive Visual Pattern Generator for Evaluating the Reliability of Vision-Language Models Without Ground Truth](https://arxiv.org/abs/2505.22305)
*Md Touhidul Islam,Imran Kabir,Md Alimoor Reza,Syed Masum Billah*

Main category: cs.CV

TL;DR: IKIWISI是一种交互式视觉模式生成器，用于评估无真实标签时的视觉语言模型，通过热图展示模型输出，并利用人类模式识别能力评估模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法依赖真实标签，而IKIWISI旨在通过可视化工具弥补这一不足，揭示人类与机器感知的差异。

Method: IKIWISI将模型输出转化为二元热图（绿色表示对象存在，红色表示不存在），并引入“间谍对象”检测模型幻觉。

Result: 15名参与者认为IKIWISI易用，其评估结果与客观指标一致，且仅需少量热图单元即可得出结论。

Conclusion: IKIWISI不仅补充传统评估方法，还揭示了提升视觉语言系统与人类感知对齐的机会。

Abstract: We present IKIWISI ("I Know It When I See It"), an interactive visual pattern
generator for assessing vision-language models in video object recognition when
ground truth is unavailable. IKIWISI transforms model outputs into a binary
heatmap where green cells indicate object presence and red cells indicate
object absence. This visualization leverages humans' innate pattern recognition
abilities to evaluate model reliability. IKIWISI introduces "spy objects":
adversarial instances users know are absent, to discern models hallucinating on
nonexistent items. The tool functions as a cognitive audit mechanism, surfacing
mismatches between human and machine perception by visualizing where models
diverge from human understanding.
  Our study with 15 participants found that users considered IKIWISI easy to
use, made assessments that correlated with objective metrics when available,
and reached informed conclusions by examining only a small fraction of heatmap
cells. This approach not only complements traditional evaluation methods
through visual assessment of model behavior with custom object sets, but also
reveals opportunities for improving alignment between human perception and
machine understanding in vision-language systems.

</details>


### [120] [Learning to Infer Parameterized Representations of Plants from 3D Scans](https://arxiv.org/abs/2505.22337)
*Samara Ghrer,Christophe Godin,Stefanie Wuhrer*

Main category: cs.CV

TL;DR: 提出了一种统一方法，通过3D扫描重建植物的参数化表示，适用于多种任务。


<details>
  <summary>Details</summary>
Motivation: 植物3D重建因器官复杂性和自遮挡问题具有挑战性，现有方法多为逆向建模或特定任务，缺乏统一解决方案。

Method: 使用基于L系统的虚拟植物训练递归神经网络，从3D点云推断参数化树状表示。

Result: 在合成植物上验证，方法在重建、分割和骨架化任务中表现与现有技术相当。

Conclusion: 该方法为植物3D重建提供了统一且高效的解决方案。

Abstract: Reconstructing faithfully the 3D architecture of plants from unstructured
observations is a challenging task. Plants frequently contain numerous organs,
organized in branching systems in more or less complex spatial networks,
leading to specific computational issues due to self-occlusion or spatial
proximity between organs. Existing works either consider inverse modeling where
the aim is to recover the procedural rules that allow to simulate virtual
plants, or focus on specific tasks such as segmentation or skeletonization. We
propose a unified approach that, given a 3D scan of a plant, allows to infer a
parameterized representation of the plant. This representation describes the
plant's branching structure, contains parametric information for each plant
organ, and can therefore be used directly in a variety of tasks. In this
data-driven approach, we train a recursive neural network with virtual plants
generated using an L-systems-based procedural model. After training, the
network allows to infer a parametric tree-like representation based on an input
3D point cloud. Our method is applicable to any plant that can be represented
as binary axial tree. We evaluate our approach on Chenopodium Album plants,
using experiments on synthetic plants to show that our unified framework allows
for different tasks including reconstruction, segmentation and skeletonization,
while achieving results on-par with state-of-the-art for each task.

</details>


### [121] [Progressive Data Dropout: An Embarrassingly Simple Approach to Faster Training](https://arxiv.org/abs/2505.22342)
*Shriram M S,Xinyue Hao,Shihao Hou,Yang Lu,Laura Sevilla-Lara,Anurag Arnab,Shreyank N Gowda*

Main category: cs.CV

TL;DR: 论文提出了一种名为Progressive Data Dropout的新训练范式，通过减少有效训练轮次至基线的12.4%，同时提升准确率高达4.82%，且无需修改模型架构或优化器。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习依赖大规模数据集训练，成本高昂；传统方法均匀采样数据集效率低下，亟需改进。

Method: 结合硬数据挖掘和dropout的简单训练范式，提出Progressive Data Dropout方法。

Result: 有效训练轮次减少至12.4%，准确率提升高达4.82%。

Conclusion: 该方法简单易用，适用于标准训练流程，具有广泛应用的潜力。

Abstract: The success of the machine learning field has reliably depended on training
on large datasets. While effective, this trend comes at an extraordinary cost.
This is due to two deeply intertwined factors: the size of models and the size
of datasets. While promising research efforts focus on reducing the size of
models, the other half of the equation remains fairly mysterious. Indeed, it is
surprising that the standard approach to training remains to iterate over and
over, uniformly sampling the training dataset. In this paper we explore a
series of alternative training paradigms that leverage insights from
hard-data-mining and dropout, simple enough to implement and use that can
become the new training standard. The proposed Progressive Data Dropout reduces
the number of effective epochs to as little as 12.4% of the baseline. This
savings actually do not come at any cost for accuracy. Surprisingly, the
proposed method improves accuracy by up to 4.82%. Our approach requires no
changes to model architecture or optimizer, and can be applied across standard
training pipelines, thus posing an excellent opportunity for wide adoption.
Code can be found here: https://github.com/bazyagami/LearningWithRevision

</details>


### [122] [VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in the Middle East and Beyond](https://arxiv.org/abs/2505.22353)
*Noora Al-Emadi,Ingmar Weber,Yin Yang,Ferda Ofli*

Main category: cs.CV

TL;DR: 论文提出了针对中东地区的车辆检测数据集VME，并展示了其在提升中东地区检测准确性上的效果，同时介绍了全球车辆检测基准数据集CDSI。


<details>
  <summary>Details</summary>
Motivation: 现有车辆检测数据集存在地理偏差，忽视中东地区，导致模型在该区域表现不佳。

Method: 构建VME数据集，包含中东12个国家的54个城市的图像，并采用手动和半自动标注方法；同时整合多源数据创建CDSI数据集。

Result: VME显著提升了中东地区的车辆检测准确性；CDSI训练的模型在全球范围内表现更优。

Conclusion: VME和CDSI填补了地理偏差的空白，为车辆检测提供了更全面的数据支持。

Abstract: Detecting vehicles in satellite images is crucial for traffic management,
urban planning, and disaster response. However, current models struggle with
real-world diversity, particularly across different regions. This challenge is
amplified by geographic bias in existing datasets, which often focus on
specific areas and overlook regions like the Middle East. To address this gap,
we present the Vehicles in the Middle East (VME) dataset, designed explicitly
for vehicle detection in high-resolution satellite images from Middle Eastern
countries. Sourced from Maxar, the VME dataset spans 54 cities across 12
countries, comprising over 4,000 image tiles and more than 100,000 vehicles,
annotated using both manual and semi-automated methods. Additionally, we
introduce the largest benchmark dataset for Car Detection in Satellite Imagery
(CDSI), combining images from multiple sources to enhance global car detection.
Our experiments demonstrate that models trained on existing datasets perform
poorly on Middle Eastern images, while the VME dataset significantly improves
detection accuracy in this region. Moreover, state-of-the-art models trained on
CDSI achieve substantial improvements in global car detection.

</details>


### [123] [Identity-Preserving Text-to-Image Generation via Dual-Level Feature Decoupling and Expert-Guided Fusion](https://arxiv.org/abs/2505.22360)
*Kewen Chen,Xiaobin Hu,Wenqi Ren*

Main category: cs.CV

TL;DR: 提出了一种新框架，通过解耦身份相关与无关特征，提升文本到图像生成的质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 当前方法难以区分输入图像中身份相关与无关信息，导致过拟合或身份丢失。

Method: 结合隐式-显式前景-背景解耦模块（IEDM）和基于专家混合（MoE）的特征融合模块（FFM）。

Result: 实验表明，该方法提升了生成图像质量、场景适应性和多样性。

Conclusion: 新框架有效解决了身份信息解耦问题，提升了文本到图像生成的性能。

Abstract: Recent advances in large-scale text-to-image generation models have led to a
surge in subject-driven text-to-image generation, which aims to produce
customized images that align with textual descriptions while preserving the
identity of specific subjects. Despite significant progress, current methods
struggle to disentangle identity-relevant information from identity-irrelevant
details in the input images, resulting in overfitting or failure to maintain
subject identity. In this work, we propose a novel framework that improves the
separation of identity-related and identity-unrelated features and introduces
an innovative feature fusion mechanism to improve the quality and text
alignment of generated images. Our framework consists of two key components: an
Implicit-Explicit foreground-background Decoupling Module (IEDM) and a Feature
Fusion Module (FFM) based on a Mixture of Experts (MoE). IEDM combines
learnable adapters for implicit decoupling at the feature level with inpainting
techniques for explicit foreground-background separation at the image level.
FFM dynamically integrates identity-irrelevant features with identity-related
features, enabling refined feature representations even in cases of incomplete
decoupling. In addition, we introduce three complementary loss functions to
guide the decoupling process. Extensive experiments demonstrate the
effectiveness of our proposed method in enhancing image generation quality,
improving flexibility in scene adaptation, and increasing the diversity of
generated outputs across various textual descriptions.

</details>


### [124] [DAM: Domain-Aware Module for Multi-Domain Dataset Condensation](https://arxiv.org/abs/2505.22387)
*Jaehyun Choi,Gyojin Han,Dong-Jae Lee,Sunghyun Baek,Junmo Kim*

Main category: cs.CV

TL;DR: 论文提出了一种多领域数据集压缩方法（MDDC），通过引入领域感知模块（DAM）提升单领域和多领域数据的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现代数据集通常包含多领域异构图像，但现有数据集压缩方法未充分考虑这一点。

Method: 提出领域感知模块（DAM），利用可学习空间掩码嵌入领域特征，并通过基于频率的伪领域标签处理无显式标签的数据。

Result: 实验表明DAM在领域内、领域外及跨架构性能上均优于基线方法。

Conclusion: MDDC通过DAM有效提升了数据集压缩在多领域场景下的性能。

Abstract: Dataset Condensation (DC) has emerged as a promising solution to mitigate the
computational and storage burdens associated with training deep learning
models. However, existing DC methods largely overlook the multi-domain nature
of modern datasets, which are increasingly composed of heterogeneous images
spanning multiple domains. In this paper, we extend DC and introduce
Multi-Domain Dataset Condensation (MDDC), which aims to condense data that
generalizes across both single-domain and multi-domain settings. To this end,
we propose the Domain-Aware Module (DAM), a training-time module that embeds
domain-related features into each synthetic image via learnable spatial masks.
As explicit domain labels are mostly unavailable in real-world datasets, we
employ frequency-based pseudo-domain labeling, which leverages low-frequency
amplitude statistics. DAM is only active during the condensation process, thus
preserving the same images per class (IPC) with prior methods. Experiments show
that DAM consistently improves in-domain, out-of-domain, and cross-architecture
performance over baseline dataset condensation methods.

</details>


### [125] [PacTure: Efficient PBR Texture Generation on Packed Views with Visual Autoregressive Models](https://arxiv.org/abs/2505.22394)
*Fan Fei,Jiajun Tang,Fei-Peng Tian,Boxin Shi,Ping Tan*

Main category: cs.CV

TL;DR: PacTure是一个新颖的框架，用于从无纹理3D网格、文本描述和可选图像提示生成基于物理的渲染（PBR）材质纹理。通过引入视图打包技术和多视图多域生成主干，显著提高了生成效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成纹理时存在全局不一致性和分辨率限制的问题，PacTure旨在解决这些缺陷。

Method: 提出视图打包技术，将多视图映射问题转化为2D矩形装箱问题，并利用多视图多域生成主干减少推理成本。

Result: 实验表明，PacTure在生成PBR纹理的质量和训练推理效率上优于现有方法。

Conclusion: PacTure通过创新技术解决了多视图纹理生成的效率和一致性问题，具有显著优势。

Abstract: We present PacTure, a novel framework for generating physically-based
rendering (PBR) material textures from an untextured 3D mesh, a text
description, and an optional image prompt. Early 2D generation-based texturing
approaches generate textures sequentially from different views, resulting in
long inference times and globally inconsistent textures. More recent approaches
adopt multi-view generation with cross-view attention to enhance global
consistency, which, however, limits the resolution for each view. In response
to these weaknesses, we first introduce view packing, a novel technique that
significantly increases the effective resolution for each view during
multi-view generation without imposing additional inference cost, by
formulating the arrangement of multi-view maps as a 2D rectangle bin packing
problem. In contrast to UV mapping, it preserves the spatial proximity
essential for image generation and maintains full compatibility with current 2D
generative models. To further reduce the inference cost, we enable fine-grained
control and multi-domain generation within the next-scale prediction
autoregressive framework to create an efficient multi-view multi-domain
generative backbone. Extensive experiments show that PacTure outperforms
state-of-the-art methods in both quality of generated PBR textures and
efficiency in training and inference.

</details>


### [126] [Zooming from Context to Cue: Hierarchical Preference Optimization for Multi-Image MLLMs](https://arxiv.org/abs/2505.22396)
*Xudong Li,Mengdan Zhang,Peixian Chen,Xiawu Zheng,Yan Zhang,Jingyuan Zheng,Yunhang Shen,Ke Li,Chaoyou Fu,Xing Sun,Rongrong Ji*

Main category: cs.CV

TL;DR: CcDPO是一种多级偏好优化框架，通过从全局上下文到局部细节的视觉线索增强多图像场景中的感知能力，显著减少幻觉并提升性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在多图像理解中存在跨模态对齐问题，导致幻觉现象（如上下文遗漏、混淆和误解）。现有方法仅优化单一图像参考，忽略了全局上下文建模。

Method: 提出CcDPO框架，包括上下文级优化（全局序列偏好）和细节级优化（区域目标视觉提示），并构建MultiScope-42k数据集支持优化。

Result: 实验表明，CcDPO显著减少幻觉，并在单图像和多图像任务中表现一致提升。

Conclusion: CcDPO通过多级优化有效解决了MLLMs在多图像理解中的局限性，为未来研究提供了新方向。

Abstract: Multi-modal Large Language Models (MLLMs) excel at single-image tasks but
struggle with multi-image understanding due to cross-modal misalignment,
leading to hallucinations (context omission, conflation, and
misinterpretation). Existing methods using Direct Preference Optimization (DPO)
constrain optimization to a solitary image reference within the input sequence,
neglecting holistic context modeling. We propose Context-to-Cue Direct
Preference Optimization (CcDPO), a multi-level preference optimization
framework that enhances per-image perception in multi-image settings by zooming
into visual clues -- from sequential context to local details. It features: (i)
Context-Level Optimization : Re-evaluates cognitive biases underlying MLLMs'
multi-image context comprehension and integrates a spectrum of low-cost global
sequence preferences for bias mitigation. (ii) Needle-Level Optimization :
Directs attention to fine-grained visual details through region-targeted visual
prompts and multimodal preference supervision. To support scalable
optimization, we also construct MultiScope-42k, an automatically generated
dataset with high-quality multi-level preference pairs. Experiments show that
CcDPO significantly reduces hallucinations and yields consistent performance
gains across general single- and multi-image tasks.

</details>


### [127] [Self-Reflective Reinforcement Learning for Diffusion-based Image Reasoning Generation](https://arxiv.org/abs/2505.22407)
*Jiadong Pan,Zhiyuan Ma,Kaiyan Zhang,Ning Ding,Bowen Zhou*

Main category: cs.CV

TL;DR: SRRL是一种自反思强化学习算法，用于扩散模型，通过在多轮去噪过程中引入反思迭代，实现逻辑图像的推理生成。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成方法在逻辑为中心的图像生成任务中存在推理困难，受CoT和RL在LLMs中的成功启发，提出SRRL以解决这一问题。

Method: SRRL将整个去噪轨迹视为CoT步骤，引入条件引导的前向过程，支持在CoT步骤间进行反思迭代。

Result: 实验结果表明，SRRL在逻辑图像生成任务中表现优异，甚至优于GPT-4o。

Conclusion: SRRL首次将图像推理引入生成任务，支持物理规律和非常规物理现象的生成。

Abstract: Diffusion models have recently demonstrated exceptional performance in image
generation task. However, existing image generation methods still significantly
suffer from the dilemma of image reasoning, especially in logic-centered image
generation tasks. Inspired by the success of Chain of Thought (CoT) and
Reinforcement Learning (RL) in LLMs, we propose SRRL, a self-reflective RL
algorithm for diffusion models to achieve reasoning generation of logical
images by performing reflection and iteration across generation trajectories.
The intermediate samples in the denoising process carry noise, making accurate
reward evaluation difficult. To address this challenge, SRRL treats the entire
denoising trajectory as a CoT step with multi-round reflective denoising
process and introduces condition guided forward process, which allows for
reflective iteration between CoT steps. Through SRRL-based iterative diffusion
training, we introduce image reasoning through CoT into generation tasks
adhering to physical laws and unconventional physical phenomena for the first
time. Notably, experimental results of case study exhibit that the superior
performance of our SRRL algorithm even compared with GPT-4o. The project page
is https://jadenpan0.github.io/srrl.github.io/.

</details>


### [128] [Frugal Incremental Generative Modeling using Variational Autoencoders](https://arxiv.org/abs/2505.22408)
*Victor Enescu,Hichem Sahbi*

Main category: cs.CV

TL;DR: 提出了一种基于变分自编码器（VAE）的无回放增量学习方法，解决了持续学习中数据量增长和灾难性遗忘的问题。


<details>
  <summary>Details</summary>
Motivation: 持续学习在深度学习中潜力巨大，但面临灾难性遗忘和数据量增长的挑战，尤其是回放方法。

Method: 设计了基于多模态潜在空间的增量生成模型，并引入正交性准则以减少VAE的灾难性遗忘。

Result: 实验表明，该方法在内存占用上比相关方法节省至少一个数量级，同时达到SOTA准确率。

Conclusion: 该方法通过静态和动态VAE变体，有效解决了持续学习中的内存和遗忘问题。

Abstract: Continual or incremental learning holds tremendous potential in deep learning
with different challenges including catastrophic forgetting. The advent of
powerful foundation and generative models has propelled this paradigm even
further, making it one of the most viable solution to train these models.
However, one of the persisting issues lies in the increasing volume of data
particularly with replay-based methods. This growth introduces challenges with
scalability since continuously expanding data becomes increasingly demanding as
the number of tasks grows. In this paper, we attenuate this issue by devising a
novel replay-free incremental learning model based on Variational Autoencoders
(VAEs). The main contribution of this work includes (i) a novel incremental
generative modelling, built upon a well designed multi-modal latent space, and
also (ii) an orthogonality criterion that mitigates catastrophic forgetting of
the learned VAEs. The proposed method considers two variants of these VAEs:
static and dynamic with no (or at most a controlled) growth in the number of
parameters. Extensive experiments show that our method is (at least) an order
of magnitude more ``memory-frugal'' compared to the closely related works while
achieving SOTA accuracy scores.

</details>


### [129] [RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network](https://arxiv.org/abs/2505.22427)
*Van-Tin Luu,Yon-Lin Cai,Vu-Hoang Tran,Wei-Chen Chiu,Yi-Ting Chen,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 提出了一种首个在线自动雷达与相机几何校准方法，通过双视角表示和选择性融合机制解决数据稀疏性和高度不确定性问题，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 雷达高度数据的稀疏性和不确定性使得系统运行时的自动校准成为长期挑战。

Method: 采用双视角表示（前视图和鸟瞰图）和选择性融合机制，结合多模态交叉注意力机制和抗噪声匹配器。

Result: 在nuScenes数据集上显著优于现有雷达-相机和LiDAR-相机校准方法。

Conclusion: 该方法为未来研究设定了新基准，代码已开源。

Abstract: This paper presents a groundbreaking approach - the first online automatic
geometric calibration method for radar and camera systems. Given the
significant data sparsity and measurement uncertainty in radar height data,
achieving automatic calibration during system operation has long been a
challenge. To address the sparsity issue, we propose a Dual-Perspective
representation that gathers features from both frontal and bird's-eye views.
The frontal view contains rich but sensitive height information, whereas the
bird's-eye view provides robust features against height uncertainty. We thereby
propose a novel Selective Fusion Mechanism to identify and fuse reliable
features from both perspectives, reducing the effect of height uncertainty.
Moreover, for each view, we incorporate a Multi-Modal Cross-Attention Mechanism
to explicitly find location correspondences through cross-modal matching.
During the training phase, we also design a Noise-Resistant Matcher to provide
better supervision and enhance the robustness of the matching mechanism against
sparsity and height uncertainty. Our experimental results, tested on the
nuScenes dataset, demonstrate that our method significantly outperforms
previous radar-camera auto-calibration methods, as well as existing
state-of-the-art LiDAR-camera calibration techniques, establishing a new
benchmark for future research. The code is available at
https://github.com/nycu-acm/RC-AutoCalib.

</details>


### [130] [Distance Transform Guided Mixup for Alzheimer's Detection](https://arxiv.org/abs/2505.22434)
*Zobia Batool,Huseyin Ozkan,Erchan Aptoula*

Main category: cs.CV

TL;DR: 论文提出了一种基于距离变换和分层混合的单域泛化方法，用于解决阿尔茨海默病检测中的类不平衡和数据集多样性不足问题。


<details>
  <summary>Details</summary>
Motivation: 医学数据集存在类不平衡、成像协议差异和多样性不足的问题，限制了模型的泛化能力。

Method: 通过计算MRI扫描的距离变换，将其空间分层并与不同样本的层混合，生成增强图像。

Result: 在ADNI和AIBL数据集上，该方法显著提升了泛化性能。

Conclusion: 提出的方法在保留大脑结构的同时，有效增强了数据多样性，改善了模型泛化能力。

Abstract: Alzheimer's detection efforts aim to develop accurate models for early
disease diagnosis. Significant advances have been achieved with convolutional
neural networks and vision transformer based approaches. However, medical
datasets suffer heavily from class imbalance, variations in imaging protocols,
and limited dataset diversity, which hinder model generalization. To overcome
these challenges, this study focuses on single-domain generalization by
extending the well-known mixup method. The key idea is to compute the distance
transform of MRI scans, separate them spatially into multiple layers and then
combine layers stemming from distinct samples to produce augmented images. The
proposed approach generates diverse data while preserving the brain's
structure. Experimental results show generalization performance improvement
across both ADNI and AIBL datasets.

</details>


### [131] [Can NeRFs See without Cameras?](https://arxiv.org/abs/2505.22441)
*Chaitanya Amballa,Sattwik Basu,Yu-Lin Wei,Zhijian Yang,Mehmet Ergezer,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: NeRFs被重新设计以从多路径信号（如WiFi）中学习环境信息，成功推断室内平面图。


<details>
  <summary>Details</summary>
Motivation: 探索多路径信号（如RF/音频）是否可用于环境推断，扩展NeRFs的应用范围。

Method: 重新设计NeRFs，使其能够从多路径信号中学习，并应用于稀疏WiFi测量数据。

Result: 成功推断出室内平面图，支持信号预测和基础光线追踪。

Conclusion: NeRFs可通过多路径信号学习环境信息，为室内应用提供新可能。

Abstract: Neural Radiance Fields (NeRFs) have been remarkably successful at
synthesizing novel views of 3D scenes by optimizing a volumetric scene
function. This scene function models how optical rays bring color information
from a 3D object to the camera pixels. Radio frequency (RF) or audio signals
can also be viewed as a vehicle for delivering information about the
environment to a sensor. However, unlike camera pixels, an RF/audio sensor
receives a mixture of signals that contain many environmental reflections (also
called "multipath"). Is it still possible to infer the environment using such
multipath signals? We show that with redesign, NeRFs can be taught to learn
from multipath signals, and thereby "see" the environment. As a grounding
application, we aim to infer the indoor floorplan of a home from sparse WiFi
measurements made at multiple locations inside the home. Although a difficult
inverse problem, our implicitly learnt floorplans look promising, and enables
forward applications, such as indoor signal prediction and basic ray tracing.

</details>


### [132] [On Geometry-Enhanced Parameter-Efficient Fine-Tuning for 3D Scene Segmentation](https://arxiv.org/abs/2505.22444)
*Liyao Tang,Zhe Chen,Dacheng Tao*

Main category: cs.CV

TL;DR: 论文提出了一种几何感知的参数高效微调模块GEM，用于3D点云模型，解决了现有方法忽略几何和空间结构的问题，性能接近全微调，但仅更新1.6%的参数。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练点云模型在3D场景理解中表现优异，但全微调计算和存储成本高。现有参数高效微调方法在3D任务中表现不佳，因其忽略了局部空间结构和全局几何上下文。

Method: 提出了几何编码混合器（GEM），结合细粒度局部位置编码和轻量级潜在注意力机制，以捕捉全局上下文。

Result: 实验表明，GEM性能接近或超过全微调，仅更新1.6%参数，训练时间和内存需求显著降低。

Conclusion: GEM为大规模3D点云模型的高效、可扩展和几何感知微调设定了新基准。

Abstract: The emergence of large-scale pre-trained point cloud models has significantly
advanced 3D scene understanding, but adapting these models to specific
downstream tasks typically demands full fine-tuning, incurring high
computational and storage costs. Parameter-efficient fine-tuning (PEFT)
techniques, successful in natural language processing and 2D vision tasks,
would underperform when naively applied to 3D point cloud models due to
significant geometric and spatial distribution shifts. Existing PEFT methods
commonly treat points as orderless tokens, neglecting important local spatial
structures and global geometric contexts in 3D modeling. To bridge this gap, we
introduce the Geometric Encoding Mixer (GEM), a novel geometry-aware PEFT
module specifically designed for 3D point cloud transformers. GEM explicitly
integrates fine-grained local positional encodings with a lightweight latent
attention mechanism to capture comprehensive global context, thereby
effectively addressing the spatial and geometric distribution mismatch.
Extensive experiments demonstrate that GEM achieves performance comparable to
or sometimes even exceeding full fine-tuning, while only updating 1.6% of the
model's parameters, fewer than other PEFT methods. With significantly reduced
training time and memory requirements, our approach thus sets a new benchmark
for efficient, scalable, and geometry-aware fine-tuning of large-scale 3D point
cloud models. Code will be released.

</details>


### [133] [NFR: Neural Feature-Guided Non-Rigid Shape Registration](https://arxiv.org/abs/2505.22445)
*Puhua Jiang,Zhangquan Chen,Mingze Sun,Ruqi Huang*

Main category: cs.CV

TL;DR: 提出了一种基于学习的3D形状配准框架，无需标注对应关系即可处理非刚性变形和部分形状匹配。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在非刚性变形和部分形状匹配中的不足，同时避免训练时依赖标注数据。

Method: 结合深度学习提取的神经特征与几何配准流程，动态更新对应关系并通过一致性先验过滤。

Result: 在多个基准测试中取得最优表现，即使对未见过的复杂变形形状也能提供高质量对应关系。

Conclusion: 该框架在非刚性配准和部分形状匹配中表现出色，且无需标注数据，具有广泛适用性。

Abstract: In this paper, we propose a novel learning-based framework for 3D shape
registration, which overcomes the challenges of significant non-rigid
deformation and partiality undergoing among input shapes, and, remarkably,
requires no correspondence annotation during training. Our key insight is to
incorporate neural features learned by deep learning-based shape matching
networks into an iterative, geometric shape registration pipeline. The
advantage of our approach is two-fold -- On one hand, neural features provide
more accurate and semantically meaningful correspondence estimation than
spatial features (e.g., coordinates), which is critical in the presence of
large non-rigid deformations; On the other hand, the correspondences are
dynamically updated according to the intermediate registrations and filtered by
consistency prior, which prominently robustify the overall pipeline. Empirical
results show that, with as few as dozens of training shapes of limited
variability, our pipeline achieves state-of-the-art results on several
benchmarks of non-rigid point cloud matching and partial shape matching across
varying settings, but also delivers high-quality correspondences between unseen
challenging shape pairs that undergo both significant extrinsic and intrinsic
deformations, in which case neither traditional registration methods nor
intrinsic methods work.

</details>


### [134] [Fostering Video Reasoning via Next-Event Prediction](https://arxiv.org/abs/2505.22457)
*Haonan Wang,Hongfu Liu,Xiangyan Liu,Chao Du,Kenji Kawaguchi,Ye Wang,Tianyu Pang*

Main category: cs.CV

TL;DR: 论文提出了一种名为“下一事件预测”（NEP）的自监督学习任务，用于增强多模态语言模型（MLLM）在视频输入上的时序推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有任务（如视频问答）依赖人工标注或更强模型，而视频描述则混淆了时空信息。NEP旨在填补这一空白，利用未来视频片段作为自监督信号。

Method: 将视频分段为过去和未来帧，MLLM基于过去帧预测未来事件摘要。构建了V1-33K数据集，并探索了多种视频指令调优策略。

Result: 实验验证NEP是一种可扩展且有效的训练范式，显著提升了MLLM的时序推理能力。

Conclusion: NEP为MLLM的时序推理提供了一种高效的自监督学习框架，未来可通过FutureBench进一步评估其预测一致性。

Abstract: Next-token prediction serves as the foundational learning task enabling
reasoning in LLMs. But what should the learning task be when aiming to equip
MLLMs with temporal reasoning capabilities over video inputs? Existing tasks
such as video question answering often rely on annotations from humans or much
stronger MLLMs, while video captioning tends to entangle temporal reasoning
with spatial information. To address this gap, we propose next-event prediction
(NEP), a learning task that harnesses future video segments as a rich,
self-supervised signal to foster temporal reasoning. We segment each video into
past and future frames: the MLLM takes the past frames as input and predicts a
summary of events derived from the future frames, thereby encouraging the model
to reason temporally in order to complete the task. To support this task, we
curate V1-33K, a dataset comprising 33,000 automatically extracted video
segments spanning diverse real-world scenarios. We further explore a range of
video instruction-tuning strategies to study their effects on temporal
reasoning. To evaluate progress, we introduce FutureBench to assess coherence
in predicting unseen future events. Experiments validate that NEP offers a
scalable and effective training paradigm for fostering temporal reasoning in
MLLMs.

</details>


### [135] [Universal Domain Adaptation for Semantic Segmentation](https://arxiv.org/abs/2505.22458)
*Seun-An Choe,Keon-Hee Park,Jinwoo Choi,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 论文提出了一种新的无监督域自适应方法UniDA-SS，用于语义分割，解决了传统方法中类别设置未知的问题，并提出了UniMAP框架。


<details>
  <summary>Details</summary>
Motivation: 传统UDA-SS方法假设源域和目标域的类别设置已知，这在现实场景中不切实际，导致性能下降。

Method: 提出了UniMAP框架，包含Domain-Specific Prototype-based Distinction (DSPD)和Target-based Image Matching (TIM)两个关键组件。

Result: 实验表明UniMAP显著优于基线方法。

Conclusion: UniMAP在无需类别设置先验知识的情况下，实现了鲁棒的域自适应。

Abstract: Unsupervised domain adaptation for semantic segmentation (UDA-SS) aims to
transfer knowledge from labeled source data to unlabeled target data. However,
traditional UDA-SS methods assume that category settings between source and
target domains are known, which is unrealistic in real-world scenarios. This
leads to performance degradation if private classes exist. To address this
limitation, we propose Universal Domain Adaptation for Semantic Segmentation
(UniDA-SS), achieving robust adaptation even without prior knowledge of
category settings. We define the problem in the UniDA-SS scenario as low
confidence scores of common classes in the target domain, which leads to
confusion with private classes. To solve this problem, we propose UniMAP:
UniDA-SS with Image Matching and Prototype-based Distinction, a novel framework
composed of two key components. First, Domain-Specific Prototype-based
Distinction (DSPD) divides each class into two domain-specific prototypes,
enabling finer separation of domain-specific features and enhancing the
identification of common classes across domains. Second, Target-based Image
Matching (TIM) selects a source image containing the most common-class pixels
based on the target pseudo-label and pairs it in a batch to promote effective
learning of common classes. We also introduce a new UniDA-SS benchmark and
demonstrate through various experiments that UniMAP significantly outperforms
baselines. The code is available at
\href{https://github.com/KU-VGI/UniMAP}{this https URL}.

</details>


### [136] [SHTOcc: Effective 3D Occupancy Prediction with Sparse Head and Tail Voxels](https://arxiv.org/abs/2505.22461)
*Qiucheng Yu,Yuan Xie,Xin Tan*

Main category: cs.CV

TL;DR: SHTOcc提出了一种稀疏头尾体素构建方法，解决了3D占用预测中的长尾分布和几何分布问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未探索体素的最本质分布模式，导致结果不理想。本文研究了体素的类间分布和几何分布，以解决长尾问题和几何分布导致的性能不佳。

Method: 提出SHTOcc，通过稀疏头尾体素构建准确识别和平衡头尾类关键体素，同时使用解耦学习减少模型对主导类别的偏向，增强对尾部类的关注。

Result: 实验表明，SHTOcc在多个基线上显著改进：GPU内存使用减少42.2%，推理速度提升58.6%，准确率提高约7%。

Conclusion: SHTOcc通过优化体素分布模式，有效提升了3D占用预测的性能和效率。

Abstract: 3D occupancy prediction has attracted much attention in the field of
autonomous driving due to its powerful geometric perception and object
recognition capabilities. However, existing methods have not explored the most
essential distribution patterns of voxels, resulting in unsatisfactory results.
This paper first explores the inter-class distribution and geometric
distribution of voxels, thereby solving the long-tail problem caused by the
inter-class distribution and the poor performance caused by the geometric
distribution. Specifically, this paper proposes SHTOcc (Sparse Head-Tail
Occupancy), which uses sparse head-tail voxel construction to accurately
identify and balance key voxels in the head and tail classes, while using
decoupled learning to reduce the model's bias towards the dominant (head)
category and enhance the focus on the tail class. Experiments show that
significant improvements have been made on multiple baselines: SHTOcc reduces
GPU memory usage by 42.2%, increases inference speed by 58.6%, and improves
accuracy by about 7%, verifying its effectiveness and efficiency. The code is
available at https://github.com/ge95net/SHTOcc

</details>


### [137] [Single Domain Generalization for Alzheimer's Detection from 3D MRIs with Pseudo-Morphological Augmentations and Contrastive Learning](https://arxiv.org/abs/2505.22465)
*Zobia Batool,Huseyin Ozkan,Erchan Aptoula*

Main category: cs.CV

TL;DR: 论文提出了一种结合可学习伪形态模块和监督对比学习的方法，用于提升阿尔茨海默病MRI检测的泛化能力，解决了类别不平衡和协议变化问题。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在阿尔茨海默病MRI检测中存在类别不平衡、协议变化和数据集多样性不足等问题，限制了模型的泛化能力。

Method: 提出可学习伪形态模块生成形状感知的解剖学有意义增强，并结合监督对比学习模块提取鲁棒的类别特定表示。

Result: 在三个数据集上的实验表明，该方法在类别不平衡和成像协议变化下表现出更好的性能和泛化能力。

Conclusion: 该方法有效提升了阿尔茨海默病MRI检测的泛化能力，尤其在数据分布差异大的情况下表现优异。

Abstract: Although Alzheimer's disease detection via MRIs has advanced significantly
thanks to contemporary deep learning models, challenges such as class
imbalance, protocol variations, and limited dataset diversity often hinder
their generalization capacity. To address this issue, this article focuses on
the single domain generalization setting, where given the data of one domain, a
model is designed and developed with maximal performance w.r.t. an unseen
domain of distinct distribution. Since brain morphology is known to play a
crucial role in Alzheimer's diagnosis, we propose the use of learnable
pseudo-morphological modules aimed at producing shape-aware, anatomically
meaningful class-specific augmentations in combination with a supervised
contrastive learning module to extract robust class-specific representations.
Experiments conducted across three datasets show improved performance and
generalization capacity, especially under class imbalance and imaging protocol
variations. The source code will be made available upon acceptance at
https://github.com/zobia111/SDG-Alzheimer.

</details>


### [138] [ProCrop: Learning Aesthetic Image Cropping from Professional Compositions](https://arxiv.org/abs/2505.22490)
*Ke Zhang,Tianyu Ding,Jiachen Jiang,Tianyi Chen,Ilya Zharkov,Vishal M. Patel,Luming Liang*

Main category: cs.CV

TL;DR: ProCrop是一种基于检索的图像裁剪方法，利用专业摄影构图指导裁剪，显著提升性能，并提供了一个大规模弱标注数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则或数据驱动的图像裁剪方法缺乏多样性或需要标注数据，ProCrop通过利用专业摄影构图解决这些问题。

Method: ProCrop通过融合专业照片特征与查询图像特征，学习专业构图，并生成242K弱标注数据集。

Result: ProCrop在监督和弱监督设置下显著优于现有方法，甚至匹配全监督方法。

Conclusion: ProCrop和数据集将公开，推动图像美学和构图分析研究。

Abstract: Image cropping is crucial for enhancing the visual appeal and narrative
impact of photographs, yet existing rule-based and data-driven approaches often
lack diversity or require annotated training data. We introduce ProCrop, a
retrieval-based method that leverages professional photography to guide
cropping decisions. By fusing features from professional photographs with those
of the query image, ProCrop learns from professional compositions,
significantly boosting performance. Additionally, we present a large-scale
dataset of 242K weakly-annotated images, generated by out-painting professional
images and iteratively refining diverse crop proposals. This composition-aware
dataset generation offers diverse high-quality crop proposals guided by
aesthetic principles and becomes the largest publicly available dataset for
image cropping. Extensive experiments show that ProCrop significantly
outperforms existing methods in both supervised and weakly-supervised settings.
Notably, when trained on the new dataset, our ProCrop surpasses previous
weakly-supervised methods and even matches fully supervised approaches. Both
the code and dataset will be made publicly available to advance research in
image aesthetics and composition analysis.

</details>


### [139] [The Meeseeks Mesh: Spatially Consistent 3D Adversarial Objects for BEV Detector](https://arxiv.org/abs/2505.22499)
*Aixuan Li,Mochu Xiang,Jing Zhang,Yuchao Dai*

Main category: cs.CV

TL;DR: 论文研究了3D物体检测模型对3D对抗攻击的脆弱性，提出了一种生成非侵入式3D对抗物体的方法，验证了其在不同视角和时间下的有效性。


<details>
  <summary>Details</summary>
Motivation: 评估3D物体检测模型在真实场景中的鲁棒性，通过对抗攻击测试其性能，确保自动驾驶系统的安全性和可靠性。

Method: 采用可微分渲染技术建模对抗物体与目标车辆的空间关系，引入遮挡感知模块增强视觉一致性，设计BEV空间特征优化策略保持攻击效果。

Result: 实验表明，该方法能有效抑制先进3D检测器的预测，且对抗物体在不同位置和距离下保持攻击效果。

Conclusion: 该方法为测试3D物体检测模型的鲁棒性提供了重要工具，生成的对抗物体具有强泛化能力。

Abstract: 3D object detection is a critical component in autonomous driving systems. It
allows real-time recognition and detection of vehicles, pedestrians and
obstacles under varying environmental conditions. Among existing methods, 3D
object detection in the Bird's Eye View (BEV) has emerged as the mainstream
framework. To guarantee a safe, robust and trustworthy 3D object detection, 3D
adversarial attacks are investigated, where attacks are placed in 3D
environments to evaluate the model performance, e.g., putting a film on a car,
clothing a pedestrian. The vulnerability of 3D object detection models to 3D
adversarial attacks serves as an important indicator to evaluate the robustness
of the model against perturbations. To investigate this vulnerability, we
generate non-invasive 3D adversarial objects tailored for real-world attack
scenarios. Our method verifies the existence of universal adversarial objects
that are spatially consistent across time and camera views. Specifically, we
employ differentiable rendering techniques to accurately model the spatial
relationship between adversarial objects and the target vehicle. Furthermore,
we introduce an occlusion-aware module to enhance visual consistency and
realism under different viewpoints. To maintain attack effectiveness across
multiple frames, we design a BEV spatial feature-guided optimization strategy.
Experimental results demonstrate that our approach can reliably suppress
vehicle predictions from state-of-the-art 3D object detectors, serving as an
important tool to test robustness of 3D object detection models before
deployment. Moreover, the generated adversarial objects exhibit strong
generalization capabilities, retaining its effectiveness at various positions
and distances in the scene.

</details>


### [140] [PathFL: Multi-Alignment Federated Learning for Pathology Image Segmentation](https://arxiv.org/abs/2505.22522)
*Yuan Zhang,Feng Chen,Yaolei Qi,Guanyu Yang,Huazhu Fu*

Main category: cs.CV

TL;DR: PathFL是一种多对齐联邦学习框架，通过图像、特征和模型聚合的三级对齐策略，解决病理图像分割中的数据异质性问题。


<details>
  <summary>Details</summary>
Motivation: 病理图像分割在多中心数据中存在异质性挑战，如成像模态、器官和扫描设备的多样性，导致表示偏差和模型泛化能力受限。

Method: PathFL采用三级对齐策略：图像级（风格增强模块）、特征级（自适应特征对齐模块）和模型聚合级（分层相似性聚合策略）。

Result: 在四种异质性病理图像数据集上的评估表明，PathFL在性能和鲁棒性上优于其他方法。

Conclusion: PathFL通过多级对齐策略有效提升了病理图像分割的泛化能力和鲁棒性。

Abstract: Pathology image segmentation across multiple centers encounters significant
challenges due to diverse sources of heterogeneity including imaging
modalities, organs, and scanning equipment, whose variability brings
representation bias and impedes the development of generalizable segmentation
models. In this paper, we propose PathFL, a novel multi-alignment Federated
Learning framework for pathology image segmentation that addresses these
challenges through three-level alignment strategies of image, feature, and
model aggregation. Firstly, at the image level, a collaborative style
enhancement module aligns and diversifies local data by facilitating style
information exchange across clients. Secondly, at the feature level, an
adaptive feature alignment module ensures implicit alignment in the
representation space by infusing local features with global insights, promoting
consistency across heterogeneous client features learning. Finally, at the
model aggregation level, a stratified similarity aggregation strategy
hierarchically aligns and aggregates models on the server, using layer-specific
similarity to account for client discrepancies and enhance global
generalization. Comprehensive evaluations on four sets of heterogeneous
pathology image datasets, encompassing cross-source, cross-modality,
cross-organ, and cross-scanner variations, validate the effectiveness of our
PathFL in achieving better performance and robustness against data
heterogeneity.

</details>


### [141] [PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image Generative Models](https://arxiv.org/abs/2505.22523)
*Junwen Chen,Heyang Jiang,Yanbin Wang,Keming Wu,Ji Li,Chao Zhang,Keiji Yanai,Dong Chen,Yuhui Yuan*

Main category: cs.CV

TL;DR: 论文提出了一种生成高质量多层透明图像的方法，包括数据集发布、训练免费合成管道和开源模型ART+，解决了多层透明数据缺乏的问题。


<details>
  <summary>Details</summary>
Motivation: 解决多层透明图像生成领域缺乏高质量数据集的问题，提升创意控制能力。

Method: 发布PrismLayersPro数据集，开发训练免费合成管道，推出开源模型ART+，结合LayerFLUX和MultiLayerFLUX技术。

Result: ART+在用户研究中表现优于原ART模型，视觉质量接近FLUX.1-[dev]模型。

Conclusion: 为多层透明图像生成任务奠定了数据集基础，支持精确、可编辑的分层图像研究与应用。

Abstract: Generating high-quality, multi-layer transparent images from text prompts can
unlock a new level of creative control, allowing users to edit each layer as
effortlessly as editing text outputs from LLMs. However, the development of
multi-layer generative models lags behind that of conventional text-to-image
models due to the absence of a large, high-quality corpus of multi-layer
transparent data. In this paper, we address this fundamental challenge by: (i)
releasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro)
dataset of 200K (20K) multilayer transparent images with accurate alpha mattes,
(ii) introducing a trainingfree synthesis pipeline that generates such data on
demand using off-the-shelf diffusion models, and (iii) delivering a strong,
open-source multi-layer generation model, ART+, which matches the aesthetics of
modern text-to-image generation models. The key technical contributions
include: LayerFLUX, which excels at generating high-quality single transparent
layers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple
LayerFLUX outputs into complete images, guided by human-annotated semantic
layout. To ensure higher quality, we apply a rigorous filtering stage to remove
artifacts and semantic mismatches, followed by human selection. Fine-tuning the
state-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which
outperforms the original ART in 60% of head-to-head user study comparisons and
even matches the visual quality of images generated by the FLUX.1-[dev] model.
We anticipate that our work will establish a solid dataset foundation for the
multi-layer transparent image generation task, enabling research and
applications that require precise, editable, and visually compelling layered
imagery.

</details>


### [142] [Thinking with Generated Images](https://arxiv.org/abs/2505.22525)
*Ethan Chern,Zhulin Hu,Steffi Chern,Siqi Kou,Jiadi Su,Yan Ma,Zhijie Deng,Pengfei Liu*

Main category: cs.CV

TL;DR: 提出了一种新范式，通过生成中间视觉思考步骤，使大型多模态模型（LMMs）能够在文本和视觉模态间进行跨模态思考，显著提升了视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前LMMs的视觉推理局限于处理固定用户提供的图像或仅通过文本链式思考（CoT），缺乏主动生成和迭代视觉假设的能力。

Method: 通过两种机制实现：（1）基于中间视觉子目标的视觉生成，将复杂任务分解为可管理的组件；（2）带自我批判的视觉生成，模型生成初始假设并通过文本分析改进。

Result: 在视觉生成基准测试中，模型在复杂多对象场景中的表现相对提升了50%（从38%到57%）。

Conclusion: 该方法为AI模型提供了类似人类的视觉想象和迭代优化能力，适用于多个领域，如生物化学、建筑设计和法医分析等。

Abstract: We present Thinking with Generated Images, a novel paradigm that
fundamentally transforms how large multimodal models (LMMs) engage with visual
reasoning by enabling them to natively think across text and vision modalities
through spontaneous generation of intermediate visual thinking steps. Current
visual reasoning with LMMs is constrained to either processing fixed
user-provided images or reasoning solely through text-based chain-of-thought
(CoT). Thinking with Generated Images unlocks a new dimension of cognitive
capability where models can actively construct intermediate visual thoughts,
critique their own visual hypotheses, and refine them as integral components of
their reasoning process. We demonstrate the effectiveness of our approach
through two complementary mechanisms: (1) vision generation with intermediate
visual subgoals, where models decompose complex visual tasks into manageable
components that are generated and integrated progressively, and (2) vision
generation with self-critique, where models generate an initial visual
hypothesis, analyze its shortcomings through textual reasoning, and produce
refined outputs based on their own critiques. Our experiments on vision
generation benchmarks show substantial improvements over baseline approaches,
with our models achieving up to 50% (from 38% to 57%) relative improvement in
handling complex multi-object scenarios. From biochemists exploring novel
protein structures, and architects iterating on spatial designs, to forensic
analysts reconstructing crime scenes, and basketball players envisioning
strategic plays, our approach enables AI models to engage in the kind of visual
imagination and iterative refinement that characterizes human creative,
analytical, and strategic thinking. We release our open-source suite at
https://github.com/GAIR-NLP/thinking-with-generated-images.

</details>


### [143] [RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting](https://arxiv.org/abs/2505.22535)
*Mohamad Hakam Shams Eddin,Yikui Zahng,Stefan Kollet,Juergen Gall*

Main category: cs.CV

TL;DR: RiverMamba是一种新型深度学习模型，用于全球河流流量和洪水预测，通过时空建模提升预测能力，优于现有AI和物理模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在洪水预测中局限于局部应用，未充分利用水体的空间联系，需开发能建模时空关系的新方法。

Method: RiverMamba利用Mamba块和长期再分析数据预训练，整合ECMWF HRES气象预报，进行时空建模。

Result: RiverMamba在7天预测期内可靠预测河流流量和极端洪水，超越现有AI和物理模型。

Conclusion: RiverMamba为科学和业务应用提供了高效的全球洪水预测解决方案。

Abstract: Recent deep learning approaches for river discharge forecasting have improved
the accuracy and efficiency in flood forecasting, enabling more reliable early
warning systems for risk management. Nevertheless, existing deep learning
approaches in hydrology remain largely confined to local-scale applications and
do not leverage the inherent spatial connections of bodies of water. Thus,
there is a strong need for new deep learning methodologies that are capable of
modeling spatio-temporal relations to improve river discharge and flood
forecasting for scientific and operational applications. To address this, we
present RiverMamba, a novel deep learning model that is pretrained with
long-term reanalysis data and that can forecast global river discharge and
floods on a $0.05^\circ$ grid up to 7 days lead time, which is of high
relevance in early warning. To achieve this, RiverMamba leverages efficient
Mamba blocks that enable the model to capture global-scale channel network
routing and enhance its forecast capability for longer lead times. The forecast
blocks integrate ECMWF HRES meteorological forecasts, while accounting for
their inaccuracies through spatio-temporal modeling. Our analysis demonstrates
that RiverMamba delivers reliable predictions of river discharge, including
extreme floods across return periods and lead times, surpassing both
operational AI- and physics-based models.

</details>


### [144] [Scaling-up Perceptual Video Quality Assessment](https://arxiv.org/abs/2505.22543)
*Ziheng Jia,Zicheng Zhang,Zeyu Zhang,Yingji Liang,Xiaorong Zhu,Chunyi Li,Jinliang Han,Haoning Wu,Bin Wang,Haoran Zhang,Guanyu Zhu,Qiyong Zhao,Xiaohong Liu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: OmniVQA框架通过构建大规模多模态指令数据库（MIDB）和数据集，解决了视频质量评估（VQA）领域数据稀缺问题，并在质量和评分任务中取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 视频质量评估领域因数据稀缺和规模不足，未能充分利用数据扩展定律的潜力。

Method: 提出OmniVQA框架，构建OmniVQA-Chat-400K和OmniVQA-MOS-20K数据集，采用互补训练策略，并设计OmniVQA-FG-Benchmark进行评估。

Result: 模型在质量和评分任务中均达到最先进性能。

Conclusion: OmniVQA框架成功解决了VQA领域的数据扩展问题，并显著提升了模型性能。

Abstract: The data scaling law has been shown to significantly enhance the performance
of large multi-modal models (LMMs) across various downstream tasks. However, in
the domain of perceptual video quality assessment (VQA), the potential of
scaling law remains unprecedented due to the scarcity of labeled resources and
the insufficient scale of datasets. To address this, we propose
\textbf{OmniVQA}, an efficient framework designed to efficiently build
high-quality, human-in-the-loop VQA multi-modal instruction databases (MIDBs).
We then scale up to create \textbf{OmniVQA-Chat-400K}, the largest MIDB in the
VQA field concurrently. Our focus is on the technical and aesthetic quality
dimensions, with abundant in-context instruction data to provide fine-grained
VQA knowledge. Additionally, we have built the \textbf{OmniVQA-MOS-20K} dataset
to enhance the model's quantitative quality rating capabilities. We then
introduce a \textbf{complementary} training strategy that effectively leverages
the knowledge from datasets for quality understanding and quality rating tasks.
Furthermore, we propose the \textbf{OmniVQA-FG (fine-grain)-Benchmark} to
evaluate the fine-grained performance of the models. Our results demonstrate
that our models achieve state-of-the-art performance in both quality
understanding and rating tasks.

</details>


### [145] [Deep Learning-Based BMD Estimation from Radiographs with Conformal Uncertainty Quantification](https://arxiv.org/abs/2505.22551)
*Long Hui,Wai Lok Yeung*

Main category: cs.CV

TL;DR: 利用深度学习从膝关节X光片中估算骨密度（BMD），提出了一种基于EfficientNet模型的方法，并通过Split Conformal Prediction提供统计严格的预测区间。


<details>
  <summary>Details</summary>
Motivation: DXA设备有限，阻碍了骨质疏松筛查，本研究旨在利用广泛可用的膝关节X光片进行BMD估算。

Method: 使用EfficientNet模型在OAI数据集上训练，预测BMD；比较两种测试时间增强（TTA）方法；采用Split Conformal Prediction生成预测区间。

Result: Pearson相关系数为0.68（传统TTA）；多样本TTA方法产生更紧密的置信区间，同时保持覆盖率。

Conclusion: 尽管膝关节X光片与标准DXA存在解剖学差异，但该方法为基于常规X光片的可信AI辅助BMD筛查奠定了基础。

Abstract: Limited DXA access hinders osteoporosis screening. This proof-of-concept
study proposes using widely available knee X-rays for opportunistic Bone
Mineral Density (BMD) estimation via deep learning, emphasizing robust
uncertainty quantification essential for clinical use. An EfficientNet model
was trained on the OAI dataset to predict BMD from bilateral knee radiographs.
Two Test-Time Augmentation (TTA) methods were compared: traditional averaging
and a multi-sample approach. Crucially, Split Conformal Prediction was
implemented to provide statistically rigorous, patient-specific prediction
intervals with guaranteed coverage. Results showed a Pearson correlation of
0.68 (traditional TTA). While traditional TTA yielded better point predictions,
the multi-sample approach produced slightly tighter confidence intervals (90%,
95%, 99%) while maintaining coverage. The framework appropriately expressed
higher uncertainty for challenging cases. Although anatomical mismatch between
knee X-rays and standard DXA limits immediate clinical use, this method
establishes a foundation for trustworthy AI-assisted BMD screening using
routine radiographs, potentially improving early osteoporosis detection.

</details>


### [146] [MultiFormer: A Multi-Person Pose Estimation System Based on CSI and Attention Mechanism](https://arxiv.org/abs/2505.22555)
*Yanyi Qu,Haoyang Ma,Wenhui Xiong*

Main category: cs.CV

TL;DR: MultiFormer是一种基于CSI的无线感知系统，通过Transformer架构和特征融合网络实现高精度多人姿态估计。


<details>
  <summary>Details</summary>
Motivation: 解决基于CSI的多人体姿态估计中的准确性和特征学习挑战。

Method: 采用基于Transformer的时频双令牌特征提取器和多阶段特征融合网络（MSFN）。

Result: 在公开和自收集数据集上表现优于现有方法，尤其是对高动态关键点的估计。

Conclusion: MultiFormer在CSI姿态估计中表现出更高的准确性和鲁棒性。

Abstract: Human pose estimation based on Channel State Information (CSI) has emerged as
a promising approach for non-intrusive and precise human activity monitoring,
yet faces challenges including accurate multi-person pose recognition and
effective CSI feature learning. This paper presents MultiFormer, a wireless
sensing system that accurately estimates human pose through CSI. The proposed
system adopts a Transformer based time-frequency dual-token feature extractor
with multi-head self-attention. This feature extractor is able to model
inter-subcarrier correlations and temporal dependencies of the CSI. The
extracted CSI features and the pose probability heatmaps are then fused by
Multi-Stage Feature Fusion Network (MSFN) to enforce the anatomical
constraints. Extensive experiments conducted on on the public MM-Fi dataset and
our self-collected dataset show that the MultiFormer achieves higher accuracy
over state-of-the-art approaches, especially for high-mobility keypoints
(wrists, elbows) that are particularly difficult for previous methods to
accurately estimate.

</details>


### [147] [PRISM: Video Dataset Condensation with Progressive Refinement and Insertion for Sparse Motion](https://arxiv.org/abs/2505.22564)
*Jaehyun Choi,Jiwan Hur,Gyojin Han,Jaemyung Yu,Junmo Kim*

Main category: cs.CV

TL;DR: PRISM是一种新颖的视频数据集压缩方法，通过渐进式细化和插入稀疏运动帧，保留空间内容与时间动态的相互依赖关系，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大规模视频数据处理的计算挑战，同时保留视频中空间内容与时间动态的复杂关系。

Method: 提出PRISM方法，通过渐进式细化和插入帧来适应动作中的运动，并考虑每帧的梯度关系。

Result: 在标准视频动作识别基准测试中，PRISM表现优于现有方法，同时保持紧凑的表示。

Conclusion: PRISM为视频数据集压缩提供了一种高效且性能优越的解决方案，适用于资源受限环境。

Abstract: Video dataset condensation has emerged as a critical technique for addressing
the computational challenges associated with large-scale video data processing
in deep learning applications. While significant progress has been made in
image dataset condensation, the video domain presents unique challenges due to
the complex interplay between spatial content and temporal dynamics. This paper
introduces PRISM, Progressive Refinement and Insertion for Sparse Motion, for
video dataset condensation, a novel approach that fundamentally reconsiders how
video data should be condensed. Unlike the previous method that separates
static content from dynamic motion, our method preserves the essential
interdependence between these elements. Our approach progressively refines and
inserts frames to fully accommodate the motion in an action while achieving
better performance but less storage, considering the relation of gradients for
each frame. Extensive experiments across standard video action recognition
benchmarks demonstrate that PRISM outperforms existing disentangled approaches
while maintaining compact representations suitable for resource-constrained
environments.

</details>


### [148] [Universal Visuo-Tactile Video Understanding for Embodied Interaction](https://arxiv.org/abs/2505.22566)
*Yifan Xie,Mingyang Li,Shoujie Li,Xingting Li,Guangyu Chen,Fei Ma,Fei Richard Yu,Wenbo Ding*

Main category: cs.CV

TL;DR: VTV-LLM是一种多模态大语言模型，结合视觉与触觉视频（VTV）理解，填补了触觉感知与自然语言之间的空白。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉和语言模态上取得了进展，但未能有效整合触觉信息，而触觉对真实世界交互至关重要。

Method: 提出VTV150K数据集，包含15万帧视频，覆盖100种物体和三种触觉传感器；开发三阶段训练范式（VTV增强、VTV-文本对齐、文本提示微调）。

Result: VTV-LLM在触觉视频理解任务中表现优异，支持复杂触觉推理能力。

Conclusion: VTV-LLM为触觉领域的人机交互奠定了基础。

Abstract: Tactile perception is essential for embodied agents to understand physical
attributes of objects that cannot be determined through visual inspection
alone. While existing approaches have made progress in visual and language
modalities for physical understanding, they fail to effectively incorporate
tactile information that provides crucial haptic feedback for real-world
interaction. In this paper, we present VTV-LLM, the first multi-modal large
language model for universal Visuo-Tactile Video (VTV) understanding that
bridges the gap between tactile perception and natural language. To address the
challenges of cross-sensor and cross-modal integration, we contribute VTV150K,
a comprehensive dataset comprising 150,000 video frames from 100 diverse
objects captured across three different tactile sensors (GelSight Mini, DIGIT,
and Tac3D), annotated with four fundamental tactile attributes (hardness,
protrusion, elasticity, and friction). We develop a novel three-stage training
paradigm that includes VTV enhancement for robust visuo-tactile representation,
VTV-text alignment for cross-modal correspondence, and text prompt finetuning
for natural language generation. Our framework enables sophisticated tactile
reasoning capabilities including feature assessment, comparative analysis,
scenario-based decision making and so on. Experimental evaluations demonstrate
that VTV-LLM achieves superior performance in tactile video understanding
tasks, establishing a foundation for more intuitive human-machine interaction
in tactile domains.

</details>


### [149] [ImageReFL: Balancing Quality and Diversity in Human-Aligned Diffusion Models](https://arxiv.org/abs/2505.22569)
*Dmitrii Sorokin,Maksim Nakhodnov,Andrey Kuznetsov,Aibek Alanov*

Main category: cs.CV

TL;DR: 论文提出两种方法解决扩散模型在人类偏好对齐与多样性之间的权衡问题：1）结合生成策略，仅在生成后期应用奖励调优模型；2）ImageReFL方法，通过多正则化器提升多样性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面表现优异，但如何与人类偏好对齐且保持多样性仍具挑战性。

Method: 1）结合生成策略；2）ImageReFL方法，结合真实图像训练和多正则化器。

Result: 方法在质量和多样性指标上优于传统奖励调优方法，用户研究证实其平衡了偏好对齐与多样性。

Conclusion: 提出的方法有效解决了扩散模型在偏好对齐与多样性之间的权衡问题。

Abstract: Recent advances in diffusion models have led to impressive image generation
capabilities, but aligning these models with human preferences remains
challenging. Reward-based fine-tuning using models trained on human feedback
improves alignment but often harms diversity, producing less varied outputs. In
this work, we address this trade-off with two contributions. First, we
introduce \textit{combined generation}, a novel sampling strategy that applies
a reward-tuned diffusion model only in the later stages of the generation
process, while preserving the base model for earlier steps. This approach
mitigates early-stage overfitting and helps retain global structure and
diversity. Second, we propose \textit{ImageReFL}, a fine-tuning method that
improves image diversity with minimal loss in quality by training on real
images and incorporating multiple regularizers, including diffusion and ReFL
losses. Our approach outperforms conventional reward tuning methods on standard
quality and diversity metrics. A user study further confirms that our method
better balances human preference alignment and visual diversity. The source
code can be found at https://github.com/ControlGenAI/ImageReFL .

</details>


### [150] [Tell me Habibi, is it Real or Fake?](https://arxiv.org/abs/2505.22581)
*Kartik Kuckreja,Parul Gupta,Injy Hamed,Thamar Solorio,Muhammad Haris Khan,Abhinav Dhall*

Main category: cs.CV

TL;DR: 论文介绍了首个大规模阿拉伯语-英语视听深度伪造数据集ArEnAV，包含387k视频和765小时内容，支持多语言和代码切换研究。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测研究多集中于单语言内容，忽略了多语言和代码切换的挑战，尤其是阿拉伯语和英语混合的情况。

Method: 通过整合四种文本到语音和两种唇同步模型，生成包含代码切换、方言变化和单语言内容的数据集。

Result: 数据集在单语言和多语言数据集、先进检测模型及人类评估中表现优异，推动了深度伪造研究。

Conclusion: ArEnAV填补了多语言深度伪造检测的空白，为未来研究提供了重要资源。

Abstract: Deepfake generation methods are evolving fast, making fake media harder to
detect and raising serious societal concerns. Most deepfake detection and
dataset creation research focuses on monolingual content, often overlooking the
challenges of multilingual and code-switched speech, where multiple languages
are mixed within the same discourse. Code-switching, especially between Arabic
and English, is common in the Arab world and is widely used in digital
communication. This linguistic mixing poses extra challenges for deepfake
detection, as it can confuse models trained mostly on monolingual data. To
address this, we introduce \textbf{ArEnAV}, the first large-scale
Arabic-English audio-visual deepfake dataset featuring intra-utterance
code-switching, dialectal variation, and monolingual Arabic content. It
\textbf{contains 387k videos and over 765 hours of real and fake videos}. Our
dataset is generated using a novel pipeline integrating four Text-To-Speech and
two lip-sync models, enabling comprehensive analysis of multilingual multimodal
deepfake detection. We benchmark our dataset against existing monolingual and
multilingual datasets, state-of-the-art deepfake detection models, and a human
evaluation, highlighting its potential to advance deepfake research. The
dataset can be accessed
\href{https://huggingface.co/datasets/kartik060702/ArEnAV-Full}{here}.

</details>


### [151] [SAM-R1: Leveraging SAM for Reward Feedback in Multimodal Segmentation via Reinforcement Learning](https://arxiv.org/abs/2505.22596)
*Jiaqi Huang,Zunnan Xu,Jun Zhou,Ting Liu,Yicheng Xiao,Mingwen Ou,Bowen Ji,Xiu Li,Kehong Yuan*

Main category: cs.CV

TL;DR: SAM-R1是一种新颖的多模态大模型框架，通过强化学习实现图像分割任务的细粒度推理，无需依赖昂贵的人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵的人工标注数据，而强化学习可以为大模型提供推理能力，无需此类数据。

Method: 结合细粒度分割设置和任务特定奖励，利用Segment Anything Model（SAM）作为奖励提供者，优化模型推理与分割对齐。

Result: 仅用3k训练样本，SAM-R1在多个基准测试中表现优异。

Conclusion: 强化学习能有效为多模态模型提供面向分割的推理能力。

Abstract: Leveraging multimodal large models for image segmentation has become a
prominent research direction. However, existing approaches typically rely
heavily on manually annotated datasets that include explicit reasoning
processes, which are costly and time-consuming to produce. Recent advances
suggest that reinforcement learning (RL) can endow large models with reasoning
capabilities without requiring such reasoning-annotated data. In this paper, we
propose SAM-R1, a novel framework that enables multimodal large models to
perform fine-grained reasoning in image understanding tasks. Our approach is
the first to incorporate fine-grained segmentation settings during the training
of multimodal reasoning models. By integrating task-specific, fine-grained
rewards with a tailored optimization objective, we further enhance the model's
reasoning and segmentation alignment. We also leverage the Segment Anything
Model (SAM) as a strong and flexible reward provider to guide the learning
process. With only 3k training samples, SAM-R1 achieves strong performance
across multiple benchmarks, demonstrating the effectiveness of reinforcement
learning in equipping multimodal models with segmentation-oriented reasoning
capabilities.

</details>


### [152] [Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective](https://arxiv.org/abs/2505.22604)
*Ruixuan Zhang,He Wang,Zhengyu Zhao,Zhiqing Guo,Xun Yang,Yunfeng Diao,Meng Wang*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练的对抗防御方法TRIM，用于检测AI生成图像（AIGI），通过信息论方法解决现有对抗训练（AT）的性能崩溃问题。


<details>
  <summary>Details</summary>
Motivation: AI生成图像的恶意使用（如伪造和虚假信息）日益增多，现有检测器普遍易受对抗攻击，且防御方法稀缺。

Method: 提出TRIM方法，基于标准检测器，利用预测熵和KL散度量化特征偏移，无需训练即可防御对抗攻击。

Result: 在多数据集和攻击场景下的实验表明，TRIM显著优于现有防御方法，如在ProGAN和GenImage上分别提升33.88%和28.91%。

Conclusion: TRIM是一种高效且无需训练的对抗防御方法，能有效解决AIGI检测中的性能崩溃问题，同时保持原始检测精度。

Abstract: Rapid advances in Artificial Intelligence Generated Images (AIGI) have
facilitated malicious use, such as forgery and misinformation. Therefore,
numerous methods have been proposed to detect fake images. Although such
detectors have been proven to be universally vulnerable to adversarial attacks,
defenses in this field are scarce. In this paper, we first identify that
adversarial training (AT), widely regarded as the most effective defense,
suffers from performance collapse in AIGI detection. Through an
information-theoretic lens, we further attribute the cause of collapse to
feature entanglement, which disrupts the preservation of feature-label mutual
information. Instead, standard detectors show clear feature separation.
Motivated by this difference, we propose Training-free Robust Detection via
Information-theoretic Measures (TRIM), the first training-free adversarial
defense for AIGI detection. TRIM builds on standard detectors and quantifies
feature shifts using prediction entropy and KL divergence. Extensive
experiments across multiple datasets and attacks validate the superiority of
our TRIM, e.g., outperforming the state-of-the-art defense by 33.88% (28.91%)
on ProGAN (GenImage), while well maintaining original accuracy.

</details>


### [153] [RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction](https://arxiv.org/abs/2505.22613)
*Yuchi Wang,Yishuo Cai,Shuhuai Ren,Sihan Yang,Linli Yao,Yuanxin Liu,Yuanxing Zhang,Pengfei Wan,Xu Sun*

Main category: cs.CV

TL;DR: RICO是一种通过视觉重建优化图像标题的新框架，利用文本到图像模型和MLLM迭代改进标题，显著提升准确性和完整性。


<details>
  <summary>Details</summary>
Motivation: 现有图像标题增强方法因幻觉和细节缺失导致不准确，需要更可靠的解决方案。

Method: 通过文本到图像模型重建参考图像，利用MLLM识别差异并迭代优化标题；RICO-Flash通过DPO减少计算成本。

Result: 在CapsBench和CompreCap上表现优于基线约10%。

Conclusion: RICO显著提升标题质量，代码已开源。

Abstract: Image recaptioning is widely used to generate training datasets with enhanced
quality for various multimodal tasks. Existing recaptioning methods typically
rely on powerful multimodal large language models (MLLMs) to enhance textual
descriptions, but often suffer from inaccuracies due to hallucinations and
incompleteness caused by missing fine-grained details. To address these
limitations, we propose RICO, a novel framework that refines captions through
visual reconstruction. Specifically, we leverage a text-to-image model to
reconstruct a caption into a reference image, and prompt an MLLM to identify
discrepancies between the original and reconstructed images to refine the
caption. This process is performed iteratively, further progressively promoting
the generation of more faithful and comprehensive descriptions. To mitigate the
additional computational cost induced by the iterative process, we introduce
RICO-Flash, which learns to generate captions like RICO using DPO. Extensive
experiments demonstrate that our approach significantly improves caption
accuracy and completeness, outperforms most baselines by approximately 10% on
both CapsBench and CompreCap. Code released at
https://github.com/wangyuchi369/RICO.

</details>


### [154] [ObjectClear: Complete Object Removal via Object-Effect Attention](https://arxiv.org/abs/2505.22636)
*Jixin Zhao,Shangchen Zhou,Zhouxia Wang,Peiqing Yang,Chen Change Loy*

Main category: cs.CV

TL;DR: 论文提出了一种新的数据集OBER和框架ObjectClear，用于解决基于扩散的图像修复方法在移除物体及其效果时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散修复方法在移除物体及其效果（如阴影、反射）时存在伪影、内容幻觉和背景改变等问题。

Method: 构建OBER数据集，包含成对图像和精确掩码；提出ObjectClear框架，引入物体效果注意力机制，分离前景移除和背景重建。

Result: ObjectClear在复杂场景中表现优异，显著提升了物体效果移除质量和背景保真度。

Conclusion: OBER数据集和ObjectClear框架为物体效果移除提供了有效解决方案，尤其在复杂场景中表现突出。

Abstract: Object removal requires eliminating not only the target object but also its
effects, such as shadows and reflections. However, diffusion-based inpainting
methods often produce artifacts, hallucinate content, alter background, and
struggle to remove object effects accurately. To address this challenge, we
introduce a new dataset for OBject-Effect Removal, named OBER, which provides
paired images with and without object effects, along with precise masks for
both objects and their associated visual artifacts. The dataset comprises
high-quality captured and simulated data, covering diverse object categories
and complex multi-object scenes. Building on OBER, we propose a novel
framework, ObjectClear, which incorporates an object-effect attention mechanism
to guide the model toward the foreground removal regions by learning attention
masks, effectively decoupling foreground removal from background
reconstruction. Furthermore, the predicted attention map enables an
attention-guided fusion strategy during inference, greatly preserving
background details. Extensive experiments demonstrate that ObjectClear
outperforms existing methods, achieving improved object-effect removal quality
and background fidelity, especially in complex scenarios.

</details>


### [155] [SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation](https://arxiv.org/abs/2505.22643)
*Dekai Zhu,Yixuan Hu,Youquan Liu,Dongyue Lu,Lingdong Kong,Slobodan Ilic*

Main category: cs.CV

TL;DR: Spiral是一种新型的LiDAR扩散模型，可同时生成深度、反射率和语义图，解决了现有范围视图方法在语义标注上的不足，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有范围视图方法无法生成带语义标签的LiDAR场景，而依赖预训练分割模型会导致跨模态一致性不佳。

Method: 提出Spiral模型，结合范围视图表示的优势（计算高效、网络设计简单），同时生成深度、反射率和语义图，并引入新的语义感知评估指标。

Result: 在SemanticKITTI和nuScenes数据集上，Spiral性能最优且参数最少，优于生成与分割模型结合的两步方法。

Conclusion: Spiral不仅提升了生成数据的质量，还能有效用于下游分割训练的合成数据增强，显著减少标注工作量。

Abstract: Leveraging recent diffusion models, LiDAR-based large-scale 3D scene
generation has achieved great success. While recent voxel-based approaches can
generate both geometric structures and semantic labels, existing range-view
methods are limited to producing unlabeled LiDAR scenes. Relying on pretrained
segmentation models to predict the semantic maps often results in suboptimal
cross-modal consistency. To address this limitation while preserving the
advantages of range-view representations, such as computational efficiency and
simplified network design, we propose Spiral, a novel range-view LiDAR
diffusion model that simultaneously generates depth, reflectance images, and
semantic maps. Furthermore, we introduce novel semantic-aware metrics to
evaluate the quality of the generated labeled range-view data. Experiments on
the SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves
state-of-the-art performance with the smallest parameter size, outperforming
two-step methods that combine the generative and segmentation models.
Additionally, we validate that range images generated by Spiral can be
effectively used for synthetic data augmentation in the downstream segmentation
training, significantly reducing the labeling effort on LiDAR data.

</details>


### [156] [Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation](https://arxiv.org/abs/2505.22647)
*Zhe Kong,Feng Gao,Yong Zhang,Zhuoliang Kang,Xiaoming Wei,Xunliang Cai,Guanying Chen,Wenhan Luo*

Main category: cs.CV

TL;DR: 论文提出了一种新任务——多人对话视频生成，并提出了MultiTalk框架，解决了多人生成中的音频绑定和指令跟随问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单人动画，难以处理多流音频输入和音频与人物绑定问题，且指令跟随能力有限。

Method: 提出Label Rotary Position Embedding (L-RoPE)方法解决音频绑定问题，并通过部分参数训练和多任务训练保持基础模型的指令跟随能力。

Result: MultiTalk在多个数据集上表现优于其他方法，展示了强大的生成能力。

Conclusion: MultiTalk框架有效解决了多人对话视频生成中的关键问题，具有广泛应用潜力。

Abstract: Audio-driven human animation methods, such as talking head and talking body
generation, have made remarkable progress in generating synchronized facial
movements and appealing visual quality videos. However, existing methods
primarily focus on single human animation and struggle with multi-stream audio
inputs, facing incorrect binding problems between audio and persons.
Additionally, they exhibit limitations in instruction-following capabilities.
To solve this problem, in this paper, we propose a novel task: Multi-Person
Conversational Video Generation, and introduce a new framework, MultiTalk, to
address the challenges during multi-person generation. Specifically, for audio
injection, we investigate several schemes and propose the Label Rotary Position
Embedding (L-RoPE) method to resolve the audio and person binding problem.
Furthermore, during training, we observe that partial parameter training and
multi-task training are crucial for preserving the instruction-following
ability of the base model. MultiTalk achieves superior performance compared to
other methods on several datasets, including talking head, talking body, and
multi-person datasets, demonstrating the powerful generation capabilities of
our approach.

</details>


### [157] [Sherlock: Self-Correcting Reasoning in Vision-Language Models](https://arxiv.org/abs/2505.22651)
*Yi Ding,Ruqi Zhang*

Main category: cs.CV

TL;DR: 论文提出Sherlock框架，通过自校正和自改进提升视觉语言模型的推理能力，仅需少量标注数据即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型对推理错误敏感，依赖大量标注数据或验证器，且泛化能力有限。

Method: 引入轨迹级自校正目标、基于视觉扰动的偏好数据构建方法及动态β偏好调整。

Result: 在八个基准测试中平均准确率达64.1（直接生成）和65.4（自校正后），优于其他模型。

Conclusion: Sherlock框架有效提升模型性能，减少对标注数据的依赖。

Abstract: Reasoning Vision-Language Models (VLMs) have shown promising performance on
complex multimodal tasks. However, they still face significant challenges: they
are highly sensitive to reasoning errors, require large volumes of annotated
data or accurate verifiers, and struggle to generalize beyond specific domains.
To address these limitations, we explore self-correction as a strategy to
enhance reasoning VLMs. We first conduct an in-depth analysis of reasoning
VLMs' self-correction abilities and identify key gaps. Based on our findings,
we introduce Sherlock, a self-correction and self-improvement training
framework. Sherlock introduces a trajectory-level self-correction objective, a
preference data construction method based on visual perturbation, and a dynamic
$\beta$ for preference tuning. Once the model acquires self-correction
capabilities using only 20k randomly sampled annotated data, it continues to
self-improve without external supervision. Built on the Llama3.2-Vision-11B
model, Sherlock achieves remarkable results across eight benchmarks, reaching
an average accuracy of 64.1 with direct generation and 65.4 after
self-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and
LlamaV-o1 (63.4) while using less than 20% of the annotated data.

</details>


### [158] [VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models](https://arxiv.org/abs/2505.22654)
*Ce Zhang,Kaixin Ma,Tianqing Fang,Wenhao Yu,Hongming Zhang,Zhisong Zhang,Yaqi Xie,Katia Sycara,Haitao Mi,Dong Yu*

Main category: cs.CV

TL;DR: VScan是一个两阶段视觉令牌减少框架，通过全局和局部扫描结合令牌合并以及语言模型中间层剪枝，显著加速推理并保持高性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）因视觉令牌序列较长导致计算成本高，影响实时部署，需优化令牌冗余问题。

Method: 提出VScan框架，包括视觉编码阶段的全局和局部扫描与令牌合并，以及语言模型中间层的剪枝。

Result: 在四个LVLMs上验证，VScan实现推理加速（如LLaVA-NeXT-7B提速2.91倍），FLOPs减少10倍，性能保留95.4%。

Conclusion: VScan通过优化令牌处理，显著提升效率且性能损失小，优于现有方法。

Abstract: Recent Large Vision-Language Models (LVLMs) have advanced multi-modal
understanding by incorporating finer-grained visual perception and encoding.
However, such methods incur significant computational costs due to longer
visual token sequences, posing challenges for real-time deployment. To mitigate
this, prior studies have explored pruning unimportant visual tokens either at
the output layer of the visual encoder or at the early layers of the language
model. In this work, we revisit these design choices and reassess their
effectiveness through comprehensive empirical studies of how visual tokens are
processed throughout the visual encoding and language decoding stages. Guided
by these insights, we propose VScan, a two-stage visual token reduction
framework that addresses token redundancy by: (1) integrating complementary
global and local scans with token merging during visual encoding, and (2)
introducing pruning at intermediate layers of the language model. Extensive
experimental results across four LVLMs validate the effectiveness of VScan in
accelerating inference and demonstrate its superior performance over current
state-of-the-arts on sixteen benchmarks. Notably, when applied to
LLaVA-NeXT-7B, VScan achieves a 2.91$\times$ speedup in prefilling and a
10$\times$ reduction in FLOPs, while retaining 95.4% of the original
performance.

</details>


### [159] [3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model](https://arxiv.org/abs/2505.22657)
*Wenbo Hu,Yining Hong,Yanjun Wang,Leison Gao,Zibu Wei,Xingcheng Yao,Nanyun Peng,Yonatan Bitton,Idan Szpektor,Kai-Wei Chang*

Main category: cs.CV

TL;DR: 论文提出了一种名为3DLLM-Mem的动态记忆管理模型，用于提升大型语言模型（LLMs）在3D环境中的空间-时间记忆能力，并通过3DMem-Bench基准测试验证其性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在动态、多房间3D环境中规划和行动能力不足，部分原因是缺乏有效的3D空间-时间记忆建模。

Method: 提出3DLLM-Mem模型，利用工作记忆令牌选择性融合来自情景记忆的空间和时间特征，以提升任务相关信息的处理效率。

Result: 3DLLM-Mem在3DMem-Bench测试中表现优异，成功率达到16.5%的提升。

Conclusion: 3DLLM-Mem通过动态记忆管理显著提升了LLMs在复杂3D环境中的任务执行能力。

Abstract: Humans excel at performing complex tasks by leveraging long-term memory
across temporal and spatial experiences. In contrast, current Large Language
Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D
environments. We posit that part of this limitation is due to the lack of
proper 3D spatial-temporal memory modeling in LLMs. To address this, we first
introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000
trajectories and 2,892 embodied tasks, question-answering and captioning,
designed to evaluate an agent's ability to reason over long-term memory in 3D
environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management
and fusion model for embodied spatial-temporal reasoning and actions in LLMs.
Our model uses working memory tokens, which represents current observations, as
queries to selectively attend to and fuse the most useful spatial and temporal
features from episodic memory, which stores past observations and interactions.
Our approach allows the agent to focus on task-relevant information while
maintaining memory efficiency in complex, long-horizon environments.
Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art
performance across various tasks, outperforming the strongest baselines by
16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied
tasks.

</details>


### [160] [Training Free Stylized Abstraction](https://arxiv.org/abs/2505.22663)
*Aimon Rahman,Kartik Narayan,Vishal M. Patel*

Main category: cs.CV

TL;DR: 提出了一种无需训练的框架，通过视觉语言模型和跨域校正流反演策略生成风格化抽象图像，支持多轮生成并引入StyleBench评估。


<details>
  <summary>Details</summary>
Motivation: 解决风格化抽象中身份特征保留与风格差异平衡的挑战，尤其是对分布外个体的处理。

Method: 使用推理时缩放提取身份特征，结合跨域校正流反演策略动态重建结构，支持多轮生成。

Result: 在多种风格（如乐高、针织玩偶、南方公园）上表现出对未见身份和风格的强泛化能力。

Conclusion: 方法在开源环境下高效生成高保真风格化抽象图像，适用于多样化风格和身份。

Abstract: Stylized abstraction synthesizes visually exaggerated yet semantically
faithful representations of subjects, balancing recognizability with perceptual
distortion. Unlike image-to-image translation, which prioritizes structural
fidelity, stylized abstraction demands selective retention of identity cues
while embracing stylistic divergence, especially challenging for
out-of-distribution individuals. We propose a training-free framework that
generates stylized abstractions from a single image using inference-time
scaling in vision-language models (VLLMs) to extract identity-relevant
features, and a novel cross-domain rectified flow inversion strategy that
reconstructs structure based on style-dependent priors. Our method adapts
structural restoration dynamically through style-aware temporal scheduling,
enabling high-fidelity reconstructions that honor both subject and style. It
supports multi-round abstraction-aware generation without fine-tuning. To
evaluate this task, we introduce StyleBench, a GPT-based human-aligned metric
suited for abstract styles where pixel-level similarity fails. Experiments
across diverse abstraction (e.g., LEGO, knitted dolls, South Park) show strong
generalization to unseen identities and styles in a fully open-source setup.

</details>


### [161] [Zero-Shot Vision Encoder Grafting via LLM Surrogates](https://arxiv.org/abs/2505.22664)
*Kaiyu Yue,Vasu Singla,Menglin Jia,John Kirchenbauer,Rifaa Qadri,Zikui Cai,Abhinav Bhatele,Furong Huang,Tom Goldstein*

Main category: cs.CV

TL;DR: 通过训练小型代理模型（共享目标大语言模型的浅层）来预训练视觉编码器，再将其迁移到大模型上，显著降低训练成本（约45%），且性能接近全模型训练。


<details>
  <summary>Details</summary>
Motivation: 降低视觉语言模型（VLM）训练成本，尤其是大语言模型（LLM）作为解码器时的计算负担。

Method: 构建小型代理模型（继承目标LLM的浅层），预训练视觉编码器后零-shot迁移到大模型（称为零-shot嫁接）。

Result: 嫁接后的模型性能超越代理模型，部分任务接近全模型训练效果，同时训练成本降低约45%。

Conclusion: 零-shot嫁接是一种高效且经济的VLM训练策略，适用于大规模LLM场景。

Abstract: Vision language models (VLMs) typically pair a modestly sized vision encoder
with a large language model (LLM), e.g., Llama-70B, making the decoder the
primary computational burden during training. To reduce costs, a potential
promising strategy is to first train the vision encoder using a small language
model before transferring it to the large one. We construct small "surrogate
models" that share the same embedding space and representation language as the
large target LLM by directly inheriting its shallow layers. Vision encoders
trained on the surrogate can then be directly transferred to the larger model,
a process we call zero-shot grafting -- when plugged directly into the
full-size target LLM, the grafted pair surpasses the encoder-surrogate pair
and, on some benchmarks, even performs on par with full decoder training with
the target LLM. Furthermore, our surrogate training approach reduces overall
VLM training costs by ~45% when using Llama-70B as the decoder.

</details>


### [162] [Enhancing Vision Transformer Explainability Using Artificial Astrocytes](https://arxiv.org/abs/2505.21513)
*Nicolas Echevarrieta-Catalan,Ana Ribas-Rodriguez,Francisco Cedron,Odelia Schwartz,Vanessa Aguiar-Pulido*

Main category: cs.CV

TL;DR: 提出了一种无需训练的ViTA方法，通过模拟神经科学中的星形胶质细胞，提升预训练深度神经网络的可解释性，使其更符合人类感知。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型的高精度与低可解释性之间的矛盾，现有方法局限性大。

Method: 提出ViTA方法，结合神经科学中的星形胶质细胞，改进预训练模型的推理能力，生成更符合人类理解的解释。

Result: 实验表明，ViTA显著提升了Grad-CAM和Grad-CAM++等XAI技术的解释与人类感知的对齐度。

Conclusion: ViTA是一种有效的训练无关方法，可显著提升模型解释的人类对齐性。

Abstract: Machine learning models achieve high precision, but their decision-making
processes often lack explainability. Furthermore, as model complexity
increases, explainability typically decreases. Existing efforts to improve
explainability primarily involve developing new eXplainable artificial
intelligence (XAI) techniques or incorporating explainability constraints
during training. While these approaches yield specific improvements, their
applicability remains limited. In this work, we propose the Vision Transformer
with artificial Astrocytes (ViTA). This training-free approach is inspired by
neuroscience and enhances the reasoning of a pretrained deep neural network to
generate more human-aligned explanations. We evaluated our approach employing
two well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a
standard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the
similarity between the heatmaps produced by the XAI techniques and a
(human-aligned) ground truth. Our results consistently demonstrate that
incorporating artificial astrocytes enhances the alignment of model
explanations with human perception, leading to statistically significant
improvements across all XAI techniques and metrics utilized.

</details>


### [163] [Do DeepFake Attribution Models Generalize?](https://arxiv.org/abs/2505.21520)
*Spiros Baxavanakis,Manos Schinas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 论文探讨了DeepFake检测的局限性，提出多分类和归因模型的重要性，并通过实验验证了其性能提升。


<details>
  <summary>Details</summary>
Motivation: 由于DeepFake技术的普及，现有二进制检测模型无法区分不同篡改方法，亟需更精细的检测模型以增强可信度和可解释性。

Method: 利用五种先进骨干模型，在六个DeepFake数据集上进行实验，比较二进制与多分类模型的泛化能力，并评估归因模型和对比方法的性能。

Result: 二进制模型泛化能力更强，但更大模型、对比方法和高质量数据可提升归因模型的性能。

Conclusion: 归因模型在DeepFake检测中具有潜力，但需进一步优化以提高跨数据集性能。

Abstract: Recent advancements in DeepFake generation, along with the proliferation of
open-source tools, have significantly lowered the barrier for creating
synthetic media. This trend poses a serious threat to the integrity and
authenticity of online information, undermining public trust in institutions
and media. State-of-the-art research on DeepFake detection has primarily
focused on binary detection models. A key limitation of these models is that
they treat all manipulation techniques as equivalent, despite the fact that
different methods introduce distinct artifacts and visual cues. Only a limited
number of studies explore DeepFake attribution models, although such models are
crucial in practical settings. By providing the specific manipulation method
employed, these models could enhance both the perceived trustworthiness and
explainability for end users. In this work, we leverage five state-of-the-art
backbone models and conduct extensive experiments across six DeepFake datasets.
First, we compare binary and multi-class models in terms of cross-dataset
generalization. Second, we examine the accuracy of attribution models in
detecting seen manipulation methods in unknown datasets, hence uncovering data
distribution shifts on the same DeepFake manipulations. Last, we assess the
effectiveness of contrastive methods in improving cross-dataset generalization
performance. Our findings indicate that while binary models demonstrate better
generalization abilities, larger models, contrastive methods, and higher data
quality can lead to performance improvements in attribution models. The code of
this work is available on GitHub.

</details>


### [164] [CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures](https://arxiv.org/abs/2505.21522)
*Shan Gao,Zhiqiang Wu,Yawen Niu,Xiaotao Li,Qingqing Xu*

Main category: cs.CV

TL;DR: 论文提出了一种硬件-算法协同设计框架CIM-NET，通过优化架构和引入伪卷积算子CIM-CONV，显著减少了矩阵向量乘法操作，提升了在内存计算芯片上的推理速度，同时保持了去噪性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度神经网络模型未考虑内存计算芯片的架构限制，限制了其加速潜力，因此需要一种针对CIM优化的解决方案。

Method: 提出了CIM-NET架构和CIM-CONV算子，结合大感受野操作和滑动处理，减少矩阵向量乘法操作。

Result: 实验表明，CIM-NET将矩阵向量乘法操作减少至原来的1/77，同时PSNR仅略有下降（35.11 dB vs. 35.56 dB）。

Conclusion: CIM-NET框架在保持性能的同时显著提升了推理效率，适用于边缘设备的实时视频去噪。

Abstract: While deep neural network (DNN)-based video denoising has demonstrated
significant performance, deploying state-of-the-art models on edge devices
remains challenging due to stringent real-time and energy efficiency
requirements. Computing-in-Memory (CIM) chips offer a promising solution by
integrating computation within memory cells, enabling rapid matrix-vector
multiplication (MVM). However, existing DNN models are often designed without
considering CIM architectural constraints, thus limiting their acceleration
potential during inference. To address this, we propose a hardware-algorithm
co-design framework incorporating two innovations: (1) a CIM-Aware
Architecture, CIM-NET, optimized for large receptive field operation and CIM's
crossbar-based MVM acceleration; and (2) a pseudo-convolutional operator,
CIM-CONV, used within CIM-NET to integrate slide-based processing with fully
connected transformations for high-quality feature extraction and
reconstruction. This framework significantly reduces the number of MVM
operations, improving inference speed on CIM chips while maintaining
competitive performance. Experimental results indicate that, compared to the
conventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM
operations with a slight decrease in denoising performance. With a stride value
of 8, CIM-NET reduces MVM operations to 1/77th of the original, while
maintaining competitive PSNR (35.11 dB vs. 35.56 dB

</details>


### [165] [Learning Shared Representations from Unpaired Data](https://arxiv.org/abs/2505.21524)
*Amitai Yacobi,Nir Ben-Ari,Ronen Talmon,Uri Shaham*

Main category: cs.CV

TL;DR: 论文提出了一种仅通过非配对数据学习共享表示的方法，基于单模态表示的随机游走矩阵谱嵌入，展示了在跨模态任务中的高效性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态表示学习方法依赖配对样本，但配对数据难以获取，因此探索如何利用非配对数据学习共享表示。

Method: 通过构建单模态表示的随机游走矩阵的谱嵌入，实现共享表示的学习。

Result: 在计算机视觉和自然语言处理任务中表现优异，支持检索、生成、零样本分类等跨模态应用。

Conclusion: 首次证明仅用非配对数据即可学习通用跨模态嵌入，为多模态学习提供了新思路。

Abstract: Learning shared representations is a primary area of multimodal
representation learning. The current approaches to achieve a shared embedding
space rely heavily on paired samples from each modality, which are
significantly harder to obtain than unpaired ones. In this work, we demonstrate
that shared representations can be learned almost exclusively from unpaired
data. Our arguments are grounded in the spectral embeddings of the random walk
matrices constructed independently from each unimodal representation. Empirical
results in computer vision and natural language processing domains support its
potential, revealing the effectiveness of unpaired data in capturing meaningful
cross-modal relations, demonstrating high capabilities in retrieval tasks,
generation, arithmetics, zero-shot, and cross-domain classification. This work,
to the best of our knowledge, is the first to demonstrate these capabilities
almost exclusively from unpaired samples, giving rise to a cross-modal
embedding that could be viewed as universal, i.e., independent of the specific
modalities of the data. Our code IS publicly available at
https://github.com/shaham-lab/SUE.

</details>


### [166] [UniDB++: Fast Sampling of Unified Diffusion Bridge](https://arxiv.org/abs/2505.21528)
*Mokai Pan,Kaizhen Zhu,Yuexin Ma,Yanwei Fu,Jingyi Yu,Jingya Wang,Ye Shi*

Main category: cs.CV

TL;DR: UniDB++是一种无需训练的采样算法，通过精确求解反向时间SDE，显著提升了UniDB的计算效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: UniDB框架在图像生成中依赖迭代采样方法，导致计算效率低，现有加速技术无法解决其独特挑战。

Method: 推导UniDB反向时间SDE的闭式解，减少误差累积；采用数据预测模型和SDE-Corrector机制提升稳定性。

Result: UniDB++在图像修复任务中表现优异，生成质量高，采样步骤减少20倍，推理时间显著降低。

Conclusion: UniDB++在SOC驱动的扩散桥模型中实现了理论通用性与实际效率的统一。

Abstract: Diffusion Bridges enable transitions between arbitrary distributions, with
the Unified Diffusion Bridge (UniDB) framework achieving high-fidelity image
generation via a Stochastic Optimal Control (SOC) formulation. However, UniDB's
reliance on iterative Euler sampling methods results in slow, computationally
expensive inference, while existing acceleration techniques for diffusion or
diffusion bridge models fail to address its unique challenges: missing terminal
mean constraints and SOC-specific penalty coefficients in its SDEs. We present
UniDB++, a training-free sampling algorithm that significantly improves upon
these limitations. The method's key advancement comes from deriving exact
closed-form solutions for UniDB's reverse-time SDEs, effectively reducing the
error accumulation inherent in Euler approximations and enabling high-quality
generation with up to 20$\times$ fewer sampling steps. This method is further
complemented by replacing conventional noise prediction with a more stable data
prediction model, along with an SDE-Corrector mechanism that maintains
perceptual quality for low-step regimes (5-10 steps). Additionally, we
demonstrate that UniDB++ aligns with existing diffusion bridge acceleration
methods by evaluating their update rules, and UniDB++ can recover DBIMs as
special cases under some theoretical conditions. Experiments demonstrate
UniDB++'s state-of-the-art performance in image restoration tasks,
outperforming Euler-based methods in fidelity and speed while reducing
inference time significantly. This work bridges the gap between theoretical
generality and practical efficiency in SOC-driven diffusion bridge models. Our
code is available at https://github.com/2769433owo/UniDB-plusplus.

</details>


### [167] [How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control](https://arxiv.org/abs/2505.21531)
*Kunhang Li,Jason Naradowsky,Yansong Feng,Yusuke Miyao*

Main category: cs.CV

TL;DR: 论文探讨了大型语言模型（LLMs）在3D虚拟角色控制中的人类运动知识，发现LLMs擅长高层次运动规划，但在精确身体部位定位和多步骤运动方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在人类运动知识方面的能力，通过3D虚拟角色控制验证其运动规划和执行效果。

Method: 通过高层次运动规划和低层次身体部位定位，将LLMs生成的指令转化为虚拟角色动画，并进行人工和自动评估。

Result: LLMs在高层次运动解释和创意运动概念化方面表现良好，但在精确空间定位和多步骤运动方面存在困难。

Conclusion: LLMs在人类运动知识方面具有潜力，但需进一步改进以处理精确空间和复杂运动任务。

Abstract: We explore Large Language Models (LLMs)' human motion knowledge through 3D
avatar control. Given a motion instruction, we prompt LLMs to first generate a
high-level movement plan with consecutive steps (High-level Planning), then
specify body part positions in each step (Low-level Planning), which we
linearly interpolate into avatar animations as a clear verification lens for
human evaluators. Through carefully designed 20 representative motion
instructions with full coverage of basic movement primitives and balanced body
part usage, we conduct comprehensive evaluations including human assessment of
both generated animations and high-level movement plans, as well as automatic
comparison with oracle positions in low-level planning. We find that LLMs are
strong at interpreting the high-level body movements but struggle with precise
body part positioning. While breaking down motion queries into atomic
components improves planning performance, LLMs have difficulty with multi-step
movements involving high-degree-of-freedom body parts. Furthermore, LLMs
provide reasonable approximation for general spatial descriptions, but fail to
handle precise spatial specifications in text, and the precise spatial-temporal
parameters needed for avatar control. Notably, LLMs show promise in
conceptualizing creative motions and distinguishing culturally-specific motion
patterns.

</details>


### [168] [EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media](https://arxiv.org/abs/2505.21532)
*Ismail Erbas,Ferhat Demirkiran,Karthik Swaminathan,Naigang Wang,Navid Ibtehaj Nizam,Stefan T. Radev,Kaoutar El Maghraoui,Xavier Intes,Vikas Pandey*

Main category: cs.CV

TL;DR: 提出了一种基于物理指导的混合专家（MoE）框架，用于解决荧光LiDAR在散射介质中的计算挑战，显著提升了深度和荧光寿命的估计精度。


<details>
  <summary>Details</summary>
Motivation: 荧光LiDAR在散射介质中面临信号复杂、难以分离光子飞行时间和荧光寿命的问题，限制了现有方法的有效性。

Method: 采用物理指导的MoE框架，结合基于证据的Dirichlet批评器（EDCs）和决策网络，自适应融合专家预测。

Result: 在模拟的荧光LiDAR数据上验证，深度估计的NRMSE为0.030，荧光寿命的NRMSE为0.074。

Conclusion: 该框架在散射介质中表现出色，为荧光LiDAR的应用提供了更可靠的解决方案。

Abstract: Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology
employed for distance and depth estimation across medical, automotive, and
other fields, encounters significant computational challenges in scattering
media. The complex nature of the acquired FLiDAR signal, particularly in such
environments, makes isolating photon time-of-flight (related to target depth)
and intrinsic fluorescence lifetime exceptionally difficult, thus limiting the
effectiveness of current analytical and computational methodologies. To
overcome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE)
framework tailored for specialized modeling of diverse temporal components. In
contrast to the conventional MoE approaches our expert models are informed by
underlying physics, such as the radiative transport equation governing photon
propagation in scattering media. Central to our approach is EvidenceMoE, which
integrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess
the reliability of each expert's output by providing per-expert quality scores
and corrective feedback. A Decider Network then leverages this information to
fuse expert predictions into a robust final estimate adaptively. We validate
our method using realistically simulated Fluorescence LiDAR (FLiDAR) data for
non-invasive cancer cell depth detection generated from photon transport models
in tissue. Our framework demonstrates strong performance, achieving a
normalized root mean squared error (NRMSE) of 0.030 for depth estimation and
0.074 for fluorescence lifetime.

</details>


### [169] [Self-Organizing Visual Prototypes for Non-Parametric Representation Learning](https://arxiv.org/abs/2505.21533)
*Thalles Silva,Helio Pedrini,Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: SOP是一种新的无监督视觉特征学习技术，通过多个语义相似的表示（SEs）来表征原型，优于传统单原型方法。


<details>
  <summary>Details</summary>
Motivation: 传统自监督学习方法依赖单一原型编码隐藏簇的特征，限制了性能。SOP旨在通过多支持嵌入（SEs）更全面地表征数据区域。

Method: 提出SOP策略，结合非参数损失函数和SOP-MIM任务（基于多局部SEs重建掩码表示）。

Result: 在检索、线性评估、微调和目标检测等任务中表现优异，预训练编码器在多个检索基准上达到SOTA。

Conclusion: SOP策略显著提升了无监督视觉特征学习的性能，尤其适用于复杂编码器。

Abstract: We present Self-Organizing Visual Prototypes (SOP), a new training technique
for unsupervised visual feature learning. Unlike existing prototypical
self-supervised learning (SSL) methods that rely on a single prototype to
encode all relevant features of a hidden cluster in the data, we propose the
SOP strategy. In this strategy, a prototype is represented by many semantically
similar representations, or support embeddings (SEs), each containing a
complementary set of features that together better characterize their region in
space and maximize training performance. We reaffirm the feasibility of
non-parametric SSL by introducing novel non-parametric adaptations of two loss
functions that implement the SOP strategy. Notably, we introduce the SOP Masked
Image Modeling (SOP-MIM) task, where masked representations are reconstructed
from the perspective of multiple non-parametric local SEs. We comprehensively
evaluate the representations learned using the SOP strategy on a range of
benchmarks, including retrieval, linear evaluation, fine-tuning, and object
detection. Our pre-trained encoders achieve state-of-the-art performance on
many retrieval benchmarks and demonstrate increasing performance gains with
more complex encoders.

</details>


### [170] [Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement](https://arxiv.org/abs/2505.21535)
*Yuxin Ren,Maxwell D Collins,Miao Hu,Huanrui Yang*

Main category: cs.CV

TL;DR: FAR框架通过用LSTM等序列模块替换预训练Transformer中的注意力机制，提高了推理效率，同时保持了模型性能。


<details>
  <summary>Details</summary>
Motivation: Transformer的注意力机制在推理时效率较低，尤其是在资源受限的边缘设备上。研究发现推理时的序列映射可以更简单，因此提出FAR框架。

Method: FAR用可学习的序列模块（如LSTM）替换注意力块，并通过蒸馏目标和结构剪枝优化多头LSTM架构。

Result: 在DeiT视觉Transformer上验证，FAR在ImageNet和下游任务中保持了原始模型的准确性，同时减少了参数和延迟。

Conclusion: FAR不仅高效，还能保留注意力模块中学到的语义关系和相关性。

Abstract: While transformers excel across vision and language pretraining tasks, their
reliance on attention mechanisms poses challenges for inference efficiency,
especially on edge and embedded accelerators with limited parallelism and
memory bandwidth. Hinted by the observed redundancy of attention at inference
time, we hypothesize that though the model learns complicated token dependency
through pretraining, the inference-time sequence-to-sequence mapping in each
attention layer is actually ''simple'' enough to be represented with a much
cheaper function. In this work, we explore FAR, a Function-preserving Attention
Replacement framework that replaces all attention blocks in pretrained
transformers with learnable sequence-to-sequence modules, exemplified by an
LSTM. FAR optimize a multi-head LSTM architecture with a block-wise
distillation objective and a global structural pruning framework to achieve a
family of efficient LSTM-based models from pretrained transformers. We validate
FAR on the DeiT vision transformer family and demonstrate that it matches the
accuracy of the original models on ImageNet and multiple downstream tasks with
reduced parameters and latency. Further analysis shows that FAR preserves the
semantic token relationships and the token-to-token correlation learned in the
transformer's attention module.

</details>


### [171] [Caption This, Reason That: VLMs Caught in the Middle](https://arxiv.org/abs/2505.21538)
*Zihan Weng,Lucas Gomez,Taylor Whittington Webb,Pouya Bashivan*

Main category: cs.CV

TL;DR: 论文分析了视觉语言模型（VLMs）在认知能力上的局限性，提出通过认知科学方法评估其表现，并发现改进方向。


<details>
  <summary>Details</summary>
Motivation: 研究VLMs在特定视觉任务（如计数或关系推理）中表现不佳的原因，探索其认知能力的局限性。

Method: 采用认知科学方法，从感知、注意力和记忆三个核心认知维度评估VLMs（包括GPT-4o）的表现，并通过视觉-文本解耦分析探究失败原因。

Result: 发现VLMs在某些任务（如类别识别）表现接近人类，但在空间理解或选择性注意力任务上仍有显著差距；通过生成文本标题推理可改善表现。

Conclusion: 研究揭示了VLMs在同时感知和推理中的瓶颈，并提出针对性微调是有效的改进方法。

Abstract: Vision-Language Models (VLMs) have shown remarkable progress in visual
understanding in recent years. Yet, they still lag behind human capabilities in
specific visual tasks such as counting or relational reasoning. To understand
the underlying limitations, we adopt methodologies from cognitive science,
analyzing VLM performance along core cognitive axes: Perception, Attention, and
Memory. Using a suite of tasks targeting these abilities, we evaluate
state-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct
cognitive profiles: while advanced models approach ceiling performance on some
tasks (e.g. category identification), a significant gap persists, particularly
in tasks requiring spatial understanding or selective attention. Investigating
the source of these failures and potential methods for improvement, we employ a
vision-text decoupling analysis, finding that models struggling with direct
visual reasoning show marked improvement when reasoning over their own
generated text captions. These experiments reveal a strong need for improved
VLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed
human performance. Furthermore, we demonstrate the potential of targeted
fine-tuning on composite visual reasoning tasks and show that fine-tuning
smaller VLMs substantially improves core cognitive abilities. While this
improvement does not translate to large enhancements on challenging,
out-of-distribution benchmarks, we show broadly that VLM performance on our
datasets strongly correlates with performance on these other benchmarks. Our
work provides a detailed analysis of VLM cognitive strengths and weaknesses and
identifies key bottlenecks in simultaneous perception and reasoning while also
providing an effective and simple solution.

</details>


### [172] [Equivariant Flow Matching for Point Cloud Assembly](https://arxiv.org/abs/2505.21539)
*Ziming Wang,Nan Xue,Rebecka Jörnsten*

Main category: cs.CV

TL;DR: 提出了一种基于流匹配模型的等变求解器（Eda），用于点云组装任务，能够高效处理非重叠输入。


<details>
  <summary>Details</summary>
Motivation: 点云组装的目标是通过对齐多个点云片段重建完整的3D形状，现有方法在非重叠输入情况下表现不佳。

Method: 提出Eda模型，通过学习与输入片段相关的向量场实现等变分布，并构建等变路径以提高训练效率。

Result: 实验表明Eda在实际数据集中表现优异，能够处理非重叠输入片段。

Conclusion: Eda是一种高效的点云组装方法，尤其适用于非重叠输入场景。

Abstract: The goal of point cloud assembly is to reconstruct a complete 3D shape by
aligning multiple point cloud pieces. This work presents a novel equivariant
solver for assembly tasks based on flow matching models. We first theoretically
show that the key to learning equivariant distributions via flow matching is to
learn related vector fields. Based on this result, we propose an assembly
model, called equivariant diffusion assembly (Eda), which learns related vector
fields conditioned on the input pieces. We further construct an equivariant
path for Eda, which guarantees high data efficiency of the training process.
Our numerical results show that Eda is highly competitive on practical
datasets, and it can even handle the challenging situation where the input
pieces are non-overlapped.

</details>


### [173] [DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers](https://arxiv.org/abs/2505.21541)
*Zitong Wang,Hang Zhao,Qianyu Zhou,Xuequan Lu,Xiangtai Li,Yiren Song*

Main category: cs.CV

TL;DR: 本文提出了一种新任务：Alpha合成图像的分层分解，并提出了DiffDecompose框架和AlphaBlend数据集，解决了透明/半透明层分解的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有图像分解方法在处理透明/半透明层时存在局限性，如依赖掩码先验、静态对象假设和数据缺乏。

Method: 提出了DiffDecompose框架，基于扩散Transformer，通过上下文分解和层位置编码克隆技术实现分层预测。

Result: 在AlphaBlend和LOGO数据集上的实验验证了DiffDecompose的有效性。

Conclusion: DiffDecompose和AlphaBlend数据集为透明/半透明层分解提供了新解决方案。

Abstract: Diffusion models have recently motivated great success in many generation
tasks like object removal. Nevertheless, existing image decomposition methods
struggle to disentangle semi-transparent or transparent layer occlusions due to
mask prior dependencies, static object assumptions, and the lack of datasets.
In this paper, we delve into a novel task: Layer-Wise Decomposition of
Alpha-Composited Images, aiming to recover constituent layers from single
overlapped images under the condition of semi-transparent/transparent alpha
layer non-linear occlusion. To address challenges in layer ambiguity,
generalization, and data scarcity, we first introduce AlphaBlend, the first
large-scale and high-quality dataset for transparent and semi-transparent layer
decomposition, supporting six real-world subtasks (e.g., translucent flare
removal, semi-transparent cell decomposition, glassware decomposition).
Building on this dataset, we present DiffDecompose, a diffusion
Transformer-based framework that learns the posterior over possible layer
decompositions conditioned on the input image, semantic prompts, and blending
type. Rather than regressing alpha mattes directly, DiffDecompose performs
In-Context Decomposition, enabling the model to predict one or multiple layers
without per-layer supervision, and introduces Layer Position Encoding Cloning
to maintain pixel-level correspondence across layers. Extensive experiments on
the proposed AlphaBlend dataset and public LOGO dataset verify the
effectiveness of DiffDecompose. The code and dataset will be available upon
paper acceptance. Our code will be available at:
https://github.com/Wangzt1121/DiffDecompose.

</details>


### [174] [Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance](https://arxiv.org/abs/2505.21544)
*Semanto Mondal*

Main category: cs.CV

TL;DR: 论文提出了一种结合对象检测、大语言模型和检索增强生成的混合AI方法，用于精准农业中的作物病害检测与诊断。


<details>
  <summary>Details</summary>
Motivation: 传统农业资源利用效率低且对环境有负面影响，需通过技术优化农业过程。

Method: 结合YOLOv8、NLP和RAG技术，构建了一个能检测咖啡叶病害并提供治疗建议的系统。

Result: 系统能实时检测病害、减少农药使用，并支持环保农业方法。

Conclusion: 该框架具有可扩展性和用户友好性，未来可广泛应用于农业领域。

Abstract: As a social being, we have an intimate bond with the environment. A plethora
of things in human life, such as lifestyle, health, and food are dependent on
the environment and agriculture. It comes under our responsibility to support
the environment as well as agriculture. However, traditional farming practices
often result in inefficient resource use and environmental challenges. To
address these issues, precision agriculture has emerged as a promising approach
that leverages advanced technologies to optimise agricultural processes. In
this work, a hybrid approach is proposed that combines the three different
potential fields of model AI: object detection, large language model (LLM), and
Retrieval-Augmented Generation (RAG). In this novel framework, we have tried to
combine the vision and language models to work together to identify potential
diseases in the tree leaf. This study introduces a novel AI-based precision
agriculture system that uses Retrieval Augmented Generation (RAG) to provide
context-aware diagnoses and natural language processing (NLP) and YOLOv8 for
crop disease detection. The system aims to tackle major issues with large
language models (LLMs), especially hallucinations and allows for adaptive
treatment plans and real-time disease detection. The system provides an
easy-to-use interface to the farmers, which they can use to detect the
different diseases related to coffee leaves by just submitting the image of the
affected leaf the model will detect the diseases as well as suggest potential
remediation methodologies which aim to lower the use of pesticides, preserving
livelihoods, and encouraging environmentally friendly methods. With an emphasis
on scalability, dependability, and user-friendliness, the project intends to
improve RAG-integrated object detection systems for wider agricultural
applications in the future.

</details>


### [175] [Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation](https://arxiv.org/abs/2505.21545)
*Chika Maduabuchi,Hao Chen,Yujin Han,Jindong Wang*

Main category: cs.CV

TL;DR: CAT-LVDM是一种针对LVDMs的鲁棒性训练框架，通过结构化噪声注入提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决LVDMs在噪声视频-文本数据集上语义漂移和时间不一致的问题。

Method: 提出Batch-Centered Noise Injection (BCNI)和Spectrum-Aware Contextual Noise (SACN)两种噪声注入方法。

Result: BCNI在WebVid-2M等数据集上降低FVD 31.9%，SACN在UCF-101上提升12.3%。

Conclusion: CAT-LVDM提供了一种可扩展的鲁棒训练方法，理论分析支持其有效性。

Abstract: Latent Video Diffusion Models (LVDMs) achieve high-quality generation but are
sensitive to imperfect conditioning, which causes semantic drift and temporal
incoherence on noisy, web-scale video-text datasets. We introduce CAT-LVDM, the
first corruption-aware training framework for LVDMs that improves robustness
through structured, data-aligned noise injection. Our method includes
Batch-Centered Noise Injection (BCNI), which perturbs embeddings along
intra-batch semantic directions to preserve temporal consistency. BCNI is
especially effective on caption-rich datasets like WebVid-2M, MSR-VTT, and
MSVD. We also propose Spectrum-Aware Contextual Noise (SACN), which injects
noise along dominant spectral directions to improve low-frequency smoothness,
showing strong results on UCF-101. On average, BCNI reduces FVD by 31.9% across
WebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101.
Ablation studies confirm the benefit of low-rank, data-aligned noise. Our
theoretical analysis further explains how such perturbations tighten entropy,
Wasserstein, score-drift, mixing-time, and generalization bounds. CAT-LVDM
establishes a principled, scalable training approach for robust video diffusion
under multimodal noise. Code and models: https://github.com/chikap421/catlvdm

</details>


### [176] [Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing](https://arxiv.org/abs/2505.21547)
*Weixing Wang,Zifeng Ding,Jindong Gu,Rui Cao,Christoph Meinel,Gerard de Melo,Haojin Yang*

Main category: cs.CV

TL;DR: 论文研究了大型视觉语言模型（LVLMs）中幻觉问题的原因，并提出了一种基于图神经网络和对比学习的缓解方法。


<details>
  <summary>Details</summary>
Motivation: 研究发现LVLMs会幻觉出不存在的对象，假设这是由于训练中视觉先验导致的某些图像标记与对象描述强关联。

Method: 通过构建图像标记共现图，使用GNN和对比学习聚类高频共现标记，提出抑制视觉缺失标记影响的生成方法。

Result: 实验表明，该方法能有效减少幻觉，同时保持模型的表达能力。

Conclusion: 研究揭示了幻觉的根源，并提出了一种可行的缓解策略。

Abstract: Large Vision-Language Models (LVLMs) with discrete image tokenizers unify
multimodal representations by encoding visual inputs into a finite set of
tokens. Despite their effectiveness, we find that these models still
hallucinate non-existent objects. We hypothesize that this may be due to visual
priors induced during training: When certain image tokens frequently co-occur
in the same spatial regions and represent shared objects, they become strongly
associated with the verbalizations of those objects. As a result, the model may
hallucinate by evoking visually absent tokens that often co-occur with present
ones. To test this assumption, we construct a co-occurrence graph of image
tokens using a segmentation dataset and employ a Graph Neural Network (GNN)
with contrastive learning followed by a clustering method to group tokens that
frequently co-occur in similar visual contexts. We find that hallucinations
predominantly correspond to clusters whose tokens dominate the input, and more
specifically, that the visually absent tokens in those clusters show much
higher correlation with hallucinated objects compared to tokens present in the
image. Based on this observation, we propose a hallucination mitigation method
that suppresses the influence of visually absent tokens by modifying latent
image embeddings during generation. Experiments show our method reduces
hallucinations while preserving expressivity. Code is available at
https://github.com/weixingW/CGC-VTD/tree/main

</details>


### [177] [Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation](https://arxiv.org/abs/2505.21549)
*Daniel Csizmadia,Andrei Codreanu,Victor Sim,Vighnesh Prabeau,Michael Lu,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.CV

TL;DR: DCLIP通过师生蒸馏框架改进CLIP模型，提升多模态图像-文本检索能力，同时保留零样本分类性能。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在图像分辨率固定和上下文有限的约束下，难以实现细粒度的跨模态理解，DCLIP旨在解决这一问题。

Method: 采用跨模态Transformer教师模型，通过双向跨注意力机制生成丰富的嵌入，指导轻量级学生模型的训练，结合对比学习和余弦相似度目标。

Result: 在仅使用少量数据训练的情况下，DCLIP显著提升检索指标（Recall@K, MAP），同时保留94%的零样本分类性能。

Conclusion: DCLIP有效平衡任务专业化和泛化能力，为视觉-语言任务提供高效、适应性强的解决方案。

Abstract: We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that
enhances multimodal image-text retrieval while preserving the original model's
strong zero-shot classification capabilities. CLIP models are typically
constrained by fixed image resolutions and limited context, which can hinder
their effectiveness in retrieval tasks that require fine-grained cross-modal
understanding. DCLIP addresses these challenges through a meta teacher-student
distillation framework, where a cross-modal transformer teacher is fine-tuned
to produce enriched embeddings via bidirectional cross-attention between
YOLO-extracted image regions and corresponding textual spans. These
semantically and spatially aligned global representations guide the training of
a lightweight student model using a hybrid loss that combines contrastive
learning and cosine similarity objectives. Despite being trained on only
~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a
fraction of CLIP's original dataset-DCLIP significantly improves image-text
retrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's
zero-shot classification performance. These results demonstrate that DCLIP
effectively mitigates the trade-off between task specialization and
generalization, offering a resource-efficient, domain-adaptive, and
detail-sensitive solution for advanced vision-language tasks. Code available at
https://anonymous.4open.science/r/DCLIP-B772/README.md.

</details>


### [178] [Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts](https://arxiv.org/abs/2505.21556)
*Hee-Seon Kim,Minbeom Kim,Wonjun Lee,Kihyun Kim,Changick Kim*

Main category: cs.CV

TL;DR: 论文提出了一种新的Benign-to-Toxic (B2T) jailbreak方法，通过优化对抗图像从良性输入诱导毒性输出，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Toxic-Continuation方法在缺乏明确毒性信号时效果不佳，需探索更有效的多模态对齐漏洞。

Method: 优化对抗图像，使其在良性条件下诱导模型生成毒性输出。

Result: B2T方法优于现有方法，适用于黑盒场景，并能与基于文本的jailbreak互补。

Conclusion: 揭示了多模态对齐中未充分探索的漏洞，为jailbreak方法提供了新方向。

Abstract: Optimization-based jailbreaks typically adopt the Toxic-Continuation setting
in large vision-language models (LVLMs), following the standard next-token
prediction objective. In this setting, an adversarial image is optimized to
make the model predict the next token of a toxic prompt. However, we find that
the Toxic-Continuation paradigm is effective at continuing already-toxic
inputs, but struggles to induce safety misalignment when explicit toxic signals
are absent. We propose a new paradigm: Benign-to-Toxic (B2T) jailbreak. Unlike
prior work, we optimize adversarial images to induce toxic outputs from benign
conditioning. Since benign conditioning contains no safety violations, the
image alone must break the model's safety mechanisms. Our method outperforms
prior approaches, transfers in black-box settings, and complements text-based
jailbreaks. These results reveal an underexplored vulnerability in multimodal
alignment and introduce a fundamentally new direction for jailbreak approaches.

</details>


### [179] [Analytical Calculation of Weights Convolutional Neural Network](https://arxiv.org/abs/2505.21557)
*Polad Geidarov*

Main category: cs.CV

TL;DR: 提出一种无需标准训练的CNN权重和阈值分析方法，仅需10张MNIST图像即可确定参数，实验显示其能快速识别半数手写数字。


<details>
  <summary>Details</summary>
Motivation: 探索无需训练即可构建和应用的CNN分类方法，减少计算资源需求。

Method: 通过解析计算CNN权重和阈值，确定层通道数，并用C++ Builder实现模块。

Result: 未经训练的CNN能识别1000张手写数字中的半数，推理速度极快。

Conclusion: 证明CNN可通过纯解析计算直接用于分类任务，无需训练。

Abstract: This paper presents an algorithm for analytically calculating the weights and
thresholds of convolutional neural networks (CNNs) without using standard
training procedures. The algorithm enables the determination of CNN parameters
based on just 10 selected images from the MNIST dataset, each representing a
digit from 0 to 9. As part of the method, the number of channels in CNN layers
is also derived analytically. A software module was implemented in C++ Builder,
and a series of experiments were conducted using the MNIST dataset. Results
demonstrate that the analytically computed CNN can recognize over half of 1000
handwritten digit images without any training, achieving inference in fractions
of a second. These findings suggest that CNNs can be constructed and applied
directly for classification tasks without training, using purely analytical
computation of weights.

</details>


### [180] [A Novel Convolutional Neural Network-Based Framework for Complex Multiclass Brassica Seed Classification](https://arxiv.org/abs/2505.21558)
*Elhoucine Elfatimia,Recep Eryigitb,Lahcen Elfatimi*

Main category: cs.CV

TL;DR: 本文提出了一种基于卷积神经网络（CNN）的新框架，用于高效分类十种常见的芸薹属种子，解决了种子图像纹理相似性的挑战，并取得了93%的高准确率。


<details>
  <summary>Details</summary>
Motivation: 由于农民缺乏时间和资源进行农场研究，种子分类对质量控制、生产效率和杂质检测具有重要意义。早期识别种子类型可降低田间出苗的成本和风险。

Method: 研究采用自定义设计的CNN架构，针对种子图像纹理相似性问题进行优化，并与几种预训练的最先进架构进行性能对比。

Result: 实验结果表明，所提出的模型在芸薹属种子数据集上达到了93%的高准确率。

Conclusion: 该CNN框架为种子分类提供了一种高效解决方案，有助于提升种子质量管理和产量估算的精确性。

Abstract: Agricultural research has accelerated in recent years, yet farmers often lack
the time and resources for on-farm research due to the demands of crop
production and farm operations. Seed classification offers valuable insights
into quality control, production efficiency, and impurity detection. Early
identification of seed types is critical to reducing the cost and risk
associated with field emergence, which can lead to yield losses or disruptions
in downstream processes like harvesting. Seed sampling supports growers in
monitoring and managing seed quality, improving precision in determining seed
purity levels, guiding management adjustments, and enhancing yield estimations.
This study proposes a novel convolutional neural network (CNN)-based framework
for the efficient classification of ten common Brassica seed types. The
approach addresses the inherent challenge of texture similarity in seed images
using a custom-designed CNN architecture. The model's performance was evaluated
against several pre-trained state-of-the-art architectures, with adjustments to
layer configurations for optimized classification. Experimental results using
our collected Brassica seed dataset demonstrate that the proposed model
achieved a high accuracy rate of 93 percent.

</details>


### [181] [Knowledge Distillation Approach for SOS Fusion Staging: Towards Fully Automated Skeletal Maturity Assessment](https://arxiv.org/abs/2505.21561)
*Omid Halimi Milani,Amanda Nikho,Marouane Tliba,Lauren Mills,Ahmet Enis Cetin,Mohammed H Elnagar*

Main category: cs.CV

TL;DR: 提出了一种基于双模型架构的深度学习框架，用于自动评估蝶枕软骨联合（SOS）融合，通过知识蒸馏和新的损失函数实现高精度诊断。


<details>
  <summary>Details</summary>
Motivation: 蝶枕软骨联合（SOS）融合是正畸学和法医人类学的重要诊断标志，但传统方法依赖外部裁剪或分割工具，效率低且不一致。

Method: 采用双模型架构（教师模型和学生模型），教师模型在裁剪图像上训练后，通过知识蒸馏和新的损失函数（结合空间logits和梯度注意力映射）指导学生模型在未裁剪图像上学习。

Result: 框架实现了高诊断准确性，无需额外预处理工具，形成临床可行的端到端流程。

Conclusion: 该方法提升了骨骼成熟评估的效率和一致性，适用于多样化临床场景。

Abstract: We introduce a novel deep learning framework for the automated staging of
spheno-occipital synchondrosis (SOS) fusion, a critical diagnostic marker in
both orthodontics and forensic anthropology. Our approach leverages a
dual-model architecture wherein a teacher model, trained on manually cropped
images, transfers its precise spatial understanding to a student model that
operates on full, uncropped images. This knowledge distillation is facilitated
by a newly formulated loss function that aligns spatial logits as well as
incorporates gradient-based attention spatial mapping, ensuring that the
student model internalizes the anatomically relevant features without relying
on external cropping or YOLO-based segmentation. By leveraging expert-curated
data and feedback at each step, our framework attains robust diagnostic
accuracy, culminating in a clinically viable end-to-end pipeline. This
streamlined approach obviates the need for additional pre-processing tools and
accelerates deployment, thereby enhancing both the efficiency and consistency
of skeletal maturation assessment in diverse clinical settings.

</details>


### [182] [Multi-instance Learning as Downstream Task of Self-Supervised Learning-based Pre-trained Model](https://arxiv.org/abs/2505.21564)
*Koki Matsuishi,Tsuyoshi Okita*

Main category: cs.CV

TL;DR: 论文提出了一种基于自监督学习的预训练模型，用于解决深度多示例学习中实例数量增加导致的性能下降问题，显著提升了脑血肿CT的分类准确性和F1分数。


<details>
  <summary>Details</summary>
Motivation: 在脑血肿CT的多示例学习中，当每个包中的实例数量增加到256时，传统深度学习方法性能显著下降。

Method: 采用自监督学习的预训练模型作为多示例学习器的下游任务。

Result: 在脑血肿CT的低密度标记分类任务中，准确率提升了5%至13%，F1分数提升了40%至55%。

Conclusion: 自监督预训练模型能有效解决多示例学习中实例数量增加带来的性能问题。

Abstract: In deep multi-instance learning, the number of applicable instances depends
on the data set. In histopathology images, deep learning multi-instance
learners usually assume there are hundreds to thousands instances in a bag.
However, when the number of instances in a bag increases to 256 in brain
hematoma CT, learning becomes extremely difficult. In this paper, we address
this drawback. To overcome this problem, we propose using a pre-trained model
with self-supervised learning for the multi-instance learner as a downstream
task. With this method, even when the original target task suffers from the
spurious correlation problem, we show improvements of 5% to 13% in accuracy and
40% to 55% in the F1 measure for the hypodensity marker classification of brain
hematoma CT.

</details>


### [183] [Diffusion Model-based Activity Completion for AI Motion Capture from Videos](https://arxiv.org/abs/2505.21566)
*Gao Huayu,Huang Tengjiu,Ye Xiaolong,Tsuyoshi Okita*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散模型的运动补全技术，用于AI动作捕捉，解决了传统方法中动作序列必须预定义的局限性，并在Human3.6M数据集上取得了竞争性结果。


<details>
  <summary>Details</summary>
Motivation: 当前AI动作捕捉方法依赖于预定义的视频序列，限制了动作的灵活性。为了在虚拟人类中实现更灵活的动作，需要填补动作片段之间的过渡。

Method: 采用扩散模型生成补全动作序列，并引入门模块和位置-时间嵌入模块，确保动作的平滑性和连续性。

Result: MDC-Net在ADE、FDE和MMADE上优于现有方法，模型更小（16.84M），且生成的动作更自然连贯。还提出了从动作序列提取传感器数据的方法。

Conclusion: 该方法有效解决了动作捕捉中的过渡问题，为虚拟人类的灵活动作提供了技术支持。

Abstract: AI-based motion capture is an emerging technology that offers a
cost-effective alternative to traditional motion capture systems. However,
current AI motion capture methods rely entirely on observed video sequences,
similar to conventional motion capture. This means that all human actions must
be predefined, and movements outside the observed sequences are not possible.
To address this limitation, we aim to apply AI motion capture to virtual
humans, where flexible actions beyond the observed sequences are required. We
assume that while many action fragments exist in the training data, the
transitions between them may be missing. To bridge these gaps, we propose a
diffusion-model-based action completion technique that generates complementary
human motion sequences, ensuring smooth and continuous movements. By
introducing a gate module and a position-time embedding module, our approach
achieves competitive results on the Human3.6M dataset. Our experimental results
show that (1) MDC-Net outperforms existing methods in ADE, FDE, and MMADE but
is slightly less accurate in MMFDE, (2) MDC-Net has a smaller model size
(16.84M) compared to HumanMAC (28.40M), and (3) MDC-Net generates more natural
and coherent motion sequences. Additionally, we propose a method for extracting
sensor data, including acceleration and angular velocity, from human motion
sequences.

</details>


### [184] [EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models](https://arxiv.org/abs/2505.21567)
*Feng Jiang,Zihao Zheng,Xiuping Cui,Maoliang Li,JIayu Chen,Xiang Chen*

Main category: cs.CV

TL;DR: 提出了一种名为EaqVLA的优化框架，通过编码对齐量化解决VLA模型的量化问题，显著降低了计算和存储成本。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型面临高昂的计算和存储成本，且现有量化方法因token对齐问题无法直接应用。

Method: 提出了一种完整的分析方法识别多粒度对齐问题，并基于此设计了编码对齐感知的混合精度量化方法。

Result: 实验表明，EaqVLA在量化性能上优于现有方法，实现了最小的量化损失和显著的加速效果。

Conclusion: EaqVLA框架有效解决了VLA模型的量化问题，为端到端控制策略提供了高效解决方案。

Abstract: With the development of Embodied Artificial intelligence, the end-to-end
control policy such as Vision-Language-Action (VLA) model has become the
mainstream. Existing VLA models faces expensive computing/storage cost, which
need to be optimized. Quantization is considered as the most effective method
which can not only reduce the memory cost but also achieve computation
acceleration. However, we find the token alignment of VLA models hinders the
application of existing quantization methods. To address this, we proposed an
optimized framework called EaqVLA, which apply encoding-aligned quantization to
VLA models. Specifically, we propose an complete analysis method to find the
misalignment in various granularity. Based on the analysis results, we propose
a mixed precision quantization with the awareness of encoding alignment.
Experiments shows that the porposed EaqVLA achieves better quantization
performance (with the minimal quantization loss for end-to-end action control
and xxx times acceleration) than existing quantization methods.

</details>


### [185] [Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks](https://arxiv.org/abs/2505.21572)
*Sungwon Kim,Namkyeong Lee,Yunyoung Doh,Seungmin Shin,Guimok Cho,Seung-Won Jeon,Sangkook Kim,Chanyoung Park*

Main category: cs.CV

TL;DR: 提出了一种厚度感知的E(3)-等变3D网格神经网络（T-EMNN），有效整合3D对象厚度，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于网格的3D静态分析方法忽略对象厚度，导致分析不准确。

Method: 提出T-EMNN框架，结合厚度信息并保持E(3)-等变性，引入数据驱动坐标。

Result: 在工业数据集上验证，T-EMNN能准确预测节点级3D变形，同时保持高效计算。

Conclusion: T-EMNN在整合厚度信息和计算效率方面表现优越。

Abstract: Mesh-based 3D static analysis methods have recently emerged as efficient
alternatives to traditional computational numerical solvers, significantly
reducing computational costs and runtime for various physics-based analyses.
However, these methods primarily focus on surface topology and geometry, often
overlooking the inherent thickness of real-world 3D objects, which exhibits
high correlations and similar behavior between opposing surfaces. This
limitation arises from the disconnected nature of these surfaces and the
absence of internal edge connections within the mesh. In this work, we propose
a novel framework, the Thickness-aware E(3)-Equivariant 3D Mesh Neural Network
(T-EMNN), that effectively integrates the thickness of 3D objects while
maintaining the computational efficiency of surface meshes. Additionally, we
introduce data-driven coordinates that encode spatial information while
preserving E(3)-equivariance or invariance properties, ensuring consistent and
robust analysis. Evaluations on a real-world industrial dataset demonstrate the
superior performance of T-EMNN in accurately predicting node-level 3D
deformations, effectively capturing thickness effects while maintaining
computational efficiency.

</details>


### [186] [Do We Need All the Synthetic Data? Towards Targeted Synthetic Image Augmentation via Diffusion Models](https://arxiv.org/abs/2505.21574)
*Dang Nguyen,Jiping Li,Jinghao Zheng,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: 通过仅增强训练数据中未被早期学习部分，提升图像分类器的泛化能力，优于全数据增强方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据增强方法在确保生成多样性和提升性能方面存在不足，需探索更高效策略。

Method: 分析双层CNN，证明部分数据增强策略通过促进特征学习速度的均匀性提升泛化能力。

Result: 仅增强30%-40%数据，性能提升达2.8%，在多种模型和数据集上优于SOTA优化器SAM。

Conclusion: 部分数据增强策略高效且兼容现有增强方法，显著提升模型性能。

Abstract: Synthetically augmenting training datasets with diffusion models has been an
effective strategy for improving generalization of image classifiers. However,
existing techniques struggle to ensure the diversity of generation and increase
the size of the data by up to 10-30x to improve the in-distribution
performance. In this work, we show that synthetically augmenting part of the
data that is not learned early in training outperforms augmenting the entire
dataset. By analyzing a two-layer CNN, we prove that this strategy improves
generalization by promoting homogeneity in feature learning speed without
amplifying noise. Our extensive experiments show that by augmenting only
30%-40% of the data, our method boosts the performance by up to 2.8% in a
variety of scenarios, including training ResNet, ViT and DenseNet on CIFAR-10,
CIFAR-100, and TinyImageNet, with a range of optimizers including SGD and SAM.
Notably, our method applied with SGD outperforms the SOTA optimizer, SAM, on
CIFAR-100 and TinyImageNet. It can also easily stack with existing weak and
strong augmentation strategies to further boost the performance.

</details>


### [187] [Do you see what I see? An Ambiguous Optical Illusion Dataset exposing limitations of Explainable AI](https://arxiv.org/abs/2505.21589)
*Carina Newen,Luca Hinkamp,Maria Ntonti,Emmanuel Müller*

Main category: cs.CV

TL;DR: 论文介绍了一个新颖的光学幻觉数据集，旨在研究机器学习和人类感知中的模糊性问题，并探讨了视觉概念对模型准确性的影响。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域（如自动驾驶和医疗诊断）中，模糊数据的重要性日益凸显。光学幻觉为研究人类和机器感知的局限性提供了独特视角，但目前相关数据集稀缺。

Method: 通过设计包含交织动物对的光学幻觉数据集，研究视觉概念（如注视方向和眼睛线索）对模型的影响，并系统生成光学幻觉以支持通用研究。

Result: 研究发现视觉概念（如注视方向）对模型准确性有显著影响，为研究机器视觉中的偏见和对齐问题提供了基础。

Conclusion: 该数据集为研究视觉学习和机器与人类视觉对齐问题提供了重要资源，代码和数据集已公开。

Abstract: From uncertainty quantification to real-world object detection, we recognize
the importance of machine learning algorithms, particularly in safety-critical
domains such as autonomous driving or medical diagnostics. In machine learning,
ambiguous data plays an important role in various machine learning domains.
Optical illusions present a compelling area of study in this context, as they
offer insight into the limitations of both human and machine perception.
Despite this relevance, optical illusion datasets remain scarce. In this work,
we introduce a novel dataset of optical illusions featuring intermingled animal
pairs designed to evoke perceptual ambiguity. We identify generalizable visual
concepts, particularly gaze direction and eye cues, as subtle yet impactful
features that significantly influence model accuracy. By confronting models
with perceptual ambiguity, our findings underscore the importance of concepts
in visual learning and provide a foundation for studying bias and alignment
between human and machine vision. To make this dataset useful for general
purposes, we generate optical illusions systematically with different concepts
discussed in our bias mitigation section. The dataset is accessible in Kaggle
via
https://kaggle.com/datasets/693bf7c6dd2cb45c8a863f9177350c8f9849a9508e9d50526e2ffcc5559a8333.
Our source code can be found at
https://github.com/KDD-OpenSource/Ambivision.git.

</details>


### [188] [Any-to-Bokeh: One-Step Video Bokeh via Multi-Plane Image Guided Diffusion](https://arxiv.org/abs/2505.21593)
*Yang Yang,Siming Zheng,Jinwei Chen,Boxi Wu,Xiaofei He,Deng Cai,Bo Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: 提出了一种新颖的一步视频散景框架，解决了现有方法在时间一致性和深度控制上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑模型无法明确控制焦点平面或调整散景强度，且图像散景方法扩展到视频时会导致时间闪烁和边缘模糊过渡问题。

Method: 利用多平面图像（MPI）表示和逐步扩展的深度采样函数，结合单步视频扩散模型和预训练模型的3D先验，实现深度感知的散景效果。

Result: 实验表明，该方法能生成高质量、可控的散景效果，并在多个评估基准上达到最先进性能。

Conclusion: 该方法成功解决了视频散景的时间一致性和深度控制问题，为可控光学效果提供了有效解决方案。

Abstract: Recent advances in diffusion based editing models have enabled realistic
camera simulation and image-based bokeh, but video bokeh remains largely
unexplored. Existing video editing models cannot explicitly control focus
planes or adjust bokeh intensity, limiting their applicability for controllable
optical effects. Moreover, naively extending image-based bokeh methods to video
often results in temporal flickering and unsatisfactory edge blur transitions
due to the lack of temporal modeling and generalization capability. To address
these challenges, we propose a novel one-step video bokeh framework that
converts arbitrary input videos into temporally coherent, depth-aware bokeh
effects. Our method leverages a multi-plane image (MPI) representation
constructed through a progressively widening depth sampling function, providing
explicit geometric guidance for depth-dependent blur synthesis. By conditioning
a single-step video diffusion model on MPI layers and utilizing the strong 3D
priors from pre-trained models such as Stable Video Diffusion, our approach
achieves realistic and consistent bokeh effects across diverse scenes.
Additionally, we introduce a progressive training strategy to enhance temporal
consistency, depth robustness, and detail preservation. Extensive experiments
demonstrate that our method produces high-quality, controllable bokeh effects
and achieves state-of-the-art performance on multiple evaluation benchmarks.

</details>


### [189] [Object Concepts Emerge from Motion](https://arxiv.org/abs/2505.21635)
*Haoqian Liang,Xiaohui Wang,Zhichao Li,Ya Yang,Naiyan Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于生物启发的无监督学习框架，利用运动边界作为对象级分组的信号，从原始视频中学习对象中心视觉表示。


<details>
  <summary>Details</summary>
Motivation: 受发展神经科学的启发，婴儿通过观察运动获得对象理解，作者希望利用运动边界作为对象级分组的信号，实现无监督学习。

Method: 通过现成的光流和聚类算法生成基于运动的实例掩码，并利用对比学习训练视觉编码器。

Result: 在三个下游任务（单目深度估计、3D对象检测和占用预测）中表现优于现有监督和自监督基线，并展示了对未见场景的强泛化能力。

Conclusion: 运动诱导的对象表示提供了一种有前景的替代方案，捕捉了视觉实例这一关键但被忽视的抽象层次。

Abstract: Object concepts play a foundational role in human visual cognition, enabling
perception, memory, and interaction in the physical world. Inspired by findings
in developmental neuroscience - where infants are shown to acquire object
understanding through observation of motion - we propose a biologically
inspired framework for learning object-centric visual representations in an
unsupervised manner. Our key insight is that motion boundary serves as a strong
signal for object-level grouping, which can be used to derive pseudo instance
supervision from raw videos. Concretely, we generate motion-based instance
masks using off-the-shelf optical flow and clustering algorithms, and use them
to train visual encoders via contrastive learning. Our framework is fully
label-free and does not rely on camera calibration, making it scalable to
large-scale unstructured video data. We evaluate our approach on three
downstream tasks spanning both low-level (monocular depth estimation) and
high-level (3D object detection and occupancy prediction) vision. Our models
outperform previous supervised and self-supervised baselines and demonstrate
strong generalization to unseen scenes. These results suggest that
motion-induced object representations offer a compelling alternative to
existing vision foundation models, capturing a crucial but overlooked level of
abstraction: the visual instance. The corresponding code will be released upon
paper acceptance.

</details>


### [190] [BaryIR: Learning Multi-Source Unified Representation in Continuous Barycenter Space for Generalizable All-in-One Image Restoration](https://arxiv.org/abs/2505.21637)
*Xiaole Tang,Xiaoyi He,Xiang Gu,Jian Sun*

Main category: cs.CV

TL;DR: BaryIR提出了一种多源表示学习框架，通过分解潜在空间为连续重心空间和源特定子空间，提升图像修复的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有全合一图像修复方法对分布外退化和图像适应性差，限制了实际应用。

Method: 引入多源潜在最优传输重心问题，学习连续重心映射，设计传输成本以对比源特定子空间表示并保持与重心空间的正交性。

Result: BaryIR在实验中表现优异，尤其在真实数据和未见退化上展现出色泛化能力。

Conclusion: BaryIR通过统一和特定语义编码，显著提升了全合一图像修复的泛化性和实际适用性。

Abstract: Despite remarkable advances made in all-in-one image restoration (AIR) for
handling different types of degradations simultaneously, existing methods
remain vulnerable to out-of-distribution degradations and images, limiting
their real-world applicability. In this paper, we propose a multi-source
representation learning framework BaryIR, which decomposes the latent space of
multi-source degraded images into a continuous barycenter space for unified
feature encoding and source-specific subspaces for specific semantic encoding.
Specifically, we seek the multi-source unified representation by introducing a
multi-source latent optimal transport barycenter problem, in which a continuous
barycenter map is learned to transport the latent representations to the
barycenter space. The transport cost is designed such that the representations
from source-specific subspaces are contrasted with each other while maintaining
orthogonality to those from the barycenter space. This enables BaryIR to learn
compact representations with unified degradation-agnostic information from the
barycenter space, as well as degradation-specific semantics from
source-specific subspaces, capturing the inherent geometry of multi-source data
manifold for generalizable AIR. Extensive experiments demonstrate that BaryIR
achieves competitive performance compared to state-of-the-art all-in-one
methods. Particularly, BaryIR exhibits superior generalization ability to
real-world data and unseen degradations. The code will be publicly available at
https://github.com/xl-tang3/BaryIR.

</details>


### [191] [Learnable Burst-Encodable Time-of-Flight Imaging for High-Fidelity Long-Distance Depth Sensing](https://arxiv.org/abs/2505.22025)
*Manchao Bao,Shengjiang Fang,Tao Yue,Xuemei Hu*

Main category: cs.CV

TL;DR: 提出了一种新型的BE-ToF成像范式，通过突发模式发射光脉冲并联合优化编码函数和深度重建网络，解决了传统iToF系统的相位缠绕和低信噪比问题。


<details>
  <summary>Details</summary>
Motivation: 长距离深度成像在自动驾驶和机器人等领域有重要应用，但现有dToF和iToF技术分别存在硬件要求高和相位缠绕、低信噪比等问题。

Method: 采用突发模式发射光脉冲，估计整个突发周期的相位延迟，并设计端到端可学习框架联合优化编码函数和深度重建网络。

Result: 通过仿真和原型实验验证了BE-ToF在高保真长距离深度成像中的有效性和实用性。

Conclusion: BE-ToF为长距离深度成像提供了一种高效且实用的解决方案。

Abstract: Long-distance depth imaging holds great promise for applications such as
autonomous driving and robotics. Direct time-of-flight (dToF) imaging offers
high-precision, long-distance depth sensing, yet demands ultra-short pulse
light sources and high-resolution time-to-digital converters. In contrast,
indirect time-of-flight (iToF) imaging often suffers from phase wrapping and
low signal-to-noise ratio (SNR) as the sensing distance increases. In this
paper, we introduce a novel ToF imaging paradigm, termed Burst-Encodable
Time-of-Flight (BE-ToF), which facilitates high-fidelity, long-distance depth
imaging. Specifically, the BE-ToF system emits light pulses in burst mode and
estimates the phase delay of the reflected signal over the entire burst period,
thereby effectively avoiding the phase wrapping inherent to conventional iToF
systems. Moreover, to address the low SNR caused by light attenuation over
increasing distances, we propose an end-to-end learnable framework that jointly
optimizes the coding functions and the depth reconstruction network. A
specialized double well function and first-order difference term are
incorporated into the framework to ensure the hardware implementability of the
coding functions. The proposed approach is rigorously validated through
comprehensive simulations and real-world prototype experiments, demonstrating
its effectiveness and practical applicability.

</details>


### [192] [Geometric Feature Prompting of Image Segmentation Models](https://arxiv.org/abs/2505.21644)
*Kenneth Ball,Erin Taylor,Nirav Patel,Andrew Bartels,Gary Koplik,James Polly,Jay Hineman*

Main category: cs.CV

TL;DR: 提出了一种基于几何特征的提示生成器（GeomPrompt），用于自动生成敏感且特异的点提示，以改进SAM在植物根系图像分割中的应用。


<details>
  <summary>Details</summary>
Motivation: 植物根系图像分割（如rhizotron图像）传统上难以自动化且人工标注耗时且主观，需要一种高效自动化的方法。

Method: 使用几何驱动的提示生成器（GeomPrompt）生成与感兴趣特征共定位的点提示，结合SAM进行分割。

Result: GeomPrompt结合SAM能够以较少的点提示自动生成高敏感性和特异性的分割结果。

Conclusion: GeomPrompt显著提升了植物根系图像分割的效率，并开源了相关软件工具。

Abstract: Advances in machine learning, especially the introduction of transformer
architectures and vision transformers, have led to the development of highly
capable computer vision foundation models. The segment anything model (known
colloquially as SAM and more recently SAM 2), is a highly capable foundation
model for segmentation of natural images and has been further applied to
medical and scientific image segmentation tasks. SAM relies on prompts --
points or regions of interest in an image -- to generate associated
segmentations.
  In this manuscript we propose the use of a geometrically motivated prompt
generator to produce prompt points that are colocated with particular features
of interest. Focused prompting enables the automatic generation of sensitive
and specific segmentations in a scientific image analysis task using SAM with
relatively few point prompts. The image analysis task examined is the
segmentation of plant roots in rhizotron or minirhizotron images, which has
historically been a difficult task to automate. Hand annotation of rhizotron
images is laborious and often subjective; SAM, initialized with GeomPrompt
local ridge prompts has the potential to dramatically improve rhizotron image
processing.
  The authors have concurrently released an open source software suite called
geomprompt https://pypi.org/project/geomprompt/ that can produce point prompts
in a format that enables direct integration with the segment-anything package.

</details>


### [193] [QuARI: Query Adaptive Retrieval Improvement](https://arxiv.org/abs/2505.21647)
*Eric Xing,Abby Stylianou,Robert Pless,Nathan Jacobs*

Main category: cs.CV

TL;DR: 本文提出了一种通过学习查询特定的特征空间线性变换来改进大规模图像检索性能的方法，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在大规模图像检索任务中表现不佳，需要更高效的改进方法。

Method: 通过学习查询特定的线性变换映射，优化特征空间以提升检索性能。

Result: 该方法在大规模检索任务中表现优于现有技术，且计算成本低。

Conclusion: 查询特定的特征空间变换是一种高效且有效的大规模图像检索解决方案。

Abstract: Massive-scale pretraining has made vision-language models increasingly
popular for image-to-image and text-to-image retrieval across a broad
collection of domains. However, these models do not perform well when used for
challenging retrieval tasks, such as instance retrieval in very large-scale
image collections. Recent work has shown that linear transformations of VLM
features trained for instance retrieval can improve performance by emphasizing
subspaces that relate to the domain of interest. In this paper, we explore a
more extreme version of this specialization by learning to map a given query to
a query-specific feature space transformation. Because this transformation is
linear, it can be applied with minimal computational cost to millions of image
embeddings, making it effective for large-scale retrieval or re-ranking.
Results show that this method consistently outperforms state-of-the-art
alternatives, including those that require many orders of magnitude more
computation at query time.

</details>


### [194] [Visual Loop Closure Detection Through Deep Graph Consensus](https://arxiv.org/abs/2505.21754)
*Martin Büchner,Liza Dahiya,Simon Dorer,Vipul Ramtekkar,Kenji Nishimiya,Daniele Cattaneo,Abhinav Valada*

Main category: cs.CV

TL;DR: LoopGNN是一种图神经网络架构，通过利用视觉相似关键帧的团来估计闭环共识，提高了闭环检测的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统闭环检测依赖计算密集的几何验证，而在线SLAM场景中时间和计算资源有限，需要一种更高效的方法。

Method: 引入LoopGNN，通过图神经网络在视觉相似关键帧团中传播深度特征编码，估计闭环共识。

Result: 在TartanDrive 2.0和NCLT数据集上，LoopGNN表现优于传统基线，且计算效率更高。

Conclusion: LoopGNN提供了一种高精度、高效率的闭环检测方法，适用于在线SLAM场景。

Abstract: Visual loop closure detection traditionally relies on place recognition
methods to retrieve candidate loops that are validated using computationally
expensive RANSAC-based geometric verification. As false positive loop closures
significantly degrade downstream pose graph estimates, verifying a large number
of candidates in online simultaneous localization and mapping scenarios is
constrained by limited time and compute resources. While most deep loop closure
detection approaches only operate on pairs of keyframes, we relax this
constraint by considering neighborhoods of multiple keyframes when detecting
loops. In this work, we introduce LoopGNN, a graph neural network architecture
that estimates loop closure consensus by leveraging cliques of visually similar
keyframes retrieved through place recognition. By propagating deep feature
encodings among nodes of the clique, our method yields high-precision estimates
while maintaining high recall. Extensive experimental evaluations on the
TartanDrive 2.0 and NCLT datasets demonstrate that LoopGNN outperforms
traditional baselines. Additionally, an ablation study across various keypoint
extractors demonstrates that our method is robust, regardless of the type of
deep feature encodings used, and exhibits higher computational efficiency
compared to classical geometric verification baselines. We release our code,
supplementary material, and keyframe data at
https://loopgnn.cs.uni-freiburg.de.

</details>


### [195] [PS4PRO: Pixel-to-pixel Supervision for Photorealistic Rendering and Optimization](https://arxiv.org/abs/2505.22616)
*Yezhi Shen,Qiuchen Zhai,Fengqing Zhu*

Main category: cs.CV

TL;DR: 提出了一种基于视频帧插值的数据增强方法PS4PRO，用于提升神经渲染在3D场景重建中的性能。


<details>
  <summary>Details</summary>
Motivation: 神经渲染方法在重建3D场景时受限于输入视角数量，尤其在复杂动态场景中表现不佳。

Method: 设计轻量级高质量视频帧插值模型PS4PRO，通过多样化视频数据训练，隐式建模相机运动和3D几何。

Result: 实验表明，该方法显著提升了静态和动态场景的重建性能。

Conclusion: PS4PRO作为一种隐式世界先验，有效增强了神经渲染的数据监督能力。

Abstract: Neural rendering methods have gained significant attention for their ability
to reconstruct 3D scenes from 2D images. The core idea is to take multiple
views as input and optimize the reconstructed scene by minimizing the
uncertainty in geometry and appearance across the views. However, the
reconstruction quality is limited by the number of input views. This limitation
is further pronounced in complex and dynamic scenes, where certain angles of
objects are never seen. In this paper, we propose to use video frame
interpolation as the data augmentation method for neural rendering.
Furthermore, we design a lightweight yet high-quality video frame interpolation
model, PS4PRO (Pixel-to-pixel Supervision for Photorealistic Rendering and
Optimization). PS4PRO is trained on diverse video datasets, implicitly modeling
camera movement as well as real-world 3D geometry. Our model performs as an
implicit world prior, enriching the photo supervision for 3D reconstruction. By
leveraging the proposed method, we effectively augment existing datasets for
neural rendering methods. Our experimental results indicate that our method
improves the reconstruction performance on both static and dynamic scenes.

</details>


### [196] [Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks](https://arxiv.org/abs/2505.21649)
*Keanu Nichols,Nazia Tasnim,Yan Yuting,Nicholas Ikechukwu,Elva Zou,Deepti Ghadiyaram,Bryan Plummer*

Main category: cs.CV

TL;DR: DORI是一个专注于物体方向理解的基准测试，揭示了当前多模态模型在方向感知上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言基准测试未能单独评估物体方向理解能力，常与其他能力混淆。

Method: 通过11个数据集的67个物体类别，设计四个方向理解维度的任务。

Result: 最佳模型在粗粒度任务上准确率为54.2%，细粒度任务为33.0%，表现不佳。

Conclusion: DORI揭示了模型在方向理解上的不足，为改进3D空间表示提供了方向。

Abstract: Object orientation understanding represents a fundamental challenge in visual
perception critical for applications like robotic manipulation and augmented
reality. Current vision-language benchmarks fail to isolate this capability,
often conflating it with positional relationships and general scene
understanding. We introduce DORI (Discriminative Orientation Reasoning
Intelligence), a comprehensive benchmark establishing object orientation
perception as a primary evaluation target. DORI assesses four dimensions of
orientation comprehension: frontal alignment, rotational transformations,
relative directional relationships, and canonical orientation understanding.
Through carefully curated tasks from 11 datasets spanning 67 object categories
across synthetic and real-world scenarios, DORI provides insights on how
multi-modal systems understand object orientations. Our evaluation of 15
state-of-the-art vision-language models reveals critical limitations: even the
best models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular
orientation judgments, with performance deteriorating for tasks requiring
reference frame shifts or compound rotations. These findings demonstrate the
need for dedicated orientation representation mechanisms, as models show
systematic inability to perform precise angular estimations, track orientation
changes across viewpoints, and understand compound rotations - suggesting
limitations in their internal 3D spatial representations. As the first
diagnostic framework specifically designed for orientation awareness in
multimodal systems, DORI offers implications for improving robotic control, 3D
scene reconstruction, and human-AI interaction in physical environments. DORI
data: https://huggingface.co/datasets/appledora/DORI-Benchmark

</details>


### [197] [Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation](https://arxiv.org/abs/2505.21653)
*Ke Zhang,Cihan Xiao,Yiqun Mei,Jiacong Xu,Vishal M. Patel*

Main category: cs.CV

TL;DR: DiffPhy是一个通过微调预训练视频扩散模型生成物理正确且逼真视频的框架，利用大语言模型推理物理上下文并指导生成。


<details>
  <summary>Details</summary>
Motivation: 当前视频扩散模型在生成视觉上吸引人的结果时，难以合成正确的物理效果，真实世界的运动、交互和动力学复杂性增加了学习物理的难度。

Method: DiffPhy利用大语言模型（LLMs）从文本提示中明确推理物理上下文，并通过多模态大语言模型（MLLM）作为监督信号，引入新的训练目标以确保物理正确性和语义一致性。

Result: 在公共基准测试中，DiffPhy在多样化的物理相关场景中取得了最先进的结果。

Conclusion: DiffPhy通过结合大语言模型和物理上下文指导，显著提升了视频生成中的物理正确性和逼真度。

Abstract: Recent video diffusion models have demonstrated their great capability in
generating visually-pleasing results, while synthesizing the correct physical
effects in generated videos remains challenging. The complexity of real-world
motions, interactions, and dynamics introduce great difficulties when learning
physics from data. In this work, we propose DiffPhy, a generic framework that
enables physically-correct and photo-realistic video generation by fine-tuning
a pre-trained video diffusion model. Our method leverages large language models
(LLMs) to explicitly reason a comprehensive physical context from the text
prompt and use it to guide the generation. To incorporate physical context into
the diffusion model, we leverage a Multimodal large language model (MLLM) as a
supervisory signal and introduce a set of novel training objectives that
jointly enforce physical correctness and semantic consistency with the input
text. We also establish a high-quality physical video dataset containing
diverse phyiscal actions and events to facilitate effective finetuning.
Extensive experiments on public benchmarks demonstrate that DiffPhy is able to
produce state-of-the-art results across diverse physics-related scenarios. Our
project page is available at https://bwgzk-keke.github.io/DiffPhy/

</details>


### [198] [From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving Autonomous Driving](https://arxiv.org/abs/2505.22067)
*Xinyu Xia,Xingjun Ma,Yunfeng Hu,Ting Qu,Hong Chen,Xun Gong*

Main category: cs.CV

TL;DR: SERA是一个基于LLM的框架，通过针对性场景推荐修复自动驾驶系统的失败案例，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有场景生成和选择方法缺乏适应性和语义相关性，限制了性能改进。

Method: SERA分析性能日志，识别失败模式，动态检索语义对齐场景，并通过LLM优化推荐。

Result: 实验表明，SERA在多基准测试中显著提升关键指标。

Conclusion: SERA能有效提升自动驾驶系统在安全关键场景下的适应性和泛化能力。

Abstract: Ensuring robust and generalizable autonomous driving requires not only broad
scenario coverage but also efficient repair of failure cases, particularly
those related to challenging and safety-critical scenarios. However, existing
scenario generation and selection methods often lack adaptivity and semantic
relevance, limiting their impact on performance improvement. In this paper, we
propose \textbf{SERA}, an LLM-powered framework that enables autonomous driving
systems to self-evolve by repairing failure cases through targeted scenario
recommendation. By analyzing performance logs, SERA identifies failure patterns
and dynamically retrieves semantically aligned scenarios from a structured
bank. An LLM-based reflection mechanism further refines these recommendations
to maximize relevance and diversity. The selected scenarios are used for
few-shot fine-tuning, enabling targeted adaptation with minimal data.
Experiments on the benchmark show that SERA consistently improves key metrics
across multiple autonomous driving baselines, demonstrating its effectiveness
and generalizability under safety-critical conditions.

</details>


### [199] [Scalable Segmentation for Ultra-High-Resolution Brain MR Images](https://arxiv.org/abs/2505.21697)
*Xiaoling Hu,Peirong Liu,Dina Zemlyanker,Jonathan Williams Ramirez,Oula Puonti,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 提出一种新框架，利用低分辨率粗标签作为空间参考，通过回归有符号距离变换图实现边界感知监督，并采用可扩展的类条件分割策略，提升性能与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 超高分辨率脑MRI分割面临标注数据不足和计算需求高的挑战，需一种高效且无需额外标注的方法。

Method: 利用低分辨率粗标签作为指导，回归有符号距离变换图，并采用类条件分割策略，逐类分割以减少内存消耗。

Result: 在合成和真实数据集上验证，性能优于传统分割方法，且具备可扩展性和泛化能力。

Conclusion: 该方法在高效性和泛化性上表现优越，适用于超高分辨率脑MRI分割。

Abstract: Although deep learning has shown great success in 3D brain MRI segmentation,
achieving accurate and efficient segmentation of ultra-high-resolution brain
images remains challenging due to the lack of labeled training data for
fine-scale anatomical structures and high computational demands. In this work,
we propose a novel framework that leverages easily accessible, low-resolution
coarse labels as spatial references and guidance, without incurring additional
annotation cost. Instead of directly predicting discrete segmentation maps, our
approach regresses per-class signed distance transform maps, enabling smooth,
boundary-aware supervision. Furthermore, to enhance scalability,
generalizability, and efficiency, we introduce a scalable class-conditional
segmentation strategy, where the model learns to segment one class at a time
conditioned on a class-specific input. This novel design not only reduces
memory consumption during both training and testing, but also allows the model
to generalize to unseen anatomical classes. We validate our method through
comprehensive experiments on both synthetic and real-world datasets,
demonstrating its superior performance and scalability compared to conventional
segmentation approaches.

</details>


### [200] [MedBridge: Bridging Foundation Vision-Language Models to Medical Image Diagnosis](https://arxiv.org/abs/2505.21698)
*Yitong Li,Morteza Ghahremani,Christian Wachinger*

Main category: cs.CV

TL;DR: MedBridge是一个轻量级多模态适应框架，通过重新利用预训练的视觉语言模型（VLM）实现准确的医学图像诊断，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言基础模型在自然图像分类上表现优异，但在医学图像上因领域差异表现不佳，而训练医学基础模型需要大量资源。MedBridge旨在以最小开销填补这一差距。

Method: MedBridge包含三个关键组件：1）Focal Sampling模块提取高分辨率局部区域；2）Query Encoder（QEncoder）注入可学习查询以对齐医学语义；3）混合专家机制利用多样化VLM的互补优势。

Result: 在五个医学影像基准测试中，MedBridge在跨领域和领域内适应任务中表现优异，尤其在多标签胸部疾病诊断中AUC提升了6-15%。

Conclusion: MedBridge有效利用基础模型实现了准确且数据高效的医学诊断，代码已开源。

Abstract: Recent vision-language foundation models deliver state-of-the-art results on
natural image classification but falter on medical images due to pronounced
domain shifts. At the same time, training a medical foundation model requires
substantial resources, including extensive annotated data and high
computational capacity. To bridge this gap with minimal overhead, we introduce
MedBridge, a lightweight multimodal adaptation framework that re-purposes
pretrained VLMs for accurate medical image diagnosis. MedBridge comprises three
key components. First, a Focal Sampling module that extracts high-resolution
local regions to capture subtle pathological features and compensate for the
limited input resolution of general-purpose VLMs. Second, a Query Encoder
(QEncoder) injects a small set of learnable queries that attend to the frozen
feature maps of VLM, aligning them with medical semantics without retraining
the entire backbone. Third, a Mixture of Experts mechanism, driven by learnable
queries, harnesses the complementary strength of diverse VLMs to maximize
diagnostic performance. We evaluate MedBridge on five medical imaging
benchmarks across three key adaptation tasks, demonstrating its superior
performance in both cross-domain and in-domain adaptation settings, even under
varying levels of training data availability. Notably, MedBridge achieved over
6-15% improvement in AUC compared to state-of-the-art VLM adaptation methods in
multi-label thoracic disease diagnosis, underscoring its effectiveness in
leveraging foundation models for accurate and data-efficient medical diagnosis.
Our code is available at https://github.com/ai-med/MedBridge.

</details>


### [201] [Task-Driven Implicit Representations for Automated Design of LiDAR Systems](https://arxiv.org/abs/2505.22344)
*Nikhil Behari,Aaron Young,Akshat Dave,Ramesh Raskar*

Main category: cs.CV

TL;DR: 提出了一种基于任务的自动化LiDAR系统设计框架，通过6D设计空间和流生成模型实现高效设计。


<details>
  <summary>Details</summary>
Motivation: LiDAR设计复杂且耗时，现有方法难以满足多样化的空间和时间采样需求。

Method: 在6D设计空间中表示LiDAR配置，通过流生成模型学习任务特定的隐式密度，并利用期望最大化合成新系统。

Result: 在3D视觉任务中验证了方法的有效性，适用于人脸扫描、机器人跟踪和物体检测。

Conclusion: 该框架能够高效、自动化地设计满足约束的LiDAR系统。

Abstract: Imaging system design is a complex, time-consuming, and largely manual
process; LiDAR design, ubiquitous in mobile devices, autonomous vehicles, and
aerial imaging platforms, adds further complexity through unique spatial and
temporal sampling requirements. In this work, we propose a framework for
automated, task-driven LiDAR system design under arbitrary constraints. To
achieve this, we represent LiDAR configurations in a continuous six-dimensional
design space and learn task-specific implicit densities in this space via
flow-based generative modeling. We then synthesize new LiDAR systems by
modeling sensors as parametric distributions in 6D space and fitting these
distributions to our learned implicit density using expectation-maximization,
enabling efficient, constraint-aware LiDAR system design. We validate our
method on diverse tasks in 3D vision, enabling automated LiDAR system design
across real-world-inspired applications in face scanning, robotic tracking, and
object detection.

</details>


### [202] [OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions](https://arxiv.org/abs/2505.21724)
*Cheng Luo,Jianghui Wang,Bing Li,Siyang Song,Bernard Ghanem*

Main category: cs.CV

TL;DR: 论文提出了一种名为OMCRG的新任务，旨在在线生成同步的言语和非言语听众反馈，并提出了OmniResponse模型和ResponseNet数据集。


<details>
  <summary>Details</summary>
Motivation: 研究自然对话中听众的多模态反馈同步问题，填补现有研究的空白。

Method: 引入文本作为中间模态，提出OmniResponse模型，结合Chrono-Text和TempoVoice组件。

Result: OmniResponse在语义内容、视听同步和生成质量上显著优于基线模型。

Conclusion: OmniResponse和ResponseNet为OMCRG研究提供了有效工具和基准。

Abstract: In this paper, we introduce Online Multimodal Conversational Response
Generation (OMCRG), a novel task that aims to online generate synchronized
verbal and non-verbal listener feedback, conditioned on the speaker's
multimodal input. OMCRG reflects natural dyadic interactions and poses new
challenges in achieving synchronization between the generated audio and facial
responses of the listener. To address these challenges, we innovatively
introduce text as an intermediate modality to bridge the audio and facial
responses. We hence propose OmniResponse, a Multimodal Large Language Model
(MLLM) that autoregressively generates high-quality multi-modal listener
responses. OmniResponse leverages a pretrained LLM enhanced with two novel
components: Chrono-Text, which temporally anchors generated text tokens, and
TempoVoice, a controllable online TTS module that produces speech synchronized
with facial reactions. To support further OMCRG research, we present
ResponseNet, a new dataset comprising 696 high-quality dyadic interactions
featuring synchronized split-screen videos, multichannel audio, transcripts,
and facial behavior annotations. Comprehensive evaluations conducted on
ResponseNet demonstrate that OmniResponse significantly outperforms baseline
models in terms of semantic speech content, audio-visual synchronization, and
generation quality.

</details>


### [203] [Moment kernels: a simple and scalable approach for equivariance to rotations and reflections in deep convolutional networks](https://arxiv.org/abs/2505.21736)
*Zachary Schlamowitz,Andrew Bennecke,Daniel J. Tward*

Main category: cs.CV

TL;DR: 论文提出了一种称为“矩核”的简单卷积核形式，用于实现旋转和反射等对称性的等变性，并证明了所有等变核必须采用这种形式。该方法通过标准卷积模块实现，适用于生物医学图像分析任务。


<details>
  <summary>Details</summary>
Motivation: 旋转和反射等对称性在生物医学图像分析中至关重要，但现有方法因数学复杂性而未被广泛采用。本文旨在简化这些对称性的利用方法。

Method: 提出“矩核”作为实现等变性的简单卷积核形式，并通过标准卷积模块实现等变神经网络。

Result: 矩核形式被证明是实现等变性的必要条件，并在分类、3D图像配准和细胞分割等任务中验证了其有效性。

Conclusion: 矩核提供了一种简单且通用的方法来实现对称性等变性，为生物医学图像分析任务提供了新的解决方案。

Abstract: The principle of translation equivariance (if an input image is translated an
output image should be translated by the same amount), led to the development
of convolutional neural networks that revolutionized machine vision. Other
symmetries, like rotations and reflections, play a similarly critical role,
especially in biomedical image analysis, but exploiting these symmetries has
not seen wide adoption. We hypothesize that this is partially due to the
mathematical complexity of methods used to exploit these symmetries, which
often rely on representation theory, a bespoke concept in differential geometry
and group theory. In this work, we show that the same equivariance can be
achieved using a simple form of convolution kernels that we call ``moment
kernels,'' and prove that all equivariant kernels must take this form. These
are a set of radially symmetric functions of a spatial position $x$, multiplied
by powers of the components of $x$ or the identity matrix. We implement
equivariant neural networks using standard convolution modules, and provide
architectures to execute several biomedical image analysis tasks that depend on
equivariance principles: classification (outputs are invariant under orthogonal
transforms), 3D image registration (outputs transform like a vector), and cell
segmentation (quadratic forms defining ellipses transform like a matrix).

</details>


### [204] [GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action Control](https://arxiv.org/abs/2505.22421)
*Anthony Chen,Wenzhao Zheng,Yida Wang,Xueyang Zhang,Kun Zhan,Peng Jia,Kurt Keutzer,Shangbang Zhang*

Main category: cs.CV

TL;DR: GeoDrive通过将3D几何条件显式整合到驾驶世界模型中，提升了自动驾驶的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前方法在保持3D几何一致性或处理遮挡时存在缺陷，影响自动驾驶的安全性评估。

Method: 首先从输入帧提取3D表示，然后基于用户指定的车辆轨迹生成2D渲染；训练时通过动态编辑模块增强渲染效果。

Result: 实验表明，GeoDrive在动作准确性和3D空间感知上显著优于现有模型，并能推广到新轨迹。

Conclusion: GeoDrive提供了更真实、适应性强且可靠的场景建模，支持交互式场景编辑，为自动驾驶提供更安全的解决方案。

Abstract: Recent advancements in world models have revolutionized dynamic environment
simulation, allowing systems to foresee future states and assess potential
actions. In autonomous driving, these capabilities help vehicles anticipate the
behavior of other road users, perform risk-aware planning, accelerate training
in simulation, and adapt to novel scenarios, thereby enhancing safety and
reliability. Current approaches exhibit deficiencies in maintaining robust 3D
geometric consistency or accumulating artifacts during occlusion handling, both
critical for reliable safety assessment in autonomous navigation tasks. To
address this, we introduce GeoDrive, which explicitly integrates robust 3D
geometry conditions into driving world models to enhance spatial understanding
and action controllability. Specifically, we first extract a 3D representation
from the input frame and then obtain its 2D rendering based on the
user-specified ego-car trajectory. To enable dynamic modeling, we propose a
dynamic editing module during training to enhance the renderings by editing the
positions of the vehicles. Extensive experiments demonstrate that our method
significantly outperforms existing models in both action accuracy and 3D
spatial awareness, leading to more realistic, adaptable, and reliable scene
modeling for safer autonomous driving. Additionally, our model can generalize
to novel trajectories and offers interactive scene editing capabilities, such
as object editing and object trajectory control.

</details>


### [205] [What is Adversarial Training for Diffusion Models?](https://arxiv.org/abs/2505.21742)
*Briglia Maria Rosaria,Mujtaba Hussain Mirza,Giuseppe Lisanti,Iacopo Masi*

Main category: cs.CV

TL;DR: 对抗训练（AT）在扩散模型（DMs）中与分类器不同，要求等变性以保持扩散过程与数据分布对齐，提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索对抗训练在扩散模型中的独特作用，区别于分类器，以提升模型对噪声和异常数据的处理能力。

Method: 通过添加随机噪声（类似随机平滑）或对抗噪声（类似AT）无缝集成到扩散训练中，无需假设噪声模型。

Result: 在低维和高维数据集及标准基准（如CIFAR-10、CelebA）上表现优异，能应对严重噪声、数据损坏和对抗攻击。

Conclusion: 对抗训练在扩散模型中通过等变性提升鲁棒性，适用于噪声数据、异常值和对抗场景。

Abstract: We answer the question in the title, showing that adversarial training (AT)
for diffusion models (DMs) fundamentally differs from classifiers: while AT in
classifiers enforces output invariance, AT in DMs requires equivariance to keep
the diffusion process aligned with the data distribution. AT is a way to
enforce smoothness in the diffusion flow, improving robustness to outliers and
corrupted data. Unlike prior art, our method makes no assumptions about the
noise model and integrates seamlessly into diffusion training by adding random
noise, similar to randomized smoothing, or adversarial noise, akin to AT. This
enables intrinsic capabilities such as handling noisy data, dealing with
extreme variability such as outliers, preventing memorization, and improving
robustness. We rigorously evaluate our approach with proof-of-concept datasets
with known distributions in low- and high-dimensional space, thereby taking a
perfect measure of errors; we further evaluate on standard benchmarks such as
CIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe
noise, data corruption, and iterative adversarial attacks.

</details>


### [206] [Zero-Shot 3D Visual Grounding from Vision-Language Models](https://arxiv.org/abs/2505.22429)
*Rong Li,Shijie Li,Lingdong Kong,Xulei Yang,Junwei Liang*

Main category: cs.CV

TL;DR: SeeGround是一个零样本3D视觉定位框架，利用2D视觉语言模型（VLMs）避免3D特定训练需求，通过混合输入格式和核心模块提升定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位方法依赖标记数据和预定义类别，限制了在开放世界中的扩展性。

Method: 引入混合输入格式（查询对齐渲染视图与空间增强文本描述），包含视角适应模块和融合对齐模块。

Result: 在ScanRefer和Nr3D上显著优于零样本基线（分别提升7.7%和7.1%），甚至媲美全监督方法。

Conclusion: SeeGround展示了在挑战性条件下的强泛化能力，为3D视觉定位提供了高效解决方案。

Abstract: 3D Visual Grounding (3DVG) seeks to locate target objects in 3D scenes using
natural language descriptions, enabling downstream applications such as
augmented reality and robotics. Existing approaches typically rely on labeled
3D data and predefined categories, limiting scalability to open-world settings.
We present SeeGround, a zero-shot 3DVG framework that leverages 2D
Vision-Language Models (VLMs) to bypass the need for 3D-specific training. To
bridge the modality gap, we introduce a hybrid input format that pairs
query-aligned rendered views with spatially enriched textual descriptions. Our
framework incorporates two core components: a Perspective Adaptation Module
that dynamically selects optimal viewpoints based on the query, and a Fusion
Alignment Module that integrates visual and spatial signals to enhance
localization precision. Extensive evaluations on ScanRefer and Nr3D confirm
that SeeGround achieves substantial improvements over existing zero-shot
baselines -- outperforming them by 7.7% and 7.1%, respectively -- and even
rivals fully supervised alternatives, demonstrating strong generalization under
challenging conditions.

</details>


### [207] [Learning to See More: UAS-Guided Super-Resolution of Satellite Imagery for Precision Agriculture](https://arxiv.org/abs/2505.21746)
*Arif Masrur,Peder A. Olsen,Paul R. Adler,Carlan Jackson,Matthew W. Myers,Nathan Sedghi,Ray R. Weil*

Main category: cs.CV

TL;DR: 该研究提出了一种融合卫星和无人机（UAS）影像的超分辨率方法，用于精准农业中的覆盖作物生物量和氮含量估算，显著提高了精度。


<details>
  <summary>Details</summary>
Motivation: 卫星和无人机在精准农业中各具优势与局限，卫星覆盖广但分辨率低，无人机分辨率高但成本高且覆盖有限。研究旨在结合两者优势，提供一种经济高效的解决方案。

Method: 通过超分辨率方法融合卫星和无人机影像，扩展无人机RGB数据的光谱范围，生成高分辨率Sentinel-2影像，并应用于生物量和氮含量估算。

Result: 生物量和氮含量估算精度分别提高了18%和31%，且无人机数据仅需部分采集，降低了成本。

Conclusion: 该方法轻量且可扩展，适用于农场实际应用，即使在卫星数据不可用时仍有效。

Abstract: Unmanned Aircraft Systems (UAS) and satellites are key data sources for
precision agriculture, yet each presents trade-offs. Satellite data offer broad
spatial, temporal, and spectral coverage but lack the resolution needed for
many precision farming applications, while UAS provide high spatial detail but
are limited by coverage and cost, especially for hyperspectral data. This study
presents a novel framework that fuses satellite and UAS imagery using
super-resolution methods. By integrating data across spatial, spectral, and
temporal domains, we leverage the strengths of both platforms cost-effectively.
We use estimation of cover crop biomass and nitrogen (N) as a case study to
evaluate our approach. By spectrally extending UAS RGB data to the vegetation
red edge and near-infrared regions, we generate high-resolution Sentinel-2
imagery and improve biomass and N estimation accuracy by 18% and 31%,
respectively. Our results show that UAS data need only be collected from a
subset of fields and time points. Farmers can then 1) enhance the spectral
detail of UAS RGB imagery; 2) increase the spatial resolution by using
satellite data; and 3) extend these enhancements spatially and across the
growing season at the frequency of the satellite flights. Our SRCNN-based
spectral extension model shows considerable promise for model transferability
over other cropping systems in the Upper and Lower Chesapeake Bay regions.
Additionally, it remains effective even when cloud-free satellite data are
unavailable, relying solely on the UAS RGB input. The spatial extension model
produces better biomass and N predictions than models built on raw UAS RGB
images. Once trained with targeted UAS RGB data, the spatial extension model
allows farmers to stop repeated UAS flights. While we introduce
super-resolution advances, the core contribution is a lightweight and scalable
system for affordable on-farm use.

</details>


### [208] [FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering](https://arxiv.org/abs/2505.21755)
*Chengyue Huang,Brisa Maneechotesuwan,Shivang Chopra,Zsolt Kira*

Main category: cs.CV

TL;DR: 提出FRAMES-VQA基准，评估VQA任务中多模态分布变化的鲁棒性微调方法，并分析模态交互作用。


<details>
  <summary>Details</summary>
Motivation: 现有VQA系统在多模态数据变化下鲁棒性不足，且评估设置多为单模态或特定OOD类型，缺乏全面性。

Method: 利用十个VQA基准数据集，分类为ID、近OOD和远OOD，计算Mahalanobis距离量化分布变化，分析模态交互作用。

Result: 提供了多模态分布变化的量化分析，揭示了模态重要性及交互作用，为鲁棒微调方法提供指导。

Conclusion: FRAMES-VQA为多模态VQA任务中的鲁棒性微调提供了新基准和分析框架，代码已开源。

Abstract: Visual question answering (VQA) systems face significant challenges when
adapting to real-world data shifts, especially in multi-modal contexts. While
robust fine-tuning strategies are essential for maintaining performance across
in-distribution (ID) and out-of-distribution (OOD) scenarios, current
evaluation settings are primarily unimodal or particular to some types of OOD,
offering limited insight into the complexities of multi-modal contexts. In this
work, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across
Multi-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We
utilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA
and others, and categorize them into ID, near and far OOD datasets covering
uni-modal, multi-modal and adversarial distribution shifts. We first conduct a
comprehensive comparison of existing robust fine-tuning methods. We then
quantify the distribution shifts by calculating the Mahalanobis distance using
uni-modal and multi-modal embeddings extracted from various models. Further, we
perform an extensive analysis to explore the interactions between uni- and
multi-modal shifts as well as modality importance for ID and OOD samples. These
analyses offer valuable guidance on developing more robust fine-tuning methods
to handle multi-modal distribution shifts. The code is available at
https://github.com/chengyuehuang511/FRAMES-VQA .

</details>


### [209] [MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning](https://arxiv.org/abs/2505.21771)
*Prasham Yatinkumar Titiya,Jainil Trivedi,Chitta Baral,Vivek Gupta*

Main category: cs.CV

TL;DR: MMTBENCH是一个包含500个真实世界多模态表格的基准测试，用于评估现有视觉语言模型在多模态表格推理上的表现，发现其在视觉推理和多步推理方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在复杂多模态表格推理上的能力尚未被充分探索，因此需要建立一个基准测试来填补这一空白。

Method: 通过收集500个真实世界的多模态表格，构建了4021个问答对，涵盖多种问题类型、推理类型和表格类型。

Result: 评估显示现有模型在视觉推理和多步推理任务上表现不佳，存在显著性能差距。

Conclusion: MMTBENCH为未来研究提供了高质量资源，突显了改进视觉与语言处理整合架构的迫切需求。

Abstract: Multimodal tables those that integrate semi structured data with visual
elements such as charts and maps are ubiquitous across real world domains, yet
they pose a formidable challenge to current vision language models (VLMs).
While Large Language models (LLMs) and VLMs have demonstrated strong
capabilities in text and image understanding, their performance on complex,
real world multimodal table reasoning remains unexplored. To bridge this gap,
we introduce MMTBENCH (Multimodal Table Benchmark), a benchmark consisting of
500 real world multimodal tables drawn from diverse real world sources, with a
total of 4021 question answer pairs. MMTBENCH questions cover four question
types (Explicit, Implicit, Answer Mention, and Visual Based), five reasoning
types (Mathematical, Extrema Identification, Fact Verification, Vision Based,
and Others), and eight table types (Single/Multiple Entity, Maps and Charts
with Entities, Single/Multiple Charts, Maps, and Visualizations). Extensive
evaluation of state of the art models on all types reveals substantial
performance gaps, particularly on questions requiring visual-based reasoning
and multi-step inference. These findings show the urgent need for improved
architectures that more tightly integrate vision and language processing. By
providing a challenging, high-quality resource that mirrors the complexity of
real-world tasks, MMTBENCH underscores its value as a resource for future
research on multimodal tables.

</details>


### [210] [Compositional Scene Understanding through Inverse Generative Modeling](https://arxiv.org/abs/2505.21780)
*Yanbo Wang,Justin Dauwels,Yilun Du*

Main category: cs.CV

TL;DR: 论文提出了一种通过逆向生成建模理解场景的方法，利用组合式生成模型从图像中推断场景结构和全局因素，并展示了其在零样本多物体感知中的应用。


<details>
  <summary>Details</summary>
Motivation: 探索生成模型不仅能合成视觉内容，还能通过逆向建模理解自然图像中的场景属性。

Method: 将场景理解建模为逆向生成问题，通过组合式生成模型推断场景结构和全局因素。

Result: 方法能够从训练数据之外的图像中推断场景结构和物体，并实现对新场景的鲁棒泛化。

Conclusion: 组合式逆向生成建模为场景理解提供了新思路，并可应用于零样本多物体感知任务。

Abstract: Generative models have demonstrated remarkable abilities in generating
high-fidelity visual content. In this work, we explore how generative models
can further be used not only to synthesize visual content but also to
understand the properties of a scene given a natural image. We formulate scene
understanding as an inverse generative modeling problem, where we seek to find
conditional parameters of a visual generative model to best fit a given natural
image. To enable this procedure to infer scene structure from images
substantially different than those seen during training, we further propose to
build this visual generative model compositionally from smaller models over
pieces of a scene. We illustrate how this procedure enables us to infer the set
of objects in a scene, enabling robust generalization to new test scenes with
an increased number of objects of new shapes. We further illustrate how this
enables us to infer global scene factors, likewise enabling robust
generalization to new scenes. Finally, we illustrate how this approach can be
directly applied to existing pretrained text-to-image generative models for
zero-shot multi-object perception. Code and visualizations are at
\href{https://energy-based-model.github.io/compositional-inference}{https://energy-based-model.github.io/compositional-inference}.

</details>


### [211] [SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation](https://arxiv.org/abs/2505.21795)
*Claudia Cuttano,Gabriele Trivigno,Giuseppe Averta,Carlo Masone*

Main category: cs.CV

TL;DR: SANSA通过重新利用SAM2的语义结构，提出了一种用于少样本分割的框架，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: SAM2虽然具备强大的分割能力，但其特征表示与任务特定线索纠缠，限制了其在需要高级语义理解的任务中的应用。

Method: 提出SANSA框架，显式利用SAM2的潜在语义结构，并通过少量任务特定修改将其重新用于少样本分割。

Result: SANSA在少样本分割基准测试中表现优异，支持多种提示方式，且速度更快、模型更紧凑。

Conclusion: SANSA成功地将SAM2重新用于少样本分割，展示了其语义结构的潜力，并在性能和灵活性上优于现有方法。

Abstract: Few-shot segmentation aims to segment unseen object categories from just a
handful of annotated examples. This requires mechanisms that can both identify
semantically related objects across images and accurately produce segmentation
masks. We note that Segment Anything 2 (SAM2), with its prompt-and-propagate
mechanism, offers both strong segmentation capabilities and a built-in feature
matching process. However, we show that its representations are entangled with
task-specific cues optimized for object tracking, which impairs its use for
tasks requiring higher level semantic understanding. Our key insight is that,
despite its class-agnostic pretraining, SAM2 already encodes rich semantic
structure in its features. We propose SANSA (Semantically AligNed Segment
Anything 2), a framework that makes this latent structure explicit, and
repurposes SAM2 for few-shot segmentation through minimal task-specific
modifications. SANSA achieves state-of-the-art performance on few-shot
segmentation benchmarks specifically designed to assess generalization,
outperforms generalist methods in the popular in-context setting, supports
various prompts flexible interaction via points, boxes, or scribbles, and
remains significantly faster and more compact than prior approaches. Code is
available at https://github.com/ClaudiaCuttano/SANSA.

</details>


### [212] [ALTER: All-in-One Layer Pruning and Temporal Expert Routing for Efficient Diffusion Generation](https://arxiv.org/abs/2505.21817)
*Xiaomeng Yang,Lei Lu,Qihui Fan,Changdi Yang,Juyi Lin,Yanzhi Wang,Xuan Zhang,Shangqian Gao*

Main category: cs.CV

TL;DR: ALTER框架通过统一层剪枝、专家路由和微调，显著提升扩散模型效率，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型计算开销大，现有加速方法未能有效捕捉时间动态且剪枝策略不优。

Method: ALTER结合可训练超网络，动态生成剪枝决策并管理时间步路由，实现单阶段优化。

Result: ALTER仅用25.9%计算量（20步推理）达到原模型50步的视觉保真度，提速3.64倍。

Conclusion: ALTER为资源受限环境提供高效且高质量的扩散模型解决方案。

Abstract: Diffusion models have demonstrated exceptional capabilities in generating
high-fidelity images. However, their iterative denoising process results in
significant computational overhead during inference, limiting their practical
deployment in resource-constrained environments. Existing acceleration methods
often adopt uniform strategies that fail to capture the temporal variations
during diffusion generation, while the commonly adopted sequential
pruning-then-fine-tuning strategy suffers from sub-optimality due to the
misalignment between pruning decisions made on pretrained weights and the
model's final parameters. To address these limitations, we introduce ALTER:
All-in-One Layer Pruning and Temporal Expert Routing, a unified framework that
transforms diffusion models into a mixture of efficient temporal experts. ALTER
achieves a single-stage optimization that unifies layer pruning, expert
routing, and model fine-tuning by employing a trainable hypernetwork, which
dynamically generates layer pruning decisions and manages timestep routing to
specialized, pruned expert sub-networks throughout the ongoing fine-tuning of
the UNet. This unified co-optimization strategy enables significant efficiency
gains while preserving high generative quality. Specifically, ALTER achieves
same-level visual fidelity to the original 50-step Stable Diffusion v2.1 model
while utilizing only 25.9% of its total MACs with just 20 inference steps and
delivering a 3.64x speedup through 35% sparsity.

</details>


### [213] [HDRSDR-VQA: A Subjective Video Quality Dataset for HDR and SDR Comparative Evaluation](https://arxiv.org/abs/2505.21831)
*Bowen Chen,Cheng-han Lee,Yixu Chen,Zaixi Shang,Hai Wei,Alan C. Bovik*

Main category: cs.CV

TL;DR: HDRSDR-VQA是一个大规模视频质量评估数据集，支持HDR和SDR内容的直接比较，包含960个视频和22,000对主观评分。


<details>
  <summary>Details</summary>
Motivation: 促进HDR和SDR内容在真实观看条件下的比较分析。

Method: 通过54个源序列生成960个视频，使用6台HDR电视和145名参与者进行主观评分。

Result: 数据集支持HDR和SDR版本的直接比较，揭示了格式偏好的原因。

Conclusion: HDRSDR-VQA为视频质量评估和感知模型开发提供了重要资源。

Abstract: We introduce HDRSDR-VQA, a large-scale video quality assessment dataset
designed to facilitate comparative analysis between High Dynamic Range (HDR)
and Standard Dynamic Range (SDR) content under realistic viewing conditions.
The dataset comprises 960 videos generated from 54 diverse source sequences,
each presented in both HDR and SDR formats across nine distortion levels. To
obtain reliable perceptual quality scores, we conducted a comprehensive
subjective study involving 145 participants and six consumer-grade HDR-capable
televisions. A total of over 22,000 pairwise comparisons were collected and
scaled into Just-Objectionable-Difference (JOD) scores. Unlike prior datasets
that focus on a single dynamic range format or use limited evaluation
protocols, HDRSDR-VQA enables direct content-level comparison between HDR and
SDR versions, supporting detailed investigations into when and why one format
is preferred over the other. The open-sourced part of the dataset is publicly
available to support further research in video quality assessment,
content-adaptive streaming, and perceptual model development.

</details>


### [214] [UniMoGen: Universal Motion Generation](https://arxiv.org/abs/2505.21837)
*Aliasghar Khani,Arianna Rampini,Evan Atherton,Bruno Roy*

Main category: cs.CV

TL;DR: UniMoGen是一种基于UNet的扩散模型，用于骨架无关的运动生成，支持多样角色（如人类和动物）的运动数据训练，无需预定义关节数。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖特定骨架结构，限制了其通用性。UniMoGen旨在克服这一限制，实现骨架无关的运动生成。

Method: 采用UNet架构的扩散模型，动态处理必要关节，支持风格和轨迹输入控制，并能延续过去帧的运动。

Result: 在100style数据集上表现优于现有方法，同时在100style和LAFAN1数据集上均实现高性能和效率。

Conclusion: UniMoGen为角色动画提供了灵活、高效且可控的解决方案，具有广泛应用潜力。

Abstract: Motion generation is a cornerstone of computer graphics, animation, gaming,
and robotics, enabling the creation of realistic and varied character
movements. A significant limitation of existing methods is their reliance on
specific skeletal structures, which restricts their versatility across
different characters. To overcome this, we introduce UniMoGen, a novel
UNet-based diffusion model designed for skeleton-agnostic motion generation.
UniMoGen can be trained on motion data from diverse characters, such as humans
and animals, without the need for a predefined maximum number of joints. By
dynamically processing only the necessary joints for each character, our model
achieves both skeleton agnosticism and computational efficiency. Key features
of UniMoGen include controllability via style and trajectory inputs, and the
ability to continue motions from past frames. We demonstrate UniMoGen's
effectiveness on the 100style dataset, where it outperforms state-of-the-art
methods in diverse character motion generation. Furthermore, when trained on
both the 100style and LAFAN1 datasets, which use different skeletons, UniMoGen
achieves high performance and improved efficiency across both skeletons. These
results highlight UniMoGen's potential to advance motion generation by
providing a flexible, efficient, and controllable solution for a wide range of
character animations.

</details>


### [215] [Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2505.21844)
*Mehrdad Noori,David Osowiechi,Gustavo Adolfo Vargas Hakim,Ali Bahri,Moslem Yazdanpanah,Sahar Dastani,Farzad Beizaee,Ismail Ben Ayed,Christian Desrosiers*

Main category: cs.CV

TL;DR: 提出了一种针对开放词汇语义分割（OVSS）的测试时适应（TTA）方法MLMP，通过多级多提示熵最小化优化视觉语言模型，无需额外数据或标签。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法主要关注图像分类，而密集预测任务如OVSS被忽视，需针对性解决方案。

Method: MLMP方法整合中间视觉编码器层特征，并在全局CLS标记和局部像素级别使用不同文本提示模板。

Result: 实验表明，MLMP在82种测试场景中显著优于直接采用TTA分类基线。

Conclusion: MLMP为OVSS提供了一种有效的TTA方法，并建立了标准化评测基准。

Abstract: Recently, test-time adaptation has attracted wide interest in the context of
vision-language models for image classification. However, to the best of our
knowledge, the problem is completely overlooked in dense prediction tasks such
as Open-Vocabulary Semantic Segmentation (OVSS). In response, we propose a
novel TTA method tailored to adapting VLMs for segmentation during test time.
Unlike TTA methods for image classification, our Multi-Level and Multi-Prompt
(MLMP) entropy minimization integrates features from intermediate
vision-encoder layers and is performed with different text-prompt templates at
both the global CLS token and local pixel-wise levels. Our approach could be
used as plug-and-play for any segmentation network, does not require additional
training data or labels, and remains effective even with a single test sample.
Furthermore, we introduce a comprehensive OVSS TTA benchmark suite, which
integrates a rigorous evaluation protocol, seven segmentation datasets, and 15
common corruptions, with a total of 82 distinct test scenarios, establishing a
standardized and comprehensive testbed for future TTA research in
open-vocabulary segmentation. Our experiments on this suite demonstrate that
our segmentation-tailored method consistently delivers significant gains over
direct adoption of TTA classification baselines.

</details>


### [216] [RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers](https://arxiv.org/abs/2505.21847)
*Xuwei Xu,Yang Li,Yudong Chen,Jiajun Liu,Sen Wang*

Main category: cs.CV

TL;DR: 研究发现FFN层是ViT推理延迟的主要来源，提出了一种通道空闲机制，通过结构重参数化优化FFN层，显著降低了延迟并保持了准确性。


<details>
  <summary>Details</summary>
Motivation: 揭示FFN层对ViT推理延迟的显著影响，探索优化大规模ViT效率的方法。

Method: 提出通道空闲机制，允许部分特征通道绕过非线性激活函数，形成线性路径以实现结构重参数化。

Result: RePaViT系列模型在延迟显著降低的同时，准确性损失小甚至提升，尤其在大型模型中表现更优。

Conclusion: RePaViT通过结构重参数化优化FFN层，为高效ViT提供了新方向。

Abstract: We reveal that feedforward network (FFN) layers, rather than attention
layers, are the primary contributors to Vision Transformer (ViT) inference
latency, with their impact signifying as model size increases. This finding
highlights a critical opportunity for optimizing the efficiency of large-scale
ViTs by focusing on FFN layers. In this work, we propose a novel channel idle
mechanism that facilitates post-training structural reparameterization for
efficient FFN layers during testing. Specifically, a set of feature channels
remains idle and bypasses the nonlinear activation function in each FFN layer,
thereby forming a linear pathway that enables structural reparameterization
during inference. This mechanism results in a family of ReParameterizable
Vision Transformers (RePaViTs), which achieve remarkable latency reductions
with acceptable sacrifices (sometimes gains) in accuracy across various ViTs.
The benefits of our method scale consistently with model sizes, demonstrating
greater speed improvements and progressively narrowing accuracy gaps or even
higher accuracies on larger models. In particular, RePa-ViT-Large and
RePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1
accuracies under the same training strategy, respectively. RePaViT is the first
to employ structural reparameterization on FFN layers to expedite ViTs to our
best knowledge, and we believe that it represents an auspicious direction for
efficient ViTs. Source code is available at
https://github.com/Ackesnal/RePaViT.

</details>


### [217] [FPAN: Mitigating Replication in Diffusion Models through the Fine-Grained Probabilistic Addition of Noise to Token Embeddings](https://arxiv.org/abs/2505.21848)
*Jingqi Xu,Chenghao Li,Yuke Zhang,Peter A. Beerel*

Main category: cs.CV

TL;DR: 论文提出了一种细粒度噪声注入技术（FPAN），以减少扩散模型对训练数据的复制，保护隐私。实验表明FPAN显著优于基线方法和其他现有策略。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高质量图像时可能复制训练数据中的敏感信息，引发隐私问题。现有方法效果有限，需改进。

Method: 通过分析不同噪声量的影响，提出FPAN技术，概率性地为词嵌入添加更大噪声。

Result: FPAN平均减少28.78%的数据复制，优于基线和其他方法，且不影响图像质量。

Conclusion: FPAN是一种有效的隐私保护方法，可与其他技术结合进一步提升效果。

Abstract: Diffusion models have demonstrated remarkable potential in generating
high-quality images. However, their tendency to replicate training data raises
serious privacy concerns, particularly when the training datasets contain
sensitive or private information. Existing mitigation strategies primarily
focus on reducing image duplication, modifying the cross-attention mechanism,
and altering the denoising backbone architecture of diffusion models. Moreover,
recent work has shown that adding a consistent small amount of noise to text
embeddings can reduce replication to some degree. In this work, we begin by
analyzing the impact of adding varying amounts of noise. Based on our analysis,
we propose a fine-grained noise injection technique that probabilistically adds
a larger amount of noise to token embeddings. We refer to our method as
Fine-grained Probabilistic Addition of Noise (FPAN). Through our extensive
experiments, we show that our proposed FPAN can reduce replication by an
average of 28.78% compared to the baseline diffusion model without
significantly impacting image quality, and outperforms the prior
consistent-magnitude-noise-addition approach by 26.51%. Moreover, when combined
with other existing mitigation methods, our FPAN approach can further reduce
replication by up to 16.82% with similar, if not improved, image quality.

</details>


### [218] [Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task](https://arxiv.org/abs/2505.21850)
*Yanbei Jiang,Yihao Ding,Chao Lei,Jiayang Ao,Jey Han Lau,Krista A. Ehinger*

Main category: cs.CV

TL;DR: 论文提出了MultiStAR基准和MSEval指标，用于评估多模态大语言模型在抽象视觉推理中的多阶段表现，发现现有模型在复杂规则检测阶段仍有挑战。


<details>
  <summary>Details</summary>
Motivation: 现有抽象视觉推理基准仅关注单步推理和最终结果，忽略了多阶段推理过程，且缺乏对中间步骤正确性的评估。

Method: 基于RAVEN设计MultiStAR基准，并提出MSEval指标评估中间步骤和最终结果的正确性。

Result: 实验表明，现有模型在基础感知任务表现良好，但在复杂规则检测阶段仍有困难。

Conclusion: MultiStAR和MSEval填补了评估空白，揭示了现有模型在复杂推理中的不足。

Abstract: Current Multimodal Large Language Models (MLLMs) excel in general visual
reasoning but remain underexplored in Abstract Visual Reasoning (AVR), which
demands higher-order reasoning to identify abstract rules beyond simple
perception. Existing AVR benchmarks focus on single-step reasoning, emphasizing
the end result but neglecting the multi-stage nature of reasoning process. Past
studies found MLLMs struggle with these benchmarks, but it doesn't explain how
they fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR
benchmark, based on RAVEN, designed to assess reasoning across varying levels
of complexity. Additionally, existing metrics like accuracy only focus on the
final outcomes while do not account for the correctness of intermediate steps.
Therefore, we propose a novel metric, MSEval, which considers the correctness
of intermediate steps in addition to the final outcomes. We conduct
comprehensive experiments on MultiStAR using 17 representative close-source and
open-source MLLMs. The results reveal that while existing MLLMs perform
adequately on basic perception tasks, they continue to face challenges in more
complex rule detection stages.

</details>


### [219] [Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification](https://arxiv.org/abs/2505.21854)
*Jun Chen,Xinke Li,Mingyue Xu,Tianrui Li,Chongshou Li*

Main category: cs.CV

TL;DR: 论文提出两种新策略（WAAttack和SubAttack）改进基于梯度的对抗攻击，通过加权梯度和自适应步长策略以及子集分解，提高攻击效果和不可感知性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未考虑点云的异质性，导致扰动过大且明显，需改进梯度更新机制。

Method: 提出WAAttack（加权梯度和自适应步长）和SubAttack（子集分解和关键区域扰动）。

Result: 实验表明新方法在生成不可感知对抗样本上优于现有基线。

Conclusion: 新策略为3D点云分类的对抗攻击提供了更有效的设计思路。

Abstract: Gradient-based adversarial attacks have become a dominant approach for
evaluating the robustness of point cloud classification models. However,
existing methods often rely on uniform update rules that fail to consider the
heterogeneous nature of point clouds, resulting in excessive and perceptible
perturbations. In this paper, we rethink the design of gradient-based attacks
by analyzing the limitations of conventional gradient update mechanisms and
propose two new strategies to improve both attack effectiveness and
imperceptibility. First, we introduce WAAttack, a novel framework that
incorporates weighted gradients and an adaptive step-size strategy to account
for the non-uniform contribution of points during optimization. This approach
enables more targeted and subtle perturbations by dynamically adjusting updates
according to the local structure and sensitivity of each point. Second, we
propose SubAttack, a complementary strategy that decomposes the point cloud
into subsets and focuses perturbation efforts on structurally critical regions.
Together, these methods represent a principled rethinking of gradient-based
adversarial attacks for 3D point cloud classification. Extensive experiments
demonstrate that our approach outperforms state-of-the-art baselines in
generating highly imperceptible adversarial examples. Code will be released
upon paper acceptance.

</details>


### [220] [Towards Scalable Language-Image Pre-training for 3D Medical Imaging](https://arxiv.org/abs/2505.21862)
*Chenhui Zhao,Yiwei Lyu,Asadur Chowdury,Edward Harake,Akhil Kondepudi,Akshay Rao,Xinhai Hou,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: HLIP是一种针对3D医学影像的可扩展预训练框架，通过轻量级分层注意力机制显著提升性能，并支持直接在大规模未筛选临床数据上训练。


<details>
  <summary>Details</summary>
Motivation: 解决3D医学影像（如CT和MRI）因计算需求高而难以在大规模未筛选临床数据上进行语言-图像预训练的问题。

Method: 采用轻量级分层注意力机制（切片、扫描、研究层次），提升计算效率和泛化能力。

Result: 在多个基准测试中表现优异，如Rad-ChestCT（+4.3% AUC）、Pub-Brain-5（+32.4% ACC）和RSNA/CQ500（+1.4%/+6.9% AUC）。

Conclusion: HLIP证明直接在大规模未筛选临床数据上预训练是3D医学影像语言-图像预训练的有效方向。

Abstract: Language-image pre-training has demonstrated strong performance in 2D medical
imaging, but its success in 3D modalities such as CT and MRI remains limited
due to the high computational demands of volumetric data, which pose a
significant barrier to training on large-scale, uncurated clinical studies. In
this study, we introduce Hierarchical attention for Language-Image Pre-training
(HLIP), a scalable pre-training framework for 3D medical imaging. HLIP adopts a
lightweight hierarchical attention mechanism inspired by the natural hierarchy
of radiology data: slice, scan, and study. This mechanism exhibits strong
generalizability, e.g., +4.3% macro AUC on the Rad-ChestCT benchmark when
pre-trained on CT-RATE. Moreover, the computational efficiency of HLIP enables
direct training on uncurated datasets. Trained on 220K patients with 3.13
million scans for brain MRI and 240K patients with 1.44 million scans for head
CT, HLIP achieves state-of-the-art performance, e.g., +32.4% balanced ACC on
the proposed publicly available brain MRI benchmark Pub-Brain-5; +1.4% and
+6.9% macro AUC on head CT benchmarks RSNA and CQ500, respectively. These
results demonstrate that, with HLIP, directly pre-training on uncurated
clinical datasets is a scalable and effective direction for language-image
pre-training in 3D medical imaging. The code is available at
https://github.com/Zch0414/hlip

</details>


### [221] [GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning](https://arxiv.org/abs/2505.21863)
*Shikhhar Siingh,Abhinav Rawat,Vivek Gupta,Chitta Baral*

Main category: cs.CV

TL;DR: GETReason框架通过提取地理空间、时间和事件信息，提升图像上下文理解的深度，并引入GREAT指标评估推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以准确提取图像中的上下文信息，限制了其在新闻和教育中的应用。

Method: 提出GETReason框架和GREAT评估指标，采用分层多智能体方法进行推理。

Result: 实验表明，该方法能有效推断图像的深层意义，并将其与事件背景关联。

Conclusion: GETReason和GREAT为图像上下文理解提供了新思路和评估标准。

Abstract: Publicly significant images from events hold valuable contextual information,
crucial for journalism and education. However, existing methods often struggle
to extract this relevance accurately. To address this, we introduce GETReason
(Geospatial Event Temporal Reasoning), a framework that moves beyond
surface-level image descriptions to infer deeper contextual meaning. We propose
that extracting global event, temporal, and geospatial information enhances
understanding of an image's significance. Additionally, we introduce GREAT
(Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric
for evaluating reasoning-based image understanding. Our layered multi-agent
approach, assessed using a reasoning-weighted metric, demonstrates that
meaningful insights can be inferred, effectively linking images to their
broader event context.

</details>


### [222] [Cross-DINO: Cross the Deep MLP and Transformer for Small Object Detection](https://arxiv.org/abs/2505.21868)
*Guiping Cao,Wenjian Huang,Xiangyuan Lan,Jianguo Zhang,Dongmei Jiang,Yaowei Wang*

Main category: cs.CV

TL;DR: 论文提出Cross-DINO方法，通过结合深度MLP网络和新的Cross Coding Twice Module（CCTM）提升小目标检测性能，并引入Category-Size（CS）标签和Boost Loss损失函数，显著提高了DETR-like模型在小目标检测上的表现。


<details>
  <summary>Details</summary>
Motivation: 小目标检测（SOD）因信息有限和模型预测分数低而面临挑战，现有Transformer-based检测器在SOD上的潜力尚未充分挖掘。

Method: 提出Cross-DINO方法，结合深度MLP网络和CCTM模块增强小目标细节，并引入CS标签和Boost Loss损失函数。

Result: 在多个数据集上验证，Cross-DINO显著提升性能，例如在COCO上达到36.4% APs，优于DINO模型。

Conclusion: Cross-DINO通过创新模块和损失函数，有效提升了小目标检测的性能和效率。

Abstract: Small Object Detection (SOD) poses significant challenges due to limited
information and the model's low class prediction score. While Transformer-based
detectors have shown promising performance, their potential for SOD remains
largely unexplored. In typical DETR-like frameworks, the CNN backbone network,
specialized in aggregating local information, struggles to capture the
necessary contextual information for SOD. The multiple attention layers in the
Transformer Encoder face difficulties in effectively attending to small objects
and can also lead to blurring of features. Furthermore, the model's lower class
prediction score of small objects compared to large objects further increases
the difficulty of SOD. To address these challenges, we introduce a novel
approach called Cross-DINO. This approach incorporates the deep MLP network to
aggregate initial feature representations with both short and long range
information for SOD. Then, a new Cross Coding Twice Module (CCTM) is applied to
integrate these initial representations to the Transformer Encoder feature,
enhancing the details of small objects. Additionally, we introduce a new kind
of soft label named Category-Size (CS), integrating the Category and Size of
objects. By treating CS as new ground truth, we propose a new loss function
called Boost Loss to improve the class prediction score of the model. Extensive
experimental results on COCO, WiderPerson, VisDrone, AI-TOD, and SODA-D
datasets demonstrate that Cross-DINO efficiently improves the performance of
DETR-like models on SOD. Specifically, our model achieves 36.4% APs on COCO for
SOD with only 45M parameters, outperforming the DINO by +4.4% APS (36.4% vs.
32.0%) with fewer parameters and FLOPs, under 12 epochs training setting. The
source codes will be available at https://github.com/Med-Process/Cross-DINO.

</details>


### [223] [EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance](https://arxiv.org/abs/2505.21876)
*Zun Wang,Jaemin Cho,Jialu Li,Han Lin,Jaehong Yoon,Yue Zhang,Mohit Bansal*

Main category: cs.CV

TL;DR: EPiC框架通过掩蔽源视频生成高质量锚点视频，无需昂贵相机轨迹标注，结合轻量级Anchor-ControlNet模块，实现高效精确的3D相机控制。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因点云估计误差和相机轨迹标注需求导致的资源消耗和精度问题。

Method: 基于首帧可见性掩蔽源视频生成锚点视频，结合Anchor-ControlNet模块指导预训练视频扩散模型。

Result: 在RealEstate10K和MiraData上实现SOTA性能，支持零样本泛化至视频到视频场景。

Conclusion: EPiC以高效、精确的方式解决了3D相机控制问题，具有广泛适用性和强泛化能力。

Abstract: Recent approaches on 3D camera control in video diffusion models (VDMs) often
create anchor videos to guide diffusion models as a structured prior by
rendering from estimated point clouds following annotated camera trajectories.
However, errors inherent in point cloud estimation often lead to inaccurate
anchor videos. Moreover, the requirement for extensive camera trajectory
annotations further increases resource demands. To address these limitations,
we introduce EPiC, an efficient and precise camera control learning framework
that automatically constructs high-quality anchor videos without expensive
camera trajectory annotations. Concretely, we create highly precise anchor
videos for training by masking source videos based on first-frame visibility.
This approach ensures high alignment, eliminates the need for camera trajectory
annotations, and thus can be readily applied to any in-the-wild video to
generate image-to-video (I2V) training pairs. Furthermore, we introduce
Anchor-ControlNet, a lightweight conditioning module that integrates anchor
video guidance in visible regions to pretrained VDMs, with less than 1% of
backbone model parameters. By combining the proposed anchor video data and
ControlNet module, EPiC achieves efficient training with substantially fewer
parameters, training steps, and less data, without requiring modifications to
the diffusion model backbone typically needed to mitigate rendering
misalignments. Although being trained on masking-based anchor videos, our
method generalizes robustly to anchor videos made with point clouds during
inference, enabling precise 3D-informed camera control. EPiC achieves SOTA
performance on RealEstate10K and MiraData for I2V camera control task,
demonstrating precise and robust camera control ability both quantitatively and
qualitatively. Notably, EPiC also exhibits strong zero-shot generalization to
video-to-video scenarios.

</details>


### [224] [Hyperspectral Gaussian Splatting](https://arxiv.org/abs/2505.21890)
*Sunil Kumar Narayanan,Lingjun Zhao,Lu Gan,Yongsheng Chen*

Main category: cs.CV

TL;DR: 论文提出了一种结合3D高斯泼溅和扩散模型的方法（HS-GS），用于高光谱场景的3D显式重建和新视角合成，解决了NeRF在训练时间和渲染速度上的限制。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像（HSI）在农业中用于无损检测植物营养成分，但现有方法如NeRF在训练和渲染效率上存在不足。

Method: 结合3D高斯泼溅（3DGS）和扩散模型，引入波长编码器和KL散度损失，优化光谱重建和去噪。

Result: 在Hyper-NeRF数据集上验证，HS-GS性能优于现有方法。

Conclusion: HS-GS为高光谱场景重建提供了高效且高质量的解决方案。

Abstract: Hyperspectral imaging (HSI) has been widely used in agricultural applications
for non-destructive estimation of plant nutrient composition and precise
determination of nutritional elements in samples. Recently, 3D reconstruction
methods have been used to create implicit neural representations of HSI scenes,
which can help localize the target object's nutrient composition spatially and
spectrally. Neural Radiance Field (NeRF) is a cutting-edge implicit
representation that can render hyperspectral channel compositions of each
spatial location from any viewing direction. However, it faces limitations in
training time and rendering speed. In this paper, we propose Hyperspectral
Gaussian Splatting (HS-GS), which combines the state-of-the-art 3D Gaussian
Splatting (3DGS) with a diffusion model to enable 3D explicit reconstruction of
the hyperspectral scenes and novel view synthesis for the entire spectral
range. To enhance the model's ability to capture fine-grained reflectance
variations across the light spectrum and leverage correlations between adjacent
wavelengths for denoising, we introduce a wavelength encoder to generate
wavelength-specific spherical harmonics offsets. We also introduce a novel
Kullback--Leibler divergence-based loss to mitigate the spectral distribution
gap between the rendered image and the ground truth. A diffusion model is
further applied for denoising the rendered images and generating photorealistic
hyperspectral images. We present extensive evaluations on five diverse
hyperspectral scenes from the Hyper-NeRF dataset to show the effectiveness of
our proposed HS-GS framework. The results demonstrate that HS-GS achieves new
state-of-the-art performance among all previously published methods. Code will
be released upon publication.

</details>


### [225] [Concentrate on Weakness: Mining Hard Prototypes for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2505.21897)
*Jianchao Jiang,Haofeng Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的少样本医学图像分割方法，通过关注弱特征改善边界模糊问题，并引入边界损失提升分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有原型生成方法易导致边界模糊，需关注弱特征以改善分割边界。

Method: 设计了支持自预测模块（SSP）识别弱特征，硬原型生成模块（HPG）生成硬原型，多相似性图融合模块（MSMF）生成最终分割掩码，并引入边界损失。

Result: 在三个公开医学图像数据集上取得最优性能。

Conclusion: 该方法通过关注弱特征和边界优化，显著提升了少样本医学图像分割的效果。

Abstract: Few-Shot Medical Image Segmentation (FSMIS) has been widely used to train a
model that can perform segmentation from only a few annotated images. However,
most existing prototype-based FSMIS methods generate multiple prototypes from
the support image solely by random sampling or local averaging, which can cause
particularly severe boundary blurring due to the tendency for normal features
accounting for the majority of features of a specific category. Consequently,
we propose to focus more attention to those weaker features that are crucial
for clear segmentation boundary. Specifically, we design a Support
Self-Prediction (SSP) module to identify such weak features by comparing true
support mask with one predicted by global support prototype. Then, a Hard
Prototypes Generation (HPG) module is employed to generate multiple hard
prototypes based on these weak features. Subsequently, a Multiple Similarity
Maps Fusion (MSMF) module is devised to generate final segmenting mask in a
dual-path fashion to mitigate the imbalance between foreground and background
in medical images. Furthermore, we introduce a boundary loss to further
constraint the edge of segmentation. Extensive experiments on three publicly
available medical image datasets demonstrate that our method achieves
state-of-the-art performance. Code is available at
https://github.com/jcjiang99/CoW.

</details>


### [226] [CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation](https://arxiv.org/abs/2505.21904)
*Pardis Taghavi,Tian Liu,Renjie Li,Reza Langari,Zhengzhong Tu*

Main category: cs.CV

TL;DR: CAST是一种半监督知识蒸馏框架，通过三阶段方法压缩预训练视觉基础模型，利用有限标注和大量未标注数据提升实例分割性能。


<details>
  <summary>Details</summary>
Motivation: 实例分割需要昂贵的像素级标注和大模型，CAST旨在通过半监督知识蒸馏解决这一问题。

Method: CAST分为三个阶段：域适应（自训练与对比像素校准）、蒸馏（多目标损失结合伪标签与实例感知对比损失）和微调（去除伪标签偏差）。

Result: 在Cityscapes和ADE20K上，CAST的学生模型比教师模型分别提升3.4 AP和1.5 AP，并超越现有半监督方法。

Conclusion: CAST通过实例感知对比损失和多阶段蒸馏，显著提升了实例分割性能，同时减少了标注需求。

Abstract: Instance segmentation demands costly per-pixel annotations and large models.
We introduce CAST, a semi-supervised knowledge distillation (SSKD) framework
that compresses pretrained vision foundation models (VFM) into compact experts
using limited labeled and abundant unlabeled data. CAST unfolds in three
stages: (1) domain adaptation of the VFM teacher(s) via self-training with
contrastive pixel calibration, (2) distillation into a compact student via a
unified multi-objective loss that couples standard supervision and
pseudo-labels with our instance-aware pixel-wise contrastive term, and (3)
fine-tuning on labeled data to remove residual pseudo-label bias. Central to
CAST is an \emph{instance-aware pixel-wise contrastive loss} that fuses mask
and class scores to mine informative negatives and enforce clear inter-instance
margins. By maintaining this contrastive signal across both adaptation and
distillation, we align teacher and student embeddings and fully leverage
unlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses
its adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs.
15.2) and outperforms state-of-the-art semi-supervised approaches.

</details>


### [227] [Reference-Guided Identity Preserving Face Restoration](https://arxiv.org/abs/2505.21905)
*Mo Zhou,Keren Ye,Viraj Shah,Kangfu Mei,Mauricio Delbracio,Peyman Milanfar,Vishal M. Patel,Hossein Talebi*

Main category: cs.CV

TL;DR: 提出了一种新方法，通过最大化参考人脸的效用，提升人脸修复和身份保留效果。


<details>
  <summary>Details</summary>
Motivation: 扩散基图像修复中，保留人脸身份是一个关键挑战，现有基于参考的方法未能充分利用参考人脸的潜力。

Method: 1) 提出复合上下文表示，融合参考人脸的多层次信息；2) 设计硬样本身份损失函数，提升身份学习效率；3) 提出无需训练的推理时多参考输入适配方法。

Result: 在FFHQ-Ref和CelebA-Ref-Test等基准测试中，实现了高质量人脸修复和最优身份保留效果。

Conclusion: 该方法显著提升了人脸修复和身份保留性能，优于现有技术。

Abstract: Preserving face identity is a critical yet persistent challenge in
diffusion-based image restoration. While reference faces offer a path forward,
existing reference-based methods often fail to fully exploit their potential.
This paper introduces a novel approach that maximizes reference face utility
for improved face restoration and identity preservation. Our method makes three
key contributions: 1) Composite Context, a comprehensive representation that
fuses multi-level (high- and low-level) information from the reference face,
offering richer guidance than prior singular representations. 2) Hard Example
Identity Loss, a novel loss function that leverages the reference face to
address the identity learning inefficiencies found in the existing identity
loss. 3) A training-free method to adapt the model to multi-reference inputs
during inference. The proposed method demonstrably restores high-quality faces
and achieves state-of-the-art identity preserving restoration on benchmarks
such as FFHQ-Ref and CelebA-Ref-Test, consistently outperforming previous work.

</details>


### [228] [AlignGen: Boosting Personalized Image Generation with Cross-Modality Prior Alignment](https://arxiv.org/abs/2505.21911)
*Yiheng Lin,Shifang Zhao,Ting Liu,Xiaochao Qu,Luoqi Liu,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: AlignGen提出了一种跨模态先验对齐机制，通过可学习令牌、鲁棒训练策略和选择性跨模态注意力掩码，解决了文本与参考图像不对齐时生成结果偏向文本先验的问题。


<details>
  <summary>Details</summary>
Motivation: 现有零样本方法在文本提示与参考图像不对齐时，生成结果偏向文本先验，导致参考内容丢失。

Method: 引入可学习令牌、鲁棒训练策略和选择性跨模态注意力掩码，对齐文本与视觉先验。

Result: AlignGen在零样本方法中表现优异，甚至超过流行的测试时优化方法。

Conclusion: AlignGen通过跨模态先验对齐机制，显著提升了个性化图像生成的质量。

Abstract: Personalized image generation aims to integrate user-provided concepts into
text-to-image models, enabling the generation of customized content based on a
given prompt. Recent zero-shot approaches, particularly those leveraging
diffusion transformers, incorporate reference image information through
multi-modal attention mechanism. This integration allows the generated output
to be influenced by both the textual prior from the prompt and the visual prior
from the reference image. However, we observe that when the prompt and
reference image are misaligned, the generated results exhibit a stronger bias
toward the textual prior, leading to a significant loss of reference content.
To address this issue, we propose AlignGen, a Cross-Modality Prior Alignment
mechanism that enhances personalized image generation by: 1) introducing a
learnable token to bridge the gap between the textual and visual priors, 2)
incorporating a robust training strategy to ensure proper prior alignment, and
3) employing a selective cross-modal attention mask within the multi-modal
attention mechanism to further align the priors. Experimental results
demonstrate that AlignGen outperforms existing zero-shot methods and even
surpasses popular test-time optimization approaches.

</details>


### [229] [LiDARDustX: A LiDAR Dataset for Dusty Unstructured Road Environments](https://arxiv.org/abs/2505.21914)
*Chenfeng Wei,Qi Wu,Si Zuo,Jiahua Xu,Boyang Zhao,Zeyu Yang,Guotao Xie,Shenhong Wang*

Main category: cs.CV

TL;DR: LiDARDustX数据集专注于高粉尘环境下的感知任务，填补了现有数据集的空白，并提供了3D检测和分割算法的基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶数据集主要针对结构化城市环境，缺乏对高粉尘等非结构化场景的覆盖，限制了相关算法的研究。

Method: 通过六种LiDAR传感器采集30,000帧数据，包含3D边界框标注和点云语义分割，80%以上为粉尘场景。

Result: 建立了高粉尘环境下3D检测和分割算法的性能基准，并分析了粉尘对感知精度的影响及其原因。

Conclusion: LiDARDustX数据集为高粉尘环境下的感知任务提供了重要资源，推动了相关算法的研究。

Abstract: Autonomous driving datasets are essential for validating the progress of
intelligent vehicle algorithms, which include localization, perception, and
prediction. However, existing datasets are predominantly focused on structured
urban environments, which limits the exploration of unstructured and
specialized scenarios, particularly those characterized by significant dust
levels. This paper introduces the LiDARDustX dataset, which is specifically
designed for perception tasks under high-dust conditions, such as those
encountered in mining areas. The LiDARDustX dataset consists of 30,000 LiDAR
frames captured by six different LiDAR sensors, each accompanied by 3D bounding
box annotations and point cloud semantic segmentation. Notably, over 80% of the
dataset comprises dust-affected scenes. By utilizing this dataset, we have
established a benchmark for evaluating the performance of state-of-the-art 3D
detection and segmentation algorithms. Additionally, we have analyzed the
impact of dust on perception accuracy and delved into the causes of these
effects. The data and further information can be accessed at:
https://github.com/vincentweikey/LiDARDustX.

</details>


### [230] [BD Open LULC Map: High-resolution land use land cover mapping & benchmarking for urban development in Dhaka, Bangladesh](https://arxiv.org/abs/2505.21915)
*Mir Sazzat Hossain,Ovi Paul,Md Akil Raihan Iftee,Rakibul Hasan Rajib,Abu Bakar Siddik Nayem,Anis Sarker,Arshad Momen,Md. Ashraful Amin,Amin Ahsan Ali,AKM Mahbubur Rahman*

Main category: cs.CV

TL;DR: BD Open LULC Map (BOLM) 提供高分辨率卫星影像的像素级标注，支持深度学习模型和领域适应任务，填补南亚/东亚LULC数据空白。


<details>
  <summary>Details</summary>
Motivation: 解决南亚/东亚发展中国家因资金不足、基础设施多样和人口密集导致的标注卫星数据稀缺问题。

Method: 使用Bing卫星影像（2.22米/像素）标注11类LULC，并通过GIS专家验证。采用DeepLab V3+进行LULC分割，对比Bing和Sentinel-2A影像性能。

Result: BOLM覆盖4,392平方公里（8.91亿像素），为深度学习模型提供可靠数据支持。

Conclusion: BOLM填补了南亚/东亚LULC数据集空白，有助于提升LULC分类的可靠性。

Abstract: Land Use Land Cover (LULC) mapping using deep learning significantly enhances
the reliability of LULC classification, aiding in understanding geography,
socioeconomic conditions, poverty levels, and urban sprawl. However, the
scarcity of annotated satellite data, especially in South/East Asian developing
countries, poses a major challenge due to limited funding, diverse
infrastructures, and dense populations. In this work, we introduce the BD Open
LULC Map (BOLM), providing pixel-wise LULC annotations across eleven classes
(e.g., Farmland, Water, Forest, Urban Structure, Rural Built-Up) for Dhaka
metropolitan city and its surroundings using high-resolution Bing satellite
imagery (2.22 m/pixel). BOLM spans 4,392 sq km (891 million pixels), with
ground truth validated through a three-stage process involving GIS experts. We
benchmark LULC segmentation using DeepLab V3+ across five major classes and
compare performance on Bing and Sentinel-2A imagery. BOLM aims to support
reliable deep models and domain adaptation tasks, addressing critical LULC
dataset gaps in South/East Asia.

</details>


### [231] [InfoSAM: Fine-Tuning the Segment Anything Model from An Information-Theoretic Perspective](https://arxiv.org/abs/2505.21920)
*Yuanhong Zhang,Muyao Yuan,Weizhan Zhang,Tieliang Gong,Wen Wen,Jiangyong Ying,Weijie Shi*

Main category: cs.CV

TL;DR: InfoSAM是一种基于信息论的方法，通过保留预训练SAM的领域不变关系，提升其在专业领域的微调效果。


<details>
  <summary>Details</summary>
Motivation: SAM在通用任务中表现优异，但在专业领域表现不佳，现有PEFT方法忽略了预训练模型中的领域不变关系。

Method: 提出InfoSAM，通过两个互信息目标（压缩领域不变关系和最大化师生模型间互信息）实现知识蒸馏。

Result: 实验验证InfoSAM能显著提升SAM在专业任务中的性能。

Conclusion: InfoSAM为SAM的PEFT提供了鲁棒的蒸馏框架，适用于专业场景。

Abstract: The Segment Anything Model (SAM), a vision foundation model, exhibits
impressive zero-shot capabilities in general tasks but struggles in specialized
domains. Parameter-efficient fine-tuning (PEFT) is a promising approach to
unleash the potential of SAM in novel scenarios. However, existing PEFT methods
for SAM neglect the domain-invariant relations encoded in the pre-trained
model. To bridge this gap, we propose InfoSAM, an information-theoretic
approach that enhances SAM fine-tuning by distilling and preserving its
pre-trained segmentation knowledge. Specifically, we formulate the knowledge
transfer process as two novel mutual information-based objectives: (i) to
compress the domain-invariant relation extracted from pre-trained SAM,
excluding pseudo-invariant information as possible, and (ii) to maximize mutual
information between the relational knowledge learned by the teacher
(pre-trained SAM) and the student (fine-tuned model). The proposed InfoSAM
establishes a robust distillation framework for PEFT of SAM. Extensive
experiments across diverse benchmarks validate InfoSAM's effectiveness in
improving SAM family's performance on real-world tasks, demonstrating its
adaptability and superiority in handling specialized scenarios.

</details>


### [232] [Point-to-Region Loss for Semi-Supervised Point-Based Crowd Counting](https://arxiv.org/abs/2505.21943)
*Wei Lin,Chenyang Zhao,Antoni B. Chan*

Main category: cs.CV

TL;DR: 论文提出了一种基于伪标记的半监督计数框架，通过点区域（P2R）方案替代点对点（P2P）监督，解决了伪标签置信度传播问题。


<details>
  <summary>Details</summary>
Motivation: 减少密集人群标注的工作量，同时保持高精度的定位和计数性能。

Method: 提出点特定激活图（PSAM）分析训练问题，并设计P2R方案，通过局部区域分割共享伪点置信度。

Result: 在半监督计数和无监督域适应实验中验证了P2R的有效性。

Conclusion: P2R方案解决了PSAM揭示的问题，提升了伪标签训练的稳定性。

Abstract: Point detection has been developed to locate pedestrians in crowded scenes by
training a counter through a point-to-point (P2P) supervision scheme. Despite
its excellent localization and counting performance, training a point-based
counter still faces challenges concerning annotation labor: hundreds to
thousands of points are required to annotate a single sample capturing a dense
crowd. In this paper, we integrate point-based methods into a semi-supervised
counting framework based on pseudo-labeling, enabling the training of a counter
with only a few annotated samples supplemented by a large volume of
pseudo-labeled data. However, during implementation, the training encounters
issues as the confidence for pseudo-labels fails to be propagated to background
pixels via the P2P. To tackle this challenge, we devise a point-specific
activation map (PSAM) to visually interpret the phenomena occurring during the
ill-posed training. Observations from the PSAM suggest that the feature map is
excessively activated by the loss for unlabeled data, causing the decoder to
misinterpret these over-activations as pedestrians. To mitigate this issue, we
propose a point-to-region (P2R) scheme to substitute P2P, which segments out
local regions rather than detects a point corresponding to a pedestrian for
supervision. Consequently, pixels in the local region can share the same
confidence with the corresponding pseudo points. Experimental results in both
semi-supervised counting and unsupervised domain adaptation highlight the
advantages of our method, illustrating P2R can resolve issues identified in
PSAM. The code is available at https://github.com/Elin24/P2RLoss.

</details>


### [233] [UniTalk: Towards Universal Active Speaker Detection in Real World Scenarios](https://arxiv.org/abs/2505.21954)
*Le Thien Phuc Nguyen,Zhuoran Yu,Khoa Quang Nhat Cao,Yuwei Guo,Tu Ho Manh Pham,Tuan Tai Nguyen,Toan Ngo Duc Vo,Lucas Poon,Soochahn Lee,Yong Jae Lee*

Main category: cs.CV

TL;DR: UniTalk是一个专为主动说话人检测设计的新数据集，强调挑战性场景以提升模型泛化能力，优于传统数据集AVA。


<details>
  <summary>Details</summary>
Motivation: 传统数据集（如AVA）主要基于老电影，存在领域差距，而UniTalk专注于多样且困难的真实场景，如多语言、嘈杂背景和拥挤场景。

Method: UniTalk包含44.5小时视频，涵盖48,693个说话人身份，提供帧级标注，覆盖多种真实视频类型。

Result: 现有模型在AVA上表现优异，但在UniTalk上未达饱和，表明真实条件下主动说话人检测任务尚未解决。UniTalk训练的模型在Talkies、ASW和AVA上泛化能力更强。

Conclusion: UniTalk为主动说话人检测设立了新基准，是开发鲁棒模型的重要资源。

Abstract: We present UniTalk, a novel dataset specifically designed for the task of
active speaker detection, emphasizing challenging scenarios to enhance model
generalization. Unlike previously established benchmarks such as AVA, which
predominantly features old movies and thus exhibits significant domain gaps,
UniTalk focuses explicitly on diverse and difficult real-world conditions.
These include underrepresented languages, noisy backgrounds, and crowded scenes
- such as multiple visible speakers speaking concurrently or in overlapping
turns. It contains over 44.5 hours of video with frame-level active speaker
annotations across 48,693 speaking identities, and spans a broad range of video
types that reflect real-world conditions. Through rigorous evaluation, we show
that state-of-the-art models, while achieving nearly perfect scores on AVA,
fail to reach saturation on UniTalk, suggesting that the ASD task remains far
from solved under realistic conditions. Nevertheless, models trained on UniTalk
demonstrate stronger generalization to modern "in-the-wild" datasets like
Talkies and ASW, as well as to AVA. UniTalk thus establishes a new benchmark
for active speaker detection, providing researchers with a valuable resource
for developing and evaluating versatile and resilient models.
  Dataset: https://huggingface.co/datasets/plnguyen2908/UniTalk-ASD
  Code: https://github.com/plnguyen2908/UniTalk-ASD-code

</details>


### [234] [Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs](https://arxiv.org/abs/2505.21955)
*Insu Lee,Wooje Park,Jaeyun Jang,Minyoung Noh,Kyuhong Shim,Byonghyo Shim*

Main category: cs.CV

TL;DR: 论文提出了一种结合第一人称和第三人称视角的框架，以增强大型视觉语言模型（LVLMs）在多视角问答任务中的表现，并提出了E3VQA基准和M3CoT提示技术。


<details>
  <summary>Details</summary>
Motivation: 第一人称视角的局限性（如视野狭窄和缺乏全局上下文）导致LVLMs在空间或上下文复杂的查询中表现不佳，因此需要结合第三人称视角提供补充信息。

Method: 通过E3VQA基准（包含4K高质量问答对）和M3CoT提示技术（整合多视角场景图）来提升LVLMs的多视角推理能力。

Result: M3CoT显著提升了模型性能（GPT-4o提升4.84%，Gemini 2.0 Flash提升5.94%），并揭示了LVLMs在多视角推理中的优势和局限性。

Conclusion: 结合第一人称和第三人称视角输入对提升LVLMs在多视角任务中的表现具有重要价值。

Abstract: Large vision-language models (LVLMs) are increasingly deployed in interactive
applications such as virtual and augmented reality, where first-person
(egocentric) view captured by head-mounted cameras serves as key input. While
this view offers fine-grained cues about user attention and hand-object
interactions, their narrow field of view and lack of global context often lead
to failures on spatially or contextually demanding queries. To address this, we
introduce a framework that augments egocentric inputs with third-person
(exocentric) views, providing complementary information such as global scene
layout and object visibility to LVLMs. We present E3VQA, the first benchmark
for multi-view question answering with 4K high-quality question-answer pairs
grounded in synchronized ego-exo image pairs. Additionally, we propose M3CoT, a
training-free prompting technique that constructs a unified scene
representation by integrating scene graphs from three complementary
perspectives. M3CoT enables LVLMs to reason more effectively across views,
yielding consistent performance gains (4.84% for GPT-4o and 5.94% for Gemini
2.0 Flash) over a recent CoT baseline. Our extensive evaluation reveals key
strengths and limitations of LVLMs in multi-view reasoning and highlights the
value of leveraging both egocentric and exocentric inputs.

</details>


### [235] [Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation](https://arxiv.org/abs/2505.21956)
*Mengdan Zhu,Senhao Cheng,Guangji Bai,Yifei Zhang,Liang Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种名为Cross-modal RAG的新框架，通过将查询和图像分解为子维度组件，实现了子查询感知的检索与生成，显著提升了多模态任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成方法在处理复杂用户查询时表现不佳，因为无法从单一图像中获取所有所需元素。

Method: 提出了一种混合检索策略，结合稀疏和密集检索器，识别帕累托最优图像集，并在生成过程中利用多模态大语言模型选择性地结合相关视觉特征。

Result: 在多个数据集上的实验表明，Cross-modal RAG在检索和生成质量上显著优于现有基线，同时保持高效性。

Conclusion: Cross-modal RAG通过子查询感知的检索与生成，有效解决了复杂查询下的多模态任务挑战。

Abstract: Text-to-image generation increasingly demands access to domain-specific,
fine-grained, and rapidly evolving knowledge that pretrained models cannot
fully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to
address this by retrieving globally relevant images, but they fail when no
single image contains all desired elements from a complex user query. We
propose Cross-modal RAG, a novel framework that decomposes both queries and
images into sub-dimensional components, enabling subquery-aware retrieval and
generation. Our method introduces a hybrid retrieval strategy - combining a
sub-dimensional sparse retriever with a dense retriever - to identify a
Pareto-optimal set of images, each contributing complementary aspects of the
query. During generation, a multimodal large language model is guided to
selectively condition on relevant visual features aligned to specific
subqueries, ensuring subquery-aware image synthesis. Extensive experiments on
MS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal
RAG significantly outperforms existing baselines in both retrieval and
generation quality, while maintaining high efficiency.

</details>


### [236] [One-Way Ticket:Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion Models](https://arxiv.org/abs/2505.21960)
*Senmao Li,Lei Wang,Kai Wang,Tao Liu,Jiehang Xie,Joost van de Weijer,Fahad Shahbaz Khan,Shiqi Yang,Yaxing Wang,Jian Yang*

Main category: cs.CV

TL;DR: 论文提出了一种名为TiUE的时间无关统一编码器，用于提升文本到图像扩散模型的推理速度和图像质量，通过共享编码器特征和并行采样显著减少计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有蒸馏文本到图像模型在减少采样步骤时面临多样性和质量下降的问题，尤其是单步模型。研究发现UNet编码器存在冗余计算，而解码器能更好地捕捉语义信息。

Method: 提出TiUE架构，共享编码器特征以实现并行采样，并引入KL散度项正则化噪声预测，提升生成图像的感知真实性和多样性。

Result: 实验表明TiUE在多样性和真实性上优于LCM、SD-Turbo和SwiftBrushv2等先进方法，同时保持计算效率。

Conclusion: TiUE通过共享编码器和并行采样，显著提升了文本到图像扩散模型的效率和生成质量。

Abstract: Text-to-Image (T2I) diffusion models have made remarkable advancements in
generative modeling; however, they face a trade-off between inference speed and
image quality, posing challenges for efficient deployment. Existing distilled
T2I models can generate high-fidelity images with fewer sampling steps, but
often struggle with diversity and quality, especially in one-step models. From
our analysis, we observe redundant computations in the UNet encoders. Our
findings suggest that, for T2I diffusion models, decoders are more adept at
capturing richer and more explicit semantic information, while encoders can be
effectively shared across decoders from diverse time steps. Based on these
observations, we introduce the first Time-independent Unified Encoder TiUE for
the student model UNet architecture, which is a loop-free image generation
approach for distilling T2I diffusion models. Using a one-pass scheme, TiUE
shares encoder features across multiple decoder time steps, enabling parallel
sampling and significantly reducing inference time complexity. In addition, we
incorporate a KL divergence term to regularize noise prediction, which enhances
the perceptual realism and diversity of the generated images. Experimental
results demonstrate that TiUE outperforms state-of-the-art methods, including
LCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results
while maintaining the computational efficiency.

</details>


### [237] [A2Seek: Towards Reasoning-Centric Benchmark for Aerial Anomaly Understanding](https://arxiv.org/abs/2505.21962)
*Mengjingcheng Mo,Xinyang Tong,Jiaxu Leng,Mingpi Tan,Jiankang Zheng,Yiran Liu,Haosheng Chen,Ji Gan,Weisheng Li,Xinbo Gao*

Main category: cs.CV

TL;DR: 论文提出A2Seek数据集和A2Seek-R1框架，用于无人机视角下的异常检测，通过推理机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和方法难以适应无人机视角的动态变化和复杂场景，导致性能下降。

Method: 提出A2Seek数据集，并设计A2Seek-R1框架，结合图推理（GoT）和奖励优化（A-GRPO），模拟无人机飞行行为。

Result: A2Seek-R1在预测准确率（AP）和异常定位（mIoU）上分别提升22.04%和13.9%。

Conclusion: A2Seek-R1在复杂环境中表现优异，数据集和代码将开源。

Abstract: While unmanned aerial vehicles (UAVs) offer wide-area, high-altitude coverage
for anomaly detection, they face challenges such as dynamic viewpoints, scale
variations, and complex scenes. Existing datasets and methods, mainly designed
for fixed ground-level views, struggle to adapt to these conditions, leading to
significant performance drops in drone-view scenarios. To bridge this gap, we
introduce A2Seek (Aerial Anomaly Seek), a large-scale, reasoning-centric
benchmark dataset for aerial anomaly understanding. This dataset covers various
scenarios and environmental conditions, providing high-resolution real-world
aerial videos with detailed annotations, including anomaly categories,
frame-level timestamps, region-level bounding boxes, and natural language
explanations for causal reasoning. Building on this dataset, we propose
A2Seek-R1, a novel reasoning framework that generalizes R1-style strategies to
aerial anomaly understanding, enabling a deeper understanding of "Where"
anomalies occur and "Why" they happen in aerial frames. To this end, A2Seek-R1
first employs a graph-of-thought (GoT)-guided supervised fine-tuning approach
to activate the model's latent reasoning capabilities on A2Seek. Then, we
introduce Aerial Group Relative Policy Optimization (A-GRPO) to design
rule-based reward functions tailored to aerial scenarios. Furthermore, we
propose a novel "seeking" mechanism that simulates UAV flight behavior by
directing the model's attention to informative regions. Extensive experiments
demonstrate that A2Seek-R1 achieves up to a 22.04% improvement in AP for
prediction accuracy and a 13.9% gain in mIoU for anomaly localization,
exhibiting strong generalization across complex environments and
out-of-distribution scenarios. Our dataset and code will be released at
https://hayneyday.github.io/A2Seek/.

</details>


### [238] [DvD: Unleashing a Generative Paradigm for Document Dewarping via Coordinates-based Diffusion Model](https://arxiv.org/abs/2505.21975)
*Weiguang Zhang,Huangcheng Lu,Maizhen Ning,Xiaowei Huang,Wei Wang,Kaizhu Huang,Qiufeng Wang*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散模型的文档去扭曲方法DvD，通过坐标级去噪和时间变体条件细化机制，显著提升了文档结构的保留能力，并提出了新的大规模基准AnyPhotoDoc6300用于全面评估。


<details>
  <summary>Details</summary>
Motivation: 现有文档去扭曲方法在保留文档结构方面仍具挑战性，而扩散模型在图像生成领域的进展为其应用提供了潜在可能，但直接应用于高分辨率复杂文档图像存在困难。

Method: DvD采用坐标级去噪而非像素级去噪，生成变形校正映射，并引入时间变体条件细化机制以增强文档结构保留。

Result: DvD在多个基准测试（包括新提出的AnyPhotoDoc6300）上实现了最先进的性能，计算效率可接受。

Conclusion: DvD为文档去扭曲提供了首个基于扩散模型的解决方案，新基准AnyPhotoDoc6300为未来研究提供了更全面的评估工具。

Abstract: Document dewarping aims to rectify deformations in photographic document
images, thus improving text readability, which has attracted much attention and
made great progress, but it is still challenging to preserve document
structures. Given recent advances in diffusion models, it is natural for us to
consider their potential applicability to document dewarping. However, it is
far from straightforward to adopt diffusion models in document dewarping due to
their unfaithful control on highly complex document images (e.g.,
2000$\times$3000 resolution). In this paper, we propose DvD, the first
generative model to tackle document \textbf{D}ewarping \textbf{v}ia a
\textbf{D}iffusion framework. To be specific, DvD introduces a coordinate-level
denoising instead of typical pixel-level denoising, generating a mapping for
deformation rectification. In addition, we further propose a time-variant
condition refinement mechanism to enhance the preservation of document
structures. In experiments, we find that current document dewarping benchmarks
can not evaluate dewarping models comprehensively. To this end, we present
AnyPhotoDoc6300, a rigorously designed large-scale document dewarping benchmark
comprising 6,300 real image pairs across three distinct domains, enabling
fine-grained evaluation of dewarping models. Comprehensive experiments
demonstrate that our proposed DvD can achieve state-of-the-art performance with
acceptable computational efficiency on multiple metrics across various
benchmarks including DocUNet, DIR300, and AnyPhotoDoc6300. The new benchmark
and code will be publicly available.

</details>


### [239] [Learning World Models for Interactive Video Generation](https://arxiv.org/abs/2505.21996)
*Taiye Chen,Xun Hu,Zihan Ding,Chi Jin*

Main category: cs.CV

TL;DR: 论文提出了一种视频检索增强生成（VRAG）方法，通过显式全局状态条件化，显著减少了长期复合错误并提高了世界模型的时空一致性。


<details>
  <summary>Details</summary>
Motivation: 现有长视频生成模型由于复合错误和内存机制不足，缺乏有效的世界建模能力，影响了未来规划和动作选择的效果。

Method: 通过动作条件和自回归框架增强图像到视频模型，并引入VRAG方法，利用显式全局状态条件化。

Result: VRAG显著减少了复合错误并提高了时空一致性，而单纯的自回归生成和检索增强生成效果较差。

Conclusion: 研究揭示了视频世界模型的基本挑战，并提出了改进视频生成模型内部世界建模能力的综合基准。

Abstract: Foundational world models must be both interactive and preserve
spatiotemporal coherence for effective future planning with action choices.
However, present models for long video generation have limited inherent world
modeling capabilities due to two main challenges: compounding errors and
insufficient memory mechanisms. We enhance image-to-video models with
interactive capabilities through additional action conditioning and
autoregressive framework, and reveal that compounding error is inherently
irreducible in autoregressive video generation, while insufficient memory
mechanism leads to incoherence of world models. We propose video retrieval
augmented generation (VRAG) with explicit global state conditioning, which
significantly reduces long-term compounding errors and increases spatiotemporal
consistency of world models. In contrast, naive autoregressive generation with
extended context windows and retrieval-augmented generation prove less
effective for video generation, primarily due to the limited in-context
learning capabilities of current video models. Our work illuminates the
fundamental challenges in video world models and establishes a comprehensive
benchmark for improving video generation models with internal world modeling
capabilities.

</details>


### [240] [D-Fusion: Direct Preference Optimization for Aligning Diffusion Models with Visually Consistent Samples](https://arxiv.org/abs/2505.22002)
*Zijing Hu,Fengda Zhang,Kun Kuang*

Main category: cs.CV

TL;DR: 论文提出D-Fusion方法，通过视觉一致性样本优化扩散模型的文本-图像对齐问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成图像与文本提示对齐方面存在不足，现有方法（如DPO）因视觉不一致性效果受限。

Method: D-Fusion通过掩码引导的自注意力融合生成视觉一致样本，并保留去噪轨迹以支持DPO训练。

Result: 实验表明D-Fusion能显著提升不同强化学习算法中的提示-图像对齐效果。

Conclusion: D-Fusion解决了视觉不一致性问题，有效优化了扩散模型的文本-图像对齐性能。

Abstract: The practical applications of diffusion models have been limited by the
misalignment between generated images and corresponding text prompts. Recent
studies have introduced direct preference optimization (DPO) to enhance the
alignment of these models. However, the effectiveness of DPO is constrained by
the issue of visual inconsistency, where the significant visual disparity
between well-aligned and poorly-aligned images prevents diffusion models from
identifying which factors contribute positively to alignment during
fine-tuning. To address this issue, this paper introduces D-Fusion, a method to
construct DPO-trainable visually consistent samples. On one hand, by performing
mask-guided self-attention fusion, the resulting images are not only
well-aligned, but also visually consistent with given poorly-aligned images. On
the other hand, D-Fusion can retain the denoising trajectories of the resulting
images, which are essential for DPO training. Extensive experiments demonstrate
the effectiveness of D-Fusion in improving prompt-image alignment when applied
to different reinforcement learning algorithms.

</details>


### [241] [Event-based Egocentric Human Pose Estimation in Dynamic Environment](https://arxiv.org/abs/2505.22007)
*Wataru Ikeda,Masashi Hatano,Ryosei Hara,Mariko Isogawa*

Main category: cs.CV

TL;DR: 提出了一种基于事件相机的头部佩戴式前视相机的人体姿态估计方法D-EventEgo，通过运动分割模块提升动态环境中的姿态估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖RGB相机，无法应对低光环境和运动模糊问题，事件相机有望解决这些挑战。

Method: 首先估计头部姿态，以此为条件生成身体姿态；引入运动分割模块去除动态物体干扰。

Result: 在合成事件数据集上的实验表明，该方法在动态环境中五项指标中的四项优于基线。

Conclusion: D-EventEgo在动态环境下表现优异，为事件相机在人体姿态估计中的应用提供了新思路。

Abstract: Estimating human pose using a front-facing egocentric camera is essential for
applications such as sports motion analysis, VR/AR, and AI for wearable
devices. However, many existing methods rely on RGB cameras and do not account
for low-light environments or motion blur. Event-based cameras have the
potential to address these challenges. In this work, we introduce a novel task
of human pose estimation using a front-facing event-based camera mounted on the
head and propose D-EventEgo, the first framework for this task. The proposed
method first estimates the head poses, and then these are used as conditions to
generate body poses. However, when estimating head poses, the presence of
dynamic objects mixed with background events may reduce head pose estimation
accuracy. Therefore, we introduce the Motion Segmentation Module to remove
dynamic objects and extract background information. Extensive experiments on
our synthetic event-based dataset derived from EgoBody, demonstrate that our
approach outperforms our baseline in four out of five evaluation metrics in
dynamic environments.

</details>


### [242] [Prototype Embedding Optimization for Human-Object Interaction Detection in Livestreaming](https://arxiv.org/abs/2505.22011)
*Menghui Zhang,Jing Zhang,Lin Chen,Li Zhuo*

Main category: cs.CV

TL;DR: 论文提出了一种原型嵌入优化方法（PeO-HOI），用于解决直播中人-物交互（HOI）检测中的对象偏差问题，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 直播中的人-物交互检测存在对象偏差问题，即过于关注物体而忽视其与主播的交互。

Method: 通过预处理提取人-物对特征，采用原型嵌入优化减少对象偏差，建模时空上下文后通过预测头获取HOI检测结果。

Result: 在VidHOI和BJUT-HOI数据集上，PeO-HOI的检测准确率显著提升，分别为37.19%和45.13%。

Conclusion: PeO-HOI有效解决了对象偏差问题，提升了直播中HOI检测的性能。

Abstract: Livestreaming often involves interactions between streamers and objects,
which is critical for understanding and regulating web content. While
human-object interaction (HOI) detection has made some progress in
general-purpose video downstream tasks, when applied to recognize the
interaction behaviors between a streamer and different objects in
livestreaming, it tends to focuses too much on the objects and neglects their
interactions with the streamer, which leads to object bias. To solve this
issue, we propose a prototype embedding optimization for human-object
interaction detection (PeO-HOI). First, the livestreaming is preprocessed using
object detection and tracking techniques to extract features of the
human-object (HO) pairs. Then, prototype embedding optimization is adopted to
mitigate the effect of object bias on HOI. Finally, after modelling the
spatio-temporal context between HO pairs, the HOI detection results are
obtained by the prediction head. The experimental results show that the
detection accuracy of the proposed PeO-HOI method has detection accuracies of
37.19%@full, 51.42%@non-rare, 26.20%@rare on the publicly available dataset
VidHOI, 45.13%@full, 62.78%@non-rare and 30.37%@rare on the self-built dataset
BJUT-HOI, which effectively improves the HOI detection performance in
livestreaming.

</details>


### [243] [PanoWan: Lifting Diffusion Video Generation Models to 360° with Latitude/Longitude-aware Mechanisms](https://arxiv.org/abs/2505.22016)
*Yifei Xia,Shuchen Weng,Siqi Yang,Jingqi Liu,Chengxuan Zhu,Minggui Teng,Zijian Jia,Han Jiang,Boxin Shi*

Main category: cs.CV

TL;DR: PanoWan利用预训练的文本到视频模型生成高质量全景视频，通过纬度感知采样和旋转语义去噪等技术解决现有模型的局限性，并贡献了一个高质量全景视频数据集PanoVid。


<details>
  <summary>Details</summary>
Motivation: 现有全景视频生成模型难以利用预训练生成先验，因数据集规模有限和空间特征表示差异。

Method: PanoWan采用纬度感知采样避免纬度失真，旋转语义去噪和平铺像素解码确保无缝过渡，并引入PanoVid数据集。

Result: PanoWan在全景视频生成中达到最先进性能，并在零样本下游任务中表现稳健。

Conclusion: PanoWan通过高效模块和高质量数据集，显著提升了全景视频生成的质量和多样性。

Abstract: Panoramic video generation enables immersive 360{\deg} content creation,
valuable in applications that demand scene-consistent world exploration.
However, existing panoramic video generation models struggle to leverage
pre-trained generative priors from conventional text-to-video models for
high-quality and diverse panoramic videos generation, due to limited dataset
scale and the gap in spatial feature representations. In this paper, we
introduce PanoWan to effectively lift pre-trained text-to-video models to the
panoramic domain, equipped with minimal modules. PanoWan employs latitude-aware
sampling to avoid latitudinal distortion, while its rotated semantic denoising
and padded pixel-wise decoding ensure seamless transitions at longitude
boundaries. To provide sufficient panoramic videos for learning these lifted
representations, we contribute PanoVid, a high-quality panoramic video dataset
with captions and diverse scenarios. Consequently, PanoWan achieves
state-of-the-art performance in panoramic video generation and demonstrates
robustness for zero-shot downstream tasks.

</details>


### [244] [GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement](https://arxiv.org/abs/2505.22021)
*Zhihong Tang,Yang Li*

Main category: cs.CV

TL;DR: GL-PGENet是一种用于多退化彩色文档图像增强的新架构，通过全局与局部参数化生成增强网络，结合分层增强框架和双分支局部细化网络，显著提升了文档图像增强的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于单一退化恢复或灰度图像处理，无法满足多退化彩色文档图像的需求，因此提出了GL-PGENet以解决这一问题。

Method: 采用分层增强框架（全局外观校正与局部细化）、双分支局部细化网络（参数化生成机制）和改进的NestUNet架构（密集块融合特征）。通过两阶段训练策略（大规模预训练和任务特定微调）提升泛化性能。

Result: 在DocUNet和RealDAE数据集上分别达到0.7721和0.9480的SSIM分数，表现出卓越的跨域适应性和计算效率。

Conclusion: GL-PGENet在多退化彩色文档图像增强任务中表现优异，具有实际应用价值。

Abstract: Document Image Enhancement (DIE) serves as a critical component in Document
AI systems, where its performance substantially determines the effectiveness of
downstream tasks. To address the limitations of existing methods confined to
single-degradation restoration or grayscale image processing, we present Global
with Local Parametric Generation Enhancement Network (GL-PGENet), a novel
architecture designed for multi-degraded color document images, ensuring both
efficiency and robustness in real-world scenarios. Our solution incorporates
three key innovations: First, a hierarchical enhancement framework that
integrates global appearance correction with local refinement, enabling
coarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network
with parametric generation mechanisms that replaces conventional direct
prediction, producing enhanced outputs through learned intermediate parametric
representations rather than pixel-wise mapping. This approach enhances local
consistency while improving model generalization. Finally, a modified NestUNet
architecture incorporating dense block to effectively fuse low-level pixel
features and high-level semantic features, specifically adapted for document
image characteristics. In addition, to enhance generalization performance, we
adopt a two-stage training strategy: large-scale pretraining on a synthetic
dataset of 500,000+ samples followed by task-specific fine-tuning. Extensive
experiments demonstrate the superiority of GL-PGENet, achieving
state-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The
model also exhibits remarkable cross-domain adaptability and maintains
computational efficiency for high-resolution images without performance
degradation, confirming its practical utility in real-world scenarios.

</details>


### [245] [Guess the Age of Photos: An Interactive Web Platform for Historical Image Age Estimation](https://arxiv.org/abs/2505.22031)
*Hasan Yucedag,Adam Jatowt*

Main category: cs.CV

TL;DR: 论文介绍了一个名为“Guess the Age of Photos”的网页平台，通过两种游戏化模式（猜年份和时间线挑战）让用户估计历史照片的年代。平台基于Python等技术构建，使用10,150张图片数据集，用户测试显示相对比较的准确性更高。


<details>
  <summary>Details</summary>
Motivation: 通过游戏化方式提升用户对历史照片年代的认知，同时为研究人类对图像时间线索的感知提供资源。

Method: 平台采用Python、Flask等技术，提供两种游戏模式，使用10,150张图片数据集，并通过动态评分和排行榜提升用户参与度。

Result: 用户测试显示，相对比较的准确性（65.9%）高于绝对年份猜测（25.6%），平台满意度评分为4.25/5。

Conclusion: 该平台不仅作为教育工具提升历史意识，还可用于研究人类感知和训练计算机视觉模型。

Abstract: This paper introduces Guess the Age of Photos, a web platform engaging users
in estimating the years of historical photographs through two gamified modes:
Guess the Year (predicting a single image's year) and Timeline Challenge
(comparing two images to identify the older). Built with Python, Flask,
Bootstrap, and PostgreSQL, it uses a 10,150-image subset of the Date Estimation
in the Wild dataset (1930-1999). Features like dynamic scoring and leaderboards
boost engagement. Evaluated with 113 users and 15,473 gameplays, the platform
earned a 4.25/5 satisfaction rating. Users excelled in relative comparisons
(65.9% accuracy) over absolute year guesses (25.6% accuracy), with older
decades easier to identify. The platform serves as an educational tool,
fostering historical awareness and analytical skills via interactive
exploration of visual heritage. Furthermore, the platform provides a valuable
resource for studying human perception of temporal cues in images and could be
used to generate annotated data for training and evaluating computer vision
models.

</details>


### [246] [Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization](https://arxiv.org/abs/2505.22038)
*Kaiyuan Li,Xiaoyue Chen,Chen Gao,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: 论文提出了一种平衡令牌修剪（BTP）方法，通过分阶段修剪图像令牌以减少计算开销，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLM）因编码大量图像令牌导致计算负担重，现有修剪方法未充分考虑局部与全局影响。

Method: BTP方法利用校准集分阶段修剪，早期关注后续层影响，后期注重局部输出一致性。

Result: 实验表明，BTP能在78%压缩率下平均保留96.7%的原始模型性能。

Conclusion: BTP是一种高效且通用的令牌修剪方法，适用于多种LVLM。

Abstract: Large Vision-Language Models (LVLMs) have shown impressive performance across
multi-modal tasks by encoding images into thousands of tokens. However, the
large number of image tokens results in significant computational overhead, and
the use of dynamic high-resolution inputs further increases this burden.
Previous approaches have attempted to reduce the number of image tokens through
token pruning, typically by selecting tokens based on attention scores or image
token diversity. Through empirical studies, we observe that existing methods
often overlook the joint impact of pruning on both the current layer's output
(local) and the outputs of subsequent layers (global), leading to suboptimal
pruning decisions. To address this challenge, we propose Balanced Token Pruning
(BTP), a plug-and-play method for pruning vision tokens. Specifically, our
method utilizes a small calibration set to divide the pruning process into
multiple stages. In the early stages, our method emphasizes the impact of
pruning on subsequent layers, whereas in the deeper stages, the focus shifts
toward preserving the consistency of local outputs. Extensive experiments
across various LVLMs demonstrate the broad effectiveness of our approach on
multiple benchmarks. Our method achieves a 78% compression rate while
preserving 96.7% of the original models' performance on average.

</details>


### [247] [OmniAD: Detect and Understand Industrial Anomaly via Multimodal Reasoning](https://arxiv.org/abs/2505.22039)
*Shifang Zhao,Yiheng Lin,Lu Han,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: OmniAD是一个结合视觉与文本推理的多模态框架，用于细粒度异常检测与分析，无需手动阈值选择，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 工业知识结合的详细异常分析仍具挑战性，OmniAD旨在填补这一空白。

Method: OmniAD通过Text-as-Mask Encoding进行视觉推理，结合视觉引导的文本推理，并采用SFT与GRPO结合的集成训练策略。

Result: 在MMAD基准测试中达到79.1分，优于Qwen2.5-VL-7B和GPT-4o，并在多个异常检测基准中表现优异。

Conclusion: 增强视觉感知对异常理解推理至关重要，OmniAD的代码和模型将开源。

Abstract: While anomaly detection has made significant progress, generating detailed
analyses that incorporate industrial knowledge remains a challenge. To address
this gap, we introduce OmniAD, a novel framework that unifies anomaly detection
and understanding for fine-grained analysis. OmniAD is a multimodal reasoner
that combines visual and textual reasoning processes. The visual reasoning
provides detailed inspection by leveraging Text-as-Mask Encoding to perform
anomaly detection through text generation without manually selected thresholds.
Following this, Visual Guided Textual Reasoning conducts comprehensive analysis
by integrating visual perception. To enhance few-shot generalization, we employ
an integrated training strategy that combines supervised fine-tuning (SFT) with
reinforcement learning (GRPO), incorporating three sophisticated reward
functions. Experimental results demonstrate that OmniAD achieves a performance
of 79.1 on the MMAD benchmark, surpassing models such as Qwen2.5-VL-7B and
GPT-4o. It also shows strong results across multiple anomaly detection
benchmarks. These results highlight the importance of enhancing visual
perception for effective reasoning in anomaly understanding. All codes and
models will be publicly available.

</details>


### [248] [LatentMove: Towards Complex Human Movement Video Generation](https://arxiv.org/abs/2505.22046)
*Ashkan Taghipour,Morteza Ghahremani,Mohammed Bennamoun,Farid Boussaid,Aref Miri Rekavandi,Zinuo Li,Qiuhong Ke,Hamid Laga*

Main category: cs.CV

TL;DR: LatentMove是一种基于DiT的框架，专注于生成高度动态的人体动画，通过条件控制分支和可学习的面部/身体标记提升视频质量，并引入CHV数据集和评估指标。


<details>
  <summary>Details</summary>
Motivation: 解决现有图像到视频生成方法在处理复杂、非重复性人体运动时的不自然变形问题。

Method: 采用DiT框架，结合条件控制分支和可学习的面部/身体标记，确保帧间一致性和细节保留。

Result: 实验表明，LatentMove显著提升了人体动画质量，尤其在处理快速复杂运动时表现突出。

Conclusion: LatentMove推动了图像到视频生成技术的边界，代码、数据集和评估指标将开源。

Abstract: Image-to-video (I2V) generation seeks to produce realistic motion sequences
from a single reference image. Although recent methods exhibit strong temporal
consistency, they often struggle when dealing with complex, non-repetitive
human movements, leading to unnatural deformations. To tackle this issue, we
present LatentMove, a DiT-based framework specifically tailored for highly
dynamic human animation. Our architecture incorporates a conditional control
branch and learnable face/body tokens to preserve consistency as well as
fine-grained details across frames. We introduce Complex-Human-Videos (CHV), a
dataset featuring diverse, challenging human motions designed to benchmark the
robustness of I2V systems. We also introduce two metrics to assess the flow and
silhouette consistency of generated videos with their ground truth.
Experimental results indicate that LatentMove substantially improves human
animation quality--particularly when handling rapid, intricate
movements--thereby pushing the boundaries of I2V generation. The code, the CHV
dataset, and the evaluation metrics will be available at https://github.com/
--.

</details>


### [249] [AquaMonitor: A multimodal multi-view image sequence dataset for real-life aquatic invertebrate biodiversity monitoring](https://arxiv.org/abs/2505.22065)
*Mikko Impiö,Philipp M. Rehsen,Tiina Laamanen,Arne J. Beermann,Florian Leese,Jenni Raitoharju*

Main category: cs.CV

TL;DR: AquaMonitor是首个大型水生无脊椎动物计算机视觉数据集，用于环境监测，包含2.7M图像和多模态数据，定义了三个基准任务并提供了基线。


<details>
  <summary>Details</summary>
Motivation: 现有物种识别数据集缺乏标准化采集协议，且未聚焦水生无脊椎动物，AquaMonitor填补了这一空白，支持自动化识别方法评估。

Method: 通过两年监测期间尽可能成像所有标本，收集图像、DNA序列、干质量和尺寸数据，构建多模态数据集。

Result: 数据集包含2.7M图像、43,189标本、1358个DNA序列和1494个测量数据，定义了监测、分类和少样本三个基准任务。

Conclusion: AquaMonitor为水生生物多样性监测提供了重要工具，可直接改进水质评估的立法实践。

Abstract: This paper presents the AquaMonitor dataset, the first large computer vision
dataset of aquatic invertebrates collected during routine environmental
monitoring. While several large species identification datasets exist, they are
rarely collected using standardized collection protocols, and none focus on
aquatic invertebrates, which are particularly laborious to collect. For
AquaMonitor, we imaged all specimens from two years of monitoring whenever
imaging was possible given practical limitations. The dataset enables the
evaluation of automated identification methods for real-life monitoring
purposes using a realistically challenging and unbiased setup. The dataset has
2.7M images from 43,189 specimens, DNA sequences for 1358 specimens, and dry
mass and size measurements for 1494 specimens, making it also one of the
largest biological multi-view and multimodal datasets to date. We define three
benchmark tasks and provide strong baselines for these: 1) Monitoring
benchmark, reflecting real-life deployment challenges such as open-set
recognition, distribution shift, and extreme class imbalance, 2) Classification
benchmark, which follows a standard fine-grained visual categorization setup,
and 3) Few-shot benchmark, which targets classes with only few training
examples from very fine-grained categories. Advancements on the Monitoring
benchmark can directly translate to improvement of aquatic biodiversity
monitoring, which is an important component of regular legislative water
quality assessment in many countries.

</details>


### [250] [Bringing CLIP to the Clinic: Dynamic Soft Labels and Negation-Aware Learning for Medical Analysis](https://arxiv.org/abs/2505.22079)
*Hanbin Ko,Chang-Min Park*

Main category: cs.CV

TL;DR: 提出了一种结合临床增强动态软标签和医学图形对齐的新方法，改进了医学对比学习的适用性，并通过否定硬负样本提升临床语言理解。


<details>
  <summary>Details</summary>
Motivation: 通用领域架构（如CLIP）在医学数据上应用存在挑战，如处理否定和医学数据不平衡问题。

Method: 整合临床增强动态软标签和医学图形对齐，引入否定硬负样本。

Result: 在零样本、微调分类和报告检索等任务中达到最优性能，并通过CXR-Align基准验证。

Conclusion: 方法易于实现，能有效提升医学视觉语言处理能力和临床语言理解。

Abstract: The development of large-scale image-text pair datasets has significantly
advanced self-supervised learning in Vision-Language Processing (VLP). However,
directly applying general-domain architectures such as CLIP to medical data
presents challenges, particularly in handling negations and addressing the
inherent data imbalance of medical datasets. To address these issues, we
propose a novel approach that integrates clinically-enhanced dynamic soft
labels and medical graphical alignment, thereby improving clinical
comprehension and the applicability of contrastive loss in medical contexts.
Furthermore, we introduce negation-based hard negatives to deepen the model's
understanding of the complexities of clinical language. Our approach is easily
integrated into the medical CLIP training pipeline and achieves
state-of-the-art performance across multiple tasks, including zero-shot,
fine-tuned classification, and report retrieval. To comprehensively evaluate
our model's capacity for understanding clinical language, we introduce
CXR-Align, a benchmark uniquely designed to evaluate the understanding of
negation and clinical information within chest X-ray (CXR) datasets.
Experimental results demonstrate that our proposed methods are straightforward
to implement and generalize effectively across contrastive learning frameworks,
enhancing medical VLP capabilities and advancing clinical language
understanding in medical imaging.

</details>


### [251] [MObyGaze: a film dataset of multimodal objectification densely annotated by experts](https://arxiv.org/abs/2505.22084)
*Julie Tores,Elisa Ancarani,Lucile Sassatelli,Hui-Yin Wu,Clement Bergman,Lea Andolfi,Victor Ecrement,Remy Sun,Frederic Precioso,Thierry Devars,Magali Guaresi,Virginie Julliard,Sarah Lecossais*

Main category: cs.CV

TL;DR: 论文提出了一种新的AI任务，通过多模态（视觉、语音、音频）时间模式来量化和表征电影中的物化现象，并发布了MObyGaze数据集。


<details>
  <summary>Details</summary>
Motivation: 理解性别表征差异及其在视听内容中的物化现象，以揭示刻板印象的传播机制。

Method: 结合电影研究和心理学，定义了物化的结构化分类体系，并构建了包含6072个片段的多模态数据集。

Result: 展示了任务的可行性，并比较了多种视觉、文本和音频模型的性能。

Conclusion: 通过MObyGaze数据集和任务定义，为研究物化现象提供了新工具和基准。

Abstract: Characterizing and quantifying gender representation disparities in
audiovisual storytelling contents is necessary to grasp how stereotypes may
perpetuate on screen. In this article, we consider the high-level construct of
objectification and introduce a new AI task to the ML community: characterize
and quantify complex multimodal (visual, speech, audio) temporal patterns
producing objectification in films. Building on film studies and psychology, we
define the construct of objectification in a structured thesaurus involving 5
sub-constructs manifesting through 11 concepts spanning 3 modalities. We
introduce the Multimodal Objectifying Gaze (MObyGaze) dataset, made of 20
movies annotated densely by experts for objectification levels and concepts
over freely delimited segments: it amounts to 6072 segments over 43 hours of
video with fine-grained localization and categorization. We formulate different
learning tasks, propose and investigate best ways to learn from the diversity
of labels among a low number of annotators, and benchmark recent vision, text
and audio models, showing the feasibility of the task. We make our code and our
dataset available to the community and described in the Croissant format:
https://anonymous.4open.science/r/MObyGaze-F600/.

</details>


### [252] [Fast Feature Matching of UAV Images via Matrix Band Reduction-based GPU Data Schedule](https://arxiv.org/abs/2505.22089)
*San Jiang,Kan You,Wanshou Jiang,Qingquan Li*

Main category: cs.CV

TL;DR: 提出了一种基于GPU数据调度算法的高效无人机图像特征匹配方法，通过矩阵带约简和GPU加速级联哈希提升效率。


<details>
  <summary>Details</summary>
Motivation: 特征匹配在SfM中耗时严重，需提升无人机图像匹配效率。

Method: 1. 使用图像检索技术选择匹配对；2. 基于MBR生成紧凑图像块；3. GPU加速级联哈希执行特征匹配，结合几何约束和RANSAC验证。

Result: 速度提升77.0至100.0倍，精度与传统方法相当。

Conclusion: 该算法为无人机图像特征匹配提供高效解决方案。

Abstract: Feature matching dominats the time costs in structure from motion (SfM). The
primary contribution of this study is a GPU data schedule algorithm for
efficient feature matching of Unmanned aerial vehicle (UAV) images. The core
idea is to divide the whole dataset into blocks based on the matrix band
reduction (MBR) and achieve efficient feature matching via GPU-accelerated
cascade hashing. First, match pairs are selected by using an image retrieval
technique, which converts images into global descriptors and searches
high-dimension nearest neighbors with graph indexing. Second, compact image
blocks are iteratively generated from a MBR-based data schedule strategy, which
exploits image connections to avoid redundant data IO (input/output) burden and
increases the usage of GPU computing power. Third, guided by the generated
image blocks, feature matching is executed sequentially within the framework of
GPU-accelerated cascade hashing, and initial candidate matches are refined by
combining a local geometric constraint and RANSAC-based global verification.
For further performance improvement, these two seps are designed to execute
parallelly in GPU and CPU. Finally, the performance of the proposed solution is
evaluated by using large-scale UAV datasets. The results demonstrate that it
increases the efficiency of feature matching with speedup ratios ranging from
77.0 to 100.0 compared with KD-Tree based matching methods, and achieves
comparable accuracy in relative and absolute bundle adjustment (BA). The
proposed algorithm is an efficient solution for feature matching of UAV images.

</details>


### [253] [UAVPairs: A Challenging Benchmark for Match Pair Retrieval of Large-scale UAV Images](https://arxiv.org/abs/2505.22098)
*Junhuan Liu,San Jiang,Wei Ge,Wei Huang,Bingxuan Guo,Qingquan Li*

Main category: cs.CV

TL;DR: 论文提出了UAVPairs数据集和训练流程，用于大规模无人机图像匹配对检索，通过几何相似性和新损失函数提升检索精度和3D重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决大规模无人机图像匹配对检索中数据集不足、训练样本挖掘成本高以及传统损失函数效果有限的问题。

Method: 构建UAVPairs数据集，提出批量非平凡样本挖掘策略和排名列表损失函数，优化全局相似性结构。

Result: 实验表明，新方法和数据集显著提高了检索精度，增强了3D重建质量，在复杂场景中表现更鲁棒。

Conclusion: UAVPairs数据集和训练流程为大规模无人机图像匹配对检索提供了高效解决方案，数据集将公开。

Abstract: The primary contribution of this paper is a challenging benchmark dataset,
UAVPairs, and a training pipeline designed for match pair retrieval of
large-scale UAV images. First, the UAVPairs dataset, comprising 21,622
high-resolution images across 30 diverse scenes, is constructed; the 3D points
and tracks generated by SfM-based 3D reconstruction are employed to define the
geometric similarity of image pairs, ensuring genuinely matchable image pairs
are used for training. Second, to solve the problem of expensive mining cost
for global hard negative mining, a batched nontrivial sample mining strategy is
proposed, leveraging the geometric similarity and multi-scene structure of the
UAVPairs to generate training samples as to accelerate training. Third,
recognizing the limitation of pair-based losses, the ranked list loss is
designed to improve the discrimination of image retrieval models, which
optimizes the global similarity structure constructed from the positive set and
negative set. Finally, the effectiveness of the UAVPairs dataset and training
pipeline is validated through comprehensive experiments on three distinct
large-scale UAV datasets. The experiment results demonstrate that models
trained with the UAVPairs dataset and the ranked list loss achieve
significantly improved retrieval accuracy compared to models trained on
existing datasets or with conventional losses. Furthermore, these improvements
translate to enhanced view graph connectivity and higher quality of
reconstructed 3D models. The models trained by the proposed approach perform
more robustly compared with hand-crafted global features, particularly in
challenging repetitively textured scenes and weakly textured scenes. For match
pair retrieval of large-scale UAV images, the trained image retrieval models
offer an effective solution. The dataset would be made publicly available at
https://github.com/json87/UAVPairs.

</details>


### [254] [On the Transferability and Discriminability of Repersentation Learning in Unsupervised Domain Adaptation](https://arxiv.org/abs/2505.22099)
*Wenwen Qiang,Ziyin Gu,Lingyu Si,Jiangmeng Li,Changwen Zheng,Fuchun Sun,Hui Xiong*

Main category: cs.CV

TL;DR: 论文提出了一种新的对抗性无监督域适应（UDA）框架RLGLC，通过结合域对齐和目标域可区分性增强约束，解决了传统方法忽视目标域特征可区分性的问题。


<details>
  <summary>Details</summary>
Motivation: 传统对抗性UDA框架仅依赖分布对齐和源域经验风险最小化，忽视了目标域特征的可区分性，导致性能不佳。

Method: 提出RLGLC框架，结合域对齐目标和可区分性增强约束，使用AR-WWD处理类别不平衡和语义维度加权，并通过局部一致性机制保留目标域细粒度信息。

Result: 在多个基准数据集上的实验表明，RLGLC显著优于现有方法。

Conclusion: 研究证实了在对抗性UDA中同时保证可迁移性和可区分性的必要性，RLGLC框架具有理论和实践价值。

Abstract: In this paper, we addressed the limitation of relying solely on distribution
alignment and source-domain empirical risk minimization in Unsupervised Domain
Adaptation (UDA). Our information-theoretic analysis showed that this standard
adversarial-based framework neglects the discriminability of target-domain
features, leading to suboptimal performance. To bridge this
theoretical-practical gap, we defined "good representation learning" as
guaranteeing both transferability and discriminability, and proved that an
additional loss term targeting target-domain discriminability is necessary.
Building on these insights, we proposed a novel adversarial-based UDA framework
that explicitly integrates a domain alignment objective with a
discriminability-enhancing constraint. Instantiated as Domain-Invariant
Representation Learning with Global and Local Consistency (RLGLC), our method
leverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD)
to address class imbalance and semantic dimension weighting, and employs a
local consistency mechanism to preserve fine-grained target-domain
discriminative information. Extensive experiments across multiple benchmark
datasets demonstrate that RLGLC consistently surpasses state-of-the-art
methods, confirming the value of our theoretical perspective and underscoring
the necessity of enforcing both transferability and discriminability in
adversarial-based UDA.

</details>


### [255] [Adapting Segment Anything Model for Power Transmission Corridor Hazard Segmentation](https://arxiv.org/abs/2505.22105)
*Hang Chen,Maoyuan Ye,Peng Yang,Haibin He,Juhua Liu,Bo Du*

Main category: cs.CV

TL;DR: ELE-SAM改进SAM模型，用于电力传输走廊危险分割（PTCHS），通过上下文感知提示适配器和高保真掩码解码器提升性能，并构建了ELE-40K数据集。


<details>
  <summary>Details</summary>
Motivation: 解决SAM在复杂电力传输走廊场景中难以分割精细结构目标的问题。

Method: 提出上下文感知提示适配器和高保真掩码解码器，结合多粒度掩码特征。

Result: ELE-SAM在ELE-40K数据集上平均提升16.8% mIoU和20.6% mBIoU，优于现有方法。

Conclusion: ELE-SAM在PTCHS任务中表现优异，为高质量通用目标分割提供了有效解决方案。

Abstract: Power transmission corridor hazard segmentation (PTCHS) aims to separate
transmission equipment and surrounding hazards from complex background,
conveying great significance to maintaining electric power transmission safety.
Recently, the Segment Anything Model (SAM) has emerged as a foundational vision
model and pushed the boundaries of segmentation tasks. However, SAM struggles
to deal with the target objects in complex transmission corridor scenario,
especially those with fine structure. In this paper, we propose ELE-SAM,
adapting SAM for the PTCHS task. Technically, we develop a Context-Aware Prompt
Adapter to achieve better prompt tokens via incorporating global-local features
and focusing more on key regions. Subsequently, to tackle the hazard objects
with fine structure in complex background, we design a High-Fidelity Mask
Decoder by leveraging multi-granularity mask features and then scaling them to
a higher resolution. Moreover, to train ELE-SAM and advance this field, we
construct the ELE-40K benchmark, the first large-scale and real-world dataset
for PTCHS including 44,094 image-mask pairs. Experimental results for ELE-40K
demonstrate the superior performance that ELE-SAM outperforms the baseline
model with the average 16.8% mIoU and 20.6% mBIoU performance improvement.
Moreover, compared with the state-of-the-art method on HQSeg-44K, the average
2.9% mIoU and 3.8% mBIoU absolute improvements further validate the
effectiveness of our method on high-quality generic object segmentation. The
source code and dataset are available at https://github.com/Hhaizee/ELE-SAM.

</details>


### [256] [Autoregression-free video prediction using diffusion model for mitigating error propagation](https://arxiv.org/abs/2505.22111)
*Woonho Ko,Jin Bok Park,Il Yong Chun*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的无自回归（ARFree）视频预测框架，解决了传统自回归方法中误差传播的问题。


<details>
  <summary>Details</summary>
Motivation: 现有长期视频预测方法依赖自回归机制，但存在误差传播问题，尤其是在远距离未来帧中。

Method: ARFree直接预测未来帧组，包含运动预测模块和训练方法，以提升运动连续性和上下文一致性。

Result: 在两个基准数据集上的实验表明，ARFree优于多种最先进的视频预测方法。

Conclusion: ARFree框架通过无自回归机制和扩散模型，显著提升了视频预测的准确性和稳定性。

Abstract: Existing long-term video prediction methods often rely on an autoregressive
video prediction mechanism. However, this approach suffers from error
propagation, particularly in distant future frames. To address this limitation,
this paper proposes the first AutoRegression-Free (ARFree) video prediction
framework using diffusion models. Different from an autoregressive video
prediction mechanism, ARFree directly predicts any future frame tuples from the
context frame tuple. The proposed ARFree consists of two key components: 1) a
motion prediction module that predicts a future motion using motion feature
extracted from the context frame tuple; 2) a training method that improves
motion continuity and contextual consistency between adjacent future frame
tuples. Our experiments with two benchmark datasets show that the proposed
ARFree video prediction framework outperforms several state-of-the-art video
prediction methods.

</details>


### [257] [SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model](https://arxiv.org/abs/2505.22126)
*Yifan Chang,Yukang Feng,Jianwen Sun,Jiaxin Ai,Chuanhao Li,S. Kevin Zhou,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 论文介绍了首个科学图表生成基准SridBench，评估AI在科学插图生成中的表现，发现现有模型如GPT-4o-image仍落后于人类。


<details>
  <summary>Details</summary>
Motivation: 科学插图生成需要高精度和专业知识，但目前缺乏评估AI在此任务上的基准，因此填补这一空白具有重要价值。

Method: 通过专家和多模态大语言模型（MLLMs）收集来自13个学科的1,120个科学图表样本，并从六个维度评估模型表现。

Result: 实验显示，即使是顶级模型如GPT-4o-image，在语义保真度和结构准确性等方面仍不及人类，存在文本/视觉清晰度和科学正确性问题。

Conclusion: 研究强调了需要更先进的推理驱动视觉生成能力，以提升AI在科学插图生成中的表现。

Abstract: Recent years have seen rapid advances in AI-driven image generation. Early
diffusion models emphasized perceptual quality, while newer multimodal models
like GPT-4o-image integrate high-level reasoning, improving semantic
understanding and structural composition. Scientific illustration generation
exemplifies this evolution: unlike general image synthesis, it demands accurate
interpretation of technical content and transformation of abstract ideas into
clear, standardized visuals. This task is significantly more
knowledge-intensive and laborious, often requiring hours of manual work and
specialized tools. Automating it in a controllable, intelligent manner would
provide substantial practical value. Yet, no benchmark currently exists to
evaluate AI on this front. To fill this gap, we introduce SridBench, the first
benchmark for scientific figure generation. It comprises 1,120 instances
curated from leading scientific papers across 13 natural and computer science
disciplines, collected via human experts and MLLMs. Each sample is evaluated
along six dimensions, including semantic fidelity and structural accuracy.
Experimental results reveal that even top-tier models like GPT-4o-image lag
behind human performance, with common issues in text/visual clarity and
scientific correctness. These findings highlight the need for more advanced
reasoning-driven visual generation capabilities.

</details>


### [258] [Real-Time Blind Defocus Deblurring for Earth Observation: The IMAGIN-e Mission Approach](https://arxiv.org/abs/2505.22128)
*Alejandro D. Mousist*

Main category: cs.CV

TL;DR: 论文提出了一种针对ISS上IMAGIN-e任务中地球观测图像机械散焦的盲去模糊方法，利用GAN框架在无参考图像下恢复图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决太空边缘计算环境下地球观测图像的机械散焦问题，提升图像质量以支持实际应用。

Method: 基于Sentinel-2数据估计散焦核，并在GAN框架中训练恢复模型，无需参考图像。

Result: 在合成退化的Sentinel-2图像上，SSIM提升72.47%，PSNR提升25.00%；在IMAGIN-e任务中，NIQE和BRISQUE分别提升60.66%和48.38%。

Conclusion: 该方法成功部署于IMAGIN-e任务，证明了其在资源受限的太空环境中的实用性和高效性。

Abstract: This work addresses mechanical defocus in Earth observation images from the
IMAGIN-e mission aboard the ISS, proposing a blind deblurring approach adapted
to space-based edge computing constraints. Leveraging Sentinel-2 data, our
method estimates the defocus kernel and trains a restoration model within a GAN
framework, effectively operating without reference images.
  On Sentinel-2 images with synthetic degradation, SSIM improved by 72.47% and
PSNR by 25.00%, confirming the model's ability to recover lost details when the
original clean image is known. On IMAGIN-e, where no reference images exist,
perceptual quality metrics indicate a substantial enhancement, with NIQE
improving by 60.66% and BRISQUE by 48.38%, validating real-world onboard
restoration. The approach is currently deployed aboard the IMAGIN-e mission,
demonstrating its practical application in an operational space environment.
  By efficiently handling high-resolution images under edge computing
constraints, the method enables applications such as water body segmentation
and contour detection while maintaining processing viability despite resource
limitations.

</details>


### [259] [What Makes for Text to 360-degree Panorama Generation with Stable Diffusion?](https://arxiv.org/abs/2505.22129)
*Jinhong Ni,Chang-Bin Zhang,Qiang Zhang,Jing Zhang*

Main category: cs.CV

TL;DR: 论文研究了如何通过微调预训练的扩散模型（如Stable Diffusion）生成360度全景图像，发现注意力模块中的查询和键矩阵共享通用信息，而值和输出权重矩阵则专门用于适应全景域。提出的UniPano框架在性能和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索预训练扩散模型在生成全景图像时的内在机制，解决视角与全景图像之间的领域差距问题。

Method: 通过分析注意力模块中不同矩阵的作用，提出UniPano框架，专注于优化值和输出权重矩阵以适应全景生成。

Result: UniPano在性能和效率上优于现有方法，同时显著减少内存占用和训练时间。

Conclusion: UniPano为全景生成提供了一个高效且可扩展的基线，揭示了预训练扩散模型在适应新领域时的关键机制。

Abstract: Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion,
has stimulated research to adapt them to 360-degree panorama generation. Prior
work has demonstrated the feasibility of using conventional low-rank adaptation
techniques on pre-trained diffusion models to generate panoramic images.
However, the substantial domain gap between perspective and panoramic images
raises questions about the underlying mechanisms enabling this empirical
success. We hypothesize and examine that the trainable counterparts exhibit
distinct behaviors when fine-tuned on panoramic data, and such an adaptation
conceals some intrinsic mechanism to leverage the prior knowledge within the
pre-trained diffusion models. Our analysis reveals the following: 1) the query
and key matrices in the attention modules are responsible for common
information that can be shared between the panoramic and perspective domains,
thus are less relevant to panorama generation; and 2) the value and output
weight matrices specialize in adapting pre-trained knowledge to the panoramic
domain, playing a more critical role during fine-tuning for panorama
generation. We empirically verify these insights by introducing a simple
framework called UniPano, with the objective of establishing an elegant
baseline for future research. UniPano not only outperforms existing methods but
also significantly reduces memory usage and training time compared to prior
dual-branch approaches, making it scalable for end-to-end panorama generation
with higher resolution. The code will be released.

</details>


### [260] [FaceEditTalker: Interactive Talking Head Generation with Facial Attribute Editing](https://arxiv.org/abs/2505.22141)
*Guanwen Feng,Zhiyuan Ma,Yunan Li,Junwei Jing,Jiahao Yang,Qiguang Miao*

Main category: cs.CV

TL;DR: FaceEditTalker是一个统一框架，支持在生成高质量音频同步说话头部视频时进行可控的面部属性编辑。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动说话头部生成方法忽视了面部属性编辑的重要性，而这一能力对个性化、品牌适配等应用至关重要。

Method: 结合图像特征空间编辑模块（提取语义和细节特征）和音频驱动视频生成模块（融合编辑特征与音频引导的面部标志），采用扩散生成器确保时序一致性和视觉保真度。

Result: 在公开数据集上，FaceEditTalker在唇同步准确性、视频质量和属性可控性方面优于现有方法。

Conclusion: FaceEditTalker为面部属性编辑和高质量视频生成提供了有效解决方案，具有广泛的应用潜力。

Abstract: Recent advances in audio-driven talking head generation have achieved
impressive results in lip synchronization and emotional expression. However,
they largely overlook the crucial task of facial attribute editing. This
capability is crucial for achieving deep personalization and expanding the
range of practical applications, including user-tailored digital avatars,
engaging online education content, and brand-specific digital customer service.
In these key domains, the flexible adjustment of visual attributes-such as
hairstyle, accessories, and subtle facial features is essential for aligning
with user preferences, reflecting diverse brand identities, and adapting to
varying contextual demands. In this paper, we present FaceEditTalker, a unified
framework that enables controllable facial attribute manipulation while
generating high-quality, audio-synchronized talking head videos. Our method
consists of two key components: an image feature space editing module, which
extracts semantic and detail features and allows flexible control over
attributes like expression, hairstyle, and accessories; and an audio-driven
video generation module, which fuses these edited features with audio-guided
facial landmarks to drive a diffusion-based generator. This design ensures
temporal coherence, visual fidelity, and identity preservation across frames.
Extensive experiments on public datasets demonstrate that our method
outperforms state-of-the-art approaches in lip-sync accuracy, video quality,
and attribute controllability. Project page:
https://peterfanfan.github.io/FaceEditTalker/

</details>


### [261] [3D Question Answering via only 2D Vision-Language Models](https://arxiv.org/abs/2505.22143)
*Fengyun Wang,Sicheng Yu,Jiawei Wu,Jinhui Tang,Hanwang Zhang,Qianru Sun*

Main category: cs.CV

TL;DR: 论文提出cdViews方法，通过自动选择关键和多样化的2D视图，利用2D视觉语言模型（LVLMs）在零样本条件下解决3D场景理解任务，如3D问答（3D-QA）。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用大型视觉语言模型（LVLMs）解决3D场景理解任务，尤其是在3D训练数据有限的情况下，避免训练3D模型。

Method: 提出cdViews方法，包括viewSelector（选择关键视图）和viewNMS（增强视图多样性），通过2D模型（如LLAVA-OV）处理3D点云的采样视图。

Result: 在ScanQA和SQA基准测试中达到最先进性能，证明2D LVLMs是解决3D任务的有效替代方案。

Conclusion: 2D LVLMs是目前解决3D任务的最有效替代方案，避免了资源密集型的3D LVLMs训练。

Abstract: Large vision-language models (LVLMs) have significantly advanced numerous
fields. In this work, we explore how to harness their potential to address 3D
scene understanding tasks, using 3D question answering (3D-QA) as a
representative example. Due to the limited training data in 3D, we do not train
LVLMs but infer in a zero-shot manner. Specifically, we sample 2D views from a
3D point cloud and feed them into 2D models to answer a given question. When
the 2D model is chosen, e.g., LLAVA-OV, the quality of sampled views matters
the most. We propose cdViews, a novel approach to automatically selecting
critical and diverse Views for 3D-QA. cdViews consists of two key components:
viewSelector prioritizing critical views based on their potential to provide
answer-specific information, and viewNMS enhancing diversity by removing
redundant views based on spatial overlap. We evaluate cdViews on the
widely-used ScanQA and SQA benchmarks, demonstrating that it achieves
state-of-the-art performance in 3D-QA while relying solely on 2D models without
fine-tuning. These findings support our belief that 2D LVLMs are currently the
most effective alternative (of the resource-intensive 3D LVLMs) for addressing
3D tasks.

</details>


### [262] [Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language](https://arxiv.org/abs/2505.22146)
*Guangfu Hao,Haojie Wen,Liangxuna Guo,Yang Chen,Yanchao Bi,Shan Yu*

Main category: cs.CV

TL;DR: 论文提出了一种基于低维属性表示的框架，用于连接视觉工具感知和语言任务理解，显著提升了工具选择任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 人类在工具选择上展现出复杂的认知能力，但现有计算模型对此能力的研究仍不足。

Method: 使用视觉编码器（ResNet或ViT）从工具图像中提取属性，并结合微调的语言模型（如GPT-2、LLaMA）从任务描述中推导所需属性。

Result: 该方法在工具选择任务中达到74%的准确率，显著优于直接工具匹配（20%）和小型多模态模型（21%-58%），并接近更大模型（如GPT-4o）的性能。

Conclusion: 该研究提供了一种参数高效且可解释的解决方案，模拟人类工具认知，推动了认知科学和实际应用的发展。

Abstract: Flexible tool selection reflects a complex cognitive ability that
distinguishes humans from other species, yet computational models that capture
this ability remain underdeveloped. We developed a framework using
low-dimensional attribute representations to bridge visual tool perception and
linguistic task understanding. We constructed a comprehensive dataset (ToolNet)
containing 115 common tools labeled with 13 carefully designed attributes
spanning physical, functional, and psychological properties, paired with
natural language scenarios describing tool usage. Visual encoders (ResNet or
ViT) extract attributes from tool images while fine-tuned language models
(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our
approach achieves 74% accuracy in tool selection tasks-significantly
outperforming direct tool matching (20%) and smaller multimodal models
(21%-58%), while approaching performance of much larger models like GPT-4o
(73%) with substantially fewer parameters. Ablation studies revealed that
manipulation-related attributes (graspability, hand-relatedness, elongation)
consistently prove most critical across modalities. This work provides a
parameter-efficient, interpretable solution that mimics human-like tool
cognition, advancing both cognitive science understanding and practical
applications in tool selection tasks.

</details>


### [263] [Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging](https://arxiv.org/abs/2505.22150)
*Runze Xia,Shuo Feng,Renzhi Wang,Congchi Yin,Xuyun Wen,Piji Li*

Main category: cs.CV

TL;DR: 提出了一种名为FgB2I的方法，通过细粒度文本作为桥梁改进脑到图像的重建，解决了现有方法中细节和语义不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的脑到图像重建方法在细节和语义一致性上表现不足，可能是由于语义信息不足。

Method: FgB2I分为三个阶段：细节增强、解码细粒度文本描述和基于文本的脑到图像重建。利用大视觉语言模型生成细粒度描述，并通过三个奖励指标指导解码。

Result: 细粒度文本描述可以整合到现有重建方法中，实现更精细的脑到图像重建。

Conclusion: FgB2I通过引入细粒度文本，显著提升了脑到图像重建的细节和语义一致性。

Abstract: Brain-to-Image reconstruction aims to recover visual stimuli perceived by
humans from brain activity. However, the reconstructed visual stimuli often
missing details and semantic inconsistencies, which may be attributed to
insufficient semantic information. To address this issue, we propose an
approach named Fine-grained Brain-to-Image reconstruction (FgB2I), which
employs fine-grained text as bridge to improve image reconstruction. FgB2I
comprises three key stages: detail enhancement, decoding fine-grained text
descriptions, and text-bridged brain-to-image reconstruction. In the
detail-enhancement stage, we leverage large vision-language models to generate
fine-grained captions for visual stimuli and experimentally validate its
importance. We propose three reward metrics (object accuracy, text-image
semantic similarity, and image-image semantic similarity) to guide the language
model in decoding fine-grained text descriptions from fMRI signals. The
fine-grained text descriptions can be integrated into existing reconstruction
methods to achieve fine-grained Brain-to-Image reconstruction.

</details>


### [264] [Learning A Robust RGB-Thermal Detector for Extreme Modality Imbalance](https://arxiv.org/abs/2505.22154)
*Chao Tian,Chao Yang,Guoqing Zhu,Qiang Wang,Zhenyu He*

Main category: cs.CV

TL;DR: 提出一种基于基础与辅助检测器的RGB-T目标检测方法，通过模态交互模块和伪退化训练提升模型在极端模态不平衡下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中RGB-T数据可能因环境或技术问题出现模态退化，导致训练和测试时的分布不一致问题。

Method: 设计基础与辅助检测器架构，引入模态交互模块动态加权模态，并通过伪退化模拟真实不平衡数据。

Result: 实验显示方法显著降低缺失率（55%），并在多种基线检测器上提升性能。

Conclusion: 该方法有效解决了RGB-T检测中的极端模态不平衡问题，增强了模型鲁棒性。

Abstract: RGB-Thermal (RGB-T) object detection utilizes thermal infrared (TIR) images
to complement RGB data, improving robustness in challenging conditions.
Traditional RGB-T detectors assume balanced training data, where both
modalities contribute equally. However, in real-world scenarios, modality
degradation-due to environmental factors or technical issues-can lead to
extreme modality imbalance, causing out-of-distribution (OOD) issues during
testing and disrupting model convergence during training. This paper addresses
these challenges by proposing a novel base-and-auxiliary detector architecture.
We introduce a modality interaction module to adaptively weigh modalities based
on their quality and handle imbalanced samples effectively. Additionally, we
leverage modality pseudo-degradation to simulate real-world imbalances in
training data. The base detector, trained on high-quality pairs, provides a
consistency constraint for the auxiliary detector, which receives degraded
samples. This framework enhances model robustness, ensuring reliable
performance even under severe modality degradation. Experimental results
demonstrate the effectiveness of our method in handling extreme modality
imbalances~(decreasing the Missing Rate by 55%) and improving performance
across various baseline detectors.

</details>


### [265] [Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers](https://arxiv.org/abs/2505.22167)
*Weilun Feng,Chuanguang Yang,Haotong Qin,Xiangqi Li,Yu Wang,Zhulin An,Libo Huang,Boyu Diao,Zixiang Zhao,Yongjun Xu,Michele Magno*

Main category: cs.CV

TL;DR: Q-VDiT是一个专为视频DiT模型设计的量化框架，通过Token-aware Quantization Estimator（TQE）和Temporal Maintenance Distillation（TMD）解决量化中的信息丢失和优化目标对齐问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图像生成模型量化方法在视频生成任务中表现不佳，主要由于量化过程中的信息丢失和优化目标与视频生成需求不匹配。

Method: 提出Q-VDiT框架，包括TQE（补偿量化误差）和TMD（保持帧间时空相关性）。

Result: W3A6 Q-VDiT在场景一致性上达到23.40，比现有量化方法性能提升1.9倍。

Conclusion: Q-VDiT为视频DiT模型提供了一种高效的量化解决方案，显著提升了性能并适用于边缘设备部署。

Abstract: Diffusion transformers (DiT) have demonstrated exceptional performance in
video generation. However, their large number of parameters and high
computational complexity limit their deployment on edge devices. Quantization
can reduce storage requirements and accelerate inference by lowering the
bit-width of model parameters. Yet, existing quantization methods for image
generation models do not generalize well to video generation tasks. We identify
two primary challenges: the loss of information during quantization and the
misalignment between optimization objectives and the unique requirements of
video generation. To address these challenges, we present Q-VDiT, a
quantization framework specifically designed for video DiT models. From the
quantization perspective, we propose the Token-aware Quantization Estimator
(TQE), which compensates for quantization errors in both the token and feature
dimensions. From the optimization perspective, we introduce Temporal
Maintenance Distillation (TMD), which preserves the spatiotemporal correlations
between frames and enables the optimization of each frame with respect to the
overall video context. Our W3A6 Q-VDiT achieves a scene consistency of 23.40,
setting a new benchmark and outperforming current state-of-the-art quantization
methods by 1.9$\times$. Code will be available at
https://github.com/cantbebetter2/Q-VDiT.

</details>


### [266] [S2AFormer: Strip Self-Attention for Efficient Vision Transformer](https://arxiv.org/abs/2505.22195)
*Guoan Xu,Wenfeng Huang,Wenjing Jia,Jiamao Li,Guangwei Gao,Guo-Jun Qi*

Main category: cs.CV

TL;DR: S2AFormer是一种高效的Vision Transformer架构，通过Strip Self-Attention（SSA）和Hybrid Perception Blocks（HPBs）结合CNN的局部感知与Transformer的全局建模能力，显著降低计算开销并保持准确性。


<details>
  <summary>Details</summary>
Motivation: Vision Transformer（ViT）因计算需求随令牌数量二次增长而效率受限，现有方法虽结合卷积与自注意力，但仍存在计算瓶颈。

Method: 提出SSA，通过压缩空间和通道维度减少计算开销，同时设计HPBs整合CNN和Transformer的优势。

Result: 在ImageNet-1k、ADE20k和COCO等基准测试中，S2AFormer在GPU和非GPU环境下均表现出高效且高精度。

Conclusion: S2AFormer在效率与性能间取得平衡，是高效视觉Transformer的有力候选。

Abstract: Vision Transformer (ViT) has made significant advancements in computer
vision, thanks to its token mixer's sophisticated ability to capture global
dependencies between all tokens. However, the quadratic growth in computational
demands as the number of tokens increases limits its practical efficiency.
Although recent methods have combined the strengths of convolutions and
self-attention to achieve better trade-offs, the expensive pairwise token
affinity and complex matrix operations inherent in self-attention remain a
bottleneck. To address this challenge, we propose S2AFormer, an efficient
Vision Transformer architecture featuring novel Strip Self-Attention (SSA). We
design simple yet effective Hybrid Perception Blocks (HPBs) to effectively
integrate the local perception capabilities of CNNs with the global context
modeling of Transformer's attention mechanisms. A key innovation of SSA lies in
its reducing the spatial dimensions of $K$ and $V$ while compressing the
channel dimensions of $Q$ and $K$. This design significantly reduces
computational overhead while preserving accuracy, striking an optimal balance
between efficiency and effectiveness. We evaluate the robustness and efficiency
of S2AFormer through extensive experiments on multiple vision benchmarks,
including ImageNet-1k for image classification, ADE20k for semantic
segmentation, and COCO for object detection and instance segmentation. Results
demonstrate that S2AFormer achieves significant accuracy gains with superior
efficiency in both GPU and non-GPU environments, making it a strong candidate
for efficient vision Transformers.

</details>


### [267] [Investigating Mechanisms for In-Context Vision Language Binding](https://arxiv.org/abs/2505.22200)
*Darshana Saravanan,Makarand Tapaswi,Vineet Gandhi*

Main category: cs.CV

TL;DR: 论文研究了视觉语言模型（VLMs）中图像与文本绑定的机制，提出了一种Binding ID机制，用于关联图像中的对象与其文本描述。


<details>
  <summary>Details</summary>
Motivation: 理解VLMs如何通过感知图像、理解文本并在多模态间建立关联，以实现图像与文本的绑定。

Method: 使用合成数据集和任务，研究VLMs中对象图像标记与其文本引用之间的Binding ID机制。

Result: 实验表明，VLMs为对象的图像标记和文本引用分配了独特的Binding ID，实现了上下文关联。

Conclusion: Binding ID机制在VLMs中有效支持图像与文本的绑定，为多模态理解提供了新视角。

Abstract: To understand a prompt, Vision-Language models (VLMs) must perceive the
image, comprehend the text, and build associations within and across both
modalities. For instance, given an 'image of a red toy car', the model should
associate this image to phrases like 'car', 'red toy', 'red object', etc. Feng
and Steinhardt propose the Binding ID mechanism in LLMs, suggesting that the
entity and its corresponding attribute tokens share a Binding ID in the model
activations. We investigate this for image-text binding in VLMs using a
synthetic dataset and task that requires models to associate 3D objects in an
image with their descriptions in the text. Our experiments demonstrate that
VLMs assign a distinct Binding ID to an object's image tokens and its textual
references, enabling in-context association.

</details>


### [268] [A Survey on Training-free Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2505.22209)
*Naomi Kombol,Ivan Martinović,Siniša Šegvić*

Main category: cs.CV

TL;DR: 该论文综述了无需训练的开放词汇语义分割方法，重点介绍了基于CLIP、辅助视觉基础模型和生成方法的30多种方法，并讨论了当前研究的局限性和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统语义分割方法需要大量计算资源和标注数据，而开放词汇语义分割要求模型超越已学习类别，因此研究者转向利用现有多模态分类模型的无需训练方法。

Method: 论文首先定义任务，概述流行模型类型，并分类介绍30多种方法，包括纯CLIP、辅助视觉基础模型和生成方法。

Result: 综述了当前无需训练开放词汇语义分割的最新进展，总结了不同方法的优缺点。

Conclusion: 论文指出了当前研究的局限性，提出了未来探索方向，并希望激发新研究者的兴趣。

Abstract: Semantic segmentation is one of the most fundamental tasks in image
understanding with a long history of research, and subsequently a myriad of
different approaches. Traditional methods strive to train models up from
scratch, requiring vast amounts of computational resources and training data.
In the advent of moving to open-vocabulary semantic segmentation, which asks
models to classify beyond learned categories, large quantities of finely
annotated data would be prohibitively expensive. Researchers have instead
turned to training-free methods where they leverage existing models made for
tasks where data is more easily acquired. Specifically, this survey will cover
the history, nuance, idea development and the state-of-the-art in training-free
open-vocabulary semantic segmentation that leverages existing multi-modal
classification models. We will first give a preliminary on the task definition
followed by an overview of popular model archetypes and then spotlight over 30
approaches split into broader research branches: purely CLIP-based, those
leveraging auxiliary visual foundation models and ones relying on generative
methods. Subsequently, we will discuss the limitations and potential problems
of current research, as well as provide some underexplored ideas for future
study. We believe this survey will serve as a good onboarding read to new
researchers and spark increased interest in the area.

</details>


### [269] [Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation](https://arxiv.org/abs/2505.22222)
*Yunsoo Kim,Jinge Wu,Su-Hwan Kim,Pardeep Vasudev,Jiashu Shen,Honghan Wu*

Main category: cs.CV

TL;DR: 提出了一种名为Look & Mark（L&M）的新方法，通过结合放射科医生的注视点和标注框，显著提升了多模态大语言模型在医学影像分析中的性能，减少了临床错误。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在医学影像分析中存在幻觉和临床错误，限制了其实际应用的可靠性。

Method: L&M方法结合了放射科医生的注视点（Look）和标注框（Mark），通过上下文学习提升模型性能，无需重新训练。

Result: L&M显著提升了模型性能，例如CXR-LLaVA的A.AVG指标提高了1.2%，LLaVA-Med提升了9.2%，并减少了临床错误（平均每份报告减少0.43个错误）。

Conclusion: L&M是一种可扩展且高效的解决方案，有望提升低资源临床环境中的诊断流程。

Abstract: Recent advancements in multimodal Large Language Models (LLMs) have
significantly enhanced the automation of medical image analysis, particularly
in generating radiology reports from chest X-rays (CXR). However, these models
still suffer from hallucinations and clinically significant errors, limiting
their reliability in real-world applications. In this study, we propose Look &
Mark (L&M), a novel grounding fixation strategy that integrates radiologist eye
fixations (Look) and bounding box annotations (Mark) into the LLM prompting
framework. Unlike conventional fine-tuning, L&M leverages in-context learning
to achieve substantial performance gains without retraining. When evaluated
across multiple domain-specific and general-purpose models, L&M demonstrates
significant gains, including a 1.2% improvement in overall metrics (A.AVG) for
CXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for
LLaVA-Med. General-purpose models also benefit from L&M combined with
in-context learning, with LLaVA-OV achieving an 87.3% clinical average
performance (C.AVG)-the highest among all models, even surpassing those
explicitly trained for CXR report generation. Expert evaluations further
confirm that L&M reduces clinically significant errors (by 0.43 average errors
per report), such as false predictions and omissions, enhancing both accuracy
and reliability. These findings highlight L&M's potential as a scalable and
efficient solution for AI-assisted radiology, paving the way for improved
diagnostic workflows in low-resource clinical settings.

</details>


### [270] [Hadaptive-Net: Efficient Vision Models via Adaptive Cross-Hadamard Synergy](https://arxiv.org/abs/2505.22226)
*Xuyang Zhang,Xi Zhang,Liang Chen,Hao Shi,Qingshan Guo*

Main category: cs.CV

TL;DR: 本文提出了一种基于Hadamard积的高效模块ACH，并构建了轻量级网络Hadaptive-Net，在速度和精度间取得了前所未有的平衡。


<details>
  <summary>Details</summary>
Motivation: 尽管Hadamard积在理论上具有增强网络表示能力和维度压缩的潜力，但其实际应用尚未系统探索。

Method: 分析了Hadamard积在跨通道交互和通道扩展中的优势，提出自适应跨通道Hadamard积模块（ACH），并构建了Hadaptive-Net网络。

Result: 实验证明Hadaptive-Net在视觉任务中实现了速度和精度的优异平衡。

Conclusion: ACH模块和Hadaptive-Net展示了Hadamard积在实际应用中的潜力，为轻量级网络设计提供了新思路。

Abstract: Recent studies have revealed the immense potential of Hadamard product in
enhancing network representational capacity and dimensional compression.
However, despite its theoretical promise, this technique has not been
systematically explored or effectively applied in practice, leaving its full
capabilities underdeveloped. In this work, we first analyze and identify the
advantages of Hadamard product over standard convolutional operations in
cross-channel interaction and channel expansion. Building upon these insights,
we propose a computationally efficient module: Adaptive Cross-Hadamard (ACH),
which leverages adaptive cross-channel Hadamard products for high-dimensional
channel expansion. Furthermore, we introduce Hadaptive-Net (Hadamard Adaptive
Network), a lightweight network backbone for visual tasks, which is
demonstrated through experiments that it achieves an unprecedented balance
between inference speed and accuracy through our proposed module.

</details>


### [271] [GoMatching++: Parameter- and Data-Efficient Arbitrary-Shaped Video Text Spotting and Benchmarking](https://arxiv.org/abs/2505.22228)
*Haibin He,Jing Zhang,Maoyuan Ye,Juhua Liu,Bo Du,Dacheng Tao*

Main category: cs.CV

TL;DR: GoMatching++是一种参数和数据高效的方法，将现成的图像文本识别器转化为视频文本识别器，通过冻结图像模型并引入轻量级跟踪器，显著提升了视频文本识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频文本识别方法性能不足，尤其是识别能力有限，即使经过端到端训练仍无法达到图像文本识别的水平。

Method: 提出GoMatching++，冻结图像文本识别器并引入轻量级可训练跟踪器，包含重评分机制和LST-Matcher以增强视频文本处理能力。

Result: 在ICDAR15-video、DSText和BOVText等基准测试中创下新记录，并显著降低训练成本。同时发布了包含弯曲文本的新基准ArTVideo。

Conclusion: GoMatching++和ArTVideo基准将推动视频文本识别领域的未来进展。

Abstract: Video text spotting (VTS) extends image text spotting (ITS) by adding text
tracking, significantly increasing task complexity. Despite progress in VTS,
existing methods still fall short of the performance seen in ITS. This paper
identifies a key limitation in current video text spotters: limited recognition
capability, even after extensive end-to-end training. To address this, we
propose GoMatching++, a parameter- and data-efficient method that transforms an
off-the-shelf image text spotter into a video specialist. The core idea lies in
freezing the image text spotter and introducing a lightweight, trainable
tracker, which can be optimized efficiently with minimal training data. Our
approach includes two key components: (1) a rescoring mechanism to bridge the
domain gap between image and video data, and (2) the LST-Matcher, which
enhances the frozen image text spotter's ability to handle video text. We
explore various architectures for LST-Matcher to ensure efficiency in both
parameters and training data. As a result, GoMatching++ sets new performance
records on challenging benchmarks such as ICDAR15-video, DSText, and BOVText,
while significantly reducing training costs. To address the lack of curved text
datasets in VTS, we introduce ArTVideo, a new benchmark featuring over 30%
curved text with detailed annotations. We also provide a comprehensive
statistical analysis and experimental results for ArTVideo. We believe that
GoMatching++ and the ArTVideo benchmark will drive future advancements in video
text spotting. The source code, models and dataset are publicly available at
https://github.com/Hxyz-123/GoMatching.

</details>


### [272] [Enjoying Information Dividend: Gaze Track-based Medical Weakly Supervised Segmentation](https://arxiv.org/abs/2505.22230)
*Zhisong Wang,Yiwen Ye,Ziyang Chen,Yong Xia*

Main category: cs.CV

TL;DR: GradTrack利用医生的注视轨迹（包括注视点、持续时间和时间顺序）提升弱监督语义分割（WSSS）性能，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学影像中的弱监督语义分割（WSSS）难以有效利用稀疏标注，而现有基于注视的方法未能充分利用注视数据中的丰富信息。

Method: GradTrack包含两个关键组件：注视轨迹图生成和轨迹注意力，通过多级注视监督在解码过程中逐步优化特征。

Result: 在Kvasir-SEG和NCI-ISBI数据集上，GradTrack的Dice分数分别提高了3.21%和2.61%，显著缩小了与全监督模型的性能差距。

Conclusion: GradTrack通过充分利用注视数据，显著提升了WSSS性能，为医学影像分析提供了更高效的解决方案。

Abstract: Weakly supervised semantic segmentation (WSSS) in medical imaging struggles
with effectively using sparse annotations. One promising direction for WSSS
leverages gaze annotations, captured via eye trackers that record regions of
interest during diagnostic procedures. However, existing gaze-based methods,
such as GazeMedSeg, do not fully exploit the rich information embedded in gaze
data. In this paper, we propose GradTrack, a framework that utilizes
physicians' gaze track, including fixation points, durations, and temporal
order, to enhance WSSS performance. GradTrack comprises two key components:
Gaze Track Map Generation and Track Attention, which collaboratively enable
progressive feature refinement through multi-level gaze supervision during the
decoding process. Experiments on the Kvasir-SEG and NCI-ISBI datasets
demonstrate that GradTrack consistently outperforms existing gaze-based
methods, achieving Dice score improvements of 3.21\% and 2.61\%, respectively.
Moreover, GradTrack significantly narrows the performance gap with fully
supervised models such as nnUNet.

</details>


### [273] [StateSpaceDiffuser: Bringing Long Context to Diffusion World Models](https://arxiv.org/abs/2505.22246)
*Nedko Savov,Naser Kazemi,Deheng Zhang,Danda Pani Paudel,Xi Wang,Luc Van Gool*

Main category: cs.CV

TL;DR: StateSpaceDiffuser通过结合状态空间模型（Mamba）的序列表示，解决了扩散模型在长上下文任务中视觉一致性丢失的问题，显著提升了长期记忆能力。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型依赖短序列观测，导致视觉一致性快速丢失，无法反映早期信息。

Method: 引入StateSpaceDiffuser，将状态空间模型的序列表示与扩散模型结合，恢复长期记忆。

Result: 实验表明，StateSpaceDiffuser在2D迷宫和复杂3D环境中均能保持视觉一致性，显著优于基线模型。

Conclusion: 状态空间表示与扩散模型结合，既能保持高保真合成，又能实现长期记忆，效果显著。

Abstract: World models have recently become promising tools for predicting realistic
visuals based on actions in complex environments. However, their reliance on a
short sequence of observations causes them to quickly lose track of context. As
a result, visual consistency breaks down after just a few steps, and generated
scenes no longer reflect information seen earlier. This limitation of the
state-of-the-art diffusion-based world models comes from their lack of a
lasting environment state. To address this problem, we introduce
StateSpaceDiffuser, where a diffusion model is enabled to perform on
long-context tasks by integrating a sequence representation from a state-space
model (Mamba), representing the entire interaction history. This design
restores long-term memory without sacrificing the high-fidelity synthesis of
diffusion models. To rigorously measure temporal consistency, we develop an
evaluation protocol that probes a model's ability to reinstantiate seen content
in extended rollouts. Comprehensive experiments show that StateSpaceDiffuser
significantly outperforms a strong diffusion-only baseline, maintaining a
coherent visual context for an order of magnitude more steps. It delivers
consistent views in both a 2D maze navigation and a complex 3D environment.
These results establish that bringing state-space representations into
diffusion models is highly effective in demonstrating both visual details and
long-term memory.

</details>


### [274] [YH-MINER: Multimodal Intelligent System for Natural Ecological Reef Metric Extraction](https://arxiv.org/abs/2505.22250)
*Mingzhuang Wang,Yvyang Li,Xiyang Zhang,Fei Tan,Qi Shi,Guotao Zhang,Siqi Chen,Yufei Liu,Lei Lei,Ming Zhou,Qiang Lin,Hongqiang Yang*

Main category: cs.CV

TL;DR: 该研究开发了YH-OSI系统，利用多模态大模型（MLLM）实现珊瑚礁生态监测的智能化，结合目标检测和语义分割，提高了分类准确性和生态指标提取效率。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁生态监测面临人工分析效率低和复杂水下场景分割精度不足的问题，亟需高效解决方案。

Method: 系统基于MLLM框架，通过目标检测模块生成空间先验框，驱动分割模块完成像素级分割，并结合多模态模型实现分类和生态指标提取。

Result: 目标检测模块mAP@0.5为0.78，分类准确率达88%，并能提取核心生态指标。

Conclusion: YH-OSI系统为珊瑚礁监测提供了高效自动化解决方案，并为未来水下机器人集成奠定了基础。

Abstract: Coral reefs, crucial for sustaining marine biodiversity and ecological
processes (e.g., nutrient cycling, habitat provision), face escalating threats,
underscoring the need for efficient monitoring. Coral reef ecological
monitoring faces dual challenges of low efficiency in manual analysis and
insufficient segmentation accuracy in complex underwater scenarios. This study
develops the YH-OSI system, establishing an intelligent framework centered on
the Multimodal Large Model (MLLM) for "object detection-semantic
segmentation-prior input". The system uses the object detection module
(mAP@0.5=0.78) to generate spatial prior boxes for coral instances, driving the
segment module to complete pixel-level segmentation in low-light and densely
occluded scenarios. The segmentation masks and finetuned classification
instructions are fed into the Qwen2-VL-based multimodal model as prior inputs,
achieving a genus-level classification accuracy of 88% and simultaneously
extracting core ecological metrics. Meanwhile, the system retains the
scalability of the multimodal model through standardized interfaces, laying a
foundation for future integration into multimodal agent-based underwater robots
and supporting the full-process automation of "image acquisition-prior
generation-real-time analysis."

</details>


### [275] [Domain Adaptation of Attention Heads for Zero-shot Anomaly Detection](https://arxiv.org/abs/2505.22259)
*Kiyoon Jeong,Jaehyuk Heo,Junyeong Son,Pilsung Kang*

Main category: cs.CV

TL;DR: HeadCLIP提出了一种新的零样本异常检测方法，通过自适应调整文本和图像编码器，结合可学习提示和动态头权重，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本异常检测方法在域适应方面存在局限性，未充分调整模型组件以适应异常检测领域。

Method: HeadCLIP通过可学习提示调整文本编码器，引入动态头权重优化图像编码器，并结合像素级信息计算联合异常分数。

Result: 在工业和医疗领域的多个数据集上，HeadCLIP在像素和图像级别的异常检测得分均优于现有方法，最高提升4.9%p。

Conclusion: HeadCLIP通过全面域适应和联合评分机制，显著提升了零样本异常检测的性能。

Abstract: Zero-shot anomaly detection (ZSAD) in images is an approach that can detect
anomalies without access to normal samples, which can be beneficial in various
realistic scenarios where model training is not possible. However, existing
ZSAD research has shown limitations by either not considering domain adaptation
of general-purpose backbone models to anomaly detection domains or by
implementing only partial adaptation to some model components. In this paper,
we propose HeadCLIP to overcome these limitations by effectively adapting both
text and image encoders to the domain. HeadCLIP generalizes the concepts of
normality and abnormality through learnable prompts in the text encoder, and
introduces learnable head weights to the image encoder to dynamically adjust
the features held by each attention head according to domain characteristics.
Additionally, we maximize the effect of domain adaptation by introducing a
joint anomaly score that utilizes domain-adapted pixel-level information for
image-level anomaly detection. Experimental results using multiple real
datasets in both industrial and medical domains show that HeadCLIP outperforms
existing ZSAD techniques at both pixel and image levels. In the industrial
domain, improvements of up to 4.9%p in pixel-level mean anomaly detection score
(mAD) and up to 3.0%p in image-level mAD were achieved, with similar
improvements (3.2%p, 3.1%p) in the medical domain.

</details>


### [276] [Learning Fine-Grained Geometry for Sparse-View Splatting via Cascade Depth Loss](https://arxiv.org/abs/2505.22279)
*Wenjun Lu,Haodong Chen,Anqi Yi,Yuk Ying Chung,Zhiyong Wang,Kun Hu*

Main category: cs.CV

TL;DR: 论文提出了一种名为HDGS的深度监督框架，通过多尺度深度一致性提升稀疏视图下的新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图条件下，现有方法（如NeRF和3DGS）因几何线索不足导致重建质量下降，细节模糊和结构伪影问题突出。

Method: 引入Hierarchical Depth-Guided Splatting (HDGS)框架，采用Cascade Pearson Correlation Loss (CPCL)在多空间尺度上对齐渲染和估计的单目深度。

Result: 在LLFF和DTU基准测试中，HDGS在稀疏视图下实现了最先进的性能，同时保持高效和高质量的渲染。

Conclusion: HDGS通过多尺度深度一致性显著提升了稀疏视图场景下的结构保真度，为新视角合成提供了有效解决方案。

Abstract: Novel view synthesis is a fundamental task in 3D computer vision that aims to
reconstruct realistic images from a set of posed input views. However,
reconstruction quality degrades significantly under sparse-view conditions due
to limited geometric cues. Existing methods, such as Neural Radiance Fields
(NeRF) and the more recent 3D Gaussian Splatting (3DGS), often suffer from
blurred details and structural artifacts when trained with insufficient views.
Recent works have identified the quality of rendered depth as a key factor in
mitigating these artifacts, as it directly affects geometric accuracy and view
consistency. In this paper, we address these challenges by introducing
Hierarchical Depth-Guided Splatting (HDGS), a depth supervision framework that
progressively refines geometry from coarse to fine levels. Central to HDGS is a
novel Cascade Pearson Correlation Loss (CPCL), which aligns rendered and
estimated monocular depths across multiple spatial scales. By enforcing
multi-scale depth consistency, our method substantially improves structural
fidelity in sparse-view scenarios. Extensive experiments on the LLFF and DTU
benchmarks demonstrate that HDGS achieves state-of-the-art performance under
sparse-view settings while maintaining efficient and high-quality rendering

</details>


### [277] [From Controlled Scenarios to Real-World: Cross-Domain Degradation Pattern Matching for All-in-One Image Restoration](https://arxiv.org/abs/2505.22284)
*Junyu Fan,Chuanlin Liao,Yi Lin*

Main category: cs.CV

TL;DR: 论文提出了一种统一的域自适应图像修复框架（UDAIR），通过跨域知识迁移和对比学习机制，解决了多退化模式图像修复中的域适应问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在真实场景中因数据分布差异导致性能下降，需要一种能适应不同退化模式的统一修复框架。

Method: 设计了代码本学习离散嵌入表示退化模式，采用跨样本对比学习捕获共享特征，并提出域适应策略和测试时自适应机制。

Result: 在10个开源数据集上验证了UDAIR的优越性能，展示了在未知条件下的退化识别能力和真实场景的鲁棒泛化性。

Conclusion: UDAIR通过域适应和对比学习，显著提升了多退化图像修复的性能，适用于真实场景。

Abstract: As a fundamental imaging task, All-in-One Image Restoration (AiOIR) aims to
achieve image restoration caused by multiple degradation patterns via a single
model with unified parameters. Although existing AiOIR approaches obtain
promising performance in closed and controlled scenarios, they still suffered
from considerable performance reduction in real-world scenarios since the gap
of data distributions between the training samples (source domain) and
real-world test samples (target domain) can lead inferior degradation awareness
ability. To address this issue, a Unified Domain-Adaptive Image Restoration
(UDAIR) framework is proposed to effectively achieve AiOIR by leveraging the
learned knowledge from source domain to target domain. To improve the
degradation identification, a codebook is designed to learn a group of discrete
embeddings to denote the degradation patterns, and the cross-sample contrastive
learning mechanism is further proposed to capture shared features from
different samples of certain degradation. To bridge the data gap, a domain
adaptation strategy is proposed to build the feature projection between the
source and target domains by dynamically aligning their codebook embeddings,
and a correlation alignment-based test-time adaptation mechanism is designed to
fine-tune the alignment discrepancies by tightening the degradation embeddings
to the corresponding cluster center in the source domain. Experimental results
on 10 open-source datasets demonstrate that UDAIR achieves new state-of-the-art
performance for the AiOIR task. Most importantly, the feature cluster validate
the degradation identification under unknown conditions, and qualitative
comparisons showcase robust generalization to real-world scenarios.

</details>


### [278] [Neural Restoration of Greening Defects in Historical Autochrome Photographs Based on Purely Synthetic Data](https://arxiv.org/abs/2505.22291)
*Saptarshi Neil Sinha,P. Julius Kuehn,Johannes Koppe,Arjan Kuijper,Michael Weinmann*

Main category: cs.CV

TL;DR: 提出了一种基于合成数据集生成和生成式AI的方法，用于自动修复早期彩色照片中的绿色缺陷，解决了现有方法在颜色还原和效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 早期彩色照片因老化和存储不当导致的绿色缺陷问题，现有方法难以准确还原颜色且需要大量人工干预。

Method: 通过合成数据集生成模拟绿色缺陷，并采用改进的加权损失函数（ChaIR方法）进行修复。

Result: 实现了高效修复，减少了时间需求，且能更准确地还原原始颜色。

Conclusion: 该方法为视觉艺术的自动修复提供了一种高效且准确的解决方案。

Abstract: The preservation of early visual arts, particularly color photographs, is
challenged by deterioration caused by aging and improper storage, leading to
issues like blurring, scratches, color bleeding, and fading defects. In this
paper, we present the first approach for the automatic removal of greening
color defects in digitized autochrome photographs. Our main contributions
include a method based on synthetic dataset generation and the use of
generative AI with a carefully designed loss function for the restoration of
visual arts. To address the lack of suitable training datasets for analyzing
greening defects in damaged autochromes, we introduce a novel approach for
accurately simulating such defects in synthetic data. We also propose a
modified weighted loss function for the ChaIR method to account for color
imbalances between defected and non-defected areas. While existing methods
struggle with accurately reproducing original colors and may require
significant manual effort, our method allows for efficient restoration with
reduced time requirements.

</details>


### [279] [CADReview: Automatically Reviewing CAD Programs with Error Detection and Correction](https://arxiv.org/abs/2505.22304)
*Jiali Chen,Xusen Hei,HongFei Liu,Yuancheng Wei,Zikun Deng,Jiayuan Xie,Yi Cai,Li Qing*

Main category: cs.CV

TL;DR: 论文提出ReCAD框架，用于自动检测和修正CAD程序中的错误，确保3D对象与参考图像一致，优于现有MLLMs。


<details>
  <summary>Details</summary>
Motivation: 设计师在CAD原型设计时需耗费大量时间检查和修正，现有MLLMs在多几何组件识别和空间操作上表现不佳，需改进。

Method: 提出ReCAD框架，结合错误检测与修正反馈，并构建CADReview数据集（20K程序-图像对）。

Result: 实验表明ReCAD显著优于现有MLLMs，在设计应用中潜力巨大。

Conclusion: ReCAD为CAD程序错误检测与修正提供了高效解决方案，推动了设计自动化。

Abstract: Computer-aided design (CAD) is crucial in prototyping 3D objects through
geometric instructions (i.e., CAD programs). In practical design workflows,
designers often engage in time-consuming reviews and refinements of these
prototypes by comparing them with reference images. To bridge this gap, we
introduce the CAD review task to automatically detect and correct potential
errors, ensuring consistency between the constructed 3D objects and reference
images. However, recent advanced multimodal large language models (MLLMs)
struggle to recognize multiple geometric components and perform spatial
geometric operations within the CAD program, leading to inaccurate reviews. In
this paper, we propose the CAD program repairer (ReCAD) framework to
effectively detect program errors and provide helpful feedback on error
correction. Additionally, we create a dataset, CADReview, consisting of over
20K program-image pairs, with diverse errors for the CAD review task. Extensive
experiments demonstrate that our ReCAD significantly outperforms existing
MLLMs, which shows great potential in design applications.

</details>


### [280] [IKIWISI: An Interactive Visual Pattern Generator for Evaluating the Reliability of Vision-Language Models Without Ground Truth](https://arxiv.org/abs/2505.22305)
*Md Touhidul Islam,Imran Kabir,Md Alimoor Reza,Syed Masum Billah*

Main category: cs.CV

TL;DR: IKIWISI是一种交互式视觉模式生成器，用于在没有真实标签时评估视觉语言模型的视频对象识别能力，通过热图可视化模型输出，并利用人类模式识别能力评估模型可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法依赖真实标签，而IKIWISI旨在通过视觉化和人类直觉评估模型行为，尤其是在缺乏真实数据时。

Method: IKIWISI将模型输出转化为二元热图（绿色表示对象存在，红色表示不存在），并引入“间谍对象”检测模型幻觉行为。

Result: 15名参与者认为IKIWISI易用且评估结果与传统指标一致，仅需少量热图单元即可得出结论。

Conclusion: IKIWISI通过视觉评估补充传统方法，揭示了改进视觉语言系统中人机感知对齐的机会。

Abstract: We present IKIWISI ("I Know It When I See It"), an interactive visual pattern
generator for assessing vision-language models in video object recognition when
ground truth is unavailable. IKIWISI transforms model outputs into a binary
heatmap where green cells indicate object presence and red cells indicate
object absence. This visualization leverages humans' innate pattern recognition
abilities to evaluate model reliability. IKIWISI introduces "spy objects":
adversarial instances users know are absent, to discern models hallucinating on
nonexistent items. The tool functions as a cognitive audit mechanism, surfacing
mismatches between human and machine perception by visualizing where models
diverge from human understanding.
  Our study with 15 participants found that users considered IKIWISI easy to
use, made assessments that correlated with objective metrics when available,
and reached informed conclusions by examining only a small fraction of heatmap
cells. This approach not only complements traditional evaluation methods
through visual assessment of model behavior with custom object sets, but also
reveals opportunities for improving alignment between human perception and
machine understanding in vision-language systems.

</details>


### [281] [Learning to Infer Parameterized Representations of Plants from 3D Scans](https://arxiv.org/abs/2505.22337)
*Samara Ghrer,Christophe Godin,Stefanie Wuhrer*

Main category: cs.CV

TL;DR: 提出了一种统一的方法，通过3D扫描植物推断其参数化表示，适用于重建、分割和骨架化等多种任务。


<details>
  <summary>Details</summary>
Motivation: 植物3D重建因器官自遮挡和空间复杂性而具有挑战性，现有方法局限于逆向建模或特定任务。

Method: 使用基于L系统的程序模型生成虚拟植物，训练递归神经网络，从3D点云推断参数化树状表示。

Result: 在合成植物上验证，该方法在重建、分割和骨架化任务中表现与现有最佳方法相当。

Conclusion: 该方法为植物3D表示提供了统一框架，适用于多种任务。

Abstract: Reconstructing faithfully the 3D architecture of plants from unstructured
observations is a challenging task. Plants frequently contain numerous organs,
organized in branching systems in more or less complex spatial networks,
leading to specific computational issues due to self-occlusion or spatial
proximity between organs. Existing works either consider inverse modeling where
the aim is to recover the procedural rules that allow to simulate virtual
plants, or focus on specific tasks such as segmentation or skeletonization. We
propose a unified approach that, given a 3D scan of a plant, allows to infer a
parameterized representation of the plant. This representation describes the
plant's branching structure, contains parametric information for each plant
organ, and can therefore be used directly in a variety of tasks. In this
data-driven approach, we train a recursive neural network with virtual plants
generated using an L-systems-based procedural model. After training, the
network allows to infer a parametric tree-like representation based on an input
3D point cloud. Our method is applicable to any plant that can be represented
as binary axial tree. We evaluate our approach on Chenopodium Album plants,
using experiments on synthetic plants to show that our unified framework allows
for different tasks including reconstruction, segmentation and skeletonization,
while achieving results on-par with state-of-the-art for each task.

</details>


### [282] [Progressive Data Dropout: An Embarrassingly Simple Approach to Faster Training](https://arxiv.org/abs/2505.22342)
*Shriram M S,Xinyue Hao,Shihao Hou,Yang Lu,Laura Sevilla-Lara,Anurag Arnab,Shreyank N Gowda*

Main category: cs.CV

TL;DR: 论文提出了一种名为Progressive Data Dropout的新训练范式，通过减少有效训练轮数至基线的12.4%，同时提升准确率高达4.82%，且无需修改模型架构或优化器。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习依赖大规模数据集训练，成本高昂；传统方法均匀采样数据集效率低下，亟需改进。

Method: 结合硬数据挖掘和dropout的简单训练范式，减少有效训练轮数。

Result: 有效训练轮数降至12.4%，准确率提升4.82%，无需额外调整。

Conclusion: Progressive Data Dropout是一种高效、易实现的训练方法，具有广泛应用潜力。

Abstract: The success of the machine learning field has reliably depended on training
on large datasets. While effective, this trend comes at an extraordinary cost.
This is due to two deeply intertwined factors: the size of models and the size
of datasets. While promising research efforts focus on reducing the size of
models, the other half of the equation remains fairly mysterious. Indeed, it is
surprising that the standard approach to training remains to iterate over and
over, uniformly sampling the training dataset. In this paper we explore a
series of alternative training paradigms that leverage insights from
hard-data-mining and dropout, simple enough to implement and use that can
become the new training standard. The proposed Progressive Data Dropout reduces
the number of effective epochs to as little as 12.4% of the baseline. This
savings actually do not come at any cost for accuracy. Surprisingly, the
proposed method improves accuracy by up to 4.82%. Our approach requires no
changes to model architecture or optimizer, and can be applied across standard
training pipelines, thus posing an excellent opportunity for wide adoption.
Code can be found here: https://github.com/bazyagami/LearningWithRevision

</details>


### [283] [VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in the Middle East and Beyond](https://arxiv.org/abs/2505.22353)
*Noora Al-Emadi,Ingmar Weber,Yin Yang,Ferda Ofli*

Main category: cs.CV

TL;DR: 论文提出了VME数据集，专注于中东地区的车辆检测，并展示了其在提升该地区检测准确性的效果。同时，引入了CDSI基准数据集，显著提升了全球车辆检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有车辆检测模型在中东地区表现不佳，主要由于数据集的地理偏差。VME数据集旨在填补这一空白，提升中东地区的车辆检测能力。

Method: 构建了VME数据集，包含54个城市的高分辨率卫星图像，并采用手动和半自动标注方法。同时，整合多源数据创建了CDSI基准数据集。

Result: 实验表明，VME显著提升了中东地区的车辆检测准确性，而CDSI则显著提升了全球车辆检测性能。

Conclusion: VME和CDSI数据集有效解决了地理偏差问题，提升了车辆检测模型的泛化能力。

Abstract: Detecting vehicles in satellite images is crucial for traffic management,
urban planning, and disaster response. However, current models struggle with
real-world diversity, particularly across different regions. This challenge is
amplified by geographic bias in existing datasets, which often focus on
specific areas and overlook regions like the Middle East. To address this gap,
we present the Vehicles in the Middle East (VME) dataset, designed explicitly
for vehicle detection in high-resolution satellite images from Middle Eastern
countries. Sourced from Maxar, the VME dataset spans 54 cities across 12
countries, comprising over 4,000 image tiles and more than 100,000 vehicles,
annotated using both manual and semi-automated methods. Additionally, we
introduce the largest benchmark dataset for Car Detection in Satellite Imagery
(CDSI), combining images from multiple sources to enhance global car detection.
Our experiments demonstrate that models trained on existing datasets perform
poorly on Middle Eastern images, while the VME dataset significantly improves
detection accuracy in this region. Moreover, state-of-the-art models trained on
CDSI achieve substantial improvements in global car detection.

</details>


### [284] [Identity-Preserving Text-to-Image Generation via Dual-Level Feature Decoupling and Expert-Guided Fusion](https://arxiv.org/abs/2505.22360)
*Kewen Chen,Xiaobin Hu,Wenqi Ren*

Main category: cs.CV

TL;DR: 提出了一种新框架，通过解耦身份相关与无关特征，提升文本到图像生成的质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 当前方法难以区分输入图像中身份相关与无关信息，导致过拟合或身份丢失。

Method: 框架包含隐式-显式前景-背景解耦模块（IEDM）和基于专家混合（MoE）的特征融合模块（FFM），并引入三种损失函数指导解耦。

Result: 实验表明，该方法提升了生成图像质量、场景适应性和输出多样性。

Conclusion: 新框架有效解决了身份信息解耦问题，显著改善了文本到图像生成的效果。

Abstract: Recent advances in large-scale text-to-image generation models have led to a
surge in subject-driven text-to-image generation, which aims to produce
customized images that align with textual descriptions while preserving the
identity of specific subjects. Despite significant progress, current methods
struggle to disentangle identity-relevant information from identity-irrelevant
details in the input images, resulting in overfitting or failure to maintain
subject identity. In this work, we propose a novel framework that improves the
separation of identity-related and identity-unrelated features and introduces
an innovative feature fusion mechanism to improve the quality and text
alignment of generated images. Our framework consists of two key components: an
Implicit-Explicit foreground-background Decoupling Module (IEDM) and a Feature
Fusion Module (FFM) based on a Mixture of Experts (MoE). IEDM combines
learnable adapters for implicit decoupling at the feature level with inpainting
techniques for explicit foreground-background separation at the image level.
FFM dynamically integrates identity-irrelevant features with identity-related
features, enabling refined feature representations even in cases of incomplete
decoupling. In addition, we introduce three complementary loss functions to
guide the decoupling process. Extensive experiments demonstrate the
effectiveness of our proposed method in enhancing image generation quality,
improving flexibility in scene adaptation, and increasing the diversity of
generated outputs across various textual descriptions.

</details>


### [285] [DAM: Domain-Aware Module for Multi-Domain Dataset Condensation](https://arxiv.org/abs/2505.22387)
*Jaehyun Choi,Gyojin Han,Dong-Jae Lee,Sunghyun Baek,Junmo Kim*

Main category: cs.CV

TL;DR: 多领域数据集压缩（MDDC）通过领域感知模块（DAM）提升跨领域性能，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现代数据集多为多领域异构数据，现有数据集压缩方法未充分考虑此特性。

Method: 提出DAM模块，通过可学习空间掩码嵌入领域特征，并采用基于频率的伪领域标注。

Result: DAM在领域内、领域外及跨架构性能上均优于基线方法。

Conclusion: MDDC通过DAM有效解决了多领域数据集压缩问题，具有广泛适用性。

Abstract: Dataset Condensation (DC) has emerged as a promising solution to mitigate the
computational and storage burdens associated with training deep learning
models. However, existing DC methods largely overlook the multi-domain nature
of modern datasets, which are increasingly composed of heterogeneous images
spanning multiple domains. In this paper, we extend DC and introduce
Multi-Domain Dataset Condensation (MDDC), which aims to condense data that
generalizes across both single-domain and multi-domain settings. To this end,
we propose the Domain-Aware Module (DAM), a training-time module that embeds
domain-related features into each synthetic image via learnable spatial masks.
As explicit domain labels are mostly unavailable in real-world datasets, we
employ frequency-based pseudo-domain labeling, which leverages low-frequency
amplitude statistics. DAM is only active during the condensation process, thus
preserving the same images per class (IPC) with prior methods. Experiments show
that DAM consistently improves in-domain, out-of-domain, and cross-architecture
performance over baseline dataset condensation methods.

</details>


### [286] [PacTure: Efficient PBR Texture Generation on Packed Views with Visual Autoregressive Models](https://arxiv.org/abs/2505.22394)
*Fan Fei,Jiajun Tang,Fei-Peng Tian,Boxin Shi,Ping Tan*

Main category: cs.CV

TL;DR: PacTure是一个新框架，通过未纹理化的3D网格、文本描述和可选图像提示生成基于物理的渲染（PBR）材质纹理。它解决了现有方法在全局一致性和分辨率上的限制，并显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成纹理时存在全局不一致性和分辨率限制的问题，PacTure旨在通过新技术解决这些问题。

Method: 引入视图打包技术，将多视图映射视为2D矩形装箱问题，提升分辨率；结合多域生成和细粒度控制，构建高效的多视图多域生成框架。

Result: 实验表明，PacTure在生成PBR纹理的质量和训练/推理效率上均优于现有方法。

Conclusion: PacTure通过创新技术解决了纹理生成中的关键问题，显著提升了效果和效率。

Abstract: We present PacTure, a novel framework for generating physically-based
rendering (PBR) material textures from an untextured 3D mesh, a text
description, and an optional image prompt. Early 2D generation-based texturing
approaches generate textures sequentially from different views, resulting in
long inference times and globally inconsistent textures. More recent approaches
adopt multi-view generation with cross-view attention to enhance global
consistency, which, however, limits the resolution for each view. In response
to these weaknesses, we first introduce view packing, a novel technique that
significantly increases the effective resolution for each view during
multi-view generation without imposing additional inference cost, by
formulating the arrangement of multi-view maps as a 2D rectangle bin packing
problem. In contrast to UV mapping, it preserves the spatial proximity
essential for image generation and maintains full compatibility with current 2D
generative models. To further reduce the inference cost, we enable fine-grained
control and multi-domain generation within the next-scale prediction
autoregressive framework to create an efficient multi-view multi-domain
generative backbone. Extensive experiments show that PacTure outperforms
state-of-the-art methods in both quality of generated PBR textures and
efficiency in training and inference.

</details>


### [287] [Zooming from Context to Cue: Hierarchical Preference Optimization for Multi-Image MLLMs](https://arxiv.org/abs/2505.22396)
*Xudong Li,Mengdan Zhang,Peixian Chen,Xiawu Zheng,Yan Zhang,Jingyuan Zheng,Yunhang Shen,Ke Li,Chaoyou Fu,Xing Sun,Rongrong Ji*

Main category: cs.CV

TL;DR: CcDPO是一种多级偏好优化框架，通过从全局上下文到局部细节的视觉线索，提升多图像场景中的感知能力，显著减少幻觉并提升性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在多图像理解中存在跨模态对齐问题，导致幻觉现象（如上下文遗漏、混淆和误解），现有方法（如DPO）仅关注单图像参考，忽略了全局上下文建模。

Method: 提出CcDPO框架，包括上下文级优化（全局序列偏好）和细节级优化（区域目标视觉提示），并构建MultiScope-42k数据集支持优化。

Result: 实验表明CcDPO显著减少幻觉，并在单图像和多图像任务中表现一致提升。

Conclusion: CcDPO通过多级优化有效解决了MLLMs在多图像理解中的问题，为未来研究提供了新方向。

Abstract: Multi-modal Large Language Models (MLLMs) excel at single-image tasks but
struggle with multi-image understanding due to cross-modal misalignment,
leading to hallucinations (context omission, conflation, and
misinterpretation). Existing methods using Direct Preference Optimization (DPO)
constrain optimization to a solitary image reference within the input sequence,
neglecting holistic context modeling. We propose Context-to-Cue Direct
Preference Optimization (CcDPO), a multi-level preference optimization
framework that enhances per-image perception in multi-image settings by zooming
into visual clues -- from sequential context to local details. It features: (i)
Context-Level Optimization : Re-evaluates cognitive biases underlying MLLMs'
multi-image context comprehension and integrates a spectrum of low-cost global
sequence preferences for bias mitigation. (ii) Needle-Level Optimization :
Directs attention to fine-grained visual details through region-targeted visual
prompts and multimodal preference supervision. To support scalable
optimization, we also construct MultiScope-42k, an automatically generated
dataset with high-quality multi-level preference pairs. Experiments show that
CcDPO significantly reduces hallucinations and yields consistent performance
gains across general single- and multi-image tasks.

</details>


### [288] [Self-Reflective Reinforcement Learning for Diffusion-based Image Reasoning Generation](https://arxiv.org/abs/2505.22407)
*Jiadong Pan,Zhiyuan Ma,Kaiyan Zhang,Ning Ding,Bowen Zhou*

Main category: cs.CV

TL;DR: SRRL是一种自反思强化学习算法，用于扩散模型，通过在多轮去噪过程中引入反思和迭代，实现逻辑图像的推理生成。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成方法在逻辑为中心的图像生成任务中表现不佳，受Chain of Thought和强化学习的启发，提出SRRL以解决这一问题。

Method: SRRL将整个去噪轨迹视为多轮反思的Chain of Thought步骤，并引入条件引导的前向过程，实现反思迭代。

Result: 实验结果表明，SRRL在逻辑图像生成任务中表现优异，甚至优于GPT-4o。

Conclusion: SRRL首次将Chain of Thought推理引入图像生成任务，为遵循物理规律和非传统物理现象的生成提供了新方法。

Abstract: Diffusion models have recently demonstrated exceptional performance in image
generation task. However, existing image generation methods still significantly
suffer from the dilemma of image reasoning, especially in logic-centered image
generation tasks. Inspired by the success of Chain of Thought (CoT) and
Reinforcement Learning (RL) in LLMs, we propose SRRL, a self-reflective RL
algorithm for diffusion models to achieve reasoning generation of logical
images by performing reflection and iteration across generation trajectories.
The intermediate samples in the denoising process carry noise, making accurate
reward evaluation difficult. To address this challenge, SRRL treats the entire
denoising trajectory as a CoT step with multi-round reflective denoising
process and introduces condition guided forward process, which allows for
reflective iteration between CoT steps. Through SRRL-based iterative diffusion
training, we introduce image reasoning through CoT into generation tasks
adhering to physical laws and unconventional physical phenomena for the first
time. Notably, experimental results of case study exhibit that the superior
performance of our SRRL algorithm even compared with GPT-4o. The project page
is https://jadenpan0.github.io/srrl.github.io/.

</details>


### [289] [Frugal Incremental Generative Modeling using Variational Autoencoders](https://arxiv.org/abs/2505.22408)
*Victor Enescu,Hichem Sahbi*

Main category: cs.CV

TL;DR: 提出了一种基于变分自编码器（VAE）的无回放增量学习模型，解决了增量学习中数据增长和灾难性遗忘的问题。


<details>
  <summary>Details</summary>
Motivation: 增量学习在深度学习中潜力巨大，但面临灾难性遗忘和数据增长带来的可扩展性挑战。

Method: 设计了基于多模态潜在空间的新型增量生成模型，并引入正交性准则以减少VAE的灾难性遗忘。模型包括静态和动态两种VAE变体。

Result: 实验表明，该方法在内存占用上比相关研究节省至少一个数量级，同时达到最先进的准确率。

Conclusion: 该方法为增量学习提供了一种高效且可扩展的解决方案。

Abstract: Continual or incremental learning holds tremendous potential in deep learning
with different challenges including catastrophic forgetting. The advent of
powerful foundation and generative models has propelled this paradigm even
further, making it one of the most viable solution to train these models.
However, one of the persisting issues lies in the increasing volume of data
particularly with replay-based methods. This growth introduces challenges with
scalability since continuously expanding data becomes increasingly demanding as
the number of tasks grows. In this paper, we attenuate this issue by devising a
novel replay-free incremental learning model based on Variational Autoencoders
(VAEs). The main contribution of this work includes (i) a novel incremental
generative modelling, built upon a well designed multi-modal latent space, and
also (ii) an orthogonality criterion that mitigates catastrophic forgetting of
the learned VAEs. The proposed method considers two variants of these VAEs:
static and dynamic with no (or at most a controlled) growth in the number of
parameters. Extensive experiments show that our method is (at least) an order
of magnitude more ``memory-frugal'' compared to the closely related works while
achieving SOTA accuracy scores.

</details>


### [290] [RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network](https://arxiv.org/abs/2505.22427)
*Van-Tin Luu,Yon-Lin Cai,Vu-Hoang Tran,Wei-Chen Chiu,Yi-Ting Chen,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 提出了一种创新的在线自动雷达与相机几何标定方法，通过双视角表示和选择性融合机制解决雷达高度数据稀疏性和不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 雷达高度数据的稀疏性和不确定性使得系统运行时的自动标定成为长期挑战。

Method: 采用双视角表示（前视图和鸟瞰图），结合选择性融合机制和多模态交叉注意力机制，设计抗噪声匹配器增强鲁棒性。

Result: 在nuScenes数据集上显著优于现有雷达-相机和LiDAR-相机标定方法。

Conclusion: 该方法为未来研究设定了新基准，代码已开源。

Abstract: This paper presents a groundbreaking approach - the first online automatic
geometric calibration method for radar and camera systems. Given the
significant data sparsity and measurement uncertainty in radar height data,
achieving automatic calibration during system operation has long been a
challenge. To address the sparsity issue, we propose a Dual-Perspective
representation that gathers features from both frontal and bird's-eye views.
The frontal view contains rich but sensitive height information, whereas the
bird's-eye view provides robust features against height uncertainty. We thereby
propose a novel Selective Fusion Mechanism to identify and fuse reliable
features from both perspectives, reducing the effect of height uncertainty.
Moreover, for each view, we incorporate a Multi-Modal Cross-Attention Mechanism
to explicitly find location correspondences through cross-modal matching.
During the training phase, we also design a Noise-Resistant Matcher to provide
better supervision and enhance the robustness of the matching mechanism against
sparsity and height uncertainty. Our experimental results, tested on the
nuScenes dataset, demonstrate that our method significantly outperforms
previous radar-camera auto-calibration methods, as well as existing
state-of-the-art LiDAR-camera calibration techniques, establishing a new
benchmark for future research. The code is available at
https://github.com/nycu-acm/RC-AutoCalib.

</details>


### [291] [Distance Transform Guided Mixup for Alzheimer's Detection](https://arxiv.org/abs/2505.22434)
*Zobia Batool,Huseyin Ozkan,Erchan Aptoula*

Main category: cs.CV

TL;DR: 论文提出了一种基于距离变换和分层混合的单域泛化方法，用于解决阿尔茨海默病检测中的数据集不平衡和泛化问题。


<details>
  <summary>Details</summary>
Motivation: 医学数据集存在类别不平衡、成像协议差异和多样性不足的问题，限制了模型的泛化能力。

Method: 通过计算MRI扫描的距离变换，将其分层并结合不同样本的层生成增强图像，以保持大脑结构。

Result: 实验结果表明，该方法在ADNI和AIBL数据集上均提高了泛化性能。

Conclusion: 提出的方法有效解决了医学图像数据集的泛化问题，为阿尔茨海默病的早期检测提供了更可靠的模型。

Abstract: Alzheimer's detection efforts aim to develop accurate models for early
disease diagnosis. Significant advances have been achieved with convolutional
neural networks and vision transformer based approaches. However, medical
datasets suffer heavily from class imbalance, variations in imaging protocols,
and limited dataset diversity, which hinder model generalization. To overcome
these challenges, this study focuses on single-domain generalization by
extending the well-known mixup method. The key idea is to compute the distance
transform of MRI scans, separate them spatially into multiple layers and then
combine layers stemming from distinct samples to produce augmented images. The
proposed approach generates diverse data while preserving the brain's
structure. Experimental results show generalization performance improvement
across both ADNI and AIBL datasets.

</details>


### [292] [Can NeRFs See without Cameras?](https://arxiv.org/abs/2505.22441)
*Chaitanya Amballa,Sattwik Basu,Yu-Lin Wei,Zhijian Yang,Mehmet Ergezer,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 论文探讨了如何通过改进NeRF技术，利用多径信号（如WiFi）推断环境信息，并成功应用于室内平面图重建。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用多径信号（如RF/音频）推断环境信息，解决传统NeRF仅适用于光学信号的局限性。

Method: 改进NeRF技术，使其能够学习多径信号（如WiFi测量数据），并应用于室内平面图推断。

Result: 通过稀疏WiFi测量数据，成功重建了室内平面图，并支持信号预测和基本光线追踪等应用。

Conclusion: 改进后的NeRF技术能够有效利用多径信号推断环境信息，为室内场景理解提供了新方法。

Abstract: Neural Radiance Fields (NeRFs) have been remarkably successful at
synthesizing novel views of 3D scenes by optimizing a volumetric scene
function. This scene function models how optical rays bring color information
from a 3D object to the camera pixels. Radio frequency (RF) or audio signals
can also be viewed as a vehicle for delivering information about the
environment to a sensor. However, unlike camera pixels, an RF/audio sensor
receives a mixture of signals that contain many environmental reflections (also
called "multipath"). Is it still possible to infer the environment using such
multipath signals? We show that with redesign, NeRFs can be taught to learn
from multipath signals, and thereby "see" the environment. As a grounding
application, we aim to infer the indoor floorplan of a home from sparse WiFi
measurements made at multiple locations inside the home. Although a difficult
inverse problem, our implicitly learnt floorplans look promising, and enables
forward applications, such as indoor signal prediction and basic ray tracing.

</details>


### [293] [On Geometry-Enhanced Parameter-Efficient Fine-Tuning for 3D Scene Segmentation](https://arxiv.org/abs/2505.22444)
*Liyao Tang,Zhe Chen,Dacheng Tao*

Main category: cs.CV

TL;DR: 论文提出了一种几何感知的参数高效微调模块GEM，用于3D点云模型，解决了现有方法忽略局部空间结构和全局几何上下文的问题，性能接近或超过全微调，同时仅更新1.6%的参数。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练点云模型在3D场景理解中表现优异，但全微调计算和存储成本高。现有参数高效微调方法在3D点云模型中表现不佳，因其忽略了重要的几何和空间结构。

Method: 提出GEM模块，结合细粒度局部位置编码和轻量级潜在注意力机制，捕捉全局上下文，解决几何和空间分布不匹配问题。

Result: 实验表明，GEM性能接近或超过全微调，仅更新1.6%参数，训练时间和内存需求显著降低。

Conclusion: GEM为大规模3D点云模型的高效、可扩展和几何感知微调设定了新基准。

Abstract: The emergence of large-scale pre-trained point cloud models has significantly
advanced 3D scene understanding, but adapting these models to specific
downstream tasks typically demands full fine-tuning, incurring high
computational and storage costs. Parameter-efficient fine-tuning (PEFT)
techniques, successful in natural language processing and 2D vision tasks,
would underperform when naively applied to 3D point cloud models due to
significant geometric and spatial distribution shifts. Existing PEFT methods
commonly treat points as orderless tokens, neglecting important local spatial
structures and global geometric contexts in 3D modeling. To bridge this gap, we
introduce the Geometric Encoding Mixer (GEM), a novel geometry-aware PEFT
module specifically designed for 3D point cloud transformers. GEM explicitly
integrates fine-grained local positional encodings with a lightweight latent
attention mechanism to capture comprehensive global context, thereby
effectively addressing the spatial and geometric distribution mismatch.
Extensive experiments demonstrate that GEM achieves performance comparable to
or sometimes even exceeding full fine-tuning, while only updating 1.6% of the
model's parameters, fewer than other PEFT methods. With significantly reduced
training time and memory requirements, our approach thus sets a new benchmark
for efficient, scalable, and geometry-aware fine-tuning of large-scale 3D point
cloud models. Code will be released.

</details>


### [294] [NFR: Neural Feature-Guided Non-Rigid Shape Registration](https://arxiv.org/abs/2505.22445)
*Puhua Jiang,Zhangquan Chen,Mingze Sun,Ruqi Huang*

Main category: cs.CV

TL;DR: 提出了一种基于学习的3D形状配准框架，无需标注对应关系即可处理非刚性变形和部分形状匹配问题。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在处理显著非刚性变形和部分形状匹配时的局限性。

Method: 结合深度学习形状匹配网络的神经特征与几何配准流程，动态更新对应关系并通过一致性先验过滤。

Result: 在多个基准测试中取得最优性能，并能处理未见过的复杂变形形状对。

Conclusion: 该框架在非刚性点云匹配和部分形状匹配中表现出色，具有广泛应用潜力。

Abstract: In this paper, we propose a novel learning-based framework for 3D shape
registration, which overcomes the challenges of significant non-rigid
deformation and partiality undergoing among input shapes, and, remarkably,
requires no correspondence annotation during training. Our key insight is to
incorporate neural features learned by deep learning-based shape matching
networks into an iterative, geometric shape registration pipeline. The
advantage of our approach is two-fold -- On one hand, neural features provide
more accurate and semantically meaningful correspondence estimation than
spatial features (e.g., coordinates), which is critical in the presence of
large non-rigid deformations; On the other hand, the correspondences are
dynamically updated according to the intermediate registrations and filtered by
consistency prior, which prominently robustify the overall pipeline. Empirical
results show that, with as few as dozens of training shapes of limited
variability, our pipeline achieves state-of-the-art results on several
benchmarks of non-rigid point cloud matching and partial shape matching across
varying settings, but also delivers high-quality correspondences between unseen
challenging shape pairs that undergo both significant extrinsic and intrinsic
deformations, in which case neither traditional registration methods nor
intrinsic methods work.

</details>


### [295] [Fostering Video Reasoning via Next-Event Prediction](https://arxiv.org/abs/2505.22457)
*Haonan Wang,Hongfu Liu,Xiangyan Liu,Chao Du,Kenji Kawaguchi,Ye Wang,Tianyu Pang*

Main category: cs.CV

TL;DR: 论文提出了一种名为“下一事件预测”（NEP）的自监督学习任务，旨在提升多模态语言模型（MLLMs）在视频输入上的时间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有任务（如视频问答）依赖人工标注或更强模型，而视频描述则混淆了时间推理与空间信息。NEP通过利用未来视频片段作为自监督信号，填补了这一空白。

Method: 将视频分为过去和未来帧，MLLM以过去帧为输入，预测未来帧的事件摘要，从而促进时间推理。为此，构建了包含33,000个视频片段的V1-33K数据集，并探索了多种视频指令调优策略。

Result: 实验验证了NEP作为一种可扩展且有效的训练范式，能够显著提升MLLMs的时间推理能力。

Conclusion: NEP为MLLMs的时间推理提供了一种新的自监督学习框架，并通过FutureBench评估验证了其有效性。

Abstract: Next-token prediction serves as the foundational learning task enabling
reasoning in LLMs. But what should the learning task be when aiming to equip
MLLMs with temporal reasoning capabilities over video inputs? Existing tasks
such as video question answering often rely on annotations from humans or much
stronger MLLMs, while video captioning tends to entangle temporal reasoning
with spatial information. To address this gap, we propose next-event prediction
(NEP), a learning task that harnesses future video segments as a rich,
self-supervised signal to foster temporal reasoning. We segment each video into
past and future frames: the MLLM takes the past frames as input and predicts a
summary of events derived from the future frames, thereby encouraging the model
to reason temporally in order to complete the task. To support this task, we
curate V1-33K, a dataset comprising 33,000 automatically extracted video
segments spanning diverse real-world scenarios. We further explore a range of
video instruction-tuning strategies to study their effects on temporal
reasoning. To evaluate progress, we introduce FutureBench to assess coherence
in predicting unseen future events. Experiments validate that NEP offers a
scalable and effective training paradigm for fostering temporal reasoning in
MLLMs.

</details>


### [296] [Universal Domain Adaptation for Semantic Segmentation](https://arxiv.org/abs/2505.22458)
*Seun-An Choe,Keon-Hee Park,Jinwoo Choi,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 论文提出了一种新的无监督域适应方法UniDA-SS，通过UniMAP框架解决类别设置未知时的语义分割问题。


<details>
  <summary>Details</summary>
Motivation: 传统UDA-SS方法假设源域和目标域的类别设置已知，这在现实中不成立，导致性能下降。

Method: 提出UniMAP框架，包含域特定原型区分（DSPD）和目标图像匹配（TIM）两个关键组件。

Result: 实验表明UniMAP显著优于基线方法。

Conclusion: UniMAP在类别设置未知的情况下实现了鲁棒的域适应。

Abstract: Unsupervised domain adaptation for semantic segmentation (UDA-SS) aims to
transfer knowledge from labeled source data to unlabeled target data. However,
traditional UDA-SS methods assume that category settings between source and
target domains are known, which is unrealistic in real-world scenarios. This
leads to performance degradation if private classes exist. To address this
limitation, we propose Universal Domain Adaptation for Semantic Segmentation
(UniDA-SS), achieving robust adaptation even without prior knowledge of
category settings. We define the problem in the UniDA-SS scenario as low
confidence scores of common classes in the target domain, which leads to
confusion with private classes. To solve this problem, we propose UniMAP:
UniDA-SS with Image Matching and Prototype-based Distinction, a novel framework
composed of two key components. First, Domain-Specific Prototype-based
Distinction (DSPD) divides each class into two domain-specific prototypes,
enabling finer separation of domain-specific features and enhancing the
identification of common classes across domains. Second, Target-based Image
Matching (TIM) selects a source image containing the most common-class pixels
based on the target pseudo-label and pairs it in a batch to promote effective
learning of common classes. We also introduce a new UniDA-SS benchmark and
demonstrate through various experiments that UniMAP significantly outperforms
baselines. The code is available at
\href{https://github.com/KU-VGI/UniMAP}{this https URL}.

</details>


### [297] [SHTOcc: Effective 3D Occupancy Prediction with Sparse Head and Tail Voxels](https://arxiv.org/abs/2505.22461)
*Qiucheng Yu,Yuan Xie,Xin Tan*

Main category: cs.CV

TL;DR: SHTOcc通过稀疏头尾体素构建和分离学习，解决了3D占用预测中的长尾问题和几何分布问题，显著提升了性能、速度和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法未探索体素的关键分布模式，导致结果不理想。本文旨在解决体素类别间分布的长尾问题和几何分布导致的性能问题。

Method: 提出SHTOcc方法，利用稀疏头尾体素构建平衡关键体素，并通过分离学习减少模型对主导类别的偏好，增强对尾部类别的关注。

Result: 实验显示，SHTOcc在多个基准上显著改进：GPU内存使用减少42.2%，推理速度提升58.6%，准确率提高约7%。

Conclusion: SHTOcc在3D占用预测中表现出高效性和有效性，代码已开源。

Abstract: 3D occupancy prediction has attracted much attention in the field of
autonomous driving due to its powerful geometric perception and object
recognition capabilities. However, existing methods have not explored the most
essential distribution patterns of voxels, resulting in unsatisfactory results.
This paper first explores the inter-class distribution and geometric
distribution of voxels, thereby solving the long-tail problem caused by the
inter-class distribution and the poor performance caused by the geometric
distribution. Specifically, this paper proposes SHTOcc (Sparse Head-Tail
Occupancy), which uses sparse head-tail voxel construction to accurately
identify and balance key voxels in the head and tail classes, while using
decoupled learning to reduce the model's bias towards the dominant (head)
category and enhance the focus on the tail class. Experiments show that
significant improvements have been made on multiple baselines: SHTOcc reduces
GPU memory usage by 42.2%, increases inference speed by 58.6%, and improves
accuracy by about 7%, verifying its effectiveness and efficiency. The code is
available at https://github.com/ge95net/SHTOcc

</details>


### [298] [Single Domain Generalization for Alzheimer's Detection from 3D MRIs with Pseudo-Morphological Augmentations and Contrastive Learning](https://arxiv.org/abs/2505.22465)
*Zobia Batool,Huseyin Ozkan,Erchan Aptoula*

Main category: cs.CV

TL;DR: 论文提出了一种结合可学习的伪形态学模块和监督对比学习的方法，以解决阿尔茨海默病MRI检测中的类别不平衡和协议差异问题，提升了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在阿尔茨海默病MRI检测中存在类别不平衡、协议差异和数据集多样性不足等问题，限制了其泛化能力。

Method: 提出使用可学习的伪形态学模块生成形状感知的解剖学有意义的数据增强，并结合监督对比学习模块提取鲁棒的类别特定表示。

Result: 在三个数据集上的实验表明，该方法在类别不平衡和成像协议差异下表现出更好的性能和泛化能力。

Conclusion: 该方法有效提升了阿尔茨海默病MRI检测的泛化能力，尤其在数据分布差异大的情况下表现优异。

Abstract: Although Alzheimer's disease detection via MRIs has advanced significantly
thanks to contemporary deep learning models, challenges such as class
imbalance, protocol variations, and limited dataset diversity often hinder
their generalization capacity. To address this issue, this article focuses on
the single domain generalization setting, where given the data of one domain, a
model is designed and developed with maximal performance w.r.t. an unseen
domain of distinct distribution. Since brain morphology is known to play a
crucial role in Alzheimer's diagnosis, we propose the use of learnable
pseudo-morphological modules aimed at producing shape-aware, anatomically
meaningful class-specific augmentations in combination with a supervised
contrastive learning module to extract robust class-specific representations.
Experiments conducted across three datasets show improved performance and
generalization capacity, especially under class imbalance and imaging protocol
variations. The source code will be made available upon acceptance at
https://github.com/zobia111/SDG-Alzheimer.

</details>


### [299] [ProCrop: Learning Aesthetic Image Cropping from Professional Compositions](https://arxiv.org/abs/2505.22490)
*Ke Zhang,Tianyu Ding,Jiachen Jiang,Tianyi Chen,Ilya Zharkov,Vishal M. Patel,Luming Liang*

Main category: cs.CV

TL;DR: ProCrop是一种基于检索的图像裁剪方法，利用专业摄影指导裁剪决策，显著提升性能，并提供了一个大规模弱标注数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则和数据驱动的图像裁剪方法缺乏多样性或需要标注数据，ProCrop旨在通过学习专业摄影构图解决这一问题。

Method: ProCrop通过融合专业照片和查询图像的特征，学习专业构图，并利用大规模弱标注数据集生成多样化高质量裁剪建议。

Result: ProCrop在监督和弱监督设置下显著优于现有方法，甚至与完全监督方法媲美。

Conclusion: ProCrop和其数据集将公开，推动图像美学和构图分析研究。

Abstract: Image cropping is crucial for enhancing the visual appeal and narrative
impact of photographs, yet existing rule-based and data-driven approaches often
lack diversity or require annotated training data. We introduce ProCrop, a
retrieval-based method that leverages professional photography to guide
cropping decisions. By fusing features from professional photographs with those
of the query image, ProCrop learns from professional compositions,
significantly boosting performance. Additionally, we present a large-scale
dataset of 242K weakly-annotated images, generated by out-painting professional
images and iteratively refining diverse crop proposals. This composition-aware
dataset generation offers diverse high-quality crop proposals guided by
aesthetic principles and becomes the largest publicly available dataset for
image cropping. Extensive experiments show that ProCrop significantly
outperforms existing methods in both supervised and weakly-supervised settings.
Notably, when trained on the new dataset, our ProCrop surpasses previous
weakly-supervised methods and even matches fully supervised approaches. Both
the code and dataset will be made publicly available to advance research in
image aesthetics and composition analysis.

</details>


### [300] [The Meeseeks Mesh: Spatially Consistent 3D Adversarial Objects for BEV Detector](https://arxiv.org/abs/2505.22499)
*Aixuan Li,Mochu Xiang,Jing Zhang,Yuchao Dai*

Main category: cs.CV

TL;DR: 该论文研究了3D对抗攻击在3D物体检测中的影响，提出了一种生成非侵入式3D对抗对象的方法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 3D物体检测在自动驾驶中至关重要，但其对对抗攻击的脆弱性尚未充分研究。通过生成3D对抗对象，可以评估模型的鲁棒性。

Method: 采用可微分渲染技术建模对抗对象与目标车辆的空间关系，引入遮挡感知模块增强视觉一致性，并设计BEV空间特征优化策略。

Result: 实验表明，该方法能有效抑制先进3D检测器的预测，且对抗对象在不同位置和距离下仍保持攻击效果。

Conclusion: 该方法为评估3D检测模型的鲁棒性提供了重要工具，生成的对抗对象具有强泛化能力。

Abstract: 3D object detection is a critical component in autonomous driving systems. It
allows real-time recognition and detection of vehicles, pedestrians and
obstacles under varying environmental conditions. Among existing methods, 3D
object detection in the Bird's Eye View (BEV) has emerged as the mainstream
framework. To guarantee a safe, robust and trustworthy 3D object detection, 3D
adversarial attacks are investigated, where attacks are placed in 3D
environments to evaluate the model performance, e.g., putting a film on a car,
clothing a pedestrian. The vulnerability of 3D object detection models to 3D
adversarial attacks serves as an important indicator to evaluate the robustness
of the model against perturbations. To investigate this vulnerability, we
generate non-invasive 3D adversarial objects tailored for real-world attack
scenarios. Our method verifies the existence of universal adversarial objects
that are spatially consistent across time and camera views. Specifically, we
employ differentiable rendering techniques to accurately model the spatial
relationship between adversarial objects and the target vehicle. Furthermore,
we introduce an occlusion-aware module to enhance visual consistency and
realism under different viewpoints. To maintain attack effectiveness across
multiple frames, we design a BEV spatial feature-guided optimization strategy.
Experimental results demonstrate that our approach can reliably suppress
vehicle predictions from state-of-the-art 3D object detectors, serving as an
important tool to test robustness of 3D object detection models before
deployment. Moreover, the generated adversarial objects exhibit strong
generalization capabilities, retaining its effectiveness at various positions
and distances in the scene.

</details>


### [301] [PathFL: Multi-Alignment Federated Learning for Pathology Image Segmentation](https://arxiv.org/abs/2505.22522)
*Yuan Zhang,Feng Chen,Yaolei Qi,Guanyu Yang,Huazhu Fu*

Main category: cs.CV

TL;DR: PathFL是一种新颖的多对齐联邦学习框架，通过图像、特征和模型聚合的三级对齐策略，解决了病理图像分割中的异构性问题。


<details>
  <summary>Details</summary>
Motivation: 病理图像分割在多中心研究中面临异构性挑战，包括成像模态、器官和扫描设备的多样性，导致表示偏差和模型泛化能力受限。

Method: PathFL采用三级对齐策略：图像级通过协作风格增强模块对齐数据；特征级通过自适应特征对齐模块实现隐式对齐；模型级通过分层相似性聚合策略增强全局泛化。

Result: 在四种异构病理图像数据集上的评估表明，PathFL在性能和鲁棒性方面优于其他方法。

Conclusion: PathFL通过多级对齐策略有效解决了病理图像分割中的异构性问题，提升了模型的泛化能力和鲁棒性。

Abstract: Pathology image segmentation across multiple centers encounters significant
challenges due to diverse sources of heterogeneity including imaging
modalities, organs, and scanning equipment, whose variability brings
representation bias and impedes the development of generalizable segmentation
models. In this paper, we propose PathFL, a novel multi-alignment Federated
Learning framework for pathology image segmentation that addresses these
challenges through three-level alignment strategies of image, feature, and
model aggregation. Firstly, at the image level, a collaborative style
enhancement module aligns and diversifies local data by facilitating style
information exchange across clients. Secondly, at the feature level, an
adaptive feature alignment module ensures implicit alignment in the
representation space by infusing local features with global insights, promoting
consistency across heterogeneous client features learning. Finally, at the
model aggregation level, a stratified similarity aggregation strategy
hierarchically aligns and aggregates models on the server, using layer-specific
similarity to account for client discrepancies and enhance global
generalization. Comprehensive evaluations on four sets of heterogeneous
pathology image datasets, encompassing cross-source, cross-modality,
cross-organ, and cross-scanner variations, validate the effectiveness of our
PathFL in achieving better performance and robustness against data
heterogeneity.

</details>


### [302] [PrismLayers: Open Data for High-Quality Multi-Layer Transparent Image Generative Models](https://arxiv.org/abs/2505.22523)
*Junwen Chen,Heyang Jiang,Yanbin Wang,Keming Wu,Ji Li,Chao Zhang,Keiji Yanai,Dong Chen,Yuhui Yuan*

Main category: cs.CV

TL;DR: 论文提出了一种生成高质量多层透明图像的方法，通过发布数据集PrismLayersPro、训练免费合成管道和开源模型ART+，解决了多层生成模型发展滞后的问题。


<details>
  <summary>Details</summary>
Motivation: 解决多层透明图像生成模型因缺乏高质量数据集而发展滞后的问题。

Method: 1. 发布PrismLayersPro数据集；2. 提出训练免费合成管道；3. 开发开源模型ART+，包含LayerFLUX和MultiLayerFLUX技术。

Result: ART+在用户研究中60%情况下优于原ART模型，视觉质量与FLUX.1-[dev]模型相当。

Conclusion: 该工作为多层透明图像生成任务奠定了数据集基础，支持需要精确、可编辑和视觉吸引力的多层图像的研究与应用。

Abstract: Generating high-quality, multi-layer transparent images from text prompts can
unlock a new level of creative control, allowing users to edit each layer as
effortlessly as editing text outputs from LLMs. However, the development of
multi-layer generative models lags behind that of conventional text-to-image
models due to the absence of a large, high-quality corpus of multi-layer
transparent data. In this paper, we address this fundamental challenge by: (i)
releasing the first open, ultra-high-fidelity PrismLayers (PrismLayersPro)
dataset of 200K (20K) multilayer transparent images with accurate alpha mattes,
(ii) introducing a trainingfree synthesis pipeline that generates such data on
demand using off-the-shelf diffusion models, and (iii) delivering a strong,
open-source multi-layer generation model, ART+, which matches the aesthetics of
modern text-to-image generation models. The key technical contributions
include: LayerFLUX, which excels at generating high-quality single transparent
layers with accurate alpha mattes, and MultiLayerFLUX, which composes multiple
LayerFLUX outputs into complete images, guided by human-annotated semantic
layout. To ensure higher quality, we apply a rigorous filtering stage to remove
artifacts and semantic mismatches, followed by human selection. Fine-tuning the
state-of-the-art ART model on our synthetic PrismLayersPro yields ART+, which
outperforms the original ART in 60% of head-to-head user study comparisons and
even matches the visual quality of images generated by the FLUX.1-[dev] model.
We anticipate that our work will establish a solid dataset foundation for the
multi-layer transparent image generation task, enabling research and
applications that require precise, editable, and visually compelling layered
imagery.

</details>


### [303] [Thinking with Generated Images](https://arxiv.org/abs/2505.22525)
*Ethan Chern,Zhulin Hu,Steffi Chern,Siqi Kou,Jiadi Su,Yan Ma,Zhijie Deng,Pengfei Liu*

Main category: cs.CV

TL;DR: 提出了一种新范式，通过生成中间视觉思考步骤，使多模态模型能在视觉推理中主动构建和优化视觉假设。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型的视觉推理局限于处理固定图像或纯文本推理，缺乏动态视觉生成能力。

Method: 通过两种机制实现：1）分解视觉任务为子目标并逐步生成；2）生成视觉假设后自我批判并优化。

Result: 在复杂多对象场景中，模型性能相对提升50%（从38%到57%）。

Conclusion: 该方法为AI模型提供了类似人类的视觉想象和迭代优化能力，适用于多个领域。

Abstract: We present Thinking with Generated Images, a novel paradigm that
fundamentally transforms how large multimodal models (LMMs) engage with visual
reasoning by enabling them to natively think across text and vision modalities
through spontaneous generation of intermediate visual thinking steps. Current
visual reasoning with LMMs is constrained to either processing fixed
user-provided images or reasoning solely through text-based chain-of-thought
(CoT). Thinking with Generated Images unlocks a new dimension of cognitive
capability where models can actively construct intermediate visual thoughts,
critique their own visual hypotheses, and refine them as integral components of
their reasoning process. We demonstrate the effectiveness of our approach
through two complementary mechanisms: (1) vision generation with intermediate
visual subgoals, where models decompose complex visual tasks into manageable
components that are generated and integrated progressively, and (2) vision
generation with self-critique, where models generate an initial visual
hypothesis, analyze its shortcomings through textual reasoning, and produce
refined outputs based on their own critiques. Our experiments on vision
generation benchmarks show substantial improvements over baseline approaches,
with our models achieving up to 50% (from 38% to 57%) relative improvement in
handling complex multi-object scenarios. From biochemists exploring novel
protein structures, and architects iterating on spatial designs, to forensic
analysts reconstructing crime scenes, and basketball players envisioning
strategic plays, our approach enables AI models to engage in the kind of visual
imagination and iterative refinement that characterizes human creative,
analytical, and strategic thinking. We release our open-source suite at
https://github.com/GAIR-NLP/thinking-with-generated-images.

</details>


### [304] [RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting](https://arxiv.org/abs/2505.22535)
*Mohamad Hakam Shams Eddin,Yikui Zahng,Stefan Kollet,Juergen Gall*

Main category: cs.CV

TL;DR: RiverMamba是一种新型深度学习模型，通过预训练和时空建模提升全球河流流量和洪水预测能力，优于现有AI和物理模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在洪水预测中局限于局部应用，未能利用水体空间联系，需开发能建模时空关系的新方法。

Method: RiverMamba利用Mamba块和长期再分析数据预训练，整合ECMWF HRES气象预报，通过时空建模提升预测能力。

Result: 模型在0.05°网格上实现7天提前预测，对极端洪水表现优异，超越现有AI和物理模型。

Conclusion: RiverMamba为科学和业务应用提供了可靠的全球河流流量和洪水预测工具。

Abstract: Recent deep learning approaches for river discharge forecasting have improved
the accuracy and efficiency in flood forecasting, enabling more reliable early
warning systems for risk management. Nevertheless, existing deep learning
approaches in hydrology remain largely confined to local-scale applications and
do not leverage the inherent spatial connections of bodies of water. Thus,
there is a strong need for new deep learning methodologies that are capable of
modeling spatio-temporal relations to improve river discharge and flood
forecasting for scientific and operational applications. To address this, we
present RiverMamba, a novel deep learning model that is pretrained with
long-term reanalysis data and that can forecast global river discharge and
floods on a $0.05^\circ$ grid up to 7 days lead time, which is of high
relevance in early warning. To achieve this, RiverMamba leverages efficient
Mamba blocks that enable the model to capture global-scale channel network
routing and enhance its forecast capability for longer lead times. The forecast
blocks integrate ECMWF HRES meteorological forecasts, while accounting for
their inaccuracies through spatio-temporal modeling. Our analysis demonstrates
that RiverMamba delivers reliable predictions of river discharge, including
extreme floods across return periods and lead times, surpassing both
operational AI- and physics-based models.

</details>


### [305] [Scaling-up Perceptual Video Quality Assessment](https://arxiv.org/abs/2505.22543)
*Ziheng Jia,Zicheng Zhang,Zeyu Zhang,Yingji Liang,Xiaorong Zhu,Chunyi Li,Jinliang Han,Haoning Wu,Bin Wang,Haoran Zhang,Guanyu Zhu,Qiyong Zhao,Xiaohong Liu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 论文提出OmniVQA框架，构建大规模多模态指令数据库（MIDB）以解决视频质量评估（VQA）领域数据稀缺问题，并开发了OmniVQA-Chat-400K和OmniVQA-MOS-20K数据集。通过互补训练策略和细粒度评估基准，模型在质量和评分任务中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决VQA领域因数据稀缺和数据集规模不足而未能充分利用数据扩展潜力的问题。

Method: 提出OmniVQA框架，构建MIDB和数据集（OmniVQA-Chat-400K和OmniVQA-MOS-20K），采用互补训练策略，并引入细粒度评估基准。

Result: 模型在质量和评分任务中达到最优性能。

Conclusion: OmniVQA框架和数据集有效解决了VQA领域的数据扩展问题，提升了模型性能。

Abstract: The data scaling law has been shown to significantly enhance the performance
of large multi-modal models (LMMs) across various downstream tasks. However, in
the domain of perceptual video quality assessment (VQA), the potential of
scaling law remains unprecedented due to the scarcity of labeled resources and
the insufficient scale of datasets. To address this, we propose
\textbf{OmniVQA}, an efficient framework designed to efficiently build
high-quality, human-in-the-loop VQA multi-modal instruction databases (MIDBs).
We then scale up to create \textbf{OmniVQA-Chat-400K}, the largest MIDB in the
VQA field concurrently. Our focus is on the technical and aesthetic quality
dimensions, with abundant in-context instruction data to provide fine-grained
VQA knowledge. Additionally, we have built the \textbf{OmniVQA-MOS-20K} dataset
to enhance the model's quantitative quality rating capabilities. We then
introduce a \textbf{complementary} training strategy that effectively leverages
the knowledge from datasets for quality understanding and quality rating tasks.
Furthermore, we propose the \textbf{OmniVQA-FG (fine-grain)-Benchmark} to
evaluate the fine-grained performance of the models. Our results demonstrate
that our models achieve state-of-the-art performance in both quality
understanding and rating tasks.

</details>


### [306] [Deep Learning-Based BMD Estimation from Radiographs with Conformal Uncertainty Quantification](https://arxiv.org/abs/2505.22551)
*Long Hui,Wai Lok Yeung*

Main category: cs.CV

TL;DR: 利用深度学习通过膝关节X光片估计骨密度（BMD），提出了一种基于EfficientNet模型的方法，并比较了两种测试时间增强（TTA）方法，同时采用Split Conformal Prediction提供统计上严格的预测区间。


<details>
  <summary>Details</summary>
Motivation: DXA设备有限，阻碍了骨质疏松筛查。本研究旨在利用广泛可用的膝关节X光片，通过深度学习估计BMD，为临床提供可靠的筛查工具。

Method: 使用OAI数据集训练EfficientNet模型，预测双侧膝关节X光片的BMD。比较了两种TTA方法（传统平均和多样本方法），并采用Split Conformal Prediction生成预测区间。

Result: Pearson相关系数为0.68（传统TTA）。多样本方法生成的置信区间更紧（90%、95%、99%），同时保持覆盖率。模型对困难案例表现出更高的不确定性。

Conclusion: 尽管膝关节X光片与标准DXA存在解剖学差异，但该方法为利用常规X光片进行可信赖的AI辅助BMD筛查奠定了基础，有望改善早期骨质疏松检测。

Abstract: Limited DXA access hinders osteoporosis screening. This proof-of-concept
study proposes using widely available knee X-rays for opportunistic Bone
Mineral Density (BMD) estimation via deep learning, emphasizing robust
uncertainty quantification essential for clinical use. An EfficientNet model
was trained on the OAI dataset to predict BMD from bilateral knee radiographs.
Two Test-Time Augmentation (TTA) methods were compared: traditional averaging
and a multi-sample approach. Crucially, Split Conformal Prediction was
implemented to provide statistically rigorous, patient-specific prediction
intervals with guaranteed coverage. Results showed a Pearson correlation of
0.68 (traditional TTA). While traditional TTA yielded better point predictions,
the multi-sample approach produced slightly tighter confidence intervals (90%,
95%, 99%) while maintaining coverage. The framework appropriately expressed
higher uncertainty for challenging cases. Although anatomical mismatch between
knee X-rays and standard DXA limits immediate clinical use, this method
establishes a foundation for trustworthy AI-assisted BMD screening using
routine radiographs, potentially improving early osteoporosis detection.

</details>


### [307] [MultiFormer: A Multi-Person Pose Estimation System Based on CSI and Attention Mechanism](https://arxiv.org/abs/2505.22555)
*Yanyi Qu,Haoyang Ma,Wenhui Xiong*

Main category: cs.CV

TL;DR: MultiFormer是一种基于CSI的无线传感系统，通过Transformer和特征融合网络实现高精度人体姿态估计。


<details>
  <summary>Details</summary>
Motivation: 解决多人体姿态识别和CSI特征学习的挑战，提升非侵入式人体活动监测的准确性。

Method: 采用基于Transformer的时频双令牌特征提取器和多阶段特征融合网络（MSFN）来建模CSI特征并融合姿态热图。

Result: 在公开和自采集数据集上表现优于现有方法，尤其在高移动性关键点（如手腕、肘部）上表现突出。

Conclusion: MultiFormer通过创新的特征提取和融合方法，显著提升了基于CSI的人体姿态估计精度。

Abstract: Human pose estimation based on Channel State Information (CSI) has emerged as
a promising approach for non-intrusive and precise human activity monitoring,
yet faces challenges including accurate multi-person pose recognition and
effective CSI feature learning. This paper presents MultiFormer, a wireless
sensing system that accurately estimates human pose through CSI. The proposed
system adopts a Transformer based time-frequency dual-token feature extractor
with multi-head self-attention. This feature extractor is able to model
inter-subcarrier correlations and temporal dependencies of the CSI. The
extracted CSI features and the pose probability heatmaps are then fused by
Multi-Stage Feature Fusion Network (MSFN) to enforce the anatomical
constraints. Extensive experiments conducted on on the public MM-Fi dataset and
our self-collected dataset show that the MultiFormer achieves higher accuracy
over state-of-the-art approaches, especially for high-mobility keypoints
(wrists, elbows) that are particularly difficult for previous methods to
accurately estimate.

</details>


### [308] [PRISM: Video Dataset Condensation with Progressive Refinement and Insertion for Sparse Motion](https://arxiv.org/abs/2505.22564)
*Jaehyun Choi,Jiwan Hur,Gyojin Han,Jaemyung Yu,Junmo Kim*

Main category: cs.CV

TL;DR: PRISM是一种新的视频数据集压缩方法，通过渐进式细化和插入稀疏运动帧，保留空间内容和时间动态的相互依赖关系，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大规模视频数据处理中的计算挑战，同时保留视频中空间内容和时间动态的复杂关系。

Method: 提出PRISM方法，通过渐进式细化和插入稀疏运动帧，结合每帧的梯度关系，压缩视频数据。

Result: 在标准视频动作识别基准测试中，PRISM表现优于现有方法，同时保持紧凑的存储需求。

Conclusion: PRISM为视频数据集压缩提供了一种高效且紧凑的解决方案，适用于资源受限环境。

Abstract: Video dataset condensation has emerged as a critical technique for addressing
the computational challenges associated with large-scale video data processing
in deep learning applications. While significant progress has been made in
image dataset condensation, the video domain presents unique challenges due to
the complex interplay between spatial content and temporal dynamics. This paper
introduces PRISM, Progressive Refinement and Insertion for Sparse Motion, for
video dataset condensation, a novel approach that fundamentally reconsiders how
video data should be condensed. Unlike the previous method that separates
static content from dynamic motion, our method preserves the essential
interdependence between these elements. Our approach progressively refines and
inserts frames to fully accommodate the motion in an action while achieving
better performance but less storage, considering the relation of gradients for
each frame. Extensive experiments across standard video action recognition
benchmarks demonstrate that PRISM outperforms existing disentangled approaches
while maintaining compact representations suitable for resource-constrained
environments.

</details>


### [309] [Universal Visuo-Tactile Video Understanding for Embodied Interaction](https://arxiv.org/abs/2505.22566)
*Yifan Xie,Mingyang Li,Shoujie Li,Xingting Li,Guangyu Chen,Fei Ma,Fei Richard Yu,Wenbo Ding*

Main category: cs.CV

TL;DR: VTV-LLM是一个多模态大语言模型，首次实现了视觉-触觉视频（VTV）的通用理解，填补了触觉感知与自然语言之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉和语言模态上取得了进展，但未能有效整合触觉信息，而触觉信息对真实世界交互至关重要。

Method: 提出了VTV150K数据集，包含15万帧视频，来自100种不同物体和三种触觉传感器。开发了三阶段训练范式：VTV增强、VTV-文本对齐和文本提示微调。

Result: VTV-LLM在触觉视频理解任务中表现优异，支持特征评估、比较分析和场景决策等高级触觉推理能力。

Conclusion: 该框架为触觉领域的人机交互奠定了更直观的基础。

Abstract: Tactile perception is essential for embodied agents to understand physical
attributes of objects that cannot be determined through visual inspection
alone. While existing approaches have made progress in visual and language
modalities for physical understanding, they fail to effectively incorporate
tactile information that provides crucial haptic feedback for real-world
interaction. In this paper, we present VTV-LLM, the first multi-modal large
language model for universal Visuo-Tactile Video (VTV) understanding that
bridges the gap between tactile perception and natural language. To address the
challenges of cross-sensor and cross-modal integration, we contribute VTV150K,
a comprehensive dataset comprising 150,000 video frames from 100 diverse
objects captured across three different tactile sensors (GelSight Mini, DIGIT,
and Tac3D), annotated with four fundamental tactile attributes (hardness,
protrusion, elasticity, and friction). We develop a novel three-stage training
paradigm that includes VTV enhancement for robust visuo-tactile representation,
VTV-text alignment for cross-modal correspondence, and text prompt finetuning
for natural language generation. Our framework enables sophisticated tactile
reasoning capabilities including feature assessment, comparative analysis,
scenario-based decision making and so on. Experimental evaluations demonstrate
that VTV-LLM achieves superior performance in tactile video understanding
tasks, establishing a foundation for more intuitive human-machine interaction
in tactile domains.

</details>


### [310] [ImageReFL: Balancing Quality and Diversity in Human-Aligned Diffusion Models](https://arxiv.org/abs/2505.22569)
*Dmitrii Sorokin,Maksim Nakhodnov,Andrey Kuznetsov,Aibek Alanov*

Main category: cs.CV

TL;DR: 论文提出两种方法改进扩散模型与人类偏好的对齐，同时保持多样性：1）结合生成策略，仅在生成后期应用奖励调优模型；2）ImageReFL方法，通过真实图像训练和多正则化提升多样性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面表现优异，但与人类偏好的对齐仍具挑战性，且奖励调优常损害多样性。

Method: 1）结合生成策略：后期应用奖励调优模型；2）ImageReFL：真实图像训练，结合扩散和ReFL损失。

Result: 方法在质量和多样性指标上优于传统奖励调优，用户研究证实其平衡了偏好与多样性。

Conclusion: 提出的方法有效解决了对齐与多样性的权衡问题，代码已开源。

Abstract: Recent advances in diffusion models have led to impressive image generation
capabilities, but aligning these models with human preferences remains
challenging. Reward-based fine-tuning using models trained on human feedback
improves alignment but often harms diversity, producing less varied outputs. In
this work, we address this trade-off with two contributions. First, we
introduce \textit{combined generation}, a novel sampling strategy that applies
a reward-tuned diffusion model only in the later stages of the generation
process, while preserving the base model for earlier steps. This approach
mitigates early-stage overfitting and helps retain global structure and
diversity. Second, we propose \textit{ImageReFL}, a fine-tuning method that
improves image diversity with minimal loss in quality by training on real
images and incorporating multiple regularizers, including diffusion and ReFL
losses. Our approach outperforms conventional reward tuning methods on standard
quality and diversity metrics. A user study further confirms that our method
better balances human preference alignment and visual diversity. The source
code can be found at https://github.com/ControlGenAI/ImageReFL .

</details>


### [311] [Tell me Habibi, is it Real or Fake?](https://arxiv.org/abs/2505.22581)
*Kartik Kuckreja,Parul Gupta,Injy Hamed,Thamar Solorio,Muhammad Haris Khan,Abhinav Dhall*

Main category: cs.CV

TL;DR: 论文介绍了首个大规模阿拉伯语-英语视听深度伪造数据集ArEnAV，包含387k视频和765小时内容，支持多语言和多模态检测研究。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测研究多关注单语内容，忽视了多语言和语码转换（如阿拉伯语-英语混合）的挑战。

Method: 通过整合四种TTS和两种唇同步模型生成数据集，并对比现有单语和多语数据集及检测模型。

Result: ArEnAV数据集为多语言深度伪造检测提供了新资源，并在实验中展示了其潜力。

Conclusion: 该数据集有望推动多语言深度伪造检测研究的发展。

Abstract: Deepfake generation methods are evolving fast, making fake media harder to
detect and raising serious societal concerns. Most deepfake detection and
dataset creation research focuses on monolingual content, often overlooking the
challenges of multilingual and code-switched speech, where multiple languages
are mixed within the same discourse. Code-switching, especially between Arabic
and English, is common in the Arab world and is widely used in digital
communication. This linguistic mixing poses extra challenges for deepfake
detection, as it can confuse models trained mostly on monolingual data. To
address this, we introduce \textbf{ArEnAV}, the first large-scale
Arabic-English audio-visual deepfake dataset featuring intra-utterance
code-switching, dialectal variation, and monolingual Arabic content. It
\textbf{contains 387k videos and over 765 hours of real and fake videos}. Our
dataset is generated using a novel pipeline integrating four Text-To-Speech and
two lip-sync models, enabling comprehensive analysis of multilingual multimodal
deepfake detection. We benchmark our dataset against existing monolingual and
multilingual datasets, state-of-the-art deepfake detection models, and a human
evaluation, highlighting its potential to advance deepfake research. The
dataset can be accessed
\href{https://huggingface.co/datasets/kartik060702/ArEnAV-Full}{here}.

</details>


### [312] [SAM-R1: Leveraging SAM for Reward Feedback in Multimodal Segmentation via Reinforcement Learning](https://arxiv.org/abs/2505.22596)
*Jiaqi Huang,Zunnan Xu,Jun Zhou,Ting Liu,Yicheng Xiao,Mingwen Ou,Bowen Ji,Xiu Li,Kehong Yuan*

Main category: cs.CV

TL;DR: SAM-R1是一种新型框架，通过强化学习赋予多模态大模型细粒度推理能力，无需依赖昂贵的人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工标注的推理数据，成本高且耗时。强化学习可以赋予模型推理能力而无需此类数据。

Method: 结合细粒度分割设置，利用任务特定的奖励和优化目标，并借助Segment Anything Model（SAM）作为奖励提供者。

Result: 仅用3k训练样本，SAM-R1在多个基准测试中表现优异。

Conclusion: 强化学习能有效为多模态模型提供面向分割的推理能力。

Abstract: Leveraging multimodal large models for image segmentation has become a
prominent research direction. However, existing approaches typically rely
heavily on manually annotated datasets that include explicit reasoning
processes, which are costly and time-consuming to produce. Recent advances
suggest that reinforcement learning (RL) can endow large models with reasoning
capabilities without requiring such reasoning-annotated data. In this paper, we
propose SAM-R1, a novel framework that enables multimodal large models to
perform fine-grained reasoning in image understanding tasks. Our approach is
the first to incorporate fine-grained segmentation settings during the training
of multimodal reasoning models. By integrating task-specific, fine-grained
rewards with a tailored optimization objective, we further enhance the model's
reasoning and segmentation alignment. We also leverage the Segment Anything
Model (SAM) as a strong and flexible reward provider to guide the learning
process. With only 3k training samples, SAM-R1 achieves strong performance
across multiple benchmarks, demonstrating the effectiveness of reinforcement
learning in equipping multimodal models with segmentation-oriented reasoning
capabilities.

</details>


### [313] [Adversarially Robust AI-Generated Image Detection for Free: An Information Theoretic Perspective](https://arxiv.org/abs/2505.22604)
*Ruixuan Zhang,He Wang,Zhengyu Zhao,Zhiqing Guo,Xun Yang,Yunfeng Diao,Meng Wang*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练的对抗防御方法TRIM，用于检测AI生成图像（AIGI），通过信息论方法解决特征纠缠问题，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: AIGI的恶意使用（如伪造和虚假信息）日益严重，现有检测器对对抗攻击普遍脆弱，且防御方法稀缺。

Method: 提出TRIM方法，基于信息论度量（预测熵和KL散度）量化特征偏移，无需额外训练。

Result: 在多个数据集和攻击场景下，TRIM表现优异，例如在ProGAN和GenImage上分别比现有最佳防御方法提升33.88%和28.91%。

Conclusion: TRIM是一种高效且无需训练的对抗防御方法，显著提升了AIGI检测的鲁棒性。

Abstract: Rapid advances in Artificial Intelligence Generated Images (AIGI) have
facilitated malicious use, such as forgery and misinformation. Therefore,
numerous methods have been proposed to detect fake images. Although such
detectors have been proven to be universally vulnerable to adversarial attacks,
defenses in this field are scarce. In this paper, we first identify that
adversarial training (AT), widely regarded as the most effective defense,
suffers from performance collapse in AIGI detection. Through an
information-theoretic lens, we further attribute the cause of collapse to
feature entanglement, which disrupts the preservation of feature-label mutual
information. Instead, standard detectors show clear feature separation.
Motivated by this difference, we propose Training-free Robust Detection via
Information-theoretic Measures (TRIM), the first training-free adversarial
defense for AIGI detection. TRIM builds on standard detectors and quantifies
feature shifts using prediction entropy and KL divergence. Extensive
experiments across multiple datasets and attacks validate the superiority of
our TRIM, e.g., outperforming the state-of-the-art defense by 33.88% (28.91%)
on ProGAN (GenImage), while well maintaining original accuracy.

</details>


### [314] [RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction](https://arxiv.org/abs/2505.22613)
*Yuchi Wang,Yishuo Cai,Shuhuai Ren,Sihan Yang,Linli Yao,Yuanxin Liu,Yuanxing Zhang,Pengfei Wan,Xu Sun*

Main category: cs.CV

TL;DR: RICO框架通过视觉重建优化图像描述，利用文本到图像模型生成参考图像，并通过多模态大语言模型（MLLM）迭代识别差异以改进描述。RICO-Flash通过DPO学习生成类似RICO的描述，显著提升了描述的准确性和完整性。


<details>
  <summary>Details</summary>
Motivation: 现有图像重描述方法依赖多模态大语言模型（MLLM），但常因幻觉和细节缺失导致不准确或不完整。

Method: 提出RICO框架，通过文本到图像模型重建参考图像，并利用MLLM迭代识别差异以优化描述。RICO-Flash通过DPO减少计算成本。

Result: 实验表明，RICO在CapsBench和CompreCap上比基线方法提升约10%。

Conclusion: RICO显著提高了图像描述的准确性和完整性，并通过RICO-Flash优化了计算效率。

Abstract: Image recaptioning is widely used to generate training datasets with enhanced
quality for various multimodal tasks. Existing recaptioning methods typically
rely on powerful multimodal large language models (MLLMs) to enhance textual
descriptions, but often suffer from inaccuracies due to hallucinations and
incompleteness caused by missing fine-grained details. To address these
limitations, we propose RICO, a novel framework that refines captions through
visual reconstruction. Specifically, we leverage a text-to-image model to
reconstruct a caption into a reference image, and prompt an MLLM to identify
discrepancies between the original and reconstructed images to refine the
caption. This process is performed iteratively, further progressively promoting
the generation of more faithful and comprehensive descriptions. To mitigate the
additional computational cost induced by the iterative process, we introduce
RICO-Flash, which learns to generate captions like RICO using DPO. Extensive
experiments demonstrate that our approach significantly improves caption
accuracy and completeness, outperforms most baselines by approximately 10% on
both CapsBench and CompreCap. Code released at
https://github.com/wangyuchi369/RICO.

</details>


### [315] [ObjectClear: Complete Object Removal via Object-Effect Attention](https://arxiv.org/abs/2505.22636)
*Jixin Zhao,Shangchen Zhou,Zhouxia Wang,Peiqing Yang,Chen Change Loy*

Main category: cs.CV

TL;DR: 论文提出了一种新数据集OBER和框架ObjectClear，用于解决扩散修复方法在去除物体及其效果时的问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散修复方法在去除物体及其效果（如阴影和反射）时存在伪影、内容幻觉和背景改变等问题。

Method: 引入OBER数据集，提供成对图像和精确掩码；提出ObjectClear框架，利用物体效果注意力机制和注意力引导融合策略。

Result: ObjectClear在复杂场景中表现优于现有方法，提高了物体效果去除质量和背景保真度。

Conclusion: OBER数据集和ObjectClear框架有效解决了物体效果去除的挑战，尤其在复杂场景中表现突出。

Abstract: Object removal requires eliminating not only the target object but also its
effects, such as shadows and reflections. However, diffusion-based inpainting
methods often produce artifacts, hallucinate content, alter background, and
struggle to remove object effects accurately. To address this challenge, we
introduce a new dataset for OBject-Effect Removal, named OBER, which provides
paired images with and without object effects, along with precise masks for
both objects and their associated visual artifacts. The dataset comprises
high-quality captured and simulated data, covering diverse object categories
and complex multi-object scenes. Building on OBER, we propose a novel
framework, ObjectClear, which incorporates an object-effect attention mechanism
to guide the model toward the foreground removal regions by learning attention
masks, effectively decoupling foreground removal from background
reconstruction. Furthermore, the predicted attention map enables an
attention-guided fusion strategy during inference, greatly preserving
background details. Extensive experiments demonstrate that ObjectClear
outperforms existing methods, achieving improved object-effect removal quality
and background fidelity, especially in complex scenarios.

</details>


### [316] [SPIRAL: Semantic-Aware Progressive LiDAR Scene Generation](https://arxiv.org/abs/2505.22643)
*Dekai Zhu,Yixuan Hu,Youquan Liu,Dongyue Lu,Lingdong Kong,Slobodan Ilic*

Main category: cs.CV

TL;DR: Spiral是一种基于扩散模型的LiDAR范围视图生成方法，同时生成深度、反射率和语义图，解决了现有方法在跨模态一致性上的不足，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有范围视图方法无法生成带语义标签的LiDAR场景，依赖预训练分割模型会导致跨模态一致性不佳。

Method: 提出Spiral模型，结合扩散模型同时生成深度、反射率和语义图，并引入新的语义感知评估指标。

Result: 在SemanticKITTI和nuScenes数据集上表现最优，参数规模最小，且生成的数据可用于下游分割任务的数据增强。

Conclusion: Spiral在生成带语义标签的LiDAR场景上具有高效性和优越性，为下游任务提供了有效支持。

Abstract: Leveraging recent diffusion models, LiDAR-based large-scale 3D scene
generation has achieved great success. While recent voxel-based approaches can
generate both geometric structures and semantic labels, existing range-view
methods are limited to producing unlabeled LiDAR scenes. Relying on pretrained
segmentation models to predict the semantic maps often results in suboptimal
cross-modal consistency. To address this limitation while preserving the
advantages of range-view representations, such as computational efficiency and
simplified network design, we propose Spiral, a novel range-view LiDAR
diffusion model that simultaneously generates depth, reflectance images, and
semantic maps. Furthermore, we introduce novel semantic-aware metrics to
evaluate the quality of the generated labeled range-view data. Experiments on
the SemanticKITTI and nuScenes datasets demonstrate that Spiral achieves
state-of-the-art performance with the smallest parameter size, outperforming
two-step methods that combine the generative and segmentation models.
Additionally, we validate that range images generated by Spiral can be
effectively used for synthetic data augmentation in the downstream segmentation
training, significantly reducing the labeling effort on LiDAR data.

</details>


### [317] [Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation](https://arxiv.org/abs/2505.22647)
*Zhe Kong,Feng Gao,Yong Zhang,Zhuoliang Kang,Xiaoming Wei,Xunliang Cai,Guanying Chen,Wenhan Luo*

Main category: cs.CV

TL;DR: 论文提出了一种新任务——多人对话视频生成，并提出了MultiTalk框架，解决了多人生成中的音频绑定和指令跟随问题。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动的人体动画方法主要针对单人，难以处理多流音频输入和音频与人的绑定问题，且指令跟随能力有限。

Method: 提出了Label Rotary Position Embedding (L-RoPE)方法解决音频绑定问题，并通过部分参数训练和多任务训练保留基础模型的指令跟随能力。

Result: MultiTalk在多个数据集上表现优异，展示了强大的生成能力。

Conclusion: MultiTalk框架有效解决了多人对话视频生成中的关键问题，性能优于现有方法。

Abstract: Audio-driven human animation methods, such as talking head and talking body
generation, have made remarkable progress in generating synchronized facial
movements and appealing visual quality videos. However, existing methods
primarily focus on single human animation and struggle with multi-stream audio
inputs, facing incorrect binding problems between audio and persons.
Additionally, they exhibit limitations in instruction-following capabilities.
To solve this problem, in this paper, we propose a novel task: Multi-Person
Conversational Video Generation, and introduce a new framework, MultiTalk, to
address the challenges during multi-person generation. Specifically, for audio
injection, we investigate several schemes and propose the Label Rotary Position
Embedding (L-RoPE) method to resolve the audio and person binding problem.
Furthermore, during training, we observe that partial parameter training and
multi-task training are crucial for preserving the instruction-following
ability of the base model. MultiTalk achieves superior performance compared to
other methods on several datasets, including talking head, talking body, and
multi-person datasets, demonstrating the powerful generation capabilities of
our approach.

</details>


### [318] [Sherlock: Self-Correcting Reasoning in Vision-Language Models](https://arxiv.org/abs/2505.22651)
*Yi Ding,Ruqi Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为Sherlock的自校正框架，通过轨迹级自校正目标和视觉扰动构建偏好数据，显著提升了视觉语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在推理任务中容易出错，依赖大量标注数据或验证器，且泛化能力有限。

Method: 引入Sherlock框架，包括轨迹级自校正目标、基于视觉扰动的偏好数据构建方法，以及动态β偏好调整。

Result: 在八个基准测试中，Sherlock平均准确率达到64.1（直接生成）和65.4（自校正后），优于其他模型，且仅需20k标注数据。

Conclusion: Sherlock通过自校正和自我改进，显著提升了视觉语言模型的推理性能和数据效率。

Abstract: Reasoning Vision-Language Models (VLMs) have shown promising performance on
complex multimodal tasks. However, they still face significant challenges: they
are highly sensitive to reasoning errors, require large volumes of annotated
data or accurate verifiers, and struggle to generalize beyond specific domains.
To address these limitations, we explore self-correction as a strategy to
enhance reasoning VLMs. We first conduct an in-depth analysis of reasoning
VLMs' self-correction abilities and identify key gaps. Based on our findings,
we introduce Sherlock, a self-correction and self-improvement training
framework. Sherlock introduces a trajectory-level self-correction objective, a
preference data construction method based on visual perturbation, and a dynamic
$\beta$ for preference tuning. Once the model acquires self-correction
capabilities using only 20k randomly sampled annotated data, it continues to
self-improve without external supervision. Built on the Llama3.2-Vision-11B
model, Sherlock achieves remarkable results across eight benchmarks, reaching
an average accuracy of 64.1 with direct generation and 65.4 after
self-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and
LlamaV-o1 (63.4) while using less than 20% of the annotated data.

</details>


### [319] [VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models](https://arxiv.org/abs/2505.22654)
*Ce Zhang,Kaixin Ma,Tianqing Fang,Wenhao Yu,Hongming Zhang,Zhisong Zhang,Yaqi Xie,Katia Sycara,Haitao Mi,Dong Yu*

Main category: cs.CV

TL;DR: VScan是一种两阶段视觉令牌减少框架，通过全局和局部扫描结合令牌合并，以及在语言模型中间层剪枝，显著加速推理并保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）因视觉令牌序列较长导致计算成本高，难以实时部署，需优化令牌处理效率。

Method: 提出VScan框架：1）在视觉编码阶段结合全局和局部扫描并合并令牌；2）在语言模型中间层剪枝。

Result: 在四个LVLMs上验证，VScan显著加速推理（LLaVA-NeXT-7B预填充速度提升2.91倍，FLOPs减少10倍），性能保留95.4%。

Conclusion: VScan通过优化令牌处理，在加速推理的同时保持高性能，优于现有方法。

Abstract: Recent Large Vision-Language Models (LVLMs) have advanced multi-modal
understanding by incorporating finer-grained visual perception and encoding.
However, such methods incur significant computational costs due to longer
visual token sequences, posing challenges for real-time deployment. To mitigate
this, prior studies have explored pruning unimportant visual tokens either at
the output layer of the visual encoder or at the early layers of the language
model. In this work, we revisit these design choices and reassess their
effectiveness through comprehensive empirical studies of how visual tokens are
processed throughout the visual encoding and language decoding stages. Guided
by these insights, we propose VScan, a two-stage visual token reduction
framework that addresses token redundancy by: (1) integrating complementary
global and local scans with token merging during visual encoding, and (2)
introducing pruning at intermediate layers of the language model. Extensive
experimental results across four LVLMs validate the effectiveness of VScan in
accelerating inference and demonstrate its superior performance over current
state-of-the-arts on sixteen benchmarks. Notably, when applied to
LLaVA-NeXT-7B, VScan achieves a 2.91$\times$ speedup in prefilling and a
10$\times$ reduction in FLOPs, while retaining 95.4% of the original
performance.

</details>


### [320] [3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model](https://arxiv.org/abs/2505.22657)
*Wenbo Hu,Yining Hong,Yanjun Wang,Leison Gao,Zibu Wei,Xingcheng Yao,Nanyun Peng,Yonatan Bitton,Idan Szpektor,Kai-Wei Chang*

Main category: cs.CV

TL;DR: 论文提出了3DMem-Bench基准和3DLLM-Mem模型，用于解决LLMs在3D环境中时空记忆建模的不足，显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在动态3D环境中缺乏有效的时空记忆建模能力，限制了其规划和行动能力。

Method: 提出3DMem-Bench基准和3DLLM-Mem模型，通过工作记忆令牌动态选择和融合时空特征。

Result: 3DLLM-Mem在3DMem-Bench上表现最佳，任务成功率提升16.5%。

Conclusion: 3DLLM-Mem有效解决了LLMs在3D环境中的记忆管理问题，提升了任务性能。

Abstract: Humans excel at performing complex tasks by leveraging long-term memory
across temporal and spatial experiences. In contrast, current Large Language
Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D
environments. We posit that part of this limitation is due to the lack of
proper 3D spatial-temporal memory modeling in LLMs. To address this, we first
introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000
trajectories and 2,892 embodied tasks, question-answering and captioning,
designed to evaluate an agent's ability to reason over long-term memory in 3D
environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management
and fusion model for embodied spatial-temporal reasoning and actions in LLMs.
Our model uses working memory tokens, which represents current observations, as
queries to selectively attend to and fuse the most useful spatial and temporal
features from episodic memory, which stores past observations and interactions.
Our approach allows the agent to focus on task-relevant information while
maintaining memory efficiency in complex, long-horizon environments.
Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art
performance across various tasks, outperforming the strongest baselines by
16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied
tasks.

</details>


### [321] [Training Free Stylized Abstraction](https://arxiv.org/abs/2505.22663)
*Aimon Rahman,Kartik Narayan,Vishal M. Patel*

Main category: cs.CV

TL;DR: 提出了一种无需训练的框架，通过视觉语言模型和跨域校正流反转策略生成风格化抽象图像，支持多轮生成并引入StyleBench评估。


<details>
  <summary>Details</summary>
Motivation: 风格化抽象需要在保留身份特征的同时实现风格化，现有方法难以处理分布外个体。

Method: 使用视觉语言模型提取身份特征，结合跨域校正流反转策略动态重建结构，支持多轮生成。

Result: 在多种风格（如LEGO、针织玩偶、南方公园）上表现优异，泛化能力强。

Conclusion: 该方法无需微调即可生成高保真风格化抽象图像，适用于多样风格和身份。

Abstract: Stylized abstraction synthesizes visually exaggerated yet semantically
faithful representations of subjects, balancing recognizability with perceptual
distortion. Unlike image-to-image translation, which prioritizes structural
fidelity, stylized abstraction demands selective retention of identity cues
while embracing stylistic divergence, especially challenging for
out-of-distribution individuals. We propose a training-free framework that
generates stylized abstractions from a single image using inference-time
scaling in vision-language models (VLLMs) to extract identity-relevant
features, and a novel cross-domain rectified flow inversion strategy that
reconstructs structure based on style-dependent priors. Our method adapts
structural restoration dynamically through style-aware temporal scheduling,
enabling high-fidelity reconstructions that honor both subject and style. It
supports multi-round abstraction-aware generation without fine-tuning. To
evaluate this task, we introduce StyleBench, a GPT-based human-aligned metric
suited for abstract styles where pixel-level similarity fails. Experiments
across diverse abstraction (e.g., LEGO, knitted dolls, South Park) show strong
generalization to unseen identities and styles in a fully open-source setup.

</details>


### [322] [Zero-Shot Vision Encoder Grafting via LLM Surrogates](https://arxiv.org/abs/2505.22664)
*Kaiyu Yue,Vasu Singla,Menglin Jia,John Kirchenbauer,Rifaa Qadri,Zikui Cai,Abhinav Bhatele,Furong Huang,Tom Goldstein*

Main category: cs.CV

TL;DR: 通过使用小型语言模型训练视觉编码器，再将其转移到大型语言模型（如Llama-70B），显著降低了视觉语言模型的训练成本（约45%），同时性能接近完整训练。


<details>
  <summary>Details</summary>
Motivation: 降低视觉语言模型（VLM）的训练成本，同时保持性能。

Method: 构建小型“代理模型”，继承目标大型语言模型的浅层，训练视觉编码器后直接转移到大型模型（零-shot嫁接）。

Result: 嫁接后的模型性能超越编码器-代理模型对，部分基准测试中甚至与完整训练相当，训练成本降低约45%。

Conclusion: 零-shot嫁接是一种高效且经济的VLM训练策略，显著降低成本的同时保持性能。

Abstract: Vision language models (VLMs) typically pair a modestly sized vision encoder
with a large language model (LLM), e.g., Llama-70B, making the decoder the
primary computational burden during training. To reduce costs, a potential
promising strategy is to first train the vision encoder using a small language
model before transferring it to the large one. We construct small "surrogate
models" that share the same embedding space and representation language as the
large target LLM by directly inheriting its shallow layers. Vision encoders
trained on the surrogate can then be directly transferred to the larger model,
a process we call zero-shot grafting -- when plugged directly into the
full-size target LLM, the grafted pair surpasses the encoder-surrogate pair
and, on some benchmarks, even performs on par with full decoder training with
the target LLM. Furthermore, our surrogate training approach reduces overall
VLM training costs by ~45% when using Llama-70B as the decoder.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [323] [High-Fidelity Functional Ultrasound Reconstruction via A Visual Auto-Regressive Framework](https://arxiv.org/abs/2505.21530)
*Xuhang Chen,Zhuo Li,Yanyan Shen,Mufti Mahmud,Hieu Pham,Chi-Man Pun,Shuqiang Wang*

Main category: eess.IV

TL;DR: fUS成像在神经血管映射中具有高分辨率，但数据稀缺和颅骨信号衰减限制了其应用和机器学习模型的公平性。


<details>
  <summary>Details</summary>
Motivation: 解决fUS成像中因数据稀缺和颅骨信号衰减导致的应用限制和模型公平性问题。

Method: 未明确提及具体方法，但提到数据稀缺和信号衰减是主要挑战。

Result: 数据稀缺和信号衰减限制了fUS成像的多样性和模型公平性。

Conclusion: 需要解决数据稀缺和信号衰减问题以提升fUS成像的实用性和模型公平性。

Abstract: Functional ultrasound (fUS) imaging provides exceptional spatiotemporal
resolution for neurovascular mapping, yet its practical application is
significantly hampered by critical challenges. Foremost among these are data
scarcity, arising from ethical considerations and signal degradation through
the cranium, which collectively limit dataset diversity and compromise the
fairness of downstream machine learning models.

</details>


### [324] [Image denoising as a conditional expectation](https://arxiv.org/abs/2505.21546)
*Sajal Chakroborty,Suddhasattwa Das*

Main category: eess.IV

TL;DR: 论文提出了一种基于概率空间解释的数据驱动去噪方法，通过条件期望恢复真实图像，并在RKHS中求解最小二乘问题。


<details>
  <summary>Details</summary>
Motivation: 传统去噪方法基于投影假设，可能无法保证无偏和收敛性，因此需要一种更通用的概率空间解释方法。

Method: 将噪声图像视为概率空间的样本，通过核积分算子估计概率空间积分，并在RKHS中求解最小二乘问题。

Result: 方法在像素数量趋于无穷时收敛，并可优化有限像素图像的去噪参数。

Conclusion: 提出的方法在理论和实践中均有效，为图像去噪提供了新的视角。

Abstract: All techniques for denoising involve a notion of a true (noise-free) image,
and a hypothesis space. The hypothesis space may reconstruct the image directly
as a grayscale valued function, or indirectly by its Fourier or wavelet
spectrum. Most common techniques estimate the true image as a projection to
some subspace. We propose an interpretation of a noisy image as a collection of
samples drawn from a certain probability space. Within this interpretation,
projection based approaches are not guaranteed to be unbiased and convergent.
We present a data-driven denoising method in which the true image is recovered
as a conditional expectation. Although the probability space is unknown
apriori, integrals on this space can be estimated by kernel integral operators.
The true image is reformulated as the least squares solution to a linear
equation in a reproducing kernel Hilbert space (RKHS), and involving various
kernel integral operators as linear transforms. Assuming the true image to be a
continuous function on a compact planar domain, the technique is shown to be
convergent as the number of pixels goes to infinity. We also show that for a
picture with finite number of pixels, the convergence result can be used to
choose the various parameters for an optimum denoising result.

</details>


### [325] [Taylor expansion-based Kolmogorov-Arnold network for blind image quality assessment](https://arxiv.org/abs/2505.21592)
*Ze Chen,Shaode Yu*

Main category: eess.IV

TL;DR: 论文提出TaylorKAN，利用泰勒展开作为可学习激活函数，提升局部逼近能力，并通过网络深度压缩和特征降维提高计算效率，在多个数据库上表现优于其他KAN相关模型。


<details>
  <summary>Details</summary>
Motivation: 传统KAN及其变体在处理高维特征时性能提升有限且计算成本高，需要改进。

Method: 提出TaylorKAN，结合泰勒展开作为激活函数，并集成网络深度压缩和特征降维。

Result: 在多个数据库上TaylorKAN表现优于其他KAN模型，验证了其泛化能力。

Conclusion: TaylorKAN是一种高效且鲁棒的高维分数回归模型。

Abstract: Kolmogorov-Arnold Network (KAN) has attracted growing interest for its strong
function approximation capability. In our previous work, KAN and its variants
were explored in score regression for blind image quality assessment (BIQA).
However, these models encounter challenges when processing high-dimensional
features, leading to limited performance gains and increased computational
cost. To address these issues, we propose TaylorKAN that leverages the Taylor
expansions as learnable activation functions to enhance local approximation
capability. To improve the computational efficiency, network depth reduction
and feature dimensionality compression are integrated into the TaylorKAN-based
score regression pipeline. On five databases (BID, CLIVE, KonIQ, SPAQ, and
FLIVE) with authentic distortions, extensive experiments demonstrate that
TaylorKAN consistently outperforms the other KAN-related models, indicating
that the local approximation via Taylor expansions is more effective than
global approximation using orthogonal functions. Its generalization capacity is
validated through inter-database experiments. The findings highlight the
potential of TaylorKAN as an efficient and robust model for high-dimensional
score regression.

</details>


### [326] [Optimizing Deep Learning for Skin Cancer Classification: A Computationally Efficient CNN with Minimal Accuracy Trade-Off](https://arxiv.org/abs/2505.21597)
*Abdullah Al Mamun,Pollob Chandra Ray,Md Rahat Ul Nasib,Akash Das,Jia Uddin,Md Nurul Absur*

Main category: eess.IV

TL;DR: 论文提出了一种轻量级CNN模型，显著减少了参数和计算量，同时保持了高分类精度，适用于资源受限环境中的皮肤癌诊断。


<details>
  <summary>Details</summary>
Motivation: 现有基于迁移学习的模型（如ResNet50）计算开销大，难以在资源受限环境中部署，因此需要一种更高效的解决方案。

Method: 设计了一种自定义CNN模型，大幅减少参数和FLOPs，同时在HAM10000数据集上验证其性能。

Result: 模型参数减少96.7%，FLOPs降至30.04百万，分类精度偏差小于0.022%，显著降低计算成本和延迟。

Conclusion: 轻量级CNN在精度和效率之间取得了平衡，适合移动和边缘设备上的皮肤癌诊断。

Abstract: The rapid advancement of deep learning in medical image analysis has greatly
enhanced the accuracy of skin cancer classification. However, current
state-of-the-art models, especially those based on transfer learning like
ResNet50, come with significant computational overhead, rendering them
impractical for deployment in resource-constrained environments. This study
proposes a custom CNN model that achieves a 96.7\% reduction in parameters
(from 23.9 million in ResNet50 to 692,000) while maintaining a classification
accuracy deviation of less than 0.022\%. Our empirical analysis of the HAM10000
dataset reveals that although transfer learning models provide a marginal
accuracy improvement of approximately 0.022\%, they result in a staggering
13,216.76\% increase in FLOPs, considerably raising computational costs and
inference latency. In contrast, our lightweight CNN architecture, which
encompasses only 30.04 million FLOPs compared to ResNet50's 4.00 billion,
significantly reduces energy consumption, memory footprint, and inference time.
These findings underscore the trade-off between the complexity of deep models
and their real-world feasibility, positioning our optimized CNN as a practical
solution for mobile and edge-based skin cancer diagnostics.

</details>


### [327] [Laparoscopic Image Desmoking Using the U-Net with New Loss Function and Integrated Differentiable Wiener Filter](https://arxiv.org/abs/2505.21634)
*Chengyu Yang,Chengjun Liu*

Main category: eess.IV

TL;DR: 提出了一种结合U-Net深度学习、新损失函数和可微分维纳滤波器的ULW方法，用于消除腹腔镜手术中的烟雾，提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜手术中烟雾导致视觉清晰度下降，影响手术和计算机辅助技术。

Method: 采用U-Net结构，结合结构相似性、感知损失和均方误差的新损失函数，以及可学习的维纳滤波器。

Result: 在公开数据集上验证，ULW方法在视觉清晰度和定量评估中表现优异。

Conclusion: ULW方法为实时增强腹腔镜图像提供了有效解决方案。

Abstract: Laparoscopic surgeries often suffer from reduced visual clarity due to the
presence of surgical smoke originated by surgical instruments, which poses
significant challenges for both surgeons and vision based computer-assisted
technologies. In order to remove the surgical smoke, a novel U-Net deep
learning with new loss function and integrated differentiable Wiener filter
(ULW) method is presented. Specifically, the new loss function integrates the
pixel, structural, and perceptual properties. Thus, the new loss function,
which combines the structural similarity index measure loss, the perceptual
loss, as well as the mean squared error loss, is able to enhance the quality
and realism of the reconstructed images. Furthermore, the learnable Wiener
filter is capable of effectively modelling the degradation process caused by
the surgical smoke. The effectiveness of the proposed ULW method is evaluated
using the publicly available paired laparoscopic smoke and smoke-free image
dataset, which provides reliable benchmarking and quantitative comparisons.
Experimental results show that the proposed ULW method excels in both visual
clarity and metric-based evaluation. As a result, the proposed ULW method
offers a promising solution for real-time enhancement of laparoscopic imagery.
The code is available at https://github.com/chengyuyang-njit/ImageDesmoke.

</details>


### [328] [Cascaded 3D Diffusion Models for Whole-body 3D 18-F FDG PET/CT synthesis from Demographics](https://arxiv.org/abs/2505.22489)
*Siyeop Yoon,Sifan Song,Pengfei Jin,Matthew Tivnan,Yujin Oh,Sekeun Kim,Dufan Wu,Xiang Li,Quanzheng Li*

Main category: eess.IV

TL;DR: 提出了一种级联3D扩散模型框架，直接从人口统计学变量合成高保真3D PET/CT体积，解决了肿瘤成像中数字孪生的需求。


<details>
  <summary>Details</summary>
Motivation: 解决传统确定性模型依赖预定义模板的局限性，满足肿瘤成像、虚拟试验和AI数据增强中对真实数字孪生的需求。

Method: 采用两阶段生成过程：首先生成低分辨率PET/CT体积，再通过超分辨率残差扩散模型提升空间分辨率。

Result: 合成图像与真实图像在器官体积和代谢活性上高度一致，代谢活性偏差在3-5%以内。

Conclusion: 级联3D扩散模型能生成解剖和代谢准确的PET/CT图像，为临床和研究提供可扩展的合成成像方案。

Abstract: We propose a cascaded 3D diffusion model framework to synthesize
high-fidelity 3D PET/CT volumes directly from demographic variables, addressing
the growing need for realistic digital twins in oncologic imaging, virtual
trials, and AI-driven data augmentation. Unlike deterministic phantoms, which
rely on predefined anatomical and metabolic templates, our method employs a
two-stage generative process. An initial score-based diffusion model
synthesizes low-resolution PET/CT volumes from demographic variables alone,
providing global anatomical structures and approximate metabolic activity. This
is followed by a super-resolution residual diffusion model that refines spatial
resolution. Our framework was trained on 18-F FDG PET/CT scans from the AutoPET
dataset and evaluated using organ-wise volume and standardized uptake value
(SUV) distributions, comparing synthetic and real data between demographic
subgroups. The organ-wise comparison demonstrated strong concordance between
synthetic and real images. In particular, most deviations in metabolic uptake
values remained within 3-5% of the ground truth in subgroup analysis. These
findings highlight the potential of cascaded 3D diffusion models to generate
anatomically and metabolically accurate PET/CT images, offering a robust
alternative to traditional phantoms and enabling scalable, population-informed
synthetic imaging for clinical and research applications.

</details>


### [329] [STA-Risk: A Deep Dive of Spatio-Temporal Asymmetries for Breast Cancer Risk Prediction](https://arxiv.org/abs/2505.21699)
*Zhengbo Zhou,Dooman Arefan,Margarita Zuley,Jules Sumkin,Shandong Wu*

Main category: eess.IV

TL;DR: 提出了一种基于Transformer的STA-Risk模型，通过空间和时间不对称性预测乳腺癌风险，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺癌风险预测模型性能有限，且忽略了纵向影像中的细微变化。

Method: STA-Risk模型通过侧编码和时间编码捕捉空间-时间不对称性，并使用定制的不对称损失函数。

Result: 在两个独立数据集上，STA-Risk在1-5年风险预测中优于四种SOTA模型。

Conclusion: STA-Risk通过空间和时间不对称性分析显著提升了乳腺癌风险预测性能。

Abstract: Predicting the risk of developing breast cancer is an important clinical tool
to guide early intervention and tailoring personalized screening strategies.
Early risk models have limited performance and recently machine learning-based
analysis of mammogram images showed encouraging risk prediction effects. These
models however are limited to the use of a single exam or tend to overlook
nuanced breast tissue evolvement in spatial and temporal details of
longitudinal imaging exams that are indicative of breast cancer risk. In this
paper, we propose STA-Risk (Spatial and Temporal Asymmetry-based Risk
Prediction), a novel Transformer-based model that captures fine-grained
mammographic imaging evolution simultaneously from bilateral and longitudinal
asymmetries for breast cancer risk prediction. STA-Risk is innovative by the
side encoding and temporal encoding to learn spatial-temporal asymmetries,
regulated by a customized asymmetry loss. We performed extensive experiments
with two independent mammogram datasets and achieved superior performance than
four representative SOTA models for 1- to 5-year future risk prediction. Source
codes will be released upon publishing of the paper.

</details>


### [330] [Privacy-Preserving Chest X-ray Report Generation via Multimodal Federated Learning with ViT and GPT-2](https://arxiv.org/abs/2505.21715)
*Md. Zahid Hossain,Mustofa Ahmed,Most. Sharmin Sultana Samu,Md. Rakibul Islam*

Main category: eess.IV

TL;DR: 论文提出了一种基于多模态联邦学习的胸部X光报告生成框架，使用ViT编码器和GPT-2生成器，通过三种FL聚合策略（FedAvg、Krum Aggregation和L-FedAvg）实现去中心化训练，Krum Aggregation表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决传统集中式方法中敏感数据传输带来的隐私问题，同时提升诊断效率。

Method: 采用Vision Transformer (ViT)作为编码器，GPT-2作为报告生成器，评估了三种FL聚合策略（FedAvg、Krum Aggregation和L-FedAvg）。

Result: Krum Aggregation在ROUGE、BLEU、BERTScore和RaTEScore等指标上表现最优，FL模型在生成临床相关且语义丰富的报告方面优于或等同于集中式模型。

Conclusion: 该轻量级且隐私保护的框架为医疗AI协作开发提供了新途径，同时确保数据机密性。

Abstract: The automated generation of radiology reports from chest X-ray images holds
significant promise in enhancing diagnostic workflows while preserving patient
privacy. Traditional centralized approaches often require sensitive data
transfer, posing privacy concerns. To address this, the study proposes a
Multimodal Federated Learning framework for chest X-ray report generation using
the IU-Xray dataset. The system utilizes a Vision Transformer (ViT) as the
encoder and GPT-2 as the report generator, enabling decentralized training
without sharing raw data. Three Federated Learning (FL) aggregation strategies:
FedAvg, Krum Aggregation and a novel Loss-aware Federated Averaging (L-FedAvg)
were evaluated. Among these, Krum Aggregation demonstrated superior performance
across lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore
and RaTEScore. The results show that FL can match or surpass centralized models
in generating clinically relevant and semantically rich radiology reports. This
lightweight and privacy-preserving framework paves the way for collaborative
medical AI development without compromising data confidentiality.

</details>


### [331] [Highly Efficient Non-Separable Transforms for Next Generation Video Coding](https://arxiv.org/abs/2505.21728)
*Amir Said,Xin Zhao,Marta Karczewicz,Hilmi E. Egilmez,Vadim Seregin,Jianle Chen*

Main category: eess.IV

TL;DR: 提出了一种新型参数化方法（HyGTs），显著降低视频压缩中的计算复杂度，同时保持压缩性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于矩阵的信号自适应变换编码计算复杂度高，限制了其在视频压缩中的应用。

Method: 通过寻找最佳变换参数定义HyGTs，而非传统全矩阵变换，降低复杂度并易于并行化。

Result: HyGTs在HEVC中实现，平均编码增益提升6%（比特率降低），内存使用比KLT矩阵减少6.8倍。

Conclusion: HyGTs是一种高效、低复杂度的视频压缩解决方案。

Abstract: For the last few decades, the application of signal-adaptive transform coding
to video compression has been stymied by the large computational complexity of
matrix-based solutions. In this paper, we propose a novel parametric approach
to greatly reduce the complexity without degrading the compression performance.
In our approach, instead of following the conventional technique of identifying
full transform matrices that yield best compression efficiency, we look for the
best transform parameters defining a new class of transforms, called HyGTs,
which have low complexity implementations that are easy to parallelize. The
proposed HyGTs are implemented as an extension of High Efficiency Video Coding
(HEVC), and our comprehensive experimental results demonstrate that proposed
HyGTs improve average coding gain by 6% bit rate reduction, while using 6.8
times less memory than KLT matrices.

</details>


### [332] [Beyond 1D: Vision Transformers and Multichannel Signal Images for PPG-to-ECG Reconstruction](https://arxiv.org/abs/2505.21767)
*Xiaoyan Li,Shixin Xu,Faisal Habib,Arvind Gupta,Huaxiong Huang*

Main category: eess.IV

TL;DR: 提出了一种基于Vision Transformer（ViT）的四通道PPG信号图像表示方法，用于ECG重建，显著提升了波形特征捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从PPG重建ECG时难以准确捕捉细粒度波形特征，尤其是单通道PPG的局限性。

Method: 采用四通道信号图像表示（原始PPG、一阶差分、二阶差分和曲线下面积），结合ViT的自注意力机制，捕捉PPG的时空和生理变化。

Result: 实验表明，该方法在PRD和RMSE上分别降低了29%和15%，并在其他评估指标上表现优异。

Conclusion: 四通道信号图像与ViT的结合有效提升了PPG到ECG的映射能力，为循环信号分析开辟了新途径。

Abstract: Reconstructing ECG from PPG is a promising yet challenging task. While recent
advancements in generative models have significantly improved ECG
reconstruction, accurately capturing fine-grained waveform features remains a
key challenge. To address this, we propose a novel PPG-to-ECG reconstruction
method that leverages a Vision Transformer (ViT) as the core network. Unlike
conventional approaches that rely on single-channel PPG, our method employs a
four-channel signal image representation, incorporating the original PPG, its
first-order difference, second-order difference, and area under the curve. This
multi-channel design enriches feature extraction by preserving both temporal
and physiological variations within the PPG. By leveraging the self-attention
mechanism in ViT, our approach effectively captures both inter-beat and
intra-beat dependencies, leading to more robust and accurate ECG
reconstruction. Experimental results demonstrate that our method consistently
outperforms existing 1D convolution-based approaches, achieving up to 29%
reduction in PRD and 15% reduction in RMSE. The proposed approach also produces
improvements in other evaluation metrics, highlighting its robustness and
effectiveness in reconstructing ECG signals. Furthermore, to ensure a
clinically relevant evaluation, we introduce new performance metrics, including
QRS area error, PR interval error, RT interval error, and RT amplitude
difference error. Our findings suggest that integrating a four-channel signal
image representation with the self-attention mechanism of ViT enables more
effective extraction of informative PPG features and improved modeling of
beat-to-beat variations for PPG-to-ECG mapping. Beyond demonstrating the
potential of PPG as a viable alternative for heart activity monitoring, our
approach opens new avenues for cyclic signal analysis and prediction.

</details>


### [333] [Targeted Unlearning Using Perturbed Sign Gradient Methods With Applications On Medical Images](https://arxiv.org/abs/2505.21872)
*George R. Nahass,Zhu Wang,Homa Rashidisabet,Won Hwa Kim,Sasha Hubschman,Jeffrey C. Peterson,Ghasem Yazdanpanah,Chad A. Purnell,Pete Setabutr,Ann Q. Tran,Darvin Yi,Sathya N. Ravi*

Main category: eess.IV

TL;DR: 论文提出了一种用于临床场景的机器遗忘方法，通过双层优化和边界遗忘实现模型修正，无需完全重新训练。


<details>
  <summary>Details</summary>
Motivation: 解决临床环境中数据变化、设备淘汰和政策调整带来的模型维护问题，将机器遗忘推广为通用工具。

Method: 采用双层优化和边界遗忘的迭代算法，支持可调损失设计和模型组合策略。

Result: 在基准和临床影像数据集上，方法在遗忘和保留指标上优于基线。

Conclusion: 机器遗忘是临床应用中模型维护的实用替代方案。

Abstract: Machine unlearning aims to remove the influence of specific training samples
from a trained model without full retraining. While prior work has largely
focused on privacy-motivated settings, we recast unlearning as a
general-purpose tool for post-deployment model revision. Specifically, we focus
on utilizing unlearning in clinical contexts where data shifts, device
deprecation, and policy changes are common. To this end, we propose a bilevel
optimization formulation of boundary-based unlearning that can be solved using
iterative algorithms. We provide convergence guarantees when first-order
algorithms are used to unlearn. Our method introduces tunable loss design for
controlling the forgetting-retention tradeoff and supports novel model
composition strategies that merge the strengths of distinct unlearning runs.
Across benchmark and real-world clinical imaging datasets, our approach
outperforms baselines on both forgetting and retention metrics, including
scenarios involving imaging devices and anatomical outliers. This work
establishes machine unlearning as a modular, practical alternative to
retraining for real-world model maintenance in clinical applications.

</details>


### [334] [MAMBO-NET: Multi-Causal Aware Modeling Backdoor-Intervention Optimization for Medical Image Segmentation Network](https://arxiv.org/abs/2505.21874)
*Ruiguo Yu,Yiyang Zhang,Yuan Tian,Yujie Diao,Di Jin,Witold Pedrycz*

Main category: eess.IV

TL;DR: 论文提出了一种多因果感知建模的后门干预优化网络（MAMBO-NET），用于解决医学图像分割中混淆因素影响的问题，通过因果干预和多高斯分布建模提升分割精度。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割通常假设图像到分割的过程无偏，但实际中存在复杂解剖变异和成像模态限制等混淆因素，影响分割结果。

Method: MAMBO-NET利用多高斯分布建模混淆因素，引入因果干预和后验概率约束，结合后门干预技术优化分割过程。

Result: 在五个医学图像数据集上的实验表明，该方法显著降低了混淆因素的影响，提高了分割准确性。

Conclusion: MAMBO-NET通过因果干预和多高斯分布建模，有效解决了医学图像分割中的混淆因素问题，提升了分割性能。

Abstract: Medical image segmentation methods generally assume that the process from
medical image to segmentation is unbiased, and use neural networks to establish
conditional probability models to complete the segmentation task. This
assumption does not consider confusion factors, which can affect medical
images, such as complex anatomical variations and imaging modality limitations.
Confusion factors obfuscate the relevance and causality of medical image
segmentation, leading to unsatisfactory segmentation results. To address this
issue, we propose a multi-causal aware modeling backdoor-intervention
optimization (MAMBO-NET) network for medical image segmentation. Drawing
insights from causal inference, MAMBO-NET utilizes self-modeling with
multi-Gaussian distributions to fit the confusion factors and introduce causal
intervention into the segmentation process. Moreover, we design appropriate
posterior probability constraints to effectively train the distributions of
confusion factors. For the distributions to effectively guide the segmentation
and mitigate and eliminate the Impact of confusion factors on the segmentation,
we introduce classical backdoor intervention techniques and analyze their
feasibility in the segmentation task. To evaluate the effectiveness of our
approach, we conducted extensive experiments on five medical image datasets.
The results demonstrate that our method significantly reduces the influence of
confusion factors, leading to enhanced segmentation accuracy.

</details>


### [335] [Patch-based Reconstruction for Unsupervised Dynamic MRI using Learnable Tensor Function with Implicit Neural Representation](https://arxiv.org/abs/2505.21894)
*Yuanyuan Liu,Yuanbiao Yang,Zhuo-Xu Cui,Qingyong Zhu,Jing Cheng,Congcong Liu,Jinwen Xie,Jingran Xu,Hairong Zheng,Dong Liang,Yanjie Zhu*

Main category: eess.IV

TL;DR: TenF-INR是一种基于隐式神经表示（INR）的无监督动态MRI加速框架，通过张量分解和连续建模提升重建效率和质量。


<details>
  <summary>Details</summary>
Motivation: 动态MRI的高时空分辨率受限于长扫描时间，现有深度学习方法依赖大量全采样数据，难以获取。INR虽能连续建模动态图像，但面临细节恢复和高维数据计算挑战。

Method: 提出TenF-INR，基于INR建模张量分解基，利用空间图像块和时间方向的强相关性，实现多维低秩性和连续建模的块重建。

Result: 实验显示TenF-INR在加速因子高达21时，图像质量、时间保真度和定量指标均优于现有方法，甚至超越监督方法。

Conclusion: TenF-INR通过无监督方式显著提升动态MRI重建效率和质量，为临床实践提供新思路。

Abstract: Dynamic MRI plays a vital role in clinical practice by capturing both spatial
details and dynamic motion, but its high spatiotemporal resolution is often
limited by long scan times. Deep learning (DL)-based methods have shown
promising performance in accelerating dynamic MRI. However, most existing
algorithms rely on large fully-sampled datasets for training, which are
difficult to acquire. Recently, implicit neural representation (INR) has
emerged as a powerful scan-specific paradigm for accelerated MRI, which models
signals as a continuous function over spatiotemporal coordinates. Although this
approach achieves efficient continuous modeling of dynamic images and robust
reconstruction, it faces challenges in recovering fine details and increasing
computational demands for high dimensional data representation. To enhance both
efficiency and reconstruction quality, we propose TenF-INR, a novel patch-based
unsupervised framework that employs INR to model bases of tensor decomposition,
enabling efficient and accurate modeling of dynamic MR images with learnable
tensor functions. By exploiting strong correlations in similar spatial image
patches and in the temporal direction, TenF-INR enforces multidimensional
low-rankness and implements patch-based reconstruction with the benefits of
continuous modeling. We compare TenF-INR with state-of-the-art methods,
including supervised DL methods and unsupervised approaches. Experimental
results demonstrate that TenF-INR achieves high acceleration factors up to 21,
outperforming all comparison methods in image quality, temporal fidelity, and
quantitative metrics, even surpassing the supervised methods.

</details>


### [336] [Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology](https://arxiv.org/abs/2505.21928)
*Lianghui Zhu,Xitong Ling,Minxi Ouyang,Xiaoping Liu,Mingxi Fu,Tian Guan,Fanglei Fu,Xuanyu Wang,Maomao Zeng,Mingxi Zhu,Yibo Jin,Liming Liu,Song Duan,Qiming He,Yizhi Wang,Luxi Xie,Houqiang Li,Yonghong He,Sufang Tian*

Main category: eess.IV

TL;DR: Digepath是一种针对胃肠道病理的专用基础模型，通过双阶段迭代优化策略，显著提升了胃肠道疾病的诊断准确性，尤其在疑难病例中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统胃肠道疾病诊断依赖病理学家的主观判断，存在重复性和诊断变异性问题，需要更精确的AI驱动方法。

Method: 开发Digepath模型，采用预训练与精细筛查相结合的双阶段策略，处理全切片图像中稀疏分布的病变区域。

Result: 在34项任务中33项达到最优性能，早期癌症筛查灵敏度达99.6%，适用于多机构验证。

Conclusion: Digepath填补了病理学实践中的关键空白，为其他病理学亚专业提供了可转移的范例。

Abstract: Gastrointestinal (GI) diseases represent a clinically significant burden,
necessitating precise diagnostic approaches to optimize patient outcomes.
Conventional histopathological diagnosis, heavily reliant on the subjective
interpretation of pathologists, suffers from limited reproducibility and
diagnostic variability. To overcome these limitations and address the lack of
pathology-specific foundation models for GI diseases, we develop Digepath, a
specialized foundation model for GI pathology. Our framework introduces a
dual-phase iterative optimization strategy combining pretraining with
fine-screening, specifically designed to address the detection of sparsely
distributed lesion areas in whole-slide images. Digepath is pretrained on more
than 353 million image patches from over 200,000 hematoxylin and eosin-stained
slides of GI diseases. It attains state-of-the-art performance on 33 out of 34
tasks related to GI pathology, including pathological diagnosis, molecular
prediction, gene mutation prediction, and prognosis evaluation, particularly in
diagnostically ambiguous cases and resolution-agnostic tissue classification.We
further translate the intelligent screening module for early GI cancer and
achieve near-perfect 99.6% sensitivity across 9 independent medical
institutions nationwide. The outstanding performance of Digepath highlights its
potential to bridge critical gaps in histopathological practice. This work not
only advances AI-driven precision pathology for GI diseases but also
establishes a transferable paradigm for other pathology subspecialties.

</details>


### [337] [Collaborative Learning for Unsupervised Multimodal Remote Sensing Image Registration: Integrating Self-Supervision and MIM-Guided Diffusion-Based Image Translation](https://arxiv.org/abs/2505.22000)
*Xiaochen Wei,Weiwei Guo,Wenxian Yu*

Main category: eess.IV

TL;DR: CoLReg是一种协作学习框架，用于无监督多模态图像配准，通过跨模态图像翻译、自监督中间配准网络和蒸馏跨模态配准网络联合优化，显著提升配准精度。


<details>
  <summary>Details</summary>
Motivation: 多模态图像配准面临模态差异大、标注数据稀缺的挑战，传统无监督方法难以稳定收敛。

Method: CoLReg包含三个组件：跨模态图像翻译网络MIMGCD、自监督中间配准网络和蒸馏跨模态配准网络，通过交替训练策略联合优化。

Result: 实验表明CoLReg在多个数据集上优于现有无监督方法，甚至超过部分有监督基线。

Conclusion: CoLReg通过协作学习有效减少模态差异，提升配准性能，具有广泛应用潜力。

Abstract: The substantial modality-induced variations in radiometric, texture, and
structural characteristics pose significant challenges for the accurate
registration of multimodal images. While supervised deep learning methods have
demonstrated strong performance, they often rely on large-scale annotated
datasets, limiting their practical application. Traditional unsupervised
methods usually optimize registration by minimizing differences in feature
representations, yet often fail to robustly capture geometric discrepancies,
particularly under substantial spatial and radiometric variations, thus
hindering convergence stability. To address these challenges, we propose a
Collaborative Learning framework for Unsupervised Multimodal Image
Registration, named CoLReg, which reformulates unsupervised registration
learning into a collaborative training paradigm comprising three components:
(1) a cross-modal image translation network, MIMGCD, which employs a learnable
Maximum Index Map (MIM) guided conditional diffusion model to synthesize
modality-consistent image pairs; (2) a self-supervised intermediate
registration network which learns to estimate geometric transformations using
accurate displacement labels derived from MIMGCD outputs; (3) a distilled
cross-modal registration network trained with pseudo-label predicted by the
intermediate network. The three networks are jointly optimized through an
alternating training strategy wherein each network enhances the performance of
the others. This mutual collaboration progressively reduces modality
discrepancies, enhances the quality of pseudo-labels, and improves registration
accuracy. Extensive experimental results on multiple datasets demonstrate that
our ColReg achieves competitive or superior performance compared to
state-of-the-art unsupervised approaches and even surpasses several supervised
baselines.

</details>


### [338] [High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models](https://arxiv.org/abs/2505.22090)
*Tristan S. W. Stevens,Oisín Nolan,Oudom Somphone,Jean-Luc Robert,Ruud J. G. van Sloun*

Main category: eess.IV

TL;DR: 本文提出了一种基于扩散模型（DMs）的3D超声重建方法，通过减少高程平面的采样数量，提高空间和时间分辨率，并在图像质量和下游任务性能上优于传统和深度学习基线方法。


<details>
  <summary>Details</summary>
Motivation: 3D超声成像在实时体积可视化方面具有优势，但高体积率和高图像质量难以兼顾。传统方法如3D发散波虽能提高体积率，但图像质量受限。本文旨在通过扩散模型解决这一问题。

Method: 采用扩散模型（DMs）从减少的高程平面集中重建3D超声图像，并利用超声序列的时间一致性加速推理。同时，通过扩散后验采样的概率性质量化重建不确定性。

Result: 实验表明，基于扩散模型的重建方法在图像质量和下游任务性能上优于传统和深度学习基线方法，并在强子采样下对异常数据表现出更高的鲁棒性。

Conclusion: 扩散模型为3D超声重建提供了一种高效且鲁棒的方法，显著提升了图像质量和任务性能，同时能够量化不确定性并适应异常数据。

Abstract: Three-dimensional ultrasound enables real-time volumetric visualization of
anatomical structures. Unlike traditional 2D ultrasound, 3D imaging reduces the
reliance on precise probe orientation, potentially making ultrasound more
accessible to clinicians with varying levels of experience and improving
automated measurements and post-exam analysis. However, achieving both high
volume rates and high image quality remains a significant challenge. While 3D
diverging waves can provide high volume rates, they suffer from limited tissue
harmonic generation and increased multipath effects, which degrade image
quality. One compromise is to retain the focusing in elevation while leveraging
unfocused diverging waves in the lateral direction to reduce the number of
transmissions per elevation plane. Reaching the volume rates achieved by full
3D diverging waves, however, requires dramatically undersampling the number of
elevation planes. Subsequently, to render the full volume, simple interpolation
techniques are applied. This paper introduces a novel approach to 3D ultrasound
reconstruction from a reduced set of elevation planes by employing diffusion
models (DMs) to achieve increased spatial and temporal resolution. We compare
both traditional and supervised deep learning-based interpolation methods on a
3D cardiac ultrasound dataset. Our results show that DM-based reconstruction
consistently outperforms the baselines in image quality and downstream task
performance. Additionally, we accelerate inference by leveraging the temporal
consistency inherent to ultrasound sequences. Finally, we explore the
robustness of the proposed method by exploiting the probabilistic nature of
diffusion posterior sampling to quantify reconstruction uncertainty and
demonstrate improved recall on out-of-distribution data with synthetic
anomalies under strong subsampling.

</details>


### [339] [MONSTR: Model-Oriented Neutron Strain Tomographic Reconstruction](https://arxiv.org/abs/2505.22187)
*Mohammad Samin Nur Chowdhury,Shimin Tang,Singanallur V. Venkatakrishnan,Hassina Z. Bilheux,Gregery T. Buzzard,Charles A. Bouman*

Main category: eess.IV

TL;DR: 论文提出了一种名为MONSTR的算法，用于从布拉格边缘应变测量中重建二维残余应变张量，解决了传统重建方法中的不适定问题。


<details>
  <summary>Details</summary>
Motivation: 残余应变是影响金属部件性能的关键材料特性，但现有重建方法因数据不足而严重不适定。

Method: MONSTR算法基于多智能体共识平衡框架，结合探测器物理、重建过程和连续介质力学约束，实现张量重建。

Result: 模拟数据显示，MONSTR即使在测量数据极少的情况下也能高质量重建应变张量。

Conclusion: MONSTR为残余应变张量的高精度重建提供了一种有效方法。

Abstract: Residual strain, a tensor quantity, is a critical material property that
impacts the overall performance of metal parts. Neutron Bragg edge strain
tomography is a technique for imaging residual strain that works by making
conventional hyperspectral computed tomography measurements, extracting the
average projected strain at each detector pixel, and processing the resulting
strain sinogram using a reconstruction algorithm. However, the reconstruction
is severely ill-posed as the underlying inverse problem involves inferring a
tensor at each voxel from scalar sinogram data.
  In this paper, we introduce the model-oriented neutron strain tomographic
reconstruction (MONSTR) algorithm that reconstructs the 2D residual strain
tensor from the neutron Bragg edge strain measurements. MONSTR is based on
using the multi-agent consensus equilibrium framework for the tensor
tomographic reconstruction. Specifically, we formulate the reconstruction as a
consensus solution of a collection of agents representing detector physics, the
tomographic reconstruction process, and physics-based constraints from
continuum mechanics. Using simulated data, we demonstrate high-quality
reconstruction of the strain tensor even when using very few measurements.

</details>


### [340] [Risk-Sensitive Conformal Prediction for Catheter Placement Detection in Chest X-rays](https://arxiv.org/abs/2505.22496)
*Long Hui*

Main category: eess.IV

TL;DR: 提出了一种结合多任务学习和风险敏感共形预测的新方法，用于胸部X光片中导管和管线位置的检测，显著提升了临床可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决临床中对导管和管线位置检测的高可靠性需求，尤其是在关键临床发现中避免高风险误判。

Method: 结合多任务学习（分类、分割和标志点检测）与风险敏感共形预测，提升模型性能和临床可靠性。

Result: 实验显示总体覆盖率90.68%，关键条件覆盖率达99.29%，且未出现高风险误判。

Conclusion: 该方法不仅提供高精度预测，还能可靠量化不确定性，适用于生命关键的医疗应用。

Abstract: This paper presents a novel approach to catheter and line position detection
in chest X-rays, combining multi-task learning with risk-sensitive conformal
prediction to address critical clinical requirements. Our model simultaneously
performs classification, segmentation, and landmark detection, leveraging the
synergistic relationship between these tasks to improve overall performance. We
further enhance clinical reliability through risk-sensitive conformal
prediction, which provides statistically guaranteed prediction sets with higher
reliability for clinically critical findings. Experimental results demonstrate
excellent performance with 90.68\% overall empirical coverage and 99.29\%
coverage for critical conditions, while maintaining remarkable precision in
prediction sets. Most importantly, our risk-sensitive approach achieves zero
high-risk mispredictions (cases where the system dangerously declares
problematic tubes as confidently normal), making the system particularly
suitable for clinical deployment. This work offers both accurate predictions
and reliably quantified uncertainty -- essential features for life-critical
medical applications.

</details>


### [341] [Surf2CT: Cascaded 3D Flow Matching Models for Torso 3D CT Synthesis from Skin Surface](https://arxiv.org/abs/2505.22511)
*Siyeop Yoon,Yujin Oh,Pengfei Jin,Sifan Song,Matthew Tivnan,Dufan Wu,Xiang Li,Quanzheng Li*

Main category: eess.IV

TL;DR: Surf2CT是一种新颖的级联流匹配框架，能够仅通过外部表面扫描和简单人口统计数据生成完整的人体躯干3D CT图像。


<details>
  <summary>Details</summary>
Motivation: 传统CT扫描需要侵入性操作且成本高，Surf2CT旨在通过非侵入性方式生成内部解剖图像，推动家庭医疗和预防医学的发展。

Method: Surf2CT分为三个阶段：表面补全、粗粒度CT合成和CT超分辨率，均基于3D流匹配技术。

Result: 在700对测试案例中，Surf2CT表现出高解剖保真度，器官体积误差小（-11.1%至4.4%），肌肉/脂肪组成与真实数据强相关（0.67至0.96）。

Conclusion: Surf2CT为非侵入性内部解剖成像提供了新范式，有望应用于家庭医疗和个性化临床评估。

Abstract: We present Surf2CT, a novel cascaded flow matching framework that synthesizes
full 3D computed tomography (CT) volumes of the human torso from external
surface scans and simple demographic data (age, sex, height, weight). This is
the first approach capable of generating realistic volumetric internal anatomy
images solely based on external body shape and demographics, without any
internal imaging. Surf2CT proceeds through three sequential stages: (1) Surface
Completion, reconstructing a complete signed distance function (SDF) from
partial torso scans using conditional 3D flow matching; (2) Coarse CT
Synthesis, generating a low-resolution CT volume from the completed SDF and
demographic information; and (3) CT Super-Resolution, refining the coarse
volume into a high-resolution CT via a patch-wise conditional flow model. Each
stage utilizes a 3D-adapted EDM2 backbone trained via flow matching. We trained
our model on a combined dataset of 3,198 torso CT scans (approximately 1.13
million axial slices) sourced from Massachusetts General Hospital (MGH) and the
AutoPET challenge. Evaluation on 700 paired torso surface-CT cases demonstrated
strong anatomical fidelity: organ volumes exhibited small mean percentage
differences (range from -11.1% to 4.4%), and muscle/fat body composition
metrics matched ground truth with strong correlation (range from 0.67 to 0.96).
Lung localization had minimal bias (mean difference -2.5 mm), and surface
completion significantly improved metrics (Chamfer distance: from 521.8 mm to
2.7 mm; Intersection-over-Union: from 0.87 to 0.98). Surf2CT establishes a new
paradigm for non-invasive internal anatomical imaging using only external data,
opening opportunities for home-based healthcare, preventive medicine, and
personalized clinical assessments without the risks associated with
conventional imaging techniques.

</details>


### [342] [ConfLUNet: Multiple sclerosis lesion instance segmentation in presence of confluent lesions](https://arxiv.org/abs/2505.22537)
*Maxence Wynen,Pedro M. Gordaliza,Maxime Istasse,Anna Stölting,Pietro Maggi,Benoît Macq,Meritxell Bach Cuadra*

Main category: eess.IV

TL;DR: 论文提出了一种新的端到端实例分割框架ConfLUNet，用于多发性硬化症（MS）病灶的精确分割，解决了现有方法在病灶计数和分割精度上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前基于连通组件（CC）的语义分割方法无法有效分离融合病灶（CLUs），导致临床需求与实际评估方法不匹配。

Method: 论文引入了CLUs的正式定义和CLU感知的检测指标，并提出了ConfLUNet，一个端到端的实例分割框架，从单一FLAIR图像中联合优化病灶检测和分割。

Result: ConfLUNet在测试集上显著优于CC和ACLS方法，在实例分割和病灶检测任务中表现更优，尤其在CLU检测中F1分数最高。

Conclusion: 通过结合严格定义、新指标、可重复评估框架和首个端到端模型，该研究为MS病灶实例分割奠定了基础。

Abstract: Accurate lesion-level segmentation on MRI is critical for multiple sclerosis
(MS) diagnosis, prognosis, and disease monitoring. However, current evaluation
practices largely rely on semantic segmentation post-processed with connected
components (CC), which cannot separate confluent lesions (aggregates of
confluent lesion units, CLUs) due to reliance on spatial connectivity. To
address this misalignment with clinical needs, we introduce formal definitions
of CLUs and associated CLU-aware detection metrics, and include them in an
exhaustive instance segmentation evaluation framework. Within this framework,
we systematically evaluate CC and post-processing-based Automated Confluent
Splitting (ACLS), the only existing methods for lesion instance segmentation in
MS. Our analysis reveals that CC consistently underestimates CLU counts, while
ACLS tends to oversplit lesions, leading to overestimated lesion counts and
reduced precision. To overcome these limitations, we propose ConfLUNet, the
first end-to-end instance segmentation framework for MS lesions. ConfLUNet
jointly optimizes lesion detection and delineation from a single FLAIR image.
Trained on 50 patients, ConfLUNet significantly outperforms CC and ACLS on the
held-out test set (n=13) in instance segmentation (Panoptic Quality: 42.0% vs.
37.5%/36.8%; p = 0.017/0.005) and lesion detection (F1: 67.3% vs. 61.6%/59.9%;
p = 0.028/0.013). For CLU detection, ConfLUNet achieves the highest F1[CLU]
(81.5%), improving recall over CC (+12.5%, p = 0.015) and precision over ACLS
(+31.2%, p = 0.003). By combining rigorous definitions, new CLU-aware metrics,
a reproducible evaluation framework, and the first dedicated end-to-end model,
this work lays the foundation for lesion instance segmentation in MS.

</details>


### [343] [Multipath cycleGAN for harmonization of paired and unpaired low-dose lung computed tomography reconstruction kernels](https://arxiv.org/abs/2505.22568)
*Aravind R. Krishnan,Thomas Z. Li,Lucas W. Remedios,Michael E. Kim,Chenyu Gao,Gaurav Rudravaram,Elyssa M. McMaster,Adam M. Saunders,Shunxing Bao,Kaiwen Xu,Lianrui Zuo,Kim L. Sandler,Fabien Maldonado,Yuankai Huo,Bennett A. Landman*

Main category: eess.IV

TL;DR: 提出了一种基于多路径cycleGAN的CT核谐波化方法，用于减少定量成像中的系统性偏差，并验证其在肺气肿量化中的效果。


<details>
  <summary>Details</summary>
Motivation: CT重建核的选择会影响空间分辨率和噪声特性，导致定量成像测量（如肺气肿量化）的系统性变异性。因此，选择适当的核对于一致的定量分析至关重要。

Method: 提出了一种多路径cycleGAN模型，结合配对和非配对数据进行训练，模型包含特定领域的编码器和解码器，共享潜在空间，并为每个领域定制判别器。

Result: 模型在配对核上减少了肺气肿评分的偏差，在非配对核上消除了混淆差异，同时保持了肌肉和脂肪的解剖结构一致性。

Conclusion: 共享潜在空间的多路径cycleGAN能够实现稳健的核谐波化，改善肺气肿量化并保持解剖保真度。

Abstract: Reconstruction kernels in computed tomography (CT) affect spatial resolution
and noise characteristics, introducing systematic variability in quantitative
imaging measurements such as emphysema quantification. Choosing an appropriate
kernel is therefore essential for consistent quantitative analysis. We propose
a multipath cycleGAN model for CT kernel harmonization, trained on a mixture of
paired and unpaired data from a low-dose lung cancer screening cohort. The
model features domain-specific encoders and decoders with a shared latent space
and uses discriminators tailored for each domain.We train the model on 42
kernel combinations using 100 scans each from seven representative kernels in
the National Lung Screening Trial (NLST) dataset. To evaluate performance, 240
scans from each kernel are harmonized to a reference soft kernel, and emphysema
is quantified before and after harmonization. A general linear model assesses
the impact of age, sex, smoking status, and kernel on emphysema. We also
evaluate harmonization from soft kernels to a reference hard kernel. To assess
anatomical consistency, we compare segmentations of lung vessels, muscle, and
subcutaneous adipose tissue generated by TotalSegmentator between harmonized
and original images. Our model is benchmarked against traditional and
switchable cycleGANs. For paired kernels, our approach reduces bias in
emphysema scores, as seen in Bland-Altman plots (p<0.05). For unpaired kernels,
harmonization eliminates confounding differences in emphysema (p>0.05). High
Dice scores confirm preservation of muscle and fat anatomy, while lung vessel
overlap remains reasonable. Overall, our shared latent space multipath cycleGAN
enables robust harmonization across paired and unpaired CT kernels, improving
emphysema quantification and preserving anatomical fidelity.

</details>


### [344] [Comparative Analysis of Machine Learning Models for Lung Cancer Mutation Detection and Staging Using 3D CT Scans](https://arxiv.org/abs/2505.22592)
*Yiheng Li,Francisco Carrillo-Perez,Mohammed Alawad,Olivier Gevaert*

Main category: eess.IV

TL;DR: 比较两种机器学习模型在肺癌突变检测和分期中的表现，监督模型FMCIB+XGBoost在突变检测中表现更优，自监督模型Dinov2+ABMIL在分期中表现较好。


<details>
  <summary>Details</summary>
Motivation: 肺癌是全球癌症死亡的主要原因，非侵入性方法检测关键突变和分期对改善患者预后至关重要。

Method: 比较FMCIB+XGBoost（监督模型）和Dinov2+ABMIL（自监督模型）在3D肺结节数据上的表现，任务包括KRAS和EGFR突变检测及癌症分期。

Result: FMCIB+XGBoost在突变检测中表现更优（KRAS:0.846，EGFR:0.883），Dinov2+ABMIL在分期中表现较好（T-stage:0.797）。

Conclusion: 监督模型在突变检测中更具临床价值，自监督模型在分期中表现潜力，但突变敏感性有待提升。

Abstract: Lung cancer is the leading cause of cancer mortality worldwide, and
non-invasive methods for detecting key mutations and staging are essential for
improving patient outcomes. Here, we compare the performance of two machine
learning models - FMCIB+XGBoost, a supervised model with domain-specific
pretraining, and Dinov2+ABMIL, a self-supervised model with attention-based
multiple-instance learning - on 3D lung nodule data from the Stanford
Radiogenomics and Lung-CT-PT-Dx cohorts. In the task of KRAS and EGFR mutation
detection, FMCIB+XGBoost consistently outperformed Dinov2+ABMIL, achieving
accuracies of 0.846 and 0.883 for KRAS and EGFR mutations, respectively. In
cancer staging, Dinov2+ABMIL demonstrated competitive generalization, achieving
an accuracy of 0.797 for T-stage prediction in the Lung-CT-PT-Dx cohort,
suggesting SSL's adaptability across diverse datasets. Our results emphasize
the clinical utility of supervised models in mutation detection and highlight
the potential of SSL to improve staging generalization, while identifying areas
for enhancement in mutation sensitivity.

</details>


### [345] [Chest Disease Detection In X-Ray Images Using Deep Learning Classification Method](https://arxiv.org/abs/2505.22609)
*Alanna Hazlett,Naomi Ohashi,Timothy Rodriguez,Sodiq Adewole*

Main category: eess.IV

TL;DR: 研究通过迁移学习技术，利用预训练的CNN模型对胸部X光片进行分类，结果显示高准确率和良好的性能指标。


<details>
  <summary>Details</summary>
Motivation: 探索多种分类模型在胸部X光片分类中的表现，特别是针对COVID-19、肺炎、结核病和正常病例的分类。

Method: 采用迁移学习技术，微调预训练的CNN模型，并使用Grad-CAM提高模型可解释性。

Result: 初步结果显示高准确率，并在精确率、召回率和F1分数等关键指标上表现优异。

Conclusion: 研究证明了迁移学习和Grad-CAM在医学影像分类中的有效性，提升了临床应用的信任和透明度。

Abstract: In this work, we investigate the performance across multiple classification
models to classify chest X-ray images into four categories of COVID-19,
pneumonia, tuberculosis (TB), and normal cases. We leveraged transfer learning
techniques with state-of-the-art pre-trained Convolutional Neural Networks
(CNNs) models. We fine-tuned these pre-trained architectures on a labeled
medical x-ray images. The initial results are promising with high accuracy and
strong performance in key classification metrics such as precision, recall, and
F1 score. We applied Gradient-weighted Class Activation Mapping (Grad-CAM) for
model interpretability to provide visual explanations for classification
decisions, improving trust and transparency in clinical applications.

</details>


### [346] [High-Fidelity Functional Ultrasound Reconstruction via A Visual Auto-Regressive Framework](https://arxiv.org/abs/2505.21530)
*Xuhang Chen,Zhuo Li,Yanyan Shen,Mufti Mahmud,Hieu Pham,Chi-Man Pun,Shuqiang Wang*

Main category: eess.IV

TL;DR: 功能性超声（fUS）成像在神经血管映射中具有出色的时空分辨率，但数据稀缺和信号衰减限制了其实际应用。


<details>
  <summary>Details</summary>
Motivation: 解决功能性超声成像在神经血管映射中因数据稀缺和信号衰减导致的实际应用受限问题。

Method: 未明确提及具体方法，但提到数据稀缺和信号衰减是主要挑战。

Result: 数据稀缺和信号衰减限制了数据集多样性，并影响下游机器学习模型的公平性。

Conclusion: 功能性超声成像的实际应用面临数据稀缺和信号衰减的挑战，需进一步解决。

Abstract: Functional ultrasound (fUS) imaging provides exceptional spatiotemporal
resolution for neurovascular mapping, yet its practical application is
significantly hampered by critical challenges. Foremost among these are data
scarcity, arising from ethical considerations and signal degradation through
the cranium, which collectively limit dataset diversity and compromise the
fairness of downstream machine learning models.

</details>


### [347] [Image denoising as a conditional expectation](https://arxiv.org/abs/2505.21546)
*Sajal Chakroborty,Suddhasattwa Das*

Main category: eess.IV

TL;DR: 论文提出了一种基于概率空间解释的数据驱动去噪方法，将真实图像恢复为条件期望，并在RKHS中通过核积分算子实现收敛。


<details>
  <summary>Details</summary>
Motivation: 传统去噪方法基于投影到子空间的假设，可能无法保证无偏和收敛，因此需要一种更通用的概率空间解释方法。

Method: 将噪声图像视为概率空间的样本，通过核积分算子估计条件期望，并在RKHS中求解最小二乘问题。

Result: 方法在像素数趋于无穷时收敛，且可用于有限像素图像的最优参数选择。

Conclusion: 提出的方法在理论和实践中均表现出优越的去噪性能，适用于连续和有限像素图像。

Abstract: All techniques for denoising involve a notion of a true (noise-free) image,
and a hypothesis space. The hypothesis space may reconstruct the image directly
as a grayscale valued function, or indirectly by its Fourier or wavelet
spectrum. Most common techniques estimate the true image as a projection to
some subspace. We propose an interpretation of a noisy image as a collection of
samples drawn from a certain probability space. Within this interpretation,
projection based approaches are not guaranteed to be unbiased and convergent.
We present a data-driven denoising method in which the true image is recovered
as a conditional expectation. Although the probability space is unknown
apriori, integrals on this space can be estimated by kernel integral operators.
The true image is reformulated as the least squares solution to a linear
equation in a reproducing kernel Hilbert space (RKHS), and involving various
kernel integral operators as linear transforms. Assuming the true image to be a
continuous function on a compact planar domain, the technique is shown to be
convergent as the number of pixels goes to infinity. We also show that for a
picture with finite number of pixels, the convergence result can be used to
choose the various parameters for an optimum denoising result.

</details>


### [348] [Taylor expansion-based Kolmogorov-Arnold network for blind image quality assessment](https://arxiv.org/abs/2505.21592)
*Ze Chen,Shaode Yu*

Main category: eess.IV

TL;DR: TaylorKAN利用泰勒展开作为可学习激活函数，提升局部逼近能力，并通过网络深度减少和特征维度压缩提高计算效率，在高维分数回归中表现优于其他KAN相关模型。


<details>
  <summary>Details</summary>
Motivation: KAN及其变体在处理高维特征时性能提升有限且计算成本高，因此提出TaylorKAN以解决这些问题。

Method: 采用泰勒展开作为可学习激活函数，结合网络深度减少和特征维度压缩，构建TaylorKAN模型。

Result: 在五个数据库（BID、CLIVE、KonIQ、SPAQ和FLIVE）上，TaylorKAN表现优于其他KAN相关模型，验证了其泛化能力。

Conclusion: TaylorKAN是一种高效且鲁棒的高维分数回归模型，局部逼近优于全局逼近。

Abstract: Kolmogorov-Arnold Network (KAN) has attracted growing interest for its strong
function approximation capability. In our previous work, KAN and its variants
were explored in score regression for blind image quality assessment (BIQA).
However, these models encounter challenges when processing high-dimensional
features, leading to limited performance gains and increased computational
cost. To address these issues, we propose TaylorKAN that leverages the Taylor
expansions as learnable activation functions to enhance local approximation
capability. To improve the computational efficiency, network depth reduction
and feature dimensionality compression are integrated into the TaylorKAN-based
score regression pipeline. On five databases (BID, CLIVE, KonIQ, SPAQ, and
FLIVE) with authentic distortions, extensive experiments demonstrate that
TaylorKAN consistently outperforms the other KAN-related models, indicating
that the local approximation via Taylor expansions is more effective than
global approximation using orthogonal functions. Its generalization capacity is
validated through inter-database experiments. The findings highlight the
potential of TaylorKAN as an efficient and robust model for high-dimensional
score regression.

</details>


### [349] [Optimizing Deep Learning for Skin Cancer Classification: A Computationally Efficient CNN with Minimal Accuracy Trade-Off](https://arxiv.org/abs/2505.21597)
*Abdullah Al Mamun,Pollob Chandra Ray,Md Rahat Ul Nasib,Akash Das,Jia Uddin,Md Nurul Absur*

Main category: eess.IV

TL;DR: 提出了一种轻量级CNN模型，显著减少参数和计算量，同时保持高分类精度，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型（如ResNet50）计算开销大，难以在资源受限环境中部署，需要更高效的解决方案。

Method: 设计了一种自定义CNN模型，大幅减少参数和FLOPs，同时保持分类精度。

Result: 模型参数减少96.7%，FLOPs显著降低，分类精度偏差小于0.022%。

Conclusion: 轻量级CNN模型在资源受限环境中更具实用性，是移动和边缘设备皮肤癌诊断的理想选择。

Abstract: The rapid advancement of deep learning in medical image analysis has greatly
enhanced the accuracy of skin cancer classification. However, current
state-of-the-art models, especially those based on transfer learning like
ResNet50, come with significant computational overhead, rendering them
impractical for deployment in resource-constrained environments. This study
proposes a custom CNN model that achieves a 96.7\% reduction in parameters
(from 23.9 million in ResNet50 to 692,000) while maintaining a classification
accuracy deviation of less than 0.022\%. Our empirical analysis of the HAM10000
dataset reveals that although transfer learning models provide a marginal
accuracy improvement of approximately 0.022\%, they result in a staggering
13,216.76\% increase in FLOPs, considerably raising computational costs and
inference latency. In contrast, our lightweight CNN architecture, which
encompasses only 30.04 million FLOPs compared to ResNet50's 4.00 billion,
significantly reduces energy consumption, memory footprint, and inference time.
These findings underscore the trade-off between the complexity of deep models
and their real-world feasibility, positioning our optimized CNN as a practical
solution for mobile and edge-based skin cancer diagnostics.

</details>


### [350] [Laparoscopic Image Desmoking Using the U-Net with New Loss Function and Integrated Differentiable Wiener Filter](https://arxiv.org/abs/2505.21634)
*Chengyu Yang,Chengjun Liu*

Main category: eess.IV

TL;DR: 提出了一种结合U-Net深度学习和可微分Wiener滤波器的ULW方法，用于去除腹腔镜手术中的烟雾，提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜手术中烟雾导致视觉清晰度下降，影响手术和计算机辅助技术。

Method: 使用U-Net结合新损失函数（结构相似性、感知损失和均方误差）和可学习Wiener滤波器。

Result: 在公开数据集上表现优异，视觉清晰度和定量评估均优于现有方法。

Conclusion: ULW方法为实时增强腹腔镜图像提供了有效解决方案。

Abstract: Laparoscopic surgeries often suffer from reduced visual clarity due to the
presence of surgical smoke originated by surgical instruments, which poses
significant challenges for both surgeons and vision based computer-assisted
technologies. In order to remove the surgical smoke, a novel U-Net deep
learning with new loss function and integrated differentiable Wiener filter
(ULW) method is presented. Specifically, the new loss function integrates the
pixel, structural, and perceptual properties. Thus, the new loss function,
which combines the structural similarity index measure loss, the perceptual
loss, as well as the mean squared error loss, is able to enhance the quality
and realism of the reconstructed images. Furthermore, the learnable Wiener
filter is capable of effectively modelling the degradation process caused by
the surgical smoke. The effectiveness of the proposed ULW method is evaluated
using the publicly available paired laparoscopic smoke and smoke-free image
dataset, which provides reliable benchmarking and quantitative comparisons.
Experimental results show that the proposed ULW method excels in both visual
clarity and metric-based evaluation. As a result, the proposed ULW method
offers a promising solution for real-time enhancement of laparoscopic imagery.
The code is available at https://github.com/chengyuyang-njit/ImageDesmoke.

</details>


### [351] [Cascaded 3D Diffusion Models for Whole-body 3D 18-F FDG PET/CT synthesis from Demographics](https://arxiv.org/abs/2505.22489)
*Siyeop Yoon,Sifan Song,Pengfei Jin,Matthew Tivnan,Yujin Oh,Sekeun Kim,Dufan Wu,Xiang Li,Quanzheng Li*

Main category: eess.IV

TL;DR: 提出了一种级联3D扩散模型框架，直接从人口统计学变量合成高保真3D PET/CT图像，用于肿瘤成像、虚拟试验和AI驱动的数据增强。


<details>
  <summary>Details</summary>
Motivation: 解决传统确定性模型依赖预定义模板的局限性，满足对真实数字孪生的需求。

Method: 采用两阶段生成过程：首先生成低分辨率图像，再通过超分辨率模型提升空间分辨率。

Result: 合成图像与真实图像在器官体积和代谢活性上高度一致，代谢偏差在3-5%以内。

Conclusion: 级联3D扩散模型为临床和研究提供了可扩展且准确的合成图像生成方案。

Abstract: We propose a cascaded 3D diffusion model framework to synthesize
high-fidelity 3D PET/CT volumes directly from demographic variables, addressing
the growing need for realistic digital twins in oncologic imaging, virtual
trials, and AI-driven data augmentation. Unlike deterministic phantoms, which
rely on predefined anatomical and metabolic templates, our method employs a
two-stage generative process. An initial score-based diffusion model
synthesizes low-resolution PET/CT volumes from demographic variables alone,
providing global anatomical structures and approximate metabolic activity. This
is followed by a super-resolution residual diffusion model that refines spatial
resolution. Our framework was trained on 18-F FDG PET/CT scans from the AutoPET
dataset and evaluated using organ-wise volume and standardized uptake value
(SUV) distributions, comparing synthetic and real data between demographic
subgroups. The organ-wise comparison demonstrated strong concordance between
synthetic and real images. In particular, most deviations in metabolic uptake
values remained within 3-5% of the ground truth in subgroup analysis. These
findings highlight the potential of cascaded 3D diffusion models to generate
anatomically and metabolically accurate PET/CT images, offering a robust
alternative to traditional phantoms and enabling scalable, population-informed
synthetic imaging for clinical and research applications.

</details>


### [352] [STA-Risk: A Deep Dive of Spatio-Temporal Asymmetries for Breast Cancer Risk Prediction](https://arxiv.org/abs/2505.21699)
*Zhengbo Zhou,Dooman Arefan,Margarita Zuley,Jules Sumkin,Shandong Wu*

Main category: eess.IV

TL;DR: 提出了一种基于Transformer的模型STA-Risk，通过捕捉乳腺影像的空间和时间不对称性，显著提升了乳腺癌风险预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺癌风险预测模型性能有限，且忽视了纵向影像中的细微变化，亟需一种能同时利用空间和时间不对称性的新方法。

Method: STA-Risk采用侧编码和时间编码学习空间-时间不对称性，并通过定制的不对称损失函数进行优化。

Result: 在两个独立乳腺影像数据集上的实验表明，STA-Risk在1-5年风险预测中优于四种代表性SOTA模型。

Conclusion: STA-Risk通过创新地结合空间和时间不对称性，为乳腺癌风险预测提供了更准确的工具。

Abstract: Predicting the risk of developing breast cancer is an important clinical tool
to guide early intervention and tailoring personalized screening strategies.
Early risk models have limited performance and recently machine learning-based
analysis of mammogram images showed encouraging risk prediction effects. These
models however are limited to the use of a single exam or tend to overlook
nuanced breast tissue evolvement in spatial and temporal details of
longitudinal imaging exams that are indicative of breast cancer risk. In this
paper, we propose STA-Risk (Spatial and Temporal Asymmetry-based Risk
Prediction), a novel Transformer-based model that captures fine-grained
mammographic imaging evolution simultaneously from bilateral and longitudinal
asymmetries for breast cancer risk prediction. STA-Risk is innovative by the
side encoding and temporal encoding to learn spatial-temporal asymmetries,
regulated by a customized asymmetry loss. We performed extensive experiments
with two independent mammogram datasets and achieved superior performance than
four representative SOTA models for 1- to 5-year future risk prediction. Source
codes will be released upon publishing of the paper.

</details>


### [353] [Privacy-Preserving Chest X-ray Report Generation via Multimodal Federated Learning with ViT and GPT-2](https://arxiv.org/abs/2505.21715)
*Md. Zahid Hossain,Mustofa Ahmed,Most. Sharmin Sultana Samu,Md. Rakibul Islam*

Main category: eess.IV

TL;DR: 论文提出了一种基于多模态联邦学习的框架，用于从胸部X光图像生成放射学报告，解决了传统集中式方法的隐私问题。


<details>
  <summary>Details</summary>
Motivation: 传统集中式方法需要传输敏感数据，存在隐私风险，因此需要一种隐私保护的方法。

Method: 采用Vision Transformer (ViT)作为编码器，GPT-2作为报告生成器，评估了三种联邦学习聚合策略：FedAvg、Krum Aggregation和新型的L-FedAvg。

Result: Krum Aggregation在ROUGE、BLEU、BERTScore和RaTEScore等指标上表现最佳，联邦学习模型可媲美或超越集中式模型。

Conclusion: 该框架为医学AI的协作开发提供了轻量级且隐私保护的解决方案。

Abstract: The automated generation of radiology reports from chest X-ray images holds
significant promise in enhancing diagnostic workflows while preserving patient
privacy. Traditional centralized approaches often require sensitive data
transfer, posing privacy concerns. To address this, the study proposes a
Multimodal Federated Learning framework for chest X-ray report generation using
the IU-Xray dataset. The system utilizes a Vision Transformer (ViT) as the
encoder and GPT-2 as the report generator, enabling decentralized training
without sharing raw data. Three Federated Learning (FL) aggregation strategies:
FedAvg, Krum Aggregation and a novel Loss-aware Federated Averaging (L-FedAvg)
were evaluated. Among these, Krum Aggregation demonstrated superior performance
across lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore
and RaTEScore. The results show that FL can match or surpass centralized models
in generating clinically relevant and semantically rich radiology reports. This
lightweight and privacy-preserving framework paves the way for collaborative
medical AI development without compromising data confidentiality.

</details>


### [354] [Highly Efficient Non-Separable Transforms for Next Generation Video Coding](https://arxiv.org/abs/2505.21728)
*Amir Said,Xin Zhao,Marta Karczewicz,Hilmi E. Egilmez,Vadim Seregin,Jianle Chen*

Main category: eess.IV

TL;DR: 提出了一种新型参数化方法（HyGTs），显著降低视频压缩中的计算复杂度，同时保持压缩性能。


<details>
  <summary>Details</summary>
Motivation: 传统矩阵方法计算复杂度高，限制了信号自适应变换编码在视频压缩中的应用。

Method: 通过寻找定义HyGTs的最佳变换参数，而非传统全矩阵方法，实现低复杂度并行化。

Result: HyGTs在HEVC中实现，平均编码增益提升6%（比特率降低），内存使用减少6.8倍。

Conclusion: HyGTs是一种高效低复杂度的视频压缩解决方案。

Abstract: For the last few decades, the application of signal-adaptive transform coding
to video compression has been stymied by the large computational complexity of
matrix-based solutions. In this paper, we propose a novel parametric approach
to greatly reduce the complexity without degrading the compression performance.
In our approach, instead of following the conventional technique of identifying
full transform matrices that yield best compression efficiency, we look for the
best transform parameters defining a new class of transforms, called HyGTs,
which have low complexity implementations that are easy to parallelize. The
proposed HyGTs are implemented as an extension of High Efficiency Video Coding
(HEVC), and our comprehensive experimental results demonstrate that proposed
HyGTs improve average coding gain by 6% bit rate reduction, while using 6.8
times less memory than KLT matrices.

</details>


### [355] [Beyond 1D: Vision Transformers and Multichannel Signal Images for PPG-to-ECG Reconstruction](https://arxiv.org/abs/2505.21767)
*Xiaoyan Li,Shixin Xu,Faisal Habib,Arvind Gupta,Huaxiong Huang*

Main category: eess.IV

TL;DR: 提出了一种基于Vision Transformer (ViT)的多通道PPG-to-ECG重建方法，通过四通道信号图像表示和自注意力机制，显著提升了ECG重建的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从PPG重建ECG时难以捕捉细粒度波形特征，而生成模型的进步为解决这一问题提供了可能。

Method: 采用四通道信号图像表示（原始PPG、一阶差分、二阶差分和曲线下面积），结合ViT的自注意力机制，捕捉PPG的时空和生理变化。

Result: 实验表明，该方法在PRD和RMSE上分别降低了29%和15%，并在其他评估指标上表现优异。

Conclusion: 四通道信号图像表示与ViT的结合为PPG-to-ECG映射提供了更有效的特征提取和建模方法，展示了PPG作为心脏活动监测替代方案的潜力。

Abstract: Reconstructing ECG from PPG is a promising yet challenging task. While recent
advancements in generative models have significantly improved ECG
reconstruction, accurately capturing fine-grained waveform features remains a
key challenge. To address this, we propose a novel PPG-to-ECG reconstruction
method that leverages a Vision Transformer (ViT) as the core network. Unlike
conventional approaches that rely on single-channel PPG, our method employs a
four-channel signal image representation, incorporating the original PPG, its
first-order difference, second-order difference, and area under the curve. This
multi-channel design enriches feature extraction by preserving both temporal
and physiological variations within the PPG. By leveraging the self-attention
mechanism in ViT, our approach effectively captures both inter-beat and
intra-beat dependencies, leading to more robust and accurate ECG
reconstruction. Experimental results demonstrate that our method consistently
outperforms existing 1D convolution-based approaches, achieving up to 29%
reduction in PRD and 15% reduction in RMSE. The proposed approach also produces
improvements in other evaluation metrics, highlighting its robustness and
effectiveness in reconstructing ECG signals. Furthermore, to ensure a
clinically relevant evaluation, we introduce new performance metrics, including
QRS area error, PR interval error, RT interval error, and RT amplitude
difference error. Our findings suggest that integrating a four-channel signal
image representation with the self-attention mechanism of ViT enables more
effective extraction of informative PPG features and improved modeling of
beat-to-beat variations for PPG-to-ECG mapping. Beyond demonstrating the
potential of PPG as a viable alternative for heart activity monitoring, our
approach opens new avenues for cyclic signal analysis and prediction.

</details>


### [356] [Targeted Unlearning Using Perturbed Sign Gradient Methods With Applications On Medical Images](https://arxiv.org/abs/2505.21872)
*George R. Nahass,Zhu Wang,Homa Rashidisabet,Won Hwa Kim,Sasha Hubschman,Jeffrey C. Peterson,Ghasem Yazdanpanah,Chad A. Purnell,Pete Setabutr,Ann Q. Tran,Darvin Yi,Sathya N. Ravi*

Main category: eess.IV

TL;DR: 该论文提出了一种基于边界优化的机器遗忘方法，用于在临床环境中动态调整模型，避免完全重新训练。


<details>
  <summary>Details</summary>
Motivation: 研究动机是将机器遗忘从隐私保护扩展到临床模型维护，应对数据变化、设备淘汰和政策调整等挑战。

Method: 采用双层优化框架设计边界遗忘方法，支持可调损失函数和模型组合策略，使用一阶算法保证收敛。

Result: 在基准和临床影像数据集上，该方法在遗忘和保留指标上优于基线，适用于设备差异和解剖异常场景。

Conclusion: 机器遗忘成为临床应用中模块化、实用的模型维护替代方案，避免完全重新训练。

Abstract: Machine unlearning aims to remove the influence of specific training samples
from a trained model without full retraining. While prior work has largely
focused on privacy-motivated settings, we recast unlearning as a
general-purpose tool for post-deployment model revision. Specifically, we focus
on utilizing unlearning in clinical contexts where data shifts, device
deprecation, and policy changes are common. To this end, we propose a bilevel
optimization formulation of boundary-based unlearning that can be solved using
iterative algorithms. We provide convergence guarantees when first-order
algorithms are used to unlearn. Our method introduces tunable loss design for
controlling the forgetting-retention tradeoff and supports novel model
composition strategies that merge the strengths of distinct unlearning runs.
Across benchmark and real-world clinical imaging datasets, our approach
outperforms baselines on both forgetting and retention metrics, including
scenarios involving imaging devices and anatomical outliers. This work
establishes machine unlearning as a modular, practical alternative to
retraining for real-world model maintenance in clinical applications.

</details>


### [357] [MAMBO-NET: Multi-Causal Aware Modeling Backdoor-Intervention Optimization for Medical Image Segmentation Network](https://arxiv.org/abs/2505.21874)
*Ruiguo Yu,Yiyang Zhang,Yuan Tian,Yujie Diao,Di Jin,Witold Pedrycz*

Main category: eess.IV

TL;DR: 论文提出了一种多因果感知建模后门干预优化网络（MAMBO-NET），用于解决医学图像分割中混淆因素的影响，显著提高了分割准确性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割通常假设图像到分割的过程无偏，忽略了混淆因素（如解剖变异和成像模态限制）的影响，导致分割结果不理想。

Method: MAMBO-NET利用多高斯分布自建模拟合混淆因素，并引入因果干预和后验概率约束，结合后门干预技术优化分割过程。

Result: 在五个医学图像数据集上的实验表明，该方法显著减少了混淆因素的影响，提升了分割精度。

Conclusion: MAMBO-NET通过因果干预和分布建模有效解决了医学图像分割中的混淆问题，为相关任务提供了新思路。

Abstract: Medical image segmentation methods generally assume that the process from
medical image to segmentation is unbiased, and use neural networks to establish
conditional probability models to complete the segmentation task. This
assumption does not consider confusion factors, which can affect medical
images, such as complex anatomical variations and imaging modality limitations.
Confusion factors obfuscate the relevance and causality of medical image
segmentation, leading to unsatisfactory segmentation results. To address this
issue, we propose a multi-causal aware modeling backdoor-intervention
optimization (MAMBO-NET) network for medical image segmentation. Drawing
insights from causal inference, MAMBO-NET utilizes self-modeling with
multi-Gaussian distributions to fit the confusion factors and introduce causal
intervention into the segmentation process. Moreover, we design appropriate
posterior probability constraints to effectively train the distributions of
confusion factors. For the distributions to effectively guide the segmentation
and mitigate and eliminate the Impact of confusion factors on the segmentation,
we introduce classical backdoor intervention techniques and analyze their
feasibility in the segmentation task. To evaluate the effectiveness of our
approach, we conducted extensive experiments on five medical image datasets.
The results demonstrate that our method significantly reduces the influence of
confusion factors, leading to enhanced segmentation accuracy.

</details>


### [358] [Patch-based Reconstruction for Unsupervised Dynamic MRI using Learnable Tensor Function with Implicit Neural Representation](https://arxiv.org/abs/2505.21894)
*Yuanyuan Liu,Yuanbiao Yang,Zhuo-Xu Cui,Qingyong Zhu,Jing Cheng,Congcong Liu,Jinwen Xie,Jingran Xu,Hairong Zheng,Dong Liang,Yanjie Zhu*

Main category: eess.IV

TL;DR: 论文提出了一种名为TenF-INR的无监督框架，通过隐式神经表示（INR）和基于补丁的方法，高效建模动态MRI图像，显著提升了重建质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 动态MRI的高时空分辨率受限于长扫描时间，现有深度学习方法依赖大量全采样数据，难以获取。INR虽能连续建模动态图像，但在细节恢复和高维数据计算上存在挑战。

Method: 提出TenF-INR框架，利用INR建模张量分解的基，通过补丁方法实现多维低秩性和连续建模，提升效率和重建质量。

Result: 实验显示TenF-INR在高达21倍的加速下，图像质量、时间保真度和定量指标均优于现有方法，甚至超过监督学习方法。

Conclusion: TenF-INR为动态MRI加速提供了一种高效、高质量的无监督解决方案，克服了现有方法的局限性。

Abstract: Dynamic MRI plays a vital role in clinical practice by capturing both spatial
details and dynamic motion, but its high spatiotemporal resolution is often
limited by long scan times. Deep learning (DL)-based methods have shown
promising performance in accelerating dynamic MRI. However, most existing
algorithms rely on large fully-sampled datasets for training, which are
difficult to acquire. Recently, implicit neural representation (INR) has
emerged as a powerful scan-specific paradigm for accelerated MRI, which models
signals as a continuous function over spatiotemporal coordinates. Although this
approach achieves efficient continuous modeling of dynamic images and robust
reconstruction, it faces challenges in recovering fine details and increasing
computational demands for high dimensional data representation. To enhance both
efficiency and reconstruction quality, we propose TenF-INR, a novel patch-based
unsupervised framework that employs INR to model bases of tensor decomposition,
enabling efficient and accurate modeling of dynamic MR images with learnable
tensor functions. By exploiting strong correlations in similar spatial image
patches and in the temporal direction, TenF-INR enforces multidimensional
low-rankness and implements patch-based reconstruction with the benefits of
continuous modeling. We compare TenF-INR with state-of-the-art methods,
including supervised DL methods and unsupervised approaches. Experimental
results demonstrate that TenF-INR achieves high acceleration factors up to 21,
outperforming all comparison methods in image quality, temporal fidelity, and
quantitative metrics, even surpassing the supervised methods.

</details>


### [359] [Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology](https://arxiv.org/abs/2505.21928)
*Lianghui Zhu,Xitong Ling,Minxi Ouyang,Xiaoping Liu,Mingxi Fu,Tian Guan,Fanglei Fu,Xuanyu Wang,Maomao Zeng,Mingxi Zhu,Yibo Jin,Liming Liu,Song Duan,Qiming He,Yizhi Wang,Luxi Xie,Houqiang Li,Yonghong He,Sufang Tian*

Main category: eess.IV

TL;DR: Digepath是一种专为胃肠道病理学设计的AI基础模型，通过双阶段优化策略在病理诊断、分子预测等任务中表现优异，尤其在早期癌症筛查中达到99.6%的灵敏度。


<details>
  <summary>Details</summary>
Motivation: 传统病理诊断依赖主观判断，存在重复性差和诊断变异性的问题，需要更精准的AI模型来优化胃肠道疾病的诊断。

Method: 开发Digepath，采用双阶段迭代优化策略（预训练+精细筛查），基于超过35.3亿个图像块和20万张胃肠道病理切片进行训练。

Result: 在34项任务中的33项达到最优性能，包括病理诊断、分子预测等，并在早期癌症筛查中实现99.6%的高灵敏度。

Conclusion: Digepath填补了病理实践的空白，为胃肠道疾病提供了AI驱动的精准病理学解决方案，并可为其他病理学领域提供可转移的范例。

Abstract: Gastrointestinal (GI) diseases represent a clinically significant burden,
necessitating precise diagnostic approaches to optimize patient outcomes.
Conventional histopathological diagnosis, heavily reliant on the subjective
interpretation of pathologists, suffers from limited reproducibility and
diagnostic variability. To overcome these limitations and address the lack of
pathology-specific foundation models for GI diseases, we develop Digepath, a
specialized foundation model for GI pathology. Our framework introduces a
dual-phase iterative optimization strategy combining pretraining with
fine-screening, specifically designed to address the detection of sparsely
distributed lesion areas in whole-slide images. Digepath is pretrained on more
than 353 million image patches from over 200,000 hematoxylin and eosin-stained
slides of GI diseases. It attains state-of-the-art performance on 33 out of 34
tasks related to GI pathology, including pathological diagnosis, molecular
prediction, gene mutation prediction, and prognosis evaluation, particularly in
diagnostically ambiguous cases and resolution-agnostic tissue classification.We
further translate the intelligent screening module for early GI cancer and
achieve near-perfect 99.6% sensitivity across 9 independent medical
institutions nationwide. The outstanding performance of Digepath highlights its
potential to bridge critical gaps in histopathological practice. This work not
only advances AI-driven precision pathology for GI diseases but also
establishes a transferable paradigm for other pathology subspecialties.

</details>


### [360] [Collaborative Learning for Unsupervised Multimodal Remote Sensing Image Registration: Integrating Self-Supervision and MIM-Guided Diffusion-Based Image Translation](https://arxiv.org/abs/2505.22000)
*Xiaochen Wei,Weiwei Guo,Wenxian Yu*

Main category: eess.IV

TL;DR: 提出了一种名为CoLReg的无监督多模态图像配准协作学习框架，通过三个网络组件协同优化，显著提升了配准精度。


<details>
  <summary>Details</summary>
Motivation: 多模态图像配准面临模态差异大、标注数据稀缺等挑战，传统无监督方法难以稳定收敛。

Method: CoLReg包含三个组件：跨模态图像翻译网络MIMGCD、自监督中间配准网络和蒸馏跨模态配准网络，通过交替训练策略协同优化。

Result: 在多个数据集上，CoLReg表现优于现有无监督方法，甚至超越部分有监督基线。

Conclusion: CoLReg通过协作学习有效减少模态差异，提升配准精度，具有广泛应用潜力。

Abstract: The substantial modality-induced variations in radiometric, texture, and
structural characteristics pose significant challenges for the accurate
registration of multimodal images. While supervised deep learning methods have
demonstrated strong performance, they often rely on large-scale annotated
datasets, limiting their practical application. Traditional unsupervised
methods usually optimize registration by minimizing differences in feature
representations, yet often fail to robustly capture geometric discrepancies,
particularly under substantial spatial and radiometric variations, thus
hindering convergence stability. To address these challenges, we propose a
Collaborative Learning framework for Unsupervised Multimodal Image
Registration, named CoLReg, which reformulates unsupervised registration
learning into a collaborative training paradigm comprising three components:
(1) a cross-modal image translation network, MIMGCD, which employs a learnable
Maximum Index Map (MIM) guided conditional diffusion model to synthesize
modality-consistent image pairs; (2) a self-supervised intermediate
registration network which learns to estimate geometric transformations using
accurate displacement labels derived from MIMGCD outputs; (3) a distilled
cross-modal registration network trained with pseudo-label predicted by the
intermediate network. The three networks are jointly optimized through an
alternating training strategy wherein each network enhances the performance of
the others. This mutual collaboration progressively reduces modality
discrepancies, enhances the quality of pseudo-labels, and improves registration
accuracy. Extensive experimental results on multiple datasets demonstrate that
our ColReg achieves competitive or superior performance compared to
state-of-the-art unsupervised approaches and even surpasses several supervised
baselines.

</details>


### [361] [High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models](https://arxiv.org/abs/2505.22090)
*Tristan S. W. Stevens,Oisín Nolan,Oudom Somphone,Jean-Luc Robert,Ruud J. G. van Sloun*

Main category: eess.IV

TL;DR: 本文提出了一种基于扩散模型（DMs）的3D超声重建方法，通过减少高程平面的采样数量，提高了空间和时间分辨率，并在图像质量和下游任务性能上优于传统和深度学习基线方法。


<details>
  <summary>Details</summary>
Motivation: 3D超声技术虽然能实时可视化解剖结构，但高体积率和高图像质量的平衡仍具挑战性。传统方法在减少高程平面采样时，图像质量会下降。

Method: 采用扩散模型（DMs）从减少的高程平面中重建3D超声图像，利用超声序列的时间一致性加速推理，并通过扩散后验采样量化重建不确定性。

Result: 实验表明，基于扩散模型的重建方法在图像质量和下游任务性能上优于基线方法，并在异常数据下表现出更高的鲁棒性。

Conclusion: 扩散模型为3D超声重建提供了一种高效且鲁棒的方法，显著提升了图像质量和分析性能。

Abstract: Three-dimensional ultrasound enables real-time volumetric visualization of
anatomical structures. Unlike traditional 2D ultrasound, 3D imaging reduces the
reliance on precise probe orientation, potentially making ultrasound more
accessible to clinicians with varying levels of experience and improving
automated measurements and post-exam analysis. However, achieving both high
volume rates and high image quality remains a significant challenge. While 3D
diverging waves can provide high volume rates, they suffer from limited tissue
harmonic generation and increased multipath effects, which degrade image
quality. One compromise is to retain the focusing in elevation while leveraging
unfocused diverging waves in the lateral direction to reduce the number of
transmissions per elevation plane. Reaching the volume rates achieved by full
3D diverging waves, however, requires dramatically undersampling the number of
elevation planes. Subsequently, to render the full volume, simple interpolation
techniques are applied. This paper introduces a novel approach to 3D ultrasound
reconstruction from a reduced set of elevation planes by employing diffusion
models (DMs) to achieve increased spatial and temporal resolution. We compare
both traditional and supervised deep learning-based interpolation methods on a
3D cardiac ultrasound dataset. Our results show that DM-based reconstruction
consistently outperforms the baselines in image quality and downstream task
performance. Additionally, we accelerate inference by leveraging the temporal
consistency inherent to ultrasound sequences. Finally, we explore the
robustness of the proposed method by exploiting the probabilistic nature of
diffusion posterior sampling to quantify reconstruction uncertainty and
demonstrate improved recall on out-of-distribution data with synthetic
anomalies under strong subsampling.

</details>


### [362] [MONSTR: Model-Oriented Neutron Strain Tomographic Reconstruction](https://arxiv.org/abs/2505.22187)
*Mohammad Samin Nur Chowdhury,Shimin Tang,Singanallur V. Venkatakrishnan,Hassina Z. Bilheux,Gregery T. Buzzard,Charles A. Bouman*

Main category: eess.IV

TL;DR: 论文提出了一种名为MONSTR的算法，用于从布拉格边缘应变测量中重建二维残余应变张量，解决了传统重建方法的不适定问题。


<details>
  <summary>Details</summary>
Motivation: 残余应变是影响金属部件性能的关键材料特性，但传统中子布拉格边缘应变断层扫描技术由于数据不足导致重建问题严重不适定。

Method: 提出MONSTR算法，基于多智能体共识平衡框架，结合探测器物理、断层重建过程和连续介质力学约束，实现张量重建。

Result: 模拟数据显示，即使使用极少测量数据，MONSTR也能高质量重建应变张量。

Conclusion: MONSTR算法有效解决了残余应变张量重建的挑战，为材料性能分析提供了新工具。

Abstract: Residual strain, a tensor quantity, is a critical material property that
impacts the overall performance of metal parts. Neutron Bragg edge strain
tomography is a technique for imaging residual strain that works by making
conventional hyperspectral computed tomography measurements, extracting the
average projected strain at each detector pixel, and processing the resulting
strain sinogram using a reconstruction algorithm. However, the reconstruction
is severely ill-posed as the underlying inverse problem involves inferring a
tensor at each voxel from scalar sinogram data.
  In this paper, we introduce the model-oriented neutron strain tomographic
reconstruction (MONSTR) algorithm that reconstructs the 2D residual strain
tensor from the neutron Bragg edge strain measurements. MONSTR is based on
using the multi-agent consensus equilibrium framework for the tensor
tomographic reconstruction. Specifically, we formulate the reconstruction as a
consensus solution of a collection of agents representing detector physics, the
tomographic reconstruction process, and physics-based constraints from
continuum mechanics. Using simulated data, we demonstrate high-quality
reconstruction of the strain tensor even when using very few measurements.

</details>


### [363] [Risk-Sensitive Conformal Prediction for Catheter Placement Detection in Chest X-rays](https://arxiv.org/abs/2505.22496)
*Long Hui*

Main category: eess.IV

TL;DR: 提出了一种结合多任务学习和风险敏感共形预测的新方法，用于胸部X光中导管和管线位置的检测，显著提高了临床可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决临床中对导管和管线位置检测的高可靠性需求，尤其是在关键临床发现中。

Method: 采用多任务学习同时进行分类、分割和标志点检测，并结合风险敏感共形预测以提供统计保证的预测集。

Result: 实验结果显示90.68%的总体覆盖率，关键条件下的覆盖率达99.29%，且未出现高风险误判。

Conclusion: 该方法不仅预测准确，还能可靠量化不确定性，非常适合临床部署。

Abstract: This paper presents a novel approach to catheter and line position detection
in chest X-rays, combining multi-task learning with risk-sensitive conformal
prediction to address critical clinical requirements. Our model simultaneously
performs classification, segmentation, and landmark detection, leveraging the
synergistic relationship between these tasks to improve overall performance. We
further enhance clinical reliability through risk-sensitive conformal
prediction, which provides statistically guaranteed prediction sets with higher
reliability for clinically critical findings. Experimental results demonstrate
excellent performance with 90.68\% overall empirical coverage and 99.29\%
coverage for critical conditions, while maintaining remarkable precision in
prediction sets. Most importantly, our risk-sensitive approach achieves zero
high-risk mispredictions (cases where the system dangerously declares
problematic tubes as confidently normal), making the system particularly
suitable for clinical deployment. This work offers both accurate predictions
and reliably quantified uncertainty -- essential features for life-critical
medical applications.

</details>


### [364] [Surf2CT: Cascaded 3D Flow Matching Models for Torso 3D CT Synthesis from Skin Surface](https://arxiv.org/abs/2505.22511)
*Siyeop Yoon,Yujin Oh,Pengfei Jin,Sifan Song,Matthew Tivnan,Dufan Wu,Xiang Li,Quanzheng Li*

Main category: eess.IV

TL;DR: Surf2CT是一种新颖的级联流匹配框架，能够仅通过外部表面扫描和简单人口统计数据生成完整的人体躯干3D CT图像。


<details>
  <summary>Details</summary>
Motivation: 传统CT扫描需要侵入性操作且成本高，Surf2CT旨在通过非侵入性方法生成内部解剖图像，为家庭医疗和预防医学提供新途径。

Method: Surf2CT分为三个阶段：表面补全、粗分辨率CT合成和高分辨率CT超分辨率，每个阶段均采用3D流匹配模型。

Result: 在700个测试案例中，Surf2CT表现出高解剖保真度，器官体积误差小（-11.1%至4.4%），肌肉/脂肪组成与真实数据强相关（0.67至0.96）。

Conclusion: Surf2CT为非侵入性内部解剖成像开辟了新范式，具有广泛的应用潜力。

Abstract: We present Surf2CT, a novel cascaded flow matching framework that synthesizes
full 3D computed tomography (CT) volumes of the human torso from external
surface scans and simple demographic data (age, sex, height, weight). This is
the first approach capable of generating realistic volumetric internal anatomy
images solely based on external body shape and demographics, without any
internal imaging. Surf2CT proceeds through three sequential stages: (1) Surface
Completion, reconstructing a complete signed distance function (SDF) from
partial torso scans using conditional 3D flow matching; (2) Coarse CT
Synthesis, generating a low-resolution CT volume from the completed SDF and
demographic information; and (3) CT Super-Resolution, refining the coarse
volume into a high-resolution CT via a patch-wise conditional flow model. Each
stage utilizes a 3D-adapted EDM2 backbone trained via flow matching. We trained
our model on a combined dataset of 3,198 torso CT scans (approximately 1.13
million axial slices) sourced from Massachusetts General Hospital (MGH) and the
AutoPET challenge. Evaluation on 700 paired torso surface-CT cases demonstrated
strong anatomical fidelity: organ volumes exhibited small mean percentage
differences (range from -11.1% to 4.4%), and muscle/fat body composition
metrics matched ground truth with strong correlation (range from 0.67 to 0.96).
Lung localization had minimal bias (mean difference -2.5 mm), and surface
completion significantly improved metrics (Chamfer distance: from 521.8 mm to
2.7 mm; Intersection-over-Union: from 0.87 to 0.98). Surf2CT establishes a new
paradigm for non-invasive internal anatomical imaging using only external data,
opening opportunities for home-based healthcare, preventive medicine, and
personalized clinical assessments without the risks associated with
conventional imaging techniques.

</details>


### [365] [ConfLUNet: Multiple sclerosis lesion instance segmentation in presence of confluent lesions](https://arxiv.org/abs/2505.22537)
*Maxence Wynen,Pedro M. Gordaliza,Maxime Istasse,Anna Stölting,Pietro Maggi,Benoît Macq,Meritxell Bach Cuadra*

Main category: eess.IV

TL;DR: 论文提出了一种新的端到端实例分割框架ConfLUNet，用于多发性硬化症（MS）病灶的精确分割，解决了现有方法在分离融合病灶（CLUs）时的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖连通组件（CC）和自动融合分割（ACLS），无法准确分离融合病灶，与临床需求不符。

Method: 提出ConfLUNet，通过联合优化病灶检测和分割，从单一FLAIR图像中实现端到端实例分割。

Result: ConfLUNet在测试集上显著优于CC和ACLS，提升了病灶分割和检测的准确性，尤其在CLU检测中表现最佳。

Conclusion: 该研究为MS病灶实例分割奠定了基础，提供了严格的定义、新指标和可复现的评估框架。

Abstract: Accurate lesion-level segmentation on MRI is critical for multiple sclerosis
(MS) diagnosis, prognosis, and disease monitoring. However, current evaluation
practices largely rely on semantic segmentation post-processed with connected
components (CC), which cannot separate confluent lesions (aggregates of
confluent lesion units, CLUs) due to reliance on spatial connectivity. To
address this misalignment with clinical needs, we introduce formal definitions
of CLUs and associated CLU-aware detection metrics, and include them in an
exhaustive instance segmentation evaluation framework. Within this framework,
we systematically evaluate CC and post-processing-based Automated Confluent
Splitting (ACLS), the only existing methods for lesion instance segmentation in
MS. Our analysis reveals that CC consistently underestimates CLU counts, while
ACLS tends to oversplit lesions, leading to overestimated lesion counts and
reduced precision. To overcome these limitations, we propose ConfLUNet, the
first end-to-end instance segmentation framework for MS lesions. ConfLUNet
jointly optimizes lesion detection and delineation from a single FLAIR image.
Trained on 50 patients, ConfLUNet significantly outperforms CC and ACLS on the
held-out test set (n=13) in instance segmentation (Panoptic Quality: 42.0% vs.
37.5%/36.8%; p = 0.017/0.005) and lesion detection (F1: 67.3% vs. 61.6%/59.9%;
p = 0.028/0.013). For CLU detection, ConfLUNet achieves the highest F1[CLU]
(81.5%), improving recall over CC (+12.5%, p = 0.015) and precision over ACLS
(+31.2%, p = 0.003). By combining rigorous definitions, new CLU-aware metrics,
a reproducible evaluation framework, and the first dedicated end-to-end model,
this work lays the foundation for lesion instance segmentation in MS.

</details>


### [366] [Multipath cycleGAN for harmonization of paired and unpaired low-dose lung computed tomography reconstruction kernels](https://arxiv.org/abs/2505.22568)
*Aravind R. Krishnan,Thomas Z. Li,Lucas W. Remedios,Michael E. Kim,Chenyu Gao,Gaurav Rudravaram,Elyssa M. McMaster,Adam M. Saunders,Shunxing Bao,Kaiwen Xu,Lianrui Zuo,Kim L. Sandler,Fabien Maldonado,Yuankai Huo,Bennett A. Landman*

Main category: eess.IV

TL;DR: 提出一种多路径cycleGAN模型，用于CT核谐调，以改善肺气肿定量分析的一致性。


<details>
  <summary>Details</summary>
Motivation: CT重建核影响空间分辨率和噪声特性，导致定量成像测量（如肺气肿量化）存在系统性差异，因此选择合适的核至关重要。

Method: 使用多路径cycleGAN模型，结合配对和非配对数据训练，具有特定领域的编码器和解码器，共享潜在空间，并为每个领域定制判别器。

Result: 模型减少了配对核的肺气肿评分偏差，消除了非配对核的混淆差异，同时保持了肌肉和脂肪解剖结构的保真度。

Conclusion: 共享潜在空间的多路径cycleGAN能够实现稳健的核谐调，改善肺气肿定量并保持解剖结构的一致性。

Abstract: Reconstruction kernels in computed tomography (CT) affect spatial resolution
and noise characteristics, introducing systematic variability in quantitative
imaging measurements such as emphysema quantification. Choosing an appropriate
kernel is therefore essential for consistent quantitative analysis. We propose
a multipath cycleGAN model for CT kernel harmonization, trained on a mixture of
paired and unpaired data from a low-dose lung cancer screening cohort. The
model features domain-specific encoders and decoders with a shared latent space
and uses discriminators tailored for each domain.We train the model on 42
kernel combinations using 100 scans each from seven representative kernels in
the National Lung Screening Trial (NLST) dataset. To evaluate performance, 240
scans from each kernel are harmonized to a reference soft kernel, and emphysema
is quantified before and after harmonization. A general linear model assesses
the impact of age, sex, smoking status, and kernel on emphysema. We also
evaluate harmonization from soft kernels to a reference hard kernel. To assess
anatomical consistency, we compare segmentations of lung vessels, muscle, and
subcutaneous adipose tissue generated by TotalSegmentator between harmonized
and original images. Our model is benchmarked against traditional and
switchable cycleGANs. For paired kernels, our approach reduces bias in
emphysema scores, as seen in Bland-Altman plots (p<0.05). For unpaired kernels,
harmonization eliminates confounding differences in emphysema (p>0.05). High
Dice scores confirm preservation of muscle and fat anatomy, while lung vessel
overlap remains reasonable. Overall, our shared latent space multipath cycleGAN
enables robust harmonization across paired and unpaired CT kernels, improving
emphysema quantification and preserving anatomical fidelity.

</details>


### [367] [Comparative Analysis of Machine Learning Models for Lung Cancer Mutation Detection and Staging Using 3D CT Scans](https://arxiv.org/abs/2505.22592)
*Yiheng Li,Francisco Carrillo-Perez,Mohammed Alawad,Olivier Gevaert*

Main category: eess.IV

TL;DR: 论文比较了两种机器学习模型（FMCIB+XGBoost和Dinov2+ABMIL）在肺癌突变检测和分期任务中的表现，发现监督模型在突变检测中表现更优，而自监督模型在分期任务中具有更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 肺癌是全球癌症死亡的主要原因，非侵入性方法检测关键突变和分期对改善患者预后至关重要。

Method: 比较了FMCIB+XGBoost（监督模型）和Dinov2+ABMIL（自监督模型）在3D肺结节数据上的性能，分别用于KRAS/EGFR突变检测和癌症分期。

Result: FMCIB+XGBoost在突变检测中表现更优（KRAS准确率0.846，EGFR准确率0.883），而Dinov2+ABMIL在分期任务中表现更好（T分期准确率0.797）。

Conclusion: 监督模型在突变检测中更具临床价值，自监督模型在分期任务中表现良好，但突变敏感性仍需改进。

Abstract: Lung cancer is the leading cause of cancer mortality worldwide, and
non-invasive methods for detecting key mutations and staging are essential for
improving patient outcomes. Here, we compare the performance of two machine
learning models - FMCIB+XGBoost, a supervised model with domain-specific
pretraining, and Dinov2+ABMIL, a self-supervised model with attention-based
multiple-instance learning - on 3D lung nodule data from the Stanford
Radiogenomics and Lung-CT-PT-Dx cohorts. In the task of KRAS and EGFR mutation
detection, FMCIB+XGBoost consistently outperformed Dinov2+ABMIL, achieving
accuracies of 0.846 and 0.883 for KRAS and EGFR mutations, respectively. In
cancer staging, Dinov2+ABMIL demonstrated competitive generalization, achieving
an accuracy of 0.797 for T-stage prediction in the Lung-CT-PT-Dx cohort,
suggesting SSL's adaptability across diverse datasets. Our results emphasize
the clinical utility of supervised models in mutation detection and highlight
the potential of SSL to improve staging generalization, while identifying areas
for enhancement in mutation sensitivity.

</details>


### [368] [Chest Disease Detection In X-Ray Images Using Deep Learning Classification Method](https://arxiv.org/abs/2505.22609)
*Alanna Hazlett,Naomi Ohashi,Timothy Rodriguez,Sodiq Adewole*

Main category: eess.IV

TL;DR: 该研究通过迁移学习技术，利用预训练的卷积神经网络（CNN）对胸部X光图像进行分类，分为COVID-19、肺炎、结核病和正常四类，取得了高准确率和良好的分类性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过深度学习技术提高胸部X光图像的分类准确性，为临床诊断提供可靠支持。

Method: 采用迁移学习技术，对预训练的CNN模型进行微调，并使用Grad-CAM增强模型的可解释性。

Result: 实验结果显示高准确率和优异的分类性能指标（如精确率、召回率和F1分数）。

Conclusion: 该方法在胸部X光图像分类中表现优异，且通过Grad-CAM提升了模型的可解释性，适用于临床实践。

Abstract: In this work, we investigate the performance across multiple classification
models to classify chest X-ray images into four categories of COVID-19,
pneumonia, tuberculosis (TB), and normal cases. We leveraged transfer learning
techniques with state-of-the-art pre-trained Convolutional Neural Networks
(CNNs) models. We fine-tuned these pre-trained architectures on a labeled
medical x-ray images. The initial results are promising with high accuracy and
strong performance in key classification metrics such as precision, recall, and
F1 score. We applied Gradient-weighted Class Activation Mapping (Grad-CAM) for
model interpretability to provide visual explanations for classification
decisions, improving trust and transparency in clinical applications.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [369] [Towards Human-Like Trajectory Prediction for Autonomous Driving: A Behavior-Centric Approach](https://arxiv.org/abs/2505.21565)
*Haicheng Liao,Zhenning Li,Guohui Zhang,Keqiang Li,Chengzhong Xu*

Main category: cs.RO

TL;DR: HiT模型通过行为感知模块和动态中心性度量提升轨迹预测，优于传统静态图方法，在复杂交通场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 提升自动驾驶系统在复杂动态交通环境中的轨迹预测能力。

Method: HiT模型结合行为感知模块和动态中心性度量，捕捉交通参与者的直接与间接交互。

Result: 在多个真实数据集（如NGSIM、HighD等）上，HiT表现优于其他模型，尤其在激进驾驶场景中。

Conclusion: HiT为轨迹预测提供了更可靠、可解释的方法，有助于提升自动驾驶的安全性和效率。

Abstract: Predicting the trajectories of vehicles is crucial for the development of
autonomous driving (AD) systems, particularly in complex and dynamic traffic
environments. In this study, we introduce HiT (Human-like Trajectory
Prediction), a novel model designed to enhance trajectory prediction by
incorporating behavior-aware modules and dynamic centrality measures. Unlike
traditional methods that primarily rely on static graph structures, HiT
leverages a dynamic framework that accounts for both direct and indirect
interactions among traffic participants. This allows the model to capture the
subtle yet significant influences of surrounding vehicles, enabling more
accurate and human-like predictions. To evaluate HiT's performance, we
conducted extensive experiments using diverse and challenging real-world
datasets, including NGSIM, HighD, RounD, ApolloScape, and MoCAD++. The results
demonstrate that HiT consistently outperforms other top models across multiple
metrics, particularly excelling in scenarios involving aggressive driving
behaviors. This research presents a significant step forward in trajectory
prediction, offering a more reliable and interpretable approach for enhancing
the safety and efficiency of fully autonomous driving systems.

</details>


### [370] [CogAD: Cognitive-Hierarchy Guided End-to-End Autonomous Driving](https://arxiv.org/abs/2505.21581)
*Zhennan Wang,Jianing Teng,Canqun Xiang,Kangliang Chen,Xing Pan,Lu Deng,Weihao Gu*

Main category: cs.RO

TL;DR: CogAD是一种新型端到端自动驾驶模型，模拟人类驾驶员的层次认知机制，通过全局到局部上下文处理和意图条件多模式轨迹生成，实现更符合人类认知的感知与规划。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法与人类认知原则在感知和规划上存在根本性不一致，因此提出CogAD以模拟人类驾驶员的层次认知机制。

Method: CogAD采用双重层次机制：全局到局部上下文处理用于感知，意图条件多模式轨迹生成用于规划，并通过双层次不确定性建模实现多样化轨迹生成。

Result: 在nuScenes和Bench2Drive数据集上，CogAD在端到端规划中达到最先进性能，尤其在长尾场景和复杂现实驾驶条件下表现优越。

Conclusion: CogAD通过模拟人类认知机制，显著提升了自动驾驶的感知和规划能力，尤其在复杂场景中表现出色。

Abstract: While end-to-end autonomous driving has advanced significantly, prevailing
methods remain fundamentally misaligned with human cognitive principles in both
perception and planning. In this paper, we propose CogAD, a novel end-to-end
autonomous driving model that emulates the hierarchical cognition mechanisms of
human drivers. CogAD implements dual hierarchical mechanisms: global-to-local
context processing for human-like perception and intent-conditioned multi-mode
trajectory generation for cognitively-inspired planning. The proposed method
demonstrates three principal advantages: comprehensive environmental
understanding through hierarchical perception, robust planning exploration
enabled by multi-level planning, and diverse yet reasonable multi-modal
trajectory generation facilitated by dual-level uncertainty modeling. Extensive
experiments on nuScenes and Bench2Drive demonstrate that CogAD achieves
state-of-the-art performance in end-to-end planning, exhibiting particular
superiority in long-tail scenarios and robust generalization to complex
real-world driving conditions.

</details>


### [371] [Fast and Cost-effective Speculative Edge-Cloud Decoding with Early Exits](https://arxiv.org/abs/2505.21594)
*Yeshwanth Venkatesha,Souvik Kundu,Priyadarshini Panda*

Main category: cs.RO

TL;DR: 提出了一种边缘-云协同的解码框架，通过小模型在设备端预生成令牌，大模型在云端验证，显著降低延迟并提升效率。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）在边缘设备上部署时的高成本和资源限制问题，提供一种快速、低成本的解决方案。

Method: 采用边缘-云协同解码框架，设备端使用小模型（Vicuna-68M）预生成令牌，云端大模型（Llama2-7B）验证并引入提前退出机制，利用空闲时间提升并行性。

Result: 实验显示，相比云端自回归解码，延迟降低35%，预生成令牌进一步提速11%；在四足机器人上实现21%的速度提升。

Conclusion: 该框架为资源受限的边缘设备提供了实时LLM和VLM应用的可行方案。

Abstract: Large Language Models (LLMs) enable various applications on edge devices such
as smartphones, wearables, and embodied robots. However, their deployment often
depends on expensive cloud-based APIs, creating high operational costs, which
limit access for smaller organizations and raise sustainability concerns.
Certain LLMs can be deployed on-device, offering a cost-effective solution with
reduced latency and improved privacy. Yet, limited computing resources
constrain the size and accuracy of models that can be deployed, necessitating a
collaborative design between edge and cloud. We propose a fast and
cost-effective speculative edge-cloud decoding framework with a large target
model on the server and a small draft model on the device. By introducing early
exits in the target model, tokens are generated mid-verification, allowing the
client to preemptively draft subsequent tokens before final verification, thus
utilizing idle time and enhancing parallelism between edge and cloud. Using an
NVIDIA Jetson Nano (client) and an A100 GPU (server) with Vicuna-68M (draft)
and Llama2-7B (target) models, our method achieves up to a 35% reduction in
latency compared to cloud-based autoregressive decoding, with an additional 11%
improvement from preemptive drafting. To demonstrate real-world applicability,
we deploy our method on the Unitree Go2 quadruped robot using Vision-Language
Model (VLM) based control, achieving a 21% speedup over traditional cloud-based
autoregressive decoding. These results demonstrate the potential of our
framework for real-time LLM and VLM applications on resource-constrained edge
devices.

</details>


### [372] [PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation](https://arxiv.org/abs/2505.21652)
*Yifan Yin,Zhengtao Han,Shivam Aarya,Jianxin Wang,Shuhang Xu,Jiawei Peng,Angtian Wang,Alan Yuille,Tianmin Shu*

Main category: cs.RO

TL;DR: 论文提出了PartInstruct，首个用于细粒度机器人操作的大规模基准数据集，包含513个物体实例和1302个任务，并评估了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有缺乏细粒度操作任务的大规模数据集，尤其是带有部件级标注和指令的数据集，限制了机器人操作模型的发展。

Method: 构建了PartInstruct数据集，包含部件级标注和任务指令，并通过3D模拟器生成专家演示。评估了多种现有机器人操作方法。

Result: 实验表明，现有模型在部件概念理解和长时任务操作上表现不佳。

Conclusion: PartInstruct为细粒度机器人操作提供了重要基准，揭示了当前模型的不足，为未来研究指明了方向。

Abstract: Fine-grained robot manipulation, such as lifting and rotating a bottle to
display the label on the cap, requires robust reasoning about object parts and
their relationships with intended tasks. Despite recent advances in training
general-purpose robot manipulation policies guided by language instructions,
there is a notable lack of large-scale datasets for fine-grained manipulation
tasks with part-level instructions and diverse 3D object instances annotated
with part-level labels. In this work, we introduce PartInstruct, the first
large-scale benchmark for training and evaluating fine-grained robot
manipulation models using part-level instructions. PartInstruct comprises 513
object instances across 14 categories, each annotated with part-level
information, and 1302 fine-grained manipulation tasks organized into 16 task
classes. Our training set consists of over 10,000 expert demonstrations
synthesized in a 3D simulator, where each demonstration is paired with a
high-level task instruction, a chain of base part-based skill instructions, and
ground-truth 3D information about the object and its parts. Additionally, we
designed a comprehensive test suite to evaluate the generalizability of learned
policies across new states, objects, and tasks. We evaluated several
state-of-the-art robot manipulation approaches, including end-to-end
vision-language policy learning and bi-level planning models for robot
manipulation on our benchmark. The experimental results reveal that current
models struggle to robustly ground part concepts and predict actions in 3D
space, and face challenges when manipulating object parts in long-horizon
tasks.

</details>


### [373] [Convergent Functions, Divergent Forms](https://arxiv.org/abs/2505.21665)
*Hyeonseong Jeon,Ainaz Eftekhar,Aaron Walsman,Kuo-Hao Zeng,Ali Farhadi,Ranjay Krishna*

Main category: cs.RO

TL;DR: LOKI是一个高效的计算框架，用于共同设计形态和控制策略，能够泛化到未见过的任务。通过共享策略和动态局部搜索，显著减少了训练成本，并探索了更多样化的设计。


<details>
  <summary>Details</summary>
Motivation: 受生物适应性启发，解决传统进化算法和多样性质量算法的低效问题，旨在设计多样且适应性强的形态。

Method: 学习收敛函数（共享策略）和动态局部搜索，以减少训练成本并促进形态多样性。

Result: 探索了780倍的设计，减少了78%的模拟步骤和40%的计算成本，生成了多样且高性能的形态。

Conclusion: LOKI在多样性和适应性上优于现有方法，具有更高的样本效率。

Abstract: We introduce LOKI, a compute-efficient framework for co-designing
morphologies and control policies that generalize across unseen tasks. Inspired
by biological adaptation -- where animals quickly adjust to morphological
changes -- our method overcomes the inefficiencies of traditional evolutionary
and quality-diversity algorithms. We propose learning convergent functions:
shared control policies trained across clusters of morphologically similar
designs in a learned latent space, drastically reducing the training cost per
design. Simultaneously, we promote divergent forms by replacing mutation with
dynamic local search, enabling broader exploration and preventing premature
convergence. The policy reuse allows us to explore 780$\times$ more designs
using 78% fewer simulation steps and 40% less compute per design. Local
competition paired with a broader search results in a diverse set of
high-performing final morphologies. Using the UNIMAL design space and a
flat-terrain locomotion task, LOKI discovers a rich variety of designs --
ranging from quadrupeds to crabs, bipedals, and spinners -- far more diverse
than those produced by prior work. These morphologies also transfer better to
unseen downstream tasks in agility, stability, and manipulation domains (e.g.,
2$\times$ higher reward on bump and push box incline tasks). Overall, our
approach produces designs that are both diverse and adaptable, with
substantially greater sample efficiency than existing co-design methods.
(Project website: https://loki-codesign.github.io/)

</details>


### [374] [Real-World Deployment of Cloud Autonomous Mobility System Using 5G Networks for Outdoor and Indoor Environments](https://arxiv.org/abs/2505.21676)
*Yufeng Yang,Minghao Ning,Keqi Shu,Aladdin Saleh,Ehsan Hashemi,Amir Khajepour*

Main category: cs.RO

TL;DR: 本文介绍了一种基于5G网络的云自主移动（CAM）系统，通过分布式传感器节点和云计算实现实时感知与通信，并在城市和室内环境中验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 解决户外和室内移动系统日益复杂的感知与通信需求，提供可扩展、经济高效的解决方案。

Method: 利用LiDAR和摄像头感知基础设施，结合5G网络的URLLC技术，通过云计算进行全局信息融合。

Result: 在城市环岛和医院走廊的实验中，系统提升了交通监控、危险检测和资产管理能力。

Conclusion: 云基础设施感知技术有望推动户外和室内智能交通系统的发展。

Abstract: The growing complexity of both outdoor and indoor mobility systems demands
scalable, cost-effective, and reliable perception and communication frameworks.
This work presents the real-world deployment and evaluation of a Cloud
Autonomous Mobility (CAM) system that leverages distributed sensor nodes
connected via 5G networks, which integrates LiDAR- and camera-based perception
at infrastructure units, cloud computing for global information fusion, and
Ultra-Reliable Low Latency Communications (URLLC) to enable real-time
situational awareness and autonomous operation. The CAM system is deployed in
two distinct environments: a dense urban roundabout and a narrow indoor
hospital corridor. Field experiments show improved traffic monitoring, hazard
detection, and asset management capabilities. The paper also discusses
practical deployment challenges and shares key insights for scaling CAM
systems. The results highlight the potential of cloud-based infrastructure
perception to advance both outdoor and indoor intelligent transportation
systems.

</details>


### [375] [MIND-Stack: Modular, Interpretable, End-to-End Differentiability for Autonomous Navigation](https://arxiv.org/abs/2505.21734)
*Felix Jahncke,Johannes Betz*

Main category: cs.RO

TL;DR: MIND-Stack是一个模块化软件堆栈，结合了定位网络和Stanley控制器，具有可解释性和端到端可微性，提升了导航算法的性能和透明度。


<details>
  <summary>Details</summary>
Motivation: 开发既高效又可解释的导航算法，解决基于规则方法和端到端神经网络的局限性。

Method: 提出MIND-Stack，包含定位网络和Stanley控制器，支持端到端训练和模块化设计。

Result: 实验表明定位模块能减少控制误差，性能优于现有算法，并在真实嵌入式平台上验证。

Conclusion: MIND-Stack展示了模块化和端到端训练的优势，未来可扩展更多模块以进一步提升性能。

Abstract: Developing robust, efficient navigation algorithms is challenging. Rule-based
methods offer interpretability and modularity but struggle with learning from
large datasets, while end-to-end neural networks excel in learning but lack
transparency and modularity. In this paper, we present MIND-Stack, a modular
software stack consisting of a localization network and a Stanley Controller
with intermediate human interpretable state representations and end-to-end
differentiability. Our approach enables the upstream localization module to
reduce the downstream control error, extending its role beyond state
estimation. Unlike existing research on differentiable algorithms that either
lack modules of the autonomous stack to span from sensor input to actuator
output or real-world implementation, MIND-Stack offers both capabilities. We
conduct experiments that demonstrate the ability of the localization module to
reduce the downstream control loss through its end-to-end differentiability
while offering better performance than state-of-the-art algorithms. We showcase
sim-to-real capabilities by deploying the algorithm on a real-world embedded
autonomous platform with limited computation power and demonstrate simultaneous
training of both the localization and controller towards one goal. While
MIND-Stack shows good results, we discuss the incorporation of additional
modules from the autonomous navigation pipeline in the future, promising even
greater stability and performance in the next iterations of the framework.

</details>


### [376] [Streaming Flow Policy: Simplifying diffusion$/$flow-matching policies by treating action trajectories as flow trajectories](https://arxiv.org/abs/2505.21851)
*Sunshine Jiang,Xiaolin Fang,Nicholas Roy,Tomás Lozano-Pérez,Leslie Pack Kaelbling,Siddharth Ancha*

Main category: cs.RO

TL;DR: 论文提出了一种简化扩散/流匹配策略的方法，通过将动作轨迹视为流轨迹，实现实时动作流传输，提高机器人控制的效率。


<details>
  <summary>Details</summary>
Motivation: 现有扩散/流匹配策略计算成本高，需等待完整采样后才能执行动作，限制了实时性。

Method: 从窄高斯分布采样，逐步积分学习的速度场，生成动作序列，支持实时流传输。

Result: 新方法在保持多模态行为建模能力的同时，提高了执行速度和传感器-运动循环的紧密性。

Conclusion: 流式流策略优于现有方法，适用于基于学习的机器人控制。

Abstract: Recent advances in diffusion$/$flow-matching policies have enabled imitation
learning of complex, multi-modal action trajectories. However, they are
computationally expensive because they sample a trajectory of trajectories: a
diffusion$/$flow trajectory of action trajectories. They discard intermediate
action trajectories, and must wait for the sampling process to complete before
any actions can be executed on the robot. We simplify diffusion$/$flow policies
by treating action trajectories as flow trajectories. Instead of starting from
pure noise, our algorithm samples from a narrow Gaussian around the last
action. Then, it incrementally integrates a velocity field learned via flow
matching to produce a sequence of actions that constitute a single trajectory.
This enables actions to be streamed to the robot on-the-fly during the flow
sampling process, and is well-suited for receding horizon policy execution.
Despite streaming, our method retains the ability to model multi-modal
behavior. We train flows that stabilize around demonstration trajectories to
reduce distribution shift and improve imitation learning performance. Streaming
flow policy outperforms prior methods while enabling faster policy execution
and tighter sensorimotor loops for learning-based robot control. Project
website: https://streaming-flow-policy.github.io/

</details>


### [377] [DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation](https://arxiv.org/abs/2505.21864)
*Mengda Xu,Han Zhang,Yifan Hou,Zhenjia Xu,Linxi Fan,Manuela Veloso,Shuran Song*

Main category: cs.RO

TL;DR: DexUMI是一个通过人类手部作为自然接口，将灵巧操作技能转移到多种机器人手的框架，结合硬件和软件适配，平均任务成功率达86%。


<details>
  <summary>Details</summary>
Motivation: 解决人类手与机器人手之间的体现差距，实现灵巧操作技能的高效转移。

Method: 硬件适配使用可穿戴手部外骨骼桥接运动学差距，软件适配通过高保真机器人手部图像替换视觉差距。

Result: 在两种灵巧机器人手平台上实现平均86%的任务成功率。

Conclusion: DexUMI通过硬件和软件适配有效缩小了人机差距，实现了高效的技能转移。

Abstract: We present DexUMI - a data collection and policy learning framework that uses
the human hand as the natural interface to transfer dexterous manipulation
skills to various robot hands. DexUMI includes hardware and software
adaptations to minimize the embodiment gap between the human hand and various
robot hands. The hardware adaptation bridges the kinematics gap using a
wearable hand exoskeleton. It allows direct haptic feedback in manipulation
data collection and adapts human motion to feasible robot hand motion. The
software adaptation bridges the visual gap by replacing the human hand in video
data with high-fidelity robot hand inpainting. We demonstrate DexUMI's
capabilities through comprehensive real-world experiments on two different
dexterous robot hand hardware platforms, achieving an average task success rate
of 86%.

</details>


### [378] [Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge](https://arxiv.org/abs/2505.21906)
*Zhongyi Zhou,Yichen Zhu,Junjie Wen,Chaomin Shen,Yi Xu*

Main category: cs.RO

TL;DR: ChatVLA-2是一种新型的混合专家视觉-语言-动作（VLA）模型，通过三阶段训练流程保留预训练视觉语言模型（VLM）的核心能力，并在机器人任务中展现卓越的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有端到端VLA系统在微调过程中会丢失VLM的关键能力，因此需要一种能够保留并扩展VLM核心能力的通用VLA模型。

Method: 提出ChatVLA-2模型，采用混合专家架构和三阶段训练流程，结合数学匹配任务验证其能力。

Result: ChatVLA-2展现出卓越的数学推理、OCR和空间推理能力，超越现有模仿学习方法。

Conclusion: ChatVLA-2为开发具有强大推理能力的通用机器人基础模型提供了重要进展。

Abstract: Vision-language-action (VLA) models have emerged as the next generation of
models in robotics. However, despite leveraging powerful pre-trained
Vision-Language Models (VLMs), existing end-to-end VLA systems often lose key
capabilities during fine-tuning as the model adapts to specific robotic tasks.
We argue that a generalizable VLA model should retain and expand upon the VLM's
core competencies: 1) Open-world embodied reasoning - the VLA should inherit
the knowledge from VLM, i.e., recognize anything that the VLM can recognize,
capable of solving math problems, possessing visual-spatial intelligence, 2)
Reasoning following - effectively translating the open-world reasoning into
actionable steps for the robot. In this work, we introduce ChatVLA-2, a novel
mixture-of-expert VLA model coupled with a specialized three-stage training
pipeline designed to preserve the VLM's original strengths while enabling
actionable reasoning. To validate our approach, we design a math-matching task
wherein a robot interprets math problems written on a whiteboard and picks
corresponding number cards from a table to solve equations. Remarkably, our
method exhibits exceptional mathematical reasoning and OCR capabilities,
despite these abilities not being explicitly trained within the VLA.
Furthermore, we demonstrate that the VLA possesses strong spatial reasoning
skills, enabling it to interpret novel directional instructions involving
previously unseen objects. Overall, our method showcases reasoning and
comprehension abilities that significantly surpass state-of-the-art imitation
learning methods such as OpenVLA, DexVLA, and pi-zero. This work represents a
substantial advancement toward developing truly generalizable robotic
foundation models endowed with robust reasoning capacities.

</details>


### [379] [Mastering Agile Tasks with Limited Trials](https://arxiv.org/abs/2505.21916)
*Yihang Hu,Pingyue Sheng,Shengjie Wang,Yang Gao*

Main category: cs.RO

TL;DR: ADAP算法通过模仿人类学习范式，在少量真实世界试验中迭代优化动作计划，实现高精度动态任务。


<details>
  <summary>Details</summary>
Motivation: 现有机器人方法在处理高敏捷性任务（如投篮）时面临数据收集、奖励设计或运动规划的高成本挑战，而人类却能轻松完成。

Method: 提出ADAP算法，通过学习先验运动模式并在少量试验中迭代调整动作计划。

Result: ADAP能在少于10次试验中完成投篮等高精度动态任务，达到人类水平。

Conclusion: ADAP为机器人处理敏捷动态任务提供了一种简单、可扩展的解决方案。

Abstract: Embodied robots nowadays can already handle many real-world manipulation
tasks. However, certain other real-world tasks (e.g., shooting a basketball
into a hoop) are highly agile and require high execution precision, presenting
additional challenges for methods primarily designed for quasi-static
manipulation tasks. This leads to increased efforts in costly data collection,
laborious reward design, or complex motion planning. Such tasks, however, are
far less challenging for humans. Say a novice basketball player typically needs
only $\sim$10 attempts to make their first successful shot, by roughly
imitating a motion prior and then iteratively adjusting their motion based on
the past outcomes. Inspired by this human learning paradigm, we propose the
Adaptive Diffusion Action Plannin (ADAP) algorithm, a simple & scalable
approach which iteratively refines its action plan by few real-world trials
within a learned prior motion pattern, until reaching a specific goal.
Experiments demonstrated that ADAP can learn and accomplish a wide range of
goal-conditioned agile dynamic tasks with human-level precision and efficiency
directly in real-world, such as throwing a basketball into the hoop in fewer
than 10 trials. Project website:https://adap-robotics.github.io/ .

</details>


### [380] [Enhanced SIRRT*: A Structure-Aware RRT* for 2D Path Planning with Hybrid Smoothing and Bidirectional Rewiring](https://arxiv.org/abs/2505.21968)
*Hyejeong Ryu*

Main category: cs.RO

TL;DR: E-SIRRT*是一种改进的基于采样的运动规划器，通过混合路径平滑和双向重布线优化了原始SIRRT*框架，显著提升了初始路径质量和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 传统RRT*和IRRT*在复杂环境中由于随机采样依赖导致收敛慢且方差高，尤其在初始解发现延迟时表现不佳。

Method: E-SIRRT*引入混合路径平滑（样条拟合和碰撞感知修正）和双向重布线（优化树连接性），并通过确定性骨架初始化提升性能。

Result: 实验表明，E-SIRRT*在初始路径质量、收敛速度和鲁棒性上均优于IRRT*和SIRRT*，且性能更稳定。

Conclusion: E-SIRRT*通过结构感知优化，显著提升了采样规划器的效率和可重复性。

Abstract: Sampling-based motion planners such as Rapidly-exploring Random Tree* (RRT*)
and its informed variant IRRT* are widely used for optimal path planning in
complex environments. However, these methods often suffer from slow convergence
and high variance due to their reliance on random sampling, particularly when
initial solution discovery is delayed. This paper presents Enhanced SIRRT*
(E-SIRRT*), a structure-aware planner that improves upon the original SIRRT*
framework by introducing two key enhancements: hybrid path smoothing and
bidirectional rewiring. Hybrid path smoothing refines the initial path through
spline fitting and collision-aware correction, while bidirectional rewiring
locally optimizes tree connectivity around the smoothed path to improve cost
propagation. Experimental results demonstrate that E-SIRRT* consistently
outperforms IRRT* and SIRRT* in terms of initial path quality, convergence
rate, and robustness across 100 trials. Unlike IRRT*, which exhibits high
variability due to stochastic initialization, E-SIRRT* achieves repeatable and
efficient performance through deterministic skeleton-based initialization and
structural refinement.

</details>


### [381] [DORAEMON: Decentralized Ontology-aware Reliable Agent with Enhanced Memory Oriented Navigation](https://arxiv.org/abs/2505.21969)
*Tianjun Gu,Linfeng Li,Xuhong Wang,Chenghua Gong,Jingyu Gong,Zhizhong Zhang,Yuan Xie,Lizhuang Ma,Xin Tan*

Main category: cs.RO

TL;DR: DORAEMON是一个受人类认知启发的导航框架，结合了Ventral和Dorsal Streams，解决了现有VLM方法的时空不连续性、记忆表示不足和任务理解不足的问题，并在多个数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 家庭服务机器人在陌生环境中的自适应导航需要低层路径规划和高层场景理解，但现有VLM方法存在时空不连续性、非结构化记忆表示和任务理解不足的问题。

Method: DORAEMON框架包括Dorsal Stream（处理时空不连续性）和Ventral Stream（提升决策能力），并开发了Nav-Ensurance以确保导航安全。

Result: 在HM3D、MP3D和GOAT数据集上，DORAEMON在成功率和路径长度加权成功率上显著优于现有方法，并引入了新的评估指标AORI。

Conclusion: DORAEMON在无需预训练或先验地图的情况下，实现了零样本自主导航的高效性和安全性。

Abstract: Adaptive navigation in unfamiliar environments is crucial for household
service robots but remains challenging due to the need for both low-level path
planning and high-level scene understanding. While recent vision-language model
(VLM) based zero-shot approaches reduce dependence on prior maps and
scene-specific training data, they face significant limitations: spatiotemporal
discontinuity from discrete observations, unstructured memory representations,
and insufficient task understanding leading to navigation failures. We propose
DORAEMON (Decentralized Ontology-aware Reliable Agent with Enhanced Memory
Oriented Navigation), a novel cognitive-inspired framework consisting of
Ventral and Dorsal Streams that mimics human navigation capabilities. The
Dorsal Stream implements the Hierarchical Semantic-Spatial Fusion and Topology
Map to handle spatiotemporal discontinuities, while the Ventral Stream combines
RAG-VLM and Policy-VLM to improve decision-making. Our approach also develops
Nav-Ensurance to ensure navigation safety and efficiency. We evaluate DORAEMON
on the HM3D, MP3D, and GOAT datasets, where it achieves state-of-the-art
performance on both success rate (SR) and success weighted by path length (SPL)
metrics, significantly outperforming existing methods. We also introduce a new
evaluation metric (AORI) to assess navigation intelligence better.
Comprehensive experiments demonstrate DORAEMON's effectiveness in zero-shot
autonomous navigation without requiring prior map building or pre-training.

</details>


### [382] [Learning Compositional Behaviors from Demonstration and Language](https://arxiv.org/abs/2505.21981)
*Weiyu Liu,Neil Nie,Ruohan Zhang,Jiayuan Mao,Jiajun Wu*

Main category: cs.RO

TL;DR: BLADE是一个结合模仿学习和模型规划的长时程机器人操作框架，利用语言标注演示和大型语言模型提取抽象动作知识，构建结构化高级动作库，无需手动标注即可泛化到新场景。


<details>
  <summary>Details</summary>
Motivation: 解决长时程机器人操作中泛化能力不足的问题，结合语言和演示数据提升动作规划的适应性。

Method: 整合语言标注演示和LLMs提取动作知识，构建结构化动作库（含前提条件和效果），并实现神经网络控制器。

Result: BLADE在模拟和真实机器人实验中表现出对新初始状态、外部扰动和新目标的强大泛化能力。

Conclusion: BLADE通过语言和演示数据自动构建动作表示，显著提升了机器人操作的泛化性和适应性。

Abstract: We introduce Behavior from Language and Demonstration (BLADE), a framework
for long-horizon robotic manipulation by integrating imitation learning and
model-based planning. BLADE leverages language-annotated demonstrations,
extracts abstract action knowledge from large language models (LLMs), and
constructs a library of structured, high-level action representations. These
representations include preconditions and effects grounded in visual perception
for each high-level action, along with corresponding controllers implemented as
neural network-based policies. BLADE can recover such structured
representations automatically, without manually labeled states or symbolic
definitions. BLADE shows significant capabilities in generalizing to novel
situations, including novel initial states, external state perturbations, and
novel goals. We validate the effectiveness of our approach both in simulation
and on real robots with a diverse set of objects with articulated parts,
partial observability, and geometric constraints.

</details>


### [383] [Soft Electrothermal Meta-Actuator for Robust Multifunctional Control](https://arxiv.org/abs/2505.21992)
*Hanseong Jo,Pavel Shafirin,Christopher Le,Caden Chan,Artur Davoyan*

Main category: cs.RO

TL;DR: 提出了一种基于薄膜热传导的元执行器架构，解决了传统软电热执行器的单向运动、环境敏感性和响应慢的问题，实现了双向运动、低环境敏感性和快速复位。


<details>
  <summary>Details</summary>
Motivation: 传统软电热执行器存在单向运动、环境敏感性和响应慢的固有局限，限制了其应用范围。

Method: 采用薄膜热传导工程设计的元执行器架构，通过电控实现多功能操作。

Result: 实现了双向大偏转（≥28%执行器长度）、低环境敏感性（比传统执行器低100倍以上）和快速复位（比被动冷却快10倍）。

Conclusion: 元执行器架构扩展了运动范围，展示了在软机器人和设备中的潜力。

Abstract: Soft electrothermal actuators are of great interest in diverse application
domains for their simplicity, compliance, and ease of control. However, the
very nature of thermally induced mechanical actuation sets inherent operation
constraints: unidirectional motion, environmental sensitivity, and slow
response times limited by passive cooling. To overcome these constraints, we
propose a meta-actuator architecture, which uses engineered heat transfer in
thin films to achieve multifunctional operation. We demonstrate electrically
selectable bidirectional motion with large deflection ($ \geq $28% of actuator
length at 0.75 W), suppressed thermal sensitivity to ambient temperature
changes when compared to conventional actuators (>100$ \times $ lower), and
actively forced return to the rest state, which is 10 times faster than that
with passive cooling. We further show that our meta-actuator approach enables
extended ranges of motions for manipulating complex objects. Versatile soft
gripper operations highlight the meta-actuator's potential for soft robotics
and devices.

</details>


### [384] [A simulation framework for autonomous lunar construction work](https://arxiv.org/abs/2505.22091)
*Mattias Linde,Daniel Lindmark,Sandra Ålstig,Martin Servin*

Main category: cs.RO

TL;DR: 提出了一种用于月球多自主机器施工的仿真框架，支持建模、执行和分析施工场景及自主解决方案。


<details>
  <summary>Details</summary>
Motivation: 为月球施工项目提供高效的仿真工具，以优化工作时间和能源消耗。

Method: 基于物理模型模拟多体动力学和可变形地形，使用行为树管理操作逻辑和错误处理，通过ROS2连接高层决策与底层控制算法。

Result: 框架在两种不同的月球施工场景中进行了测试和验证。

Conclusion: 该框架能够有效支持复杂施工行为的模拟和分析。

Abstract: We present a simulation framework for lunar construction work involving
multiple autonomous machines. The framework supports modelling of construction
scenarios and autonomy solutions, execution of the scenarios in simulation, and
analysis of work time and energy consumption throughout the construction
project. The simulations are based on physics-based models for contacting
multibody dynamics and deformable terrain, including vehicle-soil interaction
forces and soil flow in real time. A behaviour tree manages the operational
logic and error handling, which enables the representation of complex
behaviours through a discrete set of simpler tasks in a modular hierarchical
structure. High-level decision-making is separated from lower-level control
algorithms, with the two connected via ROS2. Excavation movements are
controlled through inverse kinematics and tracking controllers. The framework
is tested and demonstrated on two different lunar construction scenarios.

</details>


### [385] [ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement Learning](https://arxiv.org/abs/2505.22094)
*Tonghe Zhang,Yu Chao,Sicang Su,Yu Wang*

Main category: cs.RO

TL;DR: ReinFlow是一个简单有效的在线强化学习框架，通过注入可学习噪声优化流匹配策略，显著提升连续机器人控制任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在连续控制任务中面临探索不足和训练不稳定的问题，ReinFlow旨在通过流匹配策略的优化解决这些问题。

Method: ReinFlow将确定性流路径转化为离散时间马尔可夫过程，便于精确计算似然，支持多样化的流模型变体（如Rectified Flow和Shortcut Models）的微调。

Result: 在挑战性任务中，Rectified Flow策略的奖励平均增长135.36%，节省82.63%时间；Shortcut Model策略成功率提升40.34%，节省23.20%计算时间。

Conclusion: ReinFlow在性能和效率上均优于现有方法，适用于复杂机器人控制任务。

Abstract: We propose ReinFlow, a simple yet effective online reinforcement learning
(RL) framework that fine-tunes a family of flow matching policies for
continuous robotic control. Derived from rigorous RL theory, ReinFlow injects
learnable noise into a flow policy's deterministic path, converting the flow
into a discrete-time Markov Process for exact and straightforward likelihood
computation. This conversion facilitates exploration and ensures training
stability, enabling ReinFlow to fine-tune diverse flow model variants,
including Rectified Flow [35] and Shortcut Models [19], particularly at very
few or even one denoising step. We benchmark ReinFlow in representative
locomotion and manipulation tasks, including long-horizon planning with visual
input and sparse reward. The episode reward of Rectified Flow policies obtained
an average net growth of 135.36% after fine-tuning in challenging legged
locomotion tasks while saving denoising steps and 82.63% of wall time compared
to state-of-the-art diffusion RL fine-tuning method DPPO [43]. The success rate
of the Shortcut Model policies in state and visual manipulation tasks achieved
an average net increase of 40.34% after fine-tuning with ReinFlow at four or
even one denoising step, whose performance is comparable to fine-tuned DDIM
policies while saving computation time for an average of 23.20%. Project
Webpage: https://reinflow.github.io/

</details>


### [386] [ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation](https://arxiv.org/abs/2505.22159)
*Jiawen Yu,Hairuo Liu,Qiaojun Yu,Jieji Ren,Ce Hao,Haitong Ding,Guangyu Huang,Guofan Huang,Yan Song,Panpan Cai,Cewu Lu,Wenqiang Zhang*

Main category: cs.RO

TL;DR: ForceVLA是一种新型的端到端机器人操作框架，通过将力反馈作为视觉-语言-动作（VLA）模型的核心模态，提升了在接触密集型任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在需要精细力控制的接触密集型任务中表现不佳，尤其是在视觉遮挡或动态不确定的情况下。

Method: ForceVLA引入了FVLMoE模块，动态整合预训练的视觉-语言嵌入与实时6轴力反馈，并通过ForceVLA-Data数据集支持训练。

Result: ForceVLA在接触密集型任务中的平均成功率提高了23.2%，在插头插入等任务中达到80%的成功率。

Conclusion: ForceVLA展示了多模态集成在灵巧操作中的重要性，并为物理智能机器人控制设定了新基准。

Abstract: Vision-Language-Action (VLA) models have advanced general-purpose robotic
manipulation by leveraging pretrained visual and linguistic representations.
However, they struggle with contact-rich tasks that require fine-grained
control involving force, especially under visual occlusion or dynamic
uncertainty. To address these limitations, we propose \textbf{ForceVLA}, a
novel end-to-end manipulation framework that treats external force sensing as a
first-class modality within VLA systems. ForceVLA introduces \textbf{FVLMoE}, a
force-aware Mixture-of-Experts fusion module that dynamically integrates
pretrained visual-language embeddings with real-time 6-axis force feedback
during action decoding. This enables context-aware routing across
modality-specific experts, enhancing the robot's ability to adapt to subtle
contact dynamics. We also introduce \textbf{ForceVLA-Data}, a new dataset
comprising synchronized vision, proprioception, and force-torque signals across
five contact-rich manipulation tasks. ForceVLA improves average task success by
23.2\% over strong $\pi_0$-based baselines, achieving up to 80\% success in
tasks such as plug insertion. Our approach highlights the importance of
multimodal integration for dexterous manipulation and sets a new benchmark for
physically intelligent robotic control. Code and data will be released at
https://sites.google.com/view/forcevla2025.

</details>


### [387] [LiDAR Based Semantic Perception for Forklifts in Outdoor Environments](https://arxiv.org/abs/2505.22258)
*Benjamin Serfling,Hannes Reichert,Lorenzo Bayerlein,Konrad Doll,Kati Radkhah-Lens*

Main category: cs.RO

TL;DR: 提出了一种基于双LiDAR的语义分割框架，专为复杂户外环境中的自动叉车设计，实现了高精度的动态和静态障碍物检测。


<details>
  <summary>Details</summary>
Motivation: 为工业物料搬运任务中的自动叉车提供全面的场景理解，特别是在动态仓库和场地环境中确保安全导航。

Method: 采用双LiDAR系统（前向和向下倾斜），结合轻量级方法对高分辨率3D点云进行语义分割，识别安全关键实例和环境类别。

Result: 实验验证表明，该方法在满足严格实时性要求的同时，实现了高分割精度。

Conclusion: 该框架适用于动态仓库和场地环境中的安全感知全自动叉车导航。

Abstract: In this study, we present a novel LiDAR-based semantic segmentation framework
tailored for autonomous forklifts operating in complex outdoor environments.
Central to our approach is the integration of a dual LiDAR system, which
combines forward-facing and downward-angled LiDAR sensors to enable
comprehensive scene understanding, specifically tailored for industrial
material handling tasks. The dual configuration improves the detection and
segmentation of dynamic and static obstacles with high spatial precision. Using
high-resolution 3D point clouds captured from two sensors, our method employs a
lightweight yet robust approach that segments the point clouds into
safety-critical instance classes such as pedestrians, vehicles, and forklifts,
as well as environmental classes such as driveable ground, lanes, and
buildings. Experimental validation demonstrates that our approach achieves high
segmentation accuracy while satisfying strict runtime requirements,
establishing its viability for safety-aware, fully autonomous forklift
navigation in dynamic warehouse and yard environments.

</details>


### [388] [UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments](https://arxiv.org/abs/2505.22335)
*Wancai Zheng,Linlin Ou,Jiajie He,Libo Zhou,Xinyi Yu,Yan Wei*

Main category: cs.RO

TL;DR: UP-SLAM是一种实时RGB-D SLAM系统，通过并行化框架解耦跟踪与建图，采用概率八叉树管理高斯基元，并利用无训练不确定性估计器和时间编码器提升动态环境下的鲁棒性和渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯飞溅技术在动态环境中实时性能和鲁棒性受限，UP-SLAM旨在解决这些问题。

Method: 使用并行化框架、概率八叉树管理、无训练不确定性估计器和时间编码器，结合DINO特征增强高斯场。

Result: 在定位精度（提升59.8%）和渲染质量（提升4.57 dB PSNR）上优于现有方法，保持实时性能并生成无伪影静态地图。

Conclusion: UP-SLAM在动态环境中表现出色，为实时SLAM系统提供了高效且鲁棒的解决方案。

Abstract: Recent 3D Gaussian Splatting (3DGS) techniques for Visual Simultaneous
Localization and Mapping (SLAM) have significantly progressed in tracking and
high-fidelity mapping. However, their sequential optimization framework and
sensitivity to dynamic objects limit real-time performance and robustness in
real-world scenarios. We present UP-SLAM, a real-time RGB-D SLAM system for
dynamic environments that decouples tracking and mapping through a parallelized
framework. A probabilistic octree is employed to manage Gaussian primitives
adaptively, enabling efficient initialization and pruning without hand-crafted
thresholds. To robustly filter dynamic regions during tracking, we propose a
training-free uncertainty estimator that fuses multi-modal residuals to
estimate per-pixel motion uncertainty, achieving open-set dynamic object
handling without reliance on semantic labels. Furthermore, a temporal encoder
is designed to enhance rendering quality. Concurrently, low-dimensional
features are efficiently transformed via a shallow multilayer perceptron to
construct DINO features, which are then employed to enrich the Gaussian field
and improve the robustness of uncertainty prediction. Extensive experiments on
multiple challenging datasets suggest that UP-SLAM outperforms state-of-the-art
methods in both localization accuracy (by 59.8%) and rendering quality (by 4.57
dB PSNR), while maintaining real-time performance and producing reusable,
artifact-free static maps in dynamic environments.The project:
https://aczheng-cai.github.io/up_slam.github.io/

</details>


### [389] [Fully Packed and Ready to Go: High-Density, Rearrangement-Free, Grid-Based Storage and Retrieval](https://arxiv.org/abs/2505.22497)
*Tzvika Geft,Kostas Bekris,Jingjin Yu*

Main category: cs.RO

TL;DR: 论文研究了网格存储系统中负载的放置问题，目标是最大化空间利用率并减少重排操作。通过分析入站和出站阶段，提出了在特定条件下可实现无重排的解决方案。


<details>
  <summary>Details</summary>
Motivation: 在物流、工业和运输领域中，网格存储系统的高效空间利用和减少重排操作是关键挑战。

Method: 研究网格存储系统中负载的自由移动（如通过移动机器人），分析不同场景（如有限负载到达序列信息或狭窄开口网格）下的解决方案。

Result: 证明在特定条件下（如网格开口宽度至少为3个单元格时）可实现无重排操作，即使存储满载。

Conclusion: 研究为网格存储系统的优化提供了理论支持，并具有实际应用价值。

Abstract: Grid-based storage systems with uniformly shaped loads (e.g., containers,
pallets, totes) are commonplace in logistics, industrial, and transportation
domains. A key performance metric for such systems is the maximization of space
utilization, which requires some loads to be placed behind or below others,
preventing direct access to them. Consequently, dense storage settings bring up
the challenge of determining how to place loads while minimizing costly
rearrangement efforts necessary during retrieval. This paper considers the
setting involving an inbound phase, during which loads arrive, followed by an
outbound phase, during which loads depart. The setting is prevalent in
distribution centers, automated parking garages, and container ports. In both
phases, minimizing the number of rearrangement actions results in more optimal
(e.g., fast, energy-efficient, etc.) operations. In contrast to previous work
focusing on stack-based systems, this effort examines the case where loads can
be freely moved along the grid, e.g., by a mobile robot, expanding the range of
possible motions. We establish that for a range of scenarios, such as having
limited prior knowledge of the loads' arrival sequences or grids with a narrow
opening, a (best possible) rearrangement-free solution always exists, including
when the loads fill the grid to its capacity. In particular, when the sequences
are fully known, we establish an intriguing characterization showing that
rearrangement can always be avoided if and only if the open side of the grid
(used to access the storage) is at least 3 cells wide. We further discuss
useful practical implications of our solutions.

</details>


### [390] [From Strangers to Assistants: Fast Desire Alignment for Embodied Agent-User Adaptation](https://arxiv.org/abs/2505.22503)
*Yuanfei Wang,Xinju Huang,Fangwei Zhong,Yaodong Yang,Yizhou Wang,Yuanpei Chen,Hao Dong*

Main category: cs.RO

TL;DR: 论文提出了一种名为FAMER的框架，用于快速对齐用户潜在需求，通过心理推理和反思式通信提升具身代理的任务执行和沟通效率。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，具身代理需与陌生代理和人类用户协作，而用户目标常模糊且隐含，因此快速准确的需求对齐成为关键能力。

Method: 开发了HA-Desire仿真环境，结合LLM驱动的用户代理，提出FAMER框架，包括心理推理机制、反思式通信模块和目标相关信息提取。

Result: 实验表明，FAMER显著提升了任务执行和沟通效率，使代理能快速适应用户特定需求。

Conclusion: FAMER框架为具身代理在复杂环境中高效对齐用户需求提供了有效解决方案。

Abstract: While embodied agents have made significant progress in performing complex
physical tasks, real-world applications demand more than pure task execution.
The agents must collaborate with unfamiliar agents and human users, whose goals
are often vague and implicit. In such settings, interpreting ambiguous
instructions and uncovering underlying desires is essential for effective
assistance. Therefore, fast and accurate desire alignment becomes a critical
capability for embodied agents. In this work, we first develop a home
assistance simulation environment HA-Desire that integrates an LLM-driven human
user agent exhibiting realistic value-driven goal selection and communication.
The ego agent must interact with this proxy user to infer and adapt to the
user's latent desires. To achieve this, we present a novel framework FAMER for
fast desire alignment, which introduces a desire-based mental reasoning
mechanism to identify user intent and filter desire-irrelevant actions. We
further design a reflection-based communication module that reduces redundant
inquiries, and incorporate goal-relevant information extraction with memory
persistence to improve information reuse and reduce unnecessary exploration.
Extensive experiments demonstrate that our framework significantly enhances
both task execution and communication efficiency, enabling embodied agents to
quickly adapt to user-specific desires in complex embodied environments.

</details>


### [391] [VR-Based Control of Multi-Copter Operation](https://arxiv.org/abs/2505.22599)
*Jack T. Hughes,Mohammad Ghufran,Hossein Rastgoftar*

Main category: cs.RO

TL;DR: 利用VR技术实时扫描无人机周围环境，通过VR头显为飞行员提供第三人称视角，增强空间感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有无人机操作方案多为第一人称视角，信息有限，且传统VR方案依赖预设环境设计，无法适应动态变化。

Method: 通过机载传感器实时扫描环境，并将数据流式传输至VR头显，生成无人机及其环境的虚拟表示。

Result: 飞行员能以第三人称视角近距离观察无人机周围环境，获得更多信息，提升操作灵活性。

Conclusion: 该方案为无人机操作提供了更灵活、适应性更强的VR解决方案，尤其适用于未知环境。

Abstract: We aim to use virtual reality (VR) to improve the spatial awareness of pilots
by real-time scanning of the environment around the drone using onboard
sensors, live streaming of this environment to a VR headset, and rendering a
virtual representation of the drone and its environment for the pilot. This
way, the pilot can see the immediate environment of the drone up close from a
third-person perspective, as opposed to the first-person perspective that most
drone cameras provide. This provides much more information about the drone
surroundings for the pilot while operating the drone than existing
teleoperation solutions. Previous solutions using VR have relied upon pre-made
designs of the environment, which makes it difficult to adapt to changing
environments. Our solution, in contrast, scans the environment as you fly,
making it much more flexible for use in unknown environments.

</details>


### [392] [SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale Imitation Learning](https://arxiv.org/abs/2505.22626)
*Yu Zhang,Yuqi Xie,Huihan Liu,Rutav Shah,Michael Wan,Linxi Fan,Yuke Zhu*

Main category: cs.RO

TL;DR: SCIZOR是一种自监督数据筛选框架，通过过滤低质量的状态-动作对提升模仿学习性能，平均提升15.4%。


<details>
  <summary>Details</summary>
Motivation: 大规模模仿学习数据集中存在质量参差不齐的问题，影响性能，现有方法依赖人工标注且粒度较粗。

Method: SCIZOR结合任务进度预测器和去重模块，分别处理次优和冗余数据。

Result: 实验表明，SCIZOR能显著提升模仿学习性能，平均提升15.4%。

Conclusion: SCIZOR为模仿学习提供了一种高效的数据筛选方法，减少数据需求并提升性能。

Abstract: Imitation learning advances robot capabilities by enabling the acquisition of
diverse behaviors from human demonstrations. However, large-scale datasets used
for policy training often introduce substantial variability in quality, which
can negatively impact performance. As a result, automatically curating datasets
by filtering low-quality samples to improve quality becomes essential. Existing
robotic curation approaches rely on costly manual annotations and perform
curation at a coarse granularity, such as the dataset or trajectory level,
failing to account for the quality of individual state-action pairs. To address
this, we introduce SCIZOR, a self-supervised data curation framework that
filters out low-quality state-action pairs to improve the performance of
imitation learning policies. SCIZOR targets two complementary sources of
low-quality data: suboptimal data, which hinders learning with undesirable
actions, and redundant data, which dilutes training with repetitive patterns.
SCIZOR leverages a self-supervised task progress predictor for suboptimal data
to remove samples lacking task progression, and a deduplication module
operating on joint state-action representation for samples with redundant
patterns. Empirically, we show that SCIZOR enables imitation learning policies
to achieve higher performance with less data, yielding an average improvement
of 15.4% across multiple benchmarks. More information is available at:
https://ut-austin-rpl.github.io/SCIZOR/

</details>


### [393] [LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for Scientific Embodied Agents](https://arxiv.org/abs/2505.22634)
*Rui Li,Zixuan Hu,Wenxi Qu,Jinouwen Zhang,Zhenfei Yin,Sha Zhang,Xuantuo Huang,Hanqing Wang,Tai Wang,Jiangmiao Pang,Wanli Ouyang,Lei Bai,Wangmeng Zuo,Ling-Yu Duan,Dongzhan Zhou,Shixiang Tang*

Main category: cs.RO

TL;DR: LabUtopia是一个实验室环境下的仿真与基准测试套件，旨在推动通用化、具备推理能力的智能体发展。


<details>
  <summary>Details</summary>
Motivation: 实验室环境对感知和长期规划要求更高，但缺乏合适的仿真器和基准测试阻碍了相关研究。

Method: LabUtopia包含高保真仿真器LabSim、场景生成器LabScene和分层基准测试LabBench。

Result: 支持30种任务和200多个场景资产，为复杂环境提供大规模训练和评估。

Conclusion: LabUtopia为科学智能体的感知、规划和控制提供了强大平台，并探索了其泛化能力。

Abstract: Scientific embodied agents play a crucial role in modern laboratories by
automating complex experimental workflows. Compared to typical household
environments, laboratory settings impose significantly higher demands on
perception of physical-chemical transformations and long-horizon planning,
making them an ideal testbed for advancing embodied intelligence. However, its
development has been long hampered by the lack of suitable simulator and
benchmarks. In this paper, we address this gap by introducing LabUtopia, a
comprehensive simulation and benchmarking suite designed to facilitate the
development of generalizable, reasoning-capable embodied agents in laboratory
settings. Specifically, it integrates i) LabSim, a high-fidelity simulator
supporting multi-physics and chemically meaningful interactions; ii) LabScene,
a scalable procedural generator for diverse scientific scenes; and iii)
LabBench, a hierarchical benchmark spanning five levels of complexity from
atomic actions to long-horizon mobile manipulation. LabUtopia supports 30
distinct tasks and includes more than 200 scene and instrument assets, enabling
large-scale training and principled evaluation in high-complexity environments.
We demonstrate that LabUtopia offers a powerful platform for advancing the
integration of perception, planning, and control in scientific-purpose agents
and provides a rigorous testbed for exploring the practical capabilities and
generalization limits of embodied intelligence in future research.

</details>


### [394] [FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control](https://arxiv.org/abs/2505.22642)
*Younggyo Seo,Carmelo Sferrazza,Haoran Geng,Michal Nauman,Zhao-Heng Yin,Pieter Abbeel*

Main category: cs.RO

TL;DR: FastTD3是一种简单、快速且高效的强化学习算法，显著缩短了人形机器人的训练时间。


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人领域取得进展，但其复杂性和长训练时间仍是主要瓶颈。

Method: 通过并行模拟、大批量更新、分布评论器和调优超参数改进TD3算法。

Result: FastTD3在单个A100 GPU上3小时内完成HumanoidBench任务，训练稳定。

Conclusion: FastTD3为机器人强化学习研究提供了轻量级且易用的实现。

Abstract: Reinforcement learning (RL) has driven significant progress in robotics, but
its complexity and long training times remain major bottlenecks. In this
report, we introduce FastTD3, a simple, fast, and capable RL algorithm that
significantly speeds up training for humanoid robots in popular suites such as
HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably
simple: we train an off-policy TD3 agent with several modifications -- parallel
simulation, large-batch updates, a distributional critic, and carefully tuned
hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours
on a single A100 GPU, while remaining stable during training. We also provide a
lightweight and easy-to-use implementation of FastTD3 to accelerate RL research
in robotics.

</details>


### [395] [Towards Human-Like Trajectory Prediction for Autonomous Driving: A Behavior-Centric Approach](https://arxiv.org/abs/2505.21565)
*Haicheng Liao,Zhenning Li,Guohui Zhang,Keqiang Li,Chengzhong Xu*

Main category: cs.RO

TL;DR: HiT模型通过行为感知模块和动态中心性度量提升轨迹预测，优于传统静态图方法，在复杂交通环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 提升自动驾驶系统在动态交通环境中的轨迹预测能力，以增强安全性和效率。

Method: 引入HiT模型，结合行为感知模块和动态中心性度量，捕捉车辆间的直接和间接交互。

Result: 在NGSIM等数据集上，HiT表现优于其他模型，尤其在激进驾驶场景中。

Conclusion: HiT为轨迹预测提供了更可靠和可解释的方法，推动自动驾驶系统发展。

Abstract: Predicting the trajectories of vehicles is crucial for the development of
autonomous driving (AD) systems, particularly in complex and dynamic traffic
environments. In this study, we introduce HiT (Human-like Trajectory
Prediction), a novel model designed to enhance trajectory prediction by
incorporating behavior-aware modules and dynamic centrality measures. Unlike
traditional methods that primarily rely on static graph structures, HiT
leverages a dynamic framework that accounts for both direct and indirect
interactions among traffic participants. This allows the model to capture the
subtle yet significant influences of surrounding vehicles, enabling more
accurate and human-like predictions. To evaluate HiT's performance, we
conducted extensive experiments using diverse and challenging real-world
datasets, including NGSIM, HighD, RounD, ApolloScape, and MoCAD++. The results
demonstrate that HiT consistently outperforms other top models across multiple
metrics, particularly excelling in scenarios involving aggressive driving
behaviors. This research presents a significant step forward in trajectory
prediction, offering a more reliable and interpretable approach for enhancing
the safety and efficiency of fully autonomous driving systems.

</details>


### [396] [CogAD: Cognitive-Hierarchy Guided End-to-End Autonomous Driving](https://arxiv.org/abs/2505.21581)
*Zhennan Wang,Jianing Teng,Canqun Xiang,Kangliang Chen,Xing Pan,Lu Deng,Weihao Gu*

Main category: cs.RO

TL;DR: CogAD是一种新型端到端自动驾驶模型，模拟人类驾驶员的层次认知机制，通过全局到局部上下文处理和意图驱动的多模式轨迹生成，实现了更符合人类认知的感知与规划。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法与人类认知原则在感知和规划上存在根本性不一致，因此提出CogAD以模拟人类驾驶员的层次认知机制。

Method: CogAD采用双重层次机制：全局到局部上下文处理（感知）和意图驱动的多模式轨迹生成（规划），并通过双层次不确定性建模实现多样化轨迹生成。

Result: 在nuScenes和Bench2Drive数据集上，CogAD在端到端规划中表现最优，尤其在长尾场景和复杂现实驾驶条件下具有鲁棒泛化能力。

Conclusion: CogAD通过模拟人类认知机制，显著提升了自动驾驶系统的感知、规划能力和泛化性能。

Abstract: While end-to-end autonomous driving has advanced significantly, prevailing
methods remain fundamentally misaligned with human cognitive principles in both
perception and planning. In this paper, we propose CogAD, a novel end-to-end
autonomous driving model that emulates the hierarchical cognition mechanisms of
human drivers. CogAD implements dual hierarchical mechanisms: global-to-local
context processing for human-like perception and intent-conditioned multi-mode
trajectory generation for cognitively-inspired planning. The proposed method
demonstrates three principal advantages: comprehensive environmental
understanding through hierarchical perception, robust planning exploration
enabled by multi-level planning, and diverse yet reasonable multi-modal
trajectory generation facilitated by dual-level uncertainty modeling. Extensive
experiments on nuScenes and Bench2Drive demonstrate that CogAD achieves
state-of-the-art performance in end-to-end planning, exhibiting particular
superiority in long-tail scenarios and robust generalization to complex
real-world driving conditions.

</details>


### [397] [Fast and Cost-effective Speculative Edge-Cloud Decoding with Early Exits](https://arxiv.org/abs/2505.21594)
*Yeshwanth Venkatesha,Souvik Kundu,Priyadarshini Panda*

Main category: cs.RO

TL;DR: 提出了一种边缘-云协作的解码框架，通过小型设备模型和大型服务器模型的协同工作，显著降低了延迟并提升了效率。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）在边缘设备上部署时的高成本和资源限制问题，同时提升隐私和减少延迟。

Method: 采用边缘设备上的小型草稿模型和服务器上的大型目标模型，通过早期退出和预生成令牌实现并行解码。

Result: 实验显示，相比云端自回归解码，延迟降低35%，预生成令牌进一步提速11%。在四足机器人上实现21%的速度提升。

Conclusion: 该框架为资源受限的边缘设备提供了实时LLM和VLM应用的可行解决方案。

Abstract: Large Language Models (LLMs) enable various applications on edge devices such
as smartphones, wearables, and embodied robots. However, their deployment often
depends on expensive cloud-based APIs, creating high operational costs, which
limit access for smaller organizations and raise sustainability concerns.
Certain LLMs can be deployed on-device, offering a cost-effective solution with
reduced latency and improved privacy. Yet, limited computing resources
constrain the size and accuracy of models that can be deployed, necessitating a
collaborative design between edge and cloud. We propose a fast and
cost-effective speculative edge-cloud decoding framework with a large target
model on the server and a small draft model on the device. By introducing early
exits in the target model, tokens are generated mid-verification, allowing the
client to preemptively draft subsequent tokens before final verification, thus
utilizing idle time and enhancing parallelism between edge and cloud. Using an
NVIDIA Jetson Nano (client) and an A100 GPU (server) with Vicuna-68M (draft)
and Llama2-7B (target) models, our method achieves up to a 35% reduction in
latency compared to cloud-based autoregressive decoding, with an additional 11%
improvement from preemptive drafting. To demonstrate real-world applicability,
we deploy our method on the Unitree Go2 quadruped robot using Vision-Language
Model (VLM) based control, achieving a 21% speedup over traditional cloud-based
autoregressive decoding. These results demonstrate the potential of our
framework for real-time LLM and VLM applications on resource-constrained edge
devices.

</details>


### [398] [PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation](https://arxiv.org/abs/2505.21652)
*Yifan Yin,Zhengtao Han,Shivam Aarya,Jianxin Wang,Shuhang Xu,Jiawei Peng,Angtian Wang,Alan Yuille,Tianmin Shu*

Main category: cs.RO

TL;DR: PartInstruct是一个用于细粒度机器人操作的大规模基准数据集，包含513个对象实例和1302个任务，旨在解决现有模型在部分概念理解和3D空间动作预测上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对细粒度机器人操作的大规模数据集，特别是带有部分级标注和指令的数据集，限制了模型在部分级任务上的表现。

Method: 提出了PartInstruct数据集，包含对象实例、部分级标注和任务指令，并通过3D模拟器生成专家演示。评估了多种先进模型在数据集上的表现。

Result: 实验显示当前模型在部分概念理解和长时程任务操作上表现不佳。

Conclusion: PartInstruct为细粒度机器人操作提供了重要基准，揭示了现有模型的局限性，并推动未来研究改进。

Abstract: Fine-grained robot manipulation, such as lifting and rotating a bottle to
display the label on the cap, requires robust reasoning about object parts and
their relationships with intended tasks. Despite recent advances in training
general-purpose robot manipulation policies guided by language instructions,
there is a notable lack of large-scale datasets for fine-grained manipulation
tasks with part-level instructions and diverse 3D object instances annotated
with part-level labels. In this work, we introduce PartInstruct, the first
large-scale benchmark for training and evaluating fine-grained robot
manipulation models using part-level instructions. PartInstruct comprises 513
object instances across 14 categories, each annotated with part-level
information, and 1302 fine-grained manipulation tasks organized into 16 task
classes. Our training set consists of over 10,000 expert demonstrations
synthesized in a 3D simulator, where each demonstration is paired with a
high-level task instruction, a chain of base part-based skill instructions, and
ground-truth 3D information about the object and its parts. Additionally, we
designed a comprehensive test suite to evaluate the generalizability of learned
policies across new states, objects, and tasks. We evaluated several
state-of-the-art robot manipulation approaches, including end-to-end
vision-language policy learning and bi-level planning models for robot
manipulation on our benchmark. The experimental results reveal that current
models struggle to robustly ground part concepts and predict actions in 3D
space, and face challenges when manipulating object parts in long-horizon
tasks.

</details>


### [399] [Convergent Functions, Divergent Forms](https://arxiv.org/abs/2505.21665)
*Hyeonseong Jeon,Ainaz Eftekhar,Aaron Walsman,Kuo-Hao Zeng,Ali Farhadi,Ranjay Krishna*

Main category: cs.RO

TL;DR: LOKI是一个高效的计算框架，通过共同设计形态和控制策略，实现跨未知任务的泛化能力，显著提升样本效率和多样性。


<details>
  <summary>Details</summary>
Motivation: 受生物适应性启发，解决传统进化和质量多样性算法效率低下的问题。

Method: 学习收敛函数（共享控制策略）和动态局部搜索，减少训练成本并促进形态多样性。

Result: 探索了780倍的设计，减少78%的模拟步骤和40%的计算成本，生成多样且高性能的形态。

Conclusion: LOKI在多样性和适应性上优于现有方法，适用于复杂任务。

Abstract: We introduce LOKI, a compute-efficient framework for co-designing
morphologies and control policies that generalize across unseen tasks. Inspired
by biological adaptation -- where animals quickly adjust to morphological
changes -- our method overcomes the inefficiencies of traditional evolutionary
and quality-diversity algorithms. We propose learning convergent functions:
shared control policies trained across clusters of morphologically similar
designs in a learned latent space, drastically reducing the training cost per
design. Simultaneously, we promote divergent forms by replacing mutation with
dynamic local search, enabling broader exploration and preventing premature
convergence. The policy reuse allows us to explore 780$\times$ more designs
using 78% fewer simulation steps and 40% less compute per design. Local
competition paired with a broader search results in a diverse set of
high-performing final morphologies. Using the UNIMAL design space and a
flat-terrain locomotion task, LOKI discovers a rich variety of designs --
ranging from quadrupeds to crabs, bipedals, and spinners -- far more diverse
than those produced by prior work. These morphologies also transfer better to
unseen downstream tasks in agility, stability, and manipulation domains (e.g.,
2$\times$ higher reward on bump and push box incline tasks). Overall, our
approach produces designs that are both diverse and adaptable, with
substantially greater sample efficiency than existing co-design methods.
(Project website: https://loki-codesign.github.io/)

</details>


### [400] [Real-World Deployment of Cloud Autonomous Mobility System Using 5G Networks for Outdoor and Indoor Environments](https://arxiv.org/abs/2505.21676)
*Yufeng Yang,Minghao Ning,Keqi Shu,Aladdin Saleh,Ehsan Hashemi,Amir Khajepour*

Main category: cs.RO

TL;DR: 论文提出了一种基于5G网络的云自主移动（CAM）系统，结合分布式传感器节点和云计算，实现了实时感知与自主操作，并在城市和室内环境中验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 解决户外和室内移动系统日益复杂的感知与通信需求，提供可扩展、经济高效且可靠的解决方案。

Method: 利用5G网络连接的分布式传感器节点，结合LiDAR和摄像头感知，通过云计算进行全局信息融合，并采用URLLC技术实现低延迟通信。

Result: 在城市和室内环境中部署CAM系统，显著提升了交通监控、危险检测和资产管理能力。

Conclusion: 云基础设施感知技术有望推动户外和室内智能交通系统的发展，同时论文总结了实际部署中的挑战和扩展经验。

Abstract: The growing complexity of both outdoor and indoor mobility systems demands
scalable, cost-effective, and reliable perception and communication frameworks.
This work presents the real-world deployment and evaluation of a Cloud
Autonomous Mobility (CAM) system that leverages distributed sensor nodes
connected via 5G networks, which integrates LiDAR- and camera-based perception
at infrastructure units, cloud computing for global information fusion, and
Ultra-Reliable Low Latency Communications (URLLC) to enable real-time
situational awareness and autonomous operation. The CAM system is deployed in
two distinct environments: a dense urban roundabout and a narrow indoor
hospital corridor. Field experiments show improved traffic monitoring, hazard
detection, and asset management capabilities. The paper also discusses
practical deployment challenges and shares key insights for scaling CAM
systems. The results highlight the potential of cloud-based infrastructure
perception to advance both outdoor and indoor intelligent transportation
systems.

</details>


### [401] [MIND-Stack: Modular, Interpretable, End-to-End Differentiability for Autonomous Navigation](https://arxiv.org/abs/2505.21734)
*Felix Jahncke,Johannes Betz*

Main category: cs.RO

TL;DR: MIND-Stack是一个模块化软件堆栈，结合了定位网络和Stanley控制器，具有可解释性和端到端可微性，提升了导航算法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的方法难以从大数据中学习，而端到端神经网络缺乏透明度和模块化。MIND-Stack旨在结合两者的优势。

Method: 采用模块化设计，包括定位网络和Stanley控制器，支持端到端可微性和中间可解释状态表示。

Result: 实验表明定位模块能通过端到端可微性减少控制误差，性能优于现有算法，并在实际嵌入式平台上验证了模拟到现实的能力。

Conclusion: MIND-Stack展示了模块化和端到端学习的潜力，未来可扩展更多模块以进一步提升性能。

Abstract: Developing robust, efficient navigation algorithms is challenging. Rule-based
methods offer interpretability and modularity but struggle with learning from
large datasets, while end-to-end neural networks excel in learning but lack
transparency and modularity. In this paper, we present MIND-Stack, a modular
software stack consisting of a localization network and a Stanley Controller
with intermediate human interpretable state representations and end-to-end
differentiability. Our approach enables the upstream localization module to
reduce the downstream control error, extending its role beyond state
estimation. Unlike existing research on differentiable algorithms that either
lack modules of the autonomous stack to span from sensor input to actuator
output or real-world implementation, MIND-Stack offers both capabilities. We
conduct experiments that demonstrate the ability of the localization module to
reduce the downstream control loss through its end-to-end differentiability
while offering better performance than state-of-the-art algorithms. We showcase
sim-to-real capabilities by deploying the algorithm on a real-world embedded
autonomous platform with limited computation power and demonstrate simultaneous
training of both the localization and controller towards one goal. While
MIND-Stack shows good results, we discuss the incorporation of additional
modules from the autonomous navigation pipeline in the future, promising even
greater stability and performance in the next iterations of the framework.

</details>


### [402] [Streaming Flow Policy: Simplifying diffusion$/$flow-matching policies by treating action trajectories as flow trajectories](https://arxiv.org/abs/2505.21851)
*Sunshine Jiang,Xiaolin Fang,Nicholas Roy,Tomás Lozano-Pérez,Leslie Pack Kaelbling,Siddharth Ancha*

Main category: cs.RO

TL;DR: 论文提出了一种简化的扩散/流匹配策略，通过将动作轨迹视为流轨迹，减少计算成本并实现实时动作执行。


<details>
  <summary>Details</summary>
Motivation: 现有扩散/流匹配策略计算成本高且需等待完整采样过程，无法实时执行动作。

Method: 从最后一个动作的窄高斯分布采样，逐步积分流匹配学习的速度场，生成动作序列。

Result: 方法支持实时动作流传输，保持多模态行为建模能力，性能优于现有方法。

Conclusion: 流策略实现了更快的策略执行和更紧密的感知运动循环，适用于机器人控制。

Abstract: Recent advances in diffusion$/$flow-matching policies have enabled imitation
learning of complex, multi-modal action trajectories. However, they are
computationally expensive because they sample a trajectory of trajectories: a
diffusion$/$flow trajectory of action trajectories. They discard intermediate
action trajectories, and must wait for the sampling process to complete before
any actions can be executed on the robot. We simplify diffusion$/$flow policies
by treating action trajectories as flow trajectories. Instead of starting from
pure noise, our algorithm samples from a narrow Gaussian around the last
action. Then, it incrementally integrates a velocity field learned via flow
matching to produce a sequence of actions that constitute a single trajectory.
This enables actions to be streamed to the robot on-the-fly during the flow
sampling process, and is well-suited for receding horizon policy execution.
Despite streaming, our method retains the ability to model multi-modal
behavior. We train flows that stabilize around demonstration trajectories to
reduce distribution shift and improve imitation learning performance. Streaming
flow policy outperforms prior methods while enabling faster policy execution
and tighter sensorimotor loops for learning-based robot control. Project
website: https://streaming-flow-policy.github.io/

</details>


### [403] [DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation](https://arxiv.org/abs/2505.21864)
*Mengda Xu,Han Zhang,Yifan Hou,Zhenjia Xu,Linxi Fan,Manuela Veloso,Shuran Song*

Main category: cs.RO

TL;DR: DexUMI是一个通过人类手部作为自然接口，将灵巧操作技能转移到多种机器人手的框架，包括硬件和软件适配，成功率为86%。


<details>
  <summary>Details</summary>
Motivation: 解决人类手部与机器人手之间的体现差距问题，以实现灵巧操作技能的有效转移。

Method: 硬件适配使用可穿戴手部外骨骼桥接运动学差距，软件适配通过高保真机器人手部图像替换视觉差距。

Result: 在两个不同的灵巧机器人手硬件平台上实现平均86%的任务成功率。

Conclusion: DexUMI通过硬件和软件适配有效减少了体现差距，成功实现了技能转移。

Abstract: We present DexUMI - a data collection and policy learning framework that uses
the human hand as the natural interface to transfer dexterous manipulation
skills to various robot hands. DexUMI includes hardware and software
adaptations to minimize the embodiment gap between the human hand and various
robot hands. The hardware adaptation bridges the kinematics gap using a
wearable hand exoskeleton. It allows direct haptic feedback in manipulation
data collection and adapts human motion to feasible robot hand motion. The
software adaptation bridges the visual gap by replacing the human hand in video
data with high-fidelity robot hand inpainting. We demonstrate DexUMI's
capabilities through comprehensive real-world experiments on two different
dexterous robot hand hardware platforms, achieving an average task success rate
of 86%.

</details>


### [404] [Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge](https://arxiv.org/abs/2505.21906)
*Zhongyi Zhou,Yichen Zhu,Junjie Wen,Chaomin Shen,Yi Xu*

Main category: cs.RO

TL;DR: ChatVLA-2是一种新型的视觉-语言-动作模型，通过混合专家模型和三阶段训练流程，保留了预训练视觉语言模型的核心能力，并在机器人任务中展现出卓越的推理和空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有端到端VLA系统在微调时会丢失预训练视觉语言模型的关键能力，因此需要一种能保留并扩展这些能力的通用模型。

Method: 提出ChatVLA-2，采用混合专家模型和三阶段训练流程，确保模型在适应机器人任务时仍能保持原有推理能力。

Result: 模型在数学推理、OCR和空间推理任务中表现优异，超越现有模仿学习方法。

Conclusion: ChatVLA-2为开发具有强大推理能力的通用机器人基础模型提供了重要进展。

Abstract: Vision-language-action (VLA) models have emerged as the next generation of
models in robotics. However, despite leveraging powerful pre-trained
Vision-Language Models (VLMs), existing end-to-end VLA systems often lose key
capabilities during fine-tuning as the model adapts to specific robotic tasks.
We argue that a generalizable VLA model should retain and expand upon the VLM's
core competencies: 1) Open-world embodied reasoning - the VLA should inherit
the knowledge from VLM, i.e., recognize anything that the VLM can recognize,
capable of solving math problems, possessing visual-spatial intelligence, 2)
Reasoning following - effectively translating the open-world reasoning into
actionable steps for the robot. In this work, we introduce ChatVLA-2, a novel
mixture-of-expert VLA model coupled with a specialized three-stage training
pipeline designed to preserve the VLM's original strengths while enabling
actionable reasoning. To validate our approach, we design a math-matching task
wherein a robot interprets math problems written on a whiteboard and picks
corresponding number cards from a table to solve equations. Remarkably, our
method exhibits exceptional mathematical reasoning and OCR capabilities,
despite these abilities not being explicitly trained within the VLA.
Furthermore, we demonstrate that the VLA possesses strong spatial reasoning
skills, enabling it to interpret novel directional instructions involving
previously unseen objects. Overall, our method showcases reasoning and
comprehension abilities that significantly surpass state-of-the-art imitation
learning methods such as OpenVLA, DexVLA, and pi-zero. This work represents a
substantial advancement toward developing truly generalizable robotic
foundation models endowed with robust reasoning capacities.

</details>


### [405] [Mastering Agile Tasks with Limited Trials](https://arxiv.org/abs/2505.21916)
*Yihang Hu,Pingyue Sheng,Shengjie Wang,Yang Gao*

Main category: cs.RO

TL;DR: 论文提出了一种名为ADAP的算法，通过模仿人类学习范式，在少量真实试验中迭代优化动作计划，实现高精度动态任务。


<details>
  <summary>Details</summary>
Motivation: 现有机器人方法在处理高敏捷性和高精度任务（如投篮）时面临数据收集、奖励设计和运动规划等挑战，而人类却能轻松完成。

Method: 提出ADAP算法，通过学习先验运动模式并在真实试验中迭代调整动作计划，逐步接近目标。

Result: 实验表明，ADAP能在少于10次试验内完成投篮等高精度动态任务，达到人类水平的效率和精度。

Conclusion: ADAP算法通过模仿人类学习方式，为机器人高精度动态任务提供了一种简单且可扩展的解决方案。

Abstract: Embodied robots nowadays can already handle many real-world manipulation
tasks. However, certain other real-world tasks (e.g., shooting a basketball
into a hoop) are highly agile and require high execution precision, presenting
additional challenges for methods primarily designed for quasi-static
manipulation tasks. This leads to increased efforts in costly data collection,
laborious reward design, or complex motion planning. Such tasks, however, are
far less challenging for humans. Say a novice basketball player typically needs
only $\sim$10 attempts to make their first successful shot, by roughly
imitating a motion prior and then iteratively adjusting their motion based on
the past outcomes. Inspired by this human learning paradigm, we propose the
Adaptive Diffusion Action Plannin (ADAP) algorithm, a simple & scalable
approach which iteratively refines its action plan by few real-world trials
within a learned prior motion pattern, until reaching a specific goal.
Experiments demonstrated that ADAP can learn and accomplish a wide range of
goal-conditioned agile dynamic tasks with human-level precision and efficiency
directly in real-world, such as throwing a basketball into the hoop in fewer
than 10 trials. Project website:https://adap-robotics.github.io/ .

</details>


### [406] [Enhanced SIRRT*: A Structure-Aware RRT* for 2D Path Planning with Hybrid Smoothing and Bidirectional Rewiring](https://arxiv.org/abs/2505.21968)
*Hyejeong Ryu*

Main category: cs.RO

TL;DR: E-SIRRT*是一种基于结构的运动规划器，通过混合路径平滑和双向重布线改进SIRRT*，显著提升初始路径质量、收敛速度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统基于采样的运动规划器（如RRT*和IRRT*）依赖随机采样，导致收敛慢、方差高，尤其在初始解发现延迟时表现不佳。

Method: E-SIRRT*引入混合路径平滑（样条拟合和碰撞感知修正）和双向重布线（局部优化树连接性），结合确定性骨架初始化和结构优化。

Result: 实验表明，E-SIRRT*在100次试验中始终优于IRRT*和SIRRT*，初始路径质量、收敛速度和鲁棒性均显著提升。

Conclusion: E-SIRRT*通过确定性初始化和结构优化，实现了可重复的高效性能，解决了随机采样带来的高方差问题。

Abstract: Sampling-based motion planners such as Rapidly-exploring Random Tree* (RRT*)
and its informed variant IRRT* are widely used for optimal path planning in
complex environments. However, these methods often suffer from slow convergence
and high variance due to their reliance on random sampling, particularly when
initial solution discovery is delayed. This paper presents Enhanced SIRRT*
(E-SIRRT*), a structure-aware planner that improves upon the original SIRRT*
framework by introducing two key enhancements: hybrid path smoothing and
bidirectional rewiring. Hybrid path smoothing refines the initial path through
spline fitting and collision-aware correction, while bidirectional rewiring
locally optimizes tree connectivity around the smoothed path to improve cost
propagation. Experimental results demonstrate that E-SIRRT* consistently
outperforms IRRT* and SIRRT* in terms of initial path quality, convergence
rate, and robustness across 100 trials. Unlike IRRT*, which exhibits high
variability due to stochastic initialization, E-SIRRT* achieves repeatable and
efficient performance through deterministic skeleton-based initialization and
structural refinement.

</details>


### [407] [DORAEMON: Decentralized Ontology-aware Reliable Agent with Enhanced Memory Oriented Navigation](https://arxiv.org/abs/2505.21969)
*Tianjun Gu,Linfeng Li,Xuhong Wang,Chenghua Gong,Jingyu Gong,Zhizhong Zhang,Yuan Xie,Lizhuang Ma,Xin Tan*

Main category: cs.RO

TL;DR: DORAEMON是一种新型认知启发框架，通过模仿人类导航能力解决机器人导航中的时空不连续性和任务理解不足问题，显著提升了零样本自主导航性能。


<details>
  <summary>Details</summary>
Motivation: 家庭服务机器人在陌生环境中的自适应导航需要低层路径规划和高层场景理解，现有基于视觉语言模型的方法存在时空不连续性和任务理解不足的局限性。

Method: DORAEMON框架包含Ventral和Dorsal Streams，分别处理时空不连续性和决策优化，并引入Nav-Ensurance确保导航安全。

Result: 在HM3D、MP3D和GOAT数据集上，DORAEMON在成功率和路径长度加权成功率上均达到最优性能。

Conclusion: DORAEMON在无需预训练或先验地图的情况下，显著提升了零样本自主导航的效率和安全性。

Abstract: Adaptive navigation in unfamiliar environments is crucial for household
service robots but remains challenging due to the need for both low-level path
planning and high-level scene understanding. While recent vision-language model
(VLM) based zero-shot approaches reduce dependence on prior maps and
scene-specific training data, they face significant limitations: spatiotemporal
discontinuity from discrete observations, unstructured memory representations,
and insufficient task understanding leading to navigation failures. We propose
DORAEMON (Decentralized Ontology-aware Reliable Agent with Enhanced Memory
Oriented Navigation), a novel cognitive-inspired framework consisting of
Ventral and Dorsal Streams that mimics human navigation capabilities. The
Dorsal Stream implements the Hierarchical Semantic-Spatial Fusion and Topology
Map to handle spatiotemporal discontinuities, while the Ventral Stream combines
RAG-VLM and Policy-VLM to improve decision-making. Our approach also develops
Nav-Ensurance to ensure navigation safety and efficiency. We evaluate DORAEMON
on the HM3D, MP3D, and GOAT datasets, where it achieves state-of-the-art
performance on both success rate (SR) and success weighted by path length (SPL)
metrics, significantly outperforming existing methods. We also introduce a new
evaluation metric (AORI) to assess navigation intelligence better.
Comprehensive experiments demonstrate DORAEMON's effectiveness in zero-shot
autonomous navigation without requiring prior map building or pre-training.

</details>


### [408] [Learning Compositional Behaviors from Demonstration and Language](https://arxiv.org/abs/2505.21981)
*Weiyu Liu,Neil Nie,Ruohan Zhang,Jiayuan Mao,Jiajun Wu*

Main category: cs.RO

TL;DR: BLADE框架结合模仿学习和基于模型的规划，通过语言注释演示和大型语言模型提取抽象动作知识，构建结构化高层动作表示库，实现机器人长期操作任务。


<details>
  <summary>Details</summary>
Motivation: 解决长期机器人操作任务中动作泛化和适应新情境的挑战。

Method: 利用语言注释演示和LLMs提取动作知识，构建结构化动作表示库，包括基于视觉感知的前提条件和效果，以及神经网络策略控制器。

Result: BLADE能自动恢复结构化表示，无需人工标注状态或符号定义，并在模拟和真实机器人实验中验证了其泛化能力。

Conclusion: BLADE在长期机器人操作任务中表现出强大的泛化能力，适用于复杂情境。

Abstract: We introduce Behavior from Language and Demonstration (BLADE), a framework
for long-horizon robotic manipulation by integrating imitation learning and
model-based planning. BLADE leverages language-annotated demonstrations,
extracts abstract action knowledge from large language models (LLMs), and
constructs a library of structured, high-level action representations. These
representations include preconditions and effects grounded in visual perception
for each high-level action, along with corresponding controllers implemented as
neural network-based policies. BLADE can recover such structured
representations automatically, without manually labeled states or symbolic
definitions. BLADE shows significant capabilities in generalizing to novel
situations, including novel initial states, external state perturbations, and
novel goals. We validate the effectiveness of our approach both in simulation
and on real robots with a diverse set of objects with articulated parts,
partial observability, and geometric constraints.

</details>


### [409] [Soft Electrothermal Meta-Actuator for Robust Multifunctional Control](https://arxiv.org/abs/2505.21992)
*Hanseong Jo,Pavel Shafirin,Christopher Le,Caden Chan,Artur Davoyan*

Main category: cs.RO

TL;DR: 提出了一种基于薄膜热传导的元执行器架构，解决了传统软电热执行器的单向运动、环境敏感性和慢响应问题，实现了双向运动、低环境敏感性和快速复位。


<details>
  <summary>Details</summary>
Motivation: 传统软电热执行器存在单向运动、环境敏感性和慢响应的问题，限制了其应用范围。

Method: 采用薄膜热传导工程设计的元执行器架构，通过电选择实现多功能操作。

Result: 展示了双向运动（偏转≥28%）、低环境敏感性（比传统执行器低100倍以上）和快速复位（比被动冷却快10倍）。

Conclusion: 元执行器架构在软机器人和设备中具有广泛应用潜力，如多功能软夹持器。

Abstract: Soft electrothermal actuators are of great interest in diverse application
domains for their simplicity, compliance, and ease of control. However, the
very nature of thermally induced mechanical actuation sets inherent operation
constraints: unidirectional motion, environmental sensitivity, and slow
response times limited by passive cooling. To overcome these constraints, we
propose a meta-actuator architecture, which uses engineered heat transfer in
thin films to achieve multifunctional operation. We demonstrate electrically
selectable bidirectional motion with large deflection ($ \geq $28% of actuator
length at 0.75 W), suppressed thermal sensitivity to ambient temperature
changes when compared to conventional actuators (>100$ \times $ lower), and
actively forced return to the rest state, which is 10 times faster than that
with passive cooling. We further show that our meta-actuator approach enables
extended ranges of motions for manipulating complex objects. Versatile soft
gripper operations highlight the meta-actuator's potential for soft robotics
and devices.

</details>


### [410] [A simulation framework for autonomous lunar construction work](https://arxiv.org/abs/2505.22091)
*Mattias Linde,Daniel Lindmark,Sandra Ålstig,Martin Servin*

Main category: cs.RO

TL;DR: 提出一个用于月球多自主机器施工的仿真框架，支持建模、模拟执行及分析施工时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 为月球施工任务提供高效、可靠的仿真工具，以优化自主机器的协作与性能。

Method: 基于物理模型模拟多体动力学和可变形地形，使用行为树管理操作逻辑，通过ROS2连接高层决策与底层控制。

Result: 框架在两种月球施工场景中成功测试和演示。

Conclusion: 该框架为复杂月球施工任务提供了有效的仿真解决方案。

Abstract: We present a simulation framework for lunar construction work involving
multiple autonomous machines. The framework supports modelling of construction
scenarios and autonomy solutions, execution of the scenarios in simulation, and
analysis of work time and energy consumption throughout the construction
project. The simulations are based on physics-based models for contacting
multibody dynamics and deformable terrain, including vehicle-soil interaction
forces and soil flow in real time. A behaviour tree manages the operational
logic and error handling, which enables the representation of complex
behaviours through a discrete set of simpler tasks in a modular hierarchical
structure. High-level decision-making is separated from lower-level control
algorithms, with the two connected via ROS2. Excavation movements are
controlled through inverse kinematics and tracking controllers. The framework
is tested and demonstrated on two different lunar construction scenarios.

</details>


### [411] [ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement Learning](https://arxiv.org/abs/2505.22094)
*Tonghe Zhang,Yu Chao,Sicang Su,Yu Wang*

Main category: cs.RO

TL;DR: ReinFlow是一个简单有效的在线强化学习框架，通过注入可学习噪声优化流匹配策略，适用于连续机器人控制任务，显著提升性能并减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 现有方法在连续机器人控制任务中面临探索不足和训练不稳定的问题，ReinFlow旨在通过流匹配策略的优化解决这些问题。

Method: 通过将确定性流路径转换为离散时间马尔可夫过程，注入可学习噪声，实现精确似然计算，支持多样流模型变体的微调。

Result: 在挑战性任务中，Rectified Flow策略的奖励平均增长135.36%，节省82.63%计算时间；Shortcut Model策略成功率提升40.34%，节省23.20%计算时间。

Conclusion: ReinFlow在性能和效率上均优于现有方法，适用于复杂机器人控制任务，且支持少步去噪优化。

Abstract: We propose ReinFlow, a simple yet effective online reinforcement learning
(RL) framework that fine-tunes a family of flow matching policies for
continuous robotic control. Derived from rigorous RL theory, ReinFlow injects
learnable noise into a flow policy's deterministic path, converting the flow
into a discrete-time Markov Process for exact and straightforward likelihood
computation. This conversion facilitates exploration and ensures training
stability, enabling ReinFlow to fine-tune diverse flow model variants,
including Rectified Flow [35] and Shortcut Models [19], particularly at very
few or even one denoising step. We benchmark ReinFlow in representative
locomotion and manipulation tasks, including long-horizon planning with visual
input and sparse reward. The episode reward of Rectified Flow policies obtained
an average net growth of 135.36% after fine-tuning in challenging legged
locomotion tasks while saving denoising steps and 82.63% of wall time compared
to state-of-the-art diffusion RL fine-tuning method DPPO [43]. The success rate
of the Shortcut Model policies in state and visual manipulation tasks achieved
an average net increase of 40.34% after fine-tuning with ReinFlow at four or
even one denoising step, whose performance is comparable to fine-tuned DDIM
policies while saving computation time for an average of 23.20%. Project
Webpage: https://reinflow.github.io/

</details>


### [412] [ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation](https://arxiv.org/abs/2505.22159)
*Jiawen Yu,Hairuo Liu,Qiaojun Yu,Jieji Ren,Ce Hao,Haitong Ding,Guangyu Huang,Guofan Huang,Yan Song,Panpan Cai,Cewu Lu,Wenqiang Zhang*

Main category: cs.RO

TL;DR: ForceVLA是一种新型的端到端机器人操作框架，通过将力反馈作为VLA系统的核心模态，提升了在接触密集型任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA模型在需要精细力控制的接触密集型任务中表现不佳，尤其是在视觉遮挡或动态不确定的情况下。

Method: ForceVLA引入了FVLMoE模块，动态整合预训练的视觉-语言嵌入和实时6轴力反馈，并通过ForceVLA-Data数据集支持。

Result: ForceVLA在接触密集型任务中平均成功率提高了23.2%，在插头插入等任务中达到80%的成功率。

Conclusion: ForceVLA展示了多模态整合在灵巧操作中的重要性，并为物理智能机器人控制设定了新基准。

Abstract: Vision-Language-Action (VLA) models have advanced general-purpose robotic
manipulation by leveraging pretrained visual and linguistic representations.
However, they struggle with contact-rich tasks that require fine-grained
control involving force, especially under visual occlusion or dynamic
uncertainty. To address these limitations, we propose \textbf{ForceVLA}, a
novel end-to-end manipulation framework that treats external force sensing as a
first-class modality within VLA systems. ForceVLA introduces \textbf{FVLMoE}, a
force-aware Mixture-of-Experts fusion module that dynamically integrates
pretrained visual-language embeddings with real-time 6-axis force feedback
during action decoding. This enables context-aware routing across
modality-specific experts, enhancing the robot's ability to adapt to subtle
contact dynamics. We also introduce \textbf{ForceVLA-Data}, a new dataset
comprising synchronized vision, proprioception, and force-torque signals across
five contact-rich manipulation tasks. ForceVLA improves average task success by
23.2\% over strong $\pi_0$-based baselines, achieving up to 80\% success in
tasks such as plug insertion. Our approach highlights the importance of
multimodal integration for dexterous manipulation and sets a new benchmark for
physically intelligent robotic control. Code and data will be released at
https://sites.google.com/view/forcevla2025.

</details>


### [413] [LiDAR Based Semantic Perception for Forklifts in Outdoor Environments](https://arxiv.org/abs/2505.22258)
*Benjamin Serfling,Hannes Reichert,Lorenzo Bayerlein,Konrad Doll,Kati Radkhah-Lens*

Main category: cs.RO

TL;DR: 提出了一种基于双LiDAR的语义分割框架，专为复杂户外环境中的自动叉车设计，提高了障碍物检测和场景理解的精度。


<details>
  <summary>Details</summary>
Motivation: 为工业物料搬运任务中的自动叉车提供全面的场景理解，确保安全导航。

Method: 结合前向和向下倾斜的LiDAR传感器，利用高分辨率3D点云进行轻量级语义分割。

Result: 实验验证表明，该方法在满足实时性要求的同时，实现了高分割精度。

Conclusion: 该框架适用于动态仓库和场地环境中的安全感知全自动叉车导航。

Abstract: In this study, we present a novel LiDAR-based semantic segmentation framework
tailored for autonomous forklifts operating in complex outdoor environments.
Central to our approach is the integration of a dual LiDAR system, which
combines forward-facing and downward-angled LiDAR sensors to enable
comprehensive scene understanding, specifically tailored for industrial
material handling tasks. The dual configuration improves the detection and
segmentation of dynamic and static obstacles with high spatial precision. Using
high-resolution 3D point clouds captured from two sensors, our method employs a
lightweight yet robust approach that segments the point clouds into
safety-critical instance classes such as pedestrians, vehicles, and forklifts,
as well as environmental classes such as driveable ground, lanes, and
buildings. Experimental validation demonstrates that our approach achieves high
segmentation accuracy while satisfying strict runtime requirements,
establishing its viability for safety-aware, fully autonomous forklift
navigation in dynamic warehouse and yard environments.

</details>


### [414] [UP-SLAM: Adaptively Structured Gaussian SLAM with Uncertainty Prediction in Dynamic Environments](https://arxiv.org/abs/2505.22335)
*Wancai Zheng,Linlin Ou,Jiajie He,Libo Zhou,Xinyi Yu,Yan Wei*

Main category: cs.RO

TL;DR: UP-SLAM是一种实时RGB-D SLAM系统，通过并行化框架解耦跟踪与映射，提升动态环境下的性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS技术在动态环境中存在实时性能不足和鲁棒性差的问题。

Method: 采用概率八叉树管理高斯基元，提出无训练不确定性估计器和时间编码器，结合DINO特征增强渲染与预测。

Result: 在定位精度和渲染质量上分别提升59.8%和4.57 dB PSNR，保持实时性能。

Conclusion: UP-SLAM在动态环境中表现优异，生成无伪影的静态地图。

Abstract: Recent 3D Gaussian Splatting (3DGS) techniques for Visual Simultaneous
Localization and Mapping (SLAM) have significantly progressed in tracking and
high-fidelity mapping. However, their sequential optimization framework and
sensitivity to dynamic objects limit real-time performance and robustness in
real-world scenarios. We present UP-SLAM, a real-time RGB-D SLAM system for
dynamic environments that decouples tracking and mapping through a parallelized
framework. A probabilistic octree is employed to manage Gaussian primitives
adaptively, enabling efficient initialization and pruning without hand-crafted
thresholds. To robustly filter dynamic regions during tracking, we propose a
training-free uncertainty estimator that fuses multi-modal residuals to
estimate per-pixel motion uncertainty, achieving open-set dynamic object
handling without reliance on semantic labels. Furthermore, a temporal encoder
is designed to enhance rendering quality. Concurrently, low-dimensional
features are efficiently transformed via a shallow multilayer perceptron to
construct DINO features, which are then employed to enrich the Gaussian field
and improve the robustness of uncertainty prediction. Extensive experiments on
multiple challenging datasets suggest that UP-SLAM outperforms state-of-the-art
methods in both localization accuracy (by 59.8%) and rendering quality (by 4.57
dB PSNR), while maintaining real-time performance and producing reusable,
artifact-free static maps in dynamic environments.The project:
https://aczheng-cai.github.io/up_slam.github.io/

</details>


### [415] [Fully Packed and Ready to Go: High-Density, Rearrangement-Free, Grid-Based Storage and Retrieval](https://arxiv.org/abs/2505.22497)
*Tzvika Geft,Kostas Bekris,Jingjin Yu*

Main category: cs.RO

TL;DR: 论文研究了基于网格的存储系统中负载的放置与重排问题，提出了在特定条件下可实现无重排的解决方案。


<details>
  <summary>Details</summary>
Motivation: 在物流、工业和运输领域中，最大化空间利用率是关键，但密集存储会导致重排成本增加。本文旨在解决如何在负载进出时最小化重排操作。

Method: 研究网格系统中负载的自由移动（如通过移动机器人），分析不同场景（如有限负载到达序列信息或网格开口狭窄）下的无重排解决方案。

Result: 证明在某些场景下（如网格开口至少3个单元格宽时）可实现无重排，并给出了解决方案的实用意义。

Conclusion: 通过自由移动负载和特定网格设计，可以有效减少重排操作，优化存储系统性能。

Abstract: Grid-based storage systems with uniformly shaped loads (e.g., containers,
pallets, totes) are commonplace in logistics, industrial, and transportation
domains. A key performance metric for such systems is the maximization of space
utilization, which requires some loads to be placed behind or below others,
preventing direct access to them. Consequently, dense storage settings bring up
the challenge of determining how to place loads while minimizing costly
rearrangement efforts necessary during retrieval. This paper considers the
setting involving an inbound phase, during which loads arrive, followed by an
outbound phase, during which loads depart. The setting is prevalent in
distribution centers, automated parking garages, and container ports. In both
phases, minimizing the number of rearrangement actions results in more optimal
(e.g., fast, energy-efficient, etc.) operations. In contrast to previous work
focusing on stack-based systems, this effort examines the case where loads can
be freely moved along the grid, e.g., by a mobile robot, expanding the range of
possible motions. We establish that for a range of scenarios, such as having
limited prior knowledge of the loads' arrival sequences or grids with a narrow
opening, a (best possible) rearrangement-free solution always exists, including
when the loads fill the grid to its capacity. In particular, when the sequences
are fully known, we establish an intriguing characterization showing that
rearrangement can always be avoided if and only if the open side of the grid
(used to access the storage) is at least 3 cells wide. We further discuss
useful practical implications of our solutions.

</details>


### [416] [From Strangers to Assistants: Fast Desire Alignment for Embodied Agent-User Adaptation](https://arxiv.org/abs/2505.22503)
*Yuanfei Wang,Xinju Huang,Fangwei Zhong,Yaodong Yang,Yizhou Wang,Yuanpei Chen,Hao Dong*

Main category: cs.RO

TL;DR: 论文提出了一种快速对齐用户潜在需求的框架FAMER，通过心理推理和反思式通信模块提升任务执行和沟通效率。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，智能体需与不熟悉的用户协作，而用户目标常模糊且隐含，因此快速准确对齐需求是关键。

Method: 开发了HA-Desire模拟环境，结合LLM驱动的用户代理，提出FAMER框架，包括心理推理机制和反思式通信模块。

Result: 实验表明FAMER显著提升了任务执行和沟通效率，使智能体能快速适应用户需求。

Conclusion: FAMER框架在复杂环境中有效实现了快速需求对齐，提升了智能体的协作能力。

Abstract: While embodied agents have made significant progress in performing complex
physical tasks, real-world applications demand more than pure task execution.
The agents must collaborate with unfamiliar agents and human users, whose goals
are often vague and implicit. In such settings, interpreting ambiguous
instructions and uncovering underlying desires is essential for effective
assistance. Therefore, fast and accurate desire alignment becomes a critical
capability for embodied agents. In this work, we first develop a home
assistance simulation environment HA-Desire that integrates an LLM-driven human
user agent exhibiting realistic value-driven goal selection and communication.
The ego agent must interact with this proxy user to infer and adapt to the
user's latent desires. To achieve this, we present a novel framework FAMER for
fast desire alignment, which introduces a desire-based mental reasoning
mechanism to identify user intent and filter desire-irrelevant actions. We
further design a reflection-based communication module that reduces redundant
inquiries, and incorporate goal-relevant information extraction with memory
persistence to improve information reuse and reduce unnecessary exploration.
Extensive experiments demonstrate that our framework significantly enhances
both task execution and communication efficiency, enabling embodied agents to
quickly adapt to user-specific desires in complex embodied environments.

</details>


### [417] [VR-Based Control of Multi-Copter Operation](https://arxiv.org/abs/2505.22599)
*Jack T. Hughes,Mohammad Ghufran,Hossein Rastgoftar*

Main category: cs.RO

TL;DR: 利用VR实时扫描无人机周围环境，通过VR头显为飞行员提供第三人称视角，增强空间感知。


<details>
  <summary>Details</summary>
Motivation: 现有无人机操作方案多为第一人称视角，信息有限；传统VR方案依赖预设环境，难以适应动态变化。

Method: 通过机载传感器实时扫描环境，将数据流传输至VR头显，生成虚拟环境与无人机模型。

Result: 提供更丰富的环境信息，增强飞行员的空间感知能力，适用于未知环境。

Conclusion: 实时扫描与VR结合显著提升无人机操作的灵活性与安全性。

Abstract: We aim to use virtual reality (VR) to improve the spatial awareness of pilots
by real-time scanning of the environment around the drone using onboard
sensors, live streaming of this environment to a VR headset, and rendering a
virtual representation of the drone and its environment for the pilot. This
way, the pilot can see the immediate environment of the drone up close from a
third-person perspective, as opposed to the first-person perspective that most
drone cameras provide. This provides much more information about the drone
surroundings for the pilot while operating the drone than existing
teleoperation solutions. Previous solutions using VR have relied upon pre-made
designs of the environment, which makes it difficult to adapt to changing
environments. Our solution, in contrast, scans the environment as you fly,
making it much more flexible for use in unknown environments.

</details>


### [418] [SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale Imitation Learning](https://arxiv.org/abs/2505.22626)
*Yu Zhang,Yuqi Xie,Huihan Liu,Rutav Shah,Michael Wan,Linxi Fan,Yuke Zhu*

Main category: cs.RO

TL;DR: SCIZOR是一种自监督数据筛选框架，通过过滤低质量的状态-动作对提升模仿学习性能。


<details>
  <summary>Details</summary>
Motivation: 模仿学习依赖人类演示数据，但数据质量参差不齐影响性能，现有方法依赖人工标注且粒度较粗。

Method: SCIZOR结合任务进度预测器和去重模块，分别处理次优和冗余数据。

Result: 实验显示SCIZOR平均提升15.4%性能，且减少数据需求。

Conclusion: SCIZOR为模仿学习提供高效数据筛选方案，提升性能并减少数据依赖。

Abstract: Imitation learning advances robot capabilities by enabling the acquisition of
diverse behaviors from human demonstrations. However, large-scale datasets used
for policy training often introduce substantial variability in quality, which
can negatively impact performance. As a result, automatically curating datasets
by filtering low-quality samples to improve quality becomes essential. Existing
robotic curation approaches rely on costly manual annotations and perform
curation at a coarse granularity, such as the dataset or trajectory level,
failing to account for the quality of individual state-action pairs. To address
this, we introduce SCIZOR, a self-supervised data curation framework that
filters out low-quality state-action pairs to improve the performance of
imitation learning policies. SCIZOR targets two complementary sources of
low-quality data: suboptimal data, which hinders learning with undesirable
actions, and redundant data, which dilutes training with repetitive patterns.
SCIZOR leverages a self-supervised task progress predictor for suboptimal data
to remove samples lacking task progression, and a deduplication module
operating on joint state-action representation for samples with redundant
patterns. Empirically, we show that SCIZOR enables imitation learning policies
to achieve higher performance with less data, yielding an average improvement
of 15.4% across multiple benchmarks. More information is available at:
https://ut-austin-rpl.github.io/SCIZOR/

</details>


### [419] [LabUtopia: High-Fidelity Simulation and Hierarchical Benchmark for Scientific Embodied Agents](https://arxiv.org/abs/2505.22634)
*Rui Li,Zixuan Hu,Wenxi Qu,Jinouwen Zhang,Zhenfei Yin,Sha Zhang,Xuantuo Huang,Hanqing Wang,Tai Wang,Jiangmiao Pang,Wanli Ouyang,Lei Bai,Wangmeng Zuo,Ling-Yu Duan,Dongzhan Zhou,Shixiang Tang*

Main category: cs.RO

TL;DR: LabUtopia是一个实验室环境下的仿真与基准测试套件，旨在推动通用化、具备推理能力的智能体发展。


<details>
  <summary>Details</summary>
Motivation: 实验室环境对感知和长期规划要求高，但缺乏合适的仿真器和基准测试阻碍了发展。

Method: LabUtopia包含高保真仿真器LabSim、场景生成器LabScene和分层基准测试LabBench。

Result: 支持30种任务和200多个场景资产，为大规模训练和评估提供平台。

Conclusion: LabUtopia为科学智能体的感知、规划和控制提供了强大平台，并探索其通用性极限。

Abstract: Scientific embodied agents play a crucial role in modern laboratories by
automating complex experimental workflows. Compared to typical household
environments, laboratory settings impose significantly higher demands on
perception of physical-chemical transformations and long-horizon planning,
making them an ideal testbed for advancing embodied intelligence. However, its
development has been long hampered by the lack of suitable simulator and
benchmarks. In this paper, we address this gap by introducing LabUtopia, a
comprehensive simulation and benchmarking suite designed to facilitate the
development of generalizable, reasoning-capable embodied agents in laboratory
settings. Specifically, it integrates i) LabSim, a high-fidelity simulator
supporting multi-physics and chemically meaningful interactions; ii) LabScene,
a scalable procedural generator for diverse scientific scenes; and iii)
LabBench, a hierarchical benchmark spanning five levels of complexity from
atomic actions to long-horizon mobile manipulation. LabUtopia supports 30
distinct tasks and includes more than 200 scene and instrument assets, enabling
large-scale training and principled evaluation in high-complexity environments.
We demonstrate that LabUtopia offers a powerful platform for advancing the
integration of perception, planning, and control in scientific-purpose agents
and provides a rigorous testbed for exploring the practical capabilities and
generalization limits of embodied intelligence in future research.

</details>


### [420] [FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control](https://arxiv.org/abs/2505.22642)
*Younggyo Seo,Carmelo Sferrazza,Haoran Geng,Michal Nauman,Zhao-Heng Yin,Pieter Abbeel*

Main category: cs.RO

TL;DR: FastTD3是一种快速、简单且高效的强化学习算法，显著缩短了人形机器人的训练时间。


<details>
  <summary>Details</summary>
Motivation: 强化学习在机器人领域取得了显著进展，但其复杂性和长训练时间仍是主要瓶颈。

Method: 通过并行模拟、大批量更新、分布评论家和精心调优的超参数，对TD3算法进行改进。

Result: FastTD3在单个A100 GPU上3小时内解决了HumanoidBench任务，且训练过程稳定。

Conclusion: FastTD3为机器人领域的强化学习研究提供了轻量级且易于使用的实现。

Abstract: Reinforcement learning (RL) has driven significant progress in robotics, but
its complexity and long training times remain major bottlenecks. In this
report, we introduce FastTD3, a simple, fast, and capable RL algorithm that
significantly speeds up training for humanoid robots in popular suites such as
HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably
simple: we train an off-policy TD3 agent with several modifications -- parallel
simulation, large-batch updates, a distributional critic, and carefully tuned
hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours
on a single A100 GPU, while remaining stable during training. We also provide a
lightweight and easy-to-use implementation of FastTD3 to accelerate RL research
in robotics.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [421] [RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination](https://arxiv.org/abs/2505.21925)
*Chong Zeng,Yue Dong,Pieter Peers,Hongzhi Wu,Xin Tong*

Main category: cs.GR

TL;DR: RenderFormer是一种神经渲染管道，直接从三角形场景表示生成图像，支持全局光照效果，无需逐场景训练或微调。


<details>
  <summary>Details</summary>
Motivation: 传统渲染方法通常基于物理模拟，计算复杂且耗时。RenderFormer旨在通过序列到序列的转换简化渲染流程，提高效率。

Method: 采用两阶段流水线：1）视图无关阶段，建模三角形间的光传输；2）视图相关阶段，将光线束转换为像素值。两阶段均基于Transformer架构。

Result: 在形状和光传输复杂度不同的场景中验证了RenderFormer的有效性。

Conclusion: RenderFormer提供了一种高效且无需逐场景优化的渲染方法，展示了神经渲染的潜力。

Abstract: We present RenderFormer, a neural rendering pipeline that directly renders an
image from a triangle-based representation of a scene with full global
illumination effects and that does not require per-scene training or
fine-tuning. Instead of taking a physics-centric approach to rendering, we
formulate rendering as a sequence-to-sequence transformation where a sequence
of tokens representing triangles with reflectance properties is converted to a
sequence of output tokens representing small patches of pixels. RenderFormer
follows a two stage pipeline: a view-independent stage that models
triangle-to-triangle light transport, and a view-dependent stage that
transforms a token representing a bundle of rays to the corresponding pixel
values guided by the triangle-sequence from the view-independent stage. Both
stages are based on the transformer architecture and are learned with minimal
prior constraints. We demonstrate and evaluate RenderFormer on scenes with
varying complexity in shape and light transport.

</details>


### [422] [Fluid Simulation on Vortex Particle Flow Maps](https://arxiv.org/abs/2505.21946)
*Sinan Wang,Junwei Zhou,Fan Feng,Zhiqi Li,Yuchen Sun,Duowen Chen,Greg Turk,Bo Zhu*

Main category: cs.GR

TL;DR: VPFM方法通过涡量演化和混合欧拉-拉格朗日表示，显著延长流图距离，提升涡量保持能力。


<details>
  <summary>Details</summary>
Motivation: 模拟动态固体边界下的复杂涡流演化，传统方法流图距离较短。

Method: 结合涡量粒子流图框架、精确Hessian演化方案和固体边界处理。

Result: 流图长度比现有技术长3-12倍，有效捕捉复杂涡流和湍流现象。

Conclusion: VPFM方法在复杂涡流模拟中表现优异，显著提升性能。

Abstract: We propose the Vortex Particle Flow Map (VPFM) method to simulate
incompressible flow with complex vortical evolution in the presence of dynamic
solid boundaries. The core insight of our approach is that vorticity is an
ideal quantity for evolution on particle flow maps, enabling significantly
longer flow map distances compared to other fluid quantities like velocity or
impulse. To achieve this goal, we developed a hybrid Eulerian-Lagrangian
representation that evolves vorticity and flow map quantities on vortex
particles, while reconstructing velocity on a background grid. The method
integrates three key components: (1) a vorticity-based particle flow map
framework, (2) an accurate Hessian evolution scheme on particles, and (3) a
solid boundary treatment for no-through and no-slip conditions in VPFM. These
components collectively allow a substantially longer flow map length (3-12
times longer) than the state-of-the-art, enhancing vorticity preservation over
extended spatiotemporal domains. We validated the performance of VPFM through
diverse simulations, demonstrating its effectiveness in capturing complex
vortex dynamics and turbulence phenomena.

</details>


### [423] [STDR: Spatio-Temporal Decoupling for Real-Time Dynamic Scene Rendering](https://arxiv.org/abs/2505.22400)
*Zehao Li,Hao Jiang,Yujun Cai,Jianing Chen,Baolong Bi,Shuqin Gao,Honglong Zhao,Yiwei Wang,Tianlu Mao,Zhaoqi Wang*

Main category: cs.GR

TL;DR: 论文提出STDR模块，通过解耦时空概率分布提升动态场景重建的时空一致性。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法在动态重建中因时空纠缠导致初始化不连贯，难以准确建模动态运动。

Method: 提出STDR模块，包含时空掩码、分离变形场和一致性正则化，联合解耦时空模式。

Result: 实验表明STDR显著提升了重建质量和时空一致性。

Conclusion: STDR为动态场景重建提供了一种有效的时空解耦解决方案。

Abstract: Although dynamic scene reconstruction has long been a fundamental challenge
in 3D vision, the recent emergence of 3D Gaussian Splatting (3DGS) offers a
promising direction by enabling high-quality, real-time rendering through
explicit Gaussian primitives. However, existing 3DGS-based methods for dynamic
reconstruction often suffer from \textit{spatio-temporal incoherence} during
initialization, where canonical Gaussians are constructed by aggregating
observations from multiple frames without temporal distinction. This results in
spatio-temporally entangled representations, making it difficult to model
dynamic motion accurately. To overcome this limitation, we propose
\textbf{STDR} (Spatio-Temporal Decoupling for Real-time rendering), a
plug-and-play module that learns spatio-temporal probability distributions for
each Gaussian. STDR introduces a spatio-temporal mask, a separated deformation
field, and a consistency regularization to jointly disentangle spatial and
temporal patterns. Extensive experiments demonstrate that incorporating our
module into existing 3DGS-based dynamic scene reconstruction frameworks leads
to notable improvements in both reconstruction quality and spatio-temporal
consistency across synthetic and real-world benchmarks.

</details>


### [424] [Neural Face Skinning for Mesh-agnostic Facial Expression Cloning](https://arxiv.org/abs/2505.22416)
*Sihun Cha,Serin Yoon,Kwanggyoon Seo,Junyong Noh*

Main category: cs.GR

TL;DR: 提出了一种结合全局和局部变形模型的方法，用于面部动画重定向，实现了精确的表情克隆和直观控制。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在面部动画重定向中难以同时兼顾全局控制和局部细节捕捉，导致表达不精确或控制复杂。

Method: 通过预测目标面部网格的顶点权重，将全局潜在代码的影响局部化，并结合FACS-based blendshapes监督潜在代码。

Result: 实验表明，该方法在表情保真度、变形传递准确性和对不同网格结构的适应性上优于现有方法。

Conclusion: 该方法成功结合了全局和局部模型的优势，实现了高精度的面部表情重定向和直观控制。

Abstract: Accurately retargeting facial expressions to a face mesh while enabling
manipulation is a key challenge in facial animation retargeting. Recent
deep-learning methods address this by encoding facial expressions into a global
latent code, but they often fail to capture fine-grained details in local
regions. While some methods improve local accuracy by transferring deformations
locally, this often complicates overall control of the facial expression. To
address this, we propose a method that combines the strengths of both global
and local deformation models. Our approach enables intuitive control and
detailed expression cloning across diverse face meshes, regardless of their
underlying structures. The core idea is to localize the influence of the global
latent code on the target mesh. Our model learns to predict skinning weights
for each vertex of the target face mesh through indirect supervision from
predefined segmentation labels. These predicted weights localize the global
latent code, enabling precise and region-specific deformations even for meshes
with unseen shapes. We supervise the latent code using Facial Action Coding
System (FACS)-based blendshapes to ensure interpretability and allow
straightforward editing of the generated animation. Through extensive
experiments, we demonstrate improved performance over state-of-the-art methods
in terms of expression fidelity, deformation transfer accuracy, and
adaptability across diverse mesh structures.

</details>


### [425] [RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination](https://arxiv.org/abs/2505.21925)
*Chong Zeng,Yue Dong,Pieter Peers,Hongzhi Wu,Xin Tong*

Main category: cs.GR

TL;DR: RenderFormer是一种神经渲染管道，直接从三角形场景表示渲染图像，支持全局光照效果，无需逐场景训练或微调。


<details>
  <summary>Details</summary>
Motivation: 传统渲染方法通常基于物理模拟，计算复杂且耗时。RenderFormer旨在通过序列到序列的转换简化渲染过程，避免逐场景优化。

Method: 采用两阶段流程：1）视图无关阶段，建模三角形间的光传输；2）视图相关阶段，将光线束转换为像素值。两阶段均基于Transformer架构。

Result: 在形状和光传输复杂度不同的场景中进行了验证，展示了RenderFormer的有效性。

Conclusion: RenderFormer提供了一种高效、无需逐场景训练的渲染方法，为神经渲染开辟了新方向。

Abstract: We present RenderFormer, a neural rendering pipeline that directly renders an
image from a triangle-based representation of a scene with full global
illumination effects and that does not require per-scene training or
fine-tuning. Instead of taking a physics-centric approach to rendering, we
formulate rendering as a sequence-to-sequence transformation where a sequence
of tokens representing triangles with reflectance properties is converted to a
sequence of output tokens representing small patches of pixels. RenderFormer
follows a two stage pipeline: a view-independent stage that models
triangle-to-triangle light transport, and a view-dependent stage that
transforms a token representing a bundle of rays to the corresponding pixel
values guided by the triangle-sequence from the view-independent stage. Both
stages are based on the transformer architecture and are learned with minimal
prior constraints. We demonstrate and evaluate RenderFormer on scenes with
varying complexity in shape and light transport.

</details>


### [426] [Fluid Simulation on Vortex Particle Flow Maps](https://arxiv.org/abs/2505.21946)
*Sinan Wang,Junwei Zhou,Fan Feng,Zhiqi Li,Yuchen Sun,Duowen Chen,Greg Turk,Bo Zhu*

Main category: cs.GR

TL;DR: VPFM方法通过涡量演化和混合欧拉-拉格朗日表示，显著延长流图距离，提升涡量保持能力。


<details>
  <summary>Details</summary>
Motivation: 模拟动态固体边界下复杂涡流演化的不可压缩流动。

Method: 混合欧拉-拉格朗日表示，涡量粒子流图框架，Hessian演化方案，固体边界处理。

Result: 流图长度延长3-12倍，有效捕捉复杂涡流和湍流现象。

Conclusion: VPFM方法在涡量演化和流图距离方面优于现有技术。

Abstract: We propose the Vortex Particle Flow Map (VPFM) method to simulate
incompressible flow with complex vortical evolution in the presence of dynamic
solid boundaries. The core insight of our approach is that vorticity is an
ideal quantity for evolution on particle flow maps, enabling significantly
longer flow map distances compared to other fluid quantities like velocity or
impulse. To achieve this goal, we developed a hybrid Eulerian-Lagrangian
representation that evolves vorticity and flow map quantities on vortex
particles, while reconstructing velocity on a background grid. The method
integrates three key components: (1) a vorticity-based particle flow map
framework, (2) an accurate Hessian evolution scheme on particles, and (3) a
solid boundary treatment for no-through and no-slip conditions in VPFM. These
components collectively allow a substantially longer flow map length (3-12
times longer) than the state-of-the-art, enhancing vorticity preservation over
extended spatiotemporal domains. We validated the performance of VPFM through
diverse simulations, demonstrating its effectiveness in capturing complex
vortex dynamics and turbulence phenomena.

</details>


### [427] [STDR: Spatio-Temporal Decoupling for Real-Time Dynamic Scene Rendering](https://arxiv.org/abs/2505.22400)
*Zehao Li,Hao Jiang,Yujun Cai,Jianing Chen,Baolong Bi,Shuqin Gao,Honglong Zhao,Yiwei Wang,Tianlu Mao,Zhaoqi Wang*

Main category: cs.GR

TL;DR: STDR模块通过解耦时空概率分布，解决了3DGS动态重建中的时空不一致问题，显著提升了重建质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS动态重建方法在初始化时因缺乏时间区分导致时空纠缠，难以准确建模动态运动。

Method: 提出STDR模块，引入时空掩码、分离变形场和一致性正则化，解耦时空模式。

Result: 实验表明，STDR显著提升了重建质量和时空一致性。

Conclusion: STDR是一种有效的即插即用模块，适用于动态场景重建。

Abstract: Although dynamic scene reconstruction has long been a fundamental challenge
in 3D vision, the recent emergence of 3D Gaussian Splatting (3DGS) offers a
promising direction by enabling high-quality, real-time rendering through
explicit Gaussian primitives. However, existing 3DGS-based methods for dynamic
reconstruction often suffer from \textit{spatio-temporal incoherence} during
initialization, where canonical Gaussians are constructed by aggregating
observations from multiple frames without temporal distinction. This results in
spatio-temporally entangled representations, making it difficult to model
dynamic motion accurately. To overcome this limitation, we propose
\textbf{STDR} (Spatio-Temporal Decoupling for Real-time rendering), a
plug-and-play module that learns spatio-temporal probability distributions for
each Gaussian. STDR introduces a spatio-temporal mask, a separated deformation
field, and a consistency regularization to jointly disentangle spatial and
temporal patterns. Extensive experiments demonstrate that incorporating our
module into existing 3DGS-based dynamic scene reconstruction frameworks leads
to notable improvements in both reconstruction quality and spatio-temporal
consistency across synthetic and real-world benchmarks.

</details>


### [428] [Neural Face Skinning for Mesh-agnostic Facial Expression Cloning](https://arxiv.org/abs/2505.22416)
*Sihun Cha,Serin Yoon,Kwanggyoon Seo,Junyong Noh*

Main category: cs.GR

TL;DR: 提出了一种结合全局和局部变形模型的方法，用于面部动画重定向，实现直观控制和细节表达克隆。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法通过全局潜在编码难以捕捉局部细节，而局部变形方法又复杂化了整体控制。

Method: 通过预测目标面部网格的顶点权重，将全局潜在代码的影响局部化，并利用FACS-based blendshapes监督潜在代码。

Result: 实验表明，该方法在表达保真度、变形转移准确性和适应性方面优于现有方法。

Conclusion: 该方法成功结合了全局和局部模型的优势，实现了高保真和可控的面部动画重定向。

Abstract: Accurately retargeting facial expressions to a face mesh while enabling
manipulation is a key challenge in facial animation retargeting. Recent
deep-learning methods address this by encoding facial expressions into a global
latent code, but they often fail to capture fine-grained details in local
regions. While some methods improve local accuracy by transferring deformations
locally, this often complicates overall control of the facial expression. To
address this, we propose a method that combines the strengths of both global
and local deformation models. Our approach enables intuitive control and
detailed expression cloning across diverse face meshes, regardless of their
underlying structures. The core idea is to localize the influence of the global
latent code on the target mesh. Our model learns to predict skinning weights
for each vertex of the target face mesh through indirect supervision from
predefined segmentation labels. These predicted weights localize the global
latent code, enabling precise and region-specific deformations even for meshes
with unseen shapes. We supervise the latent code using Facial Action Coding
System (FACS)-based blendshapes to ensure interpretability and allow
straightforward editing of the generated animation. Through extensive
experiments, we demonstrate improved performance over state-of-the-art methods
in terms of expression fidelity, deformation transfer accuracy, and
adaptability across diverse mesh structures.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [429] [Ocean-E2E: Hybrid Physics-Based and Data-Driven Global Forecasting of Extreme Marine Heatwaves with End-to-End Neural Assimilation](https://arxiv.org/abs/2505.22071)
*Ruiqi Shu,Yuan Gao,Hao Wu,Ruijian Gou,Yanfei Xiang,Fan Xu,Qingsong Wen,Xian Wu,Xiaomeng Huang*

Main category: physics.geo-ph

TL;DR: 本文提出了一种名为Ocean-E2E的混合数据驱动与数值模型框架，用于全球极端海洋热浪（MHWs）的端到端预测，显著提升了40天内的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 准确预测极端海洋热浪对科学和金融领域具有重要价值，但现有方法在极端事件预测上存在局限。

Method: 基于海洋热浪的物理特性，结合数据驱动和数值模型，开发了Ocean-E2E框架，通过显式建模海洋中尺度平流和海气相互作用，实现端到端数据同化和高分辨率预测。

Result: 实验表明，Ocean-E2E在全局到区域尺度及短期到长期预测中表现优异，尤其在极端海洋热浪预测上优于现有海洋数值预测-同化模型。

Conclusion: Ocean-E2E为预测和理解海洋热浪及其他气候极端事件提供了新框架，代码已开源。

Abstract: This work focuses on the end-to-end forecast of global extreme marine
heatwaves (MHWs), which are unusually warm sea surface temperature events with
profound impacts on marine ecosystems. Accurate prediction of extreme MHWs has
significant scientific and financial worth. However, existing methods still
have certain limitations, especially in the most extreme MHWs. In this study,
to address these issues, based on the physical nature of MHWs, we created a
novel hybrid data-driven and numerical MHWs forecast framework Ocean-E2E, which
is capable of 40-day accurate MHW forecasting with end-to-end data
assimilation. Our framework significantly improves the forecast ability of
extreme MHWs by explicitly modeling the effect of oceanic mesoscale advection
and air-sea interaction based on a differentiable dynamic kernel. Furthermore,
Ocean-E2E is capable of end-to-end MHWs forecast and regional high-resolution
prediction using neural data assimilation approaches, allowing our framework to
operate completely independently of numerical models while demonstrating high
assimilation stability and accuracy, outperforming the current state-of-the-art
ocean numerical forecasting-assimilation models. Experimental results show that
the proposed framework performs excellently on global-to-regional scales and
short-to-long-term forecasts, especially in those most extreme MHWs. Overall,
our model provides a framework for forecasting and understanding MHWs and other
climate extremes. Our codes are available at
https://github.com/ChiyodaMomo01/Ocean-E2E.

</details>


### [430] [Ocean-E2E: Hybrid Physics-Based and Data-Driven Global Forecasting of Extreme Marine Heatwaves with End-to-End Neural Assimilation](https://arxiv.org/abs/2505.22071)
*Ruiqi Shu,Yuan Gao,Hao Wu,Ruijian Gou,Yanfei Xiang,Fan Xu,Qingsong Wen,Xian Wu,Xiaomeng Huang*

Main category: physics.geo-ph

TL;DR: 提出了一种名为Ocean-E2E的新型混合数据驱动与数值框架，用于40天全球极端海洋热浪（MHWs）的端到端预测，显著提升了预测能力。


<details>
  <summary>Details</summary>
Motivation: 准确预测极端海洋热浪对科学和金融具有重要价值，但现有方法在极端事件预测上存在局限性。

Method: 结合物理特性，开发了混合框架Ocean-E2E，通过可微分动态核显式建模海洋中尺度平流和海气相互作用，并利用神经数据同化实现端到端预测。

Result: 实验表明，Ocean-E2E在全局到区域尺度及短期到长期预测中表现优异，尤其在极端事件上超越现有数值模型。

Conclusion: Ocean-E2E为预测和理解海洋热浪及其他气候极端事件提供了新框架。

Abstract: This work focuses on the end-to-end forecast of global extreme marine
heatwaves (MHWs), which are unusually warm sea surface temperature events with
profound impacts on marine ecosystems. Accurate prediction of extreme MHWs has
significant scientific and financial worth. However, existing methods still
have certain limitations, especially in the most extreme MHWs. In this study,
to address these issues, based on the physical nature of MHWs, we created a
novel hybrid data-driven and numerical MHWs forecast framework Ocean-E2E, which
is capable of 40-day accurate MHW forecasting with end-to-end data
assimilation. Our framework significantly improves the forecast ability of
extreme MHWs by explicitly modeling the effect of oceanic mesoscale advection
and air-sea interaction based on a differentiable dynamic kernel. Furthermore,
Ocean-E2E is capable of end-to-end MHWs forecast and regional high-resolution
prediction using neural data assimilation approaches, allowing our framework to
operate completely independently of numerical models while demonstrating high
assimilation stability and accuracy, outperforming the current state-of-the-art
ocean numerical forecasting-assimilation models. Experimental results show that
the proposed framework performs excellently on global-to-regional scales and
short-to-long-term forecasts, especially in those most extreme MHWs. Overall,
our model provides a framework for forecasting and understanding MHWs and other
climate extremes. Our codes are available at
https://github.com/ChiyodaMomo01/Ocean-E2E.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [431] [Mitigating Audiovisual Mismatch in Visual-Guide Audio Captioning](https://arxiv.org/abs/2505.22045)
*Le Xu,Chenxing Li,Yong Ren,Yujie Chen,Yu Gu,Ruibo Fu,Shan Yang,Dong Yu*

Main category: cs.MM

TL;DR: 提出了一种熵感知门控融合框架，通过跨模态不确定性量化动态调节视觉信息流，解决了视听不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉引导音频字幕系统在真实场景中（如配音或画外音）常因视听不对齐而失效。

Method: 采用注意力熵分析识别并抑制误导性视觉线索，结合批量视听混洗技术生成合成不匹配训练对。

Result: 在AudioCaps基准测试中表现优于现有基线，尤其在不对齐场景下，推理速度提升约6倍。

Conclusion: 该框架有效提升了模型对不匹配模态的鲁棒性和效率。

Abstract: Current vision-guided audio captioning systems frequently fail to address
audiovisual misalignment in real-world scenarios, such as dubbed content or
off-screen sounds. To bridge this critical gap, we present an entropy-aware
gated fusion framework that dynamically modulates visual information flow
through cross-modal uncertainty quantification. Our novel approach employs
attention entropy analysis in cross-attention layers to automatically identify
and suppress misleading visual cues during modal fusion. Complementing this
architecture, we develop a batch-wise audiovisual shuffling technique that
generates synthetic mismatched training pairs, greatly enhancing model
resilience against alignment noise. Evaluations on the AudioCaps benchmark
demonstrate our system's superior performance over existing baselines,
especially in mismatched modality scenarios. Furthermore, our solution
demonstrates an approximately 6x improvement in inference speed compared to the
baseline.

</details>


### [432] [Mitigating Audiovisual Mismatch in Visual-Guide Audio Captioning](https://arxiv.org/abs/2505.22045)
*Le Xu,Chenxing Li,Yong Ren,Yujie Chen,Yu Gu,Ruibo Fu,Shan Yang,Dong Yu*

Main category: cs.MM

TL;DR: 提出了一种熵感知门控融合框架，通过跨模态不确定性量化动态调节视觉信息流，解决视听不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉引导音频字幕系统在真实场景中（如配音或画外音）常因视听不对齐而失效。

Method: 采用注意力熵分析识别并抑制误导性视觉线索，结合批量视听混洗技术生成合成不匹配训练对。

Result: 在AudioCaps基准测试中表现优于现有基线，尤其在模态不匹配场景下，推理速度提升约6倍。

Conclusion: 该方法有效提升了模型对对齐噪声的鲁棒性，同时显著提高了效率。

Abstract: Current vision-guided audio captioning systems frequently fail to address
audiovisual misalignment in real-world scenarios, such as dubbed content or
off-screen sounds. To bridge this critical gap, we present an entropy-aware
gated fusion framework that dynamically modulates visual information flow
through cross-modal uncertainty quantification. Our novel approach employs
attention entropy analysis in cross-attention layers to automatically identify
and suppress misleading visual cues during modal fusion. Complementing this
architecture, we develop a batch-wise audiovisual shuffling technique that
generates synthetic mismatched training pairs, greatly enhancing model
resilience against alignment noise. Evaluations on the AudioCaps benchmark
demonstrate our system's superior performance over existing baselines,
especially in mismatched modality scenarios. Furthermore, our solution
demonstrates an approximately 6x improvement in inference speed compared to the
baseline.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [433] [Detecting Cultural Differences in News Video Thumbnails via Computational Aesthetics](https://arxiv.org/abs/2505.21912)
*Marvin Limpijankit,John Kender*

Main category: cs.CY

TL;DR: 提出了一种两步法分析不同文化背景下图像风格的差异，先按内容聚类，再比较美学特征。实验表明中美YouTube缩略图在风格上有显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究不同文化背景下图像风格的差异，为视觉宣传分析提供基线。

Method: 两步法：先聚类图像内容，再比较美学特征。实验使用2400张中美YouTube缩略图。

Result: 中国缩略图更随意、生动，美国缩略图更正式、精细，色彩、饱和度、对称性等差异显著。

Conclusion: 差异反映文化偏好，方法可作为视觉宣传分析的基线。

Abstract: We propose a two-step approach for detecting differences in the style of
images across sources of differing cultural affinity, where images are first
clustered into finer visual themes based on content before their aesthetic
features are compared. We test this approach on 2,400 YouTube video thumbnails
taken equally from two U.S. and two Chinese YouTube channels, and relating
equally to COVID-19 and the Ukraine conflict. Our results suggest that while
Chinese thumbnails are less formal and more candid, U.S. channels tend to use
more deliberate, proper photographs as thumbnails. In particular, U.S.
thumbnails are less colorful, more saturated, darker, more finely detailed,
less symmetric, sparser, less varied, and more up close and personal than
Chinese thumbnails. We suggest that most of these differences reflect cultural
preferences, and that our methods and observations can serve as a baseline
against which suspected visual propaganda can be computed and compared.

</details>


### [434] [Detecting Cultural Differences in News Video Thumbnails via Computational Aesthetics](https://arxiv.org/abs/2505.21912)
*Marvin Limpijankit,John Kender*

Main category: cs.CY

TL;DR: 论文提出了一种两步法，通过先聚类图像内容再比较美学特征，检测不同文化背景下图像风格的差异。实验基于2400个YouTube缩略图，发现中美缩略图在形式、色彩等方面存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究不同文化背景下图像风格的差异，为视觉宣传分析提供基线。

Method: 采用两步法：先基于内容聚类图像，再比较美学特征。实验数据为2400个中美YouTube缩略图。

Result: 中国缩略图更随意、色彩丰富，美国缩略图更正式、饱和度高、细节丰富。

Conclusion: 差异反映了文化偏好，方法可作为视觉宣传分析的基准。

Abstract: We propose a two-step approach for detecting differences in the style of
images across sources of differing cultural affinity, where images are first
clustered into finer visual themes based on content before their aesthetic
features are compared. We test this approach on 2,400 YouTube video thumbnails
taken equally from two U.S. and two Chinese YouTube channels, and relating
equally to COVID-19 and the Ukraine conflict. Our results suggest that while
Chinese thumbnails are less formal and more candid, U.S. channels tend to use
more deliberate, proper photographs as thumbnails. In particular, U.S.
thumbnails are less colorful, more saturated, darker, more finely detailed,
less symmetric, sparser, less varied, and more up close and personal than
Chinese thumbnails. We suggest that most of these differences reflect cultural
preferences, and that our methods and observations can serve as a baseline
against which suspected visual propaganda can be computed and compared.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [435] [Synonymous Variational Inference for Perceptual Image Compression](https://arxiv.org/abs/2505.22438)
*Zijian Liang,Kai Niu,Changshuo Wang,Jin Xu,Ping Zhang*

Main category: cs.IT

TL;DR: 本文提出了一种基于同义关系的变分推理方法（SVI），用于分析感知图像压缩问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索语义信息理论中语义与句法信息的同义关系，以解决感知图像压缩问题。

Method: 提出同义变分推理（SVI）方法，构建理想同义集（Synset），并通过最小化部分语义KL散度近似其后验分布。

Result: 理论证明了感知图像压缩的优化方向遵循三重权衡，实验结果显示单一渐进式SIC编解码器具有可比性能。

Conclusion: 提出的SVI方法和SIC编解码器在感知图像压缩中表现有效，验证了理论分析的正确性。

Abstract: Recent contributions of semantic information theory reveal the set-element
relationship between semantic and syntactic information, represented as
synonymous relationships. In this paper, we propose a synonymous variational
inference (SVI) method based on this synonymity viewpoint to re-analyze the
perceptual image compression problem. It takes perceptual similarity as a
typical synonymous criterion to build an ideal synonymous set (Synset), and
approximate the posterior of its latent synonymous representation with a
parametric density by minimizing a partial semantic KL divergence. This
analysis theoretically proves that the optimization direction of perception
image compression follows a triple tradeoff that can cover the existing
rate-distortion-perception schemes. Additionally, we introduce synonymous image
compression (SIC), a new image compression scheme that corresponds to the
analytical process of SVI, and implement a progressive SIC codec to fully
leverage the model's capabilities. Experimental results demonstrate comparable
rate-distortion-perception performance using a single progressive SIC codec,
thus verifying the effectiveness of our proposed analysis method.

</details>


### [436] [Synonymous Variational Inference for Perceptual Image Compression](https://arxiv.org/abs/2505.22438)
*Zijian Liang,Kai Niu,Changshuo Wang,Jin Xu,Ping Zhang*

Main category: cs.IT

TL;DR: 提出了一种基于同义关系的变分推理方法（SVI），用于分析感知图像压缩问题，并引入同义图像压缩（SIC）方案，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 揭示语义与句法信息之间的同义关系，并将其应用于感知图像压缩问题。

Method: 通过构建理想同义集（Synset），最小化部分语义KL散度，近似潜在同义表示的后验分布。

Result: 理论证明感知图像压缩的优化方向遵循三重权衡，实验验证了SIC编解码器的性能。

Conclusion: 提出的SVI方法和SIC方案在感知图像压缩中表现出色，验证了其有效性。

Abstract: Recent contributions of semantic information theory reveal the set-element
relationship between semantic and syntactic information, represented as
synonymous relationships. In this paper, we propose a synonymous variational
inference (SVI) method based on this synonymity viewpoint to re-analyze the
perceptual image compression problem. It takes perceptual similarity as a
typical synonymous criterion to build an ideal synonymous set (Synset), and
approximate the posterior of its latent synonymous representation with a
parametric density by minimizing a partial semantic KL divergence. This
analysis theoretically proves that the optimization direction of perception
image compression follows a triple tradeoff that can cover the existing
rate-distortion-perception schemes. Additionally, we introduce synonymous image
compression (SIC), a new image compression scheme that corresponds to the
analytical process of SVI, and implement a progressive SIC codec to fully
leverage the model's capabilities. Experimental results demonstrate comparable
rate-distortion-perception performance using a single progressive SIC codec,
thus verifying the effectiveness of our proposed analysis method.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [437] [More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models](https://arxiv.org/abs/2505.21523)
*Chengzhi Liu,Zhongxing Xu,Qingyue Wei,Juncheng Wu,James Zou,Xin Eric Wang,Yuyin Zhou,Sheng Liu*

Main category: cs.CL

TL;DR: 论文研究了多模态大语言模型在生成长推理链时产生的幻觉问题，提出了RH-AUC指标和RH-Bench基准，发现模型大小和训练数据类型对推理与感知平衡的影响。


<details>
  <summary>Details</summary>
Motivation: 研究多模态大语言模型在长推理链生成中视觉基础减少导致的幻觉问题。

Method: 引入RH-AUC指标量化模型感知准确性随推理长度的变化，并发布RH-Bench基准评估推理能力与幻觉的权衡。

Result: 发现更大模型在推理与感知平衡上表现更好，且训练数据类型比总量对平衡影响更大。

Conclusion: 强调需要同时评估推理质量和感知保真度的框架。

Abstract: Test-time compute has empowered multimodal large language models to generate
extended reasoning chains, yielding strong performance on tasks such as
multimodal math reasoning. However, this improved reasoning ability often comes
with increased hallucination: as generations become longer, models tend to
drift away from image-grounded content and rely more heavily on language
priors. Attention analysis shows that longer reasoning chains lead to reduced
focus on visual inputs, which contributes to hallucination. To systematically
study this phenomenon, we introduce RH-AUC, a metric that quantifies how a
model's perception accuracy changes with reasoning length, allowing us to
evaluate whether the model preserves visual grounding during reasoning. We also
release RH-Bench, a diagnostic benchmark that spans a variety of multimodal
tasks, designed to assess the trade-off between reasoning ability and
hallucination. Our analysis reveals that (i) larger models typically achieve a
better balance between reasoning and perception, and (ii) this balance is
influenced more by the types and domains of training data than by its overall
volume. These findings underscore the importance of evaluation frameworks that
jointly consider both reasoning quality and perceptual fidelity.

</details>


### [438] [VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning](https://arxiv.org/abs/2505.22019)
*Qiuchen Wang,Ruixue Ding,Yu Zeng,Zehui Chen,Lin Chen,Shihang Wang,Pengjun Xie,Fei Huang,Feng Zhao*

Main category: cs.CL

TL;DR: 论文提出VRAG-RL，一种基于强化学习的框架，用于解决视觉丰富信息检索与推理中的问题。


<details>
  <summary>Details</summary>
Motivation: 传统文本方法无法处理视觉信息，而现有视觉RAG方法因固定流程和推理能力不足表现不佳。

Method: 引入VRAG-RL框架，通过强化学习优化视觉语言模型（VLMs）的推理能力，设计动作空间和奖励机制。

Result: VRAG-RL显著提升了视觉信息检索与推理的性能。

Conclusion: VRAG-RL为视觉RAG任务提供了有效的解决方案，并展示了强化学习在该领域的潜力。

Abstract: Effectively retrieving, reasoning and understanding visually rich information
remains a challenge for RAG methods. Traditional text-based methods cannot
handle visual-related information. On the other hand, current vision-based RAG
approaches are often limited by fixed pipelines and frequently struggle to
reason effectively due to the insufficient activation of the fundamental
capabilities of models. As RL has been proven to be beneficial for model
reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex
reasoning across visually rich information. With this framework, VLMs interact
with search engines, autonomously sampling single-turn or multi-turn reasoning
trajectories with the help of visual perception tokens and undergoing continual
optimization based on these samples. Our approach highlights key limitations of
RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely
incorporate images into the context, leading to insufficient reasoning token
allocation and neglecting visual-specific perception; and (ii) When models
interact with search engines, their queries often fail to retrieve relevant
information due to the inability to articulate requirements, thereby leading to
suboptimal performance. To address these challenges, we define an action space
tailored for visually rich inputs, with actions including cropping and scaling,
allowing the model to gather information from a coarse-to-fine perspective.
Furthermore, to bridge the gap between users' original inquiries and the
retriever, we employ a simple yet effective reward that integrates query
rewriting and retrieval performance with a model-based reward. Our VRAG-RL
optimizes VLMs for RAG tasks using specially designed RL strategies, aligning
the model with real-world applications. The code is available at
\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}.

</details>


### [439] [Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start](https://arxiv.org/abs/2505.22334)
*Lai Wei,Yuting Li,Kaipeng Zheng,Chen Wang,Yue Wang,Linghe Kong,Lichao Sun,Weiran Huang*

Main category: cs.CL

TL;DR: 论文提出了一种两阶段方法（监督微调+强化学习）来提升多模态语言模型的推理能力，并在多个基准测试中取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 研究多模态语言模型（MLLMs）中自我修正模式的存在及其与推理性能的关系，并探索如何通过结合监督微调和强化学习来提升推理能力。

Method: 采用两阶段方法：1) 监督微调（SFT）作为冷启动，引入结构化思维链推理模式；2) 通过GRPO进行强化学习（RL）进一步优化。

Result: 该方法在3B和7B规模的MLLMs上均取得最佳性能，7B模型在MathVista和We-Math上分别提升7.1%和7.5%。

Conclusion: 结合监督微调和强化学习的策略为构建先进多模态推理模型提供了实用指导。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
impressive chain-of-thought reasoning capabilities, with reinforcement learning
(RL) playing a crucial role in this progress. While "aha moment"
patterns--where models exhibit self-correction through reflection--are often
attributed to emergent properties from RL, we first demonstrate that these
patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not
necessarily correlate with improved reasoning performance. Building on these
insights, we present a comprehensive study on enhancing multimodal reasoning
through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start
with structured chain-of-thought reasoning patterns, followed by (2)
reinforcement learning via GRPO to further refine these capabilities. Our
extensive experiments show that this combined approach consistently outperforms
both SFT-only and RL-only methods across challenging multimodal reasoning
benchmarks. The resulting models achieve state-of-the-art performance among
open-source MLLMs at both 3B and 7B scales, with our 7B model showing
substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on
MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving
performance competitive with several 7B models. Overall, this work provides
practical guidance for building advanced multimodal reasoning models. Our code
is available at https://github.com/waltonfuture/RL-with-Cold-Start.

</details>


### [440] [Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO](https://arxiv.org/abs/2505.22453)
*Lai Wei,Yuting Li,Chen Wang,Yue Wang,Linghe Kong,Weiran Huang,Lichao Sun*

Main category: cs.CL

TL;DR: MM-UPT是一种基于GRPO的无监督后训练框架，通过自奖励机制提升多模态大语言模型的推理能力，无需外部监督。


<details>
  <summary>Details</summary>
Motivation: 传统监督方法（如SFT和RL）依赖昂贵的人工标注数据，而现有无监督方法复杂且难以迭代。

Method: 提出MM-UPT框架，利用GRPO算法和基于多数投票的自奖励机制，实现无监督后训练。

Result: 实验表明，MM-UPT显著提升模型性能（如MathVista和We-Math数据集），优于现有无监督方法，接近监督GRPO效果。

Conclusion: MM-UPT为无外部监督下多模态大语言模型的持续自主增强提供了新范式。

Abstract: Improving Multi-modal Large Language Models (MLLMs) in the post-training
stage typically relies on supervised fine-tuning (SFT) or reinforcement
learning (RL). However, these supervised methods require expensive and manually
annotated multi-modal data--an ultimately unsustainable resource. While recent
efforts have explored unsupervised post-training, their methods are complex and
difficult to iterate. In this work, we are the first to investigate the use of
GRPO, a stable and scalable online RL algorithm, for enabling continual
self-improvement without any external supervision. We propose MM-UPT, a simple
yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds
upon GRPO, replacing traditional reward signals with a self-rewarding mechanism
based on majority voting over multiple sampled responses. Our experiments
demonstrate that MM-UPT significantly improves the reasoning ability of
Qwen2.5-VL-7B (e.g., 66.3 %$\rightarrow$72.9 % on MathVista, 62.9
%$\rightarrow$68.7 % on We-Math), using standard dataset without ground truth
labels. MM-UPT also outperforms prior unsupervised baselines and even
approaches the results of supervised GRPO. Furthermore, we show that
incorporating synthetic questions, generated solely by MLLM itself, can boost
performance as well, highlighting a promising approach for scalable
self-improvement. Overall, MM-UPT offers a new paradigm for continual,
autonomous enhancement of MLLMs in the absence of external supervision. Our
code is available at https://github.com/waltonfuture/MM-UPT.

</details>


### [441] [Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions](https://arxiv.org/abs/2505.22627)
*Yijun Shen,Delong Chen,Fan Liu,Xingyu Wang,Chuanyi Zhang,Liang Yao,Yuhui Zheng*

Main category: cs.CL

TL;DR: CoTalk是一种AI辅助的标注方法，通过顺序标注和多模态界面优化标注效率，在固定预算下提升标注数量和全面性。


<details>
  <summary>Details</summary>
Motivation: 现有密集标注方法在优化人工标注效率方面研究不足，需探索更高效的方法。

Method: 采用顺序标注减少冗余工作，结合多模态界面（阅读输入、语音输出）提升效率。

Result: 实验表明，CoTalk在标注速度（0.42单位/秒）和检索性能（41.13%）上优于并行方法。

Conclusion: CoTalk通过顺序标注和多模态交互显著提升了标注效率和效果。

Abstract: While densely annotated image captions significantly facilitate the learning
of robust vision-language alignment, methodologies for systematically
optimizing human annotation efforts remain underexplored. We introduce
Chain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize
the number of annotated samples and improve their comprehensiveness under fixed
budget constraints (e.g., total human annotation time). The framework is built
upon two key insights. First, sequential annotation reduces redundant workload
compared to conventional parallel annotation, as subsequent annotators only
need to annotate the ``residual'' -- the missing visual information that
previous annotations have not covered. Second, humans process textual input
faster by reading while outputting annotations with much higher throughput via
talking; thus a multimodal interface enables optimized efficiency. We evaluate
our framework from two aspects: intrinsic evaluations that assess the
comprehensiveness of semantic units, obtained by parsing detailed captions into
object-attribute trees and analyzing their effective connections; extrinsic
evaluation measures the practical usage of the annotated captions in
facilitating vision-language alignment. Experiments with eight participants
show our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30
units/sec) and retrieval performance (41.13\% vs. 40.52\%) over the parallel
method.

</details>


### [442] [Spatial Knowledge Graph-Guided Multimodal Synthesis](https://arxiv.org/abs/2505.22633)
*Yida Xue,Zhen Bi,Jinnan Yang,Jungang Lou,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: SKG2Data是一种基于空间知识图的多模态数据合成方法，旨在提升多模态大语言模型的空间感知能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的空间感知能力有限，需要一种能够生成符合空间常识的合成数据的方法。

Method: 通过构建空间知识图（SKG）模拟人类对空间方向和距离的感知，并以此指导多模态数据合成。

Result: 实验表明，合成的数据显著提升了模型的空间感知和推理能力，并表现出强泛化能力。

Conclusion: 基于知识的数据合成方法有望推动空间智能的发展。

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced their capabilities; however, their spatial perception
abilities remain a notable limitation. To address this challenge, multimodal
data synthesis offers a promising solution. Yet, ensuring that synthesized data
adhere to spatial common sense is a non-trivial task. In this work, we
introduce SKG2Data, a novel multimodal synthesis approach guided by spatial
knowledge graphs, grounded in the concept of knowledge-to-data generation.
SKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate
human-like perception of spatial directions and distances, which is
subsequently utilized to guide multimodal data synthesis. Extensive experiments
demonstrate that data synthesized from diverse types of spatial knowledge,
including direction and distance, not only enhance the spatial perception and
reasoning abilities of MLLMs but also exhibit strong generalization
capabilities. We hope that the idea of knowledge-based data synthesis can
advance the development of spatial intelligence.

</details>


### [443] [More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models](https://arxiv.org/abs/2505.21523)
*Chengzhi Liu,Zhongxing Xu,Qingyue Wei,Juncheng Wu,James Zou,Xin Eric Wang,Yuyin Zhou,Sheng Liu*

Main category: cs.CL

TL;DR: 论文研究了多模态大语言模型在生成长推理链时出现的幻觉问题，提出了RH-AUC指标和RH-Bench基准，发现更大模型能更好平衡推理与感知，且训练数据类型比数量更重要。


<details>
  <summary>Details</summary>
Motivation: 研究多模态大语言模型在长推理链生成中视觉基础内容减少和幻觉增加的现象。

Method: 引入RH-AUC指标量化模型感知准确性随推理长度的变化，并发布RH-Bench基准评估推理能力与幻觉的权衡。

Result: 发现更大模型能更好平衡推理与感知，且训练数据类型比数量对平衡影响更大。

Conclusion: 强调需要同时评估推理质量和感知保真度的框架。

Abstract: Test-time compute has empowered multimodal large language models to generate
extended reasoning chains, yielding strong performance on tasks such as
multimodal math reasoning. However, this improved reasoning ability often comes
with increased hallucination: as generations become longer, models tend to
drift away from image-grounded content and rely more heavily on language
priors. Attention analysis shows that longer reasoning chains lead to reduced
focus on visual inputs, which contributes to hallucination. To systematically
study this phenomenon, we introduce RH-AUC, a metric that quantifies how a
model's perception accuracy changes with reasoning length, allowing us to
evaluate whether the model preserves visual grounding during reasoning. We also
release RH-Bench, a diagnostic benchmark that spans a variety of multimodal
tasks, designed to assess the trade-off between reasoning ability and
hallucination. Our analysis reveals that (i) larger models typically achieve a
better balance between reasoning and perception, and (ii) this balance is
influenced more by the types and domains of training data than by its overall
volume. These findings underscore the importance of evaluation frameworks that
jointly consider both reasoning quality and perceptual fidelity.

</details>


### [444] [VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning](https://arxiv.org/abs/2505.22019)
*Qiuchen Wang,Ruixue Ding,Yu Zeng,Zehui Chen,Lin Chen,Shihang Wang,Pengjun Xie,Fei Huang,Feng Zhao*

Main category: cs.CL

TL;DR: VRAG-RL是一个基于强化学习的框架，用于处理视觉丰富信息的检索和推理，通过优化视觉语言模型（VLM）与搜索引擎的交互，解决传统方法在视觉感知和推理能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统文本检索方法无法处理视觉信息，而现有视觉检索方法因固定流程和推理能力不足而受限。强化学习（RL）被证明能提升模型推理能力，因此提出VRAG-RL框架。

Method: VRAG-RL通过视觉感知令牌和动作空间（如裁剪和缩放）优化VLM与搜索引擎的交互，结合查询重写和检索性能的奖励机制。

Result: VRAG-RL解决了多模态检索中视觉感知不足和查询表达不准确的问题，提升了模型在视觉丰富信息中的推理能力。

Conclusion: VRAG-RL通过强化学习策略优化视觉语言模型，使其更适应实际应用场景，代码已开源。

Abstract: Effectively retrieving, reasoning and understanding visually rich information
remains a challenge for RAG methods. Traditional text-based methods cannot
handle visual-related information. On the other hand, current vision-based RAG
approaches are often limited by fixed pipelines and frequently struggle to
reason effectively due to the insufficient activation of the fundamental
capabilities of models. As RL has been proven to be beneficial for model
reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex
reasoning across visually rich information. With this framework, VLMs interact
with search engines, autonomously sampling single-turn or multi-turn reasoning
trajectories with the help of visual perception tokens and undergoing continual
optimization based on these samples. Our approach highlights key limitations of
RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely
incorporate images into the context, leading to insufficient reasoning token
allocation and neglecting visual-specific perception; and (ii) When models
interact with search engines, their queries often fail to retrieve relevant
information due to the inability to articulate requirements, thereby leading to
suboptimal performance. To address these challenges, we define an action space
tailored for visually rich inputs, with actions including cropping and scaling,
allowing the model to gather information from a coarse-to-fine perspective.
Furthermore, to bridge the gap between users' original inquiries and the
retriever, we employ a simple yet effective reward that integrates query
rewriting and retrieval performance with a model-based reward. Our VRAG-RL
optimizes VLMs for RAG tasks using specially designed RL strategies, aligning
the model with real-world applications. The code is available at
\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}.

</details>


### [445] [Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start](https://arxiv.org/abs/2505.22334)
*Lai Wei,Yuting Li,Kaipeng Zheng,Chen Wang,Yue Wang,Linghe Kong,Lichao Sun,Weiran Huang*

Main category: cs.CL

TL;DR: 论文提出了一种两阶段方法（监督微调+强化学习）来提升多模态推理能力，实验证明其优于单阶段方法，并在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 研究多模态大语言模型（MLLMs）中自修正模式（'aha moment'）是否源于强化学习（RL），并探索如何通过结合监督微调（SFT）和RL提升推理能力。

Method: 采用两阶段方法：1）监督微调（SFT）作为冷启动，引入结构化思维链推理模式；2）通过GRPO进行强化学习以进一步优化。

Result: 两阶段方法在3B和7B规模的MLLMs上均优于单阶段方法，7B模型在MathVista和We-Math上显著提升（如66.3%→73.4%）。

Conclusion: 结合SFT和RL的方法为构建先进多模态推理模型提供了实用指导，代码已开源。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
impressive chain-of-thought reasoning capabilities, with reinforcement learning
(RL) playing a crucial role in this progress. While "aha moment"
patterns--where models exhibit self-correction through reflection--are often
attributed to emergent properties from RL, we first demonstrate that these
patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not
necessarily correlate with improved reasoning performance. Building on these
insights, we present a comprehensive study on enhancing multimodal reasoning
through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start
with structured chain-of-thought reasoning patterns, followed by (2)
reinforcement learning via GRPO to further refine these capabilities. Our
extensive experiments show that this combined approach consistently outperforms
both SFT-only and RL-only methods across challenging multimodal reasoning
benchmarks. The resulting models achieve state-of-the-art performance among
open-source MLLMs at both 3B and 7B scales, with our 7B model showing
substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on
MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving
performance competitive with several 7B models. Overall, this work provides
practical guidance for building advanced multimodal reasoning models. Our code
is available at https://github.com/waltonfuture/RL-with-Cold-Start.

</details>


### [446] [Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO](https://arxiv.org/abs/2505.22453)
*Lai Wei,Yuting Li,Chen Wang,Yue Wang,Linghe Kong,Weiran Huang,Lichao Sun*

Main category: cs.CL

TL;DR: MM-UPT是一种基于GRPO的无监督后训练框架，通过自奖励机制提升多模态大语言模型的推理能力，无需外部监督。


<details>
  <summary>Details</summary>
Motivation: 传统监督方法（如SFT和RL）依赖昂贵的人工标注数据，而现有无监督方法复杂且难以迭代。MM-UPT旨在提供一种简单有效的无监督解决方案。

Method: MM-UPT基于GRPO算法，采用多数投票的自奖励机制替代传统奖励信号，并探索了仅由模型生成的合成问题提升性能。

Result: 实验表明，MM-UPT显著提升了Qwen2.5-VL-7B的推理能力（如MathVista和We-Math任务），优于现有无监督基线，接近监督GRPO结果。

Conclusion: MM-UPT为无外部监督下多模态大语言模型的持续自主增强提供了新范式。

Abstract: Improving Multi-modal Large Language Models (MLLMs) in the post-training
stage typically relies on supervised fine-tuning (SFT) or reinforcement
learning (RL). However, these supervised methods require expensive and manually
annotated multi-modal data--an ultimately unsustainable resource. While recent
efforts have explored unsupervised post-training, their methods are complex and
difficult to iterate. In this work, we are the first to investigate the use of
GRPO, a stable and scalable online RL algorithm, for enabling continual
self-improvement without any external supervision. We propose MM-UPT, a simple
yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds
upon GRPO, replacing traditional reward signals with a self-rewarding mechanism
based on majority voting over multiple sampled responses. Our experiments
demonstrate that MM-UPT significantly improves the reasoning ability of
Qwen2.5-VL-7B (e.g., 66.3 %$\rightarrow$72.9 % on MathVista, 62.9
%$\rightarrow$68.7 % on We-Math), using standard dataset without ground truth
labels. MM-UPT also outperforms prior unsupervised baselines and even
approaches the results of supervised GRPO. Furthermore, we show that
incorporating synthetic questions, generated solely by MLLM itself, can boost
performance as well, highlighting a promising approach for scalable
self-improvement. Overall, MM-UPT offers a new paradigm for continual,
autonomous enhancement of MLLMs in the absence of external supervision. Our
code is available at https://github.com/waltonfuture/MM-UPT.

</details>


### [447] [Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions](https://arxiv.org/abs/2505.22627)
*Yijun Shen,Delong Chen,Fan Liu,Xingyu Wang,Chuanyi Zhang,Liang Yao,Yuhui Zheng*

Main category: cs.CL

TL;DR: CoTalk是一种AI辅助的标注方法，通过序列标注和多模态界面优化标注效率，提升视觉-语言对齐性能。


<details>
  <summary>Details</summary>
Motivation: 现有密集标注方法在优化人工标注效率方面研究不足，需探索更高效的标注策略。

Method: CoTalk采用序列标注减少冗余，结合多模态界面（阅读输入、语音输出）提升效率。

Result: 实验显示CoTalk在标注速度（0.42单位/秒）和检索性能（41.13%）上优于并行方法。

Conclusion: CoTalk在固定预算下显著提升标注效率和视觉-语言对齐效果。

Abstract: While densely annotated image captions significantly facilitate the learning
of robust vision-language alignment, methodologies for systematically
optimizing human annotation efforts remain underexplored. We introduce
Chain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize
the number of annotated samples and improve their comprehensiveness under fixed
budget constraints (e.g., total human annotation time). The framework is built
upon two key insights. First, sequential annotation reduces redundant workload
compared to conventional parallel annotation, as subsequent annotators only
need to annotate the ``residual'' -- the missing visual information that
previous annotations have not covered. Second, humans process textual input
faster by reading while outputting annotations with much higher throughput via
talking; thus a multimodal interface enables optimized efficiency. We evaluate
our framework from two aspects: intrinsic evaluations that assess the
comprehensiveness of semantic units, obtained by parsing detailed captions into
object-attribute trees and analyzing their effective connections; extrinsic
evaluation measures the practical usage of the annotated captions in
facilitating vision-language alignment. Experiments with eight participants
show our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30
units/sec) and retrieval performance (41.13\% vs. 40.52\%) over the parallel
method.

</details>


### [448] [Spatial Knowledge Graph-Guided Multimodal Synthesis](https://arxiv.org/abs/2505.22633)
*Yida Xue,Zhen Bi,Jinnan Yang,Jungang Lou,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: SKG2Data是一种基于空间知识图谱的多模态数据合成方法，旨在提升多模态大语言模型的空间感知能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型的空间感知能力有限，需要一种能够生成符合空间常识的合成数据的方法。

Method: 通过构建空间知识图谱（SKG）模拟人类对空间方向和距离的感知，并指导多模态数据合成。

Result: 实验表明，合成的数据显著提升了模型的空间感知和推理能力，并展现出良好的泛化性。

Conclusion: 基于知识的数据合成方法有望推动空间智能的发展。

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced their capabilities; however, their spatial perception
abilities remain a notable limitation. To address this challenge, multimodal
data synthesis offers a promising solution. Yet, ensuring that synthesized data
adhere to spatial common sense is a non-trivial task. In this work, we
introduce SKG2Data, a novel multimodal synthesis approach guided by spatial
knowledge graphs, grounded in the concept of knowledge-to-data generation.
SKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate
human-like perception of spatial directions and distances, which is
subsequently utilized to guide multimodal data synthesis. Extensive experiments
demonstrate that data synthesized from diverse types of spatial knowledge,
including direction and distance, not only enhance the spatial perception and
reasoning abilities of MLLMs but also exhibit strong generalization
capabilities. We hope that the idea of knowledge-based data synthesis can
advance the development of spatial intelligence.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [449] [Higher-Order Group Synchronization](https://arxiv.org/abs/2505.21932)
*Adriana L. Duncan,Joe Kileel*

Main category: stat.ML

TL;DR: 本文提出了一种新颖的高阶群同步问题，通过超图处理高阶局部测量以获得全局估计，并提出了首个计算框架，展示了其在旋转和角度同步中的优势。


<details>
  <summary>Details</summary>
Motivation: 解决计算机视觉和图像处理等领域中高阶局部测量的同步问题，提供更鲁棒的全局估计方法。

Method: 定义高阶群同步问题，提出基于消息传递算法的计算框架，并进行理论分析和数值实验验证。

Result: 在旋转和角度同步中，高阶方法优于标准成对同步方法，且在模拟冷冻电镜数据中表现与标准方法相当。

Conclusion: 高阶群同步方法在特定情况下更优且更鲁棒，为相关领域提供了新的解决方案。

Abstract: Group synchronization is the problem of determining reliable global estimates
from noisy local measurements on networks. The typical task for group
synchronization is to assign elements of a group to the nodes of a graph in a
way that respects group elements given on the edges which encode information
about local pairwise relationships between the nodes. In this paper, we
introduce a novel higher-order group synchronization problem which operates on
a hypergraph and seeks to synchronize higher-order local measurements on the
hyperedges to obtain global estimates on the nodes. Higher-order group
synchronization is motivated by applications to computer vision and image
processing, among other computational problems. First, we define the problem of
higher-order group synchronization and discuss its mathematical foundations.
Specifically, we give necessary and sufficient synchronizability conditions
which establish the importance of cycle consistency in higher-order group
synchronization. Then, we propose the first computational framework for general
higher-order group synchronization; it acts globally and directly on
higher-order measurements using a message passing algorithm. We discuss
theoretical guarantees for our framework, including convergence analyses under
outliers and noise. Finally, we show potential advantages of our method through
numerical experiments. In particular, we show that in certain cases our
higher-order method applied to rotational and angular synchronization
outperforms standard pairwise synchronization methods and is more robust to
outliers. We also show that our method has comparable performance on simulated
cryo-electron microscopy (cryo-EM) data compared to a standard cryo-EM
reconstruction package.

</details>


### [450] [Higher-Order Group Synchronization](https://arxiv.org/abs/2505.21932)
*Adriana L. Duncan,Joe Kileel*

Main category: stat.ML

TL;DR: 本文提出了一种新的高阶群同步问题，基于超图同步高阶局部测量以获取全局估计，并给出了同步性条件和首个计算框架。


<details>
  <summary>Details</summary>
Motivation: 高阶群同步问题在计算机视觉和图像处理等领域有广泛应用，需要解决传统方法无法处理的高阶测量同步问题。

Method: 提出了一个基于消息传递算法的全局计算框架，直接处理高阶测量，并分析了其在噪声和异常值下的收敛性。

Result: 实验表明，该方法在某些情况下优于传统的成对同步方法，对异常值更具鲁棒性，且在模拟冷冻电镜数据上表现与标准方法相当。

Conclusion: 高阶群同步方法在理论和实践上均表现出优势，为相关领域提供了新的解决方案。

Abstract: Group synchronization is the problem of determining reliable global estimates
from noisy local measurements on networks. The typical task for group
synchronization is to assign elements of a group to the nodes of a graph in a
way that respects group elements given on the edges which encode information
about local pairwise relationships between the nodes. In this paper, we
introduce a novel higher-order group synchronization problem which operates on
a hypergraph and seeks to synchronize higher-order local measurements on the
hyperedges to obtain global estimates on the nodes. Higher-order group
synchronization is motivated by applications to computer vision and image
processing, among other computational problems. First, we define the problem of
higher-order group synchronization and discuss its mathematical foundations.
Specifically, we give necessary and sufficient synchronizability conditions
which establish the importance of cycle consistency in higher-order group
synchronization. Then, we propose the first computational framework for general
higher-order group synchronization; it acts globally and directly on
higher-order measurements using a message passing algorithm. We discuss
theoretical guarantees for our framework, including convergence analyses under
outliers and noise. Finally, we show potential advantages of our method through
numerical experiments. In particular, we show that in certain cases our
higher-order method applied to rotational and angular synchronization
outperforms standard pairwise synchronization methods and is more robust to
outliers. We also show that our method has comparable performance on simulated
cryo-electron microscopy (cryo-EM) data compared to a standard cryo-EM
reconstruction package.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [451] [Large-Area Fabrication-aware Computational Diffractive Optics](https://arxiv.org/abs/2505.22313)
*Kaixuan Wei,Hector A. Jimenez-Romero,Hadi Amata,Jipeng Sun,Qiang Fu,Felix Heide,Wolfgang Heidrich*

Main category: physics.optics

TL;DR: 论文提出了一种制造感知的设计流程，用于通过直接写入灰度光刻和纳米压印复制制造的衍射光学元件，解决了仿真与制造设备之间的质量差距问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要局限于实验室原型，仿真与制造设备之间存在较大质量差距，阻碍了学习衍射光学系统的实际应用。

Method: 提出了一种制造感知的设计流程，包括超分辨神经光刻模型和分布式大规模FFT计算的张量并行计算框架。

Result: 实现了大尺度衍射光学设计，仿真与制造原型在应用（如全息和PSF工程）中表现一致，并展示了仅由单个DOE组成的成像系统的高图像质量。

Conclusion: 研究结果消除了衍射光学和可微分光学设计在实际应用中的制造限制。

Abstract: Differentiable optics, as an emerging paradigm that jointly optimizes optics
and (optional) image processing algorithms, has made innovative optical designs
possible across a broad range of applications. Many of these systems utilize
diffractive optical components (DOEs) for holography, PSF engineering, or
wavefront shaping. Existing approaches have, however, mostly remained limited
to laboratory prototypes, owing to a large quality gap between simulation and
manufactured devices. We aim at lifting the fundamental technical barriers to
the practical use of learned diffractive optical systems. To this end, we
propose a fabrication-aware design pipeline for diffractive optics fabricated
by direct-write grayscale lithography followed by nano-imprinting replication,
which is directly suited for inexpensive mass production of large area designs.
We propose a super-resolved neural lithography model that can accurately
predict the 3D geometry generated by the fabrication process. This model can be
seamlessly integrated into existing differentiable optics frameworks, enabling
fabrication-aware, end-to-end optimization of computational optical systems. To
tackle the computational challenges, we also devise tensor-parallel compute
framework centered on distributing large-scale FFT computation across many
GPUs. As such, we demonstrate large scale diffractive optics designs up to
32.16 mm $\times$ 21.44 mm, simulated on grids of up to 128,640 by 85,760
feature points. We find adequate agreement between simulation and fabricated
prototypes for applications such as holography and PSF engineering. We also
achieve high image quality from an imaging system comprised only of a single
DOE, with images processed only by a Wiener filter utilizing the simulation
PSF. We believe our findings lift the fabrication limitations for real-world
applications of diffractive optics and differentiable optical design.

</details>


### [452] [Compressive Fourier-Domain Intensity Coupling (C-FOCUS) enables near-millimeter deep imaging in the intact mouse brain in vivo](https://arxiv.org/abs/2505.21822)
*Renzhi He,Yucheng Li,Brianna Urbina,Jiandi Wan,Yi Xue*

Main category: physics.optics

TL;DR: C-FOCUS是一种结合傅里叶域强度调制和压缩感知的主动散射校正方法，显著提升双光子显微镜的成像深度和对比度。


<details>
  <summary>Details</summary>
Motivation: 双光子显微镜的成像深度受限于组织散射，现有散射校正技术效果有限且仅适用于小区域。

Method: 提出C-FOCUS方法，结合傅里叶域强度调制和压缩感知技术。

Result: 在活体小鼠大脑中实现超过900微米深度的神经元和血管高分辨率成像，荧光强度提升20倍以上。

Conclusion: C-FOCUS为深层组织光学成像提供了一种快速、高效的解决方案。

Abstract: Two-photon microscopy is a powerful tool for in vivo imaging, but its imaging
depth is typically limited to a few hundred microns due to tissue scattering,
even with existing scattering correction techniques. Moreover, most active
scattering correction methods are restricted to small regions by the optical
memory effect. Here, we introduce compressive Fourier-domain intensity coupling
for scattering correction (C-FOCUS), an active scattering correction approach
that integrates Fourier-domain intensity modulation with compressive sensing
for two-photon microscopy. Using C-FOCUS, we demonstrate high-resolution
imaging of YFP-labeled neurons and FITC-labeled blood vessels at depths
exceeding 900 um in the intact mouse brain in vivo. Furthermore, we achieve
transcranial imaging of YFP-labeled dendritic structures through the intact
adult mouse skull. C-FOCUS enables high-contrast fluorescence imaging at depths
previously inaccessible using two-photon microscopy with 1035 nm excitation,
enhancing fluorescence intensity by over 20-fold compared to uncorrected
imaging. C-FOCUS provides a broadly applicable strategy for rapid, deep-tissue
optical imaging in vivo.

</details>


### [453] [Large-Area Fabrication-aware Computational Diffractive Optics](https://arxiv.org/abs/2505.22313)
*Kaixuan Wei,Hector A. Jimenez-Romero,Hadi Amata,Jipeng Sun,Qiang Fu,Felix Heide,Wolfgang Heidrich*

Main category: physics.optics

TL;DR: 论文提出了一种制造感知的设计流程，用于通过直接写入灰度光刻和纳米压印复制的衍射光学元件，解决了仿真与制造设备之间的质量差距问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法多限于实验室原型，仿真与制造设备之间存在较大质量差距，阻碍了衍射光学系统的实际应用。

Method: 提出制造感知设计流程，包括超分辨神经光刻模型和分布式GPU计算框架，实现大规模衍射光学设计。

Result: 仿真与制造原型在应用（如全息和PSF工程）中表现一致，单DOE成像系统通过Wiener滤波器处理图像获得高质量结果。

Conclusion: 研究解决了衍射光学和可微分光学设计在现实应用中的制造限制。

Abstract: Differentiable optics, as an emerging paradigm that jointly optimizes optics
and (optional) image processing algorithms, has made innovative optical designs
possible across a broad range of applications. Many of these systems utilize
diffractive optical components (DOEs) for holography, PSF engineering, or
wavefront shaping. Existing approaches have, however, mostly remained limited
to laboratory prototypes, owing to a large quality gap between simulation and
manufactured devices. We aim at lifting the fundamental technical barriers to
the practical use of learned diffractive optical systems. To this end, we
propose a fabrication-aware design pipeline for diffractive optics fabricated
by direct-write grayscale lithography followed by nano-imprinting replication,
which is directly suited for inexpensive mass production of large area designs.
We propose a super-resolved neural lithography model that can accurately
predict the 3D geometry generated by the fabrication process. This model can be
seamlessly integrated into existing differentiable optics frameworks, enabling
fabrication-aware, end-to-end optimization of computational optical systems. To
tackle the computational challenges, we also devise tensor-parallel compute
framework centered on distributing large-scale FFT computation across many
GPUs. As such, we demonstrate large scale diffractive optics designs up to
32.16 mm $\times$ 21.44 mm, simulated on grids of up to 128,640 by 85,760
feature points. We find adequate agreement between simulation and fabricated
prototypes for applications such as holography and PSF engineering. We also
achieve high image quality from an imaging system comprised only of a single
DOE, with images processed only by a Wiener filter utilizing the simulation
PSF. We believe our findings lift the fabrication limitations for real-world
applications of diffractive optics and differentiable optical design.

</details>


### [454] [Compressive Fourier-Domain Intensity Coupling (C-FOCUS) enables near-millimeter deep imaging in the intact mouse brain in vivo](https://arxiv.org/abs/2505.21822)
*Renzhi He,Yucheng Li,Brianna Urbina,Jiandi Wan,Yi Xue*

Main category: physics.optics

TL;DR: C-FOCUS是一种结合傅里叶域强度调制和压缩感知的散射校正方法，显著提升双光子显微镜的成像深度和对比度。


<details>
  <summary>Details</summary>
Motivation: 双光子显微镜的成像深度受限于组织散射，现有散射校正技术仅适用于小区域。

Method: 提出C-FOCUS方法，结合傅里叶域强度调制和压缩感知，用于双光子显微镜的散射校正。

Result: 在活体小鼠大脑中实现了超过900微米深度的神经元和血管高分辨率成像，荧光强度提升20倍以上。

Conclusion: C-FOCUS为深层组织光学成像提供了一种快速、高效的策略。

Abstract: Two-photon microscopy is a powerful tool for in vivo imaging, but its imaging
depth is typically limited to a few hundred microns due to tissue scattering,
even with existing scattering correction techniques. Moreover, most active
scattering correction methods are restricted to small regions by the optical
memory effect. Here, we introduce compressive Fourier-domain intensity coupling
for scattering correction (C-FOCUS), an active scattering correction approach
that integrates Fourier-domain intensity modulation with compressive sensing
for two-photon microscopy. Using C-FOCUS, we demonstrate high-resolution
imaging of YFP-labeled neurons and FITC-labeled blood vessels at depths
exceeding 900 um in the intact mouse brain in vivo. Furthermore, we achieve
transcranial imaging of YFP-labeled dendritic structures through the intact
adult mouse skull. C-FOCUS enables high-contrast fluorescence imaging at depths
previously inaccessible using two-photon microscopy with 1035 nm excitation,
enhancing fluorescence intensity by over 20-fold compared to uncorrected
imaging. C-FOCUS provides a broadly applicable strategy for rapid, deep-tissue
optical imaging in vivo.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [455] [Spot-On: A Mixed Reality Interface for Multi-Robot Cooperation](https://arxiv.org/abs/2505.22539)
*Tim Engelbracht,Petar Lukovic,Tjark Behrens,Kai Lascheit,René Zurbrügg,Marc Pollefeys,Hermann Blum,Zuria Bauer*

Main category: cs.HC

TL;DR: 提出了一种新型混合现实（MR）框架，支持多足机器人在语义多样环境中通过MR界面协作完成任务。


<details>
  <summary>Details</summary>
Motivation: 结合MR和机器人技术的最新进展，提升人机协作的复杂性和实用性。

Method: 开发了一个MR框架，支持多足机器人在复杂环境中协作操作抽屉、摆动门和高级基础设施（如电灯开关）。

Result: 用户研究表明，系统设计和可用性得到高度评价，参与者普遍给出“好”或“非常好”的评分。

Conclusion: 该框架为复杂现实场景中的MR多机器人协作提供了有效且直观的解决方案。

Abstract: Recent progress in mixed reality (MR) and robotics is enabling increasingly
sophisticated forms of human-robot collaboration. Building on these
developments, we introduce a novel MR framework that allows multiple quadruped
robots to operate in semantically diverse environments via a MR interface. Our
system supports collaborative tasks involving drawers, swing doors, and
higher-level infrastructure such as light switches. A comprehensive user study
verifies both the design and usability of our app, with participants giving a
"good" or "very good" rating in almost all cases. Overall, our approach
provides an effective and intuitive framework for MR-based multi-robot
collaboration in complex, real-world scenarios.

</details>


### [456] [Spot-On: A Mixed Reality Interface for Multi-Robot Cooperation](https://arxiv.org/abs/2505.22539)
*Tim Engelbracht,Petar Lukovic,Tjark Behrens,Kai Lascheit,René Zurbrügg,Marc Pollefeys,Hermann Blum,Zuria Bauer*

Main category: cs.HC

TL;DR: 提出了一种新型MR框架，支持多足机器人在多样化环境中通过MR界面协作完成任务。


<details>
  <summary>Details</summary>
Motivation: 结合混合现实（MR）和机器人技术，提升人机协作的复杂性和实用性。

Method: 开发了一个MR框架，支持多足机器人在语义多样化环境中协作操作，包括抽屉、摆门和高级基础设施（如电灯开关）。

Result: 用户研究显示，参与者对设计和可用性普遍给予“好”或“非常好”的评价。

Conclusion: 该框架为复杂现实场景中的MR多机器人协作提供了有效且直观的解决方案。

Abstract: Recent progress in mixed reality (MR) and robotics is enabling increasingly
sophisticated forms of human-robot collaboration. Building on these
developments, we introduce a novel MR framework that allows multiple quadruped
robots to operate in semantically diverse environments via a MR interface. Our
system supports collaborative tasks involving drawers, swing doors, and
higher-level infrastructure such as light switches. A comprehensive user study
verifies both the design and usability of our app, with participants giving a
"good" or "very good" rating in almost all cases. Overall, our approach
provides an effective and intuitive framework for MR-based multi-robot
collaboration in complex, real-world scenarios.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [457] [On the concentration distribution in turbulent thermals](https://arxiv.org/abs/2505.21707)
*Ludovic Huguet,Victor Lherm,Renaud Deguen,Joris Heyman,Tanguy Le Borgne*

Main category: physics.flu-dyn

TL;DR: 研究通过实验和数值模拟探讨了高雷诺数湍流热力中的被动标量混合，发现浓度分布具有自相似性，且近似于指数概率密度函数。


<details>
  <summary>Details</summary>
Motivation: 湍流热力在多种地球物理和工业流动中普遍存在，但其内部混合过程尚不完全清楚。

Method: 结合实验室流体动力学实验和直接数值模拟，追踪浓度场的演化并计算其矩和概率密度函数。

Result: 浓度分布随时间呈现自相似性，且近似于指数概率密度函数，且与Péclet和雷诺数无关。

Conclusion: 研究揭示了湍流热力中被动标量混合的自相似性和统计特性，为相关应用提供了理论基础。

Abstract: Turbulent thermals emerge in a wide variety of geophysical and industrial
flows, such as atmospheric cumulus convection and pollutant dispersal in oceans
and lakes. They form when a volume of buoyant fluid, arising from thermal or
compositional density differences, is instantly released from a source. As they
rise or sink, heat and mass transfers are likely to occur with the surrounding
environment, spanning multiple scales from macroscopic entrainment of ambient
fluid to microscopic diffusive processes. Although turbulent thermals are
typically investigated through their integral properties, mixing processes
specifically depend on the internal distribution of concentration or
temperature. Here, we use laboratory fluid dynamics experiments and direct
numerical simulations to investigate the mixing of a passive scalar in
turbulent thermals with large Reynolds numbers. We track the evolution of the
concentration field, computing its moments and the probability density
function. The concentration distribution exhibits self-similarity over time,
except at high concentrations, possibly because of the presence of undiluted
cores. These distributions are well approximated by an exponential probability
density function. In the investigated range, we find that concentration
distributions are largely independent of the P\'eclet and Reynolds numbers.

</details>


### [458] [On the concentration distribution in turbulent thermals](https://arxiv.org/abs/2505.21707)
*Ludovic Huguet,Victor Lherm,Renaud Deguen,Joris Heyman,Tanguy Le Borgne*

Main category: physics.flu-dyn

TL;DR: 论文通过实验和数值模拟研究了高雷诺数湍流热羽中的被动标量混合，发现浓度分布具有自相似性，且近似服从指数概率密度函数。


<details>
  <summary>Details</summary>
Motivation: 湍流热羽在多种地球物理和工业流动中出现，但其内部混合过程尚不完全清楚，特别是浓度或温度的分布特性。

Method: 结合实验室流体动力学实验和直接数值模拟，追踪浓度场的演化，计算其矩和概率密度函数。

Result: 浓度分布随时间呈现自相似性，高浓度区域除外；分布近似服从指数概率密度函数，且与Péclet和雷诺数无关。

Conclusion: 研究揭示了湍流热羽中被动标量混合的统计特性，为理解多尺度混合过程提供了新视角。

Abstract: Turbulent thermals emerge in a wide variety of geophysical and industrial
flows, such as atmospheric cumulus convection and pollutant dispersal in oceans
and lakes. They form when a volume of buoyant fluid, arising from thermal or
compositional density differences, is instantly released from a source. As they
rise or sink, heat and mass transfers are likely to occur with the surrounding
environment, spanning multiple scales from macroscopic entrainment of ambient
fluid to microscopic diffusive processes. Although turbulent thermals are
typically investigated through their integral properties, mixing processes
specifically depend on the internal distribution of concentration or
temperature. Here, we use laboratory fluid dynamics experiments and direct
numerical simulations to investigate the mixing of a passive scalar in
turbulent thermals with large Reynolds numbers. We track the evolution of the
concentration field, computing its moments and the probability density
function. The concentration distribution exhibits self-similarity over time,
except at high concentrations, possibly because of the presence of undiluted
cores. These distributions are well approximated by an exponential probability
density function. In the investigated range, we find that concentration
distributions are largely independent of the P\'eclet and Reynolds numbers.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [459] [Efficient Dynamic Shielding for Parametric Safety Specifications](https://arxiv.org/abs/2505.22104)
*Davide Corsi,Kaushik Mallik,Andoni Rodriguez,Cesar Sanchez*

Main category: cs.AI

TL;DR: 动态屏蔽是一种运行时安全执行工具，用于监控和干预AI控制器的行为，确保安全。传统静态屏蔽需重新计算以适应变化的安全需求，而动态屏蔽能快速适应参数化安全规范的变化。


<details>
  <summary>Details</summary>
Motivation: 传统静态屏蔽在安全需求变化时需重新计算，可能导致致命延迟。动态屏蔽旨在解决这一问题，适应运行时安全需求的变化。

Method: 设计动态屏蔽，利用参数化安全规范，通过简单快速的动态适应算法调整屏蔽行为，无需完全重新计算。

Result: 实验显示，动态屏蔽离线设计仅需几分钟，在线适应时间为秒级，比暴力重新计算方法快5倍。

Conclusion: 动态屏蔽能高效适应运行时安全需求变化，显著提升AI控制系统的安全性和响应速度。

Abstract: Shielding has emerged as a promising approach for ensuring safety of
AI-controlled autonomous systems. The algorithmic goal is to compute a shield,
which is a runtime safety enforcement tool that needs to monitor and intervene
the AI controller's actions if safety could be compromised otherwise.
Traditional shields are designed statically for a specific safety requirement.
Therefore, if the safety requirement changes at runtime due to changing
operating conditions, the shield needs to be recomputed from scratch, causing
delays that could be fatal. We introduce dynamic shields for parametric safety
specifications, which are succinctly represented sets of all possible safety
specifications that may be encountered at runtime. Our dynamic shields are
statically designed for a given safety parameter set, and are able to
dynamically adapt as the true safety specification (permissible by the
parameters) is revealed at runtime. The main algorithmic novelty lies in the
dynamic adaptation procedure, which is a simple and fast algorithm that
utilizes known features of standard safety shields, like maximal
permissiveness. We report experimental results for a robot navigation problem
in unknown territories, where the safety specification evolves as new obstacles
are discovered at runtime. In our experiments, the dynamic shields took a few
minutes for their offline design, and took between a fraction of a second and a
few seconds for online adaptation at each step, whereas the brute-force online
recomputation approach was up to 5 times slower.

</details>


### [460] [Efficiently Enhancing General Agents With Hierarchical-categorical Memory](https://arxiv.org/abs/2505.22006)
*Changze Qiao,Mingming Lu*

Main category: cs.AI

TL;DR: EHC是一种无需参数更新的通用代理，通过分层记忆检索和任务导向经验学习模块，在多模态任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖计算成本高的端到端训练或缺乏持续学习能力的工具使用，EHC旨在解决这些问题。

Method: EHC包含分层记忆检索（HMR）和任务导向经验学习（TOEL）模块，分别用于快速检索记忆和分类任务经验。

Result: 在多个标准数据集上，EHC表现优于现有方法，达到最先进水平。

Conclusion: EHC是一种高效的多模态任务处理代理，无需参数更新即可持续学习。

Abstract: With large language models (LLMs) demonstrating remarkable capabilities,
there has been a surge in research on leveraging LLMs to build general-purpose
multi-modal agents. However, existing approaches either rely on computationally
expensive end-to-end training using large-scale multi-modal data or adopt
tool-use methods that lack the ability to continuously learn and adapt to new
environments. In this paper, we introduce EHC, a general agent capable of
learning without parameter updates. EHC consists of a Hierarchical Memory
Retrieval (HMR) module and a Task-Category Oriented Experience Learning (TOEL)
module. The HMR module facilitates rapid retrieval of relevant memories and
continuously stores new information without being constrained by memory
capacity. The TOEL module enhances the agent's comprehension of various task
characteristics by classifying experiences and extracting patterns across
different categories. Extensive experiments conducted on multiple standard
datasets demonstrate that EHC outperforms existing methods, achieving
state-of-the-art performance and underscoring its effectiveness as a general
agent for handling complex multi-modal tasks.

</details>


### [461] [Efficient Dynamic Shielding for Parametric Safety Specifications](https://arxiv.org/abs/2505.22104)
*Davide Corsi,Kaushik Mallik,Andoni Rodriguez,Cesar Sanchez*

Main category: cs.AI

TL;DR: 动态屏蔽是一种运行时安全执行工具，用于监控和干预AI控制器的行为，确保安全性。传统屏蔽需静态设计，而动态屏蔽能适应运行时变化的安全需求。


<details>
  <summary>Details</summary>
Motivation: 传统屏蔽在安全需求变化时需重新计算，可能导致致命延迟。动态屏蔽旨在解决这一问题。

Method: 动态屏蔽基于参数化安全规范设计，利用已知的安全屏蔽特性（如最大允许性）实现快速动态适应。

Result: 实验表明，动态屏蔽的离线设计仅需几分钟，在线适应时间短，比暴力重计算方法快5倍。

Conclusion: 动态屏蔽能高效适应运行时安全需求变化，适用于未知环境中的机器人导航等问题。

Abstract: Shielding has emerged as a promising approach for ensuring safety of
AI-controlled autonomous systems. The algorithmic goal is to compute a shield,
which is a runtime safety enforcement tool that needs to monitor and intervene
the AI controller's actions if safety could be compromised otherwise.
Traditional shields are designed statically for a specific safety requirement.
Therefore, if the safety requirement changes at runtime due to changing
operating conditions, the shield needs to be recomputed from scratch, causing
delays that could be fatal. We introduce dynamic shields for parametric safety
specifications, which are succinctly represented sets of all possible safety
specifications that may be encountered at runtime. Our dynamic shields are
statically designed for a given safety parameter set, and are able to
dynamically adapt as the true safety specification (permissible by the
parameters) is revealed at runtime. The main algorithmic novelty lies in the
dynamic adaptation procedure, which is a simple and fast algorithm that
utilizes known features of standard safety shields, like maximal
permissiveness. We report experimental results for a robot navigation problem
in unknown territories, where the safety specification evolves as new obstacles
are discovered at runtime. In our experiments, the dynamic shields took a few
minutes for their offline design, and took between a fraction of a second and a
few seconds for online adaptation at each step, whereas the brute-force online
recomputation approach was up to 5 times slower.

</details>


### [462] [Efficiently Enhancing General Agents With Hierarchical-categorical Memory](https://arxiv.org/abs/2505.22006)
*Changze Qiao,Mingming Lu*

Main category: cs.AI

TL;DR: EHC是一种无需参数更新的通用代理，通过分层记忆检索和任务分类经验学习模块，在多模态任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么计算成本高，要么缺乏持续学习能力，EHC旨在解决这些问题。

Method: EHC包含分层记忆检索（HMR）和任务分类经验学习（TOEL）模块，支持快速检索和持续学习。

Result: 在多个标准数据集上，EHC表现优于现有方法，达到最先进水平。

Conclusion: EHC是一种高效、通用的多模态任务处理代理。

Abstract: With large language models (LLMs) demonstrating remarkable capabilities,
there has been a surge in research on leveraging LLMs to build general-purpose
multi-modal agents. However, existing approaches either rely on computationally
expensive end-to-end training using large-scale multi-modal data or adopt
tool-use methods that lack the ability to continuously learn and adapt to new
environments. In this paper, we introduce EHC, a general agent capable of
learning without parameter updates. EHC consists of a Hierarchical Memory
Retrieval (HMR) module and a Task-Category Oriented Experience Learning (TOEL)
module. The HMR module facilitates rapid retrieval of relevant memories and
continuously stores new information without being constrained by memory
capacity. The TOEL module enhances the agent's comprehension of various task
characteristics by classifying experiences and extracting patterns across
different categories. Extensive experiments conducted on multiple standard
datasets demonstrate that EHC outperforms existing methods, achieving
state-of-the-art performance and underscoring its effectiveness as a general
agent for handling complex multi-modal tasks.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [463] [A Coupled Hydro-Morphodynamic Model for Sediment Transport using the Moment Approach](https://arxiv.org/abs/2505.22278)
*Afroja Parvin,Giovanni Samaey,Julian Koellermeier*

Main category: math.NA

TL;DR: 论文提出了一种结合垂直速度变化和底部侵蚀-沉积效应的浅水环境沉积物输运模型，通过多项式展开方法解析速度剖面，提高了计算效率和预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统浅水模型中，垂直速度剖面通常通过经验性对数剖面重建，但实际中可能存在较大偏差。因此，需要一种能够解析垂直速度结构的模型以更准确地预测近床速度。

Method: 采用矩模型方法，通过多项式展开水平速度，结合浅水方程、矩方程、沉积物浓度演化方程和床面输运方程，构建耦合模型。使用双曲正则化技术确保稳定性。

Result: 数值测试（包括干湿前沿的溃坝案例）验证了模型的有效性，结果与实验室数据吻合良好。

Conclusion: 该模型为浅水环境中的沉积物动力学预测提供了一种高效且精确的框架。

Abstract: Sediment transport is crucial in the hydro-morphodynamic evolution of free
surface flows in shallow water environments, which is typically modeled under
the shallow water assumption. In classical shallow water modeling for sediment
transport, the vertical structure of the flow is collapsed into a
depth-averaged and near-bed velocity, usually reconstructed empirically, e.g.,
using a parameterized logarithmic profile. In practice, large variations from
such empirical profiles can occur. It is therefore essential to resolve the
vertical structure of the velocity profile within the shallow water framework
to better approximate near-bed velocity. This study introduces a model and
simulations that incorporate vertical velocity variations and bottom
erosion-deposition effects in sediment transport, providing a computationally
efficient framework for predicting sediment dynamics in shallow water
environments. We employ the so-called moment model approach for the velocity
variation, which considers a polynomial expansion of the horizontal velocity in
the scaled vertical direction. This allows the use of a complex velocity
profile with an extended set of variables determined by the polynomial basis
coefficients, resolving the vertical structure as part of the solution. The
extended model comprises four components: (1) the standard shallow water
equations; (2) moment equations governing evolution of the basis coefficients;
(3) an evolution equation for sediment concentration; and (4) a transport
equation for the bed. This enables a coupled model for bedload and suspended
load transport. We use a hyperbolic regularization technique to ensure model
stability and realistic eigenvalues. Several numerical tests, including
dam-break cases with and without wet/dry fronts, validate our results against
laboratory data.

</details>


### [464] [A Coupled Hydro-Morphodynamic Model for Sediment Transport using the Moment Approach](https://arxiv.org/abs/2505.22278)
*Afroja Parvin,Giovanni Samaey,Julian Koellermeier*

Main category: math.NA

TL;DR: 研究提出了一种结合垂直速度变化和底部侵蚀-沉积效应的沉积物输运模型，改进了浅水环境中的沉积物动力学预测。


<details>
  <summary>Details</summary>
Motivation: 传统浅水模型中的垂直流速结构通常通过经验参数化（如对数剖面）简化，但实际中可能存在较大偏差，因此需要更精确地解析垂直流速结构。

Method: 采用矩模型方法，通过多项式展开水平速度，结合浅水方程、矩方程、沉积物浓度演化方程和河床输运方程，构建耦合模型。

Result: 数值测试（如溃坝实验）验证了模型的有效性，并与实验室数据吻合。

Conclusion: 该模型为浅水环境中的沉积物输运提供了计算高效且精确的预测框架。

Abstract: Sediment transport is crucial in the hydro-morphodynamic evolution of free
surface flows in shallow water environments, which is typically modeled under
the shallow water assumption. In classical shallow water modeling for sediment
transport, the vertical structure of the flow is collapsed into a
depth-averaged and near-bed velocity, usually reconstructed empirically, e.g.,
using a parameterized logarithmic profile. In practice, large variations from
such empirical profiles can occur. It is therefore essential to resolve the
vertical structure of the velocity profile within the shallow water framework
to better approximate near-bed velocity. This study introduces a model and
simulations that incorporate vertical velocity variations and bottom
erosion-deposition effects in sediment transport, providing a computationally
efficient framework for predicting sediment dynamics in shallow water
environments. We employ the so-called moment model approach for the velocity
variation, which considers a polynomial expansion of the horizontal velocity in
the scaled vertical direction. This allows the use of a complex velocity
profile with an extended set of variables determined by the polynomial basis
coefficients, resolving the vertical structure as part of the solution. The
extended model comprises four components: (1) the standard shallow water
equations; (2) moment equations governing evolution of the basis coefficients;
(3) an evolution equation for sediment concentration; and (4) a transport
equation for the bed. This enables a coupled model for bedload and suspended
load transport. We use a hyperbolic regularization technique to ensure model
stability and realistic eigenvalues. Several numerical tests, including
dam-break cases with and without wet/dry fronts, validate our results against
laboratory data.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [465] [SHARAD Illuminates Deeper Martian Subsurface Structures with a Boost from Very Large Rolls of the MRO Spacecraft](https://arxiv.org/abs/2505.21810)
*Nathaniel E. Putzig,Gareth A. Morgan,Matthew R. Perry,Bruce A. Campbell,Jennifer L. Whitten,Fabrizio Bernardini,Alessandro DiCarlofelice,Pierfrancesco Lombardo*

Main category: astro-ph.EP

TL;DR: 火星勘测轨道飞行器（MRO）通过大幅滚动（120°）提升浅层雷达（SHARAD）信号质量，显著增强了信号清晰度和穿透深度。


<details>
  <summary>Details</summary>
Motivation: SHARAD天线位于航天器边缘，导致信号质量不佳，需要通过滚动补偿以提升信噪比（S/N）。

Method: MRO执行了120°的大幅滚动（VLR）观测，测试其对雷达探测的影响。

Result: VLR观测使S/N提升了9-14 dB，在低介电地形中实现了前所未有的基底探测深度（800-1500 m）。

Conclusion: MRO计划继续在极地和冰川区域进行VLR观测，以进一步探索地下结构。

Abstract: Throughout its mission, the Mars Reconnaissance Orbiter (MRO) has often
rolled about its along-track axis by up to 28{\deg} to partially compensate for
the suboptimal location of the Shallow Radar (SHARAD) antenna along an edge of
the spacecraft that is opposite the imaging payload deck, thereby enhancing the
signal-to-noise ratio (S/N) of echoes returned from the surface. After recent
modeling work predicted that a much larger roll would improve the S/N by ~10 dB
relative to nadir-pointed observing, MRO began a limited series of 120{\deg}
roll maneuvers to test the effects on radar sounding. Three such SHARAD
very-large-roll (VLR) observations have been acquired since May 2023, and they
show dramatic improvements in signal clarity and depth of penetration, with S/N
increasing by 9, 11, and 14 dB over that of nearly coincident observations at
0{\deg} roll angle. In low dielectric terrains, the first and second VLR
observations enabled basal detections at depths previously unachievable,
reaching depths of 800 m in Medusae Fossae materials and 1500 m through the ice
of Ultimi Scopuli, respectively. The second VLR observation also obtained
enhanced reflections throughout the ice stack. In the higher dielectric terrain
of Amazonis Planitia, the third VLR observation improved continuity of a
dipping subsurface interface, but it revealed neither an extension of the
interface to greater depths nor any deeper interfaces. The MRO mission intends
to obtain more SHARAD VLR observations of polar terrains and of midlatitude
glacial and ground ices, sediments, and volcanics.

</details>


### [466] [SHARAD Illuminates Deeper Martian Subsurface Structures with a Boost from Very Large Rolls of the MRO Spacecraft](https://arxiv.org/abs/2505.21810)
*Nathaniel E. Putzig,Gareth A. Morgan,Matthew R. Perry,Bruce A. Campbell,Jennifer L. Whitten,Fabrizio Bernardini,Alessandro DiCarlofelice,Pierfrancesco Lombardo*

Main category: astro-ph.EP

TL;DR: 火星勘测轨道飞行器（MRO）通过大幅滚动（120°）提升浅层雷达（SHARAD）信号质量，显著增强信号清晰度和穿透深度。


<details>
  <summary>Details</summary>
Motivation: SHARAD天线位置不理想，限制了雷达信号的信噪比（S/N），通过大幅滚动可以优化信号接收。

Method: MRO执行了120°的滚动机动，测试其对雷达探测的影响。

Result: 三次大幅滚动观测显示S/N提升了9-14 dB，并在低介电地形中实现了更深的探测（如1500米冰层）。

Conclusion: 大幅滚动显著提升了SHARAD的性能，未来将用于极地和冰川区域的探测。

Abstract: Throughout its mission, the Mars Reconnaissance Orbiter (MRO) has often
rolled about its along-track axis by up to 28{\deg} to partially compensate for
the suboptimal location of the Shallow Radar (SHARAD) antenna along an edge of
the spacecraft that is opposite the imaging payload deck, thereby enhancing the
signal-to-noise ratio (S/N) of echoes returned from the surface. After recent
modeling work predicted that a much larger roll would improve the S/N by ~10 dB
relative to nadir-pointed observing, MRO began a limited series of 120{\deg}
roll maneuvers to test the effects on radar sounding. Three such SHARAD
very-large-roll (VLR) observations have been acquired since May 2023, and they
show dramatic improvements in signal clarity and depth of penetration, with S/N
increasing by 9, 11, and 14 dB over that of nearly coincident observations at
0{\deg} roll angle. In low dielectric terrains, the first and second VLR
observations enabled basal detections at depths previously unachievable,
reaching depths of 800 m in Medusae Fossae materials and 1500 m through the ice
of Ultimi Scopuli, respectively. The second VLR observation also obtained
enhanced reflections throughout the ice stack. In the higher dielectric terrain
of Amazonis Planitia, the third VLR observation improved continuity of a
dipping subsurface interface, but it revealed neither an extension of the
interface to greater depths nor any deeper interfaces. The MRO mission intends
to obtain more SHARAD VLR observations of polar terrains and of midlatitude
glacial and ground ices, sediments, and volcanics.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [467] [A Provable Approach for End-to-End Safe Reinforcement Learning](https://arxiv.org/abs/2505.21852)
*Akifumi Wachi,Kohei Miyaguchi,Takumi Tanabe,Rei Sato,Youhei Akimoto*

Main category: cs.LG

TL;DR: 本文提出了一种名为PLS的方法，通过结合离线安全强化学习与安全策略部署，确保策略从学习到运行的整个生命周期中的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有安全强化学习方法难以确保策略从学习到运行的全程安全性，因此需要一种新方法来解决这一问题。

Method: PLS通过离线学习策略（基于回报条件的监督学习）和部署时谨慎优化目标回报（使用高斯过程）来实现安全性。

Result: 理论分析表明PLS能高效找到接近最优的目标回报，并高概率保证安全性；实验显示PLS在安全性和奖励性能上均优于基线方法。

Conclusion: PLS成功实现了在策略生命周期中同时保证高奖励和高安全性的目标。

Abstract: A longstanding goal in safe reinforcement learning (RL) is a method to ensure
the safety of a policy throughout the entire process, from learning to
operation. However, existing safe RL paradigms inherently struggle to achieve
this objective. We propose a method, called Provably Lifetime Safe RL (PLS),
that integrates offline safe RL with safe policy deployment to address this
challenge. Our proposed method learns a policy offline using return-conditioned
supervised learning and then deploys the resulting policy while cautiously
optimizing a limited set of parameters, known as target returns, using Gaussian
processes (GPs). Theoretically, we justify the use of GPs by analyzing the
mathematical relationship between target and actual returns. We then prove that
PLS finds near-optimal target returns while guaranteeing safety with high
probability. Empirically, we demonstrate that PLS outperforms baselines both in
safety and reward performance, thereby achieving the longstanding goal to
obtain high rewards while ensuring the safety of a policy throughout the
lifetime from learning to operation.

</details>


### [468] [Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation](https://arxiv.org/abs/2505.21525)
*Peiliang Gong,Yucheng Wang,Min Wu,Zhenghua Chen,Xiaoli Li,Daoqiang Zhang*

Main category: cs.LG

TL;DR: TERSE是一种针对多变量时间序列数据的无源域自适应方法，通过时空特征编码和任务设计实现跨域特征对齐。


<details>
  <summary>Details</summary>
Motivation: 现有SFDA方法在多变量时间序列（MTS）数据上表现不佳，未能充分利用其固有的空间相关性。

Method: 提出TERSE方法，结合时空特征编码、时间恢复和空间重连任务，实现跨域时空一致性。

Result: 在三个真实时间序列数据集上验证了TERSE的有效性和通用性。

Conclusion: TERSE是首个同时考虑时空一致性的MTS-SFDA方法，可作为插件模块集成到现有SFDA方法中。

Abstract: Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from
an annotated source domain to an unlabelled target domain without accessing the
source data, thereby preserving data privacy. While existing SFDA methods have
proven effective in reducing reliance on source data, they struggle to perform
well on multivariate time series (MTS) due to their failure to consider the
intrinsic spatial correlations inherent in MTS data. These spatial correlations
are crucial for accurately representing MTS data and preserving invariant
information across domains. To address this challenge, we propose Temporal
Restoration and Spatial Rewiring (TERSE), a novel and concise SFDA method
tailored for MTS data. Specifically, TERSE comprises a customized
spatial-temporal feature encoder designed to capture the underlying
spatial-temporal characteristics, coupled with both temporal restoration and
spatial rewiring tasks to reinstate latent representations of the temporally
masked time series and the spatially masked correlated structures. During the
target adaptation phase, the target encoder is guided to produce spatially and
temporally consistent features with the source domain by leveraging the source
pre-trained temporal restoration and spatial rewiring networks. Therefore,
TERSE can effectively model and transfer spatial-temporal dependencies across
domains, facilitating implicit feature alignment. In addition, as the first
approach to simultaneously consider spatial-temporal consistency in MTS-SFDA,
TERSE can also be integrated as a versatile plug-and-play module into
established SFDA methods. Extensive experiments on three real-world time series
datasets demonstrate the effectiveness and versatility of our approach.

</details>


### [469] [Taming Transformer Without Using Learning Rate Warmup](https://arxiv.org/abs/2505.21910)
*Xianbiao Qi,Yelin He,Jiaquan Ye,Chun-Guang Li,Bojia Zi,Xili Dai,Qin Zou,Rong Xiao*

Main category: cs.LG

TL;DR: 论文分析了Transformer训练中的模型崩溃现象，提出了一种基于谱能量集中的优化策略，避免了恶性熵崩溃，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer在大规模训练中不依赖学习率预热等技术时的崩溃现象，揭示其背后的理论原因。

Method: 通过分析${\bW_q}^{\top} \bW_k$的谱能量集中现象，提出一种基于Weyl不等式的优化策略，动态调整学习率以防止熵崩溃。

Result: 实验证明该方法能有效稳定训练ViT、Swin-Transformer和GPT，无需学习率预热。

Conclusion: 提出的优化策略解决了Transformer训练中的崩溃问题，为大规模训练提供了新思路。

Abstract: Scaling Transformer to a large scale without using some technical tricks such
as learning rate warump and using an obviously lower learning rate is an
extremely challenging task, and is increasingly gaining more attention. In this
paper, we provide a theoretical analysis for the process of training
Transformer and reveal the rationale behind the model crash phenomenon in the
training process, termed \textit{spectral energy concentration} of
${\bW_q}^{\top} \bW_k$, which is the reason for a malignant entropy collapse,
where ${\bW_q}$ and $\bW_k$ are the projection matrices for the query and the
key in Transformer, respectively. To remedy this problem, motivated by
\textit{Weyl's Inequality}, we present a novel optimization strategy, \ie,
making the weight updating in successive steps smooth -- if the ratio
$\frac{\sigma_{1}(\nabla \bW_t)}{\sigma_{1}(\bW_{t-1})}$ is larger than a
threshold, we will automatically bound the learning rate to a weighted multiple
of $\frac{\sigma_{1}(\bW_{t-1})}{\sigma_{1}(\nabla \bW_t)}$, where $\nabla
\bW_t$ is the updating quantity in step $t$. Such an optimization strategy can
prevent spectral energy concentration to only a few directions, and thus can
avoid malignant entropy collapse which will trigger the model crash. We conduct
extensive experiments using ViT, Swin-Transformer and GPT, showing that our
optimization strategy can effectively and stably train these Transformers
without using learning rate warmup.

</details>


### [470] [From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization](https://arxiv.org/abs/2505.22310)
*Shoaib Ahmed Siddiqui,Adrian Weller,David Krueger,Gintare Karolina Dziugaite,Michael Curtis Mozer,Eleni Triantafillou*

Main category: cs.LG

TL;DR: 研究发现，LLM的遗忘方法容易被重新学习攻击，即使微调少量无关样本，遗忘的知识也会重现。在视觉分类器中，仅使用保留集微调即可使遗忘集准确率从50%恢复至近100%。


<details>
  <summary>Details</summary>
Motivation: 探索LLM遗忘方法的脆弱性，揭示遗忘知识容易被重新学习的现象。

Method: 在视觉分类器中控制实验，测试不同遗忘方法对重新学习攻击的抵抗能力，分析权重空间特性（如L2距离和线性模式连通性）。

Result: 遗忘集准确率可通过保留集微调恢复至近100%，而从头训练的模型准确率保持50%。权重空间特性可预测抵抗能力。

Conclusion: 提出新方法，基于权重空间特性实现最佳抵抗重新学习攻击的效果。

Abstract: Recent unlearning methods for LLMs are vulnerable to relearning attacks:
knowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of
(even seemingly-unrelated) examples. We study this phenomenon in a controlled
setting for example-level unlearning in vision classifiers. We make the
surprising discovery that forget-set accuracy can recover from around 50%
post-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e.,
zero examples of the forget set. We observe this effect across a wide variety
of unlearning methods, whereas for a model retrained from scratch excluding the
forget set (gold standard), the accuracy remains at 50%. We observe that
resistance to relearning attacks can be predicted by weight-space properties,
specifically, $L_2$-distance and linear mode connectivity between the original
and the unlearned model. Leveraging this insight, we propose a new class of
methods that achieve state-of-the-art resistance to relearning attacks.

</details>


### [471] [A Closer Look at Multimodal Representation Collapse](https://arxiv.org/abs/2505.22483)
*Abhra Chaudhuri,Anjan Dutta,Tu Bui,Serban Georgescu*

Main category: cs.LG

TL;DR: 论文研究了模态崩溃现象，提出了一种通过显式基重分配防止模态崩溃的算法，并在多模态基准测试中验证了理论。


<details>
  <summary>Details</summary>
Motivation: 模态崩溃是多模态融合模型中常见的问题，模型倾向于依赖部分模态而忽略其他模态，影响了性能。

Method: 通过跨模态知识蒸馏和显式基重分配算法，解决模态崩溃问题。

Result: 实验验证了理论，算法在多模态基准测试中表现良好。

Conclusion: 提出的方法有效防止了模态崩溃，并适用于处理缺失模态的情况。

Abstract: We aim to develop a fundamental understanding of modality collapse, a
recently observed empirical phenomenon wherein models trained for multimodal
fusion tend to rely only on a subset of the modalities, ignoring the rest. We
show that modality collapse happens when noisy features from one modality are
entangled, via a shared set of neurons in the fusion head, with predictive
features from another, effectively masking out positive contributions from the
predictive features of the former modality and leading to its collapse. We
further prove that cross-modal knowledge distillation implicitly disentangles
such representations by freeing up rank bottlenecks in the student encoder,
denoising the fusion-head outputs without negatively impacting the predictive
features from either modality. Based on the above findings, we propose an
algorithm that prevents modality collapse through explicit basis reallocation,
with applications in dealing with missing modalities. Extensive experiments on
multiple multimodal benchmarks validate our theoretical claims. Project page:
https://abhrac.github.io/mmcollapse/.

</details>


### [472] [Understanding Adversarial Training with Energy-based Models](https://arxiv.org/abs/2505.22486)
*Mujtaba Hussain Mirza,Maria Rosaria Briglia,Filippo Bartolucci,Senad Beadini,Giuseppe Lisanti,Iacopo Masi*

Main category: cs.LG

TL;DR: 论文通过能量模型框架分析对抗训练中的灾难性过拟合和鲁棒过拟合现象，提出Delta Energy Regularizer（DER）以平滑能量景观，并探索鲁棒分类器的生成能力。


<details>
  <summary>Details</summary>
Motivation: 研究对抗训练中能量动态及其与过拟合的关系，同时探索鲁棒分类器的生成潜力。

Method: 通过能量视角分析对抗样本与自然样本的能量差异，提出DER正则化器，并改进生成技术。

Result: DER有效缓解过拟合，鲁棒分类器在生成任务中表现竞争性。

Conclusion: 能量视角为对抗训练提供新见解，DER和生成技术改进具有实际应用价值。

Abstract: We aim at using Energy-based Model (EBM) framework to better understand
adversarial training (AT) in classifiers, and additionally to analyze the
intrinsic generative capabilities of robust classifiers. By viewing standard
classifiers through an energy lens, we begin by analyzing how the energies of
adversarial examples, generated by various attacks, differ from those of the
natural samples. The central focus of our work is to understand the critical
phenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT
from an energy perspective. We analyze the impact of existing AT approaches on
the energy of samples during training and observe that the behavior of the
``delta energy' -- change in energy between original sample and its adversarial
counterpart -- diverges significantly when CO or RO occurs. After a thorough
analysis of these energy dynamics and their relationship with overfitting, we
propose a novel regularizer, the Delta Energy Regularizer (DER), designed to
smoothen the energy landscape during training. We demonstrate that DER is
effective in mitigating both CO and RO across multiple benchmarks. We further
show that robust classifiers, when being used as generative models, have limits
in handling trade-off between image quality and variability. We propose an
improved technique based on a local class-wise principal component analysis
(PCA) and energy-based guidance for better class-specific initialization and
adaptive stopping, enhancing sample diversity and generation quality.
Considering that we do not explicitly train for generative modeling, we achieve
a competitive Inception Score (IS) and Fr\'echet inception distance (FID)
compared to hybrid discriminative-generative models.

</details>


### [473] [A Provable Approach for End-to-End Safe Reinforcement Learning](https://arxiv.org/abs/2505.21852)
*Akifumi Wachi,Kohei Miyaguchi,Takumi Tanabe,Rei Sato,Youhei Akimoto*

Main category: cs.LG

TL;DR: PLS方法通过离线安全RL与安全策略部署结合，实现从学习到操作全过程的策略安全。


<details>
  <summary>Details</summary>
Motivation: 解决现有安全RL方法无法确保策略从学习到操作全过程安全的问题。

Method: 离线学习策略（基于回报条件的监督学习），部署时通过高斯过程优化目标回报参数。

Result: 理论证明PLS能高效找到接近最优的目标回报并高概率保证安全；实验显示PLS在安全和奖励性能上优于基线。

Conclusion: PLS实现了从学习到操作全过程的策略安全与高奖励性能。

Abstract: A longstanding goal in safe reinforcement learning (RL) is a method to ensure
the safety of a policy throughout the entire process, from learning to
operation. However, existing safe RL paradigms inherently struggle to achieve
this objective. We propose a method, called Provably Lifetime Safe RL (PLS),
that integrates offline safe RL with safe policy deployment to address this
challenge. Our proposed method learns a policy offline using return-conditioned
supervised learning and then deploys the resulting policy while cautiously
optimizing a limited set of parameters, known as target returns, using Gaussian
processes (GPs). Theoretically, we justify the use of GPs by analyzing the
mathematical relationship between target and actual returns. We then prove that
PLS finds near-optimal target returns while guaranteeing safety with high
probability. Empirically, we demonstrate that PLS outperforms baselines both in
safety and reward performance, thereby achieving the longstanding goal to
obtain high rewards while ensuring the safety of a policy throughout the
lifetime from learning to operation.

</details>


### [474] [Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation](https://arxiv.org/abs/2505.21525)
*Peiliang Gong,Yucheng Wang,Min Wu,Zhenghua Chen,Xiaoli Li,Daoqiang Zhang*

Main category: cs.LG

TL;DR: TERSE是一种针对多变量时间序列（MTS）的无源域自适应（SFDA）方法，通过时空特征编码和任务设计，解决了现有方法忽略空间相关性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有SFDA方法在多变量时间序列上表现不佳，主要因为它们未考虑数据的空间相关性，而TERSE旨在解决这一问题。

Method: TERSE结合了时空特征编码器、时间恢复和空间重连任务，以捕获时空特性并保持跨域一致性。

Result: 在三个真实时间序列数据集上的实验证明了TERSE的有效性和通用性。

Conclusion: TERSE是首个同时考虑时空一致性的MTS-SFDA方法，可作为插件模块集成到现有SFDA方法中。

Abstract: Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from
an annotated source domain to an unlabelled target domain without accessing the
source data, thereby preserving data privacy. While existing SFDA methods have
proven effective in reducing reliance on source data, they struggle to perform
well on multivariate time series (MTS) due to their failure to consider the
intrinsic spatial correlations inherent in MTS data. These spatial correlations
are crucial for accurately representing MTS data and preserving invariant
information across domains. To address this challenge, we propose Temporal
Restoration and Spatial Rewiring (TERSE), a novel and concise SFDA method
tailored for MTS data. Specifically, TERSE comprises a customized
spatial-temporal feature encoder designed to capture the underlying
spatial-temporal characteristics, coupled with both temporal restoration and
spatial rewiring tasks to reinstate latent representations of the temporally
masked time series and the spatially masked correlated structures. During the
target adaptation phase, the target encoder is guided to produce spatially and
temporally consistent features with the source domain by leveraging the source
pre-trained temporal restoration and spatial rewiring networks. Therefore,
TERSE can effectively model and transfer spatial-temporal dependencies across
domains, facilitating implicit feature alignment. In addition, as the first
approach to simultaneously consider spatial-temporal consistency in MTS-SFDA,
TERSE can also be integrated as a versatile plug-and-play module into
established SFDA methods. Extensive experiments on three real-world time series
datasets demonstrate the effectiveness and versatility of our approach.

</details>


### [475] [Taming Transformer Without Using Learning Rate Warmup](https://arxiv.org/abs/2505.21910)
*Xianbiao Qi,Yelin He,Jiaquan Ye,Chun-Guang Li,Bojia Zi,Xili Dai,Qin Zou,Rong Xiao*

Main category: cs.LG

TL;DR: 论文分析了Transformer训练中的崩溃现象，提出了一种新的优化策略以避免熵崩溃，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer在大规模训练中崩溃现象的原因，并提出解决方案。

Method: 通过理论分析揭示崩溃现象（谱能量集中），提出基于Weyl不等式的优化策略，动态调整学习率。

Result: 实验证明该策略能稳定训练ViT、Swin-Transformer和GPT，无需学习率预热。

Conclusion: 提出的优化策略有效解决了Transformer训练中的崩溃问题，提升了稳定性。

Abstract: Scaling Transformer to a large scale without using some technical tricks such
as learning rate warump and using an obviously lower learning rate is an
extremely challenging task, and is increasingly gaining more attention. In this
paper, we provide a theoretical analysis for the process of training
Transformer and reveal the rationale behind the model crash phenomenon in the
training process, termed \textit{spectral energy concentration} of
${\bW_q}^{\top} \bW_k$, which is the reason for a malignant entropy collapse,
where ${\bW_q}$ and $\bW_k$ are the projection matrices for the query and the
key in Transformer, respectively. To remedy this problem, motivated by
\textit{Weyl's Inequality}, we present a novel optimization strategy, \ie,
making the weight updating in successive steps smooth -- if the ratio
$\frac{\sigma_{1}(\nabla \bW_t)}{\sigma_{1}(\bW_{t-1})}$ is larger than a
threshold, we will automatically bound the learning rate to a weighted multiple
of $\frac{\sigma_{1}(\bW_{t-1})}{\sigma_{1}(\nabla \bW_t)}$, where $\nabla
\bW_t$ is the updating quantity in step $t$. Such an optimization strategy can
prevent spectral energy concentration to only a few directions, and thus can
avoid malignant entropy collapse which will trigger the model crash. We conduct
extensive experiments using ViT, Swin-Transformer and GPT, showing that our
optimization strategy can effectively and stably train these Transformers
without using learning rate warmup.

</details>


### [476] [From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization](https://arxiv.org/abs/2505.22310)
*Shoaib Ahmed Siddiqui,Adrian Weller,David Krueger,Gintare Karolina Dziugaite,Michael Curtis Mozer,Eleni Triantafillou*

Main category: cs.LG

TL;DR: 研究发现，LLM的遗忘方法易受重新学习攻击，即使微调少量无关样本，遗忘的知识也会重现。通过实验发现，仅用保留集微调即可使遗忘集准确率从50%恢复至近100%。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM遗忘方法的脆弱性，揭示遗忘知识易被重新学习的现象。

Method: 在视觉分类器中控制实验，测试多种遗忘方法，分析权重空间特性。

Result: 遗忘集准确率可通过保留集微调恢复至近100%，而从头训练的模型准确率保持50%。权重空间的L2距离和线性模式连接性可预测抗重新学习能力。

Conclusion: 提出新方法，显著提升抗重新学习攻击能力。

Abstract: Recent unlearning methods for LLMs are vulnerable to relearning attacks:
knowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of
(even seemingly-unrelated) examples. We study this phenomenon in a controlled
setting for example-level unlearning in vision classifiers. We make the
surprising discovery that forget-set accuracy can recover from around 50%
post-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e.,
zero examples of the forget set. We observe this effect across a wide variety
of unlearning methods, whereas for a model retrained from scratch excluding the
forget set (gold standard), the accuracy remains at 50%. We observe that
resistance to relearning attacks can be predicted by weight-space properties,
specifically, $L_2$-distance and linear mode connectivity between the original
and the unlearned model. Leveraging this insight, we propose a new class of
methods that achieve state-of-the-art resistance to relearning attacks.

</details>


### [477] [A Closer Look at Multimodal Representation Collapse](https://arxiv.org/abs/2505.22483)
*Abhra Chaudhuri,Anjan Dutta,Tu Bui,Serban Georgescu*

Main category: cs.LG

TL;DR: 论文研究了模态崩溃现象，提出了一种通过显式基重分配防止模态崩溃的算法，并在多模态基准上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究模态崩溃现象，即多模态融合模型倾向于依赖部分模态而忽略其他模态的原因。

Method: 通过分析噪声特征与预测特征的纠缠现象，提出利用跨模态知识蒸馏和显式基重分配算法防止模态崩溃。

Result: 实验验证了理论分析的正确性，算法在多模态基准上表现良好。

Conclusion: 跨模态知识蒸馏和显式基重分配能有效防止模态崩溃，适用于处理缺失模态问题。

Abstract: We aim to develop a fundamental understanding of modality collapse, a
recently observed empirical phenomenon wherein models trained for multimodal
fusion tend to rely only on a subset of the modalities, ignoring the rest. We
show that modality collapse happens when noisy features from one modality are
entangled, via a shared set of neurons in the fusion head, with predictive
features from another, effectively masking out positive contributions from the
predictive features of the former modality and leading to its collapse. We
further prove that cross-modal knowledge distillation implicitly disentangles
such representations by freeing up rank bottlenecks in the student encoder,
denoising the fusion-head outputs without negatively impacting the predictive
features from either modality. Based on the above findings, we propose an
algorithm that prevents modality collapse through explicit basis reallocation,
with applications in dealing with missing modalities. Extensive experiments on
multiple multimodal benchmarks validate our theoretical claims. Project page:
https://abhrac.github.io/mmcollapse/.

</details>


### [478] [Understanding Adversarial Training with Energy-based Models](https://arxiv.org/abs/2505.22486)
*Mujtaba Hussain Mirza,Maria Rosaria Briglia,Filippo Bartolucci,Senad Beadini,Giuseppe Lisanti,Iacopo Masi*

Main category: cs.LG

TL;DR: 论文通过能量模型（EBM）分析对抗训练（AT）中的灾难性过拟合（CO）和鲁棒过拟合（RO），并提出Delta Energy Regularizer（DER）缓解这些问题。同时探讨了鲁棒分类器的生成能力，并提出改进方法。


<details>
  <summary>Details</summary>
Motivation: 研究对抗训练中的过拟合现象及其能量动态，并探索鲁棒分类器的生成潜力。

Method: 通过能量视角分析对抗样本的能量差异，提出DER正则化方法，并结合局部PCA和能量引导改进生成能力。

Result: DER有效缓解CO和RO，改进的生成方法在图像质量和多样性上表现优异，达到与混合模型竞争的IS和FID。

Conclusion: 能量视角为对抗训练提供了新见解，DER和生成改进方法具有实际应用价值。

Abstract: We aim at using Energy-based Model (EBM) framework to better understand
adversarial training (AT) in classifiers, and additionally to analyze the
intrinsic generative capabilities of robust classifiers. By viewing standard
classifiers through an energy lens, we begin by analyzing how the energies of
adversarial examples, generated by various attacks, differ from those of the
natural samples. The central focus of our work is to understand the critical
phenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT
from an energy perspective. We analyze the impact of existing AT approaches on
the energy of samples during training and observe that the behavior of the
``delta energy' -- change in energy between original sample and its adversarial
counterpart -- diverges significantly when CO or RO occurs. After a thorough
analysis of these energy dynamics and their relationship with overfitting, we
propose a novel regularizer, the Delta Energy Regularizer (DER), designed to
smoothen the energy landscape during training. We demonstrate that DER is
effective in mitigating both CO and RO across multiple benchmarks. We further
show that robust classifiers, when being used as generative models, have limits
in handling trade-off between image quality and variability. We propose an
improved technique based on a local class-wise principal component analysis
(PCA) and energy-based guidance for better class-specific initialization and
adaptive stopping, enhancing sample diversity and generation quality.
Considering that we do not explicitly train for generative modeling, we achieve
a competitive Inception Score (IS) and Fr\'echet inception distance (FID)
compared to hybrid discriminative-generative models.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [479] [Efficient Precision-Scalable Hardware for Microscaling (MX) Processing in Robotics Learning](https://arxiv.org/abs/2505.22404)
*Stef Cuyckens,Xiaoling Yi,Nitish Satya Murthy,Chao Fang,Marian Verhelst*

Main category: cs.AR

TL;DR: 论文提出了一种支持所有六种MX数据类型的精度可扩展算术单元和方形共享指数组，解决了现有Dacapo处理器的局限性，实现了更高的训练效率和更低的资源消耗。


<details>
  <summary>Details</summary>
Motivation: 自主机器人需要高效的设备端学习以适应新环境，而现有Dacapo处理器在MX数据类型的支持和反向传播效率上存在不足。

Method: 1. 设计支持六种MX数据类型的精度可扩展算术单元；2. 引入方形共享指数组以优化反向传播中的权重处理。

Result: 在16nm工艺下，相比Dacapo，面积减少25.6%，内存占用降低51%，训练吞吐量提升4倍，同时保持能效。

Conclusion: 该设计为边缘设备上的机器人持续学习提供了高效解决方案。

Abstract: Autonomous robots require efficient on-device learning to adapt to new
environments without cloud dependency. For this edge training, Microscaling
(MX) data types offer a promising solution by combining integer and
floating-point representations with shared exponents, reducing energy
consumption while maintaining accuracy. However, the state-of-the-art
continuous learning processor, namely Dacapo, faces limitations with its
MXINT-only support and inefficient vector-based grouping during
backpropagation. In this paper, we present, to the best of our knowledge, the
first work that addresses these limitations with two key innovations: (1) a
precision-scalable arithmetic unit that supports all six MX data types by
exploiting sub-word parallelism and unified integer and floating-point
processing; and (2) support for square shared exponent groups to enable
efficient weight handling during backpropagation, removing storage redundancy
and quantization overhead. We evaluate our design against Dacapo under
iso-peak-throughput on four robotics workloads in TSMC 16nm FinFET technology
at 500MHz, reaching a 25.6% area reduction, a 51% lower memory footprint, and
4x higher effective training throughput while achieving comparable
energy-efficiency, enabling efficient robotics continual learning at the edge.

</details>


### [480] [Efficient Precision-Scalable Hardware for Microscaling (MX) Processing in Robotics Learning](https://arxiv.org/abs/2505.22404)
*Stef Cuyckens,Xiaoling Yi,Nitish Satya Murthy,Chao Fang,Marian Verhelst*

Main category: cs.AR

TL;DR: 论文提出了一种支持六种MX数据类型的精度可扩展算术单元和方形共享指数组，解决了现有Dacapo处理器的局限性，实现了更高的训练效率和更低的资源消耗。


<details>
  <summary>Details</summary>
Motivation: 自主机器人需要高效的设备端学习以适应新环境，而现有Dacapo处理器在MX数据类型的支持和反向传播效率上存在不足。

Method: 提出精度可扩展算术单元支持六种MX数据类型，并引入方形共享指数组优化反向传播。

Result: 在16nm工艺下，设计实现了25.6%的面积缩减、51%的内存占用降低和4倍的训练吞吐提升。

Conclusion: 该设计显著提升了边缘设备上的持续学习效率，适用于机器人应用。

Abstract: Autonomous robots require efficient on-device learning to adapt to new
environments without cloud dependency. For this edge training, Microscaling
(MX) data types offer a promising solution by combining integer and
floating-point representations with shared exponents, reducing energy
consumption while maintaining accuracy. However, the state-of-the-art
continuous learning processor, namely Dacapo, faces limitations with its
MXINT-only support and inefficient vector-based grouping during
backpropagation. In this paper, we present, to the best of our knowledge, the
first work that addresses these limitations with two key innovations: (1) a
precision-scalable arithmetic unit that supports all six MX data types by
exploiting sub-word parallelism and unified integer and floating-point
processing; and (2) support for square shared exponent groups to enable
efficient weight handling during backpropagation, removing storage redundancy
and quantization overhead. We evaluate our design against Dacapo under
iso-peak-throughput on four robotics workloads in TSMC 16nm FinFET technology
at 500MHz, reaching a 25.6% area reduction, a 51% lower memory footprint, and
4x higher effective training throughput while achieving comparable
energy-efficiency, enabling efficient robotics continual learning at the edge.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [481] [Physics-inspired Generative AI models via real hardware-based noisy quantum diffusion](https://arxiv.org/abs/2505.22193)
*Marco Parigi,Stefano Martina,Francesco Aldo Venturelli,Filippo Caruso*

Main category: quant-ph

TL;DR: 量子扩散模型（QDMs）利用量子特性提升生成AI性能，提出两种物理启发协议：量子随机游走和利用IBM量子硬件噪声生成图像，展示了量子噪声作为资源的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索量子特性如何提升经典生成AI模型的性能，并解决现有量子算法在近量子设备上的可扩展性问题。

Method: 提出两种协议：1）量子随机游走，结合量子与经典动力学；2）利用IBM量子硬件的固有噪声生成图像。

Result: 量子随机游走生成的MNIST图像FID更低；四量子比特硬件噪声成功用于图像生成。

Conclusion: 量子噪声可被利用为资源，为大规模量子生成AI算法开辟新方向。

Abstract: Quantum Diffusion Models (QDMs) are an emerging paradigm in Generative AI
that aims to use quantum properties to improve the performances of their
classical counterparts. However, existing algorithms are not easily scalable
due to the limitations of near-term quantum devices. Following our previous
work on QDMs, here we propose and implement two physics-inspired protocols. In
the first, we use the formalism of quantum stochastic walks, showing that a
specific interplay of quantum and classical dynamics in the forward process
produces statistically more robust models generating sets of MNIST images with
lower Fr\'echet Inception Distance (FID) than using totally classical dynamics.
In the second approach, we realize an algorithm to generate images by
exploiting the intrinsic noise of real IBM quantum hardware with only four
qubits. Our work could be a starting point to pave the way for new scenarios
for large-scale algorithms in quantum Generative AI, where quantum noise is
neither mitigated nor corrected, but instead exploited as a useful resource.

</details>


### [482] [Physics-inspired Generative AI models via real hardware-based noisy quantum diffusion](https://arxiv.org/abs/2505.22193)
*Marco Parigi,Stefano Martina,Francesco Aldo Venturelli,Filippo Caruso*

Main category: quant-ph

TL;DR: 量子扩散模型（QDMs）利用量子特性提升生成式AI性能，提出两种物理启发的协议：量子随机行走和利用量子硬件噪声生成图像，展示了量子噪声作为资源的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有量子扩散模型算法受限于近量子设备的可扩展性，探索量子特性以提升生成式AI性能。

Method: 提出两种协议：1）量子随机行走，结合量子与经典动力学；2）利用IBM量子硬件的固有噪声生成图像。

Result: 量子随机行走生成MNIST图像的FID更低；四量子比特硬件成功生成图像。

Conclusion: 量子噪声可作为资源，为大规模量子生成式AI算法开辟新方向。

Abstract: Quantum Diffusion Models (QDMs) are an emerging paradigm in Generative AI
that aims to use quantum properties to improve the performances of their
classical counterparts. However, existing algorithms are not easily scalable
due to the limitations of near-term quantum devices. Following our previous
work on QDMs, here we propose and implement two physics-inspired protocols. In
the first, we use the formalism of quantum stochastic walks, showing that a
specific interplay of quantum and classical dynamics in the forward process
produces statistically more robust models generating sets of MNIST images with
lower Fr\'echet Inception Distance (FID) than using totally classical dynamics.
In the second approach, we realize an algorithm to generate images by
exploiting the intrinsic noise of real IBM quantum hardware with only four
qubits. Our work could be a starting point to pave the way for new scenarios
for large-scale algorithms in quantum Generative AI, where quantum noise is
neither mitigated nor corrected, but instead exploited as a useful resource.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [483] [RESOUND: Speech Reconstruction from Silent Videos via Acoustic-Semantic Decomposed Modeling](https://arxiv.org/abs/2505.22024)
*Long-Khanh Pham,Thanh V. T. Tran,Minh-Tan Pham,Van Nguyen*

Main category: cs.SD

TL;DR: RESOUND是一种新型的唇语转语音（L2S）系统，通过分离声学路径和语义路径，结合语音单元和梅尔频谱图，生成清晰且富有表现力的语音。


<details>
  <summary>Details</summary>
Motivation: 传统L2S合成在准确性和自然度上存在挑战，主要由于对语言内容、口音和韵律的监督有限。

Method: RESOUND采用源滤波器理论，分为声学路径（预测韵律）和语义路径（提取语言特征），并结合语音单元和梅尔频谱图生成波形。

Result: 在两个标准L2S基准测试中，RESOUND在多项指标上表现出色。

Conclusion: RESOUND通过独立优化声学和语义表示，显著提升了L2S合成的性能。

Abstract: Lip-to-speech (L2S) synthesis, which reconstructs speech from visual cues,
faces challenges in accuracy and naturalness due to limited supervision in
capturing linguistic content, accents, and prosody. In this paper, we propose
RESOUND, a novel L2S system that generates intelligible and expressive speech
from silent talking face videos. Leveraging source-filter theory, our method
involves two components: an acoustic path to predict prosody and a semantic
path to extract linguistic features. This separation simplifies learning,
allowing independent optimization of each representation. Additionally, we
enhance performance by integrating speech units, a proven unsupervised speech
representation technique, into waveform generation alongside mel-spectrograms.
This allows RESOUND to synthesize prosodic speech while preserving content and
speaker identity. Experiments conducted on two standard L2S benchmarks confirm
the effectiveness of the proposed method across various metrics.

</details>


### [484] [RESOUND: Speech Reconstruction from Silent Videos via Acoustic-Semantic Decomposed Modeling](https://arxiv.org/abs/2505.22024)
*Long-Khanh Pham,Thanh V. T. Tran,Minh-Tan Pham,Van Nguyen*

Main category: cs.SD

TL;DR: RESOUND是一种新型的唇语转语音（L2S）系统，通过分离声学路径和语义路径，结合语音单元和梅尔频谱图，生成清晰且富有表现力的语音。


<details>
  <summary>Details</summary>
Motivation: 传统L2S合成在准确性和自然度上存在挑战，主要由于对语言内容、口音和韵律的监督有限。

Method: RESOUND采用源-滤波器理论，分为声学路径（预测韵律）和语义路径（提取语言特征），并结合语音单元和梅尔频谱图生成波形。

Result: 在两个标准L2S基准测试中，RESOUND在多项指标上表现出色。

Conclusion: RESOUND通过分离和优化不同表示，显著提升了L2S合成的效果。

Abstract: Lip-to-speech (L2S) synthesis, which reconstructs speech from visual cues,
faces challenges in accuracy and naturalness due to limited supervision in
capturing linguistic content, accents, and prosody. In this paper, we propose
RESOUND, a novel L2S system that generates intelligible and expressive speech
from silent talking face videos. Leveraging source-filter theory, our method
involves two components: an acoustic path to predict prosody and a semantic
path to extract linguistic features. This separation simplifies learning,
allowing independent optimization of each representation. Additionally, we
enhance performance by integrating speech units, a proven unsupervised speech
representation technique, into waveform generation alongside mel-spectrograms.
This allows RESOUND to synthesize prosodic speech while preserving content and
speaker identity. Experiments conducted on two standard L2S benchmarks confirm
the effectiveness of the proposed method across various metrics.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [485] [VideoMarkBench: Benchmarking Robustness of Video Watermarking](https://arxiv.org/abs/2505.21620)
*Zhengyuan Jiang,Moyang Guo,Kecen Li,Yuepeng Hu,Yupu Wang,Zhicong Huang,Cheng Hong,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: 论文介绍了VideoMarkBench，首个系统性评估视频水印在去除和伪造攻击下鲁棒性的基准测试，揭示了当前方法的显著脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着视频生成模型的快速发展，高度逼真的合成视频引发了与虚假信息和版权侵权相关的伦理问题，视频水印作为一种缓解策略被提出，但其鲁棒性尚未充分研究。

Method: 研究提出了VideoMarkBench基准测试，使用三种先进视频生成模型生成统一数据集，涵盖四种水印方法和七种检测聚合策略，评估12种扰动类型。

Result: 研究发现当前水印方法在多种攻击下存在显著脆弱性。

Conclusion: 研究强调了开发更鲁棒水印解决方案的紧迫性。

Abstract: The rapid development of video generative models has led to a surge in highly
realistic synthetic videos, raising ethical concerns related to disinformation
and copyright infringement. Recently, video watermarking has been proposed as a
mitigation strategy by embedding invisible marks into AI-generated videos to
enable subsequent detection. However, the robustness of existing video
watermarking methods against both common and adversarial perturbations remains
underexplored. In this work, we introduce VideoMarkBench, the first systematic
benchmark designed to evaluate the robustness of video watermarks under
watermark removal and watermark forgery attacks. Our study encompasses a
unified dataset generated by three state-of-the-art video generative models,
across three video styles, incorporating four watermarking methods and seven
aggregation strategies used during detection. We comprehensively evaluate 12
types of perturbations under white-box, black-box, and no-box threat models.
Our findings reveal significant vulnerabilities in current watermarking
approaches and highlight the urgent need for more robust solutions. Our code is
available at https://github.com/zhengyuan-jiang/VideoMarkBench.

</details>


### [486] [VideoMarkBench: Benchmarking Robustness of Video Watermarking](https://arxiv.org/abs/2505.21620)
*Zhengyuan Jiang,Moyang Guo,Kecen Li,Yuepeng Hu,Yupu Wang,Zhicong Huang,Cheng Hong,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: 本文介绍了VideoMarkBench，首个系统性评估视频水印在去除和伪造攻击下鲁棒性的基准测试，揭示了当前水印方法的显著脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着视频生成模型的快速发展，合成视频的真实性引发伦理问题，如水印的鲁棒性研究不足。

Method: 通过统一数据集，结合三种视频生成模型、四种水印方法和七种检测策略，评估12类扰动下的水印鲁棒性。

Result: 研究发现当前水印方法存在显著脆弱性，亟需更鲁棒的解决方案。

Conclusion: VideoMarkBench为视频水印鲁棒性评估提供了系统性工具，揭示了现有方法的不足。

Abstract: The rapid development of video generative models has led to a surge in highly
realistic synthetic videos, raising ethical concerns related to disinformation
and copyright infringement. Recently, video watermarking has been proposed as a
mitigation strategy by embedding invisible marks into AI-generated videos to
enable subsequent detection. However, the robustness of existing video
watermarking methods against both common and adversarial perturbations remains
underexplored. In this work, we introduce VideoMarkBench, the first systematic
benchmark designed to evaluate the robustness of video watermarks under
watermark removal and watermark forgery attacks. Our study encompasses a
unified dataset generated by three state-of-the-art video generative models,
across three video styles, incorporating four watermarking methods and seven
aggregation strategies used during detection. We comprehensively evaluate 12
types of perturbations under white-box, black-box, and no-box threat models.
Our findings reveal significant vulnerabilities in current watermarking
approaches and highlight the urgent need for more robust solutions. Our code is
available at https://github.com/zhengyuan-jiang/VideoMarkBench.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [487] [COSMOS: A Data-Driven Probabilistic Time Series simulator for Chemical Plumes across Spatial Scales](https://arxiv.org/abs/2505.22436)
*Arunava Nag,Floris van Breugel*

Main category: stat.AP

TL;DR: COSMOS是一种数据驱动的概率框架，用于生成逼真的气味时间序列，解决了传统CFD方法计算量大和基于气团的模拟无法重现自然气味统计的问题。


<details>
  <summary>Details</summary>
Motivation: 为自动化环境监测应用开发鲁棒的气味导航策略，需要在大空间尺度上模拟真实的气味时间序列。

Method: 提出COSMOS框架，通过从真实数据中提取时空特征，合成具有自然统计特性的气味时间序列。

Result: COSMOS生成的统计特征（如气味频率、持续时间和浓度）与真实数据相似，同时显著降低了计算开销。

Conclusion: COSMOS能够支持基于代理的导航策略开发，并在多种流动条件下重现关键统计特性。

Abstract: The development of robust odor navigation strategies for automated
environmental monitoring applications requires realistic simulations of odor
time series for agents moving across large spatial scales. Traditional
approaches that rely on computational fluid dynamics (CFD) methods can capture
the spatiotemporal dynamics of odor plumes, but are impractical for large-scale
simulations due to their computational expense. On the other hand, puff-based
simulations, although computationally tractable for large scales and capable of
capturing the stochastic nature of plumes, fail to reproduce naturalistic odor
statistics. Here, we present COSMOS (Configurable Odor Simulation Model over
Scalable Spaces), a data-driven probabilistic framework that synthesizes
realistic odor time series from spatial and temporal features of real datasets.
COSMOS generates similar distributions of key statistical features such as
whiff frequency, duration, and concentration as observed in real data, while
dramatically reducing computational overhead. By reproducing critical
statistical properties across a variety of flow regimes and scales, COSMOS
enables the development and evaluation of agent-based navigation strategies
with naturalistic odor experiences. To demonstrate its utility, we compare
odor-tracking agents exposed to CFD-generated plumes versus COSMOS simulations,
showing that both their odor experiences and resulting behaviors are quite
similar.

</details>


### [488] [COSMOS: A Data-Driven Probabilistic Time Series simulator for Chemical Plumes across Spatial Scales](https://arxiv.org/abs/2505.22436)
*Arunava Nag,Floris van Breugel*

Main category: stat.AP

TL;DR: COSMOS是一种数据驱动的概率框架，用于生成逼真的气味时间序列，解决了传统CFD方法计算成本高和基于气团的模拟无法复现自然气味统计的问题。


<details>
  <summary>Details</summary>
Motivation: 为自动化环境监测应用开发鲁棒的气味导航策略，需要在大空间尺度上模拟逼真的气味时间序列。传统CFD方法计算成本高，而基于气团的模拟无法复现自然气味统计。

Method: 提出COSMOS框架，利用真实数据集的时空特征合成逼真的气味时间序列，显著降低计算开销，同时复现关键统计特征。

Result: COSMOS生成的统计特征（如气味频率、持续时间和浓度）与真实数据相似，且计算效率高。CFD与COSMOS模拟的导航策略表现相似。

Conclusion: COSMOS为基于代理的导航策略开发提供了逼真的气味模拟，适用于多种流动状态和尺度。

Abstract: The development of robust odor navigation strategies for automated
environmental monitoring applications requires realistic simulations of odor
time series for agents moving across large spatial scales. Traditional
approaches that rely on computational fluid dynamics (CFD) methods can capture
the spatiotemporal dynamics of odor plumes, but are impractical for large-scale
simulations due to their computational expense. On the other hand, puff-based
simulations, although computationally tractable for large scales and capable of
capturing the stochastic nature of plumes, fail to reproduce naturalistic odor
statistics. Here, we present COSMOS (Configurable Odor Simulation Model over
Scalable Spaces), a data-driven probabilistic framework that synthesizes
realistic odor time series from spatial and temporal features of real datasets.
COSMOS generates similar distributions of key statistical features such as
whiff frequency, duration, and concentration as observed in real data, while
dramatically reducing computational overhead. By reproducing critical
statistical properties across a variety of flow regimes and scales, COSMOS
enables the development and evaluation of agent-based navigation strategies
with naturalistic odor experiences. To demonstrate its utility, we compare
odor-tracking agents exposed to CFD-generated plumes versus COSMOS simulations,
showing that both their odor experiences and resulting behaviors are quite
similar.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [489] [tenSVD algorithm for compression](https://arxiv.org/abs/2505.21686)
*Michele Gallo*

Main category: stat.CO

TL;DR: 论文提出了一种基于张量的高效图像存储方法，通过Tucker模型压缩数据，以减少存储、传输和处理能耗。


<details>
  <summary>Details</summary>
Motivation: 高维数据管理需求增加，张量分析在多个领域应用广泛，但现有方法在存储、传输和能耗方面效率不足。

Method: 将原始数据组织为高阶张量，应用Tucker模型进行压缩，并在R中实现，与基线算法对比。

Result: 通过模拟和真实数据集评估，结果显示算法在计算时间和信息保留质量上表现优异，同时显著降低能耗。

Conclusion: 该方法在高效性和可持续性方面具有优势，适用于需要低能耗和高性能的场景。

Abstract: Tensors provide a robust framework for managing high-dimensional data.
Consequently, tensor analysis has emerged as an active research area in various
domains, including machine learning, signal processing, computer vision, graph
analysis, and data mining. This study introduces an efficient image storage
approach utilizing tensors, aiming to minimize memory to store, bandwidth to
transmit and energy to processing. The proposed method organizes original data
into a higher-order tensor and applies the Tucker model for compression.
Implemented in R, this method is compared to a baseline algorithm. The
evaluation focuses on efficient of algorithm measured in term of computational
time and the quality of information preserved, using both simulated and real
datasets. A detailed analysis of the results is conducted, employing
established quantitative metrics, with significant attention paid to
sustainability in terms of energy consumption across algorithms.

</details>


### [490] [tenSVD algorithm for compression](https://arxiv.org/abs/2505.21686)
*Michele Gallo*

Main category: stat.CO

TL;DR: 提出了一种基于张量的高效图像存储方法，通过Tucker模型压缩数据，减少存储、传输和处理能耗。


<details>
  <summary>Details</summary>
Motivation: 高维数据管理需求增加，张量分析在多个领域应用广泛，但现有方法在存储和能耗方面效率不足。

Method: 将原始数据组织为高阶张量，应用Tucker模型压缩，并在R中实现，与基线算法对比。

Result: 通过模拟和真实数据集评估，关注计算时间和信息保留质量，能耗显著降低。

Conclusion: 该方法在存储和能耗方面表现优异，适用于可持续性要求高的场景。

Abstract: Tensors provide a robust framework for managing high-dimensional data.
Consequently, tensor analysis has emerged as an active research area in various
domains, including machine learning, signal processing, computer vision, graph
analysis, and data mining. This study introduces an efficient image storage
approach utilizing tensors, aiming to minimize memory to store, bandwidth to
transmit and energy to processing. The proposed method organizes original data
into a higher-order tensor and applies the Tucker model for compression.
Implemented in R, this method is compared to a baseline algorithm. The
evaluation focuses on efficient of algorithm measured in term of computational
time and the quality of information preserved, using both simulated and real
datasets. A detailed analysis of the results is conducted, employing
established quantitative metrics, with significant attention paid to
sustainability in terms of energy consumption across algorithms.

</details>
