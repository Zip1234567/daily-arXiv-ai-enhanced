<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 48]
- [eess.IV](#eess.IV) [Total: 10]
- [cs.GR](#cs.GR) [Total: 4]
- [physics.geo-ph](#physics.geo-ph) [Total: 3]
- [cs.RO](#cs.RO) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.CL](#cs.CL) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Computer Vision based Automated Quantification of Agricultural Sprayers Boom Displacement](https://arxiv.org/abs/2506.19939)
*Aryan Singh Dalal,Sidharth Rai,Rahul Singh,Treman Singh Kaloya,Rahul Harsha Cheppally,Ajay Sharda*

Main category: cs.CV

TL;DR: 开发了一种基于计算机视觉的系统，用于量化农业喷杆的运动，以提高喷施精度。


<details>
  <summary>Details</summary>
Motivation: 喷杆的不稳定性是农业喷施误差的主要因素之一，但目前缺乏定量数据来改进设计和控制系统。

Method: 使用YOLO V7、V8和V11神经网络模型实时跟踪喷杆边缘目标，并结合倾角传感器验证模型输出。

Result: 模型检测目标准确率超过90%，距离估计与传感器数据误差在0.026米内。

Conclusion: 该系统可量化喷杆运动，为设计改进提供数据支持，从而提高喷施精度。

Abstract: Application rate errors when using self-propelled agricultural sprayers for
agricultural production remain a concern. Among other factors, spray boom
instability is one of the major contributors to application errors. Spray
booms' width of 38m, combined with 30 kph driving speeds, varying terrain, and
machine dynamics when maneuvering complex field boundaries, make controls of
these booms very complex. However, there is no quantitative knowledge on the
extent of boom movement to systematically develop a solution that might include
boom designs and responsive boom control systems. Therefore, this study was
conducted to develop an automated computer vision system to quantify the boom
movement of various agricultural sprayers. A computer vision system was
developed to track a target on the edge of the sprayer boom in real time. YOLO
V7, V8, and V11 neural network models were trained to track the boom's
movements in field operations to quantify effective displacement in the
vertical and transverse directions. An inclinometer sensor was mounted on the
boom to capture boom angles and validate the neural network model output. The
results showed that the model could detect the target with more than 90 percent
accuracy, and distance estimates of the target on the boom were within 0.026 m
of the inclinometer sensor data. This system can quantify the boom movement on
the current sprayer and potentially on any other sprayer with minor
modifications. The data can be used to make design improvements to make sprayer
booms more stable and achieve greater application accuracy.

</details>


### [2] [EBC-ZIP: Improving Blockwise Crowd Counting with Zero-Inflated Poisson Regression](https://arxiv.org/abs/2506.19955)
*Yiming Ma,Victor Sanchez,Tanaya Guha*

Main category: cs.CV

TL;DR: 论文提出EBC-ZIP框架，通过零膨胀泊松回归改进人群计数中的密度图估计，解决数据稀疏性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视地面真实密度图的极端稀疏性，导致模型在稀疏区域表现不佳。

Method: 采用零膨胀泊松回归（ZIP）替代传统回归损失，结合增强块分类（EBC）框架。

Result: 在四个基准测试中，EBC-ZIP表现优于EBC并达到最先进水平。

Conclusion: EBC-ZIP通过概率损失更好地处理零密集分布，提升人群计数性能。

Abstract: Density map estimation has become the mainstream paradigm in crowd counting.
However, most existing methods overlook the extreme sparsity of ground-truth
density maps. In real-world crowd scenes, the vast majority of spatial regions
(often over 95%) contain no people, leading to heavily imbalanced count
distributions. Ignoring this imbalance can bias models toward overestimating
dense regions and underperforming in sparse areas. Furthermore, most loss
functions used in density estimation are majorly based on MSE and implicitly
assume Gaussian distributions, which are ill-suited for modeling discrete,
non-negative count data. In this paper, we propose EBC-ZIP, a crowd counting
framework that models the spatial distribution of counts using a Zero-Inflated
Poisson (ZIP) regression formulation. Our approach replaces the traditional
regression loss with the negative log-likelihood of the ZIP distribution,
enabling better handling of zero-heavy distributions while preserving count
accuracy. Built upon the recently proposed Enhanced Block Classification (EBC)
framework, EBC-ZIP inherits EBC's advantages in preserving the discreteness of
targets and ensuring training stability, while further improving performance
through a more principled probabilistic loss. We also evaluate EBC-ZIP with
backbones of varying computational complexity to assess its scalability.
Extensive experiments on four crowd counting benchmarks demonstrate that
EBC-ZIP consistently outperforms EBC and achieves state-of-the-art results.

</details>


### [3] [ToSA: Token Merging with Spatial Awareness](https://arxiv.org/abs/2506.20066)
*Hsiang-Wei Huang,Wenhao Chai,Kuang-Ming Chen,Cheng-Yen Yang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: ToSA是一种新颖的令牌合并方法，结合语义和空间信息，通过深度图像生成伪空间令牌，优化ViT的计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖视觉令牌的特征相似性，忽略了早期层中空间信息的潜力。

Method: 利用深度图像生成伪空间令牌，结合语义和空间信息指导令牌合并。

Result: 在多个视觉和具身问答基准上优于现有方法，显著减少ViT运行时间。

Conclusion: ToSA是一种高效的ViT加速解决方案，能更好地保留关键场景结构。

Abstract: Token merging has emerged as an effective strategy to accelerate Vision
Transformers (ViT) by reducing computational costs. However, existing methods
primarily rely on the visual token's feature similarity for token merging,
overlooking the potential of integrating spatial information, which can serve
as a reliable criterion for token merging in the early layers of ViT, where the
visual tokens only possess weak visual information. In this paper, we propose
ToSA, a novel token merging method that combines both semantic and spatial
awareness to guide the token merging process. ToSA leverages the depth image as
input to generate pseudo spatial tokens, which serve as auxiliary spatial
information for the visual token merging process. With the introduced spatial
awareness, ToSA achieves a more informed merging strategy that better preserves
critical scene structure. Experimental results demonstrate that ToSA
outperforms previous token merging methods across multiple benchmarks on visual
and embodied question answering while largely reducing the runtime of the ViT,
making it an efficient solution for ViT acceleration. The code will be
available at: https://github.com/hsiangwei0903/ToSA

</details>


### [4] [BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos](https://arxiv.org/abs/2506.20103)
*Jiahao Lin,Weixuan Peng,Bojia Zi,Yifeng Gao,Xianbiao Qi,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 论文介绍了BrokenVideos数据集，用于AI生成视频中的视觉伪影定位，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: AI生成视频中存在视觉伪影（如运动不一致、物理轨迹不合理等），影响真实性和用户信任，但目前缺乏专门用于伪影定位的基准数据集。

Method: 提出BrokenVideos数据集，包含3,254个AI生成视频，带有像素级标注的伪影区域，并通过人工验证确保标注质量。

Result: 实验表明，在BrokenVideos上训练的模型能显著提升伪影定位能力。

Conclusion: BrokenVideos为生成视频模型中的伪影定位研究提供了重要基准。

Abstract: Recent advances in deep generative models have led to significant progress in
video generation, yet the fidelity of AI-generated videos remains limited.
Synthesized content often exhibits visual artifacts such as temporally
inconsistent motion, physically implausible trajectories, unnatural object
deformations, and local blurring that undermine realism and user trust.
Accurate detection and spatial localization of these artifacts are crucial for
both automated quality control and for guiding the development of improved
generative models. However, the research community currently lacks a
comprehensive benchmark specifically designed for artifact localization in AI
generated videos. Existing datasets either restrict themselves to video or
frame level detection or lack the fine-grained spatial annotations necessary
for evaluating localization methods. To address this gap, we introduce
BrokenVideos, a benchmark dataset of 3,254 AI-generated videos with
meticulously annotated, pixel-level masks highlighting regions of visual
corruption. Each annotation is validated through detailed human inspection to
ensure high quality ground truth. Our experiments show that training state of
the art artifact detection models and multi modal large language models (MLLMs)
on BrokenVideos significantly improves their ability to localize corrupted
regions. Through extensive evaluation, we demonstrate that BrokenVideos
establishes a critical foundation for benchmarking and advancing research on
artifact localization in generative video models. The dataset is available at:
https://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.

</details>


### [5] [From 2D to 3D Cognition: A Brief Survey of General World Models](https://arxiv.org/abs/2506.20134)
*Ningwei Xie,Zizi Tian,Lei Yang,Xiao-Ping Zhang,Meng Guo,Jie Li*

Main category: cs.CV

TL;DR: 本文综述了从2D感知到3D认知的世界模型发展，重点分析了3D表示和世界知识整合的技术驱动，探讨了3D物理场景生成、空间推理和交互等核心能力，并指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 当前3D认知世界模型领域缺乏系统性分析，本文旨在填补这一空白，为技术发展提供结构化视角。

Method: 通过引入概念框架，对世界模型从2D到3D的演进进行结构化综述，并分析关键技术驱动和核心认知能力。

Result: 总结了3D世界模型在多个应用领域的部署情况，并识别了数据、建模和部署中的挑战。

Conclusion: 未来需进一步推动3D世界模型的鲁棒性和泛化能力，以应对实际应用中的复杂需求。

Abstract: World models have garnered increasing attention in the development of
artificial general intelligence (AGI), serving as computational frameworks for
learning representations of the external world and forecasting future states.
While early efforts focused on 2D visual perception and simulation, recent
3D-aware generative world models have demonstrated the ability to synthesize
geometrically consistent, interactive 3D environments, marking a shift toward
3D spatial cognition. Despite rapid progress, the field lacks systematic
analysis to categorize emerging techniques and clarify their roles in advancing
3D cognitive world models. This survey addresses this need by introducing a
conceptual framework, providing a structured and forward-looking review of
world models transitioning from 2D perception to 3D cognition. Within this
framework, we highlight two key technological drivers, particularly advances in
3D representations and the incorporation of world knowledge, as fundamental
pillars. Building on these, we dissect three core cognitive capabilities that
underpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning,
and 3D spatial interaction. We further examine the deployment of these
capabilities in real-world applications, including embodied AI, autonomous
driving, digital twin, and gaming/VR. Finally, we identify challenges across
data, modeling, and deployment, and outline future directions for advancing
more robust and generalizable 3D world models.

</details>


### [6] [EAR: Erasing Concepts from Unified Autoregressive Models](https://arxiv.org/abs/2506.20151)
*Haipeng Fan,Shiyuan Zhang,Baohunesitu,Zihang Guo,Huaiwen Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为EAR的微调方法，用于在自回归模型中有效且保留效用的概念擦除，并引入WGA和TLM策略以及新的基准ECGVF进行评估。


<details>
  <summary>Details</summary>
Motivation: 自回归模型在视觉理解和图像生成任务中表现优异，但如何在保持生成质量的同时去除不需要的概念仍是一个挑战。

Method: 提出EAR方法，包括WGA策略和TLM策略，并设计了ECGVF基准，通过LLMs生成提示对并进行视觉分类过滤。

Result: 在ECGVF基准上实验表明，EAR在概念擦除效果和模型效用保留方面均有显著提升。

Conclusion: EAR方法有效解决了自回归模型中的概念擦除问题，同时保持了模型的生成质量。

Abstract: Autoregressive (AR) models have achieved unified and strong performance
across both visual understanding and image generation tasks. However, removing
undesired concepts from AR models while maintaining overall generation quality
remains an open challenge. In this paper, we propose Erasure Autoregressive
Model (EAR), a fine-tuning method for effective and utility-preserving concept
erasure in AR models. Specifically, we introduce Windowed Gradient Accumulation
(WGA) strategy to align patch-level decoding with erasure objectives, and
Thresholded Loss Masking (TLM) strategy to protect content unrelated to the
target concept during fine-tuning. Furthermore, we propose a novel benchmark,
Erase Concept Generator and Visual Filter (ECGVF), aim at provide a more
rigorous and comprehensive foundation for evaluating concept erasure in AR
models. Specifically, we first employ structured templates across diverse large
language models (LLMs) to pre-generate a large-scale corpus of
target-replacement concept prompt pairs. Subsequently, we generate images from
these prompts and subject them to rigorous filtering via a visual classifier to
ensure concept fidelity and alignment. Extensive experimental results conducted
on the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR
achieves marked improvements in both erasure effectiveness and model utility
preservation. Code is available at: https://github.com/immc-lab/ear/

</details>


### [7] [Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep Neural Network Acceleration](https://arxiv.org/abs/2506.20152)
*Deepak Ghimire,Kilho Lee,Seong-heum Kim*

Main category: cs.CV

TL;DR: 本文提出了一种高效的损失感知自动结构化剪枝方法（LAASP），通过剪枝与训练结合的方式优化神经网络压缩，显著减少计算量并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 解决传统剪枝方法需要分阶段进行（训练、剪枝、微调）的问题，提出一种更高效的剪枝-训练一体化方法。

Method: 采用剪枝与训练同步进行的方式，自动选择剪枝标准和层，并通过网络损失指导剪枝，避免手动分配剪枝率。

Result: 在CIFAR-10和ImageNet数据集上，ResNet56和ResNet110模型的计算量减少52%，精度提升；ResNet50模型计算量减少42%，精度仅下降0.33%。

Conclusion: LAASP方法在减少计算量的同时保持高精度，适用于资源受限的边缘设备部署。

Abstract: Structured pruning is a well-established technique for compressing neural
networks, making it suitable for deployment in resource-limited edge devices.
This paper presents an efficient Loss-Aware Automatic Selection of Structured
Pruning Criteria (LAASP) for slimming and accelerating deep neural networks.
The majority of pruning methodologies employ a sequential process consisting of
three stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed
pruning technique adopts a pruning-while-training approach that eliminates the
first stage and integrates the second and third stages into a single cycle. The
automatic selection of magnitude or similarity-based filter pruning criteria
from a specified pool of criteria and the specific pruning layer at each
pruning iteration is guided by the network's overall loss on a small subset of
the training data. To mitigate the abrupt accuracy drop due to pruning, the
network is retrained briefly after each reduction of a predefined number of
floating-point operations (FLOPs). The optimal pruning rates for each layer in
the network are automatically determined, eliminating the need for manual
allocation of fixed or variable pruning rates for each layer. Experiments on
the VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets
demonstrate the effectiveness of the proposed method. In particular, the
ResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the
top-1 accuracy compared to state-of-the-art methods while reducing the network
FLOPs by 52\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces
FLOPs by more than 42\% with a negligible 0.33\% drop in top-5 accuracy. The
source code of this paper is publicly available online -
https://github.com/ghimiredhikura/laasp.

</details>


### [8] [Towards Efficient Exemplar Based Image Editing with Multimodal VLMs](https://arxiv.org/abs/2506.20155)
*Avadhoot Jadhav,Ashutosh Srivastava,Abhinav Java,Silky Singh,Tarun Ram Menta,Surgan Jandial,Balaji Krishnamurthy*

Main category: cs.CV

TL;DR: 本文提出了一种基于示例对的图像编辑方法，利用预训练的文本到图像扩散模型和多模态VLM，无需优化即可高效完成编辑任务。


<details>
  <summary>Details</summary>
Motivation: 文本描述难以准确表达某些图像编辑需求，而示例对能更直观地展示编辑意图。

Method: 利用预训练的文本到图像扩散模型和多模态VLM，构建端到端且无需优化的编辑流程。

Result: 实验表明，该方法在多种编辑任务上优于基线方法，且速度提升约4倍。

Conclusion: 该方法为基于示例的图像编辑提供了高效且性能优越的解决方案。

Abstract: Text-to-Image Diffusion models have enabled a wide array of image editing
applications. However, capturing all types of edits through text alone can be
challenging and cumbersome. The ambiguous nature of certain image edits is
better expressed through an exemplar pair, i.e., a pair of images depicting an
image before and after an edit respectively. In this work, we tackle
exemplar-based image editing -- the task of transferring an edit from an
exemplar pair to a content image(s), by leveraging pretrained text-to-image
diffusion models and multimodal VLMs. Even though our end-to-end pipeline is
optimization-free, our experiments demonstrate that it still outperforms
baselines on multiple types of edits while being ~4x faster.

</details>


### [9] [Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2506.20168)
*Zhentao He,Can Zhang,Ziheng Wu,Zhenghao Chen,Yufei Zhan,Yifan Li,Zhao Zhang,Xian Wang,Minghui Qiu*

Main category: cs.CV

TL;DR: 论文提出KIE-HVQA基准和GRPO框架，解决多模态模型在视觉退化场景下的幻觉问题，实验显示效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型在视觉退化场景中表现不佳，容易产生幻觉内容，需改进视觉-文本推理能力。

Method: 提出KIE-HVQA基准评估OCR幻觉，并设计GRPO框架，结合视觉不确定性和拒绝回答机制。

Result: 7B参数模型在KIE-HVQA上比GPT-4o提升22%的无幻觉准确率，且标准任务性能无显著下降。

Conclusion: GRPO框架有效减少幻觉，提升模型在视觉退化场景下的鲁棒性。

Abstract: Recent advancements in multimodal large language models have enhanced
document understanding by integrating textual and visual information. However,
existing models exhibit incompleteness within their paradigm in real-world
scenarios, particularly under visual degradation. In such conditions, the
current response paradigm often fails to adequately perceive visual degradation
and ambiguity, leading to overreliance on linguistic priors or misaligned
visual-textual reasoning. This difficulty in recognizing uncertainty frequently
results in the generation of hallucinatory content, especially when a precise
answer is not feasible. To better demonstrate and analyze this phenomenon and
problem, we propose KIE-HVQA, the first benchmark dedicated to evaluating OCR
hallucination in degraded document understanding. This dataset includes test
samples spanning identity cards and invoices, with simulated real-world
degradations for OCR reliability. This setup allows for evaluating models'
capacity, under degraded input, to distinguish reliable visual information and
answer accordingly, thereby highlighting the challenge of avoiding
hallucination on uncertain data. To achieve vision-faithful reasoning and
thereby avoid the aforementioned issues, we further introduce a GRPO-based
framework featuring a novel reward mechanism. By incorporating a self-awareness
of visual uncertainty and an analysis method that initiates refusal to answer
to increase task difficulty within our supervised fine-tuning and reinforcement
learning framework, we successfully mitigated hallucinations in ambiguous
regions. Experiments on Qwen2.5-VL demonstrate that our 7B-parameter model
achieves a 22\% absolute improvement in hallucination-free accuracy over GPT-4o
on KIE-HVQA and there is no significant performance drop in standard tasks,
highlighting both effectiveness and robustness.

</details>


### [10] [Towards Scalable and Generalizable Earth Observation Data Mining via Foundation Model Composition](https://arxiv.org/abs/2506.20174)
*Man Duc Chuc*

Main category: cs.CV

TL;DR: 研究探讨如何通过结合预训练的基础模型提升地球观测任务的性能，发现特征级集成小模型可媲美或超越大模型，且更高效。


<details>
  <summary>Details</summary>
Motivation: 探索预训练模型在地球观测数据挖掘中的复用与组合潜力，以提供更通用、可扩展的解决方案。

Method: 使用GEO-Bench基准测试，评估Prithvi、Hiera和DOFA等模型在11个数据集上的表现，采用特征级集成方法。

Result: 小模型的特征级集成性能可媲美或超越大模型，同时减少训练时间和计算资源。

Conclusion: 知识蒸馏可将集成模型的优势转移到更紧凑的模型中，为实际应用提供可行路径。

Abstract: Foundation models are rapidly transforming Earth Observation data mining by
enabling generalizable and scalable solutions for key tasks such as scene
classification and semantic segmentation. While most efforts in the geospatial
domain have focused on developing large models trained from scratch using
massive Earth Observation datasets, an alternative strategy that remains
underexplored is the reuse and combination of existing pretrained models. In
this study, we investigate whether foundation models pretrained on remote
sensing and general vision datasets can be effectively combined to improve
performance across a diverse set of key Earth Observation tasks. Using the
GEO-Bench benchmark, we evaluate several prominent models, including Prithvi,
Hiera, and DOFA, on eleven datasets covering a range of spatial resolutions,
sensor modalities, and task types. The results show that feature-level
ensembling of smaller pretrained models can match or exceed the performance of
much larger models, while requiring less training time and computational
resources. Moreover, the study highlights the potential of applying knowledge
distillation to transfer the strengths of ensembles into more compact models,
offering a practical path for deploying foundation models in real-world Earth
Observation applications.

</details>


### [11] [Progressive Alignment Degradation Learning for Pansharpening](https://arxiv.org/abs/2506.20179)
*Enzhe Zhao,Zhichang Guo,Yao Li,Fanghui Song,Boying Wu*

Main category: cs.CV

TL;DR: 论文提出了一种新的深度学习方法（PADM和HFreqdiff）来解决基于Wald协议的合成数据在泛化性上的不足，显著提升了高分辨率多光谱图像的质量。


<details>
  <summary>Details</summary>
Motivation: Wald协议生成的合成数据无法准确模拟真实世界的退化模式，限制了深度全色锐化模型的泛化能力。

Method: 提出了渐进对齐退化模块（PADM）和HFreqdiff方法，通过自适应学习退化过程和嵌入高频细节来提升性能。

Result: 实验表明，该方法在空间清晰度和图像质量上优于现有技术。

Conclusion: PADM和HFreqdiff有效解决了合成数据的局限性，显著提升了全色锐化效果。

Abstract: Deep learning-based pansharpening has been shown to effectively generate
high-resolution multispectral (HRMS) images. To create supervised ground-truth
HRMS images, synthetic data generated using the Wald protocol is commonly
employed. This protocol assumes that networks trained on artificial
low-resolution data will perform equally well on high-resolution data. However,
well-trained models typically exhibit a trade-off in performance between
reduced-resolution and full-resolution datasets. In this paper, we delve into
the Wald protocol and find that its inaccurate approximation of real-world
degradation patterns limits the generalization of deep pansharpening models. To
address this issue, we propose the Progressive Alignment Degradation Module
(PADM), which uses mutual iteration between two sub-networks, PAlignNet and
PDegradeNet, to adaptively learn accurate degradation processes without relying
on predefined operators. Building on this, we introduce HFreqdiff, which embeds
high-frequency details into a diffusion framework and incorporates CFB and BACM
modules for frequency-selective detail extraction and precise reverse process
learning. These innovations enable effective integration of high-resolution
panchromatic and multispectral images, significantly enhancing spatial
sharpness and quality. Experiments and ablation studies demonstrate the
proposed method's superior performance compared to state-of-the-art techniques.

</details>


### [12] [UniCode$^2$: Cascaded Large-scale Codebooks for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2506.20214)
*Yanzhe Chen,Huasong Zhong,Yan Li,Zhenheng Yang*

Main category: cs.CV

TL;DR: UniCode²提出了一种级联码本框架，用于大规模、语义对齐且稳定的视觉标记化，解决了现有方法词汇量小或训练不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉码本方法要么词汇量小（约16K条目），缺乏细粒度语义，要么盲目扩展导致标记利用率低和训练不稳定。

Method: 通过聚类数百万SigLIP序列嵌入，构建了一个50万条目的码本，采用级联设计：冻结码本锚定嵌入空间，可训练码本细化任务特定语义。

Result: UniCode²在多样化基准测试中表现优异，支持高质量视觉合成，且无需牺牲稳定性、语义或模块性。

Conclusion: UniCode²证明了在不牺牲稳定性、语义或模块性的情况下扩展视觉标记空间的可行性。

Abstract: Unified multimodal large language models (MLLMs) have shown promise in
jointly advancing multimodal understanding and generation, with visual
codebooks discretizing images into tokens for autoregressive modeling. Existing
codebook-based methods either rely on small vocabularies (~16K entries) that
lack fine-grained semantics or naively scale up, resulting in low token
utilization and unstable training. We propose UniCode$^2$, a cascaded codebook
framework enabling large-scale, semantically aligned, and stable visual
tokenization. By clustering millions of SigLIP sequence embeddings, we build a
500K-entry codebook that preserves vision-language alignment while expanding
capacity. Stability is ensured via a cascaded design: a frozen codebook anchors
the embedding space, and a trainable codebook refines task-specific semantics.
This decoupling promotes high utilization and robust learning. Moreover, the
alignment of our visual tokens with textual semantics enables seamless
integration with pretrained diffusion decoders, supporting high-quality visual
synthesis with minimal adaptation. UniCode^2 delivers strong performance across
diverse benchmarks, demonstrating the viability of scaling visual token spaces
without sacrificing stability, semantics, or modularity.

</details>


### [13] [Dynamic Bandwidth Allocation for Hybrid Event-RGB Transmission](https://arxiv.org/abs/2506.20222)
*Pujing Yang,Guangyi Zhang,Yunlong Cai,Lei Yu,Guanding Yu*

Main category: cs.CV

TL;DR: 提出了一种联合事件和图像（E-I）传输框架，通过贝叶斯建模和信息瓶颈方法消除冗余，优化带宽利用，同时实现实时去模糊。


<details>
  <summary>Details</summary>
Motivation: 混合系统中事件相机和RGB相机传输大量数据存在挑战，且两者输出存在冗余信息。

Method: 采用贝叶斯建模和信息瓶颈方法，分离共享和领域特定信息，动态分配传输带宽。

Result: 仿真结果表明，方案在重建质量和去模糊性能上优于传统系统。

Conclusion: 提出的框架有效优化了带宽利用，同时提升了重建和去模糊效果。

Abstract: Event cameras asynchronously capture pixel-level intensity changes with
extremely low latency. They are increasingly used in conjunction with RGB
cameras for a wide range of vision-related applications. However, a major
challenge in these hybrid systems lies in the transmission of the large volume
of triggered events and RGB images. To address this, we propose a transmission
scheme that retains efficient reconstruction performance of both sources while
accomplishing real-time deblurring in parallel. Conventional RGB cameras and
event cameras typically capture the same scene in different ways, often
resulting in significant redundant information across their outputs. To address
this, we develop a joint event and image (E-I) transmission framework to
eliminate redundancy and thereby optimize channel bandwidth utilization. Our
approach employs Bayesian modeling and the information bottleneck method to
disentangle the shared and domain-specific information within the E-I inputs.
This disentangled information bottleneck framework ensures both the compactness
and informativeness of extracted shared and domain-specific information.
Moreover, it adaptively allocates transmission bandwidth based on scene
dynamics, i.e., more symbols are allocated to events for dynamic details or to
images for static information. Simulation results demonstrate that the proposed
scheme not only achieves superior reconstruction quality compared to
conventional systems but also delivers enhanced deblurring performance.

</details>


### [14] [Recognizing Surgical Phases Anywhere: Few-Shot Test-time Adaptation and Task-graph Guided Refinement](https://arxiv.org/abs/2506.20254)
*Kun Yuan,Tingxuan Chen,Shi Li,Joel L. Lavanchy,Christian Heiliger,Ege Özsoy,Yiming Huang,Long Bai,Nassir Navab,Vinkle Srivastav,Hongliang Ren,Nicolas Padoy*

Main category: cs.CV

TL;DR: SPA是一个轻量级框架，通过少量标注和自然语言定义，实现跨机构和跨手术的通用手术工作流理解。


<details>
  <summary>Details</summary>
Motivation: 手术工作流的复杂性和多样性导致现有模型在未见手术环境中的泛化能力受限。

Method: SPA结合少量空间适应、扩散建模和动态测试时适应，提升模型在特定机构中的表现。

Result: SPA在少量标注数据下实现最先进的性能，甚至优于全标注模型。

Conclusion: SPA为医院提供了一种快速定制手术阶段识别模型的高效方法。

Abstract: The complexity and diversity of surgical workflows, driven by heterogeneous
operating room settings, institutional protocols, and anatomical variability,
present a significant challenge in developing generalizable models for
cross-institutional and cross-procedural surgical understanding. While recent
surgical foundation models pretrained on large-scale vision-language data offer
promising transferability, their zero-shot performance remains constrained by
domain shifts, limiting their utility in unseen surgical environments. To
address this, we introduce Surgical Phase Anywhere (SPA), a lightweight
framework for versatile surgical workflow understanding that adapts foundation
models to institutional settings with minimal annotation. SPA leverages
few-shot spatial adaptation to align multi-modal embeddings with
institution-specific surgical scenes and phases. It also ensures temporal
consistency through diffusion modeling, which encodes task-graph priors derived
from institutional procedure protocols. Finally, SPA employs dynamic test-time
adaptation, exploiting the mutual agreement between multi-modal phase
prediction streams to adapt the model to a given test video in a
self-supervised manner, enhancing the reliability under test-time distribution
shifts. SPA is a lightweight adaptation framework, allowing hospitals to
rapidly customize phase recognition models by defining phases in natural
language text, annotating a few images with the phase labels, and providing a
task graph defining phase transitions. The experimental results show that the
SPA framework achieves state-of-the-art performance in few-shot surgical phase
recognition across multiple institutions and procedures, even outperforming
full-shot models with 32-shot labeled data. Code is available at
https://github.com/CAMMA-public/SPA

</details>


### [15] [A Transformer Based Handwriting Recognition System Jointly Using Online and Offline Features](https://arxiv.org/abs/2506.20255)
*Ayush Lodh,Ritabrata Chakraborty,Shivakumara Palaiahnakote,Umapada Pal*

Main category: cs.CV

TL;DR: 提出了一种结合离线图像和在线笔画数据的端到端网络，通过早期融合在共享潜在空间中提升手写识别性能。


<details>
  <summary>Details</summary>
Motivation: 手写识别通常仅利用单一模态（图像或笔画数据），而忽略了二者的互补性。本文旨在通过融合两种模态提升识别准确率。

Method: 设计了一个端到端网络，包括补丁编码器（处理图像）和轻量级Transformer（处理笔画序列），通过可学习的潜在查询联合关注两种模态，生成增强的笔画嵌入。

Result: 在IAMOn-DB和VNOn-DB数据集上实现了最先进的准确率，比之前最佳方法提升1%。

Conclusion: 早期融合多模态数据能有效提升手写识别性能，且具有更强的书写独立性。

Abstract: We posit that handwriting recognition benefits from complementary cues
carried by the rasterized complex glyph and the pen's trajectory, yet most
systems exploit only one modality. We introduce an end-to-end network that
performs early fusion of offline images and online stroke data within a shared
latent space. A patch encoder converts the grayscale crop into fixed-length
visual tokens, while a lightweight transformer embeds the $(x, y, \text{pen})$
sequence. Learnable latent queries attend jointly to both token streams,
yielding context-enhanced stroke embeddings that are pooled and decoded under a
cross-entropy loss objective. Because integration occurs before any high-level
classification, temporal cues reinforce each other during representation
learning, producing stronger writer independence. Comprehensive experiments on
IAMOn-DB and VNOn-DB demonstrate that our approach achieves state-of-the-art
accuracy, exceeding previous bests by up to 1\%. Our study also shows
adaptation of this pipeline with gesturification on the ISI-Air dataset. Our
code can be found here.

</details>


### [16] [Breaking Spatial Boundaries: Spectral-Domain Registration Guided Hyperspectral and Multispectral Blind Fusion](https://arxiv.org/abs/2506.20293)
*Kunjing Yang,Libin Zheng,Minru Bai,Ting Lu,Leyuan Fang*

Main category: cs.CV

TL;DR: 提出了一种从光谱域解决未注册高光谱图像（HSI）和多光谱图像（MSI）融合问题的方法，通过轻量级光谱先验学习网络和盲稀疏融合方法，显著提升了注册和融合效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过空间变换对齐HSI和MSI，但由于分辨率差异大，效果不佳且耗时。因此，从光谱域解决注册问题成为新思路。

Method: 1. 开发轻量级SPL网络提取HSI光谱特征并增强MSI光谱分辨率；2. 通过子空间表示和循环训练策略提升注册HSI的光谱精度；3. 提出盲稀疏融合（BSF）方法，利用组稀疏正则化等效促进图像低秩性；4. 使用PAO算法求解BSF模型并分析收敛性。

Result: 在模拟和真实数据集上的实验验证了该方法在注册和融合中的有效性，并提升了分类性能。

Conclusion: 该方法从光谱域解决了HSI和MSI的注册问题，显著提升了融合效果和计算效率。

Abstract: The blind fusion of unregistered hyperspectral images (HSIs) and
multispectral images (MSIs) has attracted growing attention recently. To
address the registration challenge, most existing methods employ spatial
transformations on the HSI to achieve alignment with the MSI. However, due to
the substantial differences in spatial resolution of the images, the
performance of these methods is often unsatisfactory. Moreover, the
registration process tends to be time-consuming when dealing with large-sized
images in remote sensing. To address these issues, we propose tackling the
registration problem from the spectral domain. Initially, a lightweight
Spectral Prior Learning (SPL) network is developed to extract spectral features
from the HSI and enhance the spectral resolution of the MSI. Following this,
the obtained image undergoes spatial downsampling to produce the registered
HSI. In this process, subspace representation and cyclic training strategy are
employed to improve spectral accuracy of the registered HSI obtained. Next, we
propose a blind sparse fusion (BSF) method, which utilizes group sparsity
regularization to equivalently promote the low-rankness of the image. This
approach not only circumvents the need for rank estimation, but also reduces
computational complexity. Then, we employ the Proximal Alternating Optimization
(PAO) algorithm to solve the BSF model, and present its convergence analysis.
Finally, extensive numerical experiments on simulated and real datasets are
conducted to verify the effectiveness of our method in registration and fusion.
We also demonstrate its efficacy in enhancing classification performance.

</details>


### [17] [Hierarchical Mask-Enhanced Dual Reconstruction Network for Few-Shot Fine-Grained Image Classification](https://arxiv.org/abs/2506.20263)
*Ning Luo,Meiyin Hu,Huan Wan,Yanyan Yang,Zhuohang Jiang,Xin Wei*

Main category: cs.CV

TL;DR: HMDRN提出了一种结合双层特征重建和掩码增强的少样本细粒度图像分类方法，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在少样本细粒度图像分类中存在空间信息丢失、局部特征不对齐、缺乏分层特征利用和判别区域聚焦机制的问题。

Method: HMDRN通过双层特征重建与融合模块和掩码增强的自重建模块，结合高层语义和中层结构信息，并增强判别区域的关注。

Result: 在三个细粒度数据集上，HMDRN在Conv-4和ResNet-12架构上均优于现有方法，并通过消融实验验证了各模块的有效性。

Conclusion: HMDRN通过双层重建提升类间区分度，掩码增强减少类内差异，可视化结果证明了其优越的特征重建能力。

Abstract: Few-shot fine-grained image classification (FS-FGIC) presents a significant
challenge, requiring models to distinguish visually similar subclasses with
limited labeled examples. Existing methods have critical limitations:
metric-based methods lose spatial information and misalign local features,
while reconstruction-based methods fail to utilize hierarchical feature
information and lack mechanisms to focus on discriminative regions. We propose
the Hierarchical Mask-enhanced Dual Reconstruction Network (HMDRN), which
integrates dual-layer feature reconstruction with mask-enhanced feature
processing to improve fine-grained classification. HMDRN incorporates a
dual-layer feature reconstruction and fusion module that leverages
complementary visual information from different network hierarchies. Through
learnable fusion weights, the model balances high-level semantic
representations from the last layer with mid-level structural details from the
penultimate layer. Additionally, we design a spatial binary mask-enhanced
transformer self-reconstruction module that processes query features through
adaptive thresholding while maintaining complete support features, enhancing
focus on discriminative regions while filtering background noise. Extensive
experiments on three challenging fine-grained datasets demonstrate that HMDRN
consistently outperforms state-of-the-art methods across Conv-4 and ResNet-12
backbone architectures. Comprehensive ablation studies validate the
effectiveness of each proposed component, revealing that dual-layer
reconstruction enhances inter-class discrimination while mask-enhanced
transformation reduces intra-class variations. Visualization results provide
evidence of HMDRN's superior feature reconstruction capabilities.

</details>


### [18] [Forensic Study of Paintings Through the Comparison of Fabrics](https://arxiv.org/abs/2506.20272)
*Juan José Murillo-Fuentes,Pablo M. Olmos,Laura Alba-Carcelén*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的纺织品相似性评估新方法，用于艺术品中帆布织物的研究，无需依赖传统的线密度图匹配。


<details>
  <summary>Details</summary>
Motivation: 传统方法基于线密度图匹配，无法适用于帆布来自非连续位置的情况，因此需要一种新方法。

Method: 设计并训练了一个Siamese深度学习模型，通过扫描图像的特征表示进行比较，并提出了一种相似性估计方法。

Result: 在Museo Nacional del Prado的帆布上应用该方法，验证了其可行性和准确性。

Conclusion: 该方法为艺术品分析开辟了新途径，尤其适用于线密度相似的平纹帆布。

Abstract: The study of canvas fabrics in works of art is a crucial tool for
authentication, attribution and conservation. Traditional methods are based on
thread density map matching, which cannot be applied when canvases do not come
from contiguous positions on a roll. This paper presents a novel approach based
on deep learning to assess the similarity of textiles. We introduce an
automatic tool that evaluates the similarity between canvases without relying
on thread density maps. A Siamese deep learning model is designed and trained
to compare pairs of images by exploiting the feature representations learned
from the scans. In addition, a similarity estimation method is proposed,
aggregating predictions from multiple pairs of cloth samples to provide a
robust similarity score. Our approach is applied to canvases from the Museo
Nacional del Prado, corroborating the hypothesis that plain weave canvases,
widely used in painting, can be effectively compared even when their thread
densities are similar. The results demonstrate the feasibility and accuracy of
the proposed method, opening new avenues for the analysis of masterpieces.

</details>


### [19] [From Ideal to Real: Unified and Data-Efficient Dense Prediction for Real-World Scenarios](https://arxiv.org/abs/2506.20279)
*Changliang Xia,Chengyou Jia,Zhuohang Dang,Minnan Luo*

Main category: cs.CV

TL;DR: 论文提出了DenseDiT，一种利用生成模型视觉先验的统一策略，用于解决真实世界密集预测任务，并在新基准DenseWorld上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有密集预测方法在真实场景中泛化能力有限，且数据稀缺，需要一种更通用的解决方案。

Method: 提出DenseDiT，结合参数重用机制和两个轻量级分支，自适应整合多尺度上下文，仅需0.1%额外参数。

Result: 在DenseWorld基准上，DenseDiT表现优于现有方法，仅需0.01%的训练数据。

Conclusion: DenseDiT在真实世界密集预测任务中具有显著优势，适合实际部署。

Abstract: Dense prediction tasks hold significant importance of computer vision, aiming
to learn pixel-wise annotated label for an input image. Despite advances in
this field, existing methods primarily focus on idealized conditions, with
limited generalization to real-world scenarios and facing the challenging
scarcity of real-world data. To systematically study this problem, we first
introduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction
tasks that correspond to urgent real-world applications, featuring unified
evaluation across tasks. Then, we propose DenseDiT, which maximally exploits
generative models' visual priors to perform diverse real-world dense prediction
tasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism
and two lightweight branches that adaptively integrate multi-scale context,
working with less than 0.1% additional parameters. Evaluations on DenseWorld
reveal significant performance drops in existing general and specialized
baselines, highlighting their limited real-world generalization. In contrast,
DenseDiT achieves superior results using less than 0.01% training data of
baselines, underscoring its practical value for real-world deployment. Our
data, and checkpoints and codes are available at
https://xcltql666.github.io/DenseDiTProj

</details>


### [20] [Ctrl-Z Sampling: Diffusion Sampling with Controlled Random Zigzag Explorations](https://arxiv.org/abs/2506.20294)
*Shunqi Mao,Wei Guo,Chaoyi Zhang,Weidong Cai*

Main category: cs.CV

TL;DR: 提出了一种名为Ctrl-Z Sampling的新采样策略，用于在条件生成中检测和逃离局部最优解，提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在条件生成中容易陷入局部最优解，导致全局不一致或条件不对齐的问题，现有方法效果有限。

Method: 通过奖励模型识别局部最优解，注入噪声并回退到更早状态以逃离，动态交替进行前向优化和后向探索。

Result: 实验表明，Ctrl-Z Sampling显著提升了生成质量，仅增加约7.6倍函数评估。

Conclusion: Ctrl-Z Sampling是一种模型无关的方法，可有效提升扩散模型的生成质量。

Abstract: Diffusion models have shown strong performance in conditional generation by
progressively denoising Gaussian noise toward a target data distribution. This
denoising process can be interpreted as a form of hill climbing in a learned
latent space, where the model iteratively refines the sample toward regions of
higher probability. However, diffusion models often converge to local optima
that are locally visually coherent yet globally inconsistent or conditionally
misaligned, due to latent space complexity and suboptimal initialization. Prior
efforts attempted to address this by strengthening guidance signals or
manipulating the initial noise distribution. We introduce Controlled Random
Zigzag Sampling (Ctrl-Z Sampling), a novel sampling strategy designed to detect
and escape such local maxima during conditional generation. The method first
identifies potential local maxima using a reward model. Upon detection, it
injects noise and reverts to a previous, noisier state to escape the current
optimization plateau. The reward model then evaluates candidate trajectories,
accepting only those that offer improvement, while progressively deeper retreat
enables stronger escapes when nearby alternatives fail. This controlled random
zigzag process allows dynamic alternation between forward refinement and
backward exploration, enhancing both alignment and visual quality in the
generated outputs. The proposed Ctrl-Z Sampling is model-agnostic and
compatible with existing diffusion frameworks. Experimental results show that
Ctrl-Z Sampling substantially improves generation quality with only around 7.6X
increase in function evaluations.

</details>


### [21] [TDiR: Transformer based Diffusion for Image Restoration Tasks](https://arxiv.org/abs/2506.20302)
*Abbas Anwar,Mohammad Shullar,Ali Arshad Nasir,Mudassir Masood,Saeed Anwar*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的扩散模型，用于图像修复任务，在多个质量指标上优于现有深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 解决恶劣环境下图像退化（如噪声、色偏、模糊等）导致的质量下降问题，提升图像在下游任务中的适用性。

Method: 结合Transformer的扩散模型，用于图像增强、去噪和去雨等任务。

Result: 在公开数据集上，模型性能优于现有方法，显著提升了退化图像的质量。

Conclusion: 扩散模型与Transformer结合在图像修复中表现出色，可有效提升图像质量，适用于需要高保真视觉数据的下游任务。

Abstract: Images captured in challenging environments often experience various forms of
degradation, including noise, color cast, blur, and light scattering. These
effects significantly reduce image quality, hindering their applicability in
downstream tasks such as object detection, mapping, and classification. Our
transformer-based diffusion model was developed to address image restoration
tasks, aiming to improve the quality of degraded images. This model was
evaluated against existing deep learning methodologies across multiple quality
metrics for underwater image enhancement, denoising, and deraining on publicly
available datasets. Our findings demonstrate that the diffusion model, combined
with transformers, surpasses current methods in performance. The results of our
model highlight the efficacy of diffusion models and transformers in improving
the quality of degraded images, consequently expanding their utility in
downstream tasks that require high-fidelity visual data.

</details>


### [22] [Radiomic fingerprints for knee MR images assessment](https://arxiv.org/abs/2506.20306)
*Yaxi Chen,Simin Ni,Shaheer U. Saeed,Aleksandra Ivanova,Rikin Hargunani,Jie Huang,Chaozong Liu,Yipeng Hu*

Main category: cs.CV

TL;DR: 提出了一种动态构建放射组学特征的框架（指纹），通过深度学习模型为每位患者选择个性化特征，性能优于传统放射组学方法，且具有可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统放射组学方法使用固定特征集，无法捕捉个体病理差异，性能受限。本文旨在通过个性化特征选择提升诊断准确性和可解释性。

Method: 提出放射组学指纹框架，动态为每位患者选择特征，结合低维逻辑回归进行分类。

Result: 在多个膝关节诊断任务中表现优于或媲美端到端深度学习模型，同时提供临床可解释性。

Conclusion: 个性化放射组学特征选择能提升诊断性能，同时保留可解释性，为临床提供新见解。

Abstract: Accurate interpretation of knee MRI scans relies on expert clinical judgment,
often with high variability and limited scalability. Existing radiomic
approaches use a fixed set of radiomic features (the signature), selected at
the population level and applied uniformly to all patients. While
interpretable, these signatures are often too constrained to represent
individual pathological variations. As a result, conventional radiomic-based
approaches are found to be limited in performance, compared with recent
end-to-end deep learning (DL) alternatives without using interpretable radiomic
features. We argue that the individual-agnostic nature in current radiomic
selection is not central to its intepretability, but is responsible for the
poor generalization in our application. Here, we propose a novel radiomic
fingerprint framework, in which a radiomic feature set (the fingerprint) is
dynamically constructed for each patient, selected by a DL model. Unlike the
existing radiomic signatures, our fingerprints are derived on a per-patient
basis by predicting the feature relevance in a large radiomic feature pool, and
selecting only those that are predictive of clinical conditions for individual
patients. The radiomic-selecting model is trained simultaneously with a
low-dimensional (considered relatively explainable) logistic regression for
downstream classification. We validate our methods across multiple diagnostic
tasks including general knee abnormalities, anterior cruciate ligament (ACL)
tears, and meniscus tears, demonstrating comparable or superior diagnostic
accuracy relative to state-of-the-art end-to-end DL models. More importantly,
we show that the interpretability inherent in our approach facilitates
meaningful clinical insights and potential biomarker discovery, with detailed
discussion, quantitative and qualitative analysis of real-world clinical cases
to evidence these advantages.

</details>


### [23] [On the Burstiness of Faces in Set](https://arxiv.org/abs/2506.20312)
*Jiong Wang*

Main category: cs.CV

TL;DR: 论文探讨了在基于集合的人脸识别（SFR）中普遍存在的突发性现象，并提出三种策略检测和抑制突发性人脸，以提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 突发性现象在SFR中广泛存在，导致训练和评估阶段的性能下降，需要有效检测和抑制。

Method: 提出基于Quickshift++、特征自相似性和广义最大池化（GMP）的三种策略检测突发性人脸，并在训练和评估阶段调整采样比例或贡献。评估阶段还提出质量感知GMP。

Result: 实验证明突发性普遍存在，抑制突发性显著提升了识别性能。

Conclusion: 通过检测和抑制突发性人脸，可以有效提升SFR的性能和泛化能力。

Abstract: Burstiness, a phenomenon observed in text and image retrieval, refers to that
particular elements appear more times in a set than a statistically independent
model assumes. We argue that in the context of set-based face recognition
(SFR), burstiness exists widely and degrades the performance in two aspects:
Firstly, the bursty faces, where faces with particular attributes %exist
frequently in a face set, dominate the training instances and dominate the
training face sets and lead to poor generalization ability to unconstrained
scenarios. Secondly, the bursty faces %dominating the evaluation sets interfere
with the similarity comparison in set verification and identification when
evaluation. To detect the bursty faces in a set, we propose three strategies
based on Quickshift++, feature self-similarity, and generalized max-pooling
(GMP). We apply the burst detection results on training and evaluation stages
to enhance the sampling ratios or contributions of the infrequent faces. When
evaluation, we additionally propose the quality-aware GMP that enables
awareness of the face quality and robustness to the low-quality faces for the
original GMP. We give illustrations and extensive experiments on the SFR
benchmarks to demonstrate that burstiness is widespread and suppressing
burstiness considerably improves the recognition performance.

</details>


### [24] [From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents](https://arxiv.org/abs/2506.20326)
*Sergio Torres Aguilar*

Main category: cs.CV

TL;DR: 本文比较了五种目标检测架构在三个历史文档数据集上的性能，发现Transformer和CNN-OBB模型各有优势，OBB对非笛卡尔布局至关重要。


<details>
  <summary>Details</summary>
Motivation: 研究历史文档布局分析的鲁棒性，因复杂页面组织对自动化处理至关重要。

Method: 在三个数据集上评估了五种模型（两种Transformer和三种YOLO变体），比较其性能。

Result: Co-DETR在结构化数据集表现最佳，而YOLOv11x-OBB在复杂数据集上显著优于其他模型。

Conclusion: Transformer适合结构化布局，CNN-OBB模型在复杂文档中表现更优，OBB是建模历史文档的关键。

Abstract: Robust Document Layout Analysis (DLA) is critical for the automated
processing and understanding of historical documents with complex page
organizations. This paper benchmarks five state-of-the-art object detection
architectures on three annotated datasets representing a spectrum of
codicological complexity: The e-NDP, a corpus of Parisian medieval registers
(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval
and modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated
books of hours (ca.13th-16th centuries). We evaluate two Transformer-based
models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and
YOLO-World). Our findings reveal significant performance variations dependent
on model architecture, data set characteristics, and bounding box
representation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results
(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on
the more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB
significantly outperforms all other models (0.564 and 0.568, respectively).
This study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)
is not a minor refinement but a fundamental requirement for accurately modeling
the non-Cartesian nature of historical manuscripts. We conclude that a key
trade-off exists between the global context awareness of Transformers, ideal
for structured layouts, and the superior generalization of CNN-OBB models for
visually diverse and complex documents.

</details>


### [25] [Feature Hallucination for Self-supervised Action Recognition](https://arxiv.org/abs/2506.20342)
*Lei Wang,Piotr Koniusz*

Main category: cs.CV

TL;DR: 提出了一种深度翻译动作识别框架，通过联合预测动作概念和辅助特征提升识别精度，并引入两种新颖的领域特定描述符（ODF和SDF）以增强特征表示。


<details>
  <summary>Details</summary>
Motivation: 视频中的人类动作理解需要高层次的语义推理和多模态特征的有效整合，而现有方法在特征表示和计算效率上存在不足。

Method: 结合RGB视频帧预测动作概念和辅助特征，通过幻觉流推断缺失线索，引入ODF和SDF描述符，并整合多种模态（如光流、骨架数据等）。

Result: 在多个基准测试（如Kinetics-400、Kinetics-600等）上实现了最先进的性能，证明了其在捕捉细粒度动作动态方面的有效性。

Conclusion: 该框架通过多模态特征整合和不确定性建模，显著提升了动作识别的准确性和鲁棒性。

Abstract: Understanding human actions in videos requires more than raw pixel analysis;
it relies on high-level semantic reasoning and effective integration of
multimodal features. We propose a deep translational action recognition
framework that enhances recognition accuracy by jointly predicting action
concepts and auxiliary features from RGB video frames. At test time,
hallucination streams infer missing cues, enriching feature representations
without increasing computational overhead. To focus on action-relevant regions
beyond raw pixels, we introduce two novel domain-specific descriptors. Object
Detection Features (ODF) aggregate outputs from multiple object detectors to
capture contextual cues, while Saliency Detection Features (SDF) highlight
spatial and intensity patterns crucial for action recognition. Our framework
seamlessly integrates these descriptors with auxiliary modalities such as
optical flow, Improved Dense Trajectories, skeleton data, and audio cues. It
remains compatible with state-of-the-art architectures, including I3D,
AssembleNet, Video Transformer Network, FASTER, and recent models like VideoMAE
V2 and InternVideo2. To handle uncertainty in auxiliary features, we
incorporate aleatoric uncertainty modeling in the hallucination step and
introduce a robust loss function to mitigate feature noise. Our multimodal
self-supervised action recognition framework achieves state-of-the-art
performance on multiple benchmarks, including Kinetics-400, Kinetics-600, and
Something-Something V2, demonstrating its effectiveness in capturing
fine-grained action dynamics.

</details>


### [26] [InvZW: Invariant Feature Learning via Noise-Adversarial Training for Robust Image Zero-Watermarking](https://arxiv.org/abs/2506.20370)
*Abdullah All Tanvir,Xin Zhong*

Main category: cs.CV

TL;DR: 提出了一种基于失真不变特征学习的深度学习框架，用于鲁棒图像零水印。该方法不改变原始图像，通过学习特征空间中的参考签名实现水印。


<details>
  <summary>Details</summary>
Motivation: 传统水印方法可能破坏图像质量，零水印技术无需修改图像但需提高鲁棒性。

Method: 框架包含两个模块：1）通过噪声对抗学习训练特征提取器，生成对失真不变的特征；2）设计基于学习的多比特零水印方案，将特征投影到可训练的参考代码上。

Result: 在多种图像数据集和失真条件下，方法在特征稳定性和水印恢复方面达到最优鲁棒性。

Conclusion: 该框架在泛化性和鲁棒性上优于现有自监督和深度水印技术。

Abstract: This paper introduces a novel deep learning framework for robust image
zero-watermarking based on distortion-invariant feature learning. As a
zero-watermarking scheme, our method leaves the original image unaltered and
learns a reference signature through optimization in the feature space. The
proposed framework consists of two key modules. In the first module, a feature
extractor is trained via noise-adversarial learning to generate representations
that are both invariant to distortions and semantically expressive. This is
achieved by combining adversarial supervision against a distortion
discriminator and a reconstruction constraint to retain image content. In the
second module, we design a learning-based multibit zero-watermarking scheme
where the trained invariant features are projected onto a set of trainable
reference codes optimized to match a target binary message. Extensive
experiments on diverse image datasets and a wide range of distortions show that
our method achieves state-of-the-art robustness in both feature stability and
watermark recovery. Comparative evaluations against existing self-supervised
and deep watermarking techniques further highlight the superiority of our
framework in generalization and robustness.

</details>


### [27] [Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking](https://arxiv.org/abs/2506.20381)
*Ben Kang,Xin Chen,Jie Zhao,Chunjuan Bo,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: HiT和DyHiT是高效的视觉跟踪模型，通过轻量级Transformer和动态路由技术，在保持高性能的同时显著提升速度。


<details>
  <summary>Details</summary>
Motivation: 解决基于Transformer的视觉跟踪器在资源受限设备上速度慢的问题。

Method: HiT采用Bridge Module和双图像位置编码；DyHiT通过动态路由分类场景并选择计算路径。

Result: HiT速度达61 fps，AUC 64.6%；DyHiT最快111 fps，AUC 62.4%。加速方法使SeqTrack-B256提速2.68倍。

Conclusion: HiT和DyHiT在速度和性能上取得平衡，动态路由技术为高效跟踪提供了新思路。

Abstract: Transformer-based visual trackers have demonstrated significant advancements
due to their powerful modeling capabilities. However, their practicality is
limited on resource-constrained devices because of their slow processing
speeds. To address this challenge, we present HiT, a novel family of efficient
tracking models that achieve high performance while maintaining fast operation
across various devices. The core innovation of HiT lies in its Bridge Module,
which connects lightweight transformers to the tracking framework, enhancing
feature representation quality. Additionally, we introduce a dual-image
position encoding approach to effectively encode spatial information. HiT
achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson
AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark,
outperforming all previous efficient trackers.Building on HiT, we propose
DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by
selecting routes with varying computational requirements. DyHiT uses search
area features extracted by the backbone network and inputs them into an
efficient dynamic router to classify tracking scenarios. Based on the
classification, DyHiT applies a divide-and-conquer strategy, selecting
appropriate routes to achieve a superior trade-off between accuracy and speed.
The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while
maintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free
acceleration method based on the dynamic routing architecture of DyHiT. This
method significantly improves the execution speed of various high-performance
trackers without sacrificing accuracy. For instance, our acceleration method
enables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times
speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of
69.9% on the LaSOT.

</details>


### [28] [A Novel Large Vision Foundation Model (LVFM)-based Approach for Generating High-Resolution Canopy Height Maps in Plantations for Precision Forestry Management](https://arxiv.org/abs/2506.20388)
*Shen Tan,Xin Zhang,Liangxiu Han,Huaguo Huang,Han Wang*

Main category: cs.CV

TL;DR: 提出了一种基于大型视觉基础模型（LVFM）的高分辨率冠层高度图（CHM）生成方法，用于低成本、高精度监测人工林地上生物量（AGB）。


<details>
  <summary>Details</summary>
Motivation: 传统激光雷达方法成本高，而基于RGB图像的深度学习方法在冠层高度特征提取上存在挑战，因此需要一种更高效、准确的解决方案。

Method: 开发了一种新模型，结合特征提取器、自监督特征增强模块和高度估计器，利用1米分辨率的Google Earth图像进行测试。

Result: 模型在北京市房山区的测试中表现优异，平均绝对误差0.09米，均方根误差0.24米，与激光雷达CHM的相关系数为0.78，单木检测成功率达90%以上。

Conclusion: 该方法为碳汇评估提供了一种可扩展的工具，适用于人工林和天然林。

Abstract: Accurate, cost-effective monitoring of plantation aboveground biomass (AGB)
is crucial for supporting local livelihoods and carbon sequestration
initiatives like the China Certified Emission Reduction (CCER) program.
High-resolution canopy height maps (CHMs) are essential for this, but standard
lidar-based methods are expensive. While deep learning with RGB imagery offers
an alternative, accurately extracting canopy height features remains
challenging. To address this, we developed a novel model for high-resolution
CHM generation using a Large Vision Foundation Model (LVFM). Our model
integrates a feature extractor, a self-supervised feature enhancement module to
preserve spatial details, and a height estimator. Tested in Beijing's Fangshan
District using 1-meter Google Earth imagery, our model outperformed existing
methods, including conventional CNNs. It achieved a mean absolute error of 0.09
m, a root mean square error of 0.24 m, and a correlation of 0.78 against
lidar-based CHMs. The resulting CHMs enabled over 90% success in individual
tree detection, high accuracy in AGB estimation, and effective tracking of
plantation growth, demonstrating strong generalization to non-training areas.
This approach presents a promising, scalable tool for evaluating carbon
sequestration in both plantations and natural forests.

</details>


### [29] [Med-Art: Diffusion Transformer for 2D Medical Text-to-Image Generation](https://arxiv.org/abs/2506.20449)
*Changlu Guo,Anders Nymark Christensen,Morten Rieger Hannemose*

Main category: cs.CV

TL;DR: Med-Art是一个针对有限数据的医学图像生成框架，利用视觉语言模型生成医学图像的视觉描述，并采用基于DiT的PixArt-α模型和创新的HLDF方法，实现了高性能。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像生成中数据集小和医学文本数据稀缺的挑战。

Method: 结合视觉语言模型生成描述，基于DiT的PixArt-α模型，以及创新的HLDF方法进行微调。

Result: 在两个医学图像数据集上实现了最先进的性能（FID、KID和分类性能）。

Conclusion: Med-Art在有限数据下高效生成医学图像，解决了现有挑战。

Abstract: Text-to-image generative models have achieved remarkable breakthroughs in
recent years. However, their application in medical image generation still
faces significant challenges, including small dataset sizes, and scarcity of
medical textual data. To address these challenges, we propose Med-Art, a
framework specifically designed for medical image generation with limited data.
Med-Art leverages vision-language models to generate visual descriptions of
medical images which overcomes the scarcity of applicable medical textual data.
Med-Art adapts a large-scale pre-trained text-to-image model, PixArt-$\alpha$,
based on the Diffusion Transformer (DiT), achieving high performance under
limited data. Furthermore, we propose an innovative Hybrid-Level Diffusion
Fine-tuning (HLDF) method, which enables pixel-level losses, effectively
addressing issues such as overly saturated colors. We achieve state-of-the-art
performance on two medical image datasets, measured by FID, KID, and downstream
classification performance.

</details>


### [30] [HiWave: Training-Free High-Resolution Image Generation via Wavelet-Based Diffusion Sampling](https://arxiv.org/abs/2506.20452)
*Tobias Vontobel,Seyedmorteza Sadat,Farnood Salehi,Romann M. Weber*

Main category: cs.CV

TL;DR: HiWave是一种无需训练的方法，通过两阶段流程和基于小波的细节增强模块，显著提升预训练扩散模型在超高分辨率图像合成中的视觉保真度和结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本生成技术在超高分辨率图像合成中常产生伪影（如物体重复和空间不连贯），而高分辨率训练又计算成本高昂。

Method: HiWave采用两阶段流程：1）生成基础图像；2）通过DDIM反演和小波细节增强模块优化细节。

Result: 在Stable Diffusion XL上的评估显示，HiWave有效减少伪影，用户研究显示80%以上用户偏好HiWave。

Conclusion: HiWave无需重新训练或架构修改，即可实现高质量的超高分辨率图像合成。

Abstract: Diffusion models have emerged as the leading approach for image synthesis,
demonstrating exceptional photorealism and diversity. However, training
diffusion models at high resolutions remains computationally prohibitive, and
existing zero-shot generation techniques for synthesizing images beyond
training resolutions often produce artifacts, including object duplication and
spatial incoherence. In this paper, we introduce HiWave, a training-free,
zero-shot approach that substantially enhances visual fidelity and structural
coherence in ultra-high-resolution image synthesis using pretrained diffusion
models. Our method employs a two-stage pipeline: generating a base image from
the pretrained model followed by a patch-wise DDIM inversion step and a novel
wavelet-based detail enhancer module. Specifically, we first utilize inversion
methods to derive initial noise vectors that preserve global coherence from the
base image. Subsequently, during sampling, our wavelet-domain detail enhancer
retains low-frequency components from the base image to ensure structural
consistency, while selectively guiding high-frequency components to enrich fine
details and textures. Extensive evaluations using Stable Diffusion XL
demonstrate that HiWave effectively mitigates common visual artifacts seen in
prior methods, achieving superior perceptual quality. A user study confirmed
HiWave's performance, where it was preferred over the state-of-the-art
alternative in more than 80% of comparisons, highlighting its effectiveness for
high-quality, ultra-high-resolution image synthesis without requiring
retraining or architectural modifications.

</details>


### [31] [A Deep Learning Approach to Identify Rock Bolts in Complex 3D Point Clouds of Underground Mines Captured Using Mobile Laser Scanners](https://arxiv.org/abs/2506.20464)
*Dibyayan Patra,Pasindu Ranasinghe,Bikram Banerjee,Simit Raval*

Main category: cs.CV

TL;DR: 论文提出了一种名为DeepBolt的两阶段深度学习架构，用于在地下矿山复杂3D点云中自动高效识别岩栓，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 地下矿山中岩栓的频繁评估对维持岩体稳定至关重要，但手动检测因环境恶劣且耗时，自动化检测成为必要。现有方法在复杂点云中表现不佳，需要更鲁棒的解决方案。

Method: 提出DeepBolt，一种针对类别不平衡设计的两阶段深度学习架构，用于处理噪声大、环境多变的3D点云数据。

Result: DeepBolt在岩栓点交并比（IoU）上比现有语义分割模型提升42.5%，分类精度和召回率分别达96.41%和96.96%。

Conclusion: DeepBolt在复杂地下环境中表现出色，为岩栓自动识别提供了高效且鲁棒的解决方案。

Abstract: Rock bolts are crucial components of the subterranean support systems in
underground mines that provide adequate structural reinforcement to the rock
mass to prevent unforeseen hazards like rockfalls. This makes frequent
assessments of such bolts critical for maintaining rock mass stability and
minimising risks in underground mining operations. Where manual surveying of
rock bolts is challenging due to the low light conditions in the underground
mines and the time-intensive nature of the process, automated detection of rock
bolts serves as a plausible solution. To that end, this study focuses on the
automatic identification of rock bolts within medium to large-scale 3D point
clouds obtained from underground mines using mobile laser scanners. Existing
techniques for automated rock bolt identification primarily rely on feature
engineering and traditional machine learning approaches. However, such
techniques lack robustness as these point clouds present several challenges due
to data noise, varying environments, and complex surrounding structures.
Moreover, the target rock bolts are extremely small objects within large-scale
point clouds and are often partially obscured due to the application of
reinforcement shotcrete. Addressing these challenges, this paper proposes an
approach termed DeepBolt, which employs a novel two-stage deep learning
architecture specifically designed for handling severe class imbalance for the
automatic and efficient identification of rock bolts in complex 3D point
clouds. The proposed method surpasses state-of-the-art semantic segmentation
models by up to 42.5% in Intersection over Union (IoU) for rock bolt points.
Additionally, it outperforms existing rock bolt identification techniques,
achieving a 96.41% precision and 96.96% recall in classifying rock bolts,
demonstrating its robustness and effectiveness in complex underground
environments.

</details>


### [32] [AI-assisted radiographic analysis in detecting alveolar bone-loss severity and patterns](https://arxiv.org/abs/2506.20522)
*Chathura Wimalasiri,Piumal Rathnayake,Shamod Wijerathne,Sumudu Rasnayaka,Dhanushka Leuke Bandara,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: cs.CV

TL;DR: 提出一种基于AI的深度学习框架，用于自动检测和量化牙槽骨流失及其模式，通过IOPA放射影像实现高精度诊断。


<details>
  <summary>Details</summary>
Motivation: 牙周炎严重影响口腔健康和生活质量，准确评估骨流失严重程度和模式对诊断和治疗至关重要。

Method: 结合YOLOv8进行牙齿检测和Keypoint R-CNN模型识别解剖标志，利用YOLOv8x-seg模型分割骨水平和牙齿掩模，通过几何分析确定骨流失模式。

Result: 在1000张放射影像数据集上，骨流失严重程度检测（ICC达0.80）和模式分类（准确率87%）表现优异。

Conclusion: 该自动化系统为牙周评估提供了快速、客观且可重复的工具，有望改善早期诊断和个性化治疗计划。

Abstract: Periodontitis, a chronic inflammatory disease causing alveolar bone loss,
significantly affects oral health and quality of life. Accurate assessment of
bone loss severity and pattern is critical for diagnosis and treatment
planning. In this study, we propose a novel AI-based deep learning framework to
automatically detect and quantify alveolar bone loss and its patterns using
intraoral periapical (IOPA) radiographs. Our method combines YOLOv8 for tooth
detection with Keypoint R-CNN models to identify anatomical landmarks, enabling
precise calculation of bone loss severity. Additionally, YOLOv8x-seg models
segment bone levels and tooth masks to determine bone loss patterns (horizontal
vs. angular) via geometric analysis. Evaluated on a large, expertly annotated
dataset of 1000 radiographs, our approach achieved high accuracy in detecting
bone loss severity (intra-class correlation coefficient up to 0.80) and bone
loss pattern classification (accuracy 87%). This automated system offers a
rapid, objective, and reproducible tool for periodontal assessment, reducing
reliance on subjective manual evaluation. By integrating AI into dental
radiographic analysis, our framework has the potential to improve early
diagnosis and personalized treatment planning for periodontitis, ultimately
enhancing patient care and clinical outcomes.

</details>


### [33] [Pay Less Attention to Deceptive Artifacts: Robust Detection of Compressed Deepfakes on Online Social Networks](https://arxiv.org/abs/2506.20548)
*Manyi Li,Renshuai Tao,Yufan Liu,Chuangchuang Tan,Haotong Qin,Bing Li,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: PLADA框架通过处理压缩图像的块效应和利用配对与非配对数据，显著提升了社交网络中深度伪造图像的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法忽视压缩引入的块效应，且主要依赖原始图像，难以应对实际场景。

Method: PLADA包含块效应消除模块（B2E）和开放数据聚合模块（ODA），分别处理块效应和利用多种数据。

Result: 在26个数据集上的实验表明，PLADA优于现有方法，尤其在压缩和有限配对数据情况下。

Conclusion: PLADA为开放场景下的深度伪造检测提供了鲁棒解决方案，并首次强调了块效应的重要性。

Abstract: With the rapid advancement of deep learning, particularly through generative
adversarial networks (GANs) and diffusion models (DMs), AI-generated images, or
``deepfakes", have become nearly indistinguishable from real ones. These images
are widely shared across Online Social Networks (OSNs), raising concerns about
their misuse. Existing deepfake detection methods overlook the ``block effects"
introduced by compression in OSNs, which obscure deepfake artifacts, and
primarily focus on raw images, rarely encountered in real-world scenarios. To
address these challenges, we propose PLADA (Pay Less Attention to Deceptive
Artifacts), a novel framework designed to tackle the lack of paired data and
the ineffective use of compressed images. PLADA consists of two core modules:
Block Effect Eraser (B2E), which uses a dual-stage attention mechanism to
handle block effects, and Open Data Aggregation (ODA), which processes both
paired and unpaired data to improve detection. Extensive experiments across 26
datasets demonstrate that PLADA achieves a remarkable balance in deepfake
detection, outperforming SoTA methods in detecting deepfakes on OSNs, even with
limited paired data and compression. More importantly, this work introduces the
``block effect" as a critical factor in deepfake detection, providing a robust
solution for open-world scenarios. Our code is available at
https://github.com/ManyiLee/PLADA.

</details>


### [34] [Lightweight Multi-Frame Integration for Robust YOLO Object Detection in Videos](https://arxiv.org/abs/2506.20550)
*Yitong Quan,Benjamin Kiefer,Martin Messmer,Andreas Zell*

Main category: cs.CV

TL;DR: 提出了一种简单有效的多帧输入策略，利用时间信息提升YOLO检测器的性能，同时保持轻量化和实时性。


<details>
  <summary>Details</summary>
Motivation: 单帧检测在视频中因忽略时间上下文而性能受限，现有视频检测方法复杂且计算量大。

Method: 将连续多帧堆叠输入YOLO检测器，仅监督目标帧输出，保留架构简单性和高效性。

Result: 在MOT20Det和BOAT360数据集上验证了方法有效性，提升了轻量模型的检测鲁棒性。

Conclusion: 方法简单高效，缩小了轻量与重量级检测网络的差距，并贡献了新数据集BOAT360。

Abstract: Modern image-based object detection models, such as YOLOv7, primarily process
individual frames independently, thus ignoring valuable temporal context
naturally present in videos. Meanwhile, existing video-based detection methods
often introduce complex temporal modules, significantly increasing model size
and computational complexity. In practical applications such as surveillance
and autonomous driving, transient challenges including motion blur, occlusions,
and abrupt appearance changes can severely degrade single-frame detection
performance. To address these issues, we propose a straightforward yet highly
effective strategy: stacking multiple consecutive frames as input to a
YOLO-based detector while supervising only the output corresponding to a single
target frame. This approach leverages temporal information with minimal
modifications to existing architectures, preserving simplicity, computational
efficiency, and real-time inference capability. Extensive experiments on the
challenging MOT20Det and our BOAT360 datasets demonstrate that our method
improves detection robustness, especially for lightweight models, effectively
narrowing the gap between compact and heavy detection networks. Additionally,
we contribute the BOAT360 benchmark dataset, comprising annotated fisheye video
sequences captured from a boat, to support future research in multi-frame video
object detection in challenging real-world scenarios.

</details>


### [35] [AdvMIM: Adversarial Masked Image Modeling for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.20563)
*Lei Zhu,Jun Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.CV

TL;DR: 提出了一种对抗性掩码图像建模方法，用于半监督医学图像分割，通过增加监督信号和减少域差距，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: Transformer在医学图像分割中表现优异，但需要大量标注数据。半监督学习中标注数据有限，现有方法难以有效训练Transformer。

Method: 提出对抗性掩码图像建模方法，通过掩码输入预测完整分割掩码，利用原始标签和伪标签学习掩码域，并通过对抗训练减少域差距。

Result: 在三个公开医学图像分割数据集上显著优于现有方法。

Conclusion: 该方法有效提升了半监督学习中Transformer的性能，代码已开源。

Abstract: Vision Transformer has recently gained tremendous popularity in medical image
segmentation task due to its superior capability in capturing long-range
dependencies. However, transformer requires a large amount of labeled data to
be effective, which hinders its applicability in annotation scarce
semi-supervised learning scenario where only limited labeled data is available.
State-of-the-art semi-supervised learning methods propose combinatorial
CNN-Transformer learning to cross teach a transformer with a convolutional
neural network, which achieves promising results. However, it remains a
challenging task to effectively train the transformer with limited labeled
data. In this paper, we propose an adversarial masked image modeling method to
fully unleash the potential of transformer for semi-supervised medical image
segmentation. The key challenge in semi-supervised learning with transformer
lies in the lack of sufficient supervision signal. To this end, we propose to
construct an auxiliary masked domain from original domain with masked image
modeling and train the transformer to predict the entire segmentation mask with
masked inputs to increase supervision signal. We leverage the original labels
from labeled data and pseudo-labels from unlabeled data to learn the masked
domain. To further benefit the original domain from masked domain, we provide a
theoretical analysis of our method from a multi-domain learning perspective and
devise a novel adversarial training loss to reduce the domain gap between the
original and masked domain, which boosts semi-supervised learning performance.
We also extend adversarial masked image modeling to CNN network. Extensive
experiments on three public medical image segmentation datasets demonstrate the
effectiveness of our method, where our method outperforms existing methods
significantly. Our code is publicly available at
https://github.com/zlheui/AdvMIM.

</details>


### [36] [Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization](https://arxiv.org/abs/2506.20567)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Chuanqi Tan*

Main category: cs.CV

TL;DR: 提出了一种基于分割与摘要（DaS）的密集视频描述框架，通过将长视频分割为多个事件提案，结合视觉特征和分层注意力机制生成描述性句子。


<details>
  <summary>Details</summary>
Motivation: 解决密集视频描述任务中长视频内容复杂、难以直接生成连贯描述的问题。

Method: 将视频分割为事件提案，提取视觉特征并生成句子描述，采用两阶段LSTM网络结合分层注意力机制进行句子摘要。

Result: 在ActivityNet Captions数据集上验证了DaS框架的有效性。

Conclusion: DaS框架通过分割与摘要方法，显著提升了密集视频描述的性能。

Abstract: In this work, we propose a division-and-summarization (DaS) framework for
dense video captioning. After partitioning each untrimmed long video as
multiple event proposals, where each event proposal consists of a set of short
video segments, we extract visual feature (e.g., C3D feature) from each segment
and use the existing image/video captioning approach to generate one sentence
description for this segment. Considering that the generated sentences contain
rich semantic descriptions about the whole event proposal, we formulate the
dense video captioning task as a visual cue aided sentence summarization
problem and propose a new two stage Long Short Term Memory (LSTM) approach
equipped with a new hierarchical attention mechanism to summarize all generated
sentences as one descriptive sentence with the aid of visual features.
Specifically, the first-stage LSTM network takes all semantic words from the
generated sentences and the visual features from all segments within one event
proposal as the input, and acts as the encoder to effectively summarize both
semantic and visual information related to this event proposal. The
second-stage LSTM network takes the output from the first-stage LSTM network
and the visual features from all video segments within one event proposal as
the input, and acts as the decoder to generate one descriptive sentence for
this event proposal. Our comprehensive experiments on the ActivityNet Captions
dataset demonstrate the effectiveness of our newly proposed DaS framework for
dense video captioning.

</details>


### [37] [Causal Representation Learning with Observational Grouping for CXR Classification](https://arxiv.org/abs/2506.20582)
*Rajat Rasal,Avinash Kori,Ben Glocker*

Main category: cs.CV

TL;DR: 通过分组观测学习可识别的因果表示，提升胸部X光疾病分类的泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在医学影像中，揭示数据生成过程的真实因果关系，可以提升任务特定潜在特征的泛化性和鲁棒性。

Method: 提出一种端到端框架，通过分组观测学习可识别的因果表示，用于胸部X光疾病分类。

Result: 实验表明，通过分组（如种族、性别和成像视角）强制不变性，因果表示提升了多分类任务的泛化性和鲁棒性。

Conclusion: 分组观测学习可识别的因果表示是一种有效方法，能提升医学影像分类任务的性能。

Abstract: Identifiable causal representation learning seeks to uncover the true causal
relationships underlying a data generation process. In medical imaging, this
presents opportunities to improve the generalisability and robustness of
task-specific latent features. This work introduces the concept of grouping
observations to learn identifiable representations for disease classification
in chest X-rays via an end-to-end framework. Our experiments demonstrate that
these causal representations improve generalisability and robustness across
multiple classification tasks when grouping is used to enforce invariance w.r.t
race, sex, and imaging views.

</details>


### [38] [Dense Video Captioning using Graph-based Sentence Summarization](https://arxiv.org/abs/2506.20583)
*Zhiwang Zhang,Dong Xu,Wanli Ouyang,Luping Zhou*

Main category: cs.CV

TL;DR: 提出了一种基于图的分割与总结（GPaS）框架，用于密集视频字幕生成，通过分割事件提案并总结生成的句子来改进场景演变的描述。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分探索事件提案内的场景演变，导致在场景和对象变化较大的情况下表现不佳。

Method: 采用两阶段框架：1）分割阶段将事件提案拆分为更短的视频片段；2）总结阶段通过图卷积网络（GCN）和长短期记忆网络（LSTM）结合，总结片段句子为完整事件描述。

Result: 在ActivityNet Captions和YouCook II数据集上优于现有方法。

Conclusion: GPaS框架通过分割和总结策略有效提升了密集视频字幕生成的性能。

Abstract: Recently, dense video captioning has made attractive progress in detecting
and captioning all events in a long untrimmed video. Despite promising results
were achieved, most existing methods do not sufficiently explore the scene
evolution within an event temporal proposal for captioning, and therefore
perform less satisfactorily when the scenes and objects change over a
relatively long proposal. To address this problem, we propose a graph-based
partition-and-summarization (GPaS) framework for dense video captioning within
two stages. For the ``partition" stage, a whole event proposal is split into
short video segments for captioning at a finer level. For the ``summarization"
stage, the generated sentences carrying rich description information for each
segment are summarized into one sentence to describe the whole event. We
particularly focus on the ``summarization" stage, and propose a framework that
effectively exploits the relationship between semantic words for summarization.
We achieve this goal by treating semantic words as nodes in a graph and
learning their interactions by coupling Graph Convolutional Network (GCN) and
Long Short Term Memory (LSTM), with the aid of visual cues. Two schemes of
GCN-LSTM Interaction (GLI) modules are proposed for seamless integration of GCN
and LSTM. The effectiveness of our approach is demonstrated via an extensive
comparison with the state-of-the-arts methods on the two benchmarks ActivityNet
Captions dataset and YouCook II dataset.

</details>


### [39] [Learning-Based Distance Estimation for 360° Single-Sensor Setups](https://arxiv.org/abs/2506.20586)
*Yitong Quan,Benjamin Kiefer,Martin Messmer,Andreas Zell*

Main category: cs.CV

TL;DR: 提出了一种基于神经网络的单目360度鱼眼相机距离估计方法，优于传统几何方法和其他学习基线。


<details>
  <summary>Details</summary>
Motivation: 在全方位成像中，传统几何方法因镜头畸变和环境变化难以准确估计距离，需要更鲁棒和自适应的解决方案。

Method: 使用神经网络直接从原始全方位输入中学习和推断物体距离，无需精确镜头校准。

Result: 在三个360度数据集（LOAF、ULM360和Boat360）上验证，模型在准确性和鲁棒性上优于传统方法和其他学习基线。

Conclusion: 深度学习方法在实时全方位距离估计中具有潜力，尤其适用于低成本机器人、自主导航和监控应用。

Abstract: Accurate distance estimation is a fundamental challenge in robotic
perception, particularly in omnidirectional imaging, where traditional
geometric methods struggle with lens distortions and environmental variability.
In this work, we propose a neural network-based approach for monocular distance
estimation using a single 360{\deg} fisheye lens camera. Unlike classical
trigonometric techniques that rely on precise lens calibration, our method
directly learns and infers the distance of objects from raw omnidirectional
inputs, offering greater robustness and adaptability across diverse conditions.
We evaluate our approach on three 360{\deg} datasets (LOAF, ULM360, and a newly
captured dataset Boat360), each representing distinct environmental and sensor
setups. Our experimental results demonstrate that the proposed learning-based
model outperforms traditional geometry-based methods and other learning
baselines in both accuracy and robustness. These findings highlight the
potential of deep learning for real-time omnidirectional distance estimation,
making our approach particularly well-suited for low-cost applications in
robotics, autonomous navigation, and surveillance.

</details>


### [40] [TRIM: A Self-Supervised Video Summarization Framework Maximizing Temporal Relative Information and Representativeness](https://arxiv.org/abs/2506.20588)
*Pritam Mishra,Coloma Ballester,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: 提出了一种自监督视频摘要模型，无需注意力机制或复杂架构，在SUMME和TVSUM数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 视频内容日益普及，但现有方法依赖监督标注或计算昂贵的注意力模型，限制了跨域适用性。

Method: 采用马尔可夫过程驱动的损失指标和两阶段自监督学习框架，避免使用注意力、RNN或Transformer。

Result: 在SUMME和TVSUM数据集上达到最优性能，甚至媲美监督模型。

Conclusion: 展示了高效、无需标注的架构潜力，挑战了对复杂模型的依赖。

Abstract: The increasing ubiquity of video content and the corresponding demand for
efficient access to meaningful information have elevated video summarization
and video highlights as a vital research area. However, many state-of-the-art
methods depend heavily either on supervised annotations or on attention-based
models, which are computationally expensive and brittle in the face of
distribution shifts that hinder cross-domain applicability across datasets. We
introduce a pioneering self-supervised video summarization model that captures
both spatial and temporal dependencies without the overhead of attention, RNNs,
or transformers. Our framework integrates a novel set of Markov process-driven
loss metrics and a two-stage self supervised learning paradigm that ensures
both performance and efficiency. Our approach achieves state-of-the-art
performance on the SUMME and TVSUM datasets, outperforming all existing
unsupervised methods. It also rivals the best supervised models, demonstrating
the potential for efficient, annotation-free architectures. This paves the way
for more generalizable video summarization techniques and challenges the
prevailing reliance on complex architectures.

</details>


### [41] [WonderFree: Enhancing Novel View Quality and Cross-View Consistency for 3D Scene Exploration](https://arxiv.org/abs/2506.20590)
*Chaojun Ni,Jie Li,Haoyun Li,Hengyu Liu,Xiaofeng Wang,Zheng Zhu,Guosheng Zhao,Boyuan Wang,Chenxin Li,Guan Huang,Wenjun Mei*

Main category: cs.CV

TL;DR: WonderFree是一种交互式3D场景生成模型，解决了现有方法在探索性和视角一致性上的限制，通过WorldRestorer和ConsistView技术提升渲染质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 当前3D生成方法在探索性和视角一致性上存在局限，无法在大范围移动时保持高质量渲染，特别是在未探索区域。

Method: 提出WonderFree模型，分解问题为视角质量和一致性两个子问题，分别用WorldRestorer和ConsistView技术解决。

Result: 实验表明，WonderFree显著提升渲染质量和全局一致性，用户研究显示77.20%偏好优于WonderWorld。

Conclusion: WonderFree为3D场景生成提供了高质量和一致性的解决方案，代码和模型将公开。

Abstract: Interactive 3D scene generation from a single image has gained significant
attention due to its potential to create immersive virtual worlds. However, a
key challenge in current 3D generation methods is the limited explorability,
which cannot render high-quality images during larger maneuvers beyond the
original viewpoint, particularly when attempting to move forward into unseen
areas. To address this challenge, we propose WonderFree, the first model that
enables users to interactively generate 3D worlds with the freedom to explore
from arbitrary angles and directions. Specifically, we decouple this challenge
into two key subproblems: novel view quality, which addresses visual artifacts
and floating issues in novel views, and cross-view consistency, which ensures
spatial consistency across different viewpoints. To enhance rendering quality
in novel views, we introduce WorldRestorer, a data-driven video restoration
model designed to eliminate floaters and artifacts. In addition, a data
collection pipeline is presented to automatically gather training data for
WorldRestorer, ensuring it can handle scenes with varying styles needed for 3D
scene generation. Furthermore, to improve cross-view consistency, we propose
ConsistView, a multi-view joint restoration mechanism that simultaneously
restores multiple perspectives while maintaining spatiotemporal coherence.
Experimental results demonstrate that WonderFree not only enhances rendering
quality across diverse viewpoints but also significantly improves global
coherence and consistency. These improvements are confirmed by CLIP-based
metrics and a user study showing a 77.20% preference for WonderFree over
WonderWorld enabling a seamless and immersive 3D exploration experience. The
code, model, and data will be publicly available.

</details>


### [42] [SFNet: Fusion of Spatial and Frequency-Domain Features for Remote Sensing Image Forgery Detection](https://arxiv.org/abs/2506.20599)
*Ji Qi,Xinchang Zhang,Dingqi Ye,Yongjia Ruan,Xin Guo,Shaowen Wang,Haifeng Li*

Main category: cs.CV

TL;DR: SFNet是一种新型的伪造检测框架，通过结合空间和频域特征，提高了对多样化遥感数据的伪造图像检测能力。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展导致伪造遥感图像难以检测，可能引发错误情报和虚假信息。现有方法依赖单一视觉特征，难以适应多样化的遥感数据和不断演变的伪造技术。

Method: SFNet采用两个独立的特征提取器分别捕获空间和频域特征，并通过域特征映射模块和混合域特征细化模块（CBAM注意力）对齐和融合多域特征。

Result: 在三个数据集上的实验表明，SFNet的准确率比现有方法提高了4%-15.18%，并展现出强大的泛化能力。

Conclusion: SFNet通过多域特征融合，显著提升了伪造检测的准确性和泛化能力，为遥感图像的真实性验证提供了有效工具。

Abstract: The rapid advancement of generative artificial intelligence is producing fake
remote sensing imagery (RSI) that is increasingly difficult to detect,
potentially leading to erroneous intelligence, fake news, and even conspiracy
theories. Existing forgery detection methods typically rely on single visual
features to capture predefined artifacts, such as spatial-domain cues to detect
forged objects like roads or buildings in RSI, or frequency-domain features to
identify artifacts from up-sampling operations in adversarial generative
networks (GANs). However, the nature of artifacts can significantly differ
depending on geographic terrain, land cover types, or specific features within
the RSI. Moreover, these complex artifacts evolve as generative models become
more sophisticated. In short, over-reliance on a single visual cue makes
existing forgery detectors struggle to generalize across diverse remote sensing
data. This paper proposed a novel forgery detection framework called SFNet,
designed to identify fake images in diverse remote sensing data by leveraging
spatial and frequency domain features. Specifically, to obtain rich and
comprehensive visual information, SFNet employs two independent feature
extractors to capture spatial and frequency domain features from input RSIs. To
fully utilize the complementary domain features, the domain feature mapping
module and the hybrid domain feature refinement module(CBAM attention) of SFNet
are designed to successively align and fuse the multi-domain features while
suppressing redundant information. Experiments on three datasets show that
SFNet achieves an accuracy improvement of 4%-15.18% over the state-of-the-art
RS forgery detection methods and exhibits robust generalization capabilities.
The code is available at https://github.com/GeoX-Lab/RSTI/tree/main/SFNet.

</details>


### [43] [Video Perception Models for 3D Scene Synthesis](https://arxiv.org/abs/2506.20601)
*Rui Huang,Guangyao Zhai,Zuria Bauer,Marc Pollefeys,Federico Tombari,Leonidas Guibas,Gao Huang,Francis Engelmann*

Main category: cs.CV

TL;DR: VIPScene利用视频生成模型的3D物理世界常识知识，实现高真实感和结构一致性的3D场景合成。


<details>
  <summary>Details</summary>
Motivation: 传统3D场景合成需要专家知识和大量手动工作，自动化该过程对建筑设计、机器人模拟等领域有重要意义。现有方法（如LLMs或图像生成模型）在3D空间推理或多视角一致性方面存在局限。

Method: VIPScene结合视频生成、3D重建和开放词汇感知模型，通过文本和图像提示生成场景，并引入FPVScore评估一致性和合理性。

Result: 实验表明VIPScene显著优于现有方法，且能泛化到多样场景。

Conclusion: VIPScene通过视频感知模型解决了3D场景合成的关键问题，代码将开源。

Abstract: Traditionally, 3D scene synthesis requires expert knowledge and significant
manual effort. Automating this process could greatly benefit fields such as
architectural design, robotics simulation, virtual reality, and gaming. Recent
approaches to 3D scene synthesis often rely on the commonsense reasoning of
large language models (LLMs) or strong visual priors of modern image generation
models. However, current LLMs demonstrate limited 3D spatial reasoning ability,
which restricts their ability to generate realistic and coherent 3D scenes.
Meanwhile, image generation-based methods often suffer from constraints in
viewpoint selection and multi-view inconsistencies. In this work, we present
Video Perception models for 3D Scene synthesis (VIPScene), a novel framework
that exploits the encoded commonsense knowledge of the 3D physical world in
video generation models to ensure coherent scene layouts and consistent object
placements across views. VIPScene accepts both text and image prompts and
seamlessly integrates video generation, feedforward 3D reconstruction, and
open-vocabulary perception models to semantically and geometrically analyze
each object in a scene. This enables flexible scene synthesis with high realism
and structural consistency. For more precise analysis, we further introduce
First-Person View Score (FPVScore) for coherence and plausibility evaluation,
utilizing continuous first-person perspective to capitalize on the reasoning
ability of multimodal large language models. Extensive experiments show that
VIPScene significantly outperforms existing methods and generalizes well across
diverse scenarios. The code will be released.

</details>


### [44] [Shape2Animal: Creative Animal Generation from Natural Silhouettes](https://arxiv.org/abs/2506.20616)
*Quoc-Duy Tran,Anh-Tuan Vo,Dinh-Khoi Vo,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: Shape2Animal框架通过重新解释自然物体轮廓（如云、石头或火焰）为动物形态，模拟人类的pareidolia现象。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过技术模拟人类将模糊刺激感知为有意义模式的能力，以拓展视觉叙事、教育内容和数字艺术的应用。

Method: 使用开放词汇分割提取物体轮廓，结合视觉语言模型生成语义合适的动物概念，并利用文本到图像扩散模型合成符合输入形状的动物图像。

Result: 在多样化的真实输入上验证了Shape2Animal的鲁棒性和创造力。

Conclusion: Shape2Animal为视觉叙事、教育内容和数字艺术设计提供了新的可能性。

Abstract: Humans possess a unique ability to perceive meaningful patterns in ambiguous
stimuli, a cognitive phenomenon known as pareidolia. This paper introduces
Shape2Animal framework to mimics this imaginative capacity by reinterpreting
natural object silhouettes, such as clouds, stones, or flames, as plausible
animal forms. Our automated framework first performs open-vocabulary
segmentation to extract object silhouette and interprets semantically
appropriate animal concepts using vision-language models. It then synthesizes
an animal image that conforms to the input shape, leveraging text-to-image
diffusion model and seamlessly blends it into the original scene to generate
visually coherent and spatially consistent compositions. We evaluated
Shape2Animal on a diverse set of real-world inputs, demonstrating its
robustness and creative potential. Our Shape2Animal can offer new opportunities
for visual storytelling, educational content, digital art, and interactive
media design. Our project page is here: https://shape2image.github.io

</details>


### [45] [Joint attitude estimation and 3D neural reconstruction of non-cooperative space objects](https://arxiv.org/abs/2506.20638)
*Clément Forray,Pauline Delporte,Nicolas Delaygue,Florence Genin,Dawa Derksen*

Main category: cs.CV

TL;DR: 利用NeRF技术从模拟图像中对非合作空间物体进行3D重建，重点优化相机姿态，实验表明逐帧训练效果最佳。


<details>
  <summary>Details</summary>
Motivation: 提升空间态势感知（SSA）能力，支持主动碎片清除、在轨维护和异常检测等应用。

Method: 采用NeRF模型，联合优化相机姿态，通过均匀旋转估计姿态，并使用正则化防止姿态变化过大。

Result: 实验显示，逐帧训练能够实现最准确的3D重建。

Conclusion: NeRF结合相机姿态优化在空间物体3D重建中具有潜力，尤其在有限视角和特殊环境条件下。

Abstract: Obtaining a better knowledge of the current state and behavior of objects
orbiting Earth has proven to be essential for a range of applications such as
active debris removal, in-orbit maintenance, or anomaly detection. 3D models
represent a valuable source of information in the field of Space Situational
Awareness (SSA). In this work, we leveraged Neural Radiance Fields (NeRF) to
perform 3D reconstruction of non-cooperative space objects from simulated
images. This scenario is challenging for NeRF models due to unusual camera
characteristics and environmental conditions : mono-chromatic images, unknown
object orientation, limited viewing angles, absence of diffuse lighting etc. In
this work we focus primarly on the joint optimization of camera poses alongside
the NeRF. Our experimental results show that the most accurate 3D
reconstruction is achieved when training with successive images one-by-one. We
estimate camera poses by optimizing an uniform rotation and use regularization
to prevent successive poses from being too far apart.

</details>


### [46] [Disentangled representations of microscopy images](https://arxiv.org/abs/2506.20649)
*Jacopo Dapueto,Vito Paolo Pastore,Nicoletta Noceti,Francesca Odone*

Main category: cs.CV

TL;DR: 提出了一种解耦表示学习（DRL）方法，用于提升显微镜图像分类模型的可解释性，在准确性和可解释性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 显微镜图像分析在诊断、合成工程和环境监测等领域至关重要，但深度学习模型的可解释性仍是一个挑战。

Method: 采用解耦表示学习（DRL）框架，利用合成数据学习表示，并在三个显微镜图像数据集（浮游生物、酵母液泡和人类细胞）上进行验证。

Result: DRL方法在显微镜图像分类中实现了准确性和可解释性的良好平衡。

Conclusion: 解耦表示学习为显微镜图像分析提供了一种兼具性能和可解释性的解决方案。

Abstract: Microscopy image analysis is fundamental for different applications, from
diagnosis to synthetic engineering and environmental monitoring. Modern
acquisition systems have granted the possibility to acquire an escalating
amount of images, requiring a consequent development of a large collection of
deep learning-based automatic image analysis methods. Although deep neural
networks have demonstrated great performance in this field, interpretability,
an essential requirement for microscopy image analysis, remains an open
challenge.
  This work proposes a Disentangled Representation Learning (DRL) methodology
to enhance model interpretability for microscopy image classification.
Exploiting benchmark datasets from three different microscopic image domains
(plankton, yeast vacuoles, and human cells), we show how a DRL framework, based
on transferring a representation learnt from synthetic data, can provide a good
trade-off between accuracy and interpretability in this domain.

</details>


### [47] [MMSearch-R1: Incentivizing LMMs to Search](https://arxiv.org/abs/2506.20670)
*Jinming Wu,Zihao Deng,Wei Li,Yiding Liu,Bo You,Bo Li,Zejun Ma,Ziwei Liu*

Main category: cs.CV

TL;DR: MMSearch-R1是一个端到端强化学习框架，用于优化大型多模态模型（LMMs）在真实互联网环境中的多轮搜索行为。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如检索增强生成和提示工程搜索代理）依赖固定流程，导致搜索效率低下或过度搜索。

Method: MMSearch-R1整合图像和文本搜索工具，通过基于结果的奖励和搜索惩罚指导模型决策。

Result: 实验表明，该框架在知识密集和信息寻求任务中优于同类基线模型，并减少30%以上的搜索调用。

Conclusion: MMSearch-R1为多模态搜索研究提供了高效且按需的解决方案。

Abstract: Robust deployment of large multimodal models (LMMs) in real-world scenarios
requires access to external knowledge sources, given the complexity and dynamic
nature of real-world information. Existing approaches such as
retrieval-augmented generation (RAG) and prompt engineered search agents rely
on rigid pipelines, often leading to inefficient or excessive search behaviors.
We present MMSearch-R1, the first end-to-end reinforcement learning framework
that enables LMMs to perform on-demand, multi-turn search in real-world
Internet environments. Our framework integrates both image and text search
tools, allowing the model to reason about when and how to invoke them guided by
an outcome-based reward with a search penalty. To support training, We collect
a multimodal search VQA dataset through a semi-automated pipeline that covers
diverse visual and textual knowledge needs and curate a search-balanced subset
with both search-required and search-free samples, which proves essential for
shaping efficient and on-demand search behavior. Extensive experiments on
knowledge-intensive and info-seeking VQA tasks show that our model not only
outperforms RAG-based baselines of the same model size, but also matches the
performance of a larger RAG-based model while reducing search calls by over
30%. We further analyze key empirical findings to offer actionable insights for
advancing research in multimodal search.

</details>


### [48] [IPFormer: Visual 3D Panoptic Scene Completion with Context-Adaptive Instance Proposals](https://arxiv.org/abs/2506.20671)
*Markus Gross,Aya Fahmy,Danit Niwattananan,Dominik Muhle,Rui Song,Daniel Cremers,Henri Meeß*

Main category: cs.CV

TL;DR: IPFormer提出了一种基于视觉的3D全景场景补全方法，通过动态实例提案提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在测试时静态查询的局限性，提升场景理解的动态适应性。

Method: 利用图像上下文动态初始化实例提案，并通过注意力机制优化语义实例-体素关系。

Result: 在PQ$^\dagger$和PQ-All指标上超越现有方法，运行时间减少14倍以上。

Conclusion: 动态实例提案显著提升性能，是视觉3D全景场景补全的创新方法。

Abstract: Semantic Scene Completion (SSC) has emerged as a pivotal approach for jointly
learning scene geometry and semantics, enabling downstream applications such as
navigation in mobile robotics. The recent generalization to Panoptic Scene
Completion (PSC) advances the SSC domain by integrating instance-level
information, thereby enhancing object-level sensitivity in scene understanding.
While PSC was introduced using LiDAR modality, methods based on camera images
remain largely unexplored. Moreover, recent Transformer-based SSC approaches
utilize a fixed set of learned queries to reconstruct objects within the scene
volume. Although these queries are typically updated with image context during
training, they remain static at test time, limiting their ability to
dynamically adapt specifically to the observed scene. To overcome these
limitations, we propose IPFormer, the first approach that leverages
context-adaptive instance proposals at train and test time to address
vision-based 3D Panoptic Scene Completion. Specifically, IPFormer adaptively
initializes these queries as panoptic instance proposals derived from image
context and further refines them through attention-based encoding and decoding
to reason about semantic instance-voxel relationships. Experimental results
show that our approach surpasses state-of-the-art methods in overall panoptic
metrics PQ$^\dagger$ and PQ-All, matches performance in individual metrics, and
achieves a runtime reduction exceeding 14$\times$. Furthermore, our ablation
studies reveal that dynamically deriving instance proposals from image context,
as opposed to random initialization, leads to a 3.62% increase in PQ-All and a
remarkable average improvement of 18.65% in combined Thing-metrics. These
results highlight our introduction of context-adaptive instance proposals as a
pioneering effort in addressing vision-based 3D Panoptic Scene Completion.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [49] [VoxelOpt: Voxel-Adaptive Message Passing for Discrete Optimization in Deformable Abdominal CT Registration](https://arxiv.org/abs/2506.19975)
*Hang Zhang,Yuxi Zhang,Jiazheng Wang,Xiang Chen,Renjiu Hu,Xin Tian,Gaolei Li,Min Liu*

Main category: eess.IV

TL;DR: VoxelOpt是一种结合学习和迭代方法的变形图像配准框架，通过位移熵和多级图像金字塔优化，在精度和效率上取得平衡。


<details>
  <summary>Details</summary>
Motivation: 解决基于学习的方法在数据有限、大变形和无标签监督时表现不佳的问题，同时避免迭代方法的速度劣势。

Method: 使用位移熵测量信号强度，引入体素自适应消息传递、多级图像金字塔和预训练分割模型提取特征。

Result: 在腹部CT配准中，VoxelOpt在效率和精度上优于迭代方法，与有监督学习方法的性能相当。

Conclusion: VoxelOpt通过结合学习和迭代方法的优势，实现了高效且准确的图像配准。

Abstract: Recent developments in neural networks have improved deformable image
registration (DIR) by amortizing iterative optimization, enabling fast and
accurate DIR results. However, learning-based methods often face challenges
with limited training data, large deformations, and tend to underperform
compared to iterative approaches when label supervision is unavailable. While
iterative methods can achieve higher accuracy in such scenarios, they are
considerably slower than learning-based methods. To address these limitations,
we propose VoxelOpt, a discrete optimization-based DIR framework that combines
the strengths of learning-based and iterative methods to achieve a better
balance between registration accuracy and runtime. VoxelOpt uses displacement
entropy from local cost volumes to measure displacement signal strength at each
voxel, which differs from earlier approaches in three key aspects. First, it
introduces voxel-wise adaptive message passing, where voxels with lower entropy
receives less influence from their neighbors. Second, it employs a multi-level
image pyramid with 27-neighbor cost volumes at each level, avoiding exponential
complexity growth. Third, it replaces hand-crafted features or contrastive
learning with a pretrained foundational segmentation model for feature
extraction. In abdominal CT registration, these changes allow VoxelOpt to
outperform leading iterative in both efficiency and accuracy, while matching
state-of-the-art learning-based methods trained with label supervision. The
source code will be available at https://github.com/tinymilky/VoxelOpt

</details>


### [50] [MS-IQA: A Multi-Scale Feature Fusion Network for PET/CT Image Quality Assessment](https://arxiv.org/abs/2506.20200)
*Siqiao Li,Chen Hui,Wei Zhang,Rui Liang,Chenyue Song,Feng Jiang,Haiqi Zhu,Zhixuan Li,Hong Huang,Xiang Li*

Main category: eess.IV

TL;DR: 提出了一种多尺度特征融合网络MS-IQA，用于PET/CT图像质量评估，结合了ResNet和Swin Transformer的多尺度特征，并通过动态加权通道注意力机制融合高低层信息。同时构建了PET-CT-IQA-DS数据集，实验表明该方法优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像质量评估方法无法同时考虑低层特征（如失真）和高层特征（如器官解剖结构），导致诊断不确定性增加。

Method: 提出MS-IQA网络，结合ResNet和Swin Transformer的多尺度特征，引入动态加权通道注意力机制融合高低层信息。构建PET-CT-IQA-DS数据集。

Result: 在PET-CT-IQA-DS和LDCTIQAC2023数据集上，MS-IQA在多种IQA指标上优于现有方法。

Conclusion: MS-IQA为PET/CT提供了一种准确高效的图像质量评估方法，填补了相关数据集的空白。

Abstract: Positron Emission Tomography / Computed Tomography (PET/CT) plays a critical
role in medical imaging, combining functional and anatomical information to aid
in accurate diagnosis. However, image quality degradation due to noise,
compression and other factors could potentially lead to diagnostic uncertainty
and increase the risk of misdiagnosis. When evaluating the quality of a PET/CT
image, both low-level features like distortions and high-level features like
organ anatomical structures affect the diagnostic value of the image. However,
existing medical image quality assessment (IQA) methods are unable to account
for both feature types simultaneously. In this work, we propose MS-IQA, a novel
multi-scale feature fusion network for PET/CT IQA, which utilizes multi-scale
features from various intermediate layers of ResNet and Swin Transformer,
enhancing its ability of perceiving both local and global information. In
addition, a multi-scale feature fusion module is also introduced to effectively
combine high-level and low-level information through a dynamically weighted
channel attention mechanism. Finally, to fill the blank of PET/CT IQA dataset,
we construct PET-CT-IQA-DS, a dataset containing 2,700 varying-quality PET/CT
images with quality scores assigned by radiologists. Experiments on our dataset
and the publicly available LDCTIQAC2023 dataset demonstrate that our proposed
model has achieved superior performance against existing state-of-the-art
methods in various IQA metrics. This work provides an accurate and efficient
IQA method for PET/CT. Our code and dataset are available at
https://github.com/MS-IQA/MS-IQA/.

</details>


### [51] [Volumetric segmentation of muscle compartments using in vivo imaging and architectural validation in human finger flexors](https://arxiv.org/abs/2506.20206)
*Yang Li*

Main category: eess.IV

TL;DR: 提出了一种基于超声和MRI的肌肉分区体积分割新方法，用于测量手指控制肌肉的架构特性，并通过肌电图验证了分割准确性。


<details>
  <summary>Details</summary>
Motivation: 肌肉分区的分割和架构测量有助于运动功能评估、精确的肌肉骨骼建模和基于协同的肌电图模拟。

Method: 采用两步分段分割法，先基于分区运动在超声图像中标注区域，再通过最小能量匹配将超声数据配准到三维MRI坐标系；利用MRI纤维束成像测量架构特性。

Result: 纤维取向与尸体照片一致，分区架构特性差异显著（P < 0.001），95%的肌电图中心位于对应分区内。

Conclusion: 验证的分割方法和架构特性可为生物医学应用提供支持。

Abstract: Segmenting muscle compartments and measuring their architecture can
facilitate movement function assessment, accurate musculoskeletal modeling, and
synergy-based electromyogram simulation. Here, we presented a novel method for
volumetric segmentation of muscle compartments using in vivo imaging, focusing
on the independent compartments for finger control of flexor digitorum
superficialis (FDS). Besides, we measured the architectural properties of FDS
compartments and validated the segmentation. Specifically, ultrasound and
magnetic resonance imaging (MRI) from 10 healthy subjects were used for
segmentation and measurement, while electromyography was utilized for
validation. A two-step piecewise segmentation was proposed, first annotating
compartment regions in the cross-sectional ultrasound image based on
compartment movement, and then performing minimum energy matching to register
the ultrasound data to the three-dimensional MRI coordinate system.
Additionally, the architectural properties were measured in the compartment
masks from the segmentation using MRI tractography. Anatomical correctness was
verified by comparing known anatomy with reconstructed fiber tracts and
measured properties, while segmentation accuracy was quantified as the
percentage of finger electromyogram centers falling within their corresponding
compartments. Results demonstrated agreement for the fiber orientation between
the tractography and cadaveric photographs. Significant differences in
architectural properties (P < 0.001) were observed between compartments. The
properties of FDS and its compartments were within the physiological ranges (P
< 0.01). 95% (38/40) of the electromyogram centers were located within
respective compartments, with 2 errors occurring in the index and little
fingers. The validated segmentation method and derived architectural properties
may advance biomedical applications.

</details>


### [52] [Opportunistic Osteoporosis Diagnosis via Texture-Preserving Self-Supervision, Mixture of Experts and Multi-Task Integration](https://arxiv.org/abs/2506.20282)
*Jiaxing Huang,Heng Guo,Le Lu,Fan Yang,Minfeng Xu,Ge Yang,Wei Luo*

Main category: eess.IV

TL;DR: 提出一种深度学习框架，通过自监督学习、混合专家架构和多任务学习，解决骨质疏松诊断中未标记数据利用不足、设备差异和临床知识整合不足的问题。


<details>
  <summary>Details</summary>
Motivation: 骨质疏松诊断在资源有限地区受限，现有CT分析方法存在未标记数据利用不足、设备差异和临床知识整合不足的问题。

Method: 采用自监督学习利用未标记CT数据，混合专家架构增强跨设备适应性，多任务学习整合骨质疏松诊断、BMD回归和椎骨定位。

Result: 在三个临床站点和外部医院验证中，该方法表现出优于现有方法的泛化能力和准确性。

Conclusion: 该框架为骨质疏松筛查和诊断提供了高效且通用的解决方案。

Abstract: Osteoporosis, characterized by reduced bone mineral density (BMD) and
compromised bone microstructure, increases fracture risk in aging populations.
While dual-energy X-ray absorptiometry (DXA) is the clinical standard for BMD
assessment, its limited accessibility hinders diagnosis in resource-limited
regions. Opportunistic computed tomography (CT) analysis has emerged as a
promising alternative for osteoporosis diagnosis using existing imaging data.
Current approaches, however, face three limitations: (1) underutilization of
unlabeled vertebral data, (2) systematic bias from device-specific DXA
discrepancies, and (3) insufficient integration of clinical knowledge such as
spatial BMD distribution patterns. To address these, we propose a unified deep
learning framework with three innovations. First, a self-supervised learning
method using radiomic representations to leverage unlabeled CT data and
preserve bone texture. Second, a Mixture of Experts (MoE) architecture with
learned gating mechanisms to enhance cross-device adaptability. Third, a
multi-task learning framework integrating osteoporosis diagnosis, BMD
regression, and vertebra location prediction. Validated across three clinical
sites and an external hospital, our approach demonstrates superior
generalizability and accuracy over existing methods for opportunistic
osteoporosis screening and diagnosis.

</details>


### [53] [FundaQ-8: A Clinically-Inspired Scoring Framework for Automated Fundus Image Quality Assessment](https://arxiv.org/abs/2506.20303)
*Lee Qi Zun,Oscar Wong Jin Hao,Nor Anita Binti Che Omar,Zalifa Zakiah Binti Asnir,Mohamad Sabri bin Sinal Zainal,Goh Man Fye*

Main category: eess.IV

TL;DR: 提出了FundaQ-8框架，用于系统性评估眼底图像质量，并基于ResNet18开发了回归模型，验证了其可靠性和临床实用性。


<details>
  <summary>Details</summary>
Motivation: 解决眼底图像质量评估中的主观性和图像获取差异问题。

Method: 使用FundaQ-8作为评分参考，基于ResNet18开发回归模型，采用迁移学习和标准化预处理。

Result: 模型在验证中表现可靠，且能提升糖尿病视网膜病变分级的鲁棒性。

Conclusion: FundaQ-8框架和模型在临床应用中具有实用价值，强调了质量感知训练的重要性。

Abstract: Automated fundus image quality assessment (FIQA) remains a challenge due to
variations in image acquisition and subjective expert evaluations. We introduce
FundaQ-8, a novel expert-validated framework for systematically assessing
fundus image quality using eight critical parameters, including field coverage,
anatomical visibility, illumination, and image artifacts. Using FundaQ-8 as a
structured scoring reference, we develop a ResNet18-based regression model to
predict continuous quality scores in the 0 to 1 range. The model is trained on
1800 fundus images from real-world clinical sources and Kaggle datasets, using
transfer learning, mean squared error optimization, and standardized
preprocessing. Validation against the EyeQ dataset and statistical analyses
confirm the framework's reliability and clinical interpretability.
Incorporating FundaQ-8 into deep learning models for diabetic retinopathy
grading also improves diagnostic robustness, highlighting the value of
quality-aware training in real-world screening applications.

</details>


### [54] [Transformer Based Multi-Target Bernoulli Tracking for Maritime Radar](https://arxiv.org/abs/2506.20319)
*Caden Sweeney,Du Yong Kim,Branko Ristic,Brian Cheung*

Main category: eess.IV

TL;DR: 论文提出了一种基于Transformer和LMB滤波器的多目标跟踪方法，用于解决海杂波环境下的低信噪比目标检测与跟踪问题。


<details>
  <summary>Details</summary>
Motivation: 海杂波的非高斯和波动特性使得海上多目标跟踪成为挑战，尤其是低信噪比目标的检测与跟踪。

Method: 使用Transformer从距离-方位图中提取点测量，结合LMB滤波器进行聚类和跟踪，并开发了基于Transformer注意力图的测量驱动出生密度设计。

Result: 实验表明，基于Transformer的方法在所有目标场景中均优于传统的CFAR检测技术。

Conclusion: Transformer结合LMB滤波器的方法在海杂波环境下表现出优越性能，为低信噪比目标跟踪提供了有效解决方案。

Abstract: Multi-target tracking in the maritime domain is a challenging problem due to
the non-Gaussian and fluctuating characteristics of sea clutter. This article
investigates the use of machine learning (ML) to the detection and tracking of
low SIR targets in the maritime domain. The proposed method uses a transformer
to extract point measurements from range-azimuth maps, before clustering and
tracking using the Labelled mulit- Bernoulli (LMB) filter. A measurement driven
birth density design based on the transformer attention maps is also developed.
The error performance of the transformer based approach is presented and
compared with a constant false alarm rate (CFAR) detection technique. The LMB
filter is run in two scenarios, an ideal birth approach, and the measurement
driven birth approach. Experiments indicate that the transformer based method
has superior performance to the CFAR approach for all target scenarios
discussed

</details>


### [55] [EAGLE: An Efficient Global Attention Lesion Segmentation Model for Hepatic Echinococcosis](https://arxiv.org/abs/2506.20333)
*Jiayan Chen,Kai Li,Yulu Zhao,Jianqiang Huang,Zhan Wang*

Main category: eess.IV

TL;DR: 提出EAGLE网络，结合PVSS编码器和HVSS解码器，用于高效准确地分割肝包虫病病灶，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决肝包虫病在医疗资源匮乏地区的诊断问题，同时克服CNN和Transformer在医学图像分割中的局限性。

Method: 采用U形网络结构，结合PVSS编码器和HVSS解码器，引入CVSSB模块融合局部与全局特征，HWTB模块实现无损下采样。

Result: 在260例患者CT数据上，EAGLE的DSC达到89.76%，优于MSVM-UNet 1.61%。

Conclusion: EAGLE在肝包虫病病灶分割中表现出色，为资源匮乏地区提供了高效解决方案。

Abstract: Hepatic echinococcosis (HE) is a widespread parasitic disease in
underdeveloped pastoral areas with limited medical resources. While CNN-based
and Transformer-based models have been widely applied to medical image
segmentation, CNNs lack global context modeling due to local receptive fields,
and Transformers, though capable of capturing long-range dependencies, are
computationally expensive. Recently, state space models (SSMs), such as Mamba,
have gained attention for their ability to model long sequences with linear
complexity. In this paper, we propose EAGLE, a U-shaped network composed of a
Progressive Visual State Space (PVSS) encoder and a Hybrid Visual State Space
(HVSS) decoder that work collaboratively to achieve efficient and accurate
segmentation of hepatic echinococcosis (HE) lesions. The proposed Convolutional
Vision State Space Block (CVSSB) module is designed to fuse local and global
features, while the Haar Wavelet Transformation Block (HWTB) module compresses
spatial information into the channel dimension to enable lossless downsampling.
Due to the lack of publicly available HE datasets, we collected CT slices from
260 patients at a local hospital. Experimental results show that EAGLE achieves
state-of-the-art performance with a Dice Similarity Coefficient (DSC) of
89.76%, surpassing MSVM-UNet by 1.61%.

</details>


### [56] [Fusing Radiomic Features with Deep Representations for Gestational Age Estimation in Fetal Ultrasound Images](https://arxiv.org/abs/2506.20407)
*Fangyijie Wang,Yuan Liang,Sourav Bhattacharjee,Abey Campbell,Kathleen M. Curran,Guénolé Silvestre*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习和放射组学特征融合的框架，用于从胎儿超声图像中自动估计孕龄，无需手动测量，准确度优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手动测量孕龄依赖操作者且耗时，临床需要自动化的计算机辅助方法。

Method: 结合深度学习模型提取的深度表征和放射组学特征，通过特征融合估计孕龄。

Result: 在三孕期内的平均绝对误差为8.0天，优于现有机器学习方法，且在不同地理区域的人群中表现稳健。

Conclusion: 该框架为孕龄估计提供了一种高效、自动化的解决方案，具有临床实用性和推广潜力。

Abstract: Accurate gestational age (GA) estimation, ideally through fetal ultrasound
measurement, is a crucial aspect of providing excellent antenatal care.
However, deriving GA from manual fetal biometric measurements depends on the
operator and is time-consuming. Hence, automatic computer-assisted methods are
demanded in clinical practice. In this paper, we present a novel feature fusion
framework to estimate GA using fetal ultrasound images without any measurement
information. We adopt a deep learning model to extract deep representations
from ultrasound images. We extract radiomic features to reveal patterns and
characteristics of fetal brain growth. To harness the interpretability of
radiomics in medical imaging analysis, we estimate GA by fusing radiomic
features and deep representations. Our framework estimates GA with a mean
absolute error of 8.0 days across three trimesters, outperforming current
machine learning-based methods at these gestational ages. Experimental results
demonstrate the robustness of our framework across different populations in
diverse geographical regions. Our code is publicly available on
\href{https://github.com/13204942/RadiomicsImageFusion_FetalUS}{GitHub}.

</details>


### [57] [Papanicolaou Stain Unmixing for RGB Image Using Weighted Nucleus Sparsity and Total Variation Regularization](https://arxiv.org/abs/2506.20450)
*Nanxin Gong,Saori Takeyama,Masahiro Yamaguchi,Takumi Urata,Fumikazu Kimura,Keiko Ishii*

Main category: eess.IV

TL;DR: 提出了一种新的RGB图像Papanicolaou染色分离方法，通过优化问题实现染色量化，并在宫颈癌前病变诊断中取得高精度。


<details>
  <summary>Details</summary>
Motivation: Papanicolaou染色的颜色信息对宫颈癌筛查至关重要，但视觉观察主观且难以量化，RGB图像因染色和成像变化影响直接量化。

Method: 结合非负染色丰度、稀疏核分布和分段平滑假设，通过优化问题实现染色分离。

Result: 方法在染色量化上表现优异，并在区分宫颈癌前病变LEGH时达到98.0%准确率。

Conclusion: RGB染色分离方法在定量诊断中具有显著潜力。

Abstract: The Papanicolaou stain, consisting of eosin Y, hematoxylin, light Green SF
yellowish, orange G, and Bismarck brown Y, provides extensive color information
essential for cervical cancer screening in cytopathology. However, the visual
observation of these colors is subjective and difficult to characterize. In
digital image analysis, the RGB intensities are affected by staining and
imaging variations, hindering direct quantification of color in
Papanicolaou-stained samples. Stain unmixing is a promising alternative that
quantifies the amounts of dyes. In previous work, multispectral imaging was
utilized to estimate the dye amounts of Papanicolaou stain for quantitative
diagnosis. Still, its application to RGB images presents a challenge since the
number of dyes exceeds the three RGB channels. This paper proposes a novel
Papanicolaou stain unmixing method for RGB images that incorporates three key
assumptions: nonnegative stain abundances; a sparse spatial distribution of
hematoxylin, which binds to nuclei; and piecewise smoothness of stain
abundances. By formulating this as an optimization problem with nonnegativity,
weighted nucleus sparsity, and total variation regularizations, our method
achieved excellent performance in stain quantification when validated against
the results of multispectral imaging. We also adopted the proposed method for
discriminating lobular endocervical glandular hyperplasia (LEGH), a
precancerous lesion of gastric-type adenocarcinoma of the cervix. The resulting
quantification distinctly characterized differences between LEGH and normal
endocervical cells with stain abundance, and a classifier based on the
quantification results achieved 98.0% accuracy. This demonstrates the
significant potential of RGB-based stain unmixing for quantitative diagnosis.

</details>


### [58] [Weighted Mean Frequencies: a handcraft Fourier feature for 4D Flow MRI segmentation](https://arxiv.org/abs/2506.20614)
*Simon Perrin,Sébastien Levilly,Huajun Sun,Harold Mouchère,Jean-Michel Serfaty*

Main category: eess.IV

TL;DR: 提出了一种名为WMF的新特征，用于改善4D Flow MRI图像的分割效果，实验结果显示其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 4D Flow MRI图像的分辨率低和噪声问题影响了生物标志物的准确性，尤其是血管分割的分辨率不足。

Method: 引入WMF特征，通过加权平均频率揭示脉冲流经过的三维区域，并采用最优阈值和深度学习方法进行分割实验。

Result: WMF特征在IoU和Dice指标上分别提高了0.12和0.13，性能优于PC-MRA特征。

Conclusion: WMF特征有望为其他血管区域（如心脏或大脑）的分割提供有价值的参考。

Abstract: In recent decades, the use of 4D Flow MRI images has enabled the
quantification of velocity fields within a volume of interest and along the
cardiac cycle. However, the lack of resolution and the presence of noise in
these biomarkers are significant issues. As indicated by recent studies, it
appears that biomarkers such as wall shear stress are particularly impacted by
the poor resolution of vessel segmentation. The Phase Contrast Magnetic
Resonance Angiography (PC-MRA) is the state-of-the-art method to facilitate
segmentation. The objective of this work is to introduce a new handcraft
feature that provides a novel visualisation of 4D Flow MRI images, which is
useful in the segmentation task. This feature, termed Weighted Mean Frequencies
(WMF), is capable of revealing the region in three dimensions where a voxel has
been passed by pulsatile flow. Indeed, this feature is representative of the
hull of all pulsatile velocity voxels. The value of the feature under
discussion is illustrated by two experiments. The experiments involved
segmenting 4D Flow MRI images using optimal thresholding and deep learning
methods. The results obtained demonstrate a substantial enhancement in terms of
IoU and Dice, with a respective increase of 0.12 and 0.13 in comparison with
the PC-MRA feature, as evidenced by the deep learning task. This feature has
the potential to yield valuable insights that could inform future segmentation
processes in other vascular regions, such as the heart or the brain.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [59] [RaRa Clipper: A Clipper for Gaussian Splatting Based on Ray Tracer and Rasterizer](https://arxiv.org/abs/2506.20202)
*Da Li,Donggang Jia,Yousef Rajeh,Dominik Engel,Ivan Viola*

Main category: cs.GR

TL;DR: 提出了一种结合光栅化和光线追踪的混合渲染框架（RaRa策略），用于高效且高保真地对高斯泼溅数据进行裁剪。


<details>
  <summary>Details</summary>
Motivation: 高斯泼溅数据的裁剪问题因其体积特性难以精确定位像素级贡献，现有方法无法满足需求。

Method: 采用RaRa策略，先通过光栅化快速识别被裁剪平面相交的高斯，再用光线追踪计算部分遮挡的衰减权重，精确估计高斯对最终图像的贡献。

Result: 在多种数据集上验证了方法的有效性，用户研究表明其视觉质量高且能保持实时渲染性能。

Conclusion: 该方法实现了平滑连续的裁剪效果，同时在高保真和效率上表现优异。

Abstract: With the advancement of Gaussian Splatting techniques, a growing number of
datasets based on this representation have been developed. However, performing
accurate and efficient clipping for Gaussian Splatting remains a challenging
and unresolved problem, primarily due to the volumetric nature of Gaussian
primitives, which makes hard clipping incapable of precisely localizing their
pixel-level contributions. In this paper, we propose a hybrid rendering
framework that combines rasterization and ray tracing to achieve efficient and
high-fidelity clipping of Gaussian Splatting data. At the core of our method is
the RaRa strategy, which first leverages rasterization to quickly identify
Gaussians intersected by the clipping plane, followed by ray tracing to compute
attenuation weights based on their partial occlusion. These weights are then
used to accurately estimate each Gaussian's contribution to the final image,
enabling smooth and continuous clipping effects. We validate our approach on
diverse datasets, including general Gaussians, hair strand Gaussians, and
multi-layer Gaussians, and conduct user studies to evaluate both perceptual
quality and quantitative performance. Experimental results demonstrate that our
method delivers visually superior results while maintaining real-time rendering
performance and preserving high fidelity in the unclipped regions.

</details>


### [60] [X-SiT: Inherently Interpretable Surface Vision Transformers for Dementia Diagnosis](https://arxiv.org/abs/2506.20267)
*Fabian Bongratz,Tom Nuno Wolf,Jaume Gual Ramon,Christian Wachinger*

Main category: cs.GR

TL;DR: 论文提出了一种可解释的表面视觉变换器（X-SiT），用于医学图像分析，特别是脑部疾病的检测。


<details>
  <summary>Details</summary>
Motivation: 3D体积数据难以可视化，而皮质表面渲染更易理解，因此开发X-SiT以提供可解释的预测。

Method: X-SiT结合原型表面补丁解码器，通过案例推理和皮质原型进行分类。

Result: 在阿尔茨海默病和额颞叶痴呆检测中表现优异，并提供与疾病模式一致的原型。

Conclusion: X-SiT不仅性能优越，还能提供可解释的预测，有助于临床决策。

Abstract: Interpretable models are crucial for supporting clinical decision-making,
driving advances in their development and application for medical images.
However, the nature of 3D volumetric data makes it inherently challenging to
visualize and interpret intricate and complex structures like the cerebral
cortex. Cortical surface renderings, on the other hand, provide a more
accessible and understandable 3D representation of brain anatomy, facilitating
visualization and interactive exploration. Motivated by this advantage and the
widespread use of surface data for studying neurological disorders, we present
the eXplainable Surface Vision Transformer (X-SiT). This is the first
inherently interpretable neural network that offers human-understandable
predictions based on interpretable cortical features. As part of X-SiT, we
introduce a prototypical surface patch decoder for classifying surface patch
embeddings, incorporating case-based reasoning with spatially corresponding
cortical prototypes. The results demonstrate state-of-the-art performance in
detecting Alzheimer's disease and frontotemporal dementia while additionally
providing informative prototypes that align with known disease patterns and
reveal classification errors.

</details>


### [61] [DreamAnywhere: Object-Centric Panoramic 3D Scene Generation](https://arxiv.org/abs/2506.20367)
*Edoardo Alberto Dominici,Jozef Hladky,Floor Verhoeven,Lukas Radl,Thomas Deixelberger,Stefan Ainetter,Philipp Drescher,Stefan Hauswiesner,Arno Coomans,Giacomo Nazzaro,Konstantinos Vardis,Markus Steinberger*

Main category: cs.GR

TL;DR: DreamAnywhere是一个模块化系统，用于快速生成和原型化3D场景，解决了现有方法在视觉保真度、场景理解和多环境适应性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本到3D场景生成方法存在局限性，如仅支持正面视角、视觉保真度低、场景理解有限，且通常仅适用于室内或室外单一环境。

Method: 系统通过生成360度全景图像，分解为背景和对象，通过混合修复构建完整3D表示，并将对象掩码提升为详细3D对象。支持沉浸式导航和对象级编辑。

Result: DreamAnywhere在新视角合成一致性和图像质量上显著优于现有方法，用户研究显示其技术稳健性和实用性。

Conclusion: DreamAnywhere为低成本电影制作等场景提供了快速迭代和原型化的高效工具，模块化设计使其具有高度可定制性。

Abstract: Recent advances in text-to-3D scene generation have demonstrated significant
potential to transform content creation across multiple industries. Although
the research community has made impressive progress in addressing the
challenges of this complex task, existing methods often generate environments
that are only front-facing, lack visual fidelity, exhibit limited scene
understanding, and are typically fine-tuned for either indoor or outdoor
settings. In this work, we address these issues and propose DreamAnywhere, a
modular system for the fast generation and prototyping of 3D scenes. Our system
synthesizes a 360{\deg} panoramic image from text, decomposes it into
background and objects, constructs a complete 3D representation through hybrid
inpainting, and lifts object masks to detailed 3D objects that are placed in
the virtual environment. DreamAnywhere supports immersive navigation and
intuitive object-level editing, making it ideal for scene exploration, visual
mock-ups, and rapid prototyping -- all with minimal manual modeling. These
features make our system particularly suitable for low-budget movie production,
enabling quick iteration on scene layout and visual tone without the overhead
of traditional 3D workflows. Our modular pipeline is highly customizable as it
allows components to be replaced independently. Compared to current
state-of-the-art text and image-based 3D scene generation approaches,
DreamAnywhere shows significant improvements in coherence in novel view
synthesis and achieves competitive image quality, demonstrating its
effectiveness across diverse and challenging scenarios. A comprehensive user
study demonstrates a clear preference for our method over existing approaches,
validating both its technical robustness and practical usefulness.

</details>


### [62] [EditP23: 3D Editing via Propagation of Image Prompts to Multi-View](https://arxiv.org/abs/2506.20652)
*Roi Bar-On,Dana Cohen-Bar,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: EditP23是一种无需掩码的3D编辑方法，通过2D图像编辑传播到多视角表示，实现3D一致性编辑。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖文本提示或显式空间掩码，EditP23通过一对图像（原始视图和用户编辑后的视图）实现直观编辑，简化了3D编辑流程。

Method: 利用预训练的多视角扩散模型的潜在空间，通过编辑感知流引导编辑，实现跨视图的一致性传播，无需优化，前馈式操作。

Result: 在多种对象类别和编辑场景中表现优异，保持原始对象的结构和外观，且无需手动掩码。

Conclusion: EditP23提供了一种高效、直观的3D编辑方法，适用于广泛的应用场景。

Abstract: We present EditP23, a method for mask-free 3D editing that propagates 2D
image edits to multi-view representations in a 3D-consistent manner. In
contrast to traditional approaches that rely on text-based prompting or
explicit spatial masks, EditP23 enables intuitive edits by conditioning on a
pair of images: an original view and its user-edited counterpart. These image
prompts are used to guide an edit-aware flow in the latent space of a
pre-trained multi-view diffusion model, allowing the edit to be coherently
propagated across views. Our method operates in a feed-forward manner, without
optimization, and preserves the identity of the original object, in both
structure and appearance. We demonstrate its effectiveness across a range of
object categories and editing scenarios, achieving high fidelity to the source
while requiring no manual masks.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [63] [Broadband Dielectric Analysis of Clays: Impact of Cation, Exchange Capacity, Water Content, and Porosity](https://arxiv.org/abs/2506.20271)
*Felix Schmidt,Norman Wagner,Ines Mulder,Katja Emmerich,Thierry Bore,Jan Bumberger*

Main category: physics.geo-ph

TL;DR: 研究了四种水饱和黏土矿物在1 MHz至5 GHz频率范围内的宽带介电弛豫行为，通过实验和模型分析，揭示了黏土矿物学与介电特性的关系。


<details>
  <summary>Details</summary>
Motivation: 黏土在近地表系统中对水保留、离子交换和结构稳定性具有重要影响，其复杂的介电行为与矿物表面电荷和可交换阳离子的相互作用有关。研究旨在通过介电弛豫行为揭示黏土的物理化学特性。

Method: 使用同轴探针测量四种水饱和黏土矿物（高岭石、伊利石和两种钠活化膨润土）的介电谱，并采用GDR、CPCM、ABC-M和CRIM四种模型进行参数化分析。

Result: 结果显示黏土矿物学与介电谱特征显著相关，特别是低频范围。弛豫参数与阳离子交换容量（CEC）密切相关，膨润土因离子交换动力学表现出更强的弛豫。

Conclusion: 研究为黏土矿物物理学与电磁方法的结合提供了框架，对土壤表征、水文建模和地质工程评估具有重要应用价值。

Abstract: Clay-rich soils and sediments are key components of near-surface systems,
influencing water retention, ion exchange, and structural stability. Their
complex dielectric behavior under moist conditions arises from electrostatic
interactions between charged mineral surfaces and exchangeable cations, forming
diffuse double layers that govern transport and retention processes. This study
investigates the broadband dielectric relaxation of four water-saturated clay
minerals (kaolin, illite, and two sodium-activated bentonites) in the 1 MHz to
5 GHz frequency range using coaxial probe measurements.
  The dielectric spectra were parameterized using two phenomenological models -
the Generalized Dielectric Relaxation Model (GDR) and the Combined Permittivity
and Conductivity Model (CPCM) - alongside two theoretical mixture models: the
Augmented Broadband Complex Dielectric Mixture Model (ABC-M) and the Complex
Refractive Index Model (CRIM). These approaches were evaluated for their
ability to link dielectric relaxation behavior to petrophysical parameters such
as cation exchange capacity (CEC), volumetric water content (VWC), and
porosity.
  The results show distinct spectral signatures correlating with clay
mineralogy, particularly in the low-frequency range. Relaxation parameters,
including relaxation strength and apparent DC conductivity, exhibit strong
relationships with CEC, emphasizing the influence of clay-specific surface
properties. Expansive clays like bentonites showed enhanced relaxation due to
ion exchange dynamics, while deviations in a soda-activated bentonite
highlighted the impact of chemical treatments on dielectric behavior.
  This study provides a framework for linking clay mineral physics with
electromagnetic methods, with implications for soil characterization,
hydrological modeling, geotechnical assessment, and environmental monitoring.

</details>


### [64] [Inhomogeneous plane waves in attenuative anisotropic porous media](https://arxiv.org/abs/2506.20389)
*Lingli Gao,Weijian Mao,Qianru Xu,Wei Ouyang,Shaokang Yang,Shijun Cheng*

Main category: physics.geo-ph

TL;DR: 研究了非均匀平面波在孔隙黏弹性介质中的传播，结合速度和衰减各向异性，提出了新的Christoffel方程和能量平衡方程，并分析了两种不同的耗散因子定义。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖复波矢量，本研究基于复慢度矢量，为孔隙黏弹性各向异性介质中的波传播提供更清晰的描述。

Method: 从经典Biot理论出发，推导了分数阶微分方程，并求解了八次代数方程以确定波速和复慢度。

Result: 得出了能量速度的显式表达式，并分析了两种耗散因子的定义差异。

Conclusion: 研究结果与现有理论一致，并通过实例验证了非均匀平面波的传播特性。

Abstract: We investigate the propagation of inhomogeneous plane waves in
poro-viscoelastic media, explicitly incorporating both velocity and attenuation
anisotropy. Starting from classical Biot theory, we present a fractional
differential equation describing wave propagation in attenuative anisotropic
porous media that accommodates arbitrary anisotropy in both velocity and
attenuation. Then, instead of relying on the traditional complex wave vector
approach, we derive new Christoffel and energy balance equations for general
inhomogeneous waves by employing an alternative formulation based on the
complex slowness vector. The phase velocities and complex slownesses of
inhomogeneous fast and slow quasi-compressional (qP1 and qP2) and quasi-shear
(qS1 and qS2) waves are determined by solving an eighth-degree algebraic
equation. By invoking the derived energy balance equation along with the
computed complex slowness, we present explicit and concise expressions for
energy velocities. Additionally, we analyze dissipation factors defined by two
alternative measures: the ratio of average dissipated energy density to either
average strain energy density or average stored energy density. We clarify and
discuss the implications of these definitional differences in the context of
general poro-viscoelastic anisotropic media. Finally, our expressions are
degenerated to give their counterparts of the homogeneous waves as a special
case, and the reduced forms are identical to those presented by the existing
poro-viscoelastic theory. Several examples are provided to illustrate the
propagation characteristics of inhomogeneous plane waves in unbounded
attenuative vertical transversely isotropic porous media.

</details>


### [65] [Fast ground penetrating radar dual-parameter full waveform inversion method accelerated by hybrid compilation of CUDA kernel function and PyTorch](https://arxiv.org/abs/2506.20513)
*Lei Liu,Chao Song,Liangsheng He,Silin Wang,Xuan Feng,Cai Liu*

Main category: physics.geo-ph

TL;DR: 提出了一种高性能的双参数全波形反演框架（FWI），通过CUDA内核函数和PyTorch的混合编译加速，适用于探地雷达（GPR）数据。


<details>
  <summary>Details</summary>
Motivation: 结合GPU的计算效率和Python深度学习框架的灵活性，实现高效且准确的双参数反演。

Method: 通过将定制的CUDA内核集成到PyTorch的自动微分机制中，反演介电常数和电导率。

Result: 在合成数据和实际波场数据上的实验表明，该方法能高精度完成双参数FWI，并支持正则化策略。

Conclusion: 该框架灵活且可扩展，适用于快速地下成像，应用领域广泛。

Abstract: This study proposes a high-performance dual-parameter full waveform inversion
framework (FWI) for ground-penetrating radar (GPR), accelerated through the
hybrid compilation of CUDA kernel functions and PyTorch. The method leverages
the computational efficiency of GPU programming while preserving the
flexibility and usability of Python-based deep learning frameworks. By
integrating customized CUDA kernels into PyTorch's automatic differentiation
mechanism, the framework enables accurate and efficient inversion of both
dielectric permittivity and electrical conductivity. Experimental evaluations
on synthetic data and real wavefield data demonstrate that the proposed method
achieves dual-parameter FWI for GPR data while maintaining high accuracy.
Moreover, the framework is flexible and extensible, supporting optional
regularization strategies such as total variation and multi-scale inversion.
These features make the proposed approach a practical and scalable framework
for rapid GPR-based subsurface imaging in applications including civil
engineering, environmental monitoring, and geophysical exploration.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [66] [Consensus-Driven Uncertainty for Robotic Grasping based on RGB Perception](https://arxiv.org/abs/2506.20045)
*Eric C. Joyce,Qianwen Zhao,Nathaniel Burgdorfer,Long Wang,Philippos Mordohai*

Main category: cs.RO

TL;DR: 论文提出了一种训练轻量级深度网络的方法，用于预测基于图像姿态估计的抓取是否成功，并通过真实图像和模拟抓取生成训练数据。


<details>
  <summary>Details</summary>
Motivation: 解决深度物体姿态估计器过度自信的问题，避免在高不确定性下执行抓取任务导致失败。

Method: 通过真实图像的物体姿态估计和模拟抓取生成训练数据，训练网络预测抓取成功率。

Result: 研究发现，尽管抓取试验中物体变异性高，但网络通过联合训练所有物体仍能受益。

Conclusion: 多样化的物体数据可以共同促进抓取成功率的预测目标。

Abstract: Deep object pose estimators are notoriously overconfident. A grasping agent
that both estimates the 6-DoF pose of a target object and predicts the
uncertainty of its own estimate could avoid task failure by choosing not to act
under high uncertainty. Even though object pose estimation improves and
uncertainty quantification research continues to make strides, few studies have
connected them to the downstream task of robotic grasping. We propose a method
for training lightweight, deep networks to predict whether a grasp guided by an
image-based pose estimate will succeed before that grasp is attempted. We
generate training data for our networks via object pose estimation on real
images and simulated grasping. We also find that, despite high object
variability in grasping trials, networks benefit from training on all objects
jointly, suggesting that a diverse variety of objects can nevertheless
contribute to the same goal.

</details>


### [67] [HRIBench: Benchmarking Vision-Language Models for Real-Time Human Perception in Human-Robot Interaction](https://arxiv.org/abs/2506.20566)
*Zhonghao Shi,Enyu Zhao,Nathaniel Dennler,Jingzhen Wang,Xinyang Xu,Kaleen Shrestha,Mengxue Fu,Daniel Seita,Maja Matarić*

Main category: cs.RO

TL;DR: HRIBench是一个用于评估视觉语言模型（VLMs）在人类感知任务中性能和延迟权衡的基准测试，覆盖五个关键领域。研究发现当前VLMs在实时部署中表现不足。


<details>
  <summary>Details</summary>
Motivation: 研究VLMs在人类感知任务中的能力和性能-延迟权衡，以提升人机交互（HRI）的实时性。

Method: 构建HRIBench基准测试，包含1000个视觉问答（VQA）问题，涵盖五个领域，并评估11种VLMs。

Result: 当前VLMs在核心感知能力上表现不足，且缺乏适合实时部署的性能-延迟权衡。

Conclusion: 需要开发更小、低延迟的VLMs以提升实时人类感知能力。

Abstract: Real-time human perception is crucial for effective human-robot interaction
(HRI). Large vision-language models (VLMs) offer promising generalizable
perceptual capabilities but often suffer from high latency, which negatively
impacts user experience and limits VLM applicability in real-world scenarios.
To systematically study VLM capabilities in human perception for HRI and
performance-latency trade-offs, we introduce HRIBench, a visual
question-answering (VQA) benchmark designed to evaluate VLMs across a diverse
set of human perceptual tasks critical for HRI. HRIBench covers five key
domains: (1) non-verbal cue understanding, (2) verbal instruction
understanding, (3) human-robot object relationship understanding, (4) social
navigation, and (5) person identification. To construct HRIBench, we collected
data from real-world HRI environments to curate questions for non-verbal cue
understanding, and leveraged publicly available datasets for the remaining four
domains. We curated 200 VQA questions for each domain, resulting in a total of
1000 questions for HRIBench. We then conducted a comprehensive evaluation of
both state-of-the-art closed-source and open-source VLMs (N=11) on HRIBench.
Our results show that, despite their generalizability, current VLMs still
struggle with core perceptual capabilities essential for HRI. Moreover, none of
the models within our experiments demonstrated a satisfactory
performance-latency trade-off suitable for real-time deployment, underscoring
the need for future research on developing smaller, low-latency VLMs with
improved human perception capabilities. HRIBench and our results can be found
in this Github repository: https://github.com/interaction-lab/HRIBench.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [68] [The role of audio-visual integration in the time course of phonetic encoding in self-supervised speech models](https://arxiv.org/abs/2506.20361)
*Yi Wang,Oli Danyi Liu,Peter Bell*

Main category: eess.AS

TL;DR: AV-HuBERT模型未能充分捕捉多模态语音感知中的时间动态，限制了其对人类多模态语音感知过程的模拟能力。


<details>
  <summary>Details</summary>
Motivation: 研究人类语音感知的多模态特性，尤其是音频与视觉线索之间的异步性，并探讨自监督学习模型是否能捕捉这种异步性。

Method: 比较AV-HuBERT（音频-视觉模型）与音频-only HuBERT，通过线性分类器追踪其语音解码能力随时间的变化。

Result: AV-HuBERT的语音信息解码时间仅比HuBERT提前约20毫秒，表明其未能充分模拟人类多模态感知的时间动态。

Conclusion: AV-HuBERT在多模态语音感知的时间动态建模上存在不足，需进一步改进。

Abstract: Human speech perception is multimodal. In natural speech, lip movements can
precede corresponding voicing by a non-negligible gap of 100-300 ms, especially
for specific consonants, affecting the time course of neural phonetic encoding
in human listeners. However, it remains unexplored whether self-supervised
learning models, which have been used to simulate audio-visual integration in
humans, can capture this asynchronicity between audio and visual cues. We
compared AV-HuBERT, an audio-visual model, with audio-only HuBERT, by using
linear classifiers to track their phonetic decodability over time. We found
that phoneme information becomes available in AV-HuBERT embeddings only about
20 ms before HuBERT, likely due to AV-HuBERT's lower temporal resolution and
feature concatenation process. It suggests AV-HuBERT does not adequately
capture the temporal dynamics of multimodal speech perception, limiting its
suitability for modeling the multimodal speech perception process.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [69] [SPP location with spherical ray tracing by refractive index](https://arxiv.org/abs/2506.19859)
*Hui Qian,Xiaosan Zhu,Dongliang Liu*

Main category: physics.ao-ph

TL;DR: 论文提出了一种基于球坐标折射传播时间的定位方法，替代传统的直线传播假设，通过线性化LSQR方法反演站坐标和电磁波速度，提高了单点卫星定位精度。


<details>
  <summary>Details</summary>
Motivation: 大气层结构是影响单点卫星定位精度的主要因素，直线传播假设限制了电离层和对流层校正的准确性，而曲线传播方法能更全面地考虑射线偏折。

Method: 通过建立基于坐标的一维分层速度模型下的电磁波传播时间方程，推导理论传播时间对纬度、经度、高程和速度模型的偏导数公式，并利用线性化LSQR方法反演站坐标、接收机钟差及电磁波速度。

Result: 提出了一种新的球坐标折射传播时间定位方法，取代了传统的伪距校正，并通过实验比较了直线传播与曲线传播方法的定位精度差异。

Conclusion: 结合经典电离层和对流层模型构建折射率模型，并与曲线射线追踪方法结合，可以有效缓解定位的理论局限性。

Abstract: Atmospheric layer structure is a primary factor affecting the precision of
single-point satellite positioning. The assumption of electromagnetic wave
rectilinear propagation hinders the accurate implementation of ionospheric and
tropospheric corrections, whereas curvilinear positioning methods fully account
for ray deflection. This study aims to derive partial derivative formulas for
theoretical travel time with respect to latitude, longitude, elevation, and
velocity models by formulating electromagnetic wave travel time equations under
a coordinate-based one-dimensional layered velocity model. Subsequently, a
linearized LSQR method is employed to invert station coordinates, receiver
clock biases, and electromagnetic wave velocities at the bottom of the
ionosphere and troposphere using over six sets of observations. This replaces
conventional ionospheric/tropospheric pseudorange corrections in single-point
positioning, establishing a novel spherical coordinate refraction travel time
positioning method. The classical straight-line pseudorange positioning is
reformulated into a time-of-flight positioning approach, and the positioning
accuracy differences between straight-line and spherical coordinate refraction
travel time methods are compared. By integrating classical ionospheric and
tropospheric models to construct corresponding refractive index models and
combining them with curvilinear ray tracing methods, the inherent theoretical
limitations of positioning can be effectively mitigated.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [70] [Distillation-Enabled Knowledge Alignment for Generative Semantic Communications in AIGC Provisioning Tasks](https://arxiv.org/abs/2506.19893)
*Jingzhi Hu,Geoffrey Ye Li*

Main category: cs.LG

TL;DR: 论文提出DeKA-g算法，通过知识蒸馏和自适应传输优化生成语义通信（GSC），提升边缘生成内容与云端生成内容的一致性。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容（AIGC）的激增，从云端向边缘和移动用户传输AIGC导致网络流量剧增。生成语义通信（GSC）通过传输紧凑信息（如提示文本和潜在表示）提供解决方案，但知识对齐问题仍具挑战性。

Method: 提出DeKA-g算法，包含元词辅助知识蒸馏（MAKD）和可变速率分组SNR自适应（VGSA）两种方法，将云端生成知识蒸馏为低秩矩阵，并自适应传输知识以适应不同无线信道条件。

Result: DeKA-g将边缘生成图像与云端生成图像的对齐度提升44%，压缩率适应效率比基线高116%，在低SNR条件下性能提升28%。

Conclusion: DeKA-g通过知识蒸馏和自适应传输有效解决了GSC中的知识对齐问题，显著提升了系统性能。

Abstract: Due to the surging amount of AI-generated content (AIGC), its provisioning to
edges and mobile users from the cloud incurs substantial traffic on networks.
Generative semantic communication (GSC) offers a promising solution by
transmitting highly compact information, i.e., prompt text and latent
representations, instead of high-dimensional AIGC data. However, GSC relies on
the alignment between the knowledge in the cloud generative AI (GAI) and that
possessed by the edges and users, and between the knowledge for wireless
transmission and that of actual channels, which remains challenging. In this
paper, we propose DeKA-g, a distillation-enabled knowledge alignment algorithm
for GSC systems. The core idea is to distill the generation knowledge from the
cloud-GAI into low-rank matrices, which can be incorporated by the edge and
used to adapt the transmission knowledge to diverse wireless channel
conditions. DeKA-g comprises two novel methods: metaword-aided knowledge
distillation (MAKD) and variable-rate grouped SNR adaptation (VGSA). For MAKD,
an optimized metaword is employed to enhance the efficiency of knowledge
distillation, while VGSA enables efficient adaptation to diverse compression
rates and SNR ranges. From simulation results, DeKA-g improves the alignment
between the edge-generated images and the cloud-generated ones by 44%.
Moreover, it adapts to compression rates with 116% higher efficiency than the
baseline and enhances the performance in low-SNR conditions by 28%.

</details>


### [71] [Any-Order GPT as Masked Diffusion Model: Decoupling Formulation and Architecture](https://arxiv.org/abs/2506.19935)
*Shuchen Xue,Tianyu Xie,Tianyang Hu,Zijin Feng,Jiacheng Sun,Kenji Kawaguchi,Zhenguo Li,Zhi-Ming Ma*

Main category: cs.LG

TL;DR: 研究比较了自回归（AR）和掩码扩散模型（MDM）在解码器框架下的表现，发现解码器MDM在生成速度和困惑度上优于编码器MDM，同时探讨了目标函数的优化。


<details>
  <summary>Details</summary>
Motivation: 比较AR和MDM模型时，架构差异（解码器vs编码器）导致不公平对比，研究旨在分离范式与架构的影响。

Method: 在解码器框架下评估MDM，将其视为任意顺序AR（AO-AR），并比较标准AR范式。

Result: 解码器MDM在生成速度上显著提升（约25倍），困惑度与编码器MDM相当，但目标函数需优化。

Conclusion: 研究揭示了范式与架构的独立影响，为未来模型设计提供参考。

Abstract: Large language models (LLMs) predominantly use autoregressive (AR)
approaches, but masked diffusion models (MDMs) are emerging as viable
alternatives. A key challenge in comparing AR and MDM paradigms is their
typical architectural difference: AR models are often decoder-only, while MDMs
have largely been encoder-only. This practice of changing both the modeling
paradigm and architecture simultaneously makes direct comparisons unfair, as
it's hard to distinguish whether observed differences stem from the paradigm
itself or the architectural shift. This research evaluates MDMs within a
decoder-only framework to: (1) equitably compare MDM (as Any-Order AR, or
AO-AR) and standard AR paradigms. Our investigation suggests that the standard
AO-AR objective, which averages over all token permutations, may benefit from
refinement, as many permutations appear less informative compared to the
language's inherent left-to-right structure. (2) Investigate architectural
influences (decoder-only vs. encoder-only) within MDMs. We demonstrate that
while encoder-only MDMs model a simpler conditional probability space,
decoder-only MDMs can achieve dramatic generation speedups ($\sim25\times$) and
comparable perplexity with temperature annealing despite modeling a vastly
larger space, highlighting key trade-offs. This work thus decouples core
paradigm differences from architectural influences, offering insights for
future model design. Code is available at https://github.com/scxue/AO-GPT-MDM.

</details>


### [72] [MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations](https://arxiv.org/abs/2506.20100)
*Vardhan Dongre,Chi Gui,Shubham Garg,Hooshang Nayyeri,Gokhan Tur,Dilek Hakkani-Tür,Vikram S. Adve*

Main category: cs.LG

TL;DR: MIRAGE是一个新的多模态基准测试，专注于农业领域的专家级推理和决策，结合了自然用户查询、专家回答和图像上下文，用于评估模型在真实世界知识密集型任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常依赖明确输入和封闭分类，而MIRAGE旨在解决开放世界场景中未明确、上下文丰富的交互需求，填补了多模态模型在真实世界应用中的评估空白。

Method: 基于35,000多个真实用户-专家交互数据，通过多步骤流程构建，涵盖作物健康、害虫诊断和管理场景，包含7,000多个生物实体。

Result: MIRAGE成为目前分类最多样化的视觉语言模型基准之一，支持开放世界场景下的推理和长文本生成能力评估。

Conclusion: MIRAGE为多模态模型在复杂、知识密集型任务中的性能评估提供了高保真基准，推动了开放世界场景下的模型发展。

Abstract: We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning
and decision-making in consultative interaction settings. Designed for the
agriculture domain, MIRAGE captures the full complexity of expert consultations
by combining natural user queries, expert-authored responses, and image-based
context, offering a high-fidelity benchmark for evaluating models on grounded
reasoning, clarification strategies, and long-form generation in a real-world,
knowledge-intensive domain. Grounded in over 35,000 real user-expert
interactions and curated through a carefully designed multi-step pipeline,
MIRAGE spans diverse crop health, pest diagnosis, and crop management
scenarios. The benchmark includes more than 7,000 unique biological entities,
covering plant species, pests, and diseases, making it one of the most
taxonomically diverse benchmarks available for vision-language models, grounded
in the real world. Unlike existing benchmarks that rely on well-specified user
inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich
scenarios with open-world settings, requiring models to infer latent knowledge
gaps, handle rare entities, and either proactively guide the interaction or
respond. Project Page: https://mirage-benchmark.github.io

</details>


### [73] [FedBKD: Distilled Federated Learning to Embrace Gerneralization and Personalization on Non-IID Data](https://arxiv.org/abs/2506.20245)
*Yushan Zhao,Jinyuan He,Donglai Chen,Weijie Luo,Chong Xie,Ri Zhang,Yonghong Chen,Yan Xu*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedBKD的无数据蒸馏框架，通过生成对抗网络（GAN）生成合成数据，实现全局和局部模型之间的双向知识蒸馏，以解决联邦学习中的非独立同分布数据问题。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中非独立同分布数据的挑战，同时避免引入公共数据集带来的隐私泄露风险。

Method: 使用GAN生成合成数据，局部模型作为判别器，参数冻结。合成数据用于全局和局部模型之间的双向知识蒸馏。

Result: 在4个基准测试中，FedBKD在不同非独立同分布设置下均达到最优性能。

Conclusion: FedBKD能够同时提升全局和局部模型的性能，且无需依赖公共数据集，有效解决了非独立同分布数据问题。

Abstract: Federated learning (FL) is a decentralized collaborative machine learning
(ML) technique. It provides a solution to the issues of isolated data islands
and data privacy leakage in industrial ML practices. One major challenge in FL
is handling the non-identical and independent distributed (non-IID) data.
Current solutions either focus on constructing an all-powerful global model, or
customizing personalized local models. Few of them can provide both a
well-generalized global model and well-performed local models at the same time.
Additionally, many FL solutions to the non-IID problem are benefited from
introducing public datasets. However, this will also increase the risk of data
leakage. To tackle the problems, we propose a novel data-free distillation
framework, Federated Bidirectional Knowledge Distillation (FedBKD).
Specifically, we train Generative Adversarial Networks (GAN) for synthetic
data. During the GAN training, local models serve as discriminators and their
parameters are frozen. The synthetic data is then used for bidirectional
distillation between global and local models to achieve knowledge interactions
so that performances for both sides are improved. We conduct extensive
experiments on 4 benchmarks under different non-IID settings. The results show
that FedBKD achieves SOTA performances in every case.

</details>


### [74] [Learning Moderately Input-Sensitive Functions: A Case Study in QR Code Decoding](https://arxiv.org/abs/2506.20305)
*Kazuki Yoda,Kazuhiko Kawamoto,Hiroshi Kera*

Main category: cs.LG

TL;DR: 该研究首次基于学习的方法实现QR码解码，探讨中等敏感度函数的学习能力，发现Transformer模型能突破理论纠错限制，并泛化到不同语言和随机字符串。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索输入敏感度对学习函数的影响，特别是中等敏感度任务（如QR码解码）的学习能力。

Method: 采用Transformer模型进行QR码解码实验，分析其对数据位和纠错位的关注差异。

Result: 实验表明Transformer能成功解码QR码，甚至超越理论纠错限制，并展现出对数据位的专注。

Conclusion: Transformer在QR码解码中表现出独特机制，为中等敏感度任务的学习提供了新视角。

Abstract: The hardness of learning a function that attains a target task relates to its
input-sensitivity. For example, image classification tasks are
input-insensitive as minor corruptions should not affect the classification
results, whereas arithmetic and symbolic computation, which have been recently
attracting interest, are highly input-sensitive as each input variable connects
to the computation results. This study presents the first learning-based Quick
Response (QR) code decoding and investigates learning functions of medium
sensitivity. Our experiments reveal that Transformers can successfully decode
QR codes, even beyond the theoretical error-correction limit, by learning the
structure of embedded texts. They generalize from English-rich training data to
other languages and even random strings. Moreover, we observe that the
Transformer-based QR decoder focuses on data bits while ignoring
error-correction bits, suggesting a decoding mechanism distinct from standard
QR code readers.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [75] [An Agentic System for Rare Disease Diagnosis with Traceable Reasoning](https://arxiv.org/abs/2506.20430)
*Weike Zhao,Chaoyi Wu,Yanjie Fan,Xiaoman Zhang,Pengcheng Qiu,Yuze Sun,Xiao Zhou,Yanfeng Wang,Ya Zhang,Yongguo Yu,Kun Sun,Weidi Xie*

Main category: cs.CL

TL;DR: DeepRare是一种基于大型语言模型（LLM）的罕见病诊断系统，通过模块化设计整合多种工具和知识源，显著提升了诊断准确性和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 罕见病诊断因临床异质性、低发病率和医生知识有限而困难，亟需高效、透明的诊断工具。

Method: 系统由中央主机、长期记忆模块和多个专业代理服务器组成，整合40多种工具和最新医学知识。

Result: 在8个数据集上测试，DeepRare对1013种疾病实现100%准确率，显著优于其他15种方法，临床专家验证推理链一致性达95.40%。

Conclusion: DeepRare展示了罕见病诊断的高效性和透明性，已部署为易用的网络应用。

Abstract: Rare diseases collectively affect over 300 million individuals worldwide, yet
timely and accurate diagnosis remains a pervasive challenge. This is largely
due to their clinical heterogeneity, low individual prevalence, and the limited
familiarity most clinicians have with rare conditions. Here, we introduce
DeepRare, the first rare disease diagnosis agentic system powered by a large
language model (LLM), capable of processing heterogeneous clinical inputs. The
system generates ranked diagnostic hypotheses for rare diseases, each
accompanied by a transparent chain of reasoning that links intermediate
analytic steps to verifiable medical evidence.
  DeepRare comprises three key components: a central host with a long-term
memory module; specialized agent servers responsible for domain-specific
analytical tasks integrating over 40 specialized tools and web-scale,
up-to-date medical knowledge sources, ensuring access to the most current
clinical information. This modular and scalable design enables complex
diagnostic reasoning while maintaining traceability and adaptability. We
evaluate DeepRare on eight datasets. The system demonstrates exceptional
diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013
diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15
methods, like traditional bioinformatics diagnostic tools, LLMs, and other
agentic systems, achieving an average Recall@1 score of 57.18% and surpassing
the second-best method (Reasoning LLM) by a substantial margin of 23.79
percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at
Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of
reasoning chains by clinical experts achieves 95.40% agreements. Furthermore,
the DeepRare system has been implemented as a user-friendly web application
http://raredx.cn/doctor.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [76] [Learning-based safety lifting monitoring system for cranes on construction sites](https://arxiv.org/abs/2506.20475)
*Hao Chen,Yu Hin Ng,Ching-Wei Chang,Haobo Liang,Yanke Wang*

Main category: eess.SY

TL;DR: 提出了一种基于学习方法的自动化安全吊装监控算法，用于减少模块化集成建筑（MiC）吊装中的安全风险，并通过实时警报和3D定位提高效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 模块化集成建筑（MiC）吊装因其重量和体积大，存在安全风险，可能导致事故或人员伤害，因此需要自动化监控技术来提升安全性。

Method: 设计了一个基于学习的算法流程，结合2D目标检测和点云信息，实现MiC和人员的3D定位，并自动触发警报。

Result: 算法在MiC和人员的感知上表现良好，平均距离误差分别为1.5640米和0.7824米，成功实现了安全风险监控和警报功能。

Conclusion: 该系统有效减少了人工干预，提升了MiC吊装的安全性和效率，适用于实际工地。

Abstract: Lifting on construction sites, as a frequent operation, works still with
safety risks, especially for modular integrated construction (MiC) lifting due
to its large weight and size, probably leading to accidents, causing damage to
the modules, or more critically, posing safety hazards to on-site workers.
Aiming to reduce the safety risks in lifting scenarios, we design an automated
safe lifting monitoring algorithm pipeline based on learning-based methods, and
deploy it on construction sites. This work is potentially to increase the
safety and efficiency of MiC lifting process via automation technologies. A
dataset is created consisting of 1007 image-point cloud pairs (37 MiC
liftings). Advanced object detection models are trained for automated
two-dimensional (2D) detection of MiCs and humans. Fusing the 2D detection
results with the point cloud information allows accurate determination of the
three-dimensional (3D) positions of MiCs and humans. The system is designed to
automatically trigger alarms that notify individuals in the MiC lifting danger
zone, while providing the crane operator with real-time lifting information and
early warnings. The monitoring process minimizes the human intervention and no
or less signal men are required on real sites assisted by our system. A
quantitative analysis is conducted to evaluate the effectiveness of the
algorithmic pipeline. The pipeline shows promising results in MiC and human
perception with the mean distance error of 1.5640 m and 0.7824 m respectively.
Furthermore, the developed system successfully executes safety risk monitoring
and alarm functionalities during the MiC lifting process with limited manual
work on real construction sites.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [77] [A Multi-Modal Spatial Risk Framework for EV Charging Infrastructure Using Remote Sensing](https://arxiv.org/abs/2506.19860)
*Oktay Karakuş,Padraig Corcoran*

Main category: eess.SP

TL;DR: 该论文提出了RSERI-EV框架，结合多源数据和空间图分析，评估电动汽车充电站在环境和基础设施压力下的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 电动汽车充电基础设施对可持续交通系统至关重要，但其在环境和基础设施压力下的韧性尚未充分研究。

Method: 结合遥感数据、开放基础设施数据集和空间图分析，构建多模态风险评估框架RSERI-EV，生成综合韧性评分。

Result: 在威尔士的电动汽车充电站数据集上验证了框架的可行性，展示了多源数据融合和空间推理的价值。

Conclusion: RSERI-EV框架支持气候韧性和基础设施感知的电动汽车充电站部署。

Abstract: Electric vehicle (EV) charging infrastructure is increasingly critical to
sustainable transport systems, yet its resilience under environmental and
infrastructural stress remains underexplored. In this paper, we introduce
RSERI-EV, a spatially explicit and multi-modal risk assessment framework that
combines remote sensing data, open infrastructure datasets, and spatial graph
analytics to evaluate the vulnerability of EV charging stations. RSERI-EV
integrates diverse data layers, including flood risk maps, land surface
temperature (LST) extremes, vegetation indices (NDVI), land use/land cover
(LULC), proximity to electrical substations, and road accessibility to generate
a composite Resilience Score. We apply this framework to the country of Wales
EV charger dataset to demonstrate its feasibility. A spatial $k$-nearest
neighbours ($k$NN) graph is constructed over the charging network to enable
neighbourhood-based comparisons and graph-aware diagnostics. Our prototype
highlights the value of multi-source data fusion and interpretable spatial
reasoning in supporting climate-resilient, infrastructure-aware EV deployment.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [78] [Practical insights on the effect of different encodings, ansätze and measurements in quantum and hybrid convolutional neural networks](https://arxiv.org/abs/2506.20355)
*Jesús Lozano-Cruz,Albert Nieto-Morales,Oriol Balló-Gimbernat,Adan Garriga,Antón Rodríguez-Otero,Alejandro Borrallo-Rentero*

Main category: quant-ph

TL;DR: 研究了参数化量子电路（PQCs）在量子卷积神经网络（QCNN）和混合量子-经典神经网络（HQNN）中的设计选择，用于卫星图像分类任务。发现数据编码策略对混合架构性能影响最大，而变分结构和测量选择影响较小。纯量子模型中，测量协议和数据编码映射是关键。


<details>
  <summary>Details</summary>
Motivation: 探讨PQCs在量子神经网络中的设计选择对性能的影响，特别是在卫星图像分类任务中。

Method: 系统评估了约500种不同模型配置的数据编码技术、变分结构和测量方法。

Result: 混合架构中，数据编码策略对验证准确率影响最大（变化超过30%），而变分结构和测量选择影响较小（变化低于5%）。纯量子模型中，测量协议和数据编码映射是关键（测量策略影响达30%，编码映射影响约8%）。

Conclusion: 数据编码策略在混合量子-经典神经网络中起主导作用，而纯量子模型的性能更依赖于测量协议和数据编码映射。

Abstract: This study investigates the design choices of parameterized quantum circuits
(PQCs) within quantum and hybrid convolutional neural network (HQNN and QCNN)
architectures, applied to the task of satellite image classification using the
EuroSAT dataset. We systematically evaluate the performance implications of
data encoding techniques, variational ans\"atze, and measurement in approx. 500
distinct model configurations. Our analysis reveals a clear hierarchy of
influence on model performance. For hybrid architectures, which were
benchmarked against their direct classical equivalents (e.g. the same
architecture with the PQCs removed), the data encoding strategy is the dominant
factor, with validation accuracy varying over 30% for distinct embeddings. In
contrast, the selection of variational ans\"atze and measurement basis had a
comparatively marginal effect, with validation accuracy variations remaining
below 5%. For purely quantum models, restricted to amplitude encoding,
performance was most dependent on the measurement protocol and the
data-to-amplitude mapping. The measurement strategy varied the validation
accuracy by up to 30% and the encoding mapping by around 8 percentage points.

</details>
