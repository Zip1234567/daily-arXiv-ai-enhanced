{"id": "2506.20831", "pdf": "https://arxiv.org/pdf/2506.20831", "abs": "https://arxiv.org/abs/2506.20831", "authors": ["Sinan Rasiya Koya", "Tirthankar Roy"], "title": "Efficacy of Temporal Fusion Transformers for Runoff Simulation", "categories": ["physics.geo-ph", "cs.LG", "stat.AP"], "comment": null, "summary": "Combining attention with recurrence has shown to be valuable in sequence\nmodeling, including hydrological predictions. Here, we explore the strength of\nTemporal Fusion Transformers (TFTs) over Long Short-Term Memory (LSTM) networks\nin rainfall-runoff modeling. We train ten randomly initialized models, TFT and\nLSTM, for 531 CAMELS catchments in the US. We repeat the experiment with five\nsubsets of the Caravan dataset, each representing catchments in the US,\nAustralia, Brazil, Great Britain, and Chile. Then, the performance of the\nmodels, their variability regarding the catchment attributes, and the\ndifference according to the datasets are assessed. Our findings show that TFT\nslightly outperforms LSTM, especially in simulating the midsection and peak of\nhydrographs. Furthermore, we show the ability of TFT to handle longer sequences\nand why it can be a better candidate for higher or larger catchments. Being an\nexplainable AI technique, TFT identifies the key dynamic and static variables,\nproviding valuable scientific insights. However, both TFT and LSTM exhibit a\nconsiderable drop in performance with the Caravan dataset, indicating possible\ndata quality issues. Overall, the study highlights the potential of TFT in\nimproving hydrological modeling and understanding.", "AI": {"tldr": "TFT\u5728\u964d\u96e8\u5f84\u6d41\u5efa\u6a21\u4e2d\u7565\u4f18\u4e8eLSTM\uff0c\u5c24\u5176\u5728\u6a21\u62df\u6c34\u6587\u56fe\u4e2d\u6bb5\u548c\u5cf0\u503c\u65f6\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u80fd\u5904\u7406\u66f4\u957f\u5e8f\u5217\u3002", "motivation": "\u63a2\u7d22TFT\u5728\u964d\u96e8\u5f84\u6d41\u5efa\u6a21\u4e2d\u7684\u4f18\u52bf\uff0c\u5e76\u4e0eLSTM\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u6539\u8fdb\u6c34\u6587\u6a21\u578b\u3002", "method": "\u8bad\u7ec310\u4e2a\u968f\u673a\u521d\u59cb\u5316\u7684TFT\u548cLSTM\u6a21\u578b\uff0c\u5e94\u7528\u4e8e531\u4e2a\u7f8e\u56fdCAMELS\u6d41\u57df\uff0c\u5e76\u5728Caravan\u6570\u636e\u96c6\u76845\u4e2a\u5b50\u96c6\u4e0a\u91cd\u590d\u5b9e\u9a8c\u3002", "result": "TFT\u7565\u4f18\u4e8eLSTM\uff0c\u80fd\u5904\u7406\u66f4\u957f\u5e8f\u5217\uff0c\u4f46\u4e24\u8005\u5728Caravan\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "TFT\u5728\u6c34\u6587\u5efa\u6a21\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u6570\u636e\u8d28\u91cf\u53ef\u80fd\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2506.20703", "pdf": "https://arxiv.org/pdf/2506.20703", "abs": "https://arxiv.org/abs/2506.20703", "authors": ["Vaibhav Vavilala", "Seemandhar Jain", "Rahul Vasanth", "D. A. Forsyth", "Anand Bhattad"], "title": "Generative Blocks World: Moving Things Around in Pictures", "categories": ["cs.GR", "cs.CV"], "comment": "23 pages, 16 figures, 2 tables", "summary": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u64cd\u7eb5\u7b80\u5355\u51e0\u4f55\u62bd\u8c61\u6765\u7f16\u8f91\u751f\u6210\u56fe\u50cf\u573a\u666f\u7684\u65b9\u6cd5\uff0c\u4f7f\u75283D\u57fa\u5143\u8868\u793a\u573a\u666f\uff0c\u5e76\u901a\u8fc7\u6d41\u5f0f\u65b9\u6cd5\u751f\u6210\u56fe\u50cf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u7eb9\u7406\u4e00\u81f4\u6027\u548c\u7f16\u8f91\u7075\u6d3b\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u5c06\u573a\u666f\u8868\u793a\u4e3a3D\u57fa\u5143\u7684\u7ec4\u5408\uff0c\u901a\u8fc7\u6d41\u5f0f\u65b9\u6cd5\u751f\u6210\u56fe\u50cf\uff0c\u5229\u7528\u7eb9\u7406\u63d0\u793a\u4fdd\u6301\u4e00\u81f4\u6027\u3002", "result": "\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u3001\u53ef\u7f16\u8f91\u6027\u548c\u7ec4\u5408\u6cdb\u5316\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u56fe\u50cf\u7684\u7f16\u8f91\u548c\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.20875", "pdf": "https://arxiv.org/pdf/2506.20875", "abs": "https://arxiv.org/abs/2506.20875", "authors": ["Chengan He", "Junxuan Li", "Tobias Kirschstein", "Artem Sevastopolsky", "Shunsuke Saito", "Qingyang Tan", "Javier Romero", "Chen Cao", "Holly Rushmeier", "Giljoo Nam"], "title": "3DGH: 3D Head Generation with Composable Hair and Face", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to SIGGRAPH 2025. Project page:\n  https://c-he.github.io/projects/3dgh/", "summary": "We present 3DGH, an unconditional generative model for 3D human heads with\ncomposable hair and face components. Unlike previous work that entangles the\nmodeling of hair and face, we propose to separate them using a novel data\nrepresentation with template-based 3D Gaussian Splatting, in which deformable\nhair geometry is introduced to capture the geometric variations across\ndifferent hairstyles. Based on this data representation, we design a 3D\nGAN-based architecture with dual generators and employ a cross-attention\nmechanism to model the inherent correlation between hair and face. The model is\ntrained on synthetic renderings using carefully designed objectives to\nstabilize training and facilitate hair-face separation. We conduct extensive\nexperiments to validate the design choice of 3DGH, and evaluate it both\nqualitatively and quantitatively by comparing with several state-of-the-art 3D\nGAN methods, demonstrating its effectiveness in unconditional full-head image\nsynthesis and composable 3D hairstyle editing. More details will be available\non our project page: https://c-he.github.io/projects/3dgh/.", "AI": {"tldr": "3DGH\u662f\u4e00\u4e2a\u65e0\u6761\u4ef6\u751f\u62103D\u4eba\u5934\u6a21\u578b\uff0c\u80fd\u591f\u7ec4\u5408\u5934\u53d1\u548c\u9762\u90e8\u7ec4\u4ef6\uff0c\u901a\u8fc7\u5206\u79bb\u5efa\u6a21\u548c\u65b0\u578b\u6570\u636e\u8868\u793a\u5b9e\u73b0\u9ad8\u6548\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u5934\u53d1\u548c\u9762\u90e8\u5efa\u6a21\u6df7\u4e3a\u4e00\u8c08\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u7f16\u8f91\u80fd\u529b\uff0c3DGH\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6a21\u677f\u76843D\u9ad8\u65af\u70b9\u4e91\u8868\u793a\uff0c\u5f15\u5165\u53ef\u53d8\u5f62\u5934\u53d1\u51e0\u4f55\u4f53\uff0c\u8bbe\u8ba1\u53cc\u751f\u6210\u5668GAN\u67b6\u6784\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e863DGH\u7684\u6709\u6548\u6027\uff0c\u5728\u65e0\u6761\u4ef6\u5168\u5934\u56fe\u50cf\u5408\u6210\u548c\u53ef\u7ec4\u54083D\u53d1\u578b\u7f16\u8f91\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "3DGH\u901a\u8fc7\u5206\u79bb\u5934\u53d1\u548c\u9762\u90e8\u5efa\u6a21\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u751f\u6210\u548c\u7f16\u8f91\u80fd\u529b\uff0c\u4e3a3D\u4eba\u5934\u5408\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.20683", "pdf": "https://arxiv.org/pdf/2506.20683", "abs": "https://arxiv.org/abs/2506.20683", "authors": ["Alexander Selivanov", "Philip M\u00fcller", "\u00d6zg\u00fcn Turgut", "Nil Stolt-Ans\u00f3", "Daniel R\u00fcckert"], "title": "Global and Local Contrastive Learning for Joint Representations from Cardiac MRI and ECG", "categories": ["eess.IV", "cs.AI", "cs.CV", "eess.SP"], "comment": "accepted to MICCAI 2025 (Springer LNCS)", "summary": "An electrocardiogram (ECG) is a widely used, cost-effective tool for\ndetecting electrical abnormalities in the heart. However, it cannot directly\nmeasure functional parameters, such as ventricular volumes and ejection\nfraction, which are crucial for assessing cardiac function. Cardiac magnetic\nresonance (CMR) is the gold standard for these measurements, providing detailed\nstructural and functional insights, but is expensive and less accessible. To\nbridge this gap, we propose PTACL (Patient and Temporal Alignment Contrastive\nLearning), a multimodal contrastive learning framework that enhances ECG\nrepresentations by integrating spatio-temporal information from CMR. PTACL uses\nglobal patient-level contrastive loss and local temporal-level contrastive\nloss. The global loss aligns patient-level representations by pulling ECG and\nCMR embeddings from the same patient closer together, while pushing apart\nembeddings from different patients. Local loss enforces fine-grained temporal\nalignment within each patient by contrasting encoded ECG segments with\ncorresponding encoded CMR frames. This approach enriches ECG representations\nwith diagnostic information beyond electrical activity and transfers more\ninsights between modalities than global alignment alone, all without\nintroducing new learnable weights. We evaluate PTACL on paired ECG-CMR data\nfrom 27,951 subjects in the UK Biobank. Compared to baseline approaches, PTACL\nachieves better performance in two clinically relevant tasks: (1) retrieving\npatients with similar cardiac phenotypes and (2) predicting CMR-derived cardiac\nfunction parameters, such as ventricular volumes and ejection fraction. Our\nresults highlight the potential of PTACL to enhance non-invasive cardiac\ndiagnostics using ECG. The code is available at:\nhttps://github.com/alsalivan/ecgcmr", "AI": {"tldr": "PTACL\u662f\u4e00\u79cd\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408CMR\u7684\u65f6\u7a7a\u4fe1\u606f\u589e\u5f3aECG\u8868\u5f81\uff0c\u63d0\u5347\u5fc3\u810f\u529f\u80fd\u8bc4\u4f30\u80fd\u529b\u3002", "motivation": "ECG\u65e0\u6cd5\u76f4\u63a5\u6d4b\u91cf\u5fc3\u810f\u529f\u80fd\u53c2\u6570\uff0c\u800cCMR\u867d\u7cbe\u786e\u4f46\u6602\u8d35\u4e14\u4e0d\u6613\u83b7\u53d6\uff0cPTACL\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "PTACL\u7ed3\u5408\u5168\u5c40\u60a3\u8005\u7ea7\u548c\u5c40\u90e8\u65f6\u95f4\u7ea7\u5bf9\u6bd4\u635f\u5931\uff0c\u5bf9\u9f50ECG\u4e0eCMR\u8868\u5f81\uff0c\u65e0\u9700\u65b0\u589e\u53ef\u5b66\u4e60\u53c2\u6570\u3002", "result": "\u5728UK Biobank\u6570\u636e\u4e0a\uff0cPTACL\u5728\u60a3\u8005\u68c0\u7d22\u548c\u5fc3\u810f\u529f\u80fd\u53c2\u6570\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "PTACL\u5c55\u793a\u4e86\u901a\u8fc7ECG\u589e\u5f3a\u975e\u4fb5\u5165\u6027\u5fc3\u810f\u8bca\u65ad\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.20741", "pdf": "https://arxiv.org/pdf/2506.20741", "abs": "https://arxiv.org/abs/2506.20741", "authors": ["Qin Ren", "Yifan Wang", "Ruogu Fang", "Haibin Ling", "Chenyu You"], "title": "OTSurv: A Novel Multiple Instance Learning Framework for Survival Prediction with Heterogeneity-aware Optimal Transport", "categories": ["cs.CV"], "comment": null, "summary": "Survival prediction using whole slide images (WSIs) can be formulated as a\nmultiple instance learning (MIL) problem. However, existing MIL methods often\nfail to explicitly capture pathological heterogeneity within WSIs, both\nglobally -- through long-tailed morphological distributions, and locally\nthrough -- tile-level prediction uncertainty. Optimal transport (OT) provides a\nprincipled way of modeling such heterogeneity by incorporating marginal\ndistribution constraints. Building on this insight, we propose OTSurv, a novel\nMIL framework from an optimal transport perspective. Specifically, OTSurv\nformulates survival predictions as a heterogeneity-aware OT problem with two\nconstraints: (1) global long-tail constraint that models prior morphological\ndistributions to avert both mode collapse and excessive uniformity by\nregulating transport mass allocation, and (2) local uncertainty-aware\nconstraint that prioritizes high-confidence patches while suppressing noise by\nprogressively raising the total transport mass. We then recast the initial OT\nproblem, augmented by these constraints, into an unbalanced OT formulation that\ncan be solved with an efficient, hardware-friendly matrix scaling algorithm.\nEmpirically, OTSurv sets new state-of-the-art results across six popular\nbenchmarks, achieving an absolute 3.6% improvement in average C-index. In\naddition, OTSurv achieves statistical significance in log-rank tests and offers\nhigh interpretability, making it a powerful tool for survival prediction in\ndigital pathology. Our codes are available at\nhttps://github.com/Y-Research-SBU/OTSurv.", "AI": {"tldr": "OTSurv\u662f\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u7684\u591a\u5b9e\u4f8b\u5b66\u4e60\uff08MIL\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5168\u5207\u7247\u56fe\u50cf\uff08WSIs\uff09\u4e2d\u9884\u6d4b\u751f\u5b58\u7387\uff0c\u901a\u8fc7\u5168\u5c40\u548c\u5c40\u90e8\u7ea6\u675f\u89e3\u51b3\u75c5\u7406\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MIL\u65b9\u6cd5\u672a\u80fd\u660e\u786e\u6355\u6349WSIs\u4e2d\u7684\u75c5\u7406\u5f02\u8d28\u6027\uff0c\u5305\u62ec\u5168\u5c40\u7684\u957f\u5c3e\u5f62\u6001\u5206\u5e03\u548c\u5c40\u90e8\u7684\u74e6\u7247\u7ea7\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "method": "OTSurv\u5c06\u751f\u5b58\u9884\u6d4b\u5efa\u6a21\u4e3a\u5177\u6709\u5168\u5c40\u957f\u5c3e\u7ea6\u675f\u548c\u5c40\u90e8\u4e0d\u786e\u5b9a\u6027\u7ea6\u675f\u7684OT\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4e0d\u5e73\u8861OT\u516c\u5f0f\u9ad8\u6548\u6c42\u89e3\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOTSurv\u5b9e\u73b0\u4e86\u5e73\u5747C-index\u7edd\u5bf9\u63d0\u53473.6%\uff0c\u5e76\u5728\u5bf9\u6570\u79e9\u68c0\u9a8c\u4e2d\u8fbe\u5230\u7edf\u8ba1\u663e\u8457\u6027\u3002", "conclusion": "OTSurv\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u751f\u5b58\u9884\u6d4b\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u6570\u5b57\u75c5\u7406\u5b66\u3002"}}
{"id": "2506.20901", "pdf": "https://arxiv.org/pdf/2506.20901", "abs": "https://arxiv.org/abs/2506.20901", "authors": ["Meng Du", "Robert Amor", "Kwan-Liu Ma", "Burkhard C. W\u00fcnsche"], "title": "Data Visualization for Improving Financial Literacy: A Systematic Review", "categories": ["cs.GR"], "comment": null, "summary": "Financial literacy empowers individuals to make informed and effective\nfinancial decisions, improving their overall financial well-being and security.\nHowever, for many people understanding financial concepts can be daunting and\nonly half of US adults are considered financially literate. Data visualization\nsimplifies these concepts, making them accessible and engaging for learners of\nall ages. This systematic review analyzes 37 research papers exploring the use\nof data visualization and visual analytics in financial education and literacy\nenhancement. We classify these studies into five key areas: (1) the evolution\nof visualization use across time and space, (2) motivations for using\nvisualization tools, (3) the financial topics addressed and instructional\napproaches used, (4) the types of tools and technologies applied, and (5) how\nthe effectiveness of teaching interventions was evaluated. Furthermore, we\nidentify research gaps and highlight opportunities for advancing financial\nliteracy. Our findings offer practical insights for educators and professionals\nto effectively utilize or design visual tools for financial literacy.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff037\u7bc7\u7814\u7a76\u8bba\u6587\uff0c\u63a2\u8ba8\u4e86\u6570\u636e\u53ef\u89c6\u5316\u548c\u89c6\u89c9\u5206\u6790\u5728\u91d1\u878d\u6559\u80b2\u548c\u7d20\u517b\u63d0\u5347\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u5206\u7c7b\u4e3a\u4e94\u4e2a\u5173\u952e\u9886\u57df\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u7814\u7a76\u7a7a\u767d\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u91d1\u878d\u7d20\u517b\u5bf9\u4e2a\u4eba\u8d22\u52a1\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bb8\u591a\u4eba\u5bf9\u6b64\u611f\u5230\u56f0\u96be\u3002\u6570\u636e\u53ef\u89c6\u5316\u53ef\u4ee5\u7b80\u5316\u91d1\u878d\u6982\u5ff5\uff0c\u63d0\u9ad8\u5b66\u4e60\u6548\u679c\u3002", "method": "\u7cfb\u7edf\u7efc\u8ff037\u7bc7\u7814\u7a76\u8bba\u6587\uff0c\u5206\u7c7b\u4e3a\u4e94\u4e2a\u5173\u952e\u9886\u57df\uff1a\u53ef\u89c6\u5316\u4f7f\u7528\u7684\u6f14\u53d8\u3001\u52a8\u673a\u3001\u91d1\u878d\u4e3b\u9898\u4e0e\u6559\u5b66\u65b9\u6cd5\u3001\u5de5\u5177\u4e0e\u6280\u672f\u3001\u6559\u5b66\u5e72\u9884\u6548\u679c\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6570\u636e\u53ef\u89c6\u5316\u5728\u91d1\u878d\u6559\u80b2\u4e2d\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u5e76\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u4ee5\u4f18\u5316\u89c6\u89c9\u5de5\u5177\u7684\u8bbe\u8ba1\u548c\u4f7f\u7528\u3002", "conclusion": "\u6570\u636e\u53ef\u89c6\u5316\u662f\u63d0\u5347\u91d1\u878d\u7d20\u517b\u7684\u6709\u6548\u5de5\u5177\uff0c\u672a\u6765\u7814\u7a76\u53ef\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.20688", "pdf": "https://arxiv.org/pdf/2506.20688", "abs": "https://arxiv.org/abs/2506.20688", "authors": ["Minglong Li", "Lianlei Shan", "Weiqiang Wang", "Ke Lv", "Bin Luo", "Si-Bao Chen"], "title": "Building Lightweight Semantic Segmentation Models for Aerial Images Using Dual Relation Distillation", "categories": ["eess.IV"], "comment": null, "summary": "Recently, there have been significant improvements in the accuracy of CNN\nmodels for semantic segmentation. However, these models are often heavy and\nsuffer from low inference speed, which limits their practical application. To\naddress this issue, knowledge distillation has emerged as a promising approach\nto achieve a good trade-off between segmentation accuracy and efficiency. In\nthis paper, we propose a novel dual relation distillation (DRD) technique that\ntransfers both spatial and channel relations in feature maps from a cumbersome\nmodel (teacher) to a compact model (student). Specifically, we compute spatial\nand channel relation maps separately for the teacher and student models, and\nthen align corresponding relation maps by minimizing their distance. Since the\nteacher model usually learns more information and collects richer spatial and\nchannel correlations than the student model, transferring these correlations\nfrom the teacher to the student can help the student mimic the teacher better\nin terms of feature distribution, thus improving the segmentation accuracy of\nthe student model. We conduct comprehensive experiments on three segmentation\ndatasets, including two widely adopted benchmarks in the remote sensing field\n(Vaihingen and Potsdam datasets) and one popular benchmark in general scene\n(Cityscapes dataset). The experimental results demonstrate that our novel\ndistillation framework can significantly boost the performance of the student\nnetwork without incurring extra computational overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u53cc\u91cd\u5173\u7cfb\u84b8\u998f\uff08DRD\uff09\u6280\u672f\uff0c\u901a\u8fc7\u4f20\u9012\u7a7a\u95f4\u548c\u901a\u9053\u5173\u7cfb\uff0c\u63d0\u5347\u8f7b\u91cf\u6a21\u578b\u7684\u8bed\u4e49\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3CNN\u6a21\u578b\u5728\u8bed\u4e49\u5206\u5272\u4e2d\u56e0\u6a21\u578b\u8fc7\u91cd\u548c\u63a8\u7406\u901f\u5ea6\u6162\u800c\u9650\u5236\u5b9e\u9645\u5e94\u7528\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u6559\u5e08\u6a21\u578b\u548c\u5b66\u751f\u6a21\u578b\u7684\u7a7a\u95f4\u548c\u901a\u9053\u5173\u7cfb\u56fe\uff0c\u5e76\u6700\u5c0f\u5316\u5176\u8ddd\u79bb\uff0c\u4f20\u9012\u7279\u5f81\u5206\u5e03\u4fe1\u606f\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\uff08Vaihingen\u3001Potsdam\u548cCityscapes\uff09\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e14\u672a\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "DRD\u6280\u672f\u6709\u6548\u5e73\u8861\u4e86\u5206\u5272\u7cbe\u5ea6\u4e0e\u6548\u7387\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.20756", "pdf": "https://arxiv.org/pdf/2506.20756", "abs": "https://arxiv.org/abs/2506.20756", "authors": ["Haodong Li", "Chen Wang", "Jiahui Lei", "Kostas Daniilidis", "Lingjie Liu"], "title": "StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation", "categories": ["cs.CV"], "comment": "Work done in Nov. 2024. Project page: https://stereodiff.github.io/", "summary": "Recent video depth estimation methods achieve great performance by following\nthe paradigm of image depth estimation, i.e., typically fine-tuning pre-trained\nvideo diffusion models with massive data. However, we argue that video depth\nestimation is not a naive extension of image depth estimation. The temporal\nconsistency requirements for dynamic and static regions in videos are\nfundamentally different. Consistent video depth in static regions, typically\nbackgrounds, can be more effectively achieved via stereo matching across all\nframes, which provides much stronger global 3D cues. While the consistency for\ndynamic regions still should be learned from large-scale video depth data to\nensure smooth transitions, due to the violation of triangulation constraints.\nBased on these insights, we introduce StereoDiff, a two-stage video depth\nestimator that synergizes stereo matching for mainly the static areas with\nvideo depth diffusion for maintaining consistent depth transitions in dynamic\nareas. We mathematically demonstrate how stereo matching and video depth\ndiffusion offer complementary strengths through frequency domain analysis,\nhighlighting the effectiveness of their synergy in capturing the advantages of\nboth. Experimental results on zero-shot, real-world, dynamic video depth\nbenchmarks, both indoor and outdoor, demonstrate StereoDiff's SoTA performance,\nshowcasing its superior consistency and accuracy in video depth estimation.", "AI": {"tldr": "StereoDiff\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408\u7acb\u4f53\u5339\u914d\u548c\u89c6\u9891\u6df1\u5ea6\u6269\u6563\uff0c\u5206\u522b\u5904\u7406\u9759\u6001\u548c\u52a8\u6001\u533a\u57df\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u3002", "motivation": "\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u4e0d\u4ec5\u662f\u56fe\u50cf\u6df1\u5ea6\u4f30\u8ba1\u7684\u7b80\u5355\u6269\u5c55\uff0c\u9759\u6001\u548c\u52a8\u6001\u533a\u57df\u7684\u65f6\u7a7a\u4e00\u81f4\u6027\u9700\u6c42\u4e0d\u540c\u3002\u9759\u6001\u533a\u57df\u53ef\u901a\u8fc7\u7acb\u4f53\u5339\u914d\u83b7\u5f97\u5168\u5c403D\u7ebf\u7d22\uff0c\u800c\u52a8\u6001\u533a\u57df\u9700\u4f9d\u8d56\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u5b66\u4e60\u5e73\u6ed1\u8fc7\u6e21\u3002", "method": "StereoDiff\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9759\u6001\u533a\u57df\u901a\u8fc7\u7acb\u4f53\u5339\u914d\u5b9e\u73b0\u4e00\u81f4\u6027\uff0c\u52a8\u6001\u533a\u57df\u901a\u8fc7\u89c6\u9891\u6df1\u5ea6\u6269\u6563\u6a21\u578b\u5b66\u4e60\u5e73\u6ed1\u8fc7\u6e21\u3002\u901a\u8fc7\u9891\u57df\u5206\u6790\u8bc1\u660e\u4e24\u8005\u7684\u4e92\u8865\u6027\u3002", "result": "\u5728\u96f6\u6837\u672c\u3001\u771f\u5b9e\u4e16\u754c\u52a8\u6001\u89c6\u9891\u6df1\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cStereoDiff\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "StereoDiff\u901a\u8fc7\u7ed3\u5408\u7acb\u4f53\u5339\u914d\u548c\u89c6\u9891\u6df1\u5ea6\u6269\u6563\uff0c\u4e3a\u89c6\u9891\u6df1\u5ea6\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.20946", "pdf": "https://arxiv.org/pdf/2506.20946", "abs": "https://arxiv.org/abs/2506.20946", "authors": ["Donggoo Kang", "Jangyeong Kim", "Dasol Jeong", "Junyoung Choi", "Jeonga Wi", "Hyunmin Lee", "Joonho Gwon", "Joonki Paik"], "title": "Consistent Zero-shot 3D Texture Synthesis Using Geometry-aware Diffusion and Temporal Video Models", "categories": ["cs.GR", "cs.AI", "cs.CV", "68T45, 68U05", "I.3.7; I.4.10; I.2.10"], "comment": null, "summary": "Current texture synthesis methods, which generate textures from fixed\nviewpoints, suffer from inconsistencies due to the lack of global context and\ngeometric understanding. Meanwhile, recent advancements in video generation\nmodels have demonstrated remarkable success in achieving temporally consistent\nvideos. In this paper, we introduce VideoTex, a novel framework for seamless\ntexture synthesis that leverages video generation models to address both\nspatial and temporal inconsistencies in 3D textures. Our approach incorporates\ngeometry-aware conditions, enabling precise utilization of 3D mesh structures.\nAdditionally, we propose a structure-wise UV diffusion strategy, which enhances\nthe generation of occluded areas by preserving semantic information, resulting\nin smoother and more coherent textures. VideoTex not only achieves smoother\ntransitions across UV boundaries but also ensures high-quality, temporally\nstable textures across video frames. Extensive experiments demonstrate that\nVideoTex outperforms existing methods in texture fidelity, seam blending, and\nstability, paving the way for dynamic real-time applications that demand both\nvisual quality and temporal coherence.", "AI": {"tldr": "VideoTex\u5229\u7528\u89c6\u9891\u751f\u6210\u6a21\u578b\u89e3\u51b33D\u7eb9\u7406\u5408\u6210\u4e2d\u7684\u65f6\u7a7a\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u6761\u4ef6\u548c\u7ed3\u6784UV\u6269\u6563\u7b56\u7565\uff0c\u751f\u6210\u66f4\u5e73\u6ed1\u3001\u4e00\u81f4\u7684\u7eb9\u7406\u3002", "motivation": "\u73b0\u6709\u7eb9\u7406\u5408\u6210\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u51e0\u4f55\u7406\u89e3\u5bfc\u81f4\u4e0d\u4e00\u81f4\uff0c\u800c\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u65f6\u95f4\u4e00\u81f4\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u56e0\u6b64\u7ed3\u5408\u4e24\u8005\u89e3\u51b3\u7eb9\u7406\u5408\u6210\u95ee\u9898\u3002", "method": "\u63d0\u51faVideoTex\u6846\u67b6\uff0c\u7ed3\u5408\u51e0\u4f55\u611f\u77e5\u6761\u4ef6\u548c\u7ed3\u6784UV\u6269\u6563\u7b56\u7565\uff0c\u5229\u75283D\u7f51\u683c\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\u751f\u6210\u7eb9\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660eVideoTex\u5728\u7eb9\u7406\u4fdd\u771f\u5ea6\u3001\u63a5\u7f1d\u878d\u5408\u548c\u7a33\u5b9a\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "VideoTex\u4e3a\u52a8\u6001\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u65f6\u95f4\u7a33\u5b9a\u7684\u7eb9\u7406\u5408\u6210\u65b9\u6848\u3002"}}
{"id": "2506.20689", "pdf": "https://arxiv.org/pdf/2506.20689", "abs": "https://arxiv.org/abs/2506.20689", "authors": ["Racheal Mukisa", "Arvind K. Bansal"], "title": "U-R-VEDA: Integrating UNET, Residual Links, Edge and Dual Attention, and Vision Transformer for Accurate Semantic Segmentation of CMRs", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "I.4.6; I.2; I.5.2; I.5.1"], "comment": "15 pages, 3 figures", "summary": "Artificial intelligence, including deep learning models, will play a\ntransformative role in automated medical image analysis for the diagnosis of\ncardiac disorders and their management. Automated accurate delineation of\ncardiac images is the first necessary initial step for the quantification and\nautomated diagnosis of cardiac disorders. In this paper, we propose a deep\nlearning based enhanced UNet model, U-R-Veda, which integrates convolution\ntransformations, vision transformer, residual links, channel-attention, and\nspatial attention, together with edge-detection based skip-connections for an\naccurate fully-automated semantic segmentation of cardiac magnetic resonance\n(CMR) images. The model extracts local-features and their interrelationships\nusing a stack of combination convolution blocks, with embedded channel and\nspatial attention in the convolution block, and vision transformers. Deep\nembedding of channel and spatial attention in the convolution block identifies\nimportant features and their spatial localization. The combined edge\ninformation with channel and spatial attention as skip connection reduces\ninformation-loss during convolution transformations. The overall model\nsignificantly improves the semantic segmentation of CMR images necessary for\nimproved medical image analysis. An algorithm for the dual attention module\n(channel and spatial attention) has been presented. Performance results show\nthat U-R-Veda achieves an average accuracy of 95.2%, based on DSC metrics. The\nmodel outperforms the accuracy attained by other models, based on DSC and HD\nmetrics, especially for the delineation of right-ventricle and\nleft-ventricle-myocardium.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aU-R-Veda\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u5fc3\u810f\u78c1\u5171\u632f\u56fe\u50cf\u7684\u81ea\u52a8\u8bed\u4e49\u5206\u5272\uff0c\u7ed3\u5408\u5377\u79ef\u53d8\u6362\u3001\u89c6\u89c9\u53d8\u6362\u5668\u3001\u6ce8\u610f\u529b\u673a\u5236\u548c\u8fb9\u7f18\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u5fc3\u810f\u56fe\u50cf\u81ea\u52a8\u51c6\u786e\u5206\u5272\u662f\u5fc3\u810f\u75be\u75c5\u8bca\u65ad\u548c\u7ba1\u7406\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u4fe1\u606f\u4fdd\u7559\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "U-R-Veda\u6a21\u578b\u6574\u5408\u4e86\u5377\u79ef\u53d8\u6362\u3001\u89c6\u89c9\u53d8\u6362\u5668\u3001\u6b8b\u5dee\u8fde\u63a5\u3001\u901a\u9053\u548c\u7a7a\u95f4\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u53ca\u8fb9\u7f18\u68c0\u6d4b\u7684\u8df3\u8dc3\u8fde\u63a5\uff0c\u4ee5\u51cf\u5c11\u4fe1\u606f\u635f\u5931\u5e76\u63d0\u5347\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "result": "\u6a21\u578b\u5728DSC\u6307\u6807\u4e0a\u8fbe\u523095.2%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u5c24\u5176\u5728\u53f3\u5fc3\u5ba4\u548c\u5de6\u5fc3\u5ba4\u5fc3\u808c\u7684\u5206\u5272\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "U-R-Veda\u6a21\u578b\u901a\u8fc7\u591a\u6a21\u5757\u96c6\u6210\u663e\u8457\u63d0\u5347\u4e86\u5fc3\u810f\u56fe\u50cf\u5206\u5272\u7684\u7cbe\u5ea6\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u81ea\u52a8\u5316\u5de5\u5177\u3002"}}
{"id": "2506.20757", "pdf": "https://arxiv.org/pdf/2506.20757", "abs": "https://arxiv.org/abs/2506.20757", "authors": ["Zhiyuan Wu", "Yongqiang Zhao", "Shan Luo"], "title": "ConViTac: Aligning Visual-Tactile Fusion with Contrastive Representations", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Vision and touch are two fundamental sensory modalities for robots, offering\ncomplementary information that enhances perception and manipulation tasks.\nPrevious research has attempted to jointly learn visual-tactile representations\nto extract more meaningful information. However, these approaches often rely on\ndirect combination, such as feature addition and concatenation, for modality\nfusion, which tend to result in poor feature integration. In this paper, we\npropose ConViTac, a visual-tactile representation learning network designed to\nenhance the alignment of features during fusion using contrastive\nrepresentations. Our key contribution is a Contrastive Embedding Conditioning\n(CEC) mechanism that leverages a contrastive encoder pretrained through\nself-supervised contrastive learning to project visual and tactile inputs into\nunified latent embeddings. These embeddings are used to couple visual-tactile\nfeature fusion through cross-modal attention, aiming at aligning the unified\nrepresentations and enhancing performance on downstream tasks. We conduct\nextensive experiments to demonstrate the superiority of ConViTac in real world\nover current state-of-the-art methods and the effectiveness of our proposed CEC\nmechanism, which improves accuracy by up to 12.0% in material classification\nand grasping prediction tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aConViTac\u7684\u89c6\u89c9-\u89e6\u89c9\u8868\u5f81\u5b66\u4e60\u7f51\u7edc\uff0c\u901a\u8fc7\u5bf9\u6bd4\u8868\u5f81\u589e\u5f3a\u7279\u5f81\u878d\u5408\u7684\u5bf9\u9f50\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6750\u6599\u5206\u7c7b\u548c\u6293\u53d6\u9884\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u548c\u89e6\u89c9\u662f\u673a\u5668\u4eba\u611f\u77e5\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u4e24\u79cd\u57fa\u672c\u611f\u5b98\u6a21\u6001\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u6a21\u6001\u878d\u5408\u65f6\u7279\u5f81\u6574\u5408\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faConViTac\u7f51\u7edc\uff0c\u91c7\u7528\u5bf9\u6bd4\u5d4c\u5165\u6761\u4ef6\uff08CEC\uff09\u673a\u5236\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u9884\u8bad\u7ec3\u7684\u7f16\u7801\u5668\u5c06\u89c6\u89c9\u548c\u89e6\u89c9\u8f93\u5165\u6295\u5f71\u5230\u7edf\u4e00\u7684\u6f5c\u5728\u5d4c\u5165\u4e2d\uff0c\u5e76\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u8fdb\u884c\u7279\u5f81\u878d\u5408\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cConViTac\u5728\u6750\u6599\u5206\u7c7b\u548c\u6293\u53d6\u9884\u6d4b\u4efb\u52a1\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e8612.0%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "ConViTac\u901a\u8fc7\u5bf9\u6bd4\u8868\u5f81\u5b66\u4e60\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9-\u89e6\u89c9\u7279\u5f81\u878d\u5408\u7684\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u611f\u77e5\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21272", "pdf": "https://arxiv.org/pdf/2506.21272", "abs": "https://arxiv.org/abs/2506.21272", "authors": ["Jiayi Zheng", "Xiaodong Cun"], "title": "FairyGen: Storied Cartoon Video from a Single Child-Drawn Character", "categories": ["cs.GR", "cs.CV", "cs.MM"], "comment": "Project Page: https://jayleejia.github.io/FairyGen/ ; Code:\n  https://github.com/GVCLab/FairyGen", "summary": "We propose FairyGen, an automatic system for generating story-driven cartoon\nvideos from a single child's drawing, while faithfully preserving its unique\nartistic style. Unlike previous storytelling methods that primarily focus on\ncharacter consistency and basic motion, FairyGen explicitly disentangles\ncharacter modeling from stylized background generation and incorporates\ncinematic shot design to support expressive and coherent storytelling. Given a\nsingle character sketch, we first employ an MLLM to generate a structured\nstoryboard with shot-level descriptions that specify environment settings,\ncharacter actions, and camera perspectives. To ensure visual consistency, we\nintroduce a style propagation adapter that captures the character's visual\nstyle and applies it to the background, faithfully retaining the character's\nfull visual identity while synthesizing style-consistent scenes. A shot design\nmodule further enhances visual diversity and cinematic quality through frame\ncropping and multi-view synthesis based on the storyboard. To animate the\nstory, we reconstruct a 3D proxy of the character to derive physically\nplausible motion sequences, which are then used to fine-tune an MMDiT-based\nimage-to-video diffusion model. We further propose a two-stage motion\ncustomization adapter: the first stage learns appearance features from\ntemporally unordered frames, disentangling identity from motion; the second\nstage models temporal dynamics using a timestep-shift strategy with frozen\nidentity weights. Once trained, FairyGen directly renders diverse and coherent\nvideo scenes aligned with the storyboard. Extensive experiments demonstrate\nthat our system produces animations that are stylistically faithful,\nnarratively structured natural motion, highlighting its potential for\npersonalized and engaging story animation. The code will be available at\nhttps://github.com/GVCLab/FairyGen", "AI": {"tldr": "FairyGen\u662f\u4e00\u4e2a\u81ea\u52a8\u7cfb\u7edf\uff0c\u80fd\u4ece\u5355\u5f20\u513f\u7ae5\u7ed8\u753b\u751f\u6210\u6545\u4e8b\u9a71\u52a8\u7684\u5361\u901a\u89c6\u9891\uff0c\u5e76\u4fdd\u7559\u5176\u72ec\u7279\u7684\u827a\u672f\u98ce\u683c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u89d2\u8272\u4e00\u81f4\u6027\u548c\u57fa\u672c\u52a8\u4f5c\uff0c\u800cFairyGen\u65e8\u5728\u901a\u8fc7\u5206\u79bb\u89d2\u8272\u5efa\u6a21\u4e0e\u98ce\u683c\u5316\u80cc\u666f\u751f\u6210\uff0c\u5e76\u7ed3\u5408\u7535\u5f71\u955c\u5934\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u66f4\u5177\u8868\u73b0\u529b\u548c\u8fde\u8d2f\u6027\u7684\u6545\u4e8b\u8bb2\u8ff0\u3002", "method": "\u7cfb\u7edf\u4f7f\u7528MLLM\u751f\u6210\u7ed3\u6784\u5316\u6545\u4e8b\u677f\uff0c\u5f15\u5165\u98ce\u683c\u4f20\u64ad\u9002\u914d\u5668\u786e\u4fdd\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u955c\u5934\u8bbe\u8ba1\u6a21\u5757\u589e\u5f3a\u89c6\u89c9\u591a\u6837\u6027\u548c\u7535\u5f71\u8d28\u91cf\u3002\u89d2\u8272\u52a8\u753b\u901a\u8fc73D\u4ee3\u7406\u91cd\u5efa\u548cMMDiT\u6a21\u578b\u5b9e\u73b0\uff0c\u8fd0\u52a8\u5b9a\u5236\u9002\u914d\u5668\u5206\u4e24\u9636\u6bb5\u5b66\u4e60\u5916\u89c2\u7279\u5f81\u548c\u65f6\u5e8f\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFairyGen\u751f\u6210\u7684\u52a8\u753b\u98ce\u683c\u5fe0\u5b9e\u3001\u53d9\u4e8b\u7ed3\u6784\u81ea\u7136\uff0c\u5177\u6709\u4e2a\u6027\u5316\u6545\u4e8b\u52a8\u753b\u7684\u6f5c\u529b\u3002", "conclusion": "FairyGen\u4e3a\u4e2a\u6027\u5316\u6545\u4e8b\u52a8\u753b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u98ce\u683c\u4e00\u81f4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.20897", "pdf": "https://arxiv.org/pdf/2506.20897", "abs": "https://arxiv.org/abs/2506.20897", "authors": ["Shuki Maruyama", "Hidenori Takeshima"], "title": "Development of MR spectral analysis method robust against static magnetic field inhomogeneity", "categories": ["eess.IV", "cs.CV"], "comment": "11 pages, 6 figures", "summary": "Purpose:To develop a method that enhances the accuracy of spectral analysis\nin the presence of static magnetic field B0 inhomogeneity. Methods:The authors\nproposed a new spectral analysis method utilizing a deep learning model trained\non modeled spectra that consistently represent the spectral variations induced\nby B0 inhomogeneity. These modeled spectra were generated from the B0 map and\nmetabolite ratios of the healthy human brain. The B0 map was divided into a\npatch size of subregions, and the separately estimated metabolites and baseline\ncomponents were averaged and then integrated. The quality of the modeled\nspectra was visually and quantitatively evaluated against the measured spectra.\nThe analysis models were trained using measured, simulated, and modeled\nspectra. The performance of the proposed method was assessed using mean squared\nerrors (MSEs) of metabolite ratios. The mean absolute percentage errors (MAPEs)\nof the metabolite ratios were also compared to LCModel when analyzing the\nphantom spectra acquired under two types of B0 inhomogeneity. Results:The\nmodeled spectra exhibited broadened and narrowed spectral peaks depending on\nthe B0 inhomogeneity and were quantitatively close to the measured spectra. The\nanalysis model trained using measured spectra with modeled spectra improved\nMSEs by 49.89% compared to that trained using measured spectra alone, and by\n26.66% compared to that trained using measured spectra with simulated spectra.\nThe performance improved as the number of modeled spectra increased from 0 to\n1,000. This model showed significantly lower MAPEs than LCModel under both\ntypes of B0 inhomogeneity. Conclusion:A new spectral analysis-trained deep\nlearning model using the modeled spectra was developed. The results suggest\nthat the proposed method has the potential to improve the accuracy of spectral\nanalysis by increasing the training samples of spectra.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8c31\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62dfB0\u4e0d\u5747\u5300\u6027\u8bf1\u5bfc\u7684\u8c31\u53d8\u5316\uff0c\u63d0\u9ad8\u4e86\u8c31\u5206\u6790\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u9759\u6001\u78c1\u573aB0\u4e0d\u5747\u5300\u6027\u5bf9\u8c31\u5206\u6790\u51c6\u786e\u6027\u7684\u5f71\u54cd\u3002", "method": "\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8bad\u7ec3\u57fa\u4e8eB0\u56fe\u548c\u5065\u5eb7\u4eba\u8111\u4ee3\u8c22\u7269\u6bd4\u751f\u6210\u7684\u6a21\u62df\u8c31\uff0c\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u6a21\u62df\u8c31\u4e0e\u5b9e\u6d4b\u8c31\u63a5\u8fd1\uff0c\u6a21\u578b\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0cMSE\u548cMAPE\u5747\u6709\u6240\u6539\u5584\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u589e\u52a0\u8bad\u7ec3\u6837\u672c\uff0c\u6709\u671b\u63d0\u9ad8\u8c31\u5206\u6790\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2506.20786", "pdf": "https://arxiv.org/pdf/2506.20786", "abs": "https://arxiv.org/abs/2506.20786", "authors": ["Connor Ludwig", "Khashayar Namdar", "Farzad Khalvati"], "title": "AI-Driven MRI-based Brain Tumour Segmentation Benchmarking", "categories": ["cs.CV"], "comment": null, "summary": "Medical image segmentation has greatly aided medical diagnosis, with U-Net\nbased architectures and nnU-Net providing state-of-the-art performance. There\nhave been numerous general promptable models and medical variations introduced\nin recent years, but there is currently a lack of evaluation and comparison of\nthese models across a variety of prompt qualities on a common medical dataset.\nThis research uses Segment Anything Model (SAM), Segment Anything Model 2 (SAM\n2), MedSAM, SAM-Med-3D, and nnU-Net to obtain zero-shot inference on the BraTS\n2023 adult glioma and pediatrics dataset across multiple prompt qualities for\nboth points and bounding boxes. Several of these models exhibit promising Dice\nscores, particularly SAM and SAM 2 achieving scores of up to 0.894 and 0.893,\nrespectively when given extremely accurate bounding box prompts which exceeds\nnnU-Net's segmentation performance. However, nnU-Net remains the dominant\nmedical image segmentation network due to the impracticality of providing\nhighly accurate prompts to the models. The model and prompt evaluation, as well\nas the comparison, are extended through fine-tuning SAM, SAM 2, MedSAM, and\nSAM-Med-3D on the pediatrics dataset. The improvements in point prompt\nperformance after fine-tuning are substantial and show promise for future\ninvestigation, but are unable to achieve better segmentation than bounding\nboxes or nnU-Net.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e86\u591a\u79cd\u53ef\u63d0\u793a\u6a21\u578b\uff08\u5982SAM\u3001SAM 2\u3001MedSAM\u7b49\uff09\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u9ad8\u7cbe\u5ea6\u63d0\u793a\u4e0bSAM\u548cSAM 2\u8868\u73b0\u4f18\u4e8ennU-Net\uff0c\u4f46nnU-Net\u4ecd\u662f\u4e3b\u6d41\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5bf9\u591a\u79cd\u53ef\u63d0\u793a\u6a21\u578b\u5728\u4e0d\u540c\u63d0\u793a\u8d28\u91cf\u4e0b\u7684\u8bc4\u4f30\u548c\u6bd4\u8f83\uff0c\u5c24\u5176\u662f\u5728\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u3002", "method": "\u4f7f\u7528SAM\u3001SAM 2\u3001MedSAM\u3001SAM-Med-3D\u548cnnU-Net\u5728BraTS 2023\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u63a8\u7406\uff0c\u5e76\u5bf9\u90e8\u5206\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002", "result": "SAM\u548cSAM 2\u5728\u9ad8\u7cbe\u5ea6\u8fb9\u754c\u6846\u63d0\u793a\u4e0bDice\u5206\u6570\u8fbe0.894\u548c0.893\uff0c\u4f18\u4e8ennU-Net\uff0c\u4f46nnU-Net\u4ecd\u66f4\u5b9e\u7528\u3002\u5fae\u8c03\u540e\u70b9\u63d0\u793a\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4f46\u4ecd\u4e0d\u53ca\u8fb9\u754c\u6846\u6216nnU-Net\u3002", "conclusion": "\u9ad8\u7cbe\u5ea6\u63d0\u793a\u4e0b\u53ef\u63d0\u793a\u6a21\u578b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46nnU-Net\u4ecd\u662f\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u4e3b\u6d41\u9009\u62e9\u3002\u5fae\u8c03\u663e\u793a\u672a\u6765\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.21425", "pdf": "https://arxiv.org/pdf/2506.21425", "abs": "https://arxiv.org/abs/2506.21425", "authors": ["Pin Ren", "Yan Gao", "Zhichun Li", "Yan Chen", "Benjamin Watson"], "title": "IDGraphs: Intrusion Detection and Analysis Using Stream Compositing", "categories": ["cs.GR", "cs.CR"], "comment": null, "summary": "Traffic anomalies and attacks are commonplace in today's networks and\nidentifying them rapidly and accurately is critical for large network\noperators. For a statistical intrusion detection system (IDS), it is crucial to\ndetect at the flow-level for accurate detection and mitigation. However,\nexisting IDS systems offer only limited support for 1) interactively examining\ndetected intrusions and anomalies, 2) analyzing worm propagation patterns, 3)\nand discovering correlated attacks. These problems are becoming even more acute\nas the traffic on today's high-speed routers continues to grow.\n  IDGraphs is an interactive visualization system for intrusion detection that\naddresses these challenges. The central visualization in the system is a\nflow-level trace plotted with time on the horizontal axis and aggregated number\nof unsuccessful connections on the vertical axis. We then summarize a stack of\ntens or hundreds of thousands of these traces using the Histographs [RW05]\ntechnique, which maps data frequency at each pixel to brightness. Users may\nthen interactively query the summary view, performing analysis by highlighting\nsubsets of the traces. For example, brushing a linked correlation matrix view\nhighlights traces with similar patterns, revealing distributed attacks that are\ndifficult to detect using standard statistical analysis.\n  We apply IDGraphs system to a real network router data-set with 179M\nflow-level records representing a total traffic of 1.16TB. The system\nsuccessfully detects and analyzes a variety of attacks and anomalies, including\nport scanning, worm outbreaks, stealthy TCP SYN floodings, and some distributed\nattacks.", "AI": {"tldr": "IDGraphs\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u73b0\u6709\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u5728\u4ea4\u4e92\u5f0f\u68c0\u67e5\u3001\u5206\u6790\u8815\u866b\u4f20\u64ad\u6a21\u5f0f\u548c\u53d1\u73b0\u76f8\u5173\u653b\u51fb\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u7f51\u7edc\u6d41\u91cf\u5f02\u5e38\u548c\u653b\u51fb\u9891\u53d1\uff0c\u73b0\u6709\u5165\u4fb5\u68c0\u6d4b\u7cfb\u7edf\u5728\u4ea4\u4e92\u5f0f\u5206\u6790\u548c\u68c0\u6d4b\u5206\u5e03\u5f0f\u653b\u51fb\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "IDGraphs\u901a\u8fc7\u6d41\u7ea7\u8ddf\u8e2a\u548c\u65f6\u95f4\u8f74\u53ef\u89c6\u5316\uff0c\u7ed3\u5408Histographs\u6280\u672f\u6c47\u603b\u6570\u636e\uff0c\u7528\u6237\u53ef\u901a\u8fc7\u4ea4\u4e92\u5f0f\u67e5\u8be2\u548c\u5206\u6790\u5b50\u96c6\u6765\u68c0\u6d4b\u653b\u51fb\u3002", "result": "\u5728\u5305\u542b1.79\u4ebf\u6d41\u7ea7\u8bb0\u5f55\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cIDGraphs\u6210\u529f\u68c0\u6d4b\u5230\u7aef\u53e3\u626b\u63cf\u3001\u8815\u866b\u7206\u53d1\u3001TCP SYN\u6d2a\u6cdb\u7b49\u591a\u79cd\u653b\u51fb\u3002", "conclusion": "IDGraphs\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u76f4\u89c2\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5165\u4fb5\u68c0\u6d4b\u7684\u4ea4\u4e92\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.21162", "pdf": "https://arxiv.org/pdf/2506.21162", "abs": "https://arxiv.org/abs/2506.21162", "authors": ["Shuwei Xing", "Derek W. Cool", "David Tessier", "Elvis C. S. Chen", "Terry M. Peters", "Aaron Fenster"], "title": "A Novel Framework for Integrating 3D Ultrasound into Percutaneous Liver Tumour Ablation", "categories": ["eess.IV", "cs.AI"], "comment": "11 pages, 5 figures", "summary": "3D ultrasound (US) imaging has shown significant benefits in enhancing the\noutcomes of percutaneous liver tumour ablation. Its clinical integration is\ncrucial for transitioning 3D US into the therapeutic domain. However,\nchallenges of tumour identification in US images continue to hinder its broader\nadoption. In this work, we propose a novel framework for integrating 3D US into\nthe standard ablation workflow. We present a key component, a clinically viable\n2D US-CT/MRI registration approach, leveraging 3D US as an intermediary to\nreduce registration complexity. To facilitate efficient verification of the\nregistration workflow, we also propose an intuitive multimodal image\nvisualization technique. In our study, 2D US-CT/MRI registration achieved a\nlandmark distance error of approximately 2-4 mm with a runtime of 0.22s per\nimage pair. Additionally, non-rigid registration reduced the mean alignment\nerror by approximately 40% compared to rigid registration. Results demonstrated\nthe efficacy of the proposed 2D US-CT/MRI registration workflow. Our\nintegration framework advanced the capabilities of 3D US imaging in improving\npercutaneous tumour ablation, demonstrating the potential to expand the\ntherapeutic role of 3D US in clinical interventions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c063D\u8d85\u58f0\uff08US\uff09\u6574\u5408\u5230\u6807\u51c6\u80bf\u7624\u6d88\u878d\u5de5\u4f5c\u6d41\u7a0b\u7684\u65b0\u6846\u67b6\uff0c\u5305\u62ec\u4e00\u79cd\u4e34\u5e8a\u53ef\u884c\u76842D US-CT/MRI\u914d\u51c6\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u591a\u6a21\u6001\u56fe\u50cf\u53ef\u89c6\u5316\u6280\u672f\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "3D\u8d85\u58f0\u5728\u809d\u80bf\u7624\u6d88\u878d\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u80bf\u7624\u8bc6\u522b\u95ee\u9898\u963b\u788d\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63a8\u52a83D\u8d85\u58f0\u5728\u6cbb\u7597\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd2D US-CT/MRI\u914d\u51c6\u65b9\u6cd5\uff0c\u5229\u75283D\u8d85\u58f0\u4f5c\u4e3a\u4e2d\u4ecb\u964d\u4f4e\u914d\u51c6\u590d\u6742\u5ea6\uff0c\u5e76\u5f00\u53d1\u4e86\u591a\u6a21\u6001\u56fe\u50cf\u53ef\u89c6\u5316\u6280\u672f\u7528\u4e8e\u9a8c\u8bc1\u3002", "result": "2D US-CT/MRI\u914d\u51c6\u7684\u6807\u8bb0\u70b9\u8ddd\u79bb\u8bef\u5dee\u4e3a2-4 mm\uff0c\u8fd0\u884c\u65f6\u95f4\u4e3a0.22\u79d2/\u56fe\u50cf\u5bf9\uff1b\u975e\u521a\u6027\u914d\u51c6\u6bd4\u521a\u6027\u914d\u51c6\u51cf\u5c11\u4e8640%\u7684\u5bf9\u9f50\u8bef\u5dee\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u5347\u4e863D\u8d85\u58f0\u5728\u80bf\u7624\u6d88\u878d\u4e2d\u7684\u5e94\u7528\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u4e34\u5e8a\u6cbb\u7597\u4e2d\u7684\u6269\u5c55\u6f5c\u529b\u3002"}}
{"id": "2506.20795", "pdf": "https://arxiv.org/pdf/2506.20795", "abs": "https://arxiv.org/abs/2506.20795", "authors": ["Stephanie K\u00e4s", "Anton Burenko", "Louis Markert", "Onur Alp Culha", "Dennis Mack", "Timm Linder", "Bastian Leibe"], "title": "How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?", "categories": ["cs.CV", "cs.HC", "cs.RO", "I.2.10; I.2.9; I.5.4; I.4.8; I.4.9; H.1.2"], "comment": null, "summary": "Gestures enable non-verbal human-robot communication, especially in noisy\nenvironments like agile production. Traditional deep learning-based gesture\nrecognition relies on task-specific architectures using images, videos, or\nskeletal pose estimates as input. Meanwhile, Vision Foundation Models (VFMs)\nand Vision Language Models (VLMs) with their strong generalization abilities\noffer potential to reduce system complexity by replacing dedicated\ntask-specific modules. This study investigates adapting such models for\ndynamic, full-body gesture recognition, comparing V-JEPA (a state-of-the-art\nVFM), Gemini Flash 2.0 (a multimodal VLM), and HD-GCN (a top-performing\nskeleton-based approach). We introduce NUGGET, a dataset tailored for\nhuman-robot communication in intralogistics environments, to evaluate the\ndifferent gesture recognition approaches. In our experiments, HD-GCN achieves\nbest performance, but V-JEPA comes close with a simple, task-specific\nclassification head - thus paving a possible way towards reducing system\ncomplexity, by using it as a shared multi-task model. In contrast, Gemini\nstruggles to differentiate gestures based solely on textual descriptions in the\nzero-shot setting, highlighting the need of further research on suitable input\nrepresentations for gestures.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08VFMs\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u52a8\u6001\u5168\u8eab\u624b\u52bf\u8bc6\u522b\uff0c\u6bd4\u8f83\u4e86V-JEPA\u3001Gemini Flash 2.0\u548cHD-GCN\u7684\u6027\u80fd\uff0c\u53d1\u73b0HD-GCN\u8868\u73b0\u6700\u4f73\uff0c\u4f46V-JEPA\u63a5\u8fd1\uff0c\u4e3a\u7b80\u5316\u7cfb\u7edf\u590d\u6742\u5ea6\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "motivation": "\u5728\u5608\u6742\u73af\u5883\u4e2d\uff0c\u624b\u52bf\u662f\u975e\u4eba\u673a\u4ea4\u4e92\u7684\u91cd\u8981\u65b9\u5f0f\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u67b6\u6784\uff0c\u800cVFMs\u548cVLMs\u7684\u6cdb\u5316\u80fd\u529b\u53ef\u80fd\u964d\u4f4e\u7cfb\u7edf\u590d\u6742\u5ea6\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86V-JEPA\u3001Gemini Flash 2.0\u548cHD-GCN\u5728\u52a8\u6001\u5168\u8eab\u624b\u52bf\u8bc6\u522b\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u5f15\u5165\u4e86NUGGET\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "HD-GCN\u8868\u73b0\u6700\u4f73\uff0cV-JEPA\u63a5\u8fd1\uff0cGemini\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "V-JEPA\u4f5c\u4e3a\u5171\u4eab\u591a\u4efb\u52a1\u6a21\u578b\u6f5c\u529b\u5927\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u624b\u52bf\u7684\u8f93\u5165\u8868\u793a\u65b9\u5f0f\u3002"}}
{"id": "2506.21441", "pdf": "https://arxiv.org/pdf/2506.21441", "abs": "https://arxiv.org/abs/2506.21441", "authors": ["Benjamin Watson", "Neff Walker", "Larry F Hodges", "Martin Reddy"], "title": "An evaluation of level of detail degradation in head-mounted display peripheries", "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "A paradigm for the design of systems that manage level of detail in virtual\nenvironments is proposed. As an example of the prototyping step in this\nparadigm, a user study was performed to evaluate the effectiveness of high\ndetail insets used with head-mounted displays. Ten subjects were given a simple\nsearch task that required the location and identification of a single target\nobject. All subjects used seven different displays (the independent variable),\nvarying in inset size and peripheral detail, to perform this task. Frame rate,\ntarget location, subject input method, and order of display use were all\ncontrolled. Primary dependent measures were search time on trials with correct\nidentification, and the percentage of all trials correctly identified. ANOVAs\nof the results showed that insetless, high detail displays did not lead to\nsignificantly different search times or accuracies than displays with insets.\nIn fact, only the insetless, low detail display returned significantly\ndifferent results. Further research is being performed to examine the effect of\nvarying task complexity, inset size, and level of detail.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ba1\u7406\u865a\u62df\u73af\u5883\u4e2d\u7ec6\u8282\u7ea7\u522b\u7684\u7cfb\u7edf\u8bbe\u8ba1\u8303\u5f0f\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u4e86\u9ad8\u7ec6\u8282\u63d2\u56fe\u7684\u663e\u793a\u6548\u679c\u3002\u7ed3\u679c\u663e\u793a\uff0c\u65e0\u63d2\u56fe\u7684\u9ad8\u7ec6\u8282\u663e\u793a\u4e0e\u6709\u63d2\u56fe\u7684\u663e\u793a\u5728\u641c\u7d22\u65f6\u95f4\u548c\u51c6\u786e\u6027\u4e0a\u65e0\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u5982\u4f55\u5728\u865a\u62df\u73af\u5883\u4e2d\u6709\u6548\u7ba1\u7406\u7ec6\u8282\u7ea7\u522b\uff0c\u5c24\u5176\u662f\u901a\u8fc7\u9ad8\u7ec6\u8282\u63d2\u56fe\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u7528\u6237\u7814\u7a76\uff0c10\u540d\u53d7\u8bd5\u8005\u5728\u4e0d\u540c\u663e\u793a\u6761\u4ef6\u4e0b\u5b8c\u6210\u641c\u7d22\u4efb\u52a1\uff0c\u63a7\u5236\u53d8\u91cf\u5305\u62ec\u5e27\u7387\u3001\u76ee\u6807\u4f4d\u7f6e\u7b49\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u65e0\u63d2\u56fe\u7684\u9ad8\u7ec6\u8282\u663e\u793a\u4e0e\u6709\u63d2\u56fe\u7684\u663e\u793a\u5728\u6027\u80fd\u4e0a\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u4ec5\u65e0\u63d2\u56fe\u7684\u4f4e\u7ec6\u8282\u663e\u793a\u8868\u73b0\u663e\u8457\u4e0d\u540c\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\u9ad8\u7ec6\u8282\u63d2\u56fe\u53ef\u80fd\u5e76\u975e\u5fc5\u8981\uff0c\u672a\u6765\u7814\u7a76\u5c06\u8fdb\u4e00\u6b65\u63a2\u8ba8\u4efb\u52a1\u590d\u6742\u6027\u3001\u63d2\u56fe\u5927\u5c0f\u548c\u7ec6\u8282\u7ea7\u522b\u7684\u5f71\u54cd\u3002"}}
{"id": "2506.21171", "pdf": "https://arxiv.org/pdf/2506.21171", "abs": "https://arxiv.org/abs/2506.21171", "authors": ["Jing Yang", "Qunliang Xing", "Mai Xu", "Minglang Qiao"], "title": "Uncover Treasures in DCT: Advancing JPEG Quality Enhancement by Exploiting Latent Correlations", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Joint Photographic Experts Group (JPEG) achieves data compression by\nquantizing Discrete Cosine Transform (DCT) coefficients, which inevitably\nintroduces compression artifacts. Most existing JPEG quality enhancement\nmethods operate in the pixel domain, suffering from the high computational\ncosts of decoding. Consequently, direct enhancement of JPEG images in the DCT\ndomain has gained increasing attention. However, current DCT-domain methods\noften exhibit limited performance. To address this challenge, we identify two\ncritical types of correlations within the DCT coefficients of JPEG images.\nBuilding on this insight, we propose an Advanced DCT-domain JPEG Quality\nEnhancement (AJQE) method that fully exploits these correlations. The AJQE\nmethod enables the adaptation of numerous well-established pixel-domain models\nto the DCT domain, achieving superior performance with reduced computational\ncomplexity. Compared to the pixel-domain counterparts, the DCT-domain models\nderived by our method demonstrate a 0.35 dB improvement in PSNR and a 60.5%\nincrease in enhancement throughput on average.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDCT\u57df\u7684JPEG\u8d28\u91cf\u589e\u5f3a\u65b9\u6cd5\uff08AJQE\uff09\uff0c\u901a\u8fc7\u5229\u7528DCT\u7cfb\u6570\u4e2d\u7684\u4e24\u79cd\u5173\u952e\u76f8\u5173\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709JPEG\u8d28\u91cf\u589e\u5f3a\u65b9\u6cd5\u591a\u5728\u50cf\u7d20\u57df\u64cd\u4f5c\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff1b\u800cDCT\u57df\u65b9\u6cd5\u6027\u80fd\u6709\u9650\u3002", "method": "\u8bc6\u522bJPEG\u56fe\u50cfDCT\u7cfb\u6570\u4e2d\u7684\u4e24\u79cd\u5173\u952e\u76f8\u5173\u6027\uff0c\u63d0\u51faAJQE\u65b9\u6cd5\uff0c\u5c06\u50cf\u7d20\u57df\u6a21\u578b\u9002\u914d\u5230DCT\u57df\u3002", "result": "\u4e0e\u50cf\u7d20\u57df\u65b9\u6cd5\u76f8\u6bd4\uff0cPSNR\u63d0\u53470.35 dB\uff0c\u589e\u5f3a\u541e\u5410\u91cf\u63d0\u9ad860.5%\u3002", "conclusion": "AJQE\u65b9\u6cd5\u5728DCT\u57df\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684JPEG\u8d28\u91cf\u589e\u5f3a\u3002"}}
{"id": "2506.20832", "pdf": "https://arxiv.org/pdf/2506.20832", "abs": "https://arxiv.org/abs/2506.20832", "authors": ["Cansu Korkmaz", "Ahmet Murat Tekalp", "Zafer Dogan"], "title": "Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 9 figures, 5 tables, accepted to IEEE Transactions on\n  Circuits and Systems for Video Technology", "summary": "Super-resolution (SR) is an ill-posed inverse problem with many feasible\nsolutions consistent with a given low-resolution image. On one hand, regressive\nSR models aim to balance fidelity and perceptual quality to yield a single\nsolution, but this trade-off often introduces artifacts that create ambiguity\nin information-critical applications such as recognizing digits or letters. On\nthe other hand, diffusion models generate a diverse set of SR images, but\nselecting the most trustworthy solution from this set remains a challenge. This\npaper introduces a robust, automated framework for identifying the most\ntrustworthy SR sample from a diffusion-generated set by leveraging the semantic\nreasoning capabilities of vision-language models (VLMs). Specifically, VLMs\nsuch as BLIP-2, GPT-4o, and their variants are prompted with structured queries\nto assess semantic correctness, visual quality, and artifact presence. The\ntop-ranked SR candidates are then ensembled to yield a single trustworthy\noutput in a cost-effective manner. To rigorously assess the validity of\nVLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid\nmetric that quantifies SR reliability based on three complementary components:\nsemantic similarity via CLIP embeddings, structural integrity using SSIM on\nedge maps, and artifact sensitivity through multi-level wavelet decomposition.\nWe empirically show that TWS correlates strongly with human preference in both\nambiguous and natural images, and that VLM-guided selections consistently yield\nhigh TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail\nto reflect information fidelity, our approach offers a principled, scalable,\nand generalizable solution for navigating the uncertainty of the diffusion SR\nspace. By aligning outputs with human expectations and semantic correctness,\nthis work sets a new benchmark for trustworthiness in generative SR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4ece\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u56fe\u50cf\u4e2d\u9009\u62e9\u6700\u53ef\u4fe1\u6837\u672c\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u6307\u6807\uff08TWS\uff09\u91cf\u5316\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfSR\u6a21\u578b\u5728\u4fe1\u606f\u5173\u952e\u5e94\u7528\u4e2d\u56e0\u5e73\u8861\u4fdd\u771f\u5ea6\u548c\u611f\u77e5\u8d28\u91cf\u800c\u5f15\u5165\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u4ee5\u53ca\u6269\u6563\u6a21\u578b\u751f\u6210\u591a\u6837SR\u56fe\u50cf\u540e\u9009\u62e9\u53ef\u4fe1\u6837\u672c\u7684\u6311\u6218\u3002", "method": "\u5229\u7528BLIP-2\u3001GPT-4o\u7b49VLM\u8fdb\u884c\u8bed\u4e49\u6b63\u786e\u6027\u3001\u89c6\u89c9\u8d28\u91cf\u548c\u4f2a\u5f71\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7TWS\uff08\u7ed3\u5408CLIP\u5d4c\u5165\u3001SSIM\u548c\u5c0f\u6ce2\u5206\u89e3\uff09\u91cf\u5316\u53ef\u9760\u6027\u3002", "result": "TWS\u4e0e\u4eba\u7c7b\u504f\u597d\u9ad8\u5ea6\u76f8\u5173\uff0cVLM\u5f15\u5bfc\u7684\u9009\u62e9\u80fd\u6301\u7eed\u83b7\u5f97\u9ad8TWS\u503c\uff0c\u4f18\u4e8e\u4f20\u7edf\u6307\u6807\uff08\u5982PSNR\u3001LPIPS\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u751f\u6210SR\u7684\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8bbe\u5b9a\u4e86\u53ef\u4fe1SR\u7684\u65b0\u57fa\u51c6\u3002"}}
{"id": "2506.21456", "pdf": "https://arxiv.org/pdf/2506.21456", "abs": "https://arxiv.org/abs/2506.21456", "authors": ["Benjamin Watson", "Neff Walker", "Larry F Hodges"], "title": "Managing level of detail through head-tracked peripheral degradation: a model and resulting design principles", "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Previous work has demonstrated the utility of reductions in the level of\ndetail (LOD) in the periphery of head-tracked, large field of view displays.\nThis paper provides a psychophysically based model, centered around an eye/head\nmovement tradeoff, that explains the effectiveness of peripheral degradation\nand suggests how peripherally degraded displays should be designed. An\nexperiment evaluating the effect on search performance of the shape and area of\nthe high detail central area (inset) in peripherally degraded displays was\nperformed, results indicated that inset shape is not a significant factor in\nperformance. Inset area, however, was significant: performance with displays\nsubtending at least 30 degrees of horizontal and vertical angle was not\nsignificantly different from performance with an undegraded display. These\nresults agreed with the proposed model.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fc3\u7406\u7269\u7406\u5b66\u7684\u6a21\u578b\uff0c\u89e3\u91ca\u4e86\u5916\u56f4\u7ec6\u8282\u964d\u4f4e\uff08LOD\uff09\u5728\u5934\u6234\u5f0f\u5927\u89c6\u573a\u663e\u793a\u5668\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e2d\u5fc3\u9ad8\u7ec6\u8282\u533a\u57df\uff08inset\uff09\u7684\u5f62\u72b6\u548c\u9762\u79ef\u5bf9\u641c\u7d22\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u5916\u56f4\u7ec6\u8282\u964d\u4f4e\uff08LOD\uff09\u5728\u5934\u6234\u5f0f\u5927\u89c6\u573a\u663e\u793a\u5668\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u5176\u8bbe\u8ba1\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u773c/\u5934\u8fd0\u52a8\u6743\u8861\u7684\u5fc3\u7406\u7269\u7406\u5b66\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u4e2d\u5fc3\u9ad8\u7ec6\u8282\u533a\u57df\u7684\u5f62\u72b6\u548c\u9762\u79ef\u5bf9\u641c\u7d22\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e2d\u5fc3\u533a\u57df\u7684\u5f62\u72b6\u5bf9\u6027\u80fd\u65e0\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u9762\u79ef\uff08\u81f3\u5c1130\u5ea6\u7684\u6c34\u5e73\u548c\u5782\u76f4\u89c6\u89d2\uff09\u662f\u5173\u952e\u56e0\u7d20\uff0c\u4e0e\u672a\u964d\u4f4e\u7ec6\u8282\u7684\u663e\u793a\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u652f\u6301\u4e86\u63d0\u51fa\u7684\u6a21\u578b\uff0c\u4e3a\u5916\u56f4\u7ec6\u8282\u964d\u4f4e\u663e\u793a\u5668\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2506.21245", "pdf": "https://arxiv.org/pdf/2506.21245", "abs": "https://arxiv.org/abs/2506.21245", "authors": ["Qifei Cui", "Xinyu Lu"], "title": "GANet-Seg: Adversarial Learning for Brain Tumor Segmentation with Hybrid Generative Models", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "This work introduces a novel framework for brain tumor segmentation\nleveraging pre-trained GANs and Unet architectures. By combining a global\nanomaly detection module with a refined mask generation network, the proposed\nmodel accurately identifies tumor-sensitive regions and iteratively enhances\nsegmentation precision using adversarial loss constraints. Multi-modal MRI data\nand synthetic image augmentation are employed to improve robustness and address\nthe challenge of limited annotated datasets. Experimental results on the BraTS\ndataset demonstrate the effectiveness of the approach, achieving high\nsensitivity and accuracy in both lesion-wise Dice and HD95 metrics than the\nbaseline. This scalable method minimizes the dependency on fully annotated\ndata, paving the way for practical real-world applications in clinical\nsettings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9884\u8bad\u7ec3GAN\u548cUnet\u7684\u8111\u80bf\u7624\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u5f02\u5e38\u68c0\u6d4b\u548c\u7cbe\u7ec6\u5316\u63a9\u6a21\u751f\u6210\u7f51\u7edc\u63d0\u9ad8\u5206\u5272\u7cbe\u5ea6\uff0c\u5229\u7528\u591a\u6a21\u6001MRI\u6570\u636e\u548c\u5408\u6210\u56fe\u50cf\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u8111\u80bf\u7624\u5206\u5272\u4e2d\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u5206\u5272\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "method": "\u7ed3\u5408\u9884\u8bad\u7ec3GAN\u548cUnet\uff0c\u5f15\u5165\u5168\u5c40\u5f02\u5e38\u68c0\u6d4b\u6a21\u5757\u548c\u7cbe\u7ec6\u5316\u63a9\u6a21\u751f\u6210\u7f51\u7edc\uff0c\u5229\u7528\u5bf9\u6297\u635f\u5931\u7ea6\u675f\u548c\u591a\u6a21\u6001MRI\u6570\u636e\u589e\u5f3a\u3002", "result": "\u5728BraTS\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0cDice\u548cHD95\u6307\u6807\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u5bf9\u5168\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5177\u6709\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.20841", "pdf": "https://arxiv.org/pdf/2506.20841", "abs": "https://arxiv.org/abs/2506.20841", "authors": ["Ha Min Son", "Shahbaz Rezaei", "Xin Liu"], "title": "FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Semi-supervised domain generalization (SSDG) aims to solve the problem of\ngeneralizing to out-of-distribution data when only a few labels are available.\nDue to label scarcity, applying domain generalization methods often\nunderperform. Consequently, existing SSDG methods combine semi-supervised\nlearning methods with various regularization terms. However, these methods do\nnot explicitly regularize to learn domains invariant representations across all\ndomains, which is a key goal for domain generalization. To address this, we\nintroduce FixCLR. Inspired by success in self-supervised learning, we change\ntwo crucial components to adapt contrastive learning for explicit domain\ninvariance regularization: utilization of class information from pseudo-labels\nand using only a repelling term. FixCLR can also be added on top of most\nexisting SSDG and semi-supervised methods for complementary performance\nimprovements. Our research includes extensive experiments that have not been\npreviously explored in SSDG studies. These experiments include benchmarking\ndifferent improvements to semi-supervised methods, evaluating the performance\nof pretrained versus non-pretrained models, and testing on datasets with many\ndomains. Overall, FixCLR proves to be an effective SSDG method, especially when\ncombined with other semi-supervised methods.", "AI": {"tldr": "FixCLR\u662f\u4e00\u79cd\u65b0\u7684\u534a\u76d1\u7763\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u663e\u5f0f\u6b63\u5219\u5316\u57df\u4e0d\u53d8\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u6807\u7b7e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u6807\u7b7e\u7a00\u7f3a\uff0c\u73b0\u6709\u534a\u76d1\u7763\u57df\u6cdb\u5316\u65b9\u6cd5\u672a\u80fd\u663e\u5f0f\u6b63\u5219\u5316\u57df\u4e0d\u53d8\u8868\u793a\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u8db3\u3002", "method": "FixCLR\u7ed3\u5408\u4f2a\u6807\u7b7e\u7684\u7c7b\u522b\u4fe1\u606f\u548c\u6392\u65a5\u9879\uff0c\u5c06\u5bf9\u6bd4\u5b66\u4e60\u5e94\u7528\u4e8e\u57df\u4e0d\u53d8\u8868\u793a\u7684\u6b63\u5219\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFixCLR\u6709\u6548\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5c24\u5176\u662f\u4e0e\u5176\u4ed6\u534a\u76d1\u7763\u65b9\u6cd5\u7ed3\u5408\u65f6\u3002", "conclusion": "FixCLR\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u534a\u76d1\u7763\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u9886\u57df\u6570\u636e\u96c6\u3002"}}
{"id": "2506.21499", "pdf": "https://arxiv.org/pdf/2506.21499", "abs": "https://arxiv.org/abs/2506.21499", "authors": ["Hojat Asgariandehkordi", "Mostafa Sharifzadeh", "Hassan Rivaz"], "title": "Lightweight Physics-Informed Zero-Shot Ultrasound Plane Wave Denoising", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Ultrasound Coherent Plane Wave Compounding (CPWC) enhances image contrast by\ncombining echoes from multiple steered transmissions. While increasing the\nnumber of angles generally improves image quality, it drastically reduces the\nframe rate and can introduce blurring artifacts in fast-moving targets.\nMoreover, compounded images remain susceptible to noise, particularly when\nacquired with a limited number of transmissions. We propose a zero-shot\ndenoising framework tailored for low-angle CPWC acquisitions, which enhances\ncontrast without relying on a separate training dataset. The method divides the\navailable transmission angles into two disjoint subsets, each used to form\ncompound images that include higher noise levels. The new compounded images are\nthen used to train a deep model via a self-supervised residual learning scheme,\nenabling it to suppress incoherent noise while preserving anatomical\nstructures. Because angle-dependent artifacts vary between the subsets while\nthe underlying tissue response is similar, this physics-informed pairing allows\nthe network to learn to disentangle the inconsistent artifacts from the\nconsistent tissue signal. Unlike supervised methods, our model requires no\ndomain-specific fine-tuning or paired data, making it adaptable across\nanatomical regions and acquisition setups. The entire pipeline supports\nefficient training with low computational cost due to the use of a lightweight\narchitecture, which comprises only two convolutional layers. Evaluations on\nsimulation, phantom, and in vivo data demonstrate superior contrast enhancement\nand structure preservation compared to both classical and deep learning-based\ndenoising methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u96f6\u6837\u672c\u53bb\u566a\u6846\u67b6\uff0c\u7528\u4e8e\u4f4e\u89d2\u5ea6CPWC\u6210\u50cf\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u5347\u56fe\u50cf\u5bf9\u6bd4\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u4f20\u7edfCPWC\u6210\u50cf\u5728\u589e\u52a0\u89d2\u5ea6\u6570\u91cf\u65f6\u867d\u80fd\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\uff0c\u4f46\u4f1a\u964d\u4f4e\u5e27\u7387\u5e76\u5f15\u5165\u6a21\u7cca\u4f2a\u5f71\uff0c\u4e14\u5bf9\u566a\u58f0\u654f\u611f\u3002", "method": "\u5c06\u4f20\u8f93\u89d2\u5ea6\u5206\u4e3a\u4e24\u4e2a\u5b50\u96c6\uff0c\u751f\u6210\u542b\u566a\u58f0\u7684\u590d\u5408\u56fe\u50cf\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u6b8b\u5dee\u5b66\u4e60\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u5206\u79bb\u566a\u58f0\u4e0e\u7ec4\u7ec7\u4fe1\u53f7\u3002", "result": "\u5728\u4eff\u771f\u3001\u4f53\u6a21\u548c\u6d3b\u4f53\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u5bf9\u6bd4\u5ea6\u548c\u7ed3\u6784\u4fdd\u6301\u4f18\u4e8e\u4f20\u7edf\u53ca\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u6216\u914d\u5bf9\u6570\u636e\uff0c\u9002\u5e94\u6027\u5f3a\uff0c\u8ba1\u7b97\u6210\u672c\u4f4e\u3002"}}
{"id": "2506.20850", "pdf": "https://arxiv.org/pdf/2506.20850", "abs": "https://arxiv.org/abs/2506.20850", "authors": ["Yuting He", "Shuo Li"], "title": "Vector Contrastive Learning For Pixel-Wise Pretraining In Medical Vision", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Contrastive learning (CL) has become a cornerstone of self-supervised\npretraining (SSP) in foundation models, however, extending CL to pixel-wise\nrepresentation, crucial for medical vision, remains an open problem. Standard\nCL formulates SSP as a binary optimization problem (binary CL) where the\nexcessive pursuit of feature dispersion leads to an over-dispersion problem,\nbreaking pixel-wise feature correlation thus disrupting the intra-class\ndistribution. Our vector CL reformulates CL as a vector regression problem,\nenabling dispersion quantification in pixel-wise pretraining via modeling\nfeature distances in regressing displacement vectors. To implement this novel\nparadigm, we propose the COntrast in VEctor Regression (COVER) framework. COVER\nestablishes an extendable vector-based self-learning, enforces a consistent\noptimization flow from vector regression to distance modeling, and leverages a\nvector pyramid architecture for granularity adaptation, thus preserving\npixel-wise feature correlations in SSP. Extensive experiments across 8 tasks,\nspanning 2 dimensions and 4 modalities, show that COVER significantly improves\npixel-wise SSP, advancing generalizable medical visual foundation models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCOVER\u7684\u5411\u91cf\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5bf9\u6bd4\u5b66\u4e60\u5728\u50cf\u7d20\u7ea7\u8868\u793a\u4e2d\u7684\u8fc7\u5206\u6563\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5bf9\u6bd4\u5b66\u4e60\u5728\u50cf\u7d20\u7ea7\u8868\u793a\u4e2d\u5b58\u5728\u8fc7\u5206\u6563\u95ee\u9898\uff0c\u7834\u574f\u4e86\u50cf\u7d20\u7ea7\u7279\u5f81\u76f8\u5173\u6027\uff0c\u5f71\u54cd\u4e86\u533b\u5b66\u89c6\u89c9\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5411\u91cf\u5bf9\u6bd4\u5b66\u4e60\uff08vector CL\uff09\uff0c\u5c06\u5bf9\u6bd4\u5b66\u4e60\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5411\u91cf\u56de\u5f52\u95ee\u9898\uff0c\u901a\u8fc7COVER\u6846\u67b6\u5b9e\u73b0\u5411\u91cf\u81ea\u5b66\u4e60\u3001\u4f18\u5316\u6d41\u7a0b\u4e00\u81f4\u6027\u548c\u7c92\u5ea6\u9002\u5e94\u3002", "result": "\u57288\u4e2a\u4efb\u52a1\u30012\u4e2a\u7ef4\u5ea6\u548c4\u79cd\u6a21\u6001\u7684\u5b9e\u9a8c\u4e2d\uff0cCOVER\u663e\u8457\u63d0\u5347\u4e86\u50cf\u7d20\u7ea7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u7684\u6027\u80fd\u3002", "conclusion": "COVER\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u50cf\u7d20\u7ea7\u8868\u793a\u4e2d\u7684\u8fc7\u5206\u6563\u95ee\u9898\uff0c\u4e3a\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.21535", "pdf": "https://arxiv.org/pdf/2506.21535", "abs": "https://arxiv.org/abs/2506.21535", "authors": ["Mohammed Baharoon", "Jun Ma", "Congyu Fang", "Augustin Toma", "Bo Wang"], "title": "Exploring the Design Space of 3D MLLMs for CT Report Generation", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have emerged as a promising way to\nautomate Radiology Report Generation (RRG). In this work, we systematically\ninvestigate the design space of 3D MLLMs, including visual input\nrepresentation, projectors, Large Language Models (LLMs), and fine-tuning\ntechniques for 3D CT report generation. We also introduce two knowledge-based\nreport augmentation methods that improve performance on the GREEN score by up\nto 10\\%, achieving the 2nd place on the MICCAI 2024 AMOS-MM challenge. Our\nresults on the 1,687 cases from the AMOS-MM dataset show that RRG is largely\nindependent of the size of LLM under the same training protocol. We also show\nthat larger volume size does not always improve performance if the original ViT\nwas pre-trained on a smaller volume size. Lastly, we show that using a\nsegmentation mask along with the CT volume improves performance. The code is\npublicly available at https://github.com/bowang-lab/AMOS-MM-Solution", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e863D\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\uff08RRG\uff09\u4e2d\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5305\u62ec\u89c6\u89c9\u8f93\u5165\u8868\u793a\u3001\u6295\u5f71\u5668\u3001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u5fae\u8c03\u6280\u672f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u77e5\u8bc6\u7684\u62a5\u544a\u589e\u5f3a\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u75283D MLLMs\u81ea\u52a8\u5316\u751f\u6210\u653e\u5c04\u5b66\u62a5\u544a\uff0c\u5e76\u4f18\u5316\u5176\u6027\u80fd\u3002", "method": "\u7814\u7a76\u4e863D MLLMs\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5305\u62ec\u89c6\u89c9\u8f93\u5165\u8868\u793a\u3001\u6295\u5f71\u5668\u3001LLMs\u548c\u5fae\u8c03\u6280\u672f\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u79cd\u77e5\u8bc6\u9a71\u52a8\u7684\u62a5\u544a\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "\u5728AMOS-MM\u6570\u636e\u96c6\u4e0a\uff0c\u6027\u80fd\u63d0\u5347\u4e8610%\uff0c\u5728MICCAI 2024\u6311\u6218\u8d5b\u4e2d\u6392\u540d\u7b2c\u4e8c\uff1b\u53d1\u73b0RRG\u6027\u80fd\u4e0eLLM\u5927\u5c0f\u65e0\u5173\uff0c\u4e14\u5206\u5272\u63a9\u7801\u7684\u4f7f\u7528\u80fd\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "3D MLLMs\u5728RRG\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u6ce8\u610f\u9884\u8bad\u7ec3\u6570\u636e\u4e0e\u6a21\u578b\u8bbe\u8ba1\u7684\u5339\u914d\uff0c\u5206\u5272\u63a9\u7801\u7684\u4f7f\u7528\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2506.20867", "pdf": "https://arxiv.org/pdf/2506.20867", "abs": "https://arxiv.org/abs/2506.20867", "authors": ["Ryosuke Kawamura", "Hideaki Hayashi", "Shunsuke Otake", "Noriko Takemura", "Hajime Nagahara"], "title": "Enhancing Ambiguous Dynamic Facial Expression Recognition with Soft Label-based Data Augmentation", "categories": ["cs.CV"], "comment": null, "summary": "Dynamic facial expression recognition (DFER) is a task that estimates\nemotions from facial expression video sequences. For practical applications,\naccurately recognizing ambiguous facial expressions -- frequently encountered\nin in-the-wild data -- is essential. In this study, we propose MIDAS, a data\naugmentation method designed to enhance DFER performance for ambiguous facial\nexpression data using soft labels representing probabilities of multiple\nemotion classes. MIDAS augments training data by convexly combining pairs of\nvideo frames and their corresponding emotion class labels. This approach\nextends mixup to soft-labeled video data, offering a simple yet highly\neffective method for handling ambiguity in DFER. To evaluate MIDAS, we\nconducted experiments on both the DFEW dataset and FERV39k-Plus, a newly\nconstructed dataset that assigns soft labels to an existing DFER dataset. The\nresults demonstrate that models trained with MIDAS-augmented data achieve\nsuperior performance compared to the state-of-the-art method trained on the\noriginal dataset.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMIDAS\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u52a8\u6001\u9762\u90e8\u8868\u60c5\u8bc6\u522b\uff08DFER\uff09\u5728\u6a21\u7cca\u8868\u60c5\u6570\u636e\u4e0a\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u8f6f\u6807\u7b7e\u548c\u89c6\u9891\u5e27\u7684\u51f8\u7ec4\u5408\u5b9e\u73b0\u3002", "motivation": "\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u6a21\u7cca\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u81ea\u7136\u573a\u666f\u6570\u636e\u4e2d\u3002", "method": "\u63d0\u51faMIDAS\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f6f\u6807\u7b7e\u548c\u89c6\u9891\u5e27\u7684\u51f8\u7ec4\u5408\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u6269\u5c55\u4e86mixup\u65b9\u6cd5\u5230\u89c6\u9891\u6570\u636e\u3002", "result": "\u5728DFEW\u548cFERV39k-Plus\u6570\u636e\u96c6\u4e0a\uff0cMIDAS\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MIDAS\u662f\u4e00\u79cd\u7b80\u5355\u800c\u9ad8\u6548\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u5904\u7406DFER\u4e2d\u7684\u6a21\u7cca\u8868\u60c5\u95ee\u9898\u3002"}}
