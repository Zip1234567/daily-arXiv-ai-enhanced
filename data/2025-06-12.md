<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 102]
- [eess.IV](#eess.IV) [Total: 8]
- [cs.GR](#cs.GR) [Total: 5]
- [physics.geo-ph](#physics.geo-ph) [Total: 2]
- [cs.RO](#cs.RO) [Total: 9]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.CR](#cs.CR) [Total: 1]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.CL](#cs.CL) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices](https://arxiv.org/abs/2506.09066)
*Maoyu Wang,Yao Lu,Jiaqi Nie,Zeyu Wang,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.CV

TL;DR: ReStNet提出了一种可重用和可拼接的网络，通过动态拼接预训练模型适应异构设备资源，解决了传统压缩方法的灵活性不足问题。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在异构IoT设备上部署困难，传统压缩方法无法动态适应资源变化。

Method: 通过CKA计算层相似度选择拼接点，拼接大模型早期层和小模型深层，仅微调拼接层。

Result: 实验表明ReStNet能灵活权衡精度与效率，显著降低训练成本。

Conclusion: ReStNet为异构设备部署提供了高效灵活的解决方案。

Abstract: With the rapid development of deep learning, a growing number of pre-trained
models have been publicly available. However, deploying these fixed models in
real-world IoT applications is challenging because different devices possess
heterogeneous computational and memory resources, making it impossible to
deploy a single model across all platforms. Although traditional compression
methods, such as pruning, quantization, and knowledge distillation, can improve
efficiency, they become inflexible once applied and cannot adapt to changing
resource constraints. To address these issues, we propose ReStNet, a Reusable
and Stitchable Network that dynamically constructs a hybrid network by
stitching two pre-trained models together. Implementing ReStNet requires
addressing several key challenges, including how to select the optimal
stitching points, determine the stitching order of the two pre-trained models,
and choose an effective fine-tuning strategy. To systematically address these
challenges and adapt to varying resource constraints, ReStNet determines the
stitching point by calculating layer-wise similarity via Centered Kernel
Alignment (CKA). It then constructs the hybrid model by retaining early layers
from a larger-capacity model and appending deeper layers from a smaller one. To
facilitate efficient deployment, only the stitching layer is fine-tuned. This
design enables rapid adaptation to changing budgets while fully leveraging
available resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,
Transformer-Transformer) and heterogeneous (CNN-Transformer) stitching,
allowing to combine different model families flexibly. Extensive experiments on
multiple benchmarks demonstrate that ReStNet achieve flexible
accuracy-efficiency trade-offs at runtime while significantly reducing training
cost.

</details>


### [2] [Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations](https://arxiv.org/abs/2506.09067)
*Zhiyu Xue,Reza Abbasi-Asl,Ramtin Pedarsani*

Main category: cs.CV

TL;DR: 本文提出了一种针对生成式医学视觉语言模型（Med-VLMs）的安全防御策略，旨在解决有害查询问题，同时避免过度防御导致的性能下降。


<details>
  <summary>Details</summary>
Motivation: Med-VLMs在生成复杂医学文本时存在安全漏洞，需要拒绝有害查询（如保险欺诈指令），但现有方法可能导致过度防御，影响正常查询的性能。

Method: 提出了一种基于合成临床演示的推理时防御策略，通过多样化的医学影像数据集验证其有效性，并引入混合演示策略以平衡安全与性能。

Result: 实验表明，该策略能有效防御视觉和文本越狱攻击，且增加演示预算可缓解过度防御问题。

Conclusion: 混合演示策略在少量演示预算下实现了安全与性能的平衡，为Med-VLMs的安全应用提供了可行方案。

Abstract: Generative medical vision-language models~(Med-VLMs) are primarily designed
to generate complex textual information~(e.g., diagnostic reports) from
multimodal inputs including vision modality~(e.g., medical images) and language
modality~(e.g., clinical queries). However, their security vulnerabilities
remain underexplored. Med-VLMs should be capable of rejecting harmful queries,
such as \textit{Provide detailed instructions for using this CT scan for
insurance fraud}. At the same time, addressing security concerns introduces the
risk of over-defense, where safety-enhancing mechanisms may degrade general
performance, causing Med-VLMs to reject benign clinical queries. In this paper,
we propose a novel inference-time defense strategy to mitigate harmful queries,
enabling defense against visual and textual jailbreak attacks. Using diverse
medical imaging datasets collected from nine modalities, we demonstrate that
our defense strategy based on synthetic clinical demonstrations enhances model
safety without significantly compromising performance. Additionally, we find
that increasing the demonstration budget alleviates the over-defense issue. We
then introduce a mixed demonstration strategy as a trade-off solution for
balancing security and performance under few-shot demonstration budget
constraints.

</details>


### [3] [BG-HOP: A Bimanual Generative Hand-Object Prior](https://arxiv.org/abs/2506.09068)
*Sriram Krishna,Sravan Chittupalli,Sungjae Park*

Main category: cs.CV

TL;DR: BG-HOP是一种生成先验模型，用于建模3D中的双手-物体交互，通过扩展单手生成先验解决数据不足问题，实验展示了其生成双手交互和抓取的能力。


<details>
  <summary>Details</summary>
Motivation: 解决双手交互数据有限的问题，扩展单手生成先验以建模双手-物体交互。

Method: 扩展现有的单手生成先验，建模双手和物体的联合分布。

Result: 模型能够生成双手交互并为给定物体合成抓取动作。

Conclusion: BG-HOP展示了在双手交互建模中的潜力，代码和模型已公开。

Abstract: In this work, we present BG-HOP, a generative prior that seeks to model
bimanual hand-object interactions in 3D. We address the challenge of limited
bimanual interaction data by extending existing single-hand generative priors,
demonstrating preliminary results in capturing the joint distribution of hands
and objects. Our experiments showcase the model's capability to generate
bimanual interactions and synthesize grasps for given objects. We make code and
models publicly available.

</details>


### [4] [Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance](https://arxiv.org/abs/2506.09071)
*Peilin Li,Jun Yin,Jing Zhong,Ran Luo,Pengyu Zeng,Miao Zhang*

Main category: cs.CV

TL;DR: SAAF模型通过多模态语义引导实现建筑立面墙窗自动分割，结合自然语言处理技术提升语义理解，实验表明其分割精度优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提高建筑信息模型和计算机辅助设计的效率，需解决墙窗自动分割问题。

Method: 提出SAAF模型，结合文本描述与图像特征，开发端到端训练框架，减少人工干预。

Result: 在多个数据集上，SAAF的mIoU指标优于现有方法，表现出高精度和泛化能力。

Conclusion: SAAF为建筑计算机视觉技术发展提供参考，探索多模态学习在建筑领域的新应用。

Abstract: In the context of the digital development of architecture, the automatic
segmentation of walls and windows is a key step in improving the efficiency of
building information models and computer-aided design. This study proposes an
automatic segmentation model for building facade walls and windows based on
multimodal semantic guidance, called Segment Any Architectural Facades (SAAF).
First, SAAF has a multimodal semantic collaborative feature extraction
mechanism. By combining natural language processing technology, it can fuse the
semantic information in text descriptions with image features, enhancing the
semantic understanding of building facade components. Second, we developed an
end-to-end training framework that enables the model to autonomously learn the
mapping relationship from text descriptions to image segmentation, reducing the
influence of manual intervention on the segmentation results and improving the
automation and robustness of the model. Finally, we conducted extensive
experiments on multiple facade datasets. The segmentation results of SAAF
outperformed existing methods in the mIoU metric, indicating that the SAAF
model can maintain high-precision segmentation ability when faced with diverse
datasets. Our model has made certain progress in improving the accuracy and
generalization ability of the wall and window segmentation task. It is expected
to provide a reference for the development of architectural computer vision
technology and also explore new ideas and technical paths for the application
of multimodal learning in the architectural field.

</details>


### [5] [VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks](https://arxiv.org/abs/2506.09079)
*Xinlong Chen,Yuanxing Zhang,Yushuo Guan,Bohan Zeng,Yang Shi,Sihan Yang,Pengfei Wan,Qiang Liu,Liang Wang,Tieniu Tan*

Main category: cs.CV

TL;DR: 论文提出了两个新数据集DarkEventInfer和MixVidQA，用于提升视频理解和推理能力，并开发了VersaVid-R1模型，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 视频推理领域因缺乏高质量数据和有效训练方法而发展不足，需填补这一空白。

Method: 引入DarkEventInfer和MixVidQA数据集，结合强化学习训练VersaVid-R1模型。

Result: VersaVid-R1在多项视频理解、推理和字幕任务中表现优异。

Conclusion: VersaVid-R1为视频推理领域提供了首个多功能模型，具有广泛的应用潜力。

Abstract: Recent advancements in multimodal large language models have successfully
extended the Reason-Then-Respond paradigm to image-based reasoning, yet
video-based reasoning remains an underdeveloped frontier, primarily due to the
scarcity of high-quality reasoning-oriented data and effective training
methodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,
two novel datasets specifically designed to stimulate the model's advanced
video understanding and reasoning abilities. DarkEventinfer presents videos
with masked event segments, requiring models to infer the obscured content
based on contextual video cues. MixVidQA, on the other hand, presents
interleaved video sequences composed of two distinct clips, challenging models
to isolate and reason about one while disregarding the other. Leveraging these
carefully curated training samples together with reinforcement learning guided
by diverse reward functions, we develop VersaVid-R1, the first versatile video
understanding and reasoning model under the Reason-Then-Respond paradigm
capable of handling multiple-choice and open-ended question answering, as well
as video captioning tasks. Extensive experiments demonstrate that VersaVid-R1
significantly outperforms existing models across a broad spectrum of
benchmarks, covering video general understanding, cognitive reasoning, and
captioning tasks.

</details>


### [6] [FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation](https://arxiv.org/abs/2506.09081)
*Zheqi He,Yesheng Liu,Jing-shu Zheng,Xuejing Li,Richeng Xuan,Jin-Ge Yao,Xi Yang*

Main category: cs.CV

TL;DR: FlagEvalMM是一个开源的多模态模型评估框架，支持多种视觉-语言任务，通过独立评估服务和高效工具提升评估效率。


<details>
  <summary>Details</summary>
Motivation: 为多模态研究提供一个全面、灵活且高效的评估工具，以促进模型性能的准确分析。

Method: 采用独立评估服务、推理加速工具（如vLLM、SGLang）和异步数据加载技术。

Result: 实验表明FlagEvalMM能高效准确地评估模型性能，揭示其优缺点。

Conclusion: FlagEvalMM是一个有价值的工具，有助于推动多模态研究的发展。

Abstract: We present FlagEvalMM, an open-source evaluation framework designed to
comprehensively assess multimodal models across a diverse range of
vision-language understanding and generation tasks, such as visual question
answering, text-to-image/video generation, and image-text retrieval. We
decouple model inference from evaluation through an independent evaluation
service, thus enabling flexible resource allocation and seamless integration of
new tasks and models. Moreover, FlagEvalMM utilizes advanced inference
acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to
significantly enhance evaluation efficiency. Extensive experiments show that
FlagEvalMM offers accurate and efficient insights into model strengths and
limitations, making it a valuable tool for advancing multimodal research. The
framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.

</details>


### [7] [AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models](https://arxiv.org/abs/2506.09082)
*Zheda Mai,Arpita Chowdhury,Zihe Wang,Sooyoung Jeon,Lemeng Wang,Jiacheng Hou,Jihyung Kil,Wei-Lun Chao*

Main category: cs.CV

TL;DR: AVA-Bench是一个新的基准测试，旨在通过解耦14种原子视觉能力（AVAs）来系统评估视觉基础模型（VFMs），解决了现有VQA基准测试的数据对齐和多能力混淆问题。


<details>
  <summary>Details</summary>
Motivation: 现有VQA基准测试存在两个主要问题：指令调优数据与测试分布不匹配，以及多能力任务难以定位具体缺陷。AVA-Bench旨在解决这些问题，提供更精确的评估方法。

Method: 引入AVA-Bench，通过解耦14种原子视觉能力（如定位、深度估计等），并在每种能力内对齐训练和测试分布，从而精确评估VFMs的表现。

Result: AVA-Bench揭示了VFMs的独特“能力指纹”，并发现较小的LLM（0.5B）在评估效率上优于较大的LLM（7B），节省了8倍的GPU时间。

Conclusion: AVA-Bench为下一代VFMs提供了全面透明的评估基准，使模型选择从猜测转变为有原则的工程。

Abstract: The rise of vision foundation models (VFMs) calls for systematic evaluation.
A common approach pairs VFMs with large language models (LLMs) as
general-purpose heads, followed by evaluation on broad Visual Question
Answering (VQA) benchmarks. However, this protocol has two key blind spots: (i)
the instruction tuning data may not align with VQA test distributions, meaning
a wrong prediction can stem from such data mismatch rather than a VFM' visual
shortcomings; (ii) VQA benchmarks often require multiple visual abilities,
making it hard to tell whether errors stem from lacking all required abilities
or just a single critical one. To address these gaps, we introduce AVA-Bench,
the first benchmark that explicitly disentangles 14 Atomic Visual Abilities
(AVAs) -- foundational skills like localization, depth estimation, and spatial
understanding that collectively support complex visual reasoning tasks. By
decoupling AVAs and matching training and test distributions within each,
AVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench
to leading VFMs thus reveals distinctive "ability fingerprints," turning VFM
selection from educated guesswork into principled engineering. Notably, we find
that a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours
by 8x, enabling more efficient evaluation. By offering a comprehensive and
transparent benchmark, we hope AVA-Bench lays the foundation for the next
generation of VFMs.

</details>


### [8] [BakuFlow: A Streamlining Semi-Automatic Label Generation Tool](https://arxiv.org/abs/2506.09083)
*Jerry Lin,Partick P. W. Chen*

Main category: cs.CV

TL;DR: BakuFlow是一款半自动标注工具，通过像素级手动修正、交互式数据增强、标签传播和自动标注模块，显著提升计算机视觉任务的标注效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 大规模计算机视觉任务中，手动标注耗时且易错，现有工具仍需人工逐图标注，亟需更高效的解决方案。

Method: BakuFlow结合可调放大镜（精确修正）、交互式数据增强（多样化数据集）、标签传播（快速复制标注）和改进的YOLOE框架（灵活自动标注）。

Result: BakuFlow显著减少了标注工作量，提升了效率，特别适用于目标检测和跟踪任务。

Conclusion: BakuFlow为动态、真实世界数据集提供了灵活且可扩展的标注方案，适用于实际计算机视觉和工业场景。

Abstract: Accurately labeling (or annotation) data is still a bottleneck in computer
vision, especially for large-scale tasks where manual labeling is
time-consuming and error-prone. While tools like LabelImg can handle the
labeling task, some of them still require annotators to manually label each
image. In this paper, we introduce BakuFlow, a streamlining semi-automatic
label generation tool. Key features include (1) a live adjustable magnifier for
pixel-precise manual corrections, improving user experience; (2) an interactive
data augmentation module to diversify training datasets; (3) label propagation
for rapidly copying labeled objects between consecutive frames, greatly
accelerating annotation of video data; and (4) an automatic labeling module
powered by a modified YOLOE framework. Unlike the original YOLOE, our extension
supports adding new object classes and any number of visual prompts per class
during annotation, enabling flexible and scalable labeling for dynamic,
real-world datasets. These innovations make BakuFlow especially effective for
object detection and tracking, substantially reducing labeling workload and
improving efficiency in practical computer vision and industrial scenarios.

</details>


### [9] [Bias Analysis in Unconditional Image Generative Models](https://arxiv.org/abs/2506.09106)
*Xiaofeng Zhang,Michelle Lin,Simon Lacoste-Julien,Aaron Courville,Yash Goyal*

Main category: cs.CV

TL;DR: 研究发现生成AI模型中的属性偏移较小，但评估框架中使用的分类器对结果敏感，尤其是对连续属性。需改进标签实践和评估框架。


<details>
  <summary>Details</summary>
Motivation: 生成AI模型的广泛使用引发了对代表性伤害和歧视性结果的担忧，但无条件生成中偏见的机制尚不明确。

Method: 训练无条件图像生成模型，采用常用偏见评估框架研究训练与生成分布间的偏见偏移。

Result: 实验显示属性偏移较小，但分类器对结果敏感，尤其是连续属性。

Conclusion: 需改进标签实践、更严格审查评估框架，并认识到属性在社会复杂性中的影响。

Abstract: The widespread adoption of generative AI models has raised growing concerns
about representational harm and potential discriminatory outcomes. Yet, despite
growing literature on this topic, the mechanisms by which bias emerges -
especially in unconditional generation - remain disentangled. We define the
bias of an attribute as the difference between the probability of its presence
in the observed distribution and its expected proportion in an ideal reference
distribution. In our analysis, we train a set of unconditional image generative
models and adopt a commonly used bias evaluation framework to study bias shift
between training and generated distributions. Our experiments reveal that the
detected attribute shifts are small. We find that the attribute shifts are
sensitive to the attribute classifier used to label generated images in the
evaluation framework, particularly when its decision boundaries fall in
high-density regions. Our empirical analysis indicates that this classifier
sensitivity is often observed in attributes values that lie on a spectrum, as
opposed to exhibiting a binary nature. This highlights the need for more
representative labeling practices, understanding the shortcomings through
greater scrutiny of evaluation frameworks, and recognizing the socially complex
nature of attributes when evaluating bias.

</details>


### [10] [Generalized Gaussian Entropy Model for Point Cloud Attribute Compression with Dynamic Likelihood Intervals](https://arxiv.org/abs/2506.09510)
*Changhao Peng,Yuqi Ye,Wei Gao*

Main category: cs.CV

TL;DR: 论文提出了一种广义高斯熵模型和动态调整似然区间的Mean Error Discriminator（MED），显著提升了点云属性压缩的性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法中，熵参数估计存在未充分利用的信息，且固定似然区间限制了模型性能。

Method: 引入广义高斯熵模型控制尾部形状，并提出MED动态调整似然区间。

Result: 实验表明，该方法在三个VAE模型中显著提升了率失真性能，并适用于其他压缩任务。

Conclusion: 广义高斯熵模型和MED为点云属性压缩提供了更准确的概率估计和动态优化，具有广泛适用性。

Abstract: Gaussian and Laplacian entropy models are proved effective in learned point
cloud attribute compression, as they assist in arithmetic coding of latents.
However, we demonstrate through experiments that there is still unutilized
information in entropy parameters estimated by neural networks in current
methods, which can be used for more accurate probability estimation. Thus we
introduce generalized Gaussian entropy model, which controls the tail shape
through shape parameter to more accurately estimate the probability of latents.
Meanwhile, to the best of our knowledge, existing methods use fixed likelihood
intervals for each integer during arithmetic coding, which limits model
performance. We propose Mean Error Discriminator (MED) to determine whether the
entropy parameter estimation is accurate and then dynamically adjust likelihood
intervals. Experiments show that our method significantly improves
rate-distortion (RD) performance on three VAE-based models for point cloud
attribute compression, and our method can be applied to other compression
tasks, such as image and video compression.

</details>


### [11] [CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation](https://arxiv.org/abs/2506.09109)
*Arnav Yayavaram,Siddharth Yayavaram,Simran Khanuja,Michael Saxon,Graham Neubig*

Main category: cs.CV

TL;DR: CAIRe是一种新的评估指标，用于衡量图像生成模型在不同文化背景下的表现，解决了现有方法在测量文化偏见方面的不足。


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像模型的普及，确保其在多元文化背景下的公平性能至关重要，但目前缺乏可靠的文化偏见测量方法。

Method: CAIRe通过将图像中的实体和概念与知识库关联，利用事实信息为每个文化标签提供独立评分。

Result: CAIRe在手动构建的数据集上比基线方法提升了28%的F1分数，并在两个文化通用概念数据集上与人类评分相关性达到0.56和0.66。

Conclusion: CAIRe能够有效评估图像生成模型的文化相关性，并与人类判断高度一致。

Abstract: As text-to-image models become increasingly prevalent, ensuring their
equitable performance across diverse cultural contexts is critical. Efforts to
mitigate cross-cultural biases have been hampered by trade-offs, including a
loss in performance, factual inaccuracies, or offensive outputs. Despite
widespread recognition of these challenges, an inability to reliably measure
these biases has stalled progress. To address this gap, we introduce CAIRe, a
novel evaluation metric that assesses the degree of cultural relevance of an
image, given a user-defined set of labels. Our framework grounds entities and
concepts in the image to a knowledge base and uses factual information to give
independent graded judgments for each culture label. On a manually curated
dataset of culturally salient but rare items built using language models, CAIRe
surpasses all baselines by 28% F1 points. Additionally, we construct two
datasets for culturally universal concept, one comprising of T2I-generated
outputs and another retrieved from naturally occurring data. CAIRe achieves
Pearson's correlations of 0.56 and 0.66 with human ratings on these sets, based
on a 5-point Likert scale of cultural relevance. This demonstrates its strong
alignment with human judgment across diverse image sources.

</details>


### [12] [HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios](https://arxiv.org/abs/2506.09650)
*Kunyu Peng,Junchao Huang,Xiangsheng Huang,Di Wen,Junwei Zheng,Yufan Chen,Kailun Yang,Jiamin Wu,Chongqing Hao,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 论文提出了一种基于文本参考的多人物动作分割方法，并发布了首个相关数据集RHAS133。通过HopaDIFF框架，结合跨输入门注意力xLSTM和傅里叶条件，显著提升了动作分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对单人物固定动作序列，忽略了多人物场景。本文旨在解决多人物环境下的动作分割问题，并通过文本描述指定目标人物。

Method: 提出了HopaDIFF框架，结合跨输入门注意力xLSTM进行全局-局部长程推理，并引入傅里叶条件以精细化控制动作分割生成。

Result: HopaDIFF在RHAS133数据集上取得了最先进的性能。

Conclusion: 该方法在多人物动作分割任务中表现优异，为未来研究提供了新方向。

Abstract: Action segmentation is a core challenge in high-level video understanding,
aiming to partition untrimmed videos into segments and assign each a label from
a predefined action set. Existing methods primarily address single-person
activities with fixed action sequences, overlooking multi-person scenarios. In
this work, we pioneer textual reference-guided human action segmentation in
multi-person settings, where a textual description specifies the target person
for segmentation. We introduce the first dataset for Referring Human Action
Segmentation, i.e., RHAS133, built from 133 movies and annotated with 137
fine-grained actions with 33h video data, together with textual descriptions
for this new task. Benchmarking existing action recognition methods on RHAS133
using VLM-based feature extractors reveals limited performance and poor
aggregation of visual cues for the target person. To address this, we propose a
holistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,
leveraging a novel cross-input gate attentional xLSTM to enhance
holistic-partial long-range reasoning and a novel Fourier condition to
introduce more fine-grained control to improve the action segmentation
generation. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse
evaluation settings. The code is available at
https://github.com/KPeng9510/HopaDIFF.git.

</details>


### [13] [Seedance 1.0: Exploring the Boundaries of Video Generation Models](https://arxiv.org/abs/2506.09113)
*Yu Gao,Haoyuan Guo,Tuyen Hoang,Weilin Huang,Lu Jiang,Fangyuan Kong,Huixia Li,Jiashi Li,Liang Li,Xiaojie Li,Xunsong Li,Yifu Li,Shanchuan Lin,Zhijie Lin,Jiawei Liu,Shu Liu,Xiaonan Nie,Zhiwu Qing,Yuxi Ren,Li Sun,Zhi Tian,Rui Wang,Sen Wang,Guoqiang Wei,Guohong Wu,Jie Wu,Ruiqi Xia,Fei Xiao,Xuefeng Xiao,Jiangqiao Yan,Ceyuan Yang,Jianchao Yang,Runkai Yang,Tao Yang,Yihang Yang,Zilyu Ye,Xuejiao Zeng,Yan Zeng,Heng Zhang,Yang Zhao,Xiaozheng Zheng,Peihao Zhu,Jiaxin Zou,Feilong Zuo*

Main category: cs.CV

TL;DR: Seedance 1.0是一款高性能视频生成基础模型，通过多源数据增强、高效架构设计、优化训练方法和加速策略，显著提升了视频生成的提示跟随、运动合理性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型在视频生成中难以平衡提示跟随、运动合理性和视觉质量，Seedance 1.0旨在解决这些问题。

Method: 结合多源数据增强、高效架构设计、联合学习多任务、优化训练方法（如RLHF）和模型加速策略（如蒸馏）。

Result: 生成5秒1080p视频仅需41.4秒，具备高质量、快速生成、时空流畅性和多主题一致性。

Conclusion: Seedance 1.0在视频生成中表现卓越，优于现有技术，具备高效性和高质量输出。

Abstract: Notable breakthroughs in diffusion modeling have propelled rapid improvements
in video generation, yet current foundational model still face critical
challenges in simultaneously balancing prompt following, motion plausibility,
and visual quality. In this report, we introduce Seedance 1.0, a
high-performance and inference-efficient video foundation generation model that
integrates several core technical improvements: (i) multi-source data curation
augmented with precision and meaningful video captioning, enabling
comprehensive learning across diverse scenarios; (ii) an efficient architecture
design with proposed training paradigm, which allows for natively supporting
multi-shot generation and jointly learning of both text-to-video and
image-to-video tasks. (iii) carefully-optimized post-training approaches
leveraging fine-grained supervised fine-tuning, and video-specific RLHF with
multi-dimensional reward mechanisms for comprehensive performance improvements;
(iv) excellent model acceleration achieving ~10x inference speedup through
multi-stage distillation strategies and system-level optimizations. Seedance
1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds
(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance
1.0 stands out with high-quality and fast video generation having superior
spatiotemporal fluidity with structural stability, precise instruction
adherence in complex multi-subject contexts, native multi-shot narrative
coherence with consistent subject representation.

</details>


### [14] [Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models](https://arxiv.org/abs/2506.09229)
*Sungwon Hwang,Hyojin Jang,Kinam Kim,Minho Park,Jaegul choo*

Main category: cs.CV

TL;DR: 论文提出了一种新的正则化技术CREPA，用于改进视频扩散模型（VDMs）的微调，通过跨帧对齐隐藏状态提升视觉保真度和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 用户级微调VDMs以生成反映特定训练数据属性的视频存在挑战，且研究不足。

Method: 提出Cross-frame Representation Alignment (CREPA)，通过将帧的隐藏状态与相邻帧的外部特征对齐来优化VDMs。

Result: 在CogVideoX-5B和Hunyuan Video等大规模VDMs上，CREPA显著提升了视觉质量和跨帧语义一致性。

Conclusion: CREPA是一种广泛适用的方法，能有效改进VDMs的微调效果。

Abstract: Fine-tuning Video Diffusion Models (VDMs) at the user level to generate
videos that reflect specific attributes of training data presents notable
challenges, yet remains underexplored despite its practical importance.
Meanwhile, recent work such as Representation Alignment (REPA) has shown
promise in improving the convergence and quality of DiT-based image diffusion
models by aligning, or assimilating, its internal hidden states with external
pretrained visual features, suggesting its potential for VDM fine-tuning. In
this work, we first propose a straightforward adaptation of REPA for VDMs and
empirically show that, while effective for convergence, it is suboptimal in
preserving semantic consistency across frames. To address this limitation, we
introduce Cross-frame Representation Alignment (CREPA), a novel regularization
technique that aligns hidden states of a frame with external features from
neighboring frames. Empirical evaluations on large-scale VDMs, including
CogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual
fidelity and cross-frame semantic coherence when fine-tuned with
parameter-efficient methods such as LoRA. We further validate CREPA across
diverse datasets with varying attributes, confirming its broad applicability.
Project page: https://crepavideo.github.io

</details>


### [15] [PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies](https://arxiv.org/abs/2506.09237)
*Mojtaba Nafez,Amirhossein Koochakian,Arad Maleki,Jafar Habibi,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: PatchGuard是一种基于Vision Transformer的对抗性鲁棒异常检测与定位方法，通过引入伪异常样本和定位掩码，显著提升了在对抗环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 当前异常检测与定位方法因训练数据仅包含正常样本而易受对抗攻击，PatchGuard旨在解决这一问题。

Method: 利用前景感知伪异常样本，结合ViT架构和对抗训练，通过新型损失函数提升模型鲁棒性。

Result: 在工业和医学数据集上，PatchGuard在对抗环境下AD性能提升53.2%，AL提升68.5%，同时在非对抗环境下保持竞争力。

Conclusion: PatchGuard通过伪异常样本和理论驱动的设计，显著提升了异常检测与定位的对抗鲁棒性。

Abstract: Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields
that demand high reliability, such as medical imaging and industrial
monitoring. However, current AD and AL approaches are often susceptible to
adversarial attacks due to limitations in training data, which typically
include only normal, unlabeled samples. This study introduces PatchGuard, an
adversarially robust AD and AL method that incorporates pseudo anomalies with
localization masks within a Vision Transformer (ViT)-based architecture to
address these vulnerabilities. We begin by examining the essential properties
of pseudo anomalies, and follow it by providing theoretical insights into the
attention mechanisms required to enhance the adversarial robustness of AD and
AL systems. We then present our approach, which leverages Foreground-Aware
Pseudo-Anomalies to overcome the deficiencies of previous anomaly-aware
methods. Our method incorporates these crafted pseudo-anomaly samples into a
ViT-based framework, with adversarial training guided by a novel loss function
designed to improve model robustness, as supported by our theoretical analysis.
Experimental results on well-established industrial and medical datasets
demonstrate that PatchGuard significantly outperforms previous methods in
adversarial settings, achieving performance gains of $53.2\%$ in AD and
$68.5\%$ in AL, while also maintaining competitive accuracy in non-adversarial
settings. The code repository is available at
https://github.com/rohban-lab/PatchGuard .

</details>


### [16] [UFM: A Simple Path towards Unified Dense Correspondence with Flow](https://arxiv.org/abs/2506.09278)
*Yuchen Zhang,Nikhil Keetha,Chenwei Lyu,Bhuvan Jhamb,Yutian Chen,Yuheng Qiu,Jay Karhade,Shreyas Jha,Yaoyu Hu,Deva Ramanan,Sebastian Scherer,Wenshan Wang*

Main category: cs.CV

TL;DR: UFM模型通过统一训练在光流估计和宽基线匹配任务中均优于专用方法，实现了快速、通用的图像对应。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在光流估计和宽基线匹配任务中分离处理的问题，探索统一训练的可能性。

Method: 使用简单的通用Transformer架构，直接回归(u,v)光流，避免了传统粗到细成本体积的复杂性。

Result: UFM在光流任务中比Unimatch准确28%，在宽基线匹配中比RoMa误差减少62%，速度快6.7倍。

Conclusion: 统一训练在多个领域均优于专用方法，为多模态、长距离和实时对应任务开辟了新方向。

Abstract: Dense image correspondence is central to many applications, such as visual
odometry, 3D reconstruction, object association, and re-identification.
Historically, dense correspondence has been tackled separately for
wide-baseline scenarios and optical flow estimation, despite the common goal of
matching content between two images. In this paper, we develop a Unified Flow &
Matching model (UFM), which is trained on unified data for pixels that are
co-visible in both source and target images. UFM uses a simple, generic
transformer architecture that directly regresses the (u,v) flow. It is easier
to train and more accurate for large flows compared to the typical
coarse-to-fine cost volumes in prior work. UFM is 28% more accurate than
state-of-the-art flow methods (Unimatch), while also having 62% less error and
6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to
demonstrate that unified training can outperform specialized approaches across
both domains. This result enables fast, general-purpose correspondence and
opens new directions for multi-modal, long-range, and real-time correspondence
tasks.

</details>


### [17] [Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery](https://arxiv.org/abs/2506.09299)
*Sindhu Boddu,Arindam Mukherjee*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级且节能的空中图像目标检测方案，适用于应急响应场景。通过优化YOLOv4-Tiny模型并采用INT8量化，显著减小模型体积并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集中缺乏无人机视角的应急图像，因此作者自建数据集并优化模型，以满足实时应急检测需求。

Method: 采用YOLOv4-Tiny模型，通过后训练量化优化至INT8精度，并在自建数据集（10,820张标注图像）上训练。

Result: 量化后的模型体积减小71%（6.4 MB），推理速度提升44%，检测性能与YOLOv5-small相当。

Conclusion: 量化YOLOv4-Tiny模型适合在低功耗边缘设备上实现实时应急检测。

Abstract: This paper presents a lightweight and energy-efficient object detection
solution for aerial imagery captured during emergency response situations. We
focus on deploying the YOLOv4-Tiny model, a compact convolutional neural
network, optimized through post-training quantization to INT8 precision. The
model is trained on a custom-curated aerial emergency dataset, consisting of
10,820 annotated images covering critical emergency scenarios. Unlike prior
works that rely on publicly available datasets, we created this dataset
ourselves due to the lack of publicly available drone-view emergency imagery,
making the dataset itself a key contribution of this work. The quantized model
is evaluated against YOLOv5-small across multiple metrics, including mean
Average Precision (mAP), F1 score, inference time, and model size. Experimental
results demonstrate that the quantized YOLOv4-Tiny achieves comparable
detection performance while reducing the model size from 22.5 MB to 6.4 MB and
improving inference speed by 44\%. With a 71\% reduction in model size and a
44\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly
suitable for real-time emergency detection on low-power edge devices.

</details>


### [18] [Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5](https://arxiv.org/abs/2506.09300)
*Sindhu Boddu,Arindam Mukherjee*

Main category: cs.CV

TL;DR: 论文展示了量化YOLOv4-Tiny模型在树莓派5上的实时目标检测性能，量化后模型在功耗和速度上表现优异，适用于紧急响应应用。


<details>
  <summary>Details</summary>
Motivation: 研究资源受限的边缘设备（如树莓派5）上部署实时目标检测模型的可行性，以满足紧急响应场景的需求。

Method: 使用TensorFlow Lite后训练量化技术将YOLOv4-Tiny模型量化为INT8精度，并评估其检测速度、功耗和热性能。

Result: 量化模型每张图像推理时间为28.2毫秒，平均功耗13.85瓦，相比FP32版本显著降低功耗，同时保持对关键紧急类别的检测准确性。

Conclusion: 量化模型在低功耗嵌入式AI系统中具有实时部署潜力，适用于安全关键的紧急响应应用。

Abstract: This paper presents the deployment and performance evaluation of a quantized
YOLOv4-Tiny model for real-time object detection in aerial emergency imagery on
a resource-constrained edge device the Raspberry Pi 5. The YOLOv4-Tiny model
was quantized to INT8 precision using TensorFlow Lite post-training
quantization techniques and evaluated for detection speed, power consumption,
and thermal feasibility under embedded deployment conditions. The quantized
model achieved an inference time of 28.2 ms per image with an average power
consumption of 13.85 W, demonstrating a significant reduction in power usage
compared to its FP32 counterpart. Detection accuracy remained robust across key
emergency classes such as Ambulance, Police, Fire Engine, and Car Crash. These
results highlight the potential of low-power embedded AI systems for real-time
deployment in safety-critical emergency response applications.

</details>


### [19] [MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning](https://arxiv.org/abs/2506.09327)
*Tong Wang,Guanzhou Chen,Xiaodong Zhang,Chenxi Liu,Jiaqi Wang,Xiaoliang Tan,Wenchao Guo,Qingyuan Yang,Kaiqi Zhang*

Main category: cs.CV

TL;DR: 提出了一种多模态自监督学习框架，通过RGB图像、多光谱数据和数字表面模型（DSM）进行预训练，显著提升遥感图像解释任务的性能。


<details>
  <summary>Details</summary>
Motivation: 高质量标注数据获取成本高且耗时，亟需一种高效的自监督学习方法。

Method: 设计了信息感知自适应掩码策略、跨模态掩码机制和多任务自监督目标，捕捉多模态间的相关性和单模态内的特征结构。

Result: 在15个遥感数据集上的26个任务中表现优异，如Potsdam和Vaihingen语义分割任务分别达到78.30%和76.50%的mIoU，US3D深度估计任务的RMSE降至0.182。

Conclusion: 该方法在多模态遥感图像解释任务中显著优于现有预训练方法，具有广泛应用潜力。

Abstract: Remote sensing image interpretation plays a critical role in environmental
monitoring, urban planning, and disaster assessment. However, acquiring
high-quality labeled data is often costly and time-consuming. To address this
challenge, we proposes a multi-modal self-supervised learning framework that
leverages high-resolution RGB images, multi-spectral data, and digital surface
models (DSM) for pre-training. By designing an information-aware adaptive
masking strategy, cross-modal masking mechanism, and multi-task self-supervised
objectives, the framework effectively captures both the correlations across
different modalities and the unique feature structures within each modality. We
evaluated the proposed method on multiple downstream tasks, covering typical
remote sensing applications such as scene classification, semantic
segmentation, change detection, object detection, and depth estimation.
Experiments are conducted on 15 remote sensing datasets, encompassing 26 tasks.
The results demonstrate that the proposed method outperforms existing
pretraining approaches in most tasks. Specifically, on the Potsdam and
Vaihingen semantic segmentation tasks, our method achieved mIoU scores of
78.30\% and 76.50\%, with only 50\% train-set. For the US3D depth estimation
task, the RMSE error is reduced to 0.182, and for the binary change detection
task in SECOND dataset, our method achieved mIoU scores of 47.51\%, surpassing
the second CS-MAE by 3 percentage points. Our pretrain code, checkpoints, and
HR-Pairs dataset can be found in https://github.com/CVEO/MSSDF.

</details>


### [20] [CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation](https://arxiv.org/abs/2506.09343)
*Yuxing Long,Jiyao Zhang,Mingjie Pan,Tianshu Wu,Taewhan Kim,Hao Dong*

Main category: cs.CV

TL;DR: 论文提出首个基于手册的家电操作基准CheckManual，通过大模型辅助人工修订的数据生成流程创建手册，并设计相关挑战、指标和仿真环境。


<details>
  <summary>Details</summary>
Motivation: 家电的正确使用显著提升生活质量，但现有研究忽视手册在多页手册理解和操作规划中的重要性。

Method: 设计大模型辅助人工修订的数据生成流程，创建手册并建立基准，提出基于手册的操作规划模型ManualPlan。

Result: 建立了CheckManual基准，包括手册生成、挑战设计和仿真环境，并提出了ManualPlan模型作为基线。

Conclusion: CheckManual为家电操作研究提供了首个基于手册的基准，ManualPlan模型展示了手册在操作规划中的潜力。

Abstract: Correct use of electrical appliances has significantly improved human life
quality. Unlike simple tools that can be manipulated with common sense,
different parts of electrical appliances have specific functions defined by
manufacturers. If we want the robot to heat bread by microwave, we should
enable them to review the microwave manual first. From the manual, it can learn
about component functions, interaction methods, and representative task steps
about appliances. However, previous manual-related works remain limited to
question-answering tasks while existing manipulation researchers ignore the
manual's important role and fail to comprehend multi-page manuals. In this
paper, we propose the first manual-based appliance manipulation benchmark
CheckManual. Specifically, we design a large model-assisted human-revised data
generation pipeline to create manuals based on CAD appliance models. With these
manuals, we establish novel manual-based manipulation challenges, metrics, and
simulator environments for model performance evaluation. Furthermore, we
propose the first manual-based manipulation planning model ManualPlan to set up
a group of baselines for the CheckManual benchmark.

</details>


### [21] [An Effective End-to-End Solution for Multimodal Action Recognition](https://arxiv.org/abs/2506.09345)
*Songping Wang,Xiantao Hu,Yueming Lyu,Caifeng Shan*

Main category: cs.CV

TL;DR: 提出了一种多模态动作识别解决方案，通过数据增强、迁移学习、多模态时空特征提取和预测增强方法，实现了高精度识别。


<details>
  <summary>Details</summary>
Motivation: 由于三模态数据的稀缺性，多模态动作识别任务面临挑战，需要有效利用多模态信息。

Method: 优化数据增强技术扩展训练规模，利用RGB数据集预训练骨干网络，结合2D CNNs和TSM提取时空特征，并采用SWA、Ensemble和TTA等预测增强方法。

Result: 在竞赛排行榜上达到Top-1准确率99%和Top-5准确率100%。

Conclusion: 该解决方案在多模态动作识别任务中表现出优越性。

Abstract: Recently, multimodal tasks have strongly advanced the field of action
recognition with their rich multimodal information. However, due to the
scarcity of tri-modal data, research on tri-modal action recognition tasks
faces many challenges. To this end, we have proposed a comprehensive multimodal
action recognition solution that effectively utilizes multimodal information.
First, the existing data are transformed and expanded by optimizing data
enhancement techniques to enlarge the training scale. At the same time, more
RGB datasets are used to pre-train the backbone network, which is better
adapted to the new task by means of transfer learning. Secondly, multimodal
spatial features are extracted with the help of 2D CNNs and combined with the
Temporal Shift Module (TSM) to achieve multimodal spatial-temporal feature
extraction comparable to 3D CNNs and improve the computational efficiency. In
addition, common prediction enhancement methods, such as Stochastic Weight
Averaging (SWA), Ensemble and Test-Time augmentation (TTA), are used to
integrate the knowledge of models from different training periods of the same
architecture and different architectures, so as to predict the actions from
different perspectives and fully exploit the target information. Ultimately, we
achieved the Top-1 accuracy of 99% and the Top-5 accuracy of 100% on the
competition leaderboard, demonstrating the superiority of our solution.

</details>


### [22] [Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation](https://arxiv.org/abs/2506.09350)
*Shanchuan Lin,Ceyuan Yang,Hao He,Jianwen Jiang,Yuxi Ren,Xin Xia,Yang Zhao,Xuefeng Xiao,Lu Jiang*

Main category: cs.CV

TL;DR: 提出了一种自回归对抗后训练方法（AAPT），将预训练的潜在视频扩散模型转化为实时交互式视频生成器，支持单步生成和交互控制。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视频生成模型计算量大，无法满足实时和交互应用需求。

Method: 采用自回归对抗训练，单步生成潜在帧，并利用KV缓存提高效率，同时通过学生强制训练减少长视频生成中的误差累积。

Result: 8B模型在单H100上实现24fps、736x416分辨率的实时视频生成，或在8xH100上支持1280x720分辨率长达1分钟的视频。

Conclusion: AAPT是一种高效、实时的视频生成方法，适用于交互式应用。

Abstract: Existing large-scale video generation models are computationally intensive,
preventing adoption in real-time and interactive applications. In this work, we
propose autoregressive adversarial post-training (AAPT) to transform a
pre-trained latent video diffusion model into a real-time, interactive video
generator. Our model autoregressively generates a latent frame at a time using
a single neural function evaluation (1NFE). The model can stream the result to
the user in real time and receive interactive responses as controls to generate
the next latent frame. Unlike existing approaches, our method explores
adversarial training as an effective paradigm for autoregressive generation.
This not only allows us to design an architecture that is more efficient for
one-step generation while fully utilizing the KV cache, but also enables
training the model in a student-forcing manner that proves to be effective in
reducing error accumulation during long video generation. Our experiments
demonstrate that our 8B model achieves real-time, 24fps, streaming video
generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to
a minute long (1440 frames). Visit our research website at
https://seaweed-apt.com/2

</details>


### [23] [A new approach for image segmentation based on diffeomorphic registration and gradient fields](https://arxiv.org/abs/2506.09357)
*Junchao Zhou*

Main category: cs.CV

TL;DR: 提出了一种基于变分框架和微分同胚变换的2D图像分割方法，结合形状分析和LDDMM框架，无需依赖大数据集。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖大量训练数据，而深度学习方法需要大量数据，因此需要一种更灵活且理论基础扎实的方法。

Method: 通过微分同胚变换将模板曲线变形，利用LDDMM框架和变分表示，结合图像梯度场指导曲线演化。

Result: 实现了高精度的图像分割，方法灵活且不依赖大数据集。

Conclusion: 该框架为图像分割提供了一种理论支持强且实用的解决方案。

Abstract: Image segmentation is a fundamental task in computer vision aimed at
delineating object boundaries within images. Traditional approaches, such as
edge detection and variational methods, have been widely explored, while recent
advances in deep learning have shown promising results but often require
extensive training data. In this work, we propose a novel variational framework
for 2D image segmentation that integrates concepts from shape analysis and
diffeomorphic transformations. Our method models segmentation as the
deformation of a template curve via a diffeomorphic transformation of the image
domain, using the Large Deformation Diffeomorphic Metric Mapping (LDDMM)
framework. The curve evolution is guided by a loss function that compares the
deformed curve to the image gradient field, formulated through the varifold
representation of geometric shapes. The approach is implemented in Python with
GPU acceleration using the PyKeops library. This framework allows for accurate
segmentation with a flexible and theoretically grounded methodology that does
not rely on large datasets.

</details>


### [24] [SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing](https://arxiv.org/abs/2506.09363)
*Hongguang Zhu,Yunchao Wei,Mengyu Wang,Siyu Jiao,Yan Fang,Jiannan Huang,Yao Zhao*

Main category: cs.CV

TL;DR: SAGE提出了一种语义增强擦除方法，通过循环自检和自擦除将概念词擦除转化为概念域擦除，同时结合全局-局部协作保留机制，显著提升了扩散模型的安全生成能力。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本到图像生成中表现优异，但预训练中可能包含敏感信息，导致安全风险。现有方法仅针对固定词进行擦除，限制了泛化能力。

Method: SAGE通过语义增强擦除探索概念域的边界表示，并结合全局-局部协作保留机制，避免无关概念的退化。

Result: 实验表明，SAGE在安全生成方面全面优于其他方法。

Conclusion: SAGE为扩散模型的安全生成提供了高效且泛化的解决方案。

Abstract: Diffusion models (DMs) have achieved significant progress in text-to-image
generation. However, the inevitable inclusion of sensitive information during
pre-training poses safety risks, such as unsafe content generation and
copyright infringement. Concept erasing finetunes weights to unlearn
undesirable concepts, and has emerged as a promising solution. However,
existing methods treat unsafe concept as a fixed word and repeatedly erase it,
trapping DMs in ``word concept abyss'', which prevents generalized
concept-related erasing. To escape this abyss, we introduce semantic-augment
erasing which transforms concept word erasure into concept domain erasure by
the cyclic self-check and self-erasure. It efficiently explores and unlearns
the boundary representation of concept domain through semantic spatial
relationships between original and training DMs, without requiring additional
preprocessed data. Meanwhile, to mitigate the retention degradation of
irrelevant concepts while erasing unsafe concepts, we further propose the
global-local collaborative retention mechanism that combines global semantic
relationship alignment with local predicted noise preservation, effectively
expanding the retentive receptive field for irrelevant concepts. We name our
method SAGE, and extensive experiments demonstrate the comprehensive
superiority of SAGE compared with other methods in the safe generation of DMs.
The code and weights will be open-sourced at
https://github.com/KevinLight831/SAGE.

</details>


### [25] [ScaleLSD: Scalable Deep Line Segment Detection Streamlined](https://arxiv.org/abs/2506.09369)
*Zeran Ke,Bin Tan,Xianwei Zheng,Yujun Shen,Tianfu Wu,Nan Xue*

Main category: cs.CV

TL;DR: 本文提出了一种名为ScaleLSD的领域无关、高效的线几何检测模型，通过自监督学习从大规模未标记图像中学习，性能优于传统非深度方法。


<details>
  <summary>Details</summary>
Motivation: 研究线几何检测（LSD）问题，目标是学习一个适用于任何自然图像的鲁棒模型，解决传统方法在领域适应性和性能上的不足。

Method: 重新设计并简化了深度和非深度LSD方法的基础结构，提出ScaleLSD模型，利用超过1000万未标记图像进行自监督学习。

Result: ScaleLSD在零样本检测、单视图3D几何估计、双视图线段匹配和多视图3D线映射等任务中表现优异，性能全面超越传统非深度方法。

Conclusion: ScaleLSD是首个在各方面超越传统非深度LSD方法的深度学习方法，显著提升了图像线几何的通用性和鲁棒性。

Abstract: This paper studies the problem of Line Segment Detection (LSD) for the
characterization of line geometry in images, with the aim of learning a
domain-agnostic robust LSD model that works well for any natural images. With
the focus of scalable self-supervised learning of LSD, we revisit and
streamline the fundamental designs of (deep and non-deep) LSD approaches to
have a high-performing and efficient LSD learner, dubbed as ScaleLSD, for the
curation of line geometry at scale from over 10M unlabeled real-world images.
Our ScaleLSD works very well to detect much more number of line segments from
any natural images even than the pioneered non-deep LSD approach, having a more
complete and accurate geometric characterization of images using line segments.
Experimentally, our proposed ScaleLSD is comprehensively testified under
zero-shot protocols in detection performance, single-view 3D geometry
estimation, two-view line segment matching, and multiview 3D line mapping, all
with excellent performance obtained. Based on the thorough evaluation, our
ScaleLSD is observed to be the first deep approach that outperforms the
pioneered non-deep LSD in all aspects we have tested, significantly expanding
and reinforcing the versatility of the line geometry of images. Code and Models
are available at https://github.com/ant-research/scalelsd

</details>


### [26] [UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images](https://arxiv.org/abs/2506.09378)
*Qijian Tian,Xin Tan,Jingyu Gong,Yuan Xie,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出了一种名为UniForward的前馈高斯泼溅模型，用于统一3D场景和语义场重建，仅需未校准的稀疏视图图像输入，无需相机参数或深度真值。


<details>
  <summary>Details</summary>
Motivation: 结合3D场景与语义场以提升环境感知和理解能力，但面临将语义嵌入3D表示、实现实时重建及仅用图像输入的挑战。

Method: 通过双分支解耦解码器将语义特征嵌入3D高斯，提出损失引导的视图采样器，使用光度损失和蒸馏损失进行端到端训练。

Result: 实时重建高质量3D场景和语义场，支持任意视角的语义特征渲染和开放词汇的密集分割掩码生成。

Conclusion: 在3D场景和语义场统一重建任务中达到最先进性能。

Abstract: We propose a feed-forward Gaussian Splatting model that unifies 3D scene and
semantic field reconstruction. Combining 3D scenes with semantic fields
facilitates the perception and understanding of the surrounding environment.
However, key challenges include embedding semantics into 3D representations,
achieving generalizable real-time reconstruction, and ensuring practical
applicability by using only images as input without camera parameters or ground
truth depth. To this end, we propose UniForward, a feed-forward model to
predict 3D Gaussians with anisotropic semantic features from only uncalibrated
and unposed sparse-view images. To enable the unified representation of the 3D
scene and semantic field, we embed semantic features into 3D Gaussians and
predict them through a dual-branch decoupled decoder. During training, we
propose a loss-guided view sampler to sample views from easy to hard,
eliminating the need for ground truth depth or masks required by previous
methods and stabilizing the training process. The whole model can be trained
end-to-end using a photometric loss and a distillation loss that leverages
semantic features from a pre-trained 2D semantic model. At the inference stage,
our UniForward can reconstruct 3D scenes and the corresponding semantic fields
in real time from only sparse-view images. The reconstructed 3D scenes achieve
high-quality rendering, and the reconstructed 3D semantic field enables the
rendering of view-consistent semantic features from arbitrary views, which can
be further decoded into dense segmentation masks in an open-vocabulary manner.
Experiments on novel view synthesis and novel view segmentation demonstrate
that our method achieves state-of-the-art performances for unifying 3D scene
and semantic field reconstruction.

</details>


### [27] [ReID5o: Achieving Omni Multi-modal Person Re-identification in a Single Model](https://arxiv.org/abs/2506.09385)
*Jialong Zuo,Yongtai Deng,Mengdan Tan,Rui Jin,Dongyue Wu,Nong Sang,Liang Pan,Changxin Gao*

Main category: cs.CV

TL;DR: 论文提出了一种新的多模态行人重识别问题（OM-ReID），并构建了首个高质量多模态数据集ORBench，同时提出了多模态学习框架ReID5o，实现了跨模态对齐和协同融合。


<details>
  <summary>Details</summary>
Motivation: 现有行人重识别方法和数据集局限于单一或有限模态，无法满足实际场景中多模态查询的需求。

Method: 构建了包含五种模态（RGB、红外、彩色铅笔、素描和文本描述）的ORBench数据集，并提出ReID5o框架，采用统一编码和多专家路由机制实现多模态协同融合。

Result: 实验验证了ORBench的先进性和实用性，ReID5o模型在多种模态组合下表现最佳。

Conclusion: ORBench和ReID5o为多模态行人重识别研究提供了理想平台和有效解决方案，数据集和代码将公开。

Abstract: In real-word scenarios, person re-identification (ReID) expects to identify a
person-of-interest via the descriptive query, regardless of whether the query
is a single modality or a combination of multiple modalities. However, existing
methods and datasets remain constrained to limited modalities, failing to meet
this requirement. Therefore, we investigate a new challenging problem called
Omni Multi-modal Person Re-identification (OM-ReID), which aims to achieve
effective retrieval with varying multi-modal queries. To address dataset
scarcity, we construct ORBench, the first high-quality multi-modal dataset
comprising 1,000 unique identities across five modalities: RGB, infrared, color
pencil, sketch, and textual description. This dataset also has significant
superiority in terms of diversity, such as the painting perspectives and
textual information. It could serve as an ideal platform for follow-up
investigations in OM-ReID. Moreover, we propose ReID5o, a novel multi-modal
learning framework for person ReID. It enables synergistic fusion and
cross-modal alignment of arbitrary modality combinations in a single model,
with a unified encoding and multi-expert routing mechanism proposed. Extensive
experiments verify the advancement and practicality of our ORBench. A wide
range of possible models have been evaluated and compared on it, and our
proposed ReID5o model gives the best performance. The dataset and code will be
made publicly available at https://github.com/Zplusdragon/ReID5o_ORBench.

</details>


### [28] [Improving Out-of-Distribution Detection via Dynamic Covariance Calibration](https://arxiv.org/abs/2506.09399)
*Kaiyu Guo,Zijian Wang,Brian C. Lovell,Mahsa Baktashmotlagh*

Main category: cs.CV

TL;DR: 提出了一种动态调整先验几何以改进OOD检测的方法，通过实时更新协方差矩阵来纠正不良分布样本的影响。


<details>
  <summary>Details</summary>
Motivation: 现有基于子空间的方法因静态提取信息几何而无法处理不良分布样本导致的几何失真。

Method: 动态更新先验协方差矩阵，沿实时输入特征方向减小协方差，并在残差空间中约束调整。

Result: 在CIFAR和ImageNet-1k数据集上显著提升了OOD检测性能。

Conclusion: 动态调整先验几何能有效纠正不良分布样本的影响，提升OOD检测效果。

Abstract: Out-of-Distribution (OOD) detection is essential for the trustworthiness of
AI systems. Methods using prior information (i.e., subspace-based methods) have
shown effective performance by extracting information geometry to detect OOD
data with a more appropriate distance metric. However, these methods fail to
address the geometry distorted by ill-distributed samples, due to the
limitation of statically extracting information geometry from the training
distribution. In this paper, we argue that the influence of ill-distributed
samples can be corrected by dynamically adjusting the prior geometry in
response to new data. Based on this insight, we propose a novel approach that
dynamically updates the prior covariance matrix using real-time input features,
refining its information. Specifically, we reduce the covariance along the
direction of real-time input features and constrain adjustments to the residual
space, thus preserving essential data characteristics and avoiding effects on
unintended directions in the principal space. We evaluate our method on two
pre-trained models for the CIFAR dataset and five pre-trained models for
ImageNet-1k, including the self-supervised DINO model. Extensive experiments
demonstrate that our approach significantly enhances OOD detection across
various models. The code is released at https://github.com/workerbcd/ooddcc.

</details>


### [29] [SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation](https://arxiv.org/abs/2506.09403)
*Xinya Liu,Jianghao Wu,Tao Lu,Shaoting Zhang,Guotai Wang*

Main category: cs.CV

TL;DR: 提出了一种基于Segment Anything Model（SAM）的可靠伪标签方法（SRPL-SFDA），用于无源域适应（SFDA），通过增强伪标签质量和可靠性感知训练提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决无源域适应（SFDA）中目标域无标签数据监督不足的问题，同时保护隐私和避免源域数据访问限制。

Method: 1）测试时三分支强度增强（T3IE）提升伪标签质量；2）基于SAM输出一致性的可靠伪标签选择模块；3）可靠性感知训练。

Result: 在两个医学图像分割数据集上表现优于现有SFDA方法，接近目标域有监督训练的性能。

Conclusion: SRPL-SFDA有效提升了伪标签质量和SFDA性能，适用于医学图像分割任务。

Abstract: Domain Adaptation (DA) is crucial for robust deployment of medical image
segmentation models when applied to new clinical centers with significant
domain shifts. Source-Free Domain Adaptation (SFDA) is appealing as it can deal
with privacy concerns and access constraints on source-domain data during
adaptation to target-domain data. However, SFDA faces challenges such as
insufficient supervision in the target domain with unlabeled images. In this
work, we propose a Segment Anything Model (SAM)-guided Reliable Pseudo-Labels
method for SFDA (SRPL-SFDA) with three key components: 1) Test-Time Tri-branch
Intensity Enhancement (T3IE) that not only improves quality of raw
pseudo-labels in the target domain, but also leads to SAM-compatible inputs
with three channels to better leverage SAM's zero-shot inference ability for
refining the pseudo-labels; 2) A reliable pseudo-label selection module that
rejects low-quality pseudo-labels based on Consistency of Multiple SAM Outputs
(CMSO) under input perturbations with T3IE; and 3) A reliability-aware training
procedure in the unlabeled target domain where reliable pseudo-labels are used
for supervision and unreliable parts are regularized by entropy minimization.
Experiments conducted on two multi-domain medical image segmentation datasets
for fetal brain and the prostate respectively demonstrate that: 1) SRPL-SFDA
effectively enhances pseudo-label quality in the unlabeled target domain, and
improves SFDA performance by leveraging the reliability-aware training; 2)
SRPL-SFDA outperformed state-of-the-art SFDA methods, and its performance is
close to that of supervised training in the target domain. The code of this
work is available online: https://github.com/HiLab-git/SRPL-SFDA.

</details>


### [30] [Synthetic Human Action Video Data Generation with Pose Transfer](https://arxiv.org/abs/2506.09411)
*Vaclav Knapp,Matyas Bohacek*

Main category: cs.CV

TL;DR: 提出一种基于姿态迁移的合成人类动作视频数据生成方法，提升动作识别性能，并开源数据集。


<details>
  <summary>Details</summary>
Motivation: 合成数据在视频理解任务中常因不自然特征影响训练效果，限制了其在手语翻译、手势识别等任务中的应用潜力。

Method: 使用可控3D高斯虚拟角色模型进行姿态迁移，生成合成人类动作视频数据。

Result: 在Toyota Smarthome和NTU RGB+D数据集上验证了方法对动作识别任务的性能提升，并能有效扩展少样本数据集。

Conclusion: 该方法能弥补真实数据中代表性不足的群体，增加背景多样性，并开源了相关数据集。

Abstract: In video understanding tasks, particularly those involving human motion,
synthetic data generation often suffers from uncanny features, diminishing its
effectiveness for training. Tasks such as sign language translation, gesture
recognition, and human motion understanding in autonomous driving have thus
been unable to exploit the full potential of synthetic data. This paper
proposes a method for generating synthetic human action video data using pose
transfer (specifically, controllable 3D Gaussian avatar models). We evaluate
this method on the Toyota Smarthome and NTU RGB+D datasets and show that it
improves performance in action recognition tasks. Moreover, we demonstrate that
the method can effectively scale few-shot datasets, making up for groups
underrepresented in the real training data and adding diverse backgrounds. We
open-source the method along with RANDOM People, a dataset with videos and
avatars of novel human identities for pose transfer crowd-sourced from the
internet.

</details>


### [31] [Noise Conditional Variational Score Distillation](https://arxiv.org/abs/2506.09416)
*Xinyu Peng,Ziyang Zheng,Yaoming Wang,Han Li,Nuowen Kan,Wenrui Dai,Chenglin Li,Junni Zou,Hongkai Xiong*

Main category: cs.CV

TL;DR: NCVSD是一种新方法，将预训练的扩散模型蒸馏为生成去噪器，通过揭示无条件评分函数隐含地表征去噪后验分布的评分函数，实现快速生成和迭代优化。


<details>
  <summary>Details</summary>
Motivation: 研究旨在将扩散模型蒸馏为高效的生成去噪器，以兼顾快速生成和高质量样本。

Method: 通过将无条件评分函数的洞察融入VSD框架，实现可扩展的生成去噪器学习，支持多步采样和零样本概率推理。

Result: NCVSD在类条件图像生成和逆问题求解中表现优异，超越教师扩散模型，与更大规模的Consistency模型相当，并在逆问题中创下LPIPS记录。

Conclusion: NCVSD通过高效蒸馏实现了快速生成和高质量样本，展示了在生成任务和逆问题中的潜力。

Abstract: We propose Noise Conditional Variational Score Distillation (NCVSD), a novel
method for distilling pretrained diffusion models into generative denoisers. We
achieve this by revealing that the unconditional score function implicitly
characterizes the score function of denoising posterior distributions. By
integrating this insight into the Variational Score Distillation (VSD)
framework, we enable scalable learning of generative denoisers capable of
approximating samples from the denoising posterior distribution across a wide
range of noise levels. The proposed generative denoisers exhibit desirable
properties that allow fast generation while preserve the benefit of iterative
refinement: (1) fast one-step generation through sampling from pure Gaussian
noise at high noise levels; (2) improved sample quality by scaling the
test-time compute with multi-step sampling; and (3) zero-shot probabilistic
inference for flexible and controllable sampling. We evaluate NCVSD through
extensive experiments, including class-conditional image generation and inverse
problem solving. By scaling the test-time compute, our method outperforms
teacher diffusion models and is on par with consistency models of larger sizes.
Additionally, with significantly fewer NFEs than diffusion-based methods, we
achieve record-breaking LPIPS on inverse problems.

</details>


### [32] [ODG: Occupancy Prediction Using Dual Gaussians](https://arxiv.org/abs/2506.09417)
*Yunxiao Shi,Yinhao Zhu,Shizhong Han,Jisoo Jeong,Amin Ansari,Hong Cai,Fatih Porikli*

Main category: cs.CV

TL;DR: 论文提出了一种结合BEV和稀疏点表示的新型3D占用预测方法ODG，通过双分支设计解决现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有3D占用预测方法计算成本高，BEV和稀疏点表示各有优缺点，需要一种结合两者的高效方法。

Method: 提出ODG方法，采用双分支设计：基于查询的稀疏点分支和BEV分支，通过交叉注意力共享信息并融合输出。

Result: 在Occ3D-nuScenes和Occ3D-Waymo基准测试中表现优越，推理速度与最新高效方法相当。

Conclusion: ODG有效结合BEV和稀疏点表示，提升了3D占用预测的精度和效率。

Abstract: 3D occupancy provides fine-grained 3D geometry and semantics for scene
understanding which is critical for autonomous driving. Most existing methods,
however, carry high compute costs, requiring dense 3D feature volume and
cross-attention to effectively aggregate information. More recent works have
adopted Bird's Eye View (BEV) or sparse points as scene representation with
much reduced cost, but still suffer from their respective shortcomings. More
concretely, BEV struggles with small objects that often experience significant
information loss after being projected to the ground plane. On the other hand,
points can flexibly model little objects in 3D, but is inefficient at capturing
flat surfaces or large objects. To address these challenges, in this paper, we
present a novel 3D occupancy prediction approach, ODG, which combines BEV and
sparse points based representations. We propose a dual-branch design: a
query-based sparse points branch and a BEV branch. The 3D information learned
in the sparse points branch is shared with the BEV stream via cross-attention,
which enriches the weakened signals of difficult objects on the BEV plane. The
outputs of both branches are finally fused to generate predicted 3D occupancy.
We conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo
benchmarks that demonstrate the superiority of our proposed ODG. Moreover, ODG
also delivers competitive inference speed when compared to the latest efficient
approaches.

</details>


### [33] [A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation](https://arxiv.org/abs/2506.09427)
*Yukang Feng,Jianwen Sun,Chuanhao Li,Zizhen Li,Jiaxin Ai,Fanrui Zhang,Yifan Chang,Sizhuo Zhou,Shenglin Zhang,Yu Dai,Kaipeng Zhang*

Main category: cs.CV

TL;DR: 论文介绍了InterSyn数据集和SEIR方法，用于提升多模态模型的生成能力，并提出了SynJudge评估工具。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型在生成紧密交织的图像-文本输出方面表现不佳，主要因训练数据规模、质量和指令丰富度不足。

Method: 提出Self-Evaluation with Iterative Refinement (SEIR)方法构建InterSyn数据集，并开发SynJudge评估工具。

Result: SEIR显著提升数据集质量，基于InterSyn训练的模型在所有评估指标上均有提升。

Conclusion: InterSyn和SynJudge为多模态系统的进步提供了有效支持。

Abstract: Recent advancements in Large Multimodal Models (LMMs) have significantly
improved multimodal understanding and generation. However, these models still
struggle to generate tightly interleaved image-text outputs, primarily due to
the limited scale, quality and instructional richness of current training
datasets. To address this, we introduce InterSyn, a large-scale multimodal
dataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)
method. InterSyn features multi-turn, instruction-driven dialogues with tightly
interleaved imagetext responses, providing rich object diversity and rigorous
automated quality refinement, making it well-suited for training
next-generation instruction-following LMMs. Furthermore, to address the lack of
reliable evaluation tools capable of assessing interleaved multimodal outputs,
we introduce SynJudge, an automatic evaluation model designed to quantitatively
assess multimodal outputs along four dimensions: text content, image content,
image quality, and image-text synergy.
  Experimental studies show that the SEIR method leads to substantially higher
dataset quality compared to an otherwise identical process without refinement.
  Moreover, LMMs trained on InterSyn achieve uniform performance gains across
all evaluation metrics, confirming InterSyn's utility for advancing multimodal
systems.

</details>


### [34] [A Novel Lightweight Transformer with Edge-Aware Fusion for Remote Sensing Image Captioning](https://arxiv.org/abs/2506.09429)
*Swadhin Das,Divyansh Mundra,Priyanshu Dayal,Raksha Sharma*

Main category: cs.CV

TL;DR: 提出了一种轻量级Transformer架构，通过降低编码器层维度和使用蒸馏版GPT-2解码器，结合知识蒸馏和边缘感知增强策略，显著提升了遥感图像描述质量。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在遥感图像描述中表现优异，但计算成本高且忽略细粒度结构特征。

Method: 采用轻量级Transformer架构，降低编码器维度，使用蒸馏版GPT-2解码器，结合知识蒸馏和边缘感知增强策略。

Result: 实验表明，该方法显著优于现有技术，提升了描述质量。

Conclusion: 轻量级架构和边缘感知策略有效解决了计算成本和细粒度特征提取问题。

Abstract: Transformer-based models have achieved strong performance in remote sensing
image captioning by capturing long-range dependencies and contextual
information. However, their practical deployment is hindered by high
computational costs, especially in multi-modal frameworks that employ separate
transformer-based encoders and decoders. In addition, existing remote sensing
image captioning models primarily focus on high-level semantic extraction while
often overlooking fine-grained structural features such as edges, contours, and
object boundaries. To address these challenges, a lightweight transformer
architecture is proposed by reducing the dimensionality of the encoder layers
and employing a distilled version of GPT-2 as the decoder. A knowledge
distillation strategy is used to transfer knowledge from a more complex teacher
model to improve the performance of the lightweight network. Furthermore, an
edge-aware enhancement strategy is incorporated to enhance image representation
and object boundary understanding, enabling the model to capture fine-grained
spatial details in remote sensing images. Experimental results demonstrate that
the proposed approach significantly improves caption quality compared to
state-of-the-art methods.

</details>


### [35] [TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision](https://arxiv.org/abs/2506.09445)
*Ayush Gupta,Anirban Roy,Rama Chellappa,Nathaniel D. Bastian,Alvaro Velasquez,Susmit Jha*

Main category: cs.CV

TL;DR: 论文提出TOGA模型，用于弱监督下的视频问答任务，无需时间标注即可生成答案及时间定位。


<details>
  <summary>Details</summary>
Motivation: 解决弱监督下视频问答任务中时间定位的问题，避免依赖时间标注。

Method: 提出TOGA模型，通过指令调整联合生成答案和时间定位，利用伪标签和一致性约束确保时间定位的有效性。

Result: 在NExT-GQA、MSVD-QA和ActivityNet-QA基准测试中达到最优性能。

Conclusion: 联合生成答案和时间定位能提升问答和定位性能，TOGA在弱监督下表现优异。

Abstract: We address the problem of video question answering (video QA) with temporal
grounding in a weakly supervised setup, without any temporal annotations. Given
a video and a question, we generate an open-ended answer grounded with the
start and end time. For this task, we propose TOGA: a vision-language model for
Temporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune
TOGA to jointly generate the answer and the temporal grounding. We operate in a
weakly supervised setup where the temporal grounding annotations are not
available. We generate pseudo labels for temporal grounding and ensure the
validity of these labels by imposing a consistency constraint between the
question of a grounding response and the response generated by a question
referring to the same temporal segment. We notice that jointly generating the
answers with the grounding improves performance on question answering as well
as grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For
grounded QA, we consider the NExT-GQA benchmark which is designed to evaluate
weakly supervised grounded question answering. For open-ended QA, we consider
the MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art
performance for both tasks on these benchmarks.

</details>


### [36] [Harmonizing and Merging Source Models for CLIP-based Domain Generalization](https://arxiv.org/abs/2506.09446)
*Yuhe Ding,Jian Liang,Bo Jiang,Zi Wang,Aihua Zheng,Bin Luo*

Main category: cs.CV

TL;DR: HAM是一种基于CLIP的领域泛化框架，通过无冲突样本增强和模型合并，提升模型在未见领域的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多源训练中存在样本冲突和优化冲突，导致泛化性能不佳。HAM旨在解决这些问题。

Method: HAM通过无冲突样本增强和模型更新方向协调，结合冗余感知的历史模型合并方法，整合多源模型知识。

Result: 在五个基准数据集上，HAM实现了最先进的性能。

Conclusion: HAM有效解决了多源训练中的冲突问题，显著提升了模型的泛化能力。

Abstract: CLIP-based domain generalization aims to improve model generalization to
unseen domains by leveraging the powerful zero-shot classification capabilities
of CLIP and multiple source datasets. Existing methods typically train a single
model across multiple source domains to capture domain-shared information.
However, this paradigm inherently suffers from two types of conflicts: 1)
sample conflicts, arising from noisy samples and extreme domain shifts among
sources; and 2) optimization conflicts, stemming from competition and
trade-offs during multi-source training. Both hinder the generalization and
lead to suboptimal solutions. Recent studies have shown that model merging can
effectively mitigate the competition of multi-objective optimization and
improve generalization performance. Inspired by these findings, we propose
Harmonizing and Merging (HAM), a novel source model merging framework for
CLIP-based domain generalization. During the training process of the source
models, HAM enriches the source samples without conflicting samples, and
harmonizes the update directions of all models. Then, a redundancy-aware
historical model merging method is introduced to effectively integrate
knowledge across all source models. HAM comprehensively consolidates source
domain information while enabling mutual enhancement among source models,
ultimately yielding a final model with optimal generalization capabilities.
Extensive experiments on five widely used benchmark datasets demonstrate the
effectiveness of our approach, achieving state-of-the-art performance.

</details>


### [37] [Evidential Deep Learning with Spectral-Spatial Uncertainty Disentanglement for Open-Set Hyperspectral Domain Generalization](https://arxiv.org/abs/2506.09460)
*Amirreza Khoshbakht,Erchan Aptoula*

Main category: cs.CV

TL;DR: 提出了一种新的开放集域泛化框架，用于高光谱图像分类，解决了未知类和域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有域适应方法无法处理目标域中的未知类，且需要目标域数据训练，导致负迁移和性能下降。

Method: 结合了频谱不变频率解耦（SIFD）、双通道残差网络（DCRN）、证据深度学习（EDL）和光谱空间不确定性解耦（SSUD）。

Result: 在三个跨场景高光谱分类任务中表现优异，无需目标域数据即可达到与现有方法相当的性能。

Conclusion: 该框架为开放集域泛化问题提供了有效解决方案，具有实际应用潜力。

Abstract: Open-set domain generalization(OSDG) for hyperspectral image classification
presents significant challenges due to the presence of unknown classes in
target domains and the need for models to generalize across multiple unseen
domains without target-specific adaptation. Existing domain adaptation methods
assume access to target domain data during training and fail to address the
fundamental issue of domain shift when unknown classes are present, leading to
negative transfer and reduced classification performance. To address these
limitations, we propose a novel open-set domain generalization framework that
combines four key components: Spectrum-Invariant Frequency Disentanglement
(SIFD) for domain-agnostic feature extraction, Dual-Channel Residual Network
(DCRN) for robust spectral-spatial feature learning, Evidential Deep Learning
(EDL) for uncertainty quantification, and Spectral-Spatial Uncertainty
Disentanglement (SSUD) for reliable open-set classification. The SIFD module
extracts domain-invariant spectral features in the frequency domain through
attention-weighted frequency analysis and domain-agnostic regularization, while
DCRN captures complementary spectral and spatial information via parallel
pathways with adaptive fusion. EDL provides principled uncertainty estimation
using Dirichlet distributions, enabling the SSUD module to make reliable
open-set decisions through uncertainty-aware pathway weighting and adaptive
rejection thresholding. Experimental results on three cross-scene hyperspectral
classification tasks show that our approach achieves performance comparable to
state-of-the-art domain adaptation methods while requiring no access to the
target domain during training. The implementation will be made available at
https://github.com/amir-khb/SSUDOSDG upon acceptance.

</details>


### [38] [Optimizing Cooperative Multi-Object Tracking using Graph Signal Processing](https://arxiv.org/abs/2506.09469)
*Maria Damanaki,Nikos Piperigkos,Alexandros Gkillas,Aris S. Lalos*

Main category: cs.CV

TL;DR: 提出了一种基于图拓扑感知优化的多智能体协同多目标跟踪框架，显著提升了3D LiDAR场景中的跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 单智能体多目标跟踪因遮挡和传感器故障等问题难以全面感知环境，需整合多智能体信息以实现更全面的环境理解。

Method: 通过构建完全连接的图拓扑，利用图拉普拉斯优化技术平滑边界框位置误差，分两阶段关联精炼后的边界框与跟踪对象。

Result: 在V2V4Real数据集上的实验表明，该方法显著优于基线框架和当前最先进的深度学习方法。

Conclusion: 提出的协同多目标跟踪框架有效提升了定位和跟踪精度，适用于多智能体环境感知。

Abstract: Multi-Object Tracking (MOT) plays a crucial role in autonomous driving
systems, as it lays the foundations for advanced perception and precise path
planning modules. Nonetheless, single agent based MOT lacks in sensing
surroundings due to occlusions, sensors failures, etc. Hence, the integration
of multiagent information is essential for comprehensive understanding of the
environment. This paper proposes a novel Cooperative MOT framework for tracking
objects in 3D LiDAR scene by formulating and solving a graph topology-aware
optimization problem so as to fuse information coming from multiple vehicles.
By exploiting a fully connected graph topology defined by the detected bounding
boxes, we employ the Graph Laplacian processing optimization technique to
smooth the position error of bounding boxes and effectively combine them. In
that manner, we reveal and leverage inherent coherences of diverse multi-agent
detections, and associate the refined bounding boxes to tracked objects at two
stages, optimizing localization and tracking accuracies. An extensive
evaluation study has been conducted, using the real-world V2V4Real dataset,
where the proposed method significantly outperforms the baseline frameworks,
including the state-of-the-art deep-learning DMSTrack and V2V4Real, in various
testing sequences.

</details>


### [39] [Provoking Multi-modal Few-Shot LVLM via Exploration-Exploitation In-Context Learning](https://arxiv.org/abs/2506.09473)
*Cheng Chen,Yunpeng Zhai,Yifan Zhao,Jinyang Gao,Bolin Ding,Jia Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于探索-利用强化学习框架的方法，用于多模态演示选择，以提升大型视觉语言模型（LVLMs）的上下文学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有上下文学习方法依赖预定义演示或启发式选择策略，难以覆盖多样化任务需求且无法建模演示间交互，导致性能不足。

Method: 提出探索-利用强化学习框架，自适应选择多模态演示作为整体，并通过自我探索优化演示选择策略。

Result: 在四个视觉问答（VQA）数据集上验证了方法的优越性能，显著提升了少样本LVLMs的泛化能力。

Conclusion: 该方法通过自适应演示选择策略，有效提升了LVLMs在上下文学习中的任务理解和执行能力。

Abstract: In-context learning (ICL), a predominant trend in instruction learning, aims
at enhancing the performance of large language models by providing clear task
guidance and examples, improving their capability in task understanding and
execution. This paper investigates ICL on Large Vision-Language Models (LVLMs)
and explores the policies of multi-modal demonstration selection. Existing
research efforts in ICL face significant challenges: First, they rely on
pre-defined demonstrations or heuristic selecting strategies based on human
intuition, which are usually inadequate for covering diverse task requirements,
leading to sub-optimal solutions; Second, individually selecting each
demonstration fails in modeling the interactions between them, resulting in
information redundancy. Unlike these prevailing efforts, we propose a new
exploration-exploitation reinforcement learning framework, which explores
policies to fuse multi-modal information and adaptively select adequate
demonstrations as an integrated whole. The framework allows LVLMs to optimize
themselves by continually refining their demonstrations through
self-exploration, enabling the ability to autonomously identify and generate
the most effective selection policies for in-context learning. Experimental
results verify the superior performance of our approach on four Visual
Question-Answering (VQA) datasets, demonstrating its effectiveness in enhancing
the generalization capability of few-shot LVLMs.

</details>


### [40] [Urban1960SatSeg: Unsupervised Semantic Segmentation of Mid-20$^{th}$ century Urban Landscapes with Satellite Imageries](https://arxiv.org/abs/2506.09476)
*Tianxiang Hao,Lixian Zhang,Yingjia Zhang,Mengxuan Chen,Jinxiao Zhang,Haohuan Fu*

Main category: cs.CV

TL;DR: 论文介绍了Urban1960SatBench数据集和Urban1960SatUSM框架，用于解决历史卫星图像语义分割的挑战，并展示了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 历史卫星图像（如Keyhole数据）因质量退化和缺乏标注，难以用于语义分割，阻碍了对早期城市发展的研究。

Method: 提出Urban1960SatBench数据集（标注历史图像）和Urban1960SatUSM框架（基于自监督学习的无监督分割方法，采用置信度对齐机制和焦点置信度损失）。

Result: Urban1960SatUSM在历史城市场景分割上显著优于现有无监督方法。

Conclusion: 该工作为利用现代计算机视觉研究长期城市变化提供了新工具。

Abstract: Historical satellite imagery, such as mid-20$^{th}$ century Keyhole data,
offers rare insights into understanding early urban development and long-term
transformation. However, severe quality degradation (e.g., distortion,
misalignment, and spectral scarcity) and annotation absence have long hindered
semantic segmentation on such historical RS imagery. To bridge this gap and
enhance understanding of urban development, we introduce
$\textbf{Urban1960SatBench}$, an annotated segmentation dataset based on
historical satellite imagery with the earliest observation time among all
existing segmentation datasets, along with a benchmark framework for
unsupervised segmentation tasks, $\textbf{Urban1960SatUSM}$. First,
$\textbf{Urban1960SatBench}$ serves as a novel, expertly annotated semantic
segmentation dataset built on mid-20$^{th}$ century Keyhole imagery, covering
1,240 km$^2$ and key urban classes (buildings, roads, farmland, water). As the
earliest segmentation dataset of its kind, it provides a pioneering benchmark
for historical urban understanding. Second,
$\textbf{Urban1960SatUSM}$(Unsupervised Segmentation Model) is a novel
unsupervised semantic segmentation framework for historical RS imagery. It
employs a confidence-aware alignment mechanism and focal-confidence loss based
on a self-supervised learning architecture, which generates robust
pseudo-labels and adaptively prioritizes prediction difficulty and label
reliability to improve unsupervised segmentation on noisy historical data
without manual supervision. Experiments show Urban1960SatUSM significantly
outperforms existing unsupervised segmentation methods on Urban1960SatSeg for
segmenting historical urban scenes, promising in paving the way for
quantitative studies of long-term urban change using modern computer vision.
Our benchmark and supplementary material are available at
https://github.com/Tianxiang-Hao/Urban1960SatSeg.

</details>


### [41] [TinySplat: Feedforward Approach for Generating Compact 3D Scene Representation](https://arxiv.org/abs/2506.09479)
*Zetian Song,Jiaye Fu,Jiaqi Zhang,Xiaohan Lu,Chuanmin Jia,Siwei Ma,Wen Gao*

Main category: cs.CV

TL;DR: TinySplat提出了一种基于前馈3D高斯泼溅（3DGS）的压缩方法，通过消除冗余实现高效存储。


<details>
  <summary>Details</summary>
Motivation: 前馈3DGS方法虽然重建速度快，但存储成本高，现有压缩方法不适用。

Method: 结合View-Projection Transformation（VPT）减少几何冗余，Visibility-Aware Basis Reduction（VABR）减少感知冗余，并使用视频编解码器处理空间冗余。

Result: TinySplat实现了100倍以上的压缩比，存储大小仅为现有方法的6%，编码时间减少75%，解码时间减少99%。

Conclusion: TinySplat是一种高效且快速的3D场景压缩方法，适用于前馈3DGS。

Abstract: The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a
new paradigm to reconstruct 3D scenes. Using neural networks trained on
large-scale multi-view datasets, it can directly infer 3DGS representations
from sparse input views. Although the feedforward approach achieves high
reconstruction speed, it still suffers from the substantial storage cost of 3D
Gaussians. Existing 3DGS compression methods relying on scene-wise optimization
are not applicable due to architectural incompatibilities. To overcome this
limitation, we propose TinySplat, a complete feedforward approach for
generating compact 3D scene representations. Built upon standard feedforward
3DGS methods, TinySplat integrates a training-free compression framework that
systematically eliminates key sources of redundancy. Specifically, we introduce
View-Projection Transformation (VPT) to reduce geometric redundancy by
projecting geometric parameters into a more compact space. We further present
Visibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy
by aligning feature energy along dominant viewing directions via basis
transformation. Lastly, spatial redundancy is addressed through an
off-the-shelf video codec. Comprehensive experimental results on multiple
benchmark datasets demonstrate that TinySplat achieves over 100x compression
for 3D Gaussian data generated by feedforward methods. Compared to the
state-of-the-art compression approach, we achieve comparable quality with only
6% of the storage size. Meanwhile, our compression framework requires only 25%
of the encoding time and 1% of the decoding time.

</details>


### [42] [Marrying Autoregressive Transformer and Diffusion with Multi-Reference Autoregression](https://arxiv.org/abs/2506.09482)
*Dingcheng Zhen,Qian Qiao,Tan Yu,Kangxi Wu,Ziwei Zhang,Siyuan Liu,Shunshun Yin,Ming Tao*

Main category: cs.CV

TL;DR: TransDiff结合自回归Transformer和扩散模型，显著提升图像生成性能，并引入多参考自回归（MRAR）进一步优化生成质量。


<details>
  <summary>Details</summary>
Motivation: 结合自回归Transformer和扩散模型的优势，提升图像生成的质量和效率。

Method: TransDiff通过联合建模框架编码标签和图像为高级语义特征，并利用扩散模型估计图像样本分布。进一步提出MRAR范式，通过多参考生成提升多样性。

Result: 在ImageNet 256x256上，TransDiff的FID为1.61，IS为293.4，推理速度显著快于现有方法。MRAR将FID降至1.42。

Conclusion: TransDiff为图像生成领域开辟了新方向，结合AR和扩散模型的潜力巨大。

Abstract: We introduce TransDiff, the first image generation model that marries
Autoregressive (AR) Transformer with diffusion models. In this joint modeling
framework, TransDiff encodes labels and images into high-level semantic
features and employs a diffusion model to estimate the distribution of image
samples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms
other image generation models based on standalone AR Transformer or diffusion
models. Specifically, TransDiff achieves a Fr\'echet Inception Distance (FID)
of 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster
inference latency compared to state-of-the-art methods based on AR Transformer
and x112 faster inference compared to diffusion-only models. Furthermore,
building on the TransDiff model, we introduce a novel image generation paradigm
called Multi-Reference Autoregression (MRAR), which performs autoregressive
generation by predicting the next image. MRAR enables the model to reference
multiple previously generated images, thereby facilitating the learning of more
diverse representations and improving the quality of generated images in
subsequent iterations. By applying MRAR, the performance of TransDiff is
improved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open
up a new frontier in the field of image generation.

</details>


### [43] [HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene](https://arxiv.org/abs/2506.09518)
*Jianing Chen,Zehao Li,Yujun Cai,Hao Jiang,Chengxuan Qian,Juyuan Kang,Shuqin Gao,Honglong Zhao,Tianlu Mao,Yucheng Zhang*

Main category: cs.CV

TL;DR: HAIF-GS提出了一种基于稀疏锚点驱动的动态建模框架，解决了动态3D场景重建中的冗余更新、运动监督不足和非刚性变形建模问题。


<details>
  <summary>Details</summary>
Motivation: 动态3D场景的单目视频重建是3D视觉中的基础挑战，现有方法在动态建模中存在冗余更新、运动监督不足和非刚性变形建模不足的问题。

Method: HAIF-GS通过锚点过滤器识别运动相关区域，利用自监督的流引导变形模块和多层次锚点传播机制，实现高效且一致的动态建模。

Result: 实验表明，HAIF-GS在渲染质量、时间一致性和重建效率上显著优于现有动态3DGS方法。

Conclusion: HAIF-GS通过结构化动态建模，为动态3D场景重建提供了一种高效且高质量的解决方案。

Abstract: Reconstructing dynamic 3D scenes from monocular videos remains a fundamental
challenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time
rendering in static settings, extending it to dynamic scenes is challenging due
to the difficulty of learning structured and temporally consistent motion
representations. This challenge often manifests as three limitations in
existing methods: redundant Gaussian updates, insufficient motion supervision,
and weak modeling of complex non-rigid deformations. These issues collectively
hinder coherent and efficient dynamic reconstruction. To address these
limitations, we propose HAIF-GS, a unified framework that enables structured
and consistent dynamic modeling through sparse anchor-driven deformation. It
first identifies motion-relevant regions via an Anchor Filter to suppresses
redundant updates in static areas. A self-supervised Induced Flow-Guided
Deformation module induces anchor motion using multi-frame feature aggregation,
eliminating the need for explicit flow labels. To further handle fine-grained
deformations, a Hierarchical Anchor Propagation mechanism increases anchor
resolution based on motion complexity and propagates multi-level
transformations. Extensive experiments on synthetic and real-world benchmarks
validate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in
rendering quality, temporal coherence, and reconstruction efficiency.

</details>


### [44] [Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs](https://arxiv.org/abs/2506.09522)
*Beomsik Cho,Jaehyung Kim*

Main category: cs.CV

TL;DR: ReVisiT是一种解码方法，通过引用视觉标记来指导大型视觉语言模型（LVLM）的文本生成，提升视觉信息的利用。


<details>
  <summary>Details</summary>
Motivation: 传统LVLM解码策略未能充分利用视觉信息，导致视觉无关的响应。现有方法通常需要额外训练或多步推理，ReVisiT旨在以简单高效的方式解决这一问题。

Method: 通过将视觉标记投影到文本标记分布空间，动态选择最相关的视觉标记，并通过约束差异最小化优化输出分布。

Result: 在三个LVLM幻觉基准测试中，ReVisiT显著提升了视觉相关性，计算开销小，且性能优于现有方法。

Conclusion: ReVisiT是一种高效且无需额外训练的解码方法，显著提升了LVLM的视觉信息利用能力。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across various multimodal tasks by integrating visual perception with language
understanding. However, conventional decoding strategies of LVLMs often fail to
successfully utilize visual information, leading to visually ungrounded
responses. While various approaches have been proposed to address this
limitation, they typically require additional training, multi-step inference
procedures, or external model dependencies. This paper introduces ReVisiT, a
simple yet effective decoding method that references vision tokens to guide the
text generation process in LVLMs. Our approach leverages the semantic
information embedded within vision tokens by projecting them into the text
token distribution space, and dynamically selecting the most relevant vision
token at each decoding step through constrained divergence minimization. This
selected vision token is then used to refine the output distribution to better
incorporate visual semantics. Experiments on three LVLM hallucination
benchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances
visual grounding with minimal computational overhead. Moreover, our method
achieves competitive or superior results relative to state-of-the-art baselines
while reducing computational costs for up to $2\times$.

</details>


### [45] [Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS](https://arxiv.org/abs/2506.09534)
*Tao Wang,Mengyu Li,Geduo Zeng,Cheng Meng,Qiong Zhang*

Main category: cs.CV

TL;DR: 3D高斯泼溅（3DGS）是一种高效的辐射场渲染技术，但通常需要数百万冗余的高斯基元，导致内存和渲染资源消耗过大。本文提出了一种基于最优传输的全局高斯混合缩减方法，显著减少了高斯基元数量（仅需10%），同时保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS压缩方法基于启发式重要性评分，缺乏全局保真度保证。本文旨在填补这一空白，提供一种高效且通用的轻量化神经渲染方案。

Method: 通过最小化复合传输散度生成紧凑几何表示，并解耦几何与外观属性，优化颜色和透明度。

Result: 实验表明，该方法在仅使用10%高斯基元的情况下，渲染质量（PSNR、SSIM、LPIPS）与原始3DGS相当，且优于现有压缩技术。

Conclusion: 该方法适用于任何3DGS流程，提供了一种高效且通用的轻量化神经渲染途径。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance
field rendering, but it typically requires millions of redundant Gaussian
primitives, overwhelming memory and rendering budgets. Existing compaction
approaches address this by pruning Gaussians based on heuristic importance
scores, without global fidelity guarantee. To bridge this gap, we propose a
novel optimal transport perspective that casts 3DGS compaction as global
Gaussian mixture reduction. Specifically, we first minimize the composite
transport divergence over a KD-tree partition to produce a compact geometric
representation, and then decouple appearance from geometry by fine-tuning color
and opacity attributes with far fewer Gaussian primitives. Experiments on
benchmark datasets show that our method (i) yields negligible loss in rendering
quality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10% Gaussians;
and (ii) consistently outperforms state-of-the-art 3DGS compaction techniques.
Notably, our method is applicable to any stage of vanilla or accelerated 3DGS
pipelines, providing an efficient and agnostic pathway to lightweight neural
rendering.

</details>


### [46] [AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches](https://arxiv.org/abs/2506.09538)
*Wenjun Ji,Yuxiang Fu,Luyang Ying,Deng-Ping Fan,Yuyi Wang,Ming-Ming Cheng,Ivor Tsang,Qing Guo*

Main category: cs.CV

TL;DR: 本文研究了文本到图像（T2I）扩散模型生成的对抗性补丁的角度鲁棒性问题，提出了Angle-Robust Concept Learning（AngleRoCL）方法，显著提升了补丁在不同视角下的攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了T2I对抗性补丁在物理世界中不同视角下的攻击效果，本文旨在解决这一问题。

Method: 提出AngleRoCL方法，通过学习通用概念（文本嵌入）来生成具有角度鲁棒性的补丁。

Result: 实验表明，AngleRoCL显著提升了补丁的角度鲁棒性，攻击成功率在挑战性视角下仍保持较高水平。

Conclusion: AngleRoCL为理解物理角度鲁棒性补丁提供了新视角，揭示了文本概念与物理属性之间的关系。

Abstract: Cutting-edge works have demonstrated that text-to-image (T2I) diffusion
models can generate adversarial patches that mislead state-of-the-art object
detectors in the physical world, revealing detectors' vulnerabilities and
risks. However, these methods neglect the T2I patches' attack effectiveness
when observed from different views in the physical world (i.e., angle
robustness of the T2I adversarial patches). In this paper, we study the angle
robustness of T2I adversarial patches comprehensively, revealing their
angle-robust issues, demonstrating that texts affect the angle robustness of
generated patches significantly, and task-specific linguistic instructions fail
to enhance the angle robustness. Motivated by the studies, we introduce
Angle-Robust Concept Learning (AngleRoCL), a simple and flexible approach that
learns a generalizable concept (i.e., text embeddings in implementation)
representing the capability of generating angle-robust patches. The learned
concept can be incorporated into textual prompts and guides T2I models to
generate patches with their attack effectiveness inherently resistant to
viewpoint variations. Through extensive simulation and physical-world
experiments on five SOTA detectors across multiple views, we demonstrate that
AngleRoCL significantly enhances the angle robustness of T2I adversarial
patches compared to baseline methods. Our patches maintain high attack success
rates even under challenging viewing conditions, with over 50% average relative
improvement in attack effectiveness across multiple angles. This research
advances the understanding of physically angle-robust patches and provides
insights into the relationship between textual concepts and physical properties
in T2I-generated contents.

</details>


### [47] [3DGeoDet: General-purpose Geometry-aware Image-based 3D Object Detection](https://arxiv.org/abs/2506.09541)
*Yi Zhang,Yi Wang,Yawen Cui,Lap-Pui Chau*

Main category: cs.CV

TL;DR: 3DGeoDet是一种新颖的几何感知3D物体检测方法，通过显式和隐式3D几何表示提升性能，无需3D信号监督，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决基于图像的3D物体检测中缺乏3D几何线索导致的图像与3D表示对应模糊问题。

Method: 利用预测深度生成显式（体素占用）和隐式（TSDF）3D几何表示，结合体素占用注意力优化特征体积。

Result: 在SUN RGB-D、ScanNetV2和KITTI数据集上分别提升9.3 mAP@0.5、3.3 mAP@0.5和0.19 AP3D@0.7。

Conclusion: 3DGeoDet通过几何感知显著提升3D物体检测性能，适用于多样环境。

Abstract: This paper proposes 3DGeoDet, a novel geometry-aware 3D object detection
approach that effectively handles single- and multi-view RGB images in indoor
and outdoor environments, showcasing its general-purpose applicability. The key
challenge for image-based 3D object detection tasks is the lack of 3D geometric
cues, which leads to ambiguity in establishing correspondences between images
and 3D representations. To tackle this problem, 3DGeoDet generates efficient 3D
geometric representations in both explicit and implicit manners based on
predicted depth information. Specifically, we utilize the predicted depth to
learn voxel occupancy and optimize the voxelized 3D feature volume explicitly
through the proposed voxel occupancy attention. To further enhance 3D
awareness, the feature volume is integrated with an implicit 3D representation,
the truncated signed distance function (TSDF). Without requiring supervision
from 3D signals, we significantly improve the model's comprehension of 3D
geometry by leveraging intermediate 3D representations and achieve end-to-end
training. Our approach surpasses the performance of state-of-the-art
image-based methods on both single- and multi-view benchmark datasets across
diverse environments, achieving a 9.3 mAP@0.5 improvement on the SUN RGB-D
dataset, a 3.3 mAP@0.5 improvement on the ScanNetV2 dataset, and a 0.19
AP3D@0.7 improvement on the KITTI dataset. The project page is available at:
https://cindy0725.github.io/3DGeoDet/.

</details>


### [48] [GLD-Road:A global-local decoding road network extraction model for remote sensing images](https://arxiv.org/abs/2506.09553)
*Ligao Deng,Yupeng Deng,Yu Meng,Jingbo Chen,Zhihao Xi,Diyou Liu,Qifeng Chu*

Main category: cs.CV

TL;DR: GLD-Road是一种两阶段模型，结合全局效率和局部精度，显著提升道路网络提取性能并减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 道路网络对测绘、自动驾驶和灾害响应至关重要，但手动标注成本高，现有深度学习方法各有不足（如全局方法快速但漏节点，局部方法准确但慢）。

Method: GLD-Road采用两阶段方法：1）全局检测道路节点并通过连接模块连接；2）局部迭代修复断裂道路以减少计算量。

Result: 实验表明GLD-Road优于现有方法，APLS提升1.9%（City-Scale）和0.67%（SpaceNet3），检索时间减少40%（对比Sat2Graph）和92%（对比RNGDet++）。

Conclusion: GLD-Road在道路网络提取中实现了高效与高精度的平衡，具有实际应用潜力。

Abstract: Road networks are crucial for mapping, autonomous driving, and disaster
response. While manual annotation is costly, deep learning offers efficient
extraction. Current methods include postprocessing (prone to errors), global
parallel (fast but misses nodes), and local iterative (accurate but slow). We
propose GLD-Road, a two-stage model combining global efficiency and local
precision. First, it detects road nodes and connects them via a Connect Module.
Then, it iteratively refines broken roads using local searches, drastically
reducing computation. Experiments show GLD-Road outperforms state-of-the-art
methods, improving APLS by 1.9% (City-Scale) and 0.67% (SpaceNet3). It also
reduces retrieval time by 40% vs. Sat2Graph (global) and 92% vs. RNGDet++
(local). The experimental results are available at
https://github.com/ucas-dlg/GLD-Road.

</details>


### [49] [AD^2-Bench: A Hierarchical CoT Benchmark for MLLM in Autonomous Driving under Adverse Conditions](https://arxiv.org/abs/2506.09557)
*Zhaoyang Wei,Chenhui Qiang,Bowen Jiang,Xumeng Han,Xuehui Yu,Zhenjun Han*

Main category: cs.CV

TL;DR: AD^2-Bench是首个针对恶劣天气和复杂场景下自动驾驶的Chain-of-Thought（CoT）基准测试，填补了现有评估的空白，包含5.4k高质量标注实例，评估结果显示当前MLLMs的准确率低于60%。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未充分评估CoT在恶劣天气和复杂交通环境中的表现，亟需一个专门的标准评估平台。

Method: 构建AD^2-Bench，覆盖多样恶劣环境，提供细粒度标注和多步推理支持，并设计专用评估框架。

Result: 评估显示当前MLLMs的准确率不足60%，凸显了基准的挑战性和改进需求。

Conclusion: AD^2-Bench为自动驾驶中的MLLMs推理提供了标准化评估平台，推动研究发展。

Abstract: Chain-of-Thought (CoT) reasoning has emerged as a powerful approach to
enhance the structured, multi-step decision-making capabilities of Multi-Modal
Large Models (MLLMs), is particularly crucial for autonomous driving with
adverse weather conditions and complex traffic environments. However, existing
benchmarks have largely overlooked the need for rigorous evaluation of CoT
processes in these specific and challenging scenarios. To address this critical
gap, we introduce AD^2-Bench, the first Chain-of-Thought benchmark specifically
designed for autonomous driving with adverse weather and complex scenes.
AD^2-Bench is meticulously constructed to fulfill three key criteria:
comprehensive data coverage across diverse adverse environments, fine-grained
annotations that support multi-step reasoning, and a dedicated evaluation
framework tailored for assessing CoT performance. The core contribution of
AD^2-Bench is its extensive collection of over 5.4k high-quality, manually
annotated CoT instances. Each intermediate reasoning step in these annotations
is treated as an atomic unit with explicit ground truth, enabling unprecedented
fine-grained analysis of MLLMs' inferential processes under text-level,
point-level, and region-level visual prompts. Our comprehensive evaluation of
state-of-the-art MLLMs on AD^2-Bench reveals accuracy below 60%, highlighting
the benchmark's difficulty and the need to advance robust, interpretable
end-to-end autonomous driving systems. AD^2-Bench thus provides a standardized
evaluation platform, driving research forward by improving MLLMs' reasoning in
autonomous driving, making it an invaluable resource.

</details>


### [50] [SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields](https://arxiv.org/abs/2506.09565)
*Qijing Li,Jingxiang Sun,Liang An,Zhaoqi Su,Hongwen Zhang,Yebin Liu*

Main category: cs.CV

TL;DR: SemanticSplat是一种基于3D高斯和潜在语义属性的前馈语义感知3D重建方法，通过融合多特征场和两阶段蒸馏框架，实现稀疏视图下的多模态语义场景理解。


<details>
  <summary>Details</summary>
Motivation: 现有前馈方法（如LSM）仅能提取基于语言的语义，且几何重建质量低；而逐场景优化方法依赖密集视图，实用性差。因此，需要一种能联合建模几何、外观和语义的高效方法。

Method: SemanticSplat融合LSeg、SAM等多特征场，利用成本体积表示存储跨视图特征相似性，并通过两阶段蒸馏框架从稀疏视图重建多模态语义特征场。

Result: 实验证明，该方法在可提示和开放词汇分割等3D场景理解任务中表现优异。

Conclusion: SemanticSplat通过统一3D高斯与语义属性，实现了高效、准确的联合几何-外观-语义建模，适用于稀疏视图场景。

Abstract: Holistic 3D scene understanding, which jointly models geometry, appearance,
and semantics, is crucial for applications like augmented reality and robotic
interaction. Existing feed-forward 3D scene understanding methods (e.g., LSM)
are limited to extracting language-based semantics from scenes, failing to
achieve holistic scene comprehension. Additionally, they suffer from
low-quality geometry reconstruction and noisy artifacts. In contrast, per-scene
optimization methods rely on dense input views, which reduces practicality and
increases complexity during deployment. In this paper, we propose
SemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which
unifies 3D Gaussians with latent semantic attributes for joint
geometry-appearance-semantics modeling. To predict the semantic anisotropic
Gaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a
cost volume representation that stores cross-view feature similarities,
enhancing coherent and accurate scene comprehension. Leveraging a two-stage
distillation framework, SemanticSplat reconstructs a holistic multi-modal
semantic feature field from sparse-view images. Experiments demonstrate the
effectiveness of our method for 3D scene understanding tasks like promptable
and open-vocabulary segmentation. Video results are available at
https://semanticsplat.github.io.

</details>


### [51] [Consistent Story Generation with Asymmetry Zigzag Sampling](https://arxiv.org/abs/2506.09612)
*Mingxiao LI,mang ning,Marie-Francine Moens*

Main category: cs.CV

TL;DR: 提出了一种名为Zigzag Sampling with Asymmetric Prompts and Visual Sharing的无训练采样策略，用于提升视觉故事生成中的主题一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成多张图像时难以保持主题一致性，且资源消耗大或效果有限。

Method: 采用Zigzag采样机制，结合非对称提示和视觉共享模块，以增强主题一致性。

Result: 实验表明，该方法在生成连贯一致的视觉故事方面显著优于现有方法。

Conclusion: 该方法为视觉故事生成提供了一种高效且无需训练的一致性增强方案。

Abstract: Text-to-image generation models have made significant progress in producing
high-quality images from textual descriptions, yet they continue to struggle
with maintaining subject consistency across multiple images, a fundamental
requirement for visual storytelling. Existing methods attempt to address this
by either fine-tuning models on large-scale story visualization datasets, which
is resource-intensive, or by using training-free techniques that share
information across generations, which still yield limited success. In this
paper, we introduce a novel training-free sampling strategy called Zigzag
Sampling with Asymmetric Prompts and Visual Sharing to enhance subject
consistency in visual story generation. Our approach proposes a zigzag sampling
mechanism that alternates between asymmetric prompting to retain subject
characteristics, while a visual sharing module transfers visual cues across
generated images to %further enforce consistency. Experimental results, based
on both quantitative metrics and qualitative evaluations, demonstrate that our
method significantly outperforms previous approaches in generating coherent and
consistent visual stories. The code is available at
https://github.com/Mingxiao-Li/Asymmetry-Zigzag-StoryDiffusion.

</details>


### [52] [ECAM: A Contrastive Learning Approach to Avoid Environmental Collision in Trajectory Forecasting](https://arxiv.org/abs/2506.09626)
*Giacomo Rosin,Muhammad Rameez Ur Rahman,Sebastiano Vascon*

Main category: cs.CV

TL;DR: 论文提出了一种基于对比学习的模块ECAM，用于增强轨迹预测模型的环境碰撞避免能力，显著降低了碰撞率。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹预测方法常忽视环境因素的影响，导致预测轨迹与障碍物碰撞。

Method: 提出ECAM模块，通过对比学习增强环境碰撞避免能力，可集成到现有模型中。

Result: 在ETH/UCY数据集上实验表明，集成ECAM的模型碰撞率显著降低40-50%。

Conclusion: ECAM模块有效提升了轨迹预测模型的碰撞避免能力，且易于集成到现有方法中。

Abstract: Human trajectory forecasting is crucial in applications such as autonomous
driving, robotics and surveillance. Accurate forecasting requires models to
consider various factors, including social interactions, multi-modal
predictions, pedestrian intention and environmental context. While existing
methods account for these factors, they often overlook the impact of the
environment, which leads to collisions with obstacles. This paper introduces
ECAM (Environmental Collision Avoidance Module), a contrastive learning-based
module to enhance collision avoidance ability with the environment. The
proposed module can be integrated into existing trajectory forecasting models,
improving their ability to generate collision-free predictions. We evaluate our
method on the ETH/UCY dataset and quantitatively and qualitatively demonstrate
its collision avoidance capabilities. Our experiments show that
state-of-the-art methods significantly reduce (-40/50%) the collision rate when
integrated with the proposed module. The code is available at
https://github.com/CVML-CFU/ECAM.

</details>


### [53] [HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding](https://arxiv.org/abs/2506.09634)
*Yanzhao Shi,Xiaodan Zhang,Junzhong Ji,Haoning Jiang,Chengxin Zheng,Yinong Wang,Liangqiong Qu*

Main category: cs.CV

TL;DR: HSENet提出了一种结合3D医学图像和语言理解的框架，通过双3D视觉编码器和空间压缩技术提升诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对2D医学图像，无法捕捉复杂3D解剖结构，导致误诊。

Method: HSENet采用双3D视觉编码器感知全局和细节，结合Spatial Packer压缩高分辨率3D区域为视觉标记。

Result: 在3D视觉语言检索、医学报告生成和视觉问答任务中取得SOTA性能。

Conclusion: HSENet显著提升了3D医学图像与语言理解的性能，具有实际应用价值。

Abstract: Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based
decisions by enhancing diagnostic accuracy and workflow efficiency. While
multimodal large language models (MLLMs) exhibit promising performance in
visual-language understanding, existing methods mainly focus on 2D medical
images, which fundamentally limits their ability to capture complex 3D
anatomical structures. This limitation often leads to misinterpretation of
subtle pathologies and causes diagnostic hallucinations. In this paper, we
present Hybrid Spatial Encoding Network (HSENet), a framework that exploits
enriched 3D medical visual cues by effective visual perception and projection
for accurate and robust vision-language understanding. Specifically, HSENet
employs dual-3D vision encoders to perceive both global volumetric contexts and
fine-grained anatomical details, which are pre-trained by dual-stage alignment
with diagnostic reports. Furthermore, we propose Spatial Packer, an efficient
multimodal projector that condenses high-resolution 3D spatial regions into a
compact set of informative visual tokens via centroid-based compression. By
assigning spatial packers with dual-3D vision encoders, HSENet can seamlessly
perceive and transfer hybrid visual representations to LLM's semantic space,
facilitating accurate diagnostic text generation. Experimental results
demonstrate that our method achieves state-of-the-art performance in 3D
language-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report
generation (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering
(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.
Our code is available at https://github.com/YanzhaoShi/HSENet.

</details>


### [54] [DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning](https://arxiv.org/abs/2506.09644)
*Dongxu Liu,Yuang Peng,Haomiao Tang,Yuwei Chen,Chunrui Han,Zheng Ge,Daxin Jiang,Mingxue Liao*

Main category: cs.CV

TL;DR: DGAE通过扩散模型引导解码器，提升高压缩比下的性能，同时减少潜在空间维度。


<details>
  <summary>Details</summary>
Motivation: 解决GAN导致的训练不稳定问题，并优化潜在空间的紧凑性。

Method: 提出DGAE，利用扩散模型指导解码器恢复未完全解码的信息。

Result: 在高空间压缩率下性能显著提升，潜在空间缩小2倍，扩散模型收敛更快。

Conclusion: DGAE在图像生成中表现优异，潜在空间更紧凑且高效。

Abstract: Autoencoders empower state-of-the-art image and video generative models by
compressing pixels into a latent space through visual tokenization. Although
recent advances have alleviated the performance degradation of autoencoders
under high compression ratios, addressing the training instability caused by
GAN remains an open challenge. While improving spatial compression, we also aim
to minimize the latent space dimensionality, enabling more efficient and
compact representations. To tackle these challenges, we focus on improving the
decoder's expressiveness. Concretely, we propose DGAE, which employs a
diffusion model to guide the decoder in recovering informative signals that are
not fully decoded from the latent representation. With this design, DGAE
effectively mitigates the performance degradation under high spatial
compression rates. At the same time, DGAE achieves state-of-the-art performance
with a 2x smaller latent space. When integrated with Diffusion Models, DGAE
demonstrates competitive performance on image generation for ImageNet-1K and
shows that this compact latent representation facilitates faster convergence of
the diffusion model.

</details>


### [55] [Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation](https://arxiv.org/abs/2506.09663)
*Haowen Wang,Xiaoping Yuan,Zhao Jin,Zhen Zhao,Zhengping Che,Yousong Xue,Jin Tian,Yakun Huang,Jian Tang*

Main category: cs.CV

TL;DR: DeGSS是一种统一框架，通过可变形3D高斯场编码多部件物体的几何、外观和运动，实现无监督的部件分割和精确运动建模。


<details>
  <summary>Details</summary>
Motivation: 现有方法在缺乏人工标注时难以构建多部件物体的统一表示，DeGSS旨在解决这一问题。

Method: DeGSS将物体建模为可变形3D高斯场，通过平滑变形轨迹实现无监督的部件分割，并支持部件级重建。

Result: 实验表明，DeGSS在准确性和稳定性上优于现有方法，并发布了新的数据集RS-Art。

Conclusion: DeGSS为多部件物体提供了一种紧凑且连续的表示方法，适用于无监督场景。

Abstract: Articulated objects are ubiquitous in everyday life, and accurate 3D
representations of their geometry and motion are critical for numerous
applications. However, in the absence of human annotation, existing approaches
still struggle to build a unified representation for objects that contain
multiple movable parts. We introduce DeGSS, a unified framework that encodes
articulated objects as deformable 3D Gaussian fields, embedding geometry,
appearance, and motion in one compact representation. Each interaction state is
modeled as a smooth deformation of a shared field, and the resulting
deformation trajectories guide a progressive coarse-to-fine part segmentation
that identifies distinct rigid components, all in an unsupervised manner. The
refined field provides a spatially continuous, fully decoupled description of
every part, supporting part-level reconstruction and precise modeling of their
kinematic relationships. To evaluate generalization and realism, we enlarge the
synthetic PartNet-Mobility benchmark and release RS-Art, a real-to-sim dataset
that pairs RGB captures with accurately reverse-engineered 3D models. Extensive
experiments demonstrate that our method outperforms existing methods in both
accuracy and stability.

</details>


### [56] [CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain](https://arxiv.org/abs/2506.09668)
*Maik Dannecker,Vasiliki Sideri-Lampretsa,Sophie Starck,Angeline Mihailov,Mathieu Milh,Nadine Girard,Guillaume Auzias,Daniel Rueckert*

Main category: cs.CV

TL;DR: CINeMA是一种新型框架，用于在数据稀缺情况下创建高分辨率、多模态的胎儿和新生儿脑图谱，显著提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 研究胎儿和新生儿大脑的快速神经发育需要高分辨率图谱，但传统方法依赖大数据集，难以应用于病理情况。

Method: CINeMA在潜在空间中操作，避免了计算密集的图像配准，并支持基于解剖特征的灵活条件设置。

Result: CINeMA在准确性、效率和多功能性上优于现有方法，支持组织分割、年龄预测等任务。

Conclusion: CINeMA为脑研究提供了强大工具，适用于低数据环境，并开源了代码和图谱。

Abstract: Magnetic resonance imaging of fetal and neonatal brains reveals rapid
neurodevelopment marked by substantial anatomical changes unfolding within
days. Studying this critical stage of the developing human brain, therefore,
requires accurate brain models-referred to as atlases-of high spatial and
temporal resolution. To meet these demands, established traditional atlases and
recently proposed deep learning-based methods rely on large and comprehensive
datasets. This poses a major challenge for studying brains in the presence of
pathologies for which data remains scarce. We address this limitation with
CINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for
creating high-resolution, spatio-temporal, multimodal brain atlases, suitable
for low-data settings. Unlike established methods, CINeMA operates in latent
space, avoiding compute-intensive image registration and reducing atlas
construction times from days to minutes. Furthermore, it enables flexible
conditioning on anatomical features including GA, birth age, and pathologies
like ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA
supports downstream tasks such as tissue segmentation and age prediction
whereas its generative properties enable synthetic data creation and
anatomically informed data augmentation. Surpassing state-of-the-art methods in
accuracy, efficiency, and versatility, CINeMA represents a powerful tool for
advancing brain research. We release the code and atlases at
https://github.com/m-dannecker/CINeMA.

</details>


### [57] [Reasoning Models Are More Easily Gaslighted Than You Think](https://arxiv.org/abs/2506.09677)
*Bin Zhu,Hailong Yin,Jingjing Chen,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 论文评估了三种先进推理模型在误导性用户输入下的表现，发现其准确性显著下降，并提出了GaslightingBench-R基准以进一步测试模型的抗干扰能力。


<details>
  <summary>Details</summary>
Motivation: 探索推理模型在面对误导性用户输入时的鲁棒性，填补现有研究的空白。

Method: 系统评估了三种推理模型（OpenAI的o4-mini、Claude-3.7-Sonnet和Gemini-2.5-Flash）在三个多模态基准（MMMU、MathVista和CharXiv）上的表现，并设计了GaslightingBench-R基准。

Result: 模型在误导性提示下准确性平均下降25-29%，在GaslightingBench-R上下降超过53%。

Conclusion: 推理模型在保持信念持久性方面存在根本性局限，揭示了逐步推理与信念坚持之间的差距。

Abstract: Recent advances in reasoning-centric models promise improved robustness
through mechanisms such as chain-of-thought prompting and test-time scaling.
However, their ability to withstand misleading user input remains
underexplored. In this paper, we conduct a systematic evaluation of three
state-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet
and Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and
CharXiv. Our evaluation reveals significant accuracy drops (25-29% on average)
following gaslighting negation prompts, indicating that even top-tier reasoning
models struggle to preserve correct answers under manipulative user feedback.
Built upon the insights of the evaluation and to further probe this
vulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark
specifically designed to evaluate reasoning models' susceptibility to defend
their belief under gaslighting negation prompt. Constructed by filtering and
curating 1,025 challenging samples from the existing benchmarks,
GaslightingBench-R induces even more dramatic failures, with accuracy drops
exceeding 53% on average. Our findings reveal fundamental limitations in the
robustness of reasoning models, highlighting the gap between step-by-step
reasoning and belief persistence.

</details>


### [58] [Adding simple structure at inference improves Vision-Language Compositionality](https://arxiv.org/abs/2506.09691)
*Imanol Miranda,Ander Salaberria,Eneko Agirre,Gorka Azkune*

Main category: cs.CV

TL;DR: 论文提出了一种在推理时通过分割图像和文本来提升视觉语言模型（VLM）组合性的方法，无需训练即可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的双编码器视觉语言模型（如CLIP）在组合性任务上表现不佳，缺乏对图像和文本结构的有效利用。

Method: 在推理时，将图像分割为小块，提取文本片段（对象、属性和关系），使用VLM匹配图像块与文本片段，并聚合相似度得分。

Result: 该方法在多个数据集上显著提升了VLM的性能，尤其在属性-对象绑定任务中表现突出。

Conclusion: 推理时技术具有潜力，图像分割是关键因素，未来可进一步优化推理方法。

Abstract: Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for
image-text retrieval tasks. However, those models struggle with
compositionality, showing a bag-of-words-like behavior that limits their
retrieval performance. Many different training approaches have been proposed to
improve the vision-language compositionality capabilities of those models. In
comparison, inference-time techniques have received little attention. In this
paper, we propose to add simple structure at inference, where, given an image
and a caption: i) we divide the image into different smaller crops, ii) we
extract text segments, capturing objects, attributes and relations, iii) using
a VLM, we find the image crops that better align with text segments obtaining
matches, and iv) we compute the final image-text similarity aggregating the
individual similarities of the matches. Based on various popular dual encoder
VLMs, we evaluate our approach in controlled and natural datasets for VL
compositionality. We find that our approach consistently improves the
performance of evaluated VLMs without any training, which shows the potential
of inference-time techniques. The results are especially good for
attribute-object binding as shown in the controlled dataset. As a result of an
extensive analysis: i) we show that processing image crops is actually
essential for the observed gains in performance, and ii) we identify specific
areas to further improve inference-time approaches.

</details>


### [59] [Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model](https://arxiv.org/abs/2506.09695)
*Changwei Wu,Yifei Chen,Yuxin Du,Jinying Zong,Jie Dong,Mingxuan Liu,Yong Peng,Jin Fan,Feiwei Qin,Changmiao Wang*

Main category: cs.CV

TL;DR: FasterSNN是一种混合神经架构，结合了生物启发的LIF神经元和区域自适应卷积，用于高效、稀疏处理3D MRI数据，以提升阿尔茨海默病的早期诊断。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病（AD）的早期诊断受限于主观评估和高成本的多模态成像，而现有深度学习方法能效低且计算需求高，难以在资源受限环境中部署。

Method: 提出FasterSNN，整合LIF神经元、区域自适应卷积和多尺度脉冲注意力，以稀疏、高效地处理3D MRI数据。

Result: 在基准数据集上，FasterSNN表现出竞争性性能，同时显著提升效率和稳定性。

Conclusion: FasterSNN为AD筛查提供了一种高效、稳定的解决方案，具有实际应用潜力。

Abstract: Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive
impairment (MCI) stage, is vital yet hindered by subjective assessments and the
high cost of multimodal imaging modalities. Although deep learning methods
offer automated alternatives, their energy inefficiency and computational
demands limit real-world deployment, particularly in resource-constrained
settings. As a brain-inspired paradigm, spiking neural networks (SNNs) are
inherently well-suited for modeling the sparse, event-driven patterns of neural
degeneration in AD, offering a promising foundation for interpretable and
low-power medical diagnostics. However, existing SNNs often suffer from weak
expressiveness and unstable training, which restrict their effectiveness in
complex medical tasks. To address these limitations, we propose FasterSNN, a
hybrid neural architecture that integrates biologically inspired LIF neurons
with region-adaptive convolution and multi-scale spiking attention. This design
enables sparse, efficient processing of 3D MRI while preserving diagnostic
accuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves
competitive performance with substantially improved efficiency and stability,
supporting its potential for practical AD screening. Our source code is
available at https://github.com/wuchangw/FasterSNN.

</details>


### [60] [CHIP: A multi-sensor dataset for 6D pose estimation of chairs in industrial settings](https://arxiv.org/abs/2506.09699)
*Mattia Nardon,Mikel Mujika Agirre,Ander González Tomé,Daniel Sedano Algarabel,Josep Rueda Collell,Ana Paola Caro,Andrea Caraffa,Fabio Poiesi,Paul Ian Chippendale,Davide Boscaini*

Main category: cs.CV

TL;DR: CHIP是首个针对工业环境中机器人操作的6D姿态估计数据集，填补了现有数据集的不足，包含77,811张RGBD图像和自动标注的真实姿态。


<details>
  <summary>Details</summary>
Motivation: 现有6D姿态估计数据集多针对家庭环境，缺乏工业场景的真实性，CHIP旨在解决这一问题。

Method: CHIP数据集包含7种椅子，使用3种RGBD传感器采集，并引入干扰物和遮挡挑战。

Result: 基准测试显示现有方法在CHIP数据集上仍有较大改进空间。

Conclusion: CHIP为工业场景的6D姿态估计提供了新基准，并将公开以促进研究。

Abstract: Accurate 6D pose estimation of complex objects in 3D environments is
essential for effective robotic manipulation. Yet, existing benchmarks fall
short in evaluating 6D pose estimation methods under realistic industrial
conditions, as most datasets focus on household objects in domestic settings,
while the few available industrial datasets are limited to artificial setups
with objects placed on tables. To bridge this gap, we introduce CHIP, the first
dataset designed for 6D pose estimation of chairs manipulated by a robotic arm
in a real-world industrial environment. CHIP includes seven distinct chairs
captured using three different RGBD sensing technologies and presents unique
challenges, such as distractor objects with fine-grained differences and severe
occlusions caused by the robotic arm and human operators. CHIP comprises 77,811
RGBD images annotated with ground-truth 6D poses automatically derived from the
robot's kinematics, averaging 11,115 annotations per chair. We benchmark CHIP
using three zero-shot 6D pose estimation methods, assessing performance across
different sensor types, localization priors, and occlusion levels. Results show
substantial room for improvement, highlighting the unique challenges posed by
the dataset. CHIP will be publicly released.

</details>


### [61] [Non-Contact Health Monitoring During Daily Personal Care Routines](https://arxiv.org/abs/2506.09718)
*Xulin Ma,Jiankai Tang,Zhang Jiang,Songqin Cheng,Yuanchun Shi,Dong LI,Xin Liu,Daniel McDuff,Xiaojing Liu,Yuntao Wang*

Main category: cs.CV

TL;DR: LADH数据集结合RGB和红外视频输入，提升了非接触式生理监测的准确性和鲁棒性，心率估计的平均绝对误差为4.99 BPM。


<details>
  <summary>Details</summary>
Motivation: 解决远程光电容积描记术（rPPG）在长期个人护理场景中因环境光照变化、频繁遮挡和动态面部姿势带来的挑战。

Method: 提出LADH数据集，包含240个同步RGB和红外面部视频，结合多任务学习提升性能。

Result: RGB和红外视频输入结合显著提升生理信号监测精度，心率估计MAE为4.99 BPM。

Conclusion: LADH数据集和融合方法为长期非接触式健康监测提供了有效解决方案。

Abstract: Remote photoplethysmography (rPPG) enables non-contact, continuous monitoring
of physiological signals and offers a practical alternative to traditional
health sensing methods. Although rPPG is promising for daily health monitoring,
its application in long-term personal care scenarios, such as mirror-facing
routines in high-altitude environments, remains challenging due to ambient
lighting variations, frequent occlusions from hand movements, and dynamic
facial postures. To address these challenges, we present LADH (Long-term
Altitude Daily Health), the first long-term rPPG dataset containing 240
synchronized RGB and infrared (IR) facial videos from 21 participants across
five common personal care scenarios, along with ground-truth PPG, respiration,
and blood oxygen signals. Our experiments demonstrate that combining RGB and IR
video inputs improves the accuracy and robustness of non-contact physiological
monitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate
estimation. Furthermore, we find that multi-task learning enhances performance
across multiple physiological indicators simultaneously. Dataset and code are
open at https://github.com/McJackTang/FusionVitals.

</details>


### [62] [The Four Color Theorem for Cell Instance Segmentation](https://arxiv.org/abs/2506.09724)
*Ye Zhang,Yu Zhou,Yifeng Wang,Jun Xiao,Ziyue Wang,Yongbing Zhang,Jianxu Chen*

Main category: cs.CV

TL;DR: 提出了一种基于四色定理的新型细胞实例分割方法，通过四色编码简化实例区分过程，并设计了渐进训练策略解决编码不唯一性问题。


<details>
  <summary>Details</summary>
Motivation: 准确区分紧密接触的细胞是生物医学图像分析的关键挑战，现有方法在性能与计算效率之间难以平衡。

Method: 将细胞类比为国家，组织类比为海洋，引入四色编码方案，将实例分割转化为仅需预测四类的语义分割问题。

Result: 在多种模式下实验表明，该方法达到了最先进的性能。

Conclusion: 该方法通过四色编码和渐进训练策略，显著简化了实例分割问题，并取得了优异效果。

Abstract: Cell instance segmentation is critical to analyzing biomedical images, yet
accurately distinguishing tightly touching cells remains a persistent
challenge. Existing instance segmentation frameworks, including
detection-based, contour-based, and distance mapping-based approaches, have
made significant progress, but balancing model performance with computational
efficiency remains an open problem. In this paper, we propose a novel cell
instance segmentation method inspired by the four-color theorem. By
conceptualizing cells as countries and tissues as oceans, we introduce a
four-color encoding scheme that ensures adjacent instances receive distinct
labels. This reformulation transforms instance segmentation into a constrained
semantic segmentation problem with only four predicted classes, substantially
simplifying the instance differentiation process. To solve the training
instability caused by the non-uniqueness of four-color encoding, we design an
asymptotic training strategy and encoding transformation method. Extensive
experiments on various modes demonstrate our approach achieves state-of-the-art
performance. The code is available at https://github.com/zhangye-zoe/FCIS.

</details>


### [63] [MPFNet: A Multi-Prior Fusion Network with a Progressive Training Strategy for Micro-Expression Recognition](https://arxiv.org/abs/2506.09735)
*Chuang Ma,Shaokai Zhao,Dongdong Zhou,Yu Pei,Zhiguo Luo,Liang Xie,Ye Yan,Erwei Yin*

Main category: cs.CV

TL;DR: 该论文提出了一种多先验融合网络（MPFNet），通过渐进式训练策略优化微表情识别（MER）任务，显著提升了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 微表情识别因其短暂性和低强度而更具挑战性，现有方法未能充分利用多源先验知识。

Method: 提出基于I3D和坐标注意力机制的两个互补编码器（GFE和AFE），并设计了MPFNet-P和MPFNet-C两种变体，模拟婴儿认知发展的并行和分层处理模式。

Result: 在SMIC、CASME II和SAMM数据集上分别达到0.811、0.924和0.857的准确率，在SMIC和SAMM上达到最优性能。

Conclusion: MPFNet通过多源先验知识的融合显著提升了MER性能，为未来研究提供了新思路。

Abstract: Micro-expression recognition (MER), a critical subfield of affective
computing, presents greater challenges than macro-expression recognition due to
its brief duration and low intensity. While incorporating prior knowledge has
been shown to enhance MER performance, existing methods predominantly rely on
simplistic, singular sources of prior knowledge, failing to fully exploit
multi-source information. This paper introduces the Multi-Prior Fusion Network
(MPFNet), leveraging a progressive training strategy to optimize MER tasks. We
propose two complementary encoders: the Generic Feature Encoder (GFE) and the
Advanced Feature Encoder (AFE), both based on Inflated 3D ConvNets (I3D) with
Coordinate Attention (CA) mechanisms, to improve the model's ability to capture
spatiotemporal and channel-specific features. Inspired by developmental
psychology, we present two variants of MPFNet--MPFNet-P and
MPFNet-C--corresponding to two fundamental modes of infant cognitive
development: parallel and hierarchical processing. These variants enable the
evaluation of different strategies for integrating prior knowledge. Extensive
experiments demonstrate that MPFNet significantly improves MER accuracy while
maintaining balanced performance across categories, achieving accuracies of
0.811, 0.924, and 0.857 on the SMIC, CASME II, and SAMM datasets, respectively.
To the best of our knowledge, our approach achieves state-of-the-art
performance on the SMIC and SAMM datasets.

</details>


### [64] [Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning](https://arxiv.org/abs/2506.09736)
*Yuting Li,Lai Wei,Kaipeng Zheng,Jingyuan Huang,Linghe Kong,Lichao Sun,Weiran Huang*

Main category: cs.CV

TL;DR: 研究发现，当前多模态大语言模型（MLLMs）在视觉处理上表现不足，仅提供图像描述的纯语言模型表现可比甚至优于MLLMs。为此，作者提出了一种无需算法修改或额外数据的视觉扰动框架，显著提升了模型的感知鲁棒性和数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在视觉处理上表现不佳，纯语言模型仅通过图像描述即可达到或超越MLLMs的性能，表明MLLMs在视觉与推理的整合上存在问题。

Method: 提出了一种视觉扰动框架，包含三种扰动策略：干扰项拼接、保持主导性的混合和随机旋转，可无缝集成到现有训练流程中。

Result: 实验表明，该方法在多个数据集上显著提升了数学推理性能，性能提升与算法改进相当，并在开源7B RL调优模型中达到竞争性表现。

Conclusion: 视觉扰动在多模态数学推理中至关重要，更好的推理始于更好的视觉处理。

Abstract: Despite the rapid progress of multimodal large language models (MLLMs), they
have largely overlooked the importance of visual processing. In a simple yet
revealing experiment, we interestingly find that language-only models, when
provided with image captions, can achieve comparable or even better performance
than MLLMs that consume raw visual inputs. This suggests that current MLLMs may
generate accurate visual descriptions but fail to effectively integrate them
during reasoning. Motivated by this, we propose a simple visual perturbation
framework that enhances perceptual robustness without requiring algorithmic
modifications or additional training data. Our approach introduces three
targeted perturbations: distractor concatenation, dominance-preserving mixup,
and random rotation, that can be easily integrated into existing post-training
pipelines including SFT, DPO, and GRPO. Through extensive experiments across
multiple datasets, we demonstrate consistent improvements in mathematical
reasoning performance, with gains comparable to those achieved through
algorithmic changes. Additionally, we achieve competitive performance among
open-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual
perturbation. Through comprehensive ablation studies, we analyze the
effectiveness of different perturbation strategies, revealing that each
perturbation type contributes uniquely to different aspects of visual
reasoning. Our findings highlight the critical role of visual perturbation in
multimodal mathematical reasoning: better reasoning begins with better seeing.
Our code is available at https://github.com/YutingLi0606/Vision-Matters.

</details>


### [65] [ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models](https://arxiv.org/abs/2506.09740)
*Qin Zhou,Zhiyang Zhang,Jinglong Wang,Xiaobin Li,Jing Zhang,Qian Yu,Lu Sheng,Dong Xu*

Main category: cs.CV

TL;DR: 论文提出了一种基于ELBO的方法（ELBO-T2IAlign），用于校准扩散模型中的像素-文本对齐问题，无需训练且适用于多种架构。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中表现出色，但现有方法假设文本-图像完美对齐，实际并非如此。因此，需要评估和校准这种对齐关系。

Method: 使用零样本参考图像分割作为代理任务，分析像素-文本不对齐问题，并提出基于ELBO的校准方法ELBO-T2IAlign。

Result: 实验验证了ELBO-T2IAlign在图像分割和生成任务中的有效性，尤其在处理小尺寸、遮挡或罕见物体时表现突出。

Conclusion: ELBO-T2IAlign是一种简单通用的方法，能够有效校准扩散模型中的像素-文本对齐问题。

Abstract: Diffusion models excel at image generation. Recent studies have shown that
these models not only generate high-quality images but also encode text-image
alignment information through attention maps or loss functions. This
information is valuable for various downstream tasks, including segmentation,
text-guided image editing, and compositional image generation. However, current
methods heavily rely on the assumption of perfect text-image alignment in
diffusion models, which is not the case. In this paper, we propose using
zero-shot referring image segmentation as a proxy task to evaluate the
pixel-level image and class-level text alignment of popular diffusion models.
We conduct an in-depth analysis of pixel-text misalignment in diffusion models
from the perspective of training data bias. We find that misalignment occurs in
images with small sized, occluded, or rare object classes. Therefore, we
propose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text
alignment in diffusion models based on the evidence lower bound (ELBO) of
likelihood. Our method is training-free and generic, eliminating the need to
identify the specific cause of misalignment and works well across various
diffusion model architectures. Extensive experiments on commonly used benchmark
datasets on image segmentation and generation have verified the effectiveness
of our proposed calibration approach.

</details>


### [66] [Class Similarity-Based Multimodal Classification under Heterogeneous Category Sets](https://arxiv.org/abs/2506.09745)
*Yangrui Zhu,Junhua Bao,Yipan Wei,Yapeng Li,Bo Du*

Main category: cs.CV

TL;DR: 论文提出了一种多模态异构类别集学习（MMHCL）任务，并提出了基于类别相似性的跨模态融合模型（CSCF），显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实应用中多模态数据的类别分布不一致，阻碍模型有效利用跨模态信息识别所有类别。

Method: CSCF将模态特征对齐到共享语义空间，通过不确定性估计选择最具判别性的模态，并基于类别相似性融合跨模态信息。

Result: 实验表明CSCF在多个基准数据集上显著优于现有方法。

Conclusion: CSCF有效解决了MMHCL任务，提升了多模态数据中类别识别的性能。

Abstract: Existing multimodal methods typically assume that different modalities share
the same category set. However, in real-world applications, the category
distributions in multimodal data exhibit inconsistencies, which can hinder the
model's ability to effectively utilize cross-modal information for recognizing
all categories. In this work, we propose the practical setting termed
Multi-Modal Heterogeneous Category-set Learning (MMHCL), where models are
trained in heterogeneous category sets of multi-modal data and aim to recognize
complete classes set of all modalities during test. To effectively address this
task, we propose a Class Similarity-based Cross-modal Fusion model (CSCF).
Specifically, CSCF aligns modality-specific features to a shared semantic space
to enable knowledge transfer between seen and unseen classes. It then selects
the most discriminative modality for decision fusion through uncertainty
estimation. Finally, it integrates cross-modal information based on class
similarity, where the auxiliary modality refines the prediction of the dominant
one. Experimental results show that our method significantly outperforms
existing state-of-the-art (SOTA) approaches on multiple benchmark datasets,
effectively addressing the MMHCL task.

</details>


### [67] [Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints](https://arxiv.org/abs/2506.09748)
*Xiangkai Zhang,Xiang Zhou,Mao Chen,Yuchen Lu,Xu Yang,Zhiyong Liu*

Main category: cs.CV

TL;DR: 提出了一种分层跨源图像匹配方法，用于无人机绝对定位，结合语义感知和结构约束的粗匹配模块与轻量级细粒度匹配模块，显著提升了定位精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在GNSS信号不可用时，无人机绝对定位面临挑战，现有视觉定位方法因跨源差异和时间变化导致匹配困难。

Method: 提出分层跨源图像匹配方法，包括语义感知和结构约束的粗匹配模块与轻量级细粒度匹配模块，构建无需相对定位技术的无人机视觉定位流程。

Result: 在公共基准数据集和新引入的CS-UAV数据集上验证了方法的优越性和鲁棒性。

Conclusion: 该方法在多种挑战性条件下表现出高精度和鲁棒性，验证了其有效性。

Abstract: Absolute localization, aiming to determine an agent's location with respect
to a global reference, is crucial for unmanned aerial vehicles (UAVs) in
various applications, but it becomes challenging when global navigation
satellite system (GNSS) signals are unavailable. Vision-based absolute
localization methods, which locate the current view of the UAV in a reference
satellite map to estimate its position, have become popular in GNSS-denied
scenarios. However, existing methods mostly rely on traditional and low-level
image matching, suffering from difficulties due to significant differences
introduced by cross-source discrepancies and temporal variations. To overcome
these limitations, in this paper, we introduce a hierarchical cross-source
image matching method designed for UAV absolute localization, which integrates
a semantic-aware and structure-constrained coarse matching module with a
lightweight fine-grained matching module. Specifically, in the coarse matching
module, semantic features derived from a vision foundation model first
establish region-level correspondences under semantic and structural
constraints. Then, the fine-grained matching module is applied to extract fine
features and establish pixel-level correspondences. Building upon this, a UAV
absolute visual localization pipeline is constructed without any reliance on
relative localization techniques, mainly by employing an image retrieval module
before the proposed hierarchical image matching modules. Experimental
evaluations on public benchmark datasets and a newly introduced CS-UAV dataset
demonstrate superior accuracy and robustness of the proposed method under
various challenging conditions, confirming its effectiveness.

</details>


### [68] [Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space](https://arxiv.org/abs/2506.09777)
*Anton Razzhigaev,Matvey Mikhalchuk,Klim Kireev,Igor Udovichenko,Andrey Kuznetsov,Aleksandr Petiushko*

Main category: cs.CV

TL;DR: DarkerBB是一种从黑盒识别模型中仅使用相似性分数重建彩色人脸图像的新方法，通过PCA特征空间零阶优化实现，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决仅使用相似性分数从黑盒模型中重建人脸图像的隐私威胁问题。

Method: 在PCA特征空间中进行零阶优化，仅依赖相似性分数。

Result: 在LFW、AgeDB-30和CFP-FP基准测试中达到最先进的验证准确率，查询效率高。

Conclusion: DarkerBB在信息有限的情况下仍能高效重建人脸图像，具有实际应用潜力。

Abstract: Reconstructing facial images from black-box recognition models poses a
significant privacy threat. While many methods require access to embeddings, we
address the more challenging scenario of model inversion using only similarity
scores. This paper introduces DarkerBB, a novel approach that reconstructs
color faces by performing zero-order optimization within a PCA-derived
eigenface space. Despite this highly limited information, experiments on LFW,
AgeDB-30, and CFP-FP benchmarks demonstrate that DarkerBB achieves
state-of-the-art verification accuracies in the similarity-only setting, with
competitive query efficiency.

</details>


### [69] [Q-SAM2: Accurate Quantization for Segment Anything Model 2](https://arxiv.org/abs/2506.09782)
*Nicola Farronato,Florian Scheidegger,Mattia Rigotti,Cristiano Malossi,Michele Magno,Haotong Qin*

Main category: cs.CV

TL;DR: Q-SAM2是一种针对SAM2的低比特量化方法，通过线性层校准和量化感知训练提升效率，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: SAM2的计算和内存消耗高，难以在资源受限场景中应用，因此需要一种高效的量化方法。

Method: 提出线性层校准方法优化权重分布，并设计量化感知训练流程抑制异常值。

Result: Q-SAM2在超低2比特量化下表现优异，比未校准模型提升66% mIoU精度。

Conclusion: Q-SAM2显著提升了SAM2的效率，同时保持了高精度，适用于资源受限场景。

Abstract: The Segment Anything Model 2 (SAM2) has gained significant attention as a
foundational approach for promptable image and video segmentation. However, its
expensive computational and memory consumption poses a severe challenge for its
application in resource-constrained scenarios. In this paper, we propose an
accurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To
address the performance degradation caused by the singularities in weight and
activation distributions during quantization, Q-SAM2 introduces two novel
technical contributions. We first introduce a linear layer calibration method
for low-bit initialization of SAM2, which minimizes the Frobenius norm over a
small image batch to reposition weight distributions for improved quantization.
We then propose a Quantization-Aware Training (QAT) pipeline that applies
clipping to suppress outliers and allows the network to adapt to quantization
thresholds during training. Our comprehensive experiments demonstrate that
Q-SAM2 allows for highly accurate inference while substantially improving
efficiency. Both quantitative and visual results show that our Q-SAM2 surpasses
existing state-of-the-art general quantization schemes, especially for
ultra-low 2-bit quantization. While designed for quantization-aware training,
our proposed calibration technique also proves effective in post-training
quantization, achieving up to a 66% mIoU accuracy improvement over
non-calibrated models.

</details>


### [70] [Accurate and efficient zero-shot 6D pose estimation with frozen foundation models](https://arxiv.org/abs/2506.09784)
*Andrea Caraffa,Davide Boscaini,Fabio Poiesi*

Main category: cs.CV

TL;DR: FreeZeV2是一种无需训练的6D姿态估计方法，通过几何和视觉基础模型实现对新物体的强泛化能力，显著提升了速度和精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法需要大量任务特定训练数据的问题，提出一种无需训练的方法，以实现对新物体的高效准确姿态估计。

Method: 采用稀疏特征提取、特征感知评分机制和模块化设计，结合预训练的几何和视觉模型。

Result: 在BOP Benchmark上达到新SOTA，速度提升8倍，精度提高5%；使用分割模型集成后，精度再提升8%，速度仍快2.5倍。

Conclusion: FreeZeV2证明了无需任务特定训练即可实现高效准确的6D姿态估计，为领域提供了新思路。

Abstract: Estimating the 6D pose of objects from RGBD data is a fundamental problem in
computer vision, with applications in robotics and augmented reality. A key
challenge is achieving generalization to novel objects that were not seen
during training. Most existing approaches address this by scaling up training
on synthetic data tailored to the task, a process that demands substantial
computational resources. But is task-specific training really necessary for
accurate and efficient 6D pose estimation of novel objects? To answer No!, we
introduce FreeZeV2, the second generation of FreeZe: a training-free method
that achieves strong generalization to unseen objects by leveraging geometric
and vision foundation models pre-trained on unrelated data. FreeZeV2 improves
both accuracy and efficiency over FreeZe through three key contributions: (i) a
sparse feature extraction strategy that reduces inference-time computation
without sacrificing accuracy; (ii) a feature-aware scoring mechanism that
improves both pose selection during RANSAC-based 3D registration and the final
ranking of pose candidates; and (iii) a modular design that supports ensembles
of instance segmentation models, increasing robustness to segmentation masks
errors. We evaluate FreeZeV2 on the seven core datasets of the BOP Benchmark,
where it establishes a new state-of-the-art in 6D pose estimation of unseen
objects. When using the same segmentation masks, FreeZeV2 achieves a remarkable
8x speedup over FreeZe while also improving accuracy by 5%. When using
ensembles of segmentation models, FreeZeV2 gains an additional 8% in accuracy
while still running 2.5x faster than FreeZe. FreeZeV2 was awarded Best Overall
Method at the BOP Challenge 2024.

</details>


### [71] [DreamCS: Geometry-Aware Text-to-3D Generation with Unpaired 3D Reward Supervision](https://arxiv.org/abs/2506.09814)
*Xiandong Zou,Ruihao Xia,Hongsong Wang,Pan Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种新方法DreamCS，通过构建3D-MeshPref数据集和RewardCS奖励模型，解决了现有文本到3D生成方法中几何偏差和人类偏好不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到3D生成方法因依赖2D偏好数据而存在几何偏差，难以满足人类偏好。

Method: 构建3D-MeshPref数据集，开发RewardCS奖励模型，并整合到DreamCS框架中。

Result: DreamCS在实验中表现优于现有方法，生成的3D资产更符合人类偏好且几何更准确。

Conclusion: DreamCS通过直接学习3D偏好数据，显著提升了文本到3D生成的质量和人类对齐性。

Abstract: While text-to-3D generation has attracted growing interest, existing methods
often struggle to produce 3D assets that align well with human preferences.
Current preference alignment techniques for 3D content typically rely on
hardly-collected preference-paired multi-view 2D images to train 2D reward
models, when then guide 3D generation -- leading to geometric artifacts due to
their inherent 2D bias. To address these limitations, we construct 3D-MeshPref,
the first large-scale unpaired 3D preference dataset, featuring diverse 3D
meshes annotated by a large language model and refined by human evaluators. We
then develop RewardCS, the first reward model trained directly on unpaired
3D-MeshPref data using a novel Cauchy-Schwarz divergence objective, enabling
effective learning of human-aligned 3D geometric preferences without requiring
paired comparisons. Building on this, we propose DreamCS, a unified framework
that integrates RewardCS into text-to-3D pipelines -- enhancing both implicit
and explicit 3D generation with human preference feedback. Extensive
experiments show DreamCS outperforms prior methods, producing 3D assets that
are both geometrically faithful and human-preferred. Code and models will be
released publicly.

</details>


### [72] [MMME: A Spontaneous Multi-Modal Micro-Expression Dataset Enabling Visual-Physiological Fusion](https://arxiv.org/abs/2506.09834)
*Chuang Maa,Yu Peia,Jianhang Zhanga,Shaokai Zhaoa,Bowen Jib,Liang Xiea,Ye Yana,Erwei Yin*

Main category: cs.CV

TL;DR: 论文提出了一个多模态微表情数据集MMME，结合视觉和生理信号，显著提升了微表情识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有微表情研究仅依赖单一视觉模态，忽略了其他生理模态的情感信息，导致性能不足。

Method: 开发了MMME数据集，同步采集面部动作信号、中枢神经系统信号和外周生理信号，并进行多模态融合分析。

Result: 实验验证了数据集的可靠性，多模态融合显著提升了微表情识别和检测性能。

Conclusion: MMME是目前最全面的微表情数据集，推动了从单模态视觉分析到多模态融合的研究范式转变。

Abstract: Micro-expressions (MEs) are subtle, fleeting nonverbal cues that reveal an
individual's genuine emotional state. Their analysis has attracted considerable
interest due to its promising applications in fields such as healthcare,
criminal investigation, and human-computer interaction. However, existing ME
research is limited to single visual modality, overlooking the rich emotional
information conveyed by other physiological modalities, resulting in ME
recognition and spotting performance far below practical application needs.
Therefore, exploring the cross-modal association mechanism between ME visual
features and physiological signals (PS), and developing a multimodal fusion
framework, represents a pivotal step toward advancing ME analysis. This study
introduces a novel ME dataset, MMME, which, for the first time, enables
synchronized collection of facial action signals (MEs), central nervous system
signals (EEG), and peripheral PS (PPG, RSP, SKT, EDA, and ECG). By overcoming
the constraints of existing ME corpora, MMME comprises 634 MEs, 2,841
macro-expressions (MaEs), and 2,890 trials of synchronized multimodal PS,
establishing a robust foundation for investigating ME neural mechanisms and
conducting multimodal fusion-based analyses. Extensive experiments validate the
dataset's reliability and provide benchmarks for ME analysis, demonstrating
that integrating MEs with PS significantly enhances recognition and spotting
performance. To the best of our knowledge, MMME is the most comprehensive ME
dataset to date in terms of modality diversity. It provides critical data
support for exploring the neural mechanisms of MEs and uncovering the
visual-physiological synergistic effects, driving a paradigm shift in ME
research from single-modality visual analysis to multimodal fusion. The dataset
will be publicly available upon acceptance of this paper.

</details>


### [73] [DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction](https://arxiv.org/abs/2506.09836)
*Junli Deng,Ping Shi,Qipei Li,Jinyang Guo*

Main category: cs.CV

TL;DR: DynaSplat通过动态-静态分离和分层运动建模扩展高斯泼溅技术，实现动态场景的高精度重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理真实世界动态场景的复杂性，需要一种更高效、直观的动态重建方法。

Method: 结合变形偏移统计和2D运动流一致性分类静态与动态元素，采用分层运动建模处理全局和局部运动，并基于物理的透明度估计提升视觉效果。

Result: 在复杂数据集上，DynaSplat在准确性和真实感上超越现有方法，且更紧凑高效。

Conclusion: DynaSplat为动态场景重建提供了一种更优的解决方案。

Abstract: Reconstructing intricate, ever-changing environments remains a central
ambition in computer vision, yet existing solutions often crumble before the
complexity of real-world dynamics. We present DynaSplat, an approach that
extends Gaussian Splatting to dynamic scenes by integrating dynamic-static
separation and hierarchical motion modeling. First, we classify scene elements
as static or dynamic through a novel fusion of deformation offset statistics
and 2D motion flow consistency, refining our spatial representation to focus
precisely where motion matters. We then introduce a hierarchical motion
modeling strategy that captures both coarse global transformations and
fine-grained local movements, enabling accurate handling of intricate,
non-rigid motions. Finally, we integrate physically-based opacity estimation to
ensure visually coherent reconstructions, even under challenging occlusions and
perspective shifts. Extensive experiments on challenging datasets reveal that
DynaSplat not only surpasses state-of-the-art alternatives in accuracy and
realism but also provides a more intuitive, compact, and efficient route to
dynamic scene reconstruction.

</details>


### [74] [OctoNav: Towards Generalist Embodied Navigation](https://arxiv.org/abs/2506.09839)
*Chen Gao,Liankai Jin,Xingyu Peng,Jiazhao Zhang,Yue Deng,Annan Li,He Wang,Si Liu*

Main category: cs.CV

TL;DR: 提出了一种通用导航代理OctoNav-R1，支持多模态自由指令，并通过混合训练范式提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有导航研究分散在不同任务中，缺乏通用性，需支持多模态自由指令的导航代理。

Method: 构建OctoNav-Bench基准和TBA-CoT数据集，设计混合训练范式（HTP），包括TBA-SFT、Nav-GPRO和在线强化学习阶段。

Result: OctoNav-R1性能优于现有方法。

Conclusion: 通过多模态数据集和混合训练范式，实现了通用导航代理的推理能力提升。

Abstract: Embodied navigation stands as a foundation pillar within the broader pursuit
of embodied AI. However, previous navigation research is divided into different
tasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task
objectives and modalities, making datasets and methods are designed
individually. In this work, we take steps toward generalist navigation agents,
which can follow free-form instructions that include arbitrary compounds of
multi-modal and multi-capability. To achieve this, we propose a large-scale
benchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1.
Specifically, OctoNav-Bench features continuous environments and is constructed
via a designed annotation pipeline. We thoroughly craft instruction-trajectory
pairs, where instructions are diverse in free-form with arbitrary modality and
capability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within
OctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1,
we build it upon MLLMs and adapt it to a VLA-type model, which can produce
low-level actions solely based on 2D visual observations. Moreover, we design a
Hybrid Training Paradigm (HTP) that consists of three stages, i.e.,
Action-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains
specifically designed learning policies and rewards. Importantly, for TBA-SFT
and Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which
show impressive reasoning ability via thinking-before-answer. Thus, we aim to
investigate how to achieve thinking-before-action in the embodied navigation
field, to improve model's reasoning ability toward generalists. Specifically,
we propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a
cold-start phrase and then leverage Nav-GPRO to improve its thinking ability.
Finally, OctoNav-R1 shows superior performance compared with previous methods.

</details>


### [75] [Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition](https://arxiv.org/abs/2506.09846)
*Panagiotis Kaliosis,John Pavlopoulos*

Main category: cs.CV

TL;DR: 提出了一种新的损失函数，结合Wasserstein距离优化手写文本识别的字符频率分布，提升模型在时间和上下文变化下的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 手写文本识别因字符集的动态变化和上下文依赖性而具有挑战性，传统模型在特定子集上表现不佳。

Method: 提出了一种基于Wasserstein距离的损失函数，惩罚预测文本与目标字符频率分布的差异，并在推理时通过引导解码方案改进现有模型。

Result: 实验结果表明，该方法在多个数据集和架构上显著提升了泛化能力和性能。

Conclusion: 通过字符分布对齐，该方法有效提升了手写文本识别的准确性和鲁棒性，且无需重新训练即可改进现有模型。

Abstract: Handwritten text recognition aims to convert visual input into
machine-readable text, and it remains challenging due to the evolving and
context-dependent nature of handwriting. Character sets change over time, and
character frequency distributions shift across historical periods or regions,
often causing models trained on broad, heterogeneous corpora to underperform on
specific subsets. To tackle this, we propose a novel loss function that
incorporates the Wasserstein distance between the character frequency
distribution of the predicted text and a target distribution empirically
derived from training data. By penalizing divergence from expected
distributions, our approach enhances both accuracy and robustness under
temporal and contextual intra-dataset shifts. Furthermore, we demonstrate that
character distribution alignment can also improve existing models at inference
time without requiring retraining by integrating it as a scoring function in a
guided decoding scheme. Experimental results across multiple datasets and
architectures confirm the effectiveness of our method in boosting
generalization and performance. We open source our code at
https://github.com/pkaliosis/fada.

</details>


### [76] [IntPhys 2: Benchmarking Intuitive Physics Understanding In Complex Synthetic Environments](https://arxiv.org/abs/2506.09849)
*Florian Bordes,Quentin Garrido,Justine T Kao,Adina Williams,Michael Rabbat,Emmanuel Dupoux*

Main category: cs.CV

TL;DR: IntPhys 2是一个视频基准测试，用于评估深度学习模型对直观物理的理解能力，发现现有模型在复杂场景中表现不佳，与人类表现差距显著。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估深度学习模型对直观物理原则（如永久性、不可变性等）的理解能力，揭示当前模型的局限性。

Method: 基于违反期望框架，设计了一系列测试，评估模型在虚拟环境中区分可能和不可能事件的能力。

Result: 现有模型在复杂场景中对直观物理的理解表现接近随机水平（50%），远低于人类表现。

Conclusion: 当前模型在直观物理理解上与人类存在显著差距，需改进模型架构和训练方法。

Abstract: We present IntPhys 2, a video benchmark designed to evaluate the intuitive
physics understanding of deep learning models. Building on the original IntPhys
benchmark, IntPhys 2 focuses on four core principles related to macroscopic
objects: Permanence, Immutability, Spatio-Temporal Continuity, and Solidity.
These conditions are inspired by research into intuitive physical understanding
emerging during early childhood. IntPhys 2 offers a comprehensive suite of
tests, based on the violation of expectation framework, that challenge models
to differentiate between possible and impossible events within controlled and
diverse virtual environments. Alongside the benchmark, we provide performance
evaluations of several state-of-the-art models. Our findings indicate that
while these models demonstrate basic visual understanding, they face
significant challenges in grasping intuitive physics across the four principles
in complex scenes, with most models performing at chance levels (50%), in stark
contrast to human performance, which achieves near-perfect accuracy. This
underscores the gap between current models and human-like intuitive physics
understanding, highlighting the need for advancements in model architectures
and training methodologies.

</details>


### [77] [Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation](https://arxiv.org/abs/2506.09881)
*Siyu Chen,Ting Han,Chengzheng Fu,Changshe Zhang,Chaolei Wang,Jinhe Su,Guorong Cai,Meiliu Wu*

Main category: cs.CV

TL;DR: Vireo是一个单阶段框架，首次将开放词汇语义分割（OVSS）和领域泛化语义分割（DGSS）结合，提出开放词汇领域泛化语义分割（OV-DGSS），通过几何和语言对齐提升模型在未知类别和未知领域的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: OVSS和DGSS的互补性启发了OV-DGSS的研究，旨在解决真实场景（如自动驾驶）中未知类别和领域的像素级分割问题。

Method: Vireo基于冻结的视觉基础模型（VFMs），引入深度VFMs提取领域不变特征，并提出GeoText Prompts、CMPE和DOV-VEH三个关键组件。

Result: Vireo在领域泛化和开放词汇识别上均取得显著性能提升，超越现有方法。

Conclusion: Vireo为动态多样化环境中的视觉理解提供了统一且可扩展的解决方案。

Abstract: Open-Vocabulary semantic segmentation (OVSS) and domain generalization in
semantic segmentation (DGSS) highlight a subtle complementarity that motivates
Open-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS
aims to generate pixel-level masks for unseen categories while maintaining
robustness across unseen domains, a critical capability for real-world
scenarios such as autonomous driving in adverse conditions. We introduce Vireo,
a novel single-stage framework for OV-DGSS that unifies the strengths of OVSS
and DGSS for the first time. Vireo builds upon the frozen Visual Foundation
Models (VFMs) and incorporates scene geometry via Depth VFMs to extract
domain-invariant structural features. To bridge the gap between visual and
textual modalities under domain shift, we propose three key components: (1)
GeoText Prompts, which align geometric features with language cues and
progressively refine VFM encoder representations; (2) Coarse Mask Prior
Embedding (CMPE) for enhancing gradient flow for faster convergence and
stronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding
Head (DOV-VEH), which fuses refined structural and semantic features for robust
prediction. Comprehensive evaluation on these components demonstrates the
effectiveness of our designs. Our proposed Vireo achieves the state-of-the-art
performance and surpasses existing methods by a large margin in both domain
generalization and open-vocabulary recognition, offering a unified and scalable
solution for robust visual understanding in diverse and dynamic environments.
Code is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.

</details>


### [78] [3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation](https://arxiv.org/abs/2506.09883)
*Seonho Lee,Jiho Choi,Inha Kang,Jiwook Kim,Junsung Park,Hyunjung Shim*

Main category: cs.CV

TL;DR: 提出了一种轻量级、无需标注的微调框架Geometric Distillation，通过注入几何线索提升视觉语言模型（VLM）对3D空间结构的理解。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在3D空间理解上存在局限，需要一种高效方法提升其几何感知能力。

Method: 通过从现成的3D基础模型（如MASt3R、VGGT）中提取稀疏对应、相对深度关系和密集成本体积，注入预训练VLM中。

Result: 在3D视觉语言推理和3D感知基准测试中表现优于现有方法，计算成本显著降低。

Conclusion: 为2D训练的VLM与3D理解之间提供了一条可扩展且高效的路径，适用于空间多模态任务。

Abstract: Vision-Language Models (VLMs) have shown remarkable performance on diverse
visual and linguistic tasks, yet they remain fundamentally limited in their
understanding of 3D spatial structures. We propose Geometric Distillation, a
lightweight, annotation-free fine-tuning framework that injects human-inspired
geometric cues into pretrained VLMs without modifying their architecture. By
distilling (1) sparse correspondences, (2) relative depth relations, and (3)
dense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,
VGGT), our method shapes representations to be geometry-aware while remaining
compatible with natural image-text inputs. Through extensive evaluations on 3D
vision-language reasoning and 3D perception benchmarks, our method consistently
outperforms prior approaches, achieving improved 3D spatial reasoning with
significantly lower computational cost. Our work demonstrates a scalable and
efficient path to bridge 2D-trained VLMs with 3D understanding, opening up
wider use in spatially grounded multimodal tasks.

</details>


### [79] [The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge](https://arxiv.org/abs/2506.09885)
*Haoru Wang,Kai Ye,Yangyan Li,Wenzheng Chen,Baoquan Chen*

Main category: cs.CV

TL;DR: 论文研究了可泛化的新视角合成（NVS）问题，提出了一种减少3D先验知识和姿态依赖的方法，通过数据驱动实现高性能。


<details>
  <summary>Details</summary>
Motivation: 解决从稀疏或无姿态2D图像生成逼真新视角的挑战，减少对3D先验知识和已知相机姿态的依赖。

Method: 提出了一种最小化3D归纳偏差和姿态依赖的框架，直接从稀疏2D图像学习隐式3D感知，无需训练时的3D先验或姿态标注。

Result: 实验表明，该方法能生成逼真且3D一致的新视角，性能与依赖姿态输入的方法相当。

Conclusion: 数据驱动范式在减少3D知识依赖方面是可行且有效的，尤其是在大规模数据时代。

Abstract: We consider the problem of generalizable novel view synthesis (NVS), which
aims to generate photorealistic novel views from sparse or even unposed 2D
images without per-scene optimization. This task remains fundamentally
challenging, as it requires inferring 3D structure from incomplete and
ambiguous 2D observations. Early approaches typically rely on strong 3D
knowledge, including architectural 3D inductive biases (e.g., embedding
explicit 3D representations, such as NeRF or 3DGS, into network design) and
ground-truth camera poses for both input and target views. While recent efforts
have sought to reduce the 3D inductive bias or the dependence on known camera
poses of input views, critical questions regarding the role of 3D knowledge and
the necessity of circumventing its use remain under-explored. In this work, we
conduct a systematic analysis on the 3D knowledge and uncover a critical trend:
the performance of methods that requires less 3D knowledge accelerates more as
data scales, eventually achieving performance on par with their 3D
knowledge-driven counterparts, which highlights the increasing importance of
reducing dependence on 3D knowledge in the era of large-scale data. Motivated
by and following this trend, we propose a novel NVS framework that minimizes 3D
inductive bias and pose dependence for both input and target views. By
eliminating this 3D knowledge, our method fully leverages data scaling and
learns implicit 3D awareness directly from sparse 2D images, without any 3D
inductive bias or pose annotation during training. Extensive experiments
demonstrate that our model generates photorealistic and 3D-consistent novel
views, achieving even comparable performance with methods that rely on posed
inputs, thereby validating the feasibility and effectiveness of our
data-centric paradigm. Project page:
https://pku-vcl-geometry.github.io/Less3Depend/ .

</details>


### [80] [EquiCaps: Predictor-Free Pose-Aware Pre-Trained Capsule Networks](https://arxiv.org/abs/2506.09895)
*Athinoulla Konstantinou,Georgios Leontidis,Mamatha Thota,Aiden Durrant*

Main category: cs.CV

TL;DR: 论文提出EquiCaps，一种基于胶囊网络的自我监督方法，无需专用预测器即可实现姿态感知表示，并在姿态估计任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用胶囊网络的固有特性（如姿态感知能力）来改进自我监督表示学习，避免依赖专用预测器。

Method: 引入EquiCaps，利用胶囊网络的姿态感知能力，结合多几何变换任务复杂度，通过3DIEBench-T数据集评估性能。

Result: EquiCaps在旋转预测任务中优于现有方法，R²达到0.78，并在复杂几何变换下保持稳健性能。

Conclusion: EquiCaps展示了胶囊网络在自我监督学习中的潜力，为无需预测器的姿态感知方法提供了新方向。

Abstract: Learning self-supervised representations that are invariant and equivariant
to transformations is crucial for advancing beyond traditional visual
classification tasks. However, many methods rely on predictor architectures to
encode equivariance, despite evidence that architectural choices, such as
capsule networks, inherently excel at learning interpretable pose-aware
representations. To explore this, we introduce EquiCaps (Equivariant Capsule
Network), a capsule-based approach to pose-aware self-supervision that
eliminates the need for a specialised predictor for enforcing equivariance.
Instead, we leverage the intrinsic pose-awareness capabilities of capsules to
improve performance in pose estimation tasks. To further challenge our
assumptions, we increase task complexity via multi-geometric transformations to
enable a more thorough evaluation of invariance and equivariance by introducing
3DIEBench-T, an extension of a 3D object-rendering benchmark dataset. Empirical
results demonstrate that EquiCaps outperforms prior state-of-the-art
equivariant methods on rotation prediction, achieving a supervised-level $R^2$
of 0.78 on the 3DIEBench rotation prediction benchmark and improving upon SIE
and CapsIE by 0.05 and 0.04 $R^2$, respectively. Moreover, in contrast to
non-capsule-based equivariant approaches, EquiCaps maintains robust equivariant
performance under combined geometric transformations, underscoring its
generalisation capabilities and the promise of predictor-free capsule
architectures.

</details>


### [81] [CEM-FBGTinyDet: Context-Enhanced Foreground Balance with Gradient Tuning for tiny Objects](https://arxiv.org/abs/2506.09897)
*Tao Liu,Zhenchao Cui*

Main category: cs.CV

TL;DR: 论文提出E-FPN-BS架构，通过多尺度特征增强和自适应优化解决小目标检测中高、低层级特征训练不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统特征金字塔网络（FPN）在高层级特征（P5-P6）中常因零正样本锚点导致语义表示未训练，造成高、低层级特征的双重缺陷。

Method: 提出E-FPN-BS架构，包含上下文增强模块（CEM）和前景-背景分离模块（FBSM），并引入动态梯度平衡损失（DCLoss）。

Result: 在多个基准数据集上的实验表明，该方法具有出色的性能和泛化能力。

Conclusion: E-FPN-BS有效解决了小目标检测中的特征训练问题，提升了检测性能。

Abstract: Tiny object detection (TOD) reveals a fundamental flaw in feature pyramid
networks: high-level features (P5-P6) frequently receive zero positive anchors
under standard label assignment protocols, leaving their semantic
representations untrained due to exclusion from loss computation. This creates
dual deficiencies: (1) Stranded high-level features become semantic dead-ends
without gradient updates, while (2) low-level features lack essential semantic
context for robust classification. We propose E-FPN-BS that systematically
converts wasted high-level semantics into low-level feature enhancements. To
address these issues, we propose E-FPN-BS, a novel architecture integrating
multi-scale feature enhancement and adaptive optimization. First, our Context
Enhancement Module(CEM) employs dual-branch processing to align and compress
high-level features for effective global-local fusion. Second, the
Foreground-Background Separation Module (FBSM) generates spatial gating masks
that dynamically amplify discriminative regions. To address gradient imbalance
across object scales, we further propose a Dynamic Gradient-Balanced Loss
(DCLoss) that automatically modulates loss contributions via scale-aware
gradient equilibrium. Extensive experiments across multiple benchmark datasets
demonstrate the outstanding performance and generalization ability of our
approach.

</details>


### [82] [Only-Style: Stylistic Consistency in Image Generation without Content Leakage](https://arxiv.org/abs/2506.09916)
*Tilemachos Aravanis,Panagiotis Filntisis,Petros Maragos,George Retsinas*

Main category: cs.CV

TL;DR: 论文提出了一种名为Only-Style的新方法，旨在解决风格一致图像生成中的内容泄漏问题，通过自适应调整参数和局部化内容泄漏，实现了更好的风格一致性和内容分离。


<details>
  <summary>Details</summary>
Motivation: 现有的风格一致生成方法难以有效分离语义内容和风格元素，导致内容泄漏问题。

Method: Only-Style通过局部化内容泄漏并自适应调整风格对齐参数，在保持风格一致的同时消除内容泄漏。

Result: 实验表明，该方法在多样实例中显著优于现有方法，实现了无内容泄漏的稳健风格一致性。

Conclusion: Only-Style通过自适应参数调整和局部化内容泄漏，有效解决了风格一致生成中的内容泄漏问题。

Abstract: Generating images in a consistent reference visual style remains a
challenging computer vision task. State-of-the-art methods aiming for
style-consistent generation struggle to effectively separate semantic content
from stylistic elements, leading to content leakage from the image provided as
a reference to the targets. To address this challenge, we propose Only-Style: a
method designed to mitigate content leakage in a semantically coherent manner
while preserving stylistic consistency. Only-Style works by localizing content
leakage during inference, allowing the adaptive tuning of a parameter that
controls the style alignment process, specifically within the image patches
containing the subject in the reference image. This adaptive process best
balances stylistic consistency with leakage elimination. Moreover, the
localization of content leakage can function as a standalone component, given a
reference-target image pair, allowing the adaptive tuning of any
method-specific parameter that provides control over the impact of the
stylistic reference. In addition, we propose a novel evaluation framework to
quantify the success of style-consistent generations in avoiding undesired
content leakage. Our approach demonstrates a significant improvement over
state-of-the-art methods through extensive evaluation across diverse instances,
consistently achieving robust stylistic consistency without undesired content
leakage.

</details>


### [83] [MetricHMR: Metric Human Mesh Recovery from Monocular Images](https://arxiv.org/abs/2506.09919)
*He Zhang,Chentao Song,Hongwen Zhang,Tao Yu*

Main category: cs.CV

TL;DR: MetricHMR是一种从单目图像中恢复具有精确全局平移的度量人体网格的方法，解决了现有HMR方法在尺度和深度上的模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 现有HMR方法存在严重的尺度和深度模糊性，无法生成几何合理的身体形状和全局平移。

Method: 通过系统分析现有HMR方法的相机模型，强调标准透视投影模型的关键作用，并提出一种基于标准透视投影的射线图方法，联合编码边界框信息、相机参数和几何线索。

Result: 实验表明，该方法在度量姿态、形状和全局平移估计上达到最先进性能，优于现有HMR方法。

Conclusion: MetricHMR通过标准透视投影模型和射线图方法，成功实现了度量尺度的HMR，解决了尺度和深度模糊性问题。

Abstract: We introduce MetricHMR (Metric Human Mesh Recovery), an approach for metric
human mesh recovery with accurate global translation from monocular images. In
contrast to existing HMR methods that suffer from severe scale and depth
ambiguity, MetricHMR is able to produce geometrically reasonable body shape and
global translation in the reconstruction results. To this end, we first
systematically analyze previous HMR methods on camera models to emphasize the
critical role of the standard perspective projection model in enabling
metric-scale HMR. We then validate the acceptable ambiguity range of metric HMR
under the standard perspective projection model. Finally, we contribute a novel
approach that introduces a ray map based on the standard perspective projection
to jointly encode bounding-box information, camera parameters, and geometric
cues for End2End metric HMR without any additional metric-regularization
modules. Extensive experiments demonstrate that our method achieves
state-of-the-art performance, even compared with sequential HMR methods, in
metric pose, shape, and global translation estimation across both indoor and
in-the-wild scenarios.

</details>


### [84] [Structural-Spectral Graph Convolution with Evidential Edge Learning for Hyperspectral Image Clustering](https://arxiv.org/abs/2506.09920)
*Jianhan Qi,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.CV

TL;DR: 论文提出了一种针对高光谱图像（HSI）聚类的结构-光谱图卷积算子（SSGCO）和证据引导的自适应边学习（EGAEL）模块，通过联合提取空间和光谱特征提升表示质量，并在对比学习框架中实现聚类。


<details>
  <summary>Details</summary>
Motivation: 现有图神经网络（GNNs）未能充分利用HSI的光谱信息，且超像素拓扑图的不准确性可能导致信息聚合时不同类别语义混淆。

Method: 提出SSGCO和EGAEL模块，分别优化超像素的表示质量和边权重预测，并将其整合到对比学习框架中进行聚类。

Result: 在四个HSI数据集上，聚类准确率分别提升了2.61%、6.06%、4.96%和3.15%。

Conclusion: 所提方法显著提升了HSI聚类的准确性和鲁棒性。

Abstract: Hyperspectral image (HSI) clustering assigns similar pixels to the same class
without any annotations, which is an important yet challenging task. For
large-scale HSIs, most methods rely on superpixel segmentation and perform
superpixel-level clustering based on graph neural networks (GNNs). However,
existing GNNs cannot fully exploit the spectral information of the input HSI,
and the inaccurate superpixel topological graph may lead to the confusion of
different class semantics during information aggregation. To address these
challenges, we first propose a structural-spectral graph convolutional operator
(SSGCO) tailored for graph-structured HSI superpixels to improve their
representation quality through the co-extraction of spatial and spectral
features. Second, we propose an evidence-guided adaptive edge learning (EGAEL)
module that adaptively predicts and refines edge weights in the superpixel
topological graph. We integrate the proposed method into a contrastive learning
framework to achieve clustering, where representation learning and clustering
are simultaneously conducted. Experiments demonstrate that the proposed method
improves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best
compared methods on four HSI datasets. Our code is available at
https://github.com/jhqi/SSGCO-EGAEL.

</details>


### [85] [HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations](https://arxiv.org/abs/2506.09932)
*Marco Federici,Riccardo Del Chiaro,Boris van Breugel,Paul Whatmough,Markus Nagel*

Main category: cs.CV

TL;DR: HadaNorm是一种新的线性变换方法，通过归一化和Hadamard变换有效减少异常值，提升量化效果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中表现优异，但其高内存和计算需求限制了在资源受限设备上的部署。现有量化方法难以处理异常值，且需要额外转换。

Method: 提出HadaNorm，通过归一化激活特征通道并应用Hadamard变换，实现更激进的激活量化。

Result: HadaNorm在Transformer块各组件中持续减少量化误差，效率-性能权衡优于现有方法。

Conclusion: HadaNorm为资源受限设备上的扩散模型部署提供了高效解决方案。

Abstract: Diffusion models represent the cutting edge in image generation, but their
high memory and computational demands hinder deployment on resource-constrained
devices. Post-Training Quantization (PTQ) offers a promising solution by
reducing the bitwidth of matrix operations. However, standard PTQ methods
struggle with outliers, and achieving higher compression often requires
transforming model weights and activations before quantization. In this work,
we propose HadaNorm, a novel linear transformation that extends existing
approaches and effectively mitigates outliers by normalizing activations
feature channels before applying Hadamard transformations, enabling more
aggressive activation quantization. We demonstrate that HadaNorm consistently
reduces quantization error across the various components of transformer blocks,
achieving superior efficiency-performance trade-offs when compared to
state-of-the-art methods.

</details>


### [86] [LEO-VL: Towards 3D Vision-Language Generalists via Data Scaling with Efficient Representation](https://arxiv.org/abs/2506.09935)
*Jiangyong Huang,Xiaojian Ma,Xiongkun Linghu,Yue Fan,Junchao He,Wenxin Tan,Qing Li,Song-Chun Zhu,Yixin Chen,Baoxiong Jia,Siyuan Huang*

Main category: cs.CV

TL;DR: LEO-VL提出了一种基于CFG的高效3D场景表示方法，显著提升了3D-VL模型的性能，并在多任务和场景中实现了SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 解决3D-VL模型在能力和鲁棒性上落后于2D模型的问题，尤其是数据可扩展性和高效场景表示的不足。

Method: 提出CFG（Condensed Feature Grid）作为高效场景表示，并结合大规模3D-VL数据训练模型LEO-VL。

Result: 在多个3D QA基准测试中（如SQA3D、MSQA、Beacon3D）取得SOTA性能，并通过消融实验验证了方法的有效性。

Conclusion: LEO-VL和SceneDPO为3D-VL通用模型的开发和鲁棒性提升提供了重要贡献。

Abstract: Developing 3D-VL generalists capable of understanding 3D scenes and following
natural language instructions to perform a wide range of tasks has been a
long-standing goal in the 3D-VL community. Despite recent progress, 3D-VL
models still lag behind their 2D counterparts in capability and robustness,
falling short of the generalist standard. A key obstacle to developing 3D-VL
generalists lies in data scalability, hindered by the lack of an efficient
scene representation. We propose LEO-VL, a 3D-VL model built upon condensed
feature grid (CFG), an efficient scene representation that bridges 2D
perception and 3D spatial structure while significantly reducing token
overhead. This efficiency unlocks large-scale training towards 3D-VL
generalist, for which we curate over 700k high-quality 3D-VL data spanning four
domains of real-world indoor scenes and five tasks such as captioning and
dialogue. LEO-VL achieves state-of-the-art performance on a variety of 3D QA
benchmarks, including SQA3D, MSQA, and Beacon3D. Ablation studies confirm the
efficiency of our representation, the importance of task and scene diversity,
and the validity of our data curation principle. Furthermore, we introduce
SceneDPO, a novel post-training objective that enhances the robustness of 3D-VL
models. We hope our findings contribute to the advancement of scalable and
robust 3D-VL generalists.

</details>


### [87] [CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models](https://arxiv.org/abs/2506.09943)
*Aaron Foss,Chloe Evans,Sasha Mitts,Koustuv Sinha,Ammar Rizvi,Justine T. Kao*

Main category: cs.CV

TL;DR: CausalVQA是一个用于视频问答（VQA）的基准数据集，专注于模型对物理世界中因果关系的理解，填补了现有数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 现有VQA数据集多关注表面感知或狭窄的物理推理问题，缺乏对真实场景中因果关系的深入探究。

Method: CausalVQA包含五种问题类型（反事实、假设、预期、规划和描述性），并通过质量控制机制防止模型利用简单捷径。

Result: 当前前沿多模态模型在CausalVQA上的表现显著低于人类，尤其在预期和假设问题上。

Conclusion: CausalVQA揭示了当前系统在时空推理、物理原理理解和预测能力上的不足，为未来研究提供了挑战。

Abstract: We introduce CausalVQA, a benchmark dataset for video question answering
(VQA) composed of question-answer pairs that probe models' understanding of
causality in the physical world. Existing VQA benchmarks either tend to focus
on surface perceptual understanding of real-world videos, or on narrow physical
reasoning questions created using simulation environments. CausalVQA fills an
important gap by presenting challenging questions that are grounded in
real-world scenarios, while focusing on models' ability to predict the likely
outcomes of different actions and events through five question types:
counterfactual, hypothetical, anticipation, planning and descriptive. We
designed quality control mechanisms that prevent models from exploiting trivial
shortcuts, requiring models to base their answers on deep visual understanding
instead of linguistic cues. We find that current frontier multimodal models
fall substantially below human performance on the benchmark, especially on
anticipation and hypothetical questions. This highlights a challenge for
current systems to leverage spatial-temporal reasoning, understanding of
physical principles, and comprehension of possible alternatives to make
accurate predictions in real-world settings.

</details>


### [88] [UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting](https://arxiv.org/abs/2506.09952)
*Ziyi Wang,Yanran Zhang,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: UniPre3D是一种统一预训练方法，适用于任意尺度的点云和任意架构的3D模型，通过预测高斯基元和使用可微分高斯渲染实现像素级监督。


<details>
  <summary>Details</summary>
Motivation: 解决点云数据尺度多样性带来的统一表示学习难题，填补现有预训练方法在对象和场景级点云上的空白。

Method: 预测高斯基元作为预训练任务，采用可微分高斯渲染生成图像，结合预训练2D特征以引入纹理知识。

Result: 在多种对象和场景级任务中验证了方法的通用有效性。

Conclusion: UniPre3D是首个适用于任意尺度和架构的统一预训练方法，实验证明其广泛适用性。

Abstract: The scale diversity of point cloud data presents significant challenges in
developing unified representation learning techniques for 3D vision. Currently,
there are few unified 3D models, and no existing pre-training method is equally
effective for both object- and scene-level point clouds. In this paper, we
introduce UniPre3D, the first unified pre-training method that can be
seamlessly applied to point clouds of any scale and 3D models of any
architecture. Our approach predicts Gaussian primitives as the pre-training
task and employs differentiable Gaussian splatting to render images, enabling
precise pixel-level supervision and end-to-end optimization. To further
regulate the complexity of the pre-training task and direct the model's focus
toward geometric structures, we integrate 2D features from pre-trained image
models to incorporate well-established texture knowledge. We validate the
universal effectiveness of our proposed method through extensive experiments
across a variety of object- and scene-level tasks, using diverse point cloud
models as backbones. Code is available at https://github.com/wangzy22/UniPre3D.

</details>


### [89] [Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos](https://arxiv.org/abs/2506.09953)
*Benjamin Reichman,Constantin Patsch,Jack Truxal,Atishay Jain,Larry Heck*

Main category: cs.CV

TL;DR: 论文介绍了基于视频的视觉问答任务（OK-VQA）扩展至对话场景，提出新数据集并探讨模型需结合视觉信息和外部知识的挑战。


<details>
  <summary>Details</summary>
Motivation: 探索在视频对话中结合视觉信息和外部知识以回答问题的任务，填补现有研究的空白。

Method: 构建包含2,017个视频和5,986个对话的数据集，标注40,954个对话轮次，并设计基线模型。

Result: 数据集公开可用，基线模型展示了任务中结合视觉和外部知识的挑战。

Conclusion: 该任务为未来研究提供了新方向，数据集和基线模型为相关研究奠定基础。

Abstract: In outside knowledge visual question answering (OK-VQA), the model must
identify relevant visual information within an image and incorporate external
knowledge to accurately respond to a question. Extending this task to a
visually grounded dialogue setting based on videos, a conversational model must
both recognize pertinent visual details over time and answer questions where
the required information is not necessarily present in the visual information.
Moreover, the context of the overall conversation must be considered for the
subsequent dialogue. To explore this task, we introduce a dataset comprised of
$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$
interleaved dialogue turns. While the dialogue context is visually grounded in
specific video segments, the questions further require external knowledge that
is not visually present. Thus, the model not only has to identify relevant
video parts but also leverage external knowledge to converse within the
dialogue. We further provide several baselines evaluated on our dataset and
show future challenges associated with this task. The dataset is made publicly
available here: https://github.com/c-patsch/OKCV.

</details>


### [90] [Vision Generalist Model: A Survey](https://arxiv.org/abs/2506.09954)
*Ziyi Wang,Yongming Rao,Shuofeng Sun,Xinrun Liu,Yi Wei,Xumin Yu,Zuyan Liu,Yanbo Wang,Hongmin Liu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文综述了视觉通用模型的特点和能力，包括背景、框架设计、技术提升、相关领域联系及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 通用模型在自然语言处理中的成功激发了将其应用于计算机视觉任务的兴趣，但视觉任务的输入输出多样性带来了挑战。

Method: 回顾背景、分析现有框架设计、介绍性能提升技术、探讨相关领域联系。

Result: 提供了视觉通用模型的全面概述，并指出了实际应用场景和挑战。

Conclusion: 总结了视觉通用模型的现状，提出了未来研究方向和应用潜力。

Abstract: Recently, we have witnessed the great success of the generalist model in
natural language processing. The generalist model is a general framework
trained with massive data and is able to process various downstream tasks
simultaneously. Encouraged by their impressive performance, an increasing
number of researchers are venturing into the realm of applying these models to
computer vision tasks. However, the inputs and outputs of vision tasks are more
diverse, and it is difficult to summarize them as a unified representation. In
this paper, we provide a comprehensive overview of the vision generalist
models, delving into their characteristics and capabilities within the field.
First, we review the background, including the datasets, tasks, and benchmarks.
Then, we dig into the design of frameworks that have been proposed in existing
research, while also introducing the techniques employed to enhance their
performance. To better help the researchers comprehend the area, we take a
brief excursion into related domains, shedding light on their interconnections
and potential synergies. To conclude, we provide some real-world application
scenarios, undertake a thorough examination of the persistent challenges, and
offer insights into possible directions for future research endeavors.

</details>


### [91] [Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy](https://arxiv.org/abs/2506.09958)
*Sushant Gautam,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: Kvasir-VQA-x1是一个新的大规模胃肠道内窥镜数据集，扩展了原始数据集，增加了159,549个问题-答案对，旨在测试更深入的临床推理，并通过视觉增强模拟真实临床场景。


<details>
  <summary>Details</summary>
Motivation: 解决现有MedVQA数据集缺乏临床复杂性和视觉多样性的问题，以促进更可靠的临床决策支持系统的开发。

Method: 使用大型语言模型生成分层复杂性的问题，并引入视觉增强模拟常见成像伪影，支持两种评估轨道。

Result: Kvasir-VQA-x1提供了一个更具挑战性和临床相关性的基准，旨在加速多模态AI系统在临床环境中的应用。

Conclusion: Kvasir-VQA-x1是一个符合FAIR原则的开放数据集，为研究社区提供了有价值的资源。

Abstract: Medical Visual Question Answering (MedVQA) is a promising field for
developing clinical decision support systems, yet progress is often limited by
the available datasets, which can lack clinical complexity and visual
diversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,
large-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly
expands upon the original Kvasir-VQA by incorporating 159,549 new
question-answer pairs that are designed to test deeper clinical reasoning. We
developed a systematic method using large language models to generate these
questions, which are stratified by complexity to better assess a model's
inference capabilities. To ensure our dataset prepares models for real-world
clinical scenarios, we have also introduced a variety of visual augmentations
that mimic common imaging artifacts. The dataset is structured to support two
main evaluation tracks: one for standard VQA performance and another to test
model robustness against these visual perturbations. By providing a more
challenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate
the development of more reliable and effective multimodal AI systems for use in
clinical settings. The dataset is fully accessible and adheres to FAIR data
principles, making it a valuable resource for the wider research community.
Code and data: https://github.com/Simula/Kvasir-VQA-x1 and
https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1

</details>


### [92] [Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing](https://arxiv.org/abs/2506.09965)
*Junfei Wu,Jian Guan,Kaituo Feng,Qiang Liu,Shu Wu,Liang Wang,Wei Wu,Tieniu Tan*

Main category: cs.CV

TL;DR: 论文提出了一种新的多模态推理范式，通过视觉空间中的绘图操作增强大型视觉语言模型（LVLMs）的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态推理中主要依赖纯文本方式，导致在需要精确几何理解和空间跟踪的任务中存在局限性。

Method: 提出了一种基于绘图操作的视觉推理方法，包括标注边界框和绘制辅助线，并通过三阶段训练框架（冷启动训练、反射拒绝采样和强化学习）提升模型能力。

Result: 实验表明，模型VILASR在多种空间推理任务中平均性能提升18.4%。

Conclusion: 通过视觉绘图操作，显著提升了LVLMs的空间推理能力，克服了纯文本方法的局限性。

Abstract: As textual reasoning with large language models (LLMs) has advanced
significantly, there has been growing interest in enhancing the multimodal
reasoning capabilities of large vision-language models (LVLMs). However,
existing methods primarily approach multimodal reasoning in a straightforward,
text-centric manner, where both reasoning and answer derivation are conducted
purely through text, with the only difference being the presence of multimodal
input. As a result, these methods often encounter fundamental limitations in
spatial reasoning tasks that demand precise geometric understanding and
continuous spatial tracking-capabilities that humans achieve through mental
visualization and manipulation. To address the limitations, we propose drawing
to reason in space, a novel paradigm that enables LVLMs to reason through
elementary drawing operations in the visual space. By equipping models with
basic drawing operations, including annotating bounding boxes and drawing
auxiliary lines, we empower them to express and analyze spatial relationships
through direct visual manipulation, meanwhile avoiding the performance ceiling
imposed by specialized perception tools in previous tool-integrated reasoning
approaches. To cultivate this capability, we develop a three-stage training
framework: cold-start training with synthetic data to establish basic drawing
abilities, reflective rejection sampling to enhance self-reflection behaviors,
and reinforcement learning to directly optimize for target rewards. Extensive
experiments demonstrate that our model, named VILASR, consistently outperforms
existing methods across diverse spatial reasoning benchmarks, involving maze
navigation, static spatial reasoning, video-based reasoning, and
multi-view-based reasoning tasks, with an average improvement of 18.4%.

</details>


### [93] [Vectorized Region Based Brush Strokes for Artistic Rendering](https://arxiv.org/abs/2506.09969)
*Jeripothula Prudviraj,Vikram Jamwal*

Main category: cs.CV

TL;DR: 提出了一种基于语义引导的笔触生成方法，通过区域策略和笔触序列化提升绘画质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有笔触生成方法难以符合艺术原则和意图的问题。

Method: 结合语义引导、笔触参数计算和序列化渲染，优化区域策略。

Result: 在多种输入图像上验证了高保真和优质笔触的生成效果。

Conclusion: 该方法能更好地符合艺术创作意图，提升绘画质量。

Abstract: Creating a stroke-by-stroke evolution process of a visual artwork tries to
bridge the emotional and educational gap between the finished static artwork
and its creation process. Recent stroke-based painting systems focus on
capturing stroke details by predicting and iteratively refining stroke
parameters to maximize the similarity between the input image and the rendered
output. However, these methods often struggle to produce stroke compositions
that align with artistic principles and intent. To address this, we explore an
image-to-painting method that (i) facilitates semantic guidance for brush
strokes in targeted regions, (ii) computes the brush stroke parameters, and
(iii) establishes a sequence among segments and strokes to sequentially render
the final painting. Experimental results on various input image types, such as
face images, paintings, and photographic images, show that our method aligns
with a region-based painting strategy while rendering a painting with high
fidelity and superior stroke quality.

</details>


### [94] [Efficient Part-level 3D Object Generation via Dual Volume Packing](https://arxiv.org/abs/2506.09980)
*Jiaxiang Tang,Ruijie Lu,Zhaoshuo Li,Zekun Hao,Xuan Li,Fangyin Wei,Shuran Song,Gang Zeng,Ming-Yu Liu,Tsung-Yi Lin*

Main category: cs.CV

TL;DR: 提出了一种新的端到端框架，用于生成具有任意数量语义有意义部分的3D对象，解决了现有方法生成单一网格的限制。


<details>
  <summary>Details</summary>
Motivation: 现有3D对象生成方法通常生成单一网格，限制了部分编辑能力，且不同对象的部分数量可能不同。

Method: 采用双体积包装策略，将所有部分组织到两个互补的体积中，生成完整且交错的部件。

Result: 实验表明，该方法在质量、多样性和泛化能力上优于之前的基于图像的部分级生成方法。

Conclusion: 该方法成功实现了高质量、可编辑的部分级3D对象生成。

Abstract: Recent progress in 3D object generation has greatly improved both the quality
and efficiency. However, most existing methods generate a single mesh with all
parts fused together, which limits the ability to edit or manipulate individual
parts. A key challenge is that different objects may have a varying number of
parts. To address this, we propose a new end-to-end framework for part-level 3D
object generation. Given a single input image, our method generates
high-quality 3D objects with an arbitrary number of complete and semantically
meaningful parts. We introduce a dual volume packing strategy that organizes
all parts into two complementary volumes, allowing for the creation of complete
and interleaved parts that assemble into the final object. Experiments show
that our model achieves better quality, diversity, and generalization than
previous image-based part-level generation methods.

</details>


### [95] [ReSim: Reliable World Simulation for Autonomous Driving](https://arxiv.org/abs/2506.09981)
*Jiazhi Yang,Kashyap Chitta,Shenyuan Gao,Long Chen,Yuqian Shao,Xiaosong Jia,Hongyang Li,Andreas Geiger,Xiangyu Yue,Li Chen*

Main category: cs.CV

TL;DR: 论文提出了一种通过结合真实世界和模拟驾驶数据构建可控世界模型的方法，以解决现有模型难以模拟危险或非专家驾驶行为的问题。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶世界模型主要基于真实世界的安全专家轨迹数据，难以模拟罕见但重要的危险或非专家行为，限制了其应用范围。

Method: 通过整合真实世界和模拟驾驶数据（如CARLA），构建了一个基于扩散变换器架构的可控世界模型ReSim，并设计了多种策略以提高预测可控性和保真度。

Result: ReSim在视觉保真度上提高了44%，对专家和非专家行为的可控性提升了50%以上，并在NAVSIM上的规划和策略选择性能分别提升了2%和25%。

Conclusion: ReSim通过结合异构数据和高保真模拟，显著提升了驾驶场景模拟的可靠性和多样性，为政策评估等任务提供了更强大的工具。

Abstract: How can we reliably simulate future driving scenarios under a wide range of
ego driving behaviors? Recent driving world models, developed exclusively on
real-world driving data composed mainly of safe expert trajectories, struggle
to follow hazardous or non-expert behaviors, which are rare in such data. This
limitation restricts their applicability to tasks such as policy evaluation. In
this work, we address this challenge by enriching real-world human
demonstrations with diverse non-expert data collected from a driving simulator
(e.g., CARLA), and building a controllable world model trained on this
heterogeneous corpus. Starting with a video generator featuring a diffusion
transformer architecture, we devise several strategies to effectively integrate
conditioning signals and improve prediction controllability and fidelity. The
resulting model, ReSim, enables Reliable Simulation of diverse open-world
driving scenarios under various actions, including hazardous non-expert ones.
To close the gap between high-fidelity simulation and applications that require
reward signals to judge different actions, we introduce a Video2Reward module
that estimates a reward from ReSim's simulated future. Our ReSim paradigm
achieves up to 44% higher visual fidelity, improves controllability for both
expert and non-expert actions by over 50%, and boosts planning and policy
selection performance on NAVSIM by 2% and 25%, respectively.

</details>


### [96] [AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation](https://arxiv.org/abs/2506.09982)
*Zijie Wu,Chaohui Yu,Fan Wang,Xiang Bai*

Main category: cs.CV

TL;DR: AnimateAnyMesh是首个基于文本驱动的高效动画生成框架，通过DyMeshVAE架构和Rectified Flow训练策略，实现了高质量的3D网格动画生成，并发布了包含4M动态网格序列的数据集。


<details>
  <summary>Details</summary>
Motivation: 4D内容生成面临时空分布建模复杂和训练数据稀缺的挑战，AnimateAnyMesh旨在解决这些问题。

Method: 采用DyMeshVAE架构分离时空特征，结合Rectified Flow训练策略，在压缩潜在空间实现文本条件生成。

Result: 实验表明，该方法能快速生成语义准确且时间连贯的动画，质量和效率显著优于现有方法。

Conclusion: AnimateAnyMesh推动了4D内容生成的实用化，相关数据和模型将开源。

Abstract: Recent advances in 4D content generation have attracted increasing attention,
yet creating high-quality animated 3D models remains challenging due to the
complexity of modeling spatio-temporal distributions and the scarcity of 4D
training data. In this paper, we present AnimateAnyMesh, the first feed-forward
framework that enables efficient text-driven animation of arbitrary 3D meshes.
Our approach leverages a novel DyMeshVAE architecture that effectively
compresses and reconstructs dynamic mesh sequences by disentangling spatial and
temporal features while preserving local topological structures. To enable
high-quality text-conditional generation, we employ a Rectified Flow-based
training strategy in the compressed latent space. Additionally, we contribute
the DyMesh Dataset, containing over 4M diverse dynamic mesh sequences with text
annotations. Experimental results demonstrate that our method generates
semantically accurate and temporally coherent mesh animations in a few seconds,
significantly outperforming existing approaches in both quality and efficiency.
Our work marks a substantial step forward in making 4D content creation more
accessible and practical. All the data, code, and models will be open-released.

</details>


### [97] [InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions](https://arxiv.org/abs/2506.09984)
*Zhenzhi Wang,Jiaqi Yang,Jianwen Jiang,Chao Liang,Gaojie Lin,Zerong Zheng,Ceyuan Yang,Dahua Lin*

Main category: cs.CV

TL;DR: 论文提出了一种新框架，通过区域特定的条件绑定实现多概念（如人和物体）的精确控制，解决了现有方法在全局条件下无法处理多概念交互的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常只能对单一主体进行动画生成，并以全局方式注入条件，无法处理多概念交互场景。

Method: 提出了一种新框架，通过掩码预测器自动推断布局信息，并将局部音频条件对齐到相应区域，实现多概念视频生成。

Result: 实验证明，该方法在多模态条件下的布局控制效果优于隐式方法和其他现有方法。

Conclusion: 该框架能够高质量生成可控的多概念人中心视频，为多模态条件注入提供了新思路。

Abstract: End-to-end human animation with rich multi-modal conditions, e.g., text,
image and audio has achieved remarkable advancements in recent years. However,
most existing methods could only animate a single subject and inject conditions
in a global manner, ignoring scenarios that multiple concepts could appears in
the same video with rich human-human interactions and human-object
interactions. Such global assumption prevents precise and per-identity control
of multiple concepts including humans and objects, therefore hinders
applications. In this work, we discard the single-entity assumption and
introduce a novel framework that enforces strong, region-specific binding of
conditions from modalities to each identity's spatiotemporal footprint. Given
reference images of multiple concepts, our method could automatically infer
layout information by leveraging a mask predictor to match appearance cues
between the denoised video and each reference appearance. Furthermore, we
inject local audio condition into its corresponding region to ensure
layout-aligned modality matching in a iterative manner. This design enables the
high-quality generation of controllable multi-concept human-centric videos.
Empirical results and ablation studies validate the effectiveness of our
explicit layout control for multi-modal conditions compared to implicit
counterparts and other existing methods.

</details>


### [98] [A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs](https://arxiv.org/abs/2506.09987)
*Benno Krojer,Mojtaba Komeili,Candace Ross,Quentin Garrido,Koustuv Sinha,Nicolas Ballas,Mahmoud Assran*

Main category: cs.CV

TL;DR: 论文提出了MVP基准测试，用于评估视频语言模型的物理理解能力，通过最小变化对避免模型依赖表面视觉或文本线索。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试因依赖表面线索导致评分虚高，无法准确评估模型的时空理解和推理能力。

Method: 引入MVP基准测试，包含55K高质量多选题视频QA样本，每个样本有视觉相似但答案相反的最小变化对。

Result: 人类表现92.9%，最佳开源模型40.2%，随机表现25%。

Conclusion: MVP基准能有效避免模型依赖捷径，更准确评估物理理解能力。

Abstract: Existing benchmarks for assessing the spatio-temporal understanding and
reasoning abilities of video language models are susceptible to score inflation
due to the presence of shortcut solutions based on superficial visual or
textual cues. This paper mitigates the challenges in accurately assessing model
performance by introducing the Minimal Video Pairs (MVP) benchmark, a simple
shortcut-aware video QA benchmark for assessing the physical understanding of
video language models. The benchmark is comprised of 55K high-quality
multiple-choice video QA examples focusing on physical world understanding.
Examples are curated from nine video data sources, spanning first-person
egocentric and exocentric videos, robotic interaction data, and cognitive
science intuitive physics benchmarks. To mitigate shortcut solutions that rely
on superficial visual or textual cues and biases, each sample in MVP has a
minimal-change pair -- a visually similar video accompanied by an identical
question but an opposing answer. To answer a question correctly, a model must
provide correct answers for both examples in the minimal-change pair; as such,
models that solely rely on visual or textual biases would achieve below random
performance. Human performance on MVP is 92.9\%, while the best open-source
state-of-the-art video-language model achieves 40.2\% compared to random
performance at 25\%.

</details>


### [99] [EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits](https://arxiv.org/abs/2506.09988)
*Ron Yosef,Moran Yanuka,Yonatan Bitton,Dani Lischinski*

Main category: cs.CV

TL;DR: EditInspector是一个基于人工标注的新基准，用于评估文本引导的图像编辑质量，发现当前模型在全面评估编辑效果方面存在不足，并提出两种新方法以改进。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的发展，文本引导的图像编辑日益普及，但缺乏一个全面的框架来验证和评估编辑质量。

Method: 引入EditInspector基准，利用人工标注和模板验证编辑效果，并评估现有模型在多个维度上的表现。

Result: 当前模型在全面评估编辑效果时表现不佳，常出现幻觉描述。提出的新方法在伪影检测和差异描述生成上优于现有模型。

Conclusion: EditInspector为文本引导编辑提供了有效的评估工具，新方法在关键任务上表现更优。

Abstract: Text-guided image editing, fueled by recent advancements in generative AI, is
becoming increasingly widespread. This trend highlights the need for a
comprehensive framework to verify text-guided edits and assess their quality.
To address this need, we introduce EditInspector, a novel benchmark for
evaluation of text-guided image edits, based on human annotations collected
using an extensive template for edit verification. We leverage EditInspector to
evaluate the performance of state-of-the-art (SoTA) vision and language models
in assessing edits across various dimensions, including accuracy, artifact
detection, visual quality, seamless integration with the image scene, adherence
to common sense, and the ability to describe edit-induced changes. Our findings
indicate that current models struggle to evaluate edits comprehensively and
frequently hallucinate when describing the changes. To address these
challenges, we propose two novel methods that outperform SoTA models in both
artifact detection and difference caption generation.

</details>


### [100] [Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes](https://arxiv.org/abs/2506.09989)
*Yiming Dou,Wonseok Oh,Yuqing Luo,Antonio Loquercio,Andrew Owens*

Main category: cs.CV

TL;DR: 研究如何通过预测人手与3D场景交互的声音，使3D场景重建具有交互性。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过声音增强3D场景的交互性，模拟真实世界中人与物体的物理互动。

Method: 记录人手操作物体的视频和声音对，训练一个修正流模型，将3D手部轨迹映射到对应的音频。测试时，用户可以通过手部姿势序列查询模型生成声音。

Result: 生成的音频能准确传达材料特性和动作，人类观察者难以区分其与真实声音的差异。

Conclusion: 该方法成功实现了通过声音增强3D场景交互性，为虚拟现实和增强现实应用提供了新思路。

Abstract: We study the problem of making 3D scene reconstructions interactive by asking
the following question: can we predict the sounds of human hands physically
interacting with a scene? First, we record a video of a human manipulating
objects within a 3D scene using their hands. We then use these action-sound
pairs to train a rectified flow model to map 3D hand trajectories to their
corresponding audio. At test time, a user can query the model for other
actions, parameterized as sequences of hand poses, to estimate their
corresponding sounds. In our experiments, we find that our generated sounds
accurately convey material properties and actions, and that they are often
indistinguishable to human observers from real sounds. Project page:
https://www.yimingdou.com/hearing_hands/

</details>


### [101] [Text-Aware Image Restoration with Diffusion Models](https://arxiv.org/abs/2506.09993)
*Jaewon Min,Jin Hyeon Kim,Paul Hyunbin Cho,Jaeeun Lee,Jihye Park,Minkyu Park,Sangpil Kim,Hyunhee Park,Seungryong Kim*

Main category: cs.CV

TL;DR: 论文提出了一种新的图像修复任务TAIR，专注于同时恢复视觉内容和文本保真度，并提出了一个大规模基准SA-Text和多任务扩散框架TeReDiff，显著提升了文本识别准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的图像修复方法在自然图像修复中表现良好，但在文本区域的恢复中常出现文本图像幻觉现象，即生成看似合理但实际错误的文本模式。

Method: 提出了TAIR任务和SA-Text基准，并设计了TeReDiff框架，将扩散模型的内部特征与文本检测模块结合，通过联合训练提取丰富的文本表示作为后续去噪提示。

Result: 实验表明，TeReDiff在文本识别准确性上显著优于现有方法。

Conclusion: TAIR任务和TeReDiff框架为图像修复中的文本保真度问题提供了有效解决方案。

Abstract: Image restoration aims to recover degraded images. However, existing
diffusion-based restoration methods, despite great success in natural image
restoration, often struggle to faithfully reconstruct textual regions in
degraded images. Those methods frequently generate plausible but incorrect
text-like patterns, a phenomenon we refer to as text-image hallucination. In
this paper, we introduce Text-Aware Image Restoration (TAIR), a novel
restoration task that requires the simultaneous recovery of visual contents and
textual fidelity. To tackle this task, we present SA-Text, a large-scale
benchmark of 100K high-quality scene images densely annotated with diverse and
complex text instances. Furthermore, we propose a multi-task diffusion
framework, called TeReDiff, that integrates internal features from diffusion
models into a text-spotting module, enabling both components to benefit from
joint training. This allows for the extraction of rich text representations,
which are utilized as prompts in subsequent denoising steps. Extensive
experiments demonstrate that our approach consistently outperforms
state-of-the-art restoration methods, achieving significant gains in text
recognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/

</details>


### [102] [PlayerOne: Egocentric World Simulator](https://arxiv.org/abs/2506.09995)
*Yuanpeng Tu,Hao Luo,Xi Chen,Xiang Bai,Fan Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: PlayerOne是首个以自我为中心的逼真世界模拟器，能够生成与用户真实动作严格对齐的沉浸式视频。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够精确模拟现实世界动态环境的自我中心视角模拟器，以支持沉浸式探索。

Method: 采用从粗到细的训练流程，包括大规模文本-视频对的预训练和同步运动-视频数据的微调，设计了部分解耦的运动注入方案和联合重建框架。

Result: 实验表明，PlayerOne能够精确控制不同人体动作，并在多样化场景中实现世界一致的建模。

Conclusion: PlayerOne为自我中心视角的世界模拟开辟了新方向，为世界建模及其应用提供了新思路。

Abstract: We introduce PlayerOne, the first egocentric realistic world simulator,
facilitating immersive and unrestricted exploration within vividly dynamic
environments. Given an egocentric scene image from the user, PlayerOne can
accurately construct the corresponding world and generate egocentric videos
that are strictly aligned with the real scene human motion of the user captured
by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that
first performs pretraining on large-scale egocentric text-video pairs for
coarse-level egocentric understanding, followed by finetuning on synchronous
motion-video data extracted from egocentric-exocentric video datasets with our
automatic construction pipeline. Besides, considering the varying importance of
different components, we design a part-disentangled motion injection scheme,
enabling precise control of part-level movements. In addition, we devise a
joint reconstruction framework that progressively models both the 4D scene and
video frames, ensuring scene consistency in the long-form video generation.
Experimental results demonstrate its great generalization ability in precise
control of varying human movements and worldconsistent modeling of diverse
scenarios. It marks the first endeavor into egocentric real-world simulation
and can pave the way for the community to delve into fresh frontiers of world
modeling and its diverse applications.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [103] [Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis](https://arxiv.org/abs/2506.09065)
*Abigail Copiaco,Christian Ritz,Yassine Himeur,Valsamma Eapen,Ammar Albanna,Wathiq Mansoor*

Main category: eess.IV

TL;DR: 本文提出了一种基于AI的辅助技术，通过眼动变量生成的图像变换结合迁移学习，简化自闭症谱系障碍（ASD）的诊断和管理，提高效率和隐私保护。


<details>
  <summary>Details</summary>
Motivation: ASD诊断方法耗时且成本高，亟需一种更便捷、高效的技术来改善诊断和管理流程。

Method: 结合迁移学习和眼动变量生成的图像变换技术，开发了一种AI辅助诊断系统。

Result: 该系统实现了家庭定期诊断，减少了患者和护理人员的压力，同时保护隐私，并改善了监护人与治疗师之间的沟通。

Conclusion: 该方法为ASD提供了及时、可访问的诊断方案，同时保护隐私，改善了患者的生活质量。

Abstract: The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the
past decade, posing significant challenges in communication, behavior, and
focus for affected individuals. Current diagnostic techniques, though
effective, are time-intensive, leading to high social and economic costs. This
work introduces an AI-powered assistive technology designed to streamline ASD
diagnosis and management, enhancing convenience for individuals with ASD and
efficiency for caregivers and therapists. The system integrates transfer
learning with image transforms derived from eye gaze variables to diagnose ASD.
This facilitates and opens opportunities for in-home periodical diagnosis,
reducing stress for individuals and caregivers, while also preserving user
privacy through the use of image transforms. The accessibility of the proposed
method also offers opportunities for improved communication between guardians
and therapists, ensuring regular updates on progress and evolving support
needs. Overall, the approach proposed in this work ensures timely, accessible
diagnosis while protecting the subjects' privacy, improving outcomes for
individuals with ASD.

</details>


### [104] [Foundation Models in Medical Imaging -- A Review and Outlook](https://arxiv.org/abs/2506.09095)
*Vivien van Veldhuizen,Vanessa Botha,Chunyao Lu,Melis Erdal Cesur,Kevin Groot Lipman,Edwin D. de Jong,Hugo Horlings,Clárisa Sanchez,Cees Snoek,Ritse Mann,Eric Marcus,Jonas Teuwen*

Main category: eess.IV

TL;DR: 综述探讨了基础模型（FMs）如何通过无监督学习改变医学图像分析，并总结了其在病理学、放射学和眼科中的应用及挑战。


<details>
  <summary>Details</summary>
Motivation: 研究基础模型在医学图像分析中的潜力，减少对人工标注数据的依赖，提高临床任务的适应性。

Method: 分析了150多项研究，涵盖模型架构、自监督学习方法及下游任务适应策略。

Result: 总结了FMs在不同医学影像领域的应用，并比较了设计选择的异同。

Conclusion: 提出了未来研究的关键挑战和开放性问题。

Abstract: Foundation models (FMs) are changing the way medical images are analyzed by
learning from large collections of unlabeled data. Instead of relying on
manually annotated examples, FMs are pre-trained to learn general-purpose
visual features that can later be adapted to specific clinical tasks with
little additional supervision. In this review, we examine how FMs are being
developed and applied in pathology, radiology, and ophthalmology, drawing on
evidence from over 150 studies. We explain the core components of FM pipelines,
including model architectures, self-supervised learning methods, and strategies
for downstream adaptation. We also review how FMs are being used in each
imaging domain and compare design choices across applications. Finally, we
discuss key challenges and open questions to guide future research.

</details>


### [105] [Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction](https://arxiv.org/abs/2506.09100)
*Haonan Zhang,Guoyan Lao,Yuyao Zhang,Hongjiang Wei*

Main category: eess.IV

TL;DR: LoREIN是一种新型无监督双先验集成框架，用于加速3D多参数定量MRI重建，结合低秩先验和连续性先验，显著提升重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖单一先验或物理模型解决高度不适定逆问题，导致重建结果不理想，因此需要一种更高效的方法。

Method: LoREIN结合低秩表示（LRR）和隐式神经表示（INR），利用双先验增强重建保真度，并通过零样本学习范式优化重建。

Result: 该方法能够高保真重建加权图像，并提升定量参数图的重建精度，同时为零样本学习在医学影像重建中的应用提供了潜力。

Conclusion: LoREIN框架在多参数定量MRI重建中表现出色，为复杂时空和高维图像重建任务提供了新思路。

Abstract: Quantitative magnetic resonance imaging (qMRI) provides tissue-specific
parameters vital for clinical diagnosis. Although simultaneous multi-parametric
qMRI (MP-qMRI) technologies enhance imaging efficiency, robustly reconstructing
qMRI from highly undersampled, high-dimensional measurements remains a
significant challenge. This difficulty arises primarily because current
reconstruction methods that rely solely on a single prior or physics-informed
model to solve the highly ill-posed inverse problem, which often leads to
suboptimal results. To overcome this limitation, we propose LoREIN, a novel
unsupervised and dual-prior-integrated framework for accelerated 3D MP-qMRI
reconstruction. Technically, LoREIN incorporates both low-rank prior and
continuity prior via low-rank representation (LRR) and implicit neural
representation (INR), respectively, to enhance reconstruction fidelity. The
powerful continuous representation of INR enables the estimation of optimal
spatial bases within the low-rank subspace, facilitating high-fidelity
reconstruction of weighted images. Simultaneously, the predicted multi-contrast
weighted images provide essential structural and quantitative guidance, further
enhancing the reconstruction accuracy of quantitative parameter maps.
Furthermore, our work introduces a zero-shot learning paradigm with broad
potential in complex spatiotemporal and high-dimensional image reconstruction
tasks, further advancing the field of medical imaging.

</details>


### [106] [An Explainable Deep Learning Framework for Brain Stroke and Tumor Progression via MRI Interpretation](https://arxiv.org/abs/2506.09161)
*Rajan Das Gupta,Md Imrul Hasan Showmick,Mushfiqur Rahman Abir,Shanjida Akter,Md. Yeasin Rahat,Md. Jakir Hossen*

Main category: eess.IV

TL;DR: 本文提出了一种基于深度学习的系统，利用MobileNet V2和ResNet-50模型从MRI图像中检测脑肿瘤和中风及其阶段，训练和验证准确率分别达到93%和88%。


<details>
  <summary>Details</summary>
Motivation: 早期准确检测脑部异常（如肿瘤和中风）对及时干预和改善患者预后至关重要。

Method: 使用卷积神经网络（MobileNet V2和ResNet-50），通过迁移学习优化，将MRI扫描分类为五个诊断类别。数据集经过精心平衡和增强，采用dropout和数据增强防止过拟合。

Result: 模型表现优异，训练准确率93%，验证准确率88%。ResNet-50略优，但MobileNet V2适合资源有限环境。

Conclusion: 研究提供了早期脑部异常检测的实用AI解决方案，具有临床部署潜力，未来可通过更大数据集和多模态输入进一步优化。

Abstract: Early and accurate detection of brain abnormalities, such as tumors and
strokes, is essential for timely intervention and improved patient outcomes. In
this study, we present a deep learning-based system capable of identifying both
brain tumors and strokes from MRI images, along with their respective stages.
We have executed two groundbreaking strategies involving convolutional neural
networks, MobileNet V2 and ResNet-50-optimized through transfer learning to
classify MRI scans into five diagnostic categories. Our dataset, aggregated and
augmented from various publicly available MRI sources, was carefully curated to
ensure class balance and image diversity. To enhance model generalization and
prevent overfitting, we applied dropout layers and extensive data augmentation.
The models achieved strong performance, with training accuracy reaching 93\%
and validation accuracy up to 88\%. While ResNet-50 demonstrated slightly
better results, Mobile Net V2 remains a promising option for real-time
diagnosis in low resource settings due to its lightweight architecture. This
research offers a practical AI-driven solution for early brain abnormality
detection, with potential for clinical deployment and future enhancement
through larger datasets and multi modal inputs.

</details>


### [107] [The RSNA Lumbar Degenerative Imaging Spine Classification (LumbarDISC) Dataset](https://arxiv.org/abs/2506.09162)
*Tyler J. Richards,Adam E. Flanders,Errol Colak,Luciano M. Prevedello,Robyn L. Ball,Felipe Kitamura,John Mongan,Maryam Vazirabad,Hui-Ming Lin,Anne Kendell,Thanat Kanthawang,Salita Angkurawaranon,Emre Altinmakas,Hakan Dogan,Paulo Eduardo de Aguiar Kuriki,Arjuna Somasundaram,Christopher Ruston,Deniz Bulja,Naida Spahovic,Jennifer Sommer,Sirui Jiang,Eduardo Moreno Judice de Mattos Farina,Eduardo Caminha Nunes,Michael Brassil,Megan McNamara,Johanna Ortiz,Jacob Peoples,Vinson L. Uytana,Anthony Kam,Venkata N. S. Dola,Daniel Murphy,David Vu,Dataset Contributor Group,Dataset Annotator Group,Competition Data Notebook Group,Jason F. Talbott*

Main category: eess.IV

TL;DR: RSNA发布了LumbarDISC数据集，包含2,697名患者的8,593个MRI腰椎影像，用于深度学习模型开发，旨在改善腰椎退行性病变的诊断和临床效率。


<details>
  <summary>Details</summary>
Motivation: 促进机器学习和腰椎影像研究，以提升患者护理和临床效率。

Method: 数据集由专家放射科医生标注，包括腰椎退行性病变的分级，用于深度学习竞赛。

Result: 数据集公开可用，支持非商业用途，涵盖多国多机构数据。

Conclusion: LumbarDISC数据集为腰椎退行性病变研究提供了宝贵资源，有望推动临床进步。

Abstract: The Radiological Society of North America (RSNA) Lumbar Degenerative Imaging
Spine Classification (LumbarDISC) dataset is the largest publicly available
dataset of adult MRI lumbar spine examinations annotated for degenerative
changes. The dataset includes 2,697 patients with a total of 8,593 image series
from 8 institutions across 6 countries and 5 continents. The dataset is
available for free for non-commercial use via Kaggle and RSNA Medical Imaging
Resource of AI (MIRA). The dataset was created for the RSNA 2024 Lumbar Spine
Degenerative Classification competition where competitors developed deep
learning models to grade degenerative changes in the lumbar spine. The degree
of spinal canal, subarticular recess, and neural foraminal stenosis was graded
at each intervertebral disc level in the lumbar spine. The images were
annotated by expert volunteer neuroradiologists and musculoskeletal
radiologists from the RSNA, American Society of Neuroradiology, and the
American Society of Spine Radiology. This dataset aims to facilitate research
and development in machine learning and lumbar spine imaging to lead to
improved patient care and clinical efficiency.

</details>


### [108] [An Interpretable Two-Stage Feature Decomposition Method for Deep Learning-based SAR ATR](https://arxiv.org/abs/2506.09377)
*Chenwei Wang,Renjie Xu,Congwen Wu,Cunyi Yin,Ziyun Liao,Deqing Mao,Sitong Zhang,Hong Yan*

Main category: eess.IV

TL;DR: 本文提出了一种基于物理的两阶段特征分解方法，用于可解释的深度SAR ATR，将不可解释的深度特征转化为具有明确物理意义的属性散射中心组件（ASCC）。


<details>
  <summary>Details</summary>
Motivation: 深度SAR ATR的黑盒特性导致其在决策关键应用中可信度低、风险高，阻碍了实际部署。因此，需要提供可解释的推理基础。

Method: 采用两阶段特征分解方法：第一阶段通过特征解耦和判别模块分离深度特征为近似ASCC；第二阶段通过多层正交非负矩阵三分解（MLO-NMTF）进一步分解为独立物理组件。

Result: 在四个基准数据集上的实验验证了方法的有效性，展示了其可解释性、稳健的识别性能和强泛化能力。

Conclusion: 该方法不仅确保了可解释的推理过程，还实现了准确的识别结果。

Abstract: Synthetic aperture radar automatic target recognition (SAR ATR) has seen
significant performance improvements with deep learning. However, the black-box
nature of deep SAR ATR introduces low confidence and high risks in
decision-critical SAR applications, hindering practical deployment. To address
this issue, deep SAR ATR should provide an interpretable reasoning basis $r_b$
and logic $\lambda_w$, forming the reasoning logic $\sum_{i} {{r_b^i} \times
{\lambda_w^i}} =pred$ behind the decisions. Therefore, this paper proposes a
physics-based two-stage feature decomposition method for interpretable deep SAR
ATR, which transforms uninterpretable deep features into attribute scattering
center components (ASCC) with clear physical meanings. First, ASCCs are
obtained through a clustering algorithm. To extract independent physical
components from deep features, we propose a two-stage decomposition method. In
the first stage, a feature decoupling and discrimination module separates deep
features into approximate ASCCs with global discriminability. In the second
stage, a multilayer orthogonal non-negative matrix tri-factorization (MLO-NMTF)
further decomposes the ASCCs into independent components with distinct physical
meanings. The MLO-NMTF elegantly aligns with the clustering algorithms to
obtain ASCCs. Finally, this method ensures both an interpretable reasoning
process and accurate recognition results. Extensive experiments on four
benchmark datasets confirm its effectiveness, showcasing the method's
interpretability, robust recognition performance, and strong generalization
capability.

</details>


### [109] [A Cytology Dataset for Early Detection of Oral Squamous Cell Carcinoma](https://arxiv.org/abs/2506.09661)
*Garima Jain,Sanghamitra Pati,Mona Duggal,Amit Sethi,Abhijeet Patil,Gururaj Malekar,Nilesh Kowe,Jitender Kumar,Jatin Kashyap,Divyajeet Rout,Deepali,Hitesh,Nishi Halduniya,Sharat Kumar,Heena Tabassum,Rupinder Singh Dhaliwal,Sucheta Devi Khuraijam,Sushma Khuraijam,Sharmila Laishram,Simmi Kharb,Sunita Singh,K. Swaminadtan,Ranjana Solanki,Deepika Hemranjani,Shashank Nath Singh,Uma Handa,Manveen Kaur,Surinder Singhal,Shivani Kalhan,Rakesh Kumar Gupta,Ravi. S,D. Pavithra,Sunil Kumar Mahto,Arvind Kumar,Deepali Tirkey,Saurav Banerjee,L. Sreelakshmi*

Main category: eess.IV

TL;DR: 论文介绍了首个大型多中心口腔细胞学数据集，旨在通过AI技术提升口腔鳞状细胞癌（OSCC）的早期诊断，尤其在资源匮乏地区。


<details>
  <summary>Details</summary>
Motivation: 传统组织病理学诊断在资源匮乏地区难以普及，而口腔细胞学刷检虽成本低但存在观察者差异和专家不足的问题，AI可解决这些挑战。

Method: 收集了来自印度十个三级医疗中心的PAP和MGG染色口腔细胞学切片，由专家标注用于细胞异常分类和检测。

Result: 该数据集填补了公开口腔细胞学数据的空白，支持开发泛化能力强的AI模型。

Conclusion: 该资源有望提升自动化检测、减少诊断错误，改善OSCC早期诊断，降低死亡率。

Abstract: Oral squamous cell carcinoma OSCC is a major global health burden,
particularly in several regions across Asia, Africa, and South America, where
it accounts for a significant proportion of cancer cases. Early detection
dramatically improves outcomes, with stage I cancers achieving up to 90 percent
survival. However, traditional diagnosis based on histopathology has limited
accessibility in low-resource settings because it is invasive,
resource-intensive, and reliant on expert pathologists. On the other hand, oral
cytology of brush biopsy offers a minimally invasive and lower cost
alternative, provided that the remaining challenges, inter observer variability
and unavailability of expert pathologists can be addressed using artificial
intelligence. Development and validation of robust AI solutions requires access
to large, labeled, and multi-source datasets to train high capacity models that
generalize across domain shifts. We introduce the first large and multicenter
oral cytology dataset, comprising annotated slides stained with
Papanicolaou(PAP) and May-Grunwald-Giemsa(MGG) protocols, collected from ten
tertiary medical centers in India. The dataset is labeled and annotated by
expert pathologists for cellular anomaly classification and detection, is
designed to advance AI driven diagnostic methods. By filling the gap in
publicly available oral cytology datasets, this resource aims to enhance
automated detection, reduce diagnostic errors, and improve early OSCC diagnosis
in resource-constrained settings, ultimately contributing to reduced mortality
and better patient outcomes worldwide.

</details>


### [110] [Sampling Theory for Super-Resolution with Implicit Neural Representations](https://arxiv.org/abs/2506.09949)
*Mahrokh Najaf,Gregory Ongie*

Main category: eess.IV

TL;DR: 研究了隐式神经表示（INRs）在解决线性逆问题中的样本复杂性，提出了从低通傅里叶样本中恢复连续域图像的采样要求，并通过实验验证了理论。


<details>
  <summary>Details</summary>
Motivation: 探索INRs在解决计算机视觉和计算成像中的逆问题时所需的样本复杂性，填补传统像素表示与INRs之间的知识空白。

Method: 使用带有ReLU激活和傅里叶特征层的单隐藏层INR，结合广义权重衰减正则化，将非凸参数空间优化问题与无限维空间中的凸惩罚最小化联系起来。

Result: 确定了INR图像在解决训练问题时能够精确恢复所需的足够傅里叶样本数量，并通过实验验证了低宽度INR的精确恢复概率。

Conclusion: INRs在解决线性逆问题时具有潜力，尤其是在连续域图像的超分辨率恢复中表现良好。

Abstract: Implicit neural representations (INRs) have emerged as a powerful tool for
solving inverse problems in computer vision and computational imaging. INRs
represent images as continuous domain functions realized by a neural network
taking spatial coordinates as inputs. However, unlike traditional pixel
representations, little is known about the sample complexity of estimating
images using INRs in the context of linear inverse problems. Towards this end,
we study the sampling requirements for recovery of a continuous domain image
from its low-pass Fourier samples by fitting a single hidden-layer INR with
ReLU activation and a Fourier features layer using a generalized form of weight
decay regularization. Our key insight is to relate minimizers of this
non-convex parameter space optimization problem to minimizers of a convex
penalty defined over an infinite-dimensional space of measures. We identify a
sufficient number of Fourier samples for which an image realized by an INR is
exactly recoverable by solving the INR training problem. To validate our
theory, we empirically assess the probability of achieving exact recovery of
images realized by low-width single hidden-layer INRs, and illustrate the
performance of INRs on super-resolution recovery of continuous domain phantom
images.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [111] [STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support](https://arxiv.org/abs/2506.09070)
*Chenqi Zhang,Yu Feng,Jieru Zhao,Guangda Liu,Wenchao Ding,Chentao Wu,Minyi Guo*

Main category: cs.GR

TL;DR: STREAMINGGS是一种针对3D高斯溅射（3DGS）的算法-架构协同设计，通过内存中心渲染显著提升移动设备上的实时性能。


<details>
  <summary>Details</summary>
Motivation: 3DGS在资源受限的移动设备上难以达到90 FPS的实时要求，现有加速器忽视内存效率，导致DRAM流量冗余。

Method: 提出STREAMINGGS，从基于瓦片的渲染转变为内存中心渲染，实现细粒度流水线并减少DRAM流量。

Result: 设计在移动Ampere GPU上实现45.7倍加速和62.9倍能耗节省。

Conclusion: STREAMINGGS通过优化内存效率显著提升了3DGS在移动设备上的性能。

Abstract: 3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and
sparse Gaussian-based representation. However, 3DGS struggles to meet the
real-time requirement of 90 frames per second (FPS) on resource-constrained
mobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on
compute efficiency but overlook memory efficiency, leading to redundant DRAM
traffic. We introduce STREAMINGGS, a fully streaming 3DGS
algorithm-architecture co-design that achieves fine-grained pipelining and
reduces DRAM traffic by transforming from a tile-centric rendering to a
memory-centric rendering. Results show that our design achieves up to 45.7
$\times$ speedup and 62.9 $\times$ energy savings over mobile Ampere GPUs.

</details>


### [112] [SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach](https://arxiv.org/abs/2506.09075)
*Elly Akhoundi,Hung Yu Ling,Anup Anand Deshmukh,Judith Butepage*

Main category: cs.GR

TL;DR: 论文提出了一种基于Transformer的简单框架，用于运动插值任务，强调数据建模选择的重要性，挑战了模型复杂度决定动画质量的假设。


<details>
  <summary>Details</summary>
Motivation: 传统运动插值方法依赖复杂模型，本文旨在探索数据建模选择对性能的影响，并提出更简单的解决方案。

Method: 使用单一Transformer编码器框架，重点研究数据量、姿态表示和速度输入特征对性能的影响。

Result: 实验表明，增加数据量、优化姿态表示和引入速度特征可显著提升运动插值质量。

Conclusion: 数据建模选择比模型复杂度更能影响动画质量，为运动插值提供了更数据中心的视角。

Abstract: Motion in-betweening is a crucial tool for animators, enabling intricate
control over pose-level details in each keyframe. Recent machine learning
solutions for motion in-betweening rely on complex models, incorporating
skeleton-aware architectures or requiring multiple modules and training steps.
In this work, we introduce a simple yet effective Transformer-based framework,
employing a single Transformer encoder to synthesize realistic motions for
motion in-betweening tasks. We find that data modeling choices play a
significant role in improving in-betweening performance. Among others, we show
that increasing data volume can yield equivalent or improved motion
transitions, that the choice of pose representation is vital for achieving
high-quality results, and that incorporating velocity input features enhances
animation performance. These findings challenge the assumption that model
complexity is the primary determinant of animation quality and provide insights
into a more data-centric approach to motion interpolation. Additional videos
and supplementary material are available at https://silk-paper.github.io.

</details>


### [113] [VideoMat: Extracting PBR Materials from Video Diffusion Models](https://arxiv.org/abs/2506.09665)
*Jacob Munkberg,Zian Wang,Ruofan Liang,Tianchang Shen,Jon Hasselgren*

Main category: cs.GR

TL;DR: 利用视频扩散模型、视频内在分解和基于物理的可微分渲染，通过文本提示或单张图像为3D模型生成高质量材质。


<details>
  <summary>Details</summary>
Motivation: 解决从文本或图像生成3D模型材质的挑战，确保材质与几何和光照条件一致。

Method: 1. 使用视频扩散模型生成多视角一致的材质视频；2. 提取视频的内在属性（基础色、粗糙度、金属度）；3. 结合可微分路径追踪器生成PBR材质。

Result: 生成的材质与3D模型兼容，可直接用于常见内容创作工具。

Conclusion: 该方法为3D模型材质生成提供了一种高效且兼容性强的解决方案。

Abstract: We leverage finetuned video diffusion models, intrinsic decomposition of
videos, and physically-based differentiable rendering to generate high quality
materials for 3D models given a text prompt or a single image. We condition a
video diffusion model to respect the input geometry and lighting condition.
This model produces multiple views of a given 3D model with coherent material
properties. Secondly, we use a recent model to extract intrinsics (base color,
roughness, metallic) from the generated video. Finally, we use the intrinsics
alongside the generated video in a differentiable path tracer to robustly
extract PBR materials directly compatible with common content creation tools.

</details>


### [114] [TransGI: Real-Time Dynamic Global Illumination With Object-Centric Neural Transfer Model](https://arxiv.org/abs/2506.09909)
*Yijie Deng,Lei Han,Lu Fang*

Main category: cs.GR

TL;DR: TransGI是一种新型神经渲染方法，支持实时高保真全局光照，通过对象中心的神经传输模型和辐射共享照明系统实现高效渲染。


<details>
  <summary>Details</summary>
Motivation: 解决现有神经渲染算法在实时渲染和任意光照条件下表达不足的问题，特别是材料表示的紧凑性和表达能力。

Method: 提出对象中心的神经传输模型（MLP解码器和顶点附着潜在特征）和局部光探针辐射共享策略。

Result: 在实时渲染引擎中实现每帧渲染时间小于10毫秒，渲染质量显著优于基线方法。

Conclusion: TransGI在实时性和渲染质量之间取得了平衡，为高保真全局光照提供了有效解决方案。

Abstract: Neural rendering algorithms have revolutionized computer graphics, yet their
impact on real-time rendering under arbitrary lighting conditions remains
limited due to strict latency constraints in practical applications. The key
challenge lies in formulating a compact yet expressive material representation.
To address this, we propose TransGI, a novel neural rendering method for
real-time, high-fidelity global illumination. It comprises an object-centric
neural transfer model for material representation and a radiance-sharing
lighting system for efficient illumination. Traditional BSDF representations
and spatial neural material representations lack expressiveness, requiring
thousands of ray evaluations to converge to noise-free colors. Conversely,
real-time methods trade quality for efficiency by supporting only diffuse
materials. In contrast, our object-centric neural transfer model achieves
compactness and expressiveness through an MLP-based decoder and vertex-attached
latent features, supporting glossy effects with low memory overhead. For
dynamic, varying lighting conditions, we introduce local light probes capturing
scene radiance, coupled with an across-probe radiance-sharing strategy for
efficient probe generation. We implemented our method in a real-time rendering
engine, combining compute shaders and CUDA-based neural networks. Experimental
results demonstrate that our method achieves real-time performance of less than
10 ms to render a frame and significantly improved rendering quality compared
to baseline methods.

</details>


### [115] [DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos](https://arxiv.org/abs/2506.09997)
*Chieh Hubert Lin,Zhaoyang Lv,Songyin Wu,Zhen Xu,Thu Nguyen-Phuoc,Hung-Yu Tseng,Julian Straub,Numair Khan,Lei Xiao,Ming-Hsuan Yang,Yuheng Ren,Richard Newcombe,Zhao Dong,Zhengqin Li*

Main category: cs.GR

TL;DR: DGS-LRM是一种前馈方法，通过单目视频预测可变形3D高斯点云，用于动态场景重建。


<details>
  <summary>Details</summary>
Motivation: 现有前馈模型多限于静态场景，无法重建动态物体运动。动态场景重建面临训练数据稀缺和3D表示等挑战。

Method: 提出合成数据集、可变形3D高斯表示和大型Transformer网络，实现实时动态重建。

Result: DGS-LRM重建质量媲美优化方法，优于现有动态重建方法，3D跟踪性能优异。

Conclusion: DGS-LRM在动态场景重建和3D跟踪任务中表现卓越，具有广泛应用潜力。

Abstract: We introduce the Deformable Gaussian Splats Large Reconstruction Model
(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian
splats from a monocular posed video of any dynamic scene. Feed-forward scene
reconstruction has gained significant attention for its ability to rapidly
create digital replicas of real-world environments. However, most existing
models are limited to static scenes and fail to reconstruct the motion of
moving objects. Developing a feed-forward model for dynamic scene
reconstruction poses significant challenges, including the scarcity of training
data and the need for appropriate 3D representations and training paradigms. To
address these challenges, we introduce several key technical contributions: an
enhanced large-scale synthetic dataset with ground-truth multi-view videos and
dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian
representation that is easy to learn, supports high-quality dynamic view
synthesis, and enables long-range 3D tracking; and a large transformer network
that achieves real-time, generalizable dynamic scene reconstruction. Extensive
qualitative and quantitative experiments demonstrate that DGS-LRM achieves
dynamic scene reconstruction quality comparable to optimization-based methods,
while significantly outperforming the state-of-the-art predictive dynamic
reconstruction method on real-world examples. Its predicted physically grounded
3D deformation is accurate and can readily adapt for long-range 3D tracking
tasks, achieving performance on par with state-of-the-art monocular video 3D
tracking methods.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [116] [Inter-event time statistics of earthquakes as a gauge of volcano activity](https://arxiv.org/abs/2506.09203)
*Sumanta Kundu,Anca Opris,Yosuke Aoki,Takahiro Hatano*

Main category: physics.geo-ph

TL;DR: 研究发现火山地震的IET分布幂律指数随火山活动阶段变化，稳态阶段为0.6-0.7，爆发期峰值约1.3，预爆发阶段为1.0，可作为火山活动预警指标。


<details>
  <summary>Details</summary>
Motivation: 火山地震的IET分布及其与火山活动的关系尚不明确，研究旨在填补这一空白。

Method: 分析不同火山活动阶段的IET分布幂律指数。

Result: 稳态阶段指数0.6-0.7，爆发期1.3，预爆发阶段1.0。

Conclusion: 预爆发阶段的独特指数可作为火山活动预警信号。

Abstract: The probability distribution of inter-event time (IET) between two
consecutive earthquakes is a measure for the uncertainty in the occurrence time
of earthquakes in a region of interest. It is well known that the IET
distribution for regular earthquakes is commonly characterized by a power law
with the exponent of 0.3. However, less is known about other classes of
earthquakes, such as volcanic earthquakes, which do not manifest
mainshock-aftershocks sequences. Since volcanic earthquakes are caused by the
movement of magmas, their IET distribution may be closely related to the
volcanic activities and therefore of particular interest. Nevertheless, the
general form of IET distribution for volcanic earthquakes and its dependence on
volcanic activity are still unknown. Here we show that the power-law exponent
characterizing the IET distribution exhibits a few common values depending on
the stage of volcanic activity. Volcanoes with steady seismicity exhibit the
lowest exponent ranging from 0.6 to 0.7. During the burst period, when the
earthquake rate is highest, the exponent reaches its peak at approximately 1.3.
In the preburst phase, the exponent takes on the intermediate value of 1.0.
These values are common to several different volcanoes. Since the preburst
phase is characterized by the distinct exponent value, it may serve as an
indicator of imminent volcanic activity that is accompanied by a surge in
seismic events.

</details>


### [117] [Supershear-subshear-supershear rupture associated with the 2025 Mandalay Earthquake in Myanmar](https://arxiv.org/abs/2506.09652)
*Shiro Hirano,Ryosuke Doke,Takuto Maeda*

Main category: physics.geo-ph

TL;DR: 研究了2025年缅甸曼德勒地震（Mw 7.7）的复杂破裂动力学，发现破裂速度从超剪切减速至亚剪切，随后恢复超剪切传播。


<details>
  <summary>Details</summary>
Motivation: 通过视频记录和卫星数据，揭示地震破裂过程中的速度变化及其机制。

Method: 分析视频记录、卫星图像和断层加速度模式。

Result: 破裂速度从6 km/s减速至3 km/s，随后恢复超剪切传播，局部滑动减少导致减速。

Conclusion: 直接视频观测对理解复杂地震破裂过程具有重要价值。

Abstract: We investigate the complex rupture dynamics of the 2025 Mandalay Earthquake
(Mw 7.7), which occurred along the Sagaing Fault in Myanmar on March 28, 2025,
at 06:20 UTC. The earthquake involved a near-vertical strike-slip rupture of
approximately 400 km, with 2 to 6 meters of shallow slip. A unique video
recording of surface rupture, captured 124 km south of the epicenter, provided
crucial near-fault data that would have otherwise been unavailable.
  Analysis of the video and other data revealed that the rupture initially
propagated at supershear velocities ($\sim6$ km/s) near the hypocenter.
However, the video indicates a deceleration to subshear speeds ($\sim3$ km/s)
before reaching the camera location. This deceleration is supported by observed
fault-normal acceleration patterns. Satellite imagery further indicated a local
minimum in slip (2--3 m) around 50 km south of the epicenter, suggesting a
region of reduced stress drop, which likely caused the temporary deceleration.
Beyond this point, the rupture appears to have re-established supershear
propagation. This research underscores the value of direct video observations
for understanding complex earthquake rupture processes.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [118] [Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation](https://arxiv.org/abs/2506.09485)
*Yuxin Liu,Zhenghao Peng,Xuanhao Cui,Bolei Zhou*

Main category: cs.RO

TL;DR: Adv-BMT框架通过双向运动变换器生成多样且真实的对抗性交互，解决了自动驾驶测试中长尾安全关键场景稀缺的问题，无需碰撞数据预训练，实验表明其生成的数据能降低20%的碰撞率。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏长尾安全关键场景，限制了自动驾驶系统的测试效果。

Method: 提出Adv-BMT框架，利用双向运动变换器（BMT）逆向预测交通运动，生成多样且真实的对抗性交互。

Result: 生成的碰撞场景质量高，使用增强数据集训练可将碰撞率降低20%。

Conclusion: Adv-BMT框架有效解决了数据稀缺问题，提升了自动驾驶系统的测试性能。

Abstract: Scenario-based testing is essential for validating the performance of
autonomous driving (AD) systems. However, such testing is limited by the
scarcity of long-tailed, safety-critical scenarios in existing datasets
collected in the real world. To tackle the data issue, we propose the Adv-BMT
framework, which augments real-world scenarios with diverse and realistic
adversarial interactions. The core component of Adv-BMT is a bidirectional
motion transformer (BMT) model to perform inverse traffic motion predictions,
which takes agent information in the last time step of the scenario as input,
and reconstruct the traffic in the inverse of chronological order until the
initial time step. The Adv-BMT framework is a two-staged pipeline: it first
conducts adversarial initializations and then inverse motion predictions.
Different from previous work, we do not need any collision data for
pretraining, and are able to generate realistic and diverse collision
interactions. Our experimental results validate the quality of generated
collision scenarios by Adv-BMT: training in our augmented dataset would reduce
episode collision rates by 20\% compared to previous work.

</details>


### [119] [WD-DETR: Wavelet Denoising-Enhanced Real-Time Object Detection Transformer for Robot Perception with Event Cameras](https://arxiv.org/abs/2506.09098)
*Yangjie Cui,Boyang Gao,Yiwei Zhang,Xin Dong,Jinwu Xiang,Daochun Li,Zhan Tu*

Main category: cs.RO

TL;DR: 提出了一种基于小波去噪的检测变换器（WD-DETR），用于事件相机的目标检测，显著提升了性能并实现了实时处理。


<details>
  <summary>Details</summary>
Motivation: 密集事件表示中的累积噪声降低了检测质量，现有方法对此关注不足。

Method: 提出密集事件表示和小波去噪方法，结合变换器网络和动态重组卷积块（DRCB）进行目标预测。

Result: 在DSEC、Gen1和1Mpx数据集上表现优于现有方法，并在NVIDIA Jetson Orin NX上实现35 FPS的实时处理。

Conclusion: WD-DETR有效解决了事件相机检测中的噪声问题，适用于实时机器人感知。

Abstract: Previous studies on event camera sensing have demonstrated certain detection
performance using dense event representations. However, the accumulated noise
in such dense representations has received insufficient attention, which
degrades the representation quality and increases the likelihood of missed
detections. To address this challenge, we propose the Wavelet
Denoising-enhanced DEtection TRansformer, i.e., WD-DETR network, for event
cameras. In particular, a dense event representation is presented first, which
enables real-time reconstruction of events as tensors. Then, a wavelet
transform method is designed to filter noise in the event representations. Such
a method is integrated into the backbone for feature extraction. The extracted
features are subsequently fed into a transformer-based network for object
prediction. To further reduce inference time, we incorporate the Dynamic
Reorganization Convolution Block (DRCB) as a fusion module within the hybrid
encoder. The proposed method has been evaluated on three event-based object
detection datasets, i.e., DSEC, Gen1, and 1Mpx. The results demonstrate that
WD-DETR outperforms tested state-of-the-art methods. Additionally, we implement
our approach on a common onboard computer for robots, the NVIDIA Jetson Orin
NX, achieving a high frame rate of approximately 35 FPS using TensorRT FP16,
which is exceptionally well-suited for real-time perception of onboard robotic
systems.

</details>


### [120] [Perception Characteristics Distance: Measuring Stability and Robustness of Perception System in Dynamic Conditions under a Certain Decision Rule](https://arxiv.org/abs/2506.09217)
*Boyu Jiang,Liang Shi,Zhengzhi Lin,Loren Stowe,Feng Guo*

Main category: cs.RO

TL;DR: 论文提出了一种新的感知性能评估指标PCD，用于量化自动驾驶系统中物体检测的最远可靠距离，并结合了模型输出的不确定性。同时发布了SensorRainFall数据集，支持在不同天气条件下的评估。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统的感知性能受物体距离、场景动态和天气等环境因素影响，传统静态评估指标无法捕捉置信度的动态变化，需要一种新的分布感知指标。

Method: 提出PCD指标，结合SensorRainFall数据集（包含晴天和雨天场景的传感器数据），通过统计分析检测置信度方差的变化点，计算平均PCD（mPCD）。

Result: mPCD能够捕捉不同天气条件下感知系统的可靠性差异，而传统静态指标无法做到。

Conclusion: PCD为自动驾驶系统提供了更安全、更鲁棒的感知性能评估方法，SensorRainFall数据集为相关研究提供了基准。

Abstract: The performance of perception systems in autonomous driving systems (ADS) is
strongly influenced by object distance, scene dynamics, and environmental
conditions such as weather. AI-based perception outputs are inherently
stochastic, with variability driven by these external factors, while
traditional evaluation metrics remain static and event-independent, failing to
capture fluctuations in confidence over time. In this work, we introduce the
Perception Characteristics Distance (PCD) -- a novel evaluation metric that
quantifies the farthest distance at which an object can be reliably detected,
incorporating uncertainty in model outputs. To support this, we present the
SensorRainFall dataset, collected on the Virginia Smart Road using a
sensor-equipped vehicle (cameras, radar, LiDAR) under controlled daylight-clear
and daylight-rain scenarios, with precise ground-truth distances to the target
objects. Statistical analysis reveals the presence of change points in the
variance of detection confidence score with distance. By averaging the PCD
values across a range of detection quality thresholds and probabilistic
thresholds, we compute the mean PCD (mPCD), which captures the overall
perception characteristics of a system with respect to detection distance.
Applying state-of-the-art perception models shows that mPCD captures meaningful
reliability differences under varying weather conditions -- differences that
static metrics overlook. PCD provides a principled, distribution-aware measure
of perception performance, supporting safer and more robust ADS operation,
while the SensorRainFall dataset offers a valuable benchmark for evaluation.
The SensorRainFall dataset is publicly available at
https://www.kaggle.com/datasets/datadrivenwheels/sensorrainfall, and the
evaluation code is open-sourced at
https://github.com/datadrivenwheels/PCD_Python.

</details>


### [121] [UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation](https://arxiv.org/abs/2506.09284)
*Yihe Tang,Wenlong Huang,Yingke Wang,Chengshu Li,Roy Yuan,Ruohan Zhang,Jiajun Wu,Li Fei-Fei*

Main category: cs.RO

TL;DR: UAD是一种无需手动标注的方法，通过利用基础模型从大规模数据中提取物体功能知识，并训练轻量级任务条件解码器，实现了对野外场景和人类活动的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 理解细粒度物体功能对机器人在非结构化环境中执行开放任务至关重要，但现有方法依赖手动标注或预定义任务集。

Method: UAD通过结合大型视觉模型和视觉语言模型自动标注大规模数据集，训练任务条件解码器。

Result: UAD在仿真中训练，但在野外场景和人类活动中表现出色，模仿学习策略在少量演示后泛化到未见过的对象和任务。

Conclusion: UAD为机器人提供了一种无需手动标注的物体功能学习方法，具有广泛的应用潜力。

Abstract: Understanding fine-grained object affordances is imperative for robots to
manipulate objects in unstructured environments given open-ended task
instructions. However, existing methods of visual affordance predictions often
rely on manually annotated data or conditions only on a predefined set of
tasks. We introduce UAD (Unsupervised Affordance Distillation), a method for
distilling affordance knowledge from foundation models into a task-conditioned
affordance model without any manual annotations. By leveraging the
complementary strengths of large vision models and vision-language models, UAD
automatically annotates a large-scale dataset with detailed $<$instruction,
visual affordance$>$ pairs. Training only a lightweight task-conditioned
decoder atop frozen features, UAD exhibits notable generalization to
in-the-wild robotic scenes and to various human activities, despite only being
trained on rendered objects in simulation. Using affordance provided by UAD as
the observation space, we show an imitation learning policy that demonstrates
promising generalization to unseen object instances, object categories, and
even variations in task instructions after training on as few as 10
demonstrations. Project website: https://unsup-affordance.github.io/

</details>


### [122] [DCIRNet: Depth Completion with Iterative Refinement for Dexterous Grasping of Transparent and Reflective Objects](https://arxiv.org/abs/2506.09491)
*Guanghu Xie,Zhiduo Jiang,Yonglong Zhang,Yang Liu,Zongwu Xie,Baoshi Cao,Hong Liu*

Main category: cs.RO

TL;DR: DCIRNet是一种新型多模态深度补全网络，通过融合RGB图像和深度图提升透明和反射物体的深度估计质量，显著提高抓取成功率。


<details>
  <summary>Details</summary>
Motivation: 透明和反射物体的独特视觉特性（如镜面反射和光传输）导致深度传感器估计不准确，影响下游视觉任务。

Method: 提出DCIRNet，结合RGB图像和深度图，采用多模态特征融合模块和多阶段监督策略，优化深度补全。

Result: 实验显示DCIRNet在公开数据集上表现优异，抓取成功率提升44%。

Conclusion: DCIRNet有效解决了透明和反射物体的深度估计问题，具有强泛化能力。

Abstract: Transparent and reflective objects in everyday environments pose significant
challenges for depth sensors due to their unique visual properties, such as
specular reflections and light transmission. These characteristics often lead
to incomplete or inaccurate depth estimation, which severely impacts downstream
geometry-based vision tasks, including object recognition, scene
reconstruction, and robotic manipulation. To address the issue of missing depth
information in transparent and reflective objects, we propose DCIRNet, a novel
multimodal depth completion network that effectively integrates RGB images and
depth maps to enhance depth estimation quality. Our approach incorporates an
innovative multimodal feature fusion module designed to extract complementary
information between RGB images and incomplete depth maps. Furthermore, we
introduce a multi-stage supervision and depth refinement strategy that
progressively improves depth completion and effectively mitigates the issue of
blurred object boundaries. We integrate our depth completion model into
dexterous grasping frameworks and achieve a $44\%$ improvement in the grasp
success rate for transparent and reflective objects. We conduct extensive
experiments on public datasets, where DCIRNet demonstrates superior
performance. The experimental results validate the effectiveness of our
approach and confirm its strong generalization capability across various
transparent and reflective objects.

</details>


### [123] [Enhancing Human-Robot Collaboration: A Sim2Real Domain Adaptation Algorithm for Point Cloud Segmentation in Industrial Environments](https://arxiv.org/abs/2506.09552)
*Fatemeh Mohammadi Amin,Darwin G. Caldwell,Hans Wernher van de Venn*

Main category: cs.RO

TL;DR: 论文提出了一种名为FUSION的双流网络架构，结合DGCNN和CNN，用于3D点云数据的Sim2Real域适应，显著提升了语义分割的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在人类-机器人协作（HRC）中，3D环境的鲁棒解析对安全和效率至关重要，但真实工业标注数据稀缺，因此需要一种从模拟到现实的域适应方法。

Method: 提出双流网络FUSION，结合DGCNN和CNN，并引入残差层，用于工业环境的Sim2Real域适应。

Result: 模型在真实HRC和模拟工业点云上表现优异，分割准确率达97.76%，鲁棒性优于现有方法。

Conclusion: FUSION架构有效提升了语义分割的Sim2Real域适应能力，为HRC的安全性和实用性提供了重要支持。

Abstract: The robust interpretation of 3D environments is crucial for human-robot
collaboration (HRC) applications, where safety and operational efficiency are
paramount. Semantic segmentation plays a key role in this context by enabling a
precise and detailed understanding of the environment. Considering the intense
data hunger for real-world industrial annotated data essential for effective
semantic segmentation, this paper introduces a pioneering approach in the
Sim2Real domain adaptation for semantic segmentation of 3D point cloud data,
specifically tailored for HRC. Our focus is on developing a network that
robustly transitions from simulated environments to real-world applications,
thereby enhancing its practical utility and impact on a safe HRC.
  In this work, we propose a dual-stream network architecture (FUSION)
combining Dynamic Graph Convolutional Neural Networks (DGCNN) and Convolutional
Neural Networks (CNN) augmented with residual layers as a Sim2Real domain
adaptation algorithm for an industrial environment. The proposed model was
evaluated on real-world HRC setups and simulation industrial point clouds, it
showed increased state-of-the-art performance, achieving a segmentation
accuracy of 97.76%, and superior robustness compared to existing methods.

</details>


### [124] [From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models](https://arxiv.org/abs/2506.09930)
*Irving Fang,Juexiao Zhang,Shengbang Tong,Chen Feng*

Main category: cs.RO

TL;DR: 论文提出了一种统一的仿真任务套件，用于评估视觉-语言-动作（VLA）模型的泛化能力，发现VLM预训练虽能提升感知和规划能力，但动作执行仍存在问题。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型的评估不足，缺乏标准化的语言指令任务，且现有研究难以复现。

Method: 引入包含50个仿真任务的套件，涵盖语言指令、视觉和物体操作，并系统评估多种VLA架构。

Result: VLM预训练赋予VLA模型良好的感知和规划能力（意图），但动作执行在分布外观察中表现不佳，且微调可能损害泛化能力。

Conclusion: 发布任务套件和评估代码，旨在推动VLA研究，弥合感知与动作之间的差距。

Abstract: One promise that Vision-Language-Action (VLA) models hold over traditional
imitation learning for robotics is to leverage the broad generalization
capabilities of large Vision-Language Models (VLMs) to produce versatile,
"generalist" robot policies. However, current evaluations of VLAs remain
insufficient. Traditional imitation learning benchmarks are unsuitable due to
the lack of language instructions. Emerging benchmarks for VLAs that
incorporate language often come with limited evaluation tasks and do not intend
to investigate how much VLM pretraining truly contributes to the generalization
capabilities of the downstream robotic policy. Meanwhile, much research relies
on real-world robot setups designed in isolation by different institutions,
which creates a barrier for reproducibility and accessibility. To address this
gap, we introduce a unified probing suite of 50 simulation-based tasks across
10 subcategories spanning language instruction, vision, and objects. We
systematically evaluate several state-of-the-art VLA architectures on this
suite to understand their generalization capability. Our results show that
while VLM backbones endow VLAs with robust perceptual understanding and high
level planning, which we refer to as good intentions, this does not reliably
translate into precise motor execution: when faced with out-of-distribution
observations, policies often exhibit coherent intentions, but falter in action
execution. Moreover, finetuning on action data can erode the original VLM's
generalist reasoning abilities. We release our task suite and evaluation code
to serve as a standardized benchmark for future VLAs and to drive research on
closing the perception-to-action gap. More information, including the source
code, can be found at https://ai4ce.github.io/INT-ACT/

</details>


### [125] [Fluoroscopic Shape and Pose Tracking of Catheters with Custom Radiopaque Markers](https://arxiv.org/abs/2506.09934)
*Jared Lawson,Rohan Chitale,Nabil Simaan*

Main category: cs.RO

TL;DR: 论文提出了一种通过定制不透射线标记物实现微导管形状和姿态估计的方法，以减少对医生心理重建的依赖，并提高导航精度。


<details>
  <summary>Details</summary>
Motivation: 目前，医生需要从双平面透视图像中重建和预测导管运动，感知负担重。现有方法局限于平面分割或笨重的传感设备，不适用于神经介入中的微导管。

Method: 在导管上布置定制的不透射线标记物，设计标记物排列以减少对跟踪不确定性的敏感度，实现形状和姿态的同时估计。

Result: 在直径小于2mm的微导管上部署该方法，形状跟踪误差小于1mm，导管滚动误差低于40度。

Conclusion: 该方法为可操控导管在双平面成像下的自主导航提供了可能。

Abstract: Safe navigation of steerable and robotic catheters in the cerebral
vasculature requires awareness of the catheters shape and pose. Currently, a
significant perception burden is placed on interventionalists to mentally
reconstruct and predict catheter motions from biplane fluoroscopy images.
Efforts to track these catheters are limited to planar segmentation or bulky
sensing instrumentation, which are incompatible with microcatheters used in
neurointervention. In this work, a catheter is equipped with custom radiopaque
markers arranged to enable simultaneous shape and pose estimation under biplane
fluoroscopy. A design measure is proposed to guide the arrangement of these
markers to minimize sensitivity to marker tracking uncertainty. This approach
was deployed for microcatheters smaller than 2mm OD navigating phantom
vasculature with shape tracking errors less than 1mm and catheter roll errors
below 40 degrees. This work can enable steerable catheters to autonomously
navigate under biplane imaging.

</details>


### [126] [Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation](https://arxiv.org/abs/2506.09990)
*Wenbo Zhang,Tianrun Hu,Yanyuan Qiao,Hanbo Zhang,Yuchu Qin,Yang Li,Jiajun Liu,Tao Kong,Lingqiao Liu,Xiao Ma*

Main category: cs.RO

TL;DR: Chain-of-Action (CoA) 是一种基于轨迹自回归建模的新型视觉运动策略范式，通过反向推理生成完整轨迹，实现了全局到局部的动作约束。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅正向预测下一步动作，缺乏对任务目标的全局约束。CoA 通过反向推理和任务目标编码，解决了这一问题。

Method: CoA 采用自回归结构，首先生成关键帧动作编码任务目标，随后自回归生成后续动作。设计了连续动作表示、动态停止、反向时间集成和多令牌预测等技术。

Result: CoA 在 60 个 RLBench 任务和 8 个真实世界操作任务中达到最先进性能。

Conclusion: CoA 通过全局到局部的动作推理，实现了空间泛化能力和策略灵活性，为视觉运动策略提供了新思路。

Abstract: We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built
upon Trajectory Autoregressive Modeling. Unlike conventional approaches that
predict next step action(s) forward, CoA generates an entire trajectory by
explicit backward reasoning with task-specific goals through an action-level
Chain-of-Thought (CoT) process. This process is unified within a single
autoregressive structure: (1) the first token corresponds to a stable keyframe
action that encodes the task-specific goals; and (2) subsequent action tokens
are generated autoregressively, conditioned on the initial keyframe and
previously predicted actions. This backward action reasoning enforces a
global-to-local structure, allowing each local action to be tightly constrained
by the final goal. To further realize the action reasoning structure, CoA
incorporates four complementary designs: continuous action token
representation; dynamic stopping for variable-length trajectory generation;
reverse temporal ensemble; and multi-token prediction to balance action chunk
modeling with global structure. As a result, CoA gives strong spatial
generalization capabilities while preserving the flexibility and simplicity of
a visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art
performance across 60 RLBench tasks and 8 real-world manipulation tasks.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [127] [Devanagari Digit Recognition using Quantum Machine Learning](https://arxiv.org/abs/2506.09069)
*Sahaj Raj Malla*

Main category: quant-ph

TL;DR: 本文提出了一种混合量子-经典架构用于Devanagari手写数字识别，结合了CNN和10-qubit变分量子电路，在DHCD数据集上取得了99.80%的测试准确率。


<details>
  <summary>Details</summary>
Motivation: Devanagari等区域脚本的手写数字识别对多语言文档数字化、教育工具和文化遗产保护至关重要，但其复杂结构和有限标注数据对传统模型提出了挑战。

Method: 采用卷积神经网络（CNN）进行空间特征提取，结合10-qubit变分量子电路（VQC）进行量子增强分类。

Result: 在Devanagari手写字符数据集（DHCD）上实现了99.80%的测试准确率和0.2893的测试损失，平均每类F1分数为0.9980。

Conclusion: 该模型展示了量子机器学习在低资源语言环境中的潜力，为区域脚本识别设立了新基准。

Abstract: Handwritten digit recognition in regional scripts, such as Devanagari, is
crucial for multilingual document digitization, educational tools, and the
preservation of cultural heritage. The script's complex structure and limited
annotated datasets pose significant challenges to conventional models. This
paper introduces the first hybrid quantum-classical architecture for Devanagari
handwritten digit recognition, combining a convolutional neural network (CNN)
for spatial feature extraction with a 10-qubit variational quantum circuit
(VQC) for quantum-enhanced classification. Trained and evaluated on the
Devanagari Handwritten Character Dataset (DHCD), the proposed model achieves a
state-of-the-art test accuracy for quantum implementation of 99.80% and a test
loss of 0.2893, with an average per-class F1-score of 0.9980. Compared to
equivalent classical CNNs, our model demonstrates superior accuracy with
significantly fewer parameters and enhanced robustness. By leveraging quantum
principles such as superposition and entanglement, this work establishes a
novel benchmark for regional script recognition, highlighting the promise of
quantum machine learning (QML) in real-world, low-resource language settings.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [128] [Ming-Omni: A Unified Multimodal Model for Perception and Generation](https://arxiv.org/abs/2506.09344)
*Inclusion AI,Biao Gong,Cheng Zou,Chuanyang Zheng,Chunluan Zhou,Canxiang Yan,Chunxiang Jin,Chunjie Shen,Dandan Zheng,Fudong Wang,Furong Xu,GuangMing Yao,Jun Zhou,Jingdong Chen,Jianxin Sun,Jiajia Liu,Jianjiang Zhu,Jun Peng,Kaixiang Ji,Kaiyou Song,Kaimeng Ren,Libin Wang,Lixiang Ru,Lele Xie,Longhua Tan,Lyuxin Xue,Lan Wang,Mochen Bai,Ning Gao,Pei Chen,Qingpei Guo,Qinglong Zhang,Qiang Xu,Rui Liu,Ruijie Xiong,Sirui Gao,Tinghao Liu,Taisong Li,Weilong Chai,Xinyu Xiao,Xiaomei Wang,Xiaoxue Chen,Xiao Lu,Xiaoyu Li,Xingning Dong,Xuzheng Yu,Yi Yuan,Yuting Gao,Yunxiao Sun,Yipeng Chen,Yifei Wu,Yongjie Lyu,Ziping Ma,Zipeng Feng,Zhijiang Fang,Zhihao Qiu,Ziyuan Huang,Zhengyu He*

Main category: cs.AI

TL;DR: Ming-Omni是一个统一的多模态模型，支持图像、文本、音频和视频处理，并在语音和图像生成方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一个统一的框架，高效处理多模态输入，避免使用多个模型或任务特定的微调。

Method: 采用专用编码器提取多模态标记，并通过MoE架构（Ling）和模态特定路由器处理。

Result: 实验表明，Ming-Omni在统一感知和生成任务中表现优异，支持音频和图像生成。

Conclusion: Ming-Omni是首个开源的多模态模型，匹配GPT-4o的模态支持，并公开代码和模型权重以促进研究。

Abstract: We propose Ming-Omni, a unified multimodal model capable of processing
images, text, audio, and video, while demonstrating strong proficiency in both
speech and image generation. Ming-Omni employs dedicated encoders to extract
tokens from different modalities, which are then processed by Ling, an MoE
architecture equipped with newly proposed modality-specific routers. This
design enables a single model to efficiently process and fuse multimodal inputs
within a unified framework, thereby facilitating diverse tasks without
requiring separate models, task-specific fine-tuning, or structural redesign.
Importantly, Ming-Omni extends beyond conventional multimodal models by
supporting audio and image generation. This is achieved through the integration
of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for
high-quality image generation, which also allow the model to engage in
context-aware chatting, perform text-to-speech conversion, and conduct
versatile image editing. Our experimental results showcase Ming-Omni offers a
powerful solution for unified perception and generation across all modalities.
Notably, our proposed Ming-Omni is the first open-source model we are aware of
to match GPT-4o in modality support, and we release all code and model weights
to encourage further research and development in the community.

</details>


### [129] [V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning](https://arxiv.org/abs/2506.09985)
*Mido Assran,Adrien Bardes,David Fan,Quentin Garrido,Russell Howes,Mojtaba,Komeili,Matthew Muckley,Ammar Rizvi,Claire Roberts,Koustuv Sinha,Artem Zholus,Sergio Arnaud,Abha Gejji,Ada Martin,Francois Robert Hogan,Daniel Dugas,Piotr Bojanowski,Vasil Khalidov,Patrick Labatut,Francisco Massa,Marc Szafraniec,Kapil Krishnakumar,Yong Li,Xiaodong Ma,Sarath Chandar,Franziska Meier,Yann LeCun,Michael Rabbat,Nicolas Ballas*

Main category: cs.AI

TL;DR: 论文提出了一种自监督学习方法V-JEPA 2，结合大规模视频数据和少量机器人交互数据，实现了对物理世界的理解、预测和规划。


<details>
  <summary>Details</summary>
Motivation: 解决现代AI通过观察学习和理解世界的挑战，利用互联网规模的视频数据和少量交互数据开发通用模型。

Method: 预训练无动作的联合嵌入预测架构V-JEPA 2，结合视频和图像数据，并与大语言模型对齐，后训练动作条件世界模型V-JEPA 2-AC。

Result: V-JEPA 2在运动理解和动作预测任务中表现优异，V-JEPA 2-AC在机器人规划任务中实现零样本部署。

Conclusion: 自监督学习结合少量交互数据可生成能规划物理世界的世界模型。

Abstract: A major challenge for modern AI is to learn to understand the world and learn
to act largely by observation. This paper explores a self-supervised approach
that combines internet-scale video data with a small amount of interaction data
(robot trajectories), to develop models capable of understanding, predicting,
and planning in the physical world. We first pre-train an action-free
joint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset
comprising over 1 million hours of internet video. V-JEPA 2 achieves strong
performance on motion understanding (77.3 top-1 accuracy on Something-Something
v2) and state-of-the-art performance on human action anticipation (39.7
recall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.
Additionally, after aligning V-JEPA 2 with a large language model, we
demonstrate state-of-the-art performance on multiple video question-answering
tasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on
TempCompass). Finally, we show how self-supervised learning can be applied to
robotic planning tasks by post-training a latent action-conditioned world
model, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the
Droid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different
labs and enable picking and placing of objects using planning with image goals.
Notably, this is achieved without collecting any data from the robots in these
environments, and without any task-specific training or reward. This work
demonstrates how self-supervised learning from web-scale data and a small
amount of robot interaction data can yield a world model capable of planning in
the physical world.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [130] [Training-Free Voice Conversion with Factorized Optimal Transport](https://arxiv.org/abs/2506.09709)
*Alexander Lobashev,Assel Yermekova,Maria Larchenko*

Main category: cs.SD

TL;DR: Factorized MKL-VC是一种无需训练的改进算法，用于kNN-VC流程，仅需5秒参考音频即可实现高质量的跨语言语音转换。


<details>
  <summary>Details</summary>
Motivation: 解决kNN-VC在短参考音频下内容保留和鲁棒性不足的问题。

Method: 用因子化的最优传输映射替换kNN回归，基于WavLM嵌入子空间和Monge-Kantorovich线性解。

Result: 在LibriSpeech和FLEURS数据集上，MKL-VC显著提升性能，尤其在跨语言语音转换领域表现优于kNN-VC，接近FACodec。

Conclusion: MKL-VC在短参考音频下实现了高质量的跨语言语音转换，性能优于kNN-VC。

Abstract: This paper introduces Factorized MKL-VC, a training-free modification for
kNN-VC pipeline. In contrast with original pipeline, our algorithm performs
high quality any-to-any cross-lingual voice conversion with only 5 second of
reference audio. MKL-VC replaces kNN regression with a factorized optimal
transport map in WavLM embedding subspaces, derived from Monge-Kantorovich
Linear solution. Factorization addresses non-uniform variance across
dimensions, ensuring effective feature transformation. Experiments on
LibriSpeech and FLEURS datasets show MKL-VC significantly improves content
preservation and robustness with short reference audio, outperforming kNN-VC.
MKL-VC achieves performance comparable to FACodec, especially in cross-lingual
voice conversion domain.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [131] [Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery](https://arxiv.org/abs/2506.09063)
*Shayan Shekarforoush,David B. Lindell,Marcus A. Brubaker,David J. Fleet*

Main category: q-bio.QM

TL;DR: CryoSPIRE是一种新型的3D重建框架，通过分层高斯混合模型处理非刚性构象和成分变化的分子结构，在Cryo-EM领域取得了最新成果。


<details>
  <summary>Details</summary>
Motivation: 解决Cryo-EM中因分子构象非刚性和成分变化导致的3D结构建模难题。

Method: 采用分层高斯混合模型，结合部分分割技术，处理构象和成分变化。

Result: 在复杂实验数据集上揭示了生物学意义的结构，并在CryoBench基准测试中达到最新水平。

Conclusion: CryoSPIRE为Cryo-EM中的异构性问题提供了有效的解决方案，推动了该领域的发展。

Abstract: Cryo-EM is a transformational paradigm in molecular biology where
computational methods are used to infer 3D molecular structure at atomic
resolution from extremely noisy 2D electron microscope images. At the forefront
of research is how to model the structure when the imaged particles exhibit
non-rigid conformational flexibility and compositional variation where parts
are sometimes missing. We introduce a novel 3D reconstruction framework with a
hierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for
4D scene reconstruction. In particular, the structure of the model is grounded
in an initial process that infers a part-based segmentation of the particle,
providing essential inductive bias in order to handle both conformational and
compositional variability. The framework, called CryoSPIRE, is shown to reveal
biologically meaningful structures on complex experimental datasets, and
establishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM
heterogeneity methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [132] [MultiNet: An Open-Source Software Toolkit \& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models](https://arxiv.org/abs/2506.09172)
*Pranav Guruprasad,Yangyue Wang,Harshvardhan Sikka*

Main category: cs.LG

TL;DR: MultiNet是一个开源的多模态基准和软件生态系统，用于评估和适应视觉、语言和动作领域的模型。


<details>
  <summary>Details</summary>
Motivation: 开发通用智能代理系统需要结合视觉理解、语言理解和动作生成，MultiNet旨在提供一个标准化评估工具。

Method: MultiNet提供标准化的评估协议、开源软件和包含1.3万亿标记的复合数据集，涵盖多种任务。

Result: MultiNet已被用于研究视觉-语言-动作模型的泛化局限性。

Conclusion: MultiNet为多模态模型的研究和评估提供了重要工具和资源。

Abstract: Recent innovations in multimodal action models represent a promising
direction for developing general-purpose agentic systems, combining visual
understanding, language comprehension, and action generation. We introduce
MultiNet - a novel, fully open-source benchmark and surrounding software
ecosystem designed to rigorously evaluate and adapt models across vision,
language, and action domains. We establish standardized evaluation protocols
for assessing vision-language models (VLMs) and vision-language-action models
(VLAs), and provide open source software to download relevant data, models, and
evaluations. Additionally, we provide a composite dataset with over 1.3
trillion tokens of image captioning, visual question answering, commonsense
reasoning, robotic control, digital game-play, simulated
locomotion/manipulation, and many more tasks. The MultiNet benchmark,
framework, toolkit, and evaluation harness have been used in downstream
research on the limitations of VLA generalization.

</details>


### [133] [LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization](https://arxiv.org/abs/2506.09373)
*Jiaqi Tang,Yu Xia,Yi-Feng Wu,Yuwei Hu,Yuhui Chen,Qing-Guo Chen,Xiaogang Xu,Xiangyu Wu,Hao Lu,Yanqing Ma,Shiyin Lu,Qifeng Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为LPO的新方法，通过利用位置数据和信息熵优化GUI交互，显著提升了交互精度。


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理在空间定位上主要依赖监督微调方法，但其位置感知能力有限，现有强化学习方法也难以有效评估位置准确性。

Method: LPO利用信息熵预测信息丰富的交互区域，并引入基于物理距离的动态位置奖励函数，结合GRPO支持广泛探索GUI环境。

Result: 实验表明LPO在离线和在线评估中均达到SOTA性能。

Conclusion: LPO是一种有效提升GUI交互精度的方法，代码将公开。

Abstract: The advent of autonomous agents is transforming interactions with Graphical
User Interfaces (GUIs) by employing natural language as a powerful
intermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods
in current GUI agents for achieving spatial localization, these methods face
substantial challenges due to their limited capacity to accurately perceive
positional data. Existing strategies, such as reinforcement learning, often
fail to assess positional accuracy effectively, thereby restricting their
utility. In response, we introduce Location Preference Optimization (LPO), a
novel approach that leverages locational data to optimize interaction
preferences. LPO uses information entropy to predict interaction positions by
focusing on zones rich in information. Besides, it further introduces a dynamic
location reward function based on physical distance, reflecting the varying
importance of interaction positions. Supported by Group Relative Preference
Optimization (GRPO), LPO facilitates an extensive exploration of GUI
environments and significantly enhances interaction precision. Comprehensive
experiments demonstrate LPO's superior performance, achieving SOTA results
across both offline benchmarks and real-world online evaluations. Our code will
be made publicly available soon, at https://github.com/AIDC-AI/LPO.

</details>


### [134] [Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models](https://arxiv.org/abs/2506.09532)
*Shuai Wang,Zhenhua Liu,Jiaheng Wei,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: Athena-PRM是一种多模态过程奖励模型，用于评估复杂推理问题中每一步的奖励分数。通过利用弱和强完成者之间的一致性生成高质量标签，仅需5000样本即可高效表现。


<details>
  <summary>Details</summary>
Motivation: 传统自动标注方法（如蒙特卡洛估计）产生噪声标签且计算成本高，需要高效生成高质量过程标注数据。

Method: 提出利用弱和强完成者预测一致性作为可靠过程标签的准则，并采用ORM初始化和负数据上采样策略提升性能。

Result: Athena-PRM在多个场景和基准测试中表现优异，如WeMath提升10.2分，MathVista提升7.1分，并在VisualProcessBench创下SoTA。

Conclusion: Athena-PRM能高效评估推理步骤正确性，显著提升模型性能，适用于多种应用场景。

Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.

</details>


### [135] [FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models](https://arxiv.org/abs/2506.09638)
*Weiying Zheng,Ziyue Lin,Pengxin Guo,Yuyin Zhou,Feifei Wang,Liangqiong Qu*

Main category: cs.LG

TL;DR: 本文提出了FedVLMBench，第一个用于评估视觉语言模型（VLM）联邦微调的系统性基准，涵盖多种架构、策略和任务，揭示了数据异质性和多任务优化的关键见解。


<details>
  <summary>Details</summary>
Motivation: 现有VLM微调方法依赖集中式训练，难以满足隐私敏感领域（如医疗）的需求，且缺乏联邦学习（FL）在VLM中的全面评估基准。

Method: FedVLMBench整合了两种主流VLM架构、四种微调策略、五种FL算法及多模态数据集，通过实验分析架构、策略与数据异质性的关系。

Result: 发现2层MLP连接器与并发调优是FL中基于编码器VLM的最佳配置，且FL方法对视觉中心任务的数据异质性更敏感。

Conclusion: FedVLMBench为隐私保护的多模态基础模型联邦训练提供了标准化平台和实用指导。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable capabilities in
cross-modal understanding and generation by integrating visual and textual
information. While instruction tuning and parameter-efficient fine-tuning
methods have substantially improved the generalization of VLMs, most existing
approaches rely on centralized training, posing challenges for deployment in
domains with strict privacy requirements like healthcare. Recent efforts have
introduced Federated Learning (FL) into VLM fine-tuning to address these
privacy concerns, yet comprehensive benchmarks for evaluating federated
fine-tuning strategies, model architectures, and task generalization remain
lacking. In this work, we present \textbf{FedVLMBench}, the first systematic
benchmark for federated fine-tuning of VLMs. FedVLMBench integrates two
mainstream VLM architectures (encoder-based and encoder-free), four fine-tuning
strategies, five FL algorithms, six multimodal datasets spanning four
cross-domain single-task scenarios and two cross-domain multitask settings,
covering four distinct downstream task categories. Through extensive
experiments, we uncover key insights into the interplay between VLM
architectures, fine-tuning strategies, data heterogeneity, and multi-task
federated optimization. Notably, we find that a 2-layer multilayer perceptron
(MLP) connector with concurrent connector and LLM tuning emerges as the optimal
configuration for encoder-based VLMs in FL. Furthermore, current FL methods
exhibit significantly higher sensitivity to data heterogeneity in
vision-centric tasks than text-centric ones, across both encoder-free and
encoder-based VLM architectures. Our benchmark provides essential tools,
datasets, and empirical guidance for the research community, offering a
standardized platform to advance privacy-preserving, federated training of
multimodal foundation models.

</details>


### [136] [AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale](https://arxiv.org/abs/2506.09733)
*Minjong Cheon*

Main category: cs.LG

TL;DR: AtmosMJ挑战了传统观点，证明标准经纬度网格也能实现长期稳定的天气预测，无需依赖非标准数据表示。


<details>
  <summary>Details</summary>
Motivation: 探索是否能在标准经纬度网格上实现长期稳定的天气预测，挑战非标准数据表示的必要性。

Method: 提出AtmosMJ，一种直接在ERA5数据上运行的深度卷积网络，采用Gated Residual Fusion机制防止误差累积。

Result: AtmosMJ能稳定预测约500天，10天预测精度与Pangu-Weather和GraphCast相当，训练成本极低。

Conclusion: 高效架构设计是长期稳定天气预测的关键，而非非标准数据表示。

Abstract: The advent of Large Weather Models (LWMs) has marked a turning point in
data-driven forecasting, with many models now outperforming traditional
numerical systems in the medium range. However, achieving stable, long-range
autoregressive forecasts beyond a few weeks remains a significant challenge.
Prevailing state-of-the-art models that achieve year-long stability, such as
SFNO and DLWP-HPX, have relied on transforming input data onto non-standard
spatial domains like spherical harmonics or HEALPix meshes. This has led to the
prevailing assumption that such representations are necessary to enforce
physical consistency and long-term stability. This paper challenges that
assumption by investigating whether comparable long-range performance can be
achieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep
convolutional network that operates directly on ERA5 data without any spherical
remapping. The model's stability is enabled by a novel Gated Residual Fusion
(GRF) mechanism, which adaptively moderates feature updates to prevent error
accumulation over long recursive simulations. Our results demonstrate that
AtmosMJ produces stable and physically plausible forecasts for about 500 days.
In quantitative evaluations, it achieves competitive 10-day forecast accuracy
against models like Pangu-Weather and GraphCast, all while requiring a
remarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest
that efficient architectural design, rather than non-standard data
representation, can be the key to unlocking stable and computationally
efficient long-range weather prediction.

</details>


### [137] [Canonical Latent Representations in Conditional Diffusion Models](https://arxiv.org/abs/2506.09955)
*Yitao Xu,Tong Zhang,Ehsan Pajouheshgar,Sabine Süsstrunk*

Main category: cs.LG

TL;DR: 论文提出了一种名为CLAReps的潜在表示方法，用于解决条件扩散模型（CDMs）中类别特征与无关背景的纠缠问题，并开发了基于扩散的特征蒸馏范式CaDistill。


<details>
  <summary>Details</summary>
Motivation: 条件扩散模型在生成任务中表现优异，但其建模能力导致类别特征与无关背景纠缠，难以提取鲁棒且可解释的表征。

Method: 通过识别CLAReps（保留核心类别信息、丢弃无关信号的潜在代码），并开发CaDistill范式，利用CLAReps作为紧凑的教师信号进行特征蒸馏。

Result: 学生模型仅需10%的训练数据（CLAReps），即可实现强对抗鲁棒性和泛化能力，更专注于核心类别信号。

Conclusion: 研究表明，CDMs不仅能作为图像生成器，还可作为紧凑、可解释的教师模型，推动鲁棒表征学习。

Abstract: Conditional diffusion models (CDMs) have shown impressive performance across
a range of generative tasks. Their ability to model the full data distribution
has opened new avenues for analysis-by-synthesis in downstream discriminative
learning. However, this same modeling capacity causes CDMs to entangle the
class-defining features with irrelevant context, posing challenges to
extracting robust and interpretable representations. To this end, we identify
Canonical LAtent Representations (CLAReps), latent codes whose internal CDM
features preserve essential categorical information while discarding
non-discriminative signals. When decoded, CLAReps produce representative
samples for each class, offering an interpretable and compact summary of the
core class semantics with minimal irrelevant details. Exploiting CLAReps, we
develop a novel diffusion-based feature-distillation paradigm, CaDistill. While
the student has full access to the training set, the CDM as teacher transfers
core class knowledge only via CLAReps, which amounts to merely 10 % of the
training data in size. After training, the student achieves strong adversarial
robustness and generalization ability, focusing more on the class signals
instead of spurious background cues. Our findings suggest that CDMs can serve
not just as image generators but also as compact, interpretable teachers that
can drive robust representation learning.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [138] [DAVSP: Safety Alignment for Large Vision-Language Models via Deep Aligned Visual Safety Prompt](https://arxiv.org/abs/2506.09353)
*Yitong Zhang,Jia Li,Liyi Cai,Ge Li*

Main category: cs.CR

TL;DR: 论文提出了一种名为DAVSP的方法，通过视觉安全提示和深度对齐技术，有效抵御恶意查询并保持良性输入的实用性。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLM）在多种应用中表现优异，但对恶意查询的视觉模态攻击仍显脆弱，现有对齐方法无法有效兼顾安全性和实用性。

Method: 提出视觉安全提示（在输入图像周围添加可训练填充区域）和深度对齐（通过激活空间监督训练视觉安全提示），增强模型对恶意查询的感知能力。

Result: 在五个基准测试和两种代表性LVLM上的实验表明，DAVSP能有效抵御恶意查询并保持良性输入实用性，且具备跨模型泛化能力。

Conclusion: DAVSP通过视觉安全提示和深度对齐的联合作用，显著提升了模型的安全性和实用性，代码已开源。

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive progress across
various applications but remain vulnerable to malicious queries that exploit
the visual modality. Existing alignment approaches typically fail to resist
malicious queries while preserving utility on benign ones effectively. To
address these challenges, we propose Deep Aligned Visual Safety Prompt (DAVSP),
which is built upon two key innovations. First, we introduce the Visual Safety
Prompt, which appends a trainable padding region around the input image. It
preserves visual features and expands the optimization space. Second, we
propose Deep Alignment, a novel approach to train the visual safety prompt
through supervision in the model's activation space. It enhances the inherent
ability of LVLMs to perceive malicious queries, achieving deeper alignment than
prior works. Extensive experiments across five benchmarks on two representative
LVLMs demonstrate that DAVSP effectively resists malicious queries while
preserving benign input utility. Furthermore, DAVSP exhibits great cross-model
generation ability. Ablation studies further reveal that both the Visual Safety
Prompt and Deep Alignment are essential components, jointly contributing to its
overall effectiveness. The code is publicly available at
https://github.com/zhangyitonggg/DAVSP.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [139] [Enhancing semi-resolved CFD-DEM for dilute to dense particle-fluid systems: A point cloud based, two-step mapping strategy via coarse graining](https://arxiv.org/abs/2506.09517)
*Yuxiang Liu,Lu Jing,Xudong Fu,Huabin Shi*

Main category: physics.flu-dyn

TL;DR: 提出了一种基于点云粗粒化的两步映射CFD-DEM方法，解决了传统体积平均CFD-DEM对流体网格分辨率的依赖问题，并验证了其准确性。


<details>
  <summary>Details</summary>
Motivation: 传统CFD-DEM方法在粒子跨越流体网格时不稳定，且难以捕捉高密度颗粒系统中的孔隙流体压力效应。

Method: 采用点云粗粒化技术，将离散粒子转换为连续场，再与流体网格变量耦合。

Result: 算法在多种配置下验证有效，包括静态粒子权重分配、沉降球体、流化床等。

Conclusion: 新方法能准确模拟流体-粒子相互作用，适用于广泛的网格-粒子尺寸比和固体浓度。

Abstract: Computational fluid dynamics and discrete element method (CFD-DEM) coupling
is an efficient and powerful tool to simulate particle-fluid systems. However,
current volume-averaged CFD-DEM relying on direct grid-based mapping between
the fluid and particle phases can exhibit a strong dependence on the fluid grid
resolution, becoming unstable as particles move across fluid grids, and can
fail to capture pore fluid pressure effects in very dense granular systems.
Here we propose a two-step mapping CFD-DEM which uses a point-based coarse
graining technique for intermediate smoothing to overcome these limitations.
The discrete particles are first converted into smooth, coarse-grained
continuum fields via a multi-layer Fibonacci point cloud, independent of the
fluid grids. Then, accurate coupling is achieved between the coarse-grained,
point cloud fields and the fluid grid-based variables. The algorithm is
validated in various configurations, including weight allocation of a static
particle on one-dimensional grids and a falling particle on two-dimensional
grids, sedimentation of a sphere in a viscous fluid, size-bidisperse fluidized
beds, Ergun's pressure drop test, and immersed granular column collapse. The
proposed CFD-DEM represents a novel strategy to accurately simulate
fluid-particle interactions for a wide range of grid-to-particle size ratios
and solid concentrations, which is of potential use in many industrial and
geophysical applications.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [140] [Using Sign Language Production as Data Augmentation to enhance Sign Language Translation](https://arxiv.org/abs/2506.09643)
*Harry Walsh,Maksym Ivashechkin,Richard Bowden*

Main category: cs.CL

TL;DR: 利用手语生成技术增强手语翻译模型的性能，通过骨架生成、拼接和生成模型SignGAN与SignSplat，提升数据集多样性，使翻译模型性能提升19%。


<details>
  <summary>Details</summary>
Motivation: 手语数据集稀缺且规模小，传统收集方式成本高且受限，需通过生成技术增强数据。

Method: 采用骨架生成、拼接技术和两种生成模型（SignGAN、SignSplat）生成多样化的手语数据。

Result: 方法有效增强数据集，使手语翻译模型性能提升19%。

Conclusion: 生成技术可显著提升手语翻译性能，适用于资源受限环境。

Abstract: Machine learning models fundamentally rely on large quantities of
high-quality data. Collecting the necessary data for these models can be
challenging due to cost, scarcity, and privacy restrictions. Signed languages
are visual languages used by the deaf community and are considered low-resource
languages. Sign language datasets are often orders of magnitude smaller than
their spoken language counterparts. Sign Language Production is the task of
generating sign language videos from spoken language sentences, while Sign
Language Translation is the reverse translation task. Here, we propose
leveraging recent advancements in Sign Language Production to augment existing
sign language datasets and enhance the performance of Sign Language Translation
models. For this, we utilize three techniques: a skeleton-based approach to
production, sign stitching, and two photo-realistic generative models, SignGAN
and SignSplat. We evaluate the effectiveness of these techniques in enhancing
the performance of Sign Language Translation models by generating variation in
the signer's appearance and the motion of the skeletal data. Our results
demonstrate that the proposed methods can effectively augment existing datasets
and enhance the performance of Sign Language Translation models by up to 19%,
paving the way for more robust and accurate Sign Language Translation systems,
even in resource-constrained environments.

</details>


### [141] [ComfyUI-R1: Exploring Reasoning Models for Workflow Generation](https://arxiv.org/abs/2506.09790)
*Zhenran Xu,Yiyu Wang,Xue Yang,Longyue Wang,Weihua Luo,Kaifu Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: ComfyUI-R1是一个大型推理模型，用于自动化生成AI创作工作流，通过两阶段训练（CoT微调和强化学习）显著提升了工作流生成的准确性和复杂性。


<details>
  <summary>Details</summary>
Motivation: AI生成内容的工作流需要高度专业知识，ComfyUI-R1旨在降低用户的学习门槛，实现自动化工作流生成。

Method: 使用4K工作流数据集构建长链推理数据，通过CoT微调和强化学习训练模型，结合规则-指标混合奖励机制。

Result: 7B参数模型在格式有效性、通过率和节点/图级F1分数上显著优于GPT-4o和Claude系列模型。

Conclusion: 长链推理和代码化工作流对AI艺术创作具有重要潜力，ComfyUI-R1在复杂工作流生成中表现优异。

Abstract: AI-generated content has evolved from monolithic models to modular workflows,
particularly on platforms like ComfyUI, enabling customization in creative
pipelines. However, crafting effective workflows requires great expertise to
orchestrate numerous specialized components, presenting a steep learning curve
for users. To address this challenge, we introduce ComfyUI-R1, the first large
reasoning model for automated workflow generation. Starting with our curated
dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning
data, including node selection, workflow planning, and code-level workflow
representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT
fine-tuning for cold start, adapting models to the ComfyUI domain; (2)
reinforcement learning for incentivizing reasoning capability, guided by a
fine-grained rule-metric hybrid reward, ensuring format validity, structural
integrity, and node-level fidelity. Experiments show that our 7B-parameter
model achieves a 97\% format validity rate, along with high pass rate,
node-level and graph-level F1 scores, significantly surpassing prior
state-of-the-art methods that employ leading closed-source models such as
GPT-4o and Claude series. Further analysis highlights the critical role of the
reasoning process and the advantage of transforming workflows into code.
Qualitative comparison reveals our strength in synthesizing intricate workflows
with diverse nodes, underscoring the potential of long CoT reasoning in AI art
creation.

</details>


### [142] [Dataset of News Articles with Provenance Metadata for Media Relevance Assessment](https://arxiv.org/abs/2506.09847)
*Tomas Peterka,Matyas Bohacek*

Main category: cs.CL

TL;DR: 论文提出了一种检测新闻图片来源相关性的方法，并构建了一个数据集，评估了六种大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注图片语义与文本叙事的匹配，忽略了图片来源的时空信息，导致检测不准确。

Method: 构建了News Media Provenance Dataset数据集，提出了LOR和DTOR两个任务，并测试了六种大语言模型的零样本性能。

Result: LOR任务表现良好，但DTOR任务表现较差，表明需要进一步优化模型架构。

Conclusion: 未来研究需针对DTOR任务设计专门架构，以提升检测准确性。

Abstract: Out-of-context and misattributed imagery is the leading form of media
manipulation in today's misinformation and disinformation landscape. The
existing methods attempting to detect this practice often only consider whether
the semantics of the imagery corresponds to the text narrative, missing
manipulation so long as the depicted objects or scenes somewhat correspond to
the narrative at hand. To tackle this, we introduce News Media Provenance
Dataset, a dataset of news articles with provenance-tagged images. We formulate
two tasks on this dataset, location of origin relevance (LOR) and date and time
of origin relevance (DTOR), and present baseline results on six large language
models (LLMs). We identify that, while the zero-shot performance on LOR is
promising, the performance on DTOR hinders, leaving room for specialized
architectures and future work.

</details>
