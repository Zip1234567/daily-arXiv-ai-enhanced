<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 84]
- [eess.IV](#eess.IV) [Total: 12]
- [cs.GR](#cs.GR) [Total: 4]
- [physics.geo-ph](#physics.geo-ph) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [math.AP](#math.AP) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 6]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.HC](#cs.HC) [Total: 1]
- [cond-mat.other](#cond-mat.other) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs](https://arxiv.org/abs/2506.21656)
*Yifan Shen,Yuanzhe Liu,Jingyuan Zhu,Xu Cao,Xiaofeng Zhang,Yixiao He,Wenming Ye,James Matthew Rehg,Ismini Lourentzou*

Main category: cs.CV

TL;DR: SpatialReasoner-R1模型通过M3CTS生成高质量空间推理监督数据，结合fDPO优化方法，显著提升了空间推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在细粒度空间推理和多步逻辑对齐方面表现不足，需要改进。

Method: 采用M3CTS生成多样且逻辑一致的推理轨迹，并设计fDPO方法，通过空间奖励机制优化描述和逻辑推理。

Result: fDPO在空间质量和数量任务上分别提升4.1%和9.0%，SpatialReasoner-R1在SPATIALRGPT-Bench上创下新SoTA。

Conclusion: SpatialReasoner-R1结合fDPO显著提升了空间推理能力，同时保持通用视觉语言任务的竞争力。

Abstract: Current Vision-Language Models (VLMs) struggle with fine-grained spatial
reasoning, particularly when multi-step logic and precise spatial alignment are
required. In this work, we introduce SpatialReasoner-R1, a vision-language
reasoning model designed to address these limitations. To construct
high-quality supervision for spatial reasoning, we design a Multi-Model Monte
Carlo Tree Search (M3CTS) method that generates diverse, logically consistent
Long Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose
fine-grained Direct Preference Optimization (fDPO), which introduces
segment-specific preference granularity for descriptive grounding and logical
reasoning, guided by a spatial reward mechanism that evaluates candidate
responses based on visual consistency, spatial grounding, and logical
coherence. Experimental results demonstrate that fDPO achieves an average
improvement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%
gain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a
new SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in
average accuracy, while maintaining competitive performance on general
vision-language tasks.

</details>


### [2] [TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation](https://arxiv.org/abs/2506.21681)
*Hakan Çapuk,Andrew Bond,Muhammed Burak Kızıl,Emir Göçen,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

TL;DR: TanDiT是一种新方法，通过生成覆盖360度视图的切平面图像网格来合成全景场景，解决了现有模型在全景图像生成中的几何失真和循环一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成模型在全景图像生成中存在几何失真和循环一致性问题，需要一种新方法来解决这些挑战。

Method: TanDiT采用统一的扩散模型，在单次去噪迭代中同时生成切平面图像，并提出模型无关的后处理步骤以增强全局一致性。

Result: 实验表明，TanDiT能有效泛化到训练数据之外，处理复杂文本提示，并与其他生成模型无缝集成，生成高质量、多样化的全景图像。

Conclusion: TanDiT通过创新的切平面图像生成和后处理步骤，显著提升了全景图像生成的质量和多样性。

Abstract: Recent advances in image generation have led to remarkable improvements in
synthesizing perspective images. However, these models still struggle with
panoramic image generation due to unique challenges, including varying levels
of geometric distortion and the requirement for seamless loop-consistency. To
address these issues while leveraging the strengths of the existing models, we
introduce TanDiT, a method that synthesizes panoramic scenes by generating
grids of tangent-plane images covering the entire 360$^\circ$ view. Unlike
previous methods relying on multiple diffusion branches, TanDiT utilizes a
unified diffusion model trained to produce these tangent-plane images
simultaneously within a single denoising iteration. Furthermore, we propose a
model-agnostic post-processing step specifically designed to enhance global
coherence across the generated panoramas. To accurately assess panoramic image
quality, we also present two specialized metrics, TangentIS and TangentFID, and
provide a comprehensive benchmark comprising captioned panoramic datasets and
standardized evaluation scripts. Extensive experiments demonstrate that our
method generalizes effectively beyond its training data, robustly interprets
detailed and complex text prompts, and seamlessly integrates with various
generative models to yield high-quality, diverse panoramic images.

</details>


### [3] [FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering](https://arxiv.org/abs/2506.21710)
*Liangyu Zhong,Fabio Rosenthal,Joachim Sicking,Fabian Hüger,Thorsten Bagdonat,Hanno Gottschalk,Leo Schwinn*

Main category: cs.CV

TL;DR: FOCUS是一种无需训练的视觉裁剪方法，利用MLLM内部表示指导搜索最相关图像区域，显著提升细粒度VQA任务的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLM在图像-文本输入上表现出强大的感知和推理能力，但在关注小图像细节的VQA任务中仍存在挑战。现有视觉裁剪方法需要任务特定微调、效率低或不兼容高效注意力实现。

Method: FOCUS通过四步实现：识别VQA提示中的目标对象；使用KV缓存计算对象相关性图；基于图提出并排序相关图像区域；使用排名最高的区域执行细粒度VQA任务。

Result: FOCUS在四个细粒度VQA数据集和两种MLLM上表现优异，优于三种流行视觉裁剪方法，且计算量减少3-6.5倍。

Conclusion: FOCUS是一种高效且无需训练的视觉裁剪方法，显著提升了细粒度VQA任务的性能。

Abstract: While Multimodal Large Language Models (MLLMs) offer strong perception and
reasoning capabilities for image-text input, Visual Question Answering (VQA)
focusing on small image details still remains a challenge. Although visual
cropping techniques seem promising, recent approaches have several limitations:
the need for task-specific fine-tuning, low efficiency due to uninformed
exhaustive search, or incompatibility with efficient attention implementations.
We address these shortcomings by proposing a training-free visual cropping
method, dubbed FOCUS, that leverages MLLM-internal representations to guide the
search for the most relevant image region. This is accomplished in four steps:
first, we identify the target object(s) in the VQA prompt; second, we compute
an object relevance map using the key-value (KV) cache; third, we propose and
rank relevant image regions based on the map; and finally, we perform the
fine-grained VQA task using the top-ranked region. As a result of this informed
search strategy, FOCUS achieves strong performance across four fine-grained VQA
datasets and two types of MLLMs. It outperforms three popular visual cropping
methods in both accuracy and efficiency, and matches the best-performing
baseline, ZoomEye, while requiring 3 - 6.5 x less compute.

</details>


### [4] [CAST: Cross-Attentive Spatio-Temporal feature fusion for Deepfake detection](https://arxiv.org/abs/2506.21711)
*Aryan Thakre,Omkar Nagwekar,Vedang Talekar,Aparna Santra Biswas*

Main category: cs.CV

TL;DR: 提出了一种名为CAST的统一模型，通过交叉注意力机制融合时空特征，显著提升了深度伪造视频检测的性能。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术对数字媒体真实性构成威胁，现有CNN-Transformer模型在时空特征融合上存在不足，限制了检测能力。

Method: 采用交叉注意力机制动态融合时空特征，增强对细微时间演化伪造痕迹（如闪烁的眼睛或扭曲的嘴唇）的检测能力。

Result: 在FaceForensics++、Celeb-DF和DeepfakeDetection数据集上表现出色，AUC达99.49%，准确率97.57%，跨数据集测试AUC为93.31%。

Conclusion: 交叉注意力特征融合显著提升了深度伪造检测的鲁棒性和泛化能力。

Abstract: Deepfakes have emerged as a significant threat to digital media authenticity,
increasing the need for advanced detection techniques that can identify subtle
and time-dependent manipulations. CNNs are effective at capturing spatial
artifacts, and Transformers excel at modeling temporal inconsistencies.
However, many existing CNN-Transformer models process spatial and temporal
features independently. In particular, attention-based methods often use
separate attention mechanisms for spatial and temporal features and combine
them using naive approaches like averaging, addition, or concatenation, which
limits the depth of spatio-temporal interaction. To address this challenge, we
propose a unified CAST model that leverages cross-attention to effectively fuse
spatial and temporal features in a more integrated manner. Our approach allows
temporal features to dynamically attend to relevant spatial regions, enhancing
the model's ability to detect fine-grained, time-evolving artifacts such as
flickering eyes or warped lips. This design enables more precise localization
and deeper contextual understanding, leading to improved performance across
diverse and challenging scenarios. We evaluate the performance of our model
using the FaceForensics++, Celeb-DF, and DeepfakeDetection datasets in both
intra- and cross-dataset settings to affirm the superiority of our approach.
Our model achieves strong performance with an AUC of 99.49 percent and an
accuracy of 97.57 percent in intra-dataset evaluations. In cross-dataset
testing, it demonstrates impressive generalization by achieving a 93.31 percent
AUC on the unseen DeepfakeDetection dataset. These results highlight the
effectiveness of cross-attention-based feature fusion in enhancing the
robustness of deepfake video detection.

</details>


### [5] [ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment](https://arxiv.org/abs/2109.05721)
*Yangyu Huang,Hao Yang,Chong Li,Jongyoo Kim,Fangyun Wei*

Main category: cs.CV

TL;DR: 论文提出ADL和AAM方法解决人脸对齐中的误差偏差问题，通过优化CNN模型实现更好的收敛性，并在多个数据集上取得最优结果。


<details>
  <summary>Details</summary>
Motivation: 研究人脸对齐中误差分布的偏差问题，发现误差沿地标曲线切线方向分布，这一偏差与模糊的地标标注任务相关。

Method: 提出各向异性方向损失（ADL）和各向异性注意力模块（AAM），分别用于坐标和热图回归，并通过ADNet整合优化训练流程。

Result: ADNet在300W、WFLW和COFW数据集上取得最优性能，验证了方法的有效性和鲁棒性。

Conclusion: 通过ADL和AAM的互补作用，ADNet成功解决了误差偏差问题，提升了人脸对齐的性能。

Abstract: The recent progress of CNN has dramatically improved face alignment
performance. However, few works have paid attention to the error-bias with
respect to error distribution of facial landmarks. In this paper, we
investigate the error-bias issue in face alignment, where the distributions of
landmark errors tend to spread along the tangent line to landmark curves. This
error-bias is not trivial since it is closely connected to the ambiguous
landmark labeling task. Inspired by this observation, we seek a way to leverage
the error-bias property for better convergence of CNN model. To this end, we
propose anisotropic direction loss (ADL) and anisotropic attention module (AAM)
for coordinate and heatmap regression, respectively. ADL imposes strong binding
force in normal direction for each landmark point on facial boundaries. On the
other hand, AAM is an attention module which can get anisotropic attention mask
focusing on the region of point and its local edge connected by adjacent
points, it has a stronger response in tangent than in normal, which means
relaxed constraints in the tangent. These two methods work in a complementary
manner to learn both facial structures and texture details. Finally, we
integrate them into an optimized end-to-end training pipeline named ADNet. Our
ADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which
demonstrates the effectiveness and robustness.

</details>


### [6] [Elucidating and Endowing the Diffusion Training Paradigm for General Image Restoration](https://arxiv.org/abs/2506.21722)
*Xin Lu,Xueyang Fu,Jie Xiao,Zihao Fan,Yurui Zhu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 该论文提出了一种将扩散训练范式融入通用图像修复（IR）框架的新方法，通过系统分析时间步依赖、网络层次、噪声级别关系和多任务相关性，优化了IR网络的泛化能力和多任务性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像修复任务中表现出强大的生成能力，但其复杂架构和迭代过程限制了实际应用。现有方法主要关注网络架构和扩散路径优化，而忽略了将扩散训练范式整合到通用IR框架中。

Method: 论文提出了一种新的IR框架，通过时间步依赖、网络层次、噪声级别关系和多任务相关性的系统分析，引入正则化策略和对齐扩散目标与IR任务的方法。此外，还开发了增量训练范式和任务特定适配器。

Result: 实验表明，该方法显著提高了IR网络在单任务中的泛化能力，并在多任务统一IR中实现了优越性能。

Conclusion: 该框架可以无缝集成到现有的通用IR架构中，为图像修复任务提供了一种高效且通用的解决方案。

Abstract: While diffusion models demonstrate strong generative capabilities in image
restoration (IR) tasks, their complex architectures and iterative processes
limit their practical application compared to mainstream reconstruction-based
general ordinary IR networks. Existing approaches primarily focus on optimizing
network architecture and diffusion paths but overlook the integration of the
diffusion training paradigm within general ordinary IR frameworks. To address
these challenges, this paper elucidates key principles for adapting the
diffusion training paradigm to general IR training through systematic analysis
of time-step dependencies, network hierarchies, noise-level relationships, and
multi-restoration task correlations, proposing a new IR framework supported by
diffusion-based training. To enable IR networks to simultaneously restore
images and model generative representations, we introduce a series of
regularization strategies that align diffusion objectives with IR tasks,
improving generalization in single-task scenarios. Furthermore, recognizing
that diffusion-based generation exerts varying influences across different IR
tasks, we develop an incremental training paradigm and task-specific adaptors,
further enhancing performance in multi-task unified IR. Experiments demonstrate
that our method significantly improves the generalization of IR networks in
single-task IR and achieves superior performance in multi-task unified IR.
Notably, the proposed framework can be seamlessly integrated into existing
general IR architectures.

</details>


### [7] [FreeEnricher: Enriching Face Landmarks without Additional Cost](https://arxiv.org/abs/2212.09525)
*Yangyu Huang,Xi Chen,Jongyoo Kim,Hao Yang,Chong Li,Jiaolong Yang,Dong Chen*

Main category: cs.CV

TL;DR: 提出一种通过稀疏标记数据集生成密集面部标记的框架，利用弱监督学习和局部相似性实现高精度对齐。


<details>
  <summary>Details</summary>
Motivation: 密集面部标记在美容医学和面部美化等场景需求高，但现有研究多关注稀疏标记。

Method: 基于稀疏标记数据集（如300W、WFLW），利用局部相似性设计弱监督学习框架，通过多个操作符实现标记密度提升。

Result: 在密集300W测试集及原始稀疏300W、WFLW测试集上均达到最优精度，且无需额外成本。

Conclusion: 该框架能有效提升面部标记密度，适用于多种场景，且兼容现有网络。

Abstract: Recent years have witnessed significant growth of face alignment. Though
dense facial landmark is highly demanded in various scenarios, e.g., cosmetic
medicine and facial beautification, most works only consider sparse face
alignment. To address this problem, we present a framework that can enrich
landmark density by existing sparse landmark datasets, e.g., 300W with 68
points and WFLW with 98 points. Firstly, we observe that the local patches
along each semantic contour are highly similar in appearance. Then, we propose
a weakly-supervised idea of learning the refinement ability on original sparse
landmarks and adapting this ability to enriched dense landmarks. Meanwhile,
several operators are devised and organized together to implement the idea.
Finally, the trained model is applied as a plug-and-play module to the existing
face alignment networks. To evaluate our method, we manually label the dense
landmarks on 300W testset. Our method yields state-of-the-art accuracy not only
in newly-constructed dense 300W testset but also in the original sparse 300W
and WFLW testsets without additional cost.

</details>


### [8] [Asymmetric Dual Self-Distillation for 3D Self-Supervised Representation Learning](https://arxiv.org/abs/2506.21724)
*Remco F. Leijenaar,Hamidreza Kasaei*

Main category: cs.CV

TL;DR: AsymDSD是一种自监督学习框架，通过潜在空间预测结合掩码建模和不变性学习，显著提升了3D点云语义表示的性能。


<details>
  <summary>Details</summary>
Motivation: 解决无监督3D点云学习中缺乏大规模标注数据的问题，并改进传统掩码点建模在捕获高级语义上的局限性。

Method: 提出AsymDSD框架，采用非对称双自蒸馏方法，结合潜在空间预测、多掩码采样和多裁剪技术。

Result: 在ScanObjectNN上达到90.53%的准确率，预训练后提升至93.72%，优于现有方法。

Conclusion: AsymDSD通过创新设计显著提升了3D点云语义表示的性能，为无监督学习提供了新思路。

Abstract: Learning semantically meaningful representations from unstructured 3D point
clouds remains a central challenge in computer vision, especially in the
absence of large-scale labeled datasets. While masked point modeling (MPM) is
widely used in self-supervised 3D learning, its reconstruction-based objective
can limit its ability to capture high-level semantics. We propose AsymDSD, an
Asymmetric Dual Self-Distillation framework that unifies masked modeling and
invariance learning through prediction in the latent space rather than the
input space. AsymDSD builds on a joint embedding architecture and introduces
several key design choices: an efficient asymmetric setup, disabling attention
between masked queries to prevent shape leakage, multi-mask sampling, and a
point cloud adaptation of multi-crop. AsymDSD achieves state-of-the-art results
on ScanObjectNN (90.53%) and further improves to 93.72% when pretrained on 930k
shapes, surpassing prior methods.

</details>


### [9] [Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis](https://arxiv.org/abs/2506.21731)
*Chenqiu Zhao,Anup Basu*

Main category: cs.CV

TL;DR: 论文提出MESP和LCH两个理论框架，探讨概率生成模型的局限性，即学习全局分布导致记忆而非生成行为。MESP源自对VAE的重新思考，提出基于重叠系数的下界。BL-AE和ARVM模型在实验中表现优异，但FID分数反映记忆问题。LCH假设生成能力源于局部相关性。


<details>
  <summary>Details</summary>
Motivation: 探讨概率生成模型因学习全局分布而导致的记忆问题，而非真正的生成行为。

Method: 提出MESP框架，基于VAE的重叠问题提出下界；设计BL-AE编码二进制潜在表示，结合ARVM模型输出直方图；提出LCH假设，强调局部相关性对生成能力的影响。

Result: ARVM在标准数据集上取得竞争性FID分数，但分数反映记忆问题；LCH假设通过实验验证局部相关性的重要性。

Conclusion: MESP和LCH揭示了概率生成模型的局限性，提出局部相关性是生成能力的关键，为未来研究提供新方向。

Abstract: We propose two theoretical frameworks, the Mutually Exclusive Probability
Space (MESP) and the Local Correlation Hypothesis (LCH), to explore a potential
limitation in probabilistic generative models; namely that learning global
distributions leads to memorization rather than generative behavior. MESP
emerges from our rethinking of the Variational Autoencoder (VAE). We observe
that latent variable distributions in VAE exhibit overlap, which leads to an
optimization conflict between the reconstruction loss and KL-divergence loss. A
lower bound based on the overlap coefficient is proposed. We refer to this
phenomenon as Mutually Exclusive Probability Spaces. Based on MESP, a Binary
Latent Autoencoder (BL-AE) is proposed to encode images into binary latent
representations. These binary latents are used as the input to our
Autoregressive Random Variable Model (ARVM), a modified autoregressive model
outputting histograms. Our ARVM achieves competitive FID scores, outperforming
state-of-the-art methods on standard datasets. However, such scores reflect
memorization rather than generation. To address this issue, we propose the
Local Correlation Hypothesis (LCH), which posits that generative capability
arising from local correlations among latent variables. Comprehensive
experiments and discussions are conducted to validate our frameworks.

</details>


### [10] [Equitable Federated Learning with NCA](https://arxiv.org/abs/2506.21735)
*Nick Lemke,Mirko Konstantin,Henry John Krumb,John Kalkhof,Jonathan Stieber,Anirban Mukhopadhyay*

Main category: cs.CV

TL;DR: FedNCA是一种针对医疗图像分割任务的联邦学习系统，适用于资源有限的地区，解决了计算资源和网络连接问题。


<details>
  <summary>Details</summary>
Motivation: 在低收入和中等收入国家（LMICs），医疗资源有限，联邦学习（FL）可以促进协作模型训练，但面临计算资源和网络连接障碍。

Method: FedNCA采用轻量级Med-NCA架构，支持低成本边缘设备（如智能手机）训练，并减少通信成本，同时具备加密能力。

Result: FedNCA成功解决了基础设施和安全挑战，为资源受限地区提供了高效、轻量级且加密就绪的医疗成像解决方案。

Conclusion: FedNCA为资源受限地区实现公平医疗进步提供了可行路径。

Abstract: Federated Learning (FL) is enabling collaborative model training across
institutions without sharing sensitive patient data. This approach is
particularly valuable in low- and middle-income countries (LMICs), where access
to trained medical professionals is limited. However, FL adoption in LMICs
faces significant barriers, including limited high-performance computing
resources and unreliable internet connectivity. To address these challenges, we
introduce FedNCA, a novel FL system tailored for medical image segmentation
tasks. FedNCA leverages the lightweight Med-NCA architecture, enabling training
on low-cost edge devices, such as widely available smartphones, while
minimizing communication costs. Additionally, our encryption-ready FedNCA
proves to be suitable for compromised network communication. By overcoming
infrastructural and security challenges, FedNCA paves the way for inclusive,
efficient, lightweight, and encryption-ready medical imaging solutions,
fostering equitable healthcare advancements in resource-constrained regions.

</details>


### [11] [ImplicitQA: Going beyond frames towards Implicit Video Reasoning](https://arxiv.org/abs/2506.21742)
*Sirnam Swetha,Rohit Gupta,Parth Parag Kulkarni,David G Shatwell,Jeffrey A Chan Santiago,Nyle Siddiqui,Joseph Fioresi,Mubarak Shah*

Main category: cs.CV

TL;DR: 论文提出了ImplicitQA基准，用于测试视频问答系统在隐含推理能力上的表现，填补了现有基准的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答基准主要关注显性视觉内容，忽略了隐含推理能力，而人类擅长此类推理。

Method: 构建了包含1K QA对的ImplicitQA基准，涵盖多种推理维度，并对领先模型进行评估。

Result: 评估显示现有模型在隐含推理任务上表现不佳，依赖表面视觉线索。

Conclusion: ImplicitQA为社区提供了新挑战，旨在推动隐含推理研究。

Abstract: Video QA has made significant strides by leveraging multimodal learning to
align visual and textual modalities. However, current benchmarks overwhelmingly
focus on questions answerable through explicit visual content - actions,
objects & events directly observable within individual frames or short clips.
In contrast, creative and cinematic videos - such as movies, TV shows, and
narrative-driven content - employ storytelling techniques that deliberately
omit certain depictions, requiring viewers to infer motives, causality, and
relationships across discontinuous frames. Humans naturally excel at such
implicit reasoning, seamlessly integrating information across time and context
to construct coherent narratives. Current VideoQA systems and benchmarks fail
to capture this essential dimension of human-like understanding. To bridge this
gap, we present ImplicitQA, a novel benchmark specifically designed to test
models on implicit reasoning. It comprises 1K meticulously annotated QA pairs
derived from 320+ high-quality creative video clips, systematically categorized
into key reasoning dimensions: lateral and vertical spatial reasoning, depth
and proximity, viewpoint and visibility, motion and trajectory, causal and
motivational reasoning, social interactions, physical context, and inferred
counting. These annotations are deliberately challenging, crafted by authors
ensuring high-quality. Our extensive evaluations on leading VideoQA models
reveals performance degradation, underscoring their reliance on surface-level
visual cues and highlighting the difficulty of implicit reasoning. Performance
variations across models further illustrate the complexity and diversity of the
challenges presented by ImplicitQA. By releasing both the dataset and our data
collection framework, we aim to stimulate further research and development in
the community. https://huggingface.co/datasets/ucf-crcv/ImplicitQA.

</details>


### [12] [Early Glaucoma Detection using Deep Learning with Multiple Datasets of Fundus Images](https://arxiv.org/abs/2506.21770)
*Rishiraj Paul Chowdhury,Nirmit Shekar Karkera*

Main category: cs.CV

TL;DR: 提出了一种基于EfficientNet-B0的深度学习流程，用于从视网膜眼底图像中检测青光眼，通过多数据集训练提升泛化能力，实验显示简单预处理效果更优。


<details>
  <summary>Details</summary>
Motivation: 青光眼是不可逆失明的主要原因，早期检测对治疗效果至关重要，但传统方法通常具有侵入性或需要专业设备。

Method: 使用EfficientNet-B0架构，依次在ACRIMA、ORIGA和RIM-ONE数据集上训练和微调模型，采用简单预处理。

Result: 模型在未见数据集上表现出强判别性能，简单预处理比复杂增强方法获得更高的AUC-ROC。

Conclusion: 该流程为青光眼早期检测提供了可重复且可扩展的方法，具有潜在临床价值。

Abstract: Glaucoma is a leading cause of irreversible blindness, but early detection
can significantly improve treatment outcomes. Traditional diagnostic methods
are often invasive and require specialized equipment. In this work, we present
a deep learning pipeline using the EfficientNet-B0 architecture for glaucoma
detection from retinal fundus images. Unlike prior studies that rely on single
datasets, we sequentially train and fine-tune our model across ACRIMA, ORIGA,
and RIM-ONE datasets to enhance generalization. Our experiments show that
minimal preprocessing yields higher AUC-ROC compared to more complex
enhancements, and our model demonstrates strong discriminative performance on
unseen datasets. The proposed pipeline offers a reproducible and scalable
approach to early glaucoma detection, supporting its potential clinical
utility.

</details>


### [13] [Comparing Learning Paradigms for Egocentric Video Summarization](https://arxiv.org/abs/2506.21785)
*Daniel Wen*

Main category: cs.CV

TL;DR: 研究比较了监督学习、无监督学习和提示微调在自我中心视频理解中的表现，发现GPT-4o优于专用模型，但现有方法对第一人称视频的适应性仍需改进。


<details>
  <summary>Details</summary>
Motivation: 探索计算机视觉技术在自我中心视频中的应用，推动该领域的发展。

Method: 评估了Shotluck Holmes（监督学习）、TAC-SUM（无监督学习）和GPT-4o（提示微调）在视频摘要任务中的表现。

Result: GPT-4o表现优于专用模型，但现有方法对第一人称视频的效果较差。

Conclusion: 需进一步改进模型以适应自我中心视频的独特挑战，提示微调方法展现了潜力。

Abstract: In this study, we investigate various computer vision paradigms - supervised
learning, unsupervised learning, and prompt fine-tuning - by assessing their
ability to understand and interpret egocentric video data. Specifically, we
examine Shotluck Holmes (state-of-the-art supervised learning), TAC-SUM
(state-of-the-art unsupervised learning), and GPT-4o (a prompt fine-tuned
pre-trained model), evaluating their effectiveness in video summarization. Our
results demonstrate that current state-of-the-art models perform less
effectively on first-person videos compared to third-person videos,
highlighting the need for further advancements in the egocentric video domain.
Notably, a prompt fine-tuned general-purpose GPT-4o model outperforms these
specialized models, emphasizing the limitations of existing approaches in
adapting to the unique challenges of first-person perspectives. Although our
evaluation is conducted on a small subset of egocentric videos from the
Ego-Exo4D dataset due to resource constraints, the primary objective of this
research is to provide a comprehensive proof-of-concept analysis aimed at
advancing the application of computer vision techniques to first-person videos.
By exploring novel methodologies and evaluating their potential, we aim to
contribute to the ongoing development of models capable of effectively
processing and interpreting egocentric perspectives.

</details>


### [14] [End-to-End RGB-IR Joint Image Compression With Channel-wise Cross-modality Entropy Model](https://arxiv.org/abs/2506.21851)
*Haofeng Wang,Fangtao Zhou,Qi Zhang,Zeyuan Chen,Enci Zhang,Zhao Wang,Xiaofeng Huang,Siwei Ma*

Main category: cs.CV

TL;DR: 提出了一种用于RGB-IR图像对的联合压缩框架，通过跨模态熵模型（CCEM）提升压缩效率。


<details>
  <summary>Details</summary>
Motivation: 随着RGB-IR图像对在多模态应用中的广泛使用，数据存储和传输成本成倍增加，因此需要高效的压缩方法。

Method: 设计了CCEM模型，包含低频上下文提取块（LCEB）和低频上下文融合块（LCFB），以利用跨模态先验信息进行精确的概率建模。

Result: 在LLVIP和KAIST数据集上，该方法优于现有RGB-IR和单模态压缩方法，例如在LLVIP数据集上实现了23.1%的比特率节省。

Conclusion: 该框架通过跨模态信息的高效利用，显著提升了RGB-IR图像对的压缩性能。

Abstract: RGB-IR(RGB-Infrared) image pairs are frequently applied simultaneously in
various applications like intelligent surveillance. However, as the number of
modalities increases, the required data storage and transmission costs also
double. Therefore, efficient RGB-IR data compression is essential. This work
proposes a joint compression framework for RGB-IR image pair. Specifically, to
fully utilize cross-modality prior information for accurate context probability
modeling within and between modalities, we propose a Channel-wise
Cross-modality Entropy Model (CCEM). Among CCEM, a Low-frequency Context
Extraction Block (LCEB) and a Low-frequency Context Fusion Block (LCFB) are
designed for extracting and aggregating the global low-frequency information
from both modalities, which assist the model in predicting entropy parameters
more accurately. Experimental results demonstrate that our approach outperforms
existing RGB-IR image pair and single-modality compression methods on LLVIP and
KAIST datasets. For instance, the proposed framework achieves a 23.1% bit rate
saving on LLVIP dataset compared to the state-of-the-art RGB-IR image codec
presented at CVPR 2022.

</details>


### [15] [CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery](https://arxiv.org/abs/2506.21813)
*Felix Holm,Gözde Ünver,Ghazal Ghazaei,Nassir Navab*

Main category: cs.CV

TL;DR: 论文提出了首个白内障手术场景图数据集（CAT-SG），用于建模手术工具、解剖结构和技术的复杂交互，并提出了优于现有方法的场景图生成模型CatSGG。


<details>
  <summary>Details</summary>
Motivation: 现有数据集仅关注手术分析的孤立方面（如工具检测或阶段分割），缺乏对实体间语义关系和时间依赖性的全面建模。

Method: 引入CAT-SG数据集，提供工具-组织交互、程序变化和时间依赖的结构化标注；提出CatSGG模型生成结构化手术表示。

Result: CAT-SG提供了手术工作流的整体视图，CatSGG在生成结构化表示方面优于现有方法。

Conclusion: CAT-SG和CatSGG为AI驱动的临床实践（如手术培训和实时决策支持）提供了更智能、上下文感知的基础。

Abstract: Understanding the intricate workflows of cataract surgery requires modeling
complex interactions between surgical tools, anatomical structures, and
procedural techniques. Existing datasets primarily address isolated aspects of
surgical analysis, such as tool detection or phase segmentation, but lack
comprehensive representations that capture the semantic relationships between
entities over time. This paper introduces the Cataract Surgery Scene Graph
(CAT-SG) dataset, the first to provide structured annotations of tool-tissue
interactions, procedural variations, and temporal dependencies. By
incorporating detailed semantic relations, CAT-SG offers a holistic view of
surgical workflows, enabling more accurate recognition of surgical phases and
techniques. Additionally, we present a novel scene graph generation model,
CatSGG, which outperforms current methods in generating structured surgical
representations. The CAT-SG dataset is designed to enhance AI-driven surgical
training, real-time decision support, and workflow analysis, paving the way for
more intelligent, context-aware systems in clinical practice.

</details>


### [16] [Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models](https://arxiv.org/abs/2506.21826)
*Rafael Sterzinger,Marco Peer,Robert Sablatnig*

Main category: cs.CV

TL;DR: 提出了一种基于大型视觉基础模型和参数高效微调的少样本历史地图分割方法，显著提升了性能并减少了对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 历史地图的多样性和标注数据稀缺性给自动化处理带来挑战，需要一种高效的少样本分割方法。

Method: 结合大型视觉基础模型的语义嵌入和参数高效微调，实现少样本分割。

Result: 在Siegfried数据集上，葡萄园和铁路分割的mIoU分别提升5%和13%；在ICDAR 2021数据集上，建筑块分割的PQ达到67.3%。

Conclusion: 该方法在极低数据量下仍保持高性能，显著减少人工标注需求，推动了历史地图的自动化处理。

Abstract: As rich sources of history, maps provide crucial insights into historical
changes, yet their diverse visual representations and limited annotated data
pose significant challenges for automated processing. We propose a simple yet
effective approach for few-shot segmentation of historical maps, leveraging the
rich semantic embeddings of large vision foundation models combined with
parameter-efficient fine-tuning. Our method outperforms the state-of-the-art on
the Siegfried benchmark dataset in vineyard and railway segmentation, achieving
+5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20%
in the more challenging 5-shot setting. Additionally, it demonstrates strong
performance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3%
for building block segmentation, despite not being optimized for this
shape-sensitive metric, underscoring its generalizability. Notably, our
approach maintains high performance even in extremely low-data regimes (10- &
5-shot), while requiring only 689k trainable parameters - just 0.21% of the
total model size. Our approach enables precise segmentation of diverse
historical maps while drastically reducing the need for manual annotations,
advancing automated processing and analysis in the field. Our implementation is
publicly available at:
https://github.com/RafaelSterzinger/few-shot-map-segmentation.

</details>


### [17] [ReF-LLE: Personalized Low-Light Enhancement via Reference-Guided Deep Reinforcement Learning](https://arxiv.org/abs/2506.22216)
*Ming Zhao,Pingping Liu,Tongshun Zhang,Zhe Zhang*

Main category: cs.CV

TL;DR: ReF-LLE是一种基于傅里叶频域和深度强化学习的个性化低光图像增强方法，通过零参考图像评估策略和自适应迭代策略，显著提升了增强效果。


<details>
  <summary>Details</summary>
Motivation: 解决低光图像增强中因条件差异和主观偏好导致的挑战。

Method: 在傅里叶频域中结合深度强化学习，引入零参考图像评估策略和个性化自适应迭代策略。

Result: 在基准数据集上表现优于现有方法，具有更高的感知质量和个性化适应性。

Conclusion: ReF-LLE为低光图像增强提供了一种高效且个性化的解决方案。

Abstract: Low-light image enhancement presents two primary challenges: 1) Significant
variations in low-light images across different conditions, and 2) Enhancement
levels influenced by subjective preferences and user intent. To address these
issues, we propose ReF-LLE, a novel personalized low-light image enhancement
method that operates in the Fourier frequency domain and incorporates deep
reinforcement learning. ReF-LLE is the first to integrate deep reinforcement
learning into this domain. During training, a zero-reference image evaluation
strategy is introduced to score enhanced images, providing reward signals that
guide the model to handle varying degrees of low-light conditions effectively.
In the inference phase, ReF-LLE employs a personalized adaptive iterative
strategy, guided by the zero-frequency component in the Fourier domain, which
represents the overall illumination level. This strategy enables the model to
adaptively adjust low-light images to align with the illumination distribution
of a user-provided reference image, ensuring personalized enhancement results.
Extensive experiments on benchmark datasets demonstrate that ReF-LLE
outperforms state-of-the-art methods, achieving superior perceptual quality and
adaptability in personalized low-light image enhancement.

</details>


### [18] [TaleForge: Interactive Multimodal System for Personalized Story Creation](https://arxiv.org/abs/2506.21832)
*Minh-Loi Nguyen,Quang-Khai Le,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: TaleForge是一个结合大型语言模型和文本到图像扩散技术的个性化故事生成系统，通过将用户的面部图像嵌入叙事和插图中，提升沉浸感和参与度。


<details>
  <summary>Details</summary>
Motivation: 现有方法将用户视为被动消费者，提供有限个性化的通用情节，降低了参与感和沉浸感。

Method: TaleForge包含三个模块：故事生成（LLMs根据用户提示创建叙事和角色描述）、个性化图像生成（将用户面部和服装融入角色插图）、背景生成（创建包含个性化角色的场景背景）。

Result: 用户研究表明，当用户作为主角时，参与感和归属感显著提升。参与者赞赏系统的实时预览和直观控制，但希望有更精细的叙事编辑工具。

Conclusion: TaleForge通过个性化文本和图像的结合，推动了多模态故事叙述的发展，创造了沉浸式、以用户为中心的体验。

Abstract: Storytelling is a deeply personal and creative process, yet existing methods
often treat users as passive consumers, offering generic plots with limited
personalization. This undermines engagement and immersion, especially where
individual style or appearance is crucial. We introduce TaleForge, a
personalized story-generation system that integrates large language models
(LLMs) and text-to-image diffusion to embed users' facial images within both
narratives and illustrations. TaleForge features three interconnected modules:
Story Generation, where LLMs create narratives and character descriptions from
user prompts; Personalized Image Generation, merging users' faces and outfit
choices into character illustrations; and Background Generation, creating scene
backdrops that incorporate personalized characters. A user study demonstrated
heightened engagement and ownership when individuals appeared as protagonists.
Participants praised the system's real-time previews and intuitive controls,
though they requested finer narrative editing tools. TaleForge advances
multimodal storytelling by aligning personalized text and imagery to create
immersive, user-centric experiences.

</details>


### [19] [PrefPaint: Enhancing Image Inpainting through Expert Human Feedback](https://arxiv.org/abs/2506.21834)
*Duy-Bao Bui,Hoang-Khang Nguyen,Trung-Nghia Le*

Main category: cs.CV

TL;DR: PrefPaint通过将人类反馈整合到Stable Diffusion Inpainting训练中，提高了医学图像修复的准确性和可靠性，尤其在息肉图像生成方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 医学图像修复在专业领域（如息肉成像）中需要高准确性，但现有模型可能生成不准确的图像，影响诊断和治疗。

Method: 提出PrefPaint方法，结合人类反馈优化Stable Diffusion Inpainting训练，无需昂贵奖励模型，并开发了用户友好的Web界面。

Result: PrefPaint在多个领域优于现有方法，减少了视觉不一致性，尤其在医学图像中生成更真实的息肉图像。

Conclusion: PrefPaint通过人类反馈和交互式界面显著提升了医学图像修复的可靠性和实用性。

Abstract: Inpainting, the process of filling missing or corrupted image parts, has
broad applications, including medical imaging. However, in specialized fields
like medical polyps imaging, where accuracy and reliability are critical,
inpainting models can generate inaccurate images, leading to significant errors
in medical diagnosis and treatment. To ensure reliability, medical images
should be annotated by experts like oncologists for effective model training.
We propose PrefPaint, an approach that incorporates human feedback into the
training process of Stable Diffusion Inpainting, bypassing the need for
computationally expensive reward models. In addition, we develop a web-based
interface streamlines training, fine-tuning, and inference. This interactive
interface provides a smooth and intuitive user experience, making it easier to
offer feedback and manage the fine-tuning process. User study on various
domains shows that PrefPaint outperforms existing methods, reducing visual
inconsistencies and improving image rendering, particularly in medical
contexts, where our model generates more realistic polyps images.

</details>


### [20] [ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts](https://arxiv.org/abs/2506.21835)
*Xiaoqi Wang,Clint Sebastian,Wenbin He,Liu Ren*

Main category: cs.CV

TL;DR: ProSAM提出了一种改进的视觉参考分割方法，通过变分提示编码器预测多变量提示分布，解决了现有SAM方法在边界生成提示的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 现有SAM方法在视觉参考分割中因提示编码器不理想导致提示生成在物体边界，引发不稳定性和鲁棒性下降。

Method: ProSAM通过学习变分提示编码器预测多变量提示分布，避免在边界生成提示。

Result: 在Pascal-5$^i$和COCO-20$^i$数据集上表现优于现有方法。

Conclusion: ProSAM为视觉参考分割提供了更稳定和鲁棒的解决方案。

Abstract: The recent advancements in large foundation models have driven the success of
open-set image segmentation, a task focused on segmenting objects beyond
predefined categories. Among various prompt types (such as points, boxes,
texts, and visual references), visual reference segmentation stands out for its
unique flexibility and strong zero-shot capabilities. Recently, several
SAM-based methods have made notable progress in this task by automatically
generating prompts to guide SAM. However, these methods often generate prompts
at object boundaries due to suboptimal prompt encoder, which results in
instability and reduced robustness. In this work, we introduce ProSAM, a simple
but effective method to address the stability challenges we identified in
existing SAM-based visual reference segmentation approaches. By learning a
variational prompt encoder to predict multivariate prompt distributions, ProSAM
avoids generating prompts that lie in unstable regions, overcoming the
instability caused by less robust prompts. Our approach consistently surpasses
state-of-the-art methods on the Pascal-5$^i$ and COCO-20$^i$ datasets,
providing a more robust solution for visual reference segmentation.

</details>


### [21] [GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles](https://arxiv.org/abs/2506.21839)
*Mengyi Shan,Brian Curless,Ira Kemelmacher-Shlizerman,Steve Seitz*

Main category: cs.CV

TL;DR: 提出了一种分层多智能体框架，用于生成视觉吸引、逻辑严密且具有挑战性的逃脱房间谜题图像，解决了基础图像模型在空间关系和功能推理上的不足。


<details>
  <summary>Details</summary>
Motivation: 挑战文本到图像模型生成逃脱房间谜题图像的能力，要求图像不仅视觉吸引，还需逻辑严密和智力刺激。

Method: 采用分层多智能体框架，将任务分解为功能设计、符号场景图推理、布局合成和局部图像编辑四个阶段，通过智能体协作和迭代反馈确保场景的视觉连贯性和功能可解性。

Result: 实验表明，智能体协作显著提高了输出质量，包括可解性、避免捷径和功能清晰度，同时保持了视觉质量。

Conclusion: 该框架有效解决了基础模型在复杂场景生成中的局限性，为文本到图像生成任务提供了新的解决方案。

Abstract: We challenge text-to-image models with generating escape room puzzle images
that are visually appealing, logically solid, and intellectually stimulating.
While base image models struggle with spatial relationships and affordance
reasoning, we propose a hierarchical multi-agent framework that decomposes this
task into structured stages: functional design, symbolic scene graph reasoning,
layout synthesis, and local image editing. Specialized agents collaborate
through iterative feedback to ensure the scene is visually coherent and
functionally solvable. Experiments show that agent collaboration improves
output quality in terms of solvability, shortcut avoidance, and affordance
clarity, while maintaining visual quality.

</details>


### [22] [3D-Telepathy: Reconstructing 3D Objects from EEG Signals](https://arxiv.org/abs/2506.21843)
*Yuxiang Ge,Jionghao Cheng,Ruiquan Ge,Zhaojie Fang,Gangyong Jia,Xiang Wan,Nannan Li,Ahmed Elazab,Changmiao Wang*

Main category: cs.CV

TL;DR: 本文提出了一种创新的EEG编码器架构，结合双自注意力机制，通过混合训练策略和变分评分蒸馏技术，成功从EEG数据生成3D物体。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅将EEG数据转换为2D图像，忽略了3D空间信息的重建，限制了BCI的实际应用。本文旨在解决这一局限，利用EEG中的丰富空间信息重建3D物体。

Method: 提出双自注意力机制的EEG编码器架构，采用混合训练策略（包括交叉注意力、对比学习和自监督学习），并利用稳定扩散和变分评分蒸馏训练神经辐射场。

Result: 成功从EEG数据生成具有相似内容和结构的3D物体。

Conclusion: 该方法为EEG数据到3D物体的重建提供了新思路，扩展了BCI的应用潜力。

Abstract: Reconstructing 3D visual stimuli from Electroencephalography (EEG) data holds
significant potential for applications in Brain-Computer Interfaces (BCIs) and
aiding individuals with communication disorders. Traditionally, efforts have
focused on converting brain activity into 2D images, neglecting the translation
of EEG data into 3D objects. This limitation is noteworthy, as the human brain
inherently processes three-dimensional spatial information regardless of
whether observing 2D images or the real world. The neural activities captured
by EEG contain rich spatial information that is inevitably lost when
reconstructing only 2D images, thus limiting its practical applications in BCI.
The transition from EEG data to 3D object reconstruction faces considerable
obstacles. These include the presence of extensive noise within EEG signals and
a scarcity of datasets that include both EEG and 3D information, which
complicates the extraction process of 3D visual data. Addressing this
challenging task, we propose an innovative EEG encoder architecture that
integrates a dual self-attention mechanism. We use a hybrid training strategy
to train the EEG Encoder, which includes cross-attention, contrastive learning,
and self-supervised learning techniques. Additionally, by employing stable
diffusion as a prior distribution and utilizing Variational Score Distillation
to train a neural radiation field, we successfully generate 3D objects with
similar content and structure from EEG data.

</details>


### [23] [Periodic-MAE: Periodic Video Masked Autoencoder for rPPG Estimation](https://arxiv.org/abs/2506.21855)
*Jiho Choi,Sang Jun Lee*

Main category: cs.CV

TL;DR: 提出一种通过自监督学习从无标签面部视频中学习周期性信号表示的方法，用于远程光电容积描记术（rPPG）估计。


<details>
  <summary>Details</summary>
Motivation: 捕捉面部视频中皮肤色调的细微周期性变化，以改进rPPG估计的准确性。

Method: 使用视频掩码自编码器学习高维时空表示，结合帧掩码和生理带宽限制约束。

Result: 在多个数据集上表现优异，尤其在跨数据集评估中显著提升性能。

Conclusion: 该方法有效提升了rPPG任务的性能，代码已开源。

Abstract: In this paper, we propose a method that learns a general representation of
periodic signals from unlabeled facial videos by capturing subtle changes in
skin tone over time. The proposed framework employs the video masked
autoencoder to learn a high-dimensional spatio-temporal representation of the
facial region through self-supervised learning. Capturing quasi-periodic
signals in the video is crucial for remote photoplethysmography (rPPG)
estimation. To account for signal periodicity, we apply frame masking in terms
of video sampling, which allows the model to capture resampled quasi-periodic
signals during the pre-training stage. Moreover, the framework incorporates
physiological bandlimit constraints, leveraging the property that physiological
signals are sparse within their frequency bandwidth to provide pulse cues to
the model. The pre-trained encoder is then transferred to the rPPG task, where
it is used to extract physiological signals from facial videos. We evaluate the
proposed method through extensive experiments on the PURE, UBFC-rPPG, MMPD, and
V4V datasets. Our results demonstrate significant performance improvements,
particularly in challenging cross-dataset evaluations. Our code is available at
https://github.com/ziiho08/Periodic-MAE.

</details>


### [24] [SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space](https://arxiv.org/abs/2506.21857)
*Ekaterina Redekop,Mara Pleasure,Zichen Wang,Kimberly Flores,Anthony Sisk,William Speier,Corey W. Arnold*

Main category: cs.CV

TL;DR: SPADE是一种基础模型，整合了组织病理学和空间转录组学数据，通过对比学习在统一框架内学习图像表示，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 数字病理学和自监督深度学习的快速发展为多种疾病任务的基础模型开发提供了可能，但现有方法在整合全切片图像（WSI）和空间转录组学（ST）方面存在不足，无法充分捕捉分子异质性。

Method: SPADE采用混合数据专家技术，通过两阶段特征空间聚类创建专家，利用对比学习学习共配准的WSI斑块和基因表达谱的表示。

Result: 在HEST-1k数据集上预训练后，SPADE在14个下游任务中表现出显著优于基线模型的少样本性能。

Conclusion: SPADE通过整合形态学和分子信息到单一潜在空间，展示了其在病理学任务中的优势。

Abstract: The rapid growth of digital pathology and advances in self-supervised deep
learning have enabled the development of foundational models for various
pathology tasks across diverse diseases. While multimodal approaches
integrating diverse data sources have emerged, a critical gap remains in the
comprehensive integration of whole-slide images (WSIs) with spatial
transcriptomics (ST), which is crucial for capturing critical molecular
heterogeneity beyond standard hematoxylin & eosin (H&E) staining. We introduce
SPADE, a foundation model that integrates histopathology with ST data to guide
image representation learning within a unified framework, in effect creating an
ST-informed latent space. SPADE leverages a mixture-of-data experts technique,
where experts, created via two-stage feature-space clustering, use contrastive
learning to learn representations of co-registered WSI patches and gene
expression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is
evaluated on 14 downstream tasks, demonstrating significantly superior few-shot
performance compared to baseline models, highlighting the benefits of
integrating morphological and molecular information into one latent space.

</details>


### [25] [LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs](https://arxiv.org/abs/2506.21862)
*Boyuan Sun,Jiaxing Zhao,Xihan Wei,Qibin Hou*

Main category: cs.CV

TL;DR: LLaVA-Scissor是一种无需训练的令牌压缩策略，用于视频多模态大语言模型，通过语义连通组件（SCC）实现全面的语义覆盖，显著优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法基于注意力分数压缩令牌，但无法有效捕捉所有语义区域且导致冗余。

Method: 采用语义连通组件（SCC）将令牌分配到不同的语义区域，提出两步时空令牌压缩策略。

Result: 在多种视频理解基准测试中表现优异，尤其在低令牌保留率下。

Conclusion: LLaVA-Scissor通过SCC实现了高效的令牌压缩，显著提升了视频理解性能。

Abstract: In this paper, we present LLaVA-Scissor, a training-free token compression
strategy designed for video multimodal large language models. Previous methods
mostly attempt to compress tokens based on attention scores, but fail to
effectively capture all semantic regions and often lead to token redundancy.
Differently, we propose to leverage the Semantic Connected Components (SCC)
approach that assigns tokens to distinct semantic regions within the token set,
ensuring comprehensive semantic coverage. The outcome is a two-step
spatio-temporal token compression strategy that utilizes SCC in both spatial
and temporal domains. This strategy can effectively compress tokens by
representing the entire video with a set of non-overlapping semantic tokens. We
conduct extensive evaluations of the token compression capabilities of
LLaVA-Scissor across diverse video understanding benchmarks, including video
question answering, long video understanding, and comprehensive multi-choices
benchmarks. Experimental results show that the proposed LLaVA-Scissor
outperforms other token compression methods, achieving superior performance in
various video understanding benchmarks, particularly at low token retention
ratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.

</details>


### [26] [Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling](https://arxiv.org/abs/2506.21863)
*Sungjune Park,Yeongyun Kim,Se Yeon Kim,Yong Man Ro*

Main category: cs.CV

TL;DR: 提出了一种针对遥感图像的大视觉语言模型框架，通过语义增强的多级对齐和语义感知专家建模，解决了现有模型在遥感领域的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型在自然图像领域表现优异，但由于遥感图像在视觉外观、对象尺度和语义上的显著差异，直接应用效果有限。

Method: 框架包含两个核心组件：1) 基于检索的语义增强模块，用于对齐多级视觉特征；2) 语义感知专家建模，分层处理不同级别的语义表示。

Result: 在多个遥感任务（如场景分类和视觉问答）中表现优异，实现了跨多语义级别的一致改进。

Conclusion: 该框架有效弥合了通用大视觉语言模型与遥感领域独特需求之间的差距。

Abstract: Large Vision and Language Models (LVLMs) have shown strong performance across
various vision-language tasks in natural image domains. However, their
application to remote sensing (RS) remains underexplored due to significant
domain differences in visual appearances, object scales, and semantics. These
discrepancies hider the effective understanding of RS scenes, which contain
rich, multi-level semantic information spanning from coarse-to-fine levels.
Hence, it limits the direct adaptation of existing LVLMs to RS imagery. To
address this gap, we propose a novel LVLM framework tailored for RS
understanding, incorporating two core components: Semantic-augmented
Multi-level Alignment and Semantic-aware Expert Modeling. First, to align
multi-level visual features, we introduce the retrieval-based Semantic
Augmentation Module which enriches the visual features with relevant semantics
across fine-to-coarse levels (e.g., object- and scene-level information). It is
designed to retrieve relevant semantic cues from a RS semantic knowledge
database, followed by aggregation of semantic cues with user query and
multi-level visual features, resulting in semantically enriched representation
across multiple levels. Second, for Semantic-aware Expert Modeling, we design
semantic experts, where each expert is responsible for processing semantic
representation at different levels separately. This enables hierarchical
semantic understanding from coarse to fine levels. Evaluations across multiple
RS tasks-including scene classification and VQA, etc.-demonstrate that the
proposed framework achieves consistent improvements across multiple semantic
levels. This highlights its capability and effectiveness in bridging the gap
between general LVLMs and unique demands of RS-specific vision-language
understanding.

</details>


### [27] [Dual-Perspective United Transformer for Object Segmentation in Optical Remote Sensing Images](https://arxiv.org/abs/2506.21866)
*Yanguang Sun,Jiexi Yan,Jianjun Qian,Chunyan Xu,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: 论文提出了一种新型的双视角统一Transformer（DPU-Former），用于光学遥感图像（ORSIs）的自动分割，解决了现有模型在特征融合和复杂度方面的问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型主要基于卷积或Transformer特征，各有优势但融合困难，导致分割效果不佳。

Method: 设计了全局-局部混合注意力机制和傅里叶空间融合策略，并引入门控线性前馈网络增强表达能力，构建了DPU-Former解码器。

Result: DPU-Former在多个数据集上优于现有最优方法。

Conclusion: DPU-Former通过有效融合长程依赖和空间细节，显著提升了分割性能。

Abstract: Automatically segmenting objects from optical remote sensing images (ORSIs)
is an important task. Most existing models are primarily based on either
convolutional or Transformer features, each offering distinct advantages.
Exploiting both advantages is valuable research, but it presents several
challenges, including the heterogeneity between the two types of features, high
complexity, and large parameters of the model. However, these issues are often
overlooked in existing the ORSIs methods, causing sub-optimal segmentation. For
that, we propose a novel Dual-Perspective United Transformer (DPU-Former) with
a unique structure designed to simultaneously integrate long-range dependencies
and spatial details. In particular, we design the global-local mixed attention,
which captures diverse information through two perspectives and introduces a
Fourier-space merging strategy to obviate deviations for efficient fusion.
Furthermore, we present a gated linear feed-forward network to increase the
expressive ability. Additionally, we construct a DPU-Former decoder to
aggregate and strength features at different layers. Consequently, the
DPU-Former model outperforms the state-of-the-art methods on multiple datasets.
Code: https://github.com/CSYSI/DPU-Former.

</details>


### [28] [Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning](https://arxiv.org/abs/2506.21873)
*Tzu-Chun Chien,Chieh-Kai Lin,Shiang-Feng Tsai,Ruei-Chi Lai,Hung-Jen Chen,Min Sun*

Main category: cs.CV

TL;DR: 多模态大语言模型（MLLMs）在视觉定位中表现优异，但视觉令牌修剪会显著降低性能。研究发现修剪后位置ID错位是主要原因，并提出无需额外成本的GAP方法，恢复90%性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉令牌修剪导致的模型定位能力下降问题，提升多模态大语言模型的效率。

Method: 提出Grounding-Aware Token Pruning (GAP)，通过调整位置ID恢复修剪后的性能。

Result: GAP方法将LLaVA在RefCOCO验证集上的准确率从15.34%提升至51.42%，接近原始性能的90%。

Conclusion: GAP是一种简单有效的方法，适用于多种模型和修剪策略，显著提升性能且无需额外成本。

Abstract: Recent Multimodal Large Language Models (MLLMs) have demonstrated strong
performance in visual grounding, establishing themselves as a general interface
for various vision-language applications. This progress has driven the
development of token pruning methods to mitigate the high computational costs
associated with processing numerous visual tokens. However, we observe that
pruning significantly weakens the model's grounding ability, leading to
incorrect predictions and drastic performance degradation. In Referring
Expression Comprehension (REC), for instance, pruning causes the accuracy of
LLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis
identifies misaligned position IDs after pruning as the primary cause of this
degradation, as both the order and value of these IDs are crucial for
maintaining performance in grounding tasks. To address this issue, we propose
Grounding-Aware Token Pruning (GAP), a simple yet effective adjustment to
position IDs that recovers REC accuracy back to 51.42%, which is 90% of the
original performance in the without pruning setting, all while requiring no
additional training, memory, or computational overhead. Applied to models such
as Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves
performance across various token pruning strategies.

</details>


### [29] [GRASP-PsONet: Gradient-based Removal of Spurious Patterns for PsOriasis Severity Classification](https://arxiv.org/abs/2506.21883)
*Basudha Pal,Sharif Amit Kamran,Brendon Lutnick,Molly Lucas,Chaitanya Parmar,Asha Patel Shah,David Apfel,Steven Fakharzadeh,Lloyd Miller,Gabriela Cula,Kristopher Standish*

Main category: cs.CV

TL;DR: 提出一种基于梯度解释性的框架，自动标记训练图像中的问题样本，提升银屑病严重程度分类模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 银屑病严重程度评分在临床试验中重要，但存在评分者间差异和临床评估负担。远程成像虽具扩展性，但受光照、背景和设备质量等影响，降低模型可靠性。

Method: 采用基于梯度的解释性方法，追踪误分类验证图像的梯度，检测训练样本中的不一致评分或非临床伪影。应用于ConvNeXT弱监督模型。

Result: 移除8.2%的问题图像后，模型AUC-ROC提升5%（85%至90%）。方法能识别90%以上的评分者间不一致样本。

Conclusion: 该方法提升了远程评估的自动化评分鲁棒性，减少了对人工审核的依赖。

Abstract: Psoriasis (PsO) severity scoring is important for clinical trials but is
hindered by inter-rater variability and the burden of in person clinical
evaluation. Remote imaging using patient captured mobile photos offers
scalability but introduces challenges, such as variation in lighting,
background, and device quality that are often imperceptible to humans but can
impact model performance. These factors, along with inconsistencies in
dermatologist annotations, reduce the reliability of automated severity
scoring. We propose a framework to automatically flag problematic training
images that introduce spurious correlations which degrade model generalization,
using a gradient based interpretability approach. By tracing the gradients of
misclassified validation images, we detect training samples where model errors
align with inconsistently rated examples or are affected by subtle, nonclinical
artifacts. We apply this method to a ConvNeXT based weakly supervised model
designed to classify PsO severity from phone images. Removing 8.2% of flagged
images improves model AUC-ROC by 5% (85% to 90%) on a held out test set.
Commonly, multiple annotators and an adjudication process ensure annotation
accuracy, which is expensive and time consuming. Our method detects training
images with annotation inconsistencies, potentially removing the need for
manual review. When applied to a subset of training data rated by two
dermatologists, the method identifies over 90% of cases with inter-rater
disagreement by reviewing only the top 30% of samples. This improves automated
scoring for remote assessments, ensuring robustness despite data collection
variability.

</details>


### [30] [Integrating Multi-Modal Sensors: A Review of Fusion Techniques for Intelligent Vehicles](https://arxiv.org/abs/2506.21885)
*Chuheng Wei,Ziye Qin,Ziyan Zhang,Guoyuan Wu,Matthew J. Barth*

Main category: cs.CV

TL;DR: 本文系统综述了多传感器融合在自动驾驶中的应用，分类了数据级、特征级和决策级融合策略，并探讨了深度学习方法、多模态数据集及新兴趋势。


<details>
  <summary>Details</summary>
Motivation: 多传感器融合能克服单一传感器的局限性，提升自动驾驶的环境感知能力，尤其是在恶劣天气和复杂城市环境中。

Method: 将多传感器融合策略分为数据级、特征级和决策级，并综述了基于深度学习的对应方法。

Result: 提供了关键多模态数据集及其适用性分析，探讨了视觉语言模型和大语言模型的融合趋势。

Conclusion: 多传感器融合在提升自动驾驶系统适应性和鲁棒性方面具有潜力，本文为未来研究方向提供了见解。

Abstract: Multi-sensor fusion plays a critical role in enhancing perception for
autonomous driving, overcoming individual sensor limitations, and enabling
comprehensive environmental understanding. This paper first formalizes
multi-sensor fusion strategies into data-level, feature-level, and
decision-level categories and then provides a systematic review of deep
learning-based methods corresponding to each strategy. We present key
multi-modal datasets and discuss their applicability in addressing real-world
challenges, particularly in adverse weather conditions and complex urban
environments. Additionally, we explore emerging trends, including the
integration of Vision-Language Models (VLMs), Large Language Models (LLMs), and
the role of sensor fusion in end-to-end autonomous driving, highlighting its
potential to enhance system adaptability and robustness. Our work offers
valuable insights into current methods and future directions for multi-sensor
fusion in autonomous driving.

</details>


### [31] [DIVE: Deep-search Iterative Video Exploration A Technical Report for the CVRR Challenge at CVPR 2025](https://arxiv.org/abs/2506.21891)
*Umihiro Kamoto,Tatsuya Ishibashi,Noriyuki Kugo*

Main category: cs.CV

TL;DR: 本文介绍了在2025年复杂视频推理与鲁棒性评估挑战赛中获得第一名的解决方案DIVE，该方法通过迭代推理框架实现了81.44%的准确率。


<details>
  <summary>Details</summary>
Motivation: 挑战赛旨在评估对多样化真实世界视频片段生成准确自然语言回答的能力，使用CVRR-ES基准测试。

Method: DIVE采用迭代推理方法，将输入问题语义分解并通过逐步推理和渐进推断解决。

Result: 在CVRR-ES基准测试中，DIVE达到81.44%的准确率，位列第一。

Conclusion: DIVE的迭代推理框架在视频问答中表现出高效性和鲁棒性，代码已开源。

Abstract: In this report, we present the winning solution that achieved the 1st place
in the Complex Video Reasoning & Robustness Evaluation Challenge 2025. This
challenge evaluates the ability to generate accurate natural language answers
to questions about diverse, real-world video clips. It uses the Complex Video
Reasoning and Robustness Evaluation Suite (CVRR-ES) benchmark, which consists
of 214 unique videos and 2,400 question-answer pairs spanning 11 categories.
Our method, DIVE (Deep-search Iterative Video Exploration), adopts an iterative
reasoning approach, in which each input question is semantically decomposed and
solved through stepwise reasoning and progressive inference. This enables our
system to provide highly accurate and contextually appropriate answers to even
the most complex queries. Applied to the CVRR-ES benchmark, our approach
achieves 81.44% accuracy on the test set, securing the top position among all
participants. This report details our methodology and provides a comprehensive
analysis of the experimental results, demonstrating the effectiveness of our
iterative reasoning framework in achieving robust video question answering. The
code is available at https://github.com/PanasonicConnect/DIVE

</details>


### [32] [SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation](https://arxiv.org/abs/2506.21892)
*Adam Goodge,Xun Xu,Bryan Hooi,Wee Siong Ng,Jingyi Liao,Yongyi Su,Xulei Yang*

Main category: cs.CV

TL;DR: 提出了一种基于3D视觉语言模型的方法SODA，用于检测点云数据中的分布外对象，解决了合成数据与真实数据之间的域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 点云数据在多种应用中日益普及，但分布外（OOD）点云对象的检测问题尚未充分研究，且现有3D视觉语言模型（3D VLMs）预训练数据集的规模和多样性不足，导致域偏移问题。

Method: 提出SODA方法，通过基于邻域的分数传播方案改进OOD点云检测，无需额外模型训练。

Result: SODA在多种数据集和问题设置中实现了最先进的性能。

Conclusion: SODA有效解决了合成数据与真实数据之间的域偏移问题，提升了OOD点云检测的性能。

Abstract: As point cloud data increases in prevalence in a variety of applications, the
ability to detect out-of-distribution (OOD) point cloud objects becomes
critical for ensuring model safety and reliability. However, this problem
remains under-explored in existing research. Inspired by success in the image
domain, we propose to exploit advances in 3D vision-language models (3D VLMs)
for OOD detection in point cloud objects. However, a major challenge is that
point cloud datasets used to pre-train 3D VLMs are drastically smaller in size
and object diversity than their image-based counterparts. Critically, they
often contain exclusively computer-designed synthetic objects. This leads to a
substantial domain shift when the model is transferred to practical tasks
involving real objects scanned from the physical environment. In this paper,
our empirical experiments show that synthetic-to-real domain shift
significantly degrades the alignment of point cloud with their associated text
embeddings in the 3D VLM latent space, hindering downstream performance. To
address this, we propose a novel methodology called SODA which improves the
detection of OOD point clouds through a neighborhood-based score propagation
scheme. SODA is inference-based, requires no additional model training, and
achieves state-of-the-art performance over existing approaches across datasets
and problem settings.

</details>


### [33] [Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.21895)
*Fangling Jiang,Qi Li,Weining Wang,Gang Wang,Bing Liu,Zhenan Sun*

Main category: cs.CV

TL;DR: 提出了一种基于强化微调的面部反欺骗方法，利用多模态大语言模型的能力，通过奖励机制和优化策略提升跨域泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法容易过拟合训练数据，泛化能力差且缺乏可解释性，需改进。

Method: 设计了可验证的类别一致奖励和推理一致奖励，采用GRPO优化策略，通过迭代学习提炼决策规则。

Result: 实验表明，该方法在跨域泛化性能上达到最优，能有效应对未知攻击类型并提供可解释性。

Conclusion: 该方法显著提升了面部反欺骗的泛化能力和可解释性，无需大量标注数据。

Abstract: Recently the emergence of novel presentation attacks has drawn increasing
attention to face anti-spoofing. However, existing methods tend to memorize
data patterns from the training set, resulting in poor generalization to
unknown attack types across different scenarios and limited interpretability.
To address these challenges, this paper presents a reinforcement
fine-tuning-based face anti-spoofing method that stimulates the capabilities of
multimodal large language models to think and learn how to solve the
anti-spoofing task itself, rather than relying on the memorization of
authenticity patterns. We design verifiable class consistent reward and
reasoning consistent reward, and employ a GRPO-based optimization strategy to
guide the model in exploring reasoning policies from multiple perspectives to
maximize expected rewards. As a result, through iterative trial-and-error
learning while retaining only high-reward trajectories, the model distills
highly generalizable decision-making rules from the extensive solution space to
effectively address cross-domain face anti-spoofing tasks. Extensive
experimental results demonstrate that our method achieves state-of-the-art
cross-domain generalization performance. It generalizes well to diverse unknown
attack types in unseen target domains while providing interpretable reasoning
for its authenticity decisions without requiring labor-intensive textual
annotations for training.

</details>


### [34] [Visual Content Detection in Educational Videos with Transfer Learning and Dataset Enrichment](https://arxiv.org/abs/2506.21903)
*Dipayan Biswas,Shishir Shah,Jaspal Subhlok*

Main category: cs.CV

TL;DR: 本文提出了一种基于迁移学习的方法，用于检测讲座视频中的视觉元素，并优化了YOLO模型，同时发布了标注数据集和源代码。


<details>
  <summary>Details</summary>
Motivation: 讲座视频中的视觉元素（如图表、表格）对理解和检索内容至关重要，但自动检测这些元素存在挑战，缺乏标准结构和标注数据。

Method: 采用迁移学习方法，评估了多种目标检测模型，优化YOLO模型，并利用半监督自动标注策略。

Result: YOLO模型表现最佳，优化后成功检测讲座视频中的视觉元素，并发布了标注数据集和源代码。

Conclusion: 该方法为讲座视频中的目标检测提供了通用解决方案，推动了未来研究的发展。

Abstract: Video is transforming education with online courses and recorded lectures
supplementing and replacing classroom teaching. Recent research has focused on
enhancing information retrieval for video lectures with advanced navigation,
searchability, summarization, as well as question answering chatbots. Visual
elements like tables, charts, and illustrations are central to comprehension,
retention, and data presentation in lecture videos, yet their full potential
for improving access to video content remains underutilized. A major factor is
that accurate automatic detection of visual elements in a lecture video is
challenging; reasons include i) most visual elements, such as charts, graphs,
tables, and illustrations, are artificially created and lack any standard
structure, and ii) coherent visual objects may lack clear boundaries and may be
composed of connected text and visual components. Despite advancements in deep
learning based object detection, current models do not yield satisfactory
performance due to the unique nature of visual content in lectures and scarcity
of annotated datasets. This paper reports on a transfer learning approach for
detecting visual elements in lecture video frames. A suite of state of the art
object detection models were evaluated for their performance on lecture video
datasets. YOLO emerged as the most promising model for this task. Subsequently
YOLO was optimized for lecture video object detection with training on multiple
benchmark datasets and deploying a semi-supervised auto labeling strategy.
Results evaluate the success of this approach, also in developing a general
solution to the problem of object detection in lecture videos. Paper
contributions include a publicly released benchmark of annotated lecture video
frames, along with the source code to facilitate future research.

</details>


### [35] [RAUM-Net: Regional Attention and Uncertainty-aware Mamba Network](https://arxiv.org/abs/2506.21905)
*Mingquan Liu*

Main category: cs.CV

TL;DR: 提出了一种结合Mamba特征建模、区域注意力和贝叶斯不确定性的半监督方法，用于细粒度视觉分类（FGVC），在标注数据稀缺时表现优异。


<details>
  <summary>Details</summary>
Motivation: 细粒度视觉分类因类间差异细微且特征表示脆弱而具有挑战性，现有方法在标注数据稀缺时表现不佳。

Method: 结合Mamba特征建模、区域注意力和贝叶斯不确定性，增强局部到全局特征建模，并利用贝叶斯推理选择高质量伪标签。

Result: 在FGVC基准测试中表现优异，尤其在标注数据有限时展现出鲁棒性。

Conclusion: 该方法在细粒度视觉分类任务中有效解决了标注数据稀缺的问题，并提升了模型性能。

Abstract: Fine Grained Visual Categorization (FGVC) remains a challenging task in
computer vision due to subtle inter class differences and fragile feature
representations. Existing methods struggle in fine grained scenarios,
especially when labeled data is scarce. We propose a semi supervised method
combining Mamba based feature modeling, region attention, and Bayesian
uncertainty. Our approach enhances local to global feature modeling while
focusing on key areas during learning. Bayesian inference selects high quality
pseudo labels for stability. Experiments show strong performance on FGVC
benchmarks with occlusions, demonstrating robustness when labeled data is
limited. Code is available at https://github.com/wxqnl/RAUM Net.

</details>


### [36] [CERBERUS: Crack Evaluation & Recognition Benchmark for Engineering Reliability & Urban Stability](https://arxiv.org/abs/2506.21909)
*Justin Reinman,Sunwoong Choi*

Main category: cs.CV

TL;DR: CERBERUS是一个合成基准，用于训练和评估AI模型检测基础设施中的裂缝和缺陷，包含裂缝图像生成器和Unity构建的3D检查场景。


<details>
  <summary>Details</summary>
Motivation: 为缺陷检测系统提供一个灵活、可重复的测试方法，支持自动化基础设施检查的未来研究。

Method: 使用YOLO模型测试合成和真实裂缝数据的组合，包括简单的Fly-By墙检查和复杂的Underpass场景。

Result: 结合合成和真实数据可提高模型在真实图像上的性能。

Conclusion: CERBERUS为缺陷检测系统提供了有效的测试工具，并公开可用。

Abstract: CERBERUS is a synthetic benchmark designed to help train and evaluate AI
models for detecting cracks and other defects in infrastructure. It includes a
crack image generator and realistic 3D inspection scenarios built in Unity. The
benchmark features two types of setups: a simple Fly-By wall inspection and a
more complex Underpass scene with lighting and geometry challenges. We tested a
popular object detection model (YOLO) using different combinations of synthetic
and real crack data. Results show that combining synthetic and real data
improves performance on real-world images. CERBERUS provides a flexible,
repeatable way to test defect detection systems and supports future research in
automated infrastructure inspection. CERBERUS is publicly available at
https://github.com/justinreinman/Cerberus-Defect-Generator.

</details>


### [37] [Generating Attribute-Aware Human Motions from Textual Prompt](https://arxiv.org/abs/2506.21912)
*Xinghan Wang,Kun Xu,Fei Li,Cao Sheng,Jiazhong Yu,Yadong Mu*

Main category: cs.CV

TL;DR: 本文提出了一种新框架，通过解耦动作语义和人类属性，实现了基于文本和属性的运动生成，并引入了带属性标注的数据集HumanAttr。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动的人体运动生成方法忽略了人类属性（如年龄、性别、体重和身高）对运动模式的影响，本文旨在填补这一空白。

Method: 提出了一种受结构因果模型启发的框架，将动作语义与人类属性解耦，支持文本到语义的预测和属性控制的生成。

Result: 模型能够生成与用户文本和属性输入一致的逼真运动，并在新数据集HumanAttr上验证了有效性。

Conclusion: 本文为属性感知的文本到运动生成设定了首个基准，展示了模型的潜力。

Abstract: Text-driven human motion generation has recently attracted considerable
attention, allowing models to generate human motions based on textual
descriptions. However, current methods neglect the influence of human
attributes (such as age, gender, weight, and height) which are key factors
shaping human motion patterns. This work represents a pilot exploration for
bridging this gap. We conceptualize each motion as comprising both attribute
information and action semantics, where textual descriptions align exclusively
with action semantics. To achieve this, a new framework inspired by Structural
Causal Models is proposed to decouple action semantics from human attributes,
enabling text-to-semantics prediction and attribute-controlled generation. The
resulting model is capable of generating realistic, attribute-aware motion
aligned with the user's text and attribute inputs. For evaluation, we introduce
HumanAttr, a comprehensive dataset containing attribute annotations for
text-motion pairs, setting the first benchmark for attribute-aware
text-to-motion generation. Extensive experiments on the new dataset validate
our model's effectiveness.

</details>


### [38] [SepFormer: Coarse-to-fine Separator Regression Network for Table Structure Recognition](https://arxiv.org/abs/2506.21920)
*Nam Quan Nguyen,Xuan Phong Pham,Tuan-Anh Tran*

Main category: cs.CV

TL;DR: SepFormer是一种基于DETR架构的表格结构识别方法，通过单步分隔符回归实现快速且鲁棒的表格重建。


<details>
  <summary>Details</summary>
Motivation: 表格结构识别（TSR）是语义数据提取的基础，但现有方法在速度和鲁棒性上仍有改进空间。

Method: SepFormer采用分阶段的分隔符回归方法，通过两个Transformer解码器堆叠，从单线到线带分隔符逐步细化预测。

Result: SepFormer在多个基准数据集上达到与最先进方法相当的性能，平均运行速度为25.6 FPS。

Conclusion: SepFormer通过分阶段的分隔符回归，显著提升了表格结构识别的速度和鲁棒性。

Abstract: The automated reconstruction of the logical arrangement of tables from image
data, termed Table Structure Recognition (TSR), is fundamental for semantic
data extraction. Recently, researchers have explored a wide range of techniques
to tackle this problem, demonstrating significant progress. Each table is a set
of vertical and horizontal separators. Following this realization, we present
SepFormer, which integrates the split-and-merge paradigm into a single step
through separator regression with a DETR-style architecture, improving speed
and robustness. SepFormer is a coarse-to-fine approach that predicts table
separators from single-line to line-strip separators with a stack of two
transformer decoders. In the coarse-grained stage, the model learns to
gradually refine single-line segments through decoder layers with additional
angle loss. At the end of the fine-grained stage, the model predicts line-strip
separators by refining sampled points from each single-line segment. Our
SepFormer can run on average at 25.6 FPS while achieving comparable performance
with state-of-the-art methods on several benchmark datasets, including SciTSR,
PubTabNet, WTW, and iFLYTAB.

</details>


### [39] [ZeroReg3D: A Zero-shot Registration Pipeline for 3D Consecutive Histopathology Image Reconstruction](https://arxiv.org/abs/2506.21923)
*Juming Xiong,Ruining Deng,Jialin Yue,Siqi Lu,Junlin Guo,Marilyn Lionts,Tianyuan Yao,Can Cui,Junchao Zhu,Chongyu Qu,Mengmeng Yin,Haichun Yang,Yuankai Huo*

Main category: cs.CV

TL;DR: ZeroReg3D是一种新型零样本配准方法，用于从连续组织切片构建精确的3D模型，解决了传统方法在3D空间关系保留上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有2D组织学分析方法难以保留3D空间关系，且深度学习方法泛化性差，非深度学习方法精度不足。

Method: 结合零样本深度学习关键点匹配与基于优化的仿射和非刚性配准技术。

Result: 有效解决了组织变形、切片伪影、染色变异和光照不一致等问题，无需重新训练或微调。

Conclusion: ZeroReg3D为组织学3D重建提供了一种高效且通用的解决方案。

Abstract: Histological analysis plays a crucial role in understanding tissue structure
and pathology. While recent advancements in registration methods have improved
2D histological analysis, they often struggle to preserve critical 3D spatial
relationships, limiting their utility in both clinical and research
applications. Specifically, constructing accurate 3D models from 2D slices
remains challenging due to tissue deformation, sectioning artifacts,
variability in imaging techniques, and inconsistent illumination. Deep
learning-based registration methods have demonstrated improved performance but
suffer from limited generalizability and require large-scale training data. In
contrast, non-deep-learning approaches offer better generalizability but often
compromise on accuracy. In this study, we introduced ZeroReg3D, a novel
zero-shot registration pipeline tailored for accurate 3D reconstruction from
serial histological sections. By combining zero-shot deep learning-based
keypoint matching with optimization-based affine and non-rigid registration
techniques, ZeroReg3D effectively addresses critical challenges such as tissue
deformation, sectioning artifacts, staining variability, and inconsistent
illumination without requiring retraining or fine-tuning. The code has been
made publicly available at https://github.com/hrlblab/ZeroReg3D

</details>


### [40] [SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding](https://arxiv.org/abs/2506.21924)
*Zhao Jin,Rong-Cheng Tu,Jingyi Liao,Wenhao Sun,Xiao Luo,Shunyu Liu,Dacheng Tao*

Main category: cs.CV

TL;DR: SPAZER是一种结合3D和2D模态的零样本3D视觉定位方法，通过渐进式推理框架显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D视觉定位中过于依赖3D或2D模态，限制了实际应用效果。

Method: SPAZER采用渐进式推理框架，结合3D渲染和2D图像进行联合决策。

Result: 在ScanRefer和Nr3D基准测试中，SPAZER比现有零样本方法准确率提升9.0%和10.9%。

Conclusion: SPAZER通过融合空间和语义推理，实现了无需3D标注数据的鲁棒零样本定位。

Abstract: 3D Visual Grounding (3DVG) aims to localize target objects within a 3D scene
based on natural language queries. To alleviate the reliance on costly 3D
training data, recent studies have explored zero-shot 3DVG by leveraging the
extensive knowledge and powerful reasoning capabilities of pre-trained LLMs and
VLMs. However, existing paradigms tend to emphasize either spatial (3D-based)
or semantic (2D-based) understanding, limiting their effectiveness in complex
real-world applications. In this work, we introduce SPAZER - a VLM-driven agent
that combines both modalities in a progressive reasoning framework. It first
holistically analyzes the scene and produces a 3D rendering from the optimal
viewpoint. Based on this, anchor-guided candidate screening is conducted to
perform a coarse-level localization of potential objects. Furthermore,
leveraging retrieved relevant 2D camera images, 3D-2D joint decision-making is
efficiently performed to determine the best-matching object. By bridging
spatial and semantic reasoning neural streams, SPAZER achieves robust zero-shot
grounding without training on 3D-labeled data. Extensive experiments on
ScanRefer and Nr3D benchmarks demonstrate that SPAZER significantly outperforms
previous state-of-the-art zero-shot methods, achieving notable gains of 9.0%
and 10.9% in accuracy.

</details>


### [41] [Quality Assessment and Distortion-aware Saliency Prediction for AI-Generated Omnidirectional Images](https://arxiv.org/abs/2506.21925)
*Liu Yang,Huiyu Duan,Jiarui Wang,Jing Liu,Menghan Hu,Xiongkuo Min,Guangtao Zhai,Patrick Le Callet*

Main category: cs.CV

TL;DR: 该论文研究了AI生成的全景图像（AIGODIs）的质量评估和优化问题，提出了一个包含主观质量评分和失真显著区域的数据库OHF2024，并基于BLIP-2模型开发了两个模型（BLIP2OIQA和BLIP2OISal）用于评估视觉体验和预测失真显著区域，最终实现了自动优化过程。


<details>
  <summary>Details</summary>
Motivation: 随着AIGC技术的快速发展，AI生成的全景图像在VR和AR应用中具有巨大潜力，但其质量评估和优化研究仍不足。

Method: 建立了OHF2024数据库，包含主观质量评分和失真显著区域；基于BLIP-2模型开发了BLIP2OIQA和BLIP2OISal模型；提出了自动优化过程。

Result: BLIP2OIQA和BLIP2OISal模型在视觉体验评估和失真显著区域预测任务中达到SOTA效果，并能有效用于优化过程。

Conclusion: 该研究为AI生成全景图像的质量评估和优化提供了有效工具，数据库和代码将开源以促进未来研究。

Abstract: With the rapid advancement of Artificial Intelligence Generated Content
(AIGC) techniques, AI generated images (AIGIs) have attracted widespread
attention, among which AI generated omnidirectional images (AIGODIs) hold
significant potential for Virtual Reality (VR) and Augmented Reality (AR)
applications. AI generated omnidirectional images exhibit unique quality
issues, however, research on the quality assessment and optimization of
AI-generated omnidirectional images is still lacking. To this end, this work
first studies the quality assessment and distortion-aware saliency prediction
problems for AIGODIs, and further presents a corresponding optimization
process. Specifically, we first establish a comprehensive database to reflect
human feedback for AI-generated omnidirectionals, termed OHF2024, which
includes both subjective quality ratings evaluated from three perspectives and
distortion-aware salient regions. Based on the constructed OHF2024 database, we
propose two models with shared encoders based on the BLIP-2 model to evaluate
the human visual experience and predict distortion-aware saliency for
AI-generated omnidirectional images, which are named as BLIP2OIQA and
BLIP2OISal, respectively. Finally, based on the proposed models, we present an
automatic optimization process that utilizes the predicted visual experience
scores and distortion regions to further enhance the visual quality of an
AI-generated omnidirectional image. Extensive experiments show that our
BLIP2OIQA model and BLIP2OISal model achieve state-of-the-art (SOTA) results in
the human visual experience evaluation task and the distortion-aware saliency
prediction task for AI generated omnidirectional images, and can be effectively
used in the optimization process. The database and codes will be released on
https://github.com/IntMeGroup/AIGCOIQA to facilitate future research.

</details>


### [42] [SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images](https://arxiv.org/abs/2506.21945)
*Naftaly Wambugu,Ruisheng Wang,Bo Guo,Tianshu Yu,Sheng Xu,Mohammed Elhassan*

Main category: cs.CV

TL;DR: 本文提出了一种堆叠深度残差网络（SDRNet），用于高分辨率遥感图像的语义分割，解决了类间差异、遮挡和物体尺寸变化等问题。


<details>
  <summary>Details</summary>
Motivation: 高分辨率遥感图像的语义分割面临类间差异、遮挡和物体尺寸变化的挑战，现有深度卷积神经网络难以提取足够特征。

Method: 采用两个堆叠的编码器-解码器网络和膨胀残差块（DRB），以捕获长距离语义并保留空间信息。

Result: 在ISPRS Vaihingen和Potsdam数据集上，SDRNet表现优于现有深度卷积神经网络。

Conclusion: SDRNet能有效解决高分辨率遥感图像语义分割的挑战，具有竞争力。

Abstract: Land cover maps generated from semantic segmentation of high-resolution
remotely sensed images have drawn mucon in the photogrammetry and remote
sensing research community. Currently, massive fine-resolution remotely sensed
(FRRS) images acquired by improving sensing and imaging technologies become
available. However, accurate semantic segmentation of such FRRS images is
greatly affected by substantial class disparities, the invisibility of key
ground objects due to occlusion, and object size variation. Despite the
extraordinary potential in deep convolutional neural networks (DCNNs) in image
feature learning and representation, extracting sufficient features from FRRS
images for accurate semantic segmentation is still challenging. These
challenges demand the deep learning models to learn robust features and
generate sufficient feature descriptors. Specifically, learning
multi-contextual features to guarantee adequate coverage of varied object sizes
from the ground scene and harnessing global-local contexts to overcome class
disparities challenge even profound networks. Deeper networks significantly
lose spatial details due to gradual downsampling processes resulting in poor
segmentation results and coarse boundaries. This article presents a stacked
deep residual network (SDRNet) for semantic segmentation from FRRS images. The
proposed framework utilizes two stacked encoder-decoder networks to harness
long-range semantics yet preserve spatial information and dilated residual
blocks (DRB) between each encoder and decoder network to capture sufficient
global dependencies thus improving segmentation performance. Our experimental
results obtained using the ISPRS Vaihingen and Potsdam datasets demonstrate
that the SDRNet performs effectively and competitively against current DCNNs in
semantic segmentation.

</details>


### [43] [Exploring Semantic Masked Autoencoder for Self-supervised Point Cloud Understanding](https://arxiv.org/abs/2506.21957)
*Yixin Zha,Chuxin Wang,Wenfei Yang,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种基于语义掩码自编码器的方法，通过原型语义建模和增强掩码策略，解决了随机掩码策略在点云理解中的语义关系捕捉不足问题。


<details>
  <summary>Details</summary>
Motivation: 随机掩码策略在点云预训练中难以捕捉合理的语义关系，限制了自监督模型的性能。

Method: 提出语义掩码自编码器，包括原型语义建模模块和语义增强掩码策略，并通过语义增强提示调优提升下游任务性能。

Result: 在ScanObjectNN、ModelNet40和ShapeNetPart等数据集上的实验验证了方法的有效性。

Conclusion: 该方法显著提升了点云理解的语义关系捕捉能力，并在下游任务中表现出色。

Abstract: Point cloud understanding aims to acquire robust and general feature
representations from unlabeled data. Masked point modeling-based methods have
recently shown significant performance across various downstream tasks. These
pre-training methods rely on random masking strategies to establish the
perception of point clouds by restoring corrupted point cloud inputs, which
leads to the failure of capturing reasonable semantic relationships by the
self-supervised models. To address this issue, we propose Semantic Masked
Autoencoder, which comprises two main components: a prototype-based component
semantic modeling module and a component semantic-enhanced masking strategy.
Specifically, in the component semantic modeling module, we design a component
semantic guidance mechanism to direct a set of learnable prototypes in
capturing the semantics of different components from objects. Leveraging these
prototypes, we develop a component semantic-enhanced masking strategy that
addresses the limitations of random masking in effectively covering complete
component structures. Furthermore, we introduce a component semantic-enhanced
prompt-tuning strategy, which further leverages these prototypes to improve the
performance of pre-trained models in downstream tasks. Extensive experiments
conducted on datasets such as ScanObjectNN, ModelNet40, and ShapeNetPart
demonstrate the effectiveness of our proposed modules.

</details>


### [44] [TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models](https://arxiv.org/abs/2506.21975)
*Meng Yu,Te Cui,Qitong Chu,Wenjie Song,Yi Yang,Yufeng Yue*

Main category: cs.CV

TL;DR: TASeg提出了一种基于文本感知的RGB-T语义分割框架，通过LoRA微调技术和动态特征融合模块（DFFM）解决现有模型在视觉特征相似类别中的分割问题，并结合CLIP文本嵌入提升语义理解准确性。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-T语义分割模型依赖低层视觉特征，缺乏高层文本信息，导致在视觉特征相似类别中分割不准确；SAM在多模态整合中存在模态异质性和计算效率问题。

Method: 采用LoRA微调技术适配视觉基础模型，提出DFFM模块融合多模态视觉特征，同时冻结SAM的原始变换块，并在掩码解码器中引入CLIP生成的文本嵌入以实现语义对齐。

Result: 在多样化数据集上的实验表明，TASeg在挑战性场景中表现优异，且训练参数较少。

Conclusion: TASeg通过结合视觉和文本信息，显著提升了RGB-T语义分割的准确性和效率。

Abstract: Reliable semantic segmentation of open environments is essential for
intelligent systems, yet significant problems remain: 1) Existing RGB-T
semantic segmentation models mainly rely on low-level visual features and lack
high-level textual information, which struggle with accurate segmentation when
categories share similar visual characteristics. 2) While SAM excels in
instance-level segmentation, integrating it with thermal images and text is
hindered by modality heterogeneity and computational inefficiency. To address
these, we propose TASeg, a text-aware RGB-T segmentation framework by using
Low-Rank Adaptation (LoRA) fine-tuning technology to adapt vision foundation
models. Specifically, we propose a Dynamic Feature Fusion Module (DFFM) in the
image encoder, which effectively merges features from multiple visual
modalities while freezing SAM's original transformer blocks. Additionally, we
incorporate CLIP-generated text embeddings in the mask decoder to enable
semantic alignment, which further rectifies the classification error and
improves the semantic understanding accuracy. Experimental results across
diverse datasets demonstrate that our method achieves superior performance in
challenging scenarios with fewer trainable parameters.

</details>


### [45] [R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning](https://arxiv.org/abs/2506.21980)
*Biao Wang,Wenwen Li*

Main category: cs.CV

TL;DR: 论文提出了一种基于多模态大语言模型（MLLMs）的视觉单目标跟踪方法R1-Track，通过GRPO强化学习方法微调Qwen2.5-VL，在GOT-10k基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统视觉跟踪方法依赖大规模监督训练且缺乏灵活性，而MLLMs在基础任务中表现优异，但直接应用于跟踪任务时效果不佳。

Method: 使用GRPO强化学习方法在小规模数据集上微调Qwen2.5-VL，生成模型R1-Track。

Result: R1-Track在GOT-10k基准上表现突出，支持通过边界框或文本描述灵活初始化。

Conclusion: R1-Track展示了MLLMs在视觉跟踪任务中的潜力，并提出了进一步改进的方向。

Abstract: Visual single object tracking aims to continuously localize and estimate the
scale of a target in subsequent video frames, given only its initial state in
the first frame. This task has traditionally been framed as a template matching
problem, evolving through major phases including correlation filters,
two-stream networks, and one-stream networks with significant progress
achieved. However, these methods typically require explicit classification and
regression modeling, depend on supervised training with large-scale datasets,
and are limited to the single task of tracking, lacking flexibility. In recent
years, multi-modal large language models (MLLMs) have advanced rapidly.
Open-source models like Qwen2.5-VL, a flagship MLLMs with strong foundational
capabilities, demonstrate excellent performance in grounding tasks. This has
spurred interest in applying such models directly to visual tracking. However,
experiments reveal that Qwen2.5-VL struggles with template matching between
image pairs (i.e., tracking tasks). Inspired by deepseek-R1, we fine-tuned
Qwen2.5-VL using the group relative policy optimization (GRPO) reinforcement
learning method on a small-scale dataset with a rule-based reward function. The
resulting model, R1-Track, achieved notable performance on the GOT-10k
benchmark. R1-Track supports flexible initialization via bounding boxes or text
descriptions while retaining most of the original model's general capabilities.
And we further discuss potential improvements for R1-Track. This rough
technical report summarizes our findings as of May 2025.

</details>


### [46] [RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation](https://arxiv.org/abs/2506.22007)
*Liudi Yang,Yang Bai,George Eskandar,Fengyi Shen,Mohammad Altillawi,Dong Chen,Soumajit Majumder,Ziyuan Liu,Gitta Kutyniok,Abhinav Valada*

Main category: cs.CV

TL;DR: 提出一种新方法，通过分解任务和关键帧生成，避免自回归生成的长视频误差累积，提升机器人操作任务的视频质量和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决文本到视频扩散模型在长时程机器人任务中的局限性，避免自回归生成导致的误差累积。

Method: 1) 分解任务为原子任务并生成关键帧；2) 使用扩散模型插值关键帧；3) 引入语义保持注意力模块和轻量级策略模型。

Result: 在两个基准测试中取得视频质量和一致性的最优结果，长时程任务表现优于先前策略模型。

Conclusion: 新方法有效解决了长时程视频生成的误差问题，提升了机器人任务的视频质量和执行一致性。

Abstract: We address the problem of generating long-horizon videos for robotic
manipulation tasks. Text-to-video diffusion models have made significant
progress in photorealism, language understanding, and motion generation but
struggle with long-horizon robotic tasks. Recent works use video diffusion
models for high-quality simulation data and predictive rollouts in robot
planning. However, these works predict short sequences of the robot achieving
one task and employ an autoregressive paradigm to extend to the long horizon,
leading to error accumulations in the generated video and in the execution. To
overcome these limitations, we propose a novel pipeline that bypasses the need
for autoregressive generation. We achieve this through a threefold
contribution: 1) we first decompose the high-level goals into smaller atomic
tasks and generate keyframes aligned with these instructions. A second
diffusion model then interpolates between each of the two generated frames,
achieving the long-horizon video. 2) We propose a semantics preserving
attention module to maintain consistency between the keyframes. 3) We design a
lightweight policy model to regress the robot joint states from generated
videos. Our approach achieves state-of-the-art results on two benchmarks in
video quality and consistency while outperforming previous policy models on
long-horizon tasks.

</details>


### [47] [Towards Universal & Efficient Model Compression via Exponential Torque Pruning](https://arxiv.org/abs/2506.22015)
*Sarthak Ketanbhai Modi,Lim Zi Pong,Shourya Kuchhal,Yoshi Cao,Yupeng Cheng,Teo Yon Shin,Lin Shang-Wei,Zhiming Li*

Main category: cs.CV

TL;DR: 论文提出了一种新的模型压缩方法ETP，通过指数力应用方案改进现有方法的不足，显著提高了压缩率且几乎不影响精度。


<details>
  <summary>Details</summary>
Motivation: 现代深度神经网络（DNNs）的复杂性和规模快速增长，导致计算成本和内存使用问题，现有基于线性力的剪枝方法效果不佳。

Method: 提出指数力矩剪枝（ETP），采用指数力应用方案对神经网络模块进行正则化。

Result: 实验表明，ETP在多种领域实现了比现有方法更高的压缩率，且精度下降可忽略。

Conclusion: ETP是一种简单高效的模型压缩方法，显著优于现有技术。

Abstract: The rapid growth in complexity and size of modern deep neural networks (DNNs)
has increased challenges related to computational costs and memory usage,
spurring a growing interest in efficient model compression techniques. Previous
state-of-the-art approach proposes using a Torque-inspired regularization which
forces the weights of neural modules around a selected pivot point. Whereas, we
observe that the pruning effect of this approach is far from perfect, as the
post-trained network is still dense and also suffers from high accuracy drop.
In this work, we attribute such ineffectiveness to the default linear force
application scheme, which imposes inappropriate force on neural module of
different distances. To efficiently prune the redundant and distant modules
while retaining those that are close and necessary for effective inference, in
this work, we propose Exponential Torque Pruning (ETP), which adopts an
exponential force application scheme for regularization. Experimental results
on a broad range of domains demonstrate that, though being extremely simple,
ETP manages to achieve significantly higher compression rate than the previous
state-of-the-art pruning strategies with negligible accuracy drop.

</details>


### [48] [Advancing Facial Stylization through Semantic Preservation Constraint and Pseudo-Paired Supervision](https://arxiv.org/abs/2506.22022)
*Zhanyi Lu,Yue Zhou*

Main category: cs.CV

TL;DR: 提出一种结合语义保留约束和伪配对监督的面部风格化方法，提升内容一致性和风格化效果，并实现多模态和参考引导的风格化。


<details>
  <summary>Details</summary>
Motivation: 现有StyleGAN方法在面部风格化中存在伪影或源图像保真度不足的问题，主要由于忽略了生成器的语义偏移。

Method: 集成语义保留约束和伪配对监督，开发多级伪配对数据集以实施监督约束。

Result: 实验表明，该方法生成高保真、美观的面部风格化效果，优于现有方法。

Conclusion: 提出的方法有效解决了语义偏移问题，实现了高质量、灵活的面部风格化。

Abstract: Facial stylization aims to transform facial images into appealing,
high-quality stylized portraits, with the critical challenge of accurately
learning the target style while maintaining content consistency with the
original image. Although previous StyleGAN-based methods have made significant
advancements, the generated results still suffer from artifacts or insufficient
fidelity to the source image. We argue that these issues stem from neglecting
semantic shift of the generator during stylization. Therefore, we propose a
facial stylization method that integrates semantic preservation constraint and
pseudo-paired supervision to enhance the content correspondence and improve the
stylization effect. Additionally, we develop a methodology for creating
multi-level pseudo-paired datasets to implement supervisory constraint.
Furthermore, building upon our facial stylization framework, we achieve more
flexible multimodal and reference-guided stylization without complex network
architecture designs or additional training. Experimental results demonstrate
that our approach produces high-fidelity, aesthetically pleasing facial style
transfer that surpasses previous methods.

</details>


### [49] [Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel Dataset and Method](https://arxiv.org/abs/2506.22027)
*Han Wang,Shengyang Li,Jian Yang,Yuxuan Liu,Yixuan Lv,Zhuang Zhou*

Main category: cs.CV

TL;DR: 提出了一种结合光学和SAR传感器的低地球轨道星座数据集（HOSS ReID），用于船舶再识别，并提出了基于Vision Transformer的基线方法TransOSS。


<details>
  <summary>Details</summary>
Motivation: 现有船舶追踪方法依赖低分辨率或短时拍摄的卫星，难以满足实际需求，需一种全天候、高覆盖的解决方案。

Method: 构建HOSS ReID数据集，结合光学和SAR图像；提出TransOSS方法，改进Vision Transformer以处理跨模态任务，使用对比学习预训练。

Result: HOSS ReID数据集支持全天候船舶追踪，TransOSS能有效提取跨模态不变特征。

Conclusion: HOSS ReID和TransOSS为船舶追踪提供了新工具，解决了现有方法的局限性。

Abstract: Detecting and tracking ground objects using earth observation imagery remains
a significant challenge in the field of remote sensing. Continuous maritime
ship tracking is crucial for applications such as maritime search and rescue,
law enforcement, and shipping analysis. However, most current ship tracking
methods rely on geostationary satellites or video satellites. The former offer
low resolution and are susceptible to weather conditions, while the latter have
short filming durations and limited coverage areas, making them less suitable
for the real-world requirements of ship tracking. To address these limitations,
we present the Hybrid Optical and Synthetic Aperture Radar (SAR) Ship
Re-Identification Dataset (HOSS ReID dataset), designed to evaluate the
effectiveness of ship tracking using low-Earth orbit constellations of optical
and SAR sensors. This approach ensures shorter re-imaging cycles and enables
all-weather tracking. HOSS ReID dataset includes images of the same ship
captured over extended periods under diverse conditions, using different
satellites of different modalities at varying times and angles. Furthermore, we
propose a baseline method for cross-modal ship re-identification, TransOSS,
which is built on the Vision Transformer architecture. It refines the patch
embedding structure to better accommodate cross-modal tasks, incorporates
additional embeddings to introduce more reference information, and employs
contrastive learning to pre-train on large-scale optical-SAR image pairs,
ensuring the model's ability to extract modality-invariant features. Our
dataset and baseline method are publicly available on
https://github.com/Alioth2000/Hoss-ReID.

</details>


### [50] [Partial CLIP is Enough: Chimera-Seg for Zero-shot Semantic Segmentation](https://arxiv.org/abs/2506.22032)
*Jialei Chen,Xu Zheng,Danda Pani Paudel,Luc Van Gool,Hiroshi Murase,Daisuke Deguchi*

Main category: cs.CV

TL;DR: 论文提出Chimera-Seg和SGD方法，解决零样本语义分割中视觉-语言对齐和局部特征与全局表示的语义差距问题，实验显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 零样本语义分割（ZSS）需要从已见类别的监督中分割未见类别，现有方法在视觉-语言对齐和特征映射上存在挑战。

Method: 提出Chimera-Seg（结合分割主干和CLIP语义头）和选择性全局蒸馏（SGD），并引入语义对齐模块（SAM）。

Result: 在两个基准测试中，hIoU分别提升了0.9%和1.2%。

Conclusion: Chimera-Seg和SGD有效解决了ZSS中的关键挑战，提升了性能。

Abstract: Zero-shot Semantic Segmentation (ZSS) aims to segment both seen and unseen
classes using supervision from only seen classes. Beyond adaptation-based
methods, distillation-based approaches transfer vision-language alignment of
vision-language model, e.g., CLIP, to segmentation models. However, such
knowledge transfer remains challenging due to: (1) the difficulty of aligning
vision-based features with the textual space, which requires combining spatial
precision with vision-language alignment; and (2) the semantic gap between
CLIP's global representations and the local, fine-grained features of
segmentation models. To address challenge (1), we propose Chimera-Seg, which
integrates a segmentation backbone as the body and a CLIP-based semantic head
as the head, like the Chimera in Greek mythology, combining spatial precision
with vision-language alignment. Specifically, Chimera-Seg comprises a trainable
segmentation model and a CLIP Semantic Head (CSH), which maps dense features
into the CLIP-aligned space. The CSH incorporates a frozen subnetwork and fixed
projection layers from the CLIP visual encoder, along with lightweight
trainable components. The partial module from CLIP visual encoder, paired with
the segmentation model, retains segmentation capability while easing the
mapping to CLIP's semantic space. To address challenge (2), we propose
Selective Global Distillation (SGD), which distills knowledge from dense
features exhibiting high similarity to the CLIP CLS token, while gradually
reducing the number of features used for alignment as training progresses.
Besides, we also use a Semantic Alignment Module (SAM) to further align dense
visual features with semantic embeddings extracted from the frozen CLIP text
encoder. Experiments on two benchmarks show improvements of 0.9% and 1.2% in
hIoU.

</details>


### [51] [Few-Shot Identity Adaptation for 3D Talking Heads via Global Gaussian Field](https://arxiv.org/abs/2506.22044)
*Hong Nie,Fuyuan Cao,Lu Chen,Fengxin Chen,Yuefeng Zou,Jun Yu*

Main category: cs.CV

TL;DR: FIAG是一种高效的身份特定适应性3D说话头合成框架，通过共享的全局高斯场和通用运动场实现快速适应，仅需少量训练数据。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于重建和渲染的说话头合成方法对身份特定模型的依赖问题，提高计算效率和可扩展性。

Method: 提出FIAG框架，结合全局高斯场（支持多身份共享）和通用运动场（捕捉跨身份运动动态），实现快速身份适应。

Result: 实验表明，FIAG在质量和通用性上优于现有方法。

Conclusion: FIAG通过共享结构和运动先验，显著提升了说话头合成的效率和适应性。

Abstract: Reconstruction and rendering-based talking head synthesis methods achieve
high-quality results with strong identity preservation but are limited by their
dependence on identity-specific models. Each new identity requires training
from scratch, incurring high computational costs and reduced scalability
compared to generative model-based approaches. To overcome this limitation, we
propose FIAG, a novel 3D speaking head synthesis framework that enables
efficient identity-specific adaptation using only a few training footage. FIAG
incorporates Global Gaussian Field, which supports the representation of
multiple identities within a shared field, and Universal Motion Field, which
captures the common motion dynamics across diverse identities. Benefiting from
the shared facial structure information encoded in the Global Gaussian Field
and the general motion priors learned in the motion field, our framework
enables rapid adaptation from canonical identity representations to specific
ones with minimal data. Extensive comparative and ablation experiments
demonstrate that our method outperforms existing state-of-the-art approaches,
validating both the effectiveness and generalizability of the proposed
framework. Code is available at: \textit{https://github.com/gme-hong/FIAG}.

</details>


### [52] [EnLVAM: Enhanced Left Ventricle Linear Measurements Utilizing Anatomical Motion Mode](https://arxiv.org/abs/2506.22063)
*Durgesh K. Singh,Ahcene Boubekki,Qing Cao,Svein Arne Aase,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: 提出了一种通过强制直线约束提高左心室测量准确性的新框架，利用解剖M模式图像训练地标检测器，并在B模式空间中转换，减少了误差。


<details>
  <summary>Details</summary>
Motivation: 手动放置地标耗时且易错，现有深度学习方法常导致地标错位，影响测量准确性。

Method: 训练地标检测器于解剖M模式图像，实时从B模式视频计算并转换回B模式空间，结合半自动设计。

Result: 实验显示比标准B模式方法准确性更高，框架对不同网络架构泛化能力强。

Conclusion: 该框架通过半自动设计和直线约束，显著提高了测量准确性，同时简化了用户交互。

Abstract: Linear measurements of the left ventricle (LV) in the Parasternal Long Axis
(PLAX) view using B-mode echocardiography are crucial for cardiac assessment.
These involve placing 4-6 landmarks along a virtual scanline (SL) perpendicular
to the LV axis near the mitral valve tips. Manual placement is time-consuming
and error-prone, while existing deep learning methods often misalign landmarks,
causing inaccurate measurements. We propose a novel framework that enhances LV
measurement accuracy by enforcing straight-line constraints. A landmark
detector is trained on Anatomical M-Mode (AMM) images, computed in real time
from B-mode videos, then transformed back to B-mode space. This approach
addresses misalignment and reduces measurement errors. Experiments show
improved accuracy over standard B-mode methods, and the framework generalizes
well across network architectures. Our semi-automatic design includes a
human-in-the-loop step where the user only places the SL, simplifying
interaction while preserving alignment flexibility and clinical relevance.

</details>


### [53] [MirrorMe: Towards Realtime and High Fidelity Audio-Driven Halfbody Animation](https://arxiv.org/abs/2506.22065)
*Dechao Meng,Steven Xiao,Xindi Zhang,Guangyuan Wang,Peng Zhang,Qi Wang,Bang Zhang,Liefeng Bo*

Main category: cs.CV

TL;DR: MirrorMe是一个基于LTX视频模型的实时、可控框架，通过创新的身份注入、音频编码和渐进训练策略，解决了音频驱动肖像动画的高延迟和时序一致性问题。


<details>
  <summary>Details</summary>
Motivation: 音频驱动肖像动画在实时生成高保真、时序一致的视频方面面临挑战，现有扩散方法因逐帧处理导致高延迟和时序不一致。

Method: 提出MirrorMe框架，采用LTX视频模型，引入参考身份注入机制、因果音频编码器和渐进训练策略。

Result: 在EMTD Benchmark上，MirrorMe在保真度、唇同步准确性和时序稳定性方面表现最优。

Conclusion: MirrorMe通过高效的空间-时间压缩和创新的训练策略，实现了高质量的实时音频驱动肖像动画。

Abstract: Audio-driven portrait animation, which synthesizes realistic videos from
reference images using audio signals, faces significant challenges in real-time
generation of high-fidelity, temporally coherent animations. While recent
diffusion-based methods improve generation quality by integrating audio into
denoising processes, their reliance on frame-by-frame UNet architectures
introduces prohibitive latency and struggles with temporal consistency. This
paper introduces MirrorMe, a real-time, controllable framework built on the LTX
video model, a diffusion transformer that compresses video spatially and
temporally for efficient latent space denoising. To address LTX's trade-offs
between compression and semantic fidelity, we propose three innovations: 1. A
reference identity injection mechanism via VAE-encoded image concatenation and
self-attention, ensuring identity consistency; 2. A causal audio encoder and
adapter tailored to LTX's temporal structure, enabling precise audio-expression
synchronization; and 3. A progressive training strategy combining close-up
facial training, half-body synthesis with facial masking, and hand pose
integration for enhanced gesture control. Extensive experiments on the EMTD
Benchmark demonstrate MirrorMe's state-of-the-art performance in fidelity,
lip-sync accuracy, and temporal stability.

</details>


### [54] [Single-Scanline Relative Pose Estimation for Rolling Shutter Cameras](https://arxiv.org/abs/2506.22069)
*Petr Hruby,Marc Pollefeys*

Main category: cs.CV

TL;DR: 提出一种新方法，通过滚动快门相机单扫描线投影的交点估计相对位姿，无需显式建模相机运动。


<details>
  <summary>Details</summary>
Motivation: 解决滚动快门相机在结构从运动中位姿估计的挑战，避免对相机运动的复杂建模。

Method: 利用线投影与单扫描线的交点进行位姿估计，支持单视图或多视图场景，并开发最小求解器处理平行线和已知重力方向的情况。

Result: 在Fastec数据集上的实验验证了方法的可行性，适用于滚动快门SfM初始化。

Conclusion: 该方法为滚动快门SfM提供了基础模块，具有进一步开发的潜力。

Abstract: We propose a novel approach for estimating the relative pose between rolling
shutter cameras using the intersections of line projections with a single
scanline per image. This allows pose estimation without explicitly modeling
camera motion. Alternatively, scanlines can be selected within a single image,
enabling single-view relative pose estimation for scanlines of rolling shutter
cameras. Our approach is designed as a foundational building block for rolling
shutter structure-from-motion (SfM), where no motion model is required, and
each scanline's pose can be computed independently. % We classify minimal
solvers for this problem in both generic and specialized settings, including
cases with parallel lines and known gravity direction, assuming known
intrinsics and no lens distortion. Furthermore, we develop minimal solvers for
the parallel-lines scenario, both with and without gravity priors, by
leveraging connections between this problem and the estimation of 2D structure
from 1D cameras. % Experiments on rolling shutter images from the Fastec
dataset demonstrate the feasibility of our approach for initializing rolling
shutter SfM, highlighting its potential for further development. % The code
will be made publicly available.

</details>


### [55] [Reasoning in machine vision: learning to think fast and slow](https://arxiv.org/abs/2506.22075)
*Shaheer U. Saeed,Yipei Wang,Veeru Kasivisvanathan,Brian R. Davidson,Matthew J. Clarkson,Yipeng Hu,Daniel C. Alexander*

Main category: cs.CV

TL;DR: 论文提出了一种新型学习范式，通过增加推理时间（计算资源）提升机器在视觉任务中的推理能力，尤其在数据稀缺场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 人类推理能力强大，而机器智能仍受限于训练数据，无法在推理时动态优化解决方案。现有机器推理研究多集中于语言领域，非语言推理（如视觉感知、医学诊断）仍是挑战。

Method: 受心理学双过程理论启发，结合快速思考的System I模块和慢速思考的System II模块，通过自对弈强化学习迭代优化解决方案。

Result: 在视觉任务（包括计算机视觉基准和医学图像癌症定位）中，该方法优于大规模监督学习、基础模型甚至人类专家。

Conclusion: 该范式为非语言机器推理提供了突破性解决方案，展示了在数据稀缺场景下的潜力。

Abstract: Reasoning is a hallmark of human intelligence, enabling adaptive
decision-making in complex and unfamiliar scenarios. In contrast, machine
intelligence remains bound to training data, lacking the ability to dynamically
refine solutions at inference time. While some recent advances have explored
reasoning in machines, these efforts are largely limited to verbal domains such
as mathematical problem-solving, where explicit rules govern step-by-step
reasoning. Other critical real-world tasks - including visual perception,
spatial reasoning, and radiological diagnosis - require non-verbal reasoning,
which remains an open challenge. Here we present a novel learning paradigm that
enables machine reasoning in vision by allowing performance improvement with
increasing thinking time (inference-time compute), even under conditions where
labelled data is very limited. Inspired by dual-process theories of human
cognition in psychology, our approach integrates a fast-thinking System I
module for familiar tasks, with a slow-thinking System II module that
iteratively refines solutions using self-play reinforcement learning. This
paradigm mimics human reasoning by proposing, competing over, and refining
solutions in data-scarce scenarios. We demonstrate superior performance through
extended thinking time, compared not only to large-scale supervised learning
but also foundation models and even human experts, in real-world vision tasks.
These tasks include computer-vision benchmarks and cancer localisation on
medical images across five organs, showcasing transformative potential for
non-verbal machine reasoning.

</details>


### [56] [Towards Accurate Heart Rate Measurement from Ultra-Short Video Clips via Periodicity-Guided rPPG Estimation and Signal Reconstruction](https://arxiv.org/abs/2506.22078)
*Pei-Kai Huanga,Ya-Ting Chan,Kuan-Wen Chen,Yen-Chun Chou,Shih-Yu Yang,Chiou-Ting Hsu*

Main category: cs.CV

TL;DR: 该论文提出了一种从超短2秒视频片段中准确测量心率的方法，通过周期性引导的rPPG估计和信号生成器解决频谱泄漏问题，实现了优于现有技术的性能。


<details>
  <summary>Details</summary>
Motivation: 现有远程心率测量方法多关注10秒左右的视频片段，而忽略了超短视频片段的需求。本文旨在解决从2秒视频中准确测量心率的挑战。

Method: 提出周期性引导的rPPG估计方法，确保超短片段信号与长片段信号的周期性一致；引入生成器重构长信号以减少频谱泄漏。

Result: 在四个rPPG基准数据集上的实验表明，该方法不仅能从超短视频中准确测量心率，还优于现有技术。

Conclusion: 该方法在超短视频心率测量中表现出色，达到了最先进的性能。

Abstract: Many remote Heart Rate (HR) measurement methods focus on estimating remote
photoplethysmography (rPPG) signals from video clips lasting around 10 seconds
but often overlook the need for HR estimation from ultra-short video clips. In
this paper, we aim to accurately measure HR from ultra-short 2-second video
clips by specifically addressing two key challenges. First, to overcome the
limited number of heartbeat cycles in ultra-short video clips, we propose an
effective periodicity-guided rPPG estimation method that enforces consistent
periodicity between rPPG signals estimated from ultra-short clips and their
much longer ground truth signals. Next, to mitigate estimation inaccuracies due
to spectral leakage, we propose including a generator to reconstruct longer
rPPG signals from ultra-short ones while preserving their periodic consistency
to enable more accurate HR measurement. Extensive experiments on four rPPG
estimation benchmark datasets demonstrate that our proposed method not only
accurately measures HR from ultra-short video clips but also outperform
previous rPPG estimation techniques to achieve state-of-the-art performance.

</details>


### [57] [BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve Gaussian Splatting](https://arxiv.org/abs/2506.22099)
*Zipei Ma,Junzhe Jiang,Yurui Chen,Li Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于Bézier曲线的高斯泼溅方法（BézierGS），用于动态物体轨迹建模，减少对高精度物体位姿标注的依赖，提升街景重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖高精度物体位姿标注，限制了大规模场景重建。

Method: 利用可学习的Bézier曲线表示动态物体运动轨迹，通过时间信息和曲线建模自动修正位姿误差，并引入动态物体渲染和曲线一致性约束。

Result: 在Waymo和nuPlan数据集上表现优于现有方法，实现了动态和静态场景元素的准确重建与新视角合成。

Conclusion: BézierGS通过曲线建模和额外监督，显著提升了街景重建的准确性和扩展性。

Abstract: The realistic reconstruction of street scenes is critical for developing
real-world simulators in autonomous driving. Most existing methods rely on
object pose annotations, using these poses to reconstruct dynamic objects and
move them during the rendering process. This dependence on high-precision
object annotations limits large-scale and extensive scene reconstruction. To
address this challenge, we propose B\'ezier curve Gaussian splatting
(B\'ezierGS), which represents the motion trajectories of dynamic objects using
learnable B\'ezier curves. This approach fully leverages the temporal
information of dynamic objects and, through learnable curve modeling,
automatically corrects pose errors. By introducing additional supervision on
dynamic object rendering and inter-curve consistency constraints, we achieve
reasonable and accurate separation and reconstruction of scene elements.
Extensive experiments on the Waymo Open Dataset and the nuPlan benchmark
demonstrate that B\'ezierGS outperforms state-of-the-art alternatives in both
dynamic and static scene components reconstruction and novel view synthesis.

</details>


### [58] [Tied Prototype Model for Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2506.22101)
*Hyeongji Kim,Stine Hansen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: TPM改进ADNet，通过绑定原型位置、多原型和多类分割，提升医学图像少样本分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决ADNet在单原型、二分类和固定阈值上的局限性。

Method: 提出TPM模型，绑定前景和背景分布的原型位置，支持多原型和多类分割，自适应阈值。

Result: TPM显著提升分割精度，适应性强。

Conclusion: TPM为医学图像少样本分割提供了新视角，性能优越。

Abstract: Common prototype-based medical image few-shot segmentation (FSS) methods
model foreground and background classes using class-specific prototypes.
However, given the high variability of the background, a more promising
direction is to focus solely on foreground modeling, treating the background as
an anomaly -- an approach introduced by ADNet. Yet, ADNet faces three key
limitations: dependence on a single prototype per class, a focus on binary
classification, and fixed thresholds that fail to adapt to patient and organ
variability. To address these shortcomings, we propose the Tied Prototype Model
(TPM), a principled reformulation of ADNet with tied prototype locations for
foreground and background distributions. Building on its probabilistic
foundation, TPM naturally extends to multiple prototypes and multi-class
segmentation while effectively separating non-typical background features.
Notably, both extensions lead to improved segmentation accuracy. Finally, we
leverage naturally occurring class priors to define an ideal target for
adaptive thresholds, boosting segmentation performance. Taken together, TPM
provides a fresh perspective on prototype-based FSS for medical image
segmentation. The code can be found at https://github.com/hjk92g/TPM-FSS.

</details>


### [59] [Pedestrian Intention and Trajectory Prediction in Unstructured Traffic Using IDD-PeD](https://arxiv.org/abs/2506.22111)
*Ruthvik Bokkasam,Shankar Gangisetty,A. H. Abdul Hafez,C. V. Jawahar*

Main category: cs.CV

TL;DR: 论文介绍了一个印度驾驶行人数据集，旨在解决非结构化环境中行人行为建模的复杂性，并展示了现有预测方法在该数据集上的性能下降。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶的快速发展，准确预测行人行为对复杂交通环境中的安全至关重要。现有数据集难以捕捉非结构化环境的特点，因此需要更全面的数据集来开发更鲁棒的预测模型。

Method: 论文提出了一种印度驾驶行人数据集，包含高水平和低水平的详细标注，重点关注需要车辆注意的行人行为。

Result: 在该数据集上评估的现有意图预测方法性能下降高达15%，轨迹预测方法的MSE增加高达1208，表现显著低于标准行人数据集。

Conclusion: 该数据集为行人行为研究社区提供了新的挑战，有助于开发更鲁棒的模型。

Abstract: With the rapid advancements in autonomous driving, accurately predicting
pedestrian behavior has become essential for ensuring safety in complex and
unpredictable traffic conditions. The growing interest in this challenge
highlights the need for comprehensive datasets that capture unstructured
environments, enabling the development of more robust prediction models to
enhance pedestrian safety and vehicle navigation. In this paper, we introduce
an Indian driving pedestrian dataset designed to address the complexities of
modeling pedestrian behavior in unstructured environments, such as illumination
changes, occlusion of pedestrians, unsignalized scene types and
vehicle-pedestrian interactions. The dataset provides high-level and detailed
low-level comprehensive annotations focused on pedestrians requiring the
ego-vehicle's attention. Evaluation of the state-of-the-art intention
prediction methods on our dataset shows a significant performance drop of up to
$\mathbf{15\%}$, while trajectory prediction methods underperform with an
increase of up to $\mathbf{1208}$ MSE, defeating standard pedestrian datasets.
Additionally, we present exhaustive quantitative and qualitative analysis of
intention and trajectory baselines. We believe that our dataset will open new
challenges for the pedestrian behavior research community to build robust
models. Project Page:
https://cvit.iiit.ac.in/research/projects/cvit-projects/iddped

</details>


### [60] [Pipe Reconstruction from Point Cloud Data](https://arxiv.org/abs/2506.22118)
*Antje Alex,Jannis Stoppe*

Main category: cs.CV

TL;DR: 提出了一种从激光扫描数据中自动重建管道的流程，通过骨架曲线估计和优化步骤，实现管道属性的精确提取和3D建模。


<details>
  <summary>Details</summary>
Motivation: 工业资产（如船舶和海上平台）的数字孪生需要精确重建复杂管道网络，但手动建模耗时耗力。

Method: 使用拉普拉斯收缩估计骨架曲线，结合滚动球技术和2D圆拟合重新定位骨架轴，并通过3D平滑步骤优化。

Result: 实现了管道半径、长度和方向的精确提取，支持复杂管道网络的详细3D建模。

Conclusion: 自动化管道重建方法为数字孪生开发提供了快速、准确且低成本的解决方案。

Abstract: Accurate digital twins of industrial assets, such as ships and offshore
platforms, rely on the precise reconstruction of complex pipe networks.
However, manual modelling of pipes from laser scan data is a time-consuming and
labor-intensive process. This paper presents a pipeline for automated pipe
reconstruction from incomplete laser scan data. The approach estimates a
skeleton curve using Laplacian-based contraction, followed by curve elongation.
The skeleton axis is then recentred using a rolling sphere technique combined
with 2D circle fitting, and refined with a 3D smoothing step. This enables the
determination of pipe properties, including radius, length and orientation, and
facilitates the creation of detailed 3D models of complex pipe networks. By
automating pipe reconstruction, this approach supports the development of
digital twins, allowing for rapid and accurate modeling while reducing costs.

</details>


### [61] [Low-Rank Implicit Neural Representation via Schatten-p Quasi-Norm and Jacobian Regularization](https://arxiv.org/abs/2506.22134)
*Zhengyun Cheng,Changhao Wang,Guanwen Zhang,Yi Xu,Wei Zhou,Xiangyang Ji*

Main category: cs.CV

TL;DR: 提出了一种基于CP分解的低秩张量函数（CP-INR），通过神经网络参数化实现连续数据表示，并引入稀疏性和平滑性正则化，在多维数据恢复任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有张量分解方法（如Tucker和CP）在灵活性和可解释性之间存在权衡，且稀疏解难以获取。CP-INR旨在结合CP分解的天然结构和神经网络的非线性能力，提升数据表示效果。

Method: 1. 提出CP-INR，利用神经网络参数化CP分解；2. 引入Schatten-p拟范数的变分形式实现稀疏性；3. 设计基于雅可比矩阵谱范数的平滑正则化。

Result: 在图像修复、去噪和点云上采样等任务中，CP-INR优于现有方法，展示了其优越性和通用性。

Conclusion: CP-INR通过结合CP分解和神经网络，实现了高效且可解释的连续数据表示，同时通过稀疏性和平滑性正则化提升了性能。

Abstract: Higher-order tensors are well-suited for representing multi-dimensional data,
such as color images and videos. Low-rank tensor representation has become
essential in machine learning and computer vision, but existing methods like
Tucker decomposition offer flexibility at the expense of interpretability. In
contrast, while the CANDECOMP/PARAFAC (CP) decomposition provides a more
natural and interpretable tensor structure, obtaining sparse solutions remains
challenging. Leveraging the rich properties of CP decomposition, we propose a
CP-based low-rank tensor function parameterized by neural networks for implicit
neural representation (CP-INR). This approach enables continuous data
representation beyond structured grids, fully exploiting the non-linearity of
tensor data with theoretical guarantees on excess risk bounds. To achieve a
sparse CP decomposition, we introduce a variational form of the Schatten-p
quasi-norm and prove its relationship to multilinear rank minimization. For
smoothness, we propose a regularization term based on the spectral norm of the
Jacobian and Hutchinson's trace estimator. Our proposed smoothness
regularization is SVD-free and avoids explicit chain rule derivations. It can
serve as an alternative to Total Variation (TV) regularization in image
denoising tasks and is naturally applicable to continuous data. Extensive
experiments on multi-dimensional data recovery tasks, including image
inpainting, denoising, and point cloud upsampling, demonstrate the superiority
and versatility of our method compared to state-of-the-art approaches.

</details>


### [62] [Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs](https://arxiv.org/abs/2506.22139)
*Shaojie Zhang,Jiahui Yang,Jianqin Yin,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: Q-Frame是一种自适应帧选择和多分辨率缩放方法，用于提升视频理解任务中多模态大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频-LLM在均匀帧采样中难以有效捕捉与查询相关的关键时空线索，需要一种更高效的方法。

Method: Q-Frame通过训练无关的即插即用策略，利用CLIP等文本-图像匹配网络和Gumbel-Max技巧进行自适应帧选择。

Result: 在MLVU、LongVideoBench和Video-MME等基准数据集上，Q-Frame表现优于现有方法。

Conclusion: Q-Frame能够在计算限制内处理更多帧，保留关键时空信息，适用于多种视频理解任务。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
success in visual understanding tasks. However, challenges persist in adapting
these models for video comprehension due to the large volume of data and
temporal complexity. Existing Video-LLMs using uniform frame sampling often
struggle to capture the query-related crucial spatiotemporal clues of videos
effectively. In this paper, we introduce Q-Frame, a novel approach for adaptive
frame selection and multi-resolution scaling tailored to the video's content
and the specific query. Q-Frame employs a training-free, plug-and-play strategy
generated by a text-image matching network like CLIP, utilizing the Gumbel-Max
trick for efficient frame selection. Q-Frame allows Video-LLMs to process more
frames without exceeding computational limits, thereby preserving critical
temporal and spatial information. We demonstrate Q-Frame's effectiveness
through extensive experiments on benchmark datasets, including MLVU,
LongVideoBench, and Video-MME, illustrating its superiority over existing
methods and its applicability across various video understanding tasks.

</details>


### [63] [Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs](https://arxiv.org/abs/2506.22146)
*Amirmohammad Izadi,Mohammad Ali Banayeeanzade,Fatemeh Askari,Ali Rahimiakbar,Mohammad Mahdi Vahedi,Hosein Hasani,Mahdieh Soleymani Baghshah*

Main category: cs.CV

TL;DR: 论文提出了一种通过增强视觉输入的低级空间结构（如水平线）并结合文本提示的方法，显著提升了视觉语言模型在视觉推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在视觉推理中存在绑定问题，即难以可靠地将感知特征与正确的视觉对象关联，导致在计数、视觉搜索等任务中表现不佳。

Method: 通过为视觉输入添加低级空间结构（如水平线），并配合鼓励序列化、空间感知解析的文本提示，改进模型的视觉推理能力。

Result: 实验表明，该方法在视觉搜索、计数、场景描述和空间关系任务中分别提升了25.00%、26.83%、0.32编辑距离误差和9.50%的性能。

Conclusion: 低级的视觉结构调整是提升视觉语言模型在空间任务中性能的有效且未被充分探索的方向，优于纯语言策略。

Abstract: Despite progress in Vision-Language Models (VLMs), their capacity for visual
reasoning is often limited by the \textit{binding problem}: the failure to
reliably associate perceptual features with their correct visual referents.
This limitation underlies persistent errors in tasks such as counting, visual
search, scene description, and spatial relationship understanding. A key factor
is that current VLMs process visual features largely in parallel, lacking
mechanisms for spatially grounded, serial attention. This paper introduces a
simple yet effective intervention: augmenting visual inputs with low-level
spatial structures (e.g., horizontal lines) and pairing this with a textual
prompt that encourages sequential, spatially-aware parsing. We empirically
demonstrate substantial performance improvements across core visual reasoning
tasks. Specifically, our method improves GPT-4o visual search accuracy by
25.00%, increases counting accuracy by 26.83%, reduces edit distance error in
scene description by 0.32, and enhances performance on spatial relationship
tasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the
visual modification is essential for these gains; purely textual strategies,
including Chain-of-Thought prompting, are insufficient and can even degrade
performance. Our method enhances binding only with a single-query inference,
underscoring the importance of visual input design over purely
linguistically-based approaches. These findings suggest that low-level visual
structuring is a powerful and underexplored direction for improving
compositional visual reasoning and could serve as a general strategy for
enhancing VLM performance on spatially grounded tasks.

</details>


### [64] [RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation Models](https://arxiv.org/abs/2506.22149)
*Ronald Fecso,José Morano,Ursula Schmidt-Erfurth,Hrvoje Bogunović*

Main category: cs.CV

TL;DR: RetFiner是一种自监督视觉语言优化方案，通过结合文本数据的丰富监督信号，提升现有基础模型在视网膜OCT分类任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有OCT基础模型仅基于图像数据训练，缺乏对图像的全面语义理解，导致下游任务表现不佳，尤其是复杂任务。

Method: 提出RetFiner，利用多样化的训练目标结合文本数据的监督信号，优化现有基础模型的表示能力。

Result: 在七个OCT分类任务中，RetFiner显著提升了RETFound、UrFound和VisionFM的线性探测性能，平均分别提高5.8、3.9和2.1个百分点。

Conclusion: RetFiner通过视觉语言优化，显著提升了基础模型在下游任务中的表现，且无需昂贵的监督微调。

Abstract: The rise of imaging techniques such as optical coherence tomography (OCT) and
advances in deep learning (DL) have enabled clinicians and researchers to
streamline retinal disease staging. A popular DL approach is self-supervised
learning (SSL), where models learn from vast amounts of unlabeled data,
avoiding costly annotation. SSL has allowed the development of foundation
models (FMs), large models that can be used for a variety of downstream tasks.
However, existing FMs for OCT, trained solely on image data, lack a
comprehensive and robust semantic understanding of images, as evidenced by
their downstream performance (especially for complex tasks), and thus require
supervised fine-tuning (which may be unfeasible) to better adapt to specific
applications and populations. To address this, we propose RetFiner, an SSL
vision-language refinement scheme that improves the representations of existing
FMs and enables their efficient and direct adaptation to specific populations
for improved downstream performance. Our method uses a diverse set of training
objectives which take advantage of the rich supervisory signal found in textual
data. We tested RetFiner on the retinal FMs RETFound, UrFound, and VisionFM,
showing significant improvements in linear probing performance on seven highly
diverse OCT classification tasks, with an average increase of 5.8, 3.9, and 2.1
percentage points over their baselines, respectively. Our code and model
weights are publicly available at https://github.com/ronnief1/RetFiner.

</details>


### [65] [Attention-disentangled Uniform Orthogonal Feature Space Optimization for Few-shot Object Detection](https://arxiv.org/abs/2506.22161)
*Taijin Zhao,Heqian Qiu,Yu Dai,Lanxiao Wang,Fanman Meng,Qingbo Wu,Hongliang Li*

Main category: cs.CV

TL;DR: 提出了一种Uniform Orthogonal Feature Space (UOFS)框架，通过解耦特征空间解决Few-shot目标检测中类特定对象性标准和样本不具代表性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有Few-shot目标检测方法在共享特征空间中耦合对象性识别和前景分类，导致类特定对象性标准和样本不具代表性。

Method: UOFS将特征空间解耦为两个正交分量：幅度编码对象性，角度编码分类。采用Hybrid Background Optimization (HBO)策略和Spatial-wise Attention Disentanglement and Association (SADA)模块。

Result: 实验表明，该方法显著优于基于耦合特征空间的现有方法。

Conclusion: UOFS框架有效解决了Few-shot目标检测中的特征耦合问题，提升了性能。

Abstract: Few-shot object detection (FSOD) aims to detect objects with limited samples
for novel classes, while relying on abundant data for base classes. Existing
FSOD approaches, predominantly built on the Faster R-CNN detector, entangle
objectness recognition and foreground classification within shared feature
spaces. This paradigm inherently establishes class-specific objectness criteria
and suffers from unrepresentative novel class samples. To resolve this
limitation, we propose a Uniform Orthogonal Feature Space (UOFS) optimization
framework. First, UOFS decouples the feature space into two orthogonal
components, where magnitude encodes objectness and angle encodes
classification. This decoupling enables transferring class-agnostic objectness
knowledge from base classes to novel classes. Moreover, implementing the
disentanglement requires careful attention to two challenges: (1) Base set
images contain unlabeled foreground instances, causing confusion between
potential novel class instances and backgrounds. (2) Angular optimization
depends exclusively on base class foreground instances, inducing overfitting of
angular distributions to base classes. To address these challenges, we propose
a Hybrid Background Optimization (HBO) strategy: (1) Constructing a pure
background base set by removing unlabeled instances in original images to
provide unbiased magnitude-based objectness supervision. (2) Incorporating
unlabeled foreground instances in the original base set into angular
optimization to enhance distribution uniformity. Additionally, we propose a
Spatial-wise Attention Disentanglement and Association (SADA) module to address
task conflicts between class-agnostic and class-specific tasks. Experiments
demonstrate that our method significantly outperforms existing approaches based
on entangled feature spaces.

</details>


### [66] [Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition](https://arxiv.org/abs/2506.22179)
*Wenhan Wu,Zhishuai Guo,Chen Chen,Hongfei Xue,Aidong Lu*

Main category: cs.CV

TL;DR: 提出FS-VAE模型，通过频率分解增强骨架语义表示学习，解决零样本动作识别中细粒度动作模式忽略的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注视觉与语义对齐，但忽略了语义空间中细粒度动作模式的重要性。

Method: FS-VAE包含频率增强模块、多级对齐的语义描述和校准的交叉对齐损失。

Result: 实验验证频率增强的语义特征能有效区分视觉和语义相似的动作簇。

Conclusion: FS-VAE提升了零样本动作识别的鲁棒性和准确性。

Abstract: Zero-shot skeleton-based action recognition aims to develop models capable of
identifying actions beyond the categories encountered during training. Previous
approaches have primarily focused on aligning visual and semantic
representations but often overlooked the importance of fine-grained action
patterns in the semantic space (e.g., the hand movements in drinking water and
brushing teeth). To address these limitations, we propose a Frequency-Semantic
Enhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic
representation learning with frequency decomposition. FS-VAE consists of three
key components: 1) a frequency-based enhancement module with high- and
low-frequency adjustments to enrich the skeletal semantics learning and improve
the robustness of zero-shot action recognition; 2) a semantic-based action
description with multilevel alignment to capture both local details and global
correspondence, effectively bridging the semantic gap and compensating for the
inherent loss of information in skeleton sequences; 3) a calibrated
cross-alignment loss that enables valid skeleton-text pairs to counterbalance
ambiguous ones, mitigating discrepancies and ambiguities in skeleton and text
features, thereby ensuring robust alignment. Evaluations on the benchmarks
demonstrate the effectiveness of our approach, validating that
frequency-enhanced semantic features enable robust differentiation of visually
and semantically similar action clusters, improving zero-shot action
recognition.

</details>


### [67] [Robust and Accurate Multi-view 2D/3D Image Registration with Differentiable X-ray Rendering and Dual Cross-view Constraints](https://arxiv.org/abs/2506.22191)
*Yuxin Cui,Rui Song,Yibin Li,Max Q. -H. Meng,Zhe Min*

Main category: cs.CV

TL;DR: 提出了一种新颖的多视角2D/3D刚性配准方法，通过两阶段设计和交叉视角约束，显著提升了配准的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决单视角术中图像视野有限的问题，提升多视角2D/3D配准的精度和鲁棒性。

Method: 两阶段方法：第一阶段设计联合损失函数，引入交叉视角训练损失；第二阶段通过测试时优化细化估计位姿。

Result: 在DeepFluoro数据集上达到0.79±2.17 mm的平均目标配准误差，优于现有方法。

Conclusion: 该方法通过多视角投影位姿的相互约束，显著提升了配准性能。

Abstract: Robust and accurate 2D/3D registration, which aligns preoperative models with
intraoperative images of the same anatomy, is crucial for successful
interventional navigation. To mitigate the challenge of a limited field of view
in single-image intraoperative scenarios, multi-view 2D/3D registration is
required by leveraging multiple intraoperative images. In this paper, we
propose a novel multi-view 2D/3D rigid registration approach comprising two
stages. In the first stage, a combined loss function is designed, incorporating
both the differences between predicted and ground-truth poses and the
dissimilarities (e.g., normalized cross-correlation) between simulated and
observed intraoperative images. More importantly, additional cross-view
training loss terms are introduced for both pose and image losses to explicitly
enforce cross-view constraints. In the second stage, test-time optimization is
performed to refine the estimated poses from the coarse stage. Our method
exploits the mutual constraints of multi-view projection poses to enhance the
robustness of the registration process. The proposed framework achieves a mean
target registration error (mTRE) of $0.79 \pm 2.17$ mm on six specimens from
the DeepFluoro dataset, demonstrating superior performance compared to
state-of-the-art registration algorithms.

</details>


### [68] [Boosting Classification with Quantum-Inspired Augmentations](https://arxiv.org/abs/2506.22241)
*Matthias Tschöpe,Vitor Fortes Rey,Sogo Pierre Sanon,Paul Lukowicz,Nikolaos Palaiodimopoulos,Maximilian Kiefer-Emmanouilidis*

Main category: cs.CV

TL;DR: 该论文研究了量子门扰动作为数据增强技术对经典机器学习的影响，展示了其在图像分类任务中的性能提升，并探讨了其在隐私计算中的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究量子门扰动（常见于量子设备但经典计算机中不存在）如何作为数据增强技术，提升量子机器学习的潜力，并探讨其在经典机器学习中的应用。

Method: 采用随机Bloch球旋转（基本的SU(2)变换）作为量子启发的数据增强技术，应用于大规模ImageNet数据集，并与经典增强方法对比。

Result: 量子启发的增强方法显著提升了图像分类性能：Top-1准确率提高3%，Top-5准确率提高2.5%，F1分数从8%提升至12%。

Conclusion: 量子门扰动作为数据增强技术对图像分类任务有效，但在隐私计算中未表现出优势，需进一步研究其局限性。

Abstract: Understanding the impact of small quantum gate perturbations, which are
common in quantum digital devices but absent in classical computers, is crucial
for identifying potential advantages in quantum machine learning. While these
perturbations are typically seen as detrimental to quantum computation, they
can actually enhance performance by serving as a natural source of data
augmentation. Additionally, they can often be efficiently simulated on
classical hardware, enabling quantum-inspired approaches to improve classical
machine learning methods. In this paper, we investigate random Bloch sphere
rotations, which are fundamental SU(2) transformations, as a simple yet
effective quantum-inspired data augmentation technique. Unlike conventional
augmentations such as flipping, rotating, or cropping, quantum transformations
lack intuitive spatial interpretations, making their application to tasks like
image classification less straightforward. While common quantum augmentation
methods rely on applying quantum models or trainable quanvolutional layers to
classical datasets, we focus on the direct application of small-angle Bloch
rotations and their effect on classical data. Using the large-scale ImageNet
dataset, we demonstrate that our quantum-inspired augmentation method improves
image classification performance, increasing Top-1 accuracy by 3%, Top-5
accuracy by 2.5%, and the F$_1$ score from 8% to 12% compared to standard
classical augmentation methods. Finally, we examine the use of stronger unitary
augmentations. Although these transformations preserve information in
principle, they result in visually unrecognizable images with potential
applications for privacy computations. However, we show that our augmentation
approach and simple SU(2) transformations do not enhance differential privacy
and discuss the implications of this limitation.

</details>


### [69] [4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration](https://arxiv.org/abs/2506.22242)
*Jiahui Zhang,Yurui Chen,Yueming Xu,Ze Huang,Yanpeng Zhou,Yu-Jie Yuan,Xinyue Cai,Guowei Huang,Xingyue Quan,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: 论文提出4D-VLA方法，通过整合4D信息解决机器人数据预训练中的坐标系和状态混乱问题，提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法因输入信息不完整导致动作分布分散，影响预训练效率。

Method: 引入4D信息（深度和时间）到视觉特征中，采用记忆库采样策略提取关键帧。

Result: 在仿真和真实实验中，模型性能显著提升，成功率和空间理解能力优于OpenVLA。

Conclusion: 4D-VLA有效解决了预训练中的混乱问题，提升了模型的时空推理和适应性。

Abstract: Leveraging diverse robotic data for pretraining remains a critical challenge.
Existing methods typically model the dataset's action distribution using simple
observations as inputs. However, these inputs are often incomplete, resulting
in a dispersed conditional action distribution-an issue we refer to as
coordinate system chaos and state chaos. This inconsistency significantly
hampers pretraining efficiency. To address this, we propose 4D-VLA, a novel
approach that effectively integrates 4D information into the input to mitigate
these sources of chaos. Our model introduces depth and temporal information
into visual features with sequential RGB-D inputs, aligning the coordinate
systems of the robot and the scene. This alignment endows the model with strong
spatiotemporal reasoning capabilities while minimizing training overhead.
Additionally, we introduce memory bank sampling, a frame sampling strategy
designed to extract informative frames from historical images, further
improving effectiveness and efficiency. Experimental results demonstrate that
our pretraining method and architectural components substantially enhance model
performance. In both simulated and real-world experiments, our model achieves a
significant increase in success rate over OpenVLA. To further assess spatial
perception and generalization to novel views, we introduce MV-Bench, a
multi-view simulation benchmark. Our model consistently outperforms existing
methods, demonstrating stronger spatial understanding and adaptability.

</details>


### [70] [EAMamba: Efficient All-Around Vision State Space Model for Image Restoration](https://arxiv.org/abs/2506.22246)
*Yu-Cheng Lin,Yu-Syuan Xu,Hao-Wei Chen,Hsien-Kai Kuo,Chun-Yi Lee*

Main category: cs.CV

TL;DR: EAMamba框架通过多头部选择性扫描模块（MHSSM）和全方位扫描策略，解决了Vision Mamba在图像修复任务中的计算复杂性和局部像素遗忘问题，显著降低了FLOPs并保持性能。


<details>
  <summary>Details</summary>
Motivation: Vision Mamba在图像修复任务中表现出色，但仍面临计算复杂性和局部像素遗忘的挑战，需要改进。

Method: 提出EAMamba框架，引入MHSSM模块和全方位扫描策略，以高效聚合扫描序列并捕获全局信息。

Result: 实验表明，EAMamba在多项修复任务中显著降低FLOPs（31-89%），同时保持性能优势。

Conclusion: EAMamba通过创新设计有效解决了Vision Mamba的局限性，为图像修复任务提供了高效解决方案。

Abstract: Image restoration is a key task in low-level computer vision that aims to
reconstruct high-quality images from degraded inputs. The emergence of Vision
Mamba, which draws inspiration from the advanced state space model Mamba, marks
a significant advancement in this field. Vision Mamba demonstrates excellence
in modeling long-range dependencies with linear complexity, a crucial advantage
for image restoration tasks. Despite its strengths, Vision Mamba encounters
challenges in low-level vision tasks, including computational complexity that
scales with the number of scanning sequences and local pixel forgetting. To
address these limitations, this study introduces Efficient All-Around Mamba
(EAMamba), an enhanced framework that incorporates a Multi-Head Selective Scan
Module (MHSSM) with an all-around scanning mechanism. MHSSM efficiently
aggregates multiple scanning sequences, which avoids increases in computational
complexity and parameter count. The all-around scanning strategy implements
multiple patterns to capture holistic information and resolves the local pixel
forgetting issue. Our experimental evaluations validate these innovations
across several restoration tasks, including super resolution, denoising,
deblurring, and dehazing. The results validate that EAMamba achieves a
significant 31-89% reduction in FLOPs while maintaining favorable performance
compared to existing low-level Vision Mamba methods.

</details>


### [71] [COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication](https://arxiv.org/abs/2506.22274)
*Filippo Merlo,Ece Takmaz,Wenkai Chen,Albert Gatt*

Main category: cs.CV

TL;DR: 研究探讨视觉语言模型（VLMs）是否依赖场景上下文进行对象识别，并引入COOCO数据集测试模型在不同场景-对象一致性及干扰下的表现。


<details>
  <summary>Details</summary>
Motivation: 探索VLMs是否像人类一样依赖场景上下文生成对象引用，以理解其工作机制。

Method: 使用COOCO数据集测试VLMs在不同场景-对象一致性和干扰下的表现，并进行注意力分析。

Result: 模型会根据场景-对象语义相关性和干扰程度动态调整对上下文的依赖，尤其在高度一致或对象退化时更依赖上下文。

Conclusion: VLMs能动态平衡局部和上下文信息，注意力分析显示中层网络对目标对象的关注在中等干扰下尤为关键。

Abstract: Natural scenes provide us with rich contexts for object recognition and
reference. In particular, knowing what type of scene one is looking at
generates expectations about which objects will occur, and what their spatial
configuration should be. Do Vision-Language Models (VLMs) learn to rely on
scene contexts in a similar way, when generating references to objects? To
address this question, we introduce the \textit{Common Objects Out-of-Context
(COOCO)} dataset and test to what extent VLMs rely on scene context to refer to
objects under different degrees of scene-object congruency, and different
perturbations. Our findings show that models leverage scene context adaptively,
depending on both the semantic relatedness between object and scene and the
level of noise. In particular, models rely more on context under high
target-scene congruence or when objects are degraded. Attention analysis
reveals that successful object categorisation involves increased focus on the
target in mid-level layers, especially under moderate noise, suggesting that
VLMs dynamically balance local and contextual information for reference
generation. We make our dataset, code and models available at
\href{https://github.com/cs-nlp-uu/scenereg}{https://github.com/cs-nlp-uu/scenereg}.

</details>


### [72] [Rethinking Visual Token Reduction in LVLMs under Cross-modal Misalignment](https://arxiv.org/abs/2506.22283)
*Rui Xu,Yunke Wang,Yong Luo,Bo Du*

Main category: cs.CV

TL;DR: VisionDrop是一种无需训练的视觉标记剪枝框架，通过视觉内部注意力选择信息丰富的视觉标记，解决了跨模态不对齐问题，提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）中视觉标记数量远多于文本标记，导致计算开销大且可扩展性受限。现有方法依赖文本条件交互，但存在跨模态不对齐问题。

Method: 提出VisionDrop，基于视觉内部注意力选择视觉标记，设计渐进式剪枝流程，在模型层次中抑制冗余。

Result: 在多个基准测试中表现优于现有方法，无需额外训练或复杂修改。

Conclusion: VisionDrop简单有效，提升了推理效率并保持了任务性能。

Abstract: Large Vision-Language Models (LVLMs) encode visual inputs as dense sequences
of patch-level tokens to capture fine-grained semantics. These visual tokens
often outnumber their textual counterparts by a large margin, leading to
substantial computational overhead and limiting the scalability of LVLMs in
practice. Previous efforts have explored visual token reduction either prior to
or within the large language models (LLM). However, most in-LLM reduction
approaches rely on text-conditioned interactions, implicitly assuming that
textual tokens can reliably capture the importance of visual tokens. In this
work, we revisit this assumption and reveal causal, semantic, and spatial forms
of cross-modal misalignment. These misalignments undermine the effectiveness of
text-guided visual token reduction. To address this, we introduce VisionDrop, a
training-free, visual-only pruning framework that selects informative visual
tokens based on intra-modal (visual-to-visual) attention, without relying on
textual signals. To further suppress redundancy throughout the model hierarchy,
we treat the visual encoder and the LLM as a unified system and design a
progressive pruning pipeline. Our method performs dominant token selection and
lightweight contextual merging at multiple stages, enabling fine-grained visual
information to be retained even under aggressive token budgets. Extensive
experiments across diverse benchmarks show that VisionDrop achieves consistent
improvements over existing methods, despite requiring no additional training or
complex modifications. Its simple yet effective design enables efficient
inference while preserving strong performance across tasks.

</details>


### [73] [RoomCraft: Controllable and Complete 3D Indoor Scene Generation](https://arxiv.org/abs/2506.22291)
*Mengqi Zhou,Xipeng Wang,Yuxi Wang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: RoomCraft是一个多阶段管道，将真实图像、草图或文本描述转换为连贯的3D室内场景，通过约束驱动优化和冲突感知策略解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决现有神经生成方法在全局空间推理上的不足以及程序化方法在多约束场景下的困难，如物体碰撞和布局不完整。

Method: 结合场景生成管道和约束驱动优化框架，使用结构化信息提取、空间关系网络构建、启发式深度优先搜索算法（HDFS）和冲突感知定位策略（CAPS）。

Result: RoomCraft在生成真实、语义连贯且视觉吸引人的房间布局方面显著优于现有方法。

Conclusion: RoomCraft通过多阶段优化和冲突处理策略，有效提升了3D室内场景生成的性能和质量。

Abstract: Generating realistic 3D indoor scenes from user inputs remains a challenging
problem in computer vision and graphics, requiring careful balance of geometric
consistency, spatial relationships, and visual realism. While neural generation
methods often produce repetitive elements due to limited global spatial
reasoning, procedural approaches can leverage constraints for controllable
generation but struggle with multi-constraint scenarios. When constraints
become numerous, object collisions frequently occur, forcing the removal of
furniture items and compromising layout completeness.
  To address these limitations, we propose RoomCraft, a multi-stage pipeline
that converts real images, sketches, or text descriptions into coherent 3D
indoor scenes. Our approach combines a scene generation pipeline with a
constraint-driven optimization framework. The pipeline first extracts
high-level scene information from user inputs and organizes it into a
structured format containing room type, furniture items, and spatial relations.
It then constructs a spatial relationship network to represent furniture
arrangements and generates an optimized placement sequence using a
heuristic-based depth-first search (HDFS) algorithm to ensure layout coherence.
To handle complex multi-constraint scenarios, we introduce a unified constraint
representation that processes both formal specifications and natural language
inputs, enabling flexible constraint-oriented adjustments through a
comprehensive action space design. Additionally, we propose a Conflict-Aware
Positioning Strategy (CAPS) that dynamically adjusts placement weights to
minimize furniture collisions and ensure layout completeness.
  Extensive experiments demonstrate that RoomCraft significantly outperforms
existing methods in generating realistic, semantically coherent, and visually
appealing room layouts across diverse input modalities.

</details>


### [74] [OutDreamer: Video Outpainting with a Diffusion Transformer](https://arxiv.org/abs/2506.22298)
*Linhao Zhong,Fan Li,Yi Huang,Jianzhuang Liu,Renjing Pei,Fenglong Song*

Main category: cs.CV

TL;DR: OutDreamer是一个基于扩散变换器（DiT）的视频外绘框架，通过高效视频控制分支和条件外绘分支实现高质量视频内容生成，并在零样本任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 视频外绘任务需要同时满足时空一致性，现有方法在生成质量和适应性上仍有不足。

Method: 提出OutDreamer框架，包含视频控制分支和条件外绘分支，并引入掩码驱动的自注意力层和潜在对齐损失。

Result: 在广泛认可的基准测试中，OutDreamer在零样本任务中表现优于现有方法。

Conclusion: OutDreamer通过创新的框架设计和损失函数，显著提升了视频外绘的质量和适应性。

Abstract: Video outpainting is a challenging task that generates new video content by
extending beyond the boundaries of an original input video, requiring both
temporal and spatial consistency. Many state-of-the-art methods utilize latent
diffusion models with U-Net backbones but still struggle to achieve high
quality and adaptability in generated content. Diffusion transformers (DiTs)
have emerged as a promising alternative because of their superior performance.
We introduce OutDreamer, a DiT-based video outpainting framework comprising two
main components: an efficient video control branch and a conditional
outpainting branch. The efficient video control branch effectively extracts
masked video information, while the conditional outpainting branch generates
missing content based on these extracted conditions. Additionally, we propose a
mask-driven self-attention layer that dynamically integrates the given mask
information, further enhancing the model's adaptability to outpainting tasks.
Furthermore, we introduce a latent alignment loss to maintain overall
consistency both within and between frames. For long video outpainting, we
employ a cross-video-clip refiner to iteratively generate missing content,
ensuring temporal consistency across video clips. Extensive evaluations
demonstrate that our zero-shot OutDreamer outperforms state-of-the-art
zero-shot methods on widely recognized benchmarks.

</details>


### [75] [MatChA: Cross-Algorithm Matching with Feature Augmentation](https://arxiv.org/abs/2506.22336)
*Paula Carbó Cubero,Alberto Jaenal Gálvez,André Mateus,José Araújo,Patric Jensfelt*

Main category: cs.CV

TL;DR: 提出了一种针对跨检测器特征匹配的增强和翻译方法，显著提升了视觉定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨设备使用不同稀疏特征提取算法时表现不佳，尤其是当关键点不一致时。

Method: 通过特征描述符增强和潜在空间翻译，解决跨检测器特征匹配问题。

Result: 在多个基准测试中，该方法显著提升了图像匹配和视觉定位性能。

Conclusion: 该方法有效解决了跨检测器特征匹配的挑战，提升了实际应用中的视觉定位效果。

Abstract: State-of-the-art methods fail to solve visual localization in scenarios where
different devices use different sparse feature extraction algorithms to obtain
keypoints and their corresponding descriptors. Translating feature descriptors
is enough to enable matching. However, performance is drastically reduced in
cross-feature detector cases, because current solutions assume common
keypoints. This means that the same detector has to be used, which is rarely
the case in practice when different descriptors are used. The low repeatability
of keypoints, in addition to non-discriminatory and non-distinctive
descriptors, make the identification of true correspondences extremely
challenging. We present the first method tackling this problem, which performs
feature descriptor augmentation targeting cross-detector feature matching, and
then feature translation to a latent space. We show that our method
significantly improves image matching and visual localization in the
cross-feature scenario and evaluate the proposed method on several benchmarks.

</details>


### [76] [A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake](https://arxiv.org/abs/2506.22338)
*Luigi Russo,Deodato Tapete,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

TL;DR: 提出了一种基于单日期高分辨率SAR图像和多源地理空间数据的深度学习框架，用于快速检测建筑物损坏，无需依赖灾前数据。


<details>
  <summary>Details</summary>
Motivation: 解决光学卫星图像在灾害后因云层覆盖或缺乏灾前数据而受限的问题，实现快速灾害响应。

Method: 结合SAR图像、OSM建筑足迹、DSM数据和GEM属性，构建多模态深度学习模型，仅使用灾后数据。

Result: 在土耳其地震数据集中验证，显示结合地理空间特征显著提升检测性能和泛化能力。

Conclusion: 该框架能快速、可靠地评估建筑物损坏，支持灾害管理和恢复工作，具有广泛适用性。

Abstract: Building damage identification shortly after a disaster is crucial for
guiding emergency response and recovery efforts. Although optical satellite
imagery is commonly used for disaster mapping, its effectiveness is often
hampered by cloud cover or the absence of pre-event acquisitions. To overcome
these challenges, we introduce a novel multimodal deep learning (DL) framework
for detecting building damage using single-date very high resolution (VHR)
Synthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI)
COSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data.
Our method integrates SAR image patches, OpenStreetMap (OSM) building
footprints, digital surface model (DSM) data, and structural and exposure
attributes from the Global Earthquake Model (GEM) to improve detection accuracy
and contextual interpretation. Unlike existing approaches that depend on pre
and post event imagery, our model utilizes only post event data, facilitating
rapid deployment in critical scenarios. The framework effectiveness is
demonstrated using a new dataset from the 2023 earthquake in Turkey, covering
multiple cities with diverse urban settings. Results highlight that
incorporating geospatial features significantly enhances detection performance
and generalizability to previously unseen areas. By combining SAR imagery with
detailed vulnerability and exposure information, our approach provides reliable
and rapid building damage assessments without the dependency from available
pre-event data. Moreover, the automated and scalable data generation process
ensures the framework's applicability across diverse disaster-affected regions,
underscoring its potential to support effective disaster management and
recovery efforts. Code and data will be made available upon acceptance of the
paper.

</details>


### [77] [Closing the Performance Gap in Biometric Cryptosystems: A Deeper Analysis on Unlinkable Fuzzy Vaults](https://arxiv.org/abs/2506.22347)
*Hans Geißner,Christian Rathgeb*

Main category: cs.CV

TL;DR: 论文提出了一种基于等频区间的特征量化方法，解决了模糊保险库生物识别系统中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 模糊保险库生物识别系统因特征集大小不稳定和特征类型转换导致的信息丢失而性能下降。

Method: 提出一种基于等频区间的特征量化方法，确保固定特征集大小，并支持无训练适应任意区间数。

Result: 实验表明，该方法显著减少了模板保护引入的性能差距，且在主流生物识别系统中仅剩最小性能下降。

Conclusion: 该方法有效解决了性能下降问题，适用于多种生物识别模态。

Abstract: This paper analyses and addresses the performance gap in the fuzzy
vault-based \ac{BCS}. We identify unstable error correction capabilities, which
are caused by variable feature set sizes and their influence on similarity
thresholds, as a key source of performance degradation. This issue is further
compounded by information loss introduced through feature type transformations.
To address both problems, we propose a novel feature quantization method based
on \it{equal frequent intervals}. This method guarantees fixed feature set
sizes and supports training-free adaptation to any number of intervals. The
proposed approach significantly reduces the performance gap introduced by
template protection. Additionally, it integrates seamlessly with existing
systems to minimize the negative effects of feature transformation. Experiments
on state-of-the-art face, fingerprint, and iris recognition systems confirm
that only minimal performance degradation remains, demonstrating the
effectiveness of the method across major biometric modalities.

</details>


### [78] [From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications](https://arxiv.org/abs/2506.22360)
*Nouf Almesafri,Hector Figueiredo,Miguel Arana-Catania*

Main category: cs.CV

TL;DR: 比较CNN（ResNet34）和ViT（ViT B16）在事件相机数据上的性能，ResNet34在准确率上略优，但ViT B16在噪声环境下表现更稳健。


<details>
  <summary>Details</summary>
Motivation: 事件相机适用于动态环境（如无人机和自动驾驶），但传统深度学习模型在其上的性能尚不明确，需对比研究。

Method: 使用ResNet34和ViT B16在GEN1事件数据集上微调，评估标准条件和模拟噪声下的表现。

Result: ResNet34和ViT B16在干净数据上分别达到88%和86%准确率，但ViT B16在噪声下更稳健。

Conclusion: 研究为无人机等动态环境中的事件视觉系统提供了方法参考，ViT B16的稳健性尤其值得关注。

Abstract: This study investigates the performance of the two most relevant computer
vision deep learning architectures, Convolutional Neural Network and Vision
Transformer, for event-based cameras. These cameras capture scene changes,
unlike traditional frame-based cameras with capture static images, and are
particularly suited for dynamic environments such as UAVs and autonomous
vehicles. The deep learning models studied in this work are ResNet34 and ViT
B16, fine-tuned on the GEN1 event-based dataset. The research evaluates and
compares these models under both standard conditions and in the presence of
simulated noise. Initial evaluations on the clean GEN1 dataset reveal that
ResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with
ResNet34 showing a slight advantage in classification accuracy. However, the
ViT B16 model demonstrates notable robustness, particularly given its
pre-training on a smaller dataset. Although this study focuses on ground-based
vehicle classification, the methodologies and findings hold significant promise
for adaptation to UAV contexts, including aerial object classification and
event-based vision systems for aviation-related tasks.

</details>


### [79] [Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD Detection via Graph Score Propagation](https://arxiv.org/abs/2506.22375)
*Tiankai Chen,Yushu Li,Adam Goodge,Fei Teng,Xulei Yang,Tianrui Li,Xun Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉语言模型（VLM）的无训练框架，用于3D点云数据的OOD检测，通过图分数传播（GSP）方法显著提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 3D点云数据的OOD检测在安全感知应用中至关重要，但现有方法主要针对2D图像，3D环境下的检测仍面临挑战。

Method: 利用VLM构建基于类别原型和测试数据的图，提出GSP方法结合提示聚类和自训练负提示，优化OOD评分。

Result: GSP在合成和真实数据集上均优于现有方法，且适用于少样本场景。

Conclusion: GSP为3D点云OOD检测提供了一种高效且适应性强的解决方案。

Abstract: Out-of-distribution (OOD) detection in 3D point cloud data remains a
challenge, particularly in applications where safe and robust perception is
critical. While existing OOD detection methods have shown progress for 2D image
data, extending these to 3D environments involves unique obstacles. This paper
introduces a training-free framework that leverages Vision-Language Models
(VLMs) for effective OOD detection in 3D point clouds. By constructing a graph
based on class prototypes and testing data, we exploit the data manifold
structure to enhancing the effectiveness of VLMs for 3D OOD detection. We
propose a novel Graph Score Propagation (GSP) method that incorporates prompt
clustering and self-training negative prompting to improve OOD scoring with
VLM. Our method is also adaptable to few-shot scenarios, providing options for
practical applications. We demonstrate that GSP consistently outperforms
state-of-the-art methods across synthetic and real-world datasets 3D point
cloud OOD detection.

</details>


### [80] [Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment](https://arxiv.org/abs/2506.22385)
*Yue Zhang,Jilei Sun,Yunhui Guo,Vibhav Gogate*

Main category: cs.CV

TL;DR: 论文提出了一种新任务DVidE，旨在提升视频多模态模型在动态推理中的能力，通过分类和生成任务解决抽象推理问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频多模态模型在抽象和动态推理方面表现不足，无法根据新信息更新初始推断。

Method: 提出Chain of Counterfactual Thought框架用于分类任务，结合反事实推理和ASR增强视频内容；生成任务则结合ASR输出和LLM生成连贯更新。

Result: 实验结果表明，所提方法显著提升了模型的动态推理能力。

Conclusion: DVidE任务及相关框架有效增强了视频多模态模型的动态推理能力，并提供了新的基准数据集和评估指标。

Abstract: Video Large Multimodal Models (VLMMs) have made impressive strides in
understanding video content, but they often struggle with abstract and adaptive
reasoning-the ability to revise their interpretations when new information
emerges. In reality, conclusions are rarely set in stone; additional context
can strengthen or weaken an initial inference. To address this, we introduce
Defeasible Video Entailment (DVidE), a new task that challenges models to think
like doubters, constantly updating their reasoning based on evolving evidence.
In DVidE, given a video premise and a textual hypothesis, models must determine
whether a new update strengthens or weakens the hypothesis (classification
version) or generate a coherent update that modifies the entailment
relationship (generation version). For solving the classification task, we
propose the Chain of Counterfactual Thought framework, utilizing counterfactual
reasoning, ASR-enhanced video content, and rationale refinement to reduce
inference bias. For the generation task, we develop a framework that combines
ASR output with a Large Language Model (LLM) to produce coherent, contextually
relevant updates aligned with the intended strengthener or weakener goals.
Additionally, we introduce a novel benchmark dataset, with
strengthener/weakener annotations and an LLM-based evaluation metric
specifically designed for assessing generative performance. Experimental
results demonstrate significant improvements, highlighting our proposed method
in enhancing dynamic reasoning capabilities of VLMMs.

</details>


### [81] [Test-Time Consistency in Vision Language Models](https://arxiv.org/abs/2506.22395)
*Shih-Han Chou,Shivam Chandhok,James J. Little,Leonid Sigal*

Main category: cs.CV

TL;DR: 提出了一种无需监督重训练的测试时一致性框架，通过交叉熵一致性损失和伪标签一致性损失提升视觉语言模型的语义一致性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在语义等效输入下表现不一致，影响其可靠性和鲁棒性，现有方法需修改架构或大规模微调。

Method: 提出了一种后处理、模型无关的测试时一致性框架，利用交叉熵一致性损失和伪标签一致性损失，仅需单次测试输入即可提升一致性。

Result: 在MM-R3基准测试中，该方法显著提升了现有模型的语义一致性。

Conclusion: 该方法为多模态学习的推理时适应提供了新方向。

Abstract: Vision-Language Models (VLMs) have achieved impressive performance across a
wide range of multimodal tasks, yet they often exhibit inconsistent behavior
when faced with semantically equivalent inputs, undermining their reliability
and robustness. Recent benchmarks, such as MM-R3, highlight that even
state-of-the-art VLMs can produce divergent predictions across semantically
equivalent inputs, despite maintaining high average accuracy. Prior work
addresses this issue by modifying model architectures or conducting large-scale
fine-tuning on curated datasets. In contrast, we propose a simple and effective
test-time consistency framework that enhances semantic consistency without
supervised re-training. Our method is entirely post-hoc, model-agnostic, and
applicable to any VLM with access to its weights. Given a single test point, we
enforce consistent predictions via two complementary objectives: (i) a
Cross-Entropy Agreement Loss that aligns predictive distributions across
semantically equivalent inputs, and (ii) a Pseudo-Label Consistency Loss that
draws outputs toward a self-averaged consensus. Our method is plug-and-play and
leverages information from a single test input itself to improve consistency.
Experiments on the MM-R3 benchmark show that our framework yields substantial
gains in consistency across state-of-the-art models, establishing a new
direction for inference-time adaptation in multimodal learning.

</details>


### [82] [Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy](https://arxiv.org/abs/2506.22432)
*Yuhao Liu,Tengfei Wang,Fang Liu,Zhenwei Wang,Rynson W. H. Lau*

Main category: cs.CV

TL;DR: Shape-for-Motion 是一种新颖的视频编辑框架，通过3D代理实现精确且一致的视频编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以确保细粒度与用户意图对齐，因此需要一种更精确、一致的控制工具。

Method: 使用3D代理（时间一致的网格）进行编辑，设计双传播策略简化编辑过程，并通过解耦视频扩散模型生成结果。

Result: 支持多种精确且物理一致的操作（如姿态编辑、旋转、纹理修改等），实验证明其优越性。

Conclusion: Shape-for-Motion 是高质量、可控视频编辑工作流的重要进展。

Abstract: Recent advances in deep generative modeling have unlocked unprecedented
opportunities for video synthesis. In real-world applications, however, users
often seek tools to faithfully realize their creative editing intentions with
precise and consistent control. Despite the progress achieved by existing
methods, ensuring fine-grained alignment with user intentions remains an open
and challenging problem. In this work, we present Shape-for-Motion, a novel
framework that incorporates a 3D proxy for precise and consistent video
editing. Shape-for-Motion achieves this by converting the target object in the
input video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be
performed directly on the proxy and then inferred back to the video frames. To
simplify the editing process, we design a novel Dual-Propagation Strategy that
allows users to perform edits on the 3D mesh of a single frame, and the edits
are then automatically propagated to the 3D meshes of the other frames. The 3D
meshes for different frames are further projected onto the 2D space to produce
the edited geometry and texture renderings, which serve as inputs to a
decoupled video diffusion model for generating edited results. Our framework
supports various precise and physically-consistent manipulations across the
video frames, including pose editing, rotation, scaling, translation, texture
modification, and object composition. Our approach marks a key step toward
high-quality, controllable video editing workflows. Extensive experiments
demonstrate the superiority and effectiveness of our approach. Project page:
https://shapeformotion.github.io/

</details>


### [83] [WarpRF: Multi-View Consistency for Training-Free Uncertainty Quantification and Applications in Radiance Fields](https://arxiv.org/abs/2506.22433)
*Sadra Safadoust,Fabio Tosi,Fatma Güney,Matteo Poggi*

Main category: cs.CV

TL;DR: WarpRF是一个无需训练、通用的框架，用于量化辐射场的不确定性，通过反向映射和一致性测量实现。


<details>
  <summary>Details</summary>
Motivation: 量化辐射场的不确定性，以支持下游任务如主动视图选择和主动映射。

Method: 利用反向映射和几何一致性假设，从未见视角投影可靠渲染并测量一致性。

Result: 在不确定性量化和下游任务中表现优异，超越现有方法。

Conclusion: WarpRF简单、低成本且通用，适用于任何辐射场实现。

Abstract: We introduce WarpRF, a training-free general-purpose framework for
quantifying the uncertainty of radiance fields. Built upon the assumption that
photometric and geometric consistency should hold among images rendered by an
accurate model, WarpRF quantifies its underlying uncertainty from an unseen
point of view by leveraging backward warping across viewpoints, projecting
reliable renderings to the unseen viewpoint and measuring the consistency with
images rendered there. WarpRF is simple and inexpensive, does not require any
training, and can be applied to any radiance field implementation for free.
WarpRF excels at both uncertainty quantification and downstream tasks, e.g.,
active view selection and active mapping, outperforming any existing method
tailored to specific frameworks.

</details>


### [84] [MiCo: Multi-image Contrast for Reinforcement Visual Reasoning](https://arxiv.org/abs/2506.22434)
*Xi Chen,Mingkang Zhu,Shaoteng Liu,Xiaoyang Wu,Xiaogang Xu,Yu Liu,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

TL;DR: 该研究通过自监督视觉表示学习，利用图像三重构造和规则强化学习，实现了跨多图像的链式思维推理，无需人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法依赖人工标注的问题，探索图像内在约束作为监督信号，以提升视觉语言模型在多图像推理中的表现。

Method: 构建图像三重（两个增强视图和一个相似但不同的图像），通过规则强化学习优化模型，使其生成推理过程比较图像。

Result: 模型在视觉比较任务上训练后，能泛化到多种问题，显著提升多图像推理基准性能，并在通用视觉任务中表现优异。

Conclusion: 自监督学习和规则强化学习的结合，为跨图像推理提供了一种高效且无需人工标注的解决方案。

Abstract: This work explores enabling Chain-of-Thought (CoT) reasoning to link visual
cues across multiple images. A straightforward solution is to adapt rule-based
reinforcement learning for Vision-Language Models (VLMs). However, such methods
typically rely on manually curated question-answer pairs, which can be
particularly challenging when dealing with fine grained visual details and
complex logic across images. Inspired by self-supervised visual representation
learning, we observe that images contain inherent constraints that can serve as
supervision. Based on this insight, we construct image triplets comprising two
augmented views of the same image and a third, similar but distinct image.
During training, the model is prompted to generate a reasoning process to
compare these images (i.e., determine same or different). Then we optimize the
model with rule-based reinforcement learning. Due to the high visual similarity
and the presence of augmentations, the model must attend to subtle visual
changes and perform logical reasoning to succeed. Experiments show that,
although trained solely on visual comparison tasks, the learned reasoning
ability generalizes effectively to a wide range of questions. Without relying
on any human-annotated question-answer pairs, our method achieves significant
improvements on multi-image reasoning benchmarks and shows strong performance
on general vision tasks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [85] [PhotonSplat: 3D Scene Reconstruction and Colorization from SPAD Sensors](https://arxiv.org/abs/2506.21680)
*Sai Sri Teja,Sreevidya Chintalapati,Vinayak Gupta,Mukund Varma T,Haejoon Lee,Aswin Sankaranarayanan,Kaushik Mitra*

Main category: eess.IV

TL;DR: 提出PhotonSplat框架，利用SPAD阵列处理运动模糊问题，实现高质量3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染技术在输入图像因运动模糊而损坏时效果不佳，SPAD阵列的高速成像能力为解决这一问题提供了可能。

Method: 引入PhotonSplat框架，结合3D空间滤波技术处理SPAD二进制图像的噪声，支持无参考生成和有参考着色。

Result: 成功实现从SPAD二进制图像直接重建3D场景，并扩展至动态场景表示。

Conclusion: PhotonSplat有效解决了噪声与模糊的权衡问题，为下游任务提供了支持。

Abstract: Advances in 3D reconstruction using neural rendering have enabled
high-quality 3D capture. However, they often fail when the input imagery is
corrupted by motion blur, due to fast motion of the camera or the objects in
the scene. This work advances neural rendering techniques in such scenarios by
using single-photon avalanche diode (SPAD) arrays, an emerging sensing
technology capable of sensing images at extremely high speeds. However, the use
of SPADs presents its own set of unique challenges in the form of binary
images, that are driven by stochastic photon arrivals. To address this, we
introduce PhotonSplat, a framework designed to reconstruct 3D scenes directly
from SPAD binary images, effectively navigating the noise vs. blur trade-off.
Our approach incorporates a novel 3D spatial filtering technique to reduce
noise in the renderings. The framework also supports both no-reference using
generative priors and reference-based colorization from a single blurry image,
enabling downstream applications such as segmentation, object detection and
appearance editing tasks. Additionally, we extend our method to incorporate
dynamic scene representations, making it suitable for scenes with moving
objects. We further contribute PhotonScenes, a real-world multi-view dataset
captured with the SPAD sensors.

</details>


### [86] [TUS-REC2024: A Challenge to Reconstruct 3D Freehand Ultrasound Without External Tracker](https://arxiv.org/abs/2506.21765)
*Qi Li,Shaheer U. Saeed,Yuliang Huang,Mingyuan Luo,Zhongnuo Yan,Jiongquan Chen,Xin Yang,Dong Ni,Nektarios Winter,Phuc Nguyen,Lucas Steinberger,Caelan Haney,Yuan Zhao,Mingjie Jiang,Bowen Ren,SiYeoul Lee,Seonho Kim,MinKyung Seo,MinWoo Kim,Yimeng Dou,Zhiwei Zhang,Yin Li,Tomy Varghese,Dean C. Barratt,Matthew J. Clarkson,Tom Vercauteren,Yipeng Hu*

Main category: eess.IV

TL;DR: TUS-REC2024挑战赛旨在推动无追踪自由手超声3D重建技术的发展，提供了首个公开数据集、基线模型和评估框架，吸引了43个团队参与。


<details>
  <summary>Details</summary>
Motivation: 解决无追踪自由手超声重建中的运动估计、漂移累积和通用性问题。

Method: 挑战赛提供了数据集和评估框架，参赛团队提交了多种算法，包括循环模型、配准驱动体积优化、注意力机制和物理模型。

Result: 6个团队提交了21个有效解决方案，展示了当前技术的进展和局限性。

Conclusion: 挑战赛将持续发展，推动该领域研究，数据和方法已公开以促进可重复性。

Abstract: Trackerless freehand ultrasound reconstruction aims to reconstruct 3D volumes
from sequences of 2D ultrasound images without relying on external tracking
systems, offering a low-cost, portable, and widely deployable alternative for
volumetric imaging. However, it presents significant challenges, including
accurate inter-frame motion estimation, minimisation of drift accumulation over
long sequences, and generalisability across scanning protocols. The TUS-REC2024
Challenge was established to benchmark and accelerate progress in trackerless
3D ultrasound reconstruction by providing a publicly available dataset for the
first time, along with a baseline model and evaluation framework. The Challenge
attracted over 43 registered teams, of which 6 teams submitted 21 valid
dockerized solutions. Submitted methods spanned a wide range of algorithmic
approaches, including recurrent models, registration-driven volume refinement,
attention, and physics-informed models. This paper presents an overview of the
Challenge design, summarises the key characteristics of the dataset, provides a
concise literature review, introduces the technical details of the underlying
methodology working with tracked freehand ultrasound data, and offers a
comparative analysis of submitted methods across multiple evaluation metrics.
The results highlight both the progress and current limitations of
state-of-the-art approaches in this domain, and inform directions for future
research. The data, evaluation code, and baseline are publicly available to
facilitate ongoing development and reproducibility. As a live and evolving
benchmark, this Challenge is designed to be continuously developed and
improved. The Challenge was held at MICCAI 2024 and will be organised again at
MICCAI 2025, reflecting its growing impact and the sustained commitment to
advancing this field.

</details>


### [87] [Physical Degradation Model-Guided Interferometric Hyperspectral Reconstruction with Unfolding Transformer](https://arxiv.org/abs/2506.21880)
*Yuansheng Li,Yunhao Zou,Linwei Chen,Ying Fu*

Main category: eess.IV

TL;DR: 提出了一种新的干涉高光谱成像（IHI）重建方法，通过物理模型生成训练数据，并设计了一种新型深度学习架构IHRUT，显著提升了重建性能。


<details>
  <summary>Details</summary>
Motivation: IHI在遥感任务中具有优势，但受限于复杂误差和现有算法的不足，缺乏训练数据和难以消除特定退化成分是主要挑战。

Method: 建立简化的IHI退化模型和参数估计方法，生成合成训练数据；设计IHRUT架构，结合条纹模式增强机制和空间-光谱变换器。

Result: 实验结果表明，该方法在性能和泛化能力上表现优越。

Conclusion: 提出的方法有效解决了IHI重建中的关键问题，为深度学习在IHI中的应用提供了新思路。

Abstract: Interferometric Hyperspectral Imaging (IHI) is a critical technique for
large-scale remote sensing tasks due to its advantages in flux and spectral
resolution. However, IHI is susceptible to complex errors arising from imaging
steps, and its quality is limited by existing signal processing-based
reconstruction algorithms. Two key challenges hinder performance enhancement:
1) the lack of training datasets. 2) the difficulty in eliminating IHI-specific
degradation components through learning-based methods. To address these
challenges, we propose a novel IHI reconstruction pipeline. First, based on
imaging physics and radiometric calibration data, we establish a simplified yet
accurate IHI degradation model and a parameter estimation method. This model
enables the synthesis of realistic IHI training datasets from hyperspectral
images (HSIs), bridging the gap between IHI reconstruction and deep learning.
Second, we design the Interferometric Hyperspectral Reconstruction Unfolding
Transformer (IHRUT), which achieves effective spectral correction and detail
restoration through a stripe-pattern enhancement mechanism and a
spatial-spectral transformer architecture. Experimental results demonstrate the
superior performance and generalization capability of our method.

</details>


### [88] [UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields](https://arxiv.org/abs/2506.21884)
*Fabian Perez,Sara Rojas,Carlos Hinojosa,Hoover Rueda-Chacón,Bernard Ghanem*

Main category: eess.IV

TL;DR: UnMix-NeRF结合光谱解混技术，实现了基于NeRF的高光谱新视角合成和无监督材料分割。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF分割方法仅依赖RGB数据，缺乏材料属性信息，限制了在机器人、增强现实等应用中的准确性。

Method: 通过建模光谱反射的漫反射和镜面反射分量，利用全局端元字典表示纯材料特征，并通过点级丰度分布实现材料分割。

Result: 实验表明，UnMix-NeRF在光谱重建和材料分割上优于现有方法。

Conclusion: UnMix-NeRF为材料感知提供了新框架，支持场景编辑和灵活的材料外观操作。

Abstract: Neural Radiance Field (NeRF)-based segmentation methods focus on object
semantics and rely solely on RGB data, lacking intrinsic material properties.
This limitation restricts accurate material perception, which is crucial for
robotics, augmented reality, simulation, and other applications. We introduce
UnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling
joint hyperspectral novel view synthesis and unsupervised material
segmentation. Our method models spectral reflectance via diffuse and specular
components, where a learned dictionary of global endmembers represents pure
material signatures, and per-point abundances capture their distribution. For
material segmentation, we use spectral signature predictions along learned
endmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF
enables scene editing by modifying learned endmember dictionaries for flexible
material-based appearance manipulation. Extensive experiments validate our
approach, demonstrating superior spectral reconstruction and material
segmentation to existing methods. Project page:
https://www.factral.co/UnMix-NeRF.

</details>


### [89] [StableCodec: Taming One-Step Diffusion for Extreme Image Compression](https://arxiv.org/abs/2506.21977)
*Tianyu Zhang,Xin Luo,Li Li,Dong Liu*

Main category: eess.IV

TL;DR: StableCodec提出了一种一步扩散方法，用于高保真和高真实感的极端图像压缩，解决了现有方法在超低比特率下需要多步去噪和像素级一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的图像压缩方法在超低比特率下需要大量去噪步骤且牺牲重建保真度，限制了实时应用。

Method: 提出StableCodec，包括高效的深度压缩潜在编解码器和双分支编码结构，结合端到端优化和联合比特率约束。

Result: 在CLIC 2020、DIV2K和Kodak数据集上，StableCodec在FID、KID和DISTS指标上显著优于现有方法，比特率低至0.005 bpp。

Conclusion: StableCodec在超低比特率下实现了高真实感和高保真度，且推理速度与主流变换编码方案相当。

Abstract: Diffusion-based image compression has shown remarkable potential for
achieving ultra-low bitrate coding (less than 0.05 bits per pixel) with high
realism, by leveraging the generative priors of large pre-trained text-to-image
diffusion models. However, current approaches require a large number of
denoising steps at the decoder to generate realistic results under extreme
bitrate constraints, limiting their application in real-time compression
scenarios. Additionally, these methods often sacrifice reconstruction fidelity,
as diffusion models typically fail to guarantee pixel-level consistency. To
address these challenges, we introduce StableCodec, which enables one-step
diffusion for high-fidelity and high-realism extreme image compression with
improved coding efficiency. To achieve ultra-low bitrates, we first develop an
efficient Deep Compression Latent Codec to transmit a noisy latent
representation for a single-step denoising process. We then propose a
Dual-Branch Coding Structure, consisting of a pair of auxiliary encoder and
decoder, to enhance reconstruction fidelity. Furthermore, we adopt end-to-end
optimization with joint bitrate and pixel-level constraints. Extensive
experiments on the CLIC 2020, DIV2K, and Kodak dataset demonstrate that
StableCodec outperforms existing methods in terms of FID, KID and DISTS by a
significant margin, even at bitrates as low as 0.005 bits per pixel, while
maintaining strong fidelity. Additionally, StableCodec achieves inference
speeds comparable to mainstream transform coding schemes. All source code are
available at https://github.com/LuizScarlet/StableCodec.

</details>


### [90] [Noise-Inspired Diffusion Model for Generalizable Low-Dose CT Reconstruction](https://arxiv.org/abs/2506.22012)
*Qi Gao,Zhihao Chen,Dong Zeng,Junping Zhang,Jianhua Ma,Hongming Shan*

Main category: eess.IV

TL;DR: 论文提出了一种名为NEED的噪声启发扩散模型，用于低剂量CT重建，通过双域重建和噪声特性匹配，显著提升了重建和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在低剂量CT重建中对未见剂量数据的泛化问题，避免依赖大量配对数据或微调。

Method: 提出基于噪声特性的扩散模型：1) 投影数据去噪的移位泊松扩散模型；2) 图像重建的双引导扩散模型。

Result: 在多个数据集上，NEED在重建和泛化性能上均优于现有方法。

Conclusion: NEED仅需正常剂量数据训练，可有效扩展到不同剂量水平，具有广泛的应用潜力。

Abstract: The generalization of deep learning-based low-dose computed tomography (CT)
reconstruction models to doses unseen in the training data is important and
remains challenging. Previous efforts heavily rely on paired data to improve
the generalization performance and robustness through collecting either diverse
CT data for re-training or a few test data for fine-tuning. Recently, diffusion
models have shown promising and generalizable performance in low-dose CT (LDCT)
reconstruction, however, they may produce unrealistic structures due to the CT
image noise deviating from Gaussian distribution and imprecise prior
information from the guidance of noisy LDCT images. In this paper, we propose a
noise-inspired diffusion model for generalizable LDCT reconstruction, termed
NEED, which tailors diffusion models for noise characteristics of each domain.
First, we propose a novel shifted Poisson diffusion model to denoise projection
data, which aligns the diffusion process with the noise model in pre-log LDCT
projections. Second, we devise a doubly guided diffusion model to refine
reconstructed images, which leverages LDCT images and initial reconstructions
to more accurately locate prior information and enhance reconstruction
fidelity. By cascading these two diffusion models for dual-domain
reconstruction, our NEED requires only normal-dose data for training and can be
effectively extended to various unseen dose levels during testing via a time
step matching strategy. Extensive qualitative, quantitative, and
segmentation-based evaluations on two datasets demonstrate that our NEED
consistently outperforms state-of-the-art methods in reconstruction and
generalization performance. Source code is made available at
https://github.com/qgao21/NEED.

</details>


### [91] [Towards Scalable and Robust White Matter Lesion Localization via Multimodal Deep Learning](https://arxiv.org/abs/2506.22041)
*Julia Machnio,Sebastian Nørgaard Llambias,Mads Nielsen,Mostafa Mehdipour Ghazi*

Main category: eess.IV

TL;DR: 提出了一种深度学习框架，用于白质高信号（WMH）的分割和定位，支持单模态和多模态MRI输入，并在原生空间中操作。多模态输入显著提升了分割性能，但联合任务学习效果不如单独模型。


<details>
  <summary>Details</summary>
Motivation: 白质高信号（WMH）是神经退行性疾病的标志，其准确分割和定位对诊断和监测至关重要。现有方法在模态缺失和空间整合上存在不足。

Method: 提出深度学习框架，支持单模态（FLAIR或T1）和多模态（FLAIR+T1）输入，并引入多任务模型联合预测病变和区域掩膜。

Result: 多模态输入显著优于单模态模型；模态可互换设置牺牲精度换取鲁棒性；联合任务学习效果较差。

Conclusion: 多模态融合对WMH分析准确性和鲁棒性有显著提升，但联合建模需进一步优化。

Abstract: White matter hyperintensities (WMH) are radiological markers of small vessel
disease and neurodegeneration, whose accurate segmentation and spatial
localization are crucial for diagnosis and monitoring. While multimodal MRI
offers complementary contrasts for detecting and contextualizing WM lesions,
existing approaches often lack flexibility in handling missing modalities and
fail to integrate anatomical localization efficiently. We propose a deep
learning framework for WM lesion segmentation and localization that operates
directly in native space using single- and multi-modal MRI inputs. Our study
evaluates four input configurations: FLAIR-only, T1-only, concatenated FLAIR
and T1, and a modality-interchangeable setup. It further introduces a
multi-task model for jointly predicting lesion and anatomical region masks to
estimate region-wise lesion burden. Experiments conducted on the MICCAI WMH
Segmentation Challenge dataset demonstrate that multimodal input significantly
improves the segmentation performance, outperforming unimodal models. While the
modality-interchangeable setting trades accuracy for robustness, it enables
inference in cases with missing modalities. Joint lesion-region segmentation
using multi-task learning was less effective than separate models, suggesting
representational conflict between tasks. Our findings highlight the utility of
multimodal fusion for accurate and robust WMH analysis, and the potential of
joint modeling for integrated predictions.

</details>


### [92] [Advanced Deep Learning Techniques for Automated Segmentation of Type B Aortic Dissections](https://arxiv.org/abs/2506.22222)
*Hao Xu,Ruth Lim,Brian E. Chapman*

Main category: eess.IV

TL;DR: 该论文提出四种深度学习模型用于B型主动脉夹层的自动分割，显著提高了分割精度。


<details>
  <summary>Details</summary>
Motivation: 主动脉夹层是一种危及生命的心血管疾病，需要从CTA图像中准确分割真腔、假腔和假腔血栓以进行有效管理。手动分割耗时且存在变异性，因此需要自动化解决方案。

Method: 研究开发了四种基于深度学习的管道：单步模型、顺序模型、顺序多任务模型和集成模型，采用3D U-Net和Swin-UnetR架构。使用100例回顾性CTA图像进行训练、验证和测试。

Result: 提出的方法在分割精度上表现优异，真腔、假腔和假腔血栓的Dice系数分别为0.91±0.07、0.88±0.18和0.47±0.25，优于现有研究。

Conclusion: 所提出的管道能够准确分割B型主动脉夹层特征，为监测和治疗计划提供形态学参数。

Abstract: Purpose: Aortic dissections are life-threatening cardiovascular conditions
requiring accurate segmentation of true lumen (TL), false lumen (FL), and false
lumen thrombosis (FLT) from CTA images for effective management. Manual
segmentation is time-consuming and variable, necessitating automated solutions.
Materials and Methods: We developed four deep learning-based pipelines for Type
B aortic dissection segmentation: a single-step model, a sequential model, a
sequential multi-task model, and an ensemble model, utilizing 3D U-Net and
Swin-UnetR architectures. A dataset of 100 retrospective CTA images was split
into training (n=80), validation (n=10), and testing (n=10). Performance was
assessed using the Dice Coefficient and Hausdorff Distance. Results: Our
approach achieved superior segmentation accuracy, with Dice Coefficients of
0.91 $\pm$ 0.07 for TL, 0.88 $\pm$ 0.18 for FL, and 0.47 $\pm$ 0.25 for FLT,
outperforming Yao et al. (1), who reported 0.78 $\pm$ 0.20, 0.68 $\pm$ 0.18,
and 0.25 $\pm$ 0.31, respectively. Conclusion: The proposed pipelines provide
accurate segmentation of TBAD features, enabling derivation of morphological
parameters for surveillance and treatment planning

</details>


### [93] [Cardiovascular disease classification using radiomics and geometric features from cardiac CT](https://arxiv.org/abs/2506.22226)
*Ajay Mittal,Raghav Mehta,Omar Todd,Philipp Seeböck,Georg Langs,Ben Glocker*

Main category: eess.IV

TL;DR: 论文提出了一种基于图像分割、配准和下游分类的三步心血管疾病（CVD）分类方法，通过提取临床可解释的特征，显著提高了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法直接处理原始CT数据或结合心脏结构分割，临床解释性较差。本文旨在通过分步方法提高分类的临床可解释性和准确性。

Method: 采用Atlas-ISTN框架和分割基础模型生成解剖结构分割和健康图谱，提取放射组学特征和几何特征用于CVD分类。

Result: 在ASOCA数据集上，该方法分类准确率达87.50%，显著优于直接使用原始CT图像的模型（67.50%）。

Conclusion: 分步方法结合临床可解释特征能有效提升CVD分类性能，同时增强模型的临床实用性。

Abstract: Automatic detection and classification of Cardiovascular disease (CVD) from
Computed Tomography (CT) images play an important part in facilitating
better-informed clinical decisions. However, most of the recent deep learning
based methods either directly work on raw CT data or utilize it in pair with
anatomical cardiac structure segmentation by training an end-to-end classifier.
As such, these approaches become much more difficult to interpret from a
clinical perspective. To address this challenge, in this work, we break down
the CVD classification pipeline into three components: (i) image segmentation,
(ii) image registration, and (iii) downstream CVD classification. Specifically,
we utilize the Atlas-ISTN framework and recent segmentation foundational models
to generate anatomical structure segmentation and a normative healthy atlas.
These are further utilized to extract clinically interpretable radiomic
features as well as deformation field based geometric features (through atlas
registration) for CVD classification. Our experiments on the publicly available
ASOCA dataset show that utilizing these features leads to better CVD
classification accuracy (87.50\%) when compared against classification model
trained directly on raw CT images (67.50\%). Our code is publicly available:
https://github.com/biomedia-mira/grc-net

</details>


### [94] [DIGS: Dynamic CBCT Reconstruction using Deformation-Informed 4D Gaussian Splatting and a Low-Rank Free-Form Deformation Model](https://arxiv.org/abs/2506.22280)
*Yuliang Huang,Imraj Singh,Thomas Joyce,Kris Thielemans,Jamie R. McClelland*

Main category: eess.IV

TL;DR: 该论文提出了一种基于自由形变（FFD）的空间基函数和变形感知框架，用于动态CBCT重建，解决了现有方法计算成本高和运动不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 动态CBCT在放疗中广泛应用，但呼吸运动导致图像伪影。现有方法如4D高斯泼溅（4DGS）计算成本高且缺乏空间正则化，限制了其应用。

Method: 引入FFD空间基函数和变形感知框架，通过统一变形场耦合高斯均值位置、尺度和旋转的时间演化，确保运动一致性。

Result: 在六个CBCT数据集上验证，图像质量优于HexPlane，速度提升6倍。

Conclusion: 变形感知4DGS为高效、运动补偿的CBCT重建提供了新思路，代码已开源。

Abstract: 3D Cone-Beam CT (CBCT) is widely used in radiotherapy but suffers from motion
artifacts due to breathing. A common clinical approach mitigates this by
sorting projections into respiratory phases and reconstructing images per
phase, but this does not account for breathing variability. Dynamic CBCT
instead reconstructs images at each projection, capturing continuous motion
without phase sorting. Recent advancements in 4D Gaussian Splatting (4DGS)
offer powerful tools for modeling dynamic scenes, yet their application to
dynamic CBCT remains underexplored. Existing 4DGS methods, such as HexPlane,
use implicit motion representations, which are computationally expensive. While
explicit low-rank motion models have been proposed, they lack spatial
regularization, leading to inconsistencies in Gaussian motion. To address these
limitations, we introduce a free-form deformation (FFD)-based spatial basis
function and a deformation-informed framework that enforces consistency by
coupling the temporal evolution of Gaussian's mean position, scale, and
rotation under a unified deformation field. We evaluate our approach on six
CBCT datasets, demonstrating superior image quality with a 6x speedup over
HexPlane. These results highlight the potential of deformation-informed 4DGS
for efficient, motion-compensated CBCT reconstruction. The code is available at
https://github.com/Yuliang-Huang/DIGS.

</details>


### [95] [Dehazing Light Microscopy Images with Guided Conditional Flow Matching: finding a sweet spot between fidelity and realism](https://arxiv.org/abs/2506.22397)
*Anirban Ray,Ashesh,Florian Jug*

Main category: eess.IV

TL;DR: HazeMatching是一种新的迭代方法，用于去雾光学显微镜图像，平衡了数据保真度和感知真实性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在去雾显微镜图像时无法同时兼顾数据保真度和感知真实性的问题。

Method: 采用条件流匹配框架，通过条件速度场中的模糊观察引导生成过程。

Result: 在5个数据集上评估，HazeMatching在保真度和真实性之间取得了平衡，优于7个基线方法。

Conclusion: HazeMatching无需显式退化算子，适用于真实显微镜数据，且预测结果校准良好。

Abstract: Fluorescence microscopy is a major driver of scientific progress in the life
sciences. Although high-end confocal microscopes are capable of filtering
out-of-focus light, cheaper and more accessible microscopy modalities, such as
widefield microscopy, can not, which consequently leads to hazy image data.
Computational dehazing is trying to combine the best of both worlds, leading to
cheap microscopy but crisp-looking images. The perception-distortion trade-off
tells us that we can optimize either for data fidelity, e.g. low MSE or high
PSNR, or for data realism, measured by perceptual metrics such as LPIPS or FID.
Existing methods either prioritize fidelity at the expense of realism, or
produce perceptually convincing results that lack quantitative accuracy. In
this work, we propose HazeMatching, a novel iterative method for dehazing light
microscopy images, which effectively balances these objectives. Our goal was to
find a balanced trade-off between the fidelity of the dehazing results and the
realism of individual predictions (samples). We achieve this by adapting the
conditional flow matching framework by guiding the generative process with a
hazy observation in the conditional velocity field. We evaluate HazeMatching on
5 datasets, covering both synthetic and real data, assessing both distortion
and perceptual quality. Our method is compared against 7 baselines, achieving a
consistent balance between fidelity and realism on average. Additionally, with
calibration analysis, we show that HazeMatching produces well-calibrated
predictions. Note that our method does not need an explicit degradation
operator to exist, making it easily applicable on real microscopy data. All
data used for training and evaluation and our code will be publicly available
under a permissive license.

</details>


### [96] [Single-shot HDR using conventional image sensor shutter functions and optical randomization](https://arxiv.org/abs/2506.22426)
*Xiang Dai,Kyrollos Yanny,Kristina Monakhova,Nicholas Antipa*

Main category: eess.IV

TL;DR: 提出了一种基于全局复位释放（GRR）快门模式和光学随机排列的单次高动态范围（HDR）成像方法，解决了传统多曝光HDR成像的运动伪影问题，并在高饱和区域表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统HDR成像依赖多曝光，导致动态场景中出现运动伪影。单次HDR成像虽能缓解此问题，但在高光区域表现不佳。

Method: 利用GRR快门模式（底部行曝光时间更长）和光学随机排列技术，通过优化问题和总变分先验恢复HDR数据。

Result: 仿真显示，该方法在高饱和（10%以上）时优于其他单次HDR方法，在低饱和（1%）时表现相当。实验室原型使用商用传感器和随机光纤束，动态范围达73dB（8位传感器）。

Conclusion: 该方法通过GRR快门和光学随机排列，实现了高效的单次HDR成像，特别适用于高饱和场景。

Abstract: High-dynamic-range (HDR) imaging is an essential technique for overcoming the
dynamic range limits of image sensors. The classic method relies on multiple
exposures, which slows capture time, resulting in motion artifacts when imaging
dynamic scenes. Single-shot HDR imaging alleviates this issue by encoding HDR
data into a single exposure, then computationally recovering it. Many
established methods use strong image priors to recover improperly exposed image
detail. These approaches struggle with extended highlight regions. We utilize
the global reset release (GRR) shutter mode of an off-the-shelf sensor. GRR
shutter mode applies a longer exposure time to rows closer to the bottom of the
sensor. We use optics that relay a randomly permuted (shuffled) image onto the
sensor, effectively creating spatially randomized exposures across the scene.
The exposure diversity allows us to recover HDR data by solving an optimization
problem with a simple total variation image prior. In simulation, we
demonstrate that our method outperforms other single-shot methods when many
sensor pixels are saturated (10% or more), and is competitive at a modest
saturation (1%). Finally, we demonstrate a physical lab prototype that uses an
off-the-shelf random fiber bundle for the optical shuffling. The fiber bundle
is coupled to a low-cost commercial sensor operating in GRR shutter mode. Our
prototype achieves a dynamic range of up to 73dB using an 8-bit sensor with
48dB dynamic range.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [97] [ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes](https://arxiv.org/abs/2506.21629)
*Chenhao Zhang,Yezhi Shen,Fengqing Zhu*

Main category: cs.GR

TL;DR: 提出了一种结合ICP与优化的方法ICP-3DGS，用于大范围相机运动下的精确位姿估计和大规模场景重建。


<details>
  <summary>Details</summary>
Motivation: 解决现有神经渲染方法（如NeRF和3DGS）依赖预处理相机位姿和3D结构先验的问题，尤其在户外场景中难以获取这些数据。

Method: 结合迭代最近点（ICP）与优化细化进行相机位姿估计，并引入基于体素的场景密集化方法指导大规模场景重建。

Result: 实验表明，ICP-3DGS在相机位姿估计和新视角合成方面优于现有方法，适用于不同规模的室内外场景。

Conclusion: ICP-3DGS为大规模场景的神经渲染提供了一种有效的解决方案，代码已开源。

Abstract: In recent years, neural rendering methods such as NeRFs and 3D Gaussian
Splatting (3DGS) have made significant progress in scene reconstruction and
novel view synthesis. However, they heavily rely on preprocessed camera poses
and 3D structural priors from structure-from-motion (SfM), which are
challenging to obtain in outdoor scenarios. To address this challenge, we
propose to incorporate Iterative Closest Point (ICP) with optimization-based
refinement to achieve accurate camera pose estimation under large camera
movements. Additionally, we introduce a voxel-based scene densification
approach to guide the reconstruction in large-scale scenes. Experiments
demonstrate that our approach ICP-3DGS outperforms existing methods in both
camera pose estimation and novel view synthesis across indoor and outdoor
scenes of various scales. Source code is available at
https://github.com/Chenhao-Z/ICP-3DGS.

</details>


### [98] [SkinningGS: Editable Dynamic Human Scene Reconstruction Using Gaussian Splatting Based on a Skinning Model](https://arxiv.org/abs/2506.21632)
*Da Li,Donggang Jia,Markus Hadwiger,Ivan Viola*

Main category: cs.GR

TL;DR: 提出一种基于点云解耦和联合优化的方法，从单目视频中重建交互式人体和背景，实现高质量重建并减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 解决从单目动态场景视频中重建交互式人体和背景的挑战，同时保留人体运动的交互性。

Method: 采用点云解耦和联合优化策略，引入位置纹理细分SMPL模型表面，结合CNN预测人体点云特征。

Result: 方法在重建指标上超越HUGS，实时渲染速度达100 FPS，资源消耗更低。

Conclusion: 该方法高效且通用，适用于人体和动物场景重建。

Abstract: Reconstructing an interactive human avatar and the background from a
monocular video of a dynamic human scene is highly challenging. In this work we
adopt a strategy of point cloud decoupling and joint optimization to achieve
the decoupled reconstruction of backgrounds and human bodies while preserving
the interactivity of human motion. We introduce a position texture to subdivide
the Skinned Multi-Person Linear (SMPL) body model's surface and grow the human
point cloud. To capture fine details of human dynamics and deformations, we
incorporate a convolutional neural network structure to predict human body
point cloud features based on texture. This strategy makes our approach free of
hyperparameter tuning for densification and efficiently represents human points
with half the point cloud of HUGS. This approach ensures high-quality human
reconstruction and reduces GPU resource consumption during training. As a
result, our method surpasses the previous state-of-the-art HUGS in
reconstruction metrics while maintaining the ability to generalize to novel
poses and views. Furthermore, our technique achieves real-time rendering at
over 100 FPS, $\sim$6$\times$ the HUGS speed using only Linear Blend Skinning
(LBS) weights for human transformation. Additionally, this work demonstrates
that this framework can be extended to animal scene reconstruction when an
accurately-posed model of an animal is available.

</details>


### [99] [SAR-GS: 3D Gaussian Splatting for Synthetic Aperture Radar Target Reconstruction](https://arxiv.org/abs/2506.21633)
*Aobo Li,Zhengxin Lei,Jiangtao Wei,Feng Xu*

Main category: cs.GR

TL;DR: 本文提出了一种名为SDGR的新方法，结合高斯抛光和映射投影算法，用于SAR目标的三维重建，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: SAR图像中的复杂电磁散射机制给目标重建带来挑战，而3D高斯抛光在光学领域的成功启发了该方法在SAR领域的应用。

Method: 结合高斯抛光和映射投影算法计算高斯基元的散射强度，通过SDGR生成模拟SAR图像，并使用自定义CUDA梯度流加速梯度计算。

Result: 实验表明，SDGR能有效重建目标的几何结构和散射特性，在模拟和真实数据集上均表现良好。

Conclusion: SDGR为SAR成像领域的3D重建提供了一种新解决方案。

Abstract: Three-dimensional target reconstruction from synthetic aperture radar (SAR)
imagery is crucial for interpreting complex scattering information in SAR data.
However, the intricate electromagnetic scattering mechanisms inherent to SAR
imaging pose significant reconstruction challenges. Inspired by the remarkable
success of 3D Gaussian Splatting (3D-GS) in optical domain reconstruction, this
paper presents a novel SAR Differentiable Gaussian Splatting Rasterizer (SDGR)
specifically designed for SAR target reconstruction. Our approach combines
Gaussian splatting with the Mapping and Projection Algorithm to compute
scattering intensities of Gaussian primitives and generate simulated SAR images
through SDGR. Subsequently, the loss function between the rendered image and
the ground truth image is computed to optimize the Gaussian primitive
parameters representing the scene, while a custom CUDA gradient flow is
employed to replace automatic differentiation for accelerated gradient
computation. Through experiments involving the rendering of simplified
architectural targets and SAR images of multiple vehicle targets, we validate
the imaging rationality of SDGR on simulated SAR imagery. Furthermore, the
effectiveness of our method for target reconstruction is demonstrated on both
simulated and real-world datasets containing multiple vehicle targets, with
quantitative evaluations conducted to assess its reconstruction performance.
Experimental results indicate that our approach can effectively reconstruct the
geometric structures and scattering properties of targets, thereby providing a
novel solution for 3D reconstruction in the field of SAR imaging.

</details>


### [100] [A Design Space for Visualization Transitions of 3D Spatial Data in Hybrid AR-Desktop Environments](https://arxiv.org/abs/2506.22250)
*Yucheng Lu,Tobias Rau,Benjamin Lee,Andreas Köhn,Michael Sedlmair,Christian Sandor,Tobias Isenberg*

Main category: cs.GR

TL;DR: 本文提出了一个设计空间，用于在混合AR-桌面环境中实现3D空间数据集外观的动画过渡，以减少用户认知负荷并提供平滑体验。


<details>
  <summary>Details</summary>
Motivation: 混合接口结合传统和沉浸式显示，以在不同维度间实现平滑过渡，提升数据探索效率。

Method: 通过空间编码管道和过渡设计，在三种应用案例（天文学、放射学和化学）中验证设计空间的有效性。

Result: 设计空间简化了决策过程，并为未来设计提供了灵感。

Conclusion: 过渡设计空间在混合AR-桌面环境中有效，且适用于多种应用领域。

Abstract: We present a design space for animated transitions of the appearance of 3D
spatial datasets in a hybrid Augmented Reality (AR)-desktop context. Such
hybrid interfaces combine both traditional and immersive displays to facilitate
the exploration of 2D and 3D data representations in the environment in which
they are best displayed. One key aspect is to introduce transitional animations
that change between the different dimensionalities to illustrate the connection
between the different representations and to reduce the potential cognitive
load on the user. The specific transitions to be used depend on the type of
data, the needs of the application domain, and other factors. We summarize
these as a transition design space to simplify the decision-making process and
provide inspiration for future designs. First, we discuss 3D visualizations
from a spatial perspective: a spatial encoding pipeline, where 3D data sampled
from the physical world goes through various transformations, being mapped to
visual representations, and then being integrated into a hybrid AR-desktop
environment. The transition design then focuses on interpolating between two
spatial encoding pipelines to provide a smooth experience. To illustrate the
use of our design space, we apply it to three case studies that focus on
applications in astronomy, radiology, and chemistry; we then discuss lessons
learned from these applications.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [101] [Pressure dependence of liquid iron viscosity from machine-learning molecular dynamics](https://arxiv.org/abs/2506.21626)
*Kai Luo,Xuyang Long,R. E. Cohen*

Main category: physics.geo-ph

TL;DR: 开发了一种机器学习势能，准确模拟地球核心条件下铁的行为，通过分子动力学模拟获得液态铁粘度，发现爱因斯坦-斯托克斯关系不适用，粘度约为10 mPa·s，与先前第一性原理结果一致。


<details>
  <summary>Details</summary>
Motivation: 研究地球核心条件下铁的行为，为地球物理建模提供精确的粘度数据。

Method: 使用机器学习势能进行纳秒级平衡分子动力学模拟，计算液态铁在核心条件下的粘度。

Result: 发现爱因斯坦-斯托克斯关系不适用于核心条件，粘度约为10 mPa·s，与先前研究一致，并提供了压力-温度粘度图。

Conclusion: 机器学习势能能有效模拟核心条件下铁的行为，为地球物理建模提供了更准确的粘度数据。

Abstract: We have developed a machine-learning potential that accurately models the
behavior of iron under the conditions of Earth's core. By performing numerous
nanosecond scale equilibrium molecular dynamics simulations, the viscosities of
liquid iron for the whole outer core conditions are obtained with much less
uncertainty. We find that the Einstein-Stokes relation is not accurate for
outer core conditions. The viscosity is on the order of 10s \si{mPa.s}, in
agreement with previous first-principles results. We present a viscosity map as
a function of pressure and temperature for liquid iron useful for geophysical
modeling.

</details>


### [102] [Seismic resolution enhancement via deep Learning with Knowledge Distillation and Domain Adaptation](https://arxiv.org/abs/2506.22018)
*Hanpeng Cai,Haonan Zhang,Liyu Zhang,Suo Cheng*

Main category: physics.geo-ph

TL;DR: 本文提出了一种结合知识蒸馏和领域自适应的高分辨率地震数据处理方法（DAKD-Net），解决了传统方法在鲁棒性、计算效率和结构关系上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统高分辨率算法缺乏鲁棒性和计算效率，深度学习模型则因忽略先验知识和数据域差异导致泛化能力有限。

Method: DAKD-Net结合知识蒸馏和领域自适应机制，利用U-Net提取空间结构信息，并通过领域自适应微调提升实际应用性能。

Result: 实验表明，DAKD-Net在纵向分辨率和复杂结构恢复上优于传统方法和经典深度网络，具有强鲁棒性和实用性。

Conclusion: DAKD-Net通过整合先验知识和领域自适应，显著提升了高分辨率地震数据处理的性能和泛化能力。

Abstract: High-resolution processing of seismic signals is crucial for subsurface
geological characterization and thin-layer reservoir identification.
Traditional high-resolution algorithms can partially recover high-frequency
information but often lack robustness, computational efficiency, and
consideration of inter-trace structural relationships. Many deep learning
methods use end-to-end architectures that do not incorporate prior knowledge or
address data domain disparities, leading to limited generalization.To overcome
these challenges, this paper presents the Domain-Adaptive Knowledge
Distillation Network (DAKD-Net), which integrates a knowledge distillation
strategy with a domain adaptation mechanism for high-resolution seismic data
processing. Trained on datasets from forward modeling, DAKD-Net establishes
physical relationships between low and high-resolution data, extracting
high-frequency prior knowledge during a guided phase before detail restoration
without prior conditions. Domain adaptation enhances the model's generalization
to real seismic data, improving both generalization capability and structural
expression accuracy.DAKD-Net employs a U-Net backbone to extract spatial
structural information from multi-trace seismic profiles. The knowledge
distillation mechanism enables prior knowledge transfer, allowing recovery of
high-resolution data directly from low-resolution inputs. Domain-adaptive
fine-tuning further enhances the network's performance in actual survey areas.
Experimental results show that DAKD-Net outperforms traditional methods and
classical deep networks in longitudinal resolution and complex structural
detail restoration, demonstrating strong robustness and practicality.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [103] [Inverse Design of Diffractive Metasurfaces Using Diffusion Models](https://arxiv.org/abs/2506.21748)
*Liav Hen,Erez Yosef,Dan Raviv,Raja Giryes,Jacob Scheuer*

Main category: physics.optics

TL;DR: 该论文提出了一种利用扩散模型生成元表面设计的方法，以解决传统逆向设计中的计算复杂性和局部最优问题。


<details>
  <summary>Details</summary>
Motivation: 元表面的逆向设计通常需要专家调整，容易陷入局部最优，且计算开销大，因此需要一种更高效的方法。

Method: 通过结合扩散模型的生成能力，利用RCWA模拟器生成训练数据，训练条件扩散模型预测元原子几何形状和高度。

Result: 模型能够快速生成低误差的元表面设计，例如均匀强度分束器和偏振分束器，设计时间少于30分钟。

Conclusion: 该方法为数据驱动的元表面设计提供了高效工具，并公开了代码和数据集以支持进一步研究。

Abstract: Metasurfaces are ultra-thin optical elements composed of engineered
sub-wavelength structures that enable precise control of light. Their inverse
design - determining a geometry that yields a desired optical response - is
challenging due to the complex, nonlinear relationship between structure and
optical properties. This often requires expert tuning, is prone to local
minima, and involves significant computational overhead. In this work, we
address these challenges by integrating the generative capabilities of
diffusion models into computational design workflows. Using an RCWA simulator,
we generate training data consisting of metasurface geometries and their
corresponding far-field scattering patterns. We then train a conditional
diffusion model to predict meta-atom geometry and height from a target spatial
power distribution at a specified wavelength, sampled from a continuous
supported band. Once trained, the model can generate metasurfaces with low
error, either directly using RCWA-guided posterior sampling or by serving as an
initializer for traditional optimization methods. We demonstrate our approach
on the design of a spatially uniform intensity splitter and a polarization beam
splitter, both produced with low error in under 30 minutes. To support further
research in data-driven metasurface design, we publicly release our code and
datasets.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [104] [Hardware acceleration for ultra-fast Neural Network training on FPGA for MRF map reconstruction](https://arxiv.org/abs/2506.22156)
*Mattia Ricchi,Fabrizio Alfonsi,Camilla Marella,Marco Barbieri,Alessandra Retico,Leonardo Brizi,Alessandro Gabrielli,Claudia Testa*

Main category: cs.AR

TL;DR: 提出了一种基于FPGA的神经网络方法，用于从MRF数据中实时重建脑部参数，训练时间大幅缩短。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在MRF数据重建中需要大量训练资源，限制了实时应用。

Method: 采用FPGA加速神经网络的训练和推理，显著减少训练时间。

Result: 训练时间仅需200秒，比传统CPU方法快250倍。

Conclusion: 该方法有望实现移动设备上的实时脑部分析，推动临床决策和远程医疗的发展。

Abstract: Magnetic Resonance Fingerprinting (MRF) is a fast quantitative MR Imaging
technique that provides multi-parametric maps with a single acquisition. Neural
Networks (NNs) accelerate reconstruction but require significant resources for
training. We propose an FPGA-based NN for real-time brain parameter
reconstruction from MRF data. Training the NN takes an estimated 200 seconds,
significantly faster than standard CPU-based training, which can be up to 250
times slower. This method could enable real-time brain analysis on mobile
devices, revolutionizing clinical decision-making and telemedicine.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [105] [QuKAN: A Quantum Circuit Born Machine approach to Quantum Kolmogorov Arnold Networks](https://arxiv.org/abs/2506.22340)
*Yannick Werner,Akash Malemath,Mengxi Liu,Vitor Fortes Rey,Nikolaos Palaiodimopoulos,Paul Lukowicz,Maximilian Kiefer-Emmanouilidis*

Main category: quant-ph

TL;DR: 论文提出了一种基于Kolmogorov Arnold Networks (KANs)的量子架构QuKAN，结合经典和量子组件，展示了其可行性和性能。


<details>
  <summary>Details</summary>
Motivation: 探索KANs在量子机器学习中的潜力，利用其表达复杂函数的优势。

Method: 通过Quantum Circuit Born Machine (QCBM)实现KAN架构的混合和全量子形式，利用预训练残差函数。

Result: 展示了QuKAN的可行性、可解释性和性能。

Conclusion: QuKAN架构为量子机器学习提供了新的可能性。

Abstract: Kolmogorov Arnold Networks (KANs), built upon the Kolmogorov Arnold
representation theorem (KAR), have demonstrated promising capabilities in
expressing complex functions with fewer neurons. This is achieved by
implementing learnable parameters on the edges instead of on the nodes, unlike
traditional networks such as Multi-Layer Perceptrons (MLPs). However, KANs
potential in quantum machine learning has not yet been well explored. In this
work, we present an implementation of these KAN architectures in both hybrid
and fully quantum forms using a Quantum Circuit Born Machine (QCBM). We adapt
the KAN transfer using pre-trained residual functions, thereby exploiting the
representational power of parametrized quantum circuits. In the hybrid model we
combine classical KAN components with quantum subroutines, while the fully
quantum version the entire architecture of the residual function is translated
to a quantum model. We demonstrate the feasibility, interpretability and
performance of the proposed Quantum KAN (QuKAN) architecture.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [106] [Revisiting Graph Analytics Benchmark](https://arxiv.org/abs/2506.21811)
*Lingkai Meng,Yu Shao,Long Yuan,Longbin Lai,Peng Cheng,Xue Li,Wenyuan Yu,Wenjie Zhang,Xuemin Lin,Jingren Zhou*

Main category: cs.DB

TL;DR: 提出了一种新的图分析基准测试，改进了算法选择、数据生成和API可用性评估，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在核心算法选择、数据生成和API可用性评估方面存在不足，无法全面评估平台性能。

Method: 1. 选择8种核心算法；2. 设计高效灵活的数据生成器并生成8个新合成数据集；3. 引入基于大语言模型的多层次API可用性评估框架。

Result: 实验结果表明，所提出的基准测试在现有平台上表现优越。

Conclusion: 新基准测试解决了现有问题，为图分析平台性能评估提供了更全面的工具。

Abstract: The rise of graph analytics platforms has led to the development of various
benchmarks for evaluating and comparing platform performance. However, existing
benchmarks often fall short of fully assessing performance due to limitations
in core algorithm selection, data generation processes (and the corresponding
synthetic datasets), as well as the neglect of API usability evaluation. To
address these shortcomings, we propose a novel graph analytics benchmark.
First, we select eight core algorithms by extensively reviewing both academic
and industrial settings. Second, we design an efficient and flexible data
generator and produce eight new synthetic datasets as the default datasets for
our benchmark. Lastly, we introduce a multi-level large language model
(LLM)-based framework for API usability evaluation-the first of its kind in
graph analytics benchmarks. We conduct comprehensive experimental evaluations
on existing platforms (GraphX, PowerGraph, Flash, Grape, Pregel+, Ligra and
G-thinker). The experimental results demonstrate the superiority of our
proposed benchmark.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [107] [Hierarchical Patch Compression for ColPali: Efficient Multi-Vector Document Retrieval with Dynamic Pruning and Quantization](https://arxiv.org/abs/2506.21601)
*Duong Bach*

Main category: cs.IR

TL;DR: HPC-ColPali通过分层压缩技术提升多向量文档检索效率，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 解决ColPali因高维嵌入和延迟交互评分导致的存储和计算成本高的问题。

Method: 采用K-Means量化、注意力引导动态剪枝和可选二进制编码三种技术。

Result: 在ViDoRe和SEC-Filings数据集上，查询延迟降低30-50%，检索精度保持；在RAG管道中，幻觉率降低30%，延迟减半。

Conclusion: HPC-ColPali是一种高效、可扩展的多向量文档检索解决方案。

Abstract: Multi-vector document retrieval systems, such as ColPali, excel in
fine-grained matching for complex queries but incur significant storage and
computational costs due to their reliance on high-dimensional patch embeddings
and late-interaction scoring. To address these challenges, we propose
HPC-ColPali, a Hierarchical Patch Compression framework that enhances the
efficiency of ColPali while preserving its retrieval accuracy. Our approach
integrates three innovative techniques: (1) K-Means quantization, which
compresses patch embeddings into 1-byte centroid indices, achieving up to
32$\times$ storage reduction; (2) attention-guided dynamic pruning, utilizing
Vision-Language Model attention weights to retain only the top-$p\%$ most
salient patches, reducing late-interaction computation by up to 60\% with less
than 2\% nDCG@10 loss; and (3) optional binary encoding of centroid indices
into $b$-bit strings ($b=\lceil\log_2 K\rceil$), enabling rapid Hamming
distance-based similarity search for resource-constrained environments.
Evaluated on the ViDoRe and SEC-Filings datasets, HPC-ColPali achieves 30--50\%
lower query latency under HNSW indexing while maintaining high retrieval
precision. When integrated into a Retrieval-Augmented Generation pipeline for
legal summarization, it reduces hallucination rates by 30\% and halves
end-to-end latency. These advancements establish HPC-ColPali as a scalable and
efficient solution for multi-vector document retrieval across diverse
applications. Code is available at https://github.com/DngBack/HPC-ColPali.

</details>


### [108] [Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding](https://arxiv.org/abs/2506.21604)
*Varun Mannam,Fang Wang,Xin Chen*

Main category: cs.IR

TL;DR: 提出了一种系统化的多模态生成AI评估框架，通过量化模态权重提升企业文档智能的可信度。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态生成AI评估框架难以建立可信度，阻碍了企业采用。

Method: 引入定量基准框架，测量跨模态输入（如文本、图像、标题和OCR）的集成效果，建立技术指标与用户信任度之间的关系。

Result: 最优模态权重（30%文本、15%图像、25%标题、30%OCR）比纯文本基线提升57.3%性能，同时保持计算效率。

Conclusion: 该框架为多模态RAG在企业关键应用中的可信度量化与提升提供了严格方法，推动负责任AI部署。

Abstract: Current evaluation frameworks for multimodal generative AI struggle to
establish trustworthiness, hindering enterprise adoption where reliability is
paramount. We introduce a systematic, quantitative benchmarking framework to
measure the trustworthiness of progressively integrating cross-modal inputs
such as text, images, captions, and OCR within VisualRAG systems for enterprise
document intelligence. Our approach establishes quantitative relationships
between technical metrics and user-centric trust measures. Evaluation reveals
that optimal modality weighting with weights of 30% text, 15% image, 25%
caption, and 30% OCR improves performance by 57.3% over text-only baselines
while maintaining computational efficiency. We provide comparative assessments
of foundation models, demonstrating their differential impact on
trustworthiness in caption generation and OCR extraction-a vital consideration
for reliable enterprise AI. This work advances responsible AI deployment by
providing a rigorous framework for quantifying and enhancing trustworthiness in
multimodal RAG for critical enterprise applications.

</details>


### [109] [CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware Layout Design](https://arxiv.org/abs/2506.21934)
*Najmeh Forouzandehmehr,Reza Yousefi Maragheh,Sriram Kollipara,Kai Zhao,Topojoy Biswas,Evren Korpeoglu,Kannan Achan*

Main category: cs.IR

TL;DR: CAL-RAG是一个结合检索增强和多智能体推理的框架，用于内容感知布局生成，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型和大型语言模型在结构化内容生成中缺乏上下文设计范例的支撑，难以处理语义对齐和视觉一致性。

Method: CAL-RAG结合多模态检索、大型语言模型和协作智能体推理，通过检索布局范例、推荐布局、视觉评分和反馈迭代优化布局。

Result: 在PKU PosterLayout数据集上，CAL-RAG在多个布局指标上达到最先进性能，显著优于基线方法。

Conclusion: 检索增强与多步智能体推理结合为自动化布局生成提供了可扩展、可解释且高保真的解决方案。

Abstract: Automated content-aware layout generation -- the task of arranging visual
elements such as text, logos, and underlays on a background canvas -- remains a
fundamental yet under-explored problem in intelligent design systems. While
recent advances in deep generative models and large language models (LLMs) have
shown promise in structured content generation, most existing approaches lack
grounding in contextual design exemplars and fall short in handling semantic
alignment and visual coherence. In this work we introduce CAL-RAG, a
retrieval-augmented, agentic framework for content-aware layout generation that
integrates multimodal retrieval, large language models, and collaborative
agentic reasoning. Our system retrieves relevant layout examples from a
structured knowledge base and invokes an LLM-based layout recommender to
propose structured element placements. A vision-language grader agent evaluates
the layout with visual metrics, and a feedback agent provides targeted
refinements, enabling iterative improvement. We implement our framework using
LangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in
semantic and structural variability. CAL-RAG achieves state-of-the-art
performance across multiple layout metrics -- including underlay effectiveness,
element alignment, and overlap -- substantially outperforming strong baselines
such as LayoutPrompter. These results demonstrate that combining retrieval
augmentation with agentic multi-step reasoning yields a scalable,
interpretable, and high-fidelity solution for automated layout generation.

</details>


<div id='math.AP'></div>

# math.AP [[Back]](#toc)

### [110] [Asymptotic analysis and design of shell-based thermal lattice metamaterials](https://arxiv.org/abs/2506.22319)
*Di Zhang,Ligang Liu*

Main category: math.AP

TL;DR: 本文提出了一个严格的渐近分析框架，用于研究壳格超材料的导热性，并引入了一种新指标ADC，用于捕捉几何形状对导热性的影响。


<details>
  <summary>Details</summary>
Motivation: 扩展先前关于机械刚度的研究，将其应用于热传递领域，填补了壳格超材料导热性理论分析的空白。

Method: 提出渐近方向导热性（ADC）指标，建立收敛定理、上界及达到上界的充要条件，并开发离散算法用于计算和优化ADC。

Result: 理论证明了三重周期最小曲面的最优导热性，ADC在低体积分数下对有效导热性提供三阶近似，数值结果验证了理论的正确性和算法的有效性。

Conclusion: 该框架为壳格超材料导热性提供了理论基础和实用设计工具，ADC指标和优化算法在实际应用中表现出色。

Abstract: We present a rigorous asymptotic analysis framework for investigating the
thermal conductivity of shell lattice metamaterials, extending prior work from
mechanical stiffness to heat transfer. Central to our analysis is a new metric,
the asymptotic directional conductivity (ADC), which captures the leading-order
influence of the middle surface geometry on the effective thermal conductivity
in the vanishing-thickness limit. A convergence theorem is established for
evaluating ADC, along with a sharp upper bound and the necessary and sufficient
condition for achieving this bound. These results provide the first theoretical
justification for the optimal thermal conductivity of triply periodic minimal
surfaces. Furthermore, we show that ADC yields a third-order approximation to
the effective conductivity of shell lattices at low volume fractions. To
support practical design applications, we develop a discrete algorithm for
computing and optimizing ADC over arbitrary periodic surfaces. Numerical
results confirm the theoretical predictions and demonstrate the robustness and
effectiveness of the proposed optimization algorithm.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [111] [A Statistical Approach to Quantifying Uncertainty in Meteoroid Physical Properties](https://arxiv.org/abs/2506.21701)
*Maximilian Vovk,Denis Vida,Peter G. Brown*

Main category: astro-ph.EP

TL;DR: 该研究通过自动化方法从光学流星数据中反演流星体的物理参数，特别是体积密度及其不确定性，比较了基于观测量的PCA方法和基于RMSD的方法，发现后者更可靠。


<details>
  <summary>Details</summary>
Motivation: 流星体体积密度是评估航天器撞击风险的关键参数，但直接测量困难且不确定性高。研究旨在提供更可靠的密度估计方法。

Method: 比较基于观测量的PCA方法和基于RMSD的方法，使用合成测试案例和实际流星数据验证。

Result: RMSD方法比PCA方法更可靠，能提供更紧密的密度约束，并首次给出流星体物理性质的客观不确定性界限。

Conclusion: 该方法解决了流星正向建模中的解退化问题，可推广到其他流星群，提升流星体模型和航天器安全性。

Abstract: Meteoroid bulk density is a critical value required for assessing impact
risks to spacecraft, informing shielding and mission design. Direct bulk
density measurements for sub-millimeter to millimeter-sized meteoroids are
difficult, often relying on forward modeling without robust uncertainty
estimates. Methods based solely on select observables can overlook
noise-induced biases and non-linear relations between physical parameters. This
study aims to automate the inversion of meteoroid physical parameters from
optical meteor data, focusing on bulk density and its associated uncertainties.
We compare an observables-based selection method (PCA) with an RMSD-based
approach used to select among millions of ablation model runs using full light
and deceleration curves as constraints. After validating both approaches on six
synthetic test cases, we apply them to two Perseid meteors recorded by high
sensitivity Electron-Multiplied CCD (EMCCD) cameras and high precision
mirror-tracked meteors detected by the Canadian Automated Meteor Observatory
(CAMO). Our results show that relying only on observables, as in the PCA
approach can converge to wrong solutions and can yield unphysical solutions. In
contrast, the RMSD-based method offers more reliable density constraints,
particularly for bright and strongly decelerating meteor. Small relative
measurement precision in brightness and lag relative to the full range of
observed lag and luminosity is the key to tight solution. We provide the first
objectively derived uncertainty bounds for the physical properties of
meteoroids. Our approach solves the solution degeneracy problem inherent in
forward modelling of meteors. This strategy can be generalized to other
showers, paving the way for improved meteoroid models and enhanced spacecraft
safety.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [112] [Can Vision Language Models Understand Mimed Actions?](https://arxiv.org/abs/2506.21586)
*Hyundong Cho,Spencer Lin,Tejas Srinivasan,Michael Saxon,Deuksin Kwon,Natali T. Chavez,Jonathan May*

Main category: cs.CL

TL;DR: 论文提出MIME基准，用于评估视觉语言模型对哑剧动作的理解能力，发现现有模型表现远不如人类。


<details>
  <summary>Details</summary>
Motivation: 研究哑剧动作（mimed actions）是理解非语言沟通（NVC）的关键，因其具有明确性和低解释差异。

Method: 提出MIME基准，包含86种哑剧动作的视频问答任务，通过动作捕捉数据构建，并加入扰动以评估鲁棒性。

Result: 现有视觉语言模型在MIME上的表现显著低于人类水平。

Conclusion: 需进一步研究以提升模型对人类手势的鲁棒理解。

Abstract: Nonverbal communication (NVC) plays an integral role in human language, but
studying NVC in general is challenging because of its broad scope and high
variance in interpretation among individuals and cultures. However, mime -- the
theatrical technique of suggesting intent using only gesture, expression, and
movement -- is a subset of NVC that consists of explicit and embodied actions
with much lower human interpretation variance. We argue that a solid
understanding of mimed actions is a crucial prerequisite for vision-language
models capable of interpreting and commanding more subtle aspects of NVC.
Hence, we propose Mime Identification Multimodal Evaluation (MIME), a novel
video-based question answering benchmark comprising of 86 mimed actions.
Constructed with motion capture data, MIME consists of variations of each
action with perturbations applied to the character, background, and viewpoint
for evaluating recognition robustness. We find that both open-weight and
API-based vision-language models perform significantly worse than humans on
MIME, motivating the need for increased research for instilling more robust
understanding of human gestures.

</details>


### [113] [SignBart -- New approach with the skeleton sequence for Isolated Sign language Recognition](https://arxiv.org/abs/2506.21592)
*Tinh Nguyen,Minh Khue Phan Tran*

Main category: cs.CL

TL;DR: 提出了一种基于BART架构的新手语识别方法，通过独立编码x和y坐标并利用交叉注意力保持其关联，显著提升了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在手语识别中效率和准确性难以兼顾的问题，尤其是RNN、LSTM和GCN的梯度消失和高计算成本问题。

Method: 采用BART架构的编码器-解码器模型，独立编码x和y坐标，并通过交叉注意力维持其关联。

Result: 在LSA-64数据集上达到96.04%的准确率，参数仅749,888，优于其他百万参数模型，并在WLASL和ASL-Citizen数据集上表现优异。

Conclusion: 该方法为手语识别提供了可靠且高效的解决方案，有望提升听障人士的辅助工具。

Abstract: Sign language recognition is crucial for individuals with hearing impairments
to break communication barriers. However, previous approaches have had to
choose between efficiency and accuracy. Such as RNNs, LSTMs, and GCNs, had
problems with vanishing gradients and high computational costs. Despite
improving performance, transformer-based methods were not commonly used. This
study presents a new novel SLR approach that overcomes the challenge of
independently extracting meaningful information from the x and y coordinates of
skeleton sequences, which traditional models often treat as inseparable. By
utilizing an encoder-decoder of BART architecture, the model independently
encodes the x and y coordinates, while Cross-Attention ensures their
interrelation is maintained. With only 749,888 parameters, the model achieves
96.04% accuracy on the LSA-64 dataset, significantly outperforming previous
models with over one million parameters. The model also demonstrates excellent
performance and generalization across WLASL and ASL-Citizen datasets. Ablation
studies underscore the importance of coordinate projection, normalization, and
using multiple skeleton components for boosting model efficacy. This study
offers a reliable and effective approach for sign language recognition, with
strong potential for enhancing accessibility tools for the deaf and hard of
hearing.

</details>


### [114] [Towards Transparent AI: A Survey on Explainable Large Language Models](https://arxiv.org/abs/2506.21812)
*Avash Palikhe,Zhenyu Yu,Zichong Wang,Wenbin Zhang*

Main category: cs.CL

TL;DR: 该论文综述了针对大型语言模型（LLMs）的可解释人工智能（XAI）方法，分类并评估了不同架构下的技术，探讨了实际应用、资源、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: LLMs的决策过程缺乏透明度，限制了其在高风险领域的应用，因此需要系统化的XAI方法提升可解释性。

Method: 通过分类LLMs的架构（仅编码器、仅解码器、编码器-解码器），综述并评估了XAI技术，并探讨其应用。

Result: 提供了XAI方法的系统化分类和评估，展示了如何在实际中利用这些解释。

Conclusion: 论文为开发透明和负责任的LLMs提供了指导，并指出了未来研究方向。

Abstract: Large Language Models (LLMs) have played a pivotal role in advancing
Artificial Intelligence (AI). However, despite their achievements, LLMs often
struggle to explain their decision-making processes, making them a 'black box'
and presenting a substantial challenge to explainability. This lack of
transparency poses a significant obstacle to the adoption of LLMs in
high-stakes domain applications, where interpretability is particularly
essential. To overcome these limitations, researchers have developed various
explainable artificial intelligence (XAI) methods that provide
human-interpretable explanations for LLMs. However, a systematic understanding
of these methods remains limited. To address this gap, this survey provides a
comprehensive review of explainability techniques by categorizing XAI methods
based on the underlying transformer architectures of LLMs: encoder-only,
decoder-only, and encoder-decoder models. Then these techniques are examined in
terms of their evaluation for assessing explainability, and the survey further
explores how these explanations are leveraged in practical applications.
Finally, it discusses available resources, ongoing research challenges, and
future directions, aiming to guide continued efforts toward developing
transparent and responsible LLMs.

</details>


### [115] [Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation](https://arxiv.org/abs/2506.21876)
*Qiyue Gao,Xinyu Pi,Kevin Liu,Junrong Chen,Ruolan Yang,Xinqi Huang,Xinyu Fang,Lu Sun,Gautham Kishore,Bo Ai,Stone Tao,Mengyang Liu,Jiaxi Yang,Chao-Jung Lai,Chuanyang Jin,Jiannan Xiang,Benhao Huang,Zeming Chen,David Danks,Hao Su,Tianmin Shu,Ziqiao Ma,Lianhui Qin,Zhiting Hu*

Main category: cs.CL

TL;DR: 论文提出一个两阶段框架评估视觉语言模型（VLMs）作为世界模型（WMs）的能力，发现现有模型在基础世界建模能力上存在显著局限性。


<details>
  <summary>Details</summary>
Motivation: 评估VLMs作为通用世界模型的潜力，填补系统性评估的空白。

Method: 提出两阶段框架（感知与预测），并设计大规模基准WM-ABench，涵盖23个细粒度评估维度。

Result: 实验显示VLMs在基础世界建模能力上表现不佳，例如运动轨迹区分接近随机准确率。

Conclusion: VLMs与世界建模的人类水平存在显著差距，需进一步改进。

Abstract: Internal world models (WMs) enable agents to understand the world's state and
predict transitions, serving as the basis for advanced deliberative reasoning.
Recent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and
Gemini, exhibit potential as general-purpose WMs. While the latest studies have
evaluated and shown limitations in specific capabilities such as visual
understanding, a systematic evaluation of VLMs' fundamental WM abilities
remains absent. Drawing on comparative psychology and cognitive science, we
propose a two-stage framework that assesses Perception (visual, spatial,
temporal, quantitative, and motion) and Prediction (mechanistic simulation,
transitive inference, compositional inference) to provide an atomic evaluation
of VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale
benchmark comprising 23 fine-grained evaluation dimensions across 6 diverse
simulated environments with controlled counterfactual simulations. Through 660
experiments on 15 latest commercial and open-source VLMs, we find that these
models exhibit striking limitations in basic world modeling abilities. For
instance, almost all models perform at near-random accuracy when distinguishing
motion trajectories. Additionally, they lack disentangled understanding --
e.g., some models tend to believe blue objects move faster than green ones.
More rich results and analyses reveal significant gaps between VLMs and
human-level world modeling.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [116] [TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway Segmentation under Challenging Illumination Conditions](https://arxiv.org/abs/2506.21630)
*Yixin Sun,Li Li,Wenke E,Amir Atapour-Abarghouei,Toby P. Breckon*

Main category: cs.RO

TL;DR: 论文提出了一种针对非结构化户外环境中可通行路径检测的解决方案，包括新数据集TOMD和多模态数据融合模型。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和模型主要针对城市或宽阔的越野环境，无法满足狭窄小径类场景的需求，尤其是在搜索救援和森林火灾等关键应用中。

Method: 引入TOMD数据集，包含多模态传感器数据，并提出动态多尺度数据融合模型，比较不同融合策略在不同光照条件下的表现。

Result: 结果表明该方法有效，且光照对分割性能有显著影响。

Conclusion: TOMD数据集和提出的模型为小径类越野导航研究提供了重要支持，数据集已公开。

Abstract: Detecting traversable pathways in unstructured outdoor environments remains a
significant challenge for autonomous robots, especially in critical
applications such as wide-area search and rescue, as well as incident
management scenarios like forest fires. Existing datasets and models primarily
target urban settings or wide, vehicle-traversable off-road tracks, leaving a
substantial gap in addressing the complexity of narrow, trail-like off-road
scenarios. To address this, we introduce the Trail-based Off-road Multimodal
Dataset (TOMD), a comprehensive dataset specifically designed for such
environments. TOMD features high-fidelity multimodal sensor data -- including
128-channel LiDAR, stereo imagery, GNSS, IMU, and illumination measurements --
collected through repeated traversals under diverse conditions. We also propose
a dynamic multiscale data fusion model for accurate traversable pathway
prediction. The study analyzes the performance of early, cross, and mixed
fusion strategies under varying illumination levels. Results demonstrate the
effectiveness of our approach and the relevance of illumination in segmentation
performance. We publicly release TOMD at https://github.com/yyyxs1125/TMOD to
support future research in trail-based off-road navigation.

</details>


### [117] [AeroLite-MDNet: Lightweight Multi-task Deviation Detection Network for UAV Landing](https://arxiv.org/abs/2506.21635)
*Haiping Yang,Huaxing Liu,Wei Wu,Zuohui Chen,Ning Wu*

Main category: cs.RO

TL;DR: 论文提出了一种基于视觉的无人机着陆偏差预警系统AeroLite-MDNet，通过多尺度融合模块和分割分支提高检测和方向估计能力，并引入新评估指标AWD和新数据集UAVLandData，实验显示系统性能优越。


<details>
  <summary>Details</summary>
Motivation: 无人机在多种应用中需安全着陆，但GPS信号干扰等问题导致着陆精度不足，亟需解决方案。

Method: 提出AeroLite-MDNet模型，结合多尺度融合模块和分割分支，用于偏差检测和方向估计；引入AWD指标和新数据集UAVLandData。

Result: 系统AWD为0.7秒，偏差检测准确率达98.6%，显著提升着陆可靠性。

Conclusion: AeroLite-MDNet系统有效解决无人机着陆偏差问题，为实际应用提供可靠支持。

Abstract: Unmanned aerial vehicles (UAVs) are increasingly employed in diverse
applications such as land surveying, material transport, and environmental
monitoring. Following missions like data collection or inspection, UAVs must
land safely at docking stations for storage or recharging, which is an
essential requirement for ensuring operational continuity. However, accurate
landing remains challenging due to factors like GPS signal interference. To
address this issue, we propose a deviation warning system for UAV landings,
powered by a novel vision-based model called AeroLite-MDNet. This model
integrates a multiscale fusion module for robust cross-scale object detection
and incorporates a segmentation branch for efficient orientation estimation. We
introduce a new evaluation metric, Average Warning Delay (AWD), to quantify the
system's sensitivity to landing deviations. Furthermore, we contribute a new
dataset, UAVLandData, which captures real-world landing deviation scenarios to
support training and evaluation. Experimental results show that our system
achieves an AWD of 0.7 seconds with a deviation detection accuracy of 98.6\%,
demonstrating its effectiveness in enhancing UAV landing reliability. Code will
be available at https://github.com/ITTTTTI/Maskyolo.git

</details>


### [118] [Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation](https://arxiv.org/abs/2506.21732)
*Ameya Salvi,Venkat Krovi*

Main category: cs.RO

TL;DR: 本文提出了一种基于视觉导航的结构化学习方法，用于解决滑移转向车辆在复杂地形中的自动化问题，并通过仿真和硬件实验验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 滑移转向车辆在复杂地形中的自动化部署因缺乏准确的解析模型而受限，传统方法难以应对动态操作环境。

Method: 采用端到端学习方法（如模仿学习和深度强化学习），提出了一种结构化视觉导航框架。

Result: 通过大量仿真和硬件实验，验证了该方法在性能上显著优于现有文献中的方法。

Conclusion: 该结构化学习方法为滑移转向车辆的自动化部署提供了有效解决方案，具有实际应用潜力。

Abstract: Vision-based lane keeping is a topic of significant interest in the robotics
and autonomous ground vehicles communities in various on-road and off-road
applications. The skid-steered vehicle architecture has served as a useful
vehicle platform for human controlled operations. However, systematic modeling,
especially of the skid-slip wheel terrain interactions (primarily in off-road
settings) has created bottlenecks for automation deployment. End-to-end
learning based methods such as imitation learning and deep reinforcement
learning, have gained prominence as a viable deployment option to counter the
lack of accurate analytical models. However, the systematic formulation and
subsequent verification/validation in dynamic operation regimes (particularly
for skid-steered vehicles) remains a work in progress. To this end, a novel
approach for structured formulation for learning visual navigation is proposed
and investigated in this work. Extensive software simulations, hardware
evaluations and ablation studies now highlight the significantly improved
performance of the proposed approach against contemporary literature.

</details>


### [119] [Embodied Domain Adaptation for Object Detection](https://arxiv.org/abs/2506.21860)
*Xiangyu Shi,Yanyuan Qiao,Lingqiao Liu,Feras Dayoub*

Main category: cs.RO

TL;DR: 论文提出了一种无需源数据的域适应方法（SFDA），通过时间聚类和多尺度阈值融合改进伪标签，并结合对比学习的Mean Teacher框架，显著提升了室内动态环境中的零样本检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决标准封闭集方法和开放词汇检测（OVOD）在室内环境中因领域偏移和动态条件导致的性能下降问题。

Method: 采用源自由域适应（SFDA），结合时间聚类优化伪标签，多尺度阈值融合，以及Mean Teacher框架与对比学习。

Result: 在EDAOD基准测试中，零样本检测性能显著提升，并能灵活适应动态室内条件。

Conclusion: 提出的方法有效解决了室内环境中的领域适应问题，为移动机器人的感知任务提供了更鲁棒的解决方案。

Abstract: Mobile robots rely on object detectors for perception and object localization
in indoor environments. However, standard closed-set methods struggle to handle
the diverse objects and dynamic conditions encountered in real homes and labs.
Open-vocabulary object detection (OVOD), driven by Vision Language Models
(VLMs), extends beyond fixed labels but still struggles with domain shifts in
indoor environments. We introduce a Source-Free Domain Adaptation (SFDA)
approach that adapts a pre-trained model without accessing source data. We
refine pseudo labels via temporal clustering, employ multi-scale threshold
fusion, and apply a Mean Teacher framework with contrastive learning. Our
Embodied Domain Adaptation for Object Detection (EDAOD) benchmark evaluates
adaptation under sequential changes in lighting, layout, and object diversity.
Our experiments show significant gains in zero-shot detection performance and
flexible adaptation to dynamic indoor conditions.

</details>


### [120] [Evaluating Pointing Gestures for Target Selection in Human-Robot Collaboration](https://arxiv.org/abs/2506.22116)
*Noora Sassali,Roel Pieters*

Main category: cs.RO

TL;DR: 该研究提出了一种基于姿态估计和几何模型的方法，用于在平面工作空间中定位指向手势的目标，并评估其在机器人任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 指向手势是人机协作中的常见交互方式，但缺乏系统的方法来定位目标和评估其准确性。

Method: 采用姿态估计和基于肩-腕伸展的几何模型，从RGB-D数据流中提取手势数据，并结合多模态技术（如物体检测和语音处理）进行验证。

Result: 开发了一个概念验证机器人系统，展示了多模态协作应用的可行性，并提供了工具的性能和局限性分析。

Conclusion: 该方法为指向手势的定位和多模态机器人系统的集成提供了实用工具，但仍需进一步优化以应对复杂场景。

Abstract: Pointing gestures are a common interaction method used in Human-Robot
Collaboration for various tasks, ranging from selecting targets to guiding
industrial processes. This study introduces a method for localizing pointed
targets within a planar workspace. The approach employs pose estimation, and a
simple geometric model based on shoulder-wrist extension to extract gesturing
data from an RGB-D stream. The study proposes a rigorous methodology and
comprehensive analysis for evaluating pointing gestures and target selection in
typical robotic tasks. In addition to evaluating tool accuracy, the tool is
integrated into a proof-of-concept robotic system, which includes object
detection, speech transcription, and speech synthesis to demonstrate the
integration of multiple modalities in a collaborative application. Finally, a
discussion over tool limitations and performance is provided to understand its
role in multimodal robotic systems. All developments are available at:
https://github.com/NMKsas/gesture_pointer.git.

</details>


### [121] [KnotDLO: Toward Interpretable Knot Tying](https://arxiv.org/abs/2506.22176)
*Holly Dinkel,Raghavendra Navaratna,Jingyi Xiang,Brian Coltin,Trey Smith,Timothy Bretl*

Main category: cs.RO

TL;DR: KnotDLO是一种无需人类演示或训练的单手可变形线性物体（DLO）打结方法，具有抗遮挡性、可重复性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决DLO打结中的遮挡问题，并实现无需人类干预的自动化打结。

Method: 通过当前DLO形状规划抓取和目标路径点，基于分段线性曲线计算抓取姿态，并解耦视觉推理与控制。

Result: 在16次打结试验中，成功率为50%。

Conclusion: KnotDLO展示了在未见过的配置下实现DLO打结的潜力，但成功率有待提高。

Abstract: This work presents KnotDLO, a method for one-handed Deformable Linear Object
(DLO) knot tying that is robust to occlusion, repeatable for varying rope
initial configurations, interpretable for generating motion policies, and
requires no human demonstrations or training. Grasp and target waypoints for
future DLO states are planned from the current DLO shape. Grasp poses are
computed from indexing the tracked piecewise linear curve representing the DLO
state based on the current curve shape and are piecewise continuous. KnotDLO
computes intermediate waypoints from the geometry of the current DLO state and
the desired next state. The system decouples visual reasoning from control. In
16 trials of knot tying, KnotDLO achieves a 50% success rate in tying an
overhand knot from previously unseen configurations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [122] [TOAST: Task-Oriented Adaptive Semantic Transmission over Dynamic Wireless Environments](https://arxiv.org/abs/2506.21900)
*Sheng Yun,Jianhua Pei,Ping Wang*

Main category: cs.LG

TL;DR: TOAST是一个面向6G网络的语义感知通信框架，通过自适应任务平衡、低秩适应机制和扩散模型，在多任务优化和动态无线环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要从比特传输转向语义感知通信，强调任务相关信息。TOAST旨在解决动态无线环境中的多任务优化问题。

Method: 1. 使用深度强化学习动态调整图像重建与语义分类的权衡；2. 在Swin Transformer架构中集成低秩适应机制；3. 引入潜在空间扩散模型修复噪声损坏的特征。

Result: TOAST在低信噪比条件下显著提升了分类准确性和重建质量，并在所有测试场景中表现稳健。

Conclusion: TOAST为6G语义通信提供了一个高效、适应性强的解决方案，优于现有基线方法。

Abstract: The evolution toward 6G networks demands a fundamental shift from bit-centric
transmission to semantic-aware communication that emphasizes task-relevant
information. This work introduces TOAST (Task-Oriented Adaptive Semantic
Transmission), a unified framework designed to address the core challenge of
multi-task optimization in dynamic wireless environments through three
complementary components. First, we formulate adaptive task balancing as a
Markov decision process, employing deep reinforcement learning to dynamically
adjust the trade-off between image reconstruction fidelity and semantic
classification accuracy based on real-time channel conditions. Second, we
integrate module-specific Low-Rank Adaptation (LoRA) mechanisms throughout our
Swin Transformer-based joint source-channel coding architecture, enabling
parameter-efficient fine-tuning that dramatically reduces adaptation overhead
while maintaining full performance across diverse channel impairments including
Additive White Gaussian Noise (AWGN), fading, phase noise, and impulse
interference. Third, we incorporate an Elucidating diffusion model that
operates in the latent space to restore features corrupted by channel noises,
providing substantial quality improvements compared to baseline approaches.
Extensive experiments across multiple datasets demonstrate that TOAST achieves
superior performance compared to baseline approaches, with significant
improvements in both classification accuracy and reconstruction quality at low
Signal-to-Noise Ratio (SNR) conditions while maintaining robust performance
across all tested scenarios.

</details>


### [123] [APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization](https://arxiv.org/abs/2506.21655)
*Minjie Hong,Zirun Guo,Yan Xia,Zehan Wang,Ziang Zhang,Tao Jin,Zhou Zhao*

Main category: cs.LG

TL;DR: 论文提出APO方法，结合DADS和STCR技术，提升MLLMs的复杂推理能力，同时避免性能下降和过度推理问题。


<details>
  <summary>Details</summary>
Motivation: MLLMs在复杂推理上表现不佳，RL训练易导致性能下降和过度推理。

Method: 提出APO方法，分为正负样本处理：正样本用DADS动态调整KL权重，负样本用STCR惩罚过长响应。

Result: View-R1-3B模型在推理任务上平均提升7%，优于更大规模MLLMs，且通用任务表现稳定。

Conclusion: DADS和STCR技术有效提升MLLMs的推理能力，具有广泛适用性。

Abstract: Multimodal Large Language Models (MLLMs) are powerful at integrating diverse
data, but they often struggle with complex reasoning. While Reinforcement
learning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.
Common issues include a drop in performance on general tasks and the generation
of overly detailed or "overthinking" reasoning. Our work investigates how the
KL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric
Policy Optimization (APO) to address these issues, which divides the sampled
responses into positive and negative groups. For positive samples,
Difficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically
adjust the KL divergence weight based on their difficulty. This method prevents
policy entropy from dropping sharply, improves training stability, utilizes
samples better, and preserves the model's existing knowledge. For negative
samples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to
penalize overly long responses. This helps mitigate overthinking and encourages
more concise reasoning while preserving the model's explorative capacity. We
apply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B
significantly enhances reasoning capabilities, showing an average 7\% gain over
the base model and outperforming larger MLLMs (7-11B) on various reasoning
benchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade
on general tasks, View-R1-3B maintains consistent improvement, demonstrating
superior generalization. These results highlight the effectiveness and broad
applicability of our DADS and STCR techniques for advancing complex multimodal
reasoning in MLLMs. The code will be made available at
https://github.com/Indolent-Kawhi/View-R1.

</details>


### [124] [$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling](https://arxiv.org/abs/2506.21714)
*Denis Gudovskiy,Wenzhao Zheng,Tomoyuki Okuno,Yohei Nakata,Kurt Keutzer*

Main category: cs.LG

TL;DR: 论文提出了一种动态控制连续归一化流（CNFs）和扩散模型（DMs）质量与复杂度权衡的方法，通过重连Transformer架构块和引入时间与长度一致性项，实现高效采样。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要通过减少采样时间步数来提高效率，但本文探索了另一种方向，即动态控制时间步数和神经网络长度，以优化质量与复杂度的权衡。

Method: 通过重连Transformer架构块解决内部离散化ODE，并在训练中引入时间和长度一致性项，实现任意时间步和块数的采样。

Result: 在CelebA-HQ和ImageNet上的实验显示，最高效采样模式下延迟减少3倍，高质量采样时FID分数提升3.5分。

Conclusion: 提出的方法在时间和长度维度上均具有灵活性，显著降低了延迟和内存使用，同时提升了生成质量。

Abstract: Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have
been studied using the unified theoretical framework. Although such models can
generate high-quality data points from a noise distribution, the sampling
demands multiple iterations to solve an ordinary differential equation (ODE)
with high computational complexity. Most existing methods focus on reducing the
number of time steps during the sampling process to improve efficiency. In this
work, we explore a complementary direction in which the quality-complexity
tradeoff can be dynamically controlled in terms of time steps and in the length
of the neural network. We achieve this by rewiring the blocks in the
transformer-based architecture to solve an inner discretized ODE w.r.t. its
length. Then, we employ time- and length-wise consistency terms during flow
matching training, and as a result, the sampling can be performed with an
arbitrary number of time steps and transformer blocks. Unlike others, our
$\textrm{ODE}_t \left(\textrm{ODE}_l \right)$ approach is solver-agnostic in
time dimension and decreases both latency and memory usage. Compared to the
previous state of the art, image generation experiments on CelebA-HQ and
ImageNet show a latency reduction of up to $3\times$ in the most efficient
sampling mode, and a FID score improvement of up to $3.5$ points for
high-quality sampling. We release our code and model weights with fully
reproducible experiments.

</details>


### [125] [SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model](https://arxiv.org/abs/2506.21976)
*Shuhan Tan,John Lambert,Hong Jeon,Sakshum Kulshrestha,Yijing Bai,Jing Luo,Dragomir Anguelov,Mingxing Tan,Chiyu Max Jiang*

Main category: cs.LG

TL;DR: 论文提出了一种名为SceneDiffuser++的端到端生成世界模型，用于实现城市规模的交通模拟，整合了场景生成、动态代理行为建模等技术。


<details>
  <summary>Details</summary>
Motivation: 通过模拟补充有限的真实驾驶里程，实现从起点到终点的无缝城市交通模拟。

Method: 提出SceneDiffuser++模型，基于单一损失函数训练，整合场景生成、动态代理行为建模等技术。

Result: 在城市规模交通模拟中展示了高真实感，并在扩展的Waymo数据集上进行了评估。

Conclusion: SceneDiffuser++为城市规模交通模拟提供了高效且真实的解决方案。

Abstract: The goal of traffic simulation is to augment a potentially limited amount of
manually-driven miles that is available for testing and validation, with a much
larger amount of simulated synthetic miles. The culmination of this vision
would be a generative simulated city, where given a map of the city and an
autonomous vehicle (AV) software stack, the simulator can seamlessly simulate
the trip from point A to point B by populating the city around the AV and
controlling all aspects of the scene, from animating the dynamic agents (e.g.,
vehicles, pedestrians) to controlling the traffic light states. We refer to
this vision as CitySim, which requires an agglomeration of simulation
technologies: scene generation to populate the initial scene, agent behavior
modeling to animate the scene, occlusion reasoning, dynamic scene generation to
seamlessly spawn and remove agents, and environment simulation for factors such
as traffic lights. While some key technologies have been separately studied in
various works, others such as dynamic scene generation and environment
simulation have received less attention in the research community. We propose
SceneDiffuser++, the first end-to-end generative world model trained on a
single loss function capable of point A-to-B simulation on a city scale
integrating all the requirements above. We demonstrate the city-scale traffic
simulation capability of SceneDiffuser++ and study its superior realism under
long simulation conditions. We evaluate the simulation quality on an augmented
version of the Waymo Open Motion Dataset (WOMD) with larger map regions to
support trip-level simulation.

</details>


### [126] [Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling](https://arxiv.org/abs/2506.22304)
*Erkan Turan,Aristotelis Siozopoulos,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: 本文提出了一种基于Koopman算子理论的Koopman-CFM架构，通过将非线性流映射为线性演化空间，实现了一步采样和高效生成，同时提供了对生成过程的解释性分析。


<details>
  <summary>Details</summary>
Motivation: 传统CFM方法依赖非线性ODE求解，计算成本高且难以解释。本文旨在通过Koopman算子理论加速CFM并提升其动态过程的可解释性。

Method: 提出了一种无解码器的Koopman-CFM架构，学习一个嵌入空间，使生成动态变为线性，支持通过矩阵指数实现一步采样。

Result: 在2D数据集和真实基准（MNIST、F-MNIST、TFD）上展示了显著的加速效果，并提供了对生成行为的谱分析工具。

Conclusion: Koopman增强的流匹配方法结合了采样效率和解析结构，为快速且可解释的生成建模提供了潜在路径。

Abstract: Conditional Flow Matching (CFM) offers a simulation-free framework for
training continuous-time generative models, bridging diffusion and flow-based
approaches. However, sampling from CFM still relies on numerically solving
non-linear ODEs which can be computationally expensive and difficult to
interpret. Recent alternatives address sampling speed via trajectory
straightening, mini-batch coupling or distillation. However, these methods
typically do not shed light on the underlying \textit{structure} of the
generative process. In this work, we propose to accelerate CFM and introduce an
interpretable representation of its dynamics by integrating Koopman operator
theory, which models non-linear flows as linear evolution in a learned space of
observables. We introduce a decoder-free Koopman-CFM architecture that learns
an embedding where the generative dynamics become linear, enabling closed-form,
one-step sampling via matrix exponentiation. This results in significant
speedups over traditional CFM as demonstrated on controlled 2D datasets and
real-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face
Dataset (TFD). Unlike previous methods, our approach leads to a well-structured
Koopman generator, whose spectral properties, eigenvalues, and eigenfunctions
offer principled tools for analyzing generative behavior such as temporal
scaling, mode stability, and decomposition in Koopman latent space. By
combining sampling efficiency with analytical structure, Koopman-enhanced flow
matching offers a potential step toward fast and interpretable generative
modeling.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [127] [3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach](https://arxiv.org/abs/2506.21845)
*Zhuodi Cai*

Main category: cs.HC

TL;DR: 3Description是一种实验性的人机协作方法，通过语言和手势描述实现直观的3D建模，旨在解决传统3D建模的可访问性和可用性问题。


<details>
  <summary>Details</summary>
Motivation: 传统3D建模对非专业人士不友好，3Description希望通过人机协作降低门槛，让更多人参与3D建模。

Method: 结合定性研究、产品分析和用户测试，集成自然语言处理和计算机视觉技术（如OpenAI和MediaPipe），通过网页平台实现语言和手势输入的3D建模。

Result: 3Description提供了一种更包容、用户友好的设计流程，增强了人机共创的参与度。

Conclusion: 3Description不仅推动了3D建模的普及，还避免了技术过度主导，保留了人类创造力。

Abstract: This paper presents 3Description, an experimental human-AI collaborative
approach for intuitive 3D modeling. 3Description aims to address accessibility
and usability challenges in traditional 3D modeling by enabling
non-professional individuals to co-create 3D models using verbal and gesture
descriptions. Through a combination of qualitative research, product analysis,
and user testing, 3Description integrates AI technologies such as Natural
Language Processing and Computer Vision, powered by OpenAI and MediaPipe.
Recognizing the web has wide cross-platform capabilities, 3Description is
web-based, allowing users to describe the desired model and subsequently adjust
its components using verbal and gestural inputs. In the era of AI and emerging
media, 3Description not only contributes to a more inclusive and user-friendly
design process, empowering more people to participate in the construction of
the future 3D world, but also strives to increase human engagement in
co-creation with AI, thereby avoiding undue surrender to technology and
preserving human creativity.

</details>


<div id='cond-mat.other'></div>

# cond-mat.other [[Back]](#toc)

### [128] [Comment on "The SPOCK equation of state for condensed phases under arbitrary compression" by R. Myhill](https://arxiv.org/abs/2506.21636)
*Thomas Ruedas*

Main category: cond-mat.other

TL;DR: SPOCK状态方程与可变多方指数状态方程等价。


<details>
  <summary>Details</summary>
Motivation: 探讨SPOCK状态方程与可变多方指数状态方程之间的关系。

Method: 通过理论推导和比较两种状态方程的数学形式。

Result: 证明SPOCK状态方程与可变多方指数状态方程在数学上是等价的。

Conclusion: 两种状态方程的等价性为相关研究提供了新的理论支持。

Abstract: It is shown that the SPOCK equation of state is equivalent to the Variable
Polytrope Index equation of state.

</details>
