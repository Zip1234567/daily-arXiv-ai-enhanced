<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 86]
- [eess.IV](#eess.IV) [Total: 11]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.SC](#cs.SC) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.CL](#cs.CL) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [An Memory-Efficient Framework for Deformable Transformer with Neural Architecture Search](https://arxiv.org/abs/2507.11549)
*Wendong Mao,Mingfan Zhao,Jianfeng Guan,Qiwei Dong,Zhongfeng Wang*

Main category: cs.CV

TL;DR: 提出了一种硬件友好的优化框架，用于解决可变形注意力变换器（DAT）在硬件部署中的内存访问问题，通过NAS和切片策略优化性能。


<details>
  <summary>Details</summary>
Motivation: DAT的数据依赖性采样机制导致不规则内存访问模式，现有加速方法要么硬件开销高，要么牺牲模型精度。

Method: 1. 提出基于NAS的切片策略，自动划分输入特征为均匀块；2. 设计FPGA验证系统。

Result: ImageNet-1K实验显示精度仅下降0.2%；FPGA实验显示DRAM访问次数减少至18%。

Conclusion: 该框架在保持精度的同时显著提升了硬件效率。

Abstract: Deformable Attention Transformers (DAT) have shown remarkable performance in
computer vision tasks by adaptively focusing on informative image regions.
However, their data-dependent sampling mechanism introduces irregular memory
access patterns, posing significant challenges for efficient hardware
deployment. Existing acceleration methods either incur high hardware overhead
or compromise model accuracy. To address these issues, this paper proposes a
hardware-friendly optimization framework for DAT. First, a neural architecture
search (NAS)-based method with a new slicing strategy is proposed to
automatically divide the input feature into uniform patches during the
inference process, avoiding memory conflicts without modifying model
architecture. The method explores the optimal slice configuration by jointly
optimizing hardware cost and inference accuracy. Secondly, an FPGA-based
verification system is designed to test the performance of this framework on
edge-side hardware. Algorithm experiments on the ImageNet-1K dataset
demonstrate that our hardware-friendly framework can maintain have only 0.2%
accuracy drop compared to the baseline DAT. Hardware experiments on Xilinx FPGA
show the proposed method reduces DRAM access times to 18% compared with
existing DAT acceleration methods.

</details>


### [2] [Deformable Dynamic Convolution for Accurate yet Efficient Spatio-Temporal Traffic Prediction](https://arxiv.org/abs/2507.11550)
*Hyeonseok Jin,Geonmin Kim,Kyungbaek Kim*

Main category: cs.CV

TL;DR: 论文提出了一种名为DDCN的动态卷积网络，用于高效且准确的时空交通预测，解决了现有方法在异质性和可扩展性上的限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉时空异质性和处理大规模数据时存在局限性，尤其是图神经网络（GNNs）需要预定义邻接矩阵且复杂度高。

Method: DDCN通过动态应用可变形滤波器，结合编码器-解码器结构，利用空间和时空注意力块强调重要特征。

Result: 在四个真实数据集上的实验表明，DDCN具有竞争性的性能。

Conclusion: DDCN展示了基于CNN的方法在时空交通预测中的潜力和有效性。

Abstract: Spatio-temporal traffic prediction plays a key role in intelligent
transportation systems by enabling accurate prediction in complex urban areas.
Although not only accuracy but also efficiency for scalability is important,
some previous methods struggle to capture heterogeneity such as varying traffic
patterns across regions and time periods. Moreover, Graph Neural Networks
(GNNs), which are the mainstream of traffic prediction, not only require
predefined adjacency matrix, but also limit scalability to large-scale data
containing many nodes due to their inherent complexity. To overcome these
limitations, we propose Deformable Dynamic Convolution Network (DDCN) for
accurate yet efficient traffic prediction. Traditional Convolutional Neural
Networks (CNNs) are limited in modeling non-Euclidean spatial structures and
spatio-temporal heterogeneity, DDCN overcomes these challenges by dynamically
applying deformable filters based on offset. Specifically, DDCN decomposes
transformer-style CNN to encoder-decoder structure, and applies proposed
approaches to the spatial and spatio-temporal attention blocks of the encoder
to emphasize important features. The decoder, composed of feed-forward module,
complements the output of the encoder. This novel structure make DDCN can
perform accurate yet efficient traffic prediction. In comprehensive experiments
on four real-world datasets, DDCN achieves competitive performance, emphasizing
the potential and effectiveness of CNN-based approaches for spatio-temporal
traffic prediction.

</details>


### [3] [Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models](https://arxiv.org/abs/2507.11554)
*Zejian Li,Yize Li,Chenye Meng,Zhongni Liu,Yang Ling,Shengyuan Zhang,Guang Yang,Changyuan Yang,Zhiyuan Yang,Lingyun Sun*

Main category: cs.CV

TL;DR: Inversion-DPO是一种新的对齐框架，通过DDIM反演优化扩散模型，无需奖励模型，显著提升训练效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法计算开销大且可能影响模型精度，Inversion-DPO旨在解决这些问题。

Method: 利用DDIM反演将DPO应用于扩散模型，避免奖励建模，实现高效后训练。

Result: 在文本到图像生成和组合图像生成任务中表现优异，生成高保真且组合一致的图像。

Conclusion: Inversion-DPO为扩散模型提供了一种高效、高精度的对齐方法，适用于复杂生成任务。

Abstract: Recent advancements in diffusion models (DMs) have been propelled by
alignment methods that post-train models to better conform to human
preferences. However, these approaches typically require computation-intensive
training of a base model and a reward model, which not only incurs substantial
computational overhead but may also compromise model accuracy and training
efficiency. To address these limitations, we propose Inversion-DPO, a novel
alignment framework that circumvents reward modeling by reformulating Direct
Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts
intractable posterior sampling in Diffusion-DPO with the deterministic
inversion from winning and losing samples to noise and thus derive a new
post-training paradigm. This paradigm eliminates the need for auxiliary reward
models or inaccurate appromixation, significantly enhancing both precision and
efficiency of training. We apply Inversion-DPO to a basic task of text-to-image
generation and a challenging task of compositional image generation. Extensive
experiments show substantial performance improvements achieved by Inversion-DPO
compared to existing post-training methods and highlight the ability of the
trained generative models to generate high-fidelity compositionally coherent
images. For the post-training of compostitional image geneation, we curate a
paired dataset consisting of 11,140 images with complex structural annotations
and comprehensive scores, designed to enhance the compositional capabilities of
generative models. Inversion-DPO explores a new avenue for efficient,
high-precision alignment in diffusion models, advancing their applicability to
complex realistic generation tasks. Our code is available at
https://github.com/MIGHTYEZ/Inversion-DPO

</details>


### [4] [Reprogramming Vision Foundation Models for Spatio-Temporal Forecasting](https://arxiv.org/abs/2507.11558)
*Changlu Chen,Yanbin Liu,Chaoxi Niu,Ling Chen,Tianqing Zhu*

Main category: cs.CV

TL;DR: ST-VFM是一个新框架，通过重新编程视觉基础模型（VFMs）来解决时空预测任务中的挑战，包括缺乏时间建模能力和视觉与时空数据的模态差异。


<details>
  <summary>Details</summary>
Motivation: 基础模型在自然语言处理和计算机视觉中表现出色，但在时空预测中，大型语言模型（LLMs）难以捕捉丰富的时空相关性。

Method: ST-VFM采用双分支架构，结合原始时空输入和辅助时空流输入，并通过两个重新编程阶段（预VFM和后VFM）处理数据。

Result: 在十个时空数据集上的实验表明，ST-VFM优于现有基线，展示了其有效性和鲁棒性。

Conclusion: ST-VFM是一个强大的通用框架，适用于时空预测任务。

Abstract: Foundation models have achieved remarkable success in natural language
processing and computer vision, demonstrating strong capabilities in modeling
complex patterns. While recent efforts have explored adapting large language
models (LLMs) for time-series forecasting, LLMs primarily capture
one-dimensional sequential dependencies and struggle to model the richer
spatio-temporal (ST) correlations essential for accurate ST forecasting. In
this paper, we present \textbf{ST-VFM}, a novel framework that systematically
reprograms Vision Foundation Models (VFMs) for general-purpose spatio-temporal
forecasting. While VFMs offer powerful spatial priors, two key challenges arise
when applying them to ST tasks: (1) the lack of inherent temporal modeling
capacity and (2) the modality gap between visual and ST data. To address these,
ST-VFM adopts a \emph{dual-branch architecture} that integrates raw ST inputs
with auxiliary ST flow inputs, where the flow encodes lightweight temporal
difference signals interpretable as dynamic spatial cues. To effectively
process these dual-branch inputs, ST-VFM introduces two dedicated reprogramming
stages. The \emph{pre-VFM reprogramming} stage applies a Temporal-Aware Token
Adapter to embed temporal context and align both branches into VFM-compatible
feature spaces. The \emph{post-VFM reprogramming} stage introduces a Bilateral
Cross-Prompt Coordination module, enabling dynamic interaction between branches
through prompt-based conditioning, thus enriching joint representation learning
without modifying the frozen VFM backbone. Extensive experiments on ten
spatio-temporal datasets show that ST-VFM outperforms state-of-the-art
baselines, demonstrating effectiveness and robustness across VFM backbones
(e.g., DINO, CLIP, DEIT) and ablation studies, establishing it as a strong
general framework for spatio-temporal forecasting.

</details>


### [5] [Expert Operational GANS: Towards Real-Color Underwater Image Restoration](https://arxiv.org/abs/2507.11562)
*Ozer Can Devecioglu,Serkan Kiranyaz,Mehmet Yamac,Moncef Gabbouj*

Main category: cs.CV

TL;DR: xOp-GAN是一种新型GAN模型，通过多个专家生成器网络分别处理不同质量范围的图像，结合判别器的感知置信度选择最佳恢复图像，显著提升了水下图像恢复性能。


<details>
  <summary>Details</summary>
Motivation: 水下图像恢复因复杂的光传播、散射和深度相关衰减导致的变形伪影而具有挑战性，传统单生成器网络难以覆盖所有视觉退化范围。

Method: 提出xOp-GAN，包含多个专家生成器网络，每个生成器针对特定质量范围的图像进行训练，判别器在推理阶段选择最佳恢复图像。

Result: 在LSUI数据集上，xOp-GAN的PSNR达到25.16 dB，显著优于单回归器模型，且复杂度更低。

Conclusion: xOp-GAN通过多生成器设计和判别器的动态选择机制，有效解决了水下图像恢复的异质性问题，性能显著提升。

Abstract: The wide range of deformation artifacts that arise from complex light
propagation, scattering, and depth-dependent attenuation makes the underwater
image restoration to remain a challenging problem. Like other single deep
regressor networks, conventional GAN-based restoration methods struggle to
perform well across this heterogeneous domain, since a single generator network
is typically insufficient to capture the full range of visual degradations. In
order to overcome this limitation, we propose xOp-GAN, a novel GAN model with
several expert generator networks, each trained solely on a particular subset
with a certain image quality. Thus, each generator can learn to maximize its
restoration performance for a particular quality range. Once a xOp-GAN is
trained, each generator can restore the input image and the best restored image
can then be selected by the discriminator based on its perceptual confidence
score. As a result, xOP-GAN is the first GAN model with multiple generators
where the discriminator is being used during the inference of the regression
task. Experimental results on benchmark Large Scale Underwater Image (LSUI)
dataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB,
surpassing all single-regressor models by a large margin even, with reduced
complexity.

</details>


### [6] [Data-Driven Meta-Analysis and Public-Dataset Evaluation for Sensor-Based Gait Age Estimation](https://arxiv.org/abs/2507.11571)
*Varun Velankar*

Main category: cs.CV

TL;DR: 该论文通过综合分析59项研究和大型实验，评估了步态识别年龄的性能，并提出了降低误差的实用指南。


<details>
  <summary>Details</summary>
Motivation: 步态识别年龄在医疗、安全和人机交互中有重要应用，但现有方法的误差较大，需建立更准确的基准。

Method: 结合元分析、大型数据集实验（如OU-ISIR和VersatileGait）和可解释性技术（如Grad-CAM），评估了多种模型（CNN、SVM等）。

Result: CNN平均误差4.2年，多传感器融合误差3.4年；步态指标与年龄相关性显著；深度学习模型准确率达96%。

Conclusion: 通过综合方法，论文为步态年龄识别建立了性能基准，并提出了将误差降至3年以下的实用建议。

Abstract: Estimating a person's age from their gait has important applications in
healthcare, security and human-computer interaction. In this work, we review
fifty-nine studies involving over seventy-five thousand subjects recorded with
video, wearable and radar sensors. We observe that convolutional neural
networks produce an average error of about 4.2 years, inertial-sensor models
about 4.5 years and multi-sensor fusion as low as 3.4 years, with notable
differences between lab and real-world data. We then analyse sixty-three
thousand eight hundred forty-six gait cycles from the OU-ISIR Large-Population
dataset to quantify correlations between age and five key metrics: stride
length, walking speed, step cadence, step-time variability and joint-angle
entropy, with correlation coefficients of at least 0.27. Next, we fine-tune a
ResNet34 model and apply Grad-CAM to reveal that the network attends to the
knee and pelvic regions, consistent with known age-related gait changes.
Finally, on a one hundred thousand sample subset of the VersatileGait database,
we compare support vector machines, decision trees, random forests, multilayer
perceptrons and convolutional neural networks, finding that deep networks
achieve up to 96 percent accuracy while processing each sample in under 0.1
seconds. By combining a broad meta-analysis with new large-scale experiments
and interpretable visualizations, we establish solid performance baselines and
practical guidelines for reducing gait-age error below three years in
real-world scenarios.

</details>


### [7] [What cat is that? A re-id model for feral cats](https://arxiv.org/abs/2507.11575)
*Victor Caquilpan*

Main category: cs.CV

TL;DR: 论文探讨了利用改进的PPGNet模型（PPGNet-Cat）通过相机陷阱图像识别野猫个体，以减轻其对野生动物的影响。


<details>
  <summary>Details</summary>
Motivation: 野猫对澳大利亚野生动物造成严重威胁，需要有效监控以减少其影响。

Method: 改进PPGNet模型（原用于东北虎识别）为PPGNet-Cat，并探索对比学习方法如ArcFace损失。

Result: PPGNet-Cat表现优异，mAP达0.86，rank-1准确率为0.95。

Conclusion: PPGNet-Cat在野猫重识别领域具有竞争力。

Abstract: Feral cats exert a substantial and detrimental impact on Australian wildlife,
placing them among the most dangerous invasive species worldwide. Therefore,
closely monitoring these cats is essential labour in minimising their effects.
In this context, the potential application of Re-Identification (re-ID) emerges
to enhance monitoring activities for these animals, utilising images captured
by camera traps. This project explores different CV approaches to create a
re-ID model able to identify individual feral cats in the wild. The main
approach consists of modifying a part-pose guided network (PPGNet) model,
initially used in the re-ID of Amur tigers, to be applicable for feral cats.
This adaptation, resulting in PPGNet-Cat, which incorporates specific
modifications to suit the characteristics of feral cats images. Additionally,
various experiments were conducted, particularly exploring contrastive learning
approaches such as ArcFace loss. The main results indicate that PPGNet-Cat
excels in identifying feral cats, achieving high performance with a mean
Average Precision (mAP) of 0.86 and a rank-1 accuracy of 0.95. These outcomes
establish PPGNet-Cat as a competitive model within the realm of re-ID.

</details>


### [8] [Language-Guided Contrastive Audio-Visual Masked Autoencoder with Automatically Generated Audio-Visual-Text Triplets from Videos](https://arxiv.org/abs/2507.11967)
*Yuchi Ishikawa,Shota Nakada,Hokuto Munakata,Kazuhiro Saito,Tatsuya Komatsu,Yoshimitsu Aoki*

Main category: cs.CV

TL;DR: 提出了一种结合文本编码器的音频-视觉掩码自编码器（LG-CAV-MAE），通过自动生成音频-视觉-文本三元组提升多模态表示学习，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提升音频-视觉表示学习的效果，通过引入文本模态增强模型的多模态理解能力。

Method: 集成预训练文本编码器到对比音频-视觉掩码自编码器中，自动生成音频-视觉-文本三元组，使用CLAP过滤确保对齐。

Result: 在音频-视觉检索任务中召回率提升5.6%，分类任务准确率提升3.2%。

Conclusion: LG-CAV-MAE通过多模态学习和自动生成高质量数据，显著提升了音频-视觉任务的性能。

Abstract: In this paper, we propose Language-Guided Contrastive Audio-Visual Masked
Autoencoders (LG-CAV-MAE) to improve audio-visual representation learning.
LG-CAV-MAE integrates a pretrained text encoder into contrastive audio-visual
masked autoencoders, enabling the model to learn across audio, visual and text
modalities. To train LG-CAV-MAE, we introduce an automatic method to generate
audio-visual-text triplets from unlabeled videos. We first generate frame-level
captions using an image captioning model and then apply CLAP-based filtering to
ensure strong alignment between audio and captions. This approach yields
high-quality audio-visual-text triplets without requiring manual annotations.
We evaluate LG-CAV-MAE on audio-visual retrieval tasks, as well as an
audio-visual classification task. Our method significantly outperforms existing
approaches, achieving up to a 5.6% improvement in recall@10 for retrieval tasks
and a 3.2% improvement for the classification task.

</details>


### [9] [SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation](https://arxiv.org/abs/2507.11579)
*Sathvik Chereddy,John Femiani*

Main category: cs.CV

TL;DR: SketchDNN是一种生成模型，通过统一的连续-离散扩散过程合成CAD草图，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决CAD草图中原始参数化的异质性和原始元素的排列不变性问题。

Method: 采用Gaussian-Softmax扩散，将高斯噪声扰动的logits通过softmax变换投影到概率单纯形上。

Result: FID从16.04降至7.80，NLL从84.8降至81.33，在SketchGraphs数据集上达到新SOTA。

Conclusion: SketchDNN在CAD草图生成中表现出色，解决了关键挑战并提升了性能。

Abstract: We present SketchDNN, a generative model for synthesizing CAD sketches that
jointly models both continuous parameters and discrete class labels through a
unified continuous-discrete diffusion process. Our core innovation is
Gaussian-Softmax diffusion, where logits perturbed with Gaussian noise are
projected onto the probability simplex via a softmax transformation,
facilitating blended class labels for discrete variables. This formulation
addresses 2 key challenges, namely, the heterogeneity of primitive
parameterizations and the permutation invariance of primitives in CAD sketches.
Our approach significantly improves generation quality, reducing Fr\'echet
Inception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL)
from 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch
generation on the SketchGraphs dataset.

</details>


### [10] [Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders](https://arxiv.org/abs/2507.11638)
*Benjamin Keel,Aaron Quyn,David Jayne,Maryam Mohsin,Samuel D. Relton*

Main category: cs.CV

TL;DR: 使用变分自编码器（VAE）替代传统CNN，提高直肠癌淋巴结转移的MRI诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于淋巴结大小、形状和纹理的影像学标准诊断准确性有限，VAE能直接编码视觉特征，生成更可解释的潜在空间。

Method: 提出VAE-MLP模型，应用于168例未接受新辅助治疗患者的MRI数据集，以术后病理N分期为金标准评估。

Result: VAE-MLP在MRI数据集上表现优异，AUC 0.86，敏感性0.79，特异性0.85。

Conclusion: VAE-MLP模型在淋巴结转移诊断中达到先进水平，代码已开源。

Abstract: Effective treatment for rectal cancer relies on accurate lymph node
metastasis (LNM) staging. However, radiological criteria based on lymph node
(LN) size, shape and texture morphology have limited diagnostic accuracy. In
this work, we investigate applying a Variational Autoencoder (VAE) as a feature
encoder model to replace the large pre-trained Convolutional Neural Network
(CNN) used in existing approaches. The motivation for using a VAE is that the
generative model aims to reconstruct the images, so it directly encodes visual
features and meaningful patterns across the data. This leads to a disentangled
and structured latent space which can be more interpretable than a CNN. Models
are deployed on an in-house MRI dataset with 168 patients who did not undergo
neo-adjuvant treatment. The post-operative pathological N stage was used as the
ground truth to evaluate model predictions. Our proposed model 'VAE-MLP'
achieved state-of-the-art performance on the MRI dataset, with cross-validated
metrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85
+/- 0.05. Code is available at:
https://github.com/benkeel/Lymph_Node_Classification_MIUA.

</details>


### [11] [Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment](https://arxiv.org/abs/2507.11642)
*Abhishek Jaiswal,Nisheeth Srivastava*

Main category: cs.CV

TL;DR: 论文提出了一种基于姿势的心理状态推断方法，通过板球运动验证其有效性，F1分数超过75%，AUC-ROC超过80%。


<details>
  <summary>Details</summary>
Motivation: 姿势推断在诊断疲劳、预防伤害和提升表现方面潜力巨大，但面临数据敏感性问题。体育场景为获取多样化情绪数据提供了可行替代方案。

Method: 通过运动分析从板球活动视频中识别人类意图，利用现有数据统计作为弱监督验证。

Result: 方法在区分攻击性和防御性击球意图上表现优异，F1分数75%以上，AUC-ROC超过80%。

Conclusion: 姿势泄露了意图推断的强信号，研究为体育分析和跨领域人类行为分析提供了通用技术。

Abstract: Posture-based mental state inference has significant potential in diagnosing
fatigue, preventing injury, and enhancing performance across various domains.
Such tools must be research-validated with large datasets before being
translated into practice. Unfortunately, such vision diagnosis faces serious
challenges due to the sensitivity of human subject data. To address this, we
identify sports settings as a viable alternative for accumulating data from
human subjects experiencing diverse emotional states. We test our hypothesis in
the game of cricket and present a posture-based solution to identify human
intent from activity videos. Our method achieves over 75\% F1 score and over
80\% AUC-ROC in discriminating aggressive and defensive shot intent through
motion analysis. These findings indicate that posture leaks out strong signals
for intent inference, even with inherent noise in the data pipeline.
Furthermore, we utilize existing data statistics as weak supervision to
validate our findings, offering a potential solution for overcoming data
labelling limitations. This research contributes to generalizable techniques
for sports analytics and also opens possibilities for applying human behavior
analysis across various fields.

</details>


### [12] [VISTA: Monocular Segmentation-Based Mapping for Appearance and View-Invariant Global Localization](https://arxiv.org/abs/2507.11653)
*Hannah Shafferman,Annika Thomas,Jouko Kinnari,Michael Ricard,Jose Nino,Jonathan How*

Main category: cs.CV

TL;DR: VISTA是一种基于分割和跟踪的单目全局定位框架，能够在不同视角和季节变化下实现一致定位，无需特定领域训练。


<details>
  <summary>Details</summary>
Motivation: 解决无结构环境中因视角变化、季节变化等因素导致的传统定位方法失效问题。

Method: 结合前端对象分割与跟踪管道及子地图对应搜索，利用几何一致性对齐参考帧。

Result: 在季节和斜视角航空数据集上，召回率提升69%，地图大小仅为基线方法的0.6%。

Conclusion: VISTA在多样环境下表现优异，适合资源受限平台实时应用。

Abstract: Global localization is critical for autonomous navigation, particularly in
scenarios where an agent must localize within a map generated in a different
session or by another agent, as agents often have no prior knowledge about the
correlation between reference frames. However, this task remains challenging in
unstructured environments due to appearance changes induced by viewpoint
variation, seasonal changes, spatial aliasing, and occlusions -- known failure
modes for traditional place recognition methods. To address these challenges,
we propose VISTA (View-Invariant Segmentation-Based Tracking for Frame
Alignment), a novel open-set, monocular global localization framework that
combines: 1) a front-end, object-based, segmentation and tracking pipeline,
followed by 2) a submap correspondence search, which exploits geometric
consistencies between environment maps to align vehicle reference frames. VISTA
enables consistent localization across diverse camera viewpoints and seasonal
changes, without requiring any domain-specific training or finetuning. We
evaluate VISTA on seasonal and oblique-angle aerial datasets, achieving up to a
69% improvement in recall over baseline methods. Furthermore, we maintain a
compact object-based map that is only 0.6% the size of the most
memory-conservative baseline, making our approach capable of real-time
implementation on resource-constrained platforms.

</details>


### [13] [Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility Analysis](https://arxiv.org/abs/2507.11730)
*Maciej Szankin,Vidhyananth Venkatasamy,Lihang Ying*

Main category: cs.CV

TL;DR: 论文比较了多模态视觉语言模型（VLMs）与传统CNN OCR在户外广告文本识别中的表现，发现VLMs在场景理解上更优，但轻量级CNN在裁剪文本识别上仍具竞争力。


<details>
  <summary>Details</summary>
Motivation: 户外广告文本识别在复杂场景中仍具挑战性，传统OCR方法难以应对多变字体和天气干扰，多模态VLMs提供了新的解决方案。

Method: 系统评估了Qwen 2.5 VL 3B、InternVL3和SmolVLM2等VLMs与PaddleOCRv4（CNN基线）在两个公开数据集（ICDAR 2015和SVT）上的表现，并加入合成天气干扰模拟真实场景。

Result: 部分VLMs在整体场景推理上表现优异，但轻量级CNN在裁剪文本识别上仍具竞争力且计算成本更低。

Conclusion: VLMs在场景理解上有潜力，但CNN在特定任务中仍具优势；公开了天气增强的基准测试和评估代码以促进未来研究。

Abstract: Outdoor advertisements remain a critical medium for modern marketing, yet
accurately verifying billboard text visibility under real-world conditions is
still challenging. Traditional Optical Character Recognition (OCR) pipelines
excel at cropped text recognition but often struggle with complex outdoor
scenes, varying fonts, and weather-induced visual noise. Recently, multimodal
Vision-Language Models (VLMs) have emerged as promising alternatives, offering
end-to-end scene understanding with no explicit detection step. This work
systematically benchmarks representative VLMs - including Qwen 2.5 VL 3B,
InternVL3, and SmolVLM2 - against a compact CNN-based OCR baseline
(PaddleOCRv4) across two public datasets (ICDAR 2015 and SVT), augmented with
synthetic weather distortions to simulate realistic degradation. Our results
reveal that while selected VLMs excel at holistic scene reasoning, lightweight
CNN pipelines still achieve competitive accuracy for cropped text at a fraction
of the computational cost-an important consideration for edge deployment. To
foster future research, we release our weather-augmented benchmark and
evaluation code publicly.

</details>


### [14] [Beyond Task-Specific Reasoning: A Unified Conditional Generative Framework for Abstract Visual Reasoning](https://arxiv.org/abs/2507.11761)
*Fan Shi,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 论文提出了一种统一的条件生成求解器（UCGS），旨在通过单一框架解决多种抽象视觉推理（AVR）任务，避免了任务特定设计或参数调整的需求。


<details>
  <summary>Details</summary>
Motivation: 设计具有人类抽象视觉推理能力的智能系统是人工智能领域的长期目标，但现有方法通常需要针对不同任务进行特定设计或重新训练，增加了成本。

Method: 通过将AVR任务重新定义为预测问题面板中目标图像的可预测性，并训练一个条件生成模型，UCGS能够在统一框架下解决多种任务。

Result: 实验表明，UCGS通过多任务训练，在多种AVR任务中展现出抽象推理能力，并具备零样本推理能力，能够处理未见过的任务。

Conclusion: UCGS提供了一种高效且通用的解决方案，减少了解决AVR问题的成本，并展示了零样本推理的潜力。

Abstract: Abstract visual reasoning (AVR) enables humans to quickly discover and
generalize abstract rules to new scenarios. Designing intelligent systems with
human-like AVR abilities has been a long-standing topic in the artificial
intelligence community. Deep AVR solvers have recently achieved remarkable
success in various AVR tasks. However, they usually use task-specific designs
or parameters in different tasks. In such a paradigm, solving new tasks often
means retraining the model, and sometimes retuning the model architectures,
which increases the cost of solving AVR problems. In contrast to task-specific
approaches, this paper proposes a novel Unified Conditional Generative Solver
(UCGS), aiming to address multiple AVR tasks in a unified framework. First, we
prove that some well-known AVR tasks can be reformulated as the problem of
estimating the predictability of target images in problem panels. Then, we
illustrate that, under the proposed framework, training one conditional
generative model can solve various AVR tasks. The experiments show that with a
single round of multi-task training, UCGS demonstrates abstract reasoning
ability across various AVR tasks. Especially, UCGS exhibits the ability of
zero-shot reasoning, enabling it to perform abstract reasoning on problems from
unseen AVR tasks in the testing phase.

</details>


### [15] [CorrMoE: Mixture of Experts with De-stylization Learning for Cross-Scene and Cross-Domain Correspondence Pruning](https://arxiv.org/abs/2507.11834)
*Peiwen Xia,Tangfei Liao,Wei Zhu,Danhuai Zhao,Jianjun Ke,Kaihao Zhang,Tong Lu,Tao Wang*

Main category: cs.CV

TL;DR: CorrMoE是一种新颖的对应关系修剪框架，通过去风格化双分支和双融合专家混合模块，提升了跨域和跨场景变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在视觉域一致性和场景多样性方面的不足，提升对应关系修剪的鲁棒性。

Method: 提出De-stylization Dual Branch和Bi-Fusion Mixture of Experts模块，分别处理域偏移和场景多样性。

Result: 在基准数据集上表现出优于现有方法的准确性和泛化能力。

Conclusion: CorrMoE在跨域和跨场景变化下具有更强的鲁棒性和性能优势。

Abstract: Establishing reliable correspondences between image pairs is a fundamental
task in computer vision, underpinning applications such as 3D reconstruction
and visual localization. Although recent methods have made progress in pruning
outliers from dense correspondence sets, they often hypothesize consistent
visual domains and overlook the challenges posed by diverse scene structures.
In this paper, we propose CorrMoE, a novel correspondence pruning framework
that enhances robustness under cross-domain and cross-scene variations. To
address domain shift, we introduce a De-stylization Dual Branch, performing
style mixing on both implicit and explicit graph features to mitigate the
adverse influence of domain-specific representations. For scene diversity, we
design a Bi-Fusion Mixture of Experts module that adaptively integrates
multi-perspective features through linear-complexity attention and dynamic
expert routing. Extensive experiments on benchmark datasets demonstrate that
CorrMoE achieves superior accuracy and generalization compared to
state-of-the-art methods. The code and pre-trained models are available at
https://github.com/peiwenxia/CorrMoE.

</details>


### [16] [ProtoConNet: Prototypical Augmentation and Alignment for Open-Set Few-Shot Image Classification](https://arxiv.org/abs/2507.11845)
*Kexuan Shi,Zhuang Qi,Jingjing Zhu,Lei Meng,Yaochen Zhang,Haibei Huang,Xiangxu Meng*

Main category: cs.CV

TL;DR: ProtoConNet提出了一种原型增强与对齐方法，通过整合背景信息提升少样本图像分类的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖单张图像的视觉信息，忽略了上下文信息的整合，导致在少样本场景下泛化能力不足。

Method: ProtoConNet包含三个模块：聚类数据选择（CDS）挖掘多样数据模式，上下文增强语义细化（CSR）整合背景信息，原型对齐（PA）缩小图像表示与类原型的差距。

Result: 在两个数据集上的实验表明，ProtoConNet在少样本场景下提升了表示学习效果，并能更好识别开放集样本。

Conclusion: ProtoConNet通过整合上下文信息，显著提升了少样本开放集分类的性能。

Abstract: Open-set few-shot image classification aims to train models using a small
amount of labeled data, enabling them to achieve good generalization when
confronted with unknown environments. Existing methods mainly use visual
information from a single image to learn class representations to distinguish
known from unknown categories. However, these methods often overlook the
benefits of integrating rich contextual information. To address this issue,
this paper proposes a prototypical augmentation and alignment method, termed
ProtoConNet, which incorporates background information from different samples
to enhance the diversity of the feature space, breaking the spurious
associations between context and image subjects in few-shot scenarios.
Specifically, it consists of three main modules: the clustering-based data
selection (CDS) module mines diverse data patterns while preserving core
features; the contextual-enhanced semantic refinement (CSR) module builds a
context dictionary to integrate into image representations, which boosts the
model's robustness in various scenarios; and the prototypical alignment (PA)
module reduces the gap between image representations and class prototypes,
amplifying feature distances for known and unknown classes. Experimental
results from two datasets verified that ProtoConNet enhances the effectiveness
of representation learning in few-shot scenarios and identifies open-set
samples, making it superior to existing methods.

</details>


### [17] [From Coarse to Nuanced: Cross-Modal Alignment of Fine-Grained Linguistic Cues and Visual Salient Regions for Dynamic Emotion Recognition](https://arxiv.org/abs/2507.11892)
*Yu Liu,Leyuan Qu,Hanlei Shi,Di Gao,Yuhua Zheng,Taihao Li*

Main category: cs.CV

TL;DR: 论文提出GRACE方法，通过动态运动建模、语义文本细化和跨模态对齐，提升动态面部表情识别的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用文本中的情感线索，且缺乏有效过滤无关面部动态的机制。

Method: GRACE结合动态运动建模、语义文本细化（CATE模块）和基于熵正则化最优传输的跨模态对齐。

Result: 在三个基准数据集上显著提升识别性能，特别是在模糊或不平衡情感类别中，达到SOTA。

Conclusion: GRACE通过细粒度跨模态对齐和情感感知文本增强，有效解决了现有方法的局限性。

Abstract: Dynamic Facial Expression Recognition (DFER) aims to identify human emotions
from temporally evolving facial movements and plays a critical role in
affective computing. While recent vision-language approaches have introduced
semantic textual descriptions to guide expression recognition, existing methods
still face two key limitations: they often underutilize the subtle emotional
cues embedded in generated text, and they have yet to incorporate sufficiently
effective mechanisms for filtering out facial dynamics that are irrelevant to
emotional expression. To address these gaps, We propose GRACE, Granular
Representation Alignment for Cross-modal Emotion recognition that integrates
dynamic motion modeling, semantic text refinement, and token-level cross-modal
alignment to facilitate the precise localization of emotionally salient
spatiotemporal features. Our method constructs emotion-aware textual
descriptions via a Coarse-to-fine Affective Text Enhancement (CATE) module and
highlights expression-relevant facial motion through a motion-difference
weighting mechanism. These refined semantic and visual signals are aligned at
the token level using entropy-regularized optimal transport. Experiments on
three benchmark datasets demonstrate that our method significantly improves
recognition performance, particularly in challenging settings with ambiguous or
imbalanced emotion classes, establishing new state-of-the-art (SOTA) results in
terms of both UAR and WAR.

</details>


### [18] [Spatial Frequency Modulation for Semantic Segmentation](https://arxiv.org/abs/2507.11893)
*Linwei Chen,Ying Fu,Lin Gu,Dezhi Zheng,Jifeng Dai*

Main category: cs.CV

TL;DR: 提出了一种空间频率调制（SFM）方法，通过调制高频特征到低频以减少下采样时的失真，并通过上采样恢复细节，适用于多种任务。


<details>
  <summary>Details</summary>
Motivation: 高频信息对语义分割至关重要，但下采样会导致失真或混叠，需要一种方法保留这些细节。

Method: 使用自适应重采样（ARS）调制高频特征，并通过多尺度自适应上采样（MSAU）恢复高频信息。

Result: SFM有效减轻混叠并保留细节，适用于分类、对抗鲁棒性、实例分割等任务。

Conclusion: SFM是一种通用且有效的方法，可无缝集成到不同架构中，显著提升性能。

Abstract: High spatial frequency information, including fine details like textures,
significantly contributes to the accuracy of semantic segmentation. However,
according to the Nyquist-Shannon Sampling Theorem, high-frequency components
are vulnerable to aliasing or distortion when propagating through downsampling
layers such as strided-convolution. Here, we propose a novel Spatial Frequency
Modulation (SFM) that modulates high-frequency features to a lower frequency
before downsampling and then demodulates them back during upsampling.
Specifically, we implement modulation through adaptive resampling (ARS) and
design a lightweight add-on that can densely sample the high-frequency areas to
scale up the signal, thereby lowering its frequency in accordance with the
Frequency Scaling Property. We also propose Multi-Scale Adaptive Upsampling
(MSAU) to demodulate the modulated feature and recover high-frequency
information through non-uniform upsampling This module further improves
segmentation by explicitly exploiting information interaction between densely
and sparsely resampled areas at multiple scales. Both modules can seamlessly
integrate with various architectures, extending from convolutional neural
networks to transformers. Feature visualization and analysis confirm that our
method effectively alleviates aliasing while successfully retaining details
after demodulation. Finally, we validate the broad applicability and
effectiveness of SFM by extending it to image classification, adversarial
robustness, instance segmentation, and panoptic segmentation tasks. The code is
available at
\href{https://github.com/Linwei-Chen/SFM}{https://github.com/Linwei-Chen/SFM}.

</details>


### [19] [SEPose: A Synthetic Event-based Human Pose Estimation Dataset for Pedestrian Monitoring](https://arxiv.org/abs/2507.11910)
*Kaustav Chanda,Aayush Atul Verma,Arpitsinh Vaghela,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: SEPose是一个合成的基于事件的人体姿态估计数据集，用于固定行人感知，填补了此类数据的空白。


<details>
  <summary>Details</summary>
Motivation: 解决行人监测系统中因数据不足而难以应对挑战性条件的问题。

Method: 利用CARLA模拟器和动态视觉传感器生成SEPose数据集，包含近350K标注行人姿态。

Result: 在真实事件数据上验证了RVT和YOLOv8模型的模拟到现实的泛化能力。

Conclusion: SEPose数据集为事件传感器在行人监测中的应用提供了有力支持。

Abstract: Event-based sensors have emerged as a promising solution for addressing
challenging conditions in pedestrian and traffic monitoring systems. Their
low-latency and high dynamic range allow for improved response time in
safety-critical situations caused by distracted walking or other unusual
movements. However, the availability of data covering such scenarios remains
limited. To address this gap, we present SEPose -- a comprehensive synthetic
event-based human pose estimation dataset for fixed pedestrian perception
generated using dynamic vision sensors in the CARLA simulator. With nearly 350K
annotated pedestrians with body pose keypoints from the perspective of fixed
traffic cameras, SEPose is a comprehensive synthetic multi-person pose
estimation dataset that spans busy and light crowds and traffic across diverse
lighting and weather conditions in 4-way intersections in urban, suburban, and
rural environments. We train existing state-of-the-art models such as RVT and
YOLOv8 on our dataset and evaluate them on real event-based data to demonstrate
the sim-to-real generalization capabilities of the proposed dataset.

</details>


### [20] [Dark-EvGS: Event Camera as an Eye for Radiance Field in the Dark](https://arxiv.org/abs/2507.11931)
*Jingqian Wu,Peiqi Duan,Zongqiang Wang,Changwei Wang,Boxin Shi,Edmund Y. Lam*

Main category: cs.CV

TL;DR: 论文提出Dark-EvGS框架，利用事件相机和3D高斯溅射技术，在低光环境下重建多视角明亮图像，解决了噪声、低质量和色调不一致问题。


<details>
  <summary>Details</summary>
Motivation: 传统相机在低光环境下难以捕捉清晰多视角图像，事件相机和高斯溅射技术虽有潜力，但仍面临噪声、低质量和色调不一致的挑战。

Method: 提出Dark-EvGS框架，采用三重监督学习、色调匹配模块，并构建首个真实数据集。

Result: 实验表明，该方法在低光条件下优于现有方法，成功重建辐射场。

Conclusion: Dark-EvGS框架有效解决了低光环境下的多视角图像重建问题，为相关任务提供了新思路。

Abstract: In low-light environments, conventional cameras often struggle to capture
clear multi-view images of objects due to dynamic range limitations and motion
blur caused by long exposure. Event cameras, with their high-dynamic range and
high-speed properties, have the potential to mitigate these issues.
Additionally, 3D Gaussian Splatting (GS) enables radiance field reconstruction,
facilitating bright frame synthesis from multiple viewpoints in low-light
conditions. However, naively using an event-assisted 3D GS approach still faced
challenges because, in low light, events are noisy, frames lack quality, and
the color tone may be inconsistent. To address these issues, we propose
Dark-EvGS, the first event-assisted 3D GS framework that enables the
reconstruction of bright frames from arbitrary viewpoints along the camera
trajectory. Triplet-level supervision is proposed to gain holistic knowledge,
granular details, and sharp scene rendering. The color tone matching block is
proposed to guarantee the color consistency of the rendered frames.
Furthermore, we introduce the first real-captured dataset for the event-guided
bright frame synthesis task via 3D GS-based radiance field reconstruction.
Experiments demonstrate that our method achieves better results than existing
methods, conquering radiance field reconstruction under challenging low-light
conditions. The code and sample data are included in the supplementary
material.

</details>


### [21] [Hyperphantasia: A Benchmark for Evaluating the Mental Visualization Capabilities of Multimodal LLMs](https://arxiv.org/abs/2507.11932)
*Mohammad Shahab Sepehri,Berk Tinaz,Zalan Fabian,Mahdi Soltanolkotabi*

Main category: cs.CV

TL;DR: 论文提出Hyperphantasia基准，用于评估多模态大语言模型（MLLMs）的心理可视化能力，发现其与人类表现存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前基准主要评估被动视觉感知，缺乏对主动构建视觉模式以支持问题解决能力的评估，而心理可视化是人类认知的核心能力。

Method: 通过四个程序生成的谜题任务，分三个难度级别评估MLLMs的心理可视化能力，并探索强化学习提升视觉模拟能力的潜力。

Result: 评估显示MLLMs在心理可视化方面与人类表现差距显著，部分模型仅能识别视觉模式，但无法实现稳健的心理可视化。

Conclusion: 心理可视化仍是当前MLLMs的开放挑战，需进一步研究提升其能力。

Abstract: Mental visualization, the ability to construct and manipulate visual
representations internally, is a core component of human cognition and plays a
vital role in tasks involving reasoning, prediction, and abstraction. Despite
the rapid progress of Multimodal Large Language Models (MLLMs), current
benchmarks primarily assess passive visual perception, offering limited insight
into the more active capability of internally constructing visual patterns to
support problem solving. Yet mental visualization is a critical cognitive skill
in humans, supporting abilities such as spatial navigation, predicting physical
trajectories, and solving complex visual problems through imaginative
simulation. To bridge this gap, we introduce Hyperphantasia, a synthetic
benchmark designed to evaluate the mental visualization abilities of MLLMs
through four carefully constructed puzzles. Each task is procedurally generated
and presented at three difficulty levels, enabling controlled analysis of model
performance across increasing complexity. Our comprehensive evaluation of
state-of-the-art models reveals a substantial gap between the performance of
humans and MLLMs. Additionally, we explore the potential of reinforcement
learning to improve visual simulation capabilities. Our findings suggest that
while some models exhibit partial competence in recognizing visual patterns,
robust mental visualization remains an open challenge for current MLLMs.

</details>


### [22] [RaDL: Relation-aware Disentangled Learning for Multi-Instance Text-to-Image Generation](https://arxiv.org/abs/2507.11947)
*Geon Park,Seon Bin Kim,Gunho Jung,Seong-Whan Lee*

Main category: cs.CV

TL;DR: RaDL框架通过关系感知解耦学习，解决了多实例图像生成中的关系差异和属性泄漏问题，显著提升了生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多实例图像生成中难以处理实例间关系差异和多重属性泄漏问题。

Method: 提出RaDL框架，利用可学习参数增强实例属性，并通过关系注意力生成关系感知图像特征。

Result: 在COCO-Position等基准测试中，RaDL在位置准确性、多重属性和实例关系方面表现优于现有方法。

Conclusion: RaDL是多实例图像生成中考虑实例关系和多重属性的有效解决方案。

Abstract: With recent advancements in text-to-image (T2I) models, effectively
generating multiple instances within a single image prompt has become a crucial
challenge. Existing methods, while successful in generating positions of
individual instances, often struggle to account for relationship discrepancy
and multiple attributes leakage. To address these limitations, this paper
proposes the relation-aware disentangled learning (RaDL) framework. RaDL
enhances instance-specific attributes through learnable parameters and
generates relation-aware image features via Relation Attention, utilizing
action verbs extracted from the global prompt. Through extensive evaluations on
benchmarks such as COCO-Position, COCO-MIG, and DrawBench, we demonstrate that
RaDL outperforms existing methods, showing significant improvements in
positional accuracy, multiple attributes consideration, and the relationships
between instances. Our results present RaDL as the solution for generating
images that consider both the relationships and multiple attributes of each
instance within the multi-instance image.

</details>


### [23] [Prototypical Progressive Alignment and Reweighting for Generalizable Semantic Segmentation](https://arxiv.org/abs/2507.11955)
*Yuhang Zhang,Zhengyu Zhang,Muxin Liao,Shishun Tian,Wenbin Zou,Lu Zhang,Chen Xu*

Main category: cs.CV

TL;DR: 论文提出PPAR框架，通过渐进对齐和重加权机制解决通用语义分割中的原型对齐问题，利用CLIP模型提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 通用语义分割在未见目标域上表现不佳，现有原型对齐方法存在粗糙、易过拟合和忽略特征差异的问题。

Method: 定义OTP和VTP原型，采用渐进对齐策略和原型重加权机制，减少负迁移。

Result: 在多个基准测试中达到最优性能。

Conclusion: PPAR框架有效提升通用语义分割的泛化能力。

Abstract: Generalizable semantic segmentation aims to perform well on unseen target
domains, a critical challenge due to real-world applications requiring high
generalizability. Class-wise prototypes, representing class centroids, serve as
domain-invariant cues that benefit generalization due to their stability and
semantic consistency. However, this approach faces three challenges. First,
existing methods often adopt coarse prototypical alignment strategies, which
may hinder performance. Second, naive prototypes computed by averaging source
batch features are prone to overfitting and may be negatively affected by
unrelated source data. Third, most methods treat all source samples equally,
ignoring the fact that different features have varying adaptation difficulties.
To address these limitations, we propose a novel framework for generalizable
semantic segmentation: Prototypical Progressive Alignment and Reweighting
(PPAR), leveraging the strong generalization ability of the CLIP model.
Specifically, we define two prototypes: the Original Text Prototype (OTP) and
Visual Text Prototype (VTP), generated via CLIP to serve as a solid base for
alignment. We then introduce a progressive alignment strategy that aligns
features in an easy-to-difficult manner, reducing domain gaps gradually.
Furthermore, we propose a prototypical reweighting mechanism that estimates the
reliability of source data and adjusts its contribution, mitigating the effect
of irrelevant or harmful features (i.e., reducing negative transfer). We also
provide a theoretical analysis showing the alignment between our method and
domain generalization theory. Extensive experiments across multiple benchmarks
demonstrate that PPAR achieves state-of-the-art performance, validating its
effectiveness.

</details>


### [24] [Watch, Listen, Understand, Mislead: Tri-modal Adversarial Attacks on Short Videos for Content Appropriateness Evaluation](https://arxiv.org/abs/2507.11968)
*Sahid Hossain Mustakim,S M Jishanul Islam,Ummay Maria Muna,Montasir Chowdhury,Mohammed Jawwadul Islam,Sadia Ahmmed,Tashfia Sikder,Syed Tasdid Azam Dhrubo,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: 本文提出了一个评估多模态大语言模型（MLLMs）在短视频内容审核中安全性的框架，包括SVMA数据集和ChimeraBreak攻击策略，揭示了模型的显著漏洞。


<details>
  <summary>Details</summary>
Motivation: 当前的安全评估多依赖单模态攻击，未能全面评估多模态攻击的脆弱性，尤其是在短视频场景中。

Method: 提出了SVMA数据集和ChimeraBreak三模态攻击策略，同时挑战视觉、听觉和语义推理路径。

Result: 实验显示MLLMs存在显著漏洞，攻击成功率（ASR）高，并揭示了模型对良性或违规内容的误分类倾向。

Conclusion: 研究为开发更鲁棒和安全的MLLMs提供了关键见解。

Abstract: Multimodal Large Language Models (MLLMs) are increasingly used for content
moderation, yet their robustness in short-form video contexts remains
underexplored. Current safety evaluations often rely on unimodal attacks,
failing to address combined attack vulnerabilities. In this paper, we introduce
a comprehensive framework for evaluating the tri-modal safety of MLLMs. First,
we present the Short-Video Multimodal Adversarial (SVMA) dataset, comprising
diverse short-form videos with human-guided synthetic adversarial attacks.
Second, we propose ChimeraBreak, a novel tri-modal attack strategy that
simultaneously challenges visual, auditory, and semantic reasoning pathways.
Extensive experiments on state-of-the-art MLLMs reveal significant
vulnerabilities with high Attack Success Rates (ASR). Our findings uncover
distinct failure modes, showing model biases toward misclassifying benign or
policy-violating content. We assess results using LLM-as-a-judge, demonstrating
attack reasoning efficacy. Our dataset and findings provide crucial insights
for developing more robust and safe MLLMs.

</details>


### [25] [GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models](https://arxiv.org/abs/2507.11969)
*Zhaohong Huang,Yuxin Zhang,Jingjing Xie,Fei Chao,Rongrong Ji*

Main category: cs.CV

TL;DR: GS-Bias是一种高效的测试时适应（TTA）方法，通过全局和空间偏置学习提升视觉语言模型的性能，同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法在性能和效率之间难以平衡，要么计算开销大，要么效果不稳定。

Method: GS-Bias引入全局偏置和空间偏置，直接添加到预训练模型的输出中，避免全反向传播。

Result: 在15个基准数据集上达到SOTA性能，例如在跨数据集和领域泛化中分别提升2.23%和2.72%，且内存使用仅为TPT的6.5%。

Conclusion: GS-Bias在高效性和性能上均优于现有方法，为TTA提供了新思路。

Abstract: Recent advances in test-time adaptation (TTA) for Vision-Language Models
(VLMs) have garnered increasing attention, particularly through the use of
multiple augmented views of a single image to boost zero-shot generalization.
Unfortunately, existing methods fail to strike a satisfactory balance between
performance and efficiency, either due to excessive overhead of tuning text
prompts or unstable benefits from handcrafted, training-free visual feature
enhancement. In this paper, we present Global-Spatial Bias Learner (GS-Bias),
an efficient and effective TTA paradigm that incorporates two learnable biases
during TTA, unfolded as the global bias and spatial bias. Particularly, the
global bias captures the global semantic features of a test image by learning
consistency across augmented views, while spatial bias learns the semantic
coherence between regions in the image's spatial visual representation. It is
worth highlighting that these two sets of biases are directly added to the
logits outputed by the pretrained VLMs, which circumvent the full
backpropagation through VLM that hinders the efficiency of existing TTA
methods. This endows GS-Bias with extremely high efficiency while achieving
state-of-the-art performance on 15 benchmark datasets. For example, it achieves
a 2.23% improvement over TPT in cross-dataset generalization and a 2.72%
improvement in domain generalization, while requiring only 6.5% of TPT's memory
usage on ImageNet.

</details>


### [26] [EC-Diff: Fast and High-Quality Edge-Cloud Collaborative Inference for Diffusion Models](https://arxiv.org/abs/2507.11980)
*Jiajian Xie,Shengyu Zhang,Zhou Zhao,Fan Wu,Fei Wu*

Main category: cs.CV

TL;DR: EC-Diff通过梯度噪声估计和最优切换点选择，提升边缘-云协作框架的生成质量和推理速度。


<details>
  <summary>Details</summary>
Motivation: 解决云去噪时间过长和边缘模型输出不一致的问题。

Method: 采用K步噪声近似策略和两阶段贪心搜索算法。

Result: 生成质量优于边缘推理，推理速度比云推理快2倍。

Conclusion: EC-Diff在质量和速度上均表现优异。

Abstract: Diffusion Models have shown remarkable proficiency in image and video
synthesis. As model size and latency increase limit user experience, hybrid
edge-cloud collaborative framework was recently proposed to realize fast
inference and high-quality generation, where the cloud model initiates
high-quality semantic planning and the edge model expedites later-stage
refinement. However, excessive cloud denoising prolongs inference time, while
insufficient steps cause semantic ambiguity, leading to inconsistency in edge
model output. To address these challenges, we propose EC-Diff that accelerates
cloud inference through gradient-based noise estimation while identifying the
optimal point for cloud-edge handoff to maintain generation quality.
Specifically, we design a K-step noise approximation strategy to reduce cloud
inference frequency by using noise gradients between steps and applying cloud
inference periodically to adjust errors. Then we design a two-stage greedy
search algorithm to efficiently find the optimal parameters for noise
approximation and edge model switching. Extensive experiments demonstrate that
our method significantly enhances generation quality compared to edge
inference, while achieving up to an average $2\times$ speedup in inference
compared to cloud inference. Video samples and source code are available at
https://ec-diff.github.io/.

</details>


### [27] [Unsupervised Part Discovery via Descriptor-Based Masked Image Restoration with Optimized Constraints](https://arxiv.org/abs/2507.11985)
*Jiahao Xia,Yike Wu,Wenjian Huang,Jianguo Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为Masked Part Autoencoder (MPAE)的无监督部件发现方法，通过学习部件描述符和特征图，能够在复杂场景中稳健地发现与实际物体形状匹配的部件。


<details>
  <summary>Details</summary>
Motivation: 现有无监督部件发现方法因缺乏鲁棒性而应用受限，MPAE旨在克服这一限制。

Method: MPAE通过掩码图像学习部件描述符和特征图，利用局部特征与描述符的相似性填充掩码区域，从而对齐部件形状。

Result: 实验表明，MPAE能在多种类别和场景中稳健地发现有意义部件。

Conclusion: MPAE为无监督部件发现提供了有效范式，支持跨类别和场景的应用。

Abstract: Part-level features are crucial for image understanding, but few studies
focus on them because of the lack of fine-grained labels. Although unsupervised
part discovery can eliminate the reliance on labels, most of them cannot
maintain robustness across various categories and scenarios, which restricts
their application range. To overcome this limitation, we present a more
effective paradigm for unsupervised part discovery, named Masked Part
Autoencoder (MPAE). It first learns part descriptors as well as a feature map
from the inputs and produces patch features from a masked version of the
original images. Then, the masked regions are filled with the learned part
descriptors based on the similarity between the local features and descriptors.
By restoring these masked patches using the part descriptors, they become
better aligned with their part shapes, guided by appearance features from
unmasked patches. Finally, MPAE robustly discovers meaningful parts that
closely match the actual object shapes, even in complex scenarios. Moreover,
several looser yet more effective constraints are proposed to enable MPAE to
identify the presence of parts across various scenarios and categories in an
unsupervised manner. This provides the foundation for addressing challenges
posed by occlusion and for exploring part similarity across multiple
categories. Extensive experiments demonstrate that our method robustly
discovers meaningful parts across various categories and scenarios. The code is
available at the project https://github.com/Jiahao-UTS/MPAE.

</details>


### [28] [Style Composition within Distinct LoRA modules for Traditional Art](https://arxiv.org/abs/2507.11986)
*Jaehyun Lee,Wonhark Park,Wonsik Shin,Hyunho Lee,Hyoung Min Na,Nojun Kwak*

Main category: cs.CV

TL;DR: 提出了一种零样本扩散管道，通过融合多个风格专用模型的去噪潜在空间，实现区域特定的风格混合。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在风格混合时难以控制区域的问题，避免单一风格主导。

Method: 在去噪过程中融合不同风格专用模型的潜在空间，利用空间掩码实现区域控制，并结合ControlNet进行深度图条件化。

Result: 实验表明，该方法能根据掩码成功实现区域特定的风格混合。

Conclusion: 该方法在保持各风格保真度的同时，实现了用户引导的区域风格混合。

Abstract: Diffusion-based text-to-image models have achieved remarkable results in
synthesizing diverse images from text prompts and can capture specific artistic
styles via style personalization. However, their entangled latent space and
lack of smooth interpolation make it difficult to apply distinct painting
techniques in a controlled, regional manner, often causing one style to
dominate. To overcome this, we propose a zero-shot diffusion pipeline that
naturally blends multiple styles by performing style composition on the
denoised latents predicted during the flow-matching denoising process of
separately trained, style-specialized models. We leverage the fact that
lower-noise latents carry stronger stylistic information and fuse them across
heterogeneous diffusion pipelines using spatial masks, enabling precise,
region-specific style control. This mechanism preserves the fidelity of each
individual style while allowing user-guided mixing. Furthermore, to ensure
structural coherence across different models, we incorporate depth-map
conditioning via ControlNet into the diffusion framework. Qualitative and
quantitative experiments demonstrate that our method successfully achieves
region-specific style mixing according to the given masks.

</details>


### [29] [ID-EA: Identity-driven Text Enhancement and Adaptation with Textual Inversion for Personalized Text-to-Image Generation](https://arxiv.org/abs/2507.11990)
*Hyun-Jun Jin,Young-Eun Kim,Seong-Whan Lee*

Main category: cs.CV

TL;DR: ID-EA框架通过ID-Enhancer和ID-Adapter改进文本嵌入与视觉身份嵌入的对齐，显著提升个性化肖像生成中的身份一致性。


<details>
  <summary>Details</summary>
Motivation: 当前Textual Inversion方法在文本与视觉嵌入空间的身份语义对齐上存在不足，导致生成图像的身份一致性较差。

Method: ID-EA包含ID-Enhancer和ID-Adapter两个组件，前者通过文本ID锚点优化视觉身份嵌入，后者调整预训练UNet模型的交叉注意力模块以确保身份保留。

Result: ID-EA在身份保留指标上显著优于现有方法，且计算效率高，生成速度比现有方法快15倍。

Conclusion: ID-EA通过改进文本与视觉嵌入的对齐，有效解决了身份一致性问题，同时提升了计算效率。

Abstract: Recently, personalized portrait generation with a text-to-image diffusion
model has significantly advanced with Textual Inversion, emerging as a
promising approach for creating high-fidelity personalized images. Despite its
potential, current Textual Inversion methods struggle to maintain consistent
facial identity due to semantic misalignments between textual and visual
embedding spaces regarding identity. We introduce ID-EA, a novel framework that
guides text embeddings to align with visual identity embeddings, thereby
improving identity preservation in a personalized generation. ID-EA comprises
two key components: the ID-driven Enhancer (ID-Enhancer) and the ID-conditioned
Adapter (ID-Adapter). First, the ID-Enhancer integrates identity embeddings
with a textual ID anchor, refining visual identity embeddings derived from a
face recognition model using representative text embeddings. Then, the
ID-Adapter leverages the identity-enhanced embedding to adapt the text
condition, ensuring identity preservation by adjusting the cross-attention
module in the pre-trained UNet model. This process encourages the text features
to find the most related visual clues across the foreground snippets. Extensive
quantitative and qualitative evaluations demonstrate that ID-EA substantially
outperforms state-of-the-art methods in identity preservation metrics while
achieving remarkable computational efficiency, generating personalized
portraits approximately 15 times faster than existing approaches.

</details>


### [30] [SAMST: A Transformer framework based on SAM pseudo label filtering for remote sensing semi-supervised semantic segmentation](https://arxiv.org/abs/2507.11994)
*Jun Yin,Fei Wu,Yupeng Ren,Jisheng Huang,Qiankun Li,Heng jin,Jianhai Fu,Chanjie Cui*

Main category: cs.CV

TL;DR: SAMST是一种半监督语义分割方法，利用Segment Anything Model（SAM）的零样本泛化和边界检测能力，通过迭代优化伪标签提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 公共遥感数据集因分辨率和地物类别定义不一致而缺乏普适性，需要利用大量未标注数据。

Method: SAMST结合监督模型自训练和基于SAM的伪标签优化器，通过阈值过滤、提示生成和标签细化模块迭代优化伪标签。

Result: 在Potsdam数据集上的实验验证了SAMST的有效性和可行性。

Conclusion: SAMST通过结合大模型的泛化能力和小模型的训练效率，解决了遥感语义分割中标注数据不足的问题。

Abstract: Public remote sensing datasets often face limitations in universality due to
resolution variability and inconsistent land cover category definitions. To
harness the vast pool of unlabeled remote sensing data, we propose SAMST, a
semi-supervised semantic segmentation method. SAMST leverages the strengths of
the Segment Anything Model (SAM) in zero-shot generalization and boundary
detection. SAMST iteratively refines pseudo-labels through two main components:
supervised model self-training using both labeled and pseudo-labeled data, and
a SAM-based Pseudo-label Refiner. The Pseudo-label Refiner comprises three
modules: a Threshold Filter Module for preprocessing, a Prompt Generation
Module for extracting connected regions and generating prompts for SAM, and a
Label Refinement Module for final label stitching. By integrating the
generalization power of large models with the training efficiency of small
models, SAMST improves pseudo-label accuracy, thereby enhancing overall model
performance. Experiments on the Potsdam dataset validate the effectiveness and
feasibility of SAMST, demonstrating its potential to address the challenges
posed by limited labeled data in remote sensing semantic segmentation.

</details>


### [31] [AU-Blendshape for Fine-grained Stylized 3D Facial Expression Manipulation](https://arxiv.org/abs/2507.12001)
*Hao Li,Ju Dai,Feng Zhou,Kaida Ning,Lei Li,Junjun Pan*

Main category: cs.CV

TL;DR: 论文提出了AUBlendSet数据集和AUBlendNet方法，用于实现基于面部动作单元（AUs）的细粒度3D面部表情风格化操纵。


<details>
  <summary>Details</summary>
Motivation: 现有3D面部动画缺乏细粒度风格化表情操纵的合适数据集，因此需要开发新的数据集和方法。

Method: 基于32个标准AUs和500个身份的数据集AUBlendSet，提出AUBlendNet网络，学习不同风格的AU-Blendshape基向量，实现风格化表情操纵。

Result: 通过实验验证了AUBlendSet和AUBlendNet在风格化表情操纵、语音驱动动画和情感识别数据增强中的有效性。

Conclusion: AUBlendSet和AUBlendNet填补了3D面部动画领域的数据集和方法空白，具有重要潜力。

Abstract: While 3D facial animation has made impressive progress, challenges still
exist in realizing fine-grained stylized 3D facial expression manipulation due
to the lack of appropriate datasets. In this paper, we introduce the
AUBlendSet, a 3D facial dataset based on AU-Blendshape representation for
fine-grained facial expression manipulation across identities. AUBlendSet is a
blendshape data collection based on 32 standard facial action units (AUs)
across 500 identities, along with an additional set of facial postures
annotated with detailed AUs. Based on AUBlendSet, we propose AUBlendNet to
learn AU-Blendshape basis vectors for different character styles. AUBlendNet
predicts, in parallel, the AU-Blendshape basis vectors of the corresponding
style for a given identity mesh, thereby achieving stylized 3D emotional facial
manipulation. We comprehensively validate the effectiveness of AUBlendSet and
AUBlendNet through tasks such as stylized facial expression manipulation,
speech-driven emotional facial animation, and emotion recognition data
augmentation. Through a series of qualitative and quantitative experiments, we
demonstrate the potential and importance of AUBlendSet and AUBlendNet in 3D
facial animation tasks. To the best of our knowledge, AUBlendSet is the first
dataset, and AUBlendNet is the first network for continuous 3D facial
expression manipulation for any identity through facial AUs. Our source code is
available at https://github.com/wslh852/AUBlendNet.git.

</details>


### [32] [Frequency-Dynamic Attention Modulation for Dense Prediction](https://arxiv.org/abs/2507.12006)
*Linwei Chen,Lin Gu,Ying Fu*

Main category: cs.CV

TL;DR: 提出了一种名为FDAM的频率动态注意力调制策略，通过反转注意力矩阵中的低通滤波器和动态调整频率分量，解决了ViTs中的频率消失问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers（ViTs）的注意力机制导致每层作为低通滤波器，堆叠层架构引发频率消失，丢失关键细节和纹理。

Method: 提出FDAM，包含Attention Inversion（AttInv）和Frequency Dynamic Scaling（FreqScale），动态调制ViTs的频率响应。

Result: 在SegFormer、DeiT和MaskDINO等模型中表现一致提升，并在遥感检测中达到单尺度设置的SOTA结果。

Conclusion: FDAM有效避免了表示崩溃，显著提升了ViTs在多种任务中的性能。

Abstract: Vision Transformers (ViTs) have significantly advanced computer vision,
demonstrating strong performance across various tasks. However, the attention
mechanism in ViTs makes each layer function as a low-pass filter, and the
stacked-layer architecture in existing transformers suffers from frequency
vanishing. This leads to the loss of critical details and textures. We propose
a novel, circuit-theory-inspired strategy called Frequency-Dynamic Attention
Modulation (FDAM), which can be easily plugged into ViTs. FDAM directly
modulates the overall frequency response of ViTs and consists of two
techniques: Attention Inversion (AttInv) and Frequency Dynamic Scaling
(FreqScale). Since circuit theory uses low-pass filters as fundamental
elements, we introduce AttInv, a method that generates complementary high-pass
filtering by inverting the low-pass filter in the attention matrix, and
dynamically combining the two. We further design FreqScale to weight different
frequency components for fine-grained adjustments to the target response
function. Through feature similarity analysis and effective rank evaluation, we
demonstrate that our approach avoids representation collapse, leading to
consistent performance improvements across various models, including SegFormer,
DeiT, and MaskDINO. These improvements are evident in tasks such as semantic
segmentation, object detection, and instance segmentation. Additionally, we
apply our method to remote sensing detection, achieving state-of-the-art
results in single-scale settings. The code is available at
\href{https://github.com/Linwei-Chen/FDAM}{https://github.com/Linwei-Chen/FDAM}.

</details>


### [33] [Dual form Complementary Masking for Domain-Adaptive Image Segmentation](https://arxiv.org/abs/2507.12008)
*Jiawen Wang,Yinda Chen,Xiaoyu Liu,Che Liu,Dong Liu,Jianqing Gao,Zhiwei Xiong*

Main category: cs.CV

TL;DR: 论文将掩码图像建模（MIM）重新定义为稀疏信号重建问题，提出MaskTwins框架，通过互补掩码增强特征提取，实现无预训练的域自适应分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅将掩码视为图像变形，缺乏理论分析，未能充分利用掩码重建在特征提取中的潜力。

Method: 将掩码重建视为稀疏信号重建问题，提出MaskTwins框架，通过互补掩码一致性增强域无关特征提取。

Result: 实验表明MaskTwins在自然和生物图像分割中优于基线方法，无需预训练即可提取域不变特征。

Conclusion: MaskTwins为域自适应分割提供了新范式，展示了掩码重建在特征提取中的显著优势。

Abstract: Recent works have correlated Masked Image Modeling (MIM) with consistency
regularization in Unsupervised Domain Adaptation (UDA). However, they merely
treat masking as a special form of deformation on the input images and neglect
the theoretical analysis, which leads to a superficial understanding of masked
reconstruction and insufficient exploitation of its potential in enhancing
feature extraction and representation learning. In this paper, we reframe
masked reconstruction as a sparse signal reconstruction problem and
theoretically prove that the dual form of complementary masks possesses
superior capabilities in extracting domain-agnostic image features. Based on
this compelling insight, we propose MaskTwins, a simple yet effective UDA
framework that integrates masked reconstruction directly into the main training
pipeline. MaskTwins uncovers intrinsic structural patterns that persist across
disparate domains by enforcing consistency between predictions of images masked
in complementary ways, enabling domain generalization in an end-to-end manner.
Extensive experiments verify the superiority of MaskTwins over baseline methods
in natural and biological image segmentation. These results demonstrate the
significant advantages of MaskTwins in extracting domain-invariant features
without the need for separate pre-training, offering a new paradigm for
domain-adaptive segmentation.

</details>


### [34] [Deep Neural Encoder-Decoder Model to Relate fMRI Brain Activity with Naturalistic Stimuli](https://arxiv.org/abs/2507.12009)
*Florian David,Michael Chan,Elenor Morgenroth,Patrik Vuilleumier,Dimitri Van De Ville*

Main category: cs.CV

TL;DR: 提出了一种端到端的深度神经编码-解码模型，用于编码和解码自然刺激下的大脑活动，通过fMRI数据预测视觉皮层的活动并重建视觉输入。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用深度学习方法填补自然电影刺激与fMRI采集之间的时间分辨率差距，并探索视觉处理的神经机制。

Method: 采用时间卷积层的编码-解码模型，利用连续电影帧的时间相关性预测视觉皮层活动并重建视觉输入。

Result: 模型成功预测了视觉皮层及其周围区域的活动，并重建了边缘、面部和对比度等视觉特征。

Conclusion: 研究表明，深度学习模型可以作为研究视觉处理机制的工具，尤其是在自然电影刺激下的视觉解码。

Abstract: We propose an end-to-end deep neural encoder-decoder model to encode and
decode brain activity in response to naturalistic stimuli using functional
magnetic resonance imaging (fMRI) data. Leveraging temporally correlated input
from consecutive film frames, we employ temporal convolutional layers in our
architecture, which effectively allows to bridge the temporal resolution gap
between natural movie stimuli and fMRI acquisitions. Our model predicts
activity of voxels in and around the visual cortex and performs reconstruction
of corresponding visual inputs from neural activity. Finally, we investigate
brain regions contributing to visual decoding through saliency maps. We find
that the most contributing regions are the middle occipital area, the fusiform
area, and the calcarine, respectively employed in shape perception, complex
recognition (in particular face perception), and basic visual features such as
edges and contrasts. These functions being strongly solicited are in line with
the decoder's capability to reconstruct edges, faces, and contrasts. All in
all, this suggests the possibility to probe our understanding of visual
processing in films using as a proxy the behaviour of deep learning models such
as the one proposed in this paper.

</details>


### [35] [SS-DC: Spatial-Spectral Decoupling and Coupling Across Visible-Infrared Gap for Domain Adaptive Object Detection](https://arxiv.org/abs/2507.12017)
*Xiwei Zhang,Chunjin Yang,Yiming Xiao,Runtong Zhang,Fanman Meng*

Main category: cs.CV

TL;DR: 论文提出了一种基于解耦-耦合策略的SS-DC框架，用于解决可见光到红外（RGB-IR）领域的无监督域自适应目标检测问题，通过光谱分解和空间-光谱耦合方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将RGB领域视为单一领域，忽略了其内部的多个子域（如白天、夜晚、雾天场景），解耦领域不变（DI）和领域特定（DS）特征对RGB-IR域适应有益。

Method: 设计了光谱自适应幂等解耦（SAID）模块，通过光谱分解解耦DI和DS特征；提出基于滤波器组的光谱处理范式和自蒸馏驱动的解耦损失；引入空间-光谱耦合方法。

Result: 实验表明，该方法显著提升了基线性能，并在多个RGB-IR数据集上优于现有UDAOD方法，包括基于FLIR-ADAS数据集的新实验协议。

Conclusion: 通过解耦-耦合策略，SS-DC框架有效解决了RGB-IR域适应问题，为多子域场景下的目标检测提供了新思路。

Abstract: Unsupervised domain adaptive object detection (UDAOD) from the visible domain
to the infrared (RGB-IR) domain is challenging. Existing methods regard the RGB
domain as a unified domain and neglect the multiple subdomains within it, such
as daytime, nighttime, and foggy scenes. We argue that decoupling the
domain-invariant (DI) and domain-specific (DS) features across these multiple
subdomains is beneficial for RGB-IR domain adaptation. To this end, this paper
proposes a new SS-DC framework based on a decoupling-coupling strategy. In
terms of decoupling, we design a Spectral Adaptive Idempotent Decoupling (SAID)
module in the aspect of spectral decomposition. Due to the style and content
information being highly embedded in different frequency bands, this module can
decouple DI and DS components more accurately and interpretably. A novel filter
bank-based spectral processing paradigm and a self-distillation-driven
decoupling loss are proposed to improve the spectral domain decoupling. In
terms of coupling, a new spatial-spectral coupling method is proposed, which
realizes joint coupling through spatial and spectral DI feature pyramids.
Meanwhile, this paper introduces DS from decoupling to reduce the domain bias.
Extensive experiments demonstrate that our method can significantly improve the
baseline performance and outperform existing UDAOD methods on multiple RGB-IR
datasets, including a new experimental protocol proposed in this paper based on
the FLIR-ADAS dataset.

</details>


### [36] [Dataset Ownership Verification for Pre-trained Masked Models](https://arxiv.org/abs/2507.12022)
*Yuechen Xie,Jie Song,Yicheng Shan,Xiaoyan Zhang,Yuanyu Wan,Shengxuming Zhang,Jiarui Duan,Mingli Song*

Main category: cs.CV

TL;DR: 提出了一种针对掩码模型的数据集所有权验证方法DOV4MM，填补了现有技术空白，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 高质量开源数据集面临被滥用的风险，现有验证技术不适用于掩码模型，亟需解决方案。

Method: 基于掩码信息重建难度的差异，设计DOV4MM方法验证黑盒模型是否在特定数据集上预训练。

Result: 在多个掩码模型上验证，DOV4MM显著优于现有方法，p值远低于0.05。

Conclusion: DOV4MM为掩码模型的数据集所有权验证提供了有效工具，保护了数据集所有者的权益。

Abstract: High-quality open-source datasets have emerged as a pivotal catalyst driving
the swift advancement of deep learning, while facing the looming threat of
potential exploitation. Protecting these datasets is of paramount importance
for the interests of their owners. The verification of dataset ownership has
evolved into a crucial approach in this domain; however, existing verification
techniques are predominantly tailored to supervised models and contrastive
pre-trained models, rendering them ill-suited for direct application to the
increasingly prevalent masked models. In this work, we introduce the inaugural
methodology addressing this critical, yet unresolved challenge, termed Dataset
Ownership Verification for Masked Modeling (DOV4MM). The central objective is
to ascertain whether a suspicious black-box model has been pre-trained on a
particular unlabeled dataset, thereby assisting dataset owners in safeguarding
their rights. DOV4MM is grounded in our empirical observation that when a model
is pre-trained on the target dataset, the difficulty of reconstructing masked
information within the embedding space exhibits a marked contrast to models not
pre-trained on that dataset. We validated the efficacy of DOV4MM through ten
masked image models on ImageNet-1K and four masked language models on
WikiText-103. The results demonstrate that DOV4MM rejects the null hypothesis,
with a $p$-value considerably below 0.05, surpassing all prior approaches. Code
is available at https://github.com/xieyc99/DOV4MM.

</details>


### [37] [MVAR: MultiVariate AutoRegressive Air Pollutants Forecasting Model](https://arxiv.org/abs/2507.12023)
*Xu Fan,Zhihao Wang,Yuetan Lin,Yan Zhang,Yang Xiang,Hao Li*

Main category: cs.CV

TL;DR: 提出了一种多变量自回归空气污染物预测模型（MVAR），通过减少对长时间窗口输入的依赖并提高数据利用效率，实现了120小时长期序列预测。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单一污染物预测，忽略了污染物间的相互作用及其空间响应多样性，难以满足实际需求。

Method: 设计了多变量自回归训练范式，结合气象耦合空间变换模块，灵活耦合基于AI的气象预测数据。

Result: 实验结果表明，MVAR模型优于现有方法，验证了其架构的有效性。

Conclusion: MVAR模型为多变量空气污染物预测提供了一种高效且灵活的解决方案。

Abstract: Air pollutants pose a significant threat to the environment and human health,
thus forecasting accurate pollutant concentrations is essential for pollution
warnings and policy-making. Existing studies predominantly focus on
single-pollutant forecasting, neglecting the interactions among different
pollutants and their diverse spatial responses. To address the practical needs
of forecasting multivariate air pollutants, we propose MultiVariate
AutoRegressive air pollutants forecasting model (MVAR), which reduces the
dependency on long-time-window inputs and boosts the data utilization
efficiency. We also design the Multivariate Autoregressive Training Paradigm,
enabling MVAR to achieve 120-hour long-term sequential forecasting.
Additionally, MVAR develops Meteorological Coupled Spatial Transformer block,
enabling the flexible coupling of AI-based meteorological forecasts while
learning the interactions among pollutants and their diverse spatial responses.
As for the lack of standardized datasets in air pollutants forecasting, we
construct a comprehensive dataset covering 6 major pollutants across 75 cities
in North China from 2018 to 2023, including ERA5 reanalysis data and FuXi-2.0
forecast data. Experimental results demonstrate that the proposed model
outperforms state-of-the-art methods and validate the effectiveness of the
proposed architecture.

</details>


### [38] [3D-MoRe: Unified Modal-Contextual Reasoning for Embodied Question Answering](https://arxiv.org/abs/2507.12026)
*Rongtao Xu,Han Gao,Mingming Yu,Dong An,Shunpeng Chen,Changwei Wang,Li Guo,Xiaodan Liang,Shibiao Xu*

Main category: cs.CV

TL;DR: 3D-MoRe是一种新范式，利用基础模型生成大规模3D-语言数据集，显著提升室内场景任务性能。


<details>
  <summary>Details</summary>
Motivation: 满足室内场景任务（如问答和密集标注）对多样化、可扩展数据的需求。

Method: 整合多模态嵌入、跨模态交互和语言模型解码器，处理自然语言指令和3D场景数据，辅以数据增强和语义过滤。

Result: 在ScanQA和ScanRefer任务中，CIDEr分数分别提升2.15%和1.84%。

Conclusion: 3D-MoRe有效生成高质量数据集，显著提升性能，代码和数据集将公开。

Abstract: With the growing need for diverse and scalable data in indoor scene tasks,
such as question answering and dense captioning, we propose 3D-MoRe, a novel
paradigm designed to generate large-scale 3D-language datasets by leveraging
the strengths of foundational models. The framework integrates key components,
including multi-modal embedding, cross-modal interaction, and a language model
decoder, to process natural language instructions and 3D scene data. This
approach facilitates enhanced reasoning and response generation in complex 3D
environments. Using the ScanNet 3D scene dataset, along with text annotations
from ScanQA and ScanRefer, 3D-MoRe generates 62,000 question-answer (QA) pairs
and 73,000 object descriptions across 1,513 scenes. We also employ various data
augmentation techniques and implement semantic filtering to ensure high-quality
data. Experiments on ScanQA demonstrate that 3D-MoRe significantly outperforms
state-of-the-art baselines, with the CIDEr score improving by 2.15\%.
Similarly, on ScanRefer, our approach achieves a notable increase in CIDEr@0.5
by 1.84\%, highlighting its effectiveness in both tasks. Our code and generated
datasets will be publicly released to benefit the community, and both can be
accessed on the https://3D-MoRe.github.io.

</details>


### [39] [SGLoc: Semantic Localization System for Camera Pose Estimation from 3D Gaussian Splatting Representation](https://arxiv.org/abs/2507.12027)
*Beining Xu,Siting Zhu,Hesheng Wang*

Main category: cs.CV

TL;DR: SGLoc是一种新颖的定位系统，通过利用语义信息直接从3D高斯泼溅表示回归相机姿态，无需先验姿态信息。


<details>
  <summary>Details</summary>
Motivation: 解决在没有初始姿态先验的情况下，如何利用语义关系实现6DoF姿态估计的问题。

Method: 采用多级姿态回归策略和基于语义的全局检索算法，逐步估计和优化查询图像的姿态。

Result: 在12scenes和7scenes数据集上表现优于基线方法，展示了无需初始姿态先验的全局定位能力。

Conclusion: SGLoc通过语义信息和3DGS表示实现了高效的全局定位，具有优越性能。

Abstract: We propose SGLoc, a novel localization system that directly regresses camera
poses from 3D Gaussian Splatting (3DGS) representation by leveraging semantic
information. Our method utilizes the semantic relationship between 2D image and
3D scene representation to estimate the 6DoF pose without prior pose
information. In this system, we introduce a multi-level pose regression
strategy that progressively estimates and refines the pose of query image from
the global 3DGS map, without requiring initial pose priors. Moreover, we
introduce a semantic-based global retrieval algorithm that establishes
correspondences between 2D (image) and 3D (3DGS map). By matching the extracted
scene semantic descriptors of 2D query image and 3DGS semantic representation,
we align the image with the local region of the global 3DGS map, thereby
obtaining a coarse pose estimation. Subsequently, we refine the coarse pose by
iteratively optimizing the difference between the query image and the rendered
image from 3DGS. Our SGLoc demonstrates superior performance over baselines on
12scenes and 7scenes datasets, showing excellent capabilities in global
localization without initial pose prior. Code will be available at
https://github.com/IRMVLab/SGLoc.

</details>


### [40] [Intra-view and Inter-view Correlation Guided Multi-view Novel Class Discovery](https://arxiv.org/abs/2507.12029)
*Xinhang Wan,Jiyuan Liu,Qian Qu,Suyuan Liu,Chuyu Zhang,Fangdi Wang,Xinwang Liu,En Zhu,Kunlun He*

Main category: cs.CV

TL;DR: 提出了一种名为IICMVNCD的多视图新类发现框架，解决了现有方法在单视图数据和不稳定伪标签上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有新类发现方法主要针对单视图数据且依赖伪标签，导致性能不稳定，无法适应多视图数据（如多组学数据）的需求。

Method: 通过矩阵分解在视图内捕获已知类和新类的分布一致性，并在视图间利用已知类的关系指导新类聚类，动态调整视图权重。

Result: 实验验证了所提方法的有效性。

Conclusion: IICMVNCD是首个探索多视图新类发现的方法，解决了现有方法的局限性。

Abstract: In this paper, we address the problem of novel class discovery (NCD), which
aims to cluster novel classes by leveraging knowledge from disjoint known
classes. While recent advances have made significant progress in this area,
existing NCD methods face two major limitations. First, they primarily focus on
single-view data (e.g., images), overlooking the increasingly common multi-view
data, such as multi-omics datasets used in disease diagnosis. Second, their
reliance on pseudo-labels to supervise novel class clustering often results in
unstable performance, as pseudo-label quality is highly sensitive to factors
such as data noise and feature dimensionality. To address these challenges, we
propose a novel framework named Intra-view and Inter-view Correlation Guided
Multi-view Novel Class Discovery (IICMVNCD), which is the first attempt to
explore NCD in multi-view setting so far. Specifically, at the intra-view
level, leveraging the distributional similarity between known and novel
classes, we employ matrix factorization to decompose features into
view-specific shared base matrices and factor matrices. The base matrices
capture distributional consistency among the two datasets, while the factor
matrices model pairwise relationships between samples. At the inter-view level,
we utilize view relationships among known classes to guide the clustering of
novel classes. This includes generating predicted labels through the weighted
fusion of factor matrices and dynamically adjusting view weights of known
classes based on the supervision loss, which are then transferred to novel
class learning. Experimental results validate the effectiveness of our proposed
approach.

</details>


### [41] [MoViAD: Modular Visual Anomaly Detection](https://arxiv.org/abs/2507.12049)
*Manuel Barusco,Francesco Borsatti,Arianna Stropeni,Davide Dalle Pezze,Gian Antonio Susto*

Main category: cs.CV

TL;DR: MoViAD是一个模块化库，提供先进的视觉异常检测（VAD）模型、工具和数据集，支持多种场景和部署需求。


<details>
  <summary>Details</summary>
Motivation: 视觉异常检测领域面临异常数据稀缺和无监督训练的挑战，需要快速研究和部署工具。

Method: 开发了MoViAD库，集成多种VAD模型、训练器、数据集和实用工具，支持多种场景和优化部署。

Result: MoViAD提供了高效的模型、量化工具和评估指标，适用于研究和实际部署。

Conclusion: MoViAD为VAD研究和应用提供了灵活、高效的解决方案。

Abstract: VAD is a critical field in machine learning focused on identifying deviations
from normal patterns in images, often challenged by the scarcity of anomalous
data and the need for unsupervised training. To accelerate research and
deployment in this domain, we introduce MoViAD, a comprehensive and highly
modular library designed to provide fast and easy access to state-of-the-art
VAD models, trainers, datasets, and VAD utilities. MoViAD supports a wide array
of scenarios, including continual, semi-supervised, few-shots, noisy, and many
more. In addition, it addresses practical deployment challenges through
dedicated Edge and IoT settings, offering optimized models and backbones, along
with quantization and compression utilities for efficient on-device execution
and distributed inference. MoViAD integrates a selection of backbones, robust
evaluation VAD metrics (pixel-level and image-level) and useful profiling tools
for efficiency analysis. The library is designed for fast, effortless
deployment, enabling machine learning engineers to easily use it for their
specific setup with custom models, datasets, and backbones. At the same time,
it offers the flexibility and extensibility researchers need to develop and
experiment with new methods.

</details>


### [42] [InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing](https://arxiv.org/abs/2507.12060)
*Kun-Hsiang Lin,Yu-Wen Tseng,Kang-Yang Huang,Jhih-Ciang Wu,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: InstructFLIP是一种基于视觉语言模型（VLM）的新型指令调整框架，通过文本指导增强跨域泛化能力，显著减少训练冗余。


<details>
  <summary>Details</summary>
Motivation: 解决面部反欺骗（FAS）中攻击类型语义理解不足和跨域训练冗余的问题。

Method: 集成VLM增强视觉输入感知，采用元域策略学习统一模型，并解耦指令为内容和风格组件。

Result: 在FAS任务中超越现有最优模型，显著减少跨域训练冗余。

Conclusion: InstructFLIP通过文本指导和元域策略有效提升了FAS的泛化能力和效率。

Abstract: Face anti-spoofing (FAS) aims to construct a robust system that can withstand
diverse attacks. While recent efforts have concentrated mainly on cross-domain
generalization, two significant challenges persist: limited semantic
understanding of attack types and training redundancy across domains. We
address the first by integrating vision-language models (VLMs) to enhance the
perception of visual input. For the second challenge, we employ a meta-domain
strategy to learn a unified model that generalizes well across multiple
domains. Our proposed InstructFLIP is a novel instruction-tuned framework that
leverages VLMs to enhance generalization via textual guidance trained solely on
a single domain. At its core, InstructFLIP explicitly decouples instructions
into content and style components, where content-based instructions focus on
the essential semantics of spoofing, and style-based instructions consider
variations related to the environment and camera characteristics. Extensive
experiments demonstrate the effectiveness of InstructFLIP by outperforming SOTA
models in accuracy and substantially reducing training redundancy across
diverse domains in FAS. Project website is available at
https://kunkunlin1221.github.io/InstructFLIP.

</details>


### [43] [MS-DETR: Towards Effective Video Moment Retrieval and Highlight Detection by Joint Motion-Semantic Learning](https://arxiv.org/abs/2507.12062)
*Hongxu Ma,Guanshuo Wang,Fufu Yu,Qiong Jia,Shouhong Ding*

Main category: cs.CV

TL;DR: MS-DETR框架通过统一学习运动与语义特征，提升视频时刻检索与高光检测性能，解决数据稀疏性问题。


<details>
  <summary>Details</summary>
Motivation: 现有DETR框架未充分利用视频中运动与语义的复杂关系，且数据稀疏性限制了模型性能。

Method: 提出MS-DETR，编码器显式建模运动与语义的模态内相关性，解码器利用跨模态任务相关性进行定位与边界划分，并通过生成策略和对比去噪学习增强数据。

Result: 在四个基准测试中表现优于现有最优模型。

Conclusion: MS-DETR通过运动与语义的统一学习及数据增强，显著提升了视频时刻检索与高光检测的精度。

Abstract: Video Moment Retrieval (MR) and Highlight Detection (HD) aim to pinpoint
specific moments and assess clip-wise relevance based on the text query. While
DETR-based joint frameworks have made significant strides, there remains
untapped potential in harnessing the intricate relationships between temporal
motion and spatial semantics within video content. In this paper, we propose
the Motion-Semantics DETR (MS-DETR), a framework that captures rich
motion-semantics features through unified learning for MR/HD tasks. The encoder
first explicitly models disentangled intra-modal correlations within motion and
semantics dimensions, guided by the given text queries. Subsequently, the
decoder utilizes the task-wise correlation across temporal motion and spatial
semantics dimensions to enable precise query-guided localization for MR and
refined highlight boundary delineation for HD. Furthermore, we observe the
inherent sparsity dilemma within the motion and semantics dimensions of MR/HD
datasets. To address this issue, we enrich the corpus from both dimensions by
generation strategies and propose contrastive denoising learning to ensure the
above components learn robustly and effectively. Extensive experiments on four
MR/HD benchmarks demonstrate that our method outperforms existing
state-of-the-art models by a margin. Our code is available at
https://github.com/snailma0229/MS-DETR.git.

</details>


### [44] [Foresight in Motion: Reinforcing Trajectory Prediction with Reward Heuristics](https://arxiv.org/abs/2507.12083)
*Muleilan Pei,Shaoshuai Shi,Xuesong Chen,Xu Liu,Shaojie Shen*

Main category: cs.CV

TL;DR: 论文提出了一种基于规划视角的运动预测方法，通过先推理行为意图再预测轨迹的策略，结合逆向强化学习和分层解码器，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法直接预测轨迹，缺乏对行为意图的显式建模，而意图推理对自动驾驶安全性至关重要。

Method: 采用逆向强化学习（IRL）推理行为意图，生成奖励分布指导策略推演，结合分层DETR-like解码器生成轨迹。

Result: 在大规模Argoverse和nuScenes数据集上表现优异，显著提升了预测置信度和性能。

Conclusion: 通过显式建模行为意图，结合IRL和分层解码器，实现了更准确和可解释的运动预测。

Abstract: Motion forecasting for on-road traffic agents presents both a significant
challenge and a critical necessity for ensuring safety in autonomous driving
systems. In contrast to most existing data-driven approaches that directly
predict future trajectories, we rethink this task from a planning perspective,
advocating a "First Reasoning, Then Forecasting" strategy that explicitly
incorporates behavior intentions as spatial guidance for trajectory prediction.
To achieve this, we introduce an interpretable, reward-driven intention
reasoner grounded in a novel query-centric Inverse Reinforcement Learning (IRL)
scheme. Our method first encodes traffic agents and scene elements into a
unified vectorized representation, then aggregates contextual features through
a query-centric paradigm. This enables the derivation of a reward distribution,
a compact yet informative representation of the target agent's behavior within
the given scene context via IRL. Guided by this reward heuristic, we perform
policy rollouts to reason about multiple plausible intentions, providing
valuable priors for subsequent trajectory generation. Finally, we develop a
hierarchical DETR-like decoder integrated with bidirectional selective state
space models to produce accurate future trajectories along with their
associated probabilities. Extensive experiments on the large-scale Argoverse
and nuScenes motion forecasting datasets demonstrate that our approach
significantly enhances trajectory prediction confidence, achieving highly
competitive performance relative to state-of-the-art methods.

</details>


### [45] [YOLOv8-SMOT: An Efficient and Robust Framework for Real-Time Small Object Tracking via Slice-Assisted Training and Adaptive Association](https://arxiv.org/abs/2507.12087)
*Xiang Yu,Xinyao Liu,Guang Liang*

Main category: cs.CV

TL;DR: 本文提出了一种用于无人机视角下小型敏捷多目标（如鸟类）跟踪的冠军解决方案，结合了检测和关联的创新方法，包括SliceTrain训练框架和基于运动的跟踪器，取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决无人机视角下小型多目标跟踪的三大挑战：目标外观特征稀缺、相机与目标动态的复杂运动纠缠、以及密集群体行为导致的遮挡和身份模糊。

Method: 采用跟踪-检测范式，提出SliceTrain训练框架增强小目标检测，并设计基于运动的跟踪器，结合EMA机制和自适应相似度度量。

Result: 在SMOT4SB测试集上达到SO-HOTA 55.205的SOTA性能。

Conclusion: 验证了所提框架在解决复杂现实SMOT问题中的有效性和先进性。

Abstract: Tracking small, agile multi-objects (SMOT), such as birds, from an Unmanned
Aerial Vehicle (UAV) perspective is a highly challenging computer vision task.
The difficulty stems from three main sources: the extreme scarcity of target
appearance features, the complex motion entanglement caused by the combined
dynamics of the camera and the targets themselves, and the frequent occlusions
and identity ambiguity arising from dense flocking behavior. This paper details
our championship-winning solution in the MVA 2025 "Finding Birds" Small
Multi-Object Tracking Challenge (SMOT4SB), which adopts the
tracking-by-detection paradigm with targeted innovations at both the detection
and association levels. On the detection side, we propose a systematic training
enhancement framework named \textbf{SliceTrain}. This framework, through the
synergy of 'deterministic full-coverage slicing' and 'slice-level stochastic
augmentation, effectively addresses the problem of insufficient learning for
small objects in high-resolution image training. On the tracking side, we
designed a robust tracker that is completely independent of appearance
information. By integrating a \textbf{motion direction maintenance (EMA)}
mechanism and an \textbf{adaptive similarity metric} combining \textbf{bounding
box expansion and distance penalty} into the OC-SORT framework, our tracker can
stably handle irregular motion and maintain target identities. Our method
achieves state-of-the-art performance on the SMOT4SB public test set, reaching
an SO-HOTA score of \textbf{55.205}, which fully validates the effectiveness
and advancement of our framework in solving complex real-world SMOT problems.
The source code will be made available at
https://github.com/Salvatore-Love/YOLOv8-SMOT.

</details>


### [46] [BRUM: Robust 3D Vehicle Reconstruction from 360 Sparse Images](https://arxiv.org/abs/2507.12095)
*Davide Di Nucci,Matteo Tomei,Guido Borghi,Luca Ciuffreda,Roberto Vezzani,Rita Cucchiara*

Main category: cs.CV

TL;DR: 本文提出了一种改进的高斯泼溅方法，通过稀疏视图输入实现车辆3D重建，结合深度图和鲁棒的姿态估计架构，提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法如NeRF和高斯泼溅依赖密集视图输入，限制了实际应用。本文旨在解决稀疏视图输入下的车辆重建问题。

Method: 集成选择性光度损失和高置信度像素，采用DUSt3R架构改进相机姿态估计，并构建了包含合成和真实公共交通工具的新数据集。

Result: 实验结果表明，该方法在多个基准测试中达到最先进性能，即使在输入受限条件下也能实现高质量重建。

Conclusion: 该方法通过改进高斯泼溅和姿态估计，有效解决了稀疏视图输入的车辆3D重建问题，具有实际应用潜力。

Abstract: Accurate 3D reconstruction of vehicles is vital for applications such as
vehicle inspection, predictive maintenance, and urban planning. Existing
methods like Neural Radiance Fields and Gaussian Splatting have shown
impressive results but remain limited by their reliance on dense input views,
which hinders real-world applicability. This paper addresses the challenge of
reconstructing vehicles from sparse-view inputs, leveraging depth maps and a
robust pose estimation architecture to synthesize novel views and augment
training data. Specifically, we enhance Gaussian Splatting by integrating a
selective photometric loss, applied only to high-confidence pixels, and
replacing standard Structure-from-Motion pipelines with the DUSt3R architecture
to improve camera pose estimation. Furthermore, we present a novel dataset
featuring both synthetic and real-world public transportation vehicles,
enabling extensive evaluation of our approach. Experimental results demonstrate
state-of-the-art performance across multiple benchmarks, showcasing the
method's ability to achieve high-quality reconstructions even under constrained
input conditions.

</details>


### [47] [DeepShade: Enable Shade Simulation by Text-conditioned Image Generation](https://arxiv.org/abs/2507.12103)
*Longchao Da,Xiangrui Liu,Mithun Shivakoti,Thirulogasankar Pranav Kutralingam,Yezhou Yang,Hua Wei*

Main category: cs.CV

TL;DR: 论文提出了一种名为DeepShade的扩散模型，用于学习和合成随时间变化的阴影模式，并结合对比学习提升性能。通过模拟阴影数据集和模型改进，为高温天气下的路线规划提供支持。


<details>
  <summary>Details</summary>
Motivation: 热浪对公共健康构成威胁，但现有路线规划系统缺乏阴影信息。论文旨在解决卫星图像噪声和训练数据不足的问题，以改善阴影估计。

Method: 1. 构建包含多样地理区域和城市布局的阴影数据集；2. 提出DeepShade模型，结合RGB和Canny边缘层，利用对比学习捕捉阴影时间变化。

Result: 模型在生成阴影图像方面表现优异，并成功应用于亚利桑那州坦佩市的路线规划中。

Conclusion: 该研究为极端高温天气下的城市规划提供了实用参考，并展示了在环境应用中的潜力。

Abstract: Heatwaves pose a significant threat to public health, especially as global
warming intensifies. However, current routing systems (e.g., online maps) fail
to incorporate shade information due to the difficulty of estimating shades
directly from noisy satellite imagery and the limited availability of training
data for generative models. In this paper, we address these challenges through
two main contributions. First, we build an extensive dataset covering diverse
longitude-latitude regions, varying levels of building density, and different
urban layouts. Leveraging Blender-based 3D simulations alongside building
outlines, we capture building shadows under various solar zenith angles
throughout the year and at different times of day. These simulated shadows are
aligned with satellite images, providing a rich resource for learning shade
patterns. Second, we propose the DeepShade, a diffusion-based model designed to
learn and synthesize shade variations over time. It emphasizes the nuance of
edge features by jointly considering RGB with the Canny edge layer, and
incorporates contrastive learning to capture the temporal change rules of
shade. Then, by conditioning on textual descriptions of known conditions (e.g.,
time of day, solar angles), our framework provides improved performance in
generating shade images. We demonstrate the utility of our approach by using
our shade predictions to calculate shade ratios for real-world route planning
in Tempe, Arizona. We believe this work will benefit society by providing a
reference for urban planning in extreme heat weather and its potential
practical applications in the environment.

</details>


### [48] [Out-of-distribution data supervision towards biomedical semantic segmentation](https://arxiv.org/abs/2507.12105)
*Yiquan Gao,Duohui Xu*

Main category: cs.CV

TL;DR: Med-OoD利用OoD数据监督解决医学图像分割中的误分类问题，无需额外数据或架构修改，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割网络在有限和不完美数据集上容易误分类，OoD数据在其他视觉任务中表现优异，启发其引入分割任务。

Method: 提出Med-OoD框架，通过OoD数据监督增强分割网络，无需外部数据、特征正则化目标或额外标注。

Result: 在Lizard数据集上显著减少误分类，性能大幅提升；仅用OoD数据训练的网络达到76.1% mIoU。

Conclusion: Med-OoD展示了OoD数据在医学分割中的潜力，为学习范式提供了新思路。

Abstract: Biomedical segmentation networks easily suffer from the unexpected
misclassification between foreground and background objects when learning on
limited and imperfect medical datasets. Inspired by the strong power of
Out-of-Distribution (OoD) data on other visual tasks, we propose a data-centric
framework, Med-OoD to address this issue by introducing OoD data supervision
into fully-supervised biomedical segmentation with none of the following needs:
(i) external data sources, (ii) feature regularization objectives, (iii)
additional annotations. Our method can be seamlessly integrated into
segmentation networks without any modification on the architectures. Extensive
experiments show that Med-OoD largely prevents various segmentation networks
from the pixel misclassification on medical images and achieves considerable
performance improvements on Lizard dataset. We also present an emerging
learning paradigm of training a medical segmentation network completely using
OoD data devoid of foreground class labels, surprisingly turning out 76.1% mIoU
as test result. We hope this learning paradigm will attract people to rethink
the roles of OoD data. Code is made available at
https://github.com/StudioYG/Med-OoD.

</details>


### [49] [Non-Adaptive Adversarial Face Generation](https://arxiv.org/abs/2507.12107)
*Sunpill Kim,Seunghun Paik,Chanwoo Hwang,Minsu Kim,Jae Hong Seo*

Main category: cs.CV

TL;DR: 提出一种基于FRS特征空间结构的新方法，生成对抗性人脸图像，无需迭代优化或多次查询，成功率高且可定制属性。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击对FRS的安全和隐私构成威胁，现有方法依赖迭代优化或迁移性，效率低且受限。

Method: 利用FRS特征空间中相同属性个体形成的子空间，生成对抗性人脸，仅需单次非自适应查询。

Result: 在AWS CompareFaces API上，仅用100张图像的单次查询，成功率超过93%。

Conclusion: 该方法高效、非自适应且可定制属性，为对抗攻击提供了新思路。

Abstract: Adversarial attacks on face recognition systems (FRSs) pose serious security
and privacy threats, especially when these systems are used for identity
verification. In this paper, we propose a novel method for generating
adversarial faces-synthetic facial images that are visually distinct yet
recognized as a target identity by the FRS. Unlike iterative optimization-based
approaches (e.g., gradient descent or other iterative solvers), our method
leverages the structural characteristics of the FRS feature space. We figure
out that individuals sharing the same attribute (e.g., gender or race) form an
attributed subsphere. By utilizing such subspheres, our method achieves both
non-adaptiveness and a remarkably small number of queries. This eliminates the
need for relying on transferability and open-source surrogate models, which
have been a typical strategy when repeated adaptive queries to commercial FRSs
are impossible. Despite requiring only a single non-adaptive query consisting
of 100 face images, our method achieves a high success rate of over 93% against
AWS's CompareFaces API at its default threshold. Furthermore, unlike many
existing attacks that perturb a given image, our method can deliberately
produce adversarial faces that impersonate the target identity while exhibiting
high-level attributes chosen by the adversary.

</details>


### [50] [LidarPainter: One-Step Away From Any Lidar View To Novel Guidance](https://arxiv.org/abs/2507.12114)
*Yuzhou Ji,Ke Ma,Hong Cai,Anchun Zhang,Lizhuang Ma,Xin Tan*

Main category: cs.CV

TL;DR: LidarPainter是一种一步扩散模型，用于从稀疏LiDAR数据和损坏的渲染中实时恢复一致的驾驶视图，提升驾驶场景重建质量。


<details>
  <summary>Details</summary>
Motivation: 动态驾驶场景重建在数字孪生系统和自动驾驶模拟中非常重要，但现有方法在偏离输入轨迹时会出现质量下降问题。

Method: 提出LidarPainter，一种一步扩散模型，从稀疏LiDAR条件和损坏的渲染中实时恢复一致视图。

Result: 实验表明，LidarPainter在速度、质量和资源效率上优于现有方法，速度快7倍且仅需五分之一的GPU内存。

Conclusion: LidarPainter不仅提升了重建质量，还支持通过文本提示（如“雾天”或“夜晚”）进行风格化生成，扩展了现有资源库。

Abstract: Dynamic driving scene reconstruction is of great importance in fields like
digital twin system and autonomous driving simulation. However, unacceptable
degradation occurs when the view deviates from the input trajectory, leading to
corrupted background and vehicle models. To improve reconstruction quality on
novel trajectory, existing methods are subject to various limitations including
inconsistency, deformation, and time consumption. This paper proposes
LidarPainter, a one-step diffusion model that recovers consistent driving views
from sparse LiDAR condition and artifact-corrupted renderings in real-time,
enabling high-fidelity lane shifts in driving scene reconstruction. Extensive
experiments show that LidarPainter outperforms state-of-the-art methods in
speed, quality and resource efficiency, specifically 7 x faster than
StreetCrafter with only one fifth of GPU memory required. LidarPainter also
supports stylized generation using text prompts such as "foggy" and "night",
allowing for a diverse expansion of the existing asset library.

</details>


### [51] [Open-Vocabulary Indoor Object Grounding with 3D Hierarchical Scene Graph](https://arxiv.org/abs/2507.12123)
*Sergey Linok,Gleb Naumov*

Main category: cs.CV

TL;DR: OVIGo-3DHSG是一种基于3D分层场景图的开放词汇室内物体定位方法，利用RGB-D序列和开放词汇基础模型，结合大型语言模型进行多步推理，提升空间上下文理解。


<details>
  <summary>Details</summary>
Motivation: 解决复杂查询中对其他物体空间参考的需求，提升室内环境的空间推理能力。

Method: 通过分层场景图建模空间关系，结合大型语言模型进行多步推理，利用RGB-D序列和开放词汇基础模型。

Result: 在Habitat Matterport 3D多楼层场景中表现出高效的场景理解和鲁棒的物体定位能力。

Conclusion: OVIGo-3DHSG在需要空间推理和室内环境理解的应用中展现出强大潜力。

Abstract: We propose OVIGo-3DHSG method - Open-Vocabulary Indoor Grounding of objects
using 3D Hierarchical Scene Graph. OVIGo-3DHSG represents an extensive indoor
environment over a Hierarchical Scene Graph derived from sequences of RGB-D
frames utilizing a set of open-vocabulary foundation models and sensor data
processing. The hierarchical representation explicitly models spatial relations
across floors, rooms, locations, and objects. To effectively address complex
queries involving spatial reference to other objects, we integrate the
hierarchical scene graph with a Large Language Model for multistep reasoning.
This integration leverages inter-layer (e.g., room-to-object) and intra-layer
(e.g., object-to-object) connections, enhancing spatial contextual
understanding. We investigate the semantic and geometry accuracy of
hierarchical representation on Habitat Matterport 3D Semantic multi-floor
scenes. Our approach demonstrates efficient scene comprehension and robust
object grounding compared to existing methods. Overall OVIGo-3DHSG demonstrates
strong potential for applications requiring spatial reasoning and understanding
of indoor environments. Related materials can be found at
https://github.com/linukc/OVIGo-3DHSG.

</details>


### [52] [Block-based Symmetric Pruning and Fusion for Efficient Vision Transformers](https://arxiv.org/abs/2507.12125)
*Yi-Kuan Hsieh,Jun-Wei Hsieh,Xin Li,Yu-Ming Chang,Yu-Chee Tseng*

Main category: cs.CV

TL;DR: BSPF-ViT通过联合修剪Q/K令牌并考虑令牌交互，提高了ViT的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有ViT修剪方法独立处理Q/K令牌，忽略了令牌交互，导致性能下降。

Method: 提出Block-based Symmetric Pruning and Fusion (BSPF-ViT)，联合优化Q/K令牌修剪，并通过相似性融合保留关键信息。

Result: 在DeiT-T和DeiT-S上分别提升1.3%和2.0%的ImageNet分类准确率，计算开销减少50%，速度提升40%。

Conclusion: BSPF-ViT在保持精度的同时显著提升了ViT的计算效率。

Abstract: Vision Transformer (ViT) has achieved impressive results across various
vision tasks, yet its high computational cost limits practical applications.
Recent methods have aimed to reduce ViT's $O(n^2)$ complexity by pruning
unimportant tokens. However, these techniques often sacrifice accuracy by
independently pruning query (Q) and key (K) tokens, leading to performance
degradation due to overlooked token interactions. To address this limitation,
we introduce a novel {\bf Block-based Symmetric Pruning and Fusion} for
efficient ViT (BSPF-ViT) that optimizes the pruning of Q/K tokens jointly.
Unlike previous methods that consider only a single direction, our approach
evaluates each token and its neighbors to decide which tokens to retain by
taking token interaction into account. The retained tokens are compressed
through a similarity fusion step, preserving key information while reducing
computational costs. The shared weights of Q/K tokens create a symmetric
attention matrix, allowing pruning only the upper triangular part for speed up.
BSPF-ViT consistently outperforms state-of-the-art ViT methods at all pruning
levels, increasing ImageNet classification accuracy by 1.3% on DeiT-T and 2.0%
on DeiT-S, while reducing computational overhead by 50%. It achieves 40%
speedup with improved accuracy across various ViTs.

</details>


### [53] [Learning Pixel-adaptive Multi-layer Perceptrons for Real-time Image Enhancement](https://arxiv.org/abs/2507.12135)
*Junyu Lou,Xiaorui Zhao,Kexuan Shi,Shuhang Gu*

Main category: cs.CV

TL;DR: 提出了一种结合双边网格和MLP的BPAM框架，用于图像增强，解决了现有方法在非线性映射和局部变化处理上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有双边网格方法仅支持线性变换，难以建模复杂颜色关系；传统MLP方法参数全局共享，无法处理局部变化。

Method: 通过生成包含MLP参数的双边网格，动态为每个像素分配独特的MLP参数，并提出网格分解策略和多通道引导图优化参数生成。

Result: 在公开数据集上表现优于现有方法，同时保持实时处理能力。

Conclusion: BPAM框架有效结合了双边网格的空间建模和MLP的非线性能力，提升了图像增强效果。

Abstract: Deep learning-based bilateral grid processing has emerged as a promising
solution for image enhancement, inherently encoding spatial and intensity
information while enabling efficient full-resolution processing through slicing
operations. However, existing approaches are limited to linear affine
transformations, hindering their ability to model complex color relationships.
Meanwhile, while multi-layer perceptrons (MLPs) excel at non-linear mappings,
traditional MLP-based methods employ globally shared parameters, which is hard
to deal with localized variations. To overcome these dual challenges, we
propose a Bilateral Grid-based Pixel-Adaptive Multi-layer Perceptron (BPAM)
framework. Our approach synergizes the spatial modeling of bilateral grids with
the non-linear capabilities of MLPs. Specifically, we generate bilateral grids
containing MLP parameters, where each pixel dynamically retrieves its unique
transformation parameters and obtain a distinct MLP for color mapping based on
spatial coordinates and intensity values. In addition, we propose a novel grid
decomposition strategy that categorizes MLP parameters into distinct types
stored in separate subgrids. Multi-channel guidance maps are used to extract
category-specific parameters from corresponding subgrids, ensuring effective
utilization of color information during slicing while guiding precise parameter
generation. Extensive experiments on public datasets demonstrate that our
method outperforms state-of-the-art methods in performance while maintaining
real-time processing capabilities.

</details>


### [54] [AD-GS: Object-Aware B-Spline Gaussian Splatting for Self-Supervised Autonomous Driving](https://arxiv.org/abs/2507.12137)
*Jiawei Xu,Kai Deng,Zexin Fan,Shenlong Wang,Jin Xie,Jian Yang*

Main category: cs.CV

TL;DR: AD-GS是一种自监督框架，用于从单一日志中高质量渲染驾驶场景的自由视角，无需昂贵的手动标注。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖昂贵的手动标注或自监督方法无法准确捕捉动态物体运动，导致渲染效果不佳。

Method: 结合局部感知B样条曲线和全局感知三角函数的学习运动模型，动态高斯表示物体，双向时间可见性掩码和物理刚性正则化。

Result: 在无标注方法中显著优于现有技术，与依赖标注的方法竞争。

Conclusion: AD-GS提供了一种高效且高质量的无标注动态场景渲染解决方案。

Abstract: Modeling and rendering dynamic urban driving scenes is crucial for
self-driving simulation. Current high-quality methods typically rely on costly
manual object tracklet annotations, while self-supervised approaches fail to
capture dynamic object motions accurately and decompose scenes properly,
resulting in rendering artifacts. We introduce AD-GS, a novel self-supervised
framework for high-quality free-viewpoint rendering of driving scenes from a
single log. At its core is a novel learnable motion model that integrates
locality-aware B-spline curves with global-aware trigonometric functions,
enabling flexible yet precise dynamic object modeling. Rather than requiring
comprehensive semantic labeling, AD-GS automatically segments scenes into
objects and background with the simplified pseudo 2D segmentation, representing
objects using dynamic Gaussians and bidirectional temporal visibility masks.
Further, our model incorporates visibility reasoning and physically rigid
regularization to enhance robustness. Extensive evaluations demonstrate that
our annotation-free model significantly outperforms current state-of-the-art
annotation-free methods and is competitive with annotation-dependent
approaches.

</details>


### [55] [Neural Human Pose Prior](https://arxiv.org/abs/2507.12138)
*Michal Heker,Sefy Kararlitsky,David Tolpin*

Main category: cs.CV

TL;DR: 提出了一种基于归一化流的数据驱动方法，用于建模人体姿态的神经先验，通过RealNVP学习6D旋转格式的灵活密度分布。


<details>
  <summary>Details</summary>
Motivation: 解决现有启发式或低表达能力方法在建模6D旋转流形分布时的不足，为姿态先验提供概率基础。

Method: 利用RealNVP学习6D旋转的密度分布，通过反转Gram-Schmidt过程实现稳定训练，保持与旋转框架的兼容性。

Result: 通过定性和定量评估验证了方法的有效性，并通过消融实验分析了其影响。

Conclusion: 为人体运动捕捉和重建中的姿态先验集成提供了可靠的统计基础。

Abstract: We introduce a principled, data-driven approach for modeling a neural prior
over human body poses using normalizing flows. Unlike heuristic or
low-expressivity alternatives, our method leverages RealNVP to learn a flexible
density over poses represented in the 6D rotation format. We address the
challenge of modeling distributions on the manifold of valid 6D rotations by
inverting the Gram-Schmidt process during training, enabling stable learning
while preserving downstream compatibility with rotation-based frameworks. Our
architecture and training pipeline are framework-agnostic and easily
reproducible. We demonstrate the effectiveness of the learned prior through
both qualitative and quantitative evaluations, and we analyze its impact via
ablation studies. This work provides a sound probabilistic foundation for
integrating pose priors into human motion capture and reconstruction pipelines.

</details>


### [56] [Fine-Grained Image Recognition from Scratch with Teacher-Guided Data Augmentation](https://arxiv.org/abs/2507.12157)
*Edwin Arkel Rios,Fernando Mikael,Oswin Gosal,Femiloye Oyerinde,Hao-Chun Liang,Bo-Cheng Lai,Min-Chun Hu*

Main category: cs.CV

TL;DR: 论文提出了一种无需预训练的高性能细粒度图像识别（FGIR）框架TGDA，通过数据增强和知识蒸馏实现，显著提升了低分辨率和高分辨率输入的识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有FGIR方法依赖预训练模型，限制了在资源受限环境中的适应性和任务特定架构的开发。

Method: 引入TGDA框架，结合数据感知增强和细粒度感知教师模型的弱监督，通过知识蒸馏训练任务特定架构（如LRNets和ViTFS）。

Result: 在多个FGIR基准测试中，TGDA框架性能优于或匹配现有预训练方法，尤其在低分辨率输入下提升23%准确率，同时减少参数和计算量。

Conclusion: TGDA为细粒度视觉系统提供了一种高效且适应性强的替代方案，减少了对预训练的依赖。

Abstract: Fine-grained image recognition (FGIR) aims to distinguish visually similar
sub-categories within a broader class, such as identifying bird species. While
most existing FGIR methods rely on backbones pretrained on large-scale datasets
like ImageNet, this dependence limits adaptability to resource-constrained
environments and hinders the development of task-specific architectures
tailored to the unique challenges of FGIR.
  In this work, we challenge the conventional reliance on pretrained models by
demonstrating that high-performance FGIR systems can be trained entirely from
scratch. We introduce a novel training framework, TGDA, that integrates
data-aware augmentation with weak supervision via a fine-grained-aware teacher
model, implemented through knowledge distillation. This framework unlocks the
design of task-specific and hardware-aware architectures, including LRNets for
low-resolution FGIR and ViTFS, a family of Vision Transformers optimized for
efficient inference.
  Extensive experiments across three FGIR benchmarks over diverse settings
involving low-resolution and high-resolution inputs show that our method
consistently matches or surpasses state-of-the-art pretrained counterparts. In
particular, in the low-resolution setting, LRNets trained with TGDA improve
accuracy by up to 23\% over prior methods while requiring up to 20.6x less
parameters, lower FLOPs, and significantly less training data. Similarly,
ViTFS-T can match the performance of a ViT B-16 pretrained on ImageNet-21k
while using 15.3x fewer trainable parameters and requiring orders of magnitudes
less data. These results highlight TGDA's potential as an adaptable alternative
to pretraining, paving the way for more efficient fine-grained vision systems.

</details>


### [57] [Hybrid Ensemble Approaches: Optimal Deep Feature Fusion and Hyperparameter-Tuned Classifier Ensembling for Enhanced Brain Tumor Classification](https://arxiv.org/abs/2507.12177)
*Zahid Ullah,Dragan Pamucar,Jihie Kim*

Main category: cs.CV

TL;DR: 该研究提出了一种双集成框架，结合预训练深度学习模型和超参数优化的机器学习模型，以提高脑肿瘤MRI诊断的准确性。


<details>
  <summary>Details</summary>
Motivation: MRI诊断中人为因素可能导致误诊，如疲劳或经验不足，因此需要自动化方法提升精度。

Method: 采用预训练的深度卷积神经网络和视觉变换器网络提取特征，并结合超参数优化的机器学习分类器进行双集成。

Result: 实验表明，特征融合和分类器融合显著提升了分类性能，超参数优化进一步增强了效果。

Conclusion: 提出的双集成框架在脑肿瘤分类中优于现有方法，各组件均对精度提升有贡献。

Abstract: Magnetic Resonance Imaging (MRI) is widely recognized as the most reliable
tool for detecting tumors due to its capability to produce detailed images that
reveal their presence. However, the accuracy of diagnosis can be compromised
when human specialists evaluate these images. Factors such as fatigue, limited
expertise, and insufficient image detail can lead to errors. For example, small
tumors might go unnoticed, or overlap with healthy brain regions could result
in misidentification. To address these challenges and enhance diagnostic
precision, this study proposes a novel double ensembling framework, consisting
of ensembled pre-trained deep learning (DL) models for feature extraction and
ensembled fine-tuned hyperparameter machine learning (ML) models to efficiently
classify brain tumors. Specifically, our method includes extensive
preprocessing and augmentation, transfer learning concepts by utilizing various
pre-trained deep convolutional neural networks and vision transformer networks
to extract deep features from brain MRI, and fine-tune hyperparameters of ML
classifiers. Our experiments utilized three different publicly available Kaggle
MRI brain tumor datasets to evaluate the pre-trained DL feature extractor
models, ML classifiers, and the effectiveness of an ensemble of deep features
along with an ensemble of ML classifiers for brain tumor classification. Our
results indicate that the proposed feature fusion and classifier fusion improve
upon the state of the art, with hyperparameter fine-tuning providing a
significant enhancement over the ensemble method. Additionally, we present an
ablation study to illustrate how each component contributes to accurate brain
tumor classification.

</details>


### [58] [Wavelet-based Decoupling Framework for low-light Stereo Image Enhancement](https://arxiv.org/abs/2507.12188)
*Shuangli Du,Siming Yan,Zhenghao Shi,Zhenzhen You,Lu Sun*

Main category: cs.CV

TL;DR: 本文提出了一种基于小波变换的低光立体图像增强方法，通过特征空间解耦解决现有方法中特征高度纠缠的问题。


<details>
  <summary>Details</summary>
Motivation: 低光图像存在复杂的退化问题，现有方法将所有退化因素编码在单一潜在空间中，导致特征高度纠缠和黑盒特性，容易引发捷径学习。

Method: 利用小波变换将特征空间分解为低频分支（用于光照调整）和多个高频分支（用于纹理增强），并设计了高频引导的跨视图交互模块（HF-CIM）和基于交叉注意力的细节纹理增强模块（DTEM）。

Result: 实验结果表明，该方法在光照调整和高频信息恢复方面具有显著优势。

Conclusion: 通过小波变换和特征空间解耦，该方法有效解决了低光图像增强中的特征纠缠问题，提升了图像质量。

Abstract: Low-light images suffer from complex degradation, and existing enhancement
methods often encode all degradation factors within a single latent space. This
leads to highly entangled features and strong black-box characteristics, making
the model prone to shortcut learning. To mitigate the above issues, this paper
proposes a wavelet-based low-light stereo image enhancement method with feature
space decoupling. Our insight comes from the following findings: (1) Wavelet
transform enables the independent processing of low-frequency and
high-frequency information. (2) Illumination adjustment can be achieved by
adjusting the low-frequency component of a low-light image, extracted through
multi-level wavelet decomposition. Thus, by using wavelet transform the feature
space is decomposed into a low-frequency branch for illumination adjustment and
multiple high-frequency branches for texture enhancement. Additionally, stereo
low-light image enhancement can extract useful cues from another view to
improve enhancement. To this end, we propose a novel high-frequency guided
cross-view interaction module (HF-CIM) that operates within high-frequency
branches rather than across the entire feature space, effectively extracting
valuable image details from the other view. Furthermore, to enhance the
high-frequency information, a detail and texture enhancement module (DTEM) is
proposed based on cross-attention mechanism. The model is trained on a dataset
consisting of images with uniform illumination and images with non-uniform
illumination. Experimental results on both real and synthetic images indicate
that our algorithm offers significant advantages in light adjustment while
effectively recovering high-frequency information. The code and dataset are
publicly available at: https://github.com/Cherisherr/WDCI-Net.git.

</details>


### [59] [Revealing the Ancient Beauty: Digital Reconstruction of Temple Tiles using Computer Vision](https://arxiv.org/abs/2507.12195)
*Arkaprabha Basu*

Main category: cs.CV

TL;DR: 论文提出三种创新方法（Fractal Convolution、SSTF和Super Resolution）用于印度文化遗产的保护与修复，结合机器学习和计算机视觉技术，实现高效且低成本的自动化处理。


<details>
  <summary>Details</summary>
Motivation: 现代数字化方法为文化遗产保护带来革命性变化，但印度古迹的特殊性需要针对性解决方案。

Method: 1. Fractal Convolution用于图像分割；2. SSTF结合MosaicSlice数据增强修复West Bengal寺庙；3. Super Resolution提升图像质量。

Result: 方法实现了高细节修复和图像增强，同时保持低成本与真实性。

Conclusion: 研究为文化遗产保护提供了高效且创新的解决方案，平衡传统与技术进步。

Abstract: Modern digitised approaches have dramatically changed the preservation and
restoration of cultural treasures, integrating computer scientists into
multidisciplinary projects with ease. Machine learning, deep learning, and
computer vision techniques have revolutionised developing sectors like 3D
reconstruction, picture inpainting,IoT-based methods, genetic algorithms, and
image processing with the integration of computer scientists into
multidisciplinary initiatives. We suggest three cutting-edge techniques in
recognition of the special qualities of Indian monuments, which are famous for
their architectural skill and aesthetic appeal. First is the Fractal
Convolution methodology, a segmentation method based on image processing that
successfully reveals subtle architectural patterns within these irreplaceable
cultural buildings. The second is a revolutionary Self-Sensitive Tile Filling
(SSTF) method created especially for West Bengal's mesmerising Bankura
Terracotta Temples with a brand-new data augmentation method called MosaicSlice
on the third. Furthermore, we delve deeper into the Super Resolution strategy
to upscale the images without losing significant amount of quality. Our methods
allow for the development of seamless region-filling and highly detailed tiles
while maintaining authenticity using a novel data augmentation strategy within
affordable costs introducing automation. By providing effective solutions that
preserve the delicate balance between tradition and innovation, this study
improves the subject and eventually ensures unrivalled efficiency and aesthetic
excellence in cultural heritage protection. The suggested approaches advance
the field into an era of unmatched efficiency and aesthetic quality while
carefully upholding the delicate equilibrium between tradition and innovation.

</details>


### [60] [RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and Reducing Hallucination in Generative Models](https://arxiv.org/abs/2507.12201)
*Yiqi Tian,Pengfei Jin,Mingze Yuan,Na Li,Bo Zeng,Quanzheng Li*

Main category: cs.CV

TL;DR: RODS是一种基于优化的扩散采样方法，通过几何线索检测和修正高风险采样步骤，减少幻觉现象，提升生成模型的鲁棒性和保真度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的采样过程容易因分数近似不准确而产生幻觉，影响生成质量。

Method: 通过优化视角重新解释扩散采样，引入RODS方法，利用损失景观的几何线索检测和修正高风险步骤。

Result: 在AFHQv2、FFHQ和11k-hands数据集上，RODS能检测70%以上的幻觉样本并修正超过25%，且不引入新伪影。

Conclusion: RODS在不增加额外训练和推理成本的情况下，显著提升了扩散模型的采样鲁棒性和生成质量。

Abstract: Diffusion models have achieved state-of-the-art performance in generative
modeling, yet their sampling procedures remain vulnerable to hallucinations,
often stemming from inaccuracies in score approximation. In this work, we
reinterpret diffusion sampling through the lens of optimization and introduce
RODS (Robust Optimization-inspired Diffusion Sampler), a novel method that
detects and corrects high-risk sampling steps using geometric cues from the
loss landscape. RODS enforces smoother sampling trajectories and adaptively
adjusts perturbations, reducing hallucinations without retraining and at
minimal additional inference cost. Experiments on AFHQv2, FFHQ, and 11k-hands
demonstrate that RODS improves both sampling fidelity and robustness, detecting
over 70% of hallucinated samples and correcting more than 25%, all while
avoiding the introduction of new artifacts.

</details>


### [61] [MGFFD-VLM: Multi-Granularity Prompt Learning for Face Forgery Detection with VLM](https://arxiv.org/abs/2507.12232)
*Tao Chen,Jingyi Zhang,Decheng Liu,Chunlei Peng*

Main category: cs.CV

TL;DR: 论文提出了一种新的伪造检测框架MGFFD-VLM，通过扩展数据集和引入多粒度提示学习等方法，提升了视觉大语言模型在伪造检测中的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用人脸质量相关属性和训练策略上存在不足，限制了伪造检测的准确性和解释性。

Method: 扩展了DD-VQA+数据集，引入Attribute-Driven Hybrid LoRA策略、多粒度提示学习和伪造感知训练策略，并设计了辅助损失函数。

Result: 实验表明，该方法在文本伪造判断和分析上优于现有方法，准确性更高。

Conclusion: MGFFD-VLM框架通过多策略结合，显著提升了伪造检测的性能和可解释性。

Abstract: Recent studies have utilized visual large language models (VLMs) to answer
not only "Is this face a forgery?" but also "Why is the face a forgery?" These
studies introduced forgery-related attributes, such as forgery location and
type, to construct deepfake VQA datasets and train VLMs, achieving high
accuracy while providing human-understandable explanatory text descriptions.
However, these methods still have limitations. For example, they do not fully
leverage face quality-related attributes, which are often abnormal in forged
faces, and they lack effective training strategies for forgery-aware VLMs. In
this paper, we extend the VQA dataset to create DD-VQA+, which features a
richer set of attributes and a more diverse range of samples. Furthermore, we
introduce a novel forgery detection framework, MGFFD-VLM, which integrates an
Attribute-Driven Hybrid LoRA Strategy to enhance the capabilities of Visual
Large Language Models (VLMs). Additionally, our framework incorporates
Multi-Granularity Prompt Learning and a Forgery-Aware Training Strategy. By
transforming classification and forgery segmentation results into prompts, our
method not only improves forgery classification but also enhances
interpretability. To further boost detection performance, we design multiple
forgery-related auxiliary losses. Experimental results demonstrate that our
approach surpasses existing methods in both text-based forgery judgment and
analysis, achieving superior accuracy.

</details>


### [62] [Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models](https://arxiv.org/abs/2507.12236)
*Felix Nützel,Mischa Dombrowski,Bernhard Kainz*

Main category: cs.CV

TL;DR: 生成式文本到图像扩散模型在医学影像中的短语定位任务中表现优于现有判别式方法，通过跨注意力图和领域特定语言模型微调，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 探索生成式模型在医学影像中短语定位任务的潜力，以提升疾病定位的准确性和可解释性。

Method: 使用文本到图像扩散模型，结合跨注意力图和领域特定语言模型（如CXR-BERT）微调，并引入Bimodal Bias Merging（BBM）后处理技术优化定位结果。

Result: 生成式模型在零样本短语定位任务中表现卓越，mIoU分数是现有判别式方法的两倍，BBM进一步提高了定位精度。

Conclusion: 生成式模型为医学影像中的短语定位提供了更有效的范式，具有临床应用潜力。

Abstract: Phrase grounding, i.e., mapping natural language phrases to specific image
regions, holds significant potential for disease localization in medical
imaging through clinical reports. While current state-of-the-art methods rely
on discriminative, self-supervised contrastive models, we demonstrate that
generative text-to-image diffusion models, leveraging cross-attention maps, can
achieve superior zero-shot phrase grounding performance. Contrary to prior
assumptions, we show that fine-tuning diffusion models with a frozen,
domain-specific language model, such as CXR-BERT, substantially outperforms
domain-agnostic counterparts. This setup achieves remarkable improvements, with
mIoU scores doubling those of current discriminative methods. These findings
highlight the underexplored potential of generative models for phrase grounding
tasks. To further enhance performance, we introduce Bimodal Bias Merging (BBM),
a novel post-processing technique that aligns text and image biases to identify
regions of high certainty. BBM refines cross-attention maps, achieving even
greater localization accuracy. Our results establish generative approaches as a
more effective paradigm for phrase grounding in the medical imaging domain,
paving the way for more robust and interpretable applications in clinical
practice. The source code and model weights are available at
https://github.com/Felix-012/generate_to_ground.

</details>


### [63] [Calisthenics Skills Temporal Video Segmentation](https://arxiv.org/abs/2507.12245)
*Antonio Finocchiaro,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: 该论文提出了一种自动化工具，用于从视频中识别和分割静态健美操技能，以辅助训练和比赛评分。


<details>
  <summary>Details</summary>
Motivation: 健美操技能的评估依赖于难度和持续时间，但目前缺乏自动化工具来识别和分割视频中的静态技能。

Method: 研究提出了一个视频数据集，标注了静态技能的时间分割，并测试了基线方法来解决技能分割问题。

Result: 结果显示该问题是可行的，但仍需改进。

Conclusion: 该研究为健美操领域的自动化工具开发迈出了第一步。

Abstract: Calisthenics is a fast-growing bodyweight discipline that consists of
different categories, one of which is focused on skills. Skills in calisthenics
encompass both static and dynamic elements performed by athletes. The
evaluation of static skills is based on their difficulty level and the duration
of the hold. Automated tools able to recognize isometric skills from a video by
segmenting them to estimate their duration would be desirable to assist
athletes in their training and judges during competitions. Although the video
understanding literature on action recognition through body pose analysis is
rich, no previous work has specifically addressed the problem of calisthenics
skill temporal video segmentation. This study aims to provide an initial step
towards the implementation of automated tools within the field of Calisthenics.
To advance knowledge in this context, we propose a dataset of video footage of
static calisthenics skills performed by athletes. Each video is annotated with
a temporal segmentation which determines the extent of each skill. We hence
report the results of a baseline approach to address the problem of skill
temporal segmentation on the proposed dataset. The results highlight the
feasibility of the proposed problem, while there is still room for improvement.

</details>


### [64] [Comparative Analysis of CNN Performance in Keras, PyTorch and JAX on PathMNIST](https://arxiv.org/abs/2507.12248)
*Anida Nezović,Jalal Romano,Nada Marić,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: 该研究比较了Keras、PyTorch和JAX在医学图像分类任务中的性能，使用PathMNIST数据集评估训练效率、分类准确性和推理速度。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习框架在医学图像分类中广泛应用，但不同框架在医学影像任务中的性能比较尚未充分研究。

Method: 通过CNN实现，使用PathMNIST数据集评估Keras、PyTorch和JAX的训练效率、分类准确性和推理速度。

Result: 研究揭示了计算速度与模型准确性之间的权衡。

Conclusion: 研究结果为医学图像分析领域的研究者和从业者提供了有价值的参考。

Abstract: Deep learning has significantly advanced the field of medical image
classification, particularly with the adoption of Convolutional Neural Networks
(CNNs). Various deep learning frameworks such as Keras, PyTorch and JAX offer
unique advantages in model development and deployment. However, their
comparative performance in medical imaging tasks remains underexplored. This
study presents a comprehensive analysis of CNN implementations across these
frameworks, using the PathMNIST dataset as a benchmark. We evaluate training
efficiency, classification accuracy and inference speed to assess their
suitability for real-world applications. Our findings highlight the trade-offs
between computational speed and model accuracy, offering valuable insights for
researchers and practitioners in medical image analysis.

</details>


### [65] [Site-Level Fine-Tuning with Progressive Layer Freezing: Towards Robust Prediction of Bronchopulmonary Dysplasia from Day-1 Chest Radiographs in Extremely Preterm Infants](https://arxiv.org/abs/2507.12269)
*Sybelle Goedicke-Fritz,Michelle Bous,Annika Engel,Matthias Flotho,Pascal Hirsch,Hannah Wittig,Dino Milanovic,Dominik Mohr,Mathias Kaspar,Sogand Nemat,Dorothea Kerner,Arno Bücker,Andreas Keller,Sascha Meyer,Michael Zemlin,Philipp Flotho*

Main category: cs.CV

TL;DR: 该研究提出了一种基于深度学习的模型，利用出生24小时内的胸部X光片预测极低出生体重婴儿的支气管肺发育不良（BPD）结果，结果显示该方法优于传统方法。


<details>
  <summary>Details</summary>
Motivation: BPD是一种常见的慢性肺病，预防干预措施风险高，因此早期预测至关重要。常规的胸部X光片可作为非侵入性预测工具。

Method: 研究使用163名极低出生体重婴儿的胸部X光片，微调了预训练的ResNet-50模型，采用渐进层冻结和CutMix增强技术。

Result: 最佳模型的AUROC为0.78，平衡准确率为0.69，F1分数为0.67，显著优于ImageNet初始化和传统IRDS分级。

Conclusion: 研究表明，领域特定的预训练结合渐进冻结和线性探测，能够高效预测BPD，适合临床部署和未来联邦学习应用。

Abstract: Bronchopulmonary dysplasia (BPD) is a chronic lung disease affecting 35% of
extremely low birth weight infants. Defined by oxygen dependence at 36 weeks
postmenstrual age, it causes lifelong respiratory complications. However,
preventive interventions carry severe risks, including neurodevelopmental
impairment, ventilator-induced lung injury, and systemic complications.
Therefore, early BPD prognosis and prediction of BPD outcome is crucial to
avoid unnecessary toxicity in low risk infants. Admission radiographs of
extremely preterm infants are routinely acquired within 24h of life and could
serve as a non-invasive prognostic tool. In this work, we developed and
investigated a deep learning approach using chest X-rays from 163 extremely
low-birth-weight infants ($\leq$32 weeks gestation, 401-999g) obtained within
24 hours of birth. We fine-tuned a ResNet-50 pretrained specifically on adult
chest radiographs, employing progressive layer freezing with discriminative
learning rates to prevent overfitting and evaluated a CutMix augmentation and
linear probing. For moderate/severe BPD outcome prediction, our best performing
model with progressive freezing, linear probing and CutMix achieved an AUROC of
0.78 $\pm$ 0.10, balanced accuracy of 0.69 $\pm$ 0.10, and an F1-score of 0.67
$\pm$ 0.11. In-domain pre-training significantly outperformed ImageNet
initialization (p = 0.031) which confirms domain-specific pretraining to be
important for BPD outcome prediction. Routine IRDS grades showed limited
prognostic value (AUROC 0.57 $\pm$ 0.11), confirming the need of learned
markers. Our approach demonstrates that domain-specific pretraining enables
accurate BPD prediction from routine day-1 radiographs. Through progressive
freezing and linear probing, the method remains computationally feasible for
site-level implementation and future federated learning deployments.

</details>


### [66] [FADE: Adversarial Concept Erasure in Flow Models](https://arxiv.org/abs/2507.12283)
*Zixuan Fu,Yan Ren,Finn Carter,Chenyue Wang,Ze Niu,Dacheng Yu,Emily Davis,Bo Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为FADE的新方法，用于从文本到图像扩散模型中移除特定概念，确保隐私和公平性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面表现出色，但也存在隐私泄露和偏见传播的风险，需要一种方法移除敏感概念。

Method: 结合轨迹感知微调策略和对抗性目标，确保概念被可靠移除同时保持模型保真度。

Result: FADE在概念移除和图像质量上优于现有方法，性能提升5-10%。

Conclusion: FADE为安全公平的生成建模设定了新标准，无需从头训练即可移除指定概念。

Abstract: Diffusion models have demonstrated remarkable image generation capabilities,
but also pose risks in privacy and fairness by memorizing sensitive concepts or
perpetuating biases. We propose a novel \textbf{concept erasure} method for
text-to-image diffusion models, designed to remove specified concepts (e.g., a
private individual or a harmful stereotype) from the model's generative
repertoire. Our method, termed \textbf{FADE} (Fair Adversarial Diffusion
Erasure), combines a trajectory-aware fine-tuning strategy with an adversarial
objective to ensure the concept is reliably removed while preserving overall
model fidelity. Theoretically, we prove a formal guarantee that our approach
minimizes the mutual information between the erased concept and the model's
outputs, ensuring privacy and fairness. Empirically, we evaluate FADE on Stable
Diffusion and FLUX, using benchmarks from prior work (e.g., object, celebrity,
explicit content, and style erasure tasks from MACE). FADE achieves
state-of-the-art concept removal performance, surpassing recent baselines like
ESD, UCE, MACE, and ANT in terms of removal efficacy and image quality.
Notably, FADE improves the harmonic mean of concept removal and fidelity by
5--10\% over the best prior method. We also conduct an ablation study to
validate each component of FADE, confirming that our adversarial and
trajectory-preserving objectives each contribute to its superior performance.
Our work sets a new standard for safe and fair generative modeling by
unlearning specified concepts without retraining from scratch.

</details>


### [67] [Efficient Calisthenics Skills Classification through Foreground Instance Selection and Depth Estimation](https://arxiv.org/abs/2507.12292)
*Antonio Finocchiaro,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: 提出一种基于深度估计和运动员区域检索的体操技能分类方法，避免传统姿态估计的高计算成本，显著提升效率和分类精度。


<details>
  <summary>Details</summary>
Motivation: 传统基于姿态估计的方法计算成本高、推理时间长，限制了实时应用和移动设备的适用性。

Method: 使用Depth Anything V2进行深度估计，YOLOv10定位运动员区域，直接分割主体而非依赖姿态估计。

Result: 方法比骨架方法快38.3倍，深度补丁分类精度更高（0.837 vs. 0.815）。

Conclusion: 模块化设计支持灵活组件替换，未来可进一步优化并适应实际应用。

Abstract: Calisthenics skill classification is the computer vision task of inferring
the skill performed by an athlete from images, enabling automatic performance
assessment and personalized analytics. Traditional methods for calisthenics
skill recognition are based on pose estimation methods to determine the
position of skeletal data from images, which is later fed to a classification
algorithm to infer the performed skill. Despite the progress in human pose
estimation algorithms, they still involve high computational costs, long
inference times, and complex setups, which limit the applicability of such
approaches in real-time applications or mobile devices. This work proposes a
direct approach to calisthenics skill recognition, which leverages depth
estimation and athlete patch retrieval to avoid the computationally expensive
human pose estimation module. Using Depth Anything V2 for depth estimation and
YOLOv10 for athlete localization, we segment the subject from the background
rather than relying on traditional pose estimation techniques. This strategy
increases efficiency, reduces inference time, and improves classification
accuracy. Our approach significantly outperforms skeleton-based methods,
achieving 38.3x faster inference with RGB image patches and improved
classification accuracy with depth patches (0.837 vs. 0.815). Beyond these
performance gains, the modular design of our pipeline allows for flexible
replacement of components, enabling future enhancements and adaptation to
real-world applications.

</details>


### [68] [Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models](https://arxiv.org/abs/2507.12318)
*Samuel Lavoie,Michael Noukhovitch,Aaron Courville*

Main category: cs.CV

TL;DR: 本文提出离散潜在码（DLC）作为扩散模型的输入条件表示，通过自监督学习目标训练，提升生成保真度并支持组合性生成。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型成功的关键在于输入条件表示，探索理想表示应具备高保真、易生成和组合性。

Method: 引入DLC，一种基于Simplicial Embeddings的离散序列表示，用于扩散模型训练。

Result: DLC显著提升无条件图像生成性能，并在ImageNet上达到新SOTA，同时支持组合生成和文本到图像生成。

Conclusion: DLC为扩散模型提供了高效、组合性的表示，扩展了生成能力。

Abstract: We argue that diffusion models' success in modeling complex distributions is,
for the most part, coming from their input conditioning. This paper
investigates the representation used to condition diffusion models from the
perspective that ideal representations should improve sample fidelity, be easy
to generate, and be compositional to allow out-of-training samples generation.
We introduce Discrete Latent Code (DLC), an image representation derived from
Simplicial Embeddings trained with a self-supervised learning objective. DLCs
are sequences of discrete tokens, as opposed to the standard continuous image
embeddings. They are easy to generate and their compositionality enables
sampling of novel images beyond the training distribution. Diffusion models
trained with DLCs have improved generation fidelity, establishing a new
state-of-the-art for unconditional image generation on ImageNet. Additionally,
we show that composing DLCs allows the image generator to produce
out-of-distribution samples that coherently combine the semantics of images in
diverse ways. Finally, we showcase how DLCs can enable text-to-image generation
by leveraging large-scale pretrained language models. We efficiently finetune a
text diffusion language model to generate DLCs that produce novel samples
outside of the image generator training distribution.

</details>


### [69] [Unsupervised Monocular 3D Keypoint Discovery from Multi-View Diffusion Priors](https://arxiv.org/abs/2507.12336)
*Subin Jeon,In Cho,Junyoung Hong,Seon Joo Kim*

Main category: cs.CV

TL;DR: KeyDiff3D是一个无监督的单目3D关键点估计框架，利用预训练的多视角扩散模型生成几何先验，实现从单张图像预测3D关键点。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖昂贵的人工标注或多视角图像，KeyDiff3D旨在仅用单视角图像实现3D关键点估计。

Method: 利用扩散模型生成多视角图像作为监督信号，并提取2D多视角特征构建3D特征体积，将隐式3D先验转化为显式特征。

Result: 在Human3.6M、Stanford Dogs等数据集上验证了方法的准确性、泛化性，并支持对扩散模型生成的3D对象进行操控。

Conclusion: KeyDiff3D无需标注或多视角数据，通过扩散模型实现了高效的单目3D关键点估计与对象操控。

Abstract: This paper introduces KeyDiff3D, a framework for unsupervised monocular 3D
keypoints estimation that accurately predicts 3D keypoints from a single image.
While previous methods rely on manual annotations or calibrated multi-view
images, both of which are expensive to collect, our method enables monocular 3D
keypoints estimation using only a collection of single-view images. To achieve
this, we leverage powerful geometric priors embedded in a pretrained multi-view
diffusion model. In our framework, this model generates multi-view images from
a single image, serving as a supervision signal to provide 3D geometric cues to
our model. We also use the diffusion model as a powerful 2D multi-view feature
extractor and construct 3D feature volumes from its intermediate
representations. This transforms implicit 3D priors learned by the diffusion
model into explicit 3D features. Beyond accurate keypoints estimation, we
further introduce a pipeline that enables manipulation of 3D objects generated
by the diffusion model. Experimental results on diverse aspects and datasets,
including Human3.6M, Stanford Dogs, and several in-the-wild and out-of-domain
datasets, highlight the effectiveness of our method in terms of accuracy,
generalization, and its ability to enable manipulation of 3D objects generated
by the diffusion model from a single image.

</details>


### [70] [Improving Lightweight Weed Detection via Knowledge Distillation](https://arxiv.org/abs/2507.12344)
*Ahmet Oğuz Saltık,Max Voigt,Sourav Modak,Mike Beckworth,Anthony Stein*

Main category: cs.CV

TL;DR: 论文研究了通道知识蒸馏（CWD）和掩码生成蒸馏（MGD）在轻量级模型中的应用，以提高实时智能喷洒系统中的杂草检测性能。实验表明，两种方法均能显著提升模型精度，且不影响模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 杂草检测是精准农业的关键，但资源受限平台上的准确检测模型部署仍具挑战性，尤其是在区分视觉相似的杂草种类时。

Method: 使用YOLO11x作为教师模型，YOLO11n作为参考和学生模型，应用CWD和MGD进行知识蒸馏。

Result: CWD和MGD分别提升了2.5%和1.9%的mAP50，并在嵌入式设备上验证了实时部署的可行性。

Conclusion: CWD和MGD是提高深度学习杂草检测精度的有效、高效且实用的方法。

Abstract: Weed detection is a critical component of precision agriculture, facilitating
targeted herbicide application and reducing environmental impact. However,
deploying accurate object detection models on resource-limited platforms
remains challenging, particularly when differentiating visually similar weed
species commonly encountered in plant phenotyping applications. In this work,
we investigate Channel-wise Knowledge Distillation (CWD) and Masked Generative
Distillation (MGD) to enhance the performance of lightweight models for
real-time smart spraying systems. Utilizing YOLO11x as the teacher model and
YOLO11n as both reference and student, both CWD and MGD effectively transfer
knowledge from the teacher to the student model. Our experiments, conducted on
a real-world dataset comprising sugar beet crops and four weed types (Cirsium,
Convolvulus, Fallopia, and Echinochloa), consistently show increased AP50
across all classes. The distilled CWD student model achieves a notable
improvement of 2.5% and MGD achieves 1.9% in mAP50 over the baseline without
increasing model complexity. Additionally, we validate real-time deployment
feasibility by evaluating the student YOLO11n model on Jetson Orin Nano and
Raspberry Pi 5 embedded devices, performing five independent runs to evaluate
performance stability across random seeds. These findings confirm CWD and MGD
as an effective, efficient, and practical approach for improving deep
learning-based weed detection accuracy in precision agriculture and plant
phenotyping scenarios.

</details>


### [71] [Cluster Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/abs/2507.12359)
*Nikolaos Giakoumoglou,Tania Stathaki*

Main category: cs.CV

TL;DR: CueCo是一种结合对比学习和聚类方法的新型无监督视觉表示学习方法，通过分散和对齐特征表示提升性能。


<details>
  <summary>Details</summary>
Motivation: 结合对比学习和聚类的优势，提升无监督视觉表示学习的性能。

Method: 使用两个神经网络（查询和键），键网络通过查询输出的慢速移动平均更新，结合对比损失和聚类目标。

Result: 在CIFAR-10、CIFAR-100和ImageNet-100上分别达到91.40%、68.56%和78.65%的top-1分类准确率。

Conclusion: CueCo通过整合对比学习和聚类，为无监督视觉表示学习开辟了新方向。

Abstract: We introduce Cluster Contrast (CueCo), a novel approach to unsupervised
visual representation learning that effectively combines the strengths of
contrastive learning and clustering methods. Inspired by recent advancements,
CueCo is designed to simultaneously scatter and align feature representations
within the feature space. This method utilizes two neural networks, a query and
a key, where the key network is updated through a slow-moving average of the
query outputs. CueCo employs a contrastive loss to push dissimilar features
apart, enhancing inter-class separation, and a clustering objective to pull
together features of the same cluster, promoting intra-class compactness. Our
method achieves 91.40% top-1 classification accuracy on CIFAR-10, 68.56% on
CIFAR-100, and 78.65% on ImageNet-100 using linear evaluation with a ResNet-18
backbone. By integrating contrastive learning with clustering, CueCo sets a new
direction for advancing unsupervised visual representation learning.

</details>


### [72] [Text-driven Multiplanar Visual Interaction for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2507.12382)
*Kaiwen Huang,Yi Zhou,Huazhu Fu,Yizhe Zhang,Chen Gong,Tao Zhou*

Main category: cs.CV

TL;DR: 提出了一种名为Text-SemiSeg的文本驱动多平面视觉交互框架，用于半监督医学图像分割，通过文本增强视觉语义嵌入，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像标注成本高的问题，利用文本信息增强视觉语义理解，填补3D医学影像任务中文本数据利用的研究空白。

Method: 框架包含三个模块：文本增强多平面表示（TMR）、类别感知语义对齐（CSA）和动态认知增强（DCA），分别用于文本-视觉交互、跨模态语义对齐和减少标记与未标记数据的分布差异。

Result: 在三个公开数据集上的实验表明，该模型能有效利用文本信息增强视觉特征，性能优于其他方法。

Conclusion: Text-SemiSeg通过文本驱动的方法显著提升了半监督医学图像分割的效果，为相关领域提供了新思路。

Abstract: Semi-supervised medical image segmentation is a crucial technique for
alleviating the high cost of data annotation. When labeled data is limited,
textual information can provide additional context to enhance visual semantic
understanding. However, research exploring the use of textual data to enhance
visual semantic embeddings in 3D medical imaging tasks remains scarce. In this
paper, we propose a novel text-driven multiplanar visual interaction framework
for semi-supervised medical image segmentation (termed Text-SemiSeg), which
consists of three main modules: Text-enhanced Multiplanar Representation (TMR),
Category-aware Semantic Alignment (CSA), and Dynamic Cognitive Augmentation
(DCA). Specifically, TMR facilitates text-visual interaction through planar
mapping, thereby enhancing the category awareness of visual features. CSA
performs cross-modal semantic alignment between the text features with
introduced learnable variables and the intermediate layer of visual features.
DCA reduces the distribution discrepancy between labeled and unlabeled data
through their interaction, thus improving the model's robustness. Finally,
experiments on three public datasets demonstrate that our model effectively
enhances visual features with textual information and outperforms other
methods. Our code is available at https://github.com/taozh2017/Text-SemiSeg.

</details>


### [73] [OD-VIRAT: A Large-Scale Benchmark for Object Detection in Realistic Surveillance Environments](https://arxiv.org/abs/2507.12396)
*Hayat Ullah,Abbas Khan,Arslan Munir,Hari Kalva*

Main category: cs.CV

TL;DR: 提出了两个视觉目标检测基准数据集OD-VIRAT Large和OD-VIRAT Tiny，用于评估复杂环境下的人体监控目标检测模型性能，并测试了多种先进目标检测架构。


<details>
  <summary>Details</summary>
Motivation: 开发可靠的监控系统需要多样且具有挑战性的数据集，以全面评估模型性能。

Method: 创建了两个数据集（OD-VIRAT Large和OD-VIRAT Tiny），包含丰富的标注信息，并对多种先进目标检测架构（如RETMDET、YOLOX等）进行了性能测试。

Result: OD-VIRAT Large包含8.7百万标注实例，OD-VIRAT Tiny包含288,901标注实例。实验为复杂监控场景下的目标检测提供了性能基准。

Conclusion: 该工作为开发更高效和鲁棒的目标检测架构奠定了基础，并提供了对现有模型性能的深入见解。

Abstract: Realistic human surveillance datasets are crucial for training and evaluating
computer vision models under real-world conditions, facilitating the
development of robust algorithms for human and human-interacting object
detection in complex environments. These datasets need to offer diverse and
challenging data to enable a comprehensive assessment of model performance and
the creation of more reliable surveillance systems for public safety. To this
end, we present two visual object detection benchmarks named OD-VIRAT Large and
OD-VIRAT Tiny, aiming at advancing visual understanding tasks in surveillance
imagery. The video sequences in both benchmarks cover 10 different scenes of
human surveillance recorded from significant height and distance. The proposed
benchmarks offer rich annotations of bounding boxes and categories, where
OD-VIRAT Large has 8.7 million annotated instances in 599,996 images and
OD-VIRAT Tiny has 288,901 annotated instances in 19,860 images. This work also
focuses on benchmarking state-of-the-art object detection architectures,
including RETMDET, YOLOX, RetinaNet, DETR, and Deformable-DETR on this object
detection-specific variant of VIRAT dataset. To the best of our knowledge, it
is the first work to examine the performance of these recently published
state-of-the-art object detection architectures on realistic surveillance
imagery under challenging conditions such as complex backgrounds, occluded
objects, and small-scale objects. The proposed benchmarking and experimental
settings will help in providing insights concerning the performance of selected
object detection models and set the base for developing more efficient and
robust object detection architectures.

</details>


### [74] [AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models](https://arxiv.org/abs/2507.12414)
*Santosh Vasa,Aditi Ramadwar,Jnana Rama Krishna Darabattula,Md Zafar Anwar,Stanislaw Antol,Andrei Vatavu,Thomas Monninger,Sihao Ding*

Main category: cs.CV

TL;DR: AutoVDC框架利用视觉语言模型自动检测视觉数据集中的标注错误，提升数据质量，验证结果显示高效。


<details>
  <summary>Details</summary>
Motivation: 人工标注存在缺陷且成本高，需自动化方法提升数据集质量。

Method: 利用视觉语言模型（VLMs）自动识别错误标注，并在KITTI和nuImages数据集上验证。

Result: 方法在错误检测和数据清理实验中表现高效。

Conclusion: AutoVDC能显著提升自动驾驶大规模生产数据集的可靠性和准确性。

Abstract: Training of autonomous driving systems requires extensive datasets with
precise annotations to attain robust performance. Human annotations suffer from
imperfections, and multiple iterations are often needed to produce high-quality
datasets. However, manually reviewing large datasets is laborious and
expensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning)
framework and investigate the utilization of Vision-Language Models (VLMs) to
automatically identify erroneous annotations in vision datasets, thereby
enabling users to eliminate these errors and enhance data quality. We validate
our approach using the KITTI and nuImages datasets, which contain object
detection benchmarks for autonomous driving. To test the effectiveness of
AutoVDC, we create dataset variants with intentionally injected erroneous
annotations and observe the error detection rate of our approach. Additionally,
we compare the detection rates using different VLMs and explore the impact of
VLM fine-tuning on our pipeline. The results demonstrate our method's high
performance in error detection and data cleaning experiments, indicating its
potential to significantly improve the reliability and accuracy of large-scale
production datasets in autonomous driving.

</details>


### [75] [QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval](https://arxiv.org/abs/2507.12416)
*Jaehyun Kwak,Ramahdani Muhammad Izaaz Inhar,Se-Young Yun,Sung-Ju Lee*

Main category: cs.CV

TL;DR: 论文提出了一种名为QuRe的方法，通过硬负采样减少假阴性，提升组合图像检索（CIR）的准确性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 现有CIR方法仅关注目标图像检索，忽略了其他图像的相关性，导致假阴性问题，影响用户满意度。

Method: 提出了Query-Relevant Retrieval through Hard Negative Sampling (QuRe)，结合奖励模型目标和硬负采样策略，过滤假阴性。

Result: QuRe在FashionIQ和CIRR数据集上达到最先进性能，并在HP-FashionIQ数据集上表现出与人类偏好最强的对齐。

Conclusion: QuRe通过优化检索策略和引入新数据集，显著提升了CIR的性能和用户满意度。

Abstract: Composed Image Retrieval (CIR) retrieves relevant images based on a reference
image and accompanying text describing desired modifications. However, existing
CIR methods only focus on retrieving the target image and disregard the
relevance of other images. This limitation arises because most methods
employing contrastive learning-which treats the target image as positive and
all other images in the batch as negatives-can inadvertently include false
negatives. This may result in retrieving irrelevant images, reducing user
satisfaction even when the target image is retrieved. To address this issue, we
propose Query-Relevant Retrieval through Hard Negative Sampling (QuRe), which
optimizes a reward model objective to reduce false negatives. Additionally, we
introduce a hard negative sampling strategy that selects images positioned
between two steep drops in relevance scores following the target image, to
effectively filter false negatives. In order to evaluate CIR models on their
alignment with human satisfaction, we create Human-Preference FashionIQ
(HP-FashionIQ), a new dataset that explicitly captures user preferences beyond
target retrieval. Extensive experiments demonstrate that QuRe achieves
state-of-the-art performance on FashionIQ and CIRR datasets while exhibiting
the strongest alignment with human preferences on the HP-FashionIQ dataset. The
source code is available at https://github.com/jackwaky/QuRe.

</details>


### [76] [InterpIoU: Rethinking Bounding Box Regression with Interpolation-Based IoU Optimization](https://arxiv.org/abs/2507.12420)
*Haoyuan Liu,Hiroshi Watanabe*

Main category: cs.CV

TL;DR: 论文提出了一种新的损失函数InterpIoU，通过插值框解决IoU在非重叠情况下的不可微问题，并避免了传统几何惩罚的局限性，显著提升了小物体检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于IoU的损失函数在处理非重叠情况时存在不可微问题，且手工设计的几何惩罚对框的形状、大小和分布敏感，导致小物体检测效果不佳和框扩大问题。

Method: 提出InterpIoU，用插值框与目标框的IoU代替手工几何惩罚；进一步提出Dynamic InterpIoU，动态调整插值系数以适应不同物体分布。

Result: 在COCO、VisDrone和PASCAL VOC数据集上，InterpIoU和Dynamic InterpIoU均优于现有IoU损失函数，尤其在小物体检测中表现突出。

Conclusion: InterpIoU通过插值框有效解决了传统IoU损失的问题，Dynamic InterpIoU进一步提升了适应性，为物体检测提供了更优的损失函数。

Abstract: Bounding box regression (BBR) is fundamental to object detection, where the
regression loss is crucial for accurate localization. Existing IoU-based losses
often incorporate handcrafted geometric penalties to address IoU's
non-differentiability in non-overlapping cases and enhance BBR performance.
However, these penalties are sensitive to box shape, size, and distribution,
often leading to suboptimal optimization for small objects and undesired
behaviors such as bounding box enlargement due to misalignment with the IoU
objective. To address these limitations, we propose InterpIoU, a novel loss
function that replaces handcrafted geometric penalties with a term based on the
IoU between interpolated boxes and the target. By using interpolated boxes to
bridge the gap between predictions and ground truth, InterpIoU provides
meaningful gradients in non-overlapping cases and inherently avoids the box
enlargement issue caused by misaligned penalties. Simulation results further
show that IoU itself serves as an ideal regression target, while existing
geometric penalties are both unnecessary and suboptimal. Building on InterpIoU,
we introduce Dynamic InterpIoU, which dynamically adjusts interpolation
coefficients based on IoU values, enhancing adaptability to scenarios with
diverse object distributions. Experiments on COCO, VisDrone, and PASCAL VOC
show that our methods consistently outperform state-of-the-art IoU-based losses
across various detection frameworks, with particularly notable improvements in
small object detection, confirming their effectiveness.

</details>


### [77] [DVFL-Net: A Lightweight Distilled Video Focal Modulation Network for Spatio-Temporal Action Recognition](https://arxiv.org/abs/2507.12426)
*Hayat Ullah,Muhammad Ali Shafique,Abbas Khan,Arslan Munir*

Main category: cs.CV

TL;DR: 提出了一种轻量级的视频识别模型DVFL-Net，通过知识蒸馏和时空特征调制，在保持高性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在视频识别中表现出色但计算成本高，需要一种更高效的解决方案。

Method: 采用知识蒸馏和时空焦点调制技术，将大型预训练教师模型的知识转移到紧凑的学生模型。

Result: DVFL-Net在多个数据集上表现出色，平衡了性能和效率，适用于实时应用。

Conclusion: DVFL-Net是一种高效且实用的视频识别解决方案，适合资源受限的场景。

Abstract: The landscape of video recognition has evolved significantly, shifting from
traditional Convolutional Neural Networks (CNNs) to Transformer-based
architectures for improved accuracy. While 3D CNNs have been effective at
capturing spatiotemporal dynamics, recent Transformer models leverage
self-attention to model long-range spatial and temporal dependencies. Despite
achieving state-of-the-art performance on major benchmarks, Transformers remain
computationally expensive, particularly with dense video data. To address this,
we propose a lightweight Video Focal Modulation Network, DVFL-Net, which
distills spatiotemporal knowledge from a large pre-trained teacher into a
compact nano student model, enabling efficient on-device deployment. DVFL-Net
utilizes knowledge distillation and spatial-temporal feature modulation to
significantly reduce computation while preserving high recognition performance.
We employ forward Kullback-Leibler (KL) divergence alongside spatio-temporal
focal modulation to effectively transfer both local and global context from the
Video-FocalNet Base (teacher) to the proposed VFL-Net (student). We evaluate
DVFL-Net on UCF50, UCF101, HMDB51, SSV2, and Kinetics-400, benchmarking it
against recent state-of-the-art methods in Human Action Recognition (HAR).
Additionally, we conduct a detailed ablation study analyzing the impact of
forward KL divergence. The results confirm the superiority of DVFL-Net in
achieving an optimal balance between performance and efficiency, demonstrating
lower memory usage, reduced GFLOPs, and strong accuracy, making it a practical
solution for real-time HAR applications.

</details>


### [78] [Traffic-Aware Pedestrian Intention Prediction](https://arxiv.org/abs/2507.12433)
*Fahimeh Orvati Nia,Hai Lin*

Main category: cs.CV

TL;DR: 提出了一种交通感知时空图卷积网络（TA-STGCN），通过整合动态交通信号和场景信息，显著提升了行人意图预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有模型在行人意图预测中常忽略动态交通信号和场景信息，而这些对自动驾驶车辆的安全导航至关重要。

Method: TA-STGCN整合了交通信号状态（红、黄、绿）和边界框大小作为关键特征，捕捉复杂城市环境中的时空依赖关系。

Result: 在PIE数据集上，TA-STGCN比基线模型准确率提高了4.75%。

Conclusion: TA-STGCN通过动态交通信号和场景信息的整合，有效提升了行人意图预测的准确性。

Abstract: Accurate pedestrian intention estimation is crucial for the safe navigation
of autonomous vehicles (AVs) and hence attracts a lot of research attention.
However, current models often fail to adequately consider dynamic traffic
signals and contextual scene information, which are critical for real-world
applications. This paper presents a Traffic-Aware Spatio-Temporal Graph
Convolutional Network (TA-STGCN) that integrates traffic signs and their states
(Red, Yellow, Green) into pedestrian intention prediction. Our approach
introduces the integration of dynamic traffic signal states and bounding box
size as key features, allowing the model to capture both spatial and temporal
dependencies in complex urban environments. The model surpasses existing
methods in accuracy. Specifically, TA-STGCN achieves a 4.75% higher accuracy
compared to the baseline model on the PIE dataset, demonstrating its
effectiveness in improving pedestrian intention prediction.

</details>


### [79] [Describe Anything Model for Visual Question Answering on Text-rich Images](https://arxiv.org/abs/2507.12441)
*Yen-Linh Vu,Dinh-Thang Duong,Truong-Binh Duong,Anh-Khoi Nguyen,Thanh-Huy Nguyen,Le Thien Phuc Nguyen,Jianhua Xing,Xingjian Li,Tianyang Wang,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: DAM-QA利用区域感知视觉语言模型（DAM）提升文本密集图像的视觉问答（VQA）性能，通过多区域视图聚合答案，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 针对文本密集图像的VQA任务，需要细粒度提取文本信息，DAM的区域描述能力可能对此有益。

Method: 提出DAM-QA框架，结合DAM的区域感知能力，通过多区域视图聚合答案。

Result: 在六个VQA基准测试中表现优于基线DAM，DocVQA提升7分以上，参数更少且性能接近通用视觉语言模型。

Conclusion: DAM-QA展示了区域感知模型在文本密集VQA任务中的潜力，结合高效使用策略可缩小与通用模型的差距。

Abstract: Recent progress has been made in region-aware vision-language modeling,
particularly with the emergence of the Describe Anything Model (DAM). DAM is
capable of generating detailed descriptions of any specific image areas or
objects without the need for additional localized image-text alignment
supervision. We hypothesize that such region-level descriptive capability is
beneficial for the task of Visual Question Answering (VQA), especially in
challenging scenarios involving images with dense text. In such settings, the
fine-grained extraction of textual information is crucial to producing correct
answers. Motivated by this, we introduce DAM-QA, a framework with a tailored
evaluation protocol, developed to investigate and harness the region-aware
capabilities from DAM for the text-rich VQA problem that requires reasoning
over text-based information within images. DAM-QA incorporates a mechanism that
aggregates answers from multiple regional views of image content, enabling more
effective identification of evidence that may be tied to text-related elements.
Experiments on six VQA benchmarks show that our approach consistently
outperforms the baseline DAM, with a notable 7+ point gain on DocVQA. DAM-QA
also achieves the best overall performance among region-aware models with fewer
parameters, significantly narrowing the gap with strong generalist VLMs. These
results highlight the potential of DAM-like models for text-rich and broader
VQA tasks when paired with efficient usage and integration strategies. Our code
is publicly available at https://github.com/Linvyl/DAM-QA.git.

</details>


### [80] [Vision-based Perception for Autonomous Vehicles in Obstacle Avoidance Scenarios](https://arxiv.org/abs/2507.12449)
*Van-Hoang-Anh Phan,Chi-Tam Nguyen,Doan-Trung Au,Thanh-Danh Phan,Minh-Thien Duong,My-Ha Le*

Main category: cs.CV

TL;DR: 提出了一种基于摄像头感知和Frenet-Pure Pursuit规划的障碍物避障系统，结合YOLOv11和Depth Anything V2模型，评估了其在实际场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 确保自动驾驶车辆的安全性需要准确的感知和运动规划，尤其是在复杂环境中避免碰撞。

Method: 使用摄像头感知模块（YOLOv11和Depth Anything V2）进行物体检测和距离估计，结合Frenet-Pure Pursuit规划策略。

Result: 在多样化的校园场景中验证了系统的有效性，能够处理多种障碍物并提升自主导航能力。

Conclusion: 提出的系统在障碍物避障方面表现出色，为自动驾驶车辆的安全导航提供了可行方案。

Abstract: Obstacle avoidance is essential for ensuring the safety of autonomous
vehicles. Accurate perception and motion planning are crucial to enabling
vehicles to navigate complex environments while avoiding collisions. In this
paper, we propose an efficient obstacle avoidance pipeline that leverages a
camera-only perception module and a Frenet-Pure Pursuit-based planning
strategy. By integrating advancements in computer vision, the system utilizes
YOLOv11 for object detection and state-of-the-art monocular depth estimation
models, such as Depth Anything V2, to estimate object distances. A comparative
analysis of these models provides valuable insights into their accuracy,
efficiency, and robustness in real-world conditions. The system is evaluated in
diverse scenarios on a university campus, demonstrating its effectiveness in
handling various obstacles and enhancing autonomous navigation. The video
presenting the results of the obstacle avoidance experiments is available at:
https://www.youtube.com/watch?v=FoXiO5S_tA8

</details>


### [81] [Mitigating Object Hallucinations via Sentence-Level Early Intervention](https://arxiv.org/abs/2507.12455)
*Shangpin Peng,Senqiao Yang,Li Jiang,Zhuotao Tian*

Main category: cs.CV

TL;DR: SENTINEL框架通过句子级早期干预和领域内偏好学习，显著减少多模态大语言模型中的幻觉现象，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在跨模态理解中存在幻觉问题，现有方法成本高或引入数据分布不匹配。

Method: 提出SENTINEL框架，通过迭代采样、验证和分类构建高质量偏好数据，并采用上下文感知偏好损失（C-DPO）进行训练。

Result: 实验表明，SENTINEL将幻觉减少90%以上，并在幻觉和通用能力基准测试中优于现有方法。

Conclusion: SENTINEL通过早期干预和偏好学习有效减少幻觉，具有优越性和泛化能力。

Abstract: Multimodal large language models (MLLMs) have revolutionized cross-modal
understanding but continue to struggle with hallucinations - fabricated content
contradicting visual inputs. Existing hallucination mitigation methods either
incur prohibitive computational costs or introduce distribution mismatches
between training data and model outputs. We identify a critical insight:
hallucinations predominantly emerge at the early stages of text generation and
propagate through subsequent outputs. To address this, we propose **SENTINEL**
(**S**entence-level **E**arly i**N**tervention **T**hrough **IN**-domain
pr**E**ference **L**earning), a framework that eliminates dependency on human
annotations. Specifically, we first bootstrap high-quality in-domain preference
pairs by iteratively sampling model outputs, validating object existence
through cross-checking with two open-vocabulary detectors, and classifying
sentences into hallucinated/non-hallucinated categories. Subsequently, we use
context-coherent positive samples and hallucinated negative samples to build
context-aware preference data iteratively. Finally, we train models using a
context-aware preference loss (C-DPO) that emphasizes discriminative learning
at the sentence level where hallucinations initially manifest. Experimental
results show that SENTINEL can reduce hallucinations by over 90\% compared to
the original model and outperforms the previous state-of-the-art method on both
hallucination benchmarks and general capabilities benchmarks, demonstrating its
superiority and generalization ability. The models, datasets, and code are
available at https://github.com/pspdada/SENTINEL.

</details>


### [82] [Interpreting Radiologist's Intention from Eye Movements in Chest X-ray Diagnosis](https://arxiv.org/abs/2507.12461)
*Trong-Thang Pham,Anh Nguyen,Zhigang Deng,Carol C. Wu,Hien Van Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: 提出了一种名为RadGazeIntent的深度学习方法，用于建模放射科医生在解读医学图像时的意图驱动行为，通过处理眼动数据预测其诊断意图。


<details>
  <summary>Details</summary>
Motivation: 现有模型未能捕捉放射科医生在图像解读过程中每个注视点背后的意图，而理解这种意图驱动的行为对提升医学图像分析至关重要。

Method: 采用基于Transformer的架构，处理眼动数据的时空维度，将细粒度注视特征转化为粗粒度的诊断意图表示，并构建了三个意图标记的数据集（RadSeq、RadExplore、RadHybrid）。

Result: RadGazeIntent在所有意图标记数据集上均优于基线方法，能够准确预测放射科医生在特定时刻关注的发现。

Conclusion: RadGazeIntent成功建模了放射科医生的意图驱动行为，为医学图像分析提供了新的视角。

Abstract: Radiologists rely on eye movements to navigate and interpret medical images.
A trained radiologist possesses knowledge about the potential diseases that may
be present in the images and, when searching, follows a mental checklist to
locate them using their gaze. This is a key observation, yet existing models
fail to capture the underlying intent behind each fixation. In this paper, we
introduce a deep learning-based approach, RadGazeIntent, designed to model this
behavior: having an intention to find something and actively searching for it.
Our transformer-based architecture processes both the temporal and spatial
dimensions of gaze data, transforming fine-grained fixation features into
coarse, meaningful representations of diagnostic intent to interpret
radiologists' goals. To capture the nuances of radiologists' varied
intention-driven behaviors, we process existing medical eye-tracking datasets
to create three intention-labeled subsets: RadSeq (Systematic Sequential
Search), RadExplore (Uncertainty-driven Exploration), and RadHybrid (Hybrid
Pattern). Experimental results demonstrate RadGazeIntent's ability to predict
which findings radiologists are examining at specific moments, outperforming
baseline methods across all intention-labeled datasets.

</details>


### [83] [SpatialTrackerV2: 3D Point Tracking Made Easy](https://arxiv.org/abs/2507.12462)
*Yuxi Xiao,Jianyuan Wang,Nan Xue,Nikita Karaev,Yuri Makarov,Bingyi Kang,Xing Zhu,Hujun Bao,Yujun Shen,Xiaowei Zhou*

Main category: cs.CV

TL;DR: SpatialTrackerV2是一种基于前馈的单目视频3D点跟踪方法，通过统一几何、相机运动和物体运动，实现了高性能的端到端跟踪。


<details>
  <summary>Details</summary>
Motivation: 超越现有基于现成组件的模块化3D跟踪方法，探索几何、相机运动和物体运动的内在联系。

Method: 将3D运动分解为场景几何、相机自运动和像素级物体运动，采用完全可微的端到端架构，支持跨数据集训练。

Result: 性能超越现有3D跟踪方法30%，与领先的动态3D重建方法精度相当，但速度快50倍。

Conclusion: SpatialTrackerV2通过联合学习几何和运动，实现了高效且高精度的3D点跟踪。

Abstract: We present SpatialTrackerV2, a feed-forward 3D point tracking method for
monocular videos. Going beyond modular pipelines built on off-the-shelf
components for 3D tracking, our approach unifies the intrinsic connections
between point tracking, monocular depth, and camera pose estimation into a
high-performing and feedforward 3D point tracker. It decomposes world-space 3D
motion into scene geometry, camera ego-motion, and pixel-wise object motion,
with a fully differentiable and end-to-end architecture, allowing scalable
training across a wide range of datasets, including synthetic sequences, posed
RGB-D videos, and unlabeled in-the-wild footage. By learning geometry and
motion jointly from such heterogeneous data, SpatialTrackerV2 outperforms
existing 3D tracking methods by 30%, and matches the accuracy of leading
dynamic 3D reconstruction approaches while running 50$\times$ faster.

</details>


### [84] [MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding](https://arxiv.org/abs/2507.12463)
*Renjie Li,Ruijie Ye,Mingyang Wu,Hao Frank Yang,Zhiwen Fan,Hezhen Hu,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 提出了一个名为MMHU的大规模基准数据集，用于自动驾驶中人类行为分析，包含丰富的标注和多样化的数据来源。


<details>
  <summary>Details</summary>
Motivation: 理解人类行为对开发安全的自动驾驶系统至关重要，但目前缺乏全面的评估基准。

Method: 开发了一个包含57k人类运动片段和1.73M帧的数据集，结合多种数据来源，并采用人机协作标注流程生成行为描述。

Result: 提供了全面的数据集分析和多任务基准，包括运动预测、运动生成和行为问答。

Conclusion: MMHU基准为自动驾驶中人类行为理解提供了广泛的评估工具。

Abstract: Humans are integral components of the transportation ecosystem, and
understanding their behaviors is crucial to facilitating the development of
safe driving systems. Although recent progress has explored various aspects of
human behavior$\unicode{x2014}$such as motion, trajectories, and
intention$\unicode{x2014}$a comprehensive benchmark for evaluating human
behavior understanding in autonomous driving remains unavailable. In this work,
we propose $\textbf{MMHU}$, a large-scale benchmark for human behavior analysis
featuring rich annotations, such as human motion and trajectories, text
description for human motions, human intention, and critical behavior labels
relevant to driving safety. Our dataset encompasses 57k human motion clips and
1.73M frames gathered from diverse sources, including established driving
datasets such as Waymo, in-the-wild videos from YouTube, and self-collected
data. A human-in-the-loop annotation pipeline is developed to generate rich
behavior captions. We provide a thorough dataset analysis and benchmark
multiple tasks$\unicode{x2014}$ranging from motion prediction to motion
generation and human behavior question answering$\unicode{x2014}$thereby
offering a broad evaluation suite. Project page :
https://MMHU-Benchmark.github.io.

</details>


### [85] [CytoSAE: Interpretable Cell Embeddings for Hematology](https://arxiv.org/abs/2507.12464)
*Muhammed Furkan Dasdelen,Hyesu Lim,Michele Buck,Katharina S. Götze,Carsten Marr,Steffen Schneider*

Main category: cs.CV

TL;DR: 稀疏自编码器（SAEs）用于医学图像领域，提出CytoSAE模型，可识别形态学相关概念并验证其效果。


<details>
  <summary>Details</summary>
Motivation: 医学影像领域缺乏解释性工具，SAEs在视觉领域的成功应用启发了其在医学图像中的探索。

Method: 提出CytoSAE模型，基于40,000多张外周血单细胞图像训练，验证其在骨髓细胞学等领域的泛化能力。

Result: CytoSAE能识别形态学相关概念，并在AML亚型分类任务中达到与SOTA相当的性能。

Conclusion: CytoSAE为医学图像提供了一种可解释的工具，支持患者和疾病特异性概念生成。

Abstract: Sparse autoencoders (SAEs) emerged as a promising tool for mechanistic
interpretability of transformer-based foundation models. Very recently, SAEs
were also adopted for the visual domain, enabling the discovery of visual
concepts and their patch-wise attribution to tokens in the transformer model.
While a growing number of foundation models emerged for medical imaging, tools
for explaining their inferences are still lacking. In this work, we show the
applicability of SAEs for hematology. We propose CytoSAE, a sparse autoencoder
which is trained on over 40,000 peripheral blood single-cell images. CytoSAE
generalizes to diverse and out-of-domain datasets, including bone marrow
cytology, where it identifies morphologically relevant concepts which we
validated with medical experts. Furthermore, we demonstrate scenarios in which
CytoSAE can generate patient-specific and disease-specific concepts, enabling
the detection of pathognomonic cells and localized cellular abnormalities at
the patch level. We quantified the effect of concepts on a patient-level AML
subtype classification task and show that CytoSAE concepts reach performance
comparable to the state-of-the-art, while offering explainability on the
sub-cellular level. Source code and model weights are available at
https://github.com/dynamical-inference/cytosae.

</details>


### [86] [PhysX: Physical-Grounded 3D Asset Generation](https://arxiv.org/abs/2507.12465)
*Ziang Cao,Zhaoxi Chen,Linag Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 论文提出PhysX，一种物理基础的3D资产生成方法，包括数据集PhysXNet和生成框架PhysXGen，填补了物理属性标注的空白。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型忽视物理属性，限制了在仿真和具身AI等领域的应用。

Method: 1) 构建PhysXNet数据集，标注物理属性；2) 提出PhysXGen框架，结合3D结构与物理知识。

Result: 实验验证了PhysXGen的优越性能和泛化能力。

Conclusion: PhysX为生成物理AI提供了新范式，代码和数据将开源。

Abstract: 3D modeling is moving from virtual to physical. Existing 3D generation
primarily emphasizes geometries and textures while neglecting physical-grounded
modeling. Consequently, despite the rapid development of 3D generative models,
the synthesized 3D assets often overlook rich and important physical
properties, hampering their real-world application in physical domains like
simulation and embodied AI. As an initial attempt to address this challenge, we
propose \textbf{PhysX}, an end-to-end paradigm for physical-grounded 3D asset
generation. 1) To bridge the critical gap in physics-annotated 3D datasets, we
present PhysXNet - the first physics-grounded 3D dataset systematically
annotated across five foundational dimensions: absolute scale, material,
affordance, kinematics, and function description. In particular, we devise a
scalable human-in-the-loop annotation pipeline based on vision-language models,
which enables efficient creation of physics-first assets from raw 3D assets.2)
Furthermore, we propose \textbf{PhysXGen}, a feed-forward framework for
physics-grounded image-to-3D asset generation, injecting physical knowledge
into the pre-trained 3D structural space. Specifically, PhysXGen employs a
dual-branch architecture to explicitly model the latent correlations between 3D
structures and physical properties, thereby producing 3D assets with plausible
physical predictions while preserving the native geometry quality. Extensive
experiments validate the superior performance and promising generalization
capability of our framework. All the code, data, and models will be released to
facilitate future research in generative physical AI.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [87] [Landmark Detection for Medical Images using a General-purpose Segmentation Model](https://arxiv.org/abs/2507.11551)
*Ekaterina Stansfield,Jennifer A. Mitterer,Abdulrahman Altahhan*

Main category: eess.IV

TL;DR: 提出了一种结合YOLO和SAM的混合模型，用于精确分割骨科骨盆X光片中的解剖标志和复杂轮廓。


<details>
  <summary>Details</summary>
Motivation: 现有通用分割模型（如SAM）无法直接用于医学影像中的解剖标志分割，而医学专用模型（如MedSAM）又缺乏对骨科骨盆标志的精细识别能力。

Method: 利用YOLO检测解剖标志并生成边界框，作为SAM的输入提示，结合两者的优势构建混合模型。

Result: 混合模型在分割8个解剖标志的基础上，扩展到72个标志和16个复杂区域（如股骨皮质骨和骨盆入口），表现出色。

Conclusion: YOLO和SAM的结合为骨科骨盆X光片的解剖标志分割提供了高效且精确的解决方案。

Abstract: Radiographic images are a cornerstone of medical diagnostics in orthopaedics,
with anatomical landmark detection serving as a crucial intermediate step for
information extraction. General-purpose foundational segmentation models, such
as SAM (Segment Anything Model), do not support landmark segmentation out of
the box and require prompts to function. However, in medical imaging, the
prompts for landmarks are highly specific. Since SAM has not been trained to
recognize such landmarks, it cannot generate accurate landmark segmentations
for diagnostic purposes. Even MedSAM, a medically adapted variant of SAM, has
been trained to identify larger anatomical structures, such as organs and their
parts, and lacks the fine-grained precision required for orthopaedic pelvic
landmarks. To address this limitation, we propose leveraging another
general-purpose, non-foundational model: YOLO. YOLO excels in object detection
and can provide bounding boxes that serve as input prompts for SAM. While YOLO
is efficient at detection, it is significantly outperformed by SAM in
segmenting complex structures. In combination, these two models form a reliable
pipeline capable of segmenting not only a small pilot set of eight anatomical
landmarks but also an expanded set of 72 landmarks and 16 regions with complex
outlines, such as the femoral cortical bone and the pelvic inlet. By using
YOLO-generated bounding boxes to guide SAM, we trained the hybrid model to
accurately segment orthopaedic pelvic radiographs. Our results show that the
proposed combination of YOLO and SAM yields excellent performance in detecting
anatomical landmarks and intricate outlines in orthopaedic pelvic radiographs.

</details>


### [88] [3D Wavelet Latent Diffusion Model for Whole-Body MR-to-CT Modality Translation](https://arxiv.org/abs/2507.11557)
*Jiaxu Zheng,Meiman He,Xuhui Tang,Xiong Wang,Tuoyu Cao,Tianyi Zeng,Lichi Zhang,Chenyu You*

Main category: eess.IV

TL;DR: 提出了一种新型3D小波潜在扩散模型（3D-WLDM），用于从MR图像合成高质量的CT图像，解决了现有方法在空间对齐和图像质量上的不足。


<details>
  <summary>Details</summary>
Motivation: MR成像在临床诊断和治疗中至关重要，但现有MR-to-CT合成方法在空间对齐和图像质量上存在问题，影响下游临床应用。

Method: 采用3D小波潜在扩散模型，结合小波残差模块和双跳跃连接注意力机制，在潜在空间中分离结构和模态特征，提高图像质量。

Result: 生成的CT图像在骨结构和软组织对比度上表现更优，适用于临床任务。

Conclusion: 3D-WLDM显著提升了MR-to-CT合成的准确性和图像质量，具有临床实用价值。

Abstract: Magnetic Resonance (MR) imaging plays an essential role in contemporary
clinical diagnostics. It is increasingly integrated into advanced therapeutic
workflows, such as hybrid Positron Emission Tomography/Magnetic Resonance
(PET/MR) imaging and MR-only radiation therapy. These integrated approaches are
critically dependent on accurate estimation of radiation attenuation, which is
typically facilitated by synthesizing Computed Tomography (CT) images from MR
scans to generate attenuation maps. However, existing MR-to-CT synthesis
methods for whole-body imaging often suffer from poor spatial alignment between
the generated CT and input MR images, and insufficient image quality for
reliable use in downstream clinical tasks. In this paper, we present a novel 3D
Wavelet Latent Diffusion Model (3D-WLDM) that addresses these limitations by
performing modality translation in a learned latent space. By incorporating a
Wavelet Residual Module into the encoder-decoder architecture, we enhance the
capture and reconstruction of fine-scale features across image and latent
spaces. To preserve anatomical integrity during the diffusion process, we
disentangle structural and modality-specific characteristics and anchor the
structural component to prevent warping. We also introduce a Dual Skip
Connection Attention mechanism within the diffusion model, enabling the
generation of high-resolution CT images with improved representation of bony
structures and soft-tissue contrast.

</details>


### [89] [Predicting Pulmonary Hypertension in Newborns: A Multi-view VAE Approach](https://arxiv.org/abs/2507.11561)
*Lucas Erlacher,Samuel Ruipérez-Campillo,Holger Michel,Sven Wellmann,Thomas M. Sutter,Ece Ozkan,Julia E. Vogt*

Main category: eess.IV

TL;DR: 该论文提出了一种基于多视角变分自编码器（VAE）的方法，用于新生儿肺动脉高压（PH）的预测，通过多视角超声心动图视频提高特征提取和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 新生儿肺动脉高压（PH）的诊断依赖超声心动图，但其准确性受操作者主观性影响，且现有自动化方法多针对成人，单视角模型性能有限。

Method: 采用多视角变分自编码器（VAE）框架，从超声心动图视频中提取复杂潜在表征，并与单视角和监督学习方法进行对比。

Result: 结果显示多视角学习方法在泛化性和分类准确性上优于单视角和监督学习方法。

Conclusion: 多视角学习能有效提升新生儿PH评估的鲁棒性和准确性。

Abstract: Pulmonary hypertension (PH) in newborns is a critical condition characterized
by elevated pressure in the pulmonary arteries, leading to right ventricular
strain and heart failure. While right heart catheterization (RHC) is the
diagnostic gold standard, echocardiography is preferred due to its non-invasive
nature, safety, and accessibility. However, its accuracy highly depends on the
operator, making PH assessment subjective. While automated detection methods
have been explored, most models focus on adults and rely on single-view
echocardiographic frames, limiting their performance in diagnosing PH in
newborns. While multi-view echocardiography has shown promise in improving PH
assessment, existing models struggle with generalizability. In this work, we
employ a multi-view variational autoencoder (VAE) for PH prediction using
echocardiographic videos. By leveraging the VAE framework, our model captures
complex latent representations, improving feature extraction and robustness. We
compare its performance against single-view and supervised learning approaches.
Our results show improved generalization and classification accuracy,
highlighting the effectiveness of multi-view learning for robust PH assessment
in newborns.

</details>


### [90] [Are Vision Foundation Models Ready for Out-of-the-Box Medical Image Registration?](https://arxiv.org/abs/2507.11569)
*Hanxue Gu,Yaqian Chen,Nicholas Konz,Qihang Li,Maciej A. Mazurowski*

Main category: eess.IV

TL;DR: 该研究评估了基于基础模型的图像配准算法在乳腺MRI中的应用，发现某些模型（如SAM）在整体配准上优于传统方法，但在细节捕捉上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型在复杂、可变形解剖结构（如乳腺MRI）中的配准能力，填补现有研究在挑战性场景中的空白。

Method: 评估五种预训练编码器（DINO-v2、SAM、MedSAM、SSLSAM和MedCLIP）在四种乳腺配准任务中的表现。

Result: SAM等基础模型在整体配准上表现优异，但对纤维腺体组织的细节配准效果不佳；医学或乳腺特定图像的额外预训练未提升性能。

Conclusion: 需进一步研究领域特定训练对配准的影响，并开发提升全局配准和细节准确性的策略。

Abstract: Foundation models, pre-trained on large image datasets and capable of
capturing rich feature representations, have recently shown potential for
zero-shot image registration. However, their performance has mostly been tested
in the context of rigid or less complex structures, such as the brain or
abdominal organs, and it remains unclear whether these models can handle more
challenging, deformable anatomy. Breast MRI registration is particularly
difficult due to significant anatomical variation between patients, deformation
caused by patient positioning, and the presence of thin and complex internal
structure of fibroglandular tissue, where accurate alignment is crucial.
Whether foundation model-based registration algorithms can address this level
of complexity remains an open question. In this study, we provide a
comprehensive evaluation of foundation model-based registration algorithms for
breast MRI. We assess five pre-trained encoders, including DINO-v2, SAM,
MedSAM, SSLSAM, and MedCLIP, across four key breast registration tasks that
capture variations in different years and dates, sequences, modalities, and
patient disease status (lesion versus no lesion). Our results show that
foundation model-based algorithms such as SAM outperform traditional
registration baselines for overall breast alignment, especially under large
domain shifts, but struggle with capturing fine details of fibroglandular
tissue. Interestingly, additional pre-training or fine-tuning on medical or
breast-specific images in MedSAM and SSLSAM, does not improve registration
performance and may even decrease it in some cases. Further work is needed to
understand how domain-specific training influences registration and to explore
targeted strategies that improve both global alignment and fine structure
accuracy. We also publicly release our code at
\href{https://github.com/mazurowski-lab/Foundation-based-reg}{Github}.

</details>


### [91] [A Composite Alignment-Aware Framework for Myocardial Lesion Segmentation in Multi-sequence CMR Images](https://arxiv.org/abs/2507.11886)
*Yifan Gao,Shaohao Rui,Haoyang Su,Jinyi Xiang,Lianming Wu,Xiaosong Wang*

Main category: eess.IV

TL;DR: CAA-Seg是一种两阶段框架，用于多序列心脏MRI中精确分割心肌病变，通过选择性切片对齐和分层对齐网络解决模态间强度变化和空间错位问题。


<details>
  <summary>Details</summary>
Motivation: 多序列心脏MRI中强度变化和空间错位导致特征对应困难，影响心肌病变分割的准确性。

Method: 提出CAA-Seg框架，包括选择性切片对齐和分层对齐网络，分别处理空间对应和语义融合。

Result: 在397名患者的数据集上验证，CAA-Seg在心肌梗死分割中表现优异，比现有方法提升5.54%。

Conclusion: CAA-Seg通过两阶段方法有效解决了多序列MRI的分割挑战，性能显著优于现有技术。

Abstract: Accurate segmentation of myocardial lesions from multi-sequence cardiac
magnetic resonance imaging is essential for cardiac disease diagnosis and
treatment planning. However, achieving optimal feature correspondence is
challenging due to intensity variations across modalities and spatial
misalignment caused by inconsistent slice acquisition protocols. We propose
CAA-Seg, a composite alignment-aware framework that addresses these challenges
through a two-stage approach. First, we introduce a selective slice alignment
method that dynamically identifies and aligns anatomically corresponding slice
pairs while excluding mismatched sections, ensuring reliable spatial
correspondence between sequences. Second, we develop a hierarchical alignment
network that processes multi-sequence features at different semantic levels,
i.e., local deformation correction modules address geometric variations in
low-level features, while global semantic fusion blocks enable semantic fusion
at high levels where intensity discrepancies diminish. We validate our method
on a large-scale dataset comprising 397 patients. Experimental results show
that our proposed CAA-Seg achieves superior performance on most evaluation
metrics, with particularly strong results in myocardial infarction
segmentation, representing a substantial 5.54% improvement over
state-of-the-art approaches. The code is available at
https://github.com/yifangao112/CAA-Seg.

</details>


### [92] [Constructed Realities? Technical and Contextual Anomalies in a High-Profile Image](https://arxiv.org/abs/2507.12237)
*Matthias Wjst*

Main category: eess.IV

TL;DR: 对一张涉及安德鲁王子、弗吉尼亚·朱弗雷和吉斯莱恩·麦克斯韦的照片进行法医分析，发现其可能存在数字合成的迹象，但无法确定。


<details>
  <summary>Details</summary>
Motivation: 该照片在公共讨论和法律叙事中起关键作用，但其真实性存疑。

Method: 分析多个公开版本的照片，发现光线、姿势和身体互动的不一致。

Result: 技术异常表明照片可能是人为构造，但缺乏原始底片和可验证证据。

Conclusion: 照片的真实性未定，但在虐待、记忆和真相争议的复杂故事中具有象征意义。

Abstract: This study offers a forensic assessment of a widely circulated photograph
featuring Prince Andrew, Virginia Giuffre, and Ghislaine Maxwell - an image
that has played a pivotal role in public discourse and legal narratives.
Through analysis of multiple published versions, several inconsistencies are
identified, including irregularities in lighting, posture, and physical
interaction, which are more consistent with digital compositing than with an
unaltered snapshot. While the absence of the original negative and a verifiable
audit trail precludes definitive conclusions, the technical and contextual
anomalies suggest that the image may have been deliberately constructed.
Nevertheless, without additional evidence, the photograph remains an unresolved
but symbolically charged fragment within a complex story of abuse, memory, and
contested truth.

</details>


### [93] [Unit-Based Histopathology Tissue Segmentation via Multi-Level Feature Representation](https://arxiv.org/abs/2507.12427)
*Ashkan Shakarami,Azade Farshad,Yousef Yeganeh,Lorenzo Nicole,Peter Schuffler,Stefano Ghidoni,Nassir Navab*

Main category: eess.IV

TL;DR: UTS是一种基于单元的组织分割框架，通过分类固定大小的32*32瓦片而非单个像素，减少标注工作并提高计算效率，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 减少组织病理学分割中的标注工作和计算成本，同时保持高精度。

Method: 提出多级视觉变换器（L-ViT），利用多级特征表示捕捉细粒度形态和全局组织上下文。

Result: 在459个H&E染色区域的386,371个瓦片上评估，UTS优于U-Net变体和基于变换器的基线方法。

Conclusion: UTS在减少标注和计算负担的同时，实现了高精度的组织分割，支持临床相关任务。

Abstract: We propose UTS, a unit-based tissue segmentation framework for histopathology
that classifies each fixed-size 32 * 32 tile, rather than each pixel, as the
segmentation unit. This approach reduces annotation effort and improves
computational efficiency without compromising accuracy. To implement this
approach, we introduce a Multi-Level Vision Transformer (L-ViT), which benefits
the multi-level feature representation to capture both fine-grained morphology
and global tissue context. Trained to segment breast tissue into three
categories (infiltrating tumor, non-neoplastic stroma, and fat), UTS supports
clinically relevant tasks such as tumor-stroma quantification and surgical
margin assessment. Evaluated on 386,371 tiles from 459 H&E-stained regions, it
outperforms U-Net variants and transformer-based baselines. Code and Dataset
will be available at GitHub.

</details>


### [94] [Energy-based models for inverse imaging problems](https://arxiv.org/abs/2507.12432)
*Andreas Habring,Martin Holler,Thomas Pock,Martin Zach*

Main category: eess.IV

TL;DR: 本章详细介绍了基于能量的模型（EBMs）在逆成像问题中的应用，包括理论背景、学习方法和采样算法，并展示了数值结果。


<details>
  <summary>Details</summary>
Motivation: 探讨EBMs在逆成像问题中的潜力，提供理论支持和实用方法。

Method: 介绍了Gibbs密度建模、贝叶斯逆问题的理论框架，以及Metropolis-Hastings、Gibbs采样、Langevin Monte Carlo和Hamiltonian Monte Carlo等采样算法。

Result: 通过数值实验验证了EBMs在逆成像问题中的有效性。

Conclusion: EBMs为逆成像问题提供了强大的建模工具，理论支持与实验结果均表明其潜力。

Abstract: In this chapter we provide a thorough overview of the use of energy-based
models (EBMs) in the context of inverse imaging problems. EBMs are probability
distributions modeled via Gibbs densities $p(x) \propto \exp{-E(x)}$ with an
appropriate energy functional $E$. Within this chapter we present a rigorous
theoretical introduction to Bayesian inverse problems that includes results on
well-posedness and stability in the finite-dimensional and infinite-dimensional
setting. Afterwards we discuss the use of EBMs for Bayesian inverse problems
and explain the most relevant techniques for learning EBMs from data. As a
crucial part of Bayesian inverse problems, we cover several popular algorithms
for sampling from EBMs, namely the Metropolis-Hastings algorithm, Gibbs
sampling, Langevin Monte Carlo, and Hamiltonian Monte Carlo. Moreover, we
present numerical results for the resolution of several inverse imaging
problems obtained by leveraging an EBM that allows for the explicit
verification of those properties that are needed for valid energy-based
modeling.

</details>


### [95] [CompressedVQA-HDR: Generalized Full-reference and No-reference Quality Assessment Models for Compressed High Dynamic Range Videos](https://arxiv.org/abs/2507.11900)
*Wei Sun,Linhan Cao,Kang Fu,Dandan Zhu,Jun Jia,Menghan Hu,Xiongkuo Min,Guangtao Zhai*

Main category: eess.IV

TL;DR: 论文提出CompressedVQA-HDR框架，针对HDR视频质量评估，结合Swin Transformer和SigLip 2网络，通过预训练和微调策略，在FR和NR模型上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频质量评估方法对HDR内容泛化能力不足，需开发更有效的评估框架。

Method: FR模型使用Swin Transformer提取结构纹理相似性；NR模型利用SigLip 2提取全局特征。通过预训练和混合数据集训练优化性能。

Result: 模型在FR和NR任务中表现最优，CompressedVQA-HDR-FR在IEEE ICME 2025挑战赛中获第一名。

Conclusion: CompressedVQA-HDR框架有效解决了HDR视频质量评估的挑战，具有广泛适用性。

Abstract: Video compression is a standard procedure applied to all videos to minimize
storage and transmission demands while preserving visual quality as much as
possible. Therefore, evaluating the visual quality of compressed videos is
crucial for guiding the practical usage and further development of video
compression algorithms. Although numerous compressed video quality assessment
(VQA) methods have been proposed, they often lack the generalization capability
needed to handle the increasing diversity of video types, particularly high
dynamic range (HDR) content. In this paper, we introduce CompressedVQA-HDR, an
effective VQA framework designed to address the challenges of HDR video quality
assessment. Specifically, we adopt the Swin Transformer and SigLip 2 as the
backbone networks for the proposed full-reference (FR) and no-reference (NR)
VQA models, respectively. For the FR model, we compute deep structural and
textural similarities between reference and distorted frames using
intermediate-layer features extracted from the Swin Transformer as its
quality-aware feature representation. For the NR model, we extract the global
mean of the final-layer feature maps from SigLip 2 as its quality-aware
representation. To mitigate the issue of limited HDR training data, we
pre-train the FR model on a large-scale standard dynamic range (SDR) VQA
dataset and fine-tune it on the HDRSDR-VQA dataset. For the NR model, we employ
an iterative mixed-dataset training strategy across multiple compressed VQA
datasets, followed by fine-tuning on the HDRSDR-VQA dataset. Experimental
results show that our models achieve state-of-the-art performance compared to
existing FR and NR VQA models. Moreover, CompressedVQA-HDR-FR won first place
in the FR track of the Generalizable HDR & SDR Video Quality Measurement Grand
Challenge at IEEE ICME 2025. The code is available at
https://github.com/sunwei925/CompressedVQA-HDR.

</details>


### [96] [Identifying Signatures of Image Phenotypes to Track Treatment Response in Liver Disease](https://arxiv.org/abs/2507.12012)
*Matthias Perkonigg,Nina Bastati,Ahmed Ba-Ssalamah,Peter Mesenbrink,Alexander Goehler,Miljen Martic,Xiaofei Zhou,Michael Trauner,Georg Langs*

Main category: eess.IV

TL;DR: 无监督机器学习通过深度聚类网络识别肝脏MRI图像中的组织模式，量化弥漫性肝病的治疗反应，并在临床试验中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 量化与疾病进展和治疗反应相关的图像模式，为个体化治疗和新疗法开发提供工具。

Method: 使用深度聚类网络对医学图像块进行编码和聚类，建立组织词汇表，捕捉治疗相关的组织变化及其位置。

Result: 方法在非酒精性脂肪性肝炎患者的随机对照试验中验证，能识别治疗相关的组织变化路径，并优于非影像学指标。

Conclusion: 该方法通过非侵入性影像数据预测活检特征，并在独立队列中验证，展示了其临床应用潜力。

Abstract: Quantifiable image patterns associated with disease progression and treatment
response are critical tools for guiding individual treatment, and for
developing novel therapies. Here, we show that unsupervised machine learning
can identify a pattern vocabulary of liver tissue in magnetic resonance images
that quantifies treatment response in diffuse liver disease. Deep clustering
networks simultaneously encode and cluster patches of medical images into a
low-dimensional latent space to establish a tissue vocabulary. The resulting
tissue types capture differential tissue change and its location in the liver
associated with treatment response. We demonstrate the utility of the
vocabulary on a randomized controlled trial cohort of non-alcoholic
steatohepatitis patients. First, we use the vocabulary to compare longitudinal
liver change in a placebo and a treatment cohort. Results show that the method
identifies specific liver tissue change pathways associated with treatment, and
enables a better separation between treatment groups than established
non-imaging measures. Moreover, we show that the vocabulary can predict biopsy
derived features from non-invasive imaging data. We validate the method on a
separate replication cohort to demonstrate the applicability of the proposed
method.

</details>


### [97] [Benchmarking and Explaining Deep Learning Cortical Lesion MRI Segmentation in Multiple Sclerosis](https://arxiv.org/abs/2507.12092)
*Nataliia Molchanova,Alessandro Cagol,Mario Ocampo-Pineda,Po-Jui Lu,Matthias Weigel,Xinjie Chen,Erin Beck,Charidimos Tsagkas,Daniel Reich,Colin Vanden Bulcke,Anna Stolting,Serena Borrelli,Pietro Maggi,Adrien Depeursinge,Cristina Granziera,Henning Mueller,Pedro M. Gordaliza,Meritxell Bach Cuadra*

Main category: eess.IV

TL;DR: 提出了一种用于多发性硬化症皮质病变检测和分割的多中心基准测试方法，基于nnU-Net框架，展示了较强的检测能力，并公开了模型和实现。


<details>
  <summary>Details</summary>
Motivation: 皮质病变在多发性硬化症中具有重要诊断和预后价值，但由于MRI表现不明显、专家标注困难及缺乏标准化方法，临床应用受限。

Method: 使用656个MRI扫描数据，基于nnU-Net框架进行改进，并通过域内外测试评估模型泛化能力。

Result: 模型在域内和域外的F1分数分别为0.64和0.5，分析了数据变异性、病变模糊性和协议差异对性能的影响。

Conclusion: 研究为临床应用中数据变异性等问题提供了建议，并公开了模型以促进可重复性。

Abstract: Cortical lesions (CLs) have emerged as valuable biomarkers in multiple
sclerosis (MS), offering high diagnostic specificity and prognostic relevance.
However, their routine clinical integration remains limited due to subtle
magnetic resonance imaging (MRI) appearance, challenges in expert annotation,
and a lack of standardized automated methods. We propose a comprehensive
multi-centric benchmark of CL detection and segmentation in MRI. A total of 656
MRI scans, including clinical trial and research data from four institutions,
were acquired at 3T and 7T using MP2RAGE and MPRAGE sequences with
expert-consensus annotations. We rely on the self-configuring nnU-Net
framework, designed for medical imaging segmentation, and propose adaptations
tailored to the improved CL detection. We evaluated model generalization
through out-of-distribution testing, demonstrating strong lesion detection
capabilities with an F1-score of 0.64 and 0.5 in and out of the domain,
respectively. We also analyze internal model features and model errors for a
better understanding of AI decision-making. Our study examines how data
variability, lesion ambiguity, and protocol differences impact model
performance, offering future recommendations to address these barriers to
clinical adoption. To reinforce the reproducibility, the implementation and
models will be publicly accessible and ready to use at
https://github.com/Medical-Image-Analysis-Laboratory/ and
https://doi.org/10.5281/zenodo.15911797.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [98] [Real-Time Cloth Simulation Using WebGPU: Evaluating Limits of High-Resolution](https://arxiv.org/abs/2507.11794)
*Nak-Jun Sung,Jun Ma,TaeHeon Kim,Yoo-joo Choi,Min-Hyung Choi,Min Hong*

Main category: cs.GR

TL;DR: WebGPU显著提升了实时布料模拟的性能，相比WebGL，在高分辨率下仍能保持60fps，并能处理大规模碰撞检测。


<details>
  <summary>Details</summary>
Motivation: 探索WebGPU在实时布料模拟中的潜力，解决传统WebGL在复杂物理模拟中的性能瓶颈。

Method: 基于WebGPU框架实现质量-弹簧法的布料模拟系统，集成碰撞检测与响应处理。

Result: WebGPU在高分辨率（640K节点）下保持60fps，并能实时处理4K至100K节点与100K三角形表面的碰撞。

Conclusion: WebGPU在实时布料模拟中表现优异，平衡了性能与真实感渲染，适合复杂场景。

Abstract: This study explores the capabilities of WebGPU, an emerging web graphics
paradigm, for real-time cloth simulation. Traditional WebGL-based methods have
been in handling complex physical simulations due to their emphasis on graphics
rendering rather than general-purpose GPU (GPGPU) operations. WebGPU, designed
to provide modern 3D graphics and computational capabilities, offers
significant improvements through parallel processing and support for
computational shaders. In this work, we implemented a cloth simulation system
using the Mass-Spring Method within the WebGPU framework, integrating collision
detection and response handling with the 3D surface model. First, comparative
performance evaluations demonstrate that WebGPU substantially outperforms
WebGL, particularly in high-resolution simulations, maintaining 60 frames per
second (fps) even with up to 640K nodes. The second experiment aimed to
determine the real-time limitations of WebGPU and confirmed that WebGPU can
handle real-time collisions between 4K and 100k cloth node models and a 100K
triangle surface model in real-time. These experiments also highlight the
importance of balancing real-time performance with realistic rendering when
handling collisions between cloth models and complex 3D objects. Our source
code is available at https://github.com/nakjun/Cloth-Simulation-WebGPU

</details>


### [99] [Measuring and predicting visual fidelity](https://arxiv.org/abs/2507.11857)
*Benjamin Watson,Alinda Friedman,Aaron McGaffey*

Main category: cs.GR

TL;DR: 研究测量和预测视觉保真度的技术，使用多边形模型作为视觉刺激，通过两种简化算法改变保真度，并比较命名时间、评分和偏好三种实验方法。


<details>
  <summary>Details</summary>
Motivation: 探讨不同实验技术和自动方法在测量和预测视觉保真度方面的有效性。

Method: 使用两种模型简化算法处理动物和人造物多边形模型，通过命名时间、评分和偏好三种实验方法测量保真度变化，并尝试用基于图像和模型的自动技术预测实验结果。

Result: 实验方法对简化类型和程度敏感，但对物体类型的反应不同；自动方法能较好预测评分，但对偏好和命名时间的预测效果较差。

Conclusion: 提出了实验和自动测量视觉保真度的改进建议。

Abstract: This paper is a study of techniques for measuring and predicting visual
fidelity. As visual stimuli we use polygonal models, and vary their fidelity
with two different model simplification algorithms. We also group the stimuli
into two object types: animals and man made artifacts. We examine three
different experimental techniques for measuring these fidelity changes: naming
times, ratings, and preferences. All the measures were sensitive to the type of
simplification and level of simplification. However, the measures differed from
one another in their response to object type. We also examine several automatic
techniques for predicting these experimental measures, including techniques
based on images and on the models themselves. Automatic measures of fidelity
were successful at predicting experimental ratings, less successful at
predicting preferences, and largely failures at predicting naming times. We
conclude with suggestions for use and improvement of the experimental and
automatic measures of visual fidelity.

</details>


### [100] [MOSPA: Human Motion Generation Driven by Spatial Audio](https://arxiv.org/abs/2507.11949)
*Shuyang Xu,Zhiyang Dou,Mingyi Shi,Liang Pan,Leo Ho,Jingbo Wang,Yuan Liu,Cheng Lin,Yuexin Ma,Wenping Wang,Taku Komura*

Main category: cs.GR

TL;DR: 论文提出了一种基于空间音频驱动的人体运动生成方法（MOSPA），并发布了首个全面的空间音频与运动数据集（SAM），填补了现有研究在空间音频对运动影响方面的空白。


<details>
  <summary>Details</summary>
Motivation: 虚拟人类如何动态且真实地响应多样化的听觉刺激是角色动画中的关键挑战，但现有研究多忽略空间音频信号对运动的影响。

Method: 提出了一种基于扩散模型的生成框架（MOSPA），通过有效融合机制捕捉身体运动与空间音频的关系。

Result: MOSPA能根据不同的空间音频输入生成多样且真实的运动，实验表明其在该任务上达到最先进性能。

Conclusion: 论文填补了空间音频驱动运动生成的研究空白，提出的数据集和模型将为未来研究提供重要支持。

Abstract: Enabling virtual humans to dynamically and realistically respond to diverse
auditory stimuli remains a key challenge in character animation, demanding the
integration of perceptual modeling and motion synthesis. Despite its
significance, this task remains largely unexplored. Most previous works have
primarily focused on mapping modalities like speech, audio, and music to
generate human motion. As of yet, these models typically overlook the impact of
spatial features encoded in spatial audio signals on human motion. To bridge
this gap and enable high-quality modeling of human movements in response to
spatial audio, we introduce the first comprehensive Spatial Audio-Driven Human
Motion (SAM) dataset, which contains diverse and high-quality spatial audio and
motion data. For benchmarking, we develop a simple yet effective
diffusion-based generative framework for human MOtion generation driven by
SPatial Audio, termed MOSPA, which faithfully captures the relationship between
body motion and spatial audio through an effective fusion mechanism. Once
trained, MOSPA could generate diverse realistic human motions conditioned on
varying spatial audio inputs. We perform a thorough investigation of the
proposed dataset and conduct extensive experiments for benchmarking, where our
method achieves state-of-the-art performance on this task. Our model and
dataset will be open-sourced upon acceptance. Please refer to our supplementary
video for more details.

</details>


### [101] [HPR3D: Hierarchical Proxy Representation for High-Fidelity 3D Reconstruction and Controllable Editing](https://arxiv.org/abs/2507.11971)
*Tielong Wang,Yuxuan Xiong,Jinfan Liu,Zhifan Zhang,Ye Chen,Yue Shi,Bingbing Ni*

Main category: cs.GR

TL;DR: 提出了一种新型的3D分层代理节点表示方法，解决了现有3D表示方法在通用性、编辑性和复杂性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D表示方法（如网格、体素、点云和NeRF）存在任务专用性强、编辑复杂、渲染与结构模糊等问题，缺乏通用性。

Method: 通过稀疏的分层组织（树结构）代理节点表示物体的形状和纹理，每个节点存储局部信息，并通过轻量级解码实现高效查询。

Result: 实验表明，该方法在3D重建和编辑中表现出高效表达、高保真渲染质量和卓越的可编辑性。

Conclusion: 该3D分层代理节点表示方法在通用性、编辑性和复杂性方面优于现有技术。

Abstract: Current 3D representations like meshes, voxels, point clouds, and NeRF-based
neural implicit fields exhibit significant limitations: they are often
task-specific, lacking universal applicability across reconstruction,
generation, editing, and driving. While meshes offer high precision, their
dense vertex data complicates editing; NeRFs deliver excellent rendering but
suffer from structural ambiguity, hindering animation and manipulation; all
representations inherently struggle with the trade-off between data complexity
and fidelity. To overcome these issues, we introduce a novel 3D Hierarchical
Proxy Node representation. Its core innovation lies in representing an object's
shape and texture via a sparse set of hierarchically organized
(tree-structured) proxy nodes distributed on its surface and interior. Each
node stores local shape and texture information (implicitly encoded by a small
MLP) within its neighborhood. Querying any 3D coordinate's properties involves
efficient neural interpolation and lightweight decoding from relevant nearby
and parent nodes. This framework yields a highly compact representation where
nodes align with local semantics, enabling direct drag-and-edit manipulation,
and offers scalable quality-complexity control. Extensive experiments across 3D
reconstruction and editing demonstrate our method's expressive efficiency,
high-fidelity rendering quality, and superior editability.

</details>


### [102] [SmokeSVD: Smoke Reconstruction from A Single View via Progressive Novel View Synthesis and Refinement with Diffusion Models](https://arxiv.org/abs/2507.12156)
*Chen Li,Shanshan Dong,Sheng Qiu,Jianmin Han,Zan Gao,Kemeng Huang,Taku Komura*

Main category: cs.GR

TL;DR: SmokeSVD框架通过结合扩散模型和物理引导优化，从单视频高效重建动态烟雾，解决了稀疏视图下流体重建的挑战。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图下动态流体重建因3D信息不足而困难，现有方法受限于耗时优化和病态条件。

Method: 结合扩散模型生成侧视图，通过物理引导优化逐步生成多视角图像序列，最终利用Navier-Stokes方程重建精细密度和速度场。

Result: 实验表明，SmokeSVD在重建质量上优于现有技术。

Conclusion: SmokeSVD通过生成与物理一致的动态烟雾，实现了高效高质量的重建。

Abstract: Reconstructing dynamic fluids from sparse views is a long-standing and
challenging problem, due to the severe lack of 3D information from insufficient
view coverage. While several pioneering approaches have attempted to address
this issue using differentiable rendering or novel view synthesis, they are
often limited by time-consuming optimization and refinement processes under
ill-posed conditions. To tackle above challenges, we propose SmokeSVD, an
efficient and effective framework to progressively generate and reconstruct
dynamic smoke from a single video by integrating both the powerful generative
capabilities from diffusion models and physically guided consistency
optimization towards realistic appearance and dynamic evolution. Specifically,
we first propose a physically guided side-view synthesizer based on diffusion
models, which explicitly incorporates divergence and gradient guidance of
velocity fields to generate visually realistic and spatio-temporally consistent
side-view images frame by frame, significantly alleviating the ill-posedness of
single-view reconstruction without imposing additional constraints.
Subsequently, we determine a rough estimation of density field from the pair of
front-view input and side-view synthetic image, and further refine 2D blurry
novel-view images and 3D coarse-grained density field through an iterative
process that progressively renders and enhances the images from increasing
novel viewing angles, generating high-quality multi-view image sequences.
Finally, we reconstruct and estimate the fine-grained density field, velocity
field, and smoke source via differentiable advection by leveraging the
Navier-Stokes equations. Extensive quantitative and qualitative experiments
show that our approach achieves high-quality reconstruction and outperforms
previous state-of-the-art techniques.

</details>


### [103] [Shape Adaptation for 3D Hairstyle Retargeting](https://arxiv.org/abs/2507.12168)
*Lu Yu,Zhong Ren,Youyi Zheng,Xiang Chen,Kun Zhou*

Main category: cs.GR

TL;DR: 提出了一种自动形状适应方法，用于将3D发型重新定位到新角色上，通过多尺度策略和约束优化实现高效处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法在游戏和VR应用中为角色适配发型时复杂且耗时，艺术家难以处理复杂的头发几何和空间关系。

Method: 将发型适配过程建模为约束优化问题，采用多尺度策略从粗到细计算发丝位置，并引入发际线编辑工具。

Result: 通过定量和定性实验验证了方法的有效性，适用于多种发型和角色。

Conclusion: 该方法为发型适配提供了高效且可定制的解决方案。

Abstract: It is demanding to author an existing hairstyle for novel characters in games
and VR applications. However, it is a non-trivial task for artists due to the
complicated hair geometries and spatial interactions to preserve. In this
paper, we present an automatic shape adaptation method to retarget 3D
hairstyles. We formulate the adaptation process as a constrained optimization
problem, where all the shape properties and spatial relationships are converted
into individual objectives and constraints. To make such an optimization on
high-resolution hairstyles tractable, we adopt a multi-scale strategy to
compute the target positions of the hair strands in a coarse-to-fine manner.
The global solving for the inter-strands coupling is restricted to the coarse
level, and the solving for fine details is made local and parallel. In
addition, we present a novel hairline edit tool to allow for user customization
during retargeting. We achieve it by solving physics-based deformations of an
embedded membrane to redistribute the hair roots with minimal distortion. We
demonstrate the efficacy of our method through quantitative and qualitative
experiments on various hairstyles and characters.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [104] [Towards Autonomous Riding: A Review of Perception, Planning, and Control in Intelligent Two-Wheelers](https://arxiv.org/abs/2507.11852)
*Mohammed Hassanin,Mohammad Abu Alsheikh,Carlos C. N. Kuhn,Damith Herath,Dinh Thai Hoang,Ibrahim Radwan*

Main category: cs.RO

TL;DR: 综述分析了自动驾驶技术在两轮微出行工具（如电动滑板车和电动自行车）中的应用，指出其独特挑战和研究空白，并提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着微出行工具的快速普及，其自主骑行（AR）技术的可靠性需求日益迫切，但两轮平台的不稳定性、尺寸和功率限制以及环境不可预测性带来了安全挑战。

Method: 通过系统分析AR系统的核心组件（感知、规划、控制），借鉴自动驾驶（AD）技术，识别当前研究的不足。

Result: 发现当前AR研究存在感知系统不完善、行业和政府支持不足、研究关注度低等问题，并提出多模态传感器技术和边缘深度学习架构等方向。

Conclusion: 通过结合AD技术与AR需求，旨在推动安全、高效、可扩展的自主骑行系统发展，助力未来城市出行。

Abstract: The rapid adoption of micromobility solutions, particularly two-wheeled
vehicles like e-scooters and e-bikes, has created an urgent need for reliable
autonomous riding (AR) technologies. While autonomous driving (AD) systems have
matured significantly, AR presents unique challenges due to the inherent
instability of two-wheeled platforms, limited size, limited power, and
unpredictable environments, which pose very serious concerns about road users'
safety. This review provides a comprehensive analysis of AR systems by
systematically examining their core components, perception, planning, and
control, through the lens of AD technologies. We identify critical gaps in
current AR research, including a lack of comprehensive perception systems for
various AR tasks, limited industry and government support for such
developments, and insufficient attention from the research community. The
review analyses the gaps of AR from the perspective of AD to highlight
promising research directions, such as multimodal sensor techniques for
lightweight platforms and edge deep learning architectures. By synthesising
insights from AD research with the specific requirements of AR, this review
aims to accelerate the development of safe, efficient, and scalable autonomous
riding systems for future urban mobility.

</details>


### [105] [A Multi-Level Similarity Approach for Single-View Object Grasping: Matching, Planning, and Fine-Tuning](https://arxiv.org/abs/2507.11938)
*Hao Chen,Takuya Kiyokawa,Zhengtao Hu,Weiwei Wan,Kensuke Harada*

Main category: cs.RO

TL;DR: 论文提出了一种基于相似性匹配的新方法，通过利用已知物体的相似性来指导未知物体的抓取，解决了单视角下抓取的不确定性问题。


<details>
  <summary>Details</summary>
Motivation: 传统学习框架对感知噪声和环境变化敏感，性能鲁棒性不足，因此需要一种更通用的抓取方法。

Method: 1) 利用视觉特征进行相似性匹配；2) 基于候选模型的抓取知识规划模仿抓取；3) 通过局部微调优化抓取质量。

Result: 提出了一种多级相似性匹配框架和C-FPFH描述符，结合语义、几何和维度特征，显著提高了匹配和抓取精度。

Conclusion: 新方法通过相似性匹配和局部优化，实现了对未知物体的鲁棒抓取，克服了单视角下的不确定性。

Abstract: Grasping unknown objects from a single view has remained a challenging topic
in robotics due to the uncertainty of partial observation. Recent advances in
large-scale models have led to benchmark solutions such as GraspNet-1Billion.
However, such learning-based approaches still face a critical limitation in
performance robustness for their sensitivity to sensing noise and environmental
changes. To address this bottleneck in achieving highly generalized grasping,
we abandon the traditional learning framework and introduce a new perspective:
similarity matching, where similar known objects are utilized to guide the
grasping of unknown target objects. We newly propose a method that robustly
achieves unknown-object grasping from a single viewpoint through three key
steps: 1) Leverage the visual features of the observed object to perform
similarity matching with an existing database containing various object models,
identifying potential candidates with high similarity; 2) Use the candidate
models with pre-existing grasping knowledge to plan imitative grasps for the
unknown target object; 3) Optimize the grasp quality through a local
fine-tuning process. To address the uncertainty caused by partial and noisy
observation, we propose a multi-level similarity matching framework that
integrates semantic, geometric, and dimensional features for comprehensive
evaluation. Especially, we introduce a novel point cloud geometric descriptor,
the C-FPFH descriptor, which facilitates accurate similarity assessment between
partial point clouds of observed objects and complete point clouds of database
models. In addition, we incorporate the use of large language models, introduce
the semi-oriented bounding box, and develop a novel point cloud registration
approach based on plane detection to enhance matching accuracy under
single-view conditions. Videos are available at https://youtu.be/qQDIELMhQmk.

</details>


### [106] [EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos](https://arxiv.org/abs/2507.12440)
*Ruihan Yang,Qinxi Yu,Yecheng Wu,Rui Yan,Borui Li,An-Chieh Cheng,Xueyan Zou,Yunhao Fang,Hongxu Yin,Sifei Liu,Song Han,Yao Lu,Xiaolong Wang*

Main category: cs.RO

TL;DR: 利用人类视频训练视觉-语言-动作模型（VLA），通过逆向运动学和重定向将人类动作转换为机器人动作，显著提升机器人操作性能。


<details>
  <summary>Details</summary>
Motivation: 解决机器人硬件限制导致的数据规模不足问题，利用人类视频的丰富场景和任务多样性。

Method: 训练VLA模型预测人类手腕和手部动作，通过逆向运动学和重定向转换为机器人动作，并用少量机器人演示进行微调。

Result: 在Isaac Humanoid Manipulation Benchmark上评估，EgoVLA显著优于基线，验证了人类数据的重要性。

Conclusion: 利用人类视频训练VLA模型是一种有效的方法，能够提升机器人操作能力，且人类数据的丰富性对性能有重要影响。

Abstract: Real robot data collection for imitation learning has led to significant
advancements in robotic manipulation. However, the requirement for robot
hardware in the process fundamentally constrains the scale of the data. In this
paper, we explore training Vision-Language-Action (VLA) models using egocentric
human videos. The benefit of using human videos is not only for their scale but
more importantly for the richness of scenes and tasks. With a VLA trained on
human video that predicts human wrist and hand actions, we can perform Inverse
Kinematics and retargeting to convert the human actions to robot actions. We
fine-tune the model using a few robot manipulation demonstrations to obtain the
robot policy, namely EgoVLA. We propose a simulation benchmark called Isaac
Humanoid Manipulation Benchmark, where we design diverse bimanual manipulation
tasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid
Manipulation Benchmark and show significant improvements over baselines and
ablate the importance of human data. Videos can be found on our website:
https://rchalyang.github.io/EgoVLA

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [107] [FactorHD: A Hyperdimensional Computing Model for Multi-Object Multi-Class Representation and Factorization](https://arxiv.org/abs/2507.12366)
*Yifei Zhou,Xuchu Huang,Chenyu Ni,Min Zhou,Zheyu Yan,Xunzhao Yin,Cheng Zhuo*

Main category: cs.SC

TL;DR: FactorHD是一种新型的超维计算模型，能够高效表示和分解复杂的类-子类关系，显著提升了计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的超维计算模型在处理复杂的类-子类关系时面临挑战，尤其是在分解任务中。FactorHD旨在解决这些问题，提升神经符号AI系统的性能。

Method: FactorHD采用符号编码方法，嵌入额外的记忆子句以保留更多信息，并利用高效的分解算法选择性消除冗余类。

Result: FactorHD在表示规模为10^9时实现了约5667倍的加速，与ResNet-18结合在Cifar-10数据集上达到92.48%的分解准确率。

Conclusion: FactorHD克服了现有模型的限制，如“叠加灾难”和“问题2”，为神经符号AI提供了更高效的解决方案。

Abstract: Neuro-symbolic artificial intelligence (neuro-symbolic AI) excels in logical
analysis and reasoning. Hyperdimensional Computing (HDC), a promising
brain-inspired computational model, is integral to neuro-symbolic AI. Various
HDC models have been proposed to represent class-instance and class-class
relations, but when representing the more complex class-subclass relation,
where multiple objects associate different levels of classes and subclasses,
they face challenges for factorization, a crucial task for neuro-symbolic AI
systems. In this article, we propose FactorHD, a novel HDC model capable of
representing and factorizing the complex class-subclass relation efficiently.
FactorHD features a symbolic encoding method that embeds an extra memorization
clause, preserving more information for multiple objects. In addition, it
employs an efficient factorization algorithm that selectively eliminates
redundant classes by identifying the memorization clause of the target class.
Such model significantly enhances computing efficiency and accuracy in
representing and factorizing multiple objects with class-subclass relation,
overcoming limitations of existing HDC models such as "superposition
catastrophe" and "the problem of 2". Evaluations show that FactorHD achieves
approximately 5667x speedup at a representation size of 10^9 compared to
existing HDC models. When integrated with the ResNet-18 neural network,
FactorHD achieves 92.48% factorization accuracy on the Cifar-10 dataset.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [108] [Spontaneous Spatial Cognition Emerges during Egocentric Video Viewing through Non-invasive BCI](https://arxiv.org/abs/2507.12417)
*Weichen Dai,Yuxuan Huang,Li Zhu,Dongjun Liu,Yu Zhang,Qibin Zhao,Andrzej Cichocki,Fabio Babiloni,Ke Li,Jianyu Qiu,Gangyong Jia,Wanzeng Kong,Qing Wu*

Main category: q-bio.NC

TL;DR: 非侵入性脑机接口（BCI）通过EEG解码被动观看视频时的6D空间姿态，揭示大脑空间系统的自发运作。


<details>
  <summary>Details</summary>
Motivation: 探索自然被动体验下大规模神经动力学如何支持空间表征，填补现有研究空白。

Method: 基于EEG的BCI解码自发6D姿态，分析视觉输入对空间表征的影响，并通过反向传播识别关键EEG通道。

Result: 连续结构化视觉输入可靠地诱发可解码空间表征，100ms帧率下解码性能最佳，揭示分布式神经编码方案。

Conclusion: 大脑空间系统在被动条件下自发运作，挑战主动与被动空间认知的传统区分，为非侵入性研究提供新视角。

Abstract: Humans possess a remarkable capacity for spatial cognition, allowing for
self-localization even in novel or unfamiliar environments. While hippocampal
neurons encoding position and orientation are well documented, the large-scale
neural dynamics supporting spatial representation, particularly during
naturalistic, passive experience, remain poorly understood. Here, we
demonstrate for the first time that non-invasive brain-computer interfaces
(BCIs) based on electroencephalography (EEG) can decode spontaneous,
fine-grained egocentric 6D pose, comprising three-dimensional position and
orientation, during passive viewing of egocentric video. Despite EEG's limited
spatial resolution and high signal noise, we find that spatially coherent
visual input (i.e., continuous and structured motion) reliably evokes decodable
spatial representations, aligning with participants' subjective sense of
spatial engagement. Decoding performance further improves when visual input is
presented at a frame rate of 100 ms per image, suggesting alignment with
intrinsic neural temporal dynamics. Using gradient-based backpropagation
through a neural decoding model, we identify distinct EEG channels contributing
to position -- and orientation specific -- components, revealing a distributed
yet complementary neural encoding scheme. These findings indicate that the
brain's spatial systems operate spontaneously and continuously, even under
passive conditions, challenging traditional distinctions between active and
passive spatial cognition. Our results offer a non-invasive window into the
automatic construction of egocentric spatial maps and advance our understanding
of how the human mind transforms everyday sensory experience into structured
internal representations.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [109] [DoRF: Doppler Radiance Fields for Robust Human Activity Recognition Using Wi-Fi](https://arxiv.org/abs/2507.12132)
*Navid Hasanzadeh,Shahrokh Valaee*

Main category: eess.SP

TL;DR: 提出了一种基于Wi-Fi CSI的多普勒速度投影构建3D潜在运动表示的方法，通过均匀多普勒辐射场（DoRF）提升人类活动识别的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管Wi-Fi CSI在多普勒速度投影上取得进展，但泛化能力仍不足以实际应用，受NeRF启发，尝试构建3D运动表示以提高鲁棒性。

Method: 从Wi-Fi CSI提取一维多普勒速度投影，重建3D潜在运动表示，并构建均匀多普勒辐射场（DoRF）。

Result: 该方法显著提升了Wi-Fi HAR的泛化准确性。

Conclusion: DoRF在实用传感应用中具有强大潜力。

Abstract: Wi-Fi Channel State Information (CSI) has gained increasing interest for
remote sensing applications. Recent studies show that Doppler velocity
projections extracted from CSI can enable human activity recognition (HAR) that
is robust to environmental changes and generalizes to new users. However,
despite these advances, generalizability still remains insufficient for
practical deployment. Inspired by neural radiance fields (NeRF), which learn a
volumetric representation of a 3D scene from 2D images, this work proposes a
novel approach to reconstruct an informative 3D latent motion representation
from one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The
resulting latent representation is then used to construct a uniform Doppler
radiance field (DoRF) of the motion, providing a comprehensive view of the
performed activity and improving the robustness to environmental variability.
The results show that the proposed approach noticeably enhances the
generalization accuracy of Wi-Fi-based HAR, highlighting the strong potential
of DoRFs for practical sensing applications.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [110] [Image-Based Multi-Survey Classification of Light Curves with a Pre-Trained Vision Transformer](https://arxiv.org/abs/2507.11711)
*Daniel Moreno-Cartagena,Guillermo Cabrera-Vives,Alejandra M. Muñoz Arancibia,Pavlos Protopapas,Francisco Förster,Márcio Catelan,A. Bayo,Pablo A. Estévez,P. Sánchez-Sáez,Franz E. Bauer,M. Pavez-Herrera,L. Hernández-García,Gonzalo Rojas*

Main category: astro-ph.IM

TL;DR: Swin Transformer V2用于多巡天数据的光度分类，联合处理ZTF和ATLAS数据效果最佳。


<details>
  <summary>Details</summary>
Motivation: 探索预训练视觉Transformer在多巡天光度分类中的应用，解决数据整合问题。

Method: 使用Swin Transformer V2，评估不同数据整合策略，联合处理ZTF和ATLAS数据。

Result: 联合处理多巡天数据的架构表现最佳，强调建模巡天特性和跨巡天交互的重要性。

Conclusion: 为未来时域天文学的可扩展分类器构建提供了指导。

Abstract: We explore the use of Swin Transformer V2, a pre-trained vision Transformer,
for photometric classification in a multi-survey setting by leveraging light
curves from the Zwicky Transient Facility (ZTF) and the Asteroid
Terrestrial-impact Last Alert System (ATLAS). We evaluate different strategies
for integrating data from these surveys and find that a multi-survey
architecture which processes them jointly achieves the best performance. These
results highlight the importance of modeling survey-specific characteristics
and cross-survey interactions, and provide guidance for building scalable
classifiers for future time-domain astronomy.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [111] [A Spatial-Physics Informed Model for 3D Spiral Sample Scanned by SQUID Microscopy](https://arxiv.org/abs/2507.11853)
*J. Senthilnath,Jayasanker Jayabalan,Zhuoyi Lin,Aye Phyu Phyu Aung,Chen Hao,Kaixin Xu,Yeow Kheng Lim,F. C. Wellstood*

Main category: physics.ins-det

TL;DR: 论文提出了一种空间物理信息模型（SPIM），用于改进半导体先进封装中的非破坏性测试（NDT），通过磁成像（MFI）和电流密度转换，解决了现有方法忽略涡流效应和图像不对齐的问题。


<details>
  <summary>Details</summary>
Motivation: 半导体先进封装的非破坏性测试面临深度和复杂性的挑战，现有方法（如FFT）未考虑涡流效应和图像不对齐，限制了磁成像（MFI）的有效性。

Method: SPIM包含三个关键部分：1）通过I/Q通道图像对齐信号以减轻涡流效应；2）校正扫描显微镜与导线段的偏差；3）结合毕奥-萨伐尔定律和FFT将磁场转换为电流密度。

Result: SPIM提高了I通道清晰度0.3%，降低Q通道清晰度25%，并成功校正了0.30的旋转和偏差不对齐。

Conclusion: SPIM展示了空间分析与物理驱动模型结合在实际应用中的潜力，为半导体封装测试提供了更有效的解决方案。

Abstract: The development of advanced packaging is essential in the semiconductor
manufacturing industry. However, non-destructive testing (NDT) of advanced
packaging becomes increasingly challenging due to the depth and complexity of
the layers involved. In such a scenario, Magnetic field imaging (MFI) enables
the imaging of magnetic fields generated by currents. For MFI to be effective
in NDT, the magnetic fields must be converted into current density. This
conversion has typically relied solely on a Fast Fourier Transform (FFT) for
magnetic field inversion; however, the existing approach does not consider eddy
current effects or image misalignment in the test setup. In this paper, we
present a spatial-physics informed model (SPIM) designed for a 3D spiral sample
scanned using Superconducting QUantum Interference Device (SQUID) microscopy.
The SPIM encompasses three key components: i) magnetic image enhancement by
aligning all the "sharp" wire field signals to mitigate the eddy current effect
using both in-phase (I-channel) and quadrature-phase (Q-channel) images; (ii)
magnetic image alignment that addresses skew effects caused by any misalignment
of the scanning SQUID microscope relative to the wire segments; and (iii) an
inversion method for converting magnetic fields to magnetic currents by
integrating the Biot-Savart Law with FFT. The results show that the SPIM
improves I-channel sharpness by 0.3% and reduces Q-channel sharpness by 25%.
Also, we were able to remove rotational and skew misalignments of 0.30 in a
real image. Overall, SPIM highlights the potential of combining spatial
analysis with physics-driven models in practical applications.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [112] [Effective Fine-Tuning of Vision Transformers with Low-Rank Adaptation for Privacy-Preserving Image Classification](https://arxiv.org/abs/2507.11943)
*Haiwei Lin,Shoko Imaizumi,Hitoshi Kiya*

Main category: cs.CR

TL;DR: 提出一种低秩适应方法，用于训练隐私保护的ViT模型，冻结预训练权重并减少可训练参数，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统低秩适应方法在ViT中冻结所有层，包括patch嵌入层，限制了模型性能。本文旨在改进这一点。

Method: 在ViT的每一层注入可训练的低秩分解矩阵，且不冻结patch嵌入层。

Result: 减少了可训练参数数量，同时保持了与全参数微调相近的精度。

Conclusion: 该方法在隐私保护的ViT训练中高效且性能优越。

Abstract: We propose a low-rank adaptation method for training privacy-preserving
vision transformer (ViT) models that efficiently freezes pre-trained ViT model
weights. In the proposed method, trainable rank decomposition matrices are
injected into each layer of the ViT architecture, and moreover, the patch
embedding layer is not frozen, unlike in the case of the conventional low-rank
adaptation methods. The proposed method allows us not only to reduce the number
of trainable parameters but to also maintain almost the same accuracy as that
of full-time tuning.

</details>


### [113] [IDFace: Face Template Protection for Efficient and Secure Identification](https://arxiv.org/abs/2507.12050)
*Sunpill Kim,Seunghun Paik,Chanwoo Hwang,Dongsoo Kim,Junbum Shin,Jae Hong Seo*

Main category: cs.CR

TL;DR: IDFace是一种基于同态加密的高效人脸识别方法，通过优化模板表示和编码技术，显著提升了加密模板的匹配效率。


<details>
  <summary>Details</summary>
Motivation: 随着人脸识别系统的广泛应用，用户隐私保护变得尤为重要。传统同态加密方法因效率低下难以直接应用于人脸识别系统。

Method: IDFace采用两种新技术：模板表示转换降低匹配成本，空间高效编码减少加密算法的空间浪费。

Result: 实验表明，IDFace能在126毫秒内从100万个加密模板中识别出人脸，仅比明文识别慢2倍。

Conclusion: IDFace在保护隐私的同时，显著提升了加密人脸识别的效率，具有实际应用潜力。

Abstract: As face recognition systems (FRS) become more widely used, user privacy
becomes more important. A key privacy issue in FRS is protecting the user's
face template, as the characteristics of the user's face image can be recovered
from the template. Although recent advances in cryptographic tools such as
homomorphic encryption (HE) have provided opportunities for securing the FRS,
HE cannot be used directly with FRS in an efficient plug-and-play manner. In
particular, although HE is functionally complete for arbitrary programs, it is
basically designed for algebraic operations on encrypted data of predetermined
shape, such as a polynomial ring. Thus, a non-tailored combination of HE and
the system can yield very inefficient performance, and many previous HE-based
face template protection methods are hundreds of times slower than plain
systems without protection. In this study, we propose IDFace, a new HE-based
secure and efficient face identification method with template protection.
IDFace is designed on the basis of two novel techniques for efficient searching
on a (homomorphically encrypted) biometric database with an angular metric. The
first technique is a template representation transformation that sharply
reduces the unit cost for the matching test. The second is a space-efficient
encoding that reduces wasted space from the encryption algorithm, thus saving
the number of operations on encrypted templates. Through experiments, we show
that IDFace can identify a face template from among a database of 1M encrypted
templates in 126ms, showing only 2X overhead compared to the identification
over plaintexts.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [114] [SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery](https://arxiv.org/abs/2507.11570)
*Ha Na Cho,Sairam Sutari,Alexander Lopez,Hansen Bow,Kai Zheng*

Main category: cs.LG

TL;DR: 论文提出了一种名为SurgeryLSTM的模型，用于预测脊柱手术患者的住院时长（LOS），并通过注意力机制提高了模型的解释性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种能够准确预测脊柱手术患者住院时长的机器学习模型，并强调时间建模和模型解释性的重要性。

Method: 比较了传统机器学习模型（如线性回归、随机森林、SVM和XGBoost）与提出的SurgeryLSTM模型（一种带有注意力机制的BiLSTM模型），使用结构化电子健康记录（EHR）数据进行评估。

Result: SurgeryLSTM在预测准确性上表现最佳（R2=0.86），优于XGBoost（R2=0.85）和其他基线模型。注意力机制提高了模型的解释性，能够动态识别术前临床序列中的关键时间片段。

Conclusion: SurgeryLSTM是一种高效且可解释的AI解决方案，支持将时间建模和可解释性机器学习方法整合到临床决策支持系统中，以优化出院准备和个性化患者护理。

Abstract: Objective: To develop and evaluate machine learning (ML) models for
predicting length of stay (LOS) in elective spine surgery, with a focus on the
benefits of temporal modeling and model interpretability. Materials and
Methods: We compared traditional ML models (e.g., linear regression, random
forest, support vector machine (SVM), and XGBoost) with our developed model,
SurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an
attention, using structured perioperative electronic health records (EHR) data.
Performance was evaluated using the coefficient of determination (R2), and key
predictors were identified using explainable AI. Results: SurgeryLSTM achieved
the highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85)
and baseline models. The attention mechanism improved interpretability by
dynamically identifying influential temporal segments within preoperative
clinical sequences, allowing clinicians to trace which events or features most
contributed to each LOS prediction. Key predictors of LOS included bone
disorder, chronic kidney disease, and lumbar fusion identified as the most
impactful predictors of LOS. Discussion: Temporal modeling with attention
mechanisms significantly improves LOS prediction by capturing the sequential
nature of patient data. Unlike static models, SurgeryLSTM provides both higher
accuracy and greater interpretability, which are critical for clinical
adoption. These results highlight the potential of integrating attention-based
temporal models into hospital planning workflows. Conclusion: SurgeryLSTM
presents an effective and interpretable AI solution for LOS prediction in
elective spine surgery. Our findings support the integration of temporal,
explainable ML approaches into clinical decision support systems to enhance
discharge readiness and individualized patient care.

</details>


### [115] [The Impact of Coreset Selection on Spurious Correlations and Group Robustness](https://arxiv.org/abs/2507.11690)
*Amaya Dharmasiri,William Yang,Polina Kirichenko,Lydia Liu,Olga Russakovsky*

Main category: cs.LG

TL;DR: 本文首次全面分析了数据选择对核心集偏差水平及下游模型鲁棒性的影响，揭示了样本难度与偏差对齐之间的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 研究核心集选择方法是否会加剧或减轻数据集中的偏差，并探讨其对下游模型鲁棒性的影响。

Method: 在十个不同的虚假相关性基准上，使用五种评分指标和五种数据选择策略，分析核心集选择对偏差和模型鲁棒性的影响。

Result: 发现基于嵌入的样本评分方法在减少偏差风险方面优于基于学习动态的方法，但优先选择困难样本并不总能保证模型鲁棒性。

Conclusion: 核心集选择方法虽能降低偏差，但需谨慎设计以确保下游模型的鲁棒性。

Abstract: Coreset selection methods have shown promise in reducing the training data
size while maintaining model performance for data-efficient machine learning.
However, as many datasets suffer from biases that cause models to learn
spurious correlations instead of causal features, it is important to understand
whether and how dataset reduction methods may perpetuate, amplify, or mitigate
these biases. In this work, we conduct the first comprehensive analysis of the
implications of data selection on the spurious bias levels of the selected
coresets and the robustness of downstream models trained on them. We use an
extensive experimental setting spanning ten different spurious correlations
benchmarks, five score metrics to characterize sample importance/ difficulty,
and five data selection policies across a broad range of coreset sizes.
Thereby, we unravel a series of nontrivial nuances in interactions between
sample difficulty and bias alignment, as well as dataset bias and resultant
model robustness. For example, we find that selecting coresets using
embedding-based sample characterization scores runs a comparatively lower risk
of inadvertently exacerbating bias than selecting using characterizations based
on learning dynamics. Most importantly, our analysis reveals that although some
coreset selection methods could achieve lower bias levels by prioritizing
difficult samples, they do not reliably guarantee downstream robustness.

</details>


### [116] [MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory](https://arxiv.org/abs/2507.11821)
*Pouya Shaeri,Arash Karimi,Ariane Middel*

Main category: cs.LG

TL;DR: MNIST-Gen是一个自动化框架，用于生成特定领域的MNIST风格数据集，结合了CLIP语义理解和强化学习，支持复杂分类结构。


<details>
  <summary>Details</summary>
Motivation: 标准数据集（如MNIST）对领域特定任务不适用，而创建自定义数据集耗时且复杂。

Method: 结合CLIP语义理解、强化学习和人类反馈，采用分层语义分类和模块化设计。

Result: 生成Tree-MNIST和Food-MNIST数据集，自动分类准确率达85%，节省80%时间。

Conclusion: MNIST-Gen为领域特定任务提供了高效、灵活的数据集生成解决方案。

Abstract: Neural networks are often benchmarked using standard datasets such as MNIST,
FashionMNIST, or other variants of MNIST, which, while accessible, are limited
to generic classes such as digits or clothing items. For researchers working on
domain-specific tasks, such as classifying trees, food items, or other
real-world objects, these data sets are insufficient and irrelevant.
Additionally, creating and publishing a custom dataset can be time consuming,
legally constrained, or beyond the scope of individual projects. We present
MNIST-Gen, an automated, modular, and adaptive framework for generating
MNIST-style image datasets tailored to user-specified categories using
hierarchical semantic categorization. The system combines CLIP-based semantic
understanding with reinforcement learning and human feedback to achieve
intelligent categorization with minimal manual intervention. Our hierarchical
approach supports complex category structures with semantic characteristics,
enabling fine-grained subcategorization and multiple processing modes:
individual review for maximum control, smart batch processing for large
datasets, and fast batch processing for rapid creation. Inspired by category
theory, MNIST-Gen models each data transformation stage as a composable
morphism, enhancing clarity, modularity, and extensibility. As proof of
concept, we generate and benchmark two novel datasets-\textit{Tree-MNIST} and
\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing
task-specific evaluation data while achieving 85\% automatic categorization
accuracy and 80\% time savings compared to manual approaches.

</details>


### [117] [PRISM: Distributed Inference for Foundation Models at Edge](https://arxiv.org/abs/2507.12145)
*Muhammad Azlan Qazi,Alexandros Iosifidis,Qi Zhang*

Main category: cs.LG

TL;DR: PRISM是一种通信高效且计算感知的策略，用于在边缘设备上分布式Transformer推理，显著减少通信和计算开销。


<details>
  <summary>Details</summary>
Motivation: 基础模型在边缘部署面临挑战，需要高效策略。

Method: 利用Segment Means表示近似中间特征，重构自注意力机制以减少冗余计算，设计分区感知的因果掩码。

Result: BERT通信开销减少99.2%，计算减少51.24%，仅轻微精度下降。

Conclusion: PRISM为资源受限环境提供可扩展的解决方案。

Abstract: Foundation models (FMs) have achieved remarkable success across a wide range
of applications, from image classification to natural langurage processing, but
pose significant challenges for deployment at edge. This has sparked growing
interest in developing practical and efficient strategies for bringing
foundation models to edge environments. In this work, we propose PRISM, a
communication-efficient and compute-aware strategy for distributed Transformer
inference on edge devices. Our method leverages a Segment Means representation
to approximate intermediate output features, drastically reducing inter-device
communication. Additionally, we restructure the self-attention mechanism to
eliminate redundant computations caused by per-device Key/Value calculation in
position-wise partitioning and design a partition-aware causal masking scheme
tailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2
across diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and
CBT. Our results demonstrate substantial reductions in communication overhead
(up to 99.2% for BERT at compression rate CR = 128) and per-device computation
(51.24% for BERT at the same setting), with only minor accuracy degradation.
This method offers a scalable and practical solution for deploying foundation
models in distributed resource-constrained environments.

</details>


### [118] [RegCL: Continual Adaptation of Segment Anything Model via Model Merging](https://arxiv.org/abs/2507.12297)
*Yuan-Chen Shu,Zhiwei Lin,Yongtao Wang*

Main category: cs.LG

TL;DR: 本文提出RegCL，一种非重放的持续学习框架，通过模型合并实现多领域知识的高效整合，解决了Segment Anything Model（SAM）在特定领域性能受限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为特定领域设计，跨领域使用时性能下降，且存在灾难性遗忘问题，限制了模型的扩展性。

Method: RegCL通过合并不同领域训练的SAM适配模块（如LoRA模块）参数，以权重优化为指导，最小化预测差异。

Result: 实验表明，RegCL在多领域下游任务中表现优异，保持参数效率且无需历史数据存储。

Conclusion: RegCL有效整合多领域知识，适用于动态场景，解决了灾难性遗忘问题。

Abstract: To address the performance limitations of the Segment Anything Model (SAM) in
specific domains, existing works primarily adopt adapter-based one-step
adaptation paradigms. However, some of these methods are specific developed for
specific domains. If used on other domains may lead to performance degradation.
This issue of catastrophic forgetting severely limits the model's scalability.
To address this issue, this paper proposes RegCL, a novel non-replay continual
learning (CL) framework designed for efficient multi-domain knowledge
integration through model merging. Specifically, RegCL incorporates the model
merging algorithm into the continual learning paradigm by merging the
parameters of SAM's adaptation modules (e.g., LoRA modules) trained on
different domains. The merging process is guided by weight optimization, which
minimizes prediction discrepancies between the merged model and each of the
domain-specific models. RegCL effectively consolidates multi-domain knowledge
while maintaining parameter efficiency, i.e., the model size remains constant
regardless of the number of tasks, and no historical data storage is required.
Experimental results demonstrate that RegCL achieves favorable continual
learning performance across multiple downstream datasets, validating its
effectiveness in dynamic scenarios.

</details>


### [119] [PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning](https://arxiv.org/abs/2507.12305)
*M. Anwar Ma'sum,Mahardhika Pratama,Savitha Ramasamy,Lin Liu,Habibullah Habibullah,Ryszard Kowalczyk*

Main category: cs.LG

TL;DR: 提出了一种基于提示的在线持续学习方法，解决了数据隐私和参数增长问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线持续学习中数据隐私约束和参数增长问题限制了现有方法的实用性。

Method: 提出包含轻量级提示生成器、可训练缩放-移位器、预训练模型保持和硬软更新机制的提示方法。

Result: 在多个数据集上性能显著优于现有方法，参数较少且训练/推理时间适中。

Conclusion: 该方法在性能和效率上均优于现有技术，代码已开源。

Abstract: The data privacy constraint in online continual learning (OCL), where the
data can be seen only once, complicates the catastrophic forgetting problem in
streaming data. A common approach applied by the current SOTAs in OCL is with
the use of memory saving exemplars or features from previous classes to be
replayed in the current task. On the other hand, the prompt-based approach
performs excellently in continual learning but with the cost of a growing
number of trainable parameters. The first approach may not be applicable in
practice due to data openness policy, while the second approach has the issue
of throughput associated with the streaming data. In this study, we propose a
novel prompt-based method for online continual learning that includes 4 main
components: (1) single light-weight prompt generator as a general knowledge,
(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model
(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our
proposed method achieves significantly higher performance than the current
SOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity
analysis shows that our method requires a relatively smaller number of
parameters and achieves moderate training time, inference time, and throughput.
For further study, the source code of our method is available at
https://github.com/anwarmaxsum/PROL.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [120] [MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering](https://arxiv.org/abs/2507.11625)
*Varun Srivastava,Fan Lei,Srija Mukhopadhyay,Vivek Gupta,Ross Maciejewski*

Main category: cs.CL

TL;DR: 论文介绍了MapIQ基准数据集，用于评估多模态大语言模型（MLLMs）在地图视觉问答（Map-VQA）中的表现，涵盖多种地图类型和主题。


<details>
  <summary>Details</summary>
Motivation: 现有Map-VQA研究主要局限于等值区域图，覆盖的主题和任务有限，需要更全面的评估工具。

Method: 构建包含14,706个问答对的MapIQ数据集，涵盖三种地图类型和六种主题，评估MLLMs在六种视觉分析任务中的表现。

Result: 通过实验比较MLLMs与人类基准的表现，并分析地图设计变化对模型鲁棒性和敏感性的影响。

Conclusion: MapIQ为Map-VQA研究提供了更全面的基准，揭示了MLLMs的局限性及改进方向。

Abstract: Recent advancements in multimodal large language models (MLLMs) have driven
researchers to explore how well these models read data visualizations, e.g.,
bar charts, scatter plots. More recently, attention has shifted to visual
question answering with maps (Map-VQA). However, Map-VQA research has primarily
focused on choropleth maps, which cover only a limited range of thematic
categories and visual analytical tasks. To address these gaps, we introduce
MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three
map types: choropleth maps, cartograms, and proportional symbol maps spanning
topics from six distinct themes (e.g., housing, crime). We evaluate multiple
MLLMs using six visual analytical tasks, comparing their performance against
one another and a human baseline. An additional experiment examining the impact
of map design changes (e.g., altered color schemes, modified legend designs,
and removal of map elements) provides insights into the robustness and
sensitivity of MLLMs, their reliance on internal geographic knowledge, and
potential avenues for improving Map-VQA performance.

</details>


### [121] [A Survey of Deep Learning for Geometry Problem Solving](https://arxiv.org/abs/2507.11936)
*Jianzhe Ma,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: 本文综述了深度学习在几何问题求解中的应用，包括任务总结、方法回顾、评估指标分析及未来挑战探讨。


<details>
  <summary>Details</summary>
Motivation: 几何问题求解是数学推理的关键领域，涉及教育和人工智能能力评估等多个重要领域，深度学习的发展推动了相关研究。

Method: 通过总结几何问题求解任务、回顾深度学习方法、分析评估指标及讨论挑战与未来方向。

Result: 提供了深度学习在几何问题求解中的全面参考，促进该领域的进一步发展。

Conclusion: 本文为几何问题求解领域的研究者提供了实用参考，并指出了未来的研究方向。

Abstract: Geometry problem solving is a key area of mathematical reasoning, which is
widely involved in many important fields such as education, mathematical
ability assessment of artificial intelligence, and multimodal ability
assessment. In recent years, the rapid development of deep learning technology,
especially the rise of multimodal large language models, has triggered a
widespread research boom. This paper provides a survey of the applications of
deep learning in geometry problem solving, including (i) a comprehensive
summary of the relevant tasks in geometry problem solving; (ii) a thorough
review of related deep learning methods; (iii) a detailed analysis of
evaluation metrics and methods; and (iv) a critical discussion of the current
challenges and future directions that can be explored. Our goal is to provide a
comprehensive and practical reference of deep learning for geometry problem
solving to promote further developments in this field. We create a continuously
updated list of papers on GitHub: https://github.com/majianz/dl4gps.

</details>


### [122] [POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering](https://arxiv.org/abs/2507.11939)
*Yichen Xu,Liangyu Chen,Liang Zhang,Wenxuan Wang,Qin Jin*

Main category: cs.CL

TL;DR: PolyChartQA是一个多语言图表问答基准，覆盖10种语言的22,606张图表和26,151个问答对，旨在解决现有图表理解基准的英语中心化问题。


<details>
  <summary>Details</summary>
Motivation: 现有图表理解基准主要针对英语，限制了其全球适用性。PolyChartQA旨在填补这一空白，提供多语言支持。

Method: 采用解耦流水线方法，分离图表数据和渲染代码，通过翻译数据生成多语言图表，并利用先进的LLM翻译和质量控制确保一致性。

Result: 实验显示，英语与其他语言（尤其是非拉丁文字的低资源语言）之间存在显著性能差距。

Conclusion: PolyChartQA为推进全球包容性视觉语言模型奠定了基础。

Abstract: Charts are a universally adopted medium for interpreting and communicating
data. However, existing chart understanding benchmarks are predominantly
English-centric, limiting their accessibility and applicability to global
audiences. In this paper, we present PolyChartQA, the first large-scale
multilingual chart question answering benchmark covering 22,606 charts and
26,151 question-answering pairs across 10 diverse languages. PolyChartQA is
built using a decoupled pipeline that separates chart data from rendering code,
allowing multilingual charts to be flexibly generated by simply translating the
data and reusing the code. We leverage state-of-the-art LLM-based translation
and enforce rigorous quality control in the pipeline to ensure the linguistic
and semantic consistency of the generated multilingual charts. PolyChartQA
facilitates systematic evaluation of multilingual chart understanding.
Experiments on both open- and closed-source large vision-language models reveal
a significant performance gap between English and other languages, especially
low-resource ones with non-Latin scripts. This benchmark lays a foundation for
advancing globally inclusive vision-language models.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [123] [Stereo Sound Event Localization and Detection with Onscreen/offscreen Classification](https://arxiv.org/abs/2507.12042)
*Kazuki Shimada,Archontis Politis,Iran R. Roman,Parthasaarathy Sudarsanam,David Diaz-Guerra,Ruchi Pandey,Kengo Uchida,Yuichiro Koyama,Naoya Takahashi,Takashi Shibuya,Shusuke Takahashi,Tuomas Virtanen,Yuki Mitsufuji*

Main category: cs.SD

TL;DR: DCASE2025挑战赛任务3聚焦于立体声音频数据的声音事件定位与检测（SELD），引入新的数据集和基线系统，评估指标包括屏幕内外事件分类。


<details>
  <summary>Details</summary>
Motivation: 从传统的多通道音频转向更常见的立体声音频，以研究有限视场（FOV）下的SELD任务，并解决立体声音频固有的角度模糊问题。

Method: 使用立体声音频和对应视频帧作为输入，基线系统整合了屏幕内外事件分类任务，并修改了评估指标。

Result: 基线系统在立体声音频数据上表现良好。

Conclusion: 任务3通过立体声音频和视频数据扩展了SELD的应用场景，为有限FOV下的声音事件分析提供了新方向。

Abstract: This paper presents the objective, dataset, baseline, and metrics of Task 3
of the DCASE2025 Challenge on sound event localization and detection (SELD). In
previous editions, the challenge used four-channel audio formats of first-order
Ambisonics (FOA) and microphone array. In contrast, this year's challenge
investigates SELD with stereo audio data (termed stereo SELD). This change
shifts the focus from more specialized 360{\deg} audio and audiovisual scene
analysis to more commonplace audio and media scenarios with limited
field-of-view (FOV). Due to inherent angular ambiguities in stereo audio data,
the task focuses on direction-of-arrival (DOA) estimation in the azimuth plane
(left-right axis) along with distance estimation. The challenge remains divided
into two tracks: audio-only and audiovisual, with the audiovisual track
introducing a new sub-task of onscreen/offscreen event classification
necessitated by the limited FOV. This challenge introduces the DCASE2025 Task3
Stereo SELD Dataset, whose stereo audio and perspective video clips are sampled
and converted from the STARSS23 recordings. The baseline system is designed to
process stereo audio and corresponding video frames as inputs. In addition to
the typical SELD event classification and localization, it integrates
onscreen/offscreen classification for the audiovisual track. The evaluation
metrics have been modified to introduce an onscreen/offscreen accuracy metric,
which assesses the models' ability to identify which sound sources are
onscreen. In the experimental evaluation, the baseline system performs
reasonably well with the stereo audio data.

</details>
