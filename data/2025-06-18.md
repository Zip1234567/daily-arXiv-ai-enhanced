<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 72]
- [eess.IV](#eess.IV) [Total: 14]
- [cs.GR](#cs.GR) [Total: 6]
- [physics.geo-ph](#physics.geo-ph) [Total: 2]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.DC](#cs.DC) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Non-planar Object Detection and Identification by Features Matching and Triangulation Growth](https://arxiv.org/abs/2506.13769)
*Filippo Leveni*

Main category: cs.CV

TL;DR: 提出了一种基于特征的方法，通过逐步分组特征匹配来检测和识别场景图像中的变形模板，利用Delaunay三角剖分作为指导工具，表现优于基于单应性的RANSAC方法。


<details>
  <summary>Details</summary>
Motivation: 目标检测和识别是计算机视觉中的基础问题，广泛应用于目标跟踪、工业机器人控制等领域，但现有方法在模板非平面或严重变形时效果不佳。

Method: 采用Delaunay三角剖分作为图结构，逐步评估特征匹配的局部一致性，结合几何和光度特性进行分组。

Result: 在变形较小的情况下表现与基于单应性的RANSAC相当，变形严重时表现更优。

Conclusion: 该方法在模板非平面或严重变形时仍能有效识别目标，具有更强的鲁棒性。

Abstract: Object detection and identification is surely a fundamental topic in the computer vision field; it plays a crucial role in many applications such as object tracking, industrial robots control, image retrieval, etc. We propose a feature-based approach for detecting and identifying distorted occurrences of a given template in a scene image by incremental grouping of feature matches between the image and the template. For this purpose, we consider the Delaunay triangulation of template features as an useful tool through which to be guided in this iterative approach. The triangulation is treated as a graph and, starting from a single triangle, neighboring nodes are considered and the corresponding features are identified; then matches related to them are evaluated to determine if they are worthy to be grouped. This evaluation is based on local consistency criteria derived from geometric and photometric properties of local features. Our solution allows the identification of the object in situations where geometric models (e.g. homography) does not hold, thus enable the detection of objects such that the template is non planar or when it is planar but appears distorted in the image. We show that our approach performs just as well or better than application of homography-based RANSAC in scenarios in which distortion is nearly absent, while when the deformation becomes relevant our method shows better description performance.

</details>


### [2] [CDST: Color Disentangled Style Transfer for Universal Style Reference Customization](https://arxiv.org/abs/2506.13770)
*Shiwen Zhang,Zhuowei Chen,Lang Chen,Yanze Wu*

Main category: cs.CV

TL;DR: CDST是一种新颖的双流风格迁移训练范式，通过完全分离颜色与风格，使风格流对颜色不敏感，实现了无需调优的通用风格迁移。


<details>
  <summary>Details</summary>
Motivation: 解决传统风格迁移中颜色干扰风格表达的问题，并首次实现无需调优的特征保留风格迁移。

Method: 采用双流训练范式，分离颜色与风格，结合多特征图像嵌入压缩和基于Diffusion UNet解耦定律的CDST风格定义。

Result: 在多种风格迁移任务中取得最先进效果，并通过定性和定量实验及人工评估验证。

Conclusion: CDST通过颜色解耦和多特征压缩，显著提升了风格相似性并保留了编辑能力，成为风格迁移领域的重要突破。

Abstract: We introduce Color Disentangled Style Transfer (CDST), a novel and efficient two-stream style transfer training paradigm which completely isolates color from style and forces the style stream to be color-blinded. With one same model, CDST unlocks universal style transfer capabilities in a tuning-free manner during inference. Especially, the characteristics-preserved style transfer with style and content references is solved in the tuning-free way for the first time. CDST significantly improves the style similarity by multi-feature image embeddings compression and preserves strong editing capability via our new CDST style definition inspired by Diffusion UNet disentanglement law. By conducting thorough qualitative and quantitative experiments and human evaluations, we demonstrate that CDST achieves state-of-the-art results on various style transfer tasks.

</details>


### [3] [Hidden Bias in the Machine: Stereotypes in Text-to-Image Models](https://arxiv.org/abs/2506.13780)
*Sedat Porikli,Vedat Porikli*

Main category: cs.CV

TL;DR: 研究发现文本到图像（T2I）模型在生成图像时会放大社会偏见，尤其是在性别、种族、年龄等方面。


<details>
  <summary>Details</summary>
Motivation: 探讨T2I模型如何复制和放大社会偏见，以促进更公平的生成视觉系统。

Method: 使用Stable Diffusion 1.5和Flux-1模型生成16,000多张图像，并与Google Image Search的8,000张图像对比。

Result: 生成的图像在性别、种族等方面存在显著差异，强化了有害的社会刻板印象。

Conclusion: 需要更包容的数据集和开发实践以减少偏见。

Abstract: Text-to-Image (T2I) models have transformed visual content creation, producing highly realistic images from natural language prompts. However, concerns persist around their potential to replicate and magnify existing societal biases. To investigate these issues, we curated a diverse set of prompts spanning thematic categories such as occupations, traits, actions, ideologies, emotions, family roles, place descriptions, spirituality, and life events. For each of the 160 unique topics, we crafted multiple prompt variations to reflect a wide range of meanings and perspectives. Using Stable Diffusion 1.5 (UNet-based) and Flux-1 (DiT-based) models with original checkpoints, we generated over 16,000 images under consistent settings. Additionally, we collected 8,000 comparison images from Google Image Search. All outputs were filtered to exclude abstract, distorted, or nonsensical results. Our analysis reveals significant disparities in the representation of gender, race, age, somatotype, and other human-centric factors across generated images. These disparities often mirror and reinforce harmful stereotypes embedded in societal narratives. We discuss the implications of these findings and emphasize the need for more inclusive datasets and development practices to foster fairness in generative visual systems.

</details>


### [4] [Fake it till You Make it: Reward Modeling as Discriminative Prediction](https://arxiv.org/abs/2506.13846)
*Runtao Liu,Jiahao Zhan,Yingqing He,Chen Wei,Alan Yuille,Qifeng Chen*

Main category: cs.CV

TL;DR: GAN-RM是一种高效的奖励建模框架，通过对抗训练避免人工标注和显式质量维度设计，仅需少量目标样本即可实现。


<details>
  <summary>Details</summary>
Motivation: 当前奖励建模方法依赖大量人工标注或复杂的质量维度设计，实现复杂且不完整。

Method: 利用对抗训练，通过区分目标样本与普通生成输出来训练奖励模型，仅需少量目标样本。

Result: 实验证明GAN-RM在多种应用中有效，如测试时缩放和训练后优化。

Conclusion: GAN-RM提供了一种高效且无需人工干预的奖励建模解决方案。

Abstract: An effective reward model plays a pivotal role in reinforcement learning for post-training enhancement of visual generative models. However, current approaches of reward modeling suffer from implementation complexity due to their reliance on extensive human-annotated preference data or meticulously engineered quality dimensions that are often incomplete and engineering-intensive. Inspired by adversarial training in generative adversarial networks (GANs), this paper proposes GAN-RM, an efficient reward modeling framework that eliminates manual preference annotation and explicit quality dimension engineering. Our method trains the reward model through discrimination between a small set of representative, unpaired target samples(denoted as Preference Proxy Data) and model-generated ordinary outputs, requiring only a few hundred target samples. Comprehensive experiments demonstrate our GAN-RM's effectiveness across multiple key applications including test-time scaling implemented as Best-of-N sample filtering, post-training approaches like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).

</details>


### [5] [DeSPITE: Exploring Contrastive Deep Skeleton-Pointcloud-IMU-Text Embeddings for Advanced Point Cloud Human Activity Understanding](https://arxiv.org/abs/2506.13897)
*Thomas Kreutz,Max Mühlhäuser,Alejandro Sanchez Guinea*

Main category: cs.CV

TL;DR: DeSPITE模型通过多模态对比预训练，探索了LiDAR点云、人体骨骼姿态、IMU数据和文本的联合嵌入空间，实现了新颖的人类活动理解任务。


<details>
  <summary>Details</summary>
Motivation: 尽管LiDAR是一种隐私保护的RGB摄像头替代方案，但在多模态对比预训练中仍未充分探索。本文旨在填补这一空白。

Method: 提出DeSPITE模型，通过噪声对比估计学习四种模态的联合嵌入空间，结合LIPD和Babel数据集进行实验。

Result: 实验表明，DeSPITE支持点云序列的新任务（如匹配、检索），并在MSR-Action3D和HMPEAR中验证了其预训练有效性。

Conclusion: DeSPITE为LiDAR点云的人类活动理解提供了有效的多模态预训练策略。

Abstract: Despite LiDAR (Light Detection and Ranging) being an effective privacy-preserving alternative to RGB cameras to perceive human activities, it remains largely underexplored in the context of multi-modal contrastive pre-training for human activity understanding (e.g., human activity recognition (HAR), retrieval, or person re-identification (RE-ID)). To close this gap, our work explores learning the correspondence between LiDAR point clouds, human skeleton poses, IMU data, and text in a joint embedding space. More specifically, we present DeSPITE, a Deep Skeleton-Pointcloud-IMU-Text Embedding model, which effectively learns a joint embedding space across these four modalities through noise contrastive estimation. At the heart of our empirical exploration, we have combined the existing LIPD and Babel datasets, which enabled us to synchronize data of all four modalities, allowing us to explore the learning of a new joint embedding space. Our experiments demonstrate novel human activity understanding tasks for point cloud sequences enabled through DeSPITE, including Skeleton<->Pointcloud<->IMU matching, retrieval, and temporal moment retrieval. Furthermore, we show that DeSPITE is an effective pre-training strategy for point cloud HAR through experiments in MSR-Action3D and HMPEAR.

</details>


### [6] [OPTIMUS: Observing Persistent Transformations in Multi-temporal Unlabeled Satellite-data](https://arxiv.org/abs/2506.13902)
*Raymond Yu,Paul Han,Josh Myers-Dean,Piper Wolters,Favyen Bastani*

Main category: cs.CV

TL;DR: OPTIMUS是一种自监督学习方法，通过检测时间序列中的变化点来识别卫星图像中的持久变化，显著提升了变化检测的性能。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏标注变化标签的卫星数据，尤其是稀有类别变化，监督方法在变化检测中面临挑战。

Method: OPTIMUS基于自监督学习，通过恢复时间序列中图像的相对顺序信息来检测持久变化。

Result: OPTIMUS在区分变化与未变化时间序列上的AUROC得分从56.3%提升至87.6%。

Conclusion: OPTIMUS为卫星图像中的变化检测提供了一种有效的自监督解决方案。

Abstract: In the face of pressing environmental issues in the 21st century, monitoring surface changes on Earth is more important than ever. Large-scale remote sensing, such as satellite imagery, is an important tool for this task. However, using supervised methods to detect changes is difficult because of the lack of satellite data annotated with change labels, especially for rare categories of change. Annotation proves challenging due to the sparse occurrence of changes in satellite images. Even within a vast collection of images, only a small fraction may exhibit persistent changes of interest. To address this challenge, we introduce OPTIMUS, a self-supervised learning method based on an intuitive principle: if a model can recover information about the relative order of images in the time series, then that implies that there are long-lasting changes in the images. OPTIMUS demonstrates this principle by using change point detection methods on model outputs in a time series. We demonstrate that OPTIMUS can directly detect interesting changes in satellite images, achieving an improvement in AUROC score from 56.3% to 87.6% at distinguishing changed time series from unchanged ones compared to baselines. Our code and dataset are available at https://huggingface.co/datasets/optimus-change/optimus-dataset/.

</details>


### [7] [Intelligent Image Sensing for Crime Analysis: A ML Approach towards Enhanced Violence Detection and Investigation](https://arxiv.org/abs/2506.13910)
*Aritra Dutta,Pushpita Boral,G Suseela*

Main category: cs.CV

TL;DR: 本文提出了一种基于机器学习的暴力检测与分类框架，利用3D卷积神经网络和双向LSTM，结合多样化数据集，提高了检测效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统监控方法在及时检测多样化和突发暴力行为方面存在局限性，亟需自动化解决方案以减少人力和财产损失。

Method: 采用监督学习进行二元和多类暴力分类，使用3D卷积神经网络进行检测，可分离卷积3D模型和双向LSTM进行分类。

Result: 在多样化数据集上训练，结合实时视频处理，显著提升了计算资源效率和检测准确性。

Conclusion: 该框架为暴力检测提供了一种高效且准确的自动化解决方案，适用于多种场景。

Abstract: The increasing global crime rate, coupled with substantial human and property losses, highlights the limitations of traditional surveillance methods in promptly detecting diverse and unexpected acts of violence. Addressing this pressing need for automatic violence detection, we leverage Machine Learning to detect and categorize violent events in video streams. This paper introduces a comprehensive framework for violence detection and classification, employing Supervised Learning for both binary and multi-class violence classification. The detection model relies on 3D Convolutional Neural Networks, while the classification model utilizes the separable convolutional 3D model for feature extraction and bidirectional LSTM for temporal processing. Training is conducted on a diverse customized datasets with frame-level annotations, incorporating videos from surveillance cameras, human recordings, hockey fight, sohas and wvd dataset across various platforms. Additionally, a camera module integrated with raspberry pi is used to capture live video feed, which is sent to the ML model for processing. Thus, demonstrating improved performance in terms of computational resource efficiency and accuracy.

</details>


### [8] [HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment](https://arxiv.org/abs/2506.13925)
*Numair Nadeem,Saeed Anwar,Muhammad Hamza Asad,Abdul Bais*

Main category: cs.CV

TL;DR: HierVL是一个统一的半监督语义分割框架，通过结合文本嵌入和掩码变换器，解决了标签稀缺和领域变化的问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 半监督语义分割在标签稀缺和领域变化下表现不佳，视觉语言模型缺乏空间定位能力。

Method: HierVL整合了层次语义查询生成器、跨模态空间对齐模块和双查询变换器解码器，并引入正则化损失。

Result: 在多个基准数据集上，HierVL实现了显著的性能提升（如COCO +4.4% mIoU）。

Conclusion: 语言引导的分割填补了标签效率差距，实现了细粒度的实例感知泛化。

Abstract: Semi-supervised semantic segmentation remains challenging under severe label scarcity and domain variability. Vision-only methods often struggle to generalize, resulting in pixel misclassification between similar classes, poor generalization and boundary localization. Vision-Language Models offer robust, domain-invariant semantics but lack the spatial grounding required for dense prediction. We introduce HierVL, a unified framework that bridges this gap by integrating abstract text embeddings into a mask-transformer architecture tailored for semi-supervised segmentation. HierVL features three novel components: a Hierarchical Semantic Query Generator that filters and projects abstract class embeddings into multi-scale queries to suppress irrelevant classes and handle intra-class variability; a Cross-Modal Spatial Alignment Module that aligns semantic queries with pixel features for sharper boundaries under sparse supervision; and a Dual-Query Transformer Decoder that fuses semantic and instance-level queries to prevent instance collapse. We also introduce targeted regularization losses that maintain vision-language alignment throughout training to reinforce semantic grounding. HierVL establishes a new state-of-the-art by achieving a +4.4% mean improvement of the intersection over the union on COCO (with 232 labeled images), +3.1% on Pascal VOC (with 92 labels), +5.9% on ADE20 (with 158 labels) and +1.8% on Cityscapes (with 100 labels), demonstrating better performance under 1% supervision on four benchmark datasets. Our results show that language-guided segmentation closes the label efficiency gap and unlocks new levels of fine-grained, instance-aware generalization.

</details>


### [9] [Mapping Farmed Landscapes from Remote Sensing](https://arxiv.org/abs/2506.13993)
*Michelangelo Conserva,Alex Wilson,Charlotte Stanton,Vishal Batchu,Varun Gulshan*

Main category: cs.CV

TL;DR: Farmscapes是一种大规模、高分辨率的农村景观地图，通过深度学习模型生成，为生态学家和政策制定者提供开放工具。


<details>
  <summary>Details</summary>
Motivation: 农业景观的有效管理对全球生物多样性目标至关重要，但缺乏详细的大规模生态地图阻碍了相关努力。

Method: 使用深度学习分割模型，基于942个手动标注的航拍图像块训练，生成高分辨率地图。

Result: 模型准确识别关键栖息地，林地（96%）和农田（95%）的F1分数高，树篱分割能力较强（72%）。

Conclusion: Farmscapes为栖息地恢复、生物多样性监测和景观连通性分析提供了数据支持。

Abstract: Effective management of agricultural landscapes is critical for meeting global biodiversity targets, but efforts are hampered by the absence of detailed, large-scale ecological maps. To address this, we introduce Farmscapes, the first large-scale (covering most of England), high-resolution (25cm) map of rural landscape features, including ecologically vital elements like hedgerows, woodlands, and stone walls. This map was generated using a deep learning segmentation model trained on a novel, dataset of 942 manually annotated tiles derived from aerial imagery. Our model accurately identifies key habitats, achieving high f1-scores for woodland (96\%) and farmed land (95\%), and demonstrates strong capability in segmenting linear features, with an F1-score of 72\% for hedgerows. By releasing the England-wide map on Google Earth Engine, we provide a powerful, open-access tool for ecologists and policymakers. This work enables data-driven planning for habitat restoration, supports the monitoring of initiatives like the EU Biodiversity Strategy, and lays the foundation for advanced analysis of landscape connectivity.

</details>


### [10] [FindMeIfYouCan: Bringing Open Set metrics to $\textit{near} $, $ \textit{far} $ and $\textit{farther}$ Out-of-Distribution Object Detection](https://arxiv.org/abs/2506.14008)
*Daniel Montoya,Aymen Bouguerra,Alexandra Gomez-Villa,Fabio Arnez*

Main category: cs.CV

TL;DR: 论文指出当前OOD-OD评估协议存在问题，提出新的评估框架和指标，揭示OOD对象检测的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有OOD-OD评估协议假设ID和OOD对象不重叠，忽略了关键场景，可能导致部署时对未知对象的过度自信。

Method: 通过语义相似性手动构建新评估数据集（near、far、farther），并引入开放集领域的指标。

Result: 语义和视觉接近的OOD对象更易定位但易与ID混淆；far和farther对象更难定位但不易误判为ID。

Conclusion: 新评估框架揭示了OOD检测的复杂性，为未来研究提供了更全面的基准。

Abstract: State-of-the-art Object Detection (OD) methods predominantly operate under a closed-world assumption, where test-time categories match those encountered during training. However, detecting and localizing unknown objects is crucial for safety-critical applications in domains such as autonomous driving and medical imaging. Recently, Out-Of-Distribution (OOD) detection has emerged as a vital research direction for OD, focusing on identifying incorrect predictions typically associated with unknown objects. This paper shows that the current evaluation protocol for OOD-OD violates the assumption of non-overlapping objects with respect to the In-Distribution (ID) datasets, and obscures crucial situations such as ignoring unknown objects, potentially leading to overconfidence in deployment scenarios where truly novel objects might be encountered. To address these limitations, we manually curate, and enrich the existing benchmark by exploiting semantic similarity to create new evaluation splits categorized as $\textit{near}$, $\textit{far}$, and $\textit{farther}$ from ID distributions. Additionally, we incorporate established metrics from the Open Set community, providing deeper insights into how effectively methods detect unknowns, when they ignore them, and when they mistakenly classify OOD objects as ID. Our comprehensive evaluation demonstrates that semantically and visually close OOD objects are easier to localize than far ones, but are also more easily confounded with ID objects. $\textit{Far}$ and $\textit{farther}$ objects are harder to localize but less prone to be taken for an ID object.

</details>


### [11] [Disentangling 3D from Large Vision-Language Models for Controlled Portrait Generation](https://arxiv.org/abs/2506.14015)
*Nick Yiwen Huang,Akin Caliskan,Berkay Kicanaoglu,James Tompkin,Hyeongwoo Kim*

Main category: cs.CV

TL;DR: 论文提出了一种从大型视觉语言模型中解耦3D信息的方法，用于生成可控的3D肖像，支持文本和3D几何控制。


<details>
  <summary>Details</summary>
Motivation: 解决如何从预训练的大型视觉语言模型（LVLM）中解耦3D信息，以实现对肖像外观和几何的自由控制。

Method: 使用规范化技术将3D信息解耦到2D参考框架，并通过Jacobian正则化处理LVLM嵌入空间中的噪声。

Result: 生成的肖像在文本和3D控制下保持一致性和多样性，优于现有方法。

Conclusion: 该方法为创作者提供了无需大规模标注数据或训练大型模型的3D生成控制能力。

Abstract: We consider the problem of disentangling 3D from large vision-language models, which we show on generative 3D portraits. This allows free-form text control of appearance attributes like age, hair style, and glasses, and 3D geometry control of face expression and camera pose. In this setting, we assume we use a pre-trained large vision-language model (LVLM; CLIP) to generate from a smaller 2D dataset with no additional paired labels and with a pre-defined 3D morphable model (FLAME). First, we disentangle using canonicalization to a 2D reference frame from a deformable neural 3D triplane representation. But another form of entanglement arises from the significant noise in the LVLM's embedding space that describes irrelevant features. This damages output quality and diversity, but we overcome this with a Jacobian regularization that can be computed efficiently with a stochastic approximator. Compared to existing methods, our approach produces portraits with added text and 3D control, where portraits remain consistent when either control is changed. Broadly, this approach lets creators control 3D generators on their own 2D face data without needing resources to label large data or train large models.

</details>


### [12] [SimpleDoc: Multi-Modal Document Understanding with Dual-Cue Page Retrieval and Iterative Refinement](https://arxiv.org/abs/2506.14035)
*Chelsi Jain,Yiran Wu,Yifan Zeng,Jiale Liu,S hengyu Dai,Zhenwen Shao,Qingyun Wu,Huazheng Wang*

Main category: cs.CV

TL;DR: SimpleDoc是一个轻量级但强大的检索增强框架，用于DocVQA任务，通过双线索检索和迭代推理显著提升性能。


<details>
  <summary>Details</summary>
Motivation: DocVQA任务需要处理多页和多模态信息，现有方法虽然采用RAG流程，但仍有改进空间。

Method: SimpleDoc通过嵌入相似性检索候选页面，再基于页面摘要过滤和重排序，迭代调用VLM推理器逐步回答问题。

Result: 在4个DocVQA数据集上平均性能提升3.2%，且检索页面更少。

Conclusion: SimpleDoc通过高效检索和迭代推理，显著提升了DocVQA任务的性能。

Abstract: Document Visual Question Answering (DocVQA) is a practical yet challenging task, which is to ask questions based on documents while referring to multiple pages and different modalities of information, e.g, images and tables. To handle multi-modality, recent methods follow a similar Retrieval Augmented Generation (RAG) pipeline, but utilize Visual Language Models (VLMs) based embedding model to embed and retrieve relevant pages as images, and generate answers with VLMs that can accept an image as input. In this paper, we introduce SimpleDoc, a lightweight yet powerful retrieval - augmented framework for DocVQA. It boosts evidence page gathering by first retrieving candidates through embedding similarity and then filtering and re-ranking these candidates based on page summaries. A single VLM-based reasoner agent repeatedly invokes this dual-cue retriever, iteratively pulling fresh pages into a working memory until the question is confidently answered. SimpleDoc outperforms previous baselines by 3.2% on average on 4 DocVQA datasets with much fewer pages retrieved. Our code is available at https://github.com/ag2ai/SimpleDoc.

</details>


### [13] [Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems](https://arxiv.org/abs/2506.14096)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CV

TL;DR: 本文综述了大型语言模型（LLM）与计算机视觉结合在图像分割领域的应用，特别是在智能交通系统（ITS）中的潜力、挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统需要精确的场景理解以确保安全和效率，而LLM与计算机视觉的结合为此提供了新的可能性。

Method: 文章系统性地回顾了LLM增强图像分割的当前方法，基于提示机制和核心架构进行了分类。

Result: 这些创新方法可以提升自动驾驶、交通监控和基础设施维护中的道路场景理解能力。

Conclusion: 未来需解决实时性和安全性等关键挑战，并发展可解释、以人为本的AI技术，以推动下一代交通系统的成功部署。

Abstract: The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems.

</details>


### [14] [FADPNet: Frequency-Aware Dual-Path Network for Face Super-Resolution](https://arxiv.org/abs/2506.14121)
*Siyu Xu,Wenjie Li,Guangwei Gao,Jian Yang,Guo-Jun Qi,Chia-Wen Lin*

Main category: cs.CV

TL;DR: 提出了一种频率感知双路径网络（FADPNet），通过将面部特征分解为低频和高频分量，分别用Mamba和CNN处理，以优化计算资源分配并提升超分辨率性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法对所有面部像素同等处理，导致计算资源分配不佳和性能下降。CNN对高频特征敏感，而Mamba擅长低频特征且复杂度更低。

Method: FADPNet将特征分解为低频和高频分量，分别用Mamba-based LFEB和CNN-based DPA模块处理，辅以HFR模块细化高频表示。

Result: 方法在超分辨率质量和模型效率之间取得了优异平衡，性能优于现有方法。

Conclusion: FADPNet通过频率感知的双路径设计，显著提升了面部超分辨率的性能和效率。

Abstract: Face super-resolution (FSR) under limited computational costs remains an open problem. Existing approaches typically treat all facial pixels equally, resulting in suboptimal allocation of computational resources and degraded FSR performance. CNN is relatively sensitive to high-frequency facial features, such as component contours and facial outlines. Meanwhile, Mamba excels at capturing low-frequency features like facial color and fine-grained texture, and does so with lower complexity than Transformers. Motivated by these observations, we propose FADPNet, a Frequency-Aware Dual-Path Network that decomposes facial features into low- and high-frequency components and processes them via dedicated branches. For low-frequency regions, we introduce a Mamba-based Low-Frequency Enhancement Block (LFEB), which combines state-space attention with squeeze-and-excitation operations to extract low-frequency global interactions and emphasize informative channels. For high-frequency regions, we design a CNN-based Deep Position-Aware Attention (DPA) module to enhance spatially-dependent structural details, complemented by a lightweight High-Frequency Refinement (HFR) module that further refines frequency-specific representations. Through the above designs, our method achieves an excellent balance between FSR quality and model efficiency, outperforming existing approaches.

</details>


### [15] [KDMOS:Knowledge Distillation for Motion Segmentation](https://arxiv.org/abs/2506.14130)
*Chunyu Cao,Jintao Cheng,Zeyu Chen,Linfan Zhan,Rui Fan,Zhijian He,Xiaoyu Tang*

Main category: cs.CV

TL;DR: 提出了一种基于logits的知识蒸馏框架（KDMOS），用于运动目标分割（MOS），通过BEV投影模型（学生）和非投影模型（教师）的结合，优化了精度和实时性。


<details>
  <summary>Details</summary>
Motivation: 运动目标分割在自动驾驶中至关重要，但现有方法在精度和实时性之间难以平衡。

Method: 采用BEV投影模型作为学生，非投影模型作为教师，解耦移动与非移动类别并应用定制蒸馏策略，引入动态上采样和网络架构优化。

Result: 在SemanticKITTI-MOS隐藏测试集上达到78.8%的IoU，参数数量减少7.69%，在Apollo数据集上也表现优异。

Conclusion: KDMOS框架显著提升了运动目标分割的精度和效率，适用于自动驾驶场景。

Abstract: Motion Object Segmentation (MOS) is crucial for autonomous driving, as it enhances localization, path planning, map construction, scene flow estimation, and future state prediction. While existing methods achieve strong performance, balancing accuracy and real-time inference remains a challenge. To address this, we propose a logits-based knowledge distillation framework for MOS, aiming to improve accuracy while maintaining real-time efficiency. Specifically, we adopt a Bird's Eye View (BEV) projection-based model as the student and a non-projection model as the teacher. To handle the severe imbalance between moving and non-moving classes, we decouple them and apply tailored distillation strategies, allowing the teacher model to better learn key motion-related features. This approach significantly reduces false positives and false negatives. Additionally, we introduce dynamic upsampling, optimize the network architecture, and achieve a 7.69% reduction in parameter count, mitigating overfitting. Our method achieves a notable IoU of 78.8% on the hidden test set of the SemanticKITTI-MOS dataset and delivers competitive results on the Apollo dataset. The KDMOS implementation is available at https://github.com/SCNU-RISLAB/KDMOS.

</details>


### [16] [Interpreting Biomedical VLMs on High-Imbalance Out-of-Distributions: An Insight into BiomedCLIP on Radiology](https://arxiv.org/abs/2506.14136)
*Nafiz Sadman,Farhana Zulkernine,Benjamin Kwan*

Main category: cs.CV

TL;DR: 本文研究了BiomedCLIP在医学影像分类中的表现，分析了其嵌入空间和局限性，并探讨了零样本推理、全微调和线性探测的效果。


<details>
  <summary>Details</summary>
Motivation: 探索BiomedCLIP在医学影像分类中的表现，尤其是在数据不平衡和分布外情况下的局限性，以提升其实际应用的可靠性。

Method: 在IU-xray数据集上测试BiomedCLIP，采用零样本推理、全微调和线性探测三种方式，并使用Grad-CAM热图分析模型理解。

Result: 零样本设置下模型预测过泛，精度低；全微调改善了疾病分类；线性探测能识别重叠特征。

Conclusion: 需谨慎调整模型以适应实际应用场景，代码已开源。

Abstract: In this paper, we construct two research objectives: i) explore the learned embedding space of BiomedCLIP, an open-source large vision language model, to analyse meaningful class separations, and ii) quantify the limitations of BiomedCLIP when applied to a highly imbalanced, out-of-distribution multi-label medical dataset. We experiment on IU-xray dataset, which exhibits the aforementioned criteria, and evaluate BiomedCLIP in classifying images (radiographs) in three contexts: zero-shot inference, full finetuning, and linear probing. The results show that the model under zero-shot settings over-predicts all labels, leading to poor precision and inter-class separability. Full fine-tuning improves classification of distinct diseases, while linear probing detects overlapping features. We demonstrate visual understanding of the model using Grad-CAM heatmaps and compare with 15 annotations by a radiologist. We highlight the need for careful adaptations of the models to foster reliability and applicability in a real-world setting. The code for the experiments in this work is available and maintained on GitHub.

</details>


### [17] [FGA-NN: Film Grain Analysis Neural Network](https://arxiv.org/abs/2506.14350)
*Zoubida Ameur,Frédéric Lefebvre,Philippe De Lagrange,Miloš Radosavljević*

Main category: cs.CV

TL;DR: FGA-NN是一种基于学习的方法，用于分析和建模胶片颗粒，以在压缩后保留其艺术效果。


<details>
  <summary>Details</summary>
Motivation: 胶片颗粒在低比特率压缩中容易丢失，影响艺术效果，因此需要一种方法来分析和建模胶片颗粒。

Method: 提出FGA-NN，一种基于学习的方法，用于估计与常规合成兼容的胶片颗粒参数。

Result: FGA-NN在分析准确性和合成复杂性之间取得了优越的平衡，并表现出鲁棒性和适用性。

Conclusion: FGA-NN是一种有效的方法，能够在压缩后保留胶片颗粒的艺术效果。

Abstract: Film grain, once a by-product of analog film, is now present in most cinematographic content for aesthetic reasons. However, when such content is compressed at medium to low bitrates, film grain is lost due to its random nature. To preserve artistic intent while compressing efficiently, film grain is analyzed and modeled before encoding and synthesized after decoding. This paper introduces FGA-NN, the first learning-based film grain analysis method to estimate conventional film grain parameters compatible with conventional synthesis. Quantitative and qualitative results demonstrate FGA-NN's superior balance between analysis accuracy and synthesis complexity, along with its robustness and applicability.

</details>


### [18] [RadFabric: Agentic AI System with Reasoning Capability for Radiology](https://arxiv.org/abs/2506.14142)
*Wenting Chen,Yi Dong,Zhaojun Ding,Yucheng Shi,Yifan Zhou,Fang Zeng,Yijun Luo,Tianyu Lin,Yihang Su,Yichen Wu,Kai Zhang,Zhen Xiang,Tianming Liu,Ninghao Liu,Lichao Sun,Yixuan Yuan,Xiang Li*

Main category: cs.CV

TL;DR: RadFabric是一个多模态、多代理的框架，通过视觉和文本分析提升CXR诊断的全面性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前CXR自动诊断系统在病理覆盖、诊断准确性和多模态推理方面存在不足，RadFabric旨在解决这些问题。

Method: 基于MCP协议，结合病理检测代理、解剖解释代理和推理代理，实现多模态数据整合。

Result: 在骨折检测上达到1.000准确率，整体诊断准确率0.799，显著优于传统系统。

Conclusion: RadFabric通过多模态对齐和偏好驱动推理，推动了透明、精准且临床可操作的CXR分析。

Abstract: Chest X ray (CXR) imaging remains a critical diagnostic tool for thoracic conditions, but current automated systems face limitations in pathology coverage, diagnostic accuracy, and integration of visual and textual reasoning. To address these gaps, we propose RadFabric, a multi agent, multimodal reasoning framework that unifies visual and textual analysis for comprehensive CXR interpretation. RadFabric is built on the Model Context Protocol (MCP), enabling modularity, interoperability, and scalability for seamless integration of new diagnostic agents. The system employs specialized CXR agents for pathology detection, an Anatomical Interpretation Agent to map visual findings to precise anatomical structures, and a Reasoning Agent powered by large multimodal reasoning models to synthesize visual, anatomical, and clinical data into transparent and evidence based diagnoses. RadFabric achieves significant performance improvements, with near-perfect detection of challenging pathologies like fractures (1.000 accuracy) and superior overall diagnostic accuracy (0.799) compared to traditional systems (0.229 to 0.527). By integrating cross modal feature alignment and preference-driven reasoning, RadFabric advances AI-driven radiology toward transparent, anatomically precise, and clinically actionable CXR analysis.

</details>


### [19] [Unsupervised Imaging Inverse Problems with Diffusion Distribution Matching](https://arxiv.org/abs/2506.14605)
*Giacomo Meanti,Thomas Ryckeboer,Michael Arbel,Julien Mairal*

Main category: cs.CV

TL;DR: 本文提出了一种基于逆问题和未配对数据集的图像恢复方法，适用于现实场景中前向模型未知或数据配对困难的情况。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常需要完整的前向模型或配对数据，而现实中这些条件难以满足。本文旨在解决这一问题。

Method: 利用条件流匹配建模退化观测分布，同时通过分布匹配损失学习前向模型。

Result: 在去模糊和非均匀点扩散函数校准任务中优于单图像盲方法和无监督方法，在盲超分辨率任务中达到先进水平。

Conclusion: 该方法在现实应用中表现出色，尤其适用于数据获取成本高的场景。

Abstract: This work addresses image restoration tasks through the lens of inverse problems using unpaired datasets. In contrast to traditional approaches -- which typically assume full knowledge of the forward model or access to paired degraded and ground-truth images -- the proposed method operates under minimal assumptions and relies only on small, unpaired datasets. This makes it particularly well-suited for real-world scenarios, where the forward model is often unknown or misspecified, and collecting paired data is costly or infeasible. The method leverages conditional flow matching to model the distribution of degraded observations, while simultaneously learning the forward model via a distribution-matching loss that arises naturally from the framework. Empirically, it outperforms both single-image blind and unsupervised approaches on deblurring and non-uniform point spread function (PSF) calibration tasks. It also matches state-of-the-art performance on blind super-resolution. We also showcase the effectiveness of our method with a proof of concept for lens calibration: a real-world application traditionally requiring time-consuming experiments and specialized equipment. In contrast, our approach achieves this with minimal data acquisition effort.

</details>


### [20] [SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with LLM-Guided Walkability](https://arxiv.org/abs/2506.14144)
*Juho Bai,Inwook Shim*

Main category: cs.CV

TL;DR: 论文提出SceneAware框架，通过结合场景理解提升行人轨迹预测准确性，利用ViT和MLLM处理环境信息，实验显示性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注行人间的社交互动，但忽略了环境对行人轨迹的显著影响，需结合场景信息提升预测准确性。

Method: 结合ViT场景编码器和MLLM生成可通行掩码，通过Transformer编码时空动态与空间约束，并引入碰撞惩罚机制。

Result: 在ETH/UCY数据集上性能提升50%以上，且在不同行人运动类型中表现一致。

Conclusion: SceneAware证明了结合场景信息的有效性，能生成准确且物理合理的预测。

Abstract: Accurate prediction of pedestrian trajectories is essential for applications in robotics and surveillance systems. While existing approaches primarily focus on social interactions between pedestrians, they often overlook the rich environmental context that significantly shapes human movement patterns. In this paper, we propose SceneAware, a novel framework that explicitly incorporates scene understanding to enhance trajectory prediction accuracy. Our method leverages a Vision Transformer~(ViT) scene encoder to process environmental context from static scene images, while Multi-modal Large Language Models~(MLLMs) generate binary walkability masks that distinguish between accessible and restricted areas during training. We combine a Transformer-based trajectory encoder with the ViT-based scene encoder, capturing both temporal dynamics and spatial constraints. The framework integrates collision penalty mechanisms that discourage predicted trajectories from violating physical boundaries, ensuring physically plausible predictions. SceneAware is implemented in both deterministic and stochastic variants. Comprehensive experiments on the ETH/UCY benchmark datasets show that our approach outperforms state-of-the-art methods, with more than 50\% improvement over previous models. Our analysis based on different trajectory categories shows that the model performs consistently well across various types of pedestrian movement. This highlights the importance of using explicit scene information and shows that our scene-aware approach is both effective and reliable in generating accurate and physically plausible predictions. Code is available at: https://github.com/juho127/SceneAware.

</details>


### [21] [VideoMAR: Autoregressive Video Generatio with Continuous Tokens](https://arxiv.org/abs/2506.14168)
*Hu Yu,Biao Gong,Hangjie Yuan,DanDan Zheng,Weilong Chai,Jingdong Chen,Kecheng Zheng,Feng Zhao*

Main category: cs.CV

TL;DR: VideoMAR是一种高效的解码器自回归图像到视频模型，结合了时间因果性和空间双向性，通过课程学习和渐进分辨率训练解决了长序列建模问题，并在性能和资源效率上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索基于掩码的自回归模型在视频生成中的潜力，解决长序列建模的高成本和困难。

Method: 提出VideoMAR模型，结合时间因果性和空间双向性，采用下一帧扩散损失、课程学习和渐进分辨率训练，并引入3D旋转嵌入。

Result: 在VBench-I2V基准测试中，VideoMAR性能优于Cosmos I2V，且参数、训练数据和GPU资源需求显著降低。

Conclusion: VideoMAR展示了自回归模型在视频生成中的高效性和潜力，为未来研究提供了新方向。

Abstract: Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\%$), training data ($0.5\%$), and GPU resources ($0.2\%$).

</details>


### [22] [A multi-stage augmented multimodal interaction network for fish feeding intensity quantification](https://arxiv.org/abs/2506.14170)
*Shulong Zhang,Mingyuan Yao,Jiayin Zhao,Xiao Liu,Haihua Wang*

Main category: cs.CV

TL;DR: 本文提出了一种多阶段增强多模态交互网络（MAINet），用于量化鱼类摄食强度，通过特征提取、模态交互增强和证据推理规则，显著提高了准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前研究在多模态选择、特征提取与融合以及决策推理方面存在局限性，限制了多模态融合模型的准确性、适用性和可靠性。

Method: 提出了MAINet，包括通用特征提取框架、辅助模态增强主模态机制（ARPM）和证据推理（ER）规则。

Result: 实验结果显示MAINet在准确率、精确率、召回率和F1分数上均超过96.7%，显著优于其他模型。

Conclusion: MAINet通过改进策略提高了模型的鲁棒性和特征利用效率，有效提升了鱼类摄食强度量化结果的准确性。

Abstract: In recirculating aquaculture systems, accurate and effective assessment of fish feeding intensity is crucial for reducing feed costs and calculating optimal feeding times. However, current studies have limitations in modality selection, feature extraction and fusion, and co-inference for decision making, which restrict further improvement in the accuracy, applicability and reliability of multimodal fusion models. To address this problem, this study proposes a Multi-stage Augmented Multimodal Interaction Network (MAINet) for quantifying fish feeding intensity. Firstly, a general feature extraction framework is proposed to efficiently extract feature information from input image, audio and water wave datas. Second, an Auxiliary-modality Reinforcement Primary-modality Mechanism (ARPM) is designed for inter-modal interaction and generate enhanced features, which consists of a Channel Attention Fusion Network (CAFN) and a Dual-mode Attention Fusion Network (DAFN). Finally, an Evidence Reasoning (ER) rule is introduced to fuse the output results of each modality and make decisions, thereby completing the quantification of fish feeding intensity. The experimental results show that the constructed MAINet reaches 96.76%, 96.78%, 96.79% and 96.79% in accuracy, precision, recall and F1-Score respectively, and its performance is significantly higher than the comparison models. Compared with models that adopt single-modality, dual-modality fusion and different decision-making fusion methods, it also has obvious advantages. Meanwhile, the ablation experiments further verified the key role of the proposed improvement strategy in improving the robustness and feature utilization efficiency of model, which can effectively improve the accuracy of the quantitative results of fish feeding intensity.

</details>


### [23] [One-Shot Neural Architecture Search with Network Similarity Directed Initialization for Pathological Image Classification](https://arxiv.org/abs/2506.14176)
*Renao Yan*

Main category: cs.CV

TL;DR: 提出了一种基于网络相似性指导初始化（NSDI）的策略，结合领域自适应的一击神经架构搜索（NAS），以优化病理图像分析的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接将计算机视觉模型应用于医学任务，忽略了病理图像的独特性，导致计算效率低下，尤其在边缘计算场景中。

Method: 引入NSDI策略提升NAS的稳定性，并结合领域自适应处理病理数据集的染色和语义尺度变化。

Result: 在BRACS数据集上，该方法优于现有方法，分类性能和临床相关特征定位均表现更优。

Conclusion: NSDI结合领域自适应的一击NAS能有效提升病理图像分析的性能和效率。

Abstract: Deep learning-based pathological image analysis presents unique challenges due to the practical constraints of network design. Most existing methods apply computer vision models directly to medical tasks, neglecting the distinct characteristics of pathological images. This mismatch often leads to computational inefficiencies, particularly in edge-computing scenarios. To address this, we propose a novel Network Similarity Directed Initialization (NSDI) strategy to improve the stability of neural architecture search (NAS). Furthermore, we introduce domain adaptation into one-shot NAS to better handle variations in staining and semantic scale across pathology datasets. Experiments on the BRACS dataset demonstrate that our method outperforms existing approaches, delivering both superior classification performance and clinically relevant feature localization.

</details>


### [24] [Meta-SurDiff: Classification Diffusion Model Optimized by Meta Learning is Reliable for Online Surgical Phase Recognition](https://arxiv.org/abs/2506.14181)
*Yufei Li,Jirui Wu,Long Tian,Liming Wang,Xiaonan Liu,Zijun Liu,Xiyang Liu*

Main category: cs.CV

TL;DR: 论文提出Meta-SurDiff模型，通过分类扩散模型和元学习解决手术视频中帧模糊和相位分布不平衡的不确定性，提升在线手术阶段识别的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在线手术阶段识别对人类生命健康至关重要，但现有深度模型未充分探索手术视频中的不确定性（如帧模糊和相位分布不平衡）。

Method: 提出Meta-SurDiff模型，结合分类扩散模型（处理帧模糊）和元学习（优化相位分布不平衡），实现精细帧级分布估计。

Result: 在五个数据集（Cholec80、AutoLaparo等）上验证了Meta-SurDiff的有效性，使用四个以上实用指标。

Conclusion: Meta-SurDiff通过建模不确定性显著提升了在线手术阶段识别的可靠性，代码将公开。

Abstract: Online surgical phase recognition has drawn great attention most recently due to its potential downstream applications closely related to human life and health. Despite deep models have made significant advances in capturing the discriminative long-term dependency of surgical videos to achieve improved recognition, they rarely account for exploring and modeling the uncertainty in surgical videos, which should be crucial for reliable online surgical phase recognition. We categorize the sources of uncertainty into two types, frame ambiguity in videos and unbalanced distribution among surgical phases, which are inevitable in surgical videos. To address this pivot issue, we introduce a meta-learning-optimized classification diffusion model (Meta-SurDiff), to take full advantage of the deep generative model and meta-learning in achieving precise frame-level distribution estimation for reliable online surgical phase recognition. For coarse recognition caused by ambiguous video frames, we employ a classification diffusion model to assess the confidence of recognition results at a finer-grained frame-level instance. For coarse recognition caused by unbalanced phase distribution, we use a meta-learning based objective to learn the diffusion model, thus enhancing the robustness of classification boundaries for different surgical phases.We establish effectiveness of Meta-SurDiff in online surgical phase recognition through extensive experiments on five widely used datasets using more than four practical metrics. The datasets include Cholec80, AutoLaparo, M2Cai16, OphNet, and NurViD, where OphNet comes from ophthalmic surgeries, NurViD is the daily care dataset, while the others come from laparoscopic surgeries. We will release the code upon acceptance.

</details>


### [25] [Egocentric Human-Object Interaction Detection: A New Benchmark and Method](https://arxiv.org/abs/2506.14189)
*Kunyuan Deng,Yi Wang,Lap-Pui Chau*

Main category: cs.CV

TL;DR: 该论文提出了一个新的数据集Ego-HOIBench，用于推动以自我为中心视角的人-物交互（Ego-HOI）检测的发展，并提出了一个轻量级的HGIR方案来提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的人-物交互（HOI）检测方法主要关注第三人称视角，忽略了更直观的自我中心视角（Ego-HOI）。为此，作者提出了Ego-HOIBench数据集，并探索了Ego-HOI检测的挑战。

Method: 作者提出了Hand Geometry and Interactivity Refinement（HGIR）方案，利用手部姿态和几何信息作为交互理解的线索，通过全局手部几何特征提取和姿态-交互注意力机制优化交互特征。

Result: HGIR方案显著提升了Ego-HOI检测能力，并在Ego-HOIBench数据集上实现了最先进的性能。

Conclusion: Ego-HOIBench数据集和HGIR方案为Ego-HOI检测提供了新的基准和工具，推动了该领域的发展。

Abstract: Understanding the interaction between humans and objects has gained much attention in recent years. Existing human-object interaction (HOI) detection methods mainly focus on the third-person perspectives, overlooking a more intuitive way from the egocentric view of HOI, namely Ego-HOI. This paper introduces an Ego-HOIBench, a new dataset to promote the benchmarking and development of Ego-HOI detection. Our Ego-HOIBench comprises more than 27K egocentric images with high-quality hand-verb-object triplet annotations across 123 fine-grained interaction categories and locations, covering a rich diversity of scenarios, object types, and hand configurations in daily activities. In addition, we explore and adapt third-person HOI detection methods to Ego-HOIBench and illustrate the challenges of hand-occluded objects and the complexity of single- and two-hand interactions. To build a new baseline, we propose a Hand Geometry and Interactivity Refinement (HGIR) scheme, which leverages hand pose and geometric information as valuable cues for interpreting interactions. Specifically, the HGIR scheme explicitly extracts global hand geometric features from the estimated hand pose proposals and refines the interaction-specific features using pose-interaction attention. This scheme enables the model to obtain a robust and powerful interaction representation, significantly improving the Ego-HOI detection capability. Our approach is lightweight and effective, and it can be easily applied to HOI baselines in a plug-and-play manner to achieve state-of-the-art results on Ego-HOIBench. Our project is available at: https://dengkunyuan.github.io/EgoHOIBench/

</details>


### [26] [HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction](https://arxiv.org/abs/2506.14229)
*Changbai Li,Haodong Zhu,Hanlin Chen,Juan Zhang,Tongfei Chen,Shuo Yang,Shuwei Shao,Wenhao Dong,Baochang Zhang*

Main category: cs.CV

TL;DR: HRGS是一种内存高效的分层高斯溅射框架，通过块级优化和重要性驱动的高斯修剪，解决了3DGS在高分辨率场景中的内存扩展问题。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射（3DGS）在实时3D场景重建中取得了显著进展，但在高分辨率场景下面临内存扩展问题。

Method: HRGS首先生成低分辨率数据的全局粗高斯表示，然后将场景划分为多个块，用高分辨率数据细化每个块。通过高斯分区和训练数据分区实现任务分配，并引入重要性驱动的高斯修剪（IDGP）以减少计算需求。

Result: 在三个基准测试中，HRGS在高分辨率新视角合成（NVS）和表面重建任务中实现了最先进的性能。

Conclusion: HRGS是一种高效的方法，能够在内存受限的情况下实现高质量的高分辨率3D场景重建。

Abstract: 3D Gaussian Splatting (3DGS) has made significant strides in real-time 3D scene reconstruction, but faces memory scalability issues in high-resolution scenarios. To address this, we propose Hierarchical Gaussian Splatting (HRGS), a memory-efficient framework with hierarchical block-level optimization. First, we generate a global, coarse Gaussian representation from low-resolution data. Then, we partition the scene into multiple blocks, refining each block with high-resolution data. The partitioning involves two steps: Gaussian partitioning, where irregular scenes are normalized into a bounded cubic space with a uniform grid for task distribution, and training data partitioning, where only relevant observations are retained for each block. By guiding block refinement with the coarse Gaussian prior, we ensure seamless Gaussian fusion across adjacent blocks. To reduce computational demands, we introduce Importance-Driven Gaussian Pruning (IDGP), which computes importance scores for each Gaussian and removes those with minimal contribution, speeding up convergence and reducing memory usage. Additionally, we incorporate normal priors from a pretrained model to enhance surface reconstruction quality. Our method enables high-quality, high-resolution 3D scene reconstruction even under memory constraints. Extensive experiments on three benchmarks show that HRGS achieves state-of-the-art performance in high-resolution novel view synthesis (NVS) and surface reconstruction tasks.

</details>


### [27] [Unified Representation Space for 3D Visual Grounding](https://arxiv.org/abs/2506.14238)
*Yinuo Zheng,Lipeng Gu,Honghua Chen,Liangliang Nan,Mingqiang Wei*

Main category: cs.CV

TL;DR: UniSpace-3D提出了一种统一表示空间的方法，通过结合CLIP预训练模型和多模态对比学习，显著提升了3D视觉定位任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖独立的视觉和文本编码器，导致模态间存在几何和语义差距，影响定位和分类准确性。

Method: 1) 统一表示编码器；2) 多模态对比学习模块；3) 语言引导的查询选择模块。

Result: 在ScanRefer和Nr3D/Sr3D数据集上，性能至少提升2.24%。

Conclusion: UniSpace-3D通过统一表示空间有效缩小了模态差距，提升了3D视觉定位的准确性。

Abstract: 3D visual grounding (3DVG) is a critical task in scene understanding that aims to identify objects in 3D scenes based on text descriptions. However, existing methods rely on separately pre-trained vision and text encoders, resulting in a significant gap between the two modalities in terms of spatial geometry and semantic categories. This discrepancy often causes errors in object positioning and classification. The paper proposes UniSpace-3D, which innovatively introduces a unified representation space for 3DVG, effectively bridging the gap between visual and textual features. Specifically, UniSpace-3D incorporates three innovative designs: i) a unified representation encoder that leverages the pre-trained CLIP model to map visual and textual features into a unified representation space, effectively bridging the gap between the two modalities; ii) a multi-modal contrastive learning module that further reduces the modality gap; iii) a language-guided query selection module that utilizes the positional and semantic information to identify object candidate points aligned with textual descriptions. Extensive experiments demonstrate that UniSpace-3D outperforms baseline models by at least 2.24% on the ScanRefer and Nr3D/Sr3D datasets. The code will be made available upon acceptance of the paper.

</details>


### [28] [Cross-Modal Geometric Hierarchy Fusion: An Implicit-Submap Driven Framework for Resilient 3D Place Recognition](https://arxiv.org/abs/2506.14243)
*Xiaohui Jiang,Haijiang Zhu,Chadei Li,Fulin Tang,Ning An*

Main category: cs.CV

TL;DR: 提出了一种基于弹性点的密度无关几何推理框架，解决了LiDAR地点识别中的点云密度不一致和表示脆弱性问题，实现了多数据集上的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR地点识别方法因点云密度不一致和单层几何抽象缺乏判别力，导致描述符不稳定和表示脆弱。

Method: 引入弹性点的隐式3D表示，生成均匀分布的点云，并从中提取占用网格和法向量信息，融合鸟瞰图和3D片段的几何信息生成描述符。

Result: 在多个数据集（KITTI、KITTI-360等）上实现了最优性能，并在准确性、运行时间和内存优化之间取得平衡。

Conclusion: 该方法具有鲁棒性和可扩展性，未来将开源代码。

Abstract: LiDAR-based place recognition serves as a crucial enabler for long-term autonomy in robotics and autonomous driving systems. Yet, prevailing methodologies relying on handcrafted feature extraction face dual challenges: (1) Inconsistent point cloud density, induced by ego-motion dynamics and environmental disturbances during repeated traversals, leads to descriptor instability, and (2) Representation fragility stems from reliance on single-level geometric abstractions that lack discriminative power in structurally complex scenarios. To address these limitations, we propose a novel framework that redefines 3D place recognition through density-agnostic geometric reasoning. Specifically, we introduce an implicit 3D representation based on elastic points, which is immune to the interference of original scene point cloud density and achieves the characteristic of uniform distribution. Subsequently, we derive the occupancy grid and normal vector information of the scene from this implicit representation. Finally, with the aid of these two types of information, we obtain descriptors that fuse geometric information from both bird's-eye view (capturing macro-level spatial layouts) and 3D segment (encoding micro-scale surface geometries) perspectives. We conducted extensive experiments on numerous datasets (KITTI, KITTI-360, MulRan, NCLT) across diverse environments. The experimental results demonstrate that our method achieves state-of-the-art performance. Moreover, our approach strikes an optimal balance between accuracy, runtime, and memory optimization for historical maps, showcasing excellent Resilient and scalability. Our code will be open-sourced in the future.

</details>


### [29] [synth-dacl: Does Synthetic Defect Data Enhance Segmentation Accuracy and Robustness for Real-World Bridge Inspections?](https://arxiv.org/abs/2506.14255)
*Johannes Flotzinger,Fabian Deuser,Achref Jaziri,Heiko Neumann,Norbert Oswald,Visvanathan Ramesh,Thomas Braml*

Main category: cs.CV

TL;DR: 论文提出了一种基于合成数据的方法（synth-dacl）来改善桥梁视觉检测中缺陷分类的性能，特别是在裂缝和空洞等细粒度类别上。


<details>
  <summary>Details</summary>
Motivation: 由于桥梁老化、资源不足，自动化视觉检测需求增加，但现有数据集（dacl10k）存在类别不平衡问题，影响模型性能。

Method: 通过合成混凝土纹理扩展数据集（synth-dacl），平衡类别分布，提升模型在复杂条件下的鲁棒性。

Result: 结合合成数据后，模型在15个扰动测试集上表现显著提升，平均IoU、F1分数、召回率和精确率均提高2%。

Conclusion: 合成数据扩展有效解决了类别不平衡问题，提升了桥梁缺陷检测的准确性和鲁棒性。

Abstract: Adequate bridge inspection is increasingly challenging in many countries due to growing ailing stocks, compounded with a lack of staff and financial resources. Automating the key task of visual bridge inspection, classification of defects and building components on pixel level, improves efficiency, increases accuracy and enhances safety in the inspection process and resulting building assessment. Models overtaking this task must cope with an assortment of real-world conditions. They must be robust to variations in image quality, as well as background texture, as defects often appear on surfaces of diverse texture and degree of weathering. dacl10k is the largest and most diverse dataset for real-world concrete bridge inspections. However, the dataset exhibits class imbalance, which leads to notably poor model performance particularly when segmenting fine-grained classes such as cracks and cavities. This work introduces "synth-dacl", a compilation of three novel dataset extensions based on synthetic concrete textures. These extensions are designed to balance class distribution in dacl10k and enhance model performance, especially for crack and cavity segmentation. When incorporating the synth-dacl extensions, we observe substantial improvements in model robustness across 15 perturbed test sets. Notably, on the perturbed test set, a model trained on dacl10k combined with all synthetic extensions achieves a 2% increase in mean IoU, F1 score, Recall, and Precision compared to the same model trained solely on dacl10k.

</details>


### [30] [Comparison of Two Methods for Stationary Incident Detection Based on Background Image](https://arxiv.org/abs/2506.14256)
*Deepak Ghimire,Joonwhoan Lee*

Main category: cs.CV

TL;DR: 论文提出两种基于背景减法的静止物体检测方案，并比较其性能和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统背景减法用于检测移动物体，但本文旨在检测临时静止物体。

Method: 提出两种方案：单背景法和双背景法（不同学习率生成），并结合NCC图像比较跟踪。

Result: 方法对部分遮挡、短时完全遮挡和光照变化鲁棒，且能实时运行。

Conclusion: 双背景法在检测性能和计算复杂度上表现更优。

Abstract: In general, background subtraction-based methods are used to detect moving objects in visual tracking applications. In this paper, we employed a background subtraction-based scheme to detect the temporarily stationary objects. We proposed two schemes for stationary object detection, and we compare those in terms of detection performance and computational complexity. In the first approach, we used a single background, and in the second approach, we used dual backgrounds, generated with different learning rates, in order to detect temporarily stopped objects. Finally, we used normalized cross correlation (NCC) based image comparison to monitor and track the detected stationary object in a video scene. The proposed method is robust with partial occlusion, short-time fully occlusion, and illumination changes, and it can operate in real time.

</details>


### [31] [Exploring Non-contrastive Self-supervised Representation Learning for Image-based Profiling](https://arxiv.org/abs/2506.14265)
*Siran Dai,Qianqian Xu,Peisong Wen,Yang Liu,Qingming Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SSLProfiler的非对比自监督学习框架，专门用于细胞图像分析，解决了细胞图像与自然图像分布差异大以及多图像信息融合的挑战，并在CVPR 2025的Cell Line Transferability挑战中获胜。


<details>
  <summary>Details</summary>
Motivation: 细胞图像分析在药物发现中至关重要，但现有自监督学习方法因细胞图像与自然图像分布差异大且需处理多图像输入而表现不佳。

Method: 提出了SSLProfiler框架，包含专门的数据增强和表示后处理方法，以解决细胞图像的特殊需求。

Result: SSLProfiler在CVPR 2025的Cell Line Transferability挑战中获胜，证明了其有效性。

Conclusion: SSLProfiler为细胞图像分析提供了一种通用且鲁棒的特征提取方法，解决了现有技术的局限性。

Abstract: Image-based cell profiling aims to create informative representations of cell images. This technique is critical in drug discovery and has greatly advanced with recent improvements in computer vision. Inspired by recent developments in non-contrastive Self-Supervised Learning (SSL), this paper provides an initial exploration into training a generalizable feature extractor for cell images using such methods. However, there are two major challenges: 1) There is a large difference between the distributions of cell images and natural images, causing the view-generation process in existing SSL methods to fail; and 2) Unlike typical scenarios where each representation is based on a single image, cell profiling often involves multiple input images, making it difficult to effectively combine all available information. To overcome these challenges, we propose SSLProfiler, a non-contrastive SSL framework specifically designed for cell profiling. We introduce specialized data augmentation and representation post-processing methods tailored to cell images, which effectively address the issues mentioned above and result in a robust feature extractor. With these improvements, SSLProfiler won the Cell Line Transferability challenge at CVPR 2025.

</details>


### [32] [Leader360V: The Large-scale, Real-world 360 Video Dataset for Multi-task Learning in Diverse Environment](https://arxiv.org/abs/2506.14271)
*Weiming Zhang,Dingwen Xiao,Aobotao Dai,Yexin Liu,Tianbo Pan,Shiqi Wen,Lei Chen,Lin Wang*

Main category: cs.CV

TL;DR: 论文介绍了Leader360V，首个大规模标注的真实世界360视频数据集，用于实例分割和跟踪，并提出了一种自动化标注流程。


<details>
  <summary>Details</summary>
Motivation: 360视频的球形特性导致标注成本高且复杂，缺乏大规模标注数据集阻碍了基础模型的发展。

Method: 设计了自动化标注流程，结合预训练的2D分割器和大型语言模型，分三阶段（初始标注、自动优化标注、人工修订）完成标注。

Result: Leader360V显著提升了360视频分割和跟踪的模型性能，标注流程高效。

Conclusion: Leader360V为360场景理解提供了可扩展的数据集和标注方法。

Abstract: 360 video captures the complete surrounding scenes with the ultra-large field of view of 360X180. This makes 360 scene understanding tasks, eg, segmentation and tracking, crucial for appications, such as autonomous driving, robotics. With the recent emergence of foundation models, the community is, however, impeded by the lack of large-scale, labelled real-world datasets. This is caused by the inherent spherical properties, eg, severe distortion in polar regions, and content discontinuities, rendering the annotation costly yet complex. This paper introduces Leader360V, the first large-scale, labeled real-world 360 video datasets for instance segmentation and tracking. Our datasets enjoy high scene diversity, ranging from indoor and urban settings to natural and dynamic outdoor scenes. To automate annotation, we design an automatic labeling pipeline, which subtly coordinates pre-trained 2D segmentors and large language models to facilitate the labeling. The pipeline operates in three novel stages. Specifically, in the Initial Annotation Phase, we introduce a Semantic- and Distortion-aware Refinement module, which combines object mask proposals from multiple 2D segmentors with LLM-verified semantic labels. These are then converted into mask prompts to guide SAM2 in generating distortion-aware masks for subsequent frames. In the Auto-Refine Annotation Phase, missing or incomplete regions are corrected either by applying the SDR again or resolving the discontinuities near the horizontal borders. The Manual Revision Phase finally incorporates LLMs and human annotators to further refine and validate the annotations. Extensive user studies and evaluations demonstrate the effectiveness of our labeling pipeline. Meanwhile, experiments confirm that Leader360V significantly enhances model performance for 360 video segmentation and tracking, paving the way for more scalable 360 scene understanding.

</details>


### [33] [FRIDU: Functional Map Refinement with Guided Image Diffusion](https://arxiv.org/abs/2506.14322)
*Avigail Cohen Rimon,Mirela Ben-Chen,Or Litany*

Main category: cs.CV

TL;DR: 提出了一种基于图像扩散模型的功能映射优化方法，通过训练模型直接在功能映射空间中生成精确映射，并在推理时利用点映射引导扩散过程。


<details>
  <summary>Details</summary>
Motivation: 现有功能映射优化方法在准确性和效率上存在不足，需要一种更高效且灵活的方法。

Method: 将功能映射视为2D图像，训练图像扩散模型直接在功能映射空间生成精确映射，推理时利用点映射引导扩散过程。

Result: 方法在功能映射优化上具有竞争力，且引导扩散模型为功能映射处理提供了新途径。

Conclusion: 引导扩散模型是一种高效且有前景的功能映射优化方法。

Abstract: We propose a novel approach for refining a given correspondence map between two shapes. A correspondence map represented as a functional map, namely a change of basis matrix, can be additionally treated as a 2D image. With this perspective, we train an image diffusion model directly in the space of functional maps, enabling it to generate accurate maps conditioned on an inaccurate initial map. The training is done purely in the functional space, and thus is highly efficient. At inference time, we use the pointwise map corresponding to the current functional map as guidance during the diffusion process. The guidance can additionally encourage different functional map objectives, such as orthogonality and commutativity with the Laplace-Beltrami operator. We show that our approach is competitive with state-of-the-art methods of map refinement and that guided diffusion models provide a promising pathway to functional map processing.

</details>


### [34] [EVA02-AT: Egocentric Video-Language Understanding with Spatial-Temporal Rotary Positional Embeddings and Symmetric Optimization](https://arxiv.org/abs/2506.14356)
*Xiaoqi Wang,Yi Wang,Lap-Pui Chau*

Main category: cs.CV

TL;DR: EVA02-AT是一种基于EVA02的视频语言基础模型，通过单阶段预训练、空间-时间旋转位置嵌入和对称多相似性损失，解决了现有方法在效率、特征交互和多实例检索中的问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在预训练成本高、空间-时间编码无效以及多实例检索学习目标不精确方面的挑战。

Method: 1. 单阶段预训练；2. 空间-时间旋转位置嵌入与联合注意力；3. 对称多相似性损失（SMS）。

Result: 在Ego4D、EPIC-Kitchens-100和Charades-Ego等数据集上实现了最先进的性能，参数更少。

Conclusion: EVA02-AT在效率、特征建模和检索任务中表现优异，为自我中心视频理解提供了有效解决方案。

Abstract: Egocentric video-language understanding demands both high efficiency and accurate spatial-temporal modeling. Existing approaches face three key challenges: 1) Excessive pre-training cost arising from multi-stage pre-training pipelines, 2) Ineffective spatial-temporal encoding due to manually split 3D rotary positional embeddings that hinder feature interactions, and 3) Imprecise learning objectives in soft-label multi-instance retrieval, which neglect negative pair correlations. In this paper, we introduce EVA02-AT, a suite of EVA02-based video-language foundation models tailored to egocentric video understanding tasks. EVA02-AT first efficiently transfers an image-based CLIP model into a unified video encoder via a single-stage pretraining. Second, instead of applying rotary positional embeddings to isolated dimensions, we introduce spatial-temporal rotary positional embeddings along with joint attention, which can effectively encode both spatial and temporal information on the entire hidden dimension. This joint encoding of spatial-temporal features enables the model to learn cross-axis relationships, which are crucial for accurately modeling motion and interaction in videos. Third, focusing on multi-instance video-language retrieval tasks, we introduce the Symmetric Multi-Similarity (SMS) loss and a novel training framework that advances all soft labels for both positive and negative pairs, providing a more precise learning objective. Extensive experiments on Ego4D, EPIC-Kitchens-100, and Charades-Ego under zero-shot and fine-tuning settings demonstrate that EVA02-AT achieves state-of-the-art performance across diverse egocentric video-language tasks with fewer parameters. Models with our SMS loss also show significant performance gains on multi-instance retrieval benchmarks. Our code and models are publicly available at https://github.com/xqwang14/EVA02-AT .

</details>


### [35] [HydroChronos: Forecasting Decades of Surface Water Change](https://arxiv.org/abs/2506.14362)
*Daniele Rege Cambrin,Eleonora Poeta,Eliana Pastor,Isaac Corley,Tania Cerquitelli,Elena Baralis,Paolo Garza*

Main category: cs.CV

TL;DR: HydroChronos是一个用于地表水动态预测的大规模多模态时空数据集，填补了该领域缺乏全面数据和标准化基准的空白。AquaClimaTempo UNet模型在多个任务中显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 地表水动态预测对水资源管理和气候变化适应至关重要，但缺乏全面数据集和标准化基准。

Method: 提出HydroChronos数据集，包含多源数据，并设计AquaClimaTempo UNet模型作为基准。

Result: 模型在多个任务中表现优于基线，F1分数提升14%和11%，MAE降低0.1。

Conclusion: HydroChronos和AquaClimaTempo UNet为地表水动态预测提供了新工具，并通过可解释性分析揭示了关键影响因素。

Abstract: Forecasting surface water dynamics is crucial for water resource management and climate change adaptation. However, the field lacks comprehensive datasets and standardized benchmarks. In this paper, we introduce HydroChronos, a large-scale, multi-modal spatiotemporal dataset for surface water dynamics forecasting designed to address this gap. We couple the dataset with three forecasting tasks. The dataset includes over three decades of aligned Landsat 5 and Sentinel-2 imagery, climate data, and Digital Elevation Models for diverse lakes and rivers across Europe, North America, and South America. We also propose AquaClimaTempo UNet, a novel spatiotemporal architecture with a dedicated climate data branch, as a strong benchmark baseline. Our model significantly outperforms a Persistence baseline for forecasting future water dynamics by +14% and +11% F1 across change detection and direction of change classification tasks, and by +0.1 MAE on the magnitude of change regression. Finally, we conduct an Explainable AI analysis to identify the key climate variables and input channels that influence surface water change, providing insights to inform and guide future modeling efforts.

</details>


### [36] [DGG-XNet: A Hybrid Deep Learning Framework for Multi-Class Brain Disease Classification with Explainable AI](https://arxiv.org/abs/2506.14367)
*Sumshun Nahar Eity,Mahin Montasir Afif,Tanisha Fairooz,Md. Mortuza Ahmmed,Md Saef Ullah Miah*

Main category: cs.CV

TL;DR: DGG-XNet是一种结合VGG16和DenseNet121的深度学习模型，用于提高脑部疾病诊断的准确性和效率，测试准确率达91.33%。


<details>
  <summary>Details</summary>
Motivation: 传统MRI分析方法效率低且易出错，需要更准确和高效的脑部疾病诊断工具。

Method: 提出DGG-XNet，融合VGG16和DenseNet121的优势，利用Grad-CAM增强模型可解释性。

Result: 在BraTS 2021和Kaggle数据集上测试，准确率91.33%，各项指标均超过91%。

Conclusion: DGG-XNet是一种高效且可解释的计算机辅助诊断工具，适用于神经退行性和肿瘤性脑部疾病。

Abstract: Accurate diagnosis of brain disorders such as Alzheimer's disease and brain tumors remains a critical challenge in medical imaging. Conventional methods based on manual MRI analysis are often inefficient and error-prone. To address this, we propose DGG-XNet, a hybrid deep learning model integrating VGG16 and DenseNet121 to enhance feature extraction and classification. DenseNet121 promotes feature reuse and efficient gradient flow through dense connectivity, while VGG16 contributes strong hierarchical spatial representations. Their fusion enables robust multiclass classification of neurological conditions. Grad-CAM is applied to visualize salient regions, enhancing model transparency. Trained on a combined dataset from BraTS 2021 and Kaggle, DGG-XNet achieved a test accuracy of 91.33\%, with precision, recall, and F1-score all exceeding 91\%. These results highlight DGG-XNet's potential as an effective and interpretable tool for computer-aided diagnosis (CAD) of neurodegenerative and oncological brain disorders.

</details>


### [37] [Discrete JEPA: Learning Discrete Token Representations without Reconstruction](https://arxiv.org/abs/2506.14373)
*Junyeob Baek,Hosung Lee,Christopher Hoang,Mengye Ren,Sungjin Ahn*

Main category: cs.CV

TL;DR: 论文提出Discrete-JEPA方法，通过语义标记化和新目标改进图像标记化，显著提升符号推理任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前图像标记化方法在符号抽象和逻辑推理任务中存在局限，需改进以支持系统性推断。

Method: 扩展潜在预测编码框架，引入语义标记化和新互补目标，构建适用于符号推理的鲁棒标记化方法。

Result: Discrete-JEPA在视觉符号预测任务中显著优于基线，学习到的语义标记空间自发呈现系统性模式。

Conclusion: 该方法为人工智能系统的符号世界建模和规划能力提供了重要进展。

Abstract: The cornerstone of cognitive intelligence lies in extracting hidden patterns from observations and leveraging these principles to systematically predict future outcomes. However, current image tokenization methods demonstrate significant limitations in tasks requiring symbolic abstraction and logical reasoning capabilities essential for systematic inference. To address this challenge, we propose Discrete-JEPA, extending the latent predictive coding framework with semantic tokenization and novel complementary objectives to create robust tokenization for symbolic reasoning tasks. Discrete-JEPA dramatically outperforms baselines on visual symbolic prediction tasks, while striking visual evidence reveals the spontaneous emergence of deliberate systematic patterns within the learned semantic token space. Though an initial model, our approach promises a significant impact for advancing Symbolic world modeling and planning capabilities in artificial intelligence systems.

</details>


### [38] [DepthSeg: Depth prompting in remote sensing semantic segmentation](https://arxiv.org/abs/2506.14382)
*Ning Zhou,Shanxiong Chen,Mingting Zhou,Haigang Sui,Lieyun Hu,Han Li,Li Hua,Qiming Zhou*

Main category: cs.CV

TL;DR: DepthSeg框架通过深度提示改进2D遥感图像语义分割，解决光谱混淆和阴影遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略目标高程差异，导致复杂场景下的土地覆盖误分类。

Method: 结合轻量级适配器、深度提示器和语义分类解码器，集成深度信息。

Result: 在LiuZhou数据集上验证了DepthSeg在土地覆盖制图中的优势。

Conclusion: 深度提示对遥感语义分割具有重要意义。

Abstract: Remote sensing semantic segmentation is crucial for extracting detailed land surface information, enabling applications such as environmental monitoring, land use planning, and resource assessment. In recent years, advancements in artificial intelligence have spurred the development of automatic remote sensing semantic segmentation methods. However, the existing semantic segmentation methods focus on distinguishing spectral characteristics of different objects while ignoring the differences in the elevation of the different targets. This results in land cover misclassification in complex scenarios involving shadow occlusion and spectral confusion. In this paper, we introduce a depth prompting two-dimensional (2D) remote sensing semantic segmentation framework (DepthSeg). It automatically models depth/height information from 2D remote sensing images and integrates it into the semantic segmentation framework to mitigate the effects of spectral confusion and shadow occlusion. During the feature extraction phase of DepthSeg, we introduce a lightweight adapter to enable cost-effective fine-tuning of the large-parameter vision transformer encoder pre-trained by natural images. In the depth prompting phase, we propose a depth prompter to model depth/height features explicitly. In the semantic prediction phase, we introduce a semantic classification decoder that couples the depth prompts with high-dimensional land-cover features, enabling accurate extraction of land-cover types. Experiments on the LiuZhou dataset validate the advantages of the DepthSeg framework in land cover mapping tasks. Detailed ablation studies further highlight the significance of the depth prompts in remote sensing semantic segmentation.

</details>


### [39] [GrFormer: A Novel Transformer on Grassmann Manifold for Infrared and Visible Image Fusion](https://arxiv.org/abs/2506.14384)
*Huan Kang,Hui Li,Xiao-Jun Wu,Tianyang Xu,Rui Wang,Chunyang Cheng,Josef Kittler*

Main category: cs.CV

TL;DR: 论文提出了一种基于Grassmann流形的新型注意力机制（GrFormer），用于红外和可见光图像融合，通过多尺度语义融合提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统欧几里得方法无法捕捉非欧几里得空间中的内在拓扑结构，导致语义相似性不足，影响融合效果。

Method: 利用Grassmann流形构建低秩子空间映射，通过投影约束将注意力特征压缩到不同秩级别的子空间中，实现多尺度语义融合，并结合协方差掩模的跨模态融合策略（CMS）。

Result: 实验表明，该方法在多个图像融合基准上定性和定量均优于现有技术。

Conclusion: GrFormer通过Grassmann流形和CMS策略，有效提升了红外和可见光图像融合的性能。

Abstract: In the field of image fusion, promising progress has been made by modeling data from different modalities as linear subspaces.
  However, in practice, the source images are often located in a non-Euclidean space, where the Euclidean methods usually cannot
  encapsulate the intrinsic topological structure. Typically, the inner product performed in the Euclidean space calculates the algebraic
  similarity rather than the semantic similarity, which results in undesired attention output and a decrease in fusion performance.
  While the balance of low-level details and high-level semantics should be considered in infrared and visible image fusion task. To
  address this issue, in this paper, we propose a novel attention mechanism based on Grassmann manifold for infrared and visible
  image fusion (GrFormer). Specifically, our method constructs a low-rank subspace mapping through projection constraints on the
  Grassmann manifold, compressing attention features into subspaces of varying rank levels. This forces the features to decouple into
  high-frequency details (local low-rank) and low-frequency semantics (global low-rank), thereby achieving multi-scale semantic
  fusion. Additionally, to effectively integrate the significant information, we develop a cross-modal fusion strategy (CMS) based on
  a covariance mask to maximise the complementary properties between different modalities and to suppress the features with high
  correlation, which are deemed redundant. The experimental results demonstrate that our network outperforms SOTA methods both
  qualitatively and quantitatively on multiple image fusion benchmarks. The codes are available at https://github.com/Shaoyun2023.

</details>


### [40] [Decoupled Classifier-Free Guidance for Counterfactual Diffusion Models](https://arxiv.org/abs/2506.14399)
*Tian Xia,Fabio De Sousa Ribeiro,Rajat R Rasal,Avinash Kori,Raghav Mehta,Ben Glocker*

Main category: cs.CV

TL;DR: 论文提出了一种解耦的无分类器引导（DCFG）方法，通过分组条件控制改进反事实图像生成，解决了传统方法中属性放大的问题。


<details>
  <summary>Details</summary>
Motivation: 传统无分类器引导（CFG）方法在反事实图像生成中存在全局权重导致的身份保持差和虚假属性变化问题。

Method: 提出DCFG框架，采用属性分离嵌入策略，将语义输入解耦，实现对用户定义属性组的选择性引导。

Result: 在CelebA-HQ、MIMIC-CXR和EMBED数据集上的实验表明，DCFG提高了干预保真度，减少了意外变化，增强了可逆性。

Conclusion: DCFG能够实现更忠实和可解释的反事实图像生成。

Abstract: Counterfactual image generation aims to simulate realistic visual outcomes under specific causal interventions. Diffusion models have recently emerged as a powerful tool for this task, combining DDIM inversion with conditional generation via classifier-free guidance (CFG). However, standard CFG applies a single global weight across all conditioning variables, which can lead to poor identity preservation and spurious attribute changes - a phenomenon known as attribute amplification. To address this, we propose Decoupled Classifier-Free Guidance (DCFG), a flexible and model-agnostic framework that introduces group-wise conditioning control. DCFG builds on an attribute-split embedding strategy that disentangles semantic inputs, enabling selective guidance on user-defined attribute groups. For counterfactual generation, we partition attributes into intervened and invariant sets based on a causal graph and apply distinct guidance to each. Experiments on CelebA-HQ, MIMIC-CXR, and EMBED show that DCFG improves intervention fidelity, mitigates unintended changes, and enhances reversibility, enabling more faithful and interpretable counterfactual image generation.

</details>


### [41] [Causally Steered Diffusion for Automated Video Counterfactual Generation](https://arxiv.org/abs/2506.14404)
*Nikos Spyrou,Athanasios Vlontzos,Paraskevas Pegios,Thomas Melistas,Nefeli Gkouti,Yannis Panagakis,Giorgos Papanastasiou,Sotirios A. Tsaftaris*

Main category: cs.CV

TL;DR: 提出了一种基于因果关系的视频编辑框架，利用视觉语言模型（VLM）生成符合因果关系的反事实视频，无需修改底层编辑系统。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在视频编辑中难以保持因果关系，可能导致不现实或误导性结果。

Method: 通过优化基于因果图的文本提示，指导生成过程，无需访问或微调底层编辑系统。

Result: 实验表明，该方法能有效生成符合因果关系的反事实视频，并通过了视频质量和因果有效性等标准评估。

Conclusion: 该方法兼容任何黑盒视频编辑系统，具有在医疗和数字媒体等领域生成现实“假设”场景的潜力。

Abstract: Adapting text-to-image (T2I) latent diffusion models for video editing has shown strong visual fidelity and controllability, but challenges remain in maintaining causal relationships in video content. Edits affecting causally dependent attributes risk generating unrealistic or misleading outcomes if these relationships are ignored. In this work, we propose a causally faithful framework for counterfactual video generation, guided by a vision-language model (VLM). Our method is agnostic to the underlying video editing system and does not require access to its internal mechanisms or finetuning. Instead, we guide the generation by optimizing text prompts based on an assumed causal graph, addressing the challenge of latent space control in LDMs. We evaluate our approach using standard video quality metrics and counterfactual-specific criteria, such as causal effectiveness and minimality. Our results demonstrate that causally faithful video counterfactuals can be effectively generated within the learned distribution of LDMs through prompt-based causal steering. With its compatibility with any black-box video editing system, our method holds significant potential for generating realistic "what-if" video scenarios in diverse areas such as healthcare and digital media.

</details>


### [42] [Compositional Attribute Imbalance in Vision Datasets](https://arxiv.org/abs/2506.14418)
*Jiayi Chen,Yanbiao Ma,Andi Zhang,Weidong Tang,Wei Dai,Bowei Liu*

Main category: cs.CV

TL;DR: 论文提出了一种基于CLIP的框架，通过构建视觉属性字典解决图像分类中的属性不平衡问题，并通过调整采样概率和数据增强提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 视觉属性不平衡是图像分类中常见但未被充分研究的问题，显著影响模型性能和泛化能力。

Method: 定义图像的一级和二级属性，构建CLIP框架的视觉属性字典，分析单属性和组合属性不平衡，调整采样概率并集成数据增强技术。

Result: 实验表明，该方法有效缓解了属性不平衡，提升了深度神经网络的鲁棒性和公平性。

Conclusion: 研究强调了建模视觉属性分布的重要性，并为长尾图像分类任务提供了可扩展的解决方案。

Abstract: Visual attribute imbalance is a common yet underexplored issue in image classification, significantly impacting model performance and generalization. In this work, we first define the first-level and second-level attributes of images and then introduce a CLIP-based framework to construct a visual attribute dictionary, enabling automatic evaluation of image attributes. By systematically analyzing both single-attribute imbalance and compositional attribute imbalance, we reveal how the rarity of attributes affects model performance. To tackle these challenges, we propose adjusting the sampling probability of samples based on the rarity of their compositional attributes. This strategy is further integrated with various data augmentation techniques (such as CutMix, Fmix, and SaliencyMix) to enhance the model's ability to represent rare attributes. Extensive experiments on benchmark datasets demonstrate that our method effectively mitigates attribute imbalance, thereby improving the robustness and fairness of deep neural networks. Our research highlights the importance of modeling visual attribute distributions and provides a scalable solution for long-tail image classification tasks.

</details>


### [43] [Toward Rich Video Human-Motion2D Generation](https://arxiv.org/abs/2506.14428)
*Ruihao Xi,Xuekuan Wang,Yongcheng Li,Shuhua Li,Zichen Wang,Yiwei Wang,Feng Wei,Cairong Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散模型的RVHM2D方法，用于生成逼真且可控的2D人体运动，特别是多角色交互动作。通过引入大规模数据集Motion2D-Video-150K和两阶段训练策略，模型在生成单角色和双角色交互动作上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成逼真且可控的多角色交互动作时面临数据稀缺和建模复杂的挑战。

Method: 提出RVHM2D模型，采用扩散框架并结合双文本编码器（CLIP-L/B或T5-XXL）的增强文本条件机制。采用两阶段训练策略：先标准扩散训练，再通过强化学习微调。

Result: RVHM2D在Motion2D-Video-150K数据集上生成单角色和双角色交互动作的性能领先。

Conclusion: RVHM2D通过新数据集和两阶段训练策略，显著提升了多角色交互动作生成的逼真度和可控性。

Abstract: Generating realistic and controllable human motions, particularly those involving rich multi-character interactions, remains a significant challenge due to data scarcity and the complexities of modeling inter-personal dynamics. To address these limitations, we first introduce a new large-scale rich video human motion 2D dataset (Motion2D-Video-150K) comprising 150,000 video sequences. Motion2D-Video-150K features a balanced distribution of diverse single-character and, crucially, double-character interactive actions, each paired with detailed textual descriptions. Building upon this dataset, we propose a novel diffusion-based rich video human motion2D generation (RVHM2D) model. RVHM2D incorporates an enhanced textual conditioning mechanism utilizing either dual text encoders (CLIP-L/B) or T5-XXL with both global and local features. We devise a two-stage training strategy: the model is first trained with a standard diffusion objective, and then fine-tuned using reinforcement learning with an FID-based reward to further enhance motion realism and text alignment. Extensive experiments demonstrate that RVHM2D achieves leading performance on the Motion2D-Video-150K benchmark in generating both single and interactive double-character scenarios.

</details>


### [44] [MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal Models](https://arxiv.org/abs/2506.14435)
*Hongyu Wang,Jiayu Xu,Ruiping Wang,Yan Feng,Yitao Zhai,Peng Pei,Xunliang Cai,Xilin Chen*

Main category: cs.CV

TL;DR: MoTE是一种内存高效的方法，通过训练更多低精度专家（参数为{-1, 0, 1}）来减少内存占用，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态Mixture-of-Experts（MoEs）在稀疏升级中使用全精度专家，导致内存占用高，难以在边缘设备上部署。

Method: 提出MoTE方法，使用预训练的FFN作为共享专家，训练三元路由专家（参数为{-1, 0, 1}）。

Result: MoTE在模型规模扩展时表现良好，性能与全精度基线MoE-LLaVA相当，但内存占用更低。结合后训练量化，MoTE在相同内存占用下性能提升4.3%。

Conclusion: MoTE是一种高效且适用于内存受限设备的方法，具有潜在的实际应用价值。

Abstract: Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size to boost performance while maintaining fixed active parameters. However, previous works primarily utilized full-precision experts during sparse up-cycling. Despite they show superior performance on end tasks, the large amount of experts introduces higher memory footprint, which poses significant challenges for the deployment on edge devices. In this work, we propose MoTE, a scalable and memory-efficient approach to train Mixture-of-Ternary-Experts models from dense checkpoint. Instead of training fewer high-precision experts, we propose to train more low-precision experts during up-cycling. Specifically, we use the pre-trained FFN as a shared expert and train ternary routed experts with parameters in {-1, 0, 1}. Extensive experiments show that our approach has promising scaling trend along model size. MoTE achieves comparable performance to full-precision baseline MoE-LLaVA while offering lower memory footprint. Furthermore, our approach is compatible with post-training quantization methods and the advantage further amplifies when memory-constraint goes lower. Given the same amount of expert memory footprint of 3.4GB and combined with post-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3% average accuracy on end tasks, demonstrating its effectiveness and potential for memory-constrained devices.

</details>


### [45] [Model compression using knowledge distillation with integrated gradients](https://arxiv.org/abs/2506.14440)
*David E. Hernandez,Jose Chang,Torbjörn E. M. Nordling*

Main category: cs.CV

TL;DR: 提出了一种基于积分梯度（IG）的知识蒸馏方法，通过数据增强提升模型压缩效果，显著提高了测试准确率并减少了推理时间。


<details>
  <summary>Details</summary>
Motivation: 在资源受限设备上部署深度学习模型需要高效的压缩技术，传统方法在压缩率和准确性之间存在权衡。

Method: 在训练过程中将IG图叠加到输入图像上，增强学生对教师模型决策过程的理解。

Result: 在CIFAR-10上测试准确率达到92.6%，压缩比为4.1倍，推理时间从140毫秒降至13毫秒。

Conclusion: IG增强的知识蒸馏方法在多种架构和压缩比下均优于传统方法，适用于边缘设备部署。

Abstract: Model compression is critical for deploying deep learning models on resource-constrained devices. We introduce a novel method enhancing knowledge distillation with integrated gradients (IG) as a data augmentation strategy. Our approach overlays IG maps onto input images during training, providing student models with deeper insights into teacher models' decision-making processes. Extensive evaluation on CIFAR-10 demonstrates that our IG-augmented knowledge distillation achieves 92.6% testing accuracy with a 4.1x compression factor-a significant 1.1 percentage point improvement ($p<0.001$) over non-distilled models (91.5%). This compression reduces inference time from 140 ms to 13 ms. Our method precomputes IG maps before training, transforming substantial runtime costs into a one-time preprocessing step. Our comprehensive experiments include: (1) comparisons with attention transfer, revealing complementary benefits when combined with our approach; (2) Monte Carlo simulations confirming statistical robustness; (3) systematic evaluation of compression factor versus accuracy trade-offs across a wide range (2.2x-1122x); and (4) validation on an ImageNet subset aligned with CIFAR-10 classes, demonstrating generalisability beyond the initial dataset. These extensive ablation studies confirm that IG-based knowledge distillation consistently outperforms conventional approaches across varied architectures and compression ratios. Our results establish this framework as a viable compression technique for real-world deployment on edge devices while maintaining competitive accuracy.

</details>


### [46] [Adapting Lightweight Vision Language Models for Radiological Visual Question Answering](https://arxiv.org/abs/2506.14451)
*Aditya Shourya,Michel Dumontier,Chang Sun*

Main category: cs.CV

TL;DR: 轻量级3B参数视觉语言模型在放射学VQA中表现优异，通过合成数据生成和多阶段微调实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 解决放射学VQA模型开发中的数据稀缺、建模复杂性和评估不足问题。

Method: 使用合成问题-答案对生成和多阶段微调，训练轻量级3B参数模型。

Result: 小规模模型在开放和封闭问题中表现稳健，性能接近先进模型。

Conclusion: 轻量级模型结合适当数据和方法，可在放射学VQA中实现高效且可靠的性能。

Abstract: Recent advancements in vision-language systems have improved the accuracy of Radiological Visual Question Answering (VQA) Models. However, some challenges remain across each stage of model development: limited expert-labeled images hinders data procurement at scale; the intricate and nuanced patterns of radiological images make modeling inherently difficult; and the lack of evaluation evaluation efforts makes it difficult to identify cases where the model might be ill-conditioned. In this study, we fine-tune a lightweight 3B parameter vision-language model for Radiological VQA, demonstrating that small models, when appropriately tuned with curated data, can achieve robust performance across both open- and closed-ended questions. We propose a cost-effective training pipeline from synthetic question-answer pair generation to multi-stage fine-tuning on specialised radiological domain-targeted datasets (e.g., ROCO v2.0, MedPix v2.0). Our results show that despite operating at a fraction of the scale of state-of-the-art models such as LLaVA-Med, our model achieves promising performance given its small parameter size and the limited scale of training data. We introduce a lightweight saliency-based diagnostic tool that enables domain experts to inspect VQA model performance and identify ill-conditioned failure modes through saliency analysis.

</details>


### [47] [Dense360: Dense Understanding from Omnidirectional Panoramas](https://arxiv.org/abs/2506.14471)
*Yikang Zhou,Tao Zhang,Dizhe Zhang,Shunping Ji,Xiangtai Li,Lu Qi*

Main category: cs.CV

TL;DR: 该论文提出了一种用于全景图像的多模态大语言模型（MLLM）方法，解决了全景图像处理中的空间连续性和信息密度变化问题，并引入了一个新的数据集和基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM通过有限视场（FOV）输入理解世界，但全景图像能提供更完整、紧凑和连续的场景表示，因此需要开发适用于全景图像的方法。

Method: 提出了ERP-RoPE位置编码方案，专门用于处理全景图像的等距圆柱投影（ERP）问题，并引入了包含160K全景图像的数据集。

Result: 开发了Dense360-Bench基准测试，用于评估MLLM在全景图像标注和定位任务中的表现。

Conclusion: 该研究为全景环境下的密集视觉语言理解提供了全面的框架和工具。

Abstract: Multimodal Large Language Models (MLLMs) require comprehensive visual inputs to achieve dense understanding of the physical world. While existing MLLMs demonstrate impressive world understanding capabilities through limited field-of-view (FOV) visual inputs (e.g., 70 degree), we take the first step toward dense understanding from omnidirectional panoramas. We first introduce an omnidirectional panoramas dataset featuring a comprehensive suite of reliability-scored annotations. Specifically, our dataset contains 160K panoramas with 5M dense entity-level captions, 1M unique referring expressions, and 100K entity-grounded panoramic scene descriptions. Compared to multi-view alternatives, panoramas can provide more complete, compact, and continuous scene representations through equirectangular projections (ERP). However, the use of ERP introduces two key challenges for MLLMs: i) spatial continuity along the circle of latitude, and ii) latitude-dependent variation in information density. We address these challenges through ERP-RoPE, a position encoding scheme specifically designed for panoramic ERP. In addition, we introduce Dense360-Bench, the first benchmark for evaluating MLLMs on omnidirectional captioning and grounding, establishing a comprehensive framework for advancing dense visual-language understanding in panoramic settings.

</details>


### [48] [Foundation Model Insights and a Multi-Model Approach for Superior Fine-Grained One-shot Subset Selection](https://arxiv.org/abs/2506.14473)
*Zhijing Wan,Zhixiang Wang,Zheng Wang,Xin Xu,Shin'ichi Satoh*

Main category: cs.CV

TL;DR: 论文提出了一种基于基础模型（FMs）的一次性子集选择方法RAM-APL，用于降低深度学习训练成本，并在细粒度数据集上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统信息提取器（IE）依赖于目标数据集，限制了其通用性。基础模型（FMs）可能解决这一问题，但其在不同数据集上的表现尚不明确。

Method: 提出RAM-APL方法，利用多个FMs的互补优势，针对细粒度数据集优化子集选择。

Result: 实验表明，FMs在细粒度数据集上优于传统IE方法，但在粗粒度数据集上优势不明显。RAM-APL在多个细粒度数据集上达到最优性能。

Conclusion: RAM-APL通过结合多个FMs的优势，显著提升了细粒度数据集上的子集选择效果，为降低训练成本提供了新思路。

Abstract: One-shot subset selection serves as an effective tool to reduce deep learning training costs by identifying an informative data subset based on the information extracted by an information extractor (IE). Traditional IEs, typically pre-trained on the target dataset, are inherently dataset-dependent. Foundation models (FMs) offer a promising alternative, potentially mitigating this limitation. This work investigates two key questions: (1) Can FM-based subset selection outperform traditional IE-based methods across diverse datasets? (2) Do all FMs perform equally well as IEs for subset selection? Extensive experiments uncovered surprising insights: FMs consistently outperform traditional IEs on fine-grained datasets, whereas their advantage diminishes on coarse-grained datasets with noisy labels. Motivated by these finding, we propose RAM-APL (RAnking Mean-Accuracy of Pseudo-class Labels), a method tailored for fine-grained image datasets. RAM-APL leverages multiple FMs to enhance subset selection by exploiting their complementary strengths. Our approach achieves state-of-the-art performance on fine-grained datasets, including Oxford-IIIT Pet, Food-101, and Caltech-UCSD Birds-200-2011.

</details>


### [49] [I Speak and You Find: Robust 3D Visual Grounding with Noisy and Ambiguous Speech Inputs](https://arxiv.org/abs/2506.14495)
*Yu Qi,Lipeng Gu,Honghua Chen,Liangliang Nan,Mingqiang Wei*

Main category: cs.CV

TL;DR: SpeechRefer是一个新的3D视觉定位框架，通过语音信号和对比学习处理噪声和模糊的语音转录，提升现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位方法依赖精确的文本提示，而真实语音输入常因口音、背景噪声等问题导致转录错误，限制了方法的应用。

Method: SpeechRefer引入语音互补模块和对比互补模块，分别利用语音信号的声学相似性和对比学习来减少对错误转录的依赖。

Result: 在SpeechRefer和SpeechNr3D数据集上的实验表明，SpeechRefer显著提升了现有3DVG方法的性能。

Conclusion: SpeechRefer通过处理噪声语音输入，缩小了语音与可靠3DVG之间的差距，为多模态系统提供了更直观和实用的解决方案。

Abstract: Existing 3D visual grounding methods rely on precise text prompts to locate objects within 3D scenes. Speech, as a natural and intuitive modality, offers a promising alternative. Real-world speech inputs, however, often suffer from transcription errors due to accents, background noise, and varying speech rates, limiting the applicability of existing 3DVG methods. To address these challenges, we propose \textbf{SpeechRefer}, a novel 3DVG framework designed to enhance performance in the presence of noisy and ambiguous speech-to-text transcriptions. SpeechRefer integrates seamlessly with xisting 3DVG models and introduces two key innovations. First, the Speech Complementary Module captures acoustic similarities between phonetically related words and highlights subtle distinctions, generating complementary proposal scores from the speech signal. This reduces dependence on potentially erroneous transcriptions. Second, the Contrastive Complementary Module employs contrastive learning to align erroneous text features with corresponding speech features, ensuring robust performance even when transcription errors dominate. Extensive experiments on the SpeechRefer and peechNr3D datasets demonstrate that SpeechRefer improves the performance of existing 3DVG methods by a large margin, which highlights SpeechRefer's potential to bridge the gap between noisy speech inputs and reliable 3DVG, enabling more intuitive and practical multimodal systems.

</details>


### [50] [MOL: Joint Estimation of Micro-Expression, Optical Flow, and Landmark via Transformer-Graph-Style Convolution](https://arxiv.org/abs/2506.14511)
*Zhiwen Shao,Yifan Cheng,Feiran Li,Yong Zhou,Xuequan Lu,Yuan Xie,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer、图卷积和普通卷积的端到端微动作感知深度学习框架，用于面部微表情识别（MER），无需关键帧先验知识。


<details>
  <summary>Details</summary>
Motivation: 面部微表情识别因动作短暂且细微而具有挑战性，现有方法依赖手工特征或受限于小规模数据集。

Method: 提出F5C块（全连接卷积和通道对应卷积），结合Transformer风格的全连接卷积和图风格的通道对应卷积，联合训练MER、光流估计和面部标志点检测。

Result: 在多个基准测试中优于现有方法，同时适用于光流估计和面部标志点检测，并能捕捉与微表情相关的局部肌肉动作。

Conclusion: 该框架通过多任务联合训练和局部-全局特征提取，显著提升了微表情识别性能。

Abstract: Facial micro-expression recognition (MER) is a challenging problem, due to transient and subtle micro-expression (ME) actions. Most existing methods depend on hand-crafted features, key frames like onset, apex, and offset frames, or deep networks limited by small-scale and low-diversity datasets. In this paper, we propose an end-to-end micro-action-aware deep learning framework with advantages from transformer, graph convolution, and vanilla convolution. In particular, we propose a novel F5C block composed of fully-connected convolution and channel correspondence convolution to directly extract local-global features from a sequence of raw frames, without the prior knowledge of key frames. The transformer-style fully-connected convolution is proposed to extract local features while maintaining global receptive fields, and the graph-style channel correspondence convolution is introduced to model the correlations among feature patterns. Moreover, MER, optical flow estimation, and facial landmark detection are jointly trained by sharing the local-global features. The two latter tasks contribute to capturing facial subtle action information for MER, which can alleviate the impact of insufficient training data. Extensive experiments demonstrate that our framework (i) outperforms the state-of-the-art MER methods on CASME II, SAMM, and SMIC benchmarks, (ii) works well for optical flow estimation and facial landmark detection, and (iii) can capture facial subtle muscle actions in local regions associated with MEs. The code is available at https://github.com/CYF-cuber/MOL.

</details>


### [51] [SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex Reasoning Tasks](https://arxiv.org/abs/2506.14512)
*Zijian Song,Xiaoxin Lin,Qiuming Huang,Guangrun Wang,Liang Lin*

Main category: cs.CV

TL;DR: SIRI-Bench是一个用于评估视觉语言模型（VLMs）空间智能的视频基准测试，包含近1K个视频-问题-答案三元组，通过3D场景设计挑战空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在复杂推理方面取得进展，但视觉语言模型（VLMs）在空间上下文中的系统评估仍不足。

Method: 开发了SIRI-Bench基准测试，利用自动场景创建引擎生成3D场景，设计需要空间理解和高阶推理的问题。

Result: 实验表明，当前先进的VLMs在SIRI-Bench上表现不佳，突显空间推理的挑战。

Conclusion: 该研究旨在推动VLMs在视觉问题解决中的空间推理能力发展。

Abstract: Large Language Models (LLMs) are experiencing rapid advancements in complex reasoning, exhibiting remarkable generalization in mathematics and programming. In contrast, while spatial intelligence is fundamental for Vision-Language Models (VLMs) in real-world interaction, the systematic evaluation of their complex reasoning ability within spatial contexts remains underexplored. To bridge this gap, we introduce SIRI-Bench, a benchmark designed to evaluate VLMs' spatial intelligence through video-based reasoning tasks. SIRI-Bench comprises nearly 1K video-question-answer triplets, where each problem is embedded in a realistic 3D scene and captured by video. By carefully designing questions and corresponding 3D scenes, our benchmark ensures that solving the questions requires both spatial comprehension for extracting information and high-level reasoning for deriving solutions, making it a challenging benchmark for evaluating VLMs. To facilitate large-scale data synthesis, we develop an Automatic Scene Creation Engine. This engine, leveraging multiple specialized LLM agents, can generate realistic 3D scenes from abstract math problems, ensuring faithfulness to the original descriptions. Experimental results reveal that state-of-the-art VLMs struggle significantly on SIRI-Bench, underscoring the challenge of spatial reasoning. We hope that our study will bring researchers' attention to spatially grounded reasoning and advance VLMs in visual problem-solving.

</details>


### [52] [VisLanding: Monocular 3D Perception for UAV Safe Landing via Depth-Normal Synergy](https://arxiv.org/abs/2506.14525)
*Zhuoyue Tan,Boyong He,Yuxiang Ji,Liaoni Wu*

Main category: cs.CV

TL;DR: VisLanding是一个基于单目3D感知的框架，用于无人机安全着陆，通过深度-法线协同预测和语义分割任务提升着陆区识别精度。


<details>
  <summary>Details</summary>
Motivation: 解决无人机在复杂未知环境中自主着陆的核心挑战。

Method: 利用Metric3D V2模型的深度-法线协同预测能力，构建端到端的安全着陆区估计框架，并通过语义分割任务实现。

Result: 实验表明，VisLanding显著提高了着陆区识别精度，并保留了Metric3D V2的零样本泛化优势。

Conclusion: 该方法在跨域测试中表现出优越的泛化性和鲁棒性，并为实际应用提供关键决策支持。

Abstract: This paper presents VisLanding, a monocular 3D perception-based framework for safe UAV (Unmanned Aerial Vehicle) landing. Addressing the core challenge of autonomous UAV landing in complex and unknown environments, this study innovatively leverages the depth-normal synergy prediction capabilities of the Metric3D V2 model to construct an end-to-end safe landing zones (SLZ) estimation framework. By introducing a safe zone segmentation branch, we transform the landing zone estimation task into a binary semantic segmentation problem. The model is fine-tuned and annotated using the WildUAV dataset from a UAV perspective, while a cross-domain evaluation dataset is constructed to validate the model's robustness. Experimental results demonstrate that VisLanding significantly enhances the accuracy of safe zone identification through a depth-normal joint optimization mechanism, while retaining the zero-shot generalization advantages of Metric3D V2. The proposed method exhibits superior generalization and robustness in cross-domain testing compared to other approaches. Furthermore, it enables the estimation of landing zone area by integrating predicted depth and normal information, providing critical decision-making support for practical applications.

</details>


### [53] [Exploring Diffusion with Test-Time Training on Efficient Image Restoration](https://arxiv.org/abs/2506.14541)
*Rongchang Lu,Tianduo Luo,Yunzhi Zhang,Conghan Yue,Pei Yang,Guibao Liu,Changyang Gu*

Main category: cs.CV

TL;DR: DiffRWKVIR提出了一种结合测试时训练和高效扩散的新框架，解决了图像恢复中的特征融合、计算瓶颈和扩散效率问题。


<details>
  <summary>Details</summary>
Motivation: 图像恢复中存在特征融合不高效、计算瓶颈和扩散过程效率低的问题。

Method: 提出了三个创新点：Omni-Scale 2D State Evolution、Chunk-Optimized Flash Processing和Prior-Guided Efficient Diffusion。

Result: 在多个基准测试中，DiffRWKVIR在PSNR、SSIM、LPIPS和效率指标上优于SwinIR、HAT和MambaIR/v2。

Conclusion: DiffRWKVIR为高效、自适应的图像恢复提供了新范式，并优化了硬件利用率。

Abstract: Image restoration faces challenges including ineffective feature fusion, computational bottlenecks and inefficient diffusion processes. To address these, we propose DiffRWKVIR, a novel framework unifying Test-Time Training (TTT) with efficient diffusion. Our approach introduces three key innovations: (1) Omni-Scale 2D State Evolution extends RWKV's location-dependent parameterization to hierarchical multi-directional 2D scanning, enabling global contextual awareness with linear complexity O(L); (2) Chunk-Optimized Flash Processing accelerates intra-chunk parallelism by 3.2x via contiguous chunk processing (O(LCd) complexity), reducing sequential dependencies and computational overhead; (3) Prior-Guided Efficient Diffusion extracts a compact Image Prior Representation (IPR) in only 5-20 steps, proving 45% faster training/inference than DiffIR while solving computational inefficiency in denoising. Evaluated across super-resolution and inpainting benchmarks (Set5, Set14, BSD100, Urban100, Places365), DiffRWKVIR outperforms SwinIR, HAT, and MambaIR/v2 in PSNR, SSIM, LPIPS, and efficiency metrics. Our method establishes a new paradigm for adaptive, high-efficiency image restoration with optimized hardware utilization.

</details>


### [54] [DreamLight: Towards Harmonious and Consistent Image Relighting](https://arxiv.org/abs/2506.14549)
*Yong Liu,Wenpeng Xiao,Qianqian Wang,Junlin Chen,Shiyin Wang,Yitong Wang,Xinglong Wu,Yansong Tang*

Main category: cs.CV

TL;DR: DreamLight是一个通用的图像重光照模型，支持基于图像或文本的背景重光照，通过统一输入格式和预训练扩散模型的语义先验生成自然效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注基于图像的重光照，文本场景探索较少，且现有方法难以生成前景与背景间自然的光照交互效果。

Method: 采用统一输入格式，利用预训练扩散模型的语义先验，提出位置引导的光适配器（PGLA）和频谱前景修复器（SFF）模块。

Result: 实验和用户研究表明，DreamLight在重光照任务中表现优异。

Conclusion: DreamLight通过创新模块设计解决了现有方法的局限性，实现了高质量的重光照效果。

Abstract: We introduce a model named DreamLight for universal image relighting in this work, which can seamlessly composite subjects into a new background while maintaining aesthetic uniformity in terms of lighting and color tone. The background can be specified by natural images (image-based relighting) or generated from unlimited text prompts (text-based relighting). Existing studies primarily focus on image-based relighting, while with scant exploration into text-based scenarios. Some works employ intricate disentanglement pipeline designs relying on environment maps to provide relevant information, which grapples with the expensive data cost required for intrinsic decomposition and light source. Other methods take this task as an image translation problem and perform pixel-level transformation with autoencoder architecture. While these methods have achieved decent harmonization effects, they struggle to generate realistic and natural light interaction effects between the foreground and background. To alleviate these challenges, we reorganize the input data into a unified format and leverage the semantic prior provided by the pretrained diffusion model to facilitate the generation of natural results. Moreover, we propose a Position-Guided Light Adapter (PGLA) that condenses light information from different directions in the background into designed light query embeddings, and modulates the foreground with direction-biased masked attention. In addition, we present a post-processing module named Spectral Foreground Fixer (SFF) to adaptively reorganize different frequency components of subject and relighted background, which helps enhance the consistency of foreground appearance. Extensive comparisons and user study demonstrate that our DreamLight achieves remarkable relighting performance.

</details>


### [55] [Risk Estimation of Knee Osteoarthritis Progression via Predictive Multi-task Modelling from Efficient Diffusion Model using X-ray Images](https://arxiv.org/abs/2506.14560)
*David Butler,Adrian Hilton,Gustavo Carneiro*

Main category: cs.CV

TL;DR: 提出了一种可解释的机器学习方法，通过多任务预测建模估计膝关节骨关节炎（OA）进展风险，同时预测解剖学标志点，并利用扩散模型生成高质量未来图像。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏可解释性且复杂，临床采用受限，且无法定位解剖学标志点。

Method: 采用多任务预测建模，结合扩散模型在类别条件潜在空间中生成未来图像，用于分类未来OA严重程度和预测标志点。

Result: 在Osteoarthritis Initiative数据集上，AUC提升2%至0.71，推理时间减少约9%。

Conclusion: 该方法在提升预测性能的同时，提供了更高的可解释性和效率。

Abstract: Medical imaging plays a crucial role in assessing knee osteoarthritis (OA) risk by enabling early detection and disease monitoring. Recent machine learning methods have improved risk estimation (i.e., predicting the likelihood of disease progression) and predictive modelling (i.e., the forecasting of future outcomes based on current data) using medical images, but clinical adoption remains limited due to their lack of interpretability. Existing approaches that generate future images for risk estimation are complex and impractical. Additionally, previous methods fail to localize anatomical knee landmarks, limiting interpretability. We address these gaps with a new interpretable machine learning method to estimate the risk of knee OA progression via multi-task predictive modelling that classifies future knee OA severity and predicts anatomical knee landmarks from efficiently generated high-quality future images. Such image generation is achieved by leveraging a diffusion model in a class-conditioned latent space to forecast disease progression, offering a visual representation of how particular health conditions may evolve. Applied to the Osteoarthritis Initiative dataset, our approach improves the state-of-the-art (SOTA) by 2\%, achieving an AUC of 0.71 in predicting knee OA progression while offering ~9% faster inference time.

</details>


### [56] [Synthetic Data Augmentation for Table Detection: Re-evaluating TableNet's Performance with Automatically Generated Document Images](https://arxiv.org/abs/2506.14583)
*Krishna Sahukara,Zineddine Bettouche,Andreas Fischer*

Main category: cs.CV

TL;DR: 论文提出了一种自动化LaTeX管道，用于生成包含多样化表格布局的合成数据，以增强真实数据集Marmot，并优化TableNet的性能。


<details>
  <summary>Details</summary>
Motivation: 手动提取文档中的表格效率低且易出错，需要自动化解决方案。

Method: 使用LaTeX生成具有多样化表格布局的合成页面，并训练TableNet模型。

Result: 在合成测试集上，TableNet的像素级XOR误差为4.04%（256x256）和4.33%（1024x1024）；在Marmot基准测试中为9.18%（256x256）。

Conclusion: 合成数据有效减少了人工标注需求，并提升了表格提取的自动化性能。

Abstract: Document pages captured by smartphones or scanners often contain tables, yet manual extraction is slow and error-prone. We introduce an automated LaTeX-based pipeline that synthesizes realistic two-column pages with visually diverse table layouts and aligned ground-truth masks. The generated corpus augments the real-world Marmot benchmark and enables a systematic resolution study of TableNet. Training TableNet on our synthetic data achieves a pixel-wise XOR error of 4.04% on our synthetic test set with a 256x256 input resolution, and 4.33% with 1024x1024. The best performance on the Marmot benchmark is 9.18% (at 256x256), while cutting manual annotation effort through automation.

</details>


### [57] [PoseGRAF: Geometric-Reinforced Adaptive Fusion for Monocular 3D Human Pose Estimation](https://arxiv.org/abs/2506.14596)
*Ming Xu,Xu Zhang*

Main category: cs.CV

TL;DR: PoseGRAF框架通过双图卷积结构和跨注意力模块改进单目3D姿态估计，解决了现有方法忽视骨骼方向与角度相关性的问题，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖关节位置特征，忽略了骨骼内在的方向和角度相关性，导致在关节遮挡或快速运动变化时产生不合理的姿态。

Method: 提出PoseGRAF框架，包括双图卷积结构（分别处理关节和骨骼图）、跨注意力模块建模骨骼方向与关节特征的依赖关系、动态融合模块自适应整合特征，以及改进的Transformer编码器生成最终输出。

Result: 在Human3.6M和MPI-INF-3DHP数据集上超越现有方法，并在野外视频中验证了泛化能力。

Conclusion: PoseGRAF通过结合关节和骨骼特征，显著提升了单目3D姿态估计的准确性和鲁棒性。

Abstract: Existing monocular 3D pose estimation methods primarily rely on joint positional features, while overlooking intrinsic directional and angular correlations within the skeleton. As a result, they often produce implausible poses under joint occlusions or rapid motion changes. To address these challenges, we propose the PoseGRAF framework. We first construct a dual graph convolutional structure that separately processes joint and bone graphs, effectively capturing their local dependencies. A Cross-Attention module is then introduced to model interdependencies between bone directions and joint features. Building upon this, a dynamic fusion module is designed to adaptively integrate both feature types by leveraging the relational dependencies between joints and bones. An improved Transformer encoder is further incorporated in a residual manner to generate the final output. Experimental results on the Human3.6M and MPI-INF-3DHP datasets show that our method exceeds state-of-the-art approaches. Additional evaluations on in-the-wild videos further validate its generalizability. The code is publicly available at https://github.com/iCityLab/PoseGRAF.

</details>


### [58] [Align Your Flow: Scaling Continuous-Time Flow Map Distillation](https://arxiv.org/abs/2506.14603)
*Amirmojtaba Sabour,Sanja Fidler,Karsten Kreis*

Main category: cs.CV

TL;DR: 本文提出了一种名为Align Your Flow的流映射模型，通过新的连续时间目标和训练技术，改进了现有的一致性模型和流匹配目标，实现了高效的少步生成性能。


<details>
  <summary>Details</summary>
Motivation: 扩散和流模型虽然是最先进的生成模型，但需要大量采样步骤。一致性模型可以将其蒸馏为一步生成器，但性能随步骤增加而下降。流映射通过连接任意两个噪声级别，解决了这一问题。

Method: 提出了两种新的连续时间目标用于训练流映射，并结合自动引导和对抗微调技术，提升性能。

Result: 在ImageNet 64x64和512x512上实现了最先进的少步生成性能，并在文本到图像任务中超越了现有非对抗训练的少步采样器。

Conclusion: Align Your Flow模型通过流映射和新技术，显著提升了生成模型的效率和性能。

Abstract: Diffusion- and flow-based models have emerged as state-of-the-art generative modeling approaches, but they require many sampling steps. Consistency models can distill these models into efficient one-step generators; however, unlike flow- and diffusion-based methods, their performance inevitably degrades when increasing the number of steps, which we show both analytically and empirically. Flow maps generalize these approaches by connecting any two noise levels in a single step and remain effective across all step counts. In this paper, we introduce two new continuous-time objectives for training flow maps, along with additional novel training techniques, generalizing existing consistency and flow matching objectives. We further demonstrate that autoguidance can improve performance, using a low-quality model for guidance during distillation, and an additional boost can be achieved by adversarial finetuning, with minimal loss in sample diversity. We extensively validate our flow map models, called Align Your Flow, on challenging image generation benchmarks and achieve state-of-the-art few-step generation performance on both ImageNet 64x64 and 512x512, using small and efficient neural networks. Finally, we show text-to-image flow map models that outperform all existing non-adversarially trained few-step samplers in text-conditioned synthesis.

</details>


### [59] [VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based Mosquito Breeding Site Detection and Reasoning](https://arxiv.org/abs/2506.14629)
*Md. Adnanul Islam,Md. Faiyaz Abdullah Sayeedi,Md. Asaduzzaman Shuvo,Muhammad Ziaur Rahman,Shahanur Rahman Bappy,Raiyan Rahman,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: VisText-Mosquito是一个多模态数据集，结合视觉和文本数据，用于蚊子孳生地分析的自动化检测、分割和推理。数据集包含标注图像和自然语言文本，模型表现优异，公开可用。


<details>
  <summary>Details</summary>
Motivation: 蚊媒疾病是全球重大健康威胁，需早期检测和主动控制孳生地以预防爆发。

Method: 数据集包括1,828张标注图像用于目标检测，142张用于水面分割，以及自然语言推理文本。使用YOLOv9s和YOLOv11n-Seg模型进行检测和分割，BLIP模型进行推理生成。

Result: YOLOv9s目标检测精度0.92926，mAP@50为0.92891；YOLOv11n-Seg分割精度0.91587，mAP@50为0.79795；BLIP推理生成BLEU得分54.7，BERTScore 0.91，ROUGE-L 0.87。

Conclusion: 数据集和模型框架强调“预防胜于治疗”，展示AI如何主动应对蚊媒疾病风险。数据和代码已公开。

Abstract: Mosquito-borne diseases pose a major global health risk, requiring early detection and proactive control of breeding sites to prevent outbreaks. In this paper, we present VisText-Mosquito, a multimodal dataset that integrates visual and textual data to support automated detection, segmentation, and reasoning for mosquito breeding site analysis. The dataset includes 1,828 annotated images for object detection, 142 images for water surface segmentation, and natural language reasoning texts linked to each image. The YOLOv9s model achieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object detection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and mAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves a final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and ROUGE-L of 0.87. This dataset and model framework emphasize the theme "Prevention is Better than Cure", showcasing how AI-based detection can proactively address mosquito-borne disease risks. The dataset and implementation code are publicly available at GitHub: https://github.com/adnanul-islam-jisun/VisText-Mosquito

</details>


### [60] [3DGS-IEval-15K: A Large-scale Image Quality Evaluation Database for 3D Gaussian-Splatting](https://arxiv.org/abs/2506.14642)
*Yuke Xing,Jiarui Wang,Peizhi Niu,Wenjie Huang,Guangtao Zhai,Yiling Xu*

Main category: cs.CV

TL;DR: 3DGS-IEval-15K是首个针对压缩3D高斯泼溅（3DGS）表示的大规模图像质量评估数据集，包含15,200张图像，用于评估压缩对感知质量的影响。


<details>
  <summary>Details</summary>
Motivation: 3DGS在实时渲染中表现优异，但存储需求高，现有压缩方法缺乏对其感知影响的全面评估框架。

Method: 通过6种代表性3DGS算法在10个真实场景中生成图像，结合不同压缩级别和20个视角，收集60名观众的主观评价数据。

Result: 数据集通过场景多样性和MOS分布分析验证质量，并建立包含30种IQA指标的基准。

Conclusion: 该数据集为开发3DGS专用IQA指标提供了基础，并支持研究3DGS特有的视角依赖质量分布模式。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a promising approach for novel view synthesis, offering real-time rendering with high visual fidelity. However, its substantial storage requirements present significant challenges for practical applications. While recent state-of-the-art (SOTA) 3DGS methods increasingly incorporate dedicated compression modules, there is a lack of a comprehensive framework to evaluate their perceptual impact. Therefore we present 3DGS-IEval-15K, the first large-scale image quality assessment (IQA) dataset specifically designed for compressed 3DGS representations. Our dataset encompasses 15,200 images rendered from 10 real-world scenes through 6 representative 3DGS algorithms at 20 strategically selected viewpoints, with different compression levels leading to various distortion effects. Through controlled subjective experiments, we collect human perception data from 60 viewers. We validate dataset quality through scene diversity and MOS distribution analysis, and establish a comprehensive benchmark with 30 representative IQA metrics covering diverse types. As the largest-scale 3DGS quality assessment dataset to date, our work provides a foundation for developing 3DGS specialized IQA metrics, and offers essential data for investigating view-dependent quality distribution patterns unique to 3DGS. The database is publicly available at https://github.com/YukeXing/3DGS-IEval-15K.

</details>


### [61] [DDS-NAS: Dynamic Data Selection within Neural Architecture Search via On-line Hard Example Mining applied to Image Classification](https://arxiv.org/abs/2506.14667)
*Matt Poyser,Toby P. Breckon*

Main category: cs.CV

TL;DR: 通过动态硬样本挖掘和课程学习框架加速神经架构搜索（NAS）训练，提出DDS-NAS框架，性能提升27倍。


<details>
  <summary>Details</summary>
Motivation: 解决神经架构搜索（NAS）中的可扩展性挑战，通过优化训练样本选择加速训练过程。

Method: 利用自动编码器构建图像相似性嵌入，通过kd树结构按最远邻不相似性排序图像，动态生成NAS优化的无偏子样本数据集。

Result: DDS-NAS框架将梯度基NAS策略加速高达27倍，且无性能损失。

Conclusion: 通过最大化每个样本的训练贡献，显著减少NAS训练周期和收敛所需迭代次数。

Abstract: In order to address the scalability challenge within Neural Architecture Search (NAS), we speed up NAS training via dynamic hard example mining within a curriculum learning framework. By utilizing an autoencoder that enforces an image similarity embedding in latent space, we construct an efficient kd-tree structure to order images by furthest neighbour dissimilarity in a low-dimensional embedding. From a given query image from our subsample dataset, we can identify the most dissimilar image within the global dataset in logarithmic time. Via curriculum learning, we then dynamically re-formulate an unbiased subsample dataset for NAS optimisation, upon which the current NAS solution architecture performs poorly. We show that our DDS-NAS framework speeds up gradient-based NAS strategies by up to 27x without loss in performance. By maximising the contribution of each image sample during training, we reduce the duration of a NAS training cycle and the number of iterations required for convergence.

</details>


### [62] [Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models](https://arxiv.org/abs/2506.14674)
*Ling Li,Yao Zhou,Yuxuan Liang,Fugee Tsung,Jiaheng Wei*

Main category: cs.CV

TL;DR: 论文提出了一种新方法GLOBE，通过构建多样化数据集MP16-Reason和改进视觉语言模型的推理能力，显著提升了图像地理定位任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有地理定位方法缺乏可解释性，且数据多样性和模型推理能力有限。

Method: 提出GLOBE方法，结合任务特定奖励，联合优化定位评估、视觉线索推理和地理定位准确性。

Result: GLOBE在多样化视觉场景中表现优于现有开源LVLMs，并生成更具洞察力的推理轨迹。

Conclusion: GLOBE为地理定位任务提供了更高效且可解释的解决方案。

Abstract: Previous methods for image geo-localization have typically treated the task as either classification or retrieval, often relying on black-box decisions that lack interpretability. The rise of large vision-language models (LVLMs) has enabled a rethinking of geo-localization as a reasoning-driven task grounded in visual cues. However, two major challenges persist. On the data side, existing reasoning-focused datasets are primarily based on street-view imagery, offering limited scene diversity and constrained viewpoints. On the modeling side, current approaches predominantly rely on supervised fine-tuning, which yields only marginal improvements in reasoning capabilities. To address these challenges, we propose a novel pipeline that constructs a reasoning-oriented geo-localization dataset, MP16-Reason, using diverse social media images. We introduce GLOBE, Group-relative policy optimization for Locatability assessment and Optimized visual-clue reasoning, yielding Bi-objective geo-Enhancement for the VLM in recognition and reasoning. GLOBE incorporates task-specific rewards that jointly enhance locatability assessment, visual clue reasoning, and geolocation accuracy. Both qualitative and quantitative results demonstrate that GLOBE outperforms state-of-the-art open-source LVLMs on geo-localization tasks, particularly in diverse visual scenes, while also generating more insightful and interpretable reasoning trajectories.

</details>


### [63] [FocalClick-XL: Towards Unified and High-quality Interactive Segmentation](https://arxiv.org/abs/2506.14686)
*Xi Chen,Hengshuang Zhao*

Main category: cs.CV

TL;DR: FocalClick-XL改进交互式分割方法，通过多级子网络处理不同层次信息，支持多种交互形式并实现精细细节捕捉。


<details>
  <summary>Details</summary>
Motivation: 现有交互式分割方法支持交互形式有限且难以捕捉细节，需要更灵活且高效的方法。

Method: 提出FocalClick-XL，分解交互分割为元任务（上下文、对象、细节），每级用独立子网络处理，并通过提示层编码交互类型。

Result: 在点击基准测试中表现最佳，适应多种交互形式（如框、涂鸦、粗掩模），还能预测精细alpha遮罩。

Conclusion: FocalClick-XL是一种多功能、强大的交互式分割工具，支持多种交互形式并实现精细细节捕捉。

Abstract: Interactive segmentation enables users to extract binary masks of target objects through simple interactions such as clicks, scribbles, and boxes. However, existing methods often support only limited interaction forms and struggle to capture fine details. In this paper, we revisit the classical coarse-to-fine design of FocalClick and introduce significant extensions. Inspired by its multi-stage strategy, we propose a novel pipeline, FocalClick-XL, to address these challenges simultaneously. Following the emerging trend of large-scale pretraining, we decompose interactive segmentation into meta-tasks that capture different levels of information -- context, object, and detail -- assigning a dedicated subnet to each level.This decomposition allows each subnet to undergo scaled pretraining with independent data and supervision, maximizing its effectiveness. To enhance flexibility, we share context- and detail-level information across different interaction forms as common knowledge while introducing a prompting layer at the object level to encode specific interaction types. As a result, FocalClick-XL achieves state-of-the-art performance on click-based benchmarks and demonstrates remarkable adaptability to diverse interaction formats, including boxes, scribbles, and coarse masks. Beyond binary mask generation, it is also capable of predicting alpha mattes with fine-grained details, making it a versatile and powerful tool for interactive segmentation.

</details>


### [64] [YOLOv11-RGBT: Towards a Comprehensive Single-Stage Multispectral Object Detection Framework](https://arxiv.org/abs/2506.14696)
*Dahang Wan,Rongsheng Lu,Yang Fang,Xianli Lang,Shuangbao Shu,Jingjing Chen,Siyuan Shen,Ting Xu,Zecong Ye*

Main category: cs.CV

TL;DR: 论文提出了一种基于YOLOv11的多光谱目标检测框架YOLOv11-RGBT，通过六种融合模式和可控微调策略优化性能。


<details>
  <summary>Details</summary>
Motivation: 多光谱目标检测在跨模态交互和低光条件下仍有挑战，如缺乏统一框架和模态权重分配不合理。

Method: 基于YOLOv11，设计了六种融合模式和P3中融合策略，并提出多光谱可控微调（MCF）策略。

Result: 在LLVIP和FLIR数据集上表现优异，FLIR上mAP提升3.41%-5.65%，最高达47.61%。

Conclusion: YOLOv11-RGBT框架和MCF策略有效提升了多光谱目标检测的性能和鲁棒性。

Abstract: Multispectral object detection, which integrates information from multiple bands, can enhance detection accuracy and environmental adaptability, holding great application potential across various fields. Although existing methods have made progress in cross-modal interaction, low-light conditions, and model lightweight, there are still challenges like the lack of a unified single-stage framework, difficulty in balancing performance and fusion strategy, and unreasonable modality weight allocation. To address these, based on the YOLOv11 framework, we present YOLOv11-RGBT, a new comprehensive multimodal object detection framework. We designed six multispectral fusion modes and successfully applied them to models from YOLOv3 to YOLOv12 and RT-DETR. After reevaluating the importance of the two modalities, we proposed a P3 mid-fusion strategy and multispectral controllable fine-tuning (MCF) strategy for multispectral models. These improvements optimize feature fusion, reduce redundancy and mismatches, and boost overall model performance. Experiments show our framework excels on three major open-source multispectral object detection datasets, like LLVIP and FLIR. Particularly, the multispectral controllable fine-tuning strategy significantly enhanced model adaptability and robustness. On the FLIR dataset, it consistently improved YOLOv11 models' mAP by 3.41%-5.65%, reaching a maximum of 47.61%, verifying the framework and strategies' effectiveness. The code is available at: https://github.com/wandahangFY/YOLOv11-RGBT.

</details>


### [65] [Iterative Camera-LiDAR Extrinsic Optimization via Surrogate Diffusion](https://arxiv.org/abs/2506.14706)
*Ni Ou,Zhuo Chen,Xinru Zhang,Junzheng Wang*

Main category: cs.CV

TL;DR: 提出了一种基于替代扩散的迭代框架，用于提升相机和LiDAR外参标定方法的性能，无需修改原方法架构。


<details>
  <summary>Details</summary>
Motivation: 现有端到端标定方法多为单步预测，缺乏迭代优化能力，难以满足高精度需求。

Method: 通过替代扩散框架，将初始外参通过去噪过程迭代优化，原标定方法作为替代去噪器。

Result: 实验表明，该框架显著提升了四种先进标定方法的精度、鲁棒性和稳定性。

Conclusion: 提出的迭代框架为外参标定提供了通用且高效的优化方案。

Abstract: Cameras and LiDAR are essential sensors for autonomous vehicles. The fusion of camera and LiDAR data addresses the limitations of individual sensors but relies on precise extrinsic calibration. Recently, numerous end-to-end calibration methods have been proposed; however, most predict extrinsic parameters in a single step and lack iterative optimization capabilities. To address the increasing demand for higher accuracy, we propose a versatile iterative framework based on surrogate diffusion. This framework can enhance the performance of any calibration method without requiring architectural modifications. Specifically, the initial extrinsic parameters undergo iterative refinement through a denoising process, in which the original calibration method serves as a surrogate denoiser to estimate the final extrinsics at each step. For comparative analysis, we selected four state-of-the-art calibration methods as surrogate denoisers and compared the results of our diffusion process with those of two other iterative approaches. Extensive experiments demonstrate that when integrated with our diffusion model, all calibration methods achieve higher accuracy, improved robustness, and greater stability compared to other iterative techniques and their single-step counterparts.

</details>


### [66] [DiFuse-Net: RGB and Dual-Pixel Depth Estimation using Window Bi-directional Parallax Attention and Cross-modal Transfer Learning](https://arxiv.org/abs/2506.14709)
*Kunal Swami,Debtanu Gupta,Amrit Kumar Muduli,Chirag Jaiswal,Pankaj Kumar Bajpai*

Main category: cs.CV

TL;DR: DiFuse-Net是一种新型模态解耦网络，用于RGB和双像素（DP）深度估计，通过WBiPAM机制捕捉智能手机相机的小光圈DP视差线索，并利用RGB上下文信息提升深度预测。


<details>
  <summary>Details</summary>
Motivation: 传统深度传感器在成本、功耗和鲁棒性方面存在局限，而现代相机中普遍存在的DP技术提供了一种替代方案。

Method: 提出DiFuse-Net，采用WBiPAM机制和RGB编码器提取特征，并通过跨模态迁移学习（CmTL）利用RGB-D数据集。

Result: DiFuse-Net在DP和立体基线方法中表现出优越性，并贡献了新的DCDP数据集。

Conclusion: DiFuse-Net为智能手机相机提供了一种高效的深度估计解决方案，并通过新数据集推动了相关研究。

Abstract: Depth estimation is crucial for intelligent systems, enabling applications from autonomous navigation to augmented reality. While traditional stereo and active depth sensors have limitations in cost, power, and robustness, dual-pixel (DP) technology, ubiquitous in modern cameras, offers a compelling alternative. This paper introduces DiFuse-Net, a novel modality decoupled network design for disentangled RGB and DP based depth estimation. DiFuse-Net features a window bi-directional parallax attention mechanism (WBiPAM) specifically designed to capture the subtle DP disparity cues unique to smartphone cameras with small aperture. A separate encoder extracts contextual information from the RGB image, and these features are fused to enhance depth prediction. We also propose a Cross-modal Transfer Learning (CmTL) mechanism to utilize large-scale RGB-D datasets in the literature to cope with the limitations of obtaining large-scale RGB-DP-D dataset. Our evaluation and comparison of the proposed method demonstrates its superiority over the DP and stereo-based baseline methods. Additionally, we contribute a new, high-quality, real-world RGB-DP-D training dataset, named Dual-Camera Dual-Pixel (DCDP) dataset, created using our novel symmetric stereo camera hardware setup, stereo calibration and rectification protocol, and AI stereo disparity estimation method.

</details>


### [67] [Active InSAR monitoring of building damage in Gaza during the Israel-Hamas War](https://arxiv.org/abs/2506.14730)
*Corey Scher,Jamon Van Den Hoek*

Main category: cs.CV

TL;DR: 论文提出了一种基于合成孔径雷达（SAR）的长期相干变化检测（LT-CCD）方法，用于实时监测加沙地带在2023年以哈战争中的建筑破坏情况，结果显示92.5%的破坏被准确检测到。


<details>
  <summary>Details</summary>
Motivation: 加沙地带在2023年10月7日开始的空袭中遭受了严重的城市破坏，需要一种实时监测方法来动态评估破坏情况。

Method: 使用Sentinel-1的干涉SAR数据，采用长期时间弧相干变化检测（LT-CCD）方法，每周跟踪破坏趋势。

Result: 检测到联合国参考数据中92.5%的破坏标签，误报率仅为1.2%；研究结束时，加沙地带五分之三的建筑（191,263栋）被破坏或摧毁。

Conclusion: 该方法成本低、延迟低，为人道主义和新闻机构提供了快速获取冲突地区破坏信息的有效手段。

Abstract: Aerial bombardment of the Gaza Strip beginning October 7, 2023 is one of the most intense bombing campaigns of the twenty-first century, driving widespread urban damage. Characterizing damage over a geographically dynamic and protracted armed conflict requires active monitoring. Synthetic aperture radar (SAR) has precedence for mapping disaster-induced damage with bi-temporal methods but applications to active monitoring during sustained crises are limited. Using interferometric SAR data from Sentinel-1, we apply a long temporal-arc coherent change detection (LT-CCD) approach to track weekly damage trends over the first year of the 2023- Israel-Hamas War. We detect 92.5% of damage labels in reference data from the United Nations with a negligible (1.2%) false positive rate. The temporal fidelity of our approach reveals rapidly increasing damage during the first three months of the war focused in northern Gaza, a notable pause in damage during a temporary ceasefire, and surges of new damage as conflict hot-spots shift from north to south. Three-fifths (191,263) of all buildings are damaged or destroyed by the end of the study. With massive need for timely data on damage in armed conflict zones, our low-cost and low-latency approach enables rapid uptake of damage information at humanitarian and journalistic organizations.

</details>


### [68] [SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting](https://arxiv.org/abs/2506.14742)
*Ziqiao Peng,Wentao Hu,Junyuan Ma,Xiangyu Zhu,Xiaomei Zhang,Hao Zhao,Hui Tian,Jun He,Hongyan Liu,Zhaoxin Fan*

Main category: cs.CV

TL;DR: SyncTalk++通过动态肖像渲染器和面部同步控制器解决语音驱动视频中的同步问题，显著提升真实感和渲染速度。


<details>
  <summary>Details</summary>
Motivation: 实现语音驱动视频中身份、唇动、表情和头部姿势的高同步是重大挑战，缺乏同步会导致不真实的结果。

Method: SyncTalk++采用动态肖像渲染器（高斯散射）和面部同步控制器（3D混合形状模型），并引入头部同步稳定器、表情生成器和躯干恢复器。

Result: 实验表明SyncTalk++在同步性和真实感上优于现有方法，渲染速度达101帧/秒。

Conclusion: SyncTalk++通过多模块协同解决了同步问题，显著提升了语音驱动视频的质量和效率。

Abstract: Achieving high synchronization in the synthesis of realistic, speech-driven talking head videos presents a significant challenge. A lifelike talking head requires synchronized coordination of subject identity, lip movements, facial expressions, and head poses. The absence of these synchronizations is a fundamental flaw, leading to unrealistic results. To address the critical issue of synchronization, identified as the ''devil'' in creating realistic talking heads, we introduce SyncTalk++, which features a Dynamic Portrait Renderer with Gaussian Splatting to ensure consistent subject identity preservation and a Face-Sync Controller that aligns lip movements with speech while innovatively using a 3D facial blendshape model to reconstruct accurate facial expressions. To ensure natural head movements, we propose a Head-Sync Stabilizer, which optimizes head poses for greater stability. Additionally, SyncTalk++ enhances robustness to out-of-distribution (OOD) audio by incorporating an Expression Generator and a Torso Restorer, which generate speech-matched facial expressions and seamless torso regions. Our approach maintains consistency and continuity in visual details across frames and significantly improves rendering speed and quality, achieving up to 101 frames per second. Extensive experiments and user studies demonstrate that SyncTalk++ outperforms state-of-the-art methods in synchronization and realism. We recommend watching the supplementary video: https://ziqiaopeng.github.io/synctalk++.

</details>


### [69] [Cost-Aware Routing for Efficient Text-To-Image Generation](https://arxiv.org/abs/2506.14753)
*Qinchan,Li,Kenneth Chen,Changyue,Su,Wittawat Jitkrittum,Qi Sun,Patsorn Sangkloy*

Main category: cs.CV

TL;DR: 论文提出了一种动态路由框架，根据提示的复杂性自动选择最适合的文本到图像生成方法，以平衡质量和计算成本。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成高质量图像但计算成本高，需要一种方法根据提示复杂性动态调整计算资源。

Method: 通过自动路由提示到不同的文本到图像生成函数（如不同步数的扩散模型或其他独立模型），学习保留高成本选项仅用于复杂提示。

Result: 在COCO和DiffusionDB上验证，路由到九个预训练模型后，平均质量高于单独使用任一模型。

Conclusion: 动态路由框架能有效优化计算成本与生成质量的权衡，适用于复杂和简单提示。

Abstract: Diffusion models are well known for their ability to generate a high-fidelity image for an input prompt through an iterative denoising process. Unfortunately, the high fidelity also comes at a high computational cost due the inherently sequential generative process. In this work, we seek to optimally balance quality and computational cost, and propose a framework to allow the amount of computation to vary for each prompt, depending on its complexity. Each prompt is automatically routed to the most appropriate text-to-image generation function, which may correspond to a distinct number of denoising steps of a diffusion model, or a disparate, independent text-to-image model. Unlike uniform cost reduction techniques (e.g., distillation, model quantization), our approach achieves the optimal trade-off by learning to reserve expensive choices (e.g., 100+ denoising steps) only for a few complex prompts, and employ more economical choices (e.g., small distilled model) for less sophisticated prompts. We empirically demonstrate on COCO and DiffusionDB that by learning to route to nine already-trained text-to-image models, our approach is able to deliver an average quality that is higher than that achievable by any of these models alone.

</details>


### [70] [Scaling-Up the Pretraining of the Earth Observation Foundation Model PhilEO to the MajorTOM Dataset](https://arxiv.org/abs/2506.14765)
*Nikolaos Dionelis,Jente Bosmans,Riccardo Musto,Giancarlo Paoletti,Simone Sarti,Giacomo Cascarano,Casper Fibaek,Luke Camilleri,Bertrand Le Saux,Nicolas Longépé*

Main category: cs.CV

TL;DR: 论文提出了一种基于大规模未标记数据的EO基础模型PhilEO，通过预训练和微调在多个下游任务中表现优异，验证了数据和模型规模扩展的有效性。


<details>
  <summary>Details</summary>
Motivation: 充分利用地球观测卫星生成的海量数据，通过预训练EO基础模型，实现在少量标注数据下的高效微调。

Method: 在23TB的MajorTOM数据集和2TB的FastTOM数据集上扩展PhilEO模型，研究不同参数和架构的变体，并在PhilEO Bench上进行微调和评估。

Result: PhilEO 44M MajorTOM 23TB模型在道路密度回归任务中表现最佳，而PhilEO 200M FastTOM在道路密度估计和建筑密度回归任务中表现最优。

Conclusion: 数据和模型规模的扩展对性能提升有显著影响，同时从CNN到ViT的架构转换也值得研究。

Abstract: Today, Earth Observation (EO) satellites generate massive volumes of data, with the Copernicus Sentinel-2 constellation alone producing approximately 1.6TB per day. To fully exploit this information, it is essential to pretrain EO Foundation Models (FMs) on large unlabeled datasets, enabling efficient fine-tuning for several different downstream tasks with minimal labeled data. In this work, we present the scaling-up of our recently proposed EO Foundation Model, PhilEO Geo-Aware U-Net, on the unlabeled 23TB dataset MajorTOM, which covers the vast majority of the Earth's surface, as well as on the specialized subset FastTOM 2TB that does not include oceans and ice. We develop and study various PhilEO model variants with different numbers of parameters and architectures. Finally, we fine-tune the models on the PhilEO Bench for road density estimation, building density pixel-wise regression, and land cover semantic segmentation, and we evaluate the performance. Our results demonstrate that for all n-shots for road density regression, the PhilEO 44M MajorTOM 23TB model outperforms PhilEO Globe 0.5TB 44M. We also show that for most n-shots for road density estimation and building density regression, PhilEO 200M FastTOM outperforms all the other models. The effectiveness of both dataset and model scaling is validated using the PhilEO Bench. We also study the impact of architecture scaling, transitioning from U-Net Convolutional Neural Networks (CNN) to Vision Transformers (ViT).

</details>


### [71] [ASCD: Attention-Steerable Contrastive Decoding for Reducing Hallucination in MLLM](https://arxiv.org/abs/2506.14766)
*Yujun Wang,Jinhe Bi,Yunpu Ma,Soeren Pirk*

Main category: cs.CV

TL;DR: 本文提出了一种基于注意力机制的可控对比解码框架，通过直接干预模型的注意力分布来减少多模态大语言模型（MLLM）的幻觉问题，实验证明该方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLM）常因过度依赖部分线索而产生幻觉，现有方法（如VCD和ICD）虽有效但未深入探讨其内部机制。本文发现这些方法通过改变注意力分布发挥作用，因此提出更直接的注意力干预方法。

Method: 提出了一种注意力可控的对比解码框架，直接干预模型的注意力机制，以减少幻觉并提升性能。

Result: 实验表明，该方法在POPE、CHAIR和MMHal-Bench等基准上显著减少了幻觉，同时提升了标准VQA任务的性能。

Conclusion: 通过直接干预注意力机制，本文提出的方法为减少MLLM的幻觉问题提供了更有效且原理性的解决方案。

Abstract: Multimodal Large Language Model (MLLM) often suffer from hallucinations. They over-rely on partial cues and generate incorrect responses. Recently, methods like Visual Contrastive Decoding (VCD) and Instruction Contrastive Decoding (ICD) have been proposed to mitigate hallucinations by contrasting predictions from perturbed or negatively prefixed inputs against original outputs. In this work, we uncover that methods like VCD and ICD fundamentally influence internal attention dynamics of the model. This observation suggests that their effectiveness may not stem merely from surface-level modifications to logits but from deeper shifts in attention distribution. Inspired by this insight, we propose an attention-steerable contrastive decoding framework that directly intervenes in attention mechanisms of the model to offer a more principled approach to mitigating hallucinations. Our experiments across multiple MLLM architectures and diverse decoding methods demonstrate that our approach significantly reduces hallucinations and improves the performance on benchmarks such as POPE, CHAIR, and MMHal-Bench, while simultaneously enhancing performance on standard VQA benchmarks.

</details>


### [72] [CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion](https://arxiv.org/abs/2506.14769)
*Jiahua Ma,Yiran Qin,Yixiong Li,Xuanqi Liao,Yulan Guo,Ruimao Zhang*

Main category: cs.CV

TL;DR: CDP是一种基于Transformer的扩散模型，通过历史动作序列增强动作预测，提升机器人策略学习的连贯性和上下文感知能力。


<details>
  <summary>Details</summary>
Motivation: 硬件限制和实时约束降低了专家示范数据质量，导致机器人任务执行失败。

Method: 提出Causal Diffusion Policy (CDP)，利用历史动作序列和缓存机制减少计算冗余。

Result: 在模拟和真实环境中，CDP显著优于现有方法，且在输入质量下降时仍保持高精度。

Conclusion: CDP在现实不完美条件下展现出强大的机器人控制鲁棒性。

Abstract: Diffusion Policy (DP) enables robots to learn complex behaviors by imitating expert demonstrations through action diffusion. However, in practical applications, hardware limitations often degrade data quality, while real-time constraints restrict model inference to instantaneous state and scene observations. These limitations seriously reduce the efficacy of learning from expert demonstrations, resulting in failures in object localization, grasp planning, and long-horizon task execution. To address these challenges, we propose Causal Diffusion Policy (CDP), a novel transformer-based diffusion model that enhances action prediction by conditioning on historical action sequences, thereby enabling more coherent and context-aware visuomotor policy learning. To further mitigate the computational cost associated with autoregressive inference, a caching mechanism is also introduced to store attention key-value pairs from previous timesteps, substantially reducing redundant computations during execution. Extensive experiments in both simulated and real-world environments, spanning diverse 2D and 3D manipulation tasks, demonstrate that CDP uniquely leverages historical action sequences to achieve significantly higher accuracy than existing methods. Moreover, even when faced with degraded input observation quality, CDP maintains remarkable precision by reasoning through temporal continuity, which highlights its practical robustness for robotic control under realistic, imperfect conditions.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [73] [BraTS orchestrator : Democratizing and Disseminating state-of-the-art brain tumor image analysis](https://arxiv.org/abs/2506.13807)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Ujjwal Baid,Hendrik Möller,Josef A. Buchner,Felix Steinbauer,Eva Oswald,Ezequiel de la Rosa,Ivan Ezhov,Constantin von See,Jan Kirschke,Anton Schmick,Sarthak Pati,Akis Linardos,Carla Pitarch,Sanyukta Adap,Jeffrey Rudie,Maria Correia de Verdier,Rachit Saluja,Evan Calabrese,Dominic LaBella,Mariam Aboian,Ahmed W. Moawad,Nazanin Maleki,Udunna Anazodo,Maruf Adewole,Marius George Linguraru,Anahita Fathi Kazerooni,Zhifan Jiang,Gian Marco Conte,Hongwei Li,Juan Eugenio Iglesias,Spyridon Bakas,Benedikt Wiestler,Marie Piraud,Bjoern Menze*

Main category: eess.IV

TL;DR: BraTS orchestrator是一个开源Python包，旨在简化BraTS挑战中先进算法的使用，促进其在科研和临床中的普及。


<details>
  <summary>Details</summary>
Motivation: 尽管BraTS挑战在脑肿瘤图像分析方面取得了显著进展，但其算法和模型在科学和临床社区中的采用率有限。

Method: 通过开发一个开源Python包（BraTS orchestrator），提供直观的教程和接口，简化先进算法的部署。

Result: 该包使得研究人员和临床医生能够轻松使用BraTS挑战中的先进算法，无需深度学习专业知识。

Conclusion: BraTS orchestrator通过降低技术门槛，推动了BraTS社区成果在更广泛领域的应用。

Abstract: The Brain Tumor Segmentation (BraTS) cluster of challenges has significantly advanced brain tumor image analysis by providing large, curated datasets and addressing clinically relevant tasks. However, despite its success and popularity, algorithms and models developed through BraTS have seen limited adoption in both scientific and clinical communities. To accelerate their dissemination, we introduce BraTS orchestrator, an open-source Python package that provides seamless access to state-of-the-art segmentation and synthesis algorithms for diverse brain tumors from the BraTS challenge ecosystem. Available on GitHub (https://github.com/BrainLesion/BraTS), the package features intuitive tutorials designed for users with minimal programming experience, enabling both researchers and clinicians to easily deploy winning BraTS algorithms for inference. By abstracting the complexities of modern deep learning, BraTS orchestrator democratizes access to the specialized knowledge developed within the BraTS community, making these advances readily available to broader neuro-radiology and neuro-oncology audiences.

</details>


### [74] [Reliable Noninvasive Glucose Sensing via CNN-Based Spectroscopy](https://arxiv.org/abs/2506.13819)
*El Arbi Belfarsi,Henry Flores,Maria Valero*

Main category: eess.IV

TL;DR: 提出了一种基于短波红外光谱的双模态AI框架，结合CNN和光电二极管传感器，用于非侵入式血糖监测。


<details>
  <summary>Details</summary>
Motivation: 开发一种临床准确、成本高效且可穿戴的非侵入式血糖监测解决方案。

Method: 第一模态使用多波长SWIR成像系统和CNN捕捉空间特征；第二模态使用光电二极管电压传感器和机器学习回归器分析光学信号。

Result: CNN的MAPE为4.82%，光电二极管系统的Zone A准确率为86.4%。

Conclusion: 该框架在临床准确性、成本效率和可穿戴性方面表现优异，为非侵入式血糖监测提供了可靠方案。

Abstract: In this study, we present a dual-modal AI framework based on short-wave infrared (SWIR) spectroscopy. The first modality employs a multi-wavelength SWIR imaging system coupled with convolutional neural networks (CNNs) to capture spatial features linked to glucose absorption. The second modality uses a compact photodiode voltage sensor and machine learning regressors (e.g., random forest) on normalized optical signals. Both approaches were evaluated on synthetic blood phantoms and skin-mimicking materials across physiological glucose levels (70 to 200 mg/dL). The CNN achieved a mean absolute percentage error (MAPE) of 4.82% at 650 nm with 100% Zone A coverage in the Clarke Error Grid, while the photodiode system reached 86.4% Zone A accuracy. This framework constitutes a state-of-the-art solution that balances clinical accuracy, cost efficiency, and wearable integration, paving the way for reliable continuous non-invasive glucose monitoring.

</details>


### [75] [Comparison of ConvNeXt and Vision-Language Models for Breast Density Assessment in Screening Mammography](https://arxiv.org/abs/2506.13964)
*Yusdivia Molina-Román,David Gómez-Ortiz,Ernestina Menasalvas-Ruiz,José Gerardo Tamez-Peña,Alejandro Santos-Díaz*

Main category: eess.IV

TL;DR: 研究比较了多模态和CNN方法在乳腺密度分类中的表现，发现端到端微调的CNN模型优于多模态方法。


<details>
  <summary>Details</summary>
Motivation: 乳腺密度分类对癌症风险评估至关重要，但主观解释和观察者间差异使其具有挑战性。

Method: 使用BioMedCLIP和ConvNeXt，在零样本分类、线性探测和微调三种学习场景下评估。

Result: 微调的ConvNeXt表现最佳，多模态方法虽具潜力但效果较弱。

Conclusion: 未来需改进文本表示和领域适应，CNN在医学影像中更具优势。

Abstract: Mammographic breast density classification is essential for cancer risk assessment but remains challenging due to subjective interpretation and inter-observer variability. This study compares multimodal and CNN-based methods for automated classification using the BI-RADS system, evaluating BioMedCLIP and ConvNeXt across three learning scenarios: zero-shot classification, linear probing with textual descriptions, and fine-tuning with numerical labels. Results show that zero-shot classification achieved modest performance, while the fine-tuned ConvNeXt model outperformed the BioMedCLIP linear probe. Although linear probing demonstrated potential with pretrained embeddings, it was less effective than full fine-tuning. These findings suggest that despite the promise of multimodal learning, CNN-based models with end-to-end fine-tuning provide stronger performance for specialized medical imaging. The study underscores the need for more detailed textual representations and domain-specific adaptations in future radiology applications.

</details>


### [76] [DREAM: On hallucinations in AI-generated content for nuclear medicine imaging](https://arxiv.org/abs/2506.13995)
*Menghua Xia,Reimund Bayerlein,Yanis Chemli,Xiaofeng Liu,Jinsong Ouyang,Georges El Fakhri,Ramsey D. Badawi,Quanzheng Li,Chi Liu*

Main category: eess.IV

TL;DR: 论文探讨了AI生成内容（AIGC）在核医学影像（NMI）中的幻觉问题，提出了DREAM报告以解决定义、检测、评估和缓解策略。


<details>
  <summary>Details</summary>
Motivation: AIGC在NMI中表现出色，但幻觉问题可能误导诊断，影响临床信任，需系统性解决方案。

Method: 提出DREAM报告，涵盖定义、示例、检测指标、原因分析和缓解策略。

Result: 为AIGC在NMI中的应用提供了系统性框架，支持安全有效的临床部署。

Conclusion: 旨在建立共识，推动未来研究，提升AIGC在NMI中的可靠性和实用性。

Abstract: Artificial intelligence-generated content (AIGC) has shown remarkable performance in nuclear medicine imaging (NMI), offering cost-effective software solutions for tasks such as image enhancement, motion correction, and attenuation correction. However, these advancements come with the risk of hallucinations, generating realistic yet factually incorrect content. Hallucinations can misrepresent anatomical and functional information, compromising diagnostic accuracy and clinical trust. This paper presents a comprehensive perspective of hallucination-related challenges in AIGC for NMI, introducing the DREAM report, which covers recommendations for definition, representative examples, detection and evaluation metrics, underlying causes, and mitigation strategies. This position statement paper aims to initiate a common understanding for discussions and future research toward enhancing AIGC applications in NMI, thereby supporting their safe and effective deployment in clinical practice.

</details>


### [77] [Breaking the Multi-Enhancement Bottleneck: Domain-Consistent Quality Enhancement for Compressed Images](https://arxiv.org/abs/2506.14152)
*Qunliang Xing,Mai Xu,Jing Yang,Shengxi Li*

Main category: eess.IV

TL;DR: 论文提出了一种新方法，将现有质量增强模型转化为领域一致模型，以解决多增强场景下的性能退化问题。


<details>
  <summary>Details</summary>
Motivation: 当前质量增强方法在多增强场景下性能严重退化，需要一种方法确保多次增强后图像质量不进一步下降。

Method: 通过将低质量压缩图像首次增强到自然领域，并确保后续增强保持质量，实现领域一致性。

Result: 实验验证了方法的有效性，现有模型在多增强场景下能保持保真度和感知质量。

Conclusion: 该方法成功解决了多增强场景下的性能退化问题，提升了图像质量增强的鲁棒性。

Abstract: Quality enhancement methods have been widely integrated into visual communication pipelines to mitigate artifacts in compressed images. Ideally, these quality enhancement methods should perform robustly when applied to images that have already undergone prior enhancement during transmission. We refer to this scenario as multi-enhancement, which generalizes the well-known multi-generation scenario of image compression. Unfortunately, current quality enhancement methods suffer from severe degradation when applied in multi-enhancement. To address this challenge, we propose a novel adaptation method that transforms existing quality enhancement models into domain-consistent ones. Specifically, our method enhances a low-quality compressed image into a high-quality image within the natural domain during the first enhancement, and ensures that subsequent enhancements preserve this quality without further degradation. Extensive experiments validate the effectiveness of our method and show that various existing models can be successfully adapted to maintain both fidelity and perceptual quality in multi-enhancement scenarios.

</details>


### [78] [Latent Anomaly Detection: Masked VQ-GAN for Unsupervised Segmentation in Medical CBCT](https://arxiv.org/abs/2506.14209)
*Pengwei Wang*

Main category: eess.IV

TL;DR: 提出了一种无监督训练方法，用于自动识别ONJ影像中的异常，通过两阶段训练流程实现，并在模拟和真实数据上成功分割。


<details>
  <summary>Details</summary>
Motivation: 由于ONJ影像中标记数据的稀缺性，监督训练不切实际，因此需要开发无监督方法。

Method: 采用两阶段训练流程：第一阶段训练VQ-GAN重建正常样本；第二阶段通过随机和ONJ特定掩码训练新编码器。

Result: 方法在模拟和真实患者数据上均实现了成功分割。

Conclusion: 该方法减少了手动标记负担，并有望直接用于3D打印。

Abstract: Advances in treatment technology now allow for the use of customizable 3D-printed hydrogel wound dressings for patients with osteoradionecrosis (ORN) of the jaw (ONJ). Meanwhile, deep learning has enabled precise segmentation of 3D medical images using tools like nnUNet.
  However, the scarcity of labeled data in ONJ imaging makes supervised training impractical. This study aims to develop an unsupervised training approach for automatically identifying anomalies in imaging scans.
  We propose a novel two-stage training pipeline. In the first stage, a VQ-GAN is trained to accurately reconstruct normal subjects. In the second stage, random cube masking and ONJ-specific masking are applied to train a new encoder capable of recovering the data.
  The proposed method achieves successful segmentation on both simulated and real patient data.
  This approach provides a fast initial segmentation solution, reducing the burden of manual labeling. Additionally, it has the potential to be directly used for 3D printing when combined with hand-tuned post-processing.

</details>


### [79] [orGAN: A Synthetic Data Augmentation Pipeline for Simultaneous Generation of Surgical Images and Ground Truth Labels](https://arxiv.org/abs/2506.14303)
*Niran Nataraj,Maina Sogabe,Kenji Kawashima*

Main category: eess.IV

TL;DR: orGAN是一种基于GAN的系统，用于生成高保真、带注释的手术出血图像，解决了医疗影像中数据多样性不足和伦理问题。


<details>
  <summary>Details</summary>
Motivation: 手术中出血检测和定位因高质量数据集稀缺而具挑战性，传统方法成本高且涉及伦理问题。

Method: 利用小型“模拟器官”数据集，结合StyleGAN和关系位置学习生成逼真出血图像，并通过LaMa修复模块生成精确注释。

Result: 混合数据集在手术环境中达到90%检测准确率和99%帧级准确率。

Conclusion: orGAN为高效、低成本生成真实注释数据集提供了新途径，推动了AI在手术中的应用。

Abstract: Deep learning in medical imaging faces obstacles: limited data diversity, ethical issues, high acquisition costs, and the need for precise annotations. Bleeding detection and localization during surgery is especially challenging due to the scarcity of high-quality datasets that reflect real surgical scenarios. We propose orGAN, a GAN-based system for generating high-fidelity, annotated surgical images of bleeding. By leveraging small "mimicking organ" datasets, synthetic models that replicate tissue properties and bleeding, our approach reduces ethical concerns and data-collection costs. orGAN builds on StyleGAN with Relational Positional Learning to simulate bleeding events realistically and mark bleeding coordinates. A LaMa-based inpainting module then restores clean, pre-bleed visuals, enabling precise pixel-level annotations. In evaluations, a balanced dataset of orGAN and mimicking-organ images achieved 90% detection accuracy in surgical settings and up to 99% frame-level accuracy. While our development data lack diverse organ morphologies and contain intraoperative artifacts, orGAN markedly advances ethical, efficient, and cost-effective creation of realistic annotated bleeding datasets, supporting broader integration of AI in surgical practice.

</details>


### [80] [BRISC: Annotated Dataset for Brain Tumor Segmentation and Classification with Swin-HAFNet](https://arxiv.org/abs/2506.14318)
*Amirreza Fateh,Yasin Rezvani,Sara Moayedi,Sadjad Rezvani,Fatemeh Fateh,Mansoor Fateh*

Main category: eess.IV

TL;DR: 论文介绍了一个新的脑肿瘤MRI数据集，包含6,000个标注样本，并提出了一种基于Transformer的分割模型，取得了82.3%的IoU。


<details>
  <summary>Details</summary>
Motivation: 解决脑肿瘤MRI数据缺乏高质量、平衡和多样化数据集的问题。

Method: 提出一个新的MRI数据集，并开发了一个基于Transformer的分割模型。

Result: 模型在加权平均IoU上达到82.3%，在所有肿瘤类别中均有提升。

Conclusion: 该数据集为神经肿瘤学的机器学习应用提供了宝贵资源，支持未来研究和临床决策支持开发。

Abstract: Accurate segmentation and classification of brain tumors from Magnetic Resonance Imaging (MRI) remain key challenges in medical image analysis, largely due to the lack of high-quality, balanced, and diverse datasets. In this work, we present a new curated MRI dataset designed specifically for brain tumor segmentation and classification tasks. The dataset comprises 6,000 contrast-enhanced T1-weighted MRI scans annotated by certified radiologists and physicians, spanning three major tumor types-glioma, meningioma, and pituitary-as well as non-tumorous cases. Each sample includes high-resolution labels and is categorized across axial, sagittal, and coronal imaging planes to facilitate robust model development and cross-view generalization. To demonstrate the utility of the dataset, we propose a transformer-based segmentation model and benchmark it against established baselines. Our method achieves the highest weighted mean Intersection-over-Union (IoU) of 82.3%, with improvements observed across all tumor categories. Importantly, this study serves primarily as an introduction to the dataset, establishing foundational benchmarks for future research. We envision this dataset as a valuable resource for advancing machine learning applications in neuro-oncology, supporting both academic research and clinical decision-support development. datasetlink: https://www.kaggle.com/datasets/briscdataset/brisc2025/

</details>


### [81] [Compressed Video Super-Resolution based on Hierarchical Encoding](https://arxiv.org/abs/2506.14381)
*Yuxuan Jiang,Siyue Teng,Qiang Zhu,Chen Feng,Chengxi Zeng,Fan Zhang,Shuyuan Zhu,Bing Zeng,David Bull*

Main category: eess.IV

TL;DR: VSR-HE是一种通用视频超分辨率方法，专注于提升压缩内容的感知质量，通过分层编码变换块消除H.265/HEVC编码引入的压缩伪影。


<details>
  <summary>Details</summary>
Motivation: 针对高压缩场景下视频质量下降的问题，提出一种能够恢复细节并保持视觉保真度的超分辨率方法。

Method: 采用分层编码变换块，优化以消除多种压缩伪影，支持从180p到720p或270p到1080p的四倍放大。

Result: 模型在多样化压缩设置下训练和评估，能够有效恢复细节并保持视觉质量。

Conclusion: VSR-HE方法在视频超分辨率任务中表现出色，已提交至ICME 2025挑战赛。

Abstract: This paper presents a general-purpose video super-resolution (VSR) method, dubbed VSR-HE, specifically designed to enhance the perceptual quality of compressed content. Targeting scenarios characterized by heavy compression, the method upscales low-resolution videos by a ratio of four, from 180p to 720p or from 270p to 1080p. VSR-HE adopts hierarchical encoding transformer blocks and has been sophisticatedly optimized to eliminate a wide range of compression artifacts commonly introduced by H.265/HEVC encoding across various quantization parameter (QP) levels. To ensure robustness and generalization, the model is trained and evaluated under diverse compression settings, allowing it to effectively restore fine-grained details and preserve visual fidelity. The proposed VSR-HE has been officially submitted to the ICME 2025 Grand Challenge on VSR for Video Conferencing (Team BVI-VSR), under both the Track 1 (General-Purpose Real-World Video Content) and Track 2 (Talking Head Videos).

</details>


### [82] [A large-scale heterogeneous 3D magnetic resonance brain imaging dataset for self-supervised learning](https://arxiv.org/abs/2506.14432)
*Asbjørn Munk,Stefano Cerri,Jakob Ambsdorf,Julia Machnio,Sebastian Nørgaard Llambias,Vardan Nersesjan,Christian Hedeager Krag,Peirong Liu,Pablo Rocamora García,Mostafa Mehdipour Ghazi,Mikael Boesen,Michael Eriksen Benros,Juan Eugenio Iglesias,Mads Nielsen*

Main category: eess.IV

TL;DR: FOMO60K是一个包含60,529个脑部MRI扫描的大规模异构数据集，旨在支持医学影像中自监督学习方法的开发和基准测试。


<details>
  <summary>Details</summary>
Motivation: 为医学影像领域提供大规模、多样化的数据集，以促进自监督学习方法的研究和应用。

Method: 数据集整合了16个公开来源的MRI扫描，涵盖多种序列和病理变异，仅进行最小预处理以保留原始特征。

Result: 提供了60,529个扫描，来自13,900次会话和11,187名受试者，包括临床和研究级图像。

Conclusion: FOMO60K为医学影像的自监督学习提供了丰富的资源和工具，降低了新用户的入门门槛。

Abstract: We present FOMO60K, a large-scale, heterogeneous dataset of 60,529 brain Magnetic Resonance Imaging (MRI) scans from 13,900 sessions and 11,187 subjects, aggregated from 16 publicly available sources. The dataset includes both clinical- and research-grade images, multiple MRI sequences, and a wide range of anatomical and pathological variability, including scans with large brain anomalies. Minimal preprocessing was applied to preserve the original image characteristics while reducing barriers to entry for new users. Accompanying code for self-supervised pretraining and finetuning is provided. FOMO60K is intended to support the development and benchmarking of self-supervised learning methods in medical imaging at scale.

</details>


### [83] [Towards Reliable WMH Segmentation under Domain Shift: An Application Study using Maximum Entropy Regularization to Improve Uncertainty Estimation](https://arxiv.org/abs/2506.14497)
*Franco Matzkin,Agostina Larrazabal,Diego H Milone,Jose Dolz,Enzo Ferrante*

Main category: eess.IV

TL;DR: 研究提出最大熵正则化技术，用于增强白质高信号（WMH）分割模型的校准和不确定性估计，以应对领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 领域偏移（如MRI机器类型或采集参数的变化）对WMH分割模型的校准和不确定性估计带来挑战，影响临床决策。

Method: 采用U-Net架构，结合最大熵正则化技术，评估其在两个公开数据集上的性能，使用Dice系数、预期校准误差和基于熵的不确定性估计。

Result: 基于熵的不确定性估计能预测分割错误，最大熵正则化增强了不确定性与分割性能的关联，并改善了领域偏移下的模型校准。

Conclusion: 最大熵正则化技术能有效提升WMH分割模型在领域偏移下的校准和不确定性估计能力。

Abstract: Accurate segmentation of white matter hyperintensities (WMH) is crucial for clinical decision-making, particularly in the context of multiple sclerosis. However, domain shifts, such as variations in MRI machine types or acquisition parameters, pose significant challenges to model calibration and uncertainty estimation. This study investigates the impact of domain shift on WMH segmentation by proposing maximum-entropy regularization techniques to enhance model calibration and uncertainty estimation, with the purpose of identifying errors post-deployment using predictive uncertainty as a proxy measure that does not require ground-truth labels. To do this, we conducted experiments using a U-Net architecture to evaluate these regularization schemes on two publicly available datasets, assessing performance with the Dice coefficient, expected calibration error, and entropy-based uncertainty estimates. Our results show that entropy-based uncertainty estimates can anticipate segmentation errors, and that maximum-entropy regularization further strengthens the correlation between uncertainty and segmentation performance while also improving model calibration under domain shift.

</details>


### [84] [Integrating Radiomics with Deep Learning Enhances Multiple Sclerosis Lesion Delineation](https://arxiv.org/abs/2506.14524)
*Nadezhda Alsahanova,Pavel Bartenev,Maksim Sharaev,Milos Ljubisavljevic,Taleb Al. Mansoori,Yauhen Statsenko*

Main category: eess.IV

TL;DR: 该研究通过结合放射组学特征和原始影像数据，提升了多发性硬化（MS）病灶分割的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在MS病灶分割中存在鲁棒性挑战，研究旨在通过数据融合技术改进分割性能。

Method: 提出新的放射组学特征（浓度率和Rényi熵），并将其与原始影像数据融合，采用ResNeXt-UNet和注意力增强U-Net架构进行评估。

Result: 放射组学增强的ResNeXt-UNet显著提高了分割精度和敏感性（Dice得分0.774±0.05；p<0.001），注意力增强U-Net模型稳定性更高（性能变异性降低）。

Conclusion: 融合放射组学与原始影像数据能显著提升分割性能和模型稳定性。

Abstract: Background: Accurate lesion segmentation is critical for multiple sclerosis (MS) diagnosis, yet current deep learning approaches face robustness challenges.
  Aim: This study improves MS lesion segmentation by combining data fusion and deep learning techniques.
  Materials and Methods: We suggested novel radiomic features (concentration rate and Rényi entropy) to characterize different MS lesion types and fused these with raw imaging data. The study integrated radiomic features with imaging data through a ResNeXt-UNet architecture and attention-augmented U-Net architecture. Our approach was evaluated on scans from 46 patients (1102 slices), comparing performance before and after data fusion.
  Results: The radiomics-enhanced ResNeXt-UNet demonstrated high segmentation accuracy, achieving significant improvements in precision and sensitivity over the MRI-only baseline and a Dice score of 0.774$\pm$0.05; p<0.001 according to Bonferroni-adjusted Wilcoxon signed-rank tests. The radiomics-enhanced attention-augmented U-Net model showed a greater model stability evidenced by reduced performance variability (SDD = 0.18 $\pm$ 0.09 vs. 0.21 $\pm$ 0.06; p=0.03) and smoother validation curves with radiomics integration.
  Conclusion: These results validate our hypothesis that fusing radiomics with raw imaging data boosts segmentation performance and stability in state-of-the-art models.

</details>


### [85] [Optimization-Based Image Restoration under Implementation Constraints in Optical Analog Circuits](https://arxiv.org/abs/2506.14624)
*Taisei Kato,Ryo Hayakawa,Soma Furusawa,Kazunori Hayashi,Youji Iiguni*

Main category: eess.IV

TL;DR: 论文研究了在光学模拟电路中实现图像恢复算法的可行性，重点解决了动态变量除法和光学放大器噪声问题，通过ADMM和PDS方法设计电路结构，并在仿真中验证了去噪效果。


<details>
  <summary>Details</summary>
Motivation: 光学模拟电路因其低延迟和低功耗特性在信号处理中具有潜力，但动态变量除法和噪声问题限制了迭代算法的实现。

Method: 设计了基于ADMM和PDS方法的电路结构，避免了动态变量除法并考虑了光学放大器的噪声影响。

Result: 仿真结果显示，即使在放大器噪声存在的情况下，PSNR和SSIM指标仍能实现有效的去噪。

Conclusion: 研究表明光学模拟电路可用于图像恢复算法，为低功耗信号处理提供了新思路。

Abstract: Optical analog circuits have attracted attention as promising alternatives to traditional electronic circuits for signal processing tasks due to their potential for low-latency and low-power computations. However, implementing iterative algorithms on such circuits presents challenges, particularly due to the difficulty of performing division operations involving dynamically changing variables and the additive noise introduced by optical amplifiers. In this study, we investigate the feasibility of implementing image restoration algorithms using total variation regularization on optical analog circuits. Specifically, we design the circuit structures for the image restoration with widely used alternating direction method of multipliers (ADMM) and primal dual splitting (PDS). Our design avoids division operations involving dynamic variables and incorporate the impact of additive noise introduced by optical amplifiers. Simulation results show that the effective denoising can be achieved in terms of peak signal to noise ratio (PSNR) and structural similarity index measure (SSIM) even when the circuit noise at the amplifiers is taken into account.

</details>


### [86] [Plug-and-Play with 2.5D Artifact Reduction Prior for Fast and Accurate Industrial Computed Tomography Reconstruction](https://arxiv.org/abs/2506.14719)
*Haley Duba-Sullivan,Aniket Pramanik,Venkatakrishnan Singanallur,Amirkoushyar Ziabari*

Main category: eess.IV

TL;DR: 提出了一种基于2.5D CNN的PnP重建方法，用于稀疏视图XCT扫描，显著提升了重建质量和通用性。


<details>
  <summary>Details</summary>
Motivation: 传统2D CNN在XCT重建中仅能捕获切片独立信息，限制了性能。

Method: 采用2.5D CNN作为先验，利用相邻切片信息，提升空间上下文捕捉能力。

Result: 实验表明，2.5D先验不仅提高了重建质量，还能直接抑制常见XCT伪影，无需预处理。

Conclusion: 该方法在实验和合成数据上均表现出色，尤其在跨域通用性方面表现突出。

Abstract: Cone-beam X-ray computed tomography (XCT) is an essential imaging technique for generating 3D reconstructions of internal structures, with applications ranging from medical to industrial imaging. Producing high-quality reconstructions typically requires many X-ray measurements; this process can be slow and expensive, especially for dense materials. Recent work incorporating artifact reduction priors within a plug-and-play (PnP) reconstruction framework has shown promising results in improving image quality from sparse-view XCT scans while enhancing the generalizability of deep learning-based solutions. However, this method uses a 2D convolutional neural network (CNN) for artifact reduction, which captures only slice-independent information from the 3D reconstruction, limiting performance. In this paper, we propose a PnP reconstruction method that uses a 2.5D artifact reduction CNN as the prior. This approach leverages inter-slice information from adjacent slices, capturing richer spatial context while remaining computationally efficient. We show that this 2.5D prior not only improves the quality of reconstructions but also enables the model to directly suppress commonly occurring XCT artifacts (such as beam hardening), eliminating the need for artifact correction pre-processing. Experiments on both experimental and synthetic cone-beam XCT data demonstrate that the proposed method better preserves fine structural details, such as pore size and shape, leading to more accurate defect detection compared to 2D priors. In particular, we demonstrate strong performance on experimental XCT data using a 2.5D artifact reduction prior trained entirely on simulated scans, highlighting the proposed method's ability to generalize across domains.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [87] [ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering](https://arxiv.org/abs/2506.13814)
*Lufei Liu,Tor M. Aamodt*

Main category: cs.GR

TL;DR: ReFrame通过缓存中间特征优化实时渲染任务，在保持质量的同时实现1.4倍加速。


<details>
  <summary>Details</summary>
Motivation: 利用时间连贯性避免冗余计算，提升渲染效率。

Method: 提出ReFrame，探索不同缓存策略以优化质量与性能的权衡。

Result: 在三个实时渲染任务中平均加速1.4倍，质量损失可忽略。

Conclusion: ReFrame适用于多种编码器-解码器网络，有效提升渲染性能。

Abstract: Graphics rendering applications increasingly leverage neural networks in tasks such as denoising, supersampling, and frame extrapolation to improve image quality while maintaining frame rates. The temporal coherence inherent in these tasks presents an opportunity to reuse intermediate results from previous frames and avoid redundant computations. Recent work has shown that caching intermediate features to be reused in subsequent inferences is an effective method to reduce latency in diffusion models. We extend this idea to real-time rendering and present ReFrame, which explores different caching policies to optimize trade-offs between quality and performance in rendering workloads. ReFrame can be applied to a variety of encoder-decoder style networks commonly found in rendering pipelines. Experimental results show that we achieve 1.4x speedup on average with negligible quality loss in three real-time rendering tasks. Code available: https://ubc-aamodt-group.github.io/reframe-layer-caching/

</details>


### [88] [Balancing Preservation and Modification: A Region and Semantic Aware Metric for Instruction-Based Image Editing](https://arxiv.org/abs/2506.13827)
*Zhuoying Li,Zhu Xu,Yuxin Peng,Yang Liu*

Main category: cs.GR

TL;DR: 提出了一种名为BPM的新指标，专门用于评估基于指令的图像编辑质量，通过分离编辑相关和不相关区域进行综合评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标要么成本高，要么无法全面评估编辑质量和内容保留，导致评估偏差。

Method: BPM通过定位编辑相关区域，采用两阶段评估：区域感知判断和语义感知判断。

Result: BPM在综合数据集上验证，与人类评估一致性最高。

Conclusion: BPM是一种高效且全面的评估指标，并可提升图像编辑方法的质量。

Abstract: Instruction-based image editing, which aims to modify the image faithfully according to the instruction while preserving irrelevant content unchanged, has made significant progress. However, there still lacks a comprehensive metric for assessing the editing quality. Existing metrics either require high human evaluation costs, which hinder large-scale evaluation, or are adapted from other tasks and lose task-specific concerns, failing to comprehensively evaluate both instruction-based modification and preservation of irrelevant regions, resulting in biased evaluation. To tackle this, we introduce a new metric called Balancing Preservation and Modification (BPM), tailored for instruction-based image editing by explicitly disentangling the image into editing-relevant and irrelevant regions for specific consideration. We first identify and locate editing-relevant regions, followed by a two-tier process to assess editing quality: Region-Aware Judge evaluates whether the position and size of the edited region align with the instruction, and Semantic-Aware Judge further assesses the instruction content compliance within editing-relevant regions as well as content preservation within irrelevant regions, yielding comprehensive and interpretable quality assessment. Moreover, the editing-relevant region localization in BPM can be integrated into image editing approaches to improve editing quality, demonstrating its broad applicability. We verify the effectiveness of the BPM metric on comprehensive instruction-editing data, and the results show the highest alignment with human evaluation compared to existing metrics, indicating its efficacy. Code is available at: https://joyli-x.github.io/BPM/

</details>


### [89] [Innovating China's Intangible Cultural Heritage with DeepSeek + MidJourney: The Case of Yangliuqing theme Woodblock Prints](https://arxiv.org/abs/2506.14104)
*RuiKun Yang,ZhongLiang Wei,Longdi Xian*

Main category: cs.GR

TL;DR: 本研究通过结合DeepSeek和MidJourney的方法，生成以COVID-19和欢乐胜利者为主题的杨柳青木版年画，并通过FID评分和问卷调查验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 保护和创新杨柳青木版年画这一非物质文化遗产，同时探索现代AI技术与传统艺术的结合。

Method: 采用DeepSeek生成主题提示，MidJourney生成主题图像，并结合原始杨柳青年画和DeepSeek生成的关键提示，通过FID评分和问卷调查评估效果。

Result: 该方法获得最低平均FID评分（150.2）和最小变异性（σ=4.9），问卷调查显示参与者对其结果最具代表性，且对推广传统文化和消费AI生成图像的兴趣最高。

Conclusion: 结合传统艺术元素与现代AI驱动创新的方法，既能保护文化遗产，又能保持当代相关性。

Abstract: Yangliuqing woodblock prints, a cornerstone of China's intangible cultural heritage, are celebrated for their intricate designs and vibrant colors. However, preserving these traditional art forms while fostering innovation presents significant challenges. This study explores the DeepSeek + MidJourney approach to generating creative, themed Yangliuqing woodblock prints focused on the fight against COVID-19 and depicting joyous winners. Using Fréchet Inception Distance (FID) scores for evaluation, the method that combined DeepSeek-generated thematic prompts, MidJourney-generated thematic images, original Yangliuqing prints, and DeepSeek-generated key prompts in MidJourney-generated outputs achieved the lowest mean FID score (150.2) with minimal variability (σ = 4.9). Additionally, feedback from 62 participants, collected via questionnaires, confirmed that this hybrid approach produced the most representative results. Moreover, the questionnaire data revealed that participants demonstrated the highest willingness to promote traditional culture and the strongest interest in consuming the AI-generated images produced through this method. These findings underscore the effectiveness of an innovative approach that seamlessly blends traditional artistic elements with modern AI-driven creativity, ensuring both cultural preservation and contemporary relevance.

</details>


### [90] [ImmerseGen: Agent-Guided Immersive World Generation with Alpha-Textured Proxies](https://arxiv.org/abs/2506.14315)
*Jinyan Yuan,Bangbang Yang,Keke Wang,Panwang Pan,Lin Ma,Xuehai Zhang,Xiao Liu,Zhaopeng Cui,Yuewen Ma*

Main category: cs.GR

TL;DR: ImmerseGen提出了一种基于代理引导的轻量级几何代理和RGBA纹理合成的框架，用于高效且逼真的3D场景生成，避免了复杂的几何建模。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖高多边形建模或大量3D高斯分布，导致流程复杂或视觉真实感不足。ImmerseGen旨在简化建模流程并提升沉浸感。

Method: 采用分层轻量级几何代理（如简化地形和广告牌网格）和RGBA纹理合成，结合地形条件纹理和VLM代理实现自动化场景生成。

Result: 实验表明，ImmerseGen在逼真度、空间一致性和渲染效率上优于现有方法。

Conclusion: ImmerseGen通过轻量级代理和纹理合成，实现了高效且逼真的3D场景生成，适合移动VR设备。

Abstract: Automatic creation of 3D scenes for immersive VR presence has been a significant research focus for decades. However, existing methods often rely on either high-poly mesh modeling with post-hoc simplification or massive 3D Gaussians, resulting in a complex pipeline or limited visual realism. In this paper, we demonstrate that such exhaustive modeling is unnecessary for achieving compelling immersive experience. We introduce ImmerseGen, a novel agent-guided framework for compact and photorealistic world modeling. ImmerseGen represents scenes as hierarchical compositions of lightweight geometric proxies, i.e., simplified terrain and billboard meshes, and generates photorealistic appearance by synthesizing RGBA textures onto these proxies. Specifically, we propose terrain-conditioned texturing for user-centric base world synthesis, and RGBA asset texturing for midground and foreground scenery.This reformulation offers several advantages: (i) it simplifies modeling by enabling agents to guide generative models in producing coherent textures that integrate seamlessly with the scene; (ii) it bypasses complex geometry creation and decimation by directly synthesizing photorealistic textures on proxies, preserving visual quality without degradation; (iii) it enables compact representations suitable for real-time rendering on mobile VR headsets. To automate scene creation from text prompts, we introduce VLM-based modeling agents enhanced with semantic grid-based analysis for improved spatial reasoning and accurate asset placement. ImmerseGen further enriches scenes with dynamic effects and ambient audio to support multisensory immersion. Experiments on scene generation and live VR showcases demonstrate that ImmerseGen achieves superior photorealism, spatial coherence and rendering efficiency compared to prior methods. Project webpage: https://immersegen.github.io.

</details>


### [91] [GHAR: GeoPose-based Handheld Augmented Reality for Architectural Positioning, Manipulation and Visual Exploration](https://arxiv.org/abs/2506.14414)
*Sabahat Israr,Dawar Khan,Zhanglin Cheng,Mukhtaj Khan,Kiyoshi Kiyokawa*

Main category: cs.GR

TL;DR: 本文提出了一种基于GeoPose的无标记手持增强现实（HAR）框架GHAR，用于建筑模型的可视化与操作，相比传统标记系统在可用性、操作性和可理解性上有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于标记的HAR系统存在使用和安装困难、对光线敏感以及标记设计等问题，限制了其在民用基础设施领域的应用。

Method: 提出了一种无标记HAR框架GHAR，利用GeoPose跟踪技术和手势操作实现7自由度（平移、旋转和缩放）的建筑模型增强现实。

Result: 通过用户研究（SUS和HARUS）评估，GHAR在可用性、操作性和可理解性上显著优于基于标记的HAR系统。

Conclusion: 无标记HAR框架GHAR为建筑行业的需求分析和规划提供了高效工具，具有实际应用潜力。

Abstract: Handheld Augmented Reality (HAR) is revolutionizing the civil infrastructure application domain. The current trend in HAR relies on marker tracking technology. However, marker-based systems have several limitations, such as difficulty in use and installation, sensitivity to light, and marker design. In this paper, we propose a markerless HAR framework with GeoPose-based tracking. We use different gestures for manipulation and achieve 7 DOF (3 DOF each for translation and rotation, and 1 DOF for scaling). The proposed framework, called GHAR, is implemented for architectural building models. It augments virtual CAD models of buildings on the ground, enabling users to manipulate and visualize an architectural model before actual construction. The system offers a quick view of the building infrastructure, playing a vital role in requirement analysis and planning in construction technology. We evaluated the usability, manipulability, and comprehensibility of the proposed system using a standard user study with the System Usability Scale (SUS) and Handheld Augmented Reality User Study (HARUS). We compared our GeoPose-based markerless HAR framework with a marker-based HAR framework, finding significant improvement in the aforementioned three parameters with the markerless framework.

</details>


### [92] [SkinCells: Sparse Skinning using Voronoi Cells](https://arxiv.org/abs/2506.14714)
*Egor Larionov,Igor Santesteban,Hsiao-yu Chen,Gene Lin,Philipp Herholz,Ryan Goldade,Ladislav Kavan,Doug Roble,Tuur Stuyck*

Main category: cs.GR

TL;DR: 提出了一种全自动、鲁棒的方法，用于生成高质量的蒙皮权重，支持稀疏控制和多细节层次（LoD）应用。


<details>
  <summary>Details</summary>
Motivation: 当前蒙皮权重生成工具多为手动或自动化程度不足，无法满足复杂几何体和高性能需求。

Method: 采用基于空间优化的SkinCells函数家族，直接控制顶点骨骼影响数量，并支持多细节层次应用。

Result: 方法在复杂几何体中表现优于双调和权重计算，且适用于大规模移动平台应用。

Conclusion: 该方法为高性能需求场景提供了一种高效、自动化的蒙皮权重生成解决方案。

Abstract: For decades, efficient real-time skinning methods have played a crucial role in animating character rigs for visual effects and games. These methods remain a fundamental component of modern applications. However, animatable digital asset creation predominantly remains a manual process. Current automated tools often fall short of delivering the desired level of quality for intricate and complex geometries, requiring manual touch-ups. We propose a fully automatic and robust method for generating high quality skinning weights given a user-provided skeleton and mesh in A- or T-pose. Notably, our approach provides direct sparsity controls, limiting the number of bone influences per vertex, which is essential for efficient asset creation for large-scale mobile experiences with multiple concurrent users. Our method additionally addresses the need for level-of-detail (LoD) variations in performance-sensitive applications, which are exacerbated on mobile platforms. By optimizing weights in space rather than on discrete points, we enable a single optimization result to be seamlessly applied to all levels of detail of that asset or even variations of that asset. To achieve this, we introduce a novel parameterized family of functions called SkinCells. We demonstrate how our automatic method is able to robustly compute skinning weights in cases where biharmonic weight computation fails.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [93] [Smooth surface reconstruction of earthquake faults from distributed potency beachballs](https://arxiv.org/abs/2506.14082)
*Dye SK Sato,Yuji Yagi,Ryo Okuwaki,Yukitoshi Fukahata*

Main category: physics.geo-ph

TL;DR: 本文提出了一种从给定的位错场（potency field）重建平滑三维断层表面的逆分析方法，解决了三维表面重建中的过定问题。


<details>
  <summary>Details</summary>
Motivation: 地震断层的几何信息通常通过位错场估计，但三维断层表面的重建是一个过定问题，缺乏解决方案。本文旨在解决这一问题。

Method: 基于断层表面法向量的定义，提出了一种解析表达式，通过位错密度张量反演重建三维断层表面。

Result: 通过合成噪声法向量场验证了方法的有效性，并成功应用于2013年Balochistan地震的断层几何估计。

Conclusion: 该方法比准二维形状重建更符合实际断层痕迹，验证了数值实验的预期。

Abstract: Earthquake faults approximate smooth surfaces of displacement discontinuity in a coarse-grained linear continuum, and the beachball nodal planes of source inelastic strain (potency) indicate the normal vector (n-vector) of the crack face. This theoretical relation has been used to interpret estimated potency as geometrical information of earthquake faults. We formulate an inverse analysis for reconstructing a smooth, three-dimensional fault surface from a given potency field. The analysis is grounded on the definition of the n-vector that is orthogonality to a surface. It is shown from this definition that, whereas the estimated n-vector field is always explained by a unique curve in two dimensions, the surface reconstruction from the estimated n-vector field is an overdetermined problem with no solution in three dimensions. We resolve this overdetermination by adopting the original definition of a fault surface as a map that approximates a thin sheet of potency onto a displacement discontinuity across a literal face. An analytical expression for this fault reconstruction is derived and confirmed to reproduce the original three-dimensional smooth surface from a synthesized noisy n-vector field. By incorporating this result into surface reconstruction based on potency density tensor inversion, we estimate the three-dimensional source fault geometry of the 2013 Balochistan earthquake. The estimated geometry is more consistent with the fault trace than the quasi-two-dimensional shape reconstruction as anticipated by our numerical experiments.

</details>


### [94] [Machine learning approaches for automatic cleaning of investigative drilling data](https://arxiv.org/abs/2506.14289)
*Fei Huang,Hongyu Qin,Masoud Manafi,Ben Juett,Ben Evans*

Main category: physics.geo-ph

TL;DR: 研究提出了一种基于机器学习的自动数据清洗方法，用于处理岩石钻孔数据中的异常值，IsoForest表现最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管ID技术减少了钻孔数据噪声，但数据清洗仍是准确分类地层和预测岩土性质的关键。

Method: 使用IsoForest、one-class SVM和DBSCAN三种机器学习算法，对比传统统计方法。

Result: 机器学习算法优于传统方法，IsoForest无需调参且效果最佳，结合K-means可同时去除土壤数据和异常值。

Conclusion: 自动数据清洗策略可减少人工干预，支持构建高质量数据集，促进岩土性质与钻孔数据关系的研究。

Abstract: Investigative drilling (ID) is an innovative measurement while drilling (MWD) technique that has been implemented in various site investigation projects across Australia. While the automated drilling feature of ID substantially reduces noise within drilling data streams, data cleaning remains essential for removing anomalies to enable accurate strata classification and prediction of soil and rock properties. This study employed three machine learning algorithms--IsoForest, one-class SVM, and DBSCAN--to automate the data cleaning process for ID data in rock drilling scenarios. Two data cleaning contexts were examined: (1) removing anomalies in rock drilling data, and (2) removing both anomalies and soil drilling data in mixed rock drilling data. The analysis revealed that all three machine learning algorithms outperformed traditional statistical methods (the 3-sigma rule and IQR method) in both data cleaning tasks, achieving a good balance between true positive rate and false positive rate, though hyperparameter tuning was required for one-class SVM and DBSCAN. Among them, IsoForest was proven to be the best-performing algorithm, capable of removing anomalies effectively without the need for hyperparameter adjustment. Furthermore, IsoForest, combined with two-cluster K-means, successfully eliminated both soil drilling data and anomalies while preserving almost all the normal data. The automatic data cleaning strategy proposed in this paper has the potential to reduce laborious manual data cleaning efforts and thereby facilitate the development of large-scale, high-quality datasets for machine learning studies capable of revealing complex relationships between drilling data and rock properties.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [95] [Busting the Paper Ballot: Voting Meets Adversarial Machine Learning](https://arxiv.org/abs/2506.14582)
*Kaleel Mahmood,Caleb Manicke,Ethan Rathbun,Aayushi Verma,Sohaib Ahmad,Nicholas Stamatakis,Laurent Michel,Benjamin Fuller*

Main category: cs.CR

TL;DR: 论文探讨了在美国选举计票机中使用机器学习分类器的安全风险，展示了对抗攻击的潜在影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机是揭示机器学习模型在选举计票中的安全漏洞，尤其是对抗攻击对选举结果的潜在影响。

Method: 方法包括引入四个新数据集、训练多种模型（如SVM、CNN、ViT）、分析梯度掩蔽问题并提出解决方案，以及在物理世界中进行对抗攻击实验。

Result: 结果显示传统白盒攻击因梯度掩蔽无效，但通过改进方法可实现物理攻击，即使5%的成功率也可能改变选举结果。

Conclusion: 结论指出选举计票中的机器学习模型存在安全风险，需重视对抗攻击的潜在威胁。

Abstract: We show the security risk associated with using machine learning classifiers in United States election tabulators. The central classification task in election tabulation is deciding whether a mark does or does not appear on a bubble associated to an alternative in a contest on the ballot. Barretto et al. (E-Vote-ID 2021) reported that convolutional neural networks are a viable option in this field, as they outperform simple feature-based classifiers.
  Our contributions to election security can be divided into four parts. To demonstrate and analyze the hypothetical vulnerability of machine learning models on election tabulators, we first introduce four new ballot datasets. Second, we train and test a variety of different models on our new datasets. These models include support vector machines, convolutional neural networks (a basic CNN, VGG and ResNet), and vision transformers (Twins and CaiT). Third, using our new datasets and trained models, we demonstrate that traditional white box attacks are ineffective in the voting domain due to gradient masking. Our analyses further reveal that gradient masking is a product of numerical instability. We use a modified difference of logits ratio loss to overcome this issue (Croce and Hein, ICML 2020). Fourth, in the physical world, we conduct attacks with the adversarial examples generated using our new methods. In traditional adversarial machine learning, a high (50% or greater) attack success rate is ideal. However, for certain elections, even a 5% attack success rate can flip the outcome of a race. We show such an impact is possible in the physical domain. We thoroughly discuss attack realism, and the challenges and practicality associated with printing and scanning ballot adversarial examples.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [96] [GAF: Gaussian Action Field as a Dvnamic World Model for Robotic Mlanipulation](https://arxiv.org/abs/2506.14135)
*Ying Chai,Litao Deng,Ruizhi Shao,Jiajun Zhang,Liangjun Xing,Hongwen Zhang,Yebin Liu*

Main category: cs.RO

TL;DR: 提出了一种V-4D-A框架，通过高斯动作场（GAF）直接从运动感知的4D表示中推理动作，显著提升了机器人操作任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（V-A或V-3D-A）因复杂动态场景导致动作推断不准确，需要更高效的框架。

Method: GAF扩展了3D高斯泼溅（3DGS），引入可学习运动属性，支持场景重建、未来帧预测和动作估计。

Result: 实验显示GAF在重建质量（PSNR +11.5385 dB，LPIPS -0.5574）和任务成功率（提升10.33%）上优于现有方法。

Conclusion: GAF通过4D表示和动作场显著提升了机器人操作的准确性和效率。

Abstract: Accurate action inference is critical for vision-based robotic manipulation. Existing approaches typically follow either a Vision-to-Action (V-A) paradigm, predicting actions directly from visual inputs, or a Vision-to-3D-to-Action (V-3D-A) paradigm, leveraging intermediate 3D representations. However, these methods often struggle with action inaccuracies due to the complexity and dynamic nature of manipulation scenes. In this paper, we propose a V-4D-A framework that enables direct action reasoning from motion-aware 4D representations via a Gaussian Action Field (GAF). GAF extends 3D Gaussian Splatting (3DGS) by incorporating learnable motion attributes, allowing simultaneous modeling of dynamic scenes and manipulation actions. To learn time-varying scene geometry and action-aware robot motion, GAF supports three key query types: reconstruction of the current scene, prediction of future frames, and estimation of initial action via robot motion. Furthermore, the high-quality current and future frames generated by GAF facilitate manipulation action refinement through a GAF-guided diffusion model. Extensive experiments demonstrate significant improvements, with GAF achieving +11.5385 dB PSNR and -0.5574 LPIPS improvements in reconstruction quality, while boosting the average success rate in robotic manipulation tasks by 10.33% over state-of-the-art methods. Project page: http://chaiying1.github.io/GAF.github.io/project_page/

</details>


### [97] [AMPLIFY: Actionless Motion Priors for Robot Learning from Videos](https://arxiv.org/abs/2506.14198)
*Jeremy A. Collins,Loránd Cheng,Kunal Aneja,Albert Wilcox,Benjamin Joffe,Animesh Garg*

Main category: cs.RO

TL;DR: AMPLIFY框架利用大规模无动作视频数据，通过提取关键点轨迹生成紧凑的运动标记，解决了机器人策略学习中数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 机器人动作标注数据稀缺且昂贵，限制了学习策略的泛化能力，而无动作视频数据丰富但难以转化为有效策略。

Method: AMPLIFY通过分离视觉运动预测和动作推断，训练前向动力学模型（无动作视频）和逆向动力学模型（少量动作标注数据），实现独立扩展。

Result: AMPLIFY在动态预测准确性上显著优于现有方法（MSE提升3.7倍，像素预测精度提升2.5倍），并在下游策略学习中表现优异（低数据场景提升1.2-2.2倍）。

Conclusion: AMPLIFY展示了利用异构数据源构建高效、可泛化世界模型的新范式，适用于机器人控制及其他领域。

Abstract: Action-labeled data for robotics is scarce and expensive, limiting the generalization of learned policies. In contrast, vast amounts of action-free video data are readily available, but translating these observations into effective policies remains a challenge. We introduce AMPLIFY, a novel framework that leverages large-scale video data by encoding visual dynamics into compact, discrete motion tokens derived from keypoint trajectories. Our modular approach separates visual motion prediction from action inference, decoupling the challenges of learning what motion defines a task from how robots can perform it. We train a forward dynamics model on abundant action-free videos and an inverse dynamics model on a limited set of action-labeled examples, allowing for independent scaling. Extensive evaluations demonstrate that the learned dynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x better pixel prediction accuracy compared to prior approaches, and broadly useful. In downstream policy learning, our dynamics predictions enable a 1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by learning from action-free human videos, and the first generalization to LIBERO tasks from zero in-distribution action data. Beyond robotic control, we find the dynamics learned by AMPLIFY to be a versatile latent world model, enhancing video prediction quality. Our results present a novel paradigm leveraging heterogeneous data sources to build efficient, generalizable world models. More information can be found at https://amplify-robotics.github.io/.

</details>


### [98] [GAMORA: A Gesture Articulated Meta Operative Robotic Arm for Hazardous Material Handling in Containment-Level Environments](https://arxiv.org/abs/2506.14513)
*Farha Abdul Wasay,Mohammed Abdul Rahman,Hania Ghouse*

Main category: cs.RO

TL;DR: GAMORA是一种基于VR的机器人系统，通过手势控制实现高风险实验室任务的远程操作，提高了精度和安全性。


<details>
  <summary>Details</summary>
Motivation: 随着生物危害复杂性的增加，减少直接人类接触同时保持操作精度变得至关重要。

Method: 系统结合Oculus Quest 2、NVIDIA Jetson Nano和ROS，提供实时沉浸式控制、数字孪生模拟和逆运动学操作。

Result: GAMORA实现了2.2毫米的位置误差、0.2毫升的移液精度和1.2毫米的重复性，同时能耗降低50%。

Conclusion: GAMORA为生物医学研究提供了一种可扩展、沉浸式的机器人控制解决方案，提升了生物安全性。

Abstract: The convergence of robotics and virtual reality (VR) has enabled safer and more efficient workflows in high-risk laboratory settings, particularly virology labs. As biohazard complexity increases, minimizing direct human exposure while maintaining precision becomes essential. We propose GAMORA (Gesture Articulated Meta Operative Robotic Arm), a novel VR-guided robotic system that enables remote execution of hazardous tasks using natural hand gestures. Unlike existing scripted automation or traditional teleoperation, GAMORA integrates the Oculus Quest 2, NVIDIA Jetson Nano, and Robot Operating System (ROS) to provide real-time immersive control, digital twin simulation, and inverse kinematics-based articulation. The system supports VR-based training and simulation while executing precision tasks in physical environments via a 3D-printed robotic arm. Inverse kinematics ensure accurate manipulation for delicate operations such as specimen handling and pipetting. The pipeline includes Unity-based 3D environment construction, real-time motion planning, and hardware-in-the-loop testing. GAMORA achieved a mean positional discrepancy of 2.2 mm (improved from 4 mm), pipetting accuracy within 0.2 mL, and repeatability of 1.2 mm across 50 trials. Integrated object detection via YOLOv8 enhances spatial awareness, while energy-efficient operation (50% reduced power output) ensures sustainable deployment. The system's digital-physical feedback loop enables safe, precise, and repeatable automation of high-risk lab tasks. GAMORA offers a scalable, immersive solution for robotic control and biosafety in biomedical research environments.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [99] [VL-GenRM: Enhancing Vision-Language Verification via Vision Experts and Iterative Training](https://arxiv.org/abs/2506.13888)
*Jipeng Zhang,Kehao Miao,Renjie Pi,Zhaowei Wang,Runtao Liu,Rui Pan,Tong Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种迭代训练框架，通过结合视觉专家、思维链推理和基于边际的拒绝采样，解决了视觉语言奖励模型（VL-RM）训练中的自举困境和模态偏差问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VL）的强化微调（RFT）缺乏探索，且VL-RM训练面临自举困境和模态偏差两大挑战。

Method: 采用迭代训练框架，结合视觉专家、思维链推理和基于边际的拒绝采样，优化偏好数据集和结构化反馈。

Result: 实验表明，该方法在幻觉检测和多模态推理方面表现优异，显著提升了VL模型的对齐性能。

Conclusion: 提出的框架有效解决了VL-RM训练中的核心问题，为视觉语言模型的强化学习对齐提供了新思路。

Abstract: Reinforcement Fine-Tuning (RFT) with verifiable rewards has advanced large language models but remains underexplored for Vision-Language (VL) models. The Vision-Language Reward Model (VL-RM) is key to aligning VL models by providing structured feedback, yet training effective VL-RMs faces two major challenges. First, the bootstrapping dilemma arises as high-quality training data depends on already strong VL models, creating a cycle where self-generated supervision reinforces existing biases. Second, modality bias and negative example amplification occur when VL models hallucinate incorrect visual attributes, leading to flawed preference data that further misguides training. To address these issues, we propose an iterative training framework leveraging vision experts, Chain-of-Thought (CoT) rationales, and Margin-based Rejection Sampling. Our approach refines preference datasets, enhances structured critiques, and iteratively improves reasoning. Experiments across VL-RM benchmarks demonstrate superior performance in hallucination detection and multimodal reasoning, advancing VL model alignment with reinforcement learning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [100] [Enclosing Prototypical Variational Autoencoder for Explainable Out-of-Distribution Detection](https://arxiv.org/abs/2506.14390)
*Conrad Orglmeister,Erik Bochinski,Volker Eiselein,Elvira Fleig*

Main category: cs.LG

TL;DR: 论文提出了一种结合原型变分模型和自编码器的OOD检测方法，通过定义紧凑的ID区域并引入限制损失，提高了分类和OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 为了增强深度机器学习模型的可信度和决策可解释性，尤其是在安全相关应用中，需要改进模型的OOD检测能力。

Method: 使用变分自编码器学习潜在空间，结合高斯混合分布定义ID区域，并引入限制损失以保持ID区域的紧凑性。

Result: 在常见OOD检测基准和真实铁路数据集上表现优异，优于现有方法。

Conclusion: 该方法通过结合原型解释性和自编码器的重构能力，显著提升了OOD检测的可靠性和可解释性。

Abstract: Understanding the decision-making and trusting the reliability of Deep Machine Learning Models is crucial for adopting such methods to safety-relevant applications. We extend self-explainable Prototypical Variational models with autoencoder-based out-of-distribution (OOD) detection: A Variational Autoencoder is applied to learn a meaningful latent space which can be used for distance-based classification, likelihood estimation for OOD detection, and reconstruction. The In-Distribution (ID) region is defined by a Gaussian mixture distribution with learned prototypes representing the center of each mode. Furthermore, a novel restriction loss is introduced that promotes a compact ID region in the latent space without collapsing it into single points. The reconstructive capabilities of the Autoencoder ensure the explainability of the prototypes and the ID region of the classifier, further aiding the discrimination of OOD samples. Extensive evaluations on common OOD detection benchmarks as well as a large-scale dataset from a real-world railway application demonstrate the usefulness of the approach, outperforming previous methods.

</details>


### [101] [Train Once, Forget Precisely: Anchored Optimization for Efficient Post-Hoc Unlearning](https://arxiv.org/abs/2506.14515)
*Prabhav Sanga,Jaskaran Singh,Arun K. Dubey*

Main category: cs.LG

TL;DR: FAMR是一种高效的后处理遗忘框架，用于深度图像分类器，通过约束优化问题实现选择性遗忘，同时保留模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统越来越多地依赖受隐私法规约束的数据，选择性遗忘特定信息变得至关重要。

Method: FAMR将遗忘问题建模为约束优化问题，最小化遗忘集的均匀预测损失，并通过ℓ2惩罚锚定模型参数。

Result: 在CIFAR-10和ImageNet-100上的实验表明，FAMR在保留性能的同时实现了高效遗忘。

Conclusion: FAMR为视觉模型提供了一种可扩展且可验证的高效后处理遗忘方法。

Abstract: As machine learning systems increasingly rely on data subject to privacy regulation, selectively unlearning specific information from trained models has become essential. In image classification, this involves removing the influence of particular training samples, semantic classes, or visual styles without full retraining. We introduce \textbf{Forget-Aligned Model Reconstruction (FAMR)}, a theoretically grounded and computationally efficient framework for post-hoc unlearning in deep image classifiers. FAMR frames forgetting as a constrained optimization problem that minimizes a uniform-prediction loss on the forget set while anchoring model parameters to their original values via an $\ell_2$ penalty. A theoretical analysis links FAMR's solution to influence-function-based retraining approximations, with bounds on parameter and output deviation. Empirical results on class forgetting tasks using CIFAR-10 and ImageNet-100 demonstrate FAMR's effectiveness, with strong performance retention and minimal computational overhead. The framework generalizes naturally to concept and style erasure, offering a scalable and certifiable route to efficient post-hoc forgetting in vision models.

</details>


### [102] [Towards Desiderata-Driven Design of Visual Counterfactual Explainers](https://arxiv.org/abs/2506.14698)
*Sidney Bender,Jan Herrmann,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: 本文提出了一种新的视觉反事实解释方法（SCE），弥补了现有方法在解释全面性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视觉反事实解释器（VCEs）过于关注样本质量或最小变化，忽略了解释的保真度、可理解性和充分性等更全面的需求。

Method: 探索新的反事实生成机制，并整合为一种新颖的“平滑反事实探索器”（SCE）算法。

Result: 通过合成和真实数据的系统评估，证明了SCE算法的有效性。

Conclusion: SCE算法能够更好地满足解释的全面需求，提升图像分类器的透明度。

Abstract: Visual counterfactual explainers (VCEs) are a straightforward and promising approach to enhancing the transparency of image classifiers. VCEs complement other types of explanations, such as feature attribution, by revealing the specific data transformations to which a machine learning model responds most strongly. In this paper, we argue that existing VCEs focus too narrowly on optimizing sample quality or change minimality; they fail to consider the more holistic desiderata for an explanation, such as fidelity, understandability, and sufficiency. To address this shortcoming, we explore new mechanisms for counterfactual generation and investigate how they can help fulfill these desiderata. We combine these mechanisms into a novel 'smooth counterfactual explorer' (SCE) algorithm and demonstrate its effectiveness through systematic evaluations on synthetic and real data.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [103] [Déjà Vu: Efficient Video-Language Query Engine with Learning-based Inter-Frame Computation Reuse](https://arxiv.org/abs/2506.14107)
*Jinwoo Hwang,Daeun Kim,Sangyeop Lee,Yoonsung Kim,Guseul Heo,Hojoon Kim,Yunseok Jeong,Tadiwos Meaza,Eunhyeok Park,Jeongseob Ahn,Jongse Park*

Main category: cs.DC

TL;DR: Déjà Vu是一种视频语言查询引擎，通过重用连续帧的计算加速ViT-based VideoLMs，显著提升大规模视频分析的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有VideoLMs在处理大规模视频时因需逐帧计算ViT嵌入而面临计算瓶颈，亟需高效解决方案。

Method: 提出ReuseViT模型，检测帧间重用机会，结合内存-计算联合压缩技术，将计算节省转化为实际性能提升。

Result: 在三个VideoLM任务中，Déjà Vu将嵌入生成速度提升至2.64倍，误差控制在2%以内。

Conclusion: Déjà Vu通过计算重用和优化技术，显著提升了VideoLMs在大规模视频分析中的实用性。

Abstract: Recently, Video-Language Models (VideoLMs) have demonstrated remarkable capabilities, offering significant potential for flexible and powerful video query systems. These models typically rely on Vision Transformers (ViTs), which process video frames individually to extract visual embeddings. However, generating embeddings for large-scale videos requires ViT inferencing across numerous frames, posing a major hurdle to real-world deployment and necessitating solutions for integration into scalable video data management systems. This paper introduces Déjà Vu, a video-language query engine that accelerates ViT-based VideoLMs by reusing computations across consecutive frames. At its core is ReuseViT, a modified ViT model specifically designed for VideoLM tasks, which learns to detect inter-frame reuse opportunities, striking an effective balance between accuracy and reuse. Although ReuseViT significantly reduces computation, these savings do not directly translate into performance gains on GPUs. To overcome this, Déjà Vu integrates memory-compute joint compaction techniques that convert the FLOP savings into tangible performance gains. Evaluations on three VideoLM tasks show that Déjà Vu accelerates embedding generation by up to a 2.64x within a 2% error bound, dramatically enhancing the practicality of VideoLMs for large-scale video analytics.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [104] [MobileHolo: A Lightweight Complex-Valued Deformable CNN for High-Quality Computer-Generated Hologram](https://arxiv.org/abs/2506.14542)
*Xie Shuyang,Zhou Jie,Xu Bo,Wang Jun,Xu Renjing*

Main category: physics.optics

TL;DR: 提出了一种基于复杂值可变形卷积的深度学习方法，用于计算机生成全息图（CGH），通过动态调整卷积核形状提升有效感受野（ERF），在模拟和光学实验中均取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 全息显示在虚拟现实和增强现实中潜力巨大，但现有方法因有效感受野不足难以准确建模衍射过程。

Method: 设计复杂值可变形卷积网络，动态调整卷积核形状以增强特征提取能力。

Result: 在1920×1072分辨率下，PSNR分别比CCNN-CGH、HoloNet和Holo-encoder高2.04 dB、5.31 dB和9.71 dB，模型参数量仅为CCNN-CGH的八分之一。

Conclusion: 该方法通过灵活调整ERF，以更少参数实现更优性能，为全息显示提供了高效解决方案。

Abstract: Holographic displays have significant potential in virtual reality and augmented reality owing to their ability to provide all the depth cues. Deep learning-based methods play an important role in computer-generated holograms (CGH). During the diffraction process, each pixel exerts an influence on the reconstructed image. However, previous works face challenges in capturing sufficient information to accurately model this process, primarily due to the inadequacy of their effective receptive field (ERF). Here, we designed complex-valued deformable convolution for integration into network, enabling dynamic adjustment of the convolution kernel's shape to increase flexibility of ERF for better feature extraction. This approach allows us to utilize a single model while achieving state-of-the-art performance in both simulated and optical experiment reconstructions, surpassing existing open-source models. Specifically, our method has a peak signal-to-noise ratio that is 2.04 dB, 5.31 dB, and 9.71 dB higher than that of CCNN-CGH, HoloNet, and Holo-encoder, respectively, when the resolution is 1920$\times$1072. The number of parameters of our model is only about one-eighth of that of CCNN-CGH.

</details>
