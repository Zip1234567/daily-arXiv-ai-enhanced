<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 98]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.GR](#cs.GR) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 6]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Boosting Adversarial Transferability via Residual Perturbation Attack](https://arxiv.org/abs/2508.05689)
*Jinjia Peng,Zeze Tao,Huibing Wang,Meng Wang,Yang Wang*

Main category: cs.CV

TL;DR: 提出了一种名为ResPA的新型攻击方法，通过利用残差梯度作为扰动方向，提升对抗样本的迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有迁移攻击方法忽略了扰动方向的影响，导致迁移性有限。

Method: ResPA通过指数移动平均获取参考梯度，并利用当前梯度与参考梯度的残差捕捉全局扰动方向变化。

Result: 实验表明ResPA在迁移性上优于现有方法，且与输入变换方法结合可进一步提升效果。

Conclusion: ResPA通过优化扰动方向，显著提高了对抗样本的迁移性。

Abstract: Deep neural networks are susceptible to adversarial examples while suffering
from incorrect predictions via imperceptible perturbations. Transfer-based
attacks create adversarial examples for surrogate models and transfer these
examples to target models under black-box scenarios. Recent studies reveal that
adversarial examples in flat loss landscapes exhibit superior transferability
to alleviate overfitting on surrogate models. However, the prior arts overlook
the influence of perturbation directions, resulting in limited transferability.
In this paper, we propose a novel attack method, named Residual Perturbation
Attack (ResPA), relying on the residual gradient as the perturbation direction
to guide the adversarial examples toward the flat regions of the loss function.
Specifically, ResPA conducts an exponential moving average on the input
gradients to obtain the first moment as the reference gradient, which
encompasses the direction of historical gradients. Instead of heavily relying
on the local flatness that stems from the current gradients as the perturbation
direction, ResPA further considers the residual between the current gradient
and the reference gradient to capture the changes in the global perturbation
direction. The experimental results demonstrate the better transferability of
ResPA than the existing typical transfer-based attack methods, while the
transferability can be further improved by combining ResPA with the current
input transformation methods. The code is available at
https://github.com/ZezeTao/ResPA.

</details>


### [2] [HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing](https://arxiv.org/abs/2508.05899)
*Zixuan Bian,Ruohan Ren,Yue Yang,Chris Callison-Burch*

Main category: cs.CV

TL;DR: HOLODECK 2.0是一个基于视觉语言模型的3D场景生成框架，支持通过文本生成多样化、高语义保真度的3D场景，并支持交互式编辑。


<details>
  <summary>Details</summary>
Motivation: 当前3D场景生成依赖大量人工，且现有自动化方法难以生成开放域场景或支持灵活编辑，因此需要直接从文本生成3D世界的解决方案。

Method: HOLODECK 2.0利用视觉语言模型解析场景需求，通过先进3D生成模型生成高质量资产，并迭代应用空间约束实现语义和物理合理的布局。

Result: 实验表明，HOLODECK 2.0生成的场景在语义保真度和质量上优于基线方法，并支持灵活的编辑功能。

Conclusion: HOLODECK 2.0在游戏建模等应用中展示了高效生成沉浸式环境的潜力。

Abstract: 3D scene generation plays a crucial role in gaming, artistic creation,
virtual reality and many other domains. However, current 3D scene design still
relies heavily on extensive manual effort from creators, and existing automated
methods struggle to generate open-domain scenes or support flexible editing. As
a result, generating 3D worlds directly from text has garnered increasing
attention. In this paper, we introduce HOLODECK 2.0, an advanced
vision-language-guided framework for 3D world generation with support for
interactive scene editing based on human feedback. HOLODECK 2.0 can generate
diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and
cyberpunk styles) that exhibit high semantic fidelity to fine-grained input
descriptions, suitable for both indoor and open-domain environments. HOLODECK
2.0 leverages vision-language models (VLMs) to identify and parse the objects
required in a scene and generates corresponding high-quality assets via
state-of-the-art 3D generative models. It then iteratively applies spatial
constraints derived from the VLMs to achieve semantically coherent and
physically plausible layouts. Human evaluations and CLIP-based assessments
demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely
aligned with detailed textual descriptions, consistently outperforming
baselines across indoor and open-domain scenarios. Additionally, we provide
editing capabilities that flexibly adapt to human feedback, supporting layout
refinement and style-consistent object edits. Finally, we present a practical
application of HOLODECK 2.0 in procedural game modeling, generating visually
rich and immersive environments, potentially boosting efficiency.

</details>


### [3] [Generalized Few-Shot Out-of-Distribution Detection](https://arxiv.org/abs/2508.05732)
*Pinxuan Li,Bing Cao,Changqing Zhang,Qinghua Hu*

Main category: cs.CV

TL;DR: 提出了一种广义少样本OOD检测框架（GOOD），通过引入通用知识模型（GKM）和知识动态嵌入（KDE）机制，提升模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有少样本OOD检测方法在开放世界中的泛化能力不足，容易过拟合于有限的训练数据。

Method: 提出GOOD框架，利用GKM提供通用知识，并通过KDE机制动态调整模型输出分布以平衡通用性与特异性。

Result: 在真实OOD基准测试中表现优越，理论分析表明GS平衡降低了泛化误差上界。

Conclusion: GOOD框架通过通用知识模型和动态嵌入机制，显著提升了少样本OOD检测的泛化性能。

Abstract: Few-shot Out-of-Distribution (OOD) detection has emerged as a critical
research direction in machine learning for practical deployment. Most existing
Few-shot OOD detection methods suffer from insufficient generalization
capability for the open world. Due to the few-shot learning paradigm, the OOD
detection ability is often overfit to the limited training data itself, thus
degrading the performance on generalized data and performing inconsistently
across different scenarios. To address this challenge, we proposed a
Generalized Few-shot OOD Detection (GOOD) framework, which empowers the general
knowledge of the OOD detection model with an auxiliary General Knowledge Model
(GKM), instead of directly learning from few-shot data. We proceed to reveal
the few-shot OOD detection from a generalization perspective and theoretically
derive the Generality-Specificity balance (GS-balance) for OOD detection, which
provably reduces the upper bound of generalization error with a general
knowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE)
mechanism to adaptively modulate the guidance of general knowledge. KDE
dynamically aligns the output distributions of the OOD detection model to the
general knowledge model based on the Generalized Belief (G-Belief) of GKM,
thereby boosting the GS-balance. Experiments on real-world OOD benchmarks
demonstrate our superiority. Codes will be available.

</details>


### [4] [LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing](https://arxiv.org/abs/2508.06055)
*Wonjung Park,Suhyun Ahn,Jinah Park*

Main category: cs.CV

TL;DR: LV-Net是一种新型框架，通过变形解剖学感知的联合LV-海马模板网格，从脑MRI生成个性化的3D LV网格，提高了重建精度和形状统计准确性。


<details>
  <summary>Details</summary>
Motivation: 侧脑室（LV）形状分析作为神经疾病的生物标志物具有潜力，但由于个体间形状差异大和MRI分辨率限制导致的分割困难，仍面临挑战。

Method: LV-Net通过结合解剖学关系的联合模板减少分割伪影，并通过模板网格顶点分类增强点对应性，从而提高形状统计准确性。

Result: LV-Net在分割不完美的情况下仍实现高重建精度，并在阿尔茨海默病分析中识别出与疾病显著相关的LV子区域。

Conclusion: LV-Net为LV形状分析提供了更可靠的工具，尤其在神经疾病研究中具有应用潜力。

Abstract: Lateral ventricle (LV) shape analysis holds promise as a biomarker for
neurological diseases; however, challenges remain due to substantial shape
variability across individuals and segmentation difficulties arising from
limited MRI resolution. We introduce LV-Net, a novel framework for producing
individualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint
LV-hippocampus template mesh. By incorporating anatomical relationships
embedded within the joint template, LV-Net reduces boundary segmentation
artifacts and improves reconstruction robustness. In addition, by classifying
the vertices of the template mesh based on their anatomical adjacency, our
method enhances point correspondence across subjects, leading to more accurate
LV shape statistics. We demonstrate that LV-Net achieves superior
reconstruction accuracy, even in the presence of segmentation imperfections,
and delivers more reliable shape descriptors across diverse datasets. Finally,
we apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that
show significantly associations with the disease relative to cognitively normal
controls. The codes for LV shape modeling are available at
https://github.com/PWonjung/LV_Shape_Modeling.

</details>


### [5] [UnGuide: Learning to Forget with LoRA-Guided Diffusion Models](https://arxiv.org/abs/2508.05755)
*Agnieszka Polowczyk,Alicja Polowczyk,Dawid Malarz,Artur Kasymov,Marcin Mazur,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

TL;DR: UnGuide是一种新方法，通过动态推理机制UnGuidance，结合LoRA适配器实现精确的模型遗忘，同时保持生成内容的质量。


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型的滥用风险增加，需要在不影响整体性能的情况下移除特定知识。

Method: 采用UnGuidance机制，动态调整引导尺度，结合LoRA适配器选择性遗忘。

Result: UnGuide在概念移除任务中表现优于现有方法，同时保留了模型的表达能力。

Conclusion: UnGuide为模型遗忘提供了一种高效且可控的解决方案。

Abstract: Recent advances in large-scale text-to-image diffusion models have heightened
concerns about their potential misuse, especially in generating harmful or
misleading content. This underscores the urgent need for effective machine
unlearning, i.e., removing specific knowledge or concepts from pretrained
models without compromising overall performance. One possible approach is
Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models
for targeted unlearning. However, LoRA often inadvertently alters unrelated
content, leading to diminished image fidelity and realism. To address this
limitation, we introduce UnGuide -- a novel approach which incorporates
UnGuidance, a dynamic inference mechanism that leverages Classifier-Free
Guidance (CFG) to exert precise control over the unlearning process. UnGuide
modulates the guidance scale based on the stability of a few first steps of
denoising processes, enabling selective unlearning by LoRA adapter. For prompts
containing the erased concept, the LoRA module predominates and is
counterbalanced by the base model; for unrelated prompts, the base model
governs generation, preserving content fidelity. Empirical results demonstrate
that UnGuide achieves controlled concept removal and retains the expressive
power of diffusion models, outperforming existing LoRA-based methods in both
object erasure and explicit content removal tasks.

</details>


### [6] [Improving Masked Style Transfer using Blended Partial Convolution](https://arxiv.org/abs/2508.05769)
*Seyed Hadi Seyed,Ayberk Cansever,David Hart*

Main category: cs.CV

TL;DR: 提出了一种基于部分卷积的风格迁移网络，精准应用于感兴趣区域，并通过内部混合技术优化效果。


<details>
  <summary>Details</summary>
Motivation: 传统方法对整个图像进行风格迁移，而用户可能仅需对特定区域应用风格，现有方法在区域选择上存在不足。

Method: 采用部分卷积网络，结合内部混合技术，以解决区域选择不完美的问题。

Result: 在SA-1B数据集上验证，视觉和定量指标均显示改进。

Conclusion: 该方法在区域风格迁移中表现更优，代码已开源。

Abstract: Artistic style transfer has long been possible with the advancements of
convolution- and transformer-based neural networks. Most algorithms apply the
artistic style transfer to the whole image, but individual users may only need
to apply a style transfer to a specific region in the image. The standard
practice is to simply mask the image after the stylization. This work shows
that this approach tends to improperly capture the style features in the region
of interest. We propose a partial-convolution-based style transfer network that
accurately applies the style features exclusively to the region of interest.
Additionally, we present network-internal blending techniques that account for
imperfections in the region selection. We show that this visually and
quantitatively improves stylization using examples from the SA-1B dataset. Code
is publicly available at https://github.com/davidmhart/StyleTransferMasked.

</details>


### [7] [A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery](https://arxiv.org/abs/2508.06407)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: 论文探讨了将分类目标直接融入超分辨率过程是否能提升分类准确性，并提出了一种优化图像质量和分类性能的新方法。


<details>
  <summary>Details</summary>
Motivation: 低分辨率图像限制了自动化分析的准确性，传统超分辨率方法仅关注像素级指标，未充分探索超分辨率图像保真度与下游分类性能的关系。

Method: 提出了一种新颖的方法，通过优化同时考虑图像质量和分类性能的损失函数，提高合成孔径雷达图像的分辨率。

Result: 该方法在科学验证的图像质量指标上提升了图像质量，同时提高了分类准确性。

Conclusion: 研究表明，将分类目标融入超分辨率过程可以同时改善图像质量和分类性能。

Abstract: High-resolution imagery plays a critical role in improving the performance of
visual recognition tasks such as classification, detection, and segmentation.
In many domains, including remote sensing and surveillance, low-resolution
images can limit the accuracy of automated analysis. To address this,
super-resolution (SR) techniques have been widely adopted to attempt to
reconstruct high-resolution images from low-resolution inputs. Related
traditional approaches focus solely on enhancing image quality based on
pixel-level metrics, leaving the relationship between super-resolved image
fidelity and downstream classification performance largely underexplored. This
raises a key question: can integrating classification objectives directly into
the super-resolution process further improve classification accuracy? In this
paper, we try to respond to this question by investigating the relationship
between super-resolution and classification through the deployment of a
specialised algorithmic strategy. We propose a novel methodology that increases
the resolution of synthetic aperture radar imagery by optimising loss functions
that account for both image quality and classification performance. Our
approach improves image quality, as measured by scientifically ascertained
image quality indicators, while also enhancing classification accuracy.

</details>


### [8] [MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss](https://arxiv.org/abs/2508.05772)
*Can Zhao,Pengfei Guo,Dong Yang,Yucheng Tang,Yufan He,Benjamin Simon,Mason Belue,Stephanie Harmon,Baris Turkbey,Daguang Xu*

Main category: cs.CV

TL;DR: MAISI-v2是一个加速的3D医学图像合成框架，通过整合修正流实现快速高质量生成，并引入区域特异性对比损失提升条件一致性。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在医学图像合成中的通用性差、推理速度慢和输入条件对齐弱的问题。

Method: 整合修正流加速生成，引入区域特异性对比损失增强条件一致性。

Result: MAISI-v2实现了33倍加速的潜在扩散模型，并达到SOTA图像质量。

Conclusion: MAISI-v2在医学图像合成中表现出色，可用于数据增强，并开源了代码和模型以促进社区发展。

Abstract: Medical image synthesis is an important topic for both clinical and research
applications. Recently, diffusion models have become a leading approach in this
area. Despite their strengths, many existing methods struggle with (1) limited
generalizability that only work for specific body regions or voxel spacings,
(2) slow inference, which is a common issue for diffusion models, and (3) weak
alignment with input conditions, which is a critical issue for medical imaging.
MAISI, a previously proposed framework, addresses generalizability issues but
still suffers from slow inference and limited condition consistency. In this
work, we present MAISI-v2, the first accelerated 3D medical image synthesis
framework that integrates rectified flow to enable fast and high quality
generation. To further enhance condition fidelity, we introduce a novel
region-specific contrastive loss to enhance the sensitivity to region of
interest. Our experiments show that MAISI-v2 can achieve SOTA image quality
with $33 \times$ acceleration for latent diffusion model. We also conducted a
downstream segmentation experiment to show that the synthetic images can be
used for data augmentation. We release our code, training details, model
weights, and a GUI demo to facilitate reproducibility and promote further
development within the community.

</details>


### [9] [Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks](https://arxiv.org/abs/2508.05783)
*Mengyu Li,Guoyao Shen,Chad W. Farris,Xin Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于预训练MRI变换器的少样本部署框架，结合MAE预训练策略和混合架构，在数据有限条件下实现高效、稳定的脑成像任务。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像中标注数据稀缺的问题，提升变换器模型在实际应用中的适用性。

Method: 采用MAE预训练策略在大规模多队列脑MRI数据集上学习可迁移的潜在表示，结合轻量级线性头或混合架构（MAE-FUnet）完成分类和分割任务。

Result: 在MRI序列识别中达到最先进精度，在分割任务中优于其他基线模型。

Conclusion: 该框架高效、稳定且可扩展，适用于低资源临床环境和广泛的神经影像应用。

Abstract: Machine learning using transformers has shown great potential in medical
imaging, but its real-world applicability remains limited due to the scarcity
of annotated data. In this study, we propose a practical framework for the
few-shot deployment of pretrained MRI transformers in diverse brain imaging
tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a
large-scale, multi-cohort brain MRI dataset comprising over 31 million slices,
we obtain highly transferable latent representations that generalize well
across tasks and datasets. For high-level tasks such as classification, a
frozen MAE encoder combined with a lightweight linear head achieves
state-of-the-art accuracy in MRI sequence identification with minimal
supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a
hybrid architecture that fuses multiscale CNN features with pretrained MAE
embeddings. This model consistently outperforms other strong baselines in both
skull stripping and multi-class anatomical segmentation under data-limited
conditions. With extensive quantitative and qualitative evaluations, our
framework demonstrates efficiency, stability, and scalability, suggesting its
suitability for low-resource clinical environments and broader neuroimaging
applications.

</details>


### [10] [Optimization-Free Style Transfer for 3D Gaussian Splats](https://arxiv.org/abs/2508.05813)
*Raphael Du Sablon,David Hart*

Main category: cs.CV

TL;DR: 提出了一种无需重建或优化的3D高斯溅射风格迁移方法，通过生成图结构并基于表面进行风格化，实现快速风格迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要重建或优化高斯溅射表示，限制了效率和灵活性。

Method: 生成高斯溅射隐式表面的图结构，使用前馈表面风格化方法，并将结果插值回溅射。

Result: 实现了快速风格迁移（2分钟内），且无需额外训练或优化。

Conclusion: 该方法高效灵活，适用于任意风格图像和高斯溅射。

Abstract: The task of style transfer for 3D Gaussian splats has been explored in many
previous works, but these require reconstructing or fine-tuning the splat while
incorporating style information or optimizing a feature extraction network on
the splat representation. We propose a reconstruction- and optimization-free
approach to stylizing 3D Gaussian splats. This is done by generating a graph
structure across the implicit surface of the splat representation. A
feed-forward, surface-based stylization method is then used and interpolated
back to the individual splats in the scene. This allows for any style image and
3D Gaussian splat to be used without any additional training or optimization.
This also allows for fast stylization of splats, achieving speeds under 2
minutes even on consumer-grade hardware. We demonstrate the quality results
this approach achieves and compare to other 3D Gaussian splat style transfer
methods. Code is publicly available at
https://github.com/davidmhart/FastSplatStyler.

</details>


### [11] [MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses](https://arxiv.org/abs/2508.05819)
*Jong-Ik Park,Carlee Joe-Wong,Gary K. Fedder*

Main category: cs.CV

TL;DR: MZEN是一种改进的NeRF框架，专为处理多缩放图像集设计，通过显式学习缩放因子和新的位姿策略，显著提升了工业检测中的细节重建能力。


<details>
  <summary>Details</summary>
Motivation: 传统NeRF方法在工业检测中无法捕捉微米级细节，且多缩放图像会破坏多视角一致性，因此需要一种新的框架来解决这些问题。

Method: MZEN通过引入可学习的缩放因子和分阶段位姿策略（先全局定位，再局部细化），实现了多缩放图像的高效处理。

Result: 在多种场景下，MZEN显著优于基线方法，PSNR提升28%，SSIM提升10%，LPIPS降低222%。

Conclusion: MZEN成功扩展了NeRF在工业检测中的应用，既能保持全局精度，又能捕捉关键微米级细节。

Abstract: Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from
multiple 2D images, even those taken with unknown camera poses. However, they
still miss the fine-detailed structures that matter in industrial inspection,
e.g., detecting sub-micron defects on a production line or analyzing chips with
Scanning Electron Microscopy (SEM). In these scenarios, the sensor resolution
is fixed and compute budgets are tight, so the only way to expose fine
structure is to add zoom-in images; yet, this breaks the multi-view consistency
that pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF
(MZEN), the first NeRF framework that natively handles multi-zoom image sets.
MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom
scalar that scales the focal length, and (ii) introduces a novel pose strategy:
wide-field images are solved first to establish a global metric frame, and
zoom-in images are then pose-primed to the nearest wide-field counterpart via a
zoom-consistent crop-and-match procedure before joint refinement. Across eight
forward-facing scenes$\unicode{x2013}$synthetic TCAD models, real SEM of
micro-structures, and BLEFF objects$\unicode{x2013}$MZEN consistently
outperforms pose-free baselines and even high-resolution variants, boosting
PSNR by up to $28 \%$, SSIM by $10 \%$, and reducing LPIPS by up to $222 \%$.
MZEN, therefore, extends NeRF to real-world factory settings, preserving global
accuracy while capturing the micron-level details essential for industrial
inspection.

</details>


### [12] [TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios](https://arxiv.org/abs/2508.05829)
*Guoping Xu,Hua-Chieh Shao,You Zhang*

Main category: cs.CV

TL;DR: TSMS-SAM2是一种新型框架，通过多时间尺度视频采样增强和内存分割修剪机制，提升了手术视频中可提示视频对象分割与跟踪（VOST）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型（如SAM2）在手术视频分析中面临复杂运动动态和内存冗余的挑战，TSMS-SAM2旨在解决这些问题。

Method: 采用多时间尺度视频采样增强和内存分割修剪机制，优化运动动态处理并提高内存效率。

Result: 在EndoVis2017和EndoVis2018数据集上分别达到95.24和86.73的平均Dice分数，优于现有方法。

Conclusion: TSMS-SAM2在复杂手术场景中表现出高效、鲁棒的分割能力，具有广泛应用潜力。

Abstract: Promptable video object segmentation and tracking (VOST) has seen significant
advances with the emergence of foundation models like Segment Anything Model 2
(SAM2); however, their application in surgical video analysis remains
challenging due to complex motion dynamics and the redundancy of memory that
impedes effective learning. In this work, we propose TSMS-SAM2, a novel
framework that enhances promptable VOST in surgical videos by addressing
challenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2
introduces two key strategies: multi-temporal-scale video sampling augmentation
to improve robustness against motion variability, and a memory splitting and
pruning mechanism that organizes and filters past frame features for more
efficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018
datasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73,
respectively, outperforming prior SAM-based and task-specific methods.
Extensive ablation studies confirm the effectiveness of multiscale temporal
augmentation and memory splitting, highlighting the framework's potential for
robust, efficient segmentation in complex surgical scenarios. Our source code
will be available at https://github.com/apple1986/TSMS-SAM2.

</details>


### [13] [Temporal Cluster Assignment for Efficient Real-Time Video Segmentation](https://arxiv.org/abs/2508.05851)
*Ka-Wai Yung,Felix J. S. Bragman,Jialang Xu,Imanol Luengo,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: 提出了一种名为TCA的轻量级策略，通过利用帧间时间相关性优化视频分割中的token聚类，显著减少计算量同时保持细节。


<details>
  <summary>Details</summary>
Motivation: Swin Transformer在视频分割中计算成本高，现有token减少方法无法有效利用时间冗余。

Method: TCA（Temporal Cluster Assignment）通过时间相关性优化token聚类，无需微调。

Result: 在多个数据集上验证，TCA显著提升了现有聚类方法的精度与速度平衡。

Conclusion: TCA是一种通用且高效的策略，适用于自然和特定领域的视频分割。

Abstract: Vision Transformers have substantially advanced the capabilities of
segmentation models across both image and video domains. Among them, the Swin
Transformer stands out for its ability to capture hierarchical, multi-scale
representations, making it a popular backbone for segmentation in videos.
However, despite its window-attention scheme, it still incurs a high
computational cost, especially in larger variants commonly used for dense
prediction in videos. This remains a major bottleneck for real-time,
resource-constrained applications. Whilst token reduction methods have been
proposed to alleviate this, the window-based attention mechanism of Swin
requires a fixed number of tokens per window, limiting the applicability of
conventional pruning techniques. Meanwhile, training-free token clustering
approaches have shown promise in image segmentation while maintaining window
consistency. Nevertheless, they fail to exploit temporal redundancy, missing a
key opportunity to further optimize video segmentation performance. We
introduce Temporal Cluster Assignment (TCA), a lightweight and effective,
fine-tuning-free strategy that enhances token clustering by leveraging temporal
coherence across frames. Instead of indiscriminately dropping redundant tokens,
TCA refines token clusters using temporal correlations, thereby retaining
fine-grained details while significantly reducing computation. Extensive
evaluations on YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and a private surgical
video dataset show that TCA consistently boosts the accuracy-speed trade-off of
existing clustering-based methods. Our results demonstrate that TCA generalizes
competently across both natural and domain-specific videos.

</details>


### [14] [VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments](https://arxiv.org/abs/2508.05852)
*Kaiser Hamid,Khandakar Ashrafi Akbar,Nade Liang*

Main category: cs.CV

TL;DR: 提出了一种基于视觉-语言框架的驾驶员视觉注意力预测方法，通过自然语言描述动态变化的注意力分布，优于通用视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注静态图像中的注意力分配，缺乏对动态注意力变化的建模，且缺乏可解释性。

Method: 利用BDD-A数据集的高质量标注，通过人机交互反馈优化，微调LLaVA模型，结合低级线索和高级上下文（如路线语义、风险预测）。

Result: 微调模型在注意力转移检测和可解释性上优于通用视觉语言模型，支持少样本和零样本学习。

Conclusion: 该方法为自动驾驶中的可解释AI提供了新方向，支持下游任务如行为预测和人机协作。

Abstract: Driver visual attention prediction is a critical task in autonomous driving
and human-computer interaction (HCI) research. Most prior studies focus on
estimating attention allocation at a single moment in time, typically using
static RGB images such as driving scene pictures. In this work, we propose a
vision-language framework that models the changing landscape of drivers' gaze
through natural language, using few-shot and zero-shot learning on single RGB
images. We curate and refine high-quality captions from the BDD-A dataset using
human-in-the-loop feedback, then fine-tune LLaVA to align visual perception
with attention-centric scene understanding. Our approach integrates both
low-level cues and top-down context (e.g., route semantics, risk anticipation),
enabling language-based descriptions of gaze behavior. We evaluate performance
across training regimes (few shot, and one-shot) and introduce domain-specific
metrics for semantic alignment and response diversity. Results show that our
fine-tuned model outperforms general-purpose VLMs in attention shift detection
and interpretability. To our knowledge, this is among the first attempts to
generate driver visual attention allocation and shifting predictions in natural
language, offering a new direction for explainable AI in autonomous driving.
Our approach provides a foundation for downstream tasks such as behavior
forecasting, human-AI teaming, and multi-agent coordination.

</details>


### [15] [Multi-view Gaze Target Estimation](https://arxiv.org/abs/2508.05857)
*Qiaomu Miao,Vivek Raju Golani,Jingyi Xu,Progga Paromita Dutta,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

TL;DR: 提出了一种多视角相机方法用于视线目标估计（GTE），通过整合不同视角信息提高准确性和适用性，解决了单视角方法中的遮挡、目标模糊等问题。


<details>
  <summary>Details</summary>
Motivation: 现有单视角GTE方法存在遮挡、目标模糊和视线外目标等局限性，多视角方法能显著提升性能。

Method: 采用双视角输入，结合头部信息聚合（HIA）、不确定性视线选择（UGS）和基于极线的场景注意力（ESA）模块。

Result: 显著优于单视角基线，尤其在第二视角提供清晰人脸视图时；还能仅用第二视角图像估计第一视角的视线目标。

Conclusion: 多视角方法有效解决了单视角GTE的局限性，并提供了多视角数据集支持未来研究。

Abstract: This paper presents a method that utilizes multiple camera views for the gaze
target estimation (GTE) task. The approach integrates information from
different camera views to improve accuracy and expand applicability, addressing
limitations in existing single-view methods that face challenges such as face
occlusion, target ambiguity, and out-of-view targets. Our method processes a
pair of camera views as input, incorporating a Head Information Aggregation
(HIA) module for leveraging head information from both views for more accurate
gaze estimation, an Uncertainty-based Gaze Selection (UGS) for identifying the
most reliable gaze output, and an Epipolar-based Scene Attention (ESA) module
for cross-view background information sharing. This approach significantly
outperforms single-view baselines, especially when the second camera provides a
clear view of the person's face. Additionally, our method can estimate the gaze
target in the first view using the image of the person in the second view only,
a capability not possessed by single-view GTE methods. Furthermore, the paper
introduces a multi-view dataset for developing and evaluating multi-view GTE
methods. Data and code are available at
https://www3.cs.stonybrook.edu/~cvl/multiview_gte.html

</details>


### [16] [ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates](https://arxiv.org/abs/2508.05898)
*Hamidreza Dastmalchi,Aijun An,Ali cheraghian*

Main category: cs.CV

TL;DR: ETTA提出了一种高效的测试时适应方法，通过递归更新模块和自适应集成模块，动态优化决策边界和提示选择，显著提升了CLIP等预训练视觉语言模型在分布偏移下的性能。


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉语言模型（如CLIP）在零样本任务中表现优异，但在分布偏移下泛化能力不足。现有的测试时适应方法（TTA）多依赖提示调整或有限的高置信度样本缓存，限制了性能提升。

Method: ETTA引入递归更新模块，动态整合所有测试样本以优化决策边界；同时采用自适应集成模块，减少对提示的依赖。两者基于置信度动态结合，提升效率与准确性。

Result: 在两个基准测试中，ETTA在计算复杂度和准确性上均优于现有TTA模型，成为新的高效测试时适应标准。

Conclusion: ETTA通过动态更新和自适应集成，显著提升了预训练视觉语言模型在分布偏移下的适应能力，为测试时适应提供了高效解决方案。

Abstract: Pretrained vision-language models (VLMs) like CLIP show strong zero-shot
performance but struggle with generalization under distribution shifts.
Test-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test
data in new domains. While some TTA methods rely on prompt-tuning,
training-free cache-based approaches are preferred for efficiency. However,
current cache-based TTA models store only a limited set of high-confidence
samples, restricting the decision boundary to these samples and ignoring the
influence of other incoming test data. To address this, we propose Efficient
Test-Time Adaptation (ETTA), introducing a Recursive Updating module that
integrates all incoming test samples, progressively refining the decision
boundary. This strategy mimics an unbounded cache, dynamically updating
contextual embeddings for improved accuracy with minimal memory and
computational overhead. ETTA also includes an Adaptive Ensemble module to
reduce prompt dependency in image-to-text scores by dynamically selecting
optimal prompts for each class. Furthermore, ETTA adaptively combines scores
from both modules based on confidence levels, leveraging their complementary
strengths. Extensive experiments on two benchmarks confirm that ETTA surpasses
the state-of-the-art TTA models in computational complexity and accuracy,
setting a new standard for effective, efficient test-time adaptation. The code
has been released at https://github.com/hamidreza-dastmalchi/ETTA.

</details>


### [17] [Robust Image Stitching with Optimal Plane](https://arxiv.org/abs/2508.05903)
*Lang Nie,Yuan Mei,Kang Liao,Yunqiu Xu,Chunyu Lin,Bin Xiao*

Main category: cs.CV

TL;DR: RopStitch是一种无监督深度图像拼接框架，通过双分支架构和虚拟最优平面概念，实现了鲁棒性和自然性。


<details>
  <summary>Details</summary>
Motivation: 解决图像拼接中内容对齐与结构保持的矛盾，并提升模型在多样化场景中的泛化能力。

Method: 采用双分支架构分别提取粗粒度与细粒度特征，并通过虚拟最优平面估计最优拼接平面。

Result: 在多个数据集上显著优于现有方法，尤其在场景鲁棒性和内容自然性方面表现突出。

Conclusion: RopStitch通过创新的架构和优化策略，为图像拼接提供了高效且通用的解决方案。

Abstract: We present \textit{RopStitch}, an unsupervised deep image stitching framework
with both robustness and naturalness. To ensure the robustness of
\textit{RopStitch}, we propose to incorporate the universal prior of content
perception into the image stitching model by a dual-branch architecture. It
separately captures coarse and fine features and integrates them to achieve
highly generalizable performance across diverse unseen real-world scenes.
Concretely, the dual-branch model consists of a pretrained branch to capture
semantically invariant representations and a learnable branch to extract
fine-grained discriminative features, which are then merged into a whole by a
controllable factor at the correlation level. Besides, considering that content
alignment and structural preservation are often contradictory to each other, we
propose a concept of virtual optimal planes to relieve this conflict. To this
end, we model this problem as a process of estimating homography decomposition
coefficients, and design an iterative coefficient predictor and minimal
semantic distortion constraint to identify the optimal plane. This scheme is
finally incorporated into \textit{RopStitch} by warping both views onto the
optimal plane bidirectionally. Extensive experiments across various datasets
demonstrate that \textit{RopStitch} significantly outperforms existing methods,
particularly in scene robustness and content naturalness. The code is available
at {\color{red}https://github.com/MmelodYy/RopStitch}.

</details>


### [18] [Neural Field Representations of Mobile Computational Photography](https://arxiv.org/abs/2508.05907)
*Ilya Chugunov*

Main category: cs.CV

TL;DR: 论文探讨了如何利用神经场模型在移动成像中高效表示复杂几何和光照效果，无需复杂预处理或标记数据。


<details>
  <summary>Details</summary>
Motivation: 移动设备已成为多功能计算成像平台，结合神经场模型的潜力，探索其在移动摄影数据中的应用。

Method: 设计自正则化的神经场模型，通过随机梯度下降直接拟合智能手机原始数据，解决逆问题。

Result: 方法在深度估计、图层分离和图像拼接等应用中优于现有技术。

Conclusion: 神经场模型为移动计算成像提供了高效、无需先验知识的解决方案。

Abstract: Over the past two decades, mobile imaging has experienced a profound
transformation, with cell phones rapidly eclipsing all other forms of digital
photography in popularity. Today's cell phones are equipped with a diverse
range of imaging technologies - laser depth ranging, multi-focal camera arrays,
and split-pixel sensors - alongside non-visual sensors such as gyroscopes,
accelerometers, and magnetometers. This, combined with on-board integrated
chips for image and signal processing, makes the cell phone a versatile
pocket-sized computational imaging platform. Parallel to this, we have seen in
recent years how neural fields - small neural networks trained to map
continuous spatial input coordinates to output signals - enable the
reconstruction of complex scenes without explicit data representations such as
pixel arrays or point clouds. In this thesis, I demonstrate how carefully
designed neural field models can compactly represent complex geometry and
lighting effects. Enabling applications such as depth estimation, layer
separation, and image stitching directly from collected in-the-wild mobile
photography data. These methods outperform state-of-the-art approaches without
relying on complex pre-processing steps, labeled ground truth data, or machine
learning priors. Instead, they leverage well-constructed, self-regularized
models that tackle challenging inverse problems through stochastic gradient
descent, fitting directly to raw measurements from a smartphone.

</details>


### [19] [Enhancing Construction Site Analysis and Understanding with 3D Segmentation](https://arxiv.org/abs/2508.05922)
*Sri Ramana Saketh Vasanthawada,Pengkun Liu,Pingbo Tang*

Main category: cs.CV

TL;DR: 论文评估了SAM和Mask3D两种3D分割方法在复杂建筑工地环境中的表现，指出当前方法在户外场景的不足，并提出了改进方向。


<details>
  <summary>Details</summary>
Motivation: 传统建筑进度监测方法效率低且难以适应复杂多变的工地环境，因此需要探索基于计算机视觉的高效解决方案。

Method: 通过比较SAM和Mask3D在室内和户外建筑场景中的表现，评估其适应性和性能。

Result: 研究发现当前分割方法在户外场景缺乏基准，SAM和Mask3D在复杂工地环境中的表现存在局限性。

Conclusion: 研究强调了定制化分割工作流的必要性，以提升建筑监测的自动化和精确性。

Abstract: Monitoring construction progress is crucial yet resource-intensive, prompting
the exploration of computer-vision-based methodologies for enhanced efficiency
and scalability. Traditional data acquisition methods, primarily focusing on
indoor environments, falter in construction site's complex, cluttered, and
dynamically changing conditions. This paper critically evaluates the
application of two advanced 3D segmentation methods, Segment Anything Model
(SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained
initially on indoor datasets, both models' adaptability and performance are
assessed in real-world construction settings, highlighting the gap in current
segmentation approaches due to the absence of benchmarks for outdoor scenarios.
Through a comparative analysis, this study not only showcases the relative
effectiveness of SAM and Mask3D but also addresses the critical need for
tailored segmentation workflows capable of extracting actionable insights from
construction site data, thereby advancing the field towards more automated and
precise monitoring techniques.

</details>


### [20] [A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image](https://arxiv.org/abs/2508.05950)
*Yanxing Liang,Yinghui Wang,Jinlong Yang,Wei Li*

Main category: cs.CV

TL;DR: SINGAD提出了一种自监督框架，通过3D高斯扩散引导的单图像法线估计，解决了多视角几何不一致性和数据依赖性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖数据驱动的统计先验，缺乏对光-表面交互的显式建模，导致多视角法线方向冲突和梯度不连续问题。

Method: 结合物理驱动的光交互建模和可微分渲染重投影策略，构建光交互驱动的3D高斯重参数化模型，并设计跨域特征融合模块。

Result: 在Google Scanned Objects数据集上的定量评估显示，该方法在多个指标上优于现有技术。

Conclusion: SINGAD通过自监督优化和几何误差传播，显著提升了单图像法线估计的准确性和一致性。

Abstract: The lack of spatial dimensional information remains a challenge in normal
estimation from a single image. Recent diffusion-based methods have
demonstrated significant potential in 2D-to-3D implicit mapping, they rely on
data-driven statistical priors and miss the explicit modeling of light-surface
interaction, leading to multi-view normal direction conflicts. Moreover, the
discrete sampling mechanism of diffusion models causes gradient discontinuity
in differentiable rendering reconstruction modules, preventing 3D geometric
errors from being backpropagated to the normal generation network, thereby
forcing existing methods to depend on dense normal annotations. This paper
proposes SINGAD, a novel Self-supervised framework from a single Image for
Normal estimation via 3D GAussian splatting guided Diffusion. By integrating
physics-driven light-interaction modeling and a differentiable rendering-based
reprojection strategy, our framework directly converts 3D geometric errors into
normal optimization signals, solving the challenges of multi-view geometric
inconsistency and data dependency. Specifically, the framework constructs a
light-interaction-driven 3DGS reparameterization model to generate multi-scale
geometric features consistent with light transport principles, ensuring
multi-view normal consistency. A cross-domain feature fusion module is designed
within a conditional diffusion model, embedding geometric priors to constrain
normal generation while maintaining accurate geometric error propagation.
Furthermore, a differentiable 3D reprojection loss strategy is introduced for
self-supervised optimization that minimizes geometric error between the
reconstructed and input image, eliminating dependence on annotated normal
datasets. Quantitative evaluations on the Google Scanned Objects dataset
demonstrate that our method outperforms state-of-the-art approaches across
multiple metrics.

</details>


### [21] [Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents](https://arxiv.org/abs/2508.05954)
*Han Lin,Jaemin Cho,Amir Zadeh,Chuan Li,Mohit Bansal*

Main category: cs.CV

TL;DR: Bifrost-1是一个统一框架，通过将预训练的多模态大语言模型（MLLMs）与扩散模型结合，利用CLIP图像嵌入作为潜在变量，实现高效的高保真可控图像生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在训练成本高和图像表示未预训练的问题，同时保持MLLMs的强推理能力。

Method: 使用patch级CLIP图像嵌入作为潜在变量，通过轻量级ControlNet适配扩散模型，并在MLLM中初始化视觉生成分支。

Result: Bifrost-1在视觉保真度和多模态理解上表现优异，且训练计算成本显著降低。

Conclusion: Bifrost-1通过巧妙设计实现了高效、高质量的图像生成，同时保留了MLLMs的原始推理能力。

Abstract: There is growing interest in integrating high-fidelity visual synthesis
capabilities into large language models (LLMs) without compromising their
strong reasoning capabilities. Existing methods that directly train LLMs or
bridge LLMs and diffusion models usually suffer from costly training since the
backbone LLMs have not seen image representations during pretraining. We
present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs
(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent
variables, which are natively aligned with the MLLM's CLIP visual encoder.
These patch-level image embeddings are integrated into the diffusion model with
a lightweight adaptation of its ControlNet. To retain the original multimodal
reasoning capabilities of MLLMs, we equip the MLLM with a visual generation
branch initialized from the original MLLM parameters when predicting the
patch-level image embeddings. By seamlessly integrating pretrained MLLMs and
diffusion models with patch-level CLIP latents, our framework enables
high-fidelity controllable image generation with significant training
efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or
better performance than previous methods in terms of visual fidelity and
multimodal understanding, with substantially lower compute during training. We
also provide comprehensive ablation studies showing the effectiveness of our
design choices.

</details>


### [22] [PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation](https://arxiv.org/abs/2508.05976)
*Zhihao Zhu,Yifan Zheng,Siyu Pan,Yaohui Jin,Yao Mu*

Main category: cs.CV

TL;DR: PASG框架通过几何特征聚合和VLM驱动的语义锚定，解决了机器人操作中语义与几何特征的割裂问题，实现了动态语义-功能关系的捕捉。


<details>
  <summary>Details</summary>
Motivation: 解决机器人操作中高级任务语义与低级几何特征之间的割裂问题，并减少对人工标注的依赖。

Method: 提出PASG框架，包括自动几何基元提取、VLM驱动的语义锚定，以及空间语义推理基准和微调VLM。

Result: 在多样化机器人操作任务中表现优异，性能接近人工标注水平。

Conclusion: PASG为机器人操作中几何基元与任务语义的统一提供了新范式。

Abstract: The fragmentation between high-level task semantics and low-level geometric
features remains a persistent challenge in robotic manipulation. While
vision-language models (VLMs) have shown promise in generating affordance-aware
visual representations, the lack of semantic grounding in canonical spaces and
reliance on manual annotations severely limit their ability to capture dynamic
semantic-affordance relationships. To address these, we propose Primitive-Aware
Semantic Grounding (PASG), a closed-loop framework that introduces: (1)
Automatic primitive extraction through geometric feature aggregation, enabling
cross-category detection of keypoints and axes; (2) VLM-driven semantic
anchoring that dynamically couples geometric primitives with functional
affordances and task-relevant description; (3) A spatial-semantic reasoning
benchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's
effectiveness in practical robotic manipulation tasks across diverse scenarios,
achieving performance comparable to manual annotations. PASG achieves a
finer-grained semantic-affordance understanding of objects, establishing a
unified paradigm for bridging geometric primitives with task semantics in
robotic manipulation.

</details>


### [23] [AnimateScene: Camera-controllable Animation in Any Scene](https://arxiv.org/abs/2508.05982)
*Qingyang Liu,Bingjie Gao,Weiheng Huang,Jun Zhang,Zhongqian Sun,Yang Wei,Zelin Peng,Qianli Ma,Shuai Yang,Zhaohe Liao,Haonan Zhao,Li Niu*

Main category: cs.CV

TL;DR: AnimateScene提出了一种统一框架，解决了3D场景重建与4D人体动画无缝集成的挑战，包括人体位置与比例、光照风格对齐以及相机轨迹重建。


<details>
  <summary>Details</summary>
Motivation: 解决3D场景重建与4D人体动画集成中的关键问题，如人体位置与比例、光照风格不一致以及相机轨迹需求。

Method: 设计了一个准确的人体放置模块、无训练的风格对齐方法以及联合后重建方法，支持相机轨迹插入。

Result: 实验表明，AnimateScene能生成具有高几何细节和时空一致性的动态场景视频。

Conclusion: AnimateScene通过统一框架有效解决了3D场景与4D人体动画的集成问题，实现了高质量的视觉效果。

Abstract: 3D scene reconstruction and 4D human animation have seen rapid progress and
broad adoption in recent years. However, seamlessly integrating reconstructed
scenes with 4D human animation to produce visually engaging results remains
challenging. One key difficulty lies in placing the human at the correct
location and scale within the scene while avoiding unrealistic
interpenetration. Another challenge is that the human and the background may
exhibit different lighting and style, leading to unrealistic composites. In
addition, appealing character motion videos are often accompanied by camera
movements, which means that the viewpoints need to be reconstructed along a
specified trajectory. We present AnimateScene, which addresses the above issues
in a unified framework. First, we design an accurate placement module that
automatically determines a plausible 3D position for the human and prevents any
interpenetration within the scene during motion. Second, we propose a
training-free style alignment method that adapts the 4D human representation to
match the background's lighting and style, achieving coherent visual
integration. Finally, we design a joint post-reconstruction method for both the
4D human and the 3D scene that allows camera trajectories to be inserted,
enabling the final rendered video to feature visually appealing camera
movements. Extensive experiments show that AnimateScene generates dynamic scene
videos with high geometric detail and spatiotemporal coherence across various
camera and action combinations.

</details>


### [24] [ETA: Energy-based Test-time Adaptation for Depth Completion](https://arxiv.org/abs/2508.05989)
*Younjoon Chung,Hyoungseob Park,Patrick Rim,Xiaoran Zhang,Jihe He,Ziyao Zeng,Safa Cicek,Byung-Woo Hong,James S. Duncan,Alex Wong*

Main category: cs.CV

TL;DR: 提出了一种基于能量的测试时适应方法（ETA），用于预训练深度补全模型的测试时适应，通过对抗性扰动探索数据空间并训练能量模型，显著提升了在室内外数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 深度补全模型在从源数据转移到目标数据时，由于协变量偏移导致预测错误。需要一种无需目标数据先验的方法来适应新环境。

Method: 利用对抗性扰动探索数据空间，训练能量模型评分深度预测的局部区域是否为源数据分布。测试时通过最小化能量更新模型参数。

Result: 在三个室内和三个室外数据集上，ETA平均性能分别提升10.23%和6.94%，优于现有方法。

Conclusion: ETA通过能量模型和测试时适应，有效解决了深度补全模型在新环境中的适应问题，性能显著提升。

Abstract: We propose a method for test-time adaptation of pretrained depth completion
models. Depth completion models, trained on some ``source'' data, often predict
erroneous outputs when transferred to ``target'' data captured in novel
environmental conditions due to a covariate shift. The crux of our method lies
in quantifying the likelihood of depth predictions belonging to the source data
distribution. The challenge is in the lack of access to out-of-distribution
(target) data prior to deployment. Hence, rather than making assumptions
regarding the target distribution, we utilize adversarial perturbations as a
mechanism to explore the data space. This enables us to train an energy model
that scores local regions of depth predictions as in- or out-of-distribution.
We update the parameters of pretrained depth completion models at test time to
minimize energy, effectively aligning test-time predictions to those of the
source distribution. We call our method ``Energy-based Test-time Adaptation'',
or ETA for short. We evaluate our method across three indoor and three outdoor
datasets, where ETA improve over the previous state-of-the-art method by an
average of 6.94% for outdoors and 10.23% for indoors. Project Page:
https://fuzzythecat.github.io/eta.

</details>


### [25] [Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision](https://arxiv.org/abs/2508.05990)
*Haichao Wang,Xinyue Xi,Jiangtao Wen,Yuxing Han*

Main category: cs.CV

TL;DR: 本文提出了一种高效的视频计算机视觉系统，通过去除图像信号处理器并使用块匹配运动估计算法，显著减少了计算开销，同时保持了性能。


<details>
  <summary>Details</summary>
Motivation: 视频计算机视觉系统的高效性因视频中的时间冗余而成为挑战，现有方法未能完全减少冗余且忽略了前端计算开销。

Method: 1. 去除图像信号处理器，直接输入Bayer格式数据；2. 提出基于块匹配的运动估计算法，结合MV细化模块和上下文感知块细化网络；3. 采用帧选择策略平衡精度与效率。

Result: 在多个视频计算机视觉任务中，该方法实现了显著加速，性能损失较小。

Conclusion: 提出的系统通过优化前端计算和运动估计，有效提升了视频计算机视觉的效率。

Abstract: The efficiency of video computer vision system remains a challenging task due
to the high temporal redundancy inside a video. Existing works have been
proposed for efficient vision computer vision. However, they do not fully
reduce the temporal redundancy and neglect the front end computation overhead.
In this paper, we propose an efficient video computer vision system. First,
image signal processor is removed and Bayer-format data is directly fed into
video computer vision models, thus saving the front end computation. Second,
instead of optical flow models and video codecs, a fast block matching-based
motion estimation algorithm is proposed specifically for efficient video
computer vision, with a MV refinement module. To correct the error,
context-aware block refinement network is introduced to refine regions with
large error. To further balance the accuracy and efficiency, a frame selection
strategy is employed. Experiments on multiple video computer vision tasks
demonstrate that our method achieves significant acceleration with slight
performance loss.

</details>


### [26] [ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge](https://arxiv.org/abs/2508.05991)
*Juewen Hu,Yexin Li,Jiulin Li,Shuo Chen,Pring Wong*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态情感识别框架，通过预训练模型提取视觉、音频和文本特征，并采用双分支视觉编码器和上下文丰富方法优化特征提取。融合策略和标签优化进一步提升了性能。


<details>
  <summary>Details</summary>
Motivation: 提升人机交互中的情感识别能力，解决数据稀缺问题。

Method: 使用预训练模型提取多模态特征，设计双分支视觉编码器和上下文丰富的文本处理方法，融合策略包括自注意力机制和残差连接，并优化训练标签。

Result: 在MER2025-SEMI数据集上，加权F-score达到87.49%，显著优于基线（78.63%）。

Conclusion: 提出的框架在多模态情感识别任务中表现优异，验证了其有效性。

Abstract: Emotion recognition plays a vital role in enhancing human-computer
interaction. In this study, we tackle the MER-SEMI challenge of the MER2025
competition by proposing a novel multimodal emotion recognition framework. To
address the issue of data scarcity, we leverage large-scale pre-trained models
to extract informative features from visual, audio, and textual modalities.
Specifically, for the visual modality, we design a dual-branch visual encoder
that captures both global frame-level features and localized facial
representations. For the textual modality, we introduce a context-enriched
method that employs large language models to enrich emotional cues within the
input text. To effectively integrate these multimodal features, we propose a
fusion strategy comprising two key components, i.e., self-attention mechanisms
for dynamic modality weighting, and residual connections to preserve original
representations. Beyond architectural design, we further refine noisy labels in
the training set by a multi-source labeling strategy. Our approach achieves a
substantial performance improvement over the official baseline on the
MER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to
78.63%, thereby validating the effectiveness of the proposed framework.

</details>


### [27] [EvoMakeup: High-Fidelity and Controllable Makeup Editing with MakeupQuad](https://arxiv.org/abs/2508.05994)
*Huadong Wu,Yi Fu,Yunhao Li,Yuan Gao,Kang Du*

Main category: cs.CV

TL;DR: 论文提出MakeupQuad数据集和EvoMakeup框架，解决了现有方法在面部化妆编辑中细节粗糙、身份与化妆保真度不足的问题，实现了高质量、可控的多任务化妆编辑。


<details>
  <summary>Details</summary>
Motivation: 现有方法在面部化妆编辑中因缺乏结构化配对数据，导致结果质量低、细节粗糙，且难以同时保持身份和化妆保真度。

Method: 引入MakeupQuad数据集，并提出EvoMakeup框架，通过多阶段蒸馏减少图像退化，迭代提升数据和模型质量。

Result: EvoMakeup在真实场景中表现优异，支持高保真、可控的多任务化妆编辑，包括全脸、局部参考和文本驱动编辑。

Conclusion: 该方法在化妆保真度和身份保持方面表现优越，平衡了二者，代码和数据集将在接受后发布。

Abstract: Facial makeup editing aims to realistically transfer makeup from a reference
to a target face. Existing methods often produce low-quality results with
coarse makeup details and struggle to preserve both identity and makeup
fidelity, mainly due to the lack of structured paired data -- where source and
result share identity, and reference and result share identical makeup. To
address this, we introduce MakeupQuad, a large-scale, high-quality dataset with
non-makeup faces, references, edited results, and textual makeup descriptions.
Building on this, we propose EvoMakeup, a unified training framework that
mitigates image degradation during multi-stage distillation, enabling iterative
improvement of both data and model quality. Although trained solely on
synthetic data, EvoMakeup generalizes well and outperforms prior methods on
real-world benchmarks. It supports high-fidelity, controllable, multi-task
makeup editing -- including full-face and partial reference-based editing, as
well as text-driven makeup editing -- within a single model. Experimental
results demonstrate that our method achieves superior makeup fidelity and
identity preservation, effectively balancing both aspects. Code and dataset
will be released upon acceptance.

</details>


### [28] [MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2508.06009)
*Jun Feng,Zixin Wang,Zhentao Zhang,Yue Guo,Zhihan Zhou,Xiuyi Chen,Zhenyang Li,Dawei Yin*

Main category: cs.CV

TL;DR: MathReal数据集填补了现有MLLM在真实教育场景中视觉数学推理的评估空白，发现现有模型在真实图像处理上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM基准多基于干净或处理过的多模态输入，缺乏真实K-12教育场景中的图像数据，需填补这一空白。

Method: 引入MathReal数据集，包含2000个真实场景下的数学问题图像，分类为三大类14子类，并设计六种实验设置评估MLLM性能。

Result: 现有MLLM在真实教育场景中的问题解决能力受到显著挑战，表现不佳。

Conclusion: 通过分析MLLM的性能和错误模式，为未来改进提供了方向。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in visual mathematical reasoning across various existing
benchmarks. However, these benchmarks are predominantly based on clean or
processed multimodal inputs, without incorporating the images provided by
real-world Kindergarten through 12th grade (K-12) educational users. To address
this gap, we introduce MathReal, a meticulously curated dataset comprising
2,000 mathematical questions with images captured by handheld mobile devices in
authentic scenarios. Each question is an image, containing the question text
and visual element. We systematically classify the real images into three
primary categories: image quality degradation, perspective variation, and
irrelevant content interference, which are further delineated into 14
subcategories. Additionally, MathReal spans five core knowledge and ability
categories, which encompass three question types and are divided into three
difficulty levels. To comprehensively evaluate the multimodal mathematical
reasoning abilities of state-of-the-art MLLMs in real-world scenarios, we
design six experimental settings that enable a systematic analysis of their
performance. Through extensive experimentation, we find that the
problem-solving abilities of existing MLLMs are significantly challenged in
realistic educational contexts. Based on this, we conduct a thorough analysis
of their performance and error patterns, providing insights into their
recognition, comprehension, and reasoning capabilities, and outlining
directions for future improvements. Data and code:
https://github.com/junfeng0288/MathReal.

</details>


### [29] [ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors](https://arxiv.org/abs/2508.06014)
*Minsu Kim,Subin Jeon,In Cho,Mijin Yoo,Seon Joo Kim*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯泼溅（3DGS）的管道，通过生成额外训练视图和虚拟相机放置策略，提升新视角合成（NVS）的质量，减少伪影和缺失区域。


<details>
  <summary>Details</summary>
Motivation: 现有方法在偏离训练轨迹的视角下渲染时存在伪影和缺失区域，限制了场景的无缝探索。

Method: 采用信息增益驱动的虚拟相机放置策略最大化场景覆盖，结合视频扩散先验优化渲染结果，并通过增强视图微调3D高斯模型。

Result: 在Wild-Explore基准测试中表现优于现有3DGS方法，实现高质量、无伪影的任意视角渲染。

Conclusion: 该方法显著提升了3DGS的重建质量，为场景探索提供了更优解决方案。

Abstract: Recent advances in novel view synthesis (NVS) have enabled real-time
rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle
with artifacts and missing regions when rendering from viewpoints that deviate
from the training trajectory, limiting seamless scene exploration. To address
this, we propose a 3DGS-based pipeline that generates additional training views
to enhance reconstruction. We introduce an information-gain-driven virtual
camera placement strategy to maximize scene coverage, followed by video
diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with
these enhanced views significantly improves reconstruction quality. To evaluate
our method, we present Wild-Explore, a benchmark designed for challenging scene
exploration. Experiments demonstrate that our approach outperforms existing
3DGS-based methods, enabling high-quality, artifact-free rendering from
arbitrary viewpoints.
  https://exploregs.github.io

</details>


### [30] [Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis](https://arxiv.org/abs/2508.06021)
*Utku Ozbulak,Michaela Cohrs,Hristo L. Svilenov,Joris Vankerschaver,Wesley De Neve*

Main category: cs.CV

TL;DR: 使用深度学习结合流式成像显微镜分析亚可见颗粒，但数据稀缺和类别不平衡是主要挑战。本文开发了一种扩散模型生成高质量图像以解决数据不平衡问题，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决亚可见颗粒分析中数据稀缺和类别不平衡的问题，尤其是硅油和气泡等罕见颗粒类型。

Method: 开发扩散模型生成高保真图像，用于增强训练数据集，并训练多类深度神经网络。

Result: 生成的图像与真实颗粒图像相似，实验表明该方法显著提高了分类性能。

Conclusion: 扩散模型有效解决了数据不平衡问题，并公开了模型和工具以促进研究。

Abstract: Sub-visible particle analysis using flow imaging microscopy combined with
deep learning has proven effective in identifying particle types, enabling the
distinction of harmless components such as silicone oil from protein particles.
However, the scarcity of available data and severe imbalance between particle
types within datasets remain substantial hurdles when applying multi-class
classifiers to such problems, often forcing researchers to rely on less
effective methods. The aforementioned issue is particularly challenging for
particle types that appear unintentionally and in lower numbers, such as
silicone oil and air bubbles, as opposed to protein particles, where obtaining
large numbers of images through controlled settings is comparatively
straightforward. In this work, we develop a state-of-the-art diffusion model to
address data imbalance by generating high-fidelity images that can augment
training datasets, enabling the effective training of multi-class deep neural
networks. We validate this approach by demonstrating that the generated samples
closely resemble real particle images in terms of visual quality and structure.
To assess the effectiveness of using diffusion-generated images in training
datasets, we conduct large-scale experiments on a validation dataset comprising
500,000 protein particle images and demonstrate that this approach improves
classification performance with no negligible downside. Finally, to promote
open research and reproducibility, we publicly release both our diffusion
models and the trained multi-class deep neural network classifiers, along with
a straightforward interface for easy integration into future studies, at
https://github.com/utkuozbulak/svp-generative-ai.

</details>


### [31] [Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts](https://arxiv.org/abs/2508.06032)
*Kiran Chhatre,Christopher Peters,Srikrishna Karanam*

Main category: cs.CV

TL;DR: Spectrum是一个统一的网络，用于像素级人体部位和服装解析，通过改进的图像到纹理扩散模型实现更细粒度的分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法在人体解析中使用固定掩码类别，无法区分细粒度服装类型；而开放词汇分割方法通常将整个人归为一个类别，忽略了服装和身体部位的多样性。

Method: Spectrum利用经过微调的图像到纹理扩散模型提取特征，并通过提示引导生成语义有效的掩码，实现细粒度解析。

Result: 实验表明，Spectrum在身体部位、服装类别和未见过的服装分割任务中均优于基线方法。

Conclusion: Spectrum通过改进的扩散模型实现了更细粒度的人体解析，适用于复杂场景中的多人体分割。

Abstract: Existing methods for human parsing into body parts and clothing often use
fixed mask categories with broad labels that obscure fine-grained clothing
types. Recent open-vocabulary segmentation approaches leverage pretrained
text-to-image (T2I) diffusion model features for strong zero-shot transfer, but
typically group entire humans into a single person category, failing to
distinguish diverse clothing or detailed body parts. To address this, we
propose Spectrum, a unified network for part-level pixel parsing (body parts
and clothing) and instance-level grouping. While diffusion-based
open-vocabulary models generalize well across tasks, their internal
representations are not specialized for detailed human parsing. We observe
that, unlike diffusion models with broad representations, image-driven 3D
texture generators maintain faithful correspondence to input images, enabling
stronger representations for parsing diverse clothing and body parts. Spectrum
introduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model --
obtained by fine-tuning a T2I model on 3D human texture maps -- for improved
alignment with body parts and clothing. From an input image, we extract
human-part internal features via the I2Tx diffusion model and generate
semantically valid masks aligned to diverse clothing categories through
prompt-guided grounding. Once trained, Spectrum produces semantic segmentation
maps for every visible body part and clothing category, ignoring standalone
garments or irrelevant objects, for any number of humans in the scene. We
conduct extensive cross-dataset experiments -- separately assessing body parts,
clothing parts, unseen clothing categories, and full-body masks -- and
demonstrate that Spectrum consistently outperforms baseline methods in
prompt-based segmentation.

</details>


### [32] [InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow](https://arxiv.org/abs/2508.06033)
*Yiming Gong,Zhen Zhu,Minjia Zhang*

Main category: cs.CV

TL;DR: InstantEdit是一种基于RectifiedFlow框架的快速文本引导图像编辑方法，通过PerRFI反转策略和Inversion Latent Injection技术实现高效编辑，同时结合Disentangled Prompt Guidance和ControlNet提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法在速度和编辑质量上存在不足，InstantEdit旨在通过RectifiedFlow框架实现快速且高质量的文本引导编辑。

Method: 采用PerRFI反转策略和Inversion Latent Injection技术，结合Disentangled Prompt Guidance和Canny-conditioned ControlNet，优化编辑过程。

Result: 在PIE数据集上，InstantEdit在速度和编辑质量上均优于现有方法。

Conclusion: InstantEdit通过RectifiedFlow框架和多项创新技术，实现了快速且高质量的文本引导图像编辑。

Abstract: We propose a fast text-guided image editing method called InstantEdit based
on the RectifiedFlow framework, which is structured as a few-step editing
process that preserves critical content while following closely to textual
instructions. Our approach leverages the straight sampling trajectories of
RectifiedFlow by introducing a specialized inversion strategy called PerRFI. To
maintain consistent while editable results for RectifiedFlow model, we further
propose a novel regeneration method, Inversion Latent Injection, which
effectively reuses latent information obtained during inversion to facilitate
more coherent and detailed regeneration. Additionally, we propose a
Disentangled Prompt Guidance technique to balance editability with detail
preservation, and integrate a Canny-conditioned ControlNet to incorporate
structural cues and suppress artifacts. Evaluation on the PIE image editing
dataset demonstrates that InstantEdit is not only fast but also achieves better
qualitative and quantitative results compared to state-of-the-art few-step
editing methods.

</details>


### [33] [More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment](https://arxiv.org/abs/2508.06036)
*Jun Xie,Yingjian Zhu,Feng Chen,Zhenghao Zhang,Xiaohui Fan,Hongzhu Yi,Xinming Wang,Chen Yu,Yue Bi,Zhaoran Zhao,Xiongjun Guan,Zhepeng Wang*

Main category: cs.CV

TL;DR: 提出了一种基于混合专家（MoE）的半监督学习框架，整合多模态输入和伪标签策略，在MER2025-SEMI挑战中取得第二名。


<details>
  <summary>Details</summary>
Motivation: 解决半监督学习中的情感识别问题，通过整合多模态数据和伪标签策略提升模型性能。

Method: 结合多模态输入（如视觉语言模型和动作单元信息），采用共识伪标签策略和两阶段训练，最后通过多专家投票和重排序优化预测。

Result: 在MER2025-SEMI测试集上F1得分为0.8772，排名第二。

Conclusion: 提出的框架有效提升了半监督情感识别的性能，多模态和伪标签策略是关键。

Abstract: In this paper, we present our solution for the semi-supervised learning track
(MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the
principle that "more is better," to construct a robust Mixture of Experts (MoE)
emotion recognition system. Our approach integrates a diverse range of input
modalities as independent experts, including novel signals such as knowledge
from large Vision-Language Models (VLMs) and temporal Action Unit (AU)
information. To effectively utilize unlabeled data, we introduce a
consensus-based pseudo-labeling strategy, generating high-quality labels from
the agreement between a baseline model and Gemini, which are then used in a
two-stage training paradigm. Finally, we employ a multi-expert voting ensemble
combined with a rule-based re-ranking process to correct prediction bias and
better align the outputs with human preferences. Evaluated on the MER2025-SEMI
challenge dataset, our method achieves an F1-score of 0.8772 on the test set,
ranking 2nd in the track. Our code is available at
https://github.com/zhuyjan/MER2025-MRAC25.

</details>


### [34] [Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models](https://arxiv.org/abs/2508.06038)
*Huanyu Wang,Jushi Kai,Haoli Bai,Lu Hou,Bo Jiang,Ziwei He,Zhouhan Lin*

Main category: cs.CV

TL;DR: Fourier-VLM通过频域压缩视觉表示，显著减少计算开销和推理延迟，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型中视觉特征导致的高计算开销和推理延迟问题。

Method: 使用二维离散余弦变换（DCT）对视觉特征进行低通滤波，利用FFT高效计算。

Result: 在多个基准测试中表现优异，推理FLOPs减少83.8%，生成速度提升31.2%。

Conclusion: Fourier-VLM在高效性和实用性上表现突出，适用于多种架构。

Abstract: Vision-Language Models (VLMs) typically replace the predefined image
placeholder token (<image>) in textual instructions with visual features from
an image encoder, forming the input to a backbone Large Language Model (LLM).
However, the large number of vision tokens significantly increases the context
length, leading to high computational overhead and inference latency. While
previous efforts mitigate this by selecting only important visual features or
leveraging learnable queries to reduce token count, they often compromise
performance or introduce substantial extra costs. In response, we propose
Fourier-VLM, a simple yet efficient method that compresses visual
representations in the frequency domain. Our approach is motivated by the
observation that vision features output from the vision encoder exhibit
concentrated energy in low-frequency components. Leveraging this, we apply a
low-pass filter to the vision features using a two-dimentional Discrete Cosine
Transform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier
Transform (FFT) operator with a time complexity of $\mathcal{O}(n\log n)$,
minimizing the extra computational cost while introducing no additional
parameters. Extensive experiments across various image-based benchmarks
demonstrate that Fourier-VLM achieves competitive performance with strong
generalizability across both LLaVA and Qwen-VL architectures. Crucially, it
reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%
compared to LLaVA-v1.5, highlighting the superior efficiency and practicality.

</details>


### [35] [NEP: Autoregressive Image Editing via Next Editing Token Prediction](https://arxiv.org/abs/2508.06044)
*Huimin Wu,Xiaojian Ma,Haozhe Zhao,Yanpeng Zhao,Qing Li*

Main category: cs.CV

TL;DR: 论文提出了一种基于自回归图像生成的Next Editing-token Prediction（NEP）方法，仅需重新生成需要编辑的区域，避免了不必要的计算和对非编辑区域的干扰。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导图像编辑方法会生成整个目标图像，导致计算成本高且可能影响非编辑区域的质量。

Method: 通过预训练一个任意顺序的自回归文本到图像（T2I）模型，实现零样本图像编辑，并适应NEP方法。

Result: 在广泛使用的图像编辑基准测试中达到了新的最优性能，并支持零样本的测试时缩放（TTS）。

Conclusion: NEP方法显著提升了图像编辑的效率和准确性，同时支持灵活的零样本编辑和迭代优化。

Abstract: Text-guided image editing involves modifying a source image based on a
language instruction and, typically, requires changes to only small local
regions. However, existing approaches generate the entire target image rather
than selectively regenerate only the intended editing areas. This results in
(1) unnecessary computational costs and (2) a bias toward reconstructing
non-editing regions, which compromises the quality of the intended edits. To
resolve these limitations, we propose to formulate image editing as Next
Editing-token Prediction (NEP) based on autoregressive image generation, where
only regions that need to be edited are regenerated, thus avoiding unintended
modification to the non-editing areas. To enable any-region editing, we propose
to pre-train an any-order autoregressive text-to-image (T2I) model. Once
trained, it is capable of zero-shot image editing and can be easily adapted to
NEP for image editing, which achieves a new state-of-the-art on widely used
image editing benchmarks. Moreover, our model naturally supports test-time
scaling (TTS) through iteratively refining its generation in a zero-shot
manner. The project page is: https://nep-bigai.github.io/

</details>


### [36] [VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning](https://arxiv.org/abs/2508.06051)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Jun Jia,Kaiwei Zhang,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: VQAThinker是一个基于推理的视频质量评估框架，利用多模态模型和强化学习解决现有模型的泛化性和可解释性问题，通过三种奖励机制实现高质量评分和理解。


<details>
  <summary>Details</summary>
Motivation: 现有视频质量评估模型在泛化性和可解释性方面存在不足，限制了实际应用。

Method: 采用GRPO强化学习算法，结合三种奖励机制（回归、排序和时间一致性）模拟人类感知决策。

Result: VQAThinker在域内和域外基准测试中表现优异，同时在质量理解和解释性任务中优于现有模型。

Conclusion: 强化学习为构建仅需评分监督的泛化性和可解释性视频质量评估模型提供了有效途径。

Abstract: Video quality assessment (VQA) aims to objectively quantify perceptual
quality degradation in alignment with human visual perception. Despite recent
advances, existing VQA models still suffer from two critical limitations:
\textit{poor generalization to out-of-distribution (OOD) videos} and
\textit{limited explainability}, which restrict their applicability in
real-world scenarios. To address these challenges, we propose
\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large
multimodal models (LMMs) with reinforcement learning to jointly model video
quality understanding and scoring, emulating human perceptual decision-making.
Specifically, we adopt group relative policy optimization (GRPO), a rule-guided
reinforcement learning algorithm that enables reasoning over video quality
under score-level supervision, and introduce three VQA-specific rewards: (1) a
\textbf{bell-shaped regression reward} that increases rapidly as the prediction
error decreases and becomes progressively less sensitive near the ground truth;
(2) a \textbf{pairwise ranking reward} that guides the model to correctly
determine the relative quality between video pairs; and (3) a \textbf{temporal
consistency reward} that encourages the model to prefer temporally coherent
videos over their perturbed counterparts. Extensive experiments demonstrate
that VQAThinker achieves state-of-the-art performance on both in-domain and OOD
VQA benchmarks, showing strong generalization for video quality scoring.
Furthermore, evaluations on video quality understanding tasks validate its
superiority in distortion attribution and quality description compared to
existing explainable VQA models and LMMs. These findings demonstrate that
reinforcement learning offers an effective pathway toward building
generalizable and explainable VQA models solely with score-level supervision.

</details>


### [37] [AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?](https://arxiv.org/abs/2508.06057)
*Mojtaba Valipour,Kelly Zheng,James Lowman,Spencer Szabados,Mike Gartner,Bobby Braswell*

Main category: cs.CV

TL;DR: 论文呼吁关注卫星光谱图像作为AGI的新模态，提出需要更全面的基准来评估地球观测模型。


<details>
  <summary>Details</summary>
Motivation: 卫星光谱图像作为AGI的新模态潜力巨大，但现有基准不足以评估其泛化能力。

Method: 论证地球观测数据的价值，回顾现有基准的不足，并提出一套全面的任务集作为新基准。

Result: 强调卫星光谱图像的重要性，并指出现有基准的局限性。

Conclusion: 需要更全面的基准来推动地球观测模型的发展，以提升AGI对自然世界的理解能力。

Abstract: Artificial General Intelligence (AGI) is closer than ever to becoming a
reality, sparking widespread enthusiasm in the research community to collect
and work with various modalities, including text, image, video, and audio.
Despite recent efforts, satellite spectral imagery, as an additional modality,
has yet to receive the attention it deserves. This area presents unique
challenges, but also holds great promise in advancing the capabilities of AGI
in understanding the natural world. In this paper, we argue why Earth
Observation data is useful for an intelligent model, and then we review
existing benchmarks and highlight their limitations in evaluating the
generalization ability of foundation models in this domain. This paper
emphasizes the need for a more comprehensive benchmark to evaluate earth
observation models. To facilitate this, we propose a comprehensive set of tasks
that a benchmark should encompass to effectively assess a model's ability to
understand and interact with Earth observation data.

</details>


### [38] [Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention](https://arxiv.org/abs/2508.06058)
*Shiyang Zhou,Haijin Zeng,Yunfan Lu,Yongyong Chen,Jie Liu,Jingyong Su*

Main category: cs.CV

TL;DR: TSANet是一个轻量级的两阶段网络，通过状态空间增强的交叉注意力，分别处理事件像素修复和去马赛克问题，显著提升了HybridEVS相机的图像质量，同时降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: HybridEVS相机结合Quad Bayer CFA传感器和事件像素时，缺乏颜色信息导致去马赛克过程中出现伪影和混叠问题，现有方法难以在资源有限的移动设备上有效解决。

Method: 提出TSANet，采用两阶段网络设计，分别处理事件像素修复和去马赛克，并引入轻量级的Cross-Swin State Block，利用位置先验和状态空间模型增强全局依赖。

Result: 在模拟和真实HybridEVS数据上，TSANet在PSNR和SSIM指标上优于现有方法DemosaicFormer，同时参数和计算成本分别降低1.86倍和3.29倍。

Conclusion: TSANet为移动设备上的高效图像去马赛克提供了新思路，代码已开源。

Abstract: Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera
capture brightness changes as asynchronous "events" instead of frames, offering
advanced application on mobile photography. However, challenges arise from
combining a Quad Bayer Color Filter Array (CFA) sensor with event pixels
lacking color information, resulting in aliasing and artifacts on the
demosaicing process before downstream application. Current methods struggle to
address these issues, especially on resource-limited mobile devices. In
response, we introduce \textbf{TSANet}, a lightweight \textbf{T}wo-stage
network via \textbf{S}tate space augmented cross-\textbf{A}ttention, which can
handle event pixels inpainting and demosaicing separately, leveraging the
benefits of dividing complex tasks into manageable subtasks. Furthermore, we
introduce a lightweight Cross-Swin State Block that uniquely utilizes
positional prior for demosaicing and enhances global dependencies through the
state space model with linear complexity. In summary, TSANet demonstrates
excellent demosaicing performance on both simulated and real data of HybridEVS
while maintaining a lightweight model, averaging better results than the
previous state-of-the-art method DemosaicFormer across seven diverse datasets
in both PSNR and SSIM, while respectively reducing parameter and computation
costs by $1.86\times$ and $3.29\times$. Our approach presents new possibilities
for efficient image demosaicing on mobile devices. Code is available in the
supplementary materials.

</details>


### [39] [Distribution-Specific Learning for Joint Salient and Camouflaged Object Detection](https://arxiv.org/abs/2508.06063)
*Chao Hao,Zitong Yu,Xin Liu,Yuhao Wang,Weicheng Xie,Jingang Shi,Huanjing Yue,Jingyu Yang*

Main category: cs.CV

TL;DR: 论文提出SCJoint联合学习方案，通过最小化任务特定参数实现SOD和COD任务的联合学习，并引入SBSS策略优化训练集，最终训练出能同时处理显著和伪装物体的JoNet网络。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为SOD和COD任务的联合学习会降低网络性能，但作者提出相反观点，认为通过正确方法可以实现双任务受益。

Method: 提出SCJoint方案，在共享网络结构中插入少量任务特定参数，学习各自解码过程的分布特性；引入SBSS策略平衡训练集。

Result: 实验证明JoNet在SOD和COD任务上具有竞争性表现。

Conclusion: SCJoint和SBSS有效实现了SOD和COD的联合学习，JoNet展示了双任务处理的强大能力。

Abstract: Salient object detection (SOD) and camouflaged object detection (COD) are two
closely related but distinct computer vision tasks. Although both are
class-agnostic segmentation tasks that map from RGB space to binary space, the
former aims to identify the most salient objects in the image, while the latter
focuses on detecting perfectly camouflaged objects that blend into the
background in the image. These two tasks exhibit strong contradictory
attributes. Previous works have mostly believed that joint learning of these
two tasks would confuse the network, reducing its performance on both tasks.
However, here we present an opposite perspective: with the correct approach to
learning, the network can simultaneously possess the capability to find both
salient and camouflaged objects, allowing both tasks to benefit from joint
learning. We propose SCJoint, a joint learning scheme for SOD and COD tasks,
assuming that the decoding processes of SOD and COD have different distribution
characteristics. The key to our method is to learn the respective means and
variances of the decoding processes for both tasks by inserting a minimal
amount of task-specific learnable parameters within a fully shared network
structure, thereby decoupling the contradictory attributes of the two tasks at
a minimal cost. Furthermore, we propose a saliency-based sampling strategy
(SBSS) to sample the training set of the SOD task to balance the training set
sizes of the two tasks. In addition, SBSS improves the training set quality and
shortens the training time. Based on the proposed SCJoint and SBSS, we train a
powerful generalist network, named JoNet, which has the ability to
simultaneously capture both ``salient" and ``camouflaged". Extensive
experiments demonstrate the competitive performance and effectiveness of our
proposed method. The code is available at https://github.com/linuxsino/JoNet.

</details>


### [40] [Can Large Models Fool the Eye? A New Turing Test for Biological Animation](https://arxiv.org/abs/2508.06072)
*Zijian Chen,Lirong Deng,Zhengyu Chen,Kaiwei Zhang,Qi Jia,Yuan Tian,Yucheng Zhu,Guangtao Zhai*

Main category: cs.CV

TL;DR: BioMotion Arena 是一个通过视觉动画评估大型语言模型（LLMs）和多模态大型语言模型（MLLMs）的新框架，利用点光源成像放大模型间的性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法提供直观、即时的性能反馈，BioMotion Arena 旨在填补这一空白。

Method: 采用成对比较评估，收集了超过45k票对53个主流LLMs和MLLMs在90种生物运动变体上的表现。

Result: 数据显示众包人类投票与专家评分高度一致，且90%以上模型无法生成基本的人形点光源组。

Conclusion: BioMotion Arena 是一个具有挑战性的性能可视化基准，无需依赖真实数据。

Abstract: Evaluating the abilities of large models and manifesting their gaps are
challenging. Current benchmarks adopt either ground-truth-based score-form
evaluation on static datasets or indistinct textual chatbot-style human
preferences collection, which may not provide users with immediate, intuitive,
and perceptible feedback on performance differences. In this paper, we
introduce BioMotion Arena, a novel framework for evaluating large language
models (LLMs) and multimodal large language models (MLLMs) via visual
animation. Our methodology draws inspiration from the inherent visual
perception of motion patterns characteristic of living organisms that utilizes
point-light source imaging to amplify the performance discrepancies between
models. Specifically, we employ a pairwise comparison evaluation and collect
more than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion
variants. Data analyses show that the crowd-sourced human votes are in good
agreement with those of expert raters, demonstrating the superiority of our
BioMotion Arena in offering discriminative feedback. We also find that over
90\% of evaluated models, including the cutting-edge open-source InternVL3 and
proprietary Claude-4 series, fail to produce fundamental humanoid point-light
groups, much less smooth and biologically plausible motions. This enables
BioMotion Arena to serve as a challenging benchmark for performance
visualization and a flexible evaluation framework without restrictions on
ground-truth.

</details>


### [41] [Towards MR-Based Trochleoplasty Planning](https://arxiv.org/abs/2508.06076)
*Michael Wehrli,Alicia Durrer,Paul Friedrich,Sidaty El Hadramy,Edwin Li,Luana Brahaj,Carol C. Hasler,Philippe C. Cattin*

Main category: cs.CV

TL;DR: 提出了一种基于临床MR扫描生成超分辨率3D伪健康目标形态的管道，用于治疗滑车发育不良（TD），显著改善了手术效果。


<details>
  <summary>Details</summary>
Motivation: 当前治疗TD的方法依赖低分辨率MR扫描和外科医生的经验，导致手术效果不一致且微创技术应用有限。

Method: 使用隐式神经表示（INR）生成超分辨率MR体积，多标签网络分割骨骼，小波扩散模型（WDM）生成伪健康目标形态。

Result: 在25例TD患者中验证，显著改善了滑车角度（SA）和滑车沟深度（TGD）。

Conclusion: 该方法无需CT扫描，减少了辐射，生成的3D形态可作为术前蓝图，优化手术效果。

Abstract: To treat Trochlear Dysplasia (TD), current approaches rely mainly on
low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition.
The surgeries are planned based on surgeons experience, have limited adoption
of minimally invasive techniques, and lead to inconsistent outcomes. We propose
a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy
target morphologies from conventional clinical MR scans. First, we compute an
isotropic super-resolved MR volume using an Implicit Neural Representation
(INR). Next, we segment femur, tibia, patella, and fibula with a multi-label
custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to
generate pseudo-healthy target morphologies of the trochlear region. In
contrast to prior work producing pseudo-healthy low-resolution 3D MR images,
our approach enables the generation of sub-millimeter resolved 3D shapes
compatible for pre- and intraoperative use. These can serve as preoperative
blueprints for reshaping the femoral groove while preserving the native patella
articulation. Furthermore, and in contrast to other work, we do not require a
CT for our pipeline - reducing the amount of radiation. We evaluated our
approach on 25 TD patients and could show that our target morphologies
significantly improve the sulcus angle (SA) and trochlear groove depth (TGD).
The code and interactive visualization are available at
https://wehrlimi.github.io/sr-3d-planning/.

</details>


### [42] [DreamVE: Unified Instruction-based Image and Video Editing](https://arxiv.org/abs/2508.06080)
*Bin Xia,Jiyang Liu,Yuechen Zhang,Bohao Peng,Ruihang Chu,Yitong Wang,Xinglong Wu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: DreamVE是一个基于指令的图像和视频编辑统一模型，通过两阶段训练策略（先图像后视频）和数据合成方法（拼贴和生成模型）提升性能。


<details>
  <summary>Details</summary>
Motivation: 基于指令的编辑因交互简单高效而潜力巨大，但视频编辑因训练数据有限而受限。

Method: 提出两阶段训练策略（先图像后视频），结合拼贴和生成模型的数据合成方法，并设计高效编辑框架。

Result: DreamVE在关键编辑类型中表现优异，拼贴数据增强泛化能力，生成模型数据补充属性编辑。

Conclusion: DreamVE通过统一框架和多样化数据合成，显著提升指令编辑的实用性和性能。

Abstract: Instruction-based editing holds vast potential due to its simple and
efficient interactive editing format. However, instruction-based editing,
particularly for video, has been constrained by limited training data,
hindering its practical application. To this end, we introduce DreamVE, a
unified model for instruction-based image and video editing. Specifically, We
propose a two-stage training strategy: first image editing, then video editing.
This offers two main benefits: (1) Image data scales more easily, and models
are more efficient to train, providing useful priors for faster and better
video editing training. (2) Unifying image and video generation is natural and
aligns with current trends. Moreover, we present comprehensive training data
synthesis pipelines, including collage-based and generative model-based data
synthesis. The collage-based data synthesis combines foreground objects and
backgrounds to generate diverse editing data, such as object manipulation,
background changes, and text modifications. It can easily generate billions of
accurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE
on extensive collage-based data to achieve strong performance in key editing
types and enhance generalization and transfer capabilities. However,
collage-based data lacks some attribute editing cases, leading to a relative
drop in performance. In contrast, the generative model-based pipeline, despite
being hard to scale up, offers flexibility in handling attribute editing cases.
Therefore, we use generative model-based data to further fine-tune DreamVE.
Besides, we design an efficient and powerful editing framework for DreamVE. We
build on the SOTA T2V model and use a token concatenation with early drop
approach to inject source image guidance, ensuring strong consistency and
editability. The codes and models will be released.

</details>


### [43] [SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment](https://arxiv.org/abs/2508.06082)
*Yanxiao Sun,Jiafu Wu,Yun Cao,Chengming Xu,Yabiao Wang,Weijian Cao,Donghao Luo,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: SwiftVideo是一种结合轨迹保持和分布匹配优势的统一蒸馏框架，显著减少视频生成的推理步骤。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散或流的视频生成模型需要多次迭代采样，计算开销大，而现有蒸馏方法在少步设置下性能下降或产生更多伪影。

Method: 提出连续时间一致性蒸馏确保ODE轨迹精确保持，并引入双视角对齐（分布对齐和轨迹对齐）。

Result: 在OpenVid-1M基准测试中，SwiftVideo在少步视频生成中显著优于现有方法。

Conclusion: SwiftVideo在保持高质量视频生成的同时，大幅减少推理步骤，解决了现有方法的局限性。

Abstract: Diffusion-based or flow-based models have achieved significant progress in
video synthesis but require multiple iterative sampling steps, which incurs
substantial computational overhead. While many distillation methods that are
solely based on trajectory-preserving or distribution-matching have been
developed to accelerate video generation models, these approaches often suffer
from performance breakdown or increased artifacts under few-step settings. To
address these limitations, we propose \textbf{\emph{SwiftVideo}}, a unified and
stable distillation framework that combines the advantages of
trajectory-preserving and distribution-matching strategies. Our approach
introduces continuous-time consistency distillation to ensure precise
preservation of ODE trajectories. Subsequently, we propose a dual-perspective
alignment that includes distribution alignment between synthetic and real data
along with trajectory alignment across different inference steps. Our method
maintains high-quality video generation while substantially reducing the number
of inference steps. Quantitative evaluations on the OpenVid-1M benchmark
demonstrate that our method significantly outperforms existing approaches in
few-step video generation.

</details>


### [44] [AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance](https://arxiv.org/abs/2508.06084)
*Weichen Zhang,Zhui Zhu,Ningbo Li,Kebin Liu,Yunhao Liu*

Main category: cs.CV

TL;DR: AdaptInfer是一个动态视觉令牌剪枝框架，通过复用文本-文本注意力图和离线分析跨模态注意力变化，显著降低推理成本，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLM）在推理过程中处理大量视觉令牌导致高计算成本，现有剪枝方法未能充分利用动态内部信号。

Method: 提出动态文本引导剪枝机制和基于跨模态注意力变化的剪枝调度策略。

Result: 在LLaVA-1.5-7B上，CUDA延迟降低61.3%，平均精度保持92.9%。

Conclusion: AdaptInfer是一种轻量级、即插即用的高效剪枝方法，在多模态任务中表现优异。

Abstract: Vision-language models (VLMs) have achieved impressive performance on
multimodal reasoning tasks such as visual question answering (VQA), but their
inference cost remains a significant challenge due to the large number of
vision tokens processed during the prefill stage. Existing pruning methods
often rely on directly using the attention patterns or static text prompt
guidance, failing to exploit the dynamic internal signals generated during
inference. To address these issues, we propose AdaptInfer, a plug-and-play
framework for adaptive vision token pruning in VLMs. First, we introduce a
fine-grained, dynamic text-guided pruning mechanism that reuses layer-wise
text-to-text attention maps to construct soft priors over text-token
importance, allowing more informed scoring of vision tokens at each stage.
Second, we perform an offline analysis of cross-modal attention shifts and
identify consistent inflection locations in inference, which inspire us to
propose a more principled and efficient pruning schedule. Our method is
lightweight and plug-and-play, also generalizable across multi-modal tasks.
Experimental results have verified the effectiveness of the proposed method.
For example, it reduces CUDA latency by 61.3\% while maintaining an average
accuracy of 92.9\% on vanilla LLaVA-1.5-7B. Under the same token budget,
AdaptInfer surpasses SOTA in accuracy.

</details>


### [45] [Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation](https://arxiv.org/abs/2508.06092)
*Yachun Mi,Yu Li,Yanting Li,Shixin Sun,Chen Hui,Tong Zhang,Yuanyuan Liu,Chenyue Song,Shaohui Liu*

Main category: cs.CV

TL;DR: Q-CLIP是一种基于视觉语言模型（VLM）的视频质量评估框架，通过共享跨模态适配器（SCMA）和可学习质量提示提升性能，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前视频质量评估方法依赖大规模预训练，计算成本高且语义知识迁移不足，Q-CLIP旨在解决这些问题。

Method: Q-CLIP利用SCMA增强视觉和文本表示，引入可学习质量提示，并研究帧采样策略对性能的影响。

Result: 实验表明Q-CLIP在多个VQA数据集上表现优异。

Conclusion: Q-CLIP为视频质量评估提供了一种高效且高性能的解决方案。

Abstract: Accurate and efficient Video Quality Assessment (VQA) has long been a key
research challenge. Current mainstream VQA methods typically improve
performance by pretraining on large-scale classification datasets (e.g.,
ImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this
strategy presents two significant challenges: (1) merely transferring semantic
knowledge learned from pretraining is insufficient for VQA, as video quality
depends on multiple factors (e.g., semantics, distortion, motion, aesthetics);
(2) pretraining on large-scale datasets demands enormous computational
resources, often dozens or even hundreds of times greater than training
directly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown
remarkable generalization capabilities across a wide range of visual tasks, and
have begun to demonstrate promising potential in quality assessment. In this
work, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP
enhances both visual and textual representations through a Shared Cross-Modal
Adapter (SCMA), which contains only a minimal number of trainable parameters
and is the only component that requires training. This design significantly
reduces computational cost. In addition, we introduce a set of five learnable
quality-level prompts to guide the VLMs in perceiving subtle quality
variations, thereby further enhancing the model's sensitivity to video quality.
Furthermore, we investigate the impact of different frame sampling strategies
on VQA performance, and find that frame-difference-based sampling leads to
better generalization performance across datasets. Extensive experiments
demonstrate that Q-CLIP exhibits excellent performance on several VQA datasets.

</details>


### [46] [E-React: Towards Emotionally Controlled Synthesis of Human Reactions](https://arxiv.org/abs/2508.06093)
*Chen Zhu,Buzhen Huang,Zijing Wu,Binghui Zuo,Yangang Wang*

Main category: cs.CV

TL;DR: 提出了一种基于情感驱动的反应动作生成方法，通过半监督情感先验和扩散模型，提升了动作生成的自然性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有动作生成框架未考虑情感影响，导致生成动作不够自然，限制了在交互任务中的应用。

Method: 采用半监督学习框架训练情感先验，并结合扩散模型生成反应动作，同时考虑空间交互和情感响应。

Result: 实验表明，该方法在反应动作生成任务中优于现有方法。

Conclusion: 该方法能够生成多样且真实的反应动作，适用于不同情感条件下的交互任务。

Abstract: Emotion serves as an essential component in daily human interactions.
Existing human motion generation frameworks do not consider the impact of
emotions, which reduces naturalness and limits their application in interactive
tasks, such as human reaction synthesis. In this work, we introduce a novel
task: generating diverse reaction motions in response to different emotional
cues. However, learning emotion representation from limited motion data and
incorporating it into a motion generation framework remains a challenging
problem. To address the above obstacles, we introduce a semi-supervised emotion
prior in an actor-reactor diffusion model to facilitate emotion-driven reaction
synthesis. Specifically, based on the observation that motion clips within a
short sequence tend to share the same emotion, we first devise a
semi-supervised learning framework to train an emotion prior. With this prior,
we further train an actor-reactor diffusion model to generate reactions by
considering both spatial interaction and emotional response. Finally, given a
motion sequence of an actor, our approach can generate realistic reactions
under various emotional conditions. Experimental results demonstrate that our
model outperforms existing reaction generation methods. The code and data will
be made publicly available at https://ereact.github.io/

</details>


### [47] [UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization](https://arxiv.org/abs/2508.06101)
*Yachun Mi,Xingyang He,Shixin Sun,Yu Li,Yanting Li,Zhixuan Li,Jian Jin,Chen Hui,Shaohui Liu*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的生成框架UGD-IML，首次将IML和CIML任务统一在一个框架中，减少了对大规模标注数据的依赖，并在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 数字时代中，高级图像编辑工具威胁视觉内容的真实性，现有IML方法依赖大规模标注数据，但数据集缺乏规模和多样性，限制了模型性能。

Method: 提出UGD-IML框架，利用扩散模型学习数据分布，减少对标注数据的依赖；通过类嵌入机制和参数共享设计，实现IML和CIML任务的无缝切换。

Result: 在多个数据集上，UGD-IML在IML和CIML任务中的F1指标分别平均优于SOTA方法9.66和4.36。

Conclusion: UGD-IML框架在性能、不确定估计、可视化和鲁棒性方面表现优异，为图像篡改定位提供了高效解决方案。

Abstract: In the digital age, advanced image editing tools pose a serious threat to the
integrity of visual content, making image forgery detection and localization a
key research focus. Most existing Image Manipulation Localization (IML) methods
rely on discriminative learning and require large, high-quality annotated
datasets. However, current datasets lack sufficient scale and diversity,
limiting model performance in real-world scenarios. To overcome this, recent
studies have explored Constrained IML (CIML), which generates pixel-level
annotations through algorithmic supervision. However, existing CIML approaches
often depend on complex multi-stage pipelines, making the annotation process
inefficient. In this work, we propose a novel generative framework based on
diffusion models, named UGD-IML, which for the first time unifies both IML and
CIML tasks within a single framework. By learning the underlying data
distribution, generative diffusion models inherently reduce the reliance on
large-scale labeled datasets, allowing our approach to perform effectively even
under limited data conditions. In addition, by leveraging a class embedding
mechanism and a parameter-sharing design, our model seamlessly switches between
IML and CIML modes without extra components or training overhead. Furthermore,
the end-to-end design enables our model to avoid cumbersome steps in the data
annotation process. Extensive experimental results on multiple datasets
demonstrate that UGD-IML outperforms the SOTA methods by an average of 9.66 and
4.36 in terms of F1 metrics for IML and CIML tasks, respectively. Moreover, the
proposed method also excels in uncertainty estimation, visualization and
robustness.

</details>


### [48] [MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment](https://arxiv.org/abs/2508.06104)
*Gui Zou,Chaofan Gan,Chern Hong Lim,Supavadee Aramvith,Weiyao Lin*

Main category: cs.CV

TL;DR: 提出了一种名为MCA的鲁棒2D-3D跨模态检索框架，通过多模态联合标签校正和多层次自适应对齐策略，解决了噪声标签条件下的检索问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理噪声标签时容易过拟合，且缺乏跨模态一致性，需要更鲁棒的解决方案。

Method: 提出MCA框架，包含多模态联合标签校正（MJC）和多层次自适应对齐（MAA）策略。

Result: 在传统和噪声3D基准测试中，MCA表现出色，达到最先进性能。

Conclusion: MCA框架在噪声标签条件下具有鲁棒性和有效性，适用于跨模态检索任务。

Abstract: With the increasing availability of 2D and 3D data, significant advancements
have been made in the field of cross-modal retrieval. Nevertheless, the
existence of imperfect annotations presents considerable challenges, demanding
robust solutions for 2D-3D cross-modal retrieval in the presence of noisy label
conditions. Existing methods generally address the issue of noise by dividing
samples independently within each modality, making them susceptible to
overfitting on corrupted labels. To address these issues, we propose a robust
2D-3D \textbf{M}ulti-level cross-modal adaptive \textbf{C}orrection and
\textbf{A}lignment framework (MCA). Specifically, we introduce a Multimodal
Joint label Correction (MJC) mechanism that leverages multimodal historical
self-predictions to jointly model the modality prediction consistency, enabling
reliable label refinement. Additionally, we propose a Multi-level Adaptive
Alignment (MAA) strategy to effectively enhance cross-modal feature semantics
and discrimination across different levels. Extensive experiments demonstrate
the superiority of our method, MCA, which achieves state-of-the-art performance
on both conventional and realistic noisy 3D benchmarks, highlighting its
generality and effectiveness.

</details>


### [49] [Mask & Match: Learning to Recognize Handwritten Math with Self-Supervised Attention](https://arxiv.org/abs/2508.06107)
*Shree Mitra,Ritabrata Chakraborty,Nilkanta Sahu*

Main category: cs.CV

TL;DR: 提出了一种自监督学习框架，用于手写数学表达式识别，无需昂贵标注数据，通过渐进式空间掩码策略训练注意力网络，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 手写数学表达式识别因二维结构、符号尺度变化和复杂空间关系而具有挑战性，现有方法依赖标注数据，成本高。

Method: 结合全局和局部对比损失预训练图像编码器，设计自监督注意力网络，采用渐进空间掩码策略训练注意力机制，最后通过监督微调生成LaTeX序列。

Result: 在CROHME基准测试中，方法优于现有自监督和全监督基线，验证了渐进注意力机制的有效性。

Conclusion: 自监督学习框架和渐进注意力机制显著提升了手写数学表达式识别的性能，为减少标注依赖提供了有效解决方案。

Abstract: Recognizing handwritten mathematical expressions (HMER) is a challenging task
due to the inherent two-dimensional structure, varying symbol scales, and
complex spatial relationships among symbols. In this paper, we present a
self-supervised learning (SSL) framework for HMER that eliminates the need for
expensive labeled data. Our approach begins by pretraining an image encoder
using a combination of global and local contrastive loss, enabling the model to
learn both holistic and fine-grained representations. A key contribution of
this work is a novel self-supervised attention network, which is trained using
a progressive spatial masking strategy. This attention mechanism is designed to
learn semantically meaningful focus regions, such as operators, exponents, and
nested mathematical notation, without requiring any supervision. The
progressive masking curriculum encourages the network to become increasingly
robust to missing or occluded visual information, ultimately improving
structural understanding. Our complete pipeline consists of (1) self-supervised
pretraining of the encoder, (2) self-supervised attention learning, and (3)
supervised fine-tuning with a transformer decoder to generate LATEX sequences.
Extensive experiments on CROHME benchmarks demonstrate that our method
outperforms existing SSL and fully supervised baselines, validating the
effectiveness of our progressive attention mechanism in enhancing HMER
performance. Our codebase can be found here.

</details>


### [50] [FMCE-Net++: Feature Map Convergence Evaluation and Training](https://arxiv.org/abs/2508.06109)
*Zhibo Zhu,Renyu Huang,Lei He*

Main category: cs.CV

TL;DR: FMCE-Net++ 是一种新的训练框架，通过整合预训练的 FMCE-Net 作为辅助头，动态平衡分类损失和特征收敛优化，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络（DNNs）内部表示不透明的问题，改进 Feature Map Convergence Evaluation（FMCE）缺乏实验验证和闭环集成的局限性。

Method: 提出 FMCE-Net++，整合预训练的 FMCE-Net 作为辅助头，生成 FMCS 预测，结合任务标签通过 Representation Auxiliary Loss（RAL）动态优化。

Result: 在 MNIST、CIFAR-10 等数据集上实验，FMCE-Net++ 显著提升模型性能（如 ResNet-50/CIFAR-10 准确率提升 1.16 pp）。

Conclusion: FMCE-Net++ 无需修改架构或增加数据，即可有效提升模型性能，验证了其作为辅助训练框架的潜力。

Abstract: Deep Neural Networks (DNNs) face interpretability challenges due to their
opaque internal representations. While Feature Map Convergence Evaluation
(FMCE) quantifies module-level convergence via Feature Map Convergence Scores
(FMCS), it lacks experimental validation and closed-loop integration. To
address this limitation, we propose FMCE-Net++, a novel training framework that
integrates a pretrained, frozen FMCE-Net as an auxiliary head. This module
generates FMCS predictions, which, combined with task labels, jointly supervise
backbone optimization through a Representation Auxiliary Loss. The RAL
dynamically balances the primary classification loss and feature convergence
optimization via a tunable \Representation Abstraction Factor. Extensive
experiments conducted on MNIST, CIFAR-10, FashionMNIST, and CIFAR-100
demonstrate that FMCE-Net++ consistently enhances model performance without
architectural modifications or additional data. Key experimental outcomes
include accuracy gains of $+1.16$ pp (ResNet-50/CIFAR-10) and $+1.08$ pp
(ShuffleNet v2/CIFAR-100), validating that FMCE-Net++ can effectively elevate
state-of-the-art performance ceilings.

</details>


### [51] [GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.06113)
*Jian Wang,Chaokang Jiang,Haitao Xu*

Main category: cs.CV

TL;DR: GMF-Drive提出了一种基于门控Mamba融合的端到端自动驾驶框架，通过几何增强的LiDAR表示和高效的空间感知状态空间模型（SSM）替代传统Transformer，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的自动驾驶方法受限于Transformer的二次计算复杂性和缺乏空间先验，无法有效处理高分辨率特征和BEV表示的结构。

Method: 1. 使用几何增强的LiDAR表示（形状描述符和统计特征）；2. 提出分层门控Mamba融合（GM-Fusion）架构，用高效的SSM替代Transformer。

Result: 在NAVSIM基准测试中，GMF-Drive性能显著优于DiffusionDrive，达到新SOTA。

Conclusion: 任务特定的SSM在性能和效率上均优于通用Transformer，为自动驾驶提供了更优的解决方案。

Abstract: Diffusion-based models are redefining the state-of-the-art in end-to-end
autonomous driving, yet their performance is increasingly hampered by a
reliance on transformer-based fusion. These architectures face fundamental
limitations: quadratic computational complexity restricts the use of
high-resolution features, and a lack of spatial priors prevents them from
effectively modeling the inherent structure of Bird's Eye View (BEV)
representations. This paper introduces GMF-Drive (Gated Mamba Fusion for
Driving), an end-to-end framework that overcomes these challenges through two
principled innovations. First, we supersede the information-limited
histogram-based LiDAR representation with a geometrically-augmented pillar
format encoding shape descriptors and statistical features, preserving critical
3D geometric details. Second, we propose a novel hierarchical gated mamba
fusion (GM-Fusion) architecture that substitutes an expensive transformer with
a highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM
leverages directional sequencing and adaptive fusion mechanisms to capture
long-range dependencies with linear complexity, while explicitly respecting the
unique spatial properties of the driving scene. Extensive experiments on the
challenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new
state-of-the-art performance, significantly outperforming DiffusionDrive.
Comprehensive ablation studies validate the efficacy of each component,
demonstrating that task-specific SSMs can surpass a general-purpose transformer
in both performance and efficiency for autonomous driving.

</details>


### [52] [SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2508.06115)
*Weichen Zhang,Kebin Liu,Fan Dang,Zhui Zhu,Xikai Sun,Yunhao Liu*

Main category: cs.CV

TL;DR: SynSeg提出了一种新的弱监督方法，通过多类别对比学习（MCCL）和特征协同结构（FSS）解决开放词汇语义分割的挑战，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 开放词汇语义分割面临语义类别广泛且细粒度的挑战，现有弱监督方法依赖类别特定监督和不适合的特征构建方法，导致语义对齐差和性能低下。

Method: SynSeg采用多类别对比学习（MCCL）和特征协同结构（FSS）。MCCL结合了类别内和类别间的对齐与分离，FSS通过先验融合和语义激活图增强重构特征。

Result: 在多个基准测试中，SynSeg表现优于现有方法，如在VOC上准确率提升4.5%，Context上提升8.9%。

Conclusion: SynSeg通过MCCL和FSS有效提升了弱监督下的语义定位和区分能力，显著优于现有方法。

Abstract: Semantic segmentation in open-vocabulary scenarios presents significant
challenges due to the wide range and granularity of semantic categories.
Existing weakly-supervised methods often rely on category-specific supervision
and ill-suited feature construction methods for contrastive learning, leading
to semantic misalignment and poor performance. In this work, we propose a novel
weakly-supervised approach, SynSeg, to address the challenges. SynSeg performs
Multi-Category Contrastive Learning (MCCL) as a stronger training signal with a
new feature reconstruction framework named Feature Synergy Structure (FSS).
Specifically, MCCL strategy robustly combines both intra- and inter-category
alignment and separation in order to make the model learn the knowledge of
correlations from different categories within the same image. Moreover, FSS
reconstructs discriminative features for contrastive learning through prior
fusion and semantic-activation-map enhancement, effectively avoiding the
foreground bias introduced by the visual encoder. In general, SynSeg
effectively improves the abilities in semantic localization and discrimination
under weak supervision. Extensive experiments on benchmarks demonstrate that
our method outperforms state-of-the-art (SOTA) performance. For instance,
SynSeg achieves higher accuracy than SOTA baselines by 4.5\% on VOC, 8.9\% on
Context, 2.6\% on Object and 2.0\% on City.

</details>


### [53] [Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events](https://arxiv.org/abs/2508.06122)
*Ting-Shuo Yo,Shih-Hao Su,Chien-Ming Wu,Wei-Ting Chen,Jung-Lien Chu,Chiao-Wei Chang,Hung-Chi Kuo*

Main category: cs.CV

TL;DR: 论文研究了卫星图像表示学习算法，比较了PCA、CAE和预训练残差网络（PT）在天气事件分类中的表现，发现CAE表现最佳，但缺乏物理解释性。


<details>
  <summary>Details</summary>
Motivation: 探索不同表示学习算法在卫星图像天气事件分类中的效果，并评估其潜在空间的性能。

Method: 使用PCA、CAE和PT三种算法学习卫星图像的潜在空间，并通过分类任务评估其效果。

Result: CAE在所有分类任务中表现最佳，PCA命中率高但误报率高，PT在热带气旋识别中表现突出。高分辨率数据集对深度学习算法更有利。

Conclusion: CAE高效但缺乏物理解释性，未来可开发物理信息版本的CAE。

Abstract: This study applied representation learning algorithms to satellite images and
evaluated the learned latent spaces with classifications of various weather
events. The algorithms investigated include the classical linear
transformation, i.e., principal component analysis (PCA), state-of-the-art deep
learning method, i.e., convolutional autoencoder (CAE), and a residual network
pre-trained with large image datasets (PT). The experiment results indicated
that the latent space learned by CAE consistently showed higher threat scores
for all classification tasks. The classifications with PCA yielded high hit
rates but also high false-alarm rates. In addition, the PT performed
exceptionally well at recognizing tropical cyclones but was inferior in other
tasks. Further experiments suggested that representations learned from
higher-resolution datasets are superior in all classification tasks for
deep-learning algorithms, i.e., CAE and PT. We also found that smaller latent
space sizes had minor impact on the classification task's hit rate. Still, a
latent space dimension smaller than 128 caused a significantly higher false
alarm rate. Though the CAE can learn latent spaces effectively and efficiently,
the interpretation of the learned representation lacks direct connections to
physical attributions. Therefore, developing a physics-informed version of CAE
can be a promising outlook for the current work.

</details>


### [54] [SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning](https://arxiv.org/abs/2508.06125)
*Lin Zhang,Xianfang Zeng,Kangcong Li,Gang Yu,Tao Chen*

Main category: cs.CV

TL;DR: SC-Captioner是一个通过强化学习框架实现图像字幕模型自我纠正能力的系统，通过设计奖励函数激励准确修正，显著提升字幕质量。


<details>
  <summary>Details</summary>
Motivation: 现有图像字幕模型缺乏自我纠正能力，导致字幕质量受限，需要一种能够动态修正错误的方法。

Method: 利用场景图解析算法分解字幕为对象、属性和关系集合，通过集合差异计算奖励函数，激励准确修正并惩罚错误。

Result: 实验表明，SC-Captioner在多样化场景中生成的字幕质量显著优于直接偏好优化训练策略。

Conclusion: SC-Captioner通过强化学习和精细奖励设计，有效提升了图像字幕模型的自我纠正能力和生成质量。

Abstract: We propose SC-Captioner, a reinforcement learning framework that enables the
self-correcting capability of image caption models. Our crucial technique lies
in the design of the reward function to incentivize accurate caption
corrections. Specifically, the predicted and reference captions are decomposed
into object, attribute, and relation sets using scene-graph parsing algorithms.
We calculate the set difference between sets of initial and self-corrected
captions to identify added and removed elements. These elements are matched
against the reference sets to calculate correctness bonuses for accurate
refinements and mistake punishments for wrong additions and removals, thereby
forming the final reward. For image caption quality assessment, we propose a
set of metrics refined from CAPTURE that alleviate its incomplete precision
evaluation and inefficient relation matching problems. Furthermore, we collect
a fine-grained annotated image caption dataset, RefinedCaps, consisting of 6.5K
diverse images from COCO dataset. Experiments show that applying SC-Captioner
on large visual-language models can generate better image captions across
various scenarios, significantly outperforming the direct preference
optimization training strategy.

</details>


### [55] [SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures](https://arxiv.org/abs/2508.06127)
*Yi Qin,Rui Wang,Tao Huang,Tong Xiao,Liping Jing*

Main category: cs.CV

TL;DR: 论文提出VeSCA方法，通过利用SAM编码器生成可迁移对抗样本，评估其潜在漏洞。


<details>
  <summary>Details</summary>
Motivation: 评估Segment Anything Model（SAM）的潜在漏洞，以避免下游应用失败。

Method: 提出Vertex-Refining Simplicial Complex Attack（VeSCA），通过参数化单纯复形表征共享脆弱区域，并迭代优化顶点。

Result: VeSCA在五个领域数据集上性能提升12.7%，优于现有方法。

Conclusion: SAM的漏洞对下游模型构成风险，需开发更鲁棒的基础模型。

Abstract: While the Segment Anything Model (SAM) transforms interactive segmentation
with zero-shot abilities, its inherent vulnerabilities present a single-point
risk, potentially leading to the failure of numerous downstream applications.
Proactively evaluating these transferable vulnerabilities is thus imperative.
Prior adversarial attacks on SAM often present limited transferability due to
insufficient exploration of common weakness across domains. To address this, we
propose Vertex-Refining Simplicial Complex Attack (VeSCA), a novel method that
leverages only the encoder of SAM for generating transferable adversarial
examples. Specifically, it achieves this by explicitly characterizing the
shared vulnerable regions between SAM and downstream models through a
parametric simplicial complex. Our goal is to identify such complexes within
adversarially potent regions by iterative vertex-wise refinement. A lightweight
domain re-adaptation strategy is introduced to bridge domain divergence using
minimal reference data during the initialization of simplicial complex.
Ultimately, VeSCA generates consistently transferable adversarial examples
through random simplicial complex sampling. Extensive experiments demonstrate
that VeSCA achieves performance improved by 12.7% compared to state-of-the-art
methods across three downstream model categories across five domain-specific
datasets. Our findings further highlight the downstream model risks posed by
SAM's vulnerabilities and emphasize the urgency of developing more robust
foundation models.

</details>


### [56] [Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation](https://arxiv.org/abs/2508.06136)
*YoungChan Choi,HengFei Wang,YiHua Cheng,Boeun Kim,Hyung Jin Chang,YoungGeun Choi,Sang-Il Choi*

Main category: cs.CV

TL;DR: 提出了一种基于3D眼球结构的视线重定向框架，优于现有基于神经辐射场的方法。


<details>
  <summary>Details</summary>
Motivation: 现有视线重定向方法通常基于神经辐射场，缺乏对3D眼球旋转和平移的显式建模。

Method: 引入3D高斯点渲染（3DGS）显式表示眼球结构，并通过旋转和平移生成逼真图像；提出自适应变形模块模拟眼部肌肉运动。

Result: 在ETH-XGaze数据集上验证，生成多样化视线图像，图像质量和视线估计精度优于现有方法。

Conclusion: 显式3D眼球结构和自适应变形模块显著提升了视线重定向的逼真度和准确性。

Abstract: We propose a novel 3D gaze redirection framework that leverages an explicit
3D eyeball structure. Existing gaze redirection methods are typically based on
neural radiance fields, which employ implicit neural representations via volume
rendering. Unlike these NeRF-based approaches, where the rotation and
translation of 3D representations are not explicitly modeled, we introduce a
dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian
Splatting (3DGS). Our method generates photorealistic images that faithfully
reproduce the desired gaze direction by explicitly rotating and translating the
3D eyeball structure. In addition, we propose an adaptive deformation module
that enables the replication of subtle muscle movements around the eyes.
Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our
framework is capable of generating diverse novel gaze images, achieving
superior image quality and gaze estimation accuracy compared to previous
state-of-the-art methods.

</details>


### [57] [DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera](https://arxiv.org/abs/2508.06139)
*Shaohua Pan,Xinyu Yi,Yan Zhou,Weihua Jian,Yuan Zhang,Pengfei Wan,Feng Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于扩散模型的方法，结合稀疏IMU和单目相机进行实时人体运动捕捉，通过融合两种信号模态并考虑其特性，实现了鲁棒且高性能的姿态估计。


<details>
  <summary>Details</summary>
Motivation: 结合稀疏IMU和单目相机进行实时人体运动捕捉是一个有前景的方向，但需要解决视觉信息偶尔缺失和IMU信号稳定性问题。

Method: 将视觉信息整体转化为条件嵌入，而IMU测量与噪声姿态逐帧拼接作为扩散模型的输入，充分利用两种信号的优势。

Result: 实验表明，该方法在姿态估计中表现出色，性能优于现有工作。

Conclusion: 该方法通过巧妙融合视觉和IMU信号，实现了鲁棒且高性能的运动捕捉，代码已开源。

Abstract: Combining sparse IMUs and a monocular camera is a new promising setting to
perform real-time human motion capture. This paper proposes a diffusion-based
solution to learn human motion priors and fuse the two modalities of signals
together seamlessly in a unified framework. By delicately considering the
characteristics of the two signals, the sequential visual information is
considered as a whole and transformed into a condition embedding, while the
inertial measurement is concatenated with the noisy body pose frame by frame to
construct a sequential input for the diffusion model. Firstly, we observe that
the visual information may be unavailable in some frames due to occlusions or
subjects moving out of the camera view. Thus incorporating the sequential
visual features as a whole to get a single feature embedding is robust to the
occasional degenerations of visual information in those frames. On the other
hand, the IMU measurements are robust to occlusions and always stable when
signal transmission has no problem. So incorporating them frame-wisely could
better explore the temporal information for the system. Experiments have
demonstrated the effectiveness of the system design and its state-of-the-art
performance in pose estimation compared with the previous works. Our codes are
available for research at https://shaohua-pan.github.io/diffcap-page.

</details>


### [58] [SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models](https://arxiv.org/abs/2508.06142)
*Hanqing Wang,Yuan Tian,Mingyu Liu,Zhenhao Zhang,Xiangyang Zhu*

Main category: cs.CV

TL;DR: SDEval是一个动态评估框架，通过调整安全基准的分布和复杂性来解决MLLM的安全问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLM）的安全问题日益突出，现有数据集易过时且存在数据污染。

Method: SDEval采用文本、图像和文本-图像动态策略生成新样本，研究其对模型安全的影响。

Result: 实验表明，SDEval显著影响安全评估，减轻数据污染，并揭示MLLM的安全局限性。

Conclusion: SDEval为MLLM安全评估提供了一种灵活且通用的解决方案。

Abstract: In the rapidly evolving landscape of Multimodal Large Language Models
(MLLMs), the safety concerns of their outputs have earned significant
attention. Although numerous datasets have been proposed, they may become
outdated with MLLM advancements and are susceptible to data contamination
issues. To address these problems, we propose \textbf{SDEval}, the
\textit{first} safety dynamic evaluation framework to controllably adjust the
distribution and complexity of safety benchmarks. Specifically, SDEval mainly
adopts three dynamic strategies: text, image, and text-image dynamics to
generate new samples from original benchmarks. We first explore the individual
effects of text and image dynamics on model safety. Then, we find that
injecting text dynamics into images can further impact safety, and conversely,
injecting image dynamics into text also leads to safety risks. SDEval is
general enough to be applied to various existing safety and even capability
benchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and
capability benchmarks, MMBench and MMVet, show that SDEval significantly
influences safety evaluation, mitigates data contamination, and exposes safety
limitations of MLLMs. Code is available at https://github.com/hq-King/SDEval

</details>


### [59] [Text-guided Visual Prompt DINO for Generic Segmentation](https://arxiv.org/abs/2508.06146)
*Yuchen Guan,Chong Sun,Canmiao Fu,Zhipeng Huang,Chun Yuan,Chen Li*

Main category: cs.CV

TL;DR: Prompt-DINO提出了一种文本引导的视觉Prompt DINO框架，通过早期融合机制、顺序对齐查询选择和生成数据引擎，解决了多模态视觉模型中的特征融合和查询选择问题。


<details>
  <summary>Details</summary>
Motivation: 解决多模态视觉模型中晚期特征融合的局限性、混合提示开放世界分割中的次优查询选择以及标题派生词汇的限制。

Method: 1. 早期融合机制统一文本/视觉提示和主干特征；2. 顺序对齐查询选择优化文本和视觉查询的结构对齐；3. 生成数据引擎通过双路径交叉验证减少标签噪声。

Result: 在开放世界检测基准上达到最先进性能，语义覆盖范围显著扩展。

Conclusion: Prompt-DINO为开放世界场景中的可扩展多模态检测和数据生成建立了新范式。

Abstract: Recent advancements in multimodal vision models have highlighted limitations
in late-stage feature fusion and suboptimal query selection for hybrid prompts
open-world segmentation, alongside constraints from caption-derived
vocabularies. To address these challenges, we propose Prompt-DINO, a
text-guided visual Prompt DINO framework featuring three key innovations.
First, we introduce an early fusion mechanism that unifies text/visual prompts
and backbone features at the initial encoding stage, enabling deeper
cross-modal interactions to resolve semantic ambiguities. Second, we design
order-aligned query selection for DETR-based architectures, explicitly
optimizing the structural alignment between text and visual queries during
decoding to enhance semantic-spatial consistency. Third, we develop a
generative data engine powered by the Recognize Anything via Prompting (RAP)
model, which synthesizes 0.5B diverse training instances through a dual-path
cross-verification pipeline, reducing label noise by 80.5% compared to
conventional approaches. Extensive experiments demonstrate that Prompt-DINO
achieves state-of-the-art performance on open-world detection benchmarks while
significantly expanding semantic coverage beyond fixed-vocabulary constraints.
Our work establishes a new paradigm for scalable multimodal detection and data
generation in open-world scenarios. Data&Code are available at
https://github.com/WeChatCV/WeVisionOne.

</details>


### [60] [DSConv: Dynamic Splitting Convolution for Pansharpening](https://arxiv.org/abs/2508.06147)
*Xuanyu Liu,Bonan An*

Main category: cs.CV

TL;DR: 提出了一种名为DSConv的动态分割卷积核方法，结合注意力机制提升图像融合性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多依赖标准卷积，而自适应卷积能更好地利用遥感图像的像素相关性。

Method: 动态分割卷积核（DSConv），结合注意力机制选择感兴趣位置，分割原始卷积核为多个小核。

Result: DSConv提升了特征提取能力，增强了网络的泛化、优化和特征表示能力。

Conclusion: DSConv在图像融合任务中表现出高效性和先进性能，实验验证了其优越性和适用条件。

Abstract: Aiming to obtain a high-resolution image, pansharpening involves the fusion
of a multi-spectral image (MS) and a panchromatic image (PAN), the low-level
vision task remaining significant and challenging in contemporary research.
Most existing approaches rely predominantly on standard convolutions, few
making the effort to adaptive convolutions, which are effective owing to the
inter-pixel correlations of remote sensing images. In this paper, we propose a
novel strategy for dynamically splitting convolution kernels in conjunction
with attention, selecting positions of interest, and splitting the original
convolution kernel into multiple smaller kernels, named DSConv. The proposed
DSConv more effectively extracts features of different positions within the
receptive field, enhancing the network's generalization, optimization, and
feature representation capabilities. Furthermore, we innovate and enrich
concepts of dynamic splitting convolution and provide a novel network
architecture for pansharpening capable of achieving the tasks more efficiently,
building upon this methodology. Adequate fair experiments illustrate the
effectiveness and the state-of-the-art performance attained by
DSConv.Comprehensive and rigorous discussions proved the superiority and
optimal usage conditions of DSConv.

</details>


### [61] [VISTAR:A User-Centric and Role-Driven Benchmark for Text-to-Image Evaluation](https://arxiv.org/abs/2508.06152)
*Kaiyuan Jiang,Ruoxi Sun,Ying Cao,Yuqi Xu,Xinran Zhang,Junyan Guo,ChengSheng Deng*

Main category: cs.CV

TL;DR: VISTAR是一个用户中心的多维度文本到图像（T2I）评估基准，结合确定性指标和新型HWPQ方案，显著提升评估准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有T2I评估指标的局限性，提供更全面的评估方法。

Method: 采用两层级混合范式：确定性指标量化物理属性，HWPQ方案评估抽象语义。基于专家研究定义用户角色和评估角度。

Result: 指标与人类对齐度高（>75%），HWPQ在抽象语义上达85.9%准确率，显著优于基线。评估显示无通用最优模型。

Conclusion: VISTAR为T2I评估提供可复现的基准，支持领域特定部署。所有资源已公开。

Abstract: We present VISTAR, a user-centric, multi-dimensional benchmark for
text-to-image (T2I) evaluation that addresses the limitations of existing
metrics. VISTAR introduces a two-tier hybrid paradigm: it employs
deterministic, scriptable metrics for physically quantifiable attributes (e.g.,
text rendering, lighting) and a novel Hierarchical Weighted P/N Questioning
(HWPQ) scheme that uses constrained vision-language models to assess abstract
semantics (e.g., style fusion, cultural fidelity). Grounded in a Delphi study
with 120 experts, we defined seven user roles and nine evaluation angles to
construct the benchmark, which comprises 2,845 prompts validated by over 15,000
human pairwise comparisons. Our metrics achieve high human alignment (>75%),
with the HWPQ scheme reaching 85.9% accuracy on abstract semantics,
significantly outperforming VQA baselines. Comprehensive evaluation of
state-of-the-art models reveals no universal champion, as role-weighted scores
reorder rankings and provide actionable guidance for domain-specific
deployment. All resources are publicly released to foster reproducible T2I
assessment.

</details>


### [62] [An Interpretable Multi-Plane Fusion Framework With Kolmogorov-Arnold Network Guided Attention Enhancement for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.06157)
*Xiaoxiao Yang,Meiliang Liu,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.CV

TL;DR: 提出了一种名为MPF-KANSC的创新框架，通过多平面融合和KANSC注意力机制，提升了阿尔茨海默病（AD）的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: AD的早期精确诊断具有挑战性，现有深度学习方法难以捕捉大脑病理区域的复杂非线性关系。

Method: 结合多平面融合（MPF）和KANSC注意力机制，并行提取多平面特征并精确识别异常。

Result: 在ADNI数据集上表现优异，并发现AD进展中右偏侧化的结构变化。

Conclusion: MPF-KANSC在AD诊断中具有优越性能和潜在可解释性。

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder that
severely impairs cognitive function and quality of life. Timely intervention in
AD relies heavily on early and precise diagnosis, which remains challenging due
to the complex and subtle structural changes in the brain. Most existing deep
learning methods focus only on a single plane of structural magnetic resonance
imaging (sMRI) and struggle to accurately capture the complex and nonlinear
relationships among pathological regions of the brain, thus limiting their
ability to precisely identify atrophic features. To overcome these limitations,
we propose an innovative framework, MPF-KANSC, which integrates multi-plane
fusion (MPF) for combining features from the coronal, sagittal, and axial
planes, and a Kolmogorov-Arnold Network-guided spatial-channel attention
mechanism (KANSC) to more effectively learn and represent sMRI atrophy
features. Specifically, the proposed model enables parallel feature extraction
from multiple anatomical planes, thus capturing more comprehensive structural
information. The KANSC attention mechanism further leverages a more flexible
and accurate nonlinear function approximation technique, facilitating precise
identification and localization of disease-related abnormalities. Experiments
on the ADNI dataset confirm that the proposed MPF-KANSC achieves superior
performance in AD diagnosis. Moreover, our findings provide new evidence of
right-lateralized asymmetry in subcortical structural changes during AD
progression, highlighting the model's promising interpretability.

</details>


### [63] [Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment](https://arxiv.org/abs/2508.06160)
*Zhenbang Du,Yonggan Fu,Lifu Wang,Jiayi Qian,Xiao Luo,Yingyan,Lin*

Main category: cs.CV

TL;DR: PostDiff框架通过减少输入和模块级别的冗余，在无需微调的情况下加速预训练扩散模型，实验表明降低每步推理成本比减少去噪步数更有效。


<details>
  <summary>Details</summary>
Motivation: 扩散模型计算需求高，资源受限平台部署困难，研究在无需微调的情况下如何优化计算效率。

Method: 提出PostDiff框架，包括混合分辨率去噪方案和混合模块缓存策略，以减少输入和模块级别的冗余。

Result: PostDiff显著提升了扩散模型的保真度与效率平衡，降低每步推理成本比减少步数更有效。

Conclusion: PostDiff为资源受限平台提供了一种高效的扩散模型加速方案，且降低每步成本优于减少步数。

Abstract: Diffusion models have shown remarkable success across generative tasks, yet
their high computational demands challenge deployment on resource-limited
platforms. This paper investigates a critical question for compute-optimal
diffusion model deployment: Under a post-training setting without fine-tuning,
is it more effective to reduce the number of denoising steps or to use a
cheaper per-step inference? Intuitively, reducing the number of denoising steps
increases the variability of the distributions across steps, making the model
more sensitive to compression. In contrast, keeping more denoising steps makes
the differences smaller, preserving redundancy, and making post-training
compression more feasible. To systematically examine this, we propose PostDiff,
a training-free framework for accelerating pre-trained diffusion models by
reducing redundancy at both the input level and module level in a post-training
manner. At the input level, we propose a mixed-resolution denoising scheme
based on the insight that reducing generation resolution in early denoising
steps can enhance low-frequency components and improve final generation
fidelity. At the module level, we employ a hybrid module caching strategy to
reuse computations across denoising steps. Extensive experiments and ablation
studies demonstrate that (1) PostDiff can significantly improve the
fidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to
boost efficiency while maintaining decent generation fidelity, reducing
per-step inference cost is often more effective than reducing the number of
denoising steps. Our code is available at
https://github.com/GATECH-EIC/PostDiff.

</details>


### [64] [UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting](https://arxiv.org/abs/2508.06169)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Changting Lin,Jianfeng Dong,Chaochao Chen,Xun Zhou,Meng Han*

Main category: cs.CV

TL;DR: UW-3DGS是一种基于3D高斯泼溅（3DGS）的水下3D场景重建框架，通过可学习的物理模型和自适应噪声修剪，显著提升了水下重建的几何和颜色保真度。


<details>
  <summary>Details</summary>
Motivation: 传统方法如NeRF在水下环境中因光吸收、散射和浑浊问题导致几何和颜色保真度下降，现有扩展方法如SeaThru-NeRF因依赖MLP而效率低下。

Method: 提出UW-3DGS框架，包括基于体素的可学习水下图像形成模块和物理感知不确定性修剪分支（PAUP），通过端到端优化减少噪声高斯点。

Result: 在SeaThru-NeRF和UWBundle数据集上表现优异，PSNR达27.604，SSIM为0.868，LPIPS为0.104，浮动伪影减少约65%。

Conclusion: UW-3DGS通过结合物理模型和自适应修剪，显著提升了水下3D重建的质量和效率。

Abstract: Underwater 3D scene reconstruction faces severe challenges from light
absorption, scattering, and turbidity, which degrade geometry and color
fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF
extensions such as SeaThru-NeRF incorporate physics-based models, their MLP
reliance limits efficiency and spatial resolution in hazy environments. We
introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for
robust underwater reconstruction. Key innovations include: (1) a plug-and-play
learnable underwater image formation module using voxel-based regression for
spatially varying attenuation and backscatter; and (2) a Physics-Aware
Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating
Gaussians via uncertainty scoring, ensuring artifact-free geometry. The
pipeline operates in training and rendering stages. During training, noisy
Gaussians are optimized end-to-end with underwater parameters, guided by PAUP
pruning and scattering modeling. In rendering, refined Gaussians produce clean
Unattenuated Radiance Images (URIs) free from media effects, while learned
physics enable realistic Underwater Images (UWIs) with accurate light
transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior
performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on
SeaThru-NeRF, with ~65% reduction in floating artifacts.

</details>


### [65] [Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation](https://arxiv.org/abs/2508.06170)
*Ojonugwa Oluwafemi Ejiga Peter,Akingbola Oluwapemiisin,Amalahu Chetachi,Adeniran Opeyemi,Fahmi Khalifa,Md Mahmudur Rahman*

Main category: cs.CV

TL;DR: 研究提出了一种多方向架构框架，用于自动化结肠镜图像中的息肉检测，结合合成数据生成和检测分割算法，显著提升了检测和分割性能。


<details>
  <summary>Details</summary>
Motivation: 结肠镜检查是结直肠癌早期诊断的关键工具，但医疗数据集有限且标注复杂，研究旨在解决这些问题并提升自动化检测的准确性。

Method: 采用Faster R-CNN进行初始目标定位，结合Segment Anything Model（SAM）优化分割掩码；同时评估了五种分割模型（U-Net、PSPNet、FPN、LinkNet、MANet）。

Result: Faster R-CNN的召回率为93.08%，精确度为88.97%，F1分数为90.98%；FPN在PSNR和SSIM上表现最佳，U-Net在召回率上领先，LinkNet在IoU和Dice分数上表现均衡。

Conclusion: 该研究通过多方向框架和合成数据生成，显著提升了息肉检测和分割的性能，为结直肠癌的早期诊断提供了高效工具。

Abstract: Colonoscopy is a vital tool for the early diagnosis of colorectal cancer,
which is one of the main causes of cancer-related mortality globally; hence, it
is deemed an essential technique for the prevention and early detection of
colorectal cancer. The research introduces a unique multidirectional
architectural framework to automate polyp detection within colonoscopy images
while helping resolve limited healthcare dataset sizes and annotation
complexities. The research implements a comprehensive system that delivers
synthetic data generation through Stable Diffusion enhancements together with
detection and segmentation algorithms. This detection approach combines Faster
R-CNN for initial object localization while the Segment Anything Model (SAM)
refines the segmentation masks. The faster R-CNN detection algorithm achieved a
recall of 93.08% combined with a precision of 88.97% and an F1 score of
90.98%.SAM is then used to generate the image mask. The research evaluated five
state-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet,
and MANet using ResNet34 as a base model. The results demonstrate the superior
performance of FPN with the highest scores of PSNR (7.205893) and SSIM
(0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced
performance in IoU (64.20%) and Dice score (77.53%).

</details>


### [66] [Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor](https://arxiv.org/abs/2508.06177)
*Dominik Brämer,Diana Kleingarn,Oliver Urbann*

Main category: cs.CV

TL;DR: 提出了一种基于图表示和图卷积网络（GCNs）的机器人定位框架，利用地板特征实现高精度（0.64cm误差）和高效定位，并解决了绑架机器人问题。


<details>
  <summary>Details</summary>
Motivation: 传统定位方法（如激光雷达或QR码）在复杂环境中存在可扩展性和适应性不足的问题，需要更创新的解决方案。

Method: 使用图表示地板特征，结合图卷积网络（GCNs）进行定位，避免复杂的滤波过程。

Result: 实现了0.64cm的定位误差，并在每一帧中成功解决了绑架机器人问题。

Conclusion: 该方法为机器人导航在多样化环境中提供了新的可能性。

Abstract: Accurate localization represents a fundamental challenge in
  robotic navigation. Traditional methodologies, such as Lidar or QR-code based
systems, suffer from inherent scalability and adaptability con straints,
particularly in complex environments. In this work, we propose
  an innovative localization framework that harnesses flooring characteris tics
by employing graph-based representations and Graph Convolutional
  Networks (GCNs). Our method uses graphs to represent floor features,
  which helps localize the robot more accurately (0.64cm error) and more
  efficiently than comparing individual image features. Additionally, this
  approach successfully addresses the kidnapped robot problem in every
  frame without requiring complex filtering processes. These advancements
  open up new possibilities for robotic navigation in diverse environments.

</details>


### [67] [MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration](https://arxiv.org/abs/2508.06189)
*Cheng Liu,Daou Zhang,Tingxu Liu,Yuhan Wang,Jinyang Chen,Yuexuan Li,Xinying Xiao,Chenbo Xin,Ziru Wang,Weichao Wu*

Main category: cs.CV

TL;DR: 提出了一种基于多智能体异步协作的犯罪行为预测框架（MA-CBP），通过实时视频流分析和高低语义融合，实现潜在犯罪行为的早期预警。


<details>
  <summary>Details</summary>
Motivation: 城市化加速导致公共场景犯罪行为威胁增加，传统方法难以捕捉高级语义或满足实时需求。

Method: 将视频流转化为帧级语义描述，构建因果一致的历史摘要，融合相邻帧进行长短上下文联合推理。

Result: 在多个数据集上表现优异，为城市公共安全提供有效风险预警方案。

Conclusion: MA-CFP框架通过多尺度语言监督和语义融合，显著提升了犯罪行为预测的准确性和实时性。

Abstract: With the acceleration of urbanization, criminal behavior in public scenes
poses an increasingly serious threat to social security. Traditional anomaly
detection methods based on feature recognition struggle to capture high-level
behavioral semantics from historical information, while generative approaches
based on Large Language Models (LLMs) often fail to meet real-time
requirements. To address these challenges, we propose MA-CBP, a criminal
behavior prediction framework based on multi-agent asynchronous collaboration.
This framework transforms real-time video streams into frame-level semantic
descriptions, constructs causally consistent historical summaries, and fuses
adjacent image frames to perform joint reasoning over long- and short-term
contexts. The resulting behavioral decisions include key elements such as event
subjects, locations, and causes, enabling early warning of potential criminal
activity. In addition, we construct a high-quality criminal behavior dataset
that provides multi-scale language supervision, including frame-level,
summary-level, and event-level semantic annotations. Experimental results
demonstrate that our method achieves superior performance on multiple datasets
and offers a promising solution for risk warning in urban public safety
scenarios.

</details>


### [68] [A Semantic Segmentation Algorithm for Pleural Effusion Based on DBIF-AUNet](https://arxiv.org/abs/2508.06191)
*Ruixiang Tang,Jianglong Qin,Mingda Zhang,Yan Song,Yi Wu,Wei Wu*

Main category: cs.CV

TL;DR: 提出了一种名为DBIF-AUNet的双分支交互融合注意力模型，用于提高胸腔积液CT图像的语义分割精度，解决了现有方法在处理复杂边缘和多变形态时的不足。


<details>
  <summary>Details</summary>
Motivation: 胸腔积液CT图像的语义分割在临床诊断中至关重要，但现有方法因特征拼接导致的语义鸿沟和复杂边缘问题而表现不佳。

Method: 设计了双域特征解耦模块（DDFD）和分支交互注意力融合模块（BIAF），结合嵌套深度监督机制，实现多尺度特征互补和动态特征融合。

Result: 在1622张CT图像上验证，IoU和Dice分数分别达到80.1%和89.0%，优于U-Net++和Swin-UNet。

Conclusion: DBIF-AUNet显著提升了复杂胸腔积液CT图像的分割精度，具有临床实用价值。

Abstract: Pleural effusion semantic segmentation can significantly enhance the accuracy
and timeliness of clinical diagnosis and treatment by precisely identifying
disease severity and lesion areas. Currently, semantic segmentation of pleural
effusion CT images faces multiple challenges. These include similar gray levels
between effusion and surrounding tissues, blurred edges, and variable
morphology. Existing methods often struggle with diverse image variations and
complex edges, primarily because direct feature concatenation causes semantic
gaps. To address these challenges, we propose the Dual-Branch Interactive
Fusion Attention model (DBIF-AUNet). This model constructs a densely nested
skip-connection network and innovatively refines the Dual-Domain Feature
Disentanglement module (DDFD). The DDFD module orthogonally decouples the
functions of dual-domain modules to achieve multi-scale feature complementarity
and enhance characteristics at different levels. Concurrently, we design a
Branch Interaction Attention Fusion module (BIAF) that works synergistically
with the DDFD. This module dynamically weights and fuses global, local, and
frequency band features, thereby improving segmentation robustness.
Furthermore, we implement a nested deep supervision mechanism with hierarchical
adaptive hybrid loss to effectively address class imbalance. Through validation
on 1,622 pleural effusion CT images from Southwest Hospital, DBIF-AUNet
achieved IoU and Dice scores of 80.1% and 89.0% respectively. These results
outperform state-of-the-art medical image segmentation models U-Net++ and
Swin-UNet by 5.7%/2.7% and 2.2%/1.5% respectively, demonstrating significant
optimization in segmentation accuracy for complex pleural effusion CT images.

</details>


### [69] [LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning](https://arxiv.org/abs/2508.06202)
*Chang Che,Ziqi Wang,Pengwan Yang,Qi Wang,Hui Ma,Zenglin Shi*

Main category: cs.CV

TL;DR: 论文提出了一种名为LiLoRA的高效架构扩展方法，用于解决多模态大语言模型在持续视觉指令调优中的灾难性遗忘问题，同时显著提升参数效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在持续学习新任务时因架构扩展导致的参数冗余和可扩展性差的问题。

Method: LiLoRA通过共享LoRA矩阵A、对矩阵B进行低秩分解以减少任务特定参数，并引入余弦正则化稳定性损失来保持共享表示的一致性。

Result: 在多样化的CVIT基准测试中，LiLoRA在顺序任务学习中表现优异，且参数效率显著优于现有方法。

Conclusion: LiLoRA是一种高效且可扩展的解决方案，适用于多模态大语言模型的持续学习任务。

Abstract: Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language
Models (MLLMs) to incrementally learn new tasks over time. However, this
process is challenged by catastrophic forgetting, where performance on
previously learned tasks deteriorates as the model adapts to new ones. A common
approach to mitigate forgetting is architecture expansion, which introduces
task-specific modules to prevent interference. Yet, existing methods often
expand entire layers for each task, leading to significant parameter overhead
and poor scalability. To overcome these issues, we introduce LoRA in LoRA
(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in
MLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,
applies an additional low-rank decomposition to matrix B to minimize
task-specific parameters, and incorporates a cosine-regularized stability loss
to preserve consistency in shared representations over time. Extensive
experiments on a diverse CVIT benchmark show that LiLoRA consistently achieves
superior performance in sequential task learning while significantly improving
parameter efficiency compared to existing approaches.

</details>


### [70] [AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual Anomaly Detection](https://arxiv.org/abs/2508.06203)
*Zhaopeng Gu,Bingke Zhu,Guibo Zhu,Yingying Chen,Wei Ge,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: AnomalyMoE是一个基于Mixture-of-Experts架构的通用异常检测框架，通过分解问题为三个语义层次，显著提升了跨领域性能。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法过于专业化，泛化能力有限，无法适应多领域需求。

Method: 提出AnomalyMoE框架，采用三个专家网络分别处理局部、组件和全局异常，并引入EIR和ESB模块优化专家多样性和利用率。

Result: 在8个数据集上表现优异，显著超越领域专用方法。

Conclusion: AnomalyMoE通过分层设计和模块优化，实现了通用且高效的异常检测。

Abstract: Anomaly detection is a critical task across numerous domains and modalities,
yet existing methods are often highly specialized, limiting their
generalizability. These specialized models, tailored for specific anomaly types
like textural defects or logical errors, typically exhibit limited performance
when deployed outside their designated contexts. To overcome this limitation,
we propose AnomalyMoE, a novel and universal anomaly detection framework based
on a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose the
complex anomaly detection problem into three distinct semantic hierarchies:
local structural anomalies, component-level semantic anomalies, and global
logical anomalies. AnomalyMoE correspondingly employs three dedicated expert
networks at the patch, component, and global levels, and is specialized in
reconstructing features and identifying deviations at its designated semantic
level. This hierarchical design allows a single model to concurrently
understand and detect a wide spectrum of anomalies. Furthermore, we introduce
an Expert Information Repulsion (EIR) module to promote expert diversity and an
Expert Selection Balancing (ESB) module to ensure the comprehensive utilization
of all experts. Experiments on 8 challenging datasets spanning industrial
imaging, 3D point clouds, medical imaging, video surveillance, and logical
anomaly detection demonstrate that AnomalyMoE establishes new state-of-the-art
performance, significantly outperforming specialized methods in their
respective domains.

</details>


### [71] [PA-HOI: A Physics-Aware Human and Object Interaction Dataset](https://arxiv.org/abs/2508.06205)
*Ruiyan Wang,Lin Zuo,Zonghao Lin,Qiang Wang,Zhengxue Cheng,Rong Xie,Jun Ling,Li Song*

Main category: cs.CV

TL;DR: PA-HOI数据集填补了现有HOI数据集的不足，关注物体物理属性对人类长期运动的影响，包含562个运动序列，适用于运动生成方法。


<details>
  <summary>Details</summary>
Motivation: 现有HOI数据集多关注功能细节，忽略了物体物理属性对人类运动的影响，PA-HOI旨在填补这一空白。

Method: 构建PA-HOI数据集，包含562个运动序列，涵盖不同性别受试者与35种3D物体的交互，记录物体尺寸、形状和重量对人类运动的影响。

Result: 数据集显著扩展了对物体物理属性如何影响人类姿势、速度、运动规模和交互策略的理解。

Conclusion: PA-HOI数据集成功整合到现有运动生成方法中，验证了其传递真实物理感知的能力。

Abstract: The Human-Object Interaction (HOI) task explores the dynamic interactions
between humans and objects in physical environments, providing essential
biomechanical and cognitive-behavioral foundations for fields such as robotics,
virtual reality, and human-computer interaction. However, existing HOI data
sets focus on details of affordance, often neglecting the influence of physical
properties of objects on human long-term motion. To bridge this gap, we
introduce the PA-HOI Motion Capture dataset, which highlights the impact of
objects' physical attributes on human motion dynamics, including human posture,
moving velocity, and other motion characteristics. The dataset comprises 562
motion sequences of human-object interactions, with each sequence performed by
subjects of different genders interacting with 35 3D objects that vary in size,
shape, and weight. This dataset stands out by significantly extending the scope
of existing ones for understanding how the physical attributes of different
objects influence human posture, speed, motion scale, and interacting
strategies. We further demonstrate the applicability of the PA-HOI dataset by
integrating it with existing motion generation methods, validating its capacity
to transfer realistic physical awareness.

</details>


### [72] [Interpretable Rheumatoid Arthritis Scoring via Anatomy-aware Multiple Instance Learning](https://arxiv.org/abs/2508.06218)
*Zhiyan Bo,Laura C. Coates,Bartlomiej W. Papiez*

Main category: cs.CV

TL;DR: 提出了一种基于双手X光片的SvdH评分预测方法，通过两阶段流程和注意力机制提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: SvdH评分在临床实践中因复杂性难以广泛应用，手动评分效率低。

Method: 采用两阶段流程：提取疾病相关区域，并通过注意力机制整合特征进行预测。提出两种区域提取方案。

Result: 最佳模型PCC为0.943，RMSE为15.73；集成学习后PCC为0.945，RMSE为15.57，接近放射科医生水平。

Conclusion: 该方法能高效识别RA相关解剖结构，为临床决策提供支持。

Abstract: The Sharp/van der Heijde (SvdH) score has been widely used in clinical trials
to quantify radiographic damage in Rheumatoid Arthritis (RA), but its
complexity has limited its adoption in routine clinical practice. To address
the inefficiency of manual scoring, this work proposes a two-stage pipeline for
interpretable image-level SvdH score prediction using dual-hand radiographs.
Our approach extracts disease-relevant image regions and integrates them using
attention-based multiple instance learning to generate image-level features for
prediction. We propose two region extraction schemes: 1) sampling image tiles
most likely to contain abnormalities, and 2) cropping patches containing
disease-relevant joints. With Scheme 2, our best individual score prediction
model achieved a Pearson's correlation coefficient (PCC) of 0.943 and a root
mean squared error (RMSE) of 15.73. Ensemble learning further boosted
prediction accuracy, yielding a PCC of 0.945 and RMSE of 15.57, achieving
state-of-the-art performance that is comparable to that of experienced
radiologists (PCC = 0.97, RMSE = 18.75). Finally, our pipeline effectively
identified and made decisions based on anatomical structures which clinicians
consider relevant to RA progression.

</details>


### [73] [TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images](https://arxiv.org/abs/2508.06224)
*Guoyu Zhou,Jing Zhang,Yi Yan,Hui Zhang,Li Zhuo*

Main category: cs.CV

TL;DR: 提出了一种纹理感知和边缘引导的Transformer（TEFormer），用于解决城市遥感图像语义分割中的纹理差异和边缘模糊问题。


<details>
  <summary>Details</summary>
Motivation: 城市遥感图像中地物纹理差异小、空间结构相似，易导致语义模糊和误分类，且不规则形状和模糊边界增加了分割难度。

Method: 设计了纹理感知模块（TaM）捕捉细粒度纹理差异，构建边缘引导三分支解码器（Eg3Head）保留局部边缘，并通过边缘引导特征融合模块（EgFFM）融合上下文和边缘信息。

Result: 在Potsdam、Vaihingen和LoveDA数据集上分别达到88.57%、81.46%和53.55%的mIoU。

Conclusion: TEFormer有效提升了城市遥感图像语义分割的准确性。

Abstract: Semantic segmentation of urban remote sensing images (URSIs) is crucial for
applications such as urban planning and environmental monitoring. However,
geospatial objects often exhibit subtle texture differences and similar spatial
structures, which can easily lead to semantic ambiguity and misclassification.
Moreover, challenges such as irregular object shapes, blurred boundaries, and
overlapping spatial distributions of semantic objects contribute to complex and
diverse edge morphologies, further complicating accurate segmentation. To
tackle these issues, we propose a texture-aware and edge-guided Transformer
(TEFormer) that integrates texture awareness and edge-guidance mechanisms for
semantic segmentation of URSIs. In the encoder, a texture-aware module (TaM) is
designed to capture fine-grained texture differences between visually similar
categories to enhance semantic discrimination. Then, an edge-guided tri-branch
decoder (Eg3Head) is constructed to preserve local edges and details for
multiscale context-awareness. Finally, an edge-guided feature fusion module
(EgFFM) is to fuse contextual and detail information with edge information to
realize refined semantic segmentation. Extensive experiments show that TEFormer
achieves mIoU of 88.57%, 81.46%, and 53.55% on the Potsdam, Vaihingen, and
LoveDA datasets, respectively, shows the effectiveness in URSI semantic
segmentation.

</details>


### [74] [Depth Jitter: Seeing through the Depth](https://arxiv.org/abs/2508.06227)
*Md Sazidur Rahman,David Cabecinhas,Ricard Marxer*

Main category: cs.CV

TL;DR: Depth-Jitter是一种基于深度的数据增强技术，通过模拟自然深度变化提升模型在深度敏感环境中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统数据增强技术忽略了深度感知变换，限制了模型在真实世界深度变化中的鲁棒性。

Method: 提出Depth-Jitter，采用自适应深度偏移和深度方差阈值指导，生成合成深度扰动并保持结构完整性。

Result: 在FathomNet和UTDAC2020数据集上验证，Depth-Jitter提升了模型在深度变化下的稳定性，但绝对性能不一定优于传统方法。

Conclusion: Depth-Jitter为深度感知数据增强提供了新思路，支持深度敏感应用的进一步发展。

Abstract: Depth information is essential in computer vision, particularly in underwater
imaging, robotics, and autonomous navigation. However, conventional
augmentation techniques overlook depth aware transformations, limiting model
robustness in real world depth variations. In this paper, we introduce
Depth-Jitter, a novel depth-based augmentation technique that simulates natural
depth variations to improve generalization. Our approach applies adaptive depth
offsetting, guided by depth variance thresholds, to generate synthetic depth
perturbations while preserving structural integrity. We evaluate Depth-Jitter
on two benchmark datasets, FathomNet and UTDAC2020 demonstrating its impact on
model stability under diverse depth conditions. Extensive experiments compare
Depth-Jitter against traditional augmentation strategies such as ColorJitter,
analyzing performance across varying learning rates, encoders, and loss
functions. While Depth-Jitter does not always outperform conventional methods
in absolute performance, it consistently enhances model stability and
generalization in depth-sensitive environments. These findings highlight the
potential of depth-aware augmentation for real-world applications and provide a
foundation for further research into depth-based learning strategies. The
proposed technique is publicly available to support advancements in depth-aware
augmentation. The code is publicly available on
\href{https://github.com/mim-team/Depth-Jitter}{github}.

</details>


### [75] [Towards Unified Image Deblurring using a Mixture-of-Experts Decoder](https://arxiv.org/abs/2508.06228)
*Daniel Feijoo,Paula Garrido-Mellado,Jaesung Rim,Alvaro Garcia,Marcos V. Conde*

Main category: cs.CV

TL;DR: 提出了一种通用的图像去模糊方法，能够处理多种模糊类型，通过混合专家模块动态路由特征，实现高效恢复。


<details>
  <summary>Details</summary>
Motivation: 现有方法针对特定模糊类型设计，缺乏通用性，需要多个模型覆盖不同模糊类型，不适用于实际场景。

Method: 引入混合专家（MoE）解码模块，根据识别的模糊类型动态路由图像特征，实现端到端的精确恢复。

Result: 统一方法性能与专用模型相当，对未见过的模糊类型表现出显著的鲁棒性和泛化能力。

Conclusion: 提出的通用去模糊方法高效且实用，适用于多种模糊场景。

Abstract: Image deblurring, removing blurring artifacts from images, is a fundamental
task in computational photography and low-level computer vision. Existing
approaches focus on specialized solutions tailored to particular blur types,
thus, these solutions lack generalization. This limitation in current methods
implies requiring multiple models to cover several blur types, which is not
practical in many real scenarios. In this paper, we introduce the first
all-in-one deblurring method capable of efficiently restoring images affected
by diverse blur degradations, including global motion, local motion, blur in
low-light conditions, and defocus blur. We propose a mixture-of-experts (MoE)
decoding module, which dynamically routes image features based on the
recognized blur degradation, enabling precise and efficient restoration in an
end-to-end manner. Our unified approach not only achieves performance
comparable to dedicated task-specific models, but also demonstrates remarkable
robustness and generalization capabilities on unseen blur degradation
scenarios.

</details>


### [76] [Deepfake Detection that Generalizes Across Benchmarks](https://arxiv.org/abs/2508.06248)
*Andrii Yermakov,Jan Cech,Jiri Matas,Mario Fritz*

Main category: cs.CV

TL;DR: 本文提出了一种参数高效的方法LNCLIP-DF，通过微调预训练CLIP视觉编码器的层归一化参数（仅0.03%），并结合L2归一化和潜在空间增强，实现了对未见过的深度伪造技术的鲁棒泛化。


<details>
  <summary>Details</summary>
Motivation: 深度伪造检测器在面对未见过的伪造技术时泛化能力不足，现有方法通常引入复杂架构，但本文证明通过参数高效的方法也能实现鲁棒泛化。

Method: 仅微调CLIP模型的层归一化参数，结合L2归一化和潜在空间增强，形成超球面特征流形。

Result: 在13个基准数据集上评估，LNCLIP-DF表现最优，平均跨数据集AUROC超过更复杂的近期方法。

Conclusion: 通过有针对性的最小改动，可以实现最优泛化能力，同时强调了使用同一源视频的真实-伪造配对数据的重要性。

Abstract: The generalization of deepfake detectors to unseen manipulation techniques
remains a challenge for practical deployment. Although many approaches adapt
foundation models by introducing significant architectural complexity, this
work demonstrates that robust generalization is achievable through a
parameter-efficient adaptation of a pre-trained CLIP vision encoder. The
proposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters
(0.03% of the total) and enhances generalization by enforcing a hyperspherical
feature manifold using L2 normalization and latent space augmentations.
  We conducted an extensive evaluation on 13 benchmark datasets spanning from
2019 to 2025. The proposed method achieves state-of-the-art performance,
outperforming more complex, recent approaches in average cross-dataset AUROC.
Our analysis yields two primary findings for the field: 1) training on paired
real-fake data from the same source video is essential for mitigating shortcut
learning and improving generalization, and 2) detection difficulty on academic
datasets has not strictly increased over time, with models trained on older,
diverse datasets showing strong generalization capabilities.
  This work delivers a computationally efficient and reproducible method,
proving that state-of-the-art generalization is attainable by making targeted,
minimal changes to a pre-trained CLIP model. The code will be made publicly
available upon acceptance.

</details>


### [77] [FedX: Explanation-Guided Pruning for Communication-Efficient Federated Learning in Remote Sensing](https://arxiv.org/abs/2508.06256)
*Barış Büyüktaş,Jonas Klotz,Begüm Demir*

Main category: cs.CV

TL;DR: 论文提出了一种名为FedX的新策略，通过解释引导的剪枝减少联邦学习中的通信开销，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在遥感图像分类任务中具有潜力，但通信开销是一个关键挑战。

Method: FedX利用反向传播解释方法评估模型组件的重要性，剪枝最不相关的部分以减少传输模型的大小。

Result: 在BigEarthNet-S2和EuroSAT数据集上的实验表明，FedX显著减少了共享参数数量，同时提升了全局模型的泛化能力。

Conclusion: FedX有效解决了联邦学习中的通信开销问题，优于未剪枝模型和其他先进剪枝方法。

Abstract: Federated learning (FL) enables the collaborative training of deep neural
networks across decentralized data archives (i.e., clients), where each client
stores data locally and only shares model updates with a central server. This
makes FL a suitable learning paradigm for remote sensing (RS) image
classification tasks, where data centralization may be restricted due to legal
and privacy constraints. However, a key challenge in applying FL to RS tasks is
the communication overhead caused by the frequent exchange of large model
updates between clients and the central server. To address this issue, in this
paper we propose a novel strategy (denoted as FedX) that uses
explanation-guided pruning to reduce communication overhead by minimizing the
size of the transmitted models without compromising performance. FedX leverages
backpropagation-based explanation methods to estimate the task-specific
importance of model components and prunes the least relevant ones at the
central server. The resulting sparse global model is then sent to clients,
substantially reducing communication overhead. We evaluate FedX on multi-label
scene classification using the BigEarthNet-S2 dataset and single-label scene
classification using the EuroSAT dataset. Experimental results show the success
of FedX in significantly reducing the number of shared model parameters while
enhancing the generalization capability of the global model, compared to both
unpruned model and state-of-the-art pruning methods. The code of FedX will be
available at https://git.tu-berlin.de/rsim/FedX.

</details>


### [78] [XAG-Net: A Cross-Slice Attention and Skip Gating Network for 2.5D Femur MRI Segmentation](https://arxiv.org/abs/2508.06258)
*Byunghyun Ko,Anning Tian,Jeongkyu Lee*

Main category: cs.CV

TL;DR: XAG-Net是一种新型2.5D U-Net架构，结合像素级跨切片注意力和跳跃注意力门控机制，显著提升了股骨MRI分割的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有2D和3D深度学习方法在股骨MRI分割中存在局限性，需要更高效的解决方案。

Method: 提出XAG-Net，采用像素级跨切片注意力（CSA）和跳跃注意力门控（AG）机制，优化切片间和切片内特征建模。

Result: XAG-Net在分割精度上优于基线2D、2.5D和3D U-Net模型，同时保持计算效率。

Conclusion: XAG-Net是一种高效且准确的股骨MRI分割框架，CSA和AG模块对其性能至关重要。

Abstract: Accurate segmentation of femur structures from Magnetic Resonance Imaging
(MRI) is critical for orthopedic diagnosis and surgical planning but remains
challenging due to the limitations of existing 2D and 3D deep learning-based
segmentation approaches. In this study, we propose XAG-Net, a novel 2.5D
U-Net-based architecture that incorporates pixel-wise cross-slice attention
(CSA) and skip attention gating (AG) mechanisms to enhance inter-slice
contextual modeling and intra-slice feature refinement. Unlike previous
CSA-based models, XAG-Net applies pixel-wise softmax attention across adjacent
slices at each spatial location for fine-grained inter-slice modeling.
Extensive evaluations demonstrate that XAG-Net surpasses baseline 2D, 2.5D, and
3D U-Net models in femur segmentation accuracy while maintaining computational
efficiency. Ablation studies further validate the critical role of the CSA and
AG modules, establishing XAG-Net as a promising framework for efficient and
accurate femur MRI segmentation.

</details>


### [79] [SIFThinker: Spatially-Aware Image Focus for Visual Reasoning](https://arxiv.org/abs/2508.06259)
*Zhangquan Chen,Ruihui Zhao,Chuwei Luo,Mingze Sun,Xinlei Yu,Yangyang Kang,Ruqi Huang*

Main category: cs.CV

TL;DR: SIFThinker是一个空间感知的多模态框架，通过深度增强的边界框和自然语言交互，提升视觉任务中的注意力校正和区域聚焦能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在复杂视觉任务（如空间理解和细粒度感知）中表现不足，缺乏利用空间线索进行迭代注意力校正的能力。

Method: 提出SIFThinker框架，采用反向扩展前向推理策略生成图像-文本链式思维，并引入GRPO-SIF训练范式，结合深度信息进行视觉定位。

Result: 实验表明，SIFThinker在空间理解和细粒度视觉感知任务中优于现有方法，同时保持通用能力。

Conclusion: SIFThinker通过空间感知和动态注意力校正，显著提升了复杂视觉任务的性能。

Abstract: Current multimodal large language models (MLLMs) still face significant
challenges in complex visual tasks (e.g., spatial understanding, fine-grained
perception). Prior methods have tried to incorporate visual reasoning, however,
they fail to leverage attention correction with spatial cues to iteratively
refine their focus on prompt-relevant regions. In this paper, we introduce
SIFThinker, a spatially-aware "think-with-images" framework that mimics human
visual perception. Specifically, SIFThinker enables attention correcting and
image region focusing by interleaving depth-enhanced bounding boxes and natural
language. Our contributions are twofold: First, we introduce a
reverse-expansion-forward-inference strategy that facilitates the generation of
interleaved image-text chains of thought for process-level supervision, which
in turn leads to the construction of the SIF-50K dataset. Besides, we propose
GRPO-SIF, a reinforced training paradigm that integrates depth-informed visual
grounding into a unified reasoning pipeline, teaching the model to dynamically
correct and focus on prompt-relevant regions. Extensive experiments demonstrate
that SIFThinker outperforms state-of-the-art methods in spatial understanding
and fine-grained visual perception, while maintaining strong general
capabilities, highlighting the effectiveness of our method.

</details>


### [80] [Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding](https://arxiv.org/abs/2508.06317)
*Jian Hu,Zixu Cheng,Shaogang Gong,Isabel Guan,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.CV

TL;DR: 论文提出了一种数据高效的无标注跨域视频时间定位方法（URPA），通过少量未标注目标域视频实现跨域知识迁移，解决了传统方法依赖标注数据和高计算开销的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如GRPO）依赖标注数据且计算开销大，难以在无标注域和实时场景中应用。

Method: 提出URPA方法，利用GRPO生成多个候选预测，通过平均和方差估计置信度，指导模型训练。

Result: 在六个跨域设置中的三个数据集上验证了URPA的有效性，仅需少量未标注视频即可实现良好泛化。

Conclusion: URPA无需目标域标注，计算开销低，适用于实时部署，为跨域视频时间定位提供了高效解决方案。

Abstract: Video Temporal Grounding (TG) aims to temporally locate video segments
matching a natural language description (a query) in a long video. While
Vision-Language Models (VLMs) are effective at holistic semantic matching, they
often struggle with fine-grained temporal localisation. Recently, Group
Relative Policy Optimisation (GRPO) reformulates the inference process as a
reinforcement learning task, enabling fine-grained grounding and achieving
strong in-domain performance. However, GRPO relies on labelled data, making it
unsuitable in unlabelled domains. Moreover, because videos are large and
expensive to store and process, performing full-scale adaptation introduces
prohibitive latency and computational overhead, making it impractical for
real-time deployment. To overcome both problems, we introduce a Data-Efficient
Unlabelled Cross-domain Temporal Grounding method, from which a model is first
trained on a labelled source domain, then adapted to a target domain using only
a small number of unlabelled videos from the target domain. This approach
eliminates the need for target annotation and keeps both computational and
storage overhead low enough to run in real time. Specifically, we introduce.
Uncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain
knowledge transfer in learning video temporal grounding without target labels.
URPA generates multiple candidate predictions using GRPO rollouts, averages
them to form a pseudo label, and estimates confidence from the variance across
these rollouts. This confidence then weights the training rewards, guiding the
model to focus on reliable supervision. Experiments on three datasets across
six cross-domain settings show that URPA generalises well using only a few
unlabelled target videos. Codes will be released once published.

</details>


### [81] [Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.06318)
*Giacomo D'Amicantonio,Snehashis Majhi,Quan Kong,Lorenzo Garattoni,Gianpiero Francesca,François Bremond,Egor Bondarev*

Main category: cs.CV

TL;DR: 论文提出了一种名为GS-MoE的新框架，通过专家模型和时序高斯散射损失解决弱监督视频异常检测中的多样性和弱信号问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前弱监督视频异常检测模型在处理复杂异常事件时表现不佳，主要因为模型无法区分异常类别且弱监督信号缺乏精确时序信息。

Method: 提出GS-MoE框架，使用多个专家模型分别捕捉特定异常类型，并通过时序高斯散射损失增强弱监督信号。

Result: 在UCF-Crime数据集上达到91.58%的AUC，并在XD-Violence和MSAD数据集上表现优异。

Conclusion: GS-MoE通过类别特异性专家和时序引导，为弱监督视频异常检测设定了新基准。

Abstract: Video Anomaly Detection (VAD) is a challenging task due to the variability of
anomalous events and the limited availability of labeled data. Under the
Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided
during training, while predictions are made at the frame level. Although
state-of-the-art models perform well on simple anomalies (e.g., explosions),
they struggle with complex real-world events (e.g., shoplifting). This
difficulty stems from two key issues: (1) the inability of current models to
address the diversity of anomaly types, as they process all categories with a
shared model, overlooking category-specific features; and (2) the weak
supervision signal, which lacks precise temporal information, limiting the
ability to capture nuanced anomalous patterns blended with normal events. To
address these challenges, we propose Gaussian Splatting-guided Mixture of
Experts (GS-MoE), a novel framework that employs a set of expert models, each
specialized in capturing specific anomaly types. These experts are guided by a
temporal Gaussian splatting loss, enabling the model to leverage temporal
consistency and enhance weak supervision. The Gaussian splatting approach
encourages a more precise and comprehensive representation of anomalies by
focusing on temporal segments most likely to contain abnormal events. The
predictions from these specialized experts are integrated through a
mixture-of-experts mechanism to model complex relationships across diverse
anomaly patterns. Our approach achieves state-of-the-art performance, with a
91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on
XD-Violence and MSAD datasets. By leveraging category-specific expertise and
temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.

</details>


### [82] [Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?](https://arxiv.org/abs/2508.06327)
*Xin Ci Wong,Duygu Sarikaya,Kieran Zucker,Marc De Kamps,Nishant Ravikumar*

Main category: cs.CV

TL;DR: 提出一种基于扩散模型的方法，生成合成心脏MR图像以解决领域偏移问题，显著提升多中心心脏MR分割性能。


<details>
  <summary>Details</summary>
Motivation: 心脏MR成像因设备和协议差异导致领域偏移，限制了AI模型在实际场景中的部署。传统方法（如数据增强或迁移学习）效果有限，合成数据因解剖结构一致性约束而受限。

Method: 使用扩散模型生成合成心脏MR图像，保持空间和结构保真度，并评估其在多中心分割任务中的效果，包括领域泛化和领域适应策略。

Result: 合成数据显著提高了未见目标领域的分割性能（Welch's t-test, p < 0.01），优于仅使用真实数据训练的方法。

Conclusion: 该方法无需迁移学习或在线训练，有效解决了心脏MR图像分析中的领域偏移问题，特别适用于数据稀缺场景。

Abstract: Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain
shift due to variations in imaging devices and acquisition protocols. This
challenge limits the deployment of trained AI models in real-world scenarios,
where performance degrades on unseen domains. Traditional solutions involve
increasing the size of the dataset through ad-hoc image augmentation or
additional online training/transfer learning, which have several limitations.
Synthetic data offers a promising alternative, but anatomical/structural
consistency constraints limit the effectiveness of generative models in
creating image-label pairs. To address this, we propose a diffusion model (DM)
trained on a source domain that generates synthetic cardiac MR images that
resemble a given reference. The synthetic data maintains spatial and structural
fidelity, ensuring similarity to the source domain and compatibility with the
segmentation mask. We assess the utility of our generative approach in
multi-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and
vanilla U-Net segmentation networks. We explore domain generalisation, where,
domain-invariant segmentation models are trained on synthetic source domain
data, and domain adaptation, where, we shift target domain data towards the
source domain using the DM. Both strategies significantly improved segmentation
performance on data from an unseen target domain, in terms of surface-based
metrics (Welch's t-test, p < 0.01), compared to training segmentation models on
real data alone. The proposed method ameliorates the need for transfer learning
or online training to address domain shift challenges in cardiac MR image
analysis, especially useful in data-scarce settings.

</details>


### [83] [ViPro-2: Unsupervised State Estimation via Integrated Dynamics for Guiding Video Prediction](https://arxiv.org/abs/2508.06335)
*Patrick Takenaka,Johannes Maucher,Marco F. Huber*

Main category: cs.CV

TL;DR: 改进ViPro模型，使其能从未知初始状态推断符号状态，并在无监督下实现，扩展了3D数据集以接近现实场景。


<details>
  <summary>Details</summary>
Motivation: 解决ViPro模型依赖初始真实符号状态的问题，避免学习捷径，提升模型从观测中推断状态的能力。

Method: 在ViPro基础上改进，引入无监督学习，扩展3D数据集。

Result: 模型能从未知初始状态正确推断符号状态，适用于更复杂的动态环境。

Conclusion: 改进后的ViPro模型在无监督下有效推断状态，扩展数据集增强了实用性。

Abstract: Predicting future video frames is a challenging task with many downstream
applications. Previous work has shown that procedural knowledge enables deep
models for complex dynamical settings, however their model ViPro assumed a
given ground truth initial symbolic state. We show that this approach led to
the model learning a shortcut that does not actually connect the observed
environment with the predicted symbolic state, resulting in the inability to
estimate states given an observation if previous states are noisy. In this
work, we add several improvements to ViPro that enables the model to correctly
infer states from observations without providing a full ground truth state in
the beginning. We show that this is possible in an unsupervised manner, and
extend the original Orbits dataset with a 3D variant to close the gap to real
world scenarios.

</details>


### [84] [Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities](https://arxiv.org/abs/2508.06342)
*Kieran Elrod,Katherine Flanigan,Mario Bergés*

Main category: cs.CV

TL;DR: 研究利用街景图像和多模态大语言模型，结合社会学理论，量化分析城市社交互动质量，发现其与城市环境变量及居民归属感相关。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注行人数量而非社交互动质量，街景图像作为低成本数据源可能蕴含潜在社交信息。

Method: 分析15个城市的2,998张街景图像，结合Mehta的社交分类理论，使用线性回归模型控制多种变量。

Result: 天空视野指数与三种社交类型相关，绿色视野指数预测持久社交，归属感与短暂社交正相关。

Conclusion: 街景图像可量化分析社交互动与城市环境关系，为城市设计提供新工具。

Abstract: Designing socially active streets has long been a goal of urban planning, yet
existing quantitative research largely measures pedestrian volume rather than
the quality of social interactions. We hypothesize that street view imagery --
an inexpensive data source with global coverage -- contains latent social
information that can be extracted and interpreted through established social
science theory. As a proof of concept, we analyzed 2,998 street view images
from 15 cities using a multimodal large language model guided by Mehta's
taxonomy of passive, fleeting, and enduring sociability -- one illustrative
example of a theory grounded in urban design that could be substituted or
complemented by other sociological frameworks. We then used linear regression
models, controlling for factors like weather, time of day, and pedestrian
counts, to test whether the inferred sociability measures correlate with
city-level place attachment scores from the World Values Survey and with
environmental predictors (e.g., green, sky, and water view indices) derived
from individual street view images. Results aligned with long-standing urban
planning theory: the sky view index was associated with all three sociability
types, the green view index predicted enduring sociability, and place
attachment was positively associated with fleeting sociability. These results
provide preliminary evidence that street view images can be used to infer
relationships between specific types of social interactions and built
environment variables. Further research could establish street view imagery as
a scalable, privacy-preserving tool for studying urban sociability, enabling
cross-cultural theory testing and evidence-based design of socially vibrant
cities.

</details>


### [85] [Aligning Effective Tokens with Video Anomaly in Large Language Models](https://arxiv.org/abs/2508.06350)
*Yingxian Chen,Jiahui Liu,Ruifan Di,Yanwei Li,Chirui Chang,Shizhen Zhao,Wilton W. T. Fok,Xiaojuan Qi,Yik-Chung Wu*

Main category: cs.CV

TL;DR: VA-GPT是一种新型多模态大语言模型（MLLM），通过空间和时间有效令牌模块（SETS和TETG）优化异常事件检测，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解MLLM在处理异常事件时因空间和时间稀疏性表现不佳，需要更高效的模型。

Method: 提出VA-GPT模型，结合SETS和TETG模块，优化视觉与语言模型的令牌对齐，并构建专用数据集和跨域评估基准。

Result: VA-GPT在多个基准测试中优于现有方法。

Conclusion: VA-GPT通过有效令牌选择和生成模块显著提升了异常事件检测性能。

Abstract: Understanding abnormal events in videos is a vital and challenging task that
has garnered significant attention in a wide range of applications. Although
current video understanding Multi-modal Large Language Models (MLLMs) are
capable of analyzing general videos, they often struggle to handle anomalies
due to the spatial and temporal sparsity of abnormal events, where the
redundant information always leads to suboptimal outcomes. To address these
challenges, exploiting the representation and generalization capabilities of
Vison Language Models (VLMs) and Large Language Models (LLMs), we propose
VA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in
various videos. Our approach efficiently aligns effective tokens between visual
encoders and LLMs through two key proposed modules: Spatial Effective Token
Selection (SETS) and Temporal Effective Token Generation (TETG). These modules
enable our model to effectively capture and analyze both spatial and temporal
information associated with abnormal events, resulting in more accurate
responses and interactions. Furthermore, we construct an instruction-following
dataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a
cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed
method outperforms existing state-of-the-art methods on various benchmarks.

</details>


### [86] [An Implemention of Two-Phase Image Segmentation using the Split Bregman Method](https://arxiv.org/abs/2508.06351)
*Olakunle S. Abawonse,Günay Doğan*

Main category: cs.CV

TL;DR: 本文实现了一种基于Goldstein、Bresson和Osher提出的两阶段图像分割算法，改进了Chan-Vese能量模型，并通过分裂Bregman方法高效优化。


<details>
  <summary>Details</summary>
Motivation: 改进Chan-Vese模型以实现更高效的两阶段图像分割。

Method: 使用分裂Bregman方法最小化改进后的能量模型，实现图像的前景和背景分割。

Result: 算法在不同参数下对多幅图像进行了测试，展示了其性能。

Conclusion: 该方法高效且适用于两阶段图像分割任务。

Abstract: In this paper, we describe an implementation of the two-phase image
segmentation algorithm proposed by Goldstein, Bresson, Osher in
\cite{gold:bre}. This algorithm partitions the domain of a given 2d image into
foreground and background regions, and each pixel of the image is assigned
membership to one of these two regions. The underlying assumption for the
segmentation model is that the pixel values of the input image can be
summarized by two distinct average values, and that the region boundaries are
smooth. Accordingly, the model is defined as an energy in which the variable is
a region membership function to assign pixels to either region, originally
proposed by Chan and Vese in \cite{chan:vese}. This energy is the sum of image
data terms in the regions and a length penalty for region boundaries.
Goldstein, Bresson, Osher modify the energy of Chan-Vese in \cite{gold:bre} so
that their new energy can be minimized efficiently using the split Bregman
method to produce an equivalent two-phase segmentation. We provide a detailed
implementation of this method \cite{gold:bre}, and document its performance
with several images over a range of algorithm parameters.

</details>


### [87] [Are you In or Out (of gallery)? Wisdom from the Same-Identity Crowd](https://arxiv.org/abs/2508.06357)
*Aman Bhatta,Maria Dhakal,Michael C. King,Kevin W. Bowyer*

Main category: cs.CV

TL;DR: 本文提出了一种新方法，通过利用排名第一身份的额外注册图像来预测其是否为图库内或图库外，以减少误识别和调查时间。


<details>
  <summary>Details</summary>
Motivation: 解决一对面部识别中图库外个体的检测问题，传统方法依赖相似度阈值，而新方法利用额外注册图像信息。

Method: 生成训练数据并训练分类器，利用排名第一身份的额外注册图像特征向量预测图库内外状态。

Result: 实验表明该方法适用于多种图像质量，且在人口统计组间表现一致。

Conclusion: 该方法能有效减少误识别，且仅在使用先进损失函数训练的匹配器时有效。

Abstract: A central problem in one-to-many facial identification is that the person in
the probe image may or may not have enrolled image(s) in the gallery; that is,
may be In-gallery or Out-of-gallery. Past approaches to detect when a rank-one
result is Out-of-gallery have mostly focused on finding a suitable threshold on
the similarity score. We take a new approach, using the additional enrolled
images of the identity with the rank-one result to predict if the rank-one
result is In-gallery / Out-of-gallery. Given a gallery of identities and
images, we generate In-gallery and Out-of-gallery training data by extracting
the ranks of additional enrolled images corresponding to the rank-one identity.
We then train a classifier to utilize this feature vector to predict whether a
rank-one result is In-gallery or Out-of-gallery. Using two different datasets
and four different matchers, we present experimental results showing that our
approach is viable for mugshot quality probe images, and also, importantly, for
probes degraded by blur, reduced resolution, atmospheric turbulence and
sunglasses. We also analyze results across demographic groups, and show that
In-gallery / Out-of-gallery classification accuracy is similar across
demographics. Our approach has the potential to provide an objective estimate
of whether a one-to-many facial identification is Out-of-gallery, and thereby
to reduce false positive identifications, wrongful arrests, and wasted
investigative time. Interestingly, comparing the results of older deep
CNN-based face matchers with newer ones suggests that the effectiveness of our
Out-of-gallery detection approach emerges only with matchers trained using
advanced margin-based loss functions.

</details>


### [88] [Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning](https://arxiv.org/abs/2508.06382)
*Xiangyu Wu,Feng Yu,Yang Yang,Jianfeng Lu*

Main category: cs.CV

TL;DR: TaAM-CPT是一种通过文本数据构建通用表示模型的方法，支持无限模态扩展，无需特定模态标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量模态特定标注数据或仅适用于单一模态，限制了通用性。

Method: TaAM-CPT结合模态提示池、文本构建和模态对齐文本编码器，设计跨模态学习目标。

Result: 在视频、图像和音频分类任务中取得领先结果，无需模态特定标注数据。

Conclusion: TaAM-CPT展示了通过文本数据构建通用表示模型的潜力，支持无限模态扩展。

Abstract: The integration of prompt tuning with multimodal learning has shown
significant generalization abilities for various downstream tasks. Despite
advancements, existing methods heavily depend on massive modality-specific
labeled data (e.g., video, audio, and image), or are customized for a single
modality. In this study, we present Text as Any-Modality by Consistent Prompt
Tuning (TaAM-CPT), a scalable approach for constructing a general
representation model toward unlimited modalities using solely text data.
TaAM-CPT comprises modality prompt pools, text construction, and
modality-aligned text encoders from pre-trained models, which allows for
extending new modalities by simply adding prompt pools and modality-aligned
text encoders. To harmonize the learning across different modalities, TaAM-CPT
designs intra- and inter-modal learning objectives, which can capture category
details within modalities while maintaining semantic consistency across
different modalities. Benefiting from its scalable architecture and pre-trained
models, TaAM-CPT can be seamlessly extended to accommodate unlimited
modalities. Remarkably, without any modality-specific labeled data, TaAM-CPT
achieves leading results on diverse datasets spanning various modalities,
including video classification, image classification, and audio classification.
The code is available at https://github.com/Jinx630/TaAM-CPT.

</details>


### [89] [FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation](https://arxiv.org/abs/2508.06392)
*Wenbin Teng,Gonglin Chen,Haiwei Chen,Yajie Zhao*

Main category: cs.CV

TL;DR: FVGen框架通过视频扩散模型蒸馏技术，在仅需四步采样的情况下实现快速新视角合成，显著提升稀疏视图3D重建的时间效率。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图在3D重建中容易导致未观察区域的伪影，现有方法依赖视频扩散模型（VDMs）生成密集观测，但采样速度慢。

Method: 提出FVGen框架，通过GAN和软化反向KL散度最小化，将多步去噪教师模型蒸馏为少步去噪学生模型。

Result: 实验表明，FVGen在保持（或提升）视觉质量的同时，采样时间减少90%以上。

Conclusion: FVGen显著提升了稀疏输入视图下3D重建任务的时间效率。

Abstract: Recent progress in 3D reconstruction has enabled realistic 3D models from
dense image captures, yet challenges persist with sparse views, often leading
to artifacts in unseen areas. Recent works leverage Video Diffusion Models
(VDMs) to generate dense observations, filling the gaps when only sparse views
are available for 3D reconstruction tasks. A significant limitation of these
methods is their slow sampling speed when using VDMs. In this paper, we present
FVGen, a novel framework that addresses this challenge by enabling fast novel
view synthesis using VDMs in as few as four sampling steps. We propose a novel
video diffusion model distillation method that distills a multi-step denoising
teacher model into a few-step denoising student model using Generative
Adversarial Networks (GANs) and softened reverse KL-divergence minimization.
Extensive experiments on real-world datasets show that, compared to previous
works, our framework generates the same number of novel views with similar (or
even better) visual quality while reducing sampling time by more than 90%.
FVGen significantly improves time efficiency for downstream reconstruction
tasks, particularly when working with sparse input views (more than 2) where
pre-trained VDMs need to be run multiple times to achieve better spatial
coverage.

</details>


### [90] [Feature-Space Oversampling for Addressing Class Imbalance in SAR Ship Classification](https://arxiv.org/abs/2508.06420)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: 论文研究了在SAR船舶分类中，针对长尾数据集的特征空间过采样方法，提出了两种新算法M2m$_f$和M2m$_u$，并在两个公开数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决SAR船舶分类中长尾数据集导致的类别不平衡问题，特别是少数类别的分类困难。

Method: 提出了两种基于Major-to-minor (M2m)方法的算法M2m$_f$和M2m$_u$，并在OpenSARShip和FuSARShip数据集上测试，使用了ViT、VGG16和ResNet50作为特征提取器。

Result: 新方法在FuSARShip和OpenSARShip数据集上的平均F1分数分别提高了8.82%和4.44%，优于原始M2m方法和基线。

Conclusion: 特征空间过采样方法能有效改善SAR船舶分类中的类别不平衡问题，新算法表现优异。

Abstract: SAR ship classification faces the challenge of long-tailed datasets, which
complicates the classification of underrepresented classes. Oversampling
methods have proven effective in addressing class imbalance in optical data. In
this paper, we evaluated the effect of oversampling in the feature space for
SAR ship classification. We propose two novel algorithms inspired by the
Major-to-minor (M2m) method M2m$_f$, M2m$_u$. The algorithms are tested on two
public datasets, OpenSARShip (6 classes) and FuSARShip (9 classes), using three
state-of-the-art models as feature extractors: ViT, VGG16, and ResNet50.
Additionally, we also analyzed the impact of oversampling methods on different
class sizes. The results demonstrated the effectiveness of our novel methods
over the original M2m and baselines, with an average F1-score increase of 8.82%
for FuSARShip and 4.44% for OpenSARShip.

</details>


### [91] [SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation](https://arxiv.org/abs/2508.06429)
*Guido Manni,Clemente Lauretti,Loredana Zollo,Paolo Soda*

Main category: cs.CV

TL;DR: 提出了一种基于GAN的半监督学习框架，适用于医学图像中标记数据稀缺的场景，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像中标记数据不足导致深度学习效果受限的问题。

Method: 结合生成器、判别器和分类器的三阶段训练框架，通过图像翻译和集成伪标签技术利用未标记数据。

Result: 在11个MedMNIST数据集上显著优于六种先进方法，尤其在5-shot场景表现突出。

Conclusion: 为医学影像提供了一种在标记数据稀缺情况下仍能保持高性能的实用解决方案。

Abstract: Deep learning has revolutionized medical imaging, but its effectiveness is
severely limited by insufficient labeled training data. This paper introduces a
novel GAN-based semi-supervised learning framework specifically designed for
low labeled-data regimes, evaluated across settings with 5 to 50 labeled
samples per class. Our approach integrates three specialized neural networks --
a generator for class-conditioned image translation, a discriminator for
authenticity assessment and classification, and a dedicated classifier --
within a three-phase training framework. The method alternates between
supervised training on limited labeled data and unsupervised learning that
leverages abundant unlabeled images through image-to-image translation rather
than generation from noise. We employ ensemble-based pseudo-labeling that
combines confidence-weighted predictions from the discriminator and classifier
with temporal consistency through exponential moving averaging, enabling
reliable label estimation for unlabeled data. Comprehensive evaluation across
eleven MedMNIST datasets demonstrates that our approach achieves statistically
significant improvements over six state-of-the-art GAN-based semi-supervised
methods, with particularly strong performance in the extreme 5-shot setting
where the scarcity of labeled data is most challenging. The framework maintains
its superiority across all evaluated settings (5, 10, 20, and 50 shots per
class). Our approach offers a practical solution for medical imaging
applications where annotation costs are prohibitive, enabling robust
classification performance even with minimal labeled data. Code is available at
https://github.com/GuidoManni/SPARSE.

</details>


### [92] [MotionSwap](https://arxiv.org/abs/2508.06430)
*Om Patil,Jinesh Modi,Suryabha Mukhopadhyay,Meghaditya Giri,Chhavi Malhotra*

Main category: cs.CV

TL;DR: 本文介绍了对SimSwap框架的改进，通过引入自注意力和交叉注意力机制、动态损失加权和余弦退火学习率调度，显著提升了换脸效果。


<details>
  <summary>Details</summary>
Motivation: 提升换脸技术的身份保留、属性一致性和视觉质量。

Method: 在生成器架构中集成自注意力和交叉注意力机制，采用动态损失加权和余弦退火学习率调度。

Result: 改进后的模型在身份相似性、FID分数和视觉质量上优于基线，实验验证了各改进的重要性。

Conclusion: 未来方向包括整合StyleGAN3、改进唇同步、引入3D面部建模和视频应用的时间一致性。

Abstract: Face swapping technology has gained significant attention in both academic
research and commercial applications. This paper presents our implementation
and enhancement of SimSwap, an efficient framework for high fidelity face
swapping. We introduce several improvements to the original model, including
the integration of self and cross-attention mechanisms in the generator
architecture, dynamic loss weighting, and cosine annealing learning rate
scheduling. These enhancements lead to significant improvements in identity
preservation, attribute consistency, and overall visual quality.
  Our experimental results, spanning 400,000 training iterations, demonstrate
progressive improvements in generator and discriminator performance. The
enhanced model achieves better identity similarity, lower FID scores, and
visibly superior qualitative results compared to the baseline. Ablation studies
confirm the importance of each architectural and training improvement. We
conclude by identifying key future directions, such as integrating StyleGAN3,
improving lip synchronization, incorporating 3D facial modeling, and
introducing temporal consistency for video-based applications.

</details>


### [93] [CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment](https://arxiv.org/abs/2508.06434)
*Shengzhu Yang,Jiawei Du,Shuai Lu,Weihang Zhang,Ningli Wang,Huiqi Li*

Main category: cs.CV

TL;DR: CLIPin是一种非对比性插件，可无缝集成到CLIP架构中，提升多模态语义对齐能力。


<details>
  <summary>Details</summary>
Motivation: 解决大规模自然图像-文本数据集语义对齐松散和医学数据集内容多样性低的问题，以增强CLIP模型的鲁棒性和泛化能力。

Method: 提出CLIPin插件，设计共享预投影器，结合对比与非对比学习。

Result: 在多样化下游任务中验证了CLIPin的有效性和通用性。

Conclusion: CLIPin作为即插即用组件，适用于多种对比框架，显著提升性能。

Abstract: Large-scale natural image-text datasets, especially those automatically
collected from the web, often suffer from loose semantic alignment due to weak
supervision, while medical datasets tend to have high cross-modal correlation
but low content diversity. These properties pose a common challenge for
contrastive language-image pretraining (CLIP): they hinder the model's ability
to learn robust and generalizable representations. In this work, we propose
CLIPin, a unified non-contrastive plug-in that can be seamlessly integrated
into CLIP-style architectures to improve multimodal semantic alignment,
providing stronger supervision and enhancing alignment robustness. Furthermore,
two shared pre-projectors are designed for image and text modalities
respectively to facilitate the integration of contrastive and non-contrastive
learning in a parameter-compromise manner. Extensive experiments on diverse
downstream tasks demonstrate the effectiveness and generality of CLIPin as a
plug-and-play component compatible with various contrastive frameworks. Code is
available at https://github.com/T6Yang/CLIPin.

</details>


### [94] [TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation](https://arxiv.org/abs/2508.06452)
*Mattia Litrico,Mario Valerio Giuffrida,Sebastiano Battiato,Devis Tuia*

Main category: cs.CV

TL;DR: TRUST是一种新型无监督域适应方法，利用语言模态的鲁棒性指导视觉模型适应，通过伪标签生成和不确定性估计提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决复杂域偏移（如地理偏移）中传统UDA方法表现不佳的问题，利用语言模态的鲁棒性提升视觉模型的适应性。

Method: 1. 通过图像描述生成伪标签；2. 使用归一化CLIP相似度估计伪标签不确定性；3. 提出多模态软对比学习损失，对齐视觉和语言特征空间。

Result: 在经典域偏移（DomainNet）和复杂域偏移（GeoNet）上达到新SOTA。

Conclusion: TRUST通过语言模态的引导和不确定性估计，显著提升了UDA在复杂域偏移下的性能。

Abstract: Recent unsupervised domain adaptation (UDA) methods have shown great success
in addressing classical domain shifts (e.g., synthetic-to-real), but they still
suffer under complex shifts (e.g. geographical shift), where both the
background and object appearances differ significantly across domains. Prior
works showed that the language modality can help in the adaptation process,
exhibiting more robustness to such complex shifts. In this paper, we introduce
TRUST, a novel UDA approach that exploits the robustness of the language
modality to guide the adaptation of a vision model. TRUST generates
pseudo-labels for target samples from their captions and introduces a novel
uncertainty estimation strategy that uses normalised CLIP similarity scores to
estimate the uncertainty of the generated pseudo-labels. Such estimated
uncertainty is then used to reweight the classification loss, mitigating the
adverse effects of wrong pseudo-labels obtained from low-quality captions. To
further increase the robustness of the vision model, we propose a multimodal
soft-contrastive learning loss that aligns the vision and language feature
spaces, by leveraging captions to guide the contrastive training of the vision
model on target images. In our contrastive loss, each pair of images acts as
both a positive and a negative pair and their feature representations are
attracted and repulsed with a strength proportional to the similarity of their
captions. This solution avoids the need for hardly determining positive and
negative pairs, which is critical in the UDA setting. Our approach outperforms
previous methods, setting the new state-of-the-art on classical (DomainNet) and
complex (GeoNet) domain shifts. The code will be available upon acceptance.

</details>


### [95] [Text Embedded Swin-UMamba for DeepLesion Segmentation](https://arxiv.org/abs/2508.06453)
*Ruida Cheng,Tejas Sudharshan Mathai,Pritam Mukherjee,Benjamin Hou,Qingqing Zhu,Zhiyong Lu,Matthew McAuliffe,Ronald M. Summers*

Main category: cs.CV

TL;DR: 研究探讨了将大语言模型（LLMs）与Swin-UMamba架构结合用于病变分割的可行性，结果显示性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 通过结合影像特征和放射学报告中的病变描述，提升慢性疾病（如淋巴瘤）临床评估的自动化测量效果。

Method: 使用公开的ULS23 DeepLesion数据集及报告中的简短描述，将文本信息整合到Swin-UMamba架构中进行病变分割。

Result: 测试数据集的Dice Score达82%，Hausdorff距离为6.58像素，性能优于现有模型。

Conclusion: Text-Swin-UMamba模型在病变分割任务中表现优异，为结合文本与影像数据提供了新思路。

Abstract: Segmentation of lesions on CT enables automatic measurement for clinical
assessment of chronic diseases (e.g., lymphoma). Integrating large language
models (LLMs) into the lesion segmentation workflow offers the potential to
combine imaging features with descriptions of lesion characteristics from the
radiology reports. In this study, we investigate the feasibility of integrating
text into the Swin-UMamba architecture for the task of lesion segmentation. The
publicly available ULS23 DeepLesion dataset was used along with short-form
descriptions of the findings from the reports. On the test dataset, a high Dice
Score of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for
lesion segmentation. The proposed Text-Swin-UMamba model outperformed prior
approaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <
0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by
1.74% and 0.22%, respectively. The dataset and code can be accessed at
https://github.com/ruida/LLM-Swin-UMamba

</details>


### [96] [WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion](https://arxiv.org/abs/2508.06485)
*Sofiane Bouaziz,Adel Hafiane,Raphael Canals,Rachid Nedjai*

Main category: cs.CV

TL;DR: WGAST是一种弱监督生成网络，用于通过时空融合Terra MODIS、Landsat 8和Sentinel-2数据，实现每日10米分辨率的地表温度（LST）估计。


<details>
  <summary>Details</summary>
Motivation: 城市化、气候变化和农业压力增加了对精确及时环境监测的需求，而现有遥感系统在空间和时间分辨率之间存在权衡。

Method: WGAST采用条件生成对抗网络架构，包括特征提取、融合、LST重建和噪声抑制四个阶段，并使用弱监督训练策略。

Result: 实验表明，WGAST在定量和定性评估中均优于现有方法，平均降低RMSE 17.18%，提高SSIM 11.00%。

Conclusion: WGAST能够有效捕捉精细尺度的热模式，并对云引起的LST具有鲁棒性，代码已开源。

Abstract: Urbanization, climate change, and agricultural stress are increasing the
demand for precise and timely environmental monitoring. Land Surface
Temperature (LST) is a key variable in this context and is retrieved from
remote sensing satellites. However, these systems face a trade-off between
spatial and temporal resolution. While spatio-temporal fusion methods offer
promising solutions, few have addressed the estimation of daily LST at 10 m
resolution. In this study, we present WGAST, a Weakly-Supervised Generative
Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra
MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning
framework designed for this task. It adopts a conditional generative
adversarial architecture, with a generator composed of four stages: feature
extraction, fusion, LST reconstruction, and noise suppression. The first stage
employs a set of encoders to extract multi-level latent representations from
the inputs, which are then fused in the second stage using cosine similarity,
normalization, and temporal attention mechanisms. The third stage decodes the
fused features into high-resolution LST, followed by a Gaussian filter to
suppress high-frequency noise. Training follows a weakly supervised strategy
based on physical averaging principles and reinforced by a PatchGAN
discriminator. Experiments demonstrate that WGAST outperforms existing methods
in both quantitative and qualitative evaluations. Compared to the
best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves
SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and
effectively captures fine-scale thermal patterns, as validated against 33
ground-based sensors. The code is available at
https://github.com/Sofianebouaziz1/WGAST.git.

</details>


### [97] [Effective Training Data Synthesis for Improving MLLM Chart Understanding](https://arxiv.org/abs/2508.06492)
*Yuwei Yang,Zeyu Zhang,Yunzhong Hou,Zhuowan Li,Gaowen Liu,Ali Payani,Yuan-Sen Ting,Liang Zheng*

Main category: cs.CV

TL;DR: 通过模块化和多样化视觉细节改进图表理解能力，提出五步数据合成流程，生成高质量数据集ECD，显著提升多模态大语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态大语言模型在图表理解任务上表现不佳（成功率30%-50%），且合成图表与真实图表相似度不足，影响模型训练效果。

Method: 设计五步数据合成流程：分离数据和功能生成单图、条件生成多子图、视觉多样化、过滤低质量数据、用GPT-4生成问答对，构建包含10k+图表和300k+问答对的ECD数据集。

Result: ECD显著提升多种多模态大语言模型在真实和合成测试集上的性能。

Conclusion: 模块化和多样化视觉细节的图表生成方法有效提升模型图表理解能力，ECD数据集为后续研究提供高质量资源。

Abstract: Being able to effectively read scientific plots, or chart understanding, is a
central part toward building effective agents for science. However, existing
multimodal large language models (MLLMs), especially open-source ones, are
still falling behind with a typical success rate of 30%-50% on challenging
benchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are
often restricted by their inadequate similarity to the real charts, which could
compromise model training and performance on complex real-world charts. In this
study, we show that modularizing chart generation and diversifying visual
details improves chart understanding capabilities. In particular, we design a
five-step data synthesis pipeline, where we separate data and function creation
for single plot generation, condition the generation of later subplots on
earlier ones for multi-subplot figures, visually diversify the generated
figures, filter out low quality data, and finally generate the question-answer
(QA) pairs with GPT-4o. This approach allows us to streamline the generation of
fine-tuning datasets and introduce the effective chart dataset (ECD), which
contains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring
250+ chart type combinations with high visual complexity. We show that ECD
consistently improves the performance of various MLLMs on a range of real-world
and synthetic test sets. Code, data and models are available at:
https://github.com/yuweiyang-anu/ECD.

</details>


### [98] [LightSwitch: Multi-view Relighting with Material-guided Diffusion](https://arxiv.org/abs/2508.06494)
*Yehonathan Litman,Fernando De la Torre,Shubham Tulsiani*

Main category: cs.CV

TL;DR: Lightswitch是一种基于扩散框架的3D重光照方法，通过利用多视角和材质信息，高效地将输入图像重光照到目标光照条件。


<details>
  <summary>Details</summary>
Motivation: 现有2D重光照生成先验未能充分利用物体的固有属性或大规模多视角数据，导致重光照效果不佳。

Method: 提出Lightswitch框架，结合多视角和材质信息，采用可扩展的去噪方案，实现高效重光照。

Result: Lightswitch在2D重光照预测质量上超越现有方法，并在合成和真实物体的重光照任务中表现优异。

Conclusion: Lightswitch通过整合固有属性和多视角数据，显著提升了重光照的效率和质量。

Abstract: Recent approaches for 3D relighting have shown promise in integrating 2D
image relighting generative priors to alter the appearance of a 3D
representation while preserving the underlying structure. Nevertheless,
generative priors used for 2D relighting that directly relight from an input
image do not take advantage of intrinsic properties of the subject that can be
inferred or cannot consider multi-view data at scale, leading to subpar
relighting. In this paper, we propose Lightswitch, a novel finetuned
material-relighting diffusion framework that efficiently relights an arbitrary
number of input images to a target lighting condition while incorporating cues
from inferred intrinsic properties. By using multi-view and material
information cues together with a scalable denoising scheme, our method
consistently and efficiently relights dense multi-view data of objects with
diverse material compositions. We show that our 2D relighting prediction
quality exceeds previous state-of-the-art relighting priors that directly
relight from images. We further demonstrate that LightSwitch matches or
outperforms state-of-the-art diffusion inverse rendering methods in relighting
synthetic and real objects in as little as 2 minutes.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [99] [Transformer-Based Explainable Deep Learning for Breast Cancer Detection in Mammography: The MammoFormer Framework](https://arxiv.org/abs/2508.06137)
*Ojonugwa Oluwafemi Ejiga Peter,Daniel Emakporuena,Bamidele Dayo Tunde,Maryam Abdulkarim,Abdullahi Bn Umar*

Main category: eess.IV

TL;DR: MammoFormer框架结合Transformer架构和多特征增强技术，解决了CNN在乳腺X光片分析中的局限性，提升了性能并提供了可解释的AI功能。


<details>
  <summary>Details</summary>
Motivation: 乳腺X光片检测因微小异常和专家解读差异而困难，CNN在局部和全局信息处理及可解释性上存在不足。

Method: 开发MammoFormer框架，结合Transformer架构和多特征增强技术，测试了七种架构和四种增强方法。

Result: MammoFormer性能提升达13%，ViT结合AHE准确率达98.3%，Swin Transformer通过HOG增强提升13%。

Conclusion: MammoFormer结合Transformer和特征增强，性能优于CNN，解决了临床AI应用的障碍。

Abstract: Breast cancer detection through mammography interpretation remains difficult
because of the minimal nature of abnormalities that experts need to identify
alongside the variable interpretations between readers. The potential of CNNs
for medical image analysis faces two limitations: they fail to process both
local information and wide contextual data adequately, and do not provide
explainable AI (XAI) operations that doctors need to accept them in clinics.
The researcher developed the MammoFormer framework, which unites
transformer-based architecture with multi-feature enhancement components and
XAI functionalities within one framework. Seven different architectures
consisting of CNNs, Vision Transformer, Swin Transformer, and ConvNext were
tested alongside four enhancement techniques, including original images,
negative transformation, adaptive histogram equalization, and histogram of
oriented gradients. The MammoFormer framework addresses critical clinical
adoption barriers of AI mammography systems through: (1) systematic
optimization of transformer architectures via architecture-specific feature
enhancement, achieving up to 13% performance improvement, (2) comprehensive
explainable AI integration providing multi-perspective diagnostic
interpretability, and (3) a clinically deployable ensemble system combining CNN
reliability with transformer global context modeling. The combination of
transformer models with suitable feature enhancements enables them to achieve
equal or better results than CNN approaches. ViT achieves 98.3% accuracy
alongside AHE while Swin Transformer gains a 13.0% advantage through HOG
enhancements

</details>


### [100] [Clinically-guided Data Synthesis for Laryngeal Lesion Detection](https://arxiv.org/abs/2508.06182)
*Chiara Baldini,Kaisar Kushibar,Richard Osuala,Simone Balocco,Oliver Diaz,Karim Lekadir,Leonardo S. Mattos*

Main category: eess.IV

TL;DR: 该研究提出了一种利用潜在扩散模型（LDM）和ControlNet适配器生成喉镜图像-标注对的新方法，以解决数据稀缺问题，并提升计算机辅助诊断/检测系统的性能。


<details>
  <summary>Details</summary>
Motivation: 当前喉镜领域的计算机辅助诊断/检测系统因数据稀缺和病变异质性高而受限，活检仍是金标准但成本高。

Method: 采用LDM和ControlNet适配器生成临床相关的喉镜图像-标注对，扩展训练数据集。

Result: 仅添加10%合成数据，喉镜病变检测率在内部测试中提升9%，外部数据中提升22.1%；专家难以区分合成与真实图像。

Conclusion: 该方法可加速喉镜疾病诊断自动化工具的开发，为数据稀缺问题提供解决方案，并展示合成数据在实际场景中的适用性。

Abstract: Although computer-aided diagnosis (CADx) and detection (CADe) systems have
made significant progress in various medical domains, their application is
still limited in specialized fields such as otorhinolaryngology. In the latter,
current assessment methods heavily depend on operator expertise, and the high
heterogeneity of lesions complicates diagnosis, with biopsy persisting as the
gold standard despite its substantial costs and risks. A critical bottleneck
for specialized endoscopic CADx/e systems is the lack of well-annotated
datasets with sufficient variability for real-world generalization. This study
introduces a novel approach that exploits a Latent Diffusion Model (LDM)
coupled with a ControlNet adapter to generate laryngeal endoscopic
image-annotation pairs, guided by clinical observations. The method addresses
data scarcity by conditioning the diffusion process to produce realistic,
high-quality, and clinically relevant image features that capture diverse
anatomical conditions. The proposed approach can be leveraged to expand
training datasets for CADx/e models, empowering the assessment process in
laryngology. Indeed, during a downstream task of detection, the addition of
only 10% synthetic data improved the detection rate of laryngeal lesions by 9%
when the model was internally tested and 22.1% on out-of-domain external data.
Additionally, the realism of the generated images was evaluated by asking 5
expert otorhinolaryngologists with varying expertise to rate their confidence
in distinguishing synthetic from real images. This work has the potential to
accelerate the development of automated tools for laryngeal disease diagnosis,
offering a solution to data scarcity and demonstrating the applicability of
synthetic data in real-world scenarios.

</details>


### [101] [Deep Learning Based Reconstruction Methods for Electrical Impedance Tomography](https://arxiv.org/abs/2508.06281)
*Alexander Denker,Fabio Margotti,Jianfeng Ning,Kim Knudsen,Derick Nganyu Tanyu,Bangti Jin,Andreas Hauptmann,Peter Maass*

Main category: eess.IV

TL;DR: 本文综述了深度学习在电阻抗断层扫描（EIT）逆问题中的应用，比较了多种学习方法与传统模型方法的性能。


<details>
  <summary>Details</summary>
Motivation: EIT逆问题具有严重的病态性，需要先进的计算方法进行图像重建。深度学习的最新进展为这一问题提供了新的解决方案。

Method: 研究了基于深度神经网络的学习重建方法，包括全学习、后处理和迭代学习等方法，并与传统模型方法（如稀疏正则化、正则化高斯-牛顿迭代和水平集方法）进行比较。

Result: 学习型方法在分布内数据上优于传统方法，但在泛化性上存在挑战，混合方法在准确性和适应性上表现出较好的平衡。

Conclusion: 深度学习在EIT逆问题中具有潜力，但需要进一步研究以提高泛化能力，混合方法是一个有前景的方向。

Abstract: Electrical Impedance Tomography (EIT) is a powerful imaging modality widely
used in medical diagnostics, industrial monitoring, and environmental studies.
The EIT inverse problem is about inferring the internal conductivity
distribution of the concerned object from the voltage measurements taken on its
boundary. This problem is severely ill-posed, and requires advanced
computational approaches for accurate and reliable image reconstruction. Recent
innovations in both model-based reconstruction and deep learning have driven
significant progress in the field. In this review, we explore learned
reconstruction methods that employ deep neural networks for solving the EIT
inverse problem. The discussion focuses on the complete electrode model, one
popular mathematical model for real-world applications of EIT. We compare a
wide variety of learned approaches, including fully-learned, post-processing
and learned iterative methods, with several conventional model-based
reconstruction techniques, e.g., sparsity regularization, regularized
Gauss-Newton iteration and level set method. The evaluation is based on three
datasets: a simulated dataset of ellipses, an out-of-distribution simulated
dataset, and the KIT4 dataset, including real-world measurements. Our results
demonstrate that learned methods outperform model-based methods for
in-distribution data but face challenges in generalization, where hybrid
methods exhibit a good balance of accuracy and adaptability.

</details>


### [102] [Advanced Deep Learning Techniques for Accurate Lung Cancer Detection and Classification](https://arxiv.org/abs/2508.06287)
*Mobarak Abumohsen,Enrique Costa-Montenegro,Silvia García-Méndez,Amani Yousef Owda,Majdi Owda*

Main category: eess.IV

TL;DR: 本文提出了一种基于DenseNet201模型的创新方法，用于从CT图像中检测和分类肺癌，解决了数据不平衡和过拟合问题，达到了98.95%的高准确率。


<details>
  <summary>Details</summary>
Motivation: 肺癌是全球常见的致命癌症，CT图像因其低成本和处理速度快成为主要诊断方法，但现有技术因数据不平衡导致准确率低。

Method: 采用DenseNet201模型，结合Focal Loss、数据增强和正则化技术，解决数据不平衡和过拟合问题。

Result: 实验结果显示，该方法达到了98.95%的高准确率。

Conclusion: 提出的方法在肺癌检测和分类中表现出色，为解决数据不平衡和过拟合问题提供了有效方案。

Abstract: Lung cancer (LC) ranks among the most frequently diagnosed cancers and is one
of the most common causes of death for men and women worldwide. Computed
Tomography (CT) images are the most preferred diagnosis method because of their
low cost and their faster processing times. Many researchers have proposed
various ways of identifying lung cancer using CT images. However, such
techniques suffer from significant false positives, leading to low accuracy.
The fundamental reason results from employing a small and imbalanced dataset.
This paper introduces an innovative approach for LC detection and
classification from CT images based on the DenseNet201 model. Our approach
comprises several advanced methods such as Focal Loss, data augmentation, and
regularization to overcome the imbalanced data issue and overfitting challenge.
The findings show the appropriateness of the proposal, attaining a promising
performance of 98.95% accuracy.

</details>


### [103] [Multivariate Fields of Experts](https://arxiv.org/abs/2508.06490)
*Stanislas Ducotterd,Michael Unser*

Main category: eess.IV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce the multivariate fields of experts, a new framework for the
learning of image priors. Our model generalizes existing fields of experts
methods by incorporating multivariate potential functions constructed via
Moreau envelopes of the $\ell_\infty$-norm. We demonstrate the effectiveness of
our proposal across a range of inverse problems that include image denoising,
deblurring, compressed-sensing magnetic-resonance imaging, and computed
tomography. The proposed approach outperforms comparable univariate models and
achieves performance close to that of deep-learning-based regularizers while
being significantly faster, requiring fewer parameters, and being trained on
substantially fewer data. In addition, our model retains a relatively high
level of interpretability due to its structured design.

</details>


### [104] [Neural Field-Based 3D Surface Reconstruction of Microstructures from Multi-Detector Signals in Scanning Electron Microscopy](https://arxiv.org/abs/2508.04728)
*Shuo Chen,Yijin Li,Xi Zheng,Guofeng Zhang*

Main category: eess.IV

TL;DR: NFH-SEM是一种基于神经场的混合SEM 3D重建方法，通过多视角、多探测器2D SEM图像输入，结合几何和光度信息，实现复杂微结构的高精度重建。


<details>
  <summary>Details</summary>
Motivation: 传统2D SEM图像无法直接显示3D形貌，现有方法在复杂微结构重建中存在离散3D表示、校准需求和阴影梯度误差等限制。

Method: NFH-SEM利用神经场表示，通过端到端自校准消除手动校准，并在训练中自动分离阴影。

Result: 在真实和模拟数据集上验证，NFH-SEM能够高保真重建多种复杂样本，如双光子光刻微结构、桃花花粉和碳化硅颗粒表面。

Conclusion: NFH-SEM展示了高精度和广泛适用性，为复杂微结构的3D重建提供了新方法。

Abstract: The scanning electron microscope (SEM) is a widely used imaging device in
scientific research and industrial applications. Conventional two-dimensional
(2D) SEM images do not directly reveal the three-dimensional (3D) topography of
micro samples, motivating the development of SEM 3D surface reconstruction
methods. However, reconstruction of complex microstructures remains challenging
for existing methods due to the limitations of discrete 3D representations, the
need for calibration with reference samples, and shadow-induced gradient
errors. Here, we introduce NFH-SEM, a neural field-based hybrid SEM 3D
reconstruction method that takes multi-view, multi-detector 2D SEM images as
input and fuses geometric and photometric information into a continuous neural
field representation. NFH-SEM eliminates the manual calibration procedures
through end-to-end self-calibration and automatically disentangles shadows from
SEM images during training, enabling accurate reconstruction of intricate
microstructures. We validate the effectiveness of NFH-SEM on real and simulated
datasets. Our experiments show high-fidelity reconstructions of diverse,
challenging samples, including two-photon lithography microstructures, peach
pollen, and silicon carbide particle surfaces, demonstrating precise detail and
broad applicability.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [105] [DogFit: Domain-guided Fine-tuning for Efficient Transfer Learning of Diffusion Models](https://arxiv.org/abs/2508.05685)
*Yara Bahram,Mohammadhadi Shateri,Eric Granger*

Main category: cs.GR

TL;DR: DogFit方法通过领域感知的引导偏移和轻量级条件机制，在扩散模型迁移学习中实现了高效可控的保真度-多样性权衡，减少了计算开销。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在迁移到小目标领域时，直接微调通常泛化性差，而现有测试时引导方法计算成本高。

Method: 提出Domain-guided Fine-tuning (DogFit)，在训练损失中注入领域感知引导偏移，并通过轻量级条件机制编码引导强度。

Result: 在多个目标领域实验中，DogFit在FID和FDDINOV2指标上优于现有方法，且计算开销降低2倍。

Conclusion: DogFit是一种高效且可控的扩散模型迁移学习方法，显著提升了性能并减少了计算成本。

Abstract: Transfer learning of diffusion models to smaller target domains is
challenging, as naively fine-tuning the model often results in poor
generalization. Test-time guidance methods help mitigate this by offering
controllable improvements in image fidelity through a trade-off with sample
diversity. However, this benefit comes at a high computational cost, typically
requiring dual forward passes during sampling. We propose the Domain-guided
Fine-tuning (DogFit) method, an effective guidance mechanism for diffusion
transfer learning that maintains controllability without incurring additional
computational overhead. DogFit injects a domain-aware guidance offset into the
training loss, effectively internalizing the guided behavior during the
fine-tuning process. The domain-aware design is motivated by our observation
that during fine-tuning, the unconditional source model offers a stronger
marginal estimate than the target model. To support efficient controllable
fidelity-diversity trade-offs at inference, we encode the guidance strength
value as an additional model input through a lightweight conditioning
mechanism. We further investigate the optimal placement and timing of the
guidance offset during training and propose two simple scheduling strategies,
i.e., late-start and cut-off, which improve generation quality and training
stability. Experiments on DiT and SiT backbones across six diverse target
domains show that DogFit can outperform prior guidance methods in transfer
learning in terms of FID and FDDINOV2 while requiring up to 2x fewer sampling
TFLOPS.

</details>


### [106] [Exploring Interactive Simulation of Grass Display Color Characteristic Based on Real-World Conditions](https://arxiv.org/abs/2508.06086)
*Kojiro Tanaka,Keiichi Sato,Masahiko Mikawa,Makoto Fujisawa*

Main category: cs.GR

TL;DR: 本文提出了一种基于虚拟环境的交互式模拟方法，用于模拟真实条件下草基显示器的颜色变化特性，解决了传统方法耗时耗力的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要每次在光照或视角变化时进行实际设备实验，耗时且成本高。虽然已有研究尝试模拟草的颜色变化，但仍存在单次测量耗时过长的问题。

Method: 在虚拟环境中开发了一种交互式模拟方法，模拟草基显示器在不同视角和环境下的颜色变化特性。

Result: 模拟结果与实际特性相似，且速度更快，准确性与前人研究相当。

Conclusion: 该方法为草基显示器的颜色特性模拟提供了一种高效且准确的解决方案。

Abstract: Recent research has focused on incorporating media into living environments
via color-controlled materials and image display. In particular, grass-based
displays have drawn attention as landscape-friendly interactive interfaces. To
develop the grass display, it is important to obtain the grass color change
characteristics that depend on the real environment. However, conventional
methods require experiments on actual equipment every time the lighting or
viewpoint changes, which is time-consuming and costly. Although research has
begun on simulating grass colors, this approach still faces significant issues
as it takes many hours for a single measurement. In this paper, we explore an
interactive simulation of a grass display color change characteristic based on
real-world conditions in a virtual environment. We evaluated our method's
accuracy by simulating grass color characteristics across multiple viewpoints
and environments, and then compared the results against prior work. The results
indicated that our method tended to simulate the grass color characteristics
similar to the actual characteristics and showed the potential to do so more
quickly and with comparable accuracy to the previous study.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [107] [Scaling of strong-field spherical dynamos](https://arxiv.org/abs/2508.05639)
*Robert J. Teed,Emmanuel Dormy*

Main category: physics.geo-ph

TL;DR: 论文通过数值实验研究地磁场生成的动力学行为，识别出弱场和强场两种分支，并证明了强场分支的持久性及其与地核力平衡的关系。


<details>
  <summary>Details</summary>
Motivation: 理解地磁场生成的动力学机制，特别是强场分支的持久性和其在参数空间中的表现。

Method: 通过数值实验设计，分析分岔图中的不同分支，提出新的输出参数来衡量强场解。

Result: 证明了强场分支的持久性，并提供了其起始的标度律，新参数能有效捕捉强场解的特征。

Conclusion: 强场分支的研究为未来在参数空间中定位强场发电机提供了指导。

Abstract: Numerical experiments of dynamo action designed to understand the generation
of Earth's magnetic field produce different regime branches identified within
bifurcation diagrams. Notable are distinct branches where the resultant
magnetic field is either weak or strong. Weak-field solutions are identified by
the prominent role of viscosity (and/or inertia) on the motion, whereas the
magnetic field has a leading-order effect on the flow in strong-field
solutions. We demonstrate the persistence of the strong-field branch,
preserving the expected force balance of Earth's core, and provide scaling laws
governing its onset as parameters move toward values appropriate for the
Geodynamo. We introduce a new output parameter, based on dynamically important
parts of rotational and magnetic forces, that captures expected $O(1)$ values
of strong-field solutions throughout input parameter space. This new measure of
the field strength and our bounds on scaling laws can guide future studies in
locating strong-field dynamos in parameter space.

</details>


### [108] [The effect of induced flow on the water wave skewness](https://arxiv.org/abs/2508.05651)
*Alexey V. Slunyaev,Anna V. Kokorina*

Main category: physics.geo-ph

TL;DR: 论文研究了二阶理论框架下的非线性波诱导扰动，探讨了其对波浪统计的显著影响，尤其是对底部压力偏斜系数的变化。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解非线性波诱导的扰动如何通过深度调制波浪运动，并影响波浪统计特性。

Method: 采用二阶理论分析非线性波诱导的扰动，重点关注长扰动随深度的缓慢衰减及其对波浪形状和统计的影响。

Result: 研究发现底部压力偏斜系数随水深和波群配置变化显著，而表面位移偏斜系数影响较弱。

Conclusion: 非线性波诱导扰动会显著改变波浪统计特性，尤其是底部压力偏斜系数，其机制与非线性斯托克斯波理论存在明显偏差。

Abstract: Nonlinear wave-induced perturbations are discussed within the framework of
the second-order theory. Due to the slow attenuation of the long perturbations
with depth, they modulate motions beneath surface waves down to the bottom and
can strongly affect the wave statistics. The disturbance which corresponds to
the second harmonic, at a sufficient depth inverses the phase, thereby
qualitatively changes the shape of individual waves. The skewness coefficients
of the surface displacement and the bottom pressure fluctuations are
considered. It is shown that the bottom pressure skewness may have different
signs and magnitudes depending on the water depth and the configuration of
nonlinear wave groups which compose the wave field. The effect on the surface
displacement skewness is much weaker. Mechanisms leading to strong deviation of
the skewness coefficient from the nonlinear Stokes wave theory are discussed.

</details>


### [109] [Comment on "Mineral-water reactions in Earth's mantle: Predictions from Born theory and ab initio molecular dynamics" by Fowler et al. 2024 (Geochim. Cosmochim. Acta 372, 111-123)](https://arxiv.org/abs/2508.05656)
*Jiajia Huang,Ding Pan*

Main category: physics.geo-ph

TL;DR: 本文通过复现AIMD模拟，验证了Pan et al.关于极端条件下水介电常数的计算结果（39.4）是收敛的，与Fowler et al.的结果（51）存在差异，差异源于偶极矩计算方法的不同。


<details>
  <summary>Details</summary>
Motivation: 解决Fowler et al.和Pan et al.在极端条件下水介电常数计算结果的差异问题。

Method: 使用CP2K代码进行复现的AIMD模拟，延长模拟时间并保持系统大小一致。

Result: 验证Pan et al.的结果（39.4）是收敛的，差异源于偶极矩计算方法的不同。

Conclusion: 研究强调了周期性系统中偶极矩波动处理的重要性，对地球地幔中矿物-水相互作用的建模有重要意义。

Abstract: This comment addresses discrepancies in dielectric constant calculations of
water under extreme conditions (~10 GPa and 1000 K) between Fowler et al.'s
recent study [Geochim. Cosmochim. Acta 372, 111-123 (2024)] and the earlier
work by Pan et al. [Proc. Natl. Acad. Sci. 110, 6646-6650 (2013)]. Through
reproduced ab initio molecular dynamics (AIMD) simulations using the CP2K code
with extended duration and identical system size, we rigorously validate that
Pan et al.'s original results (39.4) are well-converged, contrasting with
Fowler et al.'s reported value of 51. The observed discrepancy cannot be
attributed to simulation duration limitations, but rather to methodological
differences in dipole moment calculation. Our analysis highlights critical
issues in the treatment of dipole moment fluctuations in periodic systems
within the framework of modern theory of polarization. This clarification has
significant implications for modeling mineral-water interactions in Earth's
mantle using Born theory.

</details>


### [110] [Covariance based estimates of near-boundary diapycnal upwelling in a submarine canyon](https://arxiv.org/abs/2508.05679)
*Kurt Polzin*

Main category: physics.geo-ph

TL;DR: 该论文通过测量爱尔兰西部大陆坡狭窄海底峡谷中的温度通量，揭示了温度通量与波浪破碎过程的关系，并发现非线性温度方差产生是温度通量分布的根本原因。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过直接测量温度通量，理解海底峡谷中的温度传输机制及其对垂直上升流的影响。

Method: 使用传统紧绷线系泊和垂直剖面仪器测量温度通量和分层结构，结合染料释放研究验证结果。

Result: 在海底50-125米处，温度通量呈上升趋势，垂直通量散度表明上升速度约1毫米/秒，与染料释放研究一致。温度通量与半日频率的波浪破碎过程相关。

Conclusion: 非线性温度方差产生是温度通量分布的根本原因，垂直再分配决定了温度通量剖面。

Abstract: We present direct covariance estimates of temperature flux from a
conventional taut wire mooring placed in a narrow submarine canyon on the
continental slope west of Ireland. Estimates of stratification from both moored
sensors and vertical profiling instrumentation are used to facilitate the
interpretation of these temperature transport estimates in terms of diathermal
upwelling. At depths of 50-125 meters above bottom, the temperature flux is
up-gradient, rather than down the mean gradient, and the corresponding vertical
divergence of the diathermal temperature flux implies a diapycnal velocity of
approximately 1 mm/s, consistent with diathermal migration estimated from a dye
release study. Cospectra document that the temperature flux is related to a
wave breaking process under-pinned by semi-diurnal frequencies. This
up-gradient flux also results in a highly non-local temperature variance
budget. We demonstrate that nonlinear temperature variance production at the
bottom boundary is the root cause, with redistribution aloft determining the
temperature flux profile.

</details>


### [111] [Heat conduction with phase change in soils with macro-pores, snow, and cryoconite. Part I: unified model derivation and examples](https://arxiv.org/abs/2508.05916)
*Malgorzata Peszynska,Praveeni Mathangadeera,Madison Phelps,Forrest Felsch,Noah Unger-Schulz*

Main category: physics.geo-ph

TL;DR: 扩展了热传导模型，从微观到宏观尺度，适用于含大孔隙的土壤、雪和冰尘，并验证了其热力学一致性和实用性。


<details>
  <summary>Details</summary>
Motivation: 解决从微观到宏观尺度的热传导问题，特别是在含大孔隙的土壤、雪和冰尘等新场景中的应用。

Method: 结合土壤模型和整体水模型，定义温度、相分数和内能的关系，并通过多值图框架验证其可逆性。

Result: 模型在热力学上一致，与文献中的实用模型相符，适用于耦合土壤和雪模型以及冰尘的模拟。

Conclusion: 提出的通用模型在理论和实践中均有效，为多尺度热传导问题提供了实用解决方案。

Abstract: In this paper we extend models of thermal conduction with phase transition
from micro- to macro-scale. Such models were previously developed for soils in
permafrost regions from pore to Darcy scale, and the Darcy scale models compare
well to empirical relationships. The new general model blends soil model with
bulk water model and thus works well for the soils with macro-pores; it also
applies to new context including modeling thermal conduction in the snow and in
cryoconite, and it is consistent with rigorous thermodynamics derivations as
well as with practical models from the literature. From mathematical point of
view, the general model relies on carefully defined relationships between
temperature, phase fraction and internal energy, which we show are invertible
in a properly defined framework of multivalued graphs. We also discuss and test
practical models for average heat conductivity. Our framework allows to create
monolithic numerical models useful for modeling of coupled soil and snow models
as well as cryoconite. We motivate and illustrate the results with practical
examples and computations.

</details>


### [112] [Turbulence in the Geomagnetic Field at Earth Surface](https://arxiv.org/abs/2508.06143)
*Mingshu Zhao,Xiaoping Zeng,Yunfang Lin*

Main category: physics.geo-ph

TL;DR: 研究通过中国地面磁观测数据发现地磁场中的湍流行为，并揭示其与大地震的相关性。


<details>
  <summary>Details</summary>
Motivation: 探索地磁场的湍流特性及其与地震触发之间的潜在物理机制。

Method: 利用空间和时间结构函数分析地面磁观测数据，验证扩展自相似性下的Kolmogorov类湍流。

Result: 发现地磁场垂直变化与大地震显著相关，湍流特性可能通过能量传递触发地震。

Conclusion: 太阳活动能量通过湍流过程传递至小尺度，可能参与地震触发机制。

Abstract: We investigated turbulence-like behavior in the geomagnetic field using
ground-based magnetic observatory data across China. Through analysis of
spatial and temporal structure functions, we find power-law scaling consistent
with Kolmogorov-like turbulence under extended self-similarity. We also
identify significant correlations between vertical geomagnetic field variations
and large earthquakes. The combination of turbulent characteristics with these
correlations suggests a physical mechanism where solar activity provides energy
that is transferred through turbulent processes to smaller scales, potentially
contributing to earthquake triggering.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [113] [Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction](https://arxiv.org/abs/2508.05838)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.RO

TL;DR: 论文提出了一种结合视觉基础模型与强化学习的新方法，提升了模拟环境中物体的交互能力。


<details>
  <summary>Details</summary>
Motivation: 通过整合先进的视觉模型（如SAM和YOLOv5）与强化学习，旨在提高智能体在复杂环境中的感知和交互能力。

Method: 使用Segment Anything Model（SAM）和YOLOv5结合PPO强化学习算法，在AI2-THOR模拟环境中进行实验。

Result: 实验显示，与基线相比，平均累积奖励提高了68%，物体交互成功率提升了52.5%，导航效率提高了33%。

Conclusion: 研究表明，将基础模型与强化学习结合，有望推动复杂机器人任务的发展，为更先进的自主智能体铺平道路。

Abstract: This paper presents a novel approach that integrates vision foundation models
with reinforcement learning to enhance object interaction capabilities in
simulated environments. By combining the Segment Anything Model (SAM) and
YOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the
AI2-THOR simulation environment, we enable the agent to perceive and interact
with objects more effectively. Our comprehensive experiments, conducted across
four diverse indoor kitchen settings, demonstrate significant improvements in
object interaction success rates and navigation efficiency compared to a
baseline agent without advanced perception. The results show a 68% increase in
average cumulative reward, a 52.5% improvement in object interaction success
rate, and a 33% increase in navigation efficiency. These findings highlight the
potential of integrating foundation models with reinforcement learning for
complex robotic tasks, paving the way for more sophisticated and capable
autonomous agents.

</details>


### [114] [Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model](https://arxiv.org/abs/2508.06206)
*Hanqing Wang,Shaoyang Wang,Yiming Zhong,Zemin Yang,Jiamin Wang,Zhiqing Cui,Jiahao Yuan,Yifan Han,Mingyu Liu,Yuexin Ma*

Main category: cs.RO

TL;DR: Affordance-R1提出了一种统一的affordance grounding框架，结合GRPO强化学习和认知推理，显著提升了零样本泛化和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型缺乏Chain-of-Thought推理能力，限制了跨领域泛化和显式推理能力。

Method: 设计了一个包含格式、感知和认知奖励的affordance函数，结合GRPO强化学习框架，并构建了ReasonAff数据集。

Result: Affordance-R1在零样本泛化和推理能力上表现优异，超越了现有方法。

Conclusion: Affordance-R1首次将GRPO强化学习与推理结合，为affordance reasoning领域提供了新思路。

Abstract: Affordance grounding focuses on predicting the specific regions of objects
that are associated with the actions to be performed by robots. It plays a
vital role in the fields of human-robot interaction, human-object interaction,
embodied manipulation, and embodied perception. Existing models often neglect
the affordance shared among different objects because they lack the
Chain-of-Thought(CoT) reasoning abilities, limiting their out-of-domain (OOD)
generalization and explicit reasoning capabilities. To address these
challenges, we propose Affordance-R1, the first unified affordance grounding
framework that integrates cognitive CoT guided Group Relative Policy
Optimization (GRPO) within a reinforcement learning paradigm. Specifically, we
designed a sophisticated affordance function, which contains format,
perception, and cognition rewards to effectively guide optimization directions.
Furthermore, we constructed a high-quality affordance-centric reasoning
dataset, ReasonAff, to support training. Trained exclusively via reinforcement
learning with GRPO and without explicit reasoning data, Affordance-R1 achieves
robust zero-shot generalization and exhibits emergent test-time reasoning
capabilities. Comprehensive experiments demonstrate that our model outperforms
well-established methods and exhibits open-world generalization. To the best of
our knowledge, Affordance-R1 is the first to integrate GRPO-based RL with
reasoning into affordance reasoning. The code of our method and our dataset is
released on https://github.com/hq-King/Affordance-R1.

</details>


### [115] [Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation](https://arxiv.org/abs/2508.06426)
*Youguang Xing,Xu Luo,Junlin Xie,Lianli Gao,Hengtao Shen,Jingkuan Song*

Main category: cs.RO

TL;DR: 论文探讨了通用机器人策略在训练数据分布外泛化能力不足的原因，发现任务无关特征的依赖（捷径学习）是主要障碍，并提出数据集收集和增强策略以改善泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究通用机器人策略在大规模数据集（如OXE）训练后泛化能力受限的根源，旨在提升其在实际任务中的表现。

Method: 通过理论和实证分析，识别了捷径学习的两个主要来源：子数据集内多样性不足和子数据集间分布差异。提出了数据集收集和增强策略以减少这些问题。

Result: 发现捷径学习源于数据集结构问题，并提出数据增强策略可有效改善通用机器人策略的泛化能力。

Conclusion: 优化数据集结构和采用数据增强策略是提升通用机器人策略泛化能力的有效途径。

Abstract: Generalist robot policies trained on large-scale datasets such as Open
X-Embodiment (OXE) demonstrate strong performance across a wide range of tasks.
However, they often struggle to generalize beyond the distribution of their
training data. In this paper, we investigate the underlying cause of this
limited generalization capability. We identify shortcut learning -- the
reliance on task-irrelevant features -- as a key impediment to generalization.
Through comprehensive theoretical and empirical analysis, we uncover two
primary contributors to shortcut learning: (1) limited diversity within
individual sub-datasets, and (2) significant distributional disparities across
sub-datasets, leading to dataset fragmentation. These issues arise from the
inherent structure of large-scale datasets like OXE, which are typically
composed of multiple sub-datasets collected independently across varied
environments and embodiments. Our findings provide critical insights into
dataset collection strategies that can reduce shortcut learning and enhance the
generalization ability of generalist robot policies. Moreover, in scenarios
where acquiring new large-scale data is impractical, we demonstrate that
carefully selected robotic data augmentation strategies can effectively reduce
shortcut learning in existing offline datasets, thereby improving
generalization capabilities of generalist robot policies, e.g., $\pi_0$, in
both simulation and real-world environments. More information at
https://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [116] [ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation](https://arxiv.org/abs/2508.06065)
*Daniel Lee,Nikhil Sharma,Donghoon Shin,DaEun Choi,Harsh Sharma,Jeonghwan Kim,Heng Ji*

Main category: cs.HC

TL;DR: ThematicPlane系统通过交互式主题设计平面帮助用户操控高级语义概念（如情绪、风格或叙事基调），弥合创意意图与系统控制之间的差距。


<details>
  <summary>Details</summary>
Motivation: 生成式AI使图像创作更易获取，但非专家用户仍难以精确表达创意意图，现有工具限制了流畅探索。

Method: 引入ThematicPlane系统，支持用户在交互式主题设计平面中导航和操控语义概念。

Result: 探索性研究（N=6）显示，用户能通过熟悉主题进行发散和收敛创作，但需更透明的控制机制。

Conclusion: ThematicPlane支持表达性迭代工作流，为生成设计工具中基于语义的交互提供了新方向。

Abstract: Generative AI has made image creation more accessible, yet aligning outputs
with nuanced creative intent remains challenging, particularly for non-experts.
Existing tools often require users to externalize ideas through prompts or
references, limiting fluid exploration. We introduce ThematicPlane, a system
that enables users to navigate and manipulate high-level semantic concepts
(e.g., mood, style, or narrative tone) within an interactive thematic design
plane. This interface bridges the gap between tacit creative intent and system
control. In our exploratory study (N=6), participants engaged in divergent and
convergent creative modes, often embracing unexpected results as inspiration or
iteration cues. While they grounded their exploration in familiar themes,
differing expectations of how themes mapped to outputs revealed a need for more
explainable controls. Overall, ThematicPlane fosters expressive, iterative
workflows and highlights new directions for intuitive, semantics-driven
interaction in generative design tools.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [117] [Universally Unfiltered and Unseen:Input-Agnostic Multimodal Jailbreaks against Text-to-Image Model Safeguards](https://arxiv.org/abs/2508.05658)
*Song Yan,Hui Wei,Jinlong Fei,Guoliang Yang,Zhengyu Zhao,Zheng Wamg*

Main category: cs.CR

TL;DR: 论文提出了一种多模态越狱攻击方法U3-Attack，通过优化图像背景的对抗性补丁和敏感词的安全改写集，以绕过文本到图像模型的安全检查器。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态越狱攻击方法存在可扩展性差和优化耗时的问题，需要一种更高效且通用的解决方案。

Method: U3-Attack通过优化图像背景的对抗性补丁和安全改写集，绕过安全检查器和提示过滤器。

Result: 实验表明，U3-Attack在开源和商业T2I模型上表现优异，成功率比现有方法高4倍。

Conclusion: U3-Attack是一种高效且通用的多模态越狱攻击方法，显著提升了绕过安全检查的成功率。

Abstract: Various (text) prompt filters and (image) safety checkers have been
implemented to mitigate the misuse of Text-to-Image (T2I) models in creating
Not-Safe-For-Work (NSFW) content.In order to expose potential security
vulnerabilities of such safeguards, multimodal jailbreaks have been
studied.However, existing jailbreaks are limited to prompt-specific and
image-specific perturbations, which suffer from poor scalability and
time-consuming optimization.To address these limitations, we propose
Universally Unfiltered and Unseen (U3)-Attack, a multimodal jailbreak attack
method against T2I safeguards.Specifically, U3-Attack optimizes an adversarial
patch on the image background to universally bypass safety checkers and
optimizes a safe paraphrase set from a sensitive word to universally bypass
prompt filters while eliminating redundant computations.Extensive experimental
results demonstrate the superiority of our U3-Attack on both open-source and
commercial T2I models.For example, on the commercial Runway-inpainting model
with both prompt filter and safety checker, our U3-Attack achieves $~4\times$
higher success rates than the state-of-the-art multimodal jailbreak attack,
MMA-Diffusion.Content Warning: This paper includes examples of NSFW content.

</details>


### [118] [Anti-Tamper Protection for Unauthorized Individual Image Generation](https://arxiv.org/abs/2508.06325)
*Zelin Li,Ruohan Zong,Yifan Liu,Ruichen Yao,Yaokun Liu,Yang Zhang,Dong Wang*

Main category: cs.CR

TL;DR: 提出了一种新型抗篡改扰动（ATP）方法，结合保护扰动和授权扰动，有效防御伪造攻击并检测净化篡改。


<details>
  <summary>Details</summary>
Motivation: 随着个性化图像生成技术的发展，伪造攻击侵犯肖像权和隐私的问题日益严重，现有保护扰动算法易被净化技术绕过。

Method: ATP在频域中引入保护扰动和授权扰动，通过掩码指导确保两者互不干扰，授权扰动分布在全图像素以保持对篡改的敏感性。

Result: 实验证明ATP在各种攻击场景下均能有效防御伪造攻击，保护肖像权和隐私。

Conclusion: ATP为肖像权和隐私保护提供了鲁棒解决方案，代码已开源。

Abstract: With the advancement of personalized image generation technologies, concerns
about forgery attacks that infringe on portrait rights and privacy are growing.
To address these concerns, protection perturbation algorithms have been
developed to disrupt forgery generation. However, the protection algorithms
would become ineffective when forgery attackers apply purification techniques
to bypass the protection. To address this issue, we present a novel approach,
Anti-Tamper Perturbation (ATP). ATP introduces a tamper-proof mechanism within
the perturbation. It consists of protection and authorization perturbations,
where the protection perturbation defends against forgery attacks, while the
authorization perturbation detects purification-based tampering. Both
protection and authorization perturbations are applied in the frequency domain
under the guidance of a mask, ensuring that the protection perturbation does
not disrupt the authorization perturbation. This design also enables the
authorization perturbation to be distributed across all image pixels,
preserving its sensitivity to purification-based tampering. ATP demonstrates
its effectiveness in defending forgery attacks across various attack settings
through extensive experiments, providing a robust solution for protecting
individuals' portrait rights and privacy. Our code is available at:
https://github.com/Seeyn/Anti-Tamper-Perturbation .

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [119] [Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports](https://arxiv.org/abs/2508.05669)
*Jin Khye Tan,En Jun Choong,Ethan Jeremiah Chitty,Yan Pheng Choo,John Hsin Yang Wong,Chern Eu Cheah*

Main category: cs.IR

TL;DR: 该研究提出了一种基于Qwen2.5-VL-7B的微调视觉语言模型，用于从马来西亚审计财务报告中高保真地生成Markdown格式的表格数据，显著优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 从财务文档中准确提取和表示表格结构是文档理解中的关键挑战，尤其是对于监管和分析用例。

Method: 使用2,152个图像-文本对的数据集和LoRA监督微调策略，优化了Qwen2.5-VL-7B模型，并采用双框架评估性能。

Result: 模型在标准评估中达到92.20%的准确率和96.53%的Markdown TEDS分数，显著优于其他模型。

Conclusion: 领域特定微调是连接非结构化财务文档与下游自动化的有效方法，且计算开销低。

Abstract: Accurately extracting and representing the structure of tabular data from
financial documents remains a critical challenge in document understanding,
particularly for regulatory and analytical use cases. This study addresses the
complexity of converting financial tables from Malaysian audited financial
reports into Markdown format, a task complicated by rotated layouts,
multi-level headers, and implicit structural cues. We propose a fine-tuned
vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for
high-fidelity Markdown generation from document images. Our approach includes a
curated dataset of 2,152 image-text pairs with augmentations and a supervised
fine-tuning strategy using LoRA. To assess performance, we evaluated our model
on 100 out-of-sample tables using a dual framework: a criteria-based
LLM-as-a-judge for fine-grained accuracy and our novel Markdown
Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural
fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based
assessment and a 96.53% Markdown TEDS score. This performance significantly
surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized
reasoning-enabled models. Compared to these self-hosted alternatives, it also
significantly reduces inference time. Furthermore, its accuracy exceeds that of
widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash.
These results demonstrate that domain-specific fine-tuning provides an
effective and efficient method to bridge the gap between unstructured financial
documents and downstream automation, rivalling much larger and more general
models without their computational overhead.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [120] [Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models](https://arxiv.org/abs/2508.06151)
*Yong Oh Lee,JeeEun Kim,Jung Woo Lee*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的口腔癌病变合成方法，显著提升了诊断模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决口腔癌诊断中标注数据不足和训练数据多样性的问题。

Method: 使用微调的扩散模型和修复技术合成逼真的口腔癌病变图像。

Result: 分类模型准确率达0.97，检测模型定位准确率为0.85。

Conclusion: 合成图像生成在医学诊断中具有潜力，可推广至其他癌症诊断。

Abstract: In oral cancer diagnostics, the limited availability of annotated datasets
frequently constrains the performance of diagnostic models, particularly due to
the variability and insufficiency of training data. To address these
challenges, this study proposed a novel approach to enhance diagnostic accuracy
by synthesizing realistic oral cancer lesions using an inpainting technique
with a fine-tuned diffusion model. We compiled a comprehensive dataset from
multiple sources, featuring a variety of oral cancer images. Our method
generated synthetic lesions that exhibit a high degree of visual fidelity to
actual lesions, thereby significantly enhancing the performance of diagnostic
algorithms. The results show that our classification model achieved a
diagnostic accuracy of 0.97 in differentiating between cancerous and
non-cancerous tissues, while our detection model accurately identified lesion
locations with 0.85 accuracy. This method validates the potential for synthetic
image generation in medical diagnostics and paves the way for further research
into extending these methods to other types of cancer diagnostics.

</details>


### [121] [FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields](https://arxiv.org/abs/2508.06301)
*Junhyeog Yun,Minui Hong,Gunhee Kim*

Main category: cs.LG

TL;DR: FedMeNF是一种新的联邦元学习方法，通过隐私保护损失函数解决传统FML的隐私泄露问题，实现高效优化和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 神经场学习需要大量数据和计算资源，传统FML方法存在隐私泄露问题。

Method: 提出FedMeNF，采用隐私保护损失函数在本地元优化中减少隐私泄露。

Result: 实验表明，FedMeNF在少样本或非IID数据下仍能快速优化并保持重建性能。

Conclusion: FedMeNF在保护隐私的同时，实现了高效学习和多模态数据适应。

Abstract: Neural fields provide a memory-efficient representation of data, which can
effectively handle diverse modalities and large-scale data. However, learning
to map neural fields often requires large amounts of training data and
computations, which can be limited to resource-constrained edge devices. One
approach to tackle this limitation is to leverage Federated Meta-Learning
(FML), but traditional FML approaches suffer from privacy leakage. To address
these issues, we introduce a novel FML approach called FedMeNF. FedMeNF
utilizes a new privacy-preserving loss function that regulates privacy leakage
in the local meta-optimization. This enables the local meta-learner to optimize
quickly and efficiently without retaining the client's private data. Our
experiments demonstrate that FedMeNF achieves fast optimization speed and
robust reconstruction performance, even with few-shot or non-IID data across
diverse data modalities, while preserving client data privacy.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [122] [KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training](https://arxiv.org/abs/2508.06001)
*Kai Zhang,Peng Wang,Sai Bi,Jianming Zhang,Yuanjun Xiong*

Main category: cs.DC

TL;DR: KnapFormer是一个高效框架，结合工作负载平衡和序列并行，优化扩散变换器（DiT）的分布式训练。


<details>
  <summary>Details</summary>
Motivation: 解决分布式训练中因变长文本输入和混合分辨率数据导致的令牌不平衡问题。

Method: 通过全局背包问题重新分配令牌，最小化每GPU工作负载方差，并集成序列并行技术。

Result: 在真实训练中实现小于1%的工作负载差异，消除滞后效应，速度提升2-3倍。

Conclusion: KnapFormer有效优化了扩散模型的训练效率，适用于混合分辨率和图像-视频联合数据。

Abstract: We present KnapFormer, an efficient and versatile framework to combine
workload balancing and sequence parallelism in distributed training of
Diffusion Transformers (DiT). KnapFormer builds on the insight that strong
synergy exists between sequence parallelism and the need to address the
significant token imbalance across ranks. This imbalance arises from
variable-length text inputs and varying visual token counts in mixed-resolution
and image-video joint training. KnapFormer redistributes tokens by first
gathering sequence length metadata across all ranks in a balancing group and
solving a global knapsack problem. The solver aims to minimize the variances of
total workload per-GPU, while accounting for the effect of sequence
parallelism. By integrating DeepSpeed-Ulysees-based sequence parallelism in the
load-balancing decision process and utilizing a simple semi-empirical workload
model, KnapFormers achieves minimal communication overhead and less than 1%
workload discrepancy in real-world training workloads with sequence length
varying from a few hundred to tens of thousands. It eliminates straggler
effects and achieves 2x to 3x speedup when training state-of-the-art diffusion
models like FLUX on mixed-resolution and image-video joint data corpora. We
open-source the KnapFormer implementation at
https://github.com/Kai-46/KnapFormer/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [123] [Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering](https://arxiv.org/abs/2508.06345)
*Yanbin Wei,Jiangyue Yan,Chun Kang,Yang Chen,Hua Liu,James T. Kwok,Yu Zhang*

Main category: cs.CL

TL;DR: 论文提出DynamicTRF框架，通过动态选择适合的图表示形式（TRF）提升大型多模态模型（LMMs）在零样本图问答任务中的准确性和简洁性。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用单一图表示形式（TRF）未能考虑不同模型或任务的特定需求，导致回答不准确或冗长。

Method: 设计一组零样本图问答专用的TRF（$F_{ZS}$），提出Graph Response Efficiency（GRE）指标衡量性能与简洁性，并开发DynamicTRF框架动态选择最佳TRF。

Result: 在7个领域内算法图问答任务和2个领域外下游任务中，DynamicTRF显著提升了LMMs的零样本图问答准确性。

Conclusion: DynamicTRF通过动态TRF选择有效提升了图问答的准确性和简洁性，为多模态模型在图任务中的应用提供了新思路。

Abstract: Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities
in diverse domain question-answering (QA) tasks, including graph QA that
involves complex graph topologies. However, most current approaches use only a
single type of graph representation, namely Topology Representation Form (TRF),
such as prompt-unified text descriptions or style-fixed visual styles. Those
"one-size-fits-all" approaches fail to consider the specific preferences of
different models or tasks, often leading to incorrect or overly long responses.
To address this, we first analyze the characteristics and weaknesses of
existing TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to
zero-shot graph QA. We then introduce a new metric, Graph Response Efficiency
(GRE), which measures the balance between the performance and the brevity in
graph QA. Built on these, we develop the DynamicTRF framework, which aims to
improve both the accuracy and conciseness of graph QA. To be specific,
DynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based
on their GRE scores, to probe the question-specific TRF preferences. Then it
trains a TRF router on the TRFP dataset, to adaptively assign the best TRF from
$F_{ZS}$ for each question during the inference. Extensive experiments across 7
in-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show
that DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms
of accuracy

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [124] [Progress and new challenges in image-based profiling](https://arxiv.org/abs/2508.05800)
*Erik Serrano,John Peters,Jesko Wagner,Rebecca E. Graham,Zhenghao Chen,Brian Feng,Gisele Miranda,Alexandr A. Kalinin,Loan Vulliard,Jenna Tomkinson,Cameron Mattson,Michael J. Lippincott,Ziqi Kang,Divya Sitani,Dave Bunten,Srijit Seal,Neil O. Carragher,Anne E. Carpenter,Shantanu Singh,Paula A. Marin Zapata,Juan C. Caicedo,Gregory P. Way*

Main category: q-bio.QM

TL;DR: 本文回顾了基于图像的细胞表型分析的计算方法进展，重点介绍了深度学习、单细胞分析和开源工具的发展，同时指出了当前挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 为研究者提供基于图像的细胞表型分析领域的技术进展和挑战的路线图。

Method: 综述了深度学习、单细胞分析和批量效应校正等方法在图像分析中的应用。

Result: 深度学习提升了特征提取和数据分析能力，开源工具促进了可重复性和合作。

Conclusion: 尽管技术进步显著，但仍需创新解决方案应对挑战，推动领域进一步发展。

Abstract: For over two decades, image-based profiling has revolutionized cellular
phenotype analysis. Image-based profiling processes rich, high-throughput,
microscopy data into unbiased measurements that reveal phenotypic patterns
powerful for drug discovery, functional genomics, and cell state
classification. Here, we review the evolving computational landscape of
image-based profiling, detailing current procedures, discussing limitations,
and highlighting future development directions. Deep learning has fundamentally
reshaped image-based profiling, improving feature extraction, scalability, and
multimodal data integration. Methodological advancements such as single-cell
analysis and batch effect correction, drawing inspiration from single-cell
transcriptomics, have enhanced analytical precision. The growth of open-source
software ecosystems and the development of community-driven standards have
further democratized access to image-based profiling, fostering reproducibility
and collaboration across research groups. Despite these advancements, the field
still faces significant challenges requiring innovative solutions. By focusing
on the technical evolution of image-based profiling rather than the
wide-ranging biological applications, our aim with this review is to provide
researchers with a roadmap for navigating the progress and new challenges in
this rapidly advancing domain.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [125] [The Beauty of Anisotropic Mesh Refinement: Omnitrees for Efficient Dyadic Discretizations](https://arxiv.org/abs/2508.06316)
*Theresa Pollinger,Masado Ishii,Jens Domke*

Main category: cs.DS

TL;DR: Omnitrees作为octrees的各向异性扩展，通过局部优化维度细化，提升AMR效率，适用于高维问题。


<details>
  <summary>Details</summary>
Motivation: 解决octrees在各向异性问题中因强制各向同性细化导致的效率低下问题。

Method: 提出omnitrees数据结构，允许局部优化维度细化，减少树深度和宽度。

Result: 在3D形状表示中，omnitrees将收敛速度提升1.5倍，存储效率更高，信息密度更大。

Conclusion: Omnitrees显著提升AMR效率，尤其适用于高维问题，为高维应用开辟新可能。

Abstract: Structured adaptive mesh refinement (AMR), commonly implemented via quadtrees
and octrees, underpins a wide range of applications including databases,
computer graphics, physics simulations, and machine learning. However, octrees
enforce isotropic refinement in regions of interest, which can be especially
inefficient for problems that are intrinsically anisotropic--much resolution is
spent where little information is gained. This paper presents omnitrees as an
anisotropic generalization of octrees and related data structures. Omnitrees
allow to refine only the locally most important dimensions, providing tree
structures that are less deep than bintrees and less wide than octrees. As a
result, the convergence of the AMR schemes can be increased by up to a factor
of the dimensionality d for very anisotropic problems, quickly offsetting their
modest increase in storage overhead. We validate this finding on the problem of
binary shape representation across 4,166 three-dimensional objects: Omnitrees
increase the mean convergence rate by 1.5x, require less storage to achieve
equivalent error bounds, and maximize the information density of the stored
function faster than octrees. These advantages are projected to be even
stronger for higher-dimensional problems. We provide a first validation by
introducing a time-dependent rotation to create four-dimensional
representations, and discuss the properties of their 4-d octree and omnitree
approximations. Overall, omnitree discretizations can make existing AMR
approaches more efficient, and open up new possibilities for high-dimensional
applications.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [126] [Optimizing MV CBCT Imaging Protocols Using NTCP and Secondary Cancer Risk: A Multi-Site Study in Breast, Pelvic, and Head & Neck Radiotherapy](https://arxiv.org/abs/2508.05725)
*Thanh Tai Duong,Tien Phat Luong,Trung Kien Tran,Tuan Linh Duong,Ngoc Anh Nguyen,Quang Hung Nguyen,Peter Sandwall,Parham Alaei,David Bradley,James C. L. Chow*

Main category: physics.med-ph

TL;DR: 研究评估了每日MV-CBCT成像剂量对乳腺癌、盆腔和头颈癌患者的累积放射生物学影响，建议根据年龄、治疗部位和器官放射敏感性个性化成像协议。


<details>
  <summary>Details</summary>
Motivation: 评估MV-CBCT成像剂量对正常组织并发症概率（NTCP）和继发恶性肿瘤绝对超额风险（EAR）的影响，以确定是否需要个性化协议。

Method: 回顾性研究，分析乳腺癌（n=30）、盆腔（n=17）和头颈癌（n=20）患者的数据，使用5 MU和10 MU两种协议，通过NTCP和EAR模型评估风险。

Result: 10 MU协议显著增加乳腺癌患者的肺NTCP和年轻患者的EAR风险；盆腔和头颈癌患者风险较低。年龄越小，继发癌症风险越高。

Conclusion: 建议对40岁以下乳腺癌患者个性化成像协议，其他部位风险较低，无需调整。

Abstract: Purpose: To evaluate the cumulative radiobiological impact of daily
Megavoltage Cone-Beam Computed Tomography (MV-CBCT) imaging dose based on
Normal Tissue Complication Probability (NTCP) and Excess Absolute Risk (EAR) of
secondary malignancies among radiotherapy patients treated for breast, pelvic,
and head and neck cancers. This study investigated whether MV-CBCT imaging dose
warrants protocol personalization according to patient age, anatomical
treatment site, and organ-specific radiosensitivity.
  Methods: This retrospective study included cohorts of breast (n=30), pelvic
(n=17), and head and neck (n=20) cancer patients undergoing radiotherapy with
daily MV-CBCT. Imaging plans using two common protocols (5 MU and 10 MU per
fraction) were analyzed. NTCP values were estimated using logistic and
Lyman-Kutcher-Burman (LKB) models, while EAR was calculated using Schneider's
Organ Equivalent Dose (OED)-based model. Statistical analysis used paired
t-tests, and results were further stratified by age (under 40, 40 to 60, over
60 years).
  Results: In breast cancer patients, NTCP for lung increased significantly
under the 10 MU protocol (p<0.001). EAR was elevated in younger breast patients
(under 40 years), with some exceeding 15 cases per 10,000 person-years. In
pelvic and head and neck groups, NTCP and EAR remained low (under 1 percent),
with no clinically meaningful differences between protocols. Across all sites,
younger age correlated with higher secondary cancer risk.
  Conclusion: Daily 10 MU MV-CBCT presents minimal additional risk in pelvic
and head and neck radiotherapy. For breast cancer patients under 40, however,
it significantly increases secondary cancer risk and lung NTCP. Personalized
imaging protocols are recommended based on age, treatment site, and
radiosensitivity.

</details>
