<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 173]
- [eess.IV](#eess.IV) [Total: 15]
- [cs.GR](#cs.GR) [Total: 11]
- [physics.geo-ph](#physics.geo-ph) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.RO](#cs.RO) [Total: 10]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.CR](#cs.CR) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.MM](#cs.MM) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.LG](#cs.LG) [Total: 12]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [EgoVIS@CVPR: What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning](https://arxiv.org/abs/2506.00101)
*Chi-Hsi Kung,Frangil Ramirez,Juhyung Ha,Yi-Ting Chen,David Crandall,Yi-Hsuan Tsai*

Main category: cs.CV

TL;DR: 论文提出了一种通过学习状态变化描述和生成反事实场景的方法，以提升视频编码器对程序性活动的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能明确学习场景状态变化，限制了程序性活动的建模能力。

Method: 利用LLMs生成状态变化描述作为监督信号，并生成反事实场景以模拟失败结果。

Result: 实验表明，该方法在时间动作分割和错误检测等任务上显著提升性能。

Conclusion: 通过状态变化描述和反事实推理，模型能更好地理解程序性活动的因果关系。

Abstract: Understanding a procedural activity requires modeling both how action steps
transform the scene, and how evolving scene transformations can influence the
sequence of action steps, even those that are accidental or erroneous. Yet,
existing work on procedure-aware video representations fails to explicitly
learned the state changes (scene transformations). In this work, we study
procedure-aware video representation learning by incorporating state-change
descriptions generated by LLMs as supervision signals for video encoders.
Moreover, we generate state-change counterfactuals that simulate hypothesized
failure outcomes, allowing models to learn by imagining the unseen ``What if''
scenarios. This counterfactual reasoning facilitates the model's ability to
understand the cause and effect of each step in an activity. To verify the
procedure awareness of our model, we conduct extensive experiments on
procedure-aware tasks, including temporal action segmentation, error detection,
and more. Our results demonstrate the effectiveness of the proposed
state-change descriptions and their counterfactuals, and achieve significant
improvements on multiple tasks.

</details>


### [2] [Visual Embodied Brain: Let Multimodal Large Language Models See, Think, and Control in Spaces](https://arxiv.org/abs/2506.00123)
*Gen Luo,Ganlin Yang,Ziyang Gong,Guanzhou Chen,Haonan Duan,Erfei Cui,Ronglei Tong,Zhi Hou,Tianyi Zhang,Zhe Chen,Shenglong Ye,Lewei Lu,Jingbo Wang,Wenhai Wang,Jifeng Dai,Yu Qiao,Rongrong Ji,Xizhou Zhu*

Main category: cs.CV

TL;DR: VeBrain是一个统一的多模态大语言模型框架，用于机器人的感知、推理和控制，通过将控制任务转化为文本任务并引入高质量数据集，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以统一多模态理解、视觉空间推理和物理交互能力，VeBrain旨在解决这一问题。

Method: VeBrain将机器人控制任务转化为2D视觉空间的文本任务，并设计适配器将文本信号转换为运动策略，同时引入VeBrain-600k数据集。

Result: 在13个多模态基准和5个空间智能基准上表现优异，相比Qwen2.5-VL在MMVet上提升5.6%，在机器人任务中平均提升50%。

Conclusion: VeBrain展示了强大的适应性、灵活性和组合能力，为多模态大语言模型在机器人领域的应用提供了新方向。

Abstract: The remarkable progress of Multimodal Large Language Models (MLLMs) has
attracted increasing attention to extend them to physical entities like legged
robot. This typically requires MLLMs to not only grasp multimodal understanding
abilities, but also integrate visual-spatial reasoning and physical interaction
capabilities. Nevertheless,existing methods struggle to unify these
capabilities due to their fundamental differences.In this paper, we present the
Visual Embodied Brain (VeBrain), a unified framework for perception, reasoning,
and control in real world. VeBrain reformulates robotic control into common
text-based MLLM tasks in the 2D visual space, thus unifying the objectives and
mapping spaces of different tasks. Then, a novel robotic adapter is proposed to
convert textual control signals from MLLMs to motion policies of real robots.
From the data perspective, we further introduce VeBrain-600k, a high-quality
instruction dataset encompassing various capabilities of VeBrain. In
VeBrain-600k, we take hundreds of hours to collect, curate and annotate the
data, and adopt multimodal chain-of-thought(CoT) to mix the different
capabilities into a single conversation. Extensive experiments on 13 multimodal
benchmarks and 5 spatial intelligence benchmarks demonstrate the superior
performance of VeBrain to existing MLLMs like Qwen2.5-VL. When deployed to
legged robots and robotic arms, VeBrain shows strong adaptability, flexibility,
and compositional capabilities compared to existing methods. For example,
compared to Qwen2.5-VL, VeBrain not only achieves substantial gains on MMVet by
+5.6%, but also excels in legged robot tasks with +50% average gains.

</details>


### [3] [Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware Sign Language Translation](https://arxiv.org/abs/2506.00129)
*Edward Fish,Richard Bowden*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent progress in Sign Language Translation (SLT) has focussed primarily on
improving the representational capacity of large language models to incorporate
Sign Language features. This work explores an alternative direction: enhancing
the geometric properties of skeletal representations themselves. We propose
Geo-Sign, a method that leverages the properties of hyperbolic geometry to
model the hierarchical structure inherent in sign language kinematics. By
projecting skeletal features derived from Spatio-Temporal Graph Convolutional
Networks (ST-GCNs) into the Poincar\'e ball model, we aim to create more
discriminative embeddings, particularly for fine-grained motions like finger
articulations. We introduce a hyperbolic projection layer, a weighted Fr\'echet
mean aggregation scheme, and a geometric contrastive loss operating directly in
hyperbolic space. These components are integrated into an end-to-end
translation framework as a regularisation function, to enhance the
representations within the language model. This work demonstrates the potential
of hyperbolic geometry to improve skeletal representations for Sign Language
Translation, improving on SOTA RGB methods while preserving privacy and
improving computational efficiency. Code available here:
https://github.com/ed-fish/geo-sign.

</details>


### [4] [Detection of Endangered Deer Species Using UAV Imagery: A Comparative Study Between Efficient Deep Learning Approaches](https://arxiv.org/abs/2506.00154)
*Agustín Roca,Gastón Castro,Gabriel Torre,Leonardo J. Colombo,Ignacio Mas,Javier Pereira,Juan I. Giribet*

Main category: cs.CV

TL;DR: 比较YOLOv11和RT-DETR模型在无人机图像中检测被植被遮挡的小目标（如沼泽鹿）的性能，通过添加精确分割掩码提升YOLO模型的检测效果。


<details>
  <summary>Details</summary>
Motivation: 提升无人机图像中野生动物（如沼泽鹿）的检测性能，尤其是在目标小且被遮挡的情况下。

Method: 扩展数据集，添加精确分割掩码，训练带有分割头的YOLO模型，并与RT-DETR模型进行对比。

Result: 带分割头的YOLO模型表现出更优的检测性能。

Conclusion: 该研究为无人机野生动物监测提供了可扩展且精准的AI检测方案。

Abstract: This study compares the performance of state-of-the-art neural networks
including variants of the YOLOv11 and RT-DETR models for detecting marsh deer
in UAV imagery, in scenarios where specimens occupy a very small portion of the
image and are occluded by vegetation. We extend previous analysis adding
precise segmentation masks for our datasets enabling a fine-grained training of
a YOLO model with a segmentation head included. Experimental results show the
effectiveness of incorporating the segmentation head achieving superior
detection performance. This work contributes valuable insights for improving
UAV-based wildlife monitoring and conservation strategies through scalable and
accurate AI-driven detection systems.

</details>


### [5] [Efficient Endangered Deer Species Monitoring with UAV Aerial Imagery and Deep Learning](https://arxiv.org/abs/2506.00164)
*Agustín Roca,Gabriel Torre,Juan I. Giribet,Gastón Castro,Leonardo Colombo,Ignacio Mas,Javier Pereira*

Main category: cs.CV

TL;DR: 论文探讨了利用无人机和深度学习技术检测濒危鹿种的可行性，开发了基于YOLO框架的算法，并在两个项目中验证了其高效性。


<details>
  <summary>Details</summary>
Motivation: 传统人工识别方法成本高且耗时，需要更高效的解决方案来支持濒危物种保护。

Method: 利用高分辨率无人机航拍图像，结合YOLO框架开发定制算法，在两个项目中测试。

Result: 算法能高效识别沼泽鹿，对潘帕斯鹿的适用性初步验证但存在局限。

Conclusion: 研究证明了AI与无人机技术结合在野生动物监测中的潜力，支持了保护工作。

Abstract: This paper examines the use of Unmanned Aerial Vehicles (UAVs) and deep
learning for detecting endangered deer species in their natural habitats. As
traditional identification processes require trained manual labor that can be
costly in resources and time, there is a need for more efficient solutions.
Leveraging high-resolution aerial imagery, advanced computer vision techniques
are applied to automate the identification process of deer across two distinct
projects in Buenos Aires, Argentina. The first project, Pantano Project,
involves the marsh deer in the Paran\'a Delta, while the second, WiMoBo,
focuses on the Pampas deer in Campos del Tuy\'u National Park. A tailored
algorithm was developed using the YOLO framework, trained on extensive datasets
compiled from UAV-captured images. The findings demonstrate that the algorithm
effectively identifies marsh deer with a high degree of accuracy and provides
initial insights into its applicability to Pampas deer, albeit with noted
limitations. This study not only supports ongoing conservation efforts but also
highlights the potential of integrating AI with UAV technology to enhance
wildlife monitoring and management practices.

</details>


### [6] [FastCAR: Fast Classification And Regression for Task Consolidation in Multi-Task Learning to Model a Continuous Property Variable of Detected Object Class](https://arxiv.org/abs/2506.00208)
*Anoop Kini,Andreas Jansche,Timo Bernthaler,Gerhard Schneider*

Main category: cs.CV

TL;DR: FastCAR是一种新颖的多任务学习方法，用于分类和回归任务，通过标签转换技术实现高效训练和推理。


<details>
  <summary>Details</summary>
Motivation: 解决分类和回归任务在异构性下的联合学习问题，特别是在科学和工程领域中对对象分类和连续属性建模的需求。

Method: 采用标签转换方法，仅需单任务回归网络架构，优化训练和推理效率。

Result: 分类准确率达99.54%，回归平均绝对百分比误差为2.4%，训练速度提升2.52倍，推理延迟降低55%。

Conclusion: FastCAR在多任务学习中表现出色，适用于需要高效分类和回归联合建模的场景。

Abstract: FastCAR is a novel task consolidation approach in Multi-Task Learning (MTL)
for a classification and a regression task, despite the non-triviality of task
heterogeneity with only a subtle correlation. The approach addresses the
classification of a detected object (occupying the entire image frame) and
regression for modeling a continuous property variable (for instances of an
object class), a crucial use case in science and engineering. FastCAR involves
a label transformation approach that is amenable for use with only a
single-task regression network architecture. FastCAR outperforms traditional
MTL model families, parametrized in the landscape of architecture and loss
weighting schemes, when learning both tasks are collectively considered
(classification accuracy of 99.54%, regression mean absolute percentage error
of 2.4%). The experiments performed used "Advanced Steel Property Dataset"
contributed by us https://github.com/fastcandr/AdvancedSteel-Property-Dataset.
The dataset comprises 4536 images of 224x224 pixels, annotated with discrete
object classes and its hardness property that can take continuous values. Our
proposed FastCAR approach for task consolidation achieves training time
efficiency (2.52x quicker) and reduced inference latency (55% faster) than
benchmark MTL networks.

</details>


### [7] [Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes](https://arxiv.org/abs/2506.00227)
*Anthony Gosselin,Ge Ya Luo,Luis Lara,Florian Golemo,Derek Nowrouzezahrai,Liam Paull,Alexia Jolicoeur-Martineau,Christopher Pal*

Main category: cs.CV

TL;DR: Ctrl-Crash是一种可控的汽车碰撞视频生成模型，通过输入边界框、碰撞类型和初始帧等信号，实现反事实场景生成，并在视频质量和物理真实性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频扩散技术在生成汽车碰撞等稀缺事件的真实图像方面存在困难，而提升交通安全需要可控且真实的模拟。

Method: 提出Ctrl-Crash模型，利用分类器无关引导（classifier-free guidance）实现细粒度控制，支持多种输入信号（如边界框、碰撞类型等）。

Result: 在定量指标（如FVD、JEDi）和人类评估的物理真实性与视频质量上，Ctrl-Crash均达到最先进水平。

Conclusion: Ctrl-Crash为交通安全的可控模拟提供了有效工具，能够生成高质量的碰撞视频。

Abstract: Video diffusion techniques have advanced significantly in recent years;
however, they struggle to generate realistic imagery of car crashes due to the
scarcity of accident events in most driving datasets. Improving traffic safety
requires realistic and controllable accident simulations. To tackle the
problem, we propose Ctrl-Crash, a controllable car crash video generation model
that conditions on signals such as bounding boxes, crash types, and an initial
image frame. Our approach enables counterfactual scenario generation where
minor variations in input can lead to dramatically different crash outcomes. To
support fine-grained control at inference time, we leverage classifier-free
guidance with independently tunable scales for each conditioning signal.
Ctrl-Crash achieves state-of-the-art performance across quantitative video
quality metrics (e.g., FVD and JEDi) and qualitative measurements based on a
human-evaluation of physical realism and video quality compared to prior
diffusion-based methods.

</details>


### [8] [ZeShot-VQA: Zero-Shot Visual Question Answering Framework with Answer Mapping for Natural Disaster Damage Assessment](https://arxiv.org/abs/2506.00238)
*Ehsan Karimi,Maryam Rahnemoonfar*

Main category: cs.CV

TL;DR: 提出了一种基于视觉语言模型（VLM）的零样本视觉问答（ZeShot-VQA）方法，用于自然灾害后的快速响应，无需微调即可处理新数据集和未见过的答案。


<details>
  <summary>Details</summary>
Motivation: 自然灾害影响广泛，传统VQA模型需微调才能处理新问题，限制了响应效率。

Method: 利用大规模预训练的VLM，提出零样本VQA方法，避免微调需求。

Result: 在FloodNet数据集上验证了方法的有效性，能够处理未见过的答案。

Conclusion: ZeShot-VQA展示了灵活性和高效性，适用于快速灾害响应。

Abstract: Natural disasters usually affect vast areas and devastate infrastructures.
Performing a timely and efficient response is crucial to minimize the impact on
affected communities, and data-driven approaches are the best choice. Visual
question answering (VQA) models help management teams to achieve in-depth
understanding of damages. However, recently published models do not possess the
ability to answer open-ended questions and only select the best answer among a
predefined list of answers. If we want to ask questions with new additional
possible answers that do not exist in the predefined list, the model needs to
be fin-tuned/retrained on a new collected and annotated dataset, which is a
time-consuming procedure. In recent years, large-scale Vision-Language Models
(VLMs) have earned significant attention. These models are trained on extensive
datasets and demonstrate strong performance on both unimodal and multimodal
vision/language downstream tasks, often without the need for fine-tuning. In
this paper, we propose a VLM-based zero-shot VQA (ZeShot-VQA) method, and
investigate the performance of on post-disaster FloodNet dataset. Since the
proposed method takes advantage of zero-shot learning, it can be applied on new
datasets without fine-tuning. In addition, ZeShot-VQA is able to process and
generate answers that has been not seen during the training procedure, which
demonstrates its flexibility.

</details>


### [9] [Chain-of-Frames: Advancing Video Understanding in Multimodal LLMs via Frame-Aware Reasoning](https://arxiv.org/abs/2506.00318)
*Sara Ghazanfari,Francesco Croce,Nicolas Flammarion,Prashanth Krishnamurthy,Farshad Khorrami,Siddharth Garg*

Main category: cs.CV

TL;DR: 论文提出了一种基于视频帧的链式推理（CoF）方法，通过生成与关键帧相关的推理步骤，显著提升了视频LLMs的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视频多模态LLMs中生成链式推理时，未能明确关联视频帧，因此提出一种更直接的方法。

Method: 创建CoF-Data数据集，包含多样化问题和帧相关的推理步骤，并基于此微调视频LLMs。

Result: 模型在多个视频理解基准测试中表现优异，超越现有视频LLMs，并显著减少幻觉率。

Conclusion: CoF方法简单有效，无需额外网络支持，显著提升了视频LLMs的推理能力。

Abstract: Recent work has shown that eliciting Large Language Models (LLMs) to generate
reasoning traces in natural language before answering the user's request can
significantly improve their performance across tasks. This approach has been
extended to multimodal LLMs, where the models can produce chain-of-thoughts
(CoT) about the content of input images and videos. In this work, we propose to
obtain video LLMs whose reasoning steps are grounded in, and explicitly refer
to, the relevant video frames. For this, we first create CoF-Data, a large
dataset of diverse questions, answers, and corresponding frame-grounded
reasoning traces about both natural and synthetic videos, spanning various
topics and tasks. Then, we fine-tune existing video LLMs on this
chain-of-frames (CoF) data. Our approach is simple and self-contained, and,
unlike existing approaches for video CoT, does not require auxiliary networks
to select or caption relevant frames. We show that our models based on CoF are
able to generate chain-of-thoughts that accurately refer to the key frames to
answer the given question. This, in turn, leads to improved performance across
multiple video understanding benchmarks, for example, surpassing leading video
LLMs on Video-MME, MVBench, and VSI-Bench, and notably reducing the
hallucination rate. Code available at
https://github.com/SaraGhazanfari/CoF}{github.com/SaraGhazanfari/CoF.

</details>


### [10] [Improving Optical Flow and Stereo Depth Estimation by Leveraging Uncertainty-Based Learning Difficulties](https://arxiv.org/abs/2506.00324)
*Jisoo Jeong,Hong Cai,Jamie Menjay Lin,Fatih Porikli*

Main category: cs.CV

TL;DR: 论文提出了一种基于不确定性的置信度映射方法，通过DB和OA损失函数分别解决像素学习难度不均和遮挡问题，显著提升了光流和立体深度任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统训练方法对所有像素使用统一的损失函数，忽略了像素和区域间学习难度的显著差异。本文旨在解决这一问题。

Method: 提出了Difficulty Balancing (DB)损失和Occlusion Avoiding (OA)损失，分别针对学习难度不均和遮挡问题。

Result: 实验表明，结合DB和OA损失显著提升了光流和立体深度任务的性能。

Conclusion: 通过DB和OA损失的结合，有效解决了训练中不同类型挑战性像素和区域的问题。

Abstract: Conventional training for optical flow and stereo depth models typically
employs a uniform loss function across all pixels. However, this
one-size-fits-all approach often overlooks the significant variations in
learning difficulty among individual pixels and contextual regions. This paper
investigates the uncertainty-based confidence maps which capture these
spatially varying learning difficulties and introduces tailored solutions to
address them. We first present the Difficulty Balancing (DB) loss, which
utilizes an error-based confidence measure to encourage the network to focus
more on challenging pixels and regions. Moreover, we identify that some
difficult pixels and regions are affected by occlusions, resulting from the
inherently ill-posed matching problem in the absence of real correspondences.
To address this, we propose the Occlusion Avoiding (OA) loss, designed to guide
the network into cycle consistency-based confident regions, where feature
matching is more reliable. By combining the DB and OA losses, we effectively
manage various types of challenging pixels and regions during training.
Experiments on both optical flow and stereo depth tasks consistently
demonstrate significant performance improvements when applying our proposed
combination of the DB and OA losses.

</details>


### [11] [Latent Wavelet Diffusion: Enabling 4K Image Synthesis for Free](https://arxiv.org/abs/2506.00433)
*Luigi Sigillo,Shengfeng He,Danilo Comminiello*

Main category: cs.CV

TL;DR: Latent Wavelet Diffusion (LWD) 是一种轻量级框架，通过增强潜在表示的频谱保真度和聚焦高频细节，实现超高清图像生成（2K至4K），无需额外计算开销。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像合成在生成模型中仍具挑战性，需平衡计算效率与细节保留。

Method: LWD引入三个关键组件：尺度一致的变分自编码器目标、小波能量图和时间依赖的掩码策略，以增强潜在表示和聚焦高频细节。

Result: LWD在超高清图像合成中显著提升感知质量并降低FID，优于基线模型。

Conclusion: 频率感知的信号驱动监督是一种高效且原理明确的高分辨率生成建模方法。

Abstract: High-resolution image synthesis remains a core challenge in generative
modeling, particularly in balancing computational efficiency with the
preservation of fine-grained visual detail. We present Latent Wavelet Diffusion
(LWD), a lightweight framework that enables any latent diffusion model to scale
to ultra-high-resolution image generation (2K to 4K) for free. LWD introduces
three key components: (1) a scale-consistent variational autoencoder objective
that enhances the spectral fidelity of latent representations; (2) wavelet
energy maps that identify and localize detail-rich spatial regions within the
latent space; and (3) a time-dependent masking strategy that focuses denoising
supervision on high-frequency components during training. LWD requires no
architectural modifications and incurs no additional computational overhead.
Despite its simplicity, it consistently improves perceptual quality and reduces
FID in ultra-high-resolution image synthesis, outperforming strong baseline
models. These results highlight the effectiveness of frequency-aware,
signal-driven supervision as a principled and efficient approach for
high-resolution generative modeling.

</details>


### [12] [Towards Effective and Efficient Adversarial Defense with Diffusion Models for Robust Visual Tracking](https://arxiv.org/abs/2506.00325)
*Long Xu,Peng Gao,Wen-Jia Tang,Fei Wang,Ru-Yue Yuan*

Main category: cs.CV

TL;DR: 论文提出了一种基于去噪扩散概率模型（DiffDf）的对抗防御方法，显著提升了视觉跟踪方法在对抗攻击下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习视觉跟踪方法取得了进展，但在对抗攻击下性能急剧下降，因此需要一种有效的防御方法。

Method: DiffDf结合像素级重建损失、语义一致性损失和结构相似性损失，通过多尺度防御机制逐步去噪抑制对抗扰动。

Result: 实验表明DiffDf对不同架构的跟踪器具有优异泛化性能，显著提升评价指标，并实现实时推理速度（30 FPS）。

Conclusion: DiffDf在防御性能和效率上表现突出，为视觉跟踪对抗防御提供了新思路。

Abstract: Although deep learning-based visual tracking methods have made significant
progress, they exhibit vulnerabilities when facing carefully designed
adversarial attacks, which can lead to a sharp decline in tracking performance.
To address this issue, this paper proposes for the first time a novel
adversarial defense method based on denoise diffusion probabilistic models,
termed DiffDf, aimed at effectively improving the robustness of existing visual
tracking methods against adversarial attacks. DiffDf establishes a multi-scale
defense mechanism by combining pixel-level reconstruction loss, semantic
consistency loss, and structural similarity loss, effectively suppressing
adversarial perturbations through a gradual denoising process. Extensive
experimental results on several mainstream datasets show that the DiffDf method
demonstrates excellent generalization performance for trackers with different
architectures, significantly improving various evaluation metrics while
achieving real-time inference speeds of over 30 FPS, showcasing outstanding
defense performance and efficiency. Codes are available at
https://github.com/pgao-lab/DiffDf.

</details>


### [13] [Latent Guidance in Diffusion Models for Perceptual Evaluations](https://arxiv.org/abs/2506.00327)
*Shreshth Saini,Ru-Ling Liao,Yan Ye,Alan C. Bovik*

Main category: cs.CV

TL;DR: 论文提出了一种名为PMG的算法，利用预训练的潜在扩散模型和感知质量特征，在NR-IQA任务中实现感知一致性。实验表明该方法在IQA数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索潜在扩散模型在NR-IQA任务中的感知一致性，并利用其特性改进图像质量评估。

Method: 提出Perceptual Manifold Guidance (PMG)算法，结合预训练扩散模型和感知特征，生成多尺度、多时间步的特征图。

Result: 实验证明PMG生成的特征与人类感知高度相关，并在IQA任务中达到SOTA性能。

Conclusion: PMG展示了扩散模型在NR-IQA任务中的优越泛化能力，为相关研究提供了新思路。

Abstract: Despite recent advancements in latent diffusion models that generate
high-dimensional image data and perform various downstream tasks, there has
been little exploration into perceptual consistency within these models on the
task of No-Reference Image Quality Assessment (NR-IQA). In this paper, we
hypothesize that latent diffusion models implicitly exhibit perceptually
consistent local regions within the data manifold. We leverage this insight to
guide on-manifold sampling using perceptual features and input measurements.
Specifically, we propose Perceptual Manifold Guidance (PMG), an algorithm that
utilizes pretrained latent diffusion models and perceptual quality features to
obtain perceptually consistent multi-scale and multi-timestep feature maps from
the denoising U-Net. We empirically demonstrate that these hyperfeatures
exhibit high correlation with human perception in IQA tasks. Our method can be
applied to any existing pretrained latent diffusion model and is
straightforward to integrate. To the best of our knowledge, this paper is the
first work on guiding diffusion model with perceptual features for NR-IQA.
Extensive experiments on IQA datasets show that our method, LGDM, achieves
state-of-the-art performance, underscoring the superior generalization
capabilities of diffusion models for NR-IQA tasks.

</details>


### [14] [Test-time Vocabulary Adaptation for Language-driven Object Detection](https://arxiv.org/abs/2506.00333)
*Mingxuan Liu,Tyler L. Hayes,Massimiliano Mancini,Elisa Ricci,Riccardo Volpi,Gabriela Csurka*

Main category: cs.CV

TL;DR: 提出了一种无需训练的Vocabulary Adapter（VocAda），通过图像描述和名词解析自动优化用户定义的词汇，提升开放词汇目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 开放词汇目标检测允许用户自由指定类别词汇，但词汇可能过于宽泛或错误，影响检测性能。

Method: VocAda在推理时通过图像描述、名词解析和词汇筛选三步优化用户词汇。

Result: 在COCO和Objects365数据集上，VocAda显著提升了三种先进检测器的性能。

Conclusion: VocAda是一种通用且无需训练的词汇优化方法，代码已开源。

Abstract: Open-vocabulary object detection models allow users to freely specify a class
vocabulary in natural language at test time, guiding the detection of desired
objects. However, vocabularies can be overly broad or even mis-specified,
hampering the overall performance of the detector. In this work, we propose a
plug-and-play Vocabulary Adapter (VocAda) to refine the user-defined
vocabulary, automatically tailoring it to categories that are relevant for a
given image. VocAda does not require any training, it operates at inference
time in three steps: i) it uses an image captionner to describe visible
objects, ii) it parses nouns from those captions, and iii) it selects relevant
classes from the user-defined vocabulary, discarding irrelevant ones.
Experiments on COCO and Objects365 with three state-of-the-art detectors show
that VocAda consistently improves performance, proving its versatility. The
code is open source.

</details>


### [15] [Feature Fusion and Knowledge-Distilled Multi-Modal Multi-Target Detection](https://arxiv.org/abs/2506.00365)
*Ngoc Tuyen Do,Tri Nhu Do*

Main category: cs.CV

TL;DR: 提出了一种基于特征融合和知识蒸馏的多模态多目标检测与分类框架，结合RGB和热成像输入，通过多阶段训练和复合损失函数优化后验概率，显著提升了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决多目标检测与分类中因异构数据输入和计算复杂性导致的挑战，特别是在资源受限的嵌入式设备上部署AI解决方案时的困难。

Method: 采用特征融合和知识蒸馏技术，结合RGB和热成像输入，设计多模态模型和多阶段训练流程，使用复合损失函数优化后验概率。

Result: 学生模型的平均精度达到教师模型的95%，推理时间减少约50%，适用于实际部署场景。

Conclusion: 该框架有效提升了多目标检测与分类的准确性和效率，适合资源受限的嵌入式设备应用。

Abstract: In the surveillance and defense domain, multi-target detection and
classification (MTD) is considered essential yet challenging due to
heterogeneous inputs from diverse data sources and the computational complexity
of algorithms designed for resource-constrained embedded devices, particularly
for Al-based solutions. To address these challenges, we propose a feature
fusion and knowledge-distilled framework for multi-modal MTD that leverages
data fusion to enhance accuracy and employs knowledge distillation for improved
domain adaptation. Specifically, our approach utilizes both RGB and thermal
image inputs within a novel fusion-based multi-modal model, coupled with a
distillation training pipeline. We formulate the problem as a posterior
probability optimization task, which is solved through a multi-stage training
pipeline supported by a composite loss function. This loss function effectively
transfers knowledge from a teacher model to a student model. Experimental
results demonstrate that our student model achieves approximately 95% of the
teacher model's mean Average Precision while reducing inference time by
approximately 50%, underscoring its suitability for practical MTD deployment
scenarios.

</details>


### [16] [Sequence-Based Identification of First-Person Camera Wearers in Third-Person Views](https://arxiv.org/abs/2506.00394)
*Ziwei Zhao,Xizi Wang,Yuchen Wang,Feng Cheng,David Crandall*

Main category: cs.CV

TL;DR: TF2025数据集扩展了多视角同步数据，并提出了一种基于序列的方法来识别第三人称视角中的第一人称佩戴者。


<details>
  <summary>Details</summary>
Motivation: 研究多摄像头佩戴者之间的互动，填补现有数据集在沉浸式学习和协作机器人等应用中的不足。

Method: 结合运动线索和行人重识别技术，提出序列化方法识别第三人称视角中的第一人称佩戴者。

Result: TF2025数据集为多视角互动研究提供了新资源，方法有效识别第一人称佩戴者。

Conclusion: TF2025填补了多摄像头互动研究的空白，为相关应用提供了支持。

Abstract: The increasing popularity of egocentric cameras has generated growing
interest in studying multi-camera interactions in shared environments. Although
large-scale datasets such as Ego4D and Ego-Exo4D have propelled egocentric
vision research, interactions between multiple camera wearers remain
underexplored-a key gap for applications like immersive learning and
collaborative robotics. To bridge this, we present TF2025, an expanded dataset
with synchronized first- and third-person views. In addition, we introduce a
sequence-based method to identify first-person wearers in third-person footage,
combining motion cues and person re-identification.

</details>


### [17] [iDPA: Instance Decoupled Prompt Attention for Incremental Medical Object Detection](https://arxiv.org/abs/2506.00406)
*Huahui Yi,Wei Xu,Ziyuan Qin,Xi Chen,Xiaohu Wu,Kang Li,Qicheng Lao*

Main category: cs.CV

TL;DR: 论文提出了一种名为\method的框架，通过解耦实例级提示生成和提示注意力，解决了医学目标检测任务中的挑战，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的方法在医学目标检测任务中面临前景-背景信息耦合以及提示与图像-文本标记耦合的问题，导致性能受限。

Method: \method框架包含两部分：1) 实例级提示生成（\ipg），解耦细粒度实例级知识并生成专注于密集预测的提示；2) 解耦提示注意力（\dpa），优化提示信息传递并减少内存使用。

Result: 在13个临床数据集上，\method在多种设置下（全数据、1-shot、10-shot、50-shot）FAP分别提升了5.44%、4.83%、12.88%和4.59%。

Conclusion: \method通过解耦设计有效解决了医学目标检测中的挑战，显著优于现有方法。

Abstract: Existing prompt-based approaches have demonstrated impressive performance in
continual learning, leveraging pre-trained large-scale models for
classification tasks; however, the tight coupling between foreground-background
information and the coupled attention between prompts and image-text tokens
present significant challenges in incremental medical object detection tasks,
due to the conceptual gap between medical and natural domains. To overcome
these challenges, we introduce the \method~framework, which comprises two main
components: 1) Instance-level Prompt Generation (\ipg), which decouples
fine-grained instance-level knowledge from images and generates prompts that
focus on dense predictions, and 2) Decoupled Prompt Attention (\dpa), which
decouples the original prompt attention, enabling a more direct and efficient
transfer of prompt information while reducing memory usage and mitigating
catastrophic forgetting. We collect 13 clinical, cross-modal, multi-organ, and
multi-category datasets, referred to as \dataset, and experiments demonstrate
that \method~outperforms existing SOTA methods, with FAP improvements of
5.44\%, 4.83\%, 12.88\%, and 4.59\% in full data, 1-shot, 10-shot, and 50-shot
settings, respectively.

</details>


### [18] [Efficient 3D Brain Tumor Segmentation with Axial-Coronal-Sagittal Embedding](https://arxiv.org/abs/2506.00434)
*Tuan-Luc Huynh,Thanh-Danh Le,Tam V. Nguyen,Trung-Nghia Le,Minh-Triet Tran*

Main category: cs.CV

TL;DR: 论文提出改进脑肿瘤分割的方法，通过整合轴向-冠状-矢状卷积和预训练权重，减少训练时间和参数，提升效率。


<details>
  <summary>Details</summary>
Motivation: 当前nnU-Net在脑肿瘤分割中表现优异，但训练需求高且预训练权重利用不足。

Method: 结合Axial-Coronal-Sagittal卷积和ImageNet预训练权重，提出两种2D到3D权重迁移策略，并探索联合分类与分割模型。

Result: 实验表明，所提方法在快速训练设置下性能可比或优于交叉验证模型集成。

Conclusion: 方法显著提升脑肿瘤分割效率与性能，尤其在挑战性标签上表现突出。

Abstract: In this paper, we address the crucial task of brain tumor segmentation in
medical imaging and propose innovative approaches to enhance its performance.
The current state-of-the-art nnU-Net has shown promising results but suffers
from extensive training requirements and underutilization of pre-trained
weights. To overcome these limitations, we integrate Axial-Coronal-Sagittal
convolutions and pre-trained weights from ImageNet into the nnU-Net framework,
resulting in reduced training epochs, reduced trainable parameters, and
improved efficiency. Two strategies for transferring 2D pre-trained weights to
the 3D domain are presented, ensuring the preservation of learned relationships
and feature representations critical for effective information propagation.
Furthermore, we explore a joint classification and segmentation model that
leverages pre-trained encoders from a brain glioma grade classification proxy
task, leading to enhanced segmentation performance, especially for challenging
tumor labels. Experimental results demonstrate that our proposed methods in the
fast training settings achieve comparable or even outperform the ensemble of
cross-validation models, a common practice in the brain tumor segmentation
literature.

</details>


### [19] [Performance Analysis of Few-Shot Learning Approaches for Bangla Handwritten Character and Digit Recognition](https://arxiv.org/abs/2506.00447)
*Mehedi Ahamed,Radib Bin Kabir,Tawsif Tashwar Dipto,Mueeze Al Mushabbir,Sabbir Ahmed,Md. Hasanul Kabir*

Main category: cs.CV

TL;DR: 本研究探讨了少样本学习（FSL）方法在识别孟加拉语手写字符和数字中的表现，提出了一种名为SynergiProtoNet的混合网络模型，显著提升了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语脚本结构复杂且数据稀缺，研究旨在验证FSL方法在此类语言中的适用性，并推广到其他类似复杂度的语言。

Method: 提出SynergiProtoNet，结合聚类技术和嵌入框架，通过多级特征提取优化原型学习框架。

Result: SynergiProtoNet在多种评估设置中均优于现有方法，成为手写字符和数字识别的少样本学习新基准。

Conclusion: SynergiProtoNet为复杂脚本的少样本学习提供了有效解决方案，代码已开源。

Abstract: This study investigates the performance of few-shot learning (FSL) approaches
in recognizing Bangla handwritten characters and numerals using limited labeled
data. It demonstrates the applicability of these methods to scripts with
intricate and complex structures, where dataset scarcity is a common challenge.
Given the complexity of Bangla script, we hypothesize that models performing
well on these characters can generalize effectively to languages of similar or
lower structural complexity. To this end, we introduce SynergiProtoNet, a
hybrid network designed to improve the recognition accuracy of handwritten
characters and digits. The model integrates advanced clustering techniques with
a robust embedding framework to capture fine-grained details and contextual
nuances. It leverages multi-level (both high- and low-level) feature extraction
within a prototypical learning framework. We rigorously benchmark
SynergiProtoNet against several state-of-the-art few-shot learning models:
BD-CSPN, Prototypical Network, Relation Network, Matching Network, and
SimpleShot, across diverse evaluation settings including Monolingual
Intra-Dataset Evaluation, Monolingual Inter-Dataset Evaluation, Cross-Lingual
Transfer, and Split Digit Testing. Experimental results show that
SynergiProtoNet consistently outperforms existing methods, establishing a new
benchmark in few-shot learning for handwritten character and digit recognition.
The code is available on GitHub:
https://github.com/MehediAhamed/SynergiProtoNet.

</details>


### [20] [BAGNet: A Boundary-Aware Graph Attention Network for 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2506.00475)
*Wei Tao,Xiaoyang Qu,Kai Lu,Jiguang Wan,Shenglin He,Jianzong Wang*

Main category: cs.CV

TL;DR: BAGNet提出了一种基于边界感知的图注意力网络，通过优化边界点特征提取和全局特征聚合，提升了点云语义分割的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 点云数据的不规则性和无结构性使得语义分割任务具有挑战性。传统图方法因计算成本高而受限，边界点的复杂空间结构信息未被充分利用。

Method: BAGNet包含边界感知图注意力层（BAGLayer）和轻量级注意力池化层，前者通过边顶点融合和注意力系数捕获边界点特征，后者提取全局特征。

Result: 在标准数据集上，BAGNet在点云语义分割中表现出更高的准确性和更短的推理时间，优于现有方法。

Conclusion: BAGNet通过边界感知和轻量级设计，有效解决了点云分割的计算和精度问题，为相关任务提供了新思路。

Abstract: Since the point cloud data is inherently irregular and unstructured, point
cloud semantic segmentation has always been a challenging task. The graph-based
method attempts to model the irregular point cloud by representing it as a
graph; however, this approach incurs substantial computational cost due to the
necessity of constructing a graph for every point within a large-scale point
cloud. In this paper, we observe that boundary points possess more intricate
spatial structural information and develop a novel graph attention network
known as the Boundary-Aware Graph attention Network (BAGNet). On one hand,
BAGNet contains a boundary-aware graph attention layer (BAGLayer), which
employs edge vertex fusion and attention coefficients to capture features of
boundary points, reducing the computation time. On the other hand, BAGNet
employs a lightweight attention pooling layer to extract the global feature of
the point cloud to maintain model accuracy. Extensive experiments on standard
datasets demonstrate that BAGNet outperforms state-of-the-art methods in point
cloud semantic segmentation with higher accuracy and less inference time.

</details>


### [21] [SSAM: Self-Supervised Association Modeling for Test-Time Adaption](https://arxiv.org/abs/2506.00513)
*Yaxiong Wang,Zhenqiang Zhang,Lechao Cheng,Zhun Zhong,Dan Guo,Meng Wang*

Main category: cs.CV

TL;DR: 论文提出了一种新的测试时适应（TTA）框架SSAM，通过动态编码器优化解决现有方法因缺乏显式监督而冻结图像编码器的问题。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法通常冻结图像编码器，忽略了其在缓解训练与测试数据分布偏移中的关键作用。

Method: SSAM框架包含两个组件：软原型估计（SPE）和原型锚定图像重建（PIR），通过双阶段关联学习动态优化编码器。

Result: 实验表明，SSAM在多种基准测试中显著优于现有TTA方法，同时保持计算效率。

Conclusion: SSAM通过动态编码器优化和架构无关设计，提升了TTA的实用性和性能。

Abstract: Test-time adaption (TTA) has witnessed important progress in recent years,
the prevailing methods typically first encode the image and the text and design
strategies to model the association between them. Meanwhile, the image encoder
is usually frozen due to the absence of explicit supervision in TTA scenarios.
We identify a critical limitation in this paradigm: While test-time images
often exhibit distribution shifts from training data, existing methods
persistently freeze the image encoder due to the absence of explicit
supervision during adaptation. This practice overlooks the image encoder's
crucial role in bridging distribution shift between training and test. To
address this challenge, we propose SSAM (Self-Supervised Association Modeling),
a new TTA framework that enables dynamic encoder refinement through dual-phase
association learning. Our method operates via two synergistic components: 1)
Soft Prototype Estimation (SPE), which estimates probabilistic category
associations to guide feature space reorganization, and 2) Prototype-anchored
Image Reconstruction (PIR), enforcing encoder stability through
cluster-conditional image feature reconstruction. Comprehensive experiments
across diverse baseline methods and benchmarks demonstrate that SSAM can
surpass state-of-the-art TTA baselines by a clear margin while maintaining
computational efficiency. The framework's architecture-agnostic design and
minimal hyperparameter dependence further enhance its practical applicability.

</details>


### [22] [SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image Distillation](https://arxiv.org/abs/2506.00523)
*Xingtong Ge,Xin Zhang,Tongda Xu,Yi Zhang,Xinjie Zhang,Yan Wang,Jun Zhang*

Main category: cs.CV

TL;DR: 论文提出了隐式分布对齐（IDA）和段内引导（ISG）方法，解决了Distribution Matching Distillation（DMD）在大规模文本到图像模型（如SD 3.5和FLUX）上的收敛问题，并最终开发出性能优越的SenseFlow模型。


<details>
  <summary>Details</summary>
Motivation: 解决DMD在大规模文本到图像模型（如SD 3.5和FLUX）上的收敛困难问题。

Method: 提出隐式分布对齐（IDA）和段内引导（ISG）方法，并结合其他改进（如扩大判别器模型）。

Result: DMD成功收敛于SD 3.5和FLUX.1 dev，最终模型SenseFlow在SDXL、SD 3.5 Large和FLUX等模型上表现出优越性能。

Conclusion: 通过IDA和ISG等方法，成功解决了DMD在大规模模型上的收敛问题，并开发出高性能的SenseFlow模型。

Abstract: The Distribution Matching Distillation (DMD) has been successfully applied to
text-to-image diffusion models such as Stable Diffusion (SD) 1.5. However,
vanilla DMD suffers from convergence difficulties on large-scale flow-based
text-to-image models, such as SD 3.5 and FLUX. In this paper, we first analyze
the issues when applying vanilla DMD on large-scale models. Then, to overcome
the scalability challenge, we propose implicit distribution alignment (IDA) to
regularize the distance between the generator and fake distribution.
Furthermore, we propose intra-segment guidance (ISG) to relocate the timestep
importance distribution from the teacher model. With IDA alone, DMD converges
for SD 3.5; employing both IDA and ISG, DMD converges for SD 3.5 and FLUX.1
dev. Along with other improvements such as scaled up discriminator models, our
final model, dubbed \textbf{SenseFlow}, achieves superior performance in
distillation for both diffusion based text-to-image models such as SDXL, and
flow-matching models such as SD 3.5 Large and FLUX. The source code will be
avaliable at https://github.com/XingtongGe/SenseFlow.

</details>


### [23] [3D Trajectory Reconstruction of Moving Points Based on Asynchronous Cameras](https://arxiv.org/abs/2506.00541)
*Huayu Huang,Banglei Guan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于异步相机的3D轨迹重建方法，同时解决了轨迹重建和相机同步两个子问题，显著提高了重建精度。


<details>
  <summary>Details</summary>
Motivation: 在光学实验力学中，定位移动目标并重建其轨迹是一个关键问题，传统方法仅能单独解决轨迹重建或相机同步问题。

Method: 扩展轨迹交会法以适用于异步相机，建立相机时间信息和目标运动模型，同时优化相机旋转、时间信息和目标运动参数。

Result: 仿真和真实实验验证了方法的可行性，真实实验中在15~20 km观测范围内实现了112.95 m的定位误差。

Conclusion: 所提方法在异步相机条件下显著提高了轨迹重建精度，尤其在相机旋转不准确时表现更优。

Abstract: Photomechanics is a crucial branch of solid mechanics. The localization of
point targets constitutes a fundamental problem in optical experimental
mechanics, with extensive applications in various missions of UAVs. Localizing
moving targets is crucial for analyzing their motion characteristics and
dynamic properties. Reconstructing the trajectories of points from asynchronous
cameras is a significant challenge. It encompasses two coupled sub-problems:
trajectory reconstruction and camera synchronization. Present methods typically
address only one of these sub-problems individually. This paper proposes a 3D
trajectory reconstruction method for point targets based on asynchronous
cameras, simultaneously solving both sub-problems. Firstly, we extend the
trajectory intersection method to asynchronous cameras to resolve the
limitation of traditional triangulation that requires camera synchronization.
Secondly, we develop models for camera temporal information and target motion,
based on imaging mechanisms and target dynamics characteristics. The parameters
are optimized simultaneously to achieve trajectory reconstruction without
accurate time parameters. Thirdly, we optimize the camera rotations alongside
the camera time information and target motion parameters, using tighter and
more continuous constraints on moving points. The reconstruction accuracy is
significantly improved, especially when the camera rotations are inaccurate.
Finally, the simulated and real-world experimental results demonstrate the
feasibility and accuracy of the proposed method. The real-world results
indicate that the proposed algorithm achieved a localization error of 112.95 m
at an observation range of 15 ~ 20 km.

</details>


### [24] [ViVo: A Dataset for Volumetric VideoReconstruction and Compression](https://arxiv.org/abs/2506.00558)
*Adrian Azzarelli,Ge Gao,Ho Man Kwan,Fan Zhang,Nantheera Anantrasirichai,Ollie Moolan-Feroze,David Bull*

Main category: cs.CV

TL;DR: ViVo数据集是一个用于神经体积视频重建和压缩的多样化、真实数据集，填补了现有数据集的不足，并展示了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有体积视频数据集缺乏多样性和真实性，无法满足实际生产需求，因此需要新的数据集。

Method: 提出ViVo数据集，包含多视角RGB和深度视频对、校准数据、音频、2D前景掩码和3D点云。

Result: 通过测试三种3D重建方法和两种压缩算法，证明了数据集的挑战性和现有方法的不足。

Conclusion: ViVo数据集为体积视频重建和压缩提供了更真实的基准，推动了相关算法的发展。

Abstract: As research on neural volumetric video reconstruction and compression
flourishes, there is a need for diverse and realistic datasets, which can be
used to develop and validate reconstruction and compression models. However,
existing volumetric video datasets lack diverse content in terms of both
semantic and low-level features that are commonly present in real-world
production pipelines. In this context, we propose a new dataset, ViVo, for
VolumetrIc VideO reconstruction and compression. The dataset is faithful to
real-world volumetric video production and is the first dataset to extend the
definition of diversity to include both human-centric characteristics (skin,
hair, etc.) and dynamic visual phenomena (transparent, reflective, liquid,
etc.). Each video sequence in this database contains raw data including
fourteen multi-view RGB and depth video pairs, synchronized at 30FPS with
per-frame calibration and audio data, and their associated 2-D foreground masks
and 3-D point clouds. To demonstrate the use of this database, we have
benchmarked three state-of-the-art (SotA) 3-D reconstruction methods and two
volumetric video compression algorithms. The obtained results evidence the
challenging nature of the proposed dataset and the limitations of existing
datasets for both volumetric video reconstruction and compression tasks,
highlighting the need to develop more effective algorithms for these
applications. The database and the associated results are available at
https://vivo-bvicr.github.io/

</details>


### [25] [SEED: A Benchmark Dataset for Sequential Facial Attribute Editing with Diffusion Models](https://arxiv.org/abs/2506.00562)
*Yule Zhu,Ping Liu,Zhedong Zheng,Wei Liu*

Main category: cs.CV

TL;DR: 论文介绍了SEED数据集和FAITH模型，用于研究基于扩散模型的渐进式面部编辑序列。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏针对渐进式编辑序列的大规模标注数据集，导致编辑归因和检测鲁棒性研究受限。

Method: 构建了SEED数据集，包含90,000多张面部图像，每张图像标注了编辑序列、属性掩码和提示；提出FAITH模型，利用高频线索增强对细微变化的敏感性。

Result: 实验表明FAITH模型在SEED数据集上表现优异，同时揭示了渐进式编辑的独特挑战。

Conclusion: SEED为研究渐进式编辑提供了灵活且具有挑战性的资源，FAITH模型为相关任务提供了有效解决方案。

Abstract: Diffusion models have recently enabled precise and photorealistic facial
editing across a wide range of semantic attributes. Beyond single-step
modifications, a growing class of applications now demands the ability to
analyze and track sequences of progressive edits, such as stepwise changes to
hair, makeup, or accessories. However, sequential editing introduces
significant challenges in edit attribution and detection robustness, further
complicated by the lack of large-scale, finely annotated benchmarks tailored
explicitly for this task. We introduce SEED, a large-scale Sequentially Edited
facE Dataset constructed via state-of-the-art diffusion models. SEED contains
over 90,000 facial images with one to four sequential attribute modifications,
generated using diverse diffusion-based editing pipelines (LEdits, SDXL, SD3).
Each image is annotated with detailed edit sequences, attribute masks, and
prompts, facilitating research on sequential edit tracking, visual provenance
analysis, and manipulation robustness assessment. To benchmark this task, we
propose FAITH, a frequency-aware transformer-based model that incorporates
high-frequency cues to enhance sensitivity to subtle sequential changes.
Comprehensive experiments, including systematic comparisons of multiple
frequency-domain methods, demonstrate the effectiveness of FAITH and the unique
challenges posed by SEED. SEED offers a challenging and flexible resource for
studying progressive diffusion-based edits at scale. Dataset and code will be
publicly released at: https://github.com/Zeus1037/SEED.

</details>


### [26] [CReFT-CAD: Boosting Orthographic Projection Reasoning for CAD via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.00568)
*Ke Niu,Zhuofan Chen,Haiyang Yu,Yuwen Chen,Teng Fu,Mengyang Zhao,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: CReFT-CAD是一种两阶段微调范式，结合课程驱动的强化学习和监督后调优，提升CAD中的正交投影推理能力，并发布TriView2CAD基准数据集。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在CAD中引入不精确尺寸且缺乏参数化编辑能力，而监督微调（SFT）易陷入模式记忆，泛化能力差。

Method: 提出CReFT-CAD：第一阶段通过课程驱动的强化学习逐步构建推理能力，第二阶段监督后调优优化指令跟随和语义提取。

Result: CReFT-CAD显著提升推理精度和分布外泛化能力，TriView2CAD数据集为研究提供支持。

Conclusion: CReFT-CAD为CAD推理研究提供新方向，TriView2CAD数据集推动领域发展。

Abstract: Computer-Aided Design (CAD) plays a pivotal role in industrial manufacturing.
Orthographic projection reasoning underpins the entire CAD workflow,
encompassing design, manufacturing, and simulation. However, prevailing
deep-learning approaches employ standard 3D reconstruction pipelines as an
alternative, which often introduce imprecise dimensions and limit the
parametric editability required for CAD workflows. Recently, some researchers
adopt vision-language models (VLMs), particularly supervised fine-tuning (SFT),
to tackle CAD-related challenges. SFT shows promise but often devolves into
pattern memorization, yielding poor out-of-distribution performance on complex
reasoning tasks. To address these gaps, we introduce CReFT-CAD, a two-stage
fine-tuning paradigm that first employs a curriculum-driven reinforcement
learning stage with difficulty-aware rewards to build reasoning ability
steadily, and then applies supervised post-tuning to hone instruction following
and semantic extraction. Complementing this, we release TriView2CAD, the first
large-scale, open-source benchmark for orthographic projection reasoning,
comprising 200,000 synthetic and 3,000 real-world orthographic projections with
precise dimension annotations and six interoperable data modalities. We
benchmark leading VLMs on orthographic projection reasoning and demonstrate
that CReFT-CAD substantially improves reasoning accuracy and
out-of-distribution generalizability in real-world scenarios, offering valuable
insights for advancing CAD reasoning research.

</details>


### [27] [Event-based multi-view photogrammetry for high-dynamic, high-velocity target measurement](https://arxiv.org/abs/2506.00578)
*Taihang Lei,Banglei Guan,Minzu Liang,Xiangyu Li,Jianbing Liu,Jing Tao,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了一种基于事件的多视角摄影测量系统，用于高动态、高速度目标运动的机械特性测量，解决了现有方法的动态范围有限、观测不连续和高成本问题。


<details>
  <summary>Details</summary>
Motivation: 高动态、高速度目标运动的机械特性测量在工业中至关重要，但现有方法存在动态范围有限、观测不连续和高成本等挑战。

Method: 利用事件时空分布的单调性提取目标前缘特征，消除拖尾效应；通过重投影误差关联事件与目标轨迹；采用目标速度衰减模型拟合数据，实现多视角数据联合计算。

Result: 在轻气枪碎片测试中，该方法与电磁测速仪的测量偏差为4.47%。

Conclusion: 该方法能够有效解决高动态目标运动的测量问题，具有较高的准确性和实用性。

Abstract: The characterization of mechanical properties for high-dynamic, high-velocity
target motion is essential in industries. It provides crucial data for
validating weapon systems and precision manufacturing processes etc. However,
existing measurement methods face challenges such as limited dynamic range,
discontinuous observations, and high costs. This paper presents a new approach
leveraging an event-based multi-view photogrammetric system, which aims to
address the aforementioned challenges. First, the monotonicity in the
spatiotemporal distribution of events is leveraged to extract the target's
leading-edge features, eliminating the tailing effect that complicates motion
measurements. Then, reprojection error is used to associate events with the
target's trajectory, providing more data than traditional intersection methods.
Finally, a target velocity decay model is employed to fit the data, enabling
accurate motion measurements via ours multi-view data joint computation. In a
light gas gun fragment test, the proposed method showed a measurement deviation
of 4.47% compared to the electromagnetic speedometer.

</details>


### [28] [Seg2Any: Open-set Segmentation-Mask-to-Image Generation with Precise Shape and Semantic Control](https://arxiv.org/abs/2506.00596)
*Danfeng li,Hui Zhang,Sheng Wang,Jiacheng Li,Zuxuan Wu*

Main category: cs.CV

TL;DR: Seg2Any是一个新的S2I框架，通过解耦分割掩码条件为语义和形状组件，实现了语义和形状一致性，并在多实体场景中防止属性泄漏。


<details>
  <summary>Details</summary>
Motivation: 解决现有S2I方法无法同时保证语义一致性和形状一致性的问题。

Method: 1. 将分割掩码条件解耦为区域语义和高频形状组件；2. 引入属性隔离注意力机制防止属性泄漏。

Result: 在开放集和封闭集S2I基准测试中达到最先进性能。

Conclusion: Seg2Any在细粒度空间和属性控制方面表现优异。

Abstract: Despite recent advances in diffusion models, top-tier text-to-image (T2I)
models still struggle to achieve precise spatial layout control, i.e.
accurately generating entities with specified attributes and locations.
Segmentation-mask-to-image (S2I) generation has emerged as a promising solution
by incorporating pixel-level spatial guidance and regional text prompts.
However, existing S2I methods fail to simultaneously ensure semantic
consistency and shape consistency. To address these challenges, we propose
Seg2Any, a novel S2I framework built upon advanced multimodal diffusion
transformers (e.g. FLUX). First, to achieve both semantic and shape
consistency, we decouple segmentation mask conditions into regional semantic
and high-frequency shape components. The regional semantic condition is
introduced by a Semantic Alignment Attention Mask, ensuring that generated
entities adhere to their assigned text prompts. The high-frequency shape
condition, representing entity boundaries, is encoded as an Entity Contour Map
and then introduced as an additional modality via multi-modal attention to
guide image spatial structure. Second, to prevent attribute leakage across
entities in multi-entity scenarios, we introduce an Attribute Isolation
Attention Mask mechanism, which constrains each entity's image tokens to attend
exclusively to themselves during image self-attention. To support open-set S2I
generation, we construct SACap-1M, a large-scale dataset containing 1 million
images with 5.9 million segmented entities and detailed regional captions,
along with a SACap-Eval benchmark for comprehensive S2I evaluation. Extensive
experiments demonstrate that Seg2Any achieves state-of-the-art performance on
both open-set and closed-set S2I benchmarks, particularly in fine-grained
spatial and attribute control of entities.

</details>


### [29] [XYZ-IBD: High-precision Bin-picking Dataset for Object 6D Pose Estimation Capturing Real-world Industrial Complexity](https://arxiv.org/abs/2506.00599)
*Junwen Huang,Jizhong Liang,Jiaqi Hu,Martin Sundermeyer,Peter KT Yu,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: XYZ-IBD是一个针对6D姿态估计的工业级数据集，包含复杂物体几何、反射材料、严重遮挡和高密度杂乱场景，填补了现有数据集在工业场景中的空白。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注家用物品，已接近饱和，而工业场景中的真实挑战（如无纹理、金属、对称物体）尚未解决。XYZ-IBD旨在提供更真实的工业环境数据。

Method: 使用高精度工业相机和商业相机采集RGB、灰度和深度图像，结合多视角深度融合和半自动标注流程，实现毫米级标注精度。同时生成大规模合成数据。

Result: 在2D检测、6D姿态估计和深度估计任务中，现有方法在XYZ-IBD上的性能显著低于家用数据集，凸显了工业场景的挑战性。

Conclusion: XYZ-IBD为未来研究提供了更真实且具有挑战性的工业场景数据，填补了学术与工业需求之间的差距。

Abstract: We introduce XYZ-IBD, a bin-picking dataset for 6D pose estimation that
captures real-world industrial complexity, including challenging object
geometries, reflective materials, severe occlusions, and dense clutter. The
dataset reflects authentic robotic manipulation scenarios with
millimeter-accurate annotations. Unlike existing datasets that primarily focus
on household objects, which approach saturation,XYZ-IBD represents the unsolved
realistic industrial conditions. The dataset features 15 texture-less,
metallic, and mostly symmetrical objects of varying shapes and sizes. These
objects are heavily occluded and randomly arranged in bins with high density,
replicating the challenges of real-world bin-picking. XYZ-IBD was collected
using two high-precision industrial cameras and one commercially available
camera, providing RGB, grayscale, and depth images. It contains 75 multi-view
real-world scenes, along with a large-scale synthetic dataset rendered under
simulated bin-picking conditions. We employ a meticulous annotation pipeline
that includes anti-reflection spray, multi-view depth fusion, and
semi-automatic annotation, achieving millimeter-level pose labeling accuracy
required for industrial manipulation. Quantification in simulated environments
confirms the reliability of the ground-truth annotations. We benchmark
state-of-the-art methods on 2D detection, 6D pose estimation, and depth
estimation tasks on our dataset, revealing significant performance degradation
in our setups compared to current academic household benchmarks. By capturing
the complexity of real-world bin-picking scenarios, XYZ-IBD introduces more
realistic and challenging problems for future research. The dataset and
benchmark are publicly available at https://xyz-ibd.github.io/XYZ-IBD/.

</details>


### [30] [SatDreamer360: Geometry Consistent Street-View Video Generation from Satellite Imagery](https://arxiv.org/abs/2506.00600)
*Xianghui Ze,Beiyi Zhu,Zhenbo Song,Jianfeng Lu,Yujiao Shi*

Main category: cs.CV

TL;DR: SatDreamer360 是一种新框架，可从单张卫星图像和预定义轨迹生成几何和时间一致的地面视频，无需额外几何先验。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要生成单张地面图像，依赖辅助输入且无法保证时间一致性，限制了在模拟、自动驾驶等领域的应用。

Method: 采用紧凑的三平面表示从卫星图像编码场景几何，结合基于光线的像素注意力机制和极线约束的时间注意力模块，确保跨视图和多帧一致性。

Result: 在 VIGOR++ 数据集上实验表明，SatDreamer360 在保真度、连贯性和几何对齐方面表现优异。

Conclusion: SatDreamer360 提供了一种高效且无需额外先验的方法，为跨视图视频生成开辟了新方向。

Abstract: Generating continuous ground-level video from satellite imagery is a
challenging task with significant potential for applications in simulation,
autonomous navigation, and digital twin cities. Existing approaches primarily
focus on synthesizing individual ground-view images, often relying on auxiliary
inputs like height maps or handcrafted projections, and fall short in producing
temporally consistent sequences. In this paper, we propose {SatDreamer360}, a
novel framework that generates geometrically and temporally consistent
ground-view video from a single satellite image and a predefined trajectory. To
bridge the large viewpoint gap, we introduce a compact tri-plane representation
that encodes scene geometry directly from the satellite image. A ray-based
pixel attention mechanism retrieves view-dependent features from the tri-plane,
enabling accurate cross-view correspondence without requiring additional
geometric priors. To ensure multi-frame consistency, we propose an
epipolar-constrained temporal attention module that aligns features across
frames using the known relative poses along the trajectory. To support
evaluation, we introduce {VIGOR++}, a large-scale dataset for cross-view video
generation, with dense trajectory annotations and high-quality ground-view
sequences. Extensive experiments demonstrate that SatDreamer360 achieves
superior performance in fidelity, coherence, and geometric alignment across
diverse urban scenes.

</details>


### [31] [Parallel Rescaling: Rebalancing Consistency Guidance for Personalized Diffusion Models](https://arxiv.org/abs/2506.00607)
*JungWoo Chae,Jiyoon Kim,Sangheum Hwang*

Main category: cs.CV

TL;DR: 提出了一种并行重缩放技术，用于个性化扩散模型，通过分解一致性引导信号以减少对分类器自由引导的干扰，从而提升生成图像与文本提示的对齐和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如DreamBooth和Textual Inversion）在少量参考图像下容易过拟合，导致生成图像与文本提示不一致。DCO虽部分缓解问题，但对复杂或风格化提示仍表现不佳。

Method: 提出并行重缩放技术，将一致性引导信号分解为与分类器自由引导（CFG）平行和正交的分量，通过重缩放平行分量减少干扰，同时保持主体身份。

Result: 实验表明，该方法在提示对齐和视觉保真度上优于基线方法，尤其对挑战性风格化提示表现更优。

Conclusion: 并行重缩放技术为多样化用户输入提供了更稳定和准确的个性化生成方案。

Abstract: Personalizing diffusion models to specific users or concepts remains
challenging, particularly when only a few reference images are available.
Existing methods such as DreamBooth and Textual Inversion often overfit to
limited data, causing misalignment between generated images and text prompts
when attempting to balance identity fidelity with prompt adherence. While
Direct Consistency Optimization (DCO) with its consistency-guided sampling
partially alleviates this issue, it still struggles with complex or stylized
prompts. In this paper, we propose a parallel rescaling technique for
personalized diffusion models. Our approach explicitly decomposes the
consistency guidance signal into parallel and orthogonal components relative to
classifier free guidance (CFG). By rescaling the parallel component, we
minimize disruptive interference with CFG while preserving the subject's
identity. Unlike prior personalization methods, our technique does not require
additional training data or expensive annotations. Extensive experiments show
improved prompt alignment and visual fidelity compared to baseline methods,
even on challenging stylized prompts. These findings highlight the potential of
parallel rescaled guidance to yield more stable and accurate personalization
for diverse user inputs.

</details>


### [32] [Long-Tailed Visual Recognition via Permutation-Invariant Head-to-Tail Feature Fusion](https://arxiv.org/abs/2506.00625)
*Mengke Li,Zhikai Hu,Yang Lu,Weichao Lan,Yiu-ming Cheung,Hui Huang*

Main category: cs.CV

TL;DR: PI-H2T方法通过特征融合和语义信息转移，解决了长尾数据分布不平衡问题，优化了表示空间和分类器性能。


<details>
  <summary>Details</summary>
Motivation: 长尾数据分布不平衡导致深度学习模型偏向头部类别，忽视尾部类别，主要问题是表示空间变形和分类器偏差。

Method: 提出PI-H2T方法，包括置换不变表示融合（PIF）和头到尾特征融合（H2TF），优化表示空间并调整分类器。

Result: 实验证明PI-H2T能有效提升尾部类别的多样性，并在长尾基准测试中表现优异。

Conclusion: PI-H2T是一种即插即用的方法，可无缝集成到现有方法中，显著提升性能。

Abstract: The imbalanced distribution of long-tailed data presents a significant
challenge for deep learning models, causing them to prioritize head classes
while neglecting tail classes. Two key factors contributing to low recognition
accuracy are the deformed representation space and a biased classifier,
stemming from insufficient semantic information in tail classes. To address
these issues, we propose permutation-invariant and head-to-tail feature fusion
(PI-H2T), a highly adaptable method. PI-H2T enhances the representation space
through permutation-invariant representation fusion (PIF), yielding more
clustered features and automatic class margins. Additionally, it adjusts the
biased classifier by transferring semantic information from head to tail
classes via head-to-tail fusion (H2TF), improving tail class diversity.
Theoretical analysis and experiments show that PI-H2T optimizes both the
representation space and decision boundaries. Its plug-and-play design ensures
seamless integration into existing methods, providing a straightforward path to
further performance improvements. Extensive experiments on long-tailed
benchmarks confirm the effectiveness of PI-H2T.

</details>


### [33] [Text-to-CT Generation via 3D Latent Diffusion Model with Contrastive Vision-Language Pretraining](https://arxiv.org/abs/2506.00633)
*Daniele Molino,Camillo Maria Caruso,Filippo Ruffini,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 本文提出了一种结合潜在扩散模型和3D对比视觉语言预训练的新架构，用于从文本生成CT图像，解决了3D医学影像生成中的高维度和解剖复杂性挑战。


<details>
  <summary>Details</summary>
Motivation: 当前文本条件生成模型在2D医学影像（如胸部X光）上取得了进展，但在3D CT图像生成中仍面临高维度、解剖复杂性和缺乏视觉语言对齐框架的挑战。

Method: 采用双编码器CLIP风格模型，结合预训练的3D VAE压缩CT体积，并通过对比预训练和体积扩散实现高效的3D去噪扩散。

Result: 在CT-RATE数据集上评估，模型在图像保真度、临床相关性和语义对齐方面表现优异，显著优于基线方法，并能有效增强下游诊断性能。

Conclusion: 研究表明，特定模态的视觉语言对齐是高质量3D医学影像生成的关键，该方法为数据增强、医学教育和临床模拟提供了可扩展的解决方案。

Abstract: Objective: While recent advances in text-conditioned generative models have
enabled the synthesis of realistic medical images, progress has been largely
confined to 2D modalities such as chest X-rays. Extending text-to-image
generation to volumetric Computed Tomography (CT) remains a significant
challenge, due to its high dimensionality, anatomical complexity, and the
absence of robust frameworks that align vision-language data in 3D medical
imaging. Methods: We introduce a novel architecture for Text-to-CT generation
that combines a latent diffusion model with a 3D contrastive vision-language
pretraining scheme. Our approach leverages a dual-encoder CLIP-style model
trained on paired CT volumes and radiology reports to establish a shared
embedding space, which serves as the conditioning input for generation. CT
volumes are compressed into a low-dimensional latent space via a pretrained
volumetric VAE, enabling efficient 3D denoising diffusion without requiring
external super-resolution stages. Results: We evaluate our method on the
CT-RATE dataset and conduct a comprehensive assessment of image fidelity,
clinical relevance, and semantic alignment. Our model achieves competitive
performance across all tasks, significantly outperforming prior baselines for
text-to-CT generation. Moreover, we demonstrate that CT scans synthesized by
our framework can effectively augment real data, improving downstream
diagnostic performance. Conclusion: Our results show that modality-specific
vision-language alignment is a key component for high-quality 3D medical image
generation. By integrating contrastive pretraining and volumetric diffusion,
our method offers a scalable and controllable solution for synthesizing
clinically meaningful CT volumes from text, paving the way for new applications
in data augmentation, medical education, and automated clinical simulation.

</details>


### [34] [Video Signature: In-generation Watermarking for Latent Video Diffusion Models](https://arxiv.org/abs/2506.00652)
*Yu Huang,Junhao Chen,Qi Zheng,Hanqian Li,Shuliang Liu,Xuming Hu*

Main category: cs.CV

TL;DR: VIDSIG是一种在潜在视频扩散模型中嵌入水印的方法，通过部分微调潜在解码器，实现水印的隐式和自适应集成，平衡了视频质量与水印提取效果。


<details>
  <summary>Details</summary>
Motivation: AIGC的快速发展引发了对知识产权保护和内容追溯的担忧，现有后生成水印方法存在计算开销大且难以平衡视频质量与水印提取的问题。

Method: 提出VIDSIG方法，通过Perturbation-Aware Suppression（PAS）预识别并冻结感知敏感层以保持视觉质量，并引入轻量级时间对齐模块增强时间一致性。

Result: 实验表明VIDSIG在水印提取、视觉质量和生成效率上表现最佳，并对空间和时间篡改具有强鲁棒性。

Conclusion: VIDSIG是一种实用且高效的在生成过程中嵌入水印的方法，适用于实际场景。

Abstract: The rapid development of Artificial Intelligence Generated Content (AIGC) has
led to significant progress in video generation but also raises serious
concerns about intellectual property protection and reliable content tracing.
Watermarking is a widely adopted solution to this issue, but existing methods
for video generation mainly follow a post-generation paradigm, which introduces
additional computational overhead and often fails to effectively balance the
trade-off between video quality and watermark extraction. To address these
issues, we propose Video Signature (VIDSIG), an in-generation watermarking
method for latent video diffusion models, which enables implicit and adaptive
watermark integration during generation. Specifically, we achieve this by
partially fine-tuning the latent decoder, where Perturbation-Aware Suppression
(PAS) pre-identifies and freezes perceptually sensitive layers to preserve
visual quality. Beyond spatial fidelity, we further enhance temporal
consistency by introducing a lightweight Temporal Alignment module that guides
the decoder to generate coherent frame sequences during fine-tuning.
Experimental results show that VIDSIG achieves the best overall performance in
watermark extraction, visual quality, and generation efficiency. It also
demonstrates strong robustness against both spatial and temporal tampering,
highlighting its practicality in real-world scenarios.

</details>


### [35] [Poster: Adapting Pretrained Vision Transformers with LoRA Against Attack Vectors](https://arxiv.org/abs/2506.00661)
*Richard E. Neddo,Sean Willis,Zander Blasingame,Chen Liu*

Main category: cs.CV

TL;DR: 本文提出了一种通过低秩适应调整预训练视觉变换器的权重和类别的方法，以增强对抗攻击的鲁棒性，并实现无需重新训练的可扩展微调。


<details>
  <summary>Details</summary>
Motivation: 图像分类器（如自动驾驶导航中使用的）容易受到针对输入图像集的对抗攻击，现有研究已广泛讨论了此类攻击。

Method: 通过低秩适应调整预训练视觉变换器的权重和类别，以提高对抗攻击的鲁棒性，并支持可扩展的微调。

Result: 提出的方法能够有效增强模型对对抗攻击的鲁棒性，同时避免重新训练的开销。

Conclusion: 低秩适应是一种有效的对抗攻击防御策略，适用于预训练视觉变换器的可扩展微调。

Abstract: Image classifiers, such as those used for autonomous vehicle navigation, are
largely known to be susceptible to adversarial attacks that target the input
image set. There is extensive discussion on adversarial attacks including
perturbations that alter the input images to cause malicious misclassifications
without perceivable modification. This work proposes a countermeasure for such
attacks by adjusting the weights and classes of pretrained vision transformers
with a low-rank adaptation to become more robust against adversarial attacks
and allow for scalable fine-tuning without retraining.

</details>


### [36] [Scene Detection Policies and Keyframe Extraction Strategies for Large-Scale Video Analysis](https://arxiv.org/abs/2506.00667)
*Vasilii Korolkov*

Main category: cs.CV

TL;DR: 提出了一种统一的、自适应的场景分割和关键帧提取框架，适用于多种视频类型和时长，具有高效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多样化的视频类型和时长上缺乏通用性，需要一种适应性强的解决方案。

Method: 根据视频长度动态选择分割策略（短视频用自适应阈值，中长视频用混合策略，长视频用基于间隔的分割），并采用轻量级模块评分关键帧。

Result: 系统已在商业视频分析平台中部署，适用于媒体、教育、研究和安全领域，提供一致的分割粒度和高效处理。

Conclusion: 该框架为下游应用提供了可扩展和可解释的解决方案，未来将探索音频感知分割和强化学习评分。

Abstract: Robust scene segmentation and keyframe extraction are essential preprocessing
steps in video understanding pipelines, supporting tasks such as indexing,
summarization, and semantic retrieval. However, existing methods often lack
generalizability across diverse video types and durations. We present a
unified, adaptive framework for automatic scene detection and keyframe
selection that handles formats ranging from short-form media to long-form
films, archival content, and surveillance footage. Our system dynamically
selects segmentation policies based on video length: adaptive thresholding for
short videos, hybrid strategies for mid-length ones, and interval-based
splitting for extended recordings. This ensures consistent granularity and
efficient processing across domains. For keyframe selection, we employ a
lightweight module that scores sampled frames using a composite metric of
sharpness, luminance, and temporal spread, avoiding complex saliency models
while ensuring visual relevance. Designed for high-throughput workflows, the
system is deployed in a commercial video analysis platform and has processed
content from media, education, research, and security domains. It offers a
scalable and interpretable solution suitable for downstream applications such
as UI previews, embedding pipelines, and content filtering. We discuss
practical implementation details and outline future enhancements, including
audio-aware segmentation and reinforcement-learned frame scoring.

</details>


### [37] [Concept-Centric Token Interpretation for Vector-Quantized Generative Models](https://arxiv.org/abs/2506.00698)
*Tianze Yang,Yucheng Shi,Mengnan Du,Xuansheng Wu,Qiaoyu Tan,Jin Sun,Ninghao Liu*

Main category: cs.CV

TL;DR: CORTEX是一种解释VQGM中离散令牌代码书的新方法，通过识别概念特定的令牌组合，提供样本级和代码书级的解释。


<details>
  <summary>Details</summary>
Motivation: VQGM的代码书（离散令牌）在图像生成中的作用尚不明确，需要一种方法来解释哪些令牌对生成特定概念的图像至关重要。

Method: CORTEX采用两种方法：(1)样本级解释，分析单个图像中令牌的重要性分数；(2)代码书级解释，探索整个代码书以找到全局相关令牌。

Result: 实验表明，CORTEX在解释生成过程中令牌使用方面优于基线方法，并可用于图像编辑和特征检测。

Conclusion: CORTEX提高了VQGM的透明度，并在实际应用中展示了其价值。

Abstract: Vector-Quantized Generative Models (VQGMs) have emerged as powerful tools for
image generation. However, the key component of VQGMs -- the codebook of
discrete tokens -- is still not well understood, e.g., which tokens are
critical to generate an image of a certain concept? This paper introduces
Concept-Oriented Token Explanation (CORTEX), a novel approach for interpreting
VQGMs by identifying concept-specific token combinations. Our framework employs
two methods: (1) a sample-level explanation method that analyzes token
importance scores in individual images, and (2) a codebook-level explanation
method that explores the entire codebook to find globally relevant tokens.
Experimental results demonstrate CORTEX's efficacy in providing clear
explanations of token usage in the generative process, outperforming baselines
across multiple pretrained VQGMs. Besides enhancing VQGMs transparency, CORTEX
is useful in applications such as targeted image editing and shortcut feature
detection. Our code is available at https://github.com/YangTianze009/CORTEX.

</details>


### [38] [Fovea Stacking: Imaging with Dynamic Localized Aberration Correction](https://arxiv.org/abs/2506.00716)
*Shi Mao,Yogeshwar Mishra,Wolfgang Heidrich*

Main category: cs.CV

TL;DR: 论文提出了一种名为Fovea Stacking的新型成像系统，利用可变形相位板（DPPs）局部校正像差，通过堆叠多个聚焦图像生成无像差图像。


<details>
  <summary>Details</summary>
Motivation: 小型化相机需求推动了对简化光学系统的探索，但这些系统通常存在严重像差，难以通过软件完全校正。

Method: 通过可微分光学模型优化DPP变形，局部校正离轴像差，并结合神经网络控制模型优化性能。

Result: Fovea Stacking在扩展景深成像中优于传统方法，并能通过动态调整实现实时聚焦视频。

Conclusion: 该系统为小型化相机提供了高质量的成像解决方案，适用于监控和虚拟现实等应用。

Abstract: The desire for cameras with smaller form factors has recently lead to a push
for exploring computational imaging systems with reduced optical complexity
such as a smaller number of lens elements. Unfortunately such simplified
optical systems usually suffer from severe aberrations, especially in off-axis
regions, which can be difficult to correct purely in software.
  In this paper we introduce Fovea Stacking, a new type of imaging system that
utilizes emerging dynamic optical components called deformable phase plates
(DPPs) for localized aberration correction anywhere on the image sensor. By
optimizing DPP deformations through a differentiable optical model, off-axis
aberrations are corrected locally, producing a foveated image with enhanced
sharpness at the fixation point - analogous to the eye's fovea. Stacking
multiple such foveated images, each with a different fixation point, yields a
composite image free from aberrations. To efficiently cover the entire field of
view, we propose joint optimization of DPP deformations under imaging budget
constraints. Due to the DPP device's non-linear behavior, we introduce a neural
network-based control model for improved alignment between simulation-hardware
performance.
  We further demonstrated that for extended depth-of-field imaging, fovea
stacking outperforms traditional focus stacking in image quality. By
integrating object detection or eye-tracking, the system can dynamically adjust
the lens to track the object of interest-enabling real-time foveated video
suitable for downstream applications such as surveillance or foveated virtual
reality displays.

</details>


### [39] [From Local Cues to Global Percepts: Emergent Gestalt Organization in Self-Supervised Vision Models](https://arxiv.org/abs/2506.00718)
*Tianqin Li,Ziqi Wen,Leiran Song,Jun Liu,Zhi Jing,Tai Sing Lee*

Main category: cs.CV

TL;DR: 现代视觉模型（如ViT和ConvNeXt）在自监督训练（如MAE）下表现出类似Gestalt原则的行为，但分类微调会削弱这种能力。DiSRT测试表明，自监督模型对全局结构的敏感性优于监督模型，甚至有时超过人类表现。


<details>
  <summary>Details</summary>
Motivation: 研究现代视觉模型是否能够像人类视觉一样通过Gestalt原则（如闭合、邻近性）组织局部线索为全局形式，并探索其训练条件。

Method: 使用Vision Transformers（ViT）和ConvNeXt模型，通过Masked Autoencoding（MAE）等自监督方法训练，并设计DiSRT测试评估模型对全局空间扰动的敏感性。

Result: 自监督模型（如MAE、CLIP）在DiSRT测试中表现优于监督模型，甚至有时超过人类。ConvNeXt模型也表现出类似Gestalt的行为，但分类微调会削弱这种能力。通过Top-K激活稀疏机制可以恢复全局敏感性。

Conclusion: 自监督训练（如MAE）能促进模型表现出类似Gestalt原则的行为，而分类微调会抑制这种能力。DiSRT测试可作为评估模型全局结构敏感性的工具。

Abstract: Human vision organizes local cues into coherent global forms using Gestalt
principles like closure, proximity, and figure-ground assignment -- functions
reliant on global spatial structure. We investigate whether modern vision
models show similar behaviors, and under what training conditions these emerge.
We find that Vision Transformers (ViTs) trained with Masked Autoencoding (MAE)
exhibit activation patterns consistent with Gestalt laws, including illusory
contour completion, convexity preference, and dynamic figure-ground
segregation. To probe the computational basis, we hypothesize that modeling
global dependencies is necessary for Gestalt-like organization. We introduce
the Distorted Spatial Relationship Testbench (DiSRT), which evaluates
sensitivity to global spatial perturbations while preserving local textures.
Using DiSRT, we show that self-supervised models (e.g., MAE, CLIP) outperform
supervised baselines and sometimes even exceed human performance. ConvNeXt
models trained with MAE also exhibit Gestalt-compatible representations,
suggesting such sensitivity can arise without attention architectures. However,
classification finetuning degrades this ability. Inspired by biological vision,
we show that a Top-K activation sparsity mechanism can restore global
sensitivity. Our findings identify training conditions that promote or suppress
Gestalt-like perception and establish DiSRT as a diagnostic for global
structure sensitivity across models.

</details>


### [40] [Common Inpainted Objects In-N-Out of Context](https://arxiv.org/abs/2506.00721)
*Tianze Yang,Tyson Jordan,Ninghao Liu,Jin Sun*

Main category: cs.CV

TL;DR: COinCO是一个新数据集，通过扩散修复技术生成包含上下文一致和不一致场景的图像，用于上下文学习。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉数据集中缺乏上下文不一致样本的问题。

Method: 使用扩散修复技术替换COCO图像中的对象，并通过多模态大语言模型验证和分类。

Result: 揭示了语义先验对修复成功的影响，并支持三种关键任务：上下文分类、对象预测和假检测。

Conclusion: COinCO为上下文感知视觉理解提供了基础，推动了计算机视觉和图像取证的发展。

Abstract: We present Common Inpainted Objects In-N-Out of Context (COinCO), a novel
dataset addressing the scarcity of out-of-context examples in existing vision
datasets. By systematically replacing objects in COCO images through
diffusion-based inpainting, we create 97,722 unique images featuring both
contextually coherent and inconsistent scenes, enabling effective context
learning. Each inpainted object is meticulously verified and categorized as in-
or out-of-context through a multimodal large language model assessment. Our
analysis reveals significant patterns in semantic priors that influence
inpainting success across object categories. We demonstrate three key tasks
enabled by COinCO: (1) training context classifiers that effectively determine
whether existing objects belong in their context; (2) a novel
Objects-from-Context prediction task that determines which new objects
naturally belong in given scenes at both instance and clique levels, and (3)
context-enhanced fake detection on state-of-the-art methods without
fine-tuning. COinCO provides a controlled testbed with contextual variations,
establishing a foundation for advancing context-aware visual understanding in
computer vision and image forensics. Our code and data are at:
https://github.com/YangTianze009/COinCO.

</details>


### [41] [Involution-Infused DenseNet with Two-Step Compression for Resource-Efficient Plant Disease Classification](https://arxiv.org/abs/2506.00735)
*T. Ahmed,S. Jannat,Md. F. Islam,J. Noor*

Main category: cs.CV

TL;DR: 论文提出了一种结合权重剪枝和知识蒸馏的两步模型压缩方法，并融合了DenseNet与Involutional Layers，以降低计算需求，适用于资源受限设备。


<details>
  <summary>Details</summary>
Motivation: 农业对全球粮食安全至关重要，但作物易受病害影响。传统CNN模型计算需求高，难以在资源受限设备上部署。

Method: 采用权重剪枝和知识蒸馏的两步压缩方法，并融合DenseNet与Involutional Layers，优化模型效率和空间特征捕捉能力。

Result: 压缩后的ResNet50在PlantVillage和PaddyLeaf数据集上分别达到99.55%和98.99%准确率；DenseNet模型为99.21%和93.96%；混合模型为98.87%和97.10%。

Conclusion: 该方法显著降低模型计算需求，适用于实时应用，支持精准农业和可持续种植。

Abstract: Agriculture is vital for global food security, but crops are vulnerable to
diseases that impact yield and quality. While Convolutional Neural Networks
(CNNs) accurately classify plant diseases using leaf images, their high
computational demands hinder their deployment in resource-constrained settings
such as smartphones, edge devices, and real-time monitoring systems. This study
proposes a two-step model compression approach integrating Weight Pruning and
Knowledge Distillation, along with the hybridization of DenseNet with
Involutional Layers. Pruning reduces model size and computational load, while
distillation improves the smaller student models performance by transferring
knowledge from a larger teacher network. The hybridization enhances the models
ability to capture spatial features efficiently. These compressed models are
suitable for real-time applications, promoting precision agriculture through
rapid disease identification and crop management. The results demonstrate
ResNet50s superior performance post-compression, achieving 99.55% and 98.99%
accuracy on the PlantVillage and PaddyLeaf datasets, respectively. The
DenseNet-based model, optimized for efficiency, recorded 99.21% and 93.96%
accuracy with a minimal parameter count. Furthermore, the hybrid model achieved
98.87% and 97.10% accuracy, supporting the practical deployment of
energy-efficient devices for timely disease intervention and sustainable
farming practices.

</details>


### [42] [ArtiScene: Language-Driven Artistic 3D Scene Generation Through Image Intermediary](https://arxiv.org/abs/2506.00742)
*Zeqi Gu,Yin Cui,Zhaoshuo Li,Fangyin Wei,Yunhao Ge,Jinwei Gu,Ming-Yu Liu,Abe Davis,Yifan Ding*

Main category: cs.CV

TL;DR: ArtiScene利用文本生成2D图像作为中介，指导3D场景合成，无需额外训练，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统3D场景设计复杂且依赖高质量3D数据，而现有文本到3D方法受限于数据不足。

Method: 通过文本生成2D图像，提取对象形状和外观信息，结合几何和姿态信息合成3D场景。

Result: 在布局和美学质量上显著优于现有方法，用户研究和GPT-4o评估中表现优异。

Conclusion: ArtiScene提供了一种高效、灵活的3D场景设计方法，无需额外训练，性能卓越。

Abstract: Designing 3D scenes is traditionally a challenging task that demands both
artistic expertise and proficiency with complex software. Recent advances in
text-to-3D generation have greatly simplified this process by letting users
create scenes based on simple text descriptions. However, as these methods
generally require extra training or in-context learning, their performance is
often hindered by the limited availability of high-quality 3D data. In
contrast, modern text-to-image models learned from web-scale images can
generate scenes with diverse, reliable spatial layouts and consistent, visually
appealing styles. Our key insight is that instead of learning directly from 3D
scenes, we can leverage generated 2D images as an intermediary to guide 3D
synthesis. In light of this, we introduce ArtiScene, a training-free automated
pipeline for scene design that integrates the flexibility of free-form
text-to-image generation with the diversity and reliability of 2D intermediary
layouts.
  First, we generate 2D images from a scene description, then extract the shape
and appearance of objects to create 3D models. These models are assembled into
the final scene using geometry, position, and pose information derived from the
same intermediary image. Being generalizable to a wide range of scenes and
styles, ArtiScene outperforms state-of-the-art benchmarks by a large margin in
layout and aesthetic quality by quantitative metrics. It also averages a 74.89%
winning rate in extensive user studies and 95.07% in GPT-4o evaluation. Project
page: https://artiscene-cvpr.github.io/

</details>


### [43] [EcoLens: Leveraging Multi-Objective Bayesian Optimization for Energy-Efficient Video Processing on Edge Devices](https://arxiv.org/abs/2506.00754)
*Benjamin Civjan,Bo Chen,Ruixiao Zhang,Klara Nahrstedt*

Main category: cs.CV

TL;DR: 论文提出了一种动态优化视频处理配置的系统，以在边缘设备上最小化能耗，同时保持深度学习推理所需的视频特征。


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境中，实时视频分析需要平衡能耗与视频语义，这是一个重要挑战。

Method: 通过离线分析不同配置对能耗和推理准确性的影响，并在线使用多目标贝叶斯优化动态调整配置。

Result: 实验证明系统能有效降低能耗，同时保持高性能分析。

Conclusion: 该系统为智能设备和边缘计算提供了一种实用的节能解决方案。

Abstract: Video processing for real-time analytics in resource-constrained environments
presents a significant challenge in balancing energy consumption and video
semantics. This paper addresses the problem of energy-efficient video
processing by proposing a system that dynamically optimizes processing
configurations to minimize energy usage on the edge, while preserving essential
video features for deep learning inference. We first gather an extensive
offline profile of various configurations consisting of device CPU frequencies,
frame filtering features, difference thresholds, and video bitrates, to
establish apriori knowledge of their impact on energy consumption and inference
accuracy. Leveraging this insight, we introduce an online system that employs
multi-objective Bayesian optimization to intelligently explore and adapt
configurations in real time. Our approach continuously refines processing
settings to meet a target inference accuracy with minimal edge device energy
expenditure. Experimental results demonstrate the system's effectiveness in
reducing video processing energy use while maintaining high analytical
performance, offering a practical solution for smart devices and edge computing
applications.

</details>


### [44] [Depth-Aware Scoring and Hierarchical Alignment for Multiple Object Tracking](https://arxiv.org/abs/2506.00774)
*Milad Khanchi,Maria Amer,Charalambos Poullis*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度感知的多目标跟踪框架，通过零样本深度估计和分层对齐分数改进关联准确性，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有基于运动的多目标跟踪方法依赖IoU，在遮挡或视觉相似对象场景下效果不佳，需要引入3D特征提升性能。

Method: 使用零样本方法估计深度，并将其作为独立特征用于关联；引入分层对齐分数，结合粗粒度边界框重叠和细粒度像素级对齐。

Result: 在挑战性基准测试中达到最先进水平，无需训练或微调。

Conclusion: 首次将单目深度作为独立决策矩阵引入多目标跟踪，显著提升了关联准确性。

Abstract: Current motion-based multiple object tracking (MOT) approaches rely heavily
on Intersection-over-Union (IoU) for object association. Without using 3D
features, they are ineffective in scenarios with occlusions or visually similar
objects. To address this, our paper presents a novel depth-aware framework for
MOT. We estimate depth using a zero-shot approach and incorporate it as an
independent feature in the association process. Additionally, we introduce a
Hierarchical Alignment Score that refines IoU by integrating both coarse
bounding box overlap and fine-grained (pixel-level) alignment to improve
association accuracy without requiring additional learnable parameters. To our
knowledge, this is the first MOT framework to incorporate 3D features
(monocular depth) as an independent decision matrix in the association step.
Our framework achieves state-of-the-art results on challenging benchmarks
without any training nor fine-tuning. The code is available at
https://github.com/Milad-Khanchi/DepthMOT

</details>


### [45] [Aiding Medical Diagnosis through Image Synthesis and Classification](https://arxiv.org/abs/2506.00786)
*Kanishk Choudhary*

Main category: cs.CV

TL;DR: 论文提出了一种基于文本生成医学图像的系统，并通过分类模型验证其准确性，旨在解决现有医学图像资源多样性和可访问性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 医学专业人员（尤其是培训中的）依赖视觉参考材料进行准确诊断和模式识别，但现有资源缺乏多样性和可访问性。

Method: 使用预训练的稳定扩散模型，通过LoRA在PathMNIST数据集上微调，生成医学图像，并通过ResNet-18分类模型验证图像准确性。

Result: 生成模型的F1得分为0.6727，部分组织类型（如脂肪组织和淋巴细胞）分类完美，其他类型因结构复杂更具挑战性。

Conclusion: 该系统在生成和分类部分均表现出高准确性，适用于诊断支持和临床教育，未来可扩展至其他医学影像领域。

Abstract: Medical professionals, especially those in training, often depend on visual
reference materials to support an accurate diagnosis and develop pattern
recognition skills. However, existing resources may lack the diversity and
accessibility needed for broad and effective clinical learning. This paper
presents a system designed to generate realistic medical images from textual
descriptions and validate their accuracy through a classification model. A
pretrained stable diffusion model was fine-tuned using Low-Rank Adaptation
(LoRA) on the PathMNIST dataset, consisting of nine colorectal histopathology
tissue types. The generative model was trained multiple times using different
training parameter configurations, guided by domain-specific prompts to capture
meaningful features. To ensure quality control, a ResNet-18 classification
model was trained on the same dataset, achieving 99.76% accuracy in detecting
the correct label of a colorectal histopathological medical image. Generated
images were then filtered using the trained classifier and an iterative
process, where inaccurate outputs were discarded and regenerated until they
were correctly classified. The highest performing version of the generative
model from experimentation achieved an F1 score of 0.6727, with precision and
recall scores of 0.6817 and 0.7111, respectively. Some types of tissue, such as
adipose tissue and lymphocytes, reached perfect classification scores, while
others proved more challenging due to structural complexity. The
self-validating approach created demonstrates a reliable method for
synthesizing domain-specific medical images because of high accuracy in both
the generation and classification portions of the system, with potential
applications in both diagnostic support and clinical education. Future work
includes improving prompt-specific accuracy and extending the system to other
areas of medical imaging.

</details>


### [46] [HSCR: Hierarchical Self-Contrastive Rewarding for Aligning Medical Vision Language Models](https://arxiv.org/abs/2506.00805)
*Songtao Jiang,Yan Zhang,Yeying Jin,Zhihang Tang,Yangyang Wu,Yang Feng,Jian Wu,Zuozhu Liu*

Main category: cs.CV

TL;DR: HSCR是一种解决医学视觉语言模型（Med-VLMs）模态不对齐问题的新方法，通过生成高质量偏好数据和多级偏好优化提升对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有Med-VLMs方法忽视模态不对齐问题，导致临床场景中不可靠的响应。

Method: HSCR利用模型生成不偏好响应，通过视觉标记丢弃分析对齐奖励函数，并采用多级偏好优化策略。

Result: 实验表明HSCR在多种医学任务中提升零样本性能，仅需2000条训练数据即可显著改善对齐和可信度。

Conclusion: HSCR有效解决了Med-VLMs的模态对齐问题，提升了模型的临床适用性。

Abstract: Medical Vision-Language Models (Med-VLMs) have achieved success across
various tasks, yet most existing methods overlook the modality misalignment
issue that can lead to untrustworthy responses in clinical settings. In this
paper, we propose Hierarchical Self-Contrastive Rewarding (HSCR), a novel
approach that addresses two critical challenges in Med-VLM alignment: 1)
Cost-effective generation of high-quality preference data; 2) Capturing nuanced
and context-aware preferences for improved alignment. HSCR first leverages the
inherent capability of Med-VLMs to generate dispreferred responses with higher
sampling probability. By analyzing output logit shifts after visual token
dropout, we identify modality-coupled tokens that induce misalignment and
derive an implicit alignment reward function. This function guides token
replacement with hallucinated ones during decoding, producing high-quality
dispreferred data. Furthermore, HSCR introduces a multi-level preference
optimization strategy, which extends beyond traditional adjacent-level
optimization by incorporating nuanced implicit preferences, leveraging relative
quality in dispreferred data to capture subtle alignment cues for more precise
and context-aware optimization. Extensive experiments across multiple medical
tasks, including Med-VQA, medical image captioning and instruction following,
demonstrate that HSCR not only enhances zero-shot performance but also
significantly improves modality alignment and trustworthiness with just 2,000
training entries.

</details>


### [47] [TIME: TabPFN-Integrated Multimodal Engine for Robust Tabular-Image Learning](https://arxiv.org/abs/2506.00813)
*Jiaqi Luo,Yuan Yuan,Shixin Xu*

Main category: cs.CV

TL;DR: 论文提出了一种名为TIME的多模态框架，结合TabPFN作为表格编码器，解决了表格数据缺乏标准化预训练表示和缺失值处理的问题。


<details>
  <summary>Details</summary>
Motivation: 表格与图像多模态学习在医学等领域潜力巨大，但缺乏标准化表格预训练表示和缺失值处理能力。

Method: 利用TabPFN作为冻结表格编码器生成鲁棒嵌入，结合预训练视觉主干提取的图像特征，探索多种融合策略。

Result: 在完整和不完整表格输入下，TIME均优于基线方法，验证了其实际应用价值。

Conclusion: TIME框架在多模态学习中表现出色，尤其在医学领域具有广泛应用前景。

Abstract: Tabular-image multimodal learning, which integrates structured tabular data
with imaging data, holds great promise for a variety of tasks, especially in
medical applications. Yet, two key challenges remain: (1) the lack of a
standardized, pretrained representation for tabular data, as is commonly
available in vision and language domains; and (2) the difficulty of handling
missing values in the tabular modality, which are common in real-world medical
datasets. To address these issues, we propose the TabPFN-Integrated Multimodal
Engine (TIME), a novel multimodal framework that builds on the recently
introduced tabular foundation model, TabPFN. TIME leverages TabPFN as a frozen
tabular encoder to generate robust, strong embeddings that are naturally
resilient to missing data, and combines them with image features from
pretrained vision backbones. We explore a range of fusion strategies and
tabular encoders, and evaluate our approach on both natural and medical
datasets. Extensive experiments demonstrate that TIME consistently outperforms
competitive baselines across both complete and incomplete tabular inputs,
underscoring its practical value in real-world multimodal learning scenarios.

</details>


### [48] [L3A: Label-Augmented Analytic Adaptation for Multi-Label Class Incremental Learning](https://arxiv.org/abs/2506.00816)
*Xiang Zhang,Run He,Jiao Chen,Di Fang,Ming Li,Ziqian Zeng,Cen Chen,Huiping Zhuang*

Main category: cs.CV

TL;DR: 论文提出了一种名为L3A的无需存储历史样本的方法，通过伪标签模块和加权分析分类器解决多标签增量学习中的标签缺失和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 多标签增量学习（MLCIL）中存在标签缺失和类别不平衡的挑战，需要一种无需存储历史样本的解决方案。

Method: L3A方法包含伪标签模块（生成伪标签解决标签缺失）和加权分析分类器（通过样本特定权重平衡类别贡献）。

Result: 在MS-COCO和PASCAL VOC数据集上，L3A优于现有方法。

Conclusion: L3A有效解决了MLCIL中的标签缺失和类别不平衡问题，且无需存储历史样本。

Abstract: Class-incremental learning (CIL) enables models to learn new classes
continually without forgetting previously acquired knowledge. Multi-label CIL
(MLCIL) extends CIL to a real-world scenario where each sample may belong to
multiple classes, introducing several challenges: label absence, which leads to
incomplete historical information due to missing labels, and class imbalance,
which results in the model bias toward majority classes. To address these
challenges, we propose Label-Augmented Analytic Adaptation (L3A), an
exemplar-free approach without storing past samples. L3A integrates two key
modules. The pseudo-label (PL) module implements label augmentation by
generating pseudo-labels for current phase samples, addressing the label
absence problem. The weighted analytic classifier (WAC) derives a closed-form
solution for neural networks. It introduces sample-specific weights to
adaptively balance the class contribution and mitigate class imbalance.
Experiments on MS-COCO and PASCAL VOC datasets demonstrate that L3A outperforms
existing methods in MLCIL tasks. Our code is available at
https://github.com/scut-zx/L3A.

</details>


### [49] [QuantFace: Low-Bit Post-Training Quantization for One-Step Diffusion Face Restoration](https://arxiv.org/abs/2506.00820)
*Jiatong Li,Libo Zhu,Haotong Qin,Jingkai Wang,Linghe Kong,Guihai Chen,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: QuantFace是一种针对一步扩散人脸修复模型的低比特量化方法，将32位权重和激活量化为4~6位，通过旋转缩放通道平衡和QD-LoRA优化量化与蒸馏性能，并结合自适应比特分配策略，显著优于现有低比特量化方法。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在人脸修复中表现优异，但计算量大，难以部署在智能手机等设备上。

Method: 提出旋转缩放通道平衡和QD-LoRA联合优化量化与蒸馏性能，并设计自适应比特分配策略。

Result: 在6位和4位量化下，QuantFace在合成和真实数据集上表现优异，优于现有低比特量化方法。

Conclusion: QuantFace通过高效量化策略，显著提升了扩散模型在资源受限设备上的部署能力。

Abstract: Diffusion models have been achieving remarkable performance in face
restoration. However, the heavy computations of diffusion models make it
difficult to deploy them on devices like smartphones. In this work, we propose
QuantFace, a novel low-bit quantization for one-step diffusion face restoration
models, where the full-precision (\ie, 32-bit) weights and activations are
quantized to 4$\sim$6-bit. We first analyze the data distribution within
activations and find that they are highly variant. To preserve the original
data information, we employ rotation-scaling channel balancing. Furthermore, we
propose Quantization-Distillation Low-Rank Adaptation (QD-LoRA) that jointly
optimizes for quantization and distillation performance. Finally, we propose an
adaptive bit-width allocation strategy. We formulate such a strategy as an
integer programming problem, which combines quantization error and perceptual
metrics to find a satisfactory resource allocation. Extensive experiments on
the synthetic and real-world datasets demonstrate the effectiveness of
QuantFace under 6-bit and 4-bit. QuantFace achieves significant advantages over
recent leading low-bit quantization methods for face restoration. The code is
available at https://github.com/jiatongli2024/QuantFace.

</details>


### [50] [Improving Keystep Recognition in Ego-Video via Dexterous Focus](https://arxiv.org/abs/2506.00827)
*Zachary Chavis,Stephen J. Guy,Hyun Soo Park*

Main category: cs.CV

TL;DR: 提出一种通过稳定和聚焦手部区域的视频输入来改进自我中心视角活动识别的框架，无需修改模型架构即可超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统活动识别技术在自我中心视频中面临动态头部运动的挑战，需要一种独立于网络架构的解决方案。

Method: 限制输入为稳定且聚焦手部的视频，无需改变模型架构。

Result: 在Ego-Exo4D细粒度关键步骤识别基准上表现优于现有基线。

Conclusion: 简单视频变换可显著提升自我中心活动识别性能。

Abstract: In this paper, we address the challenge of understanding human activities
from an egocentric perspective. Traditional activity recognition techniques
face unique challenges in egocentric videos due to the highly dynamic nature of
the head during many activities. We propose a framework that seeks to address
these challenges in a way that is independent of network architecture by
restricting the ego-video input to a stabilized, hand-focused video. We
demonstrate that this straightforward video transformation alone outperforms
existing egocentric video baselines on the Ego-Exo4D Fine-Grained Keystep
Recognition benchmark without requiring any alteration of the underlying model
infrastructure.

</details>


### [51] [SkyReels-Audio: Omni Audio-Conditioned Talking Portraits in Video Diffusion Transformers](https://arxiv.org/abs/2506.00830)
*Zhengcong Fei,Hao Jiang,Di Qiu,Baoxuan Gu,Youqiang Zhang,Jiahua Wang,Jialin Bai,Debang Li,Mingyuan Fan,Guibin Chen,Yahui Zhou*

Main category: cs.CV

TL;DR: SkyReels-Audio是一个统一框架，通过多模态输入生成和编辑音频驱动的说话肖像视频，支持无限长度生成和编辑，并实现高质量和时序一致的视频合成。


<details>
  <summary>Details</summary>
Motivation: 目前基于多模态输入（文本、图像、视频）的音频驱动说话肖像生成和编辑研究较少，需要一种能够支持高质量、可控且时序一致的方法。

Method: 基于预训练的视频扩散变换器，采用混合课程学习策略对齐音频与面部运动，引入面部掩码损失和音频引导的无分类器指导机制，并使用滑动窗口去噪方法确保时序一致性。

Result: 在唇同步准确性、身份一致性和真实面部动态方面表现优异，尤其在复杂条件下。

Conclusion: SkyReels-Audio通过多模态输入和创新的训练策略，实现了高质量、可控且时序一致的说话肖像视频生成与编辑。

Abstract: The generation and editing of audio-conditioned talking portraits guided by
multimodal inputs, including text, images, and videos, remains under explored.
In this paper, we present SkyReels-Audio, a unified framework for synthesizing
high-fidelity and temporally coherent talking portrait videos. Built upon
pretrained video diffusion transformers, our framework supports infinite-length
generation and editing, while enabling diverse and controllable conditioning
through multimodal inputs. We employ a hybrid curriculum learning strategy to
progressively align audio with facial motion, enabling fine-grained multimodal
control over long video sequences. To enhance local facial coherence, we
introduce a facial mask loss and an audio-guided classifier-free guidance
mechanism. A sliding-window denoising approach further fuses latent
representations across temporal segments, ensuring visual fidelity and temporal
consistency across extended durations and diverse identities. More importantly,
we construct a dedicated data pipeline for curating high-quality triplets
consisting of synchronized audio, video, and textual descriptions.
Comprehensive benchmark evaluations show that SkyReels-Audio achieves superior
performance in lip-sync accuracy, identity consistency, and realistic facial
dynamics, particularly under complex and challenging conditions.

</details>


### [52] [Advancing from Automated to Autonomous Beamline by Leveraging Computer Vision](https://arxiv.org/abs/2506.00836)
*Baolu Li,Hongkai Yu,Huiming Sun,Jin Ma,Yuewei Lin,Lu Ma,Yonghua Du*

Main category: cs.CV

TL;DR: 论文提出了一种基于计算机视觉的系统，用于实现同步辐射光束线的自主操作，通过深度学习和多视角摄像头实现实时碰撞检测。


<details>
  <summary>Details</summary>
Motivation: 当前同步辐射光束线仍依赖人工安全监督，需要实现自动化与自主操作之间的过渡。

Method: 系统结合设备分割、跟踪和几何分析，利用迁移学习增强鲁棒性，并开发了交互式标注模块以适应新物体类别。

Result: 在真实光束线数据集上的实验表明，系统具有高精度、实时性能和自主操作的潜力。

Conclusion: 该系统为同步辐射光束线的自主操作提供了可行方案。

Abstract: The synchrotron light source, a cutting-edge large-scale user facility,
requires autonomous synchrotron beamline operations, a crucial technique that
should enable experiments to be conducted automatically, reliably, and safely
with minimum human intervention. However, current state-of-the-art synchrotron
beamlines still heavily rely on human safety oversight. To bridge the gap
between automated and autonomous operation, a computer vision-based system is
proposed, integrating deep learning and multiview cameras for real-time
collision detection. The system utilizes equipment segmentation, tracking, and
geometric analysis to assess potential collisions with transfer learning that
enhances robustness. In addition, an interactive annotation module has been
developed to improve the adaptability to new object classes. Experiments on a
real beamline dataset demonstrate high accuracy, real-time performance, and
strong potential for autonomous synchrotron beamline operations.

</details>


### [53] [Towards Predicting Any Human Trajectory In Context](https://arxiv.org/abs/2506.00871)
*Ryo Fujii,Hideo Saito,Ryo Hachiuma*

Main category: cs.CV

TL;DR: TrajICL是一种基于上下文学习（ICL）的行人轨迹预测框架，无需微调即可快速适应不同场景，通过时空相似性示例选择（STES）和预测引导示例选择（PG-ES）提升预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖场景特定数据微调，但边缘设备计算资源有限，难以实现。TrajICL旨在解决这一问题，实现快速适应。

Method: 提出STES方法选择相似运动模式的轨迹示例，并引入PG-ES结合过去和预测轨迹优化选择。利用大规模合成数据集训练模型。

Result: TrajICL在多个公开基准测试中表现优异，适应性强，优于微调方法。

Conclusion: TrajICL通过上下文学习和示例选择方法，实现了高效的行人轨迹预测，适用于不同场景和领域。

Abstract: Predicting accurate future trajectories of pedestrians is essential for
autonomous systems but remains a challenging task due to the need for
adaptability in different environments and domains. A common approach involves
collecting scenario-specific data and performing fine-tuning via
backpropagation. However, this process is often impractical on edge devices due
to constrained computational resources. To address this challenge, we introduce
TrajICL, an In-Context Learning (ICL) framework for pedestrian trajectory
prediction that enables rapid adaptation without fine-tuning on the
scenario-specific data. We propose a spatio-temporal similarity-based example
selection (STES) method that selects relevant examples from previously observed
trajectories within the same scene by identifying similar motion patterns at
corresponding locations. To further refine this selection, we introduce
prediction-guided example selection (PG-ES), which selects examples based on
both the past trajectory and the predicted future trajectory, rather than
relying solely on the past trajectory. This approach allows the model to
account for long-term dynamics when selecting examples. Finally, instead of
relying on small real-world datasets with limited scenario diversity, we train
our model on a large-scale synthetic dataset to enhance its prediction ability
by leveraging in-context examples. Extensive experiments demonstrate that
TrajICL achieves remarkable adaptation across both in-domain and cross-domain
scenarios, outperforming even fine-tuned approaches across multiple public
benchmarks. The code will be released at
https://fujiry0.github.io/TrajICL-project-page.

</details>


### [54] [Breaking Latent Prior Bias in Detectors for Generalizable AIGC Image Detection](https://arxiv.org/abs/2506.00874)
*Yue Zhou,Xinan He,KaiQing Lin,Bin Fan,Feng Ding,Bin Li*

Main category: cs.CV

TL;DR: AIGC检测器在训练生成器上表现优异，但对未见生成器泛化能力差。提出OMAT方法，通过优化潜在噪声生成对抗样本，提升跨生成器性能。


<details>
  <summary>Details</summary>
Motivation: 解决AIGC检测器因潜在先验偏差导致的泛化能力不足问题。

Method: 提出On-Manifold Adversarial Training (OMAT)，优化扩散模型的初始潜在噪声生成对抗样本。

Result: OMAT显著提升检测器在跨生成器场景下的性能，无需网络重新设计。

Conclusion: OMAT为构建更鲁棒的AIGC检测方法提供了新思路，并指导未来数据集和评估标准的设计。

Abstract: Current AIGC detectors often achieve near-perfect accuracy on images produced
by the same generator used for training but struggle to generalize to outputs
from unseen generators. We trace this failure in part to latent prior bias:
detectors learn shortcuts tied to patterns stemming from the initial noise
vector rather than learning robust generative artifacts. To address this, we
propose On-Manifold Adversarial Training (OMAT): by optimizing the initial
latent noise of diffusion models under fixed conditioning, we generate
on-manifold adversarial examples that remain on the generator's output
manifold-unlike pixel-space attacks, which introduce off-manifold perturbations
that the generator itself cannot reproduce and that can obscure the true
discriminative artifacts. To test against state-of-the-art generative models,
we introduce GenImage++, a test-only benchmark of outputs from advanced
generators (Flux.1, SD3) with extended prompts and diverse styles. We apply our
adversarial-training paradigm to ResNet50 and CLIP baselines and evaluate
across existing AIGC forensic benchmarks and recent challenge datasets.
Extensive experiments show that adversarially trained detectors significantly
improve cross-generator performance without any network redesign. Our findings
on latent-prior bias offer valuable insights for future dataset construction
and detector evaluation, guiding the development of more robust and
generalizable AIGC forensic methodologies.

</details>


### [55] [Uneven Event Modeling for Partially Relevant Video Retrieval](https://arxiv.org/abs/2506.00891)
*Sa Zhu,Huashan Chen,Wanqian Zhang,Jinchao Zhang,Zexian Yang,Xiaoshuai Hao,Bo Li*

Main category: cs.CV

TL;DR: 提出了一种名为UEM的框架，通过PGVS模块和CAER模块解决PRVR任务中的事件边界模糊和文本-视频对齐问题，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将视频分割为固定长度的片段，导致事件边界模糊，且使用平均池化计算事件表示，引入不对齐问题。

Method: 提出UEM框架，包括PGVS模块（基于时间和语义相似性迭代分割视频）和CAER模块（通过文本交叉注意力优化事件表示）。

Result: 在两个PRVR基准测试中取得了最先进的性能。

Conclusion: UEM框架通过明确的事件边界和优化的文本-视频对齐，显著提升了PRVR任务的性能。

Abstract: Given a text query, partially relevant video retrieval (PRVR) aims to
retrieve untrimmed videos containing relevant moments, wherein event modeling
is crucial for partitioning the video into smaller temporal events that
partially correspond to the text. Previous methods typically segment videos
into a fixed number of equal-length clips, resulting in ambiguous event
boundaries. Additionally, they rely on mean pooling to compute event
representations, inevitably introducing undesired misalignment. To address
these, we propose an Uneven Event Modeling (UEM) framework for PRVR. We first
introduce the Progressive-Grouped Video Segmentation (PGVS) module, to
iteratively formulate events in light of both temporal dependencies and
semantic similarity between consecutive frames, enabling clear event
boundaries. Furthermore, we also propose the Context-Aware Event Refinement
(CAER) module to refine the event representation conditioned the text's
cross-attention. This enables event representations to focus on the most
relevant frames for a given text, facilitating more precise text-video
alignment. Extensive experiments demonstrate that our method achieves
state-of-the-art performance on two PRVR benchmarks.

</details>


### [56] [Leveraging CLIP Encoder for Multimodal Emotion Recognition](https://arxiv.org/abs/2506.00903)
*Yehun Song,Sunyoung Cho*

Main category: cs.CV

TL;DR: 论文提出了一种基于CLIP的多模态情感识别框架MER-CLIP，通过标签编码器和跨模态解码器增强情感特征表示，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感识别（MER）因数据获取受限而性能提升困难，需利用大规模预训练模型的语义知识改进。

Method: 采用CLIP架构，引入标签编码器将标签转为文本嵌入，设计跨模态解码器对齐多模态特征到共享空间。

Result: 在CMU-MOSI和CMU-MOSEI数据集上超越现有最佳方法。

Conclusion: MER-CLIP通过语义标签嵌入和跨模态对齐，有效提升了多模态情感识别的性能。

Abstract: Multimodal emotion recognition (MER) aims to identify human emotions by
combining data from various modalities such as language, audio, and vision.
Despite the recent advances of MER approaches, the limitations in obtaining
extensive datasets impede the improvement of performance. To mitigate this
issue, we leverage a Contrastive Language-Image Pre-training (CLIP)-based
architecture and its semantic knowledge from massive datasets that aims to
enhance the discriminative multimodal representation. We propose a label
encoder-guided MER framework based on CLIP (MER-CLIP) to learn emotion-related
representations across modalities. Our approach introduces a label encoder that
treats labels as text embeddings to incorporate their semantic information,
leading to the learning of more representative emotional features. To further
exploit label semantics, we devise a cross-modal decoder that aligns each
modality to a shared embedding space by sequentially fusing modality features
based on emotion-related input from the label encoder. Finally, the label
encoder-guided prediction enables generalization across diverse labels by
embedding their semantic information as well as word labels. Experimental
results show that our method outperforms the state-of-the-art MER methods on
the benchmark datasets, CMU-MOSI and CMU-MOSEI.

</details>


### [57] [Towards Edge-Based Idle State Detection in Construction Machinery Using Surveillance Cameras](https://arxiv.org/abs/2506.00904)
*Xander Küpers,Jeroen Klein Brinke,Rob Bemthuis,Ozlem Durmaz Incel*

Main category: cs.CV

TL;DR: Edge-IMI框架通过边缘计算设备优化建筑机械的闲置检测，减少运营成本。


<details>
  <summary>Details</summary>
Motivation: 建筑行业机械利用率低导致成本增加和项目延误，需实时监控设备活动以提高效率。

Method: 提出Edge-IMI框架，包含目标检测、跟踪和闲置状态识别三部分，适用于资源受限的边缘设备。

Result: 目标检测F1分数71.75%，闲置识别模块准确区分活动与闲置状态，减少误报。

Conclusion: Edge-IMI实现高效现场推理，减少对高带宽云服务和昂贵硬件的依赖。

Abstract: The construction industry faces significant challenges in optimizing
equipment utilization, as underused machinery leads to increased operational
costs and project delays. Accurate and timely monitoring of equipment activity
is therefore key to identifying idle periods and improving overall efficiency.
This paper presents the Edge-IMI framework for detecting idle construction
machinery, specifically designed for integration with surveillance camera
systems. The proposed solution consists of three components: object detection,
tracking, and idle state identification, which are tailored for execution on
resource-constrained, CPU-based edge computing devices. The performance of
Edge-IMI is evaluated using a combined dataset derived from the ACID and MOCS
benchmarks. Experimental results confirm that the object detector achieves an
F1 score of 71.75%, indicating robust real-world detection capabilities. The
logistic regression-based idle identification module reliably distinguishes
between active and idle machinery with minimal false positives. Integrating all
three modules, Edge-IMI enables efficient on-site inference, reducing reliance
on high-bandwidth cloud services and costly hardware accelerators. We also
evaluate the performance of object detection models on Raspberry Pi 5 and an
Intel NUC platforms, as example edge computing platforms. We assess the
feasibility of real-time processing and the impact of model optimization
techniques.

</details>


### [58] [DS-VTON: High-Quality Virtual Try-on via Disentangled Dual-Scale Generation](https://arxiv.org/abs/2506.00908)
*Xianbing Sun,Yan Hong,Jiahui Zhan,Jun Lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: DS-VTON是一个双尺度虚拟试穿框架，通过分阶段处理结构对齐与纹理保留，实现了更高效的建模。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿方法难以同时解决服装图像与目标人体的精确对齐以及细粒度纹理保留的问题。

Method: DS-VTON采用两阶段方法：首先生成低分辨率结果以捕获语义对应关系，第二阶段通过残差引导扩散过程重建高分辨率输出。

Result: 实验表明，DS-VTON在多个标准虚拟试穿基准测试中实现了最先进的性能。

Conclusion: DS-VTON通过双尺度设计和无掩模生成范式，显著提升了虚拟试穿的效果。

Abstract: Despite recent progress, most existing virtual try-on methods still struggle
to simultaneously address two core challenges: accurately aligning the garment
image with the target human body, and preserving fine-grained garment textures
and patterns. In this paper, we propose DS-VTON, a dual-scale virtual try-on
framework that explicitly disentangles these objectives for more effective
modeling. DS-VTON consists of two stages: the first stage generates a
low-resolution try-on result to capture the semantic correspondence between
garment and body, where reduced detail facilitates robust structural alignment.
The second stage introduces a residual-guided diffusion process that
reconstructs high-resolution outputs by refining the residual between the two
scales, focusing on texture fidelity. In addition, our method adopts a fully
mask-free generation paradigm, eliminating reliance on human parsing maps or
segmentation masks. By leveraging the semantic priors embedded in pretrained
diffusion models, this design more effectively preserves the person's
appearance and geometric consistency. Extensive experiments demonstrate that
DS-VTON achieves state-of-the-art performance in both structural alignment and
texture preservation across multiple standard virtual try-on benchmarks.

</details>


### [59] [3D Skeleton-Based Action Recognition: A Review](https://arxiv.org/abs/2506.00915)
*Mengyuan Liu,Hong Liu,Qianshuo Hu,Bin Ren,Junsong Yuan,Jiaying Lin,Jiajun Wen*

Main category: cs.CV

TL;DR: 该论文提出了一种任务导向的框架，全面分析3D骨架动作识别的子任务，包括预处理、特征提取和时空建模，并探讨了最新技术进展和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有综述多从模型角度出发，忽略了骨架动作识别的关键步骤，阻碍了对任务的深入理解。本文旨在填补这一空白。

Method: 通过分解任务为子任务，重点讨论预处理、特征提取、时空建模等技术，并涵盖最新框架如混合架构、Mamba模型和生成模型。

Result: 提供了全面的子任务分析和最新技术进展，总结了公开数据集及先进算法表现。

Conclusion: 本文为3D骨架动作识别领域提供了结构化的路线图，促进了对该任务的深入理解和未来发展。

Abstract: With the inherent advantages of skeleton representation, 3D skeleton-based
action recognition has become a prominent topic in the field of computer
vision. However, previous reviews have predominantly adopted a model-oriented
perspective, often neglecting the fundamental steps involved in skeleton-based
action recognition. This oversight tends to ignore key components of
skeleton-based action recognition beyond model design and has hindered deeper,
more intrinsic understanding of the task. To bridge this gap, our review aims
to address these limitations by presenting a comprehensive, task-oriented
framework for understanding skeleton-based action recognition. We begin by
decomposing the task into a series of sub-tasks, placing particular emphasis on
preprocessing steps such as modality derivation and data augmentation. The
subsequent discussion delves into critical sub-tasks, including feature
extraction and spatio-temporal modeling techniques. Beyond foundational action
recognition networks, recently advanced frameworks such as hybrid
architectures, Mamba models, large language models (LLMs), and generative
models have also been highlighted. Finally, a comprehensive overview of public
3D skeleton datasets is presented, accompanied by an analysis of
state-of-the-art algorithms evaluated on these benchmarks. By integrating
task-oriented discussions, comprehensive examinations of sub-tasks, and an
emphasis on the latest advancements, our review provides a fundamental and
accessible structured roadmap for understanding and advancing the field of 3D
skeleton-based action recognition.

</details>


### [60] [Deep Temporal Reasoning in Video Language Models: A Cross-Linguistic Evaluation of Action Duration and Completion through Perfect Times](https://arxiv.org/abs/2506.00928)
*Olga Loginova,Sofía Ortega Loguinova*

Main category: cs.CV

TL;DR: 论文介绍了多语言数据集Perfect Times，用于评估视频-语言模型在时间推理上的表现，发现现有模型难以模拟人类的时间与因果推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究人类如何通过语言和视觉线索区分已完成和进行中的动作，并评估视频-语言模型在此类任务上的表现。

Method: 构建了多语言（英语、意大利语、俄语、日语）的多选题数据集Perfect Times，结合日常活动视频和完成度标签，测试模型的时间推理能力。

Result: 实验显示，现有模型在基于文本的任务上表现良好，但在视频中模拟人类时间和因果推理时表现不佳。

Conclusion: 研究强调了整合多模态线索以捕捉动作持续性和完成度的必要性，为视频-语言模型的时间推理评估设定了新标准。

Abstract: Human perception of events is intrinsically tied to distinguishing between
completed (perfect and telic) and ongoing (durative) actions, a process
mediated by both linguistic structure and visual cues. In this work, we
introduce the \textbf{Perfect Times} dataset, a novel, quadrilingual (English,
Italian, Russian, and Japanese) multiple-choice question-answering benchmark
designed to assess video-language models (VLMs) on temporal reasoning. By
pairing everyday activity videos with event completion labels and
perfectivity-tailored distractors, our dataset probes whether models truly
comprehend temporal dynamics or merely latch onto superficial markers.
Experimental results indicate that state-of-the-art models, despite their
success on text-based tasks, struggle to mirror human-like temporal and causal
reasoning grounded in video. This study underscores the necessity of
integrating deep multimodal cues to capture the nuances of action duration and
completion within temporal and causal video dynamics, setting a new standard
for evaluating and advancing temporal reasoning in VLMs.

</details>


### [61] [Deformable registration and generative modelling of aortic anatomies by auto-decoders and neural ODEs](https://arxiv.org/abs/2506.00947)
*Riccardo Tenderini,Luca Pegolotti,Fanwei Kong,Stefano Pagani,Francesco Regazzoni,Alison L. Marsden,Simone Deparis*

Main category: cs.CV

TL;DR: AD-SVFD是一种深度学习模型，用于血管形状的可变形配准和合成解剖结构的生成，通过点云和ODE建模实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 解决血管形状配准和生成合成解剖结构的计算效率与精度问题。

Method: 使用加权点云表示几何形状，通过ODE建模空间变形，优化Chamfer距离，采用自解码器结构实现权重共享。

Result: 在健康主动脉解剖结构上展示了高精度和计算效率。

Conclusion: AD-SVFD在血管形状配准和合成解剖生成方面表现出色，具有高效和精确的特点。

Abstract: This work introduces AD-SVFD, a deep learning model for the deformable
registration of vascular shapes to a pre-defined reference and for the
generation of synthetic anatomies. AD-SVFD operates by representing each
geometry as a weighted point cloud and models ambient space deformations as
solutions at unit time of ODEs, whose time-independent right-hand sides are
expressed through artificial neural networks. The model parameters are
optimized by minimizing the Chamfer Distance between the deformed and reference
point clouds, while backward integration of the ODE defines the inverse
transformation. A distinctive feature of AD-SVFD is its auto-decoder structure,
that enables generalization across shape cohorts and favors efficient weight
sharing. In particular, each anatomy is associated with a low-dimensional code
that acts as a self-conditioning field and that is jointly optimized with the
network parameters during training. At inference, only the latent codes are
fine-tuned, substantially reducing computational overheads. Furthermore, the
use of implicit shape representations enables generative applications: new
anatomies can be synthesized by suitably sampling from the latent space and
applying the corresponding inverse transformations to the reference geometry.
Numerical experiments, conducted on healthy aortic anatomies, showcase the
high-quality results of AD-SVFD, which yields extremely accurate approximations
at competitive computational costs.

</details>


### [62] [TIGeR: Text-Instructed Generation and Refinement for Template-Free Hand-Object Interaction](https://arxiv.org/abs/2506.00953)
*Yiyao Huang,Zhedong Zheng,Yu Ziwei,Yaxiong Wang,Tze Ho Elden Tse,Angela Yao*

Main category: cs.CV

TL;DR: TIGeR框架通过文本驱动生成和视觉引导细化，解决了预定义3D模板在重建手-物交互中的局限性，提升了适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 预定义3D模板需要大量手动工作且适应性受限，尤其是在遮挡严重的情况下。

Method: 采用两阶段框架：文本指令生成形状先验，再通过2D-3D协作注意力校准原型。

Result: 在Dex-YCB和Obman数据集上表现优异，超越现有无模板方法。

Conclusion: TIGeR框架在遮挡情况下表现鲁棒，且兼容多种先验来源。

Abstract: Pre-defined 3D object templates are widely used in 3D reconstruction of
hand-object interactions. However, they often require substantial manual
efforts to capture or source, and inherently restrict the adaptability of
models to unconstrained interaction scenarios, e.g., heavily-occluded objects.
To overcome this bottleneck, we propose a new Text-Instructed Generation and
Refinement (TIGeR) framework, harnessing the power of intuitive text-driven
priors to steer the object shape refinement and pose estimation. We use a
two-stage framework: a text-instructed prior generation and vision-guided
refinement. As the name implies, we first leverage off-the-shelf models to
generate shape priors according to the text description without tedious 3D
crafting. Considering the geometric gap between the synthesized prototype and
the real object interacted with the hand, we further calibrate the synthesized
prototype via 2D-3D collaborative attention. TIGeR achieves competitive
performance, i.e., 1.979 and 5.468 object Chamfer distance on the widely-used
Dex-YCB and Obman datasets, respectively, surpassing existing template-free
methods. Notably, the proposed framework shows robustness to occlusion, while
maintaining compatibility with heterogeneous prior sources, e.g., retrieved
hand-crafted prototypes, in practical deployment scenarios.

</details>


### [63] [Continual-MEGA: A Large-scale Benchmark for Generalizable Continual Anomaly Detection](https://arxiv.org/abs/2506.00956)
*Geonu Lee,Yujeong Oh,Geonhui Jang,Soyoung Lee,Jeonghyo Song,Sungmin Cha,YoungJoon Yoo*

Main category: cs.CV

TL;DR: 论文提出了一个新的持续学习基准Continual-MEGA，用于异常检测，包含多样化的数据集和新场景，提出了一种统一基线算法，并验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 为了更好地反映真实世界中的部署场景，提出一个更全面的持续学习基准，并解决现有方法在零样本泛化和像素级缺陷定位上的不足。

Method: 结合现有数据集和新数据集ContinualAD，提出零样本泛化场景，并设计统一基线算法以提高鲁棒性和泛化能力。

Result: 现有方法仍有改进空间，新方法表现优于现有方法，ContinualAD数据集提升了模型性能。

Conclusion: Continual-MEGA为持续学习提供了更全面的评估基准，新算法和数据集显著提升了异常检测性能。

Abstract: In this paper, we introduce a new benchmark for continual learning in anomaly
detection, aimed at better reflecting real-world deployment scenarios. Our
benchmark, Continual-MEGA, includes a large and diverse dataset that
significantly expands existing evaluation settings by combining carefully
curated existing datasets with our newly proposed dataset, ContinualAD. In
addition to standard continual learning with expanded quantity, we propose a
novel scenario that measures zero-shot generalization to unseen classes, those
not observed during continual adaptation. This setting poses a new problem
setting that continual adaptation also enhances zero-shot performance. We also
present a unified baseline algorithm that improves robustness in few-shot
detection and maintains strong generalization. Through extensive evaluations,
we report three key findings: (1) existing methods show substantial room for
improvement, particularly in pixel-level defect localization; (2) our proposed
method consistently outperforms prior approaches; and (3) the newly introduced
ContinualAD dataset enhances the performance of strong anomaly detection
models. We release the benchmark and code in
https://github.com/Continual-Mega/Continual-Mega.

</details>


### [64] [Camera Trajectory Generation: A Comprehensive Survey of Methods, Metrics, and Future Directions](https://arxiv.org/abs/2506.00974)
*Zahra Dehghanian,Pouya Ardekhani,Amir Vahedi,Hamid Beigy,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: 本文首次全面综述了相机轨迹生成领域，涵盖基础定义到高级方法，并分析了评估指标与数据集，指出了当前研究的局限与未来机会。


<details>
  <summary>Details</summary>
Motivation: 该领域缺乏系统性综述，本文旨在填补这一空白，为研究者提供基础资源并推动领域发展。

Method: 介绍相机表示方法，综述轨迹生成模型（规则、优化、机器学习、混合方法），分析评估指标与数据集。

Result: 提供了领域内知识整合与未来研究方向。

Conclusion: 本文为研究者提供基础，并推动相机轨迹系统在多样化应用中的创新。

Abstract: Camera trajectory generation is a cornerstone in computer graphics, robotics,
virtual reality, and cinematography, enabling seamless and adaptive camera
movements that enhance visual storytelling and immersive experiences. Despite
its growing prominence, the field lacks a systematic and unified survey that
consolidates essential knowledge and advancements in this domain. This paper
addresses this gap by providing the first comprehensive review of the field,
covering from foundational definitions to advanced methodologies. We introduce
the different approaches to camera representation and present an in-depth
review of available camera trajectory generation models, starting with
rule-based approaches and progressing through optimization-based techniques,
machine learning advancements, and hybrid methods that integrate multiple
strategies. Additionally, we gather and analyze the metrics and datasets
commonly used for evaluating camera trajectory systems, offering insights into
how these tools measure performance, aesthetic quality, and practical
applicability. Finally, we highlight existing limitations, critical gaps in
current research, and promising opportunities for investment and innovation in
the field. This paper not only serves as a foundational resource for
researchers entering the field but also paves the way for advancing adaptive,
efficient, and creative camera trajectory systems across diverse applications.

</details>


### [65] [CAPAA: Classifier-Agnostic Projector-Based Adversarial Attack](https://arxiv.org/abs/2506.00978)
*Zhan Li,Mingyu Zhao,Xin Dong,Haibin Ling,Bingyao Huang*

Main category: cs.CV

TL;DR: CAPAA提出了一种分类器无关的投影对抗攻击方法，通过聚合多个分类器的对抗和隐蔽性损失梯度，并结合注意力机制，提高了攻击成功率和隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对单一分类器和固定相机姿态，忽略了多分类器和变化相机姿态的复杂性，限制了其实际应用。

Method: 开发了分类器无关的对抗损失和优化框架，结合注意力机制加权梯度，集中扰动于高分类激活区域。

Result: 实验表明，CAPAA在攻击成功率和隐蔽性上优于现有基线。

Conclusion: CAPAA为多分类器和变化相机姿态场景提供了更有效的对抗攻击解决方案。

Abstract: Projector-based adversarial attack aims to project carefully designed light
patterns (i.e., adversarial projections) onto scenes to deceive deep image
classifiers. It has potential applications in privacy protection and the
development of more robust classifiers. However, existing approaches primarily
focus on individual classifiers and fixed camera poses, often neglecting the
complexities of multi-classifier systems and scenarios with varying camera
poses. This limitation reduces their effectiveness when introducing new
classifiers or camera poses. In this paper, we introduce Classifier-Agnostic
Projector-Based Adversarial Attack (CAPAA) to address these issues. First, we
develop a novel classifier-agnostic adversarial loss and optimization framework
that aggregates adversarial and stealthiness loss gradients from multiple
classifiers. Then, we propose an attention-based gradient weighting mechanism
that concentrates perturbations on regions of high classification activation,
thereby improving the robustness of adversarial projections when applied to
scenes with varying camera poses. Our extensive experimental evaluations
demonstrate that CAPAA achieves both a higher attack success rate and greater
stealthiness compared to existing baselines. Codes are available at:
https://github.com/ZhanLiQxQ/CAPAA.

</details>


### [66] [IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and Video AIGC Detection](https://arxiv.org/abs/2506.00979)
*Wayne Zhang,Changjiang Jiang,Zhonghao Zhang,Chenyang Si,Fengchang Yu,Wei Peng*

Main category: cs.CV

TL;DR: 论文提出了IVY-FAKE数据集和IVY-XDETECTOR模型，用于统一且可解释的多模态AIGC检测，解决了当前方法的局限性和透明性问题。


<details>
  <summary>Details</summary>
Motivation: AIGC技术的快速发展带来了内容真实性和完整性的挑战，现有检测方法缺乏统一性和可解释性。

Method: 提出IVY-FAKE数据集（包含丰富注释的图像和视频样本）和IVY-XDETECTOR模型（统一的可解释检测架构）。

Result: 模型在多个图像和视频检测基准上达到最先进性能。

Conclusion: IVY-FAKE和IVY-XDETECTOR显著提升了AIGC检测的统一性和可解释性。

Abstract: The rapid advancement of Artificial Intelligence Generated Content (AIGC) in
visual domains has resulted in highly realistic synthetic images and videos,
driven by sophisticated generative frameworks such as diffusion-based
architectures. While these breakthroughs open substantial opportunities, they
simultaneously raise critical concerns about content authenticity and
integrity. Many current AIGC detection methods operate as black-box binary
classifiers, which offer limited interpretability, and no approach supports
detecting both images and videos in a unified framework. This dual limitation
compromises model transparency, reduces trustworthiness, and hinders practical
deployment. To address these challenges, we introduce IVY-FAKE , a novel,
unified, and large-scale dataset specifically designed for explainable
multimodal AIGC detection. Unlike prior benchmarks, which suffer from
fragmented modality coverage and sparse annotations, IVY-FAKE contains over
150,000 richly annotated training samples (images and videos) and 18,700
evaluation examples, each accompanied by detailed natural-language reasoning
beyond simple binary labels. Building on this, we propose Ivy Explainable
Detector (IVY-XDETECTOR), a unified AIGC detection and explainable architecture
that jointly performs explainable detection for both image and video content.
Our unified vision-language model achieves state-of-the-art performance across
multiple image and video detection benchmarks, highlighting the significant
advancements enabled by our dataset and modeling framework. Our data is
publicly available at https://huggingface.co/datasets/AI-Safeguard/Ivy-Fake.

</details>


### [67] [GOBench: Benchmarking Geometric Optics Generation and Understanding of MLLMs](https://arxiv.org/abs/2506.00991)
*Xiaorong Zhu,Ziheng Jia,Jiarui Wang,Xiangyu Zhao,Haodong Duan,Xiongkuo Min,Jia Wang,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: GOBench是一个新基准，用于评估多模态大语言模型（MLLMs）在几何光学领域的生成和理解能力，发现当前模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在几何光学领域的细粒度物理原理评估不足，需要系统化的能力测试。

Method: 构建GOBench-Gen-1k数据集，通过主观实验评估生成图像的视觉真实性和美学质量，并设计评估指令测试11种MLLMs的光学理解能力。

Result: GPT-4o-Image在生成任务中表现最好但仍不完美，Gemini-2.5Pro在理解任务中准确率仅为37.35%。

Conclusion: 当前MLLMs在几何光学的生成和理解任务中仍面临重大挑战。

Abstract: The rapid evolution of Multi-modality Large Language Models (MLLMs) is
driving significant advancements in visual understanding and generation.
Nevertheless, a comprehensive assessment of their capabilities, concerning the
fine-grained physical principles especially in geometric optics, remains
underexplored. To address this gap, we introduce GOBench, the first benchmark
to systematically evaluate MLLMs' ability across two tasks: 1) Generating
Optically Authentic Imagery and 2) Understanding Underlying Optical Phenomena.
We curates high-quality prompts of geometric optical scenarios and use MLLMs to
construct GOBench-Gen-1k dataset.We then organize subjective experiments to
assess the generated imagery based on Optical Authenticity, Aesthetic Quality,
and Instruction Fidelity, revealing MLLMs' generation flaws that violate
optical principles. For the understanding task, we apply crafted evaluation
instructions to test optical understanding ability of eleven prominent MLLMs.
The experimental results demonstrate that current models face significant
challenges in both optical generation and understanding. The top-performing
generative model, GPT-4o-Image, cannot perfectly complete all generation tasks,
and the best-performing MLLM model, Gemini-2.5Pro, attains a mere 37.35\%
accuracy in optical understanding.

</details>


### [68] [Quotient Network -- A Network Similar to ResNet but Learning Quotients](https://arxiv.org/abs/2506.00992)
*Peng Hui,Jiamuyang Zhao,Changxin Li,Qingzhen Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种基于商数学习的网络（Quotient Network），解决了ResNet中特征差异学习的问题，并通过实验证明其性能优于ResNet。


<details>
  <summary>Details</summary>
Motivation: ResNet通过特征差异学习训练深度网络，但差异学习缺乏独立意义且对特征大小敏感。

Method: 提出商数网络，学习目标特征与现有特征的商数，并设计训练规则以提高性能。

Result: 在CIFAR10、CIFAR100和SVHN数据集上，商数网络性能优于ResNet，且无需增加参数。

Conclusion: 商数网络有效解决了ResNet的问题，性能更优，具有实际应用潜力。

Abstract: The emergence of ResNet provides a powerful tool for training extremely deep
networks. The core idea behind it is to change the learning goals of the
network. It no longer learns new features from scratch but learns the
difference between the target and existing features. However, the difference
between the two kinds of features does not have an independent and clear
meaning, and the amount of learning is based on the absolute rather than the
relative difference, which is sensitive to the size of existing features. We
propose a new network that perfectly solves these two problems while still
having the advantages of ResNet. Specifically, it chooses to learn the quotient
of the target features with the existing features, so we call it the quotient
network. In order to enable this network to learn successfully and achieve
higher performance, we propose some design rules for this network so that it
can be trained efficiently and achieve better performance than ResNet.
Experiments on the CIFAR10, CIFAR100, and SVHN datasets prove that this network
can stably achieve considerable improvements over ResNet by simply making tiny
corresponding changes to the original ResNet network without adding new
parameters.

</details>


### [69] [FlexSelect: Flexible Token Selection for Efficient Long Video Understanding](https://arxiv.org/abs/2506.00993)
*Yunzhu Zhang,Yu Lu,Tianyi Wang,Fengyun Rao,Yi Yang,Linchao Zhu*

Main category: cs.CV

TL;DR: FlexSelect是一种灵活高效的令牌选择策略，用于处理长视频，通过跨模态注意力模式识别并保留语义相关内容，显著提升视频大语言模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 长视频理解对视频大语言模型（VideoLLMs）提出了高计算和内存需求的挑战，需要一种高效的方法来处理长视频内容。

Method: FlexSelect包括两个关键组件：1）无训练的令牌排名管道，利用跨模态注意力权重估计令牌重要性；2）轻量级选择器，训练以复制这些排名并过滤冗余令牌。

Result: FlexSelect在多个长视频基准测试（如VideoMME、MLVU等）中表现优异，速度提升显著（例如LLaVA-Video-7B模型上提升9倍）。

Conclusion: FlexSelect是一种通用且高效的解决方案，可无缝集成到多种VideoLLM架构中，显著提升长视频理解的效率和性能。

Abstract: Long-form video understanding poses a significant challenge for video large
language models (VideoLLMs) due to prohibitively high computational and memory
demands. In this paper, we propose FlexSelect, a flexible and efficient token
selection strategy for processing long videos. FlexSelect identifies and
retains the most semantically relevant content by leveraging cross-modal
attention patterns from a reference transformer layer. It comprises two key
components: (1) a training-free token ranking pipeline that leverages faithful
cross-modal attention weights to estimate each video token's importance, and
(2) a rank-supervised lightweight selector that is trained to replicate these
rankings and filter redundant tokens. This generic approach can be seamlessly
integrated into various VideoLLM architectures, such as LLaVA-Video, InternVL
and Qwen-VL, serving as a plug-and-play module to extend their temporal context
length. Empirically, FlexSelect delivers strong gains across multiple
long-video benchmarks including VideoMME, MLVU, LongVB, and LVBench. Moreover,
it achieves significant speed-ups (for example, up to 9 times on a
LLaVA-Video-7B model), highlighting FlexSelect's promise for efficient
long-form video understanding. Project page available at:
https://yunzhuzhang0918.github.io/flex_select

</details>


### [70] [Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion Models](https://arxiv.org/abs/2506.00996)
*Kinam Kim,Junha Hyung,Jaegul Choo*

Main category: cs.CV

TL;DR: TIC-FT是一种高效且通用的方法，用于适应预训练的视频扩散模型到多样化的条件生成任务，无需架构修改，仅需少量样本即可实现高性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部编码器或架构修改，需要大数据集且灵活性受限，TIC-FT旨在解决这些问题。

Method: 通过沿时间轴连接条件和目标帧，并插入噪声逐渐增加的缓冲帧，实现平滑过渡和高效微调。

Result: 在图像到视频和视频到视频生成任务中，TIC-FT在条件保真度和视觉质量上优于基线方法。

Conclusion: TIC-FT是一种高效、灵活且可扩展的条件生成方法，适用于多种任务。

Abstract: Recent advances in text-to-video diffusion models have enabled high-quality
video synthesis, but controllable generation remains challenging, particularly
under limited data and compute. Existing fine-tuning methods for conditional
generation often rely on external encoders or architectural modifications,
which demand large datasets and are typically restricted to spatially aligned
conditioning, limiting flexibility and scalability. In this work, we introduce
Temporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach
for adapting pretrained video diffusion models to diverse conditional
generation tasks. Our key idea is to concatenate condition and target frames
along the temporal axis and insert intermediate buffer frames with
progressively increasing noise levels. These buffer frames enable smooth
transitions, aligning the fine-tuning process with the pretrained model's
temporal dynamics. TIC-FT requires no architectural changes and achieves strong
performance with as few as 10-30 training samples. We validate our method
across a range of tasks, including image-to-video and video-to-video
generation, using large-scale base models such as CogVideoX-5B and Wan-14B.
Extensive experiments show that TIC-FT outperforms existing baselines in both
condition fidelity and visual quality, while remaining highly efficient in both
training and inference. For additional results, visit
https://kinam0252.github.io/TIC-FT/

</details>


### [71] [Pseudo-Labeling Driven Refinement of Benchmark Object Detection Datasets via Analysis of Learning Patterns](https://arxiv.org/abs/2506.00997)
*Min Je Kim,Muhammad Munsif,Altaf Hussain,Hikmat Yar,Sung Wook Baik*

Main category: cs.CV

TL;DR: 论文提出了一种名为MJ-COCO的MS-COCO数据集改进版本，通过自动化方法修正标注错误，显著提升了目标检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: MS-COCO数据集虽然广泛使用，但其标注错误（如缺失标签、类别错误等）影响了模型的训练和泛化能力。

Method: 采用基于损失和梯度的错误检测，结合四阶段伪标签修正流程（包括边界框生成、重复去除、类别验证和空间调整）。

Result: 实验表明，使用MJ-COCO训练的模型在多个数据集上表现优于MS-COCO，且标注覆盖率显著提升（如小目标标注增加20万）。

Conclusion: MJ-COCO通过自动化修正标注错误，为目标检测任务提供了更可靠的数据集。

Abstract: Benchmark object detection (OD) datasets play a pivotal role in advancing
computer vision applications such as autonomous driving, and surveillance, as
well as in training and evaluating deep learning-based state-of-the-art
detection models. Among them, MS-COCO has become a standard benchmark due to
its diverse object categories and complex scenes. However, despite its wide
adoption, MS-COCO suffers from various annotation issues, including missing
labels, incorrect class assignments, inaccurate bounding boxes, duplicate
labels, and group labeling inconsistencies. These errors not only hinder model
training but also degrade the reliability and generalization of OD models. To
address these challenges, we propose a comprehensive refinement framework and
present MJ-COCO, a newly re-annotated version of MS-COCO. Our approach begins
with loss and gradient-based error detection to identify potentially mislabeled
or hard-to-learn samples. Next, we apply a four-stage pseudo-labeling
refinement process: (1) bounding box generation using invertible
transformations, (2) IoU-based duplicate removal and confidence merging, (3)
class consistency verification via expert objects recognizer, and (4) spatial
adjustment based on object region activation map analysis. This integrated
pipeline enables scalable and accurate correction of annotation errors without
manual re-labeling. Extensive experiments were conducted across four validation
datasets: MS-COCO, Sama COCO, Objects365, and PASCAL VOC. Models trained on
MJ-COCO consistently outperformed those trained on MS-COCO, achieving
improvements in Average Precision (AP) and APS metrics. MJ-COCO also
demonstrated significant gains in annotation coverage: for example, the number
of small object annotations increased by more than 200,000 compared to MS-COCO.

</details>


### [72] [Motion-Aware Concept Alignment for Consistent Video Editing](https://arxiv.org/abs/2506.01004)
*Tong Zhang,Juan C Leon Alcazar,Bernard Ghanem*

Main category: cs.CV

TL;DR: MoCA-Video是一个无需训练的视频语义混合框架，通过参考图像将语义特征注入视频中的特定对象，同时保留原始运动和视觉上下文。


<details>
  <summary>Details</summary>
Motivation: 解决图像域语义混合与视频之间的差距，实现可控的高质量视频合成。

Method: 利用对角去噪调度和类无关分割检测和跟踪对象，结合动量语义校正和伽马残差噪声稳定化确保时间一致性。

Result: 在SSIM、LPIPS和新型CASS指标上表现优异，优于现有基线。

Conclusion: MoCA-Video展示了在扩散噪声轨迹中进行结构化操作可实现可控的高质量视频合成。

Abstract: We introduce MoCA-Video (Motion-Aware Concept Alignment in Video), a
training-free framework bridging the gap between image-domain semantic mixing
and video. Given a generated video and a user-provided reference image,
MoCA-Video injects the semantic features of the reference image into a specific
object within the video, while preserving the original motion and visual
context. Our approach leverages a diagonal denoising schedule and
class-agnostic segmentation to detect and track objects in the latent space and
precisely control the spatial location of the blended objects. To ensure
temporal coherence, we incorporate momentum-based semantic corrections and
gamma residual noise stabilization for smooth frame transitions. We evaluate
MoCA's performance using the standard SSIM, image-level LPIPS, temporal LPIPS,
and introduce a novel metric CASS (Conceptual Alignment Shift Score) to
evaluate the consistency and effectiveness of the visual shifts between the
source prompt and the modified video frames. Using self-constructed dataset,
MoCA-Video outperforms current baselines, achieving superior spatial
consistency, coherent motion, and a significantly higher CASS score, despite
having no training or fine-tuning. MoCA-Video demonstrates that structured
manipulation in the diffusion noise trajectory allows for controllable,
high-quality video synthesis.

</details>


### [73] [AuralSAM2: Enabling SAM2 Hear Through Pyramid Audio-Visual Feature Prompting](https://arxiv.org/abs/2506.01015)
*Yuyuan Liu,Yuanhong Chen,Chong Wang,Junlin Han,Junde Wu,Can Peng,Jingkun Chen,Yu Tian,Gustavo Carneiro*

Main category: cs.CV

TL;DR: AuralSAM2通过AuralFuser模块将音频与视觉特征融合，改进SAM2在视频中的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在音频与视觉模态融合上效率低或定位不精确，且忽略了多模态间的语义交互。

Method: 提出AuralFuser模块，结合特征金字塔和音频引导对比学习，优化多模态特征融合。

Result: 在公开基准测试中表现优于现有方法。

Conclusion: AuralSAM2有效提升了多模态场景下的分割性能。

Abstract: Segment Anything Model 2 (SAM2) exhibits strong generalisation for promptable
segmentation in video clips; however, its integration with the audio modality
remains underexplored. Existing approaches mainly follow two directions: (1)
injecting adapters into the image encoder to receive audio signals, which
incurs efficiency costs during prompt engineering, and (2) leveraging
additional foundation models to generate visual prompts for the sounding
objects, which are often imprecisely localised, leading to misguidance in SAM2.
Moreover, these methods overlook the rich semantic interplay between
hierarchical visual features and other modalities, resulting in suboptimal
cross-modal fusion. In this work, we propose AuralSAM2, comprising the novel
AuralFuser module, which externally attaches to SAM2 to integrate features from
different modalities and generate feature-level prompts, guiding SAM2's decoder
in segmenting sounding targets. Such integration is facilitated by a feature
pyramid, further refining semantic understanding and enhancing object awareness
in multimodal scenarios. Additionally, the audio-guided contrastive learning is
introduced to explicitly align audio and visual representations and to also
mitigate biases caused by dominant visual patterns. Results on public
benchmarks show that our approach achieves remarkable improvements over the
previous methods in the field. Code is available at
https://github.com/yyliu01/AuralSAM2.

</details>


### [74] [Modality Translation and Registration of MR and Ultrasound Images Using Diffusion Models](https://arxiv.org/abs/2506.01025)
*Xudong Ma,Nantheera Anantrasirichai,Stefanos Bolomytis,Alin Achim*

Main category: cs.CV

TL;DR: 提出了一种基于层次特征解缠设计的解剖学一致性模态转换网络（ACMT），用于解决多模态MR-US配准中的模态差异问题。


<details>
  <summary>Details</summary>
Motivation: 多模态MR-US配准对前列腺癌诊断至关重要，但现有方法难以对齐关键边界且对无关细节过于敏感。

Method: 通过浅层特征保持纹理一致性，深层特征保留边界，并引入中间伪模态设计，将MR和US图像转换至该中间域。

Result: 实验表明，ACMT能减少模态差异并保留关键解剖边界，定量评估显示其模态相似性优于现有方法。

Conclusion: ACMT框架在多模态前列腺图像配准中表现出色，下游配准实验验证了其鲁棒性。

Abstract: Multimodal MR-US registration is critical for prostate cancer diagnosis.
However, this task remains challenging due to significant modality
discrepancies. Existing methods often fail to align critical boundaries while
being overly sensitive to irrelevant details. To address this, we propose an
anatomically coherent modality translation (ACMT) network based on a
hierarchical feature disentanglement design. We leverage shallow-layer features
for texture consistency and deep-layer features for boundary preservation.
Unlike conventional modality translation methods that convert one modality into
another, our ACMT introduces the customized design of an intermediate pseudo
modality. Both MR and US images are translated toward this intermediate domain,
effectively addressing the bottlenecks faced by traditional translation methods
in the downstream registration task. Experiments demonstrate that our method
mitigates modality-specific discrepancies while preserving crucial anatomical
boundaries for accurate registration. Quantitative evaluations show superior
modality similarity compared to state-of-the-art modality translation methods.
Furthermore, downstream registration experiments confirm that our translated
images achieve the best alignment performance, highlighting the robustness of
our framework for multi-modal prostate image registration.

</details>


### [75] [NavBench: Probing Multimodal Large Language Models for Embodied Navigation](https://arxiv.org/abs/2506.01031)
*Yanyuan Qiao,Haodong Hong,Wenqi Lyu,Dong An,Siqi Zhang,Yutong Xie,Xinyu Wang,Qi Wu*

Main category: cs.CV

TL;DR: NavBench是一个评估多模态大语言模型（MLLMs）在零样本设置下导航能力的基准，包含导航理解和逐步执行两部分。研究发现GPT-4o表现优异，但多数模型在时间理解方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 探索MLLMs在具身环境中的理解和行动能力，填补现有研究的空白。

Method: 通过NavBench评估MLLMs的导航能力，包括导航理解（3,200个问题）和逐步执行（432个场景）。

Result: GPT-4o表现最佳，轻量开源模型在简单任务中表现良好。模型理解能力与执行性能正相关，但时间理解是主要挑战。

Conclusion: NavBench为MLLMs的导航能力提供了评估标准，时间理解是未来改进的关键方向。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated strong
generalization in vision-language tasks, yet their ability to understand and
act within embodied environments remains underexplored. We present NavBench, a
benchmark to evaluate the embodied navigation capabilities of MLLMs under
zero-shot settings. NavBench consists of two components: (1) navigation
comprehension, assessed through three cognitively grounded tasks including
global instruction alignment, temporal progress estimation, and local
observation-action reasoning, covering 3,200 question-answer pairs; and (2)
step-by-step execution in 432 episodes across 72 indoor scenes, stratified by
spatial, cognitive, and execution complexity. To support real-world deployment,
we introduce a pipeline that converts MLLMs' outputs into robotic actions. We
evaluate both proprietary and open-source models, finding that GPT-4o performs
well across tasks, while lighter open-source models succeed in simpler cases.
Results also show that models with higher comprehension scores tend to achieve
better execution performance. Providing map-based context improves decision
accuracy, especially in medium-difficulty scenarios. However, most models
struggle with temporal understanding, particularly in estimating progress
during navigation, which may pose a key challenge.

</details>


### [76] [Self-supervised ControlNet with Spatio-Temporal Mamba for Real-world Video Super-resolution](https://arxiv.org/abs/2506.01037)
*Shijun Shi,Jing Xu,Lijing Lu,Zhihang Li,Kai Hu*

Main category: cs.CV

TL;DR: 提出了一种基于自监督学习和Mamba的噪声鲁棒性视频超分辨率框架，通过改进扩散模型和引入自监督ControlNet，显著提升了视频质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的视频超分辨率方法因随机性易引入复杂退化和明显伪影，需改进以提升鲁棒性和质量。

Method: 结合自监督学习和Mamba，改进扩散模型为全局时空注意力机制，引入自监督ControlNet以减少伪影，并采用三阶段训练策略。

Result: 在真实视频超分辨率基准数据集上，算法表现出优于现有技术的感知质量。

Conclusion: 所提出的模型设计和训练策略有效提升了视频超分辨率的鲁棒性和生成质量。

Abstract: Existing diffusion-based video super-resolution (VSR) methods are susceptible
to introducing complex degradations and noticeable artifacts into
high-resolution videos due to their inherent randomness. In this paper, we
propose a noise-robust real-world VSR framework by incorporating
self-supervised learning and Mamba into pre-trained latent diffusion models. To
ensure content consistency across adjacent frames, we enhance the diffusion
model with a global spatio-temporal attention mechanism using the Video
State-Space block with a 3D Selective Scan module, which reinforces coherence
at an affordable computational cost. To further reduce artifacts in generated
details, we introduce a self-supervised ControlNet that leverages HR features
as guidance and employs contrastive learning to extract degradation-insensitive
features from LR videos. Finally, a three-stage training strategy based on a
mixture of HR-LR videos is proposed to stabilize VSR training. The proposed
Self-supervised ControlNet with Spatio-Temporal Continuous Mamba based VSR
algorithm achieves superior perceptual quality than state-of-the-arts on
real-world VSR benchmark datasets, validating the effectiveness of the proposed
model design and training strategies.

</details>


### [77] [ECP-Mamba: An Efficient Multi-scale Self-supervised Contrastive Learning Method with State Space Model for PolSAR Image Classification](https://arxiv.org/abs/2506.01040)
*Zuzheng Kuang,Haixia Bi,Chen Xu,Jian Sun*

Main category: cs.CV

TL;DR: ECP-Mamba框架结合多尺度自监督对比学习和状态空间模型，解决了PolSAR图像分类中标注数据稀缺和计算效率低的问题，实现了高精度与资源效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的PolSAR分类方法依赖大量标注数据且计算效率低，ECP-Mamba旨在解决这些问题。

Method: 通过多尺度预测任务和自蒸馏范式解决标注稀缺问题；采用Mamba架构和螺旋扫描策略提升计算效率；引入轻量级Cross Mamba模块实现多尺度特征交互。

Result: 在四个基准数据集上表现优异，Flevoland 1989数据集的总体准确率达99.70%。

Conclusion: ECP-Mamba在PolSAR分类中实现了高精度与高效计算的平衡，具有显著优势。

Abstract: Recently, polarimetric synthetic aperture radar (PolSAR) image classification
has been greatly promoted by deep neural networks. However,current deep
learning-based PolSAR classification methods encounter difficulties due to its
dependence on extensive labeled data and the computational inefficiency of
architectures like Transformers. This paper presents ECP-Mamba, an efficient
framework integrating multi-scale self-supervised contrastive learning with a
state space model (SSM) backbone. Specifically, ECP-Mamba addresses annotation
scarcity through a multi-scale predictive pretext task based on local-to-global
feature correspondences, which uses a simplified self-distillation paradigm
without negative sample pairs. To enhance computational efficiency,the Mamba
architecture (a selective SSM) is first tailored for pixel-wise PolSAR
classification task by designing a spiral scan strategy. This strategy
prioritizes causally relevant features near the central pixel, leveraging the
localized nature of pixel-wise classification tasks. Additionally, the
lightweight Cross Mamba module is proposed to facilitates complementary
multi-scale feature interaction with minimal overhead. Extensive experiments
across four benchmark datasets demonstrate ECP-Mamba's effectiveness in
balancing high accuracy with resource efficiency. On the Flevoland 1989
dataset, ECP-Mamba achieves state-of-the-art performance with an overall
accuracy of 99.70%, average accuracy of 99.64% and Kappa coefficient of
99.62e-2. Our code will be available at
https://github.com/HaixiaBi1982/ECP_Mamba.

</details>


### [78] [AceVFI: A Comprehensive Survey of Advances in Video Frame Interpolation](https://arxiv.org/abs/2506.01061)
*Dahyeon Kye,Changhyun Roh,Sukhun Ko,Chanho Eom,Jihyong Oh*

Main category: cs.CV

TL;DR: AceVFI是一篇关于视频帧插值（VFI）的全面综述，涵盖250多篇论文，系统分类了VFI方法，分析了关键挑战，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 视频帧插值是低级视觉任务中的基础问题，旨在生成中间帧并保持时空一致性。随着技术从传统方法发展到深度学习，需要一篇全面综述来梳理现有方法。

Method: AceVFI系统整理了VFI方法，包括基于核、光流、混合、相位、GAN、Transformer、Mamba和扩散模型的方法，并分类为CTFI和ATFI。

Result: 综述详细分析了VFI的核心原理、设计假设和技术特点，总结了关键挑战（如大运动、遮挡等）、数据集、评估指标和应用场景。

Conclusion: AceVFI旨在为新手和专家提供现代VFI领域的统一参考，并指出了未来研究方向以推动领域进步。

Abstract: Video Frame Interpolation (VFI) is a fundamental Low-Level Vision (LLV) task
that synthesizes intermediate frames between existing ones while maintaining
spatial and temporal coherence. VFI techniques have evolved from classical
motion compensation-based approach to deep learning-based approach, including
kernel-, flow-, hybrid-, phase-, GAN-, Transformer-, Mamba-, and more recently
diffusion model-based approach. We introduce AceVFI, the most comprehensive
survey on VFI to date, covering over 250+ papers across these approaches. We
systematically organize and describe VFI methodologies, detailing the core
principles, design assumptions, and technical characteristics of each approach.
We categorize the learning paradigm of VFI methods namely, Center-Time Frame
Interpolation (CTFI) and Arbitrary-Time Frame Interpolation (ATFI). We analyze
key challenges of VFI such as large motion, occlusion, lighting variation, and
non-linear motion. In addition, we review standard datasets, loss functions,
evaluation metrics. We examine applications of VFI including event-based,
cartoon, medical image VFI and joint VFI with other LLV tasks. We conclude by
outlining promising future research directions to support continued progress in
the field. This survey aims to serve as a unified reference for both newcomers
and experts seeking a deep understanding of modern VFI landscapes.

</details>


### [79] [Fighting Fire with Fire (F3): A Training-free and Efficient Visual Adversarial Example Purification Method in LVLMs](https://arxiv.org/abs/2506.01064)
*Yudong Zhang,Ruobing Xie,Yiqing Huang,Jiansheng Chen,Xingwu Sun,Zhanhui Kang,Di Wang,Yu Wang*

Main category: cs.CV

TL;DR: F3是一种对抗性净化框架，通过引入简单扰动来抵消视觉对抗攻击的影响，无需训练且高效。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）易受视觉对抗攻击影响，但现有净化方法研究较少。

Method: F3采用“以毒攻毒”策略，通过随机扰动对抗样本并利用跨模态注意力优化输出。

Result: F3显著提升了净化效果，且具有训练自由、高效和易实现的优势。

Conclusion: F3适用于大规模工业应用，平衡了鲁棒性和效率。

Abstract: Recent advances in large vision-language models (LVLMs) have showcased their
remarkable capabilities across a wide range of multimodal vision-language
tasks. However, these models remain vulnerable to visual adversarial attacks,
which can substantially compromise their performance. Despite their potential
impact, the development of effective methods for purifying such adversarial
examples has received relatively limited attention. In this paper, we introduce
F3, a novel adversarial purification framework that employs a counterintuitive
"fighting fire with fire" strategy: intentionally introducing simple
perturbations to adversarial examples to mitigate their harmful effects.
Specifically, F3 leverages cross-modal attentions derived from randomly
perturbed adversary examples as reference targets. By injecting noise into
these adversarial examples, F3 effectively refines their attention, resulting
in cleaner and more reliable model outputs. Remarkably, this seemingly
paradoxical approach of employing noise to counteract adversarial attacks
yields impressive purification results. Furthermore, F3 offers several distinct
advantages: it is training-free and straightforward to implement, and exhibits
significant computational efficiency improvements compared to existing
purification methods. These attributes render F3 particularly suitable for
large-scale industrial applications where both robust performance and
operational efficiency are critical priorities. The code will be made publicly
available.

</details>


### [80] [Revolutionizing Blood Banks: AI-Driven Fingerprint-Blood Group Correlation for Enhanced Safety](https://arxiv.org/abs/2506.01069)
*Malik A. Altayar,Muhyeeddin Alqaraleh,Mowafaq Salem Alzboon,Wesam T. Almagharbeh*

Main category: cs.CV

TL;DR: 研究探讨了指纹模式与ABO血型的关系，发现两者关联性较弱，血型数据对指纹识别的改进作用有限。


<details>
  <summary>Details</summary>
Motivation: 探索低成本、易实施的生物识别方法，以补充现有高成本技术。

Method: 分析200名受试者的指纹模式（环、涡、弓）与血型，使用卡方检验和皮尔逊相关性分析。

Result: 环状指纹最常见，O+血型最普遍，但指纹模式与血型无显著统计学关联。

Conclusion: 血型数据对指纹识别的改进作用有限，未来可结合多模态生物特征或机器学习提升识别效果。

Abstract: Identification of a person is central in forensic science, security, and
healthcare. Methods such as iris scanning and genomic profiling are more
accurate but expensive, time-consuming, and more difficult to implement. This
study focuses on the relationship between the fingerprint patterns and the ABO
blood group as a biometric identification tool. A total of 200 subjects were
included in the study, and fingerprint types (loops, whorls, and arches) and
blood groups were compared. Associations were evaluated with statistical tests,
including chi-square and Pearson correlation. The study found that the loops
were the most common fingerprint pattern and the O+ blood group was the most
prevalent. Even though there was some associative pattern, there was no
statistically significant difference in the fingerprint patterns of different
blood groups. Overall, the results indicate that blood group data do not
significantly improve personal identification when used in conjunction with
fingerprinting. Although the study shows weak correlation, it may emphasize the
efforts of multi-modal based biometric systems in enhancing the current
biometric systems. Future studies may focus on larger and more diverse samples,
and possibly machine learning and additional biometrics to improve
identification methods. This study addresses an element of the ever-changing
nature of the fields of forensic science and biometric identification,
highlighting the importance of resilient analytical methods for personal
identification.

</details>


### [81] [Aligned Contrastive Loss for Long-Tailed Recognition](https://arxiv.org/abs/2506.01071)
*Jiali Ma,Jiequan Cui,Maeno Kazuki,Lakshmi Subramanian,Karlekar Jayashree,Sugiri Pranata,Hanwang Zhang*

Main category: cs.CV

TL;DR: 提出了一种对齐对比学习（ACL）算法，解决了长尾识别问题，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 多视图训练虽然能提升性能，但对比学习在视图增加时未能持续增强模型泛化能力，需解决梯度冲突和不平衡问题。

Method: 通过理论梯度分析发现问题，设计了ACL算法以消除梯度冲突和不平衡问题。

Result: 在多个基准测试中，ACL表现出色，实现了新的最优性能。

Conclusion: ACL算法有效解决了长尾识别问题，并在实验中验证了其优越性。

Abstract: In this paper, we propose an Aligned Contrastive Learning (ACL) algorithm to
address the long-tailed recognition problem. Our findings indicate that while
multi-view training boosts the performance, contrastive learning does not
consistently enhance model generalization as the number of views increases.
Through theoretical gradient analysis of supervised contrastive learning (SCL),
we identify gradient conflicts, and imbalanced attraction and repulsion
gradients between positive and negative pairs as the underlying issues. Our ACL
algorithm is designed to eliminate these problems and demonstrates strong
performance across multiple benchmarks. We validate the effectiveness of ACL
through experiments on long-tailed CIFAR, ImageNet, Places, and iNaturalist
datasets. Results show that ACL achieves new state-of-the-art performance.

</details>


### [82] [A Large Convolutional Neural Network for Clinical Target and Multi-organ Segmentation in Gynecologic Brachytherapy with Multi-stage Learning](https://arxiv.org/abs/2506.01073)
*Mingzhe Hu,Yuan Gao,Yuheng Li,Ricahrd LJ Qiu,Chih-Wei Chang,Keyur D. Shah,Priyanka Kapoor,Beth Bradshaw,Yuan Shao,Justin Roper,Jill Remick,Zhen Tian,Xiaofeng Yang*

Main category: cs.CV

TL;DR: GynBTNet是一种多阶段学习框架，通过自监督预训练和分层微调策略，显著提升了妇科近距离放射治疗中临床靶区和危及器官的分割性能。


<details>
  <summary>Details</summary>
Motivation: 妇科近距离放射治疗中，临床靶区和危及器官的准确分割对治疗计划至关重要，但解剖变异性、CT成像的低软组织对比度及有限标注数据带来了挑战。

Method: GynBTNet采用三阶段训练策略：自监督预训练、多器官分割数据集的监督微调及针对妇科近距离放射治疗的特定任务微调。

Result: GynBTNet在Dice相似系数、Hausdorff距离和平均表面距离等指标上显著优于基线模型，尤其对复杂边界结构的分割效果提升明显。

Conclusion: GynBTNet通过自监督预训练和分层微调策略，显著提升了分割性能，但乙状结肠的分割仍具挑战性。

Abstract: Purpose: Accurate segmentation of clinical target volumes (CTV) and
organs-at-risk is crucial for optimizing gynecologic brachytherapy (GYN-BT)
treatment planning. However, anatomical variability, low soft-tissue contrast
in CT imaging, and limited annotated datasets pose significant challenges. This
study presents GynBTNet, a novel multi-stage learning framework designed to
enhance segmentation performance through self-supervised pretraining and
hierarchical fine-tuning strategies. Methods: GynBTNet employs a three-stage
training strategy: (1) self-supervised pretraining on large-scale CT datasets
using sparse submanifold convolution to capture robust anatomical
representations, (2) supervised fine-tuning on a comprehensive multi-organ
segmentation dataset to refine feature extraction, and (3) task-specific
fine-tuning on a dedicated GYN-BT dataset to optimize segmentation performance
for clinical applications. The model was evaluated against state-of-the-art
methods using the Dice Similarity Coefficient (DSC), 95th percentile Hausdorff
Distance (HD95), and Average Surface Distance (ASD). Results: Our GynBTNet
achieved superior segmentation performance, significantly outperforming nnU-Net
and Swin-UNETR. Notably, it yielded a DSC of 0.837 +/- 0.068 for CTV, 0.940 +/-
0.052 for the bladder, 0.842 +/- 0.070 for the rectum, and 0.871 +/- 0.047 for
the uterus, with reduced HD95 and ASD compared to baseline models.
Self-supervised pretraining led to consistent performance improvements,
particularly for structures with complex boundaries. However, segmentation of
the sigmoid colon remained challenging, likely due to anatomical ambiguities
and inter-patient variability. Statistical significance analysis confirmed that
GynBTNet's improvements were significant compared to baseline models.

</details>


### [83] [GThinker: Towards General Multimodal Reasoning via Cue-Guided Rethinking](https://arxiv.org/abs/2506.01078)
*Yufei Zhan,Ziheng Wu,Yousong Zhu,Rongkun Xue,Ruipu Luo,Zhenghao Chen,Can Zhang,Yifan Li,Zhentao He,Zheming Yang,Ming Tang,Minghui Qiu,Jinqiao Wang*

Main category: cs.CV

TL;DR: GThinker是一种新型多模态推理模型，通过Cue-Rethinking模式和两阶段训练方法，显著提升了视觉中心多模态推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉中心推理任务中表现不佳，主要依赖逻辑和知识推理，未能有效整合视觉信息。

Method: 提出Cue-Rethinking模式，结合两阶段训练（模式引导冷启动和激励强化学习），并使用GThinker-11K数据集支持训练。

Result: 在M$^3$CoT基准测试中达到81.5%，优于O4-mini模型，并在通用场景多模态推理中平均提升2.1%。

Conclusion: GThinker通过视觉线索的迭代重新解释和强化学习，显著提升了多模态推理能力，填补了通用多模态推理的数据空白。

Abstract: Despite notable advancements in multimodal reasoning, leading Multimodal
Large Language Models (MLLMs) still underperform on vision-centric multimodal
reasoning tasks in general scenarios. This shortfall stems from their
predominant reliance on logic- and knowledge-based slow thinking strategies,
while effective for domains like math and science, fail to integrate visual
information effectively during reasoning. Consequently, these models often fail
to adequately ground visual cues, resulting in suboptimal performance in tasks
that require multiple plausible visual interpretations and inferences. To
address this, we present GThinker (General Thinker), a novel reasoning MLLM
excelling in multimodal reasoning across general scenarios, mathematics, and
science. GThinker introduces Cue-Rethinking, a flexible reasoning pattern that
grounds inferences in visual cues and iteratively reinterprets these cues to
resolve inconsistencies. Building on this pattern, we further propose a
two-stage training pipeline, including pattern-guided cold start and incentive
reinforcement learning, designed to enable multimodal reasoning capabilities
across domains. Furthermore, to support the training, we construct
GThinker-11K, comprising 7K high-quality, iteratively-annotated reasoning paths
and 4K curated reinforcement learning samples, filling the data gap toward
general multimodal reasoning. Extensive experiments demonstrate that GThinker
achieves 81.5% on the challenging comprehensive multimodal reasoning benchmark
M$^3$CoT, surpassing the latest O4-mini model. It also shows an average
improvement of 2.1% on general scenario multimodal reasoning benchmarks, while
maintaining on-par performance in mathematical reasoning compared to
counterpart advanced reasoning models. The code, model, and data will be
released soon at https://github.com/jefferyZhan/GThinker.

</details>


### [84] [Learning What Matters: Prioritized Concept Learning via Relative Error-driven Sample Selection](https://arxiv.org/abs/2506.01085)
*Shivam Chandhok,Qian Yang,Oscar Manas,Kanishk Jain,Leonid Sigal,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: PROGRESS是一种动态选择学习样本的高效框架，通过跟踪学习进度并优先选择最有价值的数据，显著减少了训练所需的数据和计算资源。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型（VLMs）在指令调优过程中需要大规模数据、高质量标注和高计算成本的问题。

Method: 提出PROGRESS框架，动态选择未掌握且难度适中的样本进行学习，优先选择学习进度最快的技能。

Result: 在多个指令调优数据集上表现优于现有方法，且数据量和监督需求大幅减少。

Conclusion: PROGRESS是一种高效、可扩展的学习解决方案，适用于不同规模的模型。

Abstract: Instruction tuning has been central to the success of recent vision-language
models (VLMs), but it remains expensive-requiring large-scale datasets,
high-quality annotations, and large compute budgets. We propose PRioritized
cOncept learninG via Relative Error-driven Sample Selection (PROGRESS), a data-
and compute-efficient framework that enables VLMs to dynamically select what to
learn next based on their evolving needs during training. At each stage, the
model tracks its learning progress across skills and selects the most
informative samples-those it has not already mastered and that are not too
difficult to learn at the current stage of training. This strategy effectively
controls skill acquisition and the order in which skills are learned.
Specifically, we sample from skills showing the highest learning progress,
prioritizing those with the most rapid improvement. Unlike prior methods,
PROGRESS requires no upfront answer annotations, queries answers only on a need
basis, avoids reliance on additional supervision from auxiliary VLMs, and does
not require compute-heavy gradient computations for data selection. Experiments
across multiple instruction-tuning datasets of varying scales demonstrate that
PROGRESS consistently outperforms state-of-the-art baselines with much less
data and supervision. Additionally, we show strong cross-architecture
generalization and transferability to larger models, validating PROGRESS as a
scalable solution for efficient learning.

</details>


### [85] [Generic Token Compression in Multimodal Large Language Models from an Explainability Perspective](https://arxiv.org/abs/2506.01097)
*Lei Lei,Jie Gu,Xiaokang Ma,Chu Tang,Jingmin Chen,Tong Xu*

Main category: cs.CV

TL;DR: 研究发现，通过适当选择，可以在LLM输入阶段压缩视觉令牌，性能损失可忽略。提出了一种基于解释性方法的令牌重要性评估和轻量级卷积网络映射方法，实验证明其高效性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs处理大量视觉令牌导致计算成本高且效率低下，研究旨在探索在LLM输入阶段压缩令牌的可行性。

Method: 利用解释性方法评估视觉令牌重要性，并通过轻量级卷积网络学习从第一层LLM注意力映射到解释结果的映射，避免完整推理。

Result: 在10个图像和视频基准测试中，压缩50%视觉令牌仍保留96%以上性能，且表现出强泛化能力。

Conclusion: 研究表明，输入阶段令牌压缩可行且高效，为MLLMs的实际部署提供了新思路。

Abstract: Existing Multimodal Large Language Models (MLLMs) process a large number of
visual tokens, leading to significant computational costs and inefficiency.
Previous works generally assume that all visual tokens are necessary in the
shallow layers of LLMs, and therefore token compression typically occurs in
intermediate layers. In contrast, our study reveals an interesting insight:
with proper selection, token compression is feasible at the input stage of LLM
with negligible performance loss. Specifically, we reveal that explainability
methods can effectively evaluate the importance of each visual token with
respect to the given instruction, which can well guide the token compression.
Furthermore, we propose to learn a mapping from the attention map of the first
LLM layer to the explanation results, thereby avoiding the need for a full
inference pass and facilitating practical deployment. Interestingly, this
mapping can be learned using a simple and lightweight convolutional network,
whose training is efficient and independent of MLLMs. Extensive experiments on
10 image and video benchmarks across three leading MLLMs (Qwen2-VL,
LLaVA-OneVision, and VILA1.5) demonstrate the effectiveness of our approach,
e.g., pruning 50% visual tokens while retaining more than 96% of the original
performance across all benchmarks for all these three MLLMs. It also exhibits
strong generalization, even when the number of tokens in inference far exceeds
that used in training.

</details>


### [86] [Keystep Recognition using Graph Neural Networks](https://arxiv.org/abs/2506.01102)
*Julia Lee Romero,Kyle Min,Subarna Tripathi,Morteza Karimzadeh*

Main category: cs.CV

TL;DR: GLEVR是一种基于图学习的框架，用于细粒度关键步骤识别，通过利用自我中心视频中的长期依赖关系，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决自我中心视频中细粒度关键步骤识别的挑战，利用图学习框架捕捉长期依赖关系。

Method: 将视频片段作为节点构建稀疏图，结合自我中心与外中心视频的对齐，并引入自动字幕作为额外模态。

Result: 在Ego-Exo4D数据集上，GLEVR显著优于现有方法。

Conclusion: GLEVR框架通过灵活的图学习有效提升了关键步骤识别的性能。

Abstract: We pose keystep recognition as a node classification task, and propose a
flexible graph-learning framework for fine-grained keystep recognition that is
able to effectively leverage long-term dependencies in egocentric videos. Our
approach, termed GLEVR, consists of constructing a graph where each video clip
of the egocentric video corresponds to a node. The constructed graphs are
sparse and computationally efficient, outperforming existing larger models
substantially. We further leverage alignment between egocentric and exocentric
videos during training for improved inference on egocentric videos, as well as
adding automatic captioning as an additional modality. We consider each clip of
each exocentric video (if available) or video captions as additional nodes
during training. We examine several strategies to define connections across
these nodes. We perform extensive experiments on the Ego-Exo4D dataset and show
that our proposed flexible graph-based framework notably outperforms existing
methods.

</details>


### [87] [DeepVerse: 4D Autoregressive Video Generation as a World Model](https://arxiv.org/abs/2506.01103)
*Junyi Chen,Haoyi Zhu,Xianglong He,Yifan Wang,Jianjun Zhou,Wenzheng Chang,Yang Zhou,Zizun Li,Zhoujie Fu,Jiangmiao Pang,Tong He*

Main category: cs.CV

TL;DR: DeepVerse是一种新型4D交互世界模型，通过显式结合几何预测和动作条件，显著提升了时空一致性和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有交互模型主要预测视觉观察，忽略了隐藏状态（如几何结构和空间一致性），导致误差累积和时间不一致。

Method: 引入DeepVerse，将先前时间步的几何预测显式结合到当前动作条件下的预测中。

Result: 实验表明，DeepVerse显著减少了漂移，提升了时空一致性，并在预测准确性、视觉真实性和场景合理性上取得显著改进。

Conclusion: DeepVerse为几何感知的动态预测提供了高效解决方案，支持高保真度的长期预测。

Abstract: World models serve as essential building blocks toward Artificial General
Intelligence (AGI), enabling intelligent agents to predict future states and
plan actions by simulating complex physical interactions. However, existing
interactive models primarily predict visual observations, thereby neglecting
crucial hidden states like geometric structures and spatial coherence. This
leads to rapid error accumulation and temporal inconsistency. To address these
limitations, we introduce DeepVerse, a novel 4D interactive world model
explicitly incorporating geometric predictions from previous timesteps into
current predictions conditioned on actions. Experiments demonstrate that by
incorporating explicit geometric constraints, DeepVerse captures richer
spatio-temporal relationships and underlying physical dynamics. This capability
significantly reduces drift and enhances temporal consistency, enabling the
model to reliably generate extended future sequences and achieve substantial
improvements in prediction accuracy, visual realism, and scene rationality.
Furthermore, our method provides an effective solution for geometry-aware
memory retrieval, effectively preserving long-term spatial consistency. We
validate the effectiveness of DeepVerse across diverse scenarios, establishing
its capacity for high-fidelity, long-horizon predictions grounded in
geometry-aware dynamics.

</details>


### [88] [CountingFruit: Real-Time 3D Fruit Counting with Language-Guided Semantic Gaussian Splatting](https://arxiv.org/abs/2506.01109)
*Fengze Li,Yangle Liu,Jieming Ma,Hai-Ning Liang,Yaochun Shen,Huangxiang Li,Zhijing Wu*

Main category: cs.CV

TL;DR: FruitLangGS是一个实时3D水果计数框架，通过空间重建、语义嵌入和语言引导的实例估计解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决农业环境中水果计数的挑战，如视觉遮挡、语义模糊和3D重建的高计算需求。

Method: 使用自适应高斯喷洒管道重建果园场景，每个高斯点编码压缩的CLIP对齐语言嵌入，支持语义控制。通过分布感知采样和聚类估计水果数量。

Result: 实验表明，FruitLangGS在渲染速度、语义灵活性和计数准确性上优于现有方法。

Conclusion: FruitLangGS为开放世界场景中的语言驱动实时神经渲染提供了新视角。

Abstract: Accurate fruit counting in real-world agricultural environments is a
longstanding challenge due to visual occlusions, semantic ambiguity, and the
high computational demands of 3D reconstruction. Existing methods based on
neural radiance fields suffer from low inference speed, limited generalization,
and lack support for open-set semantic control. This paper presents
FruitLangGS, a real-time 3D fruit counting framework that addresses these
limitations through spatial reconstruction, semantic embedding, and
language-guided instance estimation. FruitLangGS first reconstructs
orchard-scale scenes using an adaptive Gaussian splatting pipeline with
radius-aware pruning and tile-based rasterization for efficient rendering. To
enable semantic control, each Gaussian encodes a compressed CLIP-aligned
language embedding, forming a compact and queryable 3D representation. At
inference time, prompt-based semantic filtering is applied directly in 3D
space, without relying on image-space segmentation or view-level fusion. The
selected Gaussians are then converted into dense point clouds via
distribution-aware sampling and clustered to estimate fruit counts.
Experimental results on real orchard data demonstrate that FruitLangGS achieves
higher rendering speed, semantic flexibility, and counting accuracy compared to
prior approaches, offering a new perspective for language-driven, real-time
neural rendering across open-world scenarios.

</details>


### [89] [Revolutionizing Radiology Workflow with Factual and Efficient CXR Report Generation](https://arxiv.org/abs/2506.01118)
*Pimchanok Sukjai,Apiradee Boonmee*

Main category: cs.CV

TL;DR: CXR-PathFinder是一种基于大型语言模型（LLM）的自动化胸片报告生成模型，通过临床医生引导的对抗微调（CGAFT）和知识图谱增强模块（KGAM）显著提高了报告的准确性和临床实用性。


<details>
  <summary>Details</summary>
Motivation: 医疗影像解读需求激增，需要高效、准确的人工智能解决方案来提升放射学诊断水平。

Method: 提出CGAFT训练范式，结合临床医生反馈和对抗学习框架；引入KGAM模块，动态验证生成报告与权威知识库的一致性。

Result: CXR-PathFinder在多项定量指标（如临床准确率）上显著优于现有模型，并通过放射科医生的盲测验证了其优越性。

Conclusion: 该方法在诊断准确性和计算效率之间取得了平衡，为自动化医疗报告生成提供了可靠解决方案。

Abstract: The escalating demand for medical image interpretation underscores the
critical need for advanced artificial intelligence solutions to enhance the
efficiency and accuracy of radiological diagnoses. This paper introduces
CXR-PathFinder, a novel Large Language Model (LLM)-centric foundation model
specifically engineered for automated chest X-ray (CXR) report generation. We
propose a unique training paradigm, Clinician-Guided Adversarial Fine-Tuning
(CGAFT), which meticulously integrates expert clinical feedback into an
adversarial learning framework to mitigate factual inconsistencies and improve
diagnostic precision. Complementing this, our Knowledge Graph Augmentation
Module (KGAM) acts as an inference-time safeguard, dynamically verifying
generated medical statements against authoritative knowledge bases to minimize
hallucinations and ensure standardized terminology. Leveraging a comprehensive
dataset of millions of paired CXR images and expert reports, our experiments
demonstrate that CXR-PathFinder significantly outperforms existing
state-of-the-art medical vision-language models across various quantitative
metrics, including clinical accuracy (Macro F1 (14): 46.5, Micro F1 (14):
59.5). Furthermore, blinded human evaluation by board-certified radiologists
confirms CXR-PathFinder's superior clinical utility, completeness, and
accuracy, establishing its potential as a reliable and efficient aid for
radiological practice. The developed method effectively balances high
diagnostic fidelity with computational efficiency, providing a robust solution
for automated medical report generation.

</details>


### [90] [MOOSE: Pay Attention to Temporal Dynamics for Video Understanding via Optical Flows](https://arxiv.org/abs/2506.01119)
*Hong Nguyen,Dung Tran,Hieu Hoang,Phong Nguyen,Shrikanth Narayanan*

Main category: cs.CV

TL;DR: MOOSE是一种新型视频编码器，通过结合光流与空间嵌入高效建模时间信息，减少计算复杂度并提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 视频分析中时间动态建模是核心挑战，现有方法需大量计算资源和细粒度标注。MOOSE旨在高效且可解释地建模时间信息。

Method: MOOSE整合预训练视觉和光流编码器，避免从头训练，提出时间中心架构。

Result: 在临床、医学和标准动作识别数据集上实现最优性能。

Conclusion: MOOSE高效、可解释且广泛适用，为视频分析任务提供新解决方案。

Abstract: Many motion-centric video analysis tasks, such as atomic actions, detecting
atypical motor behavior in individuals with autism, or analyzing articulatory
motion in real-time MRI of human speech, require efficient and interpretable
temporal modeling. Capturing temporal dynamics is a central challenge in video
analysis, often requiring significant computational resources and fine-grained
annotations that are not widely available. This paper presents MOOSE (Motion
Flow Over Spatial Space), a novel temporally-centric video encoder explicitly
integrating optical flow with spatial embeddings to model temporal information
efficiently, inspired by human perception of motion. Unlike prior models, MOOSE
takes advantage of rich, widely available pre-trained visual and optical flow
encoders instead of training video models from scratch. This significantly
reduces computational complexity while enhancing temporal interpretability. Our
primary contributions includes (1) proposing a computationally efficient
temporally-centric architecture for video understanding (2) demonstrating
enhanced interpretability in modeling temporal dynamics; and (3) achieving
state-of-the-art performance on diverse benchmarks, including clinical,
medical, and standard action recognition datasets, confirming the broad
applicability and effectiveness of our approach.

</details>


### [91] [ProstaTD: A Large-scale Multi-source Dataset for Structured Surgical Triplet Detection](https://arxiv.org/abs/2506.01130)
*Yiliang Chen,Zhixi Li,Cheng Xu,Alex Qinyang Liu,Xuemiao Xu,Jeremy Yuen-Chun Teoh,Shengfeng He,Jing Qin*

Main category: cs.CV

TL;DR: ProstaTD是一个大规模、多机构的手术三重检测数据集，解决了现有数据集在空间标注、时间标签和数据来源上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集（如CholecT50）缺乏精确的空间标注、时间标签不一致且临床依据不足，且数据来源单一，限制了模型的泛化能力。

Method: 通过机器人辅助前列腺切除术领域开发ProstaTD数据集，提供临床定义的时间边界和高精度空间标注，包含60,529帧视频和165,567个标注实例，来自21次手术。

Result: ProstaTD是目前最大且最多样化的手术三重检测数据集，为公平基准测试、可靠手术AI系统开发和可扩展培训工具提供了基础。

Conclusion: ProstaTD解决了现有数据集的不足，为手术视频分析和培训提供了更可靠的资源。

Abstract: Surgical triplet detection has emerged as a pivotal task in surgical video
analysis, with significant implications for performance assessment and the
training of novice surgeons. However, existing datasets such as CholecT50
exhibit critical limitations: they lack precise spatial bounding box
annotations, provide inconsistent and clinically ungrounded temporal labels,
and rely on a single data source, which limits model generalizability.To
address these shortcomings, we introduce ProstaTD, a large-scale,
multi-institutional dataset for surgical triplet detection, developed from the
technically demanding domain of robot-assisted prostatectomy. ProstaTD offers
clinically defined temporal boundaries and high-precision bounding box
annotations for each structured triplet action. The dataset comprises 60,529
video frames and 165,567 annotated triplet instances, collected from 21
surgeries performed across multiple institutions, reflecting a broad range of
surgical practices and intraoperative conditions. The annotation process was
conducted under rigorous medical supervision and involved more than 50
contributors, including practicing surgeons and medically trained annotators,
through multiple iterative phases of labeling and verification. ProstaTD is the
largest and most diverse surgical triplet dataset to date, providing a robust
foundation for fair benchmarking, the development of reliable surgical AI
systems, and scalable tools for procedural training.

</details>


### [92] [FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video Generation](https://arxiv.org/abs/2506.01144)
*Ariel Shaulov,Itay Hazan,Lior Wolf,Hila Chefer*

Main category: cs.CV

TL;DR: FlowMo是一种无需额外训练或辅助输入的训练自由引导方法，通过提取预训练模型的预测来增强视频扩散模型的时间一致性。


<details>
  <summary>Details</summary>
Motivation: 解决文本到视频扩散模型在建模时间动态方面的局限性，无需重新训练或引入外部条件信号。

Method: FlowMo通过测量连续帧潜在表示的距离，提取外观去偏的时间表示，并通过动态减少时间维度上的补丁方差来引导模型。

Result: 实验表明，FlowMo显著提高了运动一致性，同时保持了视觉质量和提示对齐。

Conclusion: FlowMo为预训练视频扩散模型提供了一种有效的即插即用解决方案，提升了时间保真度。

Abstract: Text-to-video diffusion models are notoriously limited in their ability to
model temporal aspects such as motion, physics, and dynamic interactions.
Existing approaches address this limitation by retraining the model or
introducing external conditioning signals to enforce temporal consistency. In
this work, we explore whether a meaningful temporal representation can be
extracted directly from the predictions of a pre-trained model without any
additional training or auxiliary inputs. We introduce \textbf{FlowMo}, a novel
training-free guidance method that enhances motion coherence using only the
model's own predictions in each diffusion step. FlowMo first derives an
appearance-debiased temporal representation by measuring the distance between
latents corresponding to consecutive frames. This highlights the implicit
temporal structure predicted by the model. It then estimates motion coherence
by measuring the patch-wise variance across the temporal dimension and guides
the model to reduce this variance dynamically during sampling. Extensive
experiments across multiple text-to-video models demonstrate that FlowMo
significantly improves motion coherence without sacrificing visual quality or
prompt alignment, offering an effective plug-and-play solution for enhancing
the temporal fidelity of pre-trained video diffusion models.

</details>


### [93] [SVarM: Linear Support Varifold Machines for Classification and Regression on Geometric Data](https://arxiv.org/abs/2506.01189)
*Emmanuel Hartman,Nicolas Charon*

Main category: cs.CV

TL;DR: 论文提出了一种名为SVarM的方法，利用形状的varifold表示及其与测试函数的对偶性，构建了一个在无限维varifold空间上操作的机器学习框架，用于形状数据的分类和回归。


<details>
  <summary>Details</summary>
Motivation: 几何深度学习领域快速发展，但由于形状空间的非欧几里得性质，对几何数据（如曲线、图或曲面）进行统计分析仍具挑战性。需要构建能够结合形状参数不变性的机器学习框架，以确保模型的泛化能力。

Method: 提出了SVarM方法，利用varifold表示形状作为测度，并通过与测试函数的对偶性构建框架。采用神经网络表示可训练的测试函数，开发了形状数据集的分类和回归模型。

Result: 该方法在多种形状图和表面数据集上表现出强大的性能和鲁棒性，结果与最先进方法相当，同时显著减少了可训练参数的数量。

Conclusion: SVarM为形状数据的统计分析提供了一个高效且通用的框架，具有较好的泛化能力和参数效率。

Abstract: Despite progress in the rapidly developing field of geometric deep learning,
performing statistical analysis on geometric data--where each observation is a
shape such as a curve, graph, or surface--remains challenging due to the
non-Euclidean nature of shape spaces, which are defined as equivalence classes
under invariance groups. Building machine learning frameworks that incorporate
such invariances, notably to shape parametrization, is often crucial to ensure
generalizability of the trained models to new observations. This work proposes
SVarM to exploit varifold representations of shapes as measures and their
duality with test functions $h:\mathbb{R}^n \times S^{n-1} \to \mathbb{R}$.
This method provides a general framework akin to linear support vector machines
but operating instead over the infinite-dimensional space of varifolds. We
develop classification and regression models on shape datasets by introducing a
neural network-based representation of the trainable test function $h$. This
approach demonstrates strong performance and robustness across various shape
graph and surface datasets, achieving results comparable to state-of-the-art
methods while significantly reducing the number of trainable parameters.

</details>


### [94] [Perceptual Inductive Bias Is What You Need Before Contrastive Learning](https://arxiv.org/abs/2506.01201)
*Tianqin Li,Junru Zhao,Dunhan Jiang,Shenghao Wu,Alan Ramirez,Tai Sing Lee*

Main category: cs.CV

TL;DR: 论文提出了一种基于Marr多阶段视觉理论的预训练方法，通过先构建边界和表面级表征，再学习语义表征，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有对比表征学习框架直接学习语义表征空间，忽略了视觉的归纳偏置，导致收敛速度慢和纹理偏差。

Method: 利用Marr的多阶段理论，先构建边界和表面级表征，再训练语义表征。

Result: 在ResNet18上实现2倍收敛速度提升，并在语义分割、深度估计和物体识别任务中表现更优。

Conclusion: 提出在对比表征预训练前增加预训练阶段，利用人类视觉系统的归纳偏置提升表征质量和收敛速度。

Abstract: David Marr's seminal theory of human perception stipulates that visual
processing is a multi-stage process, prioritizing the derivation of boundary
and surface properties before forming semantic object representations. In
contrast, contrastive representation learning frameworks typically bypass this
explicit multi-stage approach, defining their objective as the direct learning
of a semantic representation space for objects. While effective in general
contexts, this approach sacrifices the inductive biases of vision, leading to
slower convergence speed and learning shortcut resulting in texture bias. In
this work, we demonstrate that leveraging Marr's multi-stage theory-by first
constructing boundary and surface-level representations using perceptual
constructs from early visual processing stages and subsequently training for
object semantics-leads to 2x faster convergence on ResNet18, improved final
representations on semantic segmentation, depth estimation, and object
recognition, and enhanced robustness and out-of-distribution capability.
Together, we propose a pretraining stage before the general contrastive
representation pretraining to further enhance the final representation quality
and reduce the overall convergence time via inductive bias from human vision
systems.

</details>


### [95] [Self-Supervised Multi-View Representation Learning using Vision-Language Model for 3D/4D Facial Expression Recognition](https://arxiv.org/abs/2506.01203)
*Muzammil Behzad*

Main category: cs.CV

TL;DR: SMILE-VLM是一种自监督视觉语言模型，用于3D/4D面部表情识别，通过多视图视觉表示学习和自然语言监督实现高性能。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别在情感计算中有广泛应用，但现有方法需要大量标注数据，SMILE-VLM旨在提供一种高效且可扩展的解决方案。

Method: 提出三个核心组件：多视图去相关、视觉语言对比对齐和跨模态冗余最小化。

Result: 在多个基准测试中达到最先进性能，并在4D微表情识别任务中表现优异。

Conclusion: SMILE-VLM不仅优于无监督方法，还能匹配或超越有监督基线，为面部行为理解提供了高效解决方案。

Abstract: Facial expression recognition (FER) is a fundamental task in affective
computing with applications in human-computer interaction, mental health
analysis, and behavioral understanding. In this paper, we propose SMILE-VLM, a
self-supervised vision-language model for 3D/4D FER that unifies multiview
visual representation learning with natural language supervision. SMILE-VLM
learns robust, semantically aligned, and view-invariant embeddings by proposing
three core components: multiview decorrelation via a Barlow Twins-style loss,
vision-language contrastive alignment, and cross-modal redundancy minimization.
Our framework achieves the state-of-the-art performance on multiple benchmarks.
We further extend SMILE-VLM to the task of 4D micro-expression recognition
(MER) to recognize the subtle affective cues. The extensive results demonstrate
that SMILE-VLM not only surpasses existing unsupervised methods but also
matches or exceeds supervised baselines, offering a scalable and
annotation-efficient solution for expressive facial behavior understanding.

</details>


### [96] [A Review on Coarse to Fine-Grained Animal Action Recognition](https://arxiv.org/abs/2506.01214)
*Ali Zia,Renuka Sharma,Abdelwahed Khamis,Xuesong Li,Muhammad Husnain,Numan Shafi,Saeed Anwar,Sabine Schmoelzl,Eric Stone,Lars Petersson,Vivien Rolland*

Main category: cs.CV

TL;DR: 本文综述了动物行为识别领域的现状，重点探讨了粗粒度（CG）和细粒度（FG）技术，并分析了其在户外环境中的独特挑战。


<details>
  <summary>Details</summary>
Motivation: 研究动物行为识别的动机在于解决非刚性身体结构、频繁遮挡和缺乏大规模标注数据集等挑战，这些问题与人行为识别显著不同。

Method: 通过评估时空深度学习框架（如SlowFast）和现有数据集的局限性，探讨了动物行为识别的技术方法。

Result: 研究发现动物行为识别面临高物种内变异性和自然栖息地复杂性等独特挑战，现有方法在准确性和泛化性上仍有不足。

Conclusion: 未来研究方向包括改进细粒度行为识别技术，以提高跨物种行为分析的准确性和泛化性。

Abstract: This review provides an in-depth exploration of the field of animal action
recognition, focusing on coarse-grained (CG) and fine-grained (FG) techniques.
The primary aim is to examine the current state of research in animal behaviour
recognition and to elucidate the unique challenges associated with recognising
subtle animal actions in outdoor environments. These challenges differ
significantly from those encountered in human action recognition due to factors
such as non-rigid body structures, frequent occlusions, and the lack of
large-scale, annotated datasets. The review begins by discussing the evolution
of human action recognition, a more established field, highlighting how it
progressed from broad, coarse actions in controlled settings to the demand for
fine-grained recognition in dynamic environments. This shift is particularly
relevant for animal action recognition, where behavioural variability and
environmental complexity present unique challenges that human-centric models
cannot fully address. The review then underscores the critical differences
between human and animal action recognition, with an emphasis on high
intra-species variability, unstructured datasets, and the natural complexity of
animal habitats. Techniques like spatio-temporal deep learning frameworks
(e.g., SlowFast) are evaluated for their effectiveness in animal behaviour
analysis, along with the limitations of existing datasets. By assessing the
strengths and weaknesses of current methodologies and introducing a
recently-published dataset, the review outlines future directions for advancing
fine-grained action recognition, aiming to improve accuracy and
generalisability in behaviour analysis across species.

</details>


### [97] [Dirty and Clean-Label attack detection using GAN discriminators](https://arxiv.org/abs/2506.01224)
*John Smutny*

Main category: cs.CV

TL;DR: 使用GAN判别器检测和防止图像标签污染，保护计算机视觉模型的特定类别。


<details>
  <summary>Details</summary>
Motivation: 收集足够且干净的图像训练数据困难，手动检查不切实际，现有方法耗时。

Method: 利用GAN判别器训练单类别，通过置信度评分阈值检测污染图像。

Result: 在扰动强度0.20以上，能100%识别污染图像。

Conclusion: 开发者可基于此方法训练判别器，保护高价值类别。

Abstract: Gathering enough images to train a deep computer vision model is a constant
challenge. Unfortunately, collecting images from unknown sources can leave your
model s behavior at risk of being manipulated by a dirty-label or clean-label
attack unless the images are properly inspected. Manually inspecting each
image-label pair is impractical and common poison-detection methods that
involve re-training your model can be time consuming. This research uses GAN
discriminators to protect a single class against mislabeled and different
levels of modified images. The effect of said perturbation on a basic
convolutional neural network classifier is also included for reference. The
results suggest that after training on a single class, GAN discriminator s
confidence scores can provide a threshold to identify mislabeled images and
identify 100% of the tested poison starting at a perturbation epsilon magnitude
of 0.20, after decision threshold calibration using in-class samples.
Developers can use this report as a basis to train their own discriminators to
protect high valued classes in their CV models.

</details>


### [98] [Fourier-Modulated Implicit Neural Representation for Multispectral Satellite Image Compression](https://arxiv.org/abs/2506.01234)
*Woojin Cho,Steve Andreas Immanuel,Junhyuk Heo,Darongsae Kwon*

Main category: cs.CV

TL;DR: ImpliSat是一个统一框架，通过高效的压缩和重建多光谱卫星数据，解决其高维性和大容量问题。


<details>
  <summary>Details</summary>
Motivation: 多光谱卫星图像在农业、渔业和环境监测中至关重要，但其高维性、大数据量和多通道空间分辨率差异带来了压缩和分析的挑战。

Method: ImpliSat利用隐式神经表示（INR）将卫星图像建模为坐标空间上的连续函数，并结合傅里叶调制算法动态适应各波段的光谱和空间特性。

Result: 该方法能够捕捉不同空间分辨率的精细细节，同时实现最优压缩并保留关键图像信息。

Conclusion: ImpliSat为多光谱卫星数据的压缩和分析提供了一种高效且统一的解决方案。

Abstract: Multispectral satellite images play a vital role in agriculture, fisheries,
and environmental monitoring. However, their high dimensionality, large data
volumes, and diverse spatial resolutions across multiple channels pose
significant challenges for data compression and analysis. This paper presents
ImpliSat, a unified framework specifically designed to address these challenges
through efficient compression and reconstruction of multispectral satellite
data. ImpliSat leverages Implicit Neural Representations (INR) to model
satellite images as continuous functions over coordinate space, capturing fine
spatial details across varying spatial resolutions. Furthermore, we introduce a
Fourier modulation algorithm that dynamically adjusts to the spectral and
spatial characteristics of each band, ensuring optimal compression while
preserving critical image details.

</details>


### [99] [Visual Sparse Steering: Improving Zero-shot Image Classification with Sparsity Guided Steering Vectors](https://arxiv.org/abs/2506.01247)
*Gerasimos Chatzoudis,Zhuowei Li,Gemma E. Moran,Hao Wang,Dimitris N. Metaxas*

Main category: cs.CV

TL;DR: VS2是一种轻量级的测试时方法，通过稀疏自编码器学习稀疏特征，无需对比数据即可引导视觉模型，显著提升性能。VS2++是其检索增强版本，进一步放大相关特征。PASS通过原型对齐损失优化稀疏特征，性能更优。


<details>
  <summary>Details</summary>
Motivation: 在动态或资源受限的环境中，无需重新训练或大量标记数据即可引导视觉基础模型是一个重要但具有挑战性的目标。

Method: 提出VS2和VS2++方法，利用稀疏自编码器学习稀疏特征进行模型引导；PASS通过原型对齐损失优化稀疏特征。

Result: VS2在多个数据集上超越零样本CLIP，VS2++进一步显著提升性能；PASS在CIFAR-100上比VS2提升6.12%。

Conclusion: 稀疏引导方法有效提升模型性能，尤其在特定类别上表现突出，原型对齐进一步优化稀疏特征学习。

Abstract: Steering vision foundation models at inference time without retraining or
access to large labeled datasets is a desirable yet challenging objective,
particularly in dynamic or resource-constrained settings. In this paper, we
introduce Visual Sparse Steering (VS2), a lightweight, test-time method that
guides vision models using steering vectors derived from sparse features
learned by top-$k$ Sparse Autoencoders without requiring contrastive data.
Specifically, VS2 surpasses zero-shot CLIP by 4.12% on CIFAR-100, 1.08% on
CUB-200, and 1.84% on Tiny-ImageNet. We further propose VS2++, a
retrieval-augmented variant that selectively amplifies relevant sparse features
using pseudo-labeled neighbors at inference time. With oracle positive/negative
sets, VS2++ achieves absolute top-1 gains over CLIP zero-shot of up to 21.44%
on CIFAR-100, 7.08% on CUB-200, and 20.47% on Tiny-ImageNet. Interestingly, VS2
and VS2++ raise per-class accuracy by up to 25% and 38%, respectively, showing
that sparse steering benefits specific classes by disambiguating visually or
taxonomically proximate categories rather than providing a uniform boost.
Finally, to better align the sparse features learned through the SAE
reconstruction task with those relevant for downstream performance, we propose
Prototype-Aligned Sparse Steering (PASS). By incorporating a
prototype-alignment loss during SAE training, using labels only during training
while remaining fully test-time unsupervised, PASS consistently, though
modestly, outperforms VS2, achieving a 6.12% gain over VS2 only on CIFAR-100
with ViT-B/32.

</details>


### [100] [ReFoCUS: Reinforcement-guided Frame Optimization for Contextual Understanding](https://arxiv.org/abs/2506.01274)
*Hosu Lee,Junho Kim,Hyunjun Kim,Yong Man Ro*

Main category: cs.CV

TL;DR: ReFoCUS是一种通过强化学习优化视频帧选择的新框架，旨在提升视频问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频多模态模型在帧选择上依赖静态启发式或外部检索，可能无法提供查询相关信息。

Method: 采用强化学习框架，通过参考LMM的奖励信号学习帧选择策略，并使用自回归条件选择架构降低复杂度。

Result: 在多个视频问答基准测试中显著提升了推理性能。

Conclusion: 通过将帧选择与模型内部效用对齐，ReFoCUS展示了无需显式监督即可优化视频理解的潜力。

Abstract: Recent progress in Large Multi-modal Models (LMMs) has enabled effective
vision-language reasoning, yet the ability to understand video content remains
constrained by suboptimal frame selection strategies. Existing approaches often
rely on static heuristics or external retrieval modules to feed frame
information into video-LLMs, which may fail to provide the query-relevant
information. In this work, we introduce ReFoCUS (Reinforcement-guided Frame
Optimization for Contextual UnderStanding), a novel frame-level policy
optimization framework that shifts the optimization target from textual
responses to visual input selection. ReFoCUS learns a frame selection policy
via reinforcement learning, using reward signals derived from a reference LMM
to reflect the model's intrinsic preferences for frames that best support
temporally grounded responses. To efficiently explore the large combinatorial
frame space, we employ an autoregressive, conditional selection architecture
that ensures temporal coherence while reducing complexity. Our approach does
not require explicit supervision at the frame-level and consistently improves
reasoning performance across multiple video QA benchmarks, highlighting the
benefits of aligning frame selection with model-internal utility.

</details>


### [101] [Abstractive Visual Understanding of Multi-modal Structured Knowledge: A New Perspective for MLLM Evaluation](https://arxiv.org/abs/2506.01293)
*Yichi Zhang,Zhuo Chen,Lingbing Guo,Yajing Xu,Min Zhang,Wen Zhang,Huajun Chen*

Main category: cs.CV

TL;DR: 论文提出了M3STR基准，用于评估多模态大语言模型（MLLMs）在结构化视觉知识理解上的能力，填补了现有评测的空白。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs评测基准忽视了模型对结构化视觉知识的理解能力，亟需新的评测方法。

Method: 提出M3STR基准，基于多模态知识图谱生成包含多模态实体及其关系的图像，要求MLLMs识别实体并解析关系拓扑。

Result: 对26个前沿MLLMs的测试显示其在处理结构化视觉知识时仍存在显著不足。

Conclusion: M3STR为提升MLLMs的整体推理能力指明了方向，相关代码和数据已开源。

Abstract: Multi-modal large language models (MLLMs) incorporate heterogeneous
modalities into LLMs, enabling a comprehensive understanding of diverse
scenarios and objects. Despite the proliferation of evaluation benchmarks and
leaderboards for MLLMs, they predominantly overlook the critical capacity of
MLLMs to comprehend world knowledge with structured abstractions that appear in
visual form. To address this gap, we propose a novel evaluation paradigm and
devise M3STR, an innovative benchmark grounded in the Multi-Modal Map for
STRuctured understanding. This benchmark leverages multi-modal knowledge graphs
to synthesize images encapsulating subgraph architectures enriched with
multi-modal entities. M3STR necessitates that MLLMs not only recognize the
multi-modal entities within the visual inputs but also decipher intricate
relational topologies among them. We delineate the benchmark's statistical
profiles and automated construction pipeline, accompanied by an extensive
empirical analysis of 26 state-of-the-art MLLMs. Our findings reveal persistent
deficiencies in processing abstractive visual information with structured
knowledge, thereby charting a pivotal trajectory for advancing MLLMs' holistic
reasoning capacities. Our code and data are released at
https://github.com/zjukg/M3STR

</details>


### [102] [ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding](https://arxiv.org/abs/2506.01300)
*Yiyang Zhou,Yangfan He,Yaofeng Su,Siwei Han,Joel Jang,Gedas Bertasius,Mohit Bansal,Huaxiu Yao*

Main category: cs.CV

TL;DR: ReAgent-V是一种新型视频理解框架，通过实时奖励生成和多视角反馈机制提升推理能力，并在多个任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统视频理解方法缺乏动态反馈，无法自我修正和适应复杂场景，现有方法存在高标注成本、低推理效率等问题。

Method: 提出ReAgent-V框架，结合高效帧选择和实时奖励生成，通过多视角反馈机制迭代优化答案，并支持灵活工具集成。

Result: 在12个数据集上的实验显示，视频理解、推理增强和视觉-语言-动作对齐任务分别提升6.9%、2.1%和9.8%。

Conclusion: ReAgent-V轻量、模块化且可扩展，显著提升了视频理解的泛化能力和推理效果。

Abstract: Video understanding is fundamental to tasks such as action recognition, video
reasoning, and robotic control. Early video understanding methods based on
large vision-language models (LVLMs) typically adopt a single-pass reasoning
paradigm without dynamic feedback, limiting the model's capacity to
self-correct and adapt in complex scenarios. Recent efforts have attempted to
address this limitation by incorporating reward models and reinforcement
learning to enhance reasoning, or by employing tool-agent frameworks. However,
these approaches face several challenges, including high annotation costs,
reward signals that fail to capture real-time reasoning states, and low
inference efficiency. To overcome these issues, we propose ReAgent-V, a novel
agentic video understanding framework that integrates efficient frame selection
with real-time reward generation during inference. These reward signals not
only guide iterative answer refinement through a multi-perspective reflection
mechanism-adjusting predictions from conservative, neutral, and aggressive
viewpoints-but also enable automatic filtering of high-quality data for
supervised fine-tuning (SFT), direct preference optimization (DPO), and group
relative policy optimization (GRPO). ReAgent-V is lightweight, modular, and
extensible, supporting flexible tool integration tailored to diverse tasks.
Extensive experiments on 12 datasets across three core applications-video
understanding, video reasoning enhancement, and vision-language-action model
alignment-demonstrate significant gains in generalization and reasoning, with
improvements of up to 6.9%, 2.1%, and 9.8%, respectively, highlighting the
effectiveness and versatility of the proposed framework.

</details>


### [103] [SAM-I2V: Upgrading SAM to Support Promptable Video Segmentation with Less than 0.2% Training Cost](https://arxiv.org/abs/2506.01304)
*Haiyang Mei,Pengyu Zhang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: SAM-I2V通过升级预训练的SAM模型，以更低的训练成本实现视频分割，性能接近SAM 2的90%。


<details>
  <summary>Details</summary>
Motivation: 解决视频分割中精确且时间一致的掩码传播问题，同时降低训练成本和资源需求。

Method: 引入三个创新：图像到视频特征提取升级器、记忆过滤策略和记忆作为提示机制。

Result: 实验表明，方法性能达到SAM 2的90%以上，训练成本仅为0.2%。

Conclusion: SAM-I2V为视频分割提供了资源高效的解决方案，降低了研究门槛并推动领域发展。

Abstract: Foundation models like the Segment Anything Model (SAM) have significantly
advanced promptable image segmentation in computer vision. However, extending
these capabilities to videos presents substantial challenges, particularly in
ensuring precise and temporally consistent mask propagation in dynamic scenes.
SAM 2 attempts to address this by training a model on massive image and video
data from scratch to learn complex spatiotemporal associations, resulting in
huge training costs that hinder research and practical deployment. In this
paper, we introduce SAM-I2V, an effective image-to-video upgradation method for
cultivating a promptable video segmentation (PVS) model. Our approach
strategically upgrades the pre-trained SAM to support PVS, significantly
reducing training complexity and resource requirements. To achieve this, we
introduce three key innovations: (i) an image-to-video feature extraction
upgrader built upon SAM's static image encoder to enable spatiotemporal video
perception, (ii) a memory filtering strategy that selects the most relevant
past frames for more effective utilization of historical information, and (iii)
a memory-as-prompt mechanism leveraging object memory to ensure temporally
consistent mask propagation in dynamic scenes. Comprehensive experiments
demonstrate that our method achieves over 90% of SAM 2's performance while
using only 0.2% of its training cost. Our work presents a resource-efficient
pathway to PVS, lowering barriers for further research in PVS model design and
enabling broader applications and advancements in the field. Code and model are
available at: https://github.com/showlab/SAM-I2V.

</details>


### [104] [Ultra-High-Resolution Image Synthesis: Data, Method and Evaluation](https://arxiv.org/abs/2506.01331)
*Jinjin Zhang,Qiuyu Huang,Junjie Liu,Xiefan Guo,Di Huang*

Main category: cs.CV

TL;DR: 论文提出了Aesthetic-4K数据集和Diffusion-4K框架，用于超高清图像合成，并引入新评估指标。


<details>
  <summary>Details</summary>
Motivation: 超高清图像合成潜力巨大但缺乏标准化基准和计算资源，需解决这些问题。

Method: 提出SC-VAE和WLF技术，结合扩散模型直接生成4K图像，并设计新评估指标。

Result: Diffusion-4K在超高清图像合成中表现优异，尤其结合大型扩散模型。

Conclusion: Diffusion-4K为超高清图像合成提供了高效解决方案，并公开了源代码。

Abstract: Ultra-high-resolution image synthesis holds significant potential, yet
remains an underexplored challenge due to the absence of standardized
benchmarks and computational constraints. In this paper, we establish
Aesthetic-4K, a meticulously curated dataset containing dedicated training and
evaluation subsets specifically designed for comprehensive research on
ultra-high-resolution image synthesis. This dataset consists of high-quality 4K
images accompanied by descriptive captions generated by GPT-4o. Furthermore, we
propose Diffusion-4K, an innovative framework for the direct generation of
ultra-high-resolution images. Our approach incorporates the Scale Consistent
Variational Auto-Encoder (SC-VAE) and Wavelet-based Latent Fine-tuning (WLF),
which are designed for efficient visual token compression and the capture of
intricate details in ultra-high-resolution images, thereby facilitating direct
training with photorealistic 4K data. This method is applicable to various
latent diffusion models and demonstrates its efficacy in synthesizing highly
detailed 4K images. Additionally, we propose novel metrics, namely the GLCM
Score and Compression Ratio, to assess the texture richness and fine details in
local patches, in conjunction with holistic measures such as FID, Aesthetics,
and CLIPScore, enabling a thorough and multifaceted evaluation of
ultra-high-resolution image synthesis. Consequently, Diffusion-4K achieves
impressive performance in ultra-high-resolution image synthesis, particularly
when powered by state-of-the-art large-scale diffusion models (eg, Flux-12B).
The source code is publicly available at
https://github.com/zhang0jhon/diffusion-4k.

</details>


### [105] [A 2-Stage Model for Vehicle Class and Orientation Detection with Photo-Realistic Image Generation](https://arxiv.org/abs/2506.01338)
*Youngmin Kim,Donghwa Kang,Hyeongboo Baek*

Main category: cs.CV

TL;DR: 提出了一种两阶段检测模型，通过生成逼真图像解决合成数据训练中的类别不平衡和真实图像预测困难问题。


<details>
  <summary>Details</summary>
Motivation: 训练数据中类别分布不平衡，且合成图像训练的模型难以预测真实图像中的车辆类别和方向。

Method: 1. 构建包含图像、类别和位置信息的表格；2. 将合成图像转换为真实风格并合并到元表格；3. 使用元表格中的图像分类车辆类别和方向；4. 结合位置信息和预测类别完成检测。

Result: 在IEEE BigData Challenge 2022 VOD中排名第4。

Conclusion: 提出的方法通过两阶段模型和图像风格转换，有效解决了合成数据训练的局限性。

Abstract: We aim to detect the class and orientation of a vehicle by training a model
with synthetic data. However, the distribution of the classes in the training
data is imbalanced, and the model trained on the synthetic image is difficult
to predict in real-world images. We propose a two-stage detection model with
photo-realistic image generation to tackle this issue. Our model mainly takes
four steps to detect the class and orientation of the vehicle. (1) It builds a
table containing the image, class, and location information of objects in the
image, (2) transforms the synthetic images into real-world images style, and
merges them into the meta table. (3) Classify vehicle class and orientation
using images from the meta-table. (4) Finally, the vehicle class and
orientation are detected by combining the pre-extracted location information
and the predicted classes. We achieved 4th place in IEEE BigData Challenge 2022
Vehicle class and Orientation Detection (VOD) with our approach.

</details>


### [106] [Rethinking Image Histogram Matching for Image Classification](https://arxiv.org/abs/2506.01346)
*Rikuto Otsuka,Yuho Shoji,Yuka Ogino,Takahiro Toizumi,Atsushi Ito*

Main category: cs.CV

TL;DR: 本文重新思考了图像直方图匹配（HM），并提出了一种可微分且参数化的HM预处理方法，用于下游分类器。通过优化目标像素值分布，该方法在恶劣天气条件下提升了分类器性能。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络在分类任务中表现优异，但在低对比度图像（如恶劣天气条件下拍摄的图像）中性能下降。传统直方图均衡化（HE）虽常用，但其目标分布为均匀分布，可能并非最优。本文假设单一且精心设计的目标分布可能更有效。

Method: 提出了一种可微分且参数化的HM方法，通过下游分类器的损失函数优化目标分布，将输入图像的任意分布转换为分类器优化的目标分布。该方法仅使用正常天气图像进行训练。

Result: 实验结果表明，使用所提出的HM方法训练的分类器在恶劣天气条件下优于传统预处理方法。

Conclusion: 通过优化目标像素值分布，可微分且参数化的HM方法显著提升了分类器在恶劣天气条件下的性能。

Abstract: This paper rethinks image histogram matching (HM) and proposes a
differentiable and parametric HM preprocessing for a downstream classifier.
Convolutional neural networks have demonstrated remarkable achievements in
classification tasks. However, they often exhibit degraded performance on
low-contrast images captured under adverse weather conditions. To maintain
classifier performance under low-contrast images, histogram equalization (HE)
is commonly used. HE is a special case of HM using a uniform distribution as a
target pixel value distribution. In this paper, we focus on the shape of the
target pixel value distribution. Compared to a uniform distribution, a single,
well-designed distribution could have potential to improve the performance of
the downstream classifier across various adverse weather conditions. Based on
this hypothesis, we propose a differentiable and parametric HM that optimizes
the target distribution using the loss function of the downstream classifier.
This method addresses pixel value imbalances by transforming input images with
arbitrary distributions into a target distribution optimized for the
classifier. Our HM is trained on only normal weather images using the
classifier. Experimental results show that a classifier trained with our
proposed HM outperforms conventional preprocessing methods under adverse
weather conditions.

</details>


### [107] [Target Driven Adaptive Loss For Infrared Small Target Detection](https://arxiv.org/abs/2506.01349)
*Yuho Shoji,Takahiro Toizumi,Atsushi Ito*

Main category: cs.CV

TL;DR: 提出了一种目标驱动自适应（TDA）损失函数，用于提升红外小目标检测（IRSTD）性能。通过局部区域和自适应调整策略，解决了现有损失函数在局部目标检测和小尺度低对比度目标上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有损失函数（如二元交叉熵损失和IoU损失）在红外小目标检测中存在局部区域检测性能不足和对小尺度低对比度目标鲁棒性差的问题。

Method: 提出TDA损失函数，引入基于块的机制和自适应调整策略，使模型更关注目标周围的局部区域，并特别处理小尺度和低对比度目标。

Result: 在三个IRSTD数据集上验证，TDA损失比现有损失函数表现更好。

Conclusion: TDA损失有效提升了红外小目标检测的性能，尤其在局部区域和小尺度低对比度目标上表现突出。

Abstract: We propose a target driven adaptive (TDA) loss to enhance the performance of
infrared small target detection (IRSTD). Prior works have used loss functions,
such as binary cross-entropy loss and IoU loss, to train segmentation models
for IRSTD. Minimizing these loss functions guides models to extract pixel-level
features or global image context. However, they have two issues: improving
detection performance for local regions around the targets and enhancing
robustness to small scale and low local contrast. To address these issues, the
proposed TDA loss introduces a patch-based mechanism, and an adaptive
adjustment strategy to scale and local contrast. The proposed TDA loss leads
the model to focus on local regions around the targets and pay particular
attention to targets with smaller scales and lower local contrast. We evaluate
the proposed method on three datasets for IRSTD. The results demonstrate that
the proposed TDA loss achieves better detection performance than existing
losses on these datasets.

</details>


### [108] [CLIP-driven rain perception: Adaptive deraining with pattern-aware network routing and mask-guided cross-attention](https://arxiv.org/abs/2506.01366)
*Cong Guan,Osamu Yoshie*

Main category: cs.CV

TL;DR: 提出了一种基于CLIP的雨感知网络（CLIP-RPN），通过视觉-语言匹配分数自动识别雨模式，并自适应路由到子网络处理不同雨模式，显著提升了模型处理多样降雨条件的能力。


<details>
  <summary>Details</summary>
Motivation: 现有去雨模型使用单一网络处理所有雨图像，但不同雨模式差异显著，单一网络难以处理多样化的雨滴和雨线。

Method: 利用CLIP的跨模态视觉-语言对齐能力识别雨模式，动态激活专用处理分支；引入掩模引导的跨注意力机制（MGCA）和多尺度预测雨掩模；提出动态损失调度机制（DLS）优化训练过程。

Result: 在多个数据集上达到最先进性能，尤其在复杂混合数据集上表现优异。

Conclusion: CLIP-RPN通过雨模式感知和自适应子网络路由，显著提升了去雨模型的性能，适用于多样化降雨条件。

Abstract: Existing deraining models process all rainy images within a single network.
However, different rain patterns have significant variations, which makes it
challenging for a single network to handle diverse types of raindrops and
streaks. To address this limitation, we propose a novel CLIP-driven rain
perception network (CLIP-RPN) that leverages CLIP to automatically perceive
rain patterns by computing visual-language matching scores and adaptively
routing to sub-networks to handle different rain patterns, such as varying
raindrop densities, streak orientations, and rainfall intensity. CLIP-RPN
establishes semantic-aware rain pattern recognition through CLIP's cross-modal
visual-language alignment capabilities, enabling automatic identification of
precipitation characteristics across different rain scenarios. This rain
pattern awareness drives an adaptive subnetwork routing mechanism where
specialized processing branches are dynamically activated based on the detected
rain type, significantly enhancing the model's capacity to handle diverse
rainfall conditions. Furthermore, within sub-networks of CLIP-RPN, we introduce
a mask-guided cross-attention mechanism (MGCA) that predicts precise rain masks
at multi-scale to facilitate contextual interactions between rainy regions and
clean background areas by cross-attention. We also introduces a dynamic loss
scheduling mechanism (DLS) to adaptively adjust the gradients for the
optimization process of CLIP-RPN. Compared with the commonly used $l_1$ or
$l_2$ loss, DLS is more compatible with the inherent dynamics of the network
training process, thus achieving enhanced outcomes. Our method achieves
state-of-the-art performance across multiple datasets, particularly excelling
in complex mixed datasets.

</details>


### [109] [Synthetic Data Augmentation using Pre-trained Diffusion Models for Long-tailed Food Image Classification](https://arxiv.org/abs/2506.01368)
*GaYeon Koh,Hyun-Jic Oh,Jeonghyun Noh,Won-Ki Jeong*

Main category: cs.CV

TL;DR: 提出了一种两阶段合成数据增强框架，利用预训练扩散模型解决长尾食物分类问题，通过正负提示条件生成数据，提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 真实食物图像分布不均，导致模型偏向多数类，少数类性能下降。合成数据增强是潜在解决方案，但现有方法存在局限性。

Method: 两阶段框架：首先生成参考集（正提示），再选择相似特征的类作为负提示，生成合成数据增强集，结合采样策略提升类内多样性和类间分离。

Result: 在两个长尾食物基准数据集上表现优异，Top-1准确率优于先前工作。

Conclusion: 提出的方法通过正负提示和采样策略，有效解决了长尾分类问题，提升了性能。

Abstract: Deep learning-based food image classification enables precise identification
of food categories, further facilitating accurate nutritional analysis.
However, real-world food images often show a skewed distribution, with some
food types being more prevalent than others. This class imbalance can be
problematic, causing models to favor the majority (head) classes with overall
performance degradation for the less common (tail) classes. Recently, synthetic
data augmentation using diffusion-based generative models has emerged as a
promising solution to address this issue. By generating high-quality synthetic
images, these models can help uniformize the data distribution, potentially
improving classification performance. However, existing approaches face
challenges: fine-tuning-based methods need a uniformly distributed dataset,
while pre-trained model-based approaches often overlook inter-class separation
in synthetic data. In this paper, we propose a two-stage synthetic data
augmentation framework, leveraging pre-trained diffusion models for long-tailed
food classification. We generate a reference set conditioned by a positive
prompt on the generation target and then select a class that shares similar
features with the generation target as a negative prompt. Subsequently, we
generate a synthetic augmentation set using positive and negative prompt
conditions by a combined sampling strategy that promotes intra-class diversity
and inter-class separation. We demonstrate the efficacy of the proposed method
on two long-tailed food benchmark datasets, achieving superior performance
compared to previous works in terms of top-1 accuracy.

</details>


### [110] [PointT2I: LLM-based text-to-image generation via keypoints](https://arxiv.org/abs/2506.01370)
*Taekyung Lee,Donggyu Lee,Myungjoo Kang*

Main category: cs.CV

TL;DR: PointT2I是一个利用大语言模型（LLM）生成与文本提示中人体姿势准确对应的图像框架，包含关键点生成、图像生成和反馈系统三个组件。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像（T2I）生成模型已取得显著进展，但在处理复杂概念（如人体姿势）时仍存在挑战。

Method: 通过LLM直接生成关键点，结合文本提示生成图像，并利用反馈系统评估语义一致性。

Result: 无需微调即可生成与文本提示中姿势准确对齐的图像。

Conclusion: PointT2I是首个利用LLM进行关键点引导图像生成的框架，仅依赖文本提示即可实现高精度姿势对齐。

Abstract: Text-to-image (T2I) generation model has made significant advancements,
resulting in high-quality images aligned with an input prompt. However, despite
T2I generation's ability to generate fine-grained images, it still faces
challenges in accurately generating images when the input prompt contains
complex concepts, especially human pose. In this paper, we propose PointT2I, a
framework that effectively generates images that accurately correspond to the
human pose described in the prompt by using a large language model (LLM).
PointT2I consists of three components: Keypoint generation, Image generation,
and Feedback system. The keypoint generation uses an LLM to directly generate
keypoints corresponding to a human pose, solely based on the input prompt,
without external references. Subsequently, the image generation produces images
based on both the text prompt and the generated keypoints to accurately reflect
the target pose. To refine the outputs of the preceding stages, we incorporate
an LLM-based feedback system that assesses the semantic consistency between the
generated contents and the given prompts. Our framework is the first approach
to leveraging LLM for keypoints-guided image generation without any
fine-tuning, producing accurate pose-aligned images based solely on textual
prompts.

</details>


### [111] [SVQA-R1: Reinforcing Spatial Reasoning in MLLMs via View-Consistent Reward Optimization](https://arxiv.org/abs/2506.01371)
*Peiyao Wang,Haibin Ling*

Main category: cs.CV

TL;DR: SVQA-R1框架通过基于规则的强化学习提升视觉语言模型的空间推理能力，显著提高了空间VQA任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在空间推理能力上表现不足，尤其是需要理解相对位置、距离和物体配置的空间VQA任务。

Method: 提出SVQA-R1框架，采用Spatial-GRPO策略，通过扰动物体间空间关系（如镜像翻转）构建视图一致的奖励，增强模型对空间的理解。

Result: SVQA-R1在多个空间推理基准测试中显著提升了准确性，并展示了可解释的推理路径。

Conclusion: SVQA-R1通过新颖的强化学习策略有效提升了空间VQA任务的性能，且无需监督微调数据。

Abstract: Spatial reasoning remains a critical yet underdeveloped capability in
existing vision-language models (VLMs), especially for Spatial Visual Question
Answering (Spatial VQA) tasks that require understanding relative positions,
distances, and object configurations. Inspired by the R1 paradigm introduced in
DeepSeek-R1, which enhances reasoning in language models through rule-based
reinforcement learning (RL), we propose SVQA-R1, the first framework to extend
R1-style training to spatial VQA. In particular, we introduce Spatial-GRPO, a
novel group-wise RL strategy that constructs view-consistent rewards by
perturbing spatial relations between objects, e.g., mirror flipping, thereby
encouraging the model to develop a consistent and grounded understanding of
space. Our model, SVQA-R1, not only achieves dramatically improved accuracy on
spatial VQA benchmarks but also exhibits interpretable reasoning paths even
without using supervised fine-tuning (SFT) data. Extensive experiments and
visualization demonstrate the effectiveness of SVQA-R1 across multiple spatial
reasoning benchmarks.

</details>


### [112] [No Train Yet Gain: Towards Generic Multi-Object Tracking in Sports and Beyond](https://arxiv.org/abs/2506.01373)
*Tomasz Stanczyk,Seongro Yoon,Francois Bremond*

Main category: cs.CV

TL;DR: McByte是一种无需训练的多目标跟踪框架，通过整合时间传播的分割掩码作为关联线索，提高了跟踪的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 体育分析中的多目标跟踪面临快速运动、遮挡和相机移动等挑战，传统方法需要大量调优或难以处理轨迹。

Method: McByte结合时间传播的分割掩码作为关联线索，无需训练，仅依赖预训练模型和常用目标检测器。

Result: 在SportsMOT、DanceTrack、SoccerNet-tracking 2022和MOT17数据集上表现优异，适用于体育和行人跟踪。

Conclusion: McByte展示了掩码传播的优势，提供了一种更适应性强且通用的多目标跟踪方法。

Abstract: Multi-object tracking (MOT) is essential for sports analytics, enabling
performance evaluation and tactical insights. However, tracking in sports is
challenging due to fast movements, occlusions, and camera shifts. Traditional
tracking-by-detection methods require extensive tuning, while
segmentation-based approaches struggle with track processing. We propose
McByte, a tracking-by-detection framework that integrates temporally propagated
segmentation mask as an association cue to improve robustness without per-video
tuning. Unlike many existing methods, McByte does not require training, relying
solely on pre-trained models and object detectors commonly used in the
community. Evaluated on SportsMOT, DanceTrack, SoccerNet-tracking 2022 and
MOT17, McByte demonstrates strong performance across sports and general
pedestrian tracking. Our results highlight the benefits of mask propagation for
a more adaptable and generalizable MOT approach. Code will be made available at
https://github.com/tstanczyk95/McByte.

</details>


### [113] [RadarSplat: Radar Gaussian Splatting for High-Fidelity Data Synthesis and 3D Reconstruction of Autonomous Driving Scenes](https://arxiv.org/abs/2506.01379)
*Pou-Chun Kung,Skanda Harisha,Ram Vasudevan,Aline Eid,Katherine A. Skinner*

Main category: cs.CV

TL;DR: RadarSplat结合高斯散射和雷达噪声建模，提升3D场景重建和雷达数据合成的真实性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 雷达在恶劣天气中表现优于光学传感器，但现有雷达重建方法在噪声场景中表现不佳，无法合成真实雷达数据。

Method: 提出RadarSplat，整合高斯散射与新型雷达噪声建模技术。

Result: 在雷达图像合成（PSNR +3.4，SSIM 2.6倍）和几何重建（RMSE -40%，精度1.5倍）上显著提升。

Conclusion: RadarSplat能有效生成高保真雷达数据并提升场景重建质量。

Abstract: High-Fidelity 3D scene reconstruction plays a crucial role in autonomous
driving by enabling novel data generation from existing datasets. This allows
simulating safety-critical scenarios and augmenting training datasets without
incurring further data collection costs. While recent advances in radiance
fields have demonstrated promising results in 3D reconstruction and sensor data
synthesis using cameras and LiDAR, their potential for radar remains largely
unexplored. Radar is crucial for autonomous driving due to its robustness in
adverse weather conditions like rain, fog, and snow, where optical sensors
often struggle. Although the state-of-the-art radar-based neural representation
shows promise for 3D driving scene reconstruction, it performs poorly in
scenarios with significant radar noise, including receiver saturation and
multipath reflection. Moreover, it is limited to synthesizing preprocessed,
noise-excluded radar images, failing to address realistic radar data synthesis.
To address these limitations, this paper proposes RadarSplat, which integrates
Gaussian Splatting with novel radar noise modeling to enable realistic radar
data synthesis and enhanced 3D reconstruction. Compared to the
state-of-the-art, RadarSplat achieves superior radar image synthesis (+3.4 PSNR
/ 2.6x SSIM) and improved geometric reconstruction (-40% RMSE / 1.5x Accuracy),
demonstrating its effectiveness in generating high-fidelity radar data and
scene reconstruction. A project page is available at
https://umautobots.github.io/radarsplat.

</details>


### [114] [Playing with Transformer at 30+ FPS via Next-Frame Diffusion](https://arxiv.org/abs/2506.01380)
*Xinle Cheng,Tianyu He,Jiayi Xu,Junliang Guo,Di He,Jiang Bian*

Main category: cs.CV

TL;DR: 本文提出了一种自回归扩散变换器（NFD），通过块级因果注意力和并行令牌生成实现高效推理，并结合一致性蒸馏和推测采样技术，首次在A100 GPU上实现超过30 FPS的自回归视频生成。


<details>
  <summary>Details</summary>
Motivation: 自回归视频模型在交互式视频内容和流媒体应用中具有优势，但实时生成仍面临计算成本和硬件效率的挑战。

Method: 提出NFD模型，结合块级因果注意力和并行令牌生成；引入一致性蒸馏和推测采样技术以优化推理效率。

Result: NFD在视觉质量和采样效率上优于基线模型，首次实现A100 GPU上超过30 FPS的自回归视频生成。

Conclusion: NFD通过技术创新解决了自回归视频模型的高效推理问题，为实时视频生成提供了可行方案。

Abstract: Autoregressive video models offer distinct advantages over bidirectional
diffusion models in creating interactive video content and supporting streaming
applications with arbitrary duration. In this work, we present Next-Frame
Diffusion (NFD), an autoregressive diffusion transformer that incorporates
block-wise causal attention, enabling iterative sampling and efficient
inference via parallel token generation within each frame. Nonetheless,
achieving real-time video generation remains a significant challenge for such
models, primarily due to the high computational cost associated with diffusion
sampling and the hardware inefficiencies inherent to autoregressive generation.
To address this, we introduce two innovations: (1) We extend consistency
distillation to the video domain and adapt it specifically for video models,
enabling efficient inference with few sampling steps; (2) To fully leverage
parallel computation, motivated by the observation that adjacent frames often
share the identical action input, we propose speculative sampling. In this
approach, the model generates next few frames using current action input, and
discard speculatively generated frames if the input action differs. Experiments
on a large-scale action-conditioned video generation benchmark demonstrate that
NFD beats autoregressive baselines in terms of both visual quality and sampling
efficiency. We, for the first time, achieves autoregressive video generation at
over 30 Frames Per Second (FPS) on an A100 GPU using a 310M model.

</details>


### [115] [VRD-IU: Lessons from Visually Rich Document Intelligence and Understanding](https://arxiv.org/abs/2506.01388)
*Yihao Ding,Soyeon Caren Han,Yan Li,Josiah Poon*

Main category: cs.CV

TL;DR: VRD-IU竞赛聚焦于从多格式表单中提取和定位关键信息，展示了多种先进方法，并为视觉丰富文档理解（VRDU）领域设定了新基准。


<details>
  <summary>Details</summary>
Motivation: 解决表单类文档因复杂布局、多利益相关者和高结构可变性带来的独特挑战。

Method: 竞赛分为两个赛道：Track A（基于实体的关键信息检索）和Track B（从原始文档图像中端到端定位关键信息），采用了分层分解、基于Transformer的检索、多模态特征融合和高级目标检测技术。

Result: 超过20个团队参与，展示了多种先进方法，并设定了VRDU领域的新基准。

Conclusion: 竞赛为文档智能领域提供了宝贵见解，推动了VRDU技术的发展。

Abstract: Visually Rich Document Understanding (VRDU) has emerged as a critical field
in document intelligence, enabling automated extraction of key information from
complex documents across domains such as medical, financial, and educational
applications. However, form-like documents pose unique challenges due to their
complex layouts, multi-stakeholder involvement, and high structural
variability. Addressing these issues, the VRD-IU Competition was introduced,
focusing on extracting and localizing key information from multi-format forms
within the Form-NLU dataset, which includes digital, printed, and handwritten
documents. This paper presents insights from the competition, which featured
two tracks: Track A, emphasizing entity-based key information retrieval, and
Track B, targeting end-to-end key information localization from raw document
images. With over 20 participating teams, the competition showcased various
state-of-the-art methodologies, including hierarchical decomposition,
transformer-based retrieval, multimodal feature fusion, and advanced object
detection techniques. The top-performing models set new benchmarks in VRDU,
providing valuable insights into document intelligence.

</details>


### [116] [Neural shape reconstruction from multiple views with static pattern projection](https://arxiv.org/abs/2506.01389)
*Ryo Furukawa,Kota Nishihara,Hiroshi Kawasaki*

Main category: cs.CV

TL;DR: 提出了一种基于主动立体视觉的3D形状测量方法，通过动态校准相机和投影仪的相对位姿，提高系统灵活性。


<details>
  <summary>Details</summary>
Motivation: 传统主动立体系统需要固定相机和投影仪，校准复杂，限制了使用便利性。

Method: 通过动态捕捉图像，利用神经符号距离场（NeuralSDF）和体积差分渲染技术自动校准相机和投影仪的相对位姿。

Result: 在合成和真实图像上进行了3D重建实验，验证了方法的有效性。

Conclusion: 该方法显著提升了主动立体系统的灵活性和实用性。

Abstract: Active-stereo-based 3D shape measurement is crucial for various purposes,
such as industrial inspection, reverse engineering, and medical systems, due to
its strong ability to accurately acquire the shape of textureless objects.
Active stereo systems typically consist of a camera and a pattern projector,
tightly fixed to each other, and precise calibration between a camera and a
projector is required, which in turn decreases the usability of the system. If
a camera and a projector can be freely moved during shape scanning process, it
will drastically increase the convenience of the usability of the system. To
realize it, we propose a technique to recover the shape of the target object by
capturing multiple images while both the camera and the projector are in
motion, and their relative poses are auto-calibrated by our neural
signed-distance-field (NeuralSDF) using novel volumetric differential rendering
technique. In the experiment, the proposed method is evaluated by performing 3D
reconstruction using both synthetic and real images.

</details>


### [117] [ViTA-PAR: Visual and Textual Attribute Alignment with Attribute Prompting for Pedestrian Attribute Recognition](https://arxiv.org/abs/2506.01411)
*Minjeong Park,Hongbeen Park,Jinkyu Kim*

Main category: cs.CV

TL;DR: ViTA-PAR通过视觉和文本属性对齐及属性提示，提升行人属性识别性能，支持从全局到局部的多粒度特征捕捉，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于固定水平区域的属性识别，导致属性出现在不同位置时性能下降，需更灵活的多模态方法。

Method: 提出ViTA-PAR，结合视觉属性提示（全局到局部语义）和可学习的文本提示模板，对齐视觉与文本特征。

Result: 在四个PAR基准测试中表现优异，推理高效。

Conclusion: ViTA-PAR通过多模态提示和对齐机制，显著提升了行人属性识别的鲁棒性和准确性。

Abstract: The Pedestrian Attribute Recognition (PAR) task aims to identify various
detailed attributes of an individual, such as clothing, accessories, and
gender. To enhance PAR performance, a model must capture features ranging from
coarse-grained global attributes (e.g., for identifying gender) to fine-grained
local details (e.g., for recognizing accessories) that may appear in diverse
regions. Recent research suggests that body part representation can enhance the
model's robustness and accuracy, but these methods are often restricted to
attribute classes within fixed horizontal regions, leading to degraded
performance when attributes appear in varying or unexpected body locations. In
this paper, we propose Visual and Textual Attribute Alignment with Attribute
Prompting for Pedestrian Attribute Recognition, dubbed as ViTA-PAR, to enhance
attribute recognition through specialized multimodal prompting and
vision-language alignment. We introduce visual attribute prompts that capture
global-to-local semantics, enabling diverse attribute representations. To
enrich textual embeddings, we design a learnable prompt template, termed person
and attribute context prompting, to learn person and attributes context.
Finally, we align visual and textual attribute features for effective fusion.
ViTA-PAR is validated on four PAR benchmarks, achieving competitive performance
with efficient inference. We release our code and model at
https://github.com/mlnjeongpark/ViTA-PAR.

</details>


### [118] [Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models](https://arxiv.org/abs/2506.01413)
*Yulei Qin,Gang Li,Zongyi Li,Zihan Xu,Yuchen Shi,Zhekai Lin,Xiao Cui,Ke Li,Xing Sun*

Main category: cs.CV

TL;DR: 论文提出了一种系统性方法，通过激励推理来提升大语言模型（LLMs）处理复杂指令的能力，解决了传统链式思维（CoT）方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在遵循复杂指令时表现不佳，尤其是涉及多约束并行、链式和分支结构时。传统链式思维方法因浅层推理模式反而降低了性能。

Method: 1. 基于现有分类法分解复杂指令并提出可复现的数据获取方法；2. 利用强化学习（RL）和可验证规则奖励信号培养推理能力；3. 通过样本对比和行为克隆优化推理过程。

Result: 在七个基准测试中，1.5B参数的LLM性能提升11.74%，达到与8B参数模型相当的水平。

Conclusion: 该方法有效解决了复杂指令下的推理问题，显著提升了LLMs的性能。

Abstract: Existing large language models (LLMs) face challenges of following complex
instructions, especially when multiple constraints are present and organized in
paralleling, chaining, and branching structures. One intuitive solution, namely
chain-of-thought (CoT), is expected to universally improve capabilities of
LLMs. However, we find that the vanilla CoT exerts a negative impact on
performance due to its superficial reasoning pattern of simply paraphrasing the
instructions. It fails to peel back the compositions of constraints for
identifying their relationship across hierarchies of types and dimensions. To
this end, we propose a systematic method to boost LLMs in dealing with complex
instructions via incentivizing reasoning for test-time compute scaling. First,
we stem from the decomposition of complex instructions under existing
taxonomies and propose a reproducible data acquisition method. Second, we
exploit reinforcement learning (RL) with verifiable rule-centric reward signals
to cultivate reasoning specifically for instruction following. We address the
shallow, non-essential nature of reasoning under complex instructions via
sample-wise contrast for superior CoT enforcement. We also exploit behavior
cloning of experts to facilitate steady distribution shift from fast-thinking
LLMs to skillful reasoners. Extensive evaluations on seven comprehensive
benchmarks confirm the validity of the proposed method, where a 1.5B LLM
achieves 11.74% gains with performance comparable to a 8B LLM. Codes and data
are available at https://github.com/yuleiqin/RAIF.

</details>


### [119] [DNAEdit: Direct Noise Alignment for Text-Guided Rectified Flow Editing](https://arxiv.org/abs/2506.01430)
*Chenxi Xie,Minghan Li,Shuai Li,Yuhui Wu,Qiaosi Yi,Lei Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为DNAEdit的新方法，通过直接优化噪声域中的高斯噪声（DNA）和移动速度引导（MVG），显著减少了误差累积，提升了图像编辑性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散或整流流的方法在反向合成轨迹时引入噪声，导致误差累积和重建精度下降。

Method: 提出Direct Noise Alignment（DNA）直接优化噪声域中的高斯噪声，并通过Mobile Velocity Guidance（MVG）平衡背景保留和目标编辑。

Result: 实验证明DNAEdit在文本引导的图像编辑中优于现有方法。

Conclusion: DNAEdit通过减少误差累积和引入MVG，显著提升了图像编辑的准确性和可控性。

Abstract: Leveraging the powerful generation capability of large-scale pretrained
text-to-image models, training-free methods have demonstrated impressive image
editing results. Conventional diffusion-based methods, as well as recent
rectified flow (RF)-based methods, typically reverse synthesis trajectories by
gradually adding noise to clean images, during which the noisy latent at the
current timestep is used to approximate that at the next timesteps, introducing
accumulated drift and degrading reconstruction accuracy. Considering the fact
that in RF the noisy latent is estimated through direct interpolation between
Gaussian noises and clean images at each timestep, we propose Direct Noise
Alignment (DNA), which directly refines the desired Gaussian noise in the noise
domain, significantly reducing the error accumulation in previous methods.
Specifically, DNA estimates the velocity field of the interpolated noised
latent at each timestep and adjusts the Gaussian noise by computing the
difference between the predicted and expected velocity field. We validate the
effectiveness of DNA and reveal its relationship with existing RF-based
inversion methods. Additionally, we introduce a Mobile Velocity Guidance (MVG)
to control the target prompt-guided generation process, balancing image
background preservation and target object editability. DNA and MVG collectively
constitute our proposed method, namely DNAEdit. Finally, we introduce
DNA-Bench, a long-prompt benchmark, to evaluate the performance of advanced
image editing models. Experimental results demonstrate that our DNAEdit
achieves superior performance to state-of-the-art text-guided editing methods.
Codes and benchmark will be available at \href{
https://xiechenxi99.github.io/DNAEdit/}{https://xiechenxi99.github.io/DNAEdit/}.

</details>


### [120] [Semantic Palette-Guided Color Propagation](https://arxiv.org/abs/2506.01441)
*Zi-Yu Zhang,Bing-Feng Seng,Ya-Feng Du,Kang Li,Zhe-Cheng Wang,Zheng-Jun Du*

Main category: cs.CV

TL;DR: 提出了一种基于语义调色板的颜色传播方法，通过提取语义调色板并优化能量函数，实现内容感知的局部颜色编辑。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖低层次视觉线索（如颜色、纹理或亮度）难以实现内容感知的颜色传播，而现有引入语义信息的方法常导致不自然的全局颜色变化。

Method: 首先从输入图像中提取语义调色板，然后通过优化设计的能量函数生成编辑后的调色板，最后将局部编辑精确传播到具有相似语义的区域。

Result: 实验证明该方法能够高效且准确地进行像素级颜色编辑，并确保局部颜色变化以内容感知的方式传播。

Conclusion: 该方法克服了传统方法的局限性，实现了更自然、更准确的颜色传播。

Abstract: Color propagation aims to extend local color edits to similar regions across
the input image. Conventional approaches often rely on low-level visual cues
such as color, texture, or lightness to measure pixel similarity, making it
difficult to achieve content-aware color propagation. While some recent
approaches attempt to introduce semantic information into color editing, but
often lead to unnatural, global color change in color adjustments. To overcome
these limitations, we present a semantic palette-guided approach for color
propagation. We first extract a semantic palette from an input image. Then, we
solve an edited palette by minimizing a well-designed energy function based on
user edits. Finally, local edits are accurately propagated to regions that
share similar semantics via the solved palette. Our approach enables efficient
yet accurate pixel-level color editing and ensures that local color changes are
propagated in a content-aware manner. Extensive experiments demonstrated the
effectiveness of our method.

</details>


### [121] [MS-RAFT-3D: A Multi-Scale Architecture for Recurrent Image-Based Scene Flow](https://arxiv.org/abs/2506.01443)
*Jakob Schmid,Azin Jahedi,Noah Berenguel Senn,Andrés Bruhn*

Main category: cs.CV

TL;DR: 本文提出了一种多尺度方法，将光流中的分层思想推广到基于图像的场景流中，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多尺度概念在光流和立体视觉的循环网络架构中已被证明有效，但尚未应用于基于图像的场景流。

Method: 基于单尺度循环场景流主干，开发了多尺度方法，改进了特征和上下文编码器、粗到细框架及训练损失。

Result: 在KITTI和Spring数据集上分别以8.7%和65.8%的优势超越了当前最优方法。

Conclusion: 多尺度方法显著提升了场景流估计的性能，代码已开源。

Abstract: Although multi-scale concepts have recently proven useful for recurrent
network architectures in the field of optical flow and stereo, they have not
been considered for image-based scene flow so far. Hence, based on a
single-scale recurrent scene flow backbone, we develop a multi-scale approach
that generalizes successful hierarchical ideas from optical flow to image-based
scene flow. By considering suitable concepts for the feature and the context
encoder, the overall coarse-to-fine framework and the training loss, we succeed
to design a scene flow approach that outperforms the current state of the art
on KITTI and Spring by 8.7%(3.89 vs. 4.26) and 65.8% (9.13 vs. 26.71),
respectively. Our code is available at
https://github.com/cv-stuttgart/MS-RAFT-3D.

</details>


### [122] [A Novel Context-Adaptive Fusion of Shadow and Highlight Regions for Efficient Sonar Image Classification](https://arxiv.org/abs/2506.01445)
*Kamal Basha S,Anukul Kiran B,Athira Nambiar,Suresh Rajendran*

Main category: cs.CV

TL;DR: 提出了一种上下文自适应的声纳图像分类框架，结合阴影和高光特征，并引入阴影特定分类器和自适应分割，同时提出区域感知去噪模型和扩展数据集S3Simulator+。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要基于高光分析，阴影分类研究不足，限制了水下探测的准确性。

Method: 提出上下文自适应分类框架，结合阴影和高光特征，引入阴影特定分类器和自适应分割；开发区域感知去噪模型，通过特征重要性优化去噪。

Result: 框架提升了特征表示和分类鲁棒性，去噪模型增强了图像质量和可解释性，扩展数据集支持更稳健的AI模型开发。

Conclusion: 该工作通过新分类策略和增强数据集，解决了声纳图像分析中的关键挑战，推动了自主水下感知的发展。

Abstract: Sonar imaging is fundamental to underwater exploration, with critical
applications in defense, navigation, and marine research. Shadow regions, in
particular, provide essential cues for object detection and classification, yet
existing studies primarily focus on highlight-based analysis, leaving
shadow-based classification underexplored. To bridge this gap, we propose a
Context-adaptive sonar image classification framework that leverages advanced
image processing techniques to extract and integrate discriminative shadow and
highlight features. Our framework introduces a novel shadow-specific classifier
and adaptive shadow segmentation, enabling effective classification based on
the dominant region. This approach ensures optimal feature representation,
improving robustness against noise and occlusions. In addition, we introduce a
Region-aware denoising model that enhances sonar image quality by preserving
critical structural details while suppressing noise. This model incorporates an
explainability-driven optimization strategy, ensuring that denoising is guided
by feature importance, thereby improving interpretability and classification
reliability. Furthermore, we present S3Simulator+, an extended dataset
incorporating naval mine scenarios with physics-informed noise specifically
tailored for the underwater sonar domain, fostering the development of robust
AI models. By combining novel classification strategies with an enhanced
dataset, our work addresses key challenges in sonar image analysis,
contributing
  to the advancement of autonomous underwater perception.

</details>


### [123] [DiffuseSlide: Training-Free High Frame Rate Video Generation Diffusion](https://arxiv.org/abs/2506.01454)
*Geunmin Hwang,Hyun-kyu Ko,Younghyun Kim,Seungryong Lee,Eunbyung Park*

Main category: cs.CV

TL;DR: DiffuseSlide提出了一种无需额外训练的方法，利用预训练扩散模型生成高帧率视频，解决了现有方法在长序列中闪烁和质量下降的问题。


<details>
  <summary>Details</summary>
Motivation: 高帧率视频生成面临闪烁和质量下降的挑战，现有方法计算效率低且难以保持视频质量。

Method: 通过关键帧提取、噪声重注入和滑动窗口潜在去噪技术，无需微调即可生成高质量视频。

Result: 实验表明，该方法显著提升了视频质量，增强了时间一致性和空间保真度。

Conclusion: DiffuseSlide计算高效且适用于多种视频生成任务，适合虚拟现实、游戏等内容创作。

Abstract: Recent advancements in diffusion models have revolutionized video generation,
enabling the creation of high-quality, temporally consistent videos. However,
generating high frame-rate (FPS) videos remains a significant challenge due to
issues such as flickering and degradation in long sequences, particularly in
fast-motion scenarios. Existing methods often suffer from computational
inefficiencies and limitations in maintaining video quality over extended
frames. In this paper, we present a novel, training-free approach for high FPS
video generation using pre-trained diffusion models. Our method, DiffuseSlide,
introduces a new pipeline that leverages key frames from low FPS videos and
applies innovative techniques, including noise re-injection and sliding window
latent denoising, to achieve smooth, consistent video outputs without the need
for additional fine-tuning. Through extensive experiments, we demonstrate that
our approach significantly improves video quality, offering enhanced temporal
coherence and spatial fidelity. The proposed method is not only computationally
efficient but also adaptable to various video generation tasks, making it ideal
for applications such as virtual reality, video games, and high-quality content
creation.

</details>


### [124] [Towards Scalable Video Anomaly Retrieval: A Synthetic Video-Text Benchmark](https://arxiv.org/abs/2506.01466)
*Shuyu Yang,Yilun Wang,Yaxiong Wang,Li Zhu,Zhedong Zheng*

Main category: cs.CV

TL;DR: SVTA是一个通过生成模型解决视频异常检索数据稀缺和隐私问题的大规模合成数据集。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据集因真实异常事件的长尾性和隐私限制导致的数据稀缺问题。

Method: 利用大型语言模型生成异常事件描述，并指导视频生成模型创建多样化的高质量视频。

Result: SVTA包含41,315个视频（1.36M帧），涵盖30种正常活动和68种异常事件，测试表明其具有挑战性和有效性。

Conclusion: SVTA在避免隐私风险的同时，提供了逼真的场景，为跨模态检索方法提供了有效评估基准。

Abstract: Video anomaly retrieval aims to localize anomalous events in videos using
natural language queries to facilitate public safety. However, existing
datasets suffer from severe limitations: (1) data scarcity due to the long-tail
nature of real-world anomalies, and (2) privacy constraints that impede
large-scale collection. To address the aforementioned issues in one go, we
introduce SVTA (Synthetic Video-Text Anomaly benchmark), the first large-scale
dataset for cross-modal anomaly retrieval, leveraging generative models to
overcome data availability challenges. Specifically, we collect and generate
video descriptions via the off-the-shelf LLM (Large Language Model) covering 68
anomaly categories, e.g., throwing, stealing, and shooting. These descriptions
encompass common long-tail events. We adopt these texts to guide the video
generative model to produce diverse and high-quality videos. Finally, our SVTA
involves 41,315 videos (1.36M frames) with paired captions, covering 30 normal
activities, e.g., standing, walking, and sports, and 68 anomalous events, e.g.,
falling, fighting, theft, explosions, and natural disasters. We adopt three
widely-used video-text retrieval baselines to comprehensively test our SVTA,
revealing SVTA's challenging nature and its effectiveness in evaluating a
robust cross-modal retrieval method. SVTA eliminates privacy risks associated
with real-world anomaly collection while maintaining realistic scenarios. The
dataset demo is available at: [https://svta-mm.github.io/SVTA.github.io/].

</details>


### [125] [Sheep Facial Pain Assessment Under Weighted Graph Neural Networks](https://arxiv.org/abs/2506.01468)
*Alam Noor,Luis Almeida,Mohamed Daoudi,Kai Li,Eduardo Tovar*

Main category: cs.CV

TL;DR: 论文提出了一种基于加权图神经网络（WGNN）的模型，用于通过绵羊面部标志点检测和预测疼痛水平，并创建了一个新的绵羊面部标志点数据集。


<details>
  <summary>Details</summary>
Motivation: 准确识别和评估绵羊的疼痛对动物健康和福利至关重要，但现有方法在自动监测方面存在局限性。

Method: 使用加权图神经网络（WGNN）模型连接检测到的面部标志点，并定义疼痛水平；同时提出新的绵羊面部标志点数据集。

Result: YOLOv8n检测器在绵羊面部标志点数据集上的平均精度为59.30%，WGNN框架的准确率达到92.71%。

Conclusion: WGNN模型在绵羊疼痛检测中表现出高精度，为自动监测提供了有效工具。

Abstract: Accurately recognizing and assessing pain in sheep is key to discern animal
health and mitigating harmful situations. However, such accuracy is limited by
the ability to manage automatic monitoring of pain in those animals. Facial
expression scoring is a widely used and useful method to evaluate pain in both
humans and other living beings. Researchers also analyzed the facial
expressions of sheep to assess their health state and concluded that facial
landmark detection and pain level prediction are essential. For this purpose,
we propose a novel weighted graph neural network (WGNN) model to link sheep's
detected facial landmarks and define pain levels. Furthermore, we propose a new
sheep facial landmarks dataset that adheres to the parameters of the Sheep
Facial Expression Scale (SPFES). Currently, there is no comprehensive
performance benchmark that specifically evaluates the use of graph neural
networks (GNNs) on sheep facial landmark data to detect and measure pain
levels. The YOLOv8n detector architecture achieves a mean average precision
(mAP) of 59.30% with the sheep facial landmarks dataset, among seven other
detection models. The WGNN framework has an accuracy of 92.71% for tracking
multiple facial parts expressions with the YOLOv8n lightweight on-board device
deployment-capable model.

</details>


### [126] [SemiVT-Surge: Semi-Supervised Video Transformer for Surgical Phase Recognition](https://arxiv.org/abs/2506.01471)
*Yiping Li,Ronald de Jong,Sahar Nasirihaghighi,Tim Jaspers,Romy van Jaarsveld,Gino Kuiper,Richard van Hillegersberg,Fons van der Sommen,Jelle Ruurda,Marcel Breeuwer,Yasmina Al Khalil*

Main category: cs.CV

TL;DR: 提出了一种基于视频Transformer的模型，结合伪标签框架和对比学习，用于半监督手术阶段识别，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 手术视频标注耗时，研究旨在利用未标注数据减少标注需求，提升识别性能。

Method: 采用视频Transformer模型，结合时间一致性正则化和对比学习，利用伪标签优化特征空间。

Result: 在RAMIE数据集上准确率提升4.9%，在Cholec80上仅用1/4标注数据即达到全监督效果。

Conclusion: 为半监督手术阶段识别设定了新基准，推动了该领域未来研究。

Abstract: Accurate surgical phase recognition is crucial for computer-assisted
interventions and surgical video analysis. Annotating long surgical videos is
labor-intensive, driving research toward leveraging unlabeled data for strong
performance with minimal annotations. Although self-supervised learning has
gained popularity by enabling large-scale pretraining followed by fine-tuning
on small labeled subsets, semi-supervised approaches remain largely
underexplored in the surgical domain. In this work, we propose a video
transformer-based model with a robust pseudo-labeling framework. Our method
incorporates temporal consistency regularization for unlabeled data and
contrastive learning with class prototypes, which leverages both labeled data
and pseudo-labels to refine the feature space. Through extensive experiments on
the private RAMIE (Robot-Assisted Minimally Invasive Esophagectomy) dataset and
the public Cholec80 dataset, we demonstrate the effectiveness of our approach.
By incorporating unlabeled data, we achieve state-of-the-art performance on
RAMIE with a 4.9% accuracy increase and obtain comparable results to full
supervision while using only 1/4 of the labeled data on Cholec80. Our findings
establish a strong benchmark for semi-supervised surgical phase recognition,
paving the way for future research in this domain.

</details>


### [127] [Unlocking Aha Moments via Reinforcement Learning: Advancing Collaborative Visual Comprehension and Generation](https://arxiv.org/abs/2506.01480)
*Kaihang Pan,Yang Wu,Wendong Bu,Kai Shen,Juncheng Li,Yingting Wang,Yunfei Li,Siliang Tang,Jun Xiao,Fei Wu,Hang Zhao,Yueting Zhuang*

Main category: cs.CV

TL;DR: 论文提出了一种协同进化视觉理解与生成的方法，通过两阶段训练（监督微调和强化学习）实现迭代内省式图像生成，提升了多模态大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）中视觉理解与生成能力相互独立，未能充分利用语言模型的推理机制改进图像生成。

Method: 采用两阶段训练：监督微调赋予MLLM生成视觉生成的真实CoT能力，强化学习通过探索-利用权衡激活其潜力。

Result: 模型在文本到图像生成、图像编辑和图像语义评估任务中表现优异，视觉理解能力显著提升。

Conclusion: 该方法成功将视觉理解与生成协同进化，实现了从文本到图像任务到统一图像生成的突破。

Abstract: Recent endeavors in Multimodal Large Language Models (MLLMs) aim to unify
visual comprehension and generation. However, these two capabilities remain
largely independent, as if they are two separate functions encapsulated within
the same model. Consequently, visual comprehension does not enhance visual
generation, and the reasoning mechanisms of LLMs have not been fully integrated
to revolutionize image generation. In this paper, we propose to enable the
collaborative co-evolution of visual comprehension and generation, advancing
image generation into an iterative introspective process. We introduce a
two-stage training approach: supervised fine-tuning teaches the MLLM with the
foundational ability to generate genuine CoT for visual generation, while
reinforcement learning activates its full potential via an
exploration-exploitation trade-off. Ultimately, we unlock the Aha moment in
visual generation, advancing MLLMs from text-to-image tasks to unified image
generation. Extensive experiments demonstrate that our model not only excels in
text-to-image generation and image editing, but also functions as a superior
image semantic evaluator with enhanced visual comprehension capabilities.
Project Page: https://janus-pro-r1.github.io.

</details>


### [128] [FDSG: Forecasting Dynamic Scene Graphs](https://arxiv.org/abs/2506.01487)
*Yi Yang,Yuren Cong,Hao Cheng,Bodo Rosenhahn,Michael Ying Yang*

Main category: cs.CV

TL;DR: 论文提出FDSG框架，预测未来帧的实体标签、边界框和关系，同时生成观察帧的场景图，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能显式建模时间动态或仅预测关系，限制了视频场景理解。

Method: FDSG通过查询分解和神经随机微分方程建模动态，并利用时间聚合模块优化预测。

Result: 在Action Genome数据集上，FDSG在动态场景图生成、预测和前瞻任务中表现最佳。

Conclusion: FDSG为未来场景图预测提供了新框架，显著提升了性能。

Abstract: Dynamic scene graph generation extends scene graph generation from images to
videos by modeling entity relationships and their temporal evolution. However,
existing methods either generate scene graphs from observed frames without
explicitly modeling temporal dynamics, or predict only relationships while
assuming static entity labels and locations. These limitations hinder effective
extrapolation of both entity and relationship dynamics, restricting video scene
understanding. We propose Forecasting Dynamic Scene Graphs (FDSG), a novel
framework that predicts future entity labels, bounding boxes, and
relationships, for unobserved frames, while also generating scene graphs for
observed frames. Our scene graph forecast module leverages query decomposition
and neural stochastic differential equations to model entity and relationship
dynamics. A temporal aggregation module further refines predictions by
integrating forecasted and observed information via cross-attention. To
benchmark FDSG, we introduce Scene Graph Forecasting, a new task for full
future scene graph prediction. Experiments on Action Genome show that FDSG
outperforms state-of-the-art methods on dynamic scene graph generation, scene
graph anticipation, and scene graph forecasting. Codes will be released upon
publication.

</details>


### [129] [Efficiency without Compromise: CLIP-aided Text-to-Image GANs with Increased Diversity](https://arxiv.org/abs/2506.01493)
*Yuya Kobayashi,Yuhta Takida,Takashi Shibuya,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: SCAD模型通过结合预训练模型和专用判别器，显著降低了训练成本，同时提升了生成多样性和保真度。


<details>
  <summary>Details</summary>
Motivation: 降低大规模GANs的训练成本，同时避免生成多样性的损失。

Method: 使用两个专用判别器和Slicing Adversarial Networks (SANs)来优化文本到图像任务。

Result: SCAD在训练成本大幅降低的情况下，实现了与最新大规模GANs相当的零样本FID，并显著提升了生成多样性。

Conclusion: SCAD是一种高效且高质量的文本到图像生成模型，适用于低成本研究和应用。

Abstract: Recently, Generative Adversarial Networks (GANs) have been successfully
scaled to billion-scale large text-to-image datasets. However, training such
models entails a high training cost, limiting some applications and research
usage. To reduce the cost, one promising direction is the incorporation of
pre-trained models. The existing method of utilizing pre-trained models for a
generator significantly reduced the training cost compared with the other
large-scale GANs, but we found the model loses the diversity of generation for
a given prompt by a large margin. To build an efficient and high-fidelity
text-to-image GAN without compromise, we propose to use two specialized
discriminators with Slicing Adversarial Networks (SANs) adapted for
text-to-image tasks. Our proposed model, called SCAD, shows a notable
enhancement in diversity for a given prompt with better sample fidelity. We
also propose to use a metric called Per-Prompt Diversity (PPD) to evaluate the
diversity of text-to-image models quantitatively. SCAD achieved a zero-shot FID
competitive with the latest large-scale GANs at two orders of magnitude less
training cost.

</details>


### [130] [Enhancing Diffusion-based Unrestricted Adversarial Attacks via Adversary Preferences Alignment](https://arxiv.org/abs/2506.01511)
*Kaixun Jiang,Zhaoyu Chen,Haijing Guo,Jinglun Li,Jiyuan Fu,Pinxue Guo,Hao Tang,Bo Li,Wenqiang Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种对抗性偏好对齐框架APA，通过两阶段优化解决视觉一致性与攻击效果的冲突，显著提升了攻击迁移性。


<details>
  <summary>Details</summary>
Motivation: 传统偏好对齐主要关注良性人类偏好（如审美），而本文探讨对抗性偏好对齐，解决视觉一致性与攻击效果的冲突问题。

Method: APA框架分为两阶段：1）使用LoRA微调提升视觉一致性；2）基于替代分类器反馈优化图像潜在表示或提示嵌入，并引入扩散增强策略提升黑盒迁移性。

Result: 实验表明APA在保持高视觉一致性的同时，显著提升了攻击迁移性。

Conclusion: APA为对抗性攻击研究提供了新的对齐视角，代码将开源。

Abstract: Preference alignment in diffusion models has primarily focused on benign
human preferences (e.g., aesthetic). In this paper, we propose a novel
perspective: framing unrestricted adversarial example generation as a problem
of aligning with adversary preferences. Unlike benign alignment, adversarial
alignment involves two inherently conflicting preferences: visual consistency
and attack effectiveness, which often lead to unstable optimization and reward
hacking (e.g., reducing visual quality to improve attack success). To address
this, we propose APA (Adversary Preferences Alignment), a two-stage framework
that decouples conflicting preferences and optimizes each with differentiable
rewards. In the first stage, APA fine-tunes LoRA to improve visual consistency
using rule-based similarity reward. In the second stage, APA updates either the
image latent or prompt embedding based on feedback from a substitute
classifier, guided by trajectory-level and step-wise rewards. To enhance
black-box transferability, we further incorporate a diffusion augmentation
strategy. Experiments demonstrate that APA achieves significantly better attack
transferability while maintaining high visual consistency, inspiring further
research to approach adversarial attacks from an alignment perspective. Code
will be available at https://github.com/deep-kaixun/APA.

</details>


### [131] [Speed-up of Vision Transformer Models by Attention-aware Token Filtering](https://arxiv.org/abs/2506.01519)
*Takahiro Naruko,Hiroaki Akutsu*

Main category: cs.CV

TL;DR: 提出了一种名为ATF的新方法，用于加速ViT模型，通过动态和静态过滤策略减少计算负担，同时保持任务准确性。


<details>
  <summary>Details</summary>
Motivation: ViT模型在图像嵌入提取中表现出色，但计算负担高，需要一种高效的速度提升方法。

Method: ATF包括一个新颖的标记过滤模块和过滤策略，动态保留特定对象类型的标记，静态保留高注意力区域的标记。

Result: 在检索任务中，ATF将ViT模型SigLIP的速度提升了2.8倍，同时保持了检索召回率。

Conclusion: ATF是一种有效的ViT模型加速方法，兼顾了速度和准确性。

Abstract: Vision Transformer (ViT) models have made breakthroughs in image embedding
extraction, which provide state-of-the-art performance in tasks such as
zero-shot image classification. However, the models suffer from a high
computational burden. In this paper, we propose a novel speed-up method for ViT
models called Attention-aware Token Filtering (ATF). ATF consists of two main
ideas: a novel token filtering module and a filtering strategy. The token
filtering module is introduced between a tokenizer and a transformer encoder of
the ViT model, without modifying or fine-tuning of the transformer encoder. The
module filters out tokens inputted to the encoder so that it keeps tokens in
regions of specific object types dynamically and keeps tokens in regions that
statically receive high attention in the transformer encoder. This filtering
strategy maintains task accuracy while filtering out tokens inputted to the
transformer encoder. Evaluation results on retrieval tasks show that ATF
provides $2.8\times$ speed-up to a ViT model, SigLIP, while maintaining the
retrieval recall rate.

</details>


### [132] [Beyond black and white: A more nuanced approach to facial recognition with continuous ethnicity labels](https://arxiv.org/abs/2506.01532)
*Pedro C. Neto,Naser Damer,Jaime S. Cardoso,Ana F. Sequeira*

Main category: cs.CV

TL;DR: 论文提出将种族标签从离散值改为连续变量，以更准确地平衡数据集，实验证明连续空间平衡的数据集训练出的模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 人脸识别模型中存在偏见问题，传统方法对数据偏见的缓解有限且缺乏对问题本质的洞察。

Method: 将种族标签作为连续变量而非离散值，通过实验和理论验证其有效性，并训练65个不同模型和20个子数据集。

Result: 连续空间平衡的数据集训练的模型表现优于离散空间平衡的数据集。

Conclusion: 种族标签的连续化处理能更有效地平衡数据集，提升模型性能。

Abstract: Bias has been a constant in face recognition models. Over the years,
researchers have looked at it from both the model and the data point of view.
However, their approach to mitigation of data bias was limited and lacked
insight on the real nature of the problem. Here, in this document, we propose
to revise our use of ethnicity labels as a continuous variable instead of a
discrete value per identity. We validate our formulation both experimentally
and theoretically, showcasing that not all identities from one ethnicity
contribute equally to the balance of the dataset; thus, having the same number
of identities per ethnicity does not represent a balanced dataset. We further
show that models trained on datasets balanced in the continuous space
consistently outperform models trained on data balanced in the discrete space.
We trained more than 65 different models, and created more than 20 subsets of
the original datasets.

</details>


### [133] [G4Seg: Generation for Inexact Segmentation Refinement with Diffusion Models](https://arxiv.org/abs/2506.01539)
*Tianjiao Zhang,Fei Zhang,Jiangchao Yao,Ya Zhang,Yanfeng Wang*

Main category: cs.CV

TL;DR: 利用Stable Diffusion的生成先验解决Inexact Segmentation任务，通过生成图像与原始图像的差异实现粗到细的分割优化。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖判别模型或密集视觉表示，而本文探索生成模型在分割任务中的潜力。

Method: 利用Stable Diffusion的生成先验，通过原始图像与掩码条件生成图像的差异，建立语义对齐并更新前景概率。

Result: 实验验证了方法的有效性和优越性，展示了生成差异在密集表示建模中的潜力。

Conclusion: 生成方法在判别任务中具有潜力，值得进一步探索。

Abstract: This paper considers the problem of utilizing a large-scale text-to-image
diffusion model to tackle the challenging Inexact Segmentation (IS) task.
Unlike traditional approaches that rely heavily on discriminative-model-based
paradigms or dense visual representations derived from internal attention
mechanisms, our method focuses on the intrinsic generative priors in Stable
Diffusion~(SD). Specifically, we exploit the pattern discrepancies between
original images and mask-conditional generated images to facilitate a
coarse-to-fine segmentation refinement by establishing a semantic
correspondence alignment and updating the foreground probability. Comprehensive
quantitative and qualitative experiments validate the effectiveness and
superiority of our plug-and-play design, underscoring the potential of
leveraging generation discrepancies to model dense representations and
encouraging further exploration of generative approaches for solving
discriminative tasks.

</details>


### [134] [LongDWM: Cross-Granularity Distillation for Building a Long-Term Driving World Model](https://arxiv.org/abs/2506.01546)
*Xiaodong Wang,Zhirong Wu,Peixi Peng*

Main category: cs.CV

TL;DR: 提出了一种分层解耦和蒸馏方法的长时驾驶世界模型，显著提升了视频生成的连贯性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前驾驶世界模型在长时预测中存在误差累积问题，且训练与推理之间存在差距，限制了实际应用。

Method: 分层解耦为粗粒度运动学习和双向连续运动学习，并设计自监督蒸馏方法提升视频连贯性。

Result: 在NuScenes基准测试中，FVD提升27%，推理时间减少85%，支持110+帧生成。

Conclusion: 该方法有效解决了长时视频生成的连贯性和效率问题，具有实际应用潜力。

Abstract: Driving world models are used to simulate futures by video generation based
on the condition of the current state and actions. However, current models
often suffer serious error accumulations when predicting the long-term future,
which limits the practical application. Recent studies utilize the Diffusion
Transformer (DiT) as the backbone of driving world models to improve learning
flexibility. However, these models are always trained on short video clips
(high fps and short duration), and multiple roll-out generations struggle to
produce consistent and reasonable long videos due to the training-inference
gap. To this end, we propose several solutions to build a simple yet effective
long-term driving world model. First, we hierarchically decouple world model
learning into large motion learning and bidirectional continuous motion
learning. Then, considering the continuity of driving scenes, we propose a
simple distillation method where fine-grained video flows are self-supervised
signals for coarse-grained flows. The distillation is designed to improve the
coherence of infinite video generation. The coarse-grained and fine-grained
modules are coordinated to generate long-term and temporally coherent videos.
In the public benchmark NuScenes, compared with the state-of-the-art front-view
model, our model improves FVD by $27\%$ and reduces inference time by $85\%$
for the video task of generating 110+ frames. More videos (including 90s
duration) are available at https://Wang-Xiaodong1899.github.io/longdwm/.

</details>


### [135] [EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation](https://arxiv.org/abs/2506.01551)
*Bingqian Lin,Yunshuang Nie,Khun Loun Zai,Ziming Wei,Mingfei Han,Rongtao Xu,Minzhe Niu,Jianhua Han,Liang Lin,Cewu Lu,Xiaodan Liang*

Main category: cs.CV

TL;DR: EvolveNav是一个两阶段的自我改进框架，通过形式化的思维链（CoT）和监督后训练提升基于LLM的视觉语言导航能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中直接输入-输出映射导致的导航决策难以学习和解释的问题。

Method: 1. 形式化CoT监督微调；2. 自我反思后训练，利用模型自身推理输出作为标签增强监督多样性。

Result: 在主流VLN基准测试中表现优于现有LLM-based方法。

Conclusion: EvolveNav通过自我改进框架显著提升了导航决策的准确性和可解释性。

Abstract: Building Vision-Language Navigation (VLN) agents which can navigate following
natural language instructions is a long-standing goal in human-robot
interaction applications. Recent studies have revealed the potential of
training open-source Large Language Models (LLMs) to unleash LLMs' reasoning
ability for improving navigation, and simultaneously mitigate the domain gap
between LLMs' training corpus and the VLN task. However, these approaches
primarily adopt direct input-output mapping paradigms, causing the mapping
learning difficult and the navigational decisions unexplainable.
Chain-of-Thought (CoT) training is a promising way to improve both navigational
decision accuracy and interpretability, while the complexity of the navigation
task makes the perfect CoT labels unavailable and may lead to overfitting
through pure CoT supervised fine-tuning. In this paper, we propose a novel
sElf-improving embodied reasoning framework for boosting LLM-based
vision-language Navigation, dubbed EvolveNav. Our EvolveNav consists of two
stages: (1) Formalized CoT Supervised Fine-Tuning, where we train the model
with formalized CoT labels to both activate the model's navigational reasoning
capabilities and increase the reasoning speed; (2) Self-Reflective
Post-Training, where the model is iteratively trained with its own reasoning
outputs as self-enriched CoT labels to enhance the supervision diversity. A
self-reflective auxiliary task is also introduced to encourage learning correct
reasoning patterns by contrasting with wrong ones. Experimental results on the
popular VLN benchmarks demonstrate the superiority of EvolveNav over previous
LLM-based VLN approaches. Code is available at
https://github.com/expectorlin/EvolveNav.

</details>


### [136] [SAM2-LOVE: Segment Anything Model 2 in Language-aided Audio-Visual Scenes](https://arxiv.org/abs/2506.01558)
*Yuji Wang,Haoran Xu,Yong Liu,Jiaze Li,Yansong Tang*

Main category: cs.CV

TL;DR: SAM2-LOVE是一种新框架，通过整合文本、音频和视觉表示来提升参考音频-视觉分割（Ref-AVS）任务的性能，解决了多模态一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法因缺乏第三模态或时空一致性不足而表现不佳，需要一种更有效的多模态融合方法。

Method: 提出SAM2-LOVE框架，结合多模态融合模块、令牌传播和累积策略，以增强时空一致性。

Result: 在Ref-AVS基准测试中，SAM2-LOVE比现有最佳方法性能提升8.5%。

Conclusion: SAM2-LOVE通过简单有效的组件显著提升了多模态场景理解的性能。

Abstract: Reference Audio-Visual Segmentation (Ref-AVS) aims to provide a pixel-wise
scene understanding in Language-aided Audio-Visual Scenes (LAVS). This task
requires the model to continuously segment objects referred to by text and
audio from a video. Previous dual-modality methods always fail due to the lack
of a third modality and the existing triple-modality method struggles with
spatio-temporal consistency, leading to the target shift of different frames.
In this work, we introduce a novel framework, termed SAM2-LOVE, which
integrates textual, audio, and visual representations into a learnable token to
prompt and align SAM2 for achieving Ref-AVS in the LAVS. Technically, our
approach includes a multimodal fusion module aimed at improving multimodal
understanding of SAM2, as well as token propagation and accumulation strategies
designed to enhance spatio-temporal consistency without forgetting historical
information. We conducted extensive experiments to demonstrate that SAM2-LOVE
outperforms the SOTA by 8.5\% in $\mathcal{J\&F}$ on the Ref-AVS benchmark and
showcase the simplicity and effectiveness of the components. Our code will be
available here.

</details>


### [137] [HOSIG: Full-Body Human-Object-Scene Interaction Generation with Hierarchical Scene Perception](https://arxiv.org/abs/2506.01579)
*Wei Yao,Yunlian Sun,Hongwen Zhang,Yebin Liu,Jinhui Tang*

Main category: cs.CV

TL;DR: HOSIG框架通过分层场景感知合成全身交互，解决了现有方法在场景上下文和精细操作协调上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在人类-物体交互中常忽略场景上下文，导致不合理的穿透；而人类-场景交互方法难以协调精细操作与长距离导航。

Method: HOSIG框架包含三个关键组件：场景感知抓取姿势生成器、启发式导航算法和场景引导运动扩散模型。

Result: 在TRUMANS数据集上表现优于现有方法，支持无限长度运动生成且需最小人工干预。

Conclusion: HOSIG填补了场景感知导航与灵巧物体操作之间的关键空白，推动了交互合成的前沿。

Abstract: Generating high-fidelity full-body human interactions with dynamic objects
and static scenes remains a critical challenge in computer graphics and
animation. Existing methods for human-object interaction often neglect scene
context, leading to implausible penetrations, while human-scene interaction
approaches struggle to coordinate fine-grained manipulations with long-range
navigation. To address these limitations, we propose HOSIG, a novel framework
for synthesizing full-body interactions through hierarchical scene perception.
Our method decouples the task into three key components: 1) a scene-aware grasp
pose generator that ensures collision-free whole-body postures with precise
hand-object contact by integrating local geometry constraints, 2) a heuristic
navigation algorithm that autonomously plans obstacle-avoiding paths in complex
indoor environments via compressed 2D floor maps and dual-component spatial
reasoning, and 3) a scene-guided motion diffusion model that generates
trajectory-controlled, full-body motions with finger-level accuracy by
incorporating spatial anchors and dual-space classifier-free guidance.
Extensive experiments on the TRUMANS dataset demonstrate superior performance
over state-of-the-art methods. Notably, our framework supports unlimited motion
length through autoregressive generation and requires minimal manual
intervention. This work bridges the critical gap between scene-aware navigation
and dexterous object manipulation, advancing the frontier of embodied
interaction synthesis. Codes will be available after publication. Project page:
http://yw0208.github.io/hosig

</details>


### [138] [Multi-Modal Dataset Distillation in the Wild](https://arxiv.org/abs/2506.01586)
*Zhuohang Dang,Minnan Luo,Chengyou Jia,Hangwei Qian,Xiaojun Chang,Ivor W. Tsang*

Main category: cs.CV

TL;DR: MDW框架通过蒸馏噪声多模态数据集为紧凑干净的数据集，解决大规模数据存储和噪声问题，提升模型训练效率和效果。


<details>
  <summary>Details</summary>
Motivation: 多模态模型训练需要大规模数据集，但存储和计算成本高，且数据噪声（如不匹配对）会降低模型性能。

Method: MDW引入可学习的细粒度对应关系，通过双轨协作学习优化蒸馏数据，强调对应区分区域，提高信息密度和效果。

Result: 实验表明MDW在多种压缩比下优于先前方法15%以上，具有显著的可扩展性和实用性。

Conclusion: MDW通过噪声容忍和高效数据蒸馏，为多模态模型训练提供了实用解决方案。

Abstract: Recent multi-modal models have shown remarkable versatility in real-world
applications. However, their rapid development encounters two critical data
challenges. First, the training process requires large-scale datasets, leading
to substantial storage and computational costs. Second, these data are
typically web-crawled with inevitable noise, i.e., partially mismatched pairs,
severely degrading model performance. To these ends, we propose Multi-modal
dataset Distillation in the Wild, i.e., MDW, the first framework to distill
noisy multi-modal datasets into compact clean ones for effective and efficient
model training. Specifically, MDW introduces learnable fine-grained
correspondences during distillation and adaptively optimizes distilled data to
emphasize correspondence-discriminative regions, thereby enhancing distilled
data's information density and efficacy. Moreover, to capture robust
cross-modal correspondence prior knowledge from real data, MDW proposes
dual-track collaborative learning to avoid the risky data noise, alleviating
information loss with certifiable noise tolerance. Extensive experiments
validate MDW's theoretical and empirical efficacy with remarkable scalability,
surpassing prior methods by over 15% across various compression ratios,
highlighting its appealing practicality for applications with diverse efficacy
and resource needs.

</details>


### [139] [EPFL-Smart-Kitchen-30: Densely annotated cooking dataset with 3D kinematics to challenge video and language models](https://arxiv.org/abs/2506.01608)
*Andy Bonnetto,Haozhe Qi,Franklin Leong,Matea Tashkovska,Mahdi Rad,Solaiman Shokur,Friedhelm Hummel,Silvestro Micera,Marc Pollefeys,Alexander Mathis*

Main category: cs.CV

TL;DR: EPFL-Smart-Kitchen-30数据集是一个多模态厨房行为数据集，用于研究人类复杂动作，包含多视角同步数据，并提出了四个基准测试。


<details>
  <summary>Details</summary>
Motivation: 厨房环境适合研究人类运动和认知功能，但缺乏高质量的多模态数据集。

Method: 使用RGB-D摄像头、IMU和HoloLens~2头显采集16名受试者在厨房中的多模态数据，并密集标注动作序列。

Result: 数据集包含29.7小时的多模态数据，提出了四个行为理解和建模的基准测试。

Conclusion: 该数据集有望推动生态有效人类行为理解的方法和见解。

Abstract: Understanding behavior requires datasets that capture humans while carrying
out complex tasks. The kitchen is an excellent environment for assessing human
motor and cognitive function, as many complex actions are naturally exhibited
in kitchens from chopping to cleaning. Here, we introduce the
EPFL-Smart-Kitchen-30 dataset, collected in a noninvasive motion capture
platform inside a kitchen environment. Nine static RGB-D cameras, inertial
measurement units (IMUs) and one head-mounted HoloLens~2 headset were used to
capture 3D hand, body, and eye movements. The EPFL-Smart-Kitchen-30 dataset is
a multi-view action dataset with synchronized exocentric, egocentric, depth,
IMUs, eye gaze, body and hand kinematics spanning 29.7 hours of 16 subjects
cooking four different recipes. Action sequences were densely annotated with
33.78 action segments per minute. Leveraging this multi-modal dataset, we
propose four benchmarks to advance behavior understanding and modeling through
1) a vision-language benchmark, 2) a semantic text-to-motion generation
benchmark, 3) a multi-modal action recognition benchmark, 4) a pose-based
action segmentation benchmark. We expect the EPFL-Smart-Kitchen-30 dataset to
pave the way for better methods as well as insights to understand the nature of
ecologically-valid human behavior. Code and data are available at
https://github.com/amathislab/EPFL-Smart-Kitchen

</details>


### [140] [Visual Explanation via Similar Feature Activation for Metric Learning](https://arxiv.org/abs/2506.01636)
*Yi Liao,Ugochukwu Ejike Akpudo,Jue Zhang,Yongsheng Gao,Jun Zhou,Wenyi Zeng,Weichuan Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为SFAM的新视觉解释方法，用于解决传统CAM方法无法直接应用于度量学习模型的问题。


<details>
  <summary>Details</summary>
Motivation: 现有CAM方法无法直接应用于缺乏全连接分类层的度量学习模型，限制了其解释能力。

Method: 提出SFAM方法，通过通道贡献重要性分数（CIS）衡量特征重要性，并基于相似度测量构建解释图。

Result: 实验表明，SFAM能为使用欧氏距离或余弦相似度的CNN模型提供高质量的视觉解释。

Conclusion: SFAM是一种有效的视觉解释方法，适用于度量学习模型，增强了模型的可解释性。

Abstract: Visual explanation maps enhance the trustworthiness of decisions made by deep
learning models and offer valuable guidance for developing new algorithms in
image recognition tasks. Class activation maps (CAM) and their variants (e.g.,
Grad-CAM and Relevance-CAM) have been extensively employed to explore the
interpretability of softmax-based convolutional neural networks, which require
a fully connected layer as the classifier for decision-making. However, these
methods cannot be directly applied to metric learning models, as such models
lack a fully connected layer functioning as a classifier. To address this
limitation, we propose a novel visual explanation method termed Similar Feature
Activation Map (SFAM). This method introduces the channel-wise contribution
importance score (CIS) to measure feature importance, derived from the
similarity measurement between two image embeddings. The explanation map is
constructed by linearly combining the proposed importance weights with the
feature map from a CNN model. Quantitative and qualitative experiments show
that SFAM provides highly promising interpretable visual explanations for CNN
models using Euclidean distance or cosine similarity as the similarity metric.

</details>


### [141] [Zoom-Refine: Boosting High-Resolution Multimodal Understanding via Localized Zoom and Self-Refinement](https://arxiv.org/abs/2506.01663)
*Xuan Yu,Dayan Guan,Michael Ying Yang,Yanfeng Gu*

Main category: cs.CV

TL;DR: Zoom-Refine是一种无需训练的方法，通过局部放大和自我细化提升多模态大语言模型（MLLM）对高分辨率图像的解析能力。


<details>
  <summary>Details</summary>
Motivation: MLLM在处理高分辨率图像时难以准确捕捉细粒度细节，影响复杂视觉理解。

Method: 结合局部放大（Localized Zoom）和自我细化（Self-Refinement），利用MLLM的初步响应定位关键区域，并通过高分辨率裁剪重新评估和优化结果。

Result: 在两个高分辨率多模态基准测试中验证了Zoom-Refine的有效性。

Conclusion: Zoom-Refine无需额外训练或外部专家，显著提升了MLLM对高分辨率图像的理解能力。

Abstract: Multimodal Large Language Models (MLLM) often struggle to interpret
high-resolution images accurately, where fine-grained details are crucial for
complex visual understanding. We introduce Zoom-Refine, a novel training-free
method that enhances MLLM capabilities to address this issue. Zoom-Refine
operates through a synergistic process of \textit{Localized Zoom} and
\textit{Self-Refinement}. In the \textit{Localized Zoom} step, Zoom-Refine
leverages the MLLM to provide a preliminary response to an input query and
identifies the most task-relevant image region by predicting its bounding box
coordinates. During the \textit{Self-Refinement} step, Zoom-Refine then
integrates fine-grained details from the high-resolution crop (identified by
\textit{Localized Zoom}) with its initial reasoning to re-evaluate and refine
its preliminary response. Our method harnesses the MLLM's inherent capabilities
for spatial localization, contextual reasoning and comparative analysis without
requiring additional training or external experts. Comprehensive experiments
demonstrate the efficacy of Zoom-Refine on two challenging high-resolution
multimodal benchmarks. Code is available at
\href{https://github.com/xavier-yu114/Zoom-Refine}{\color{magenta}github.com/xavier-yu114/Zoom-Refine}

</details>


### [142] [EarthMind: Towards Multi-Granular and Multi-Sensor Earth Observation with Large Multimodal Models](https://arxiv.org/abs/2506.01667)
*Yan Shu,Bin Ren,Zhitong Xiong,Danda Pani Paudel,Luc Van Gool,Begum Demir,Nicu Sebe,Paolo Rota*

Main category: cs.CV

TL;DR: EarthMind是一个新型视觉语言框架，用于多粒度和多传感器地球观测数据理解，通过空间注意力提示和跨模态融合提升性能，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的大型多模态模型在地球观测数据理解方面表现不足，而地球观测数据对环境监测至关重要。

Method: EarthMind包含两个核心组件：空间注意力提示（SAP）和跨模态融合，分别增强像素级理解和模态对齐。

Result: EarthMind在EarthMind-Bench和多个公共基准测试中表现优异，超越GPT-4o和其他现有方法。

Conclusion: EarthMind展示了在多粒度和多传感器挑战中的潜力，为地球观测数据理解提供了统一框架。

Abstract: Large Multimodal Models (LMMs) have demonstrated strong performance in
various vision-language tasks. However, they often struggle to comprehensively
understand Earth Observation (EO) data, which is critical for monitoring the
environment and the effects of human activity on it. In this work, we present
EarthMind, a novel vision-language framework for multi-granular and
multi-sensor EO data understanding. EarthMind features two core components: (1)
Spatial Attention Prompting (SAP), which reallocates attention within the LLM
to enhance pixel-level understanding; and (2) Cross-modal Fusion, which aligns
heterogeneous modalities into a shared space and adaptively reweighs tokens
based on their information density for effective fusion. To facilitate
multi-sensor fusion evaluation, we propose EarthMind-Bench, a comprehensive
benchmark with over 2,000 human-annotated multi-sensor image-question pairs,
covering a wide range of perception and reasoning tasks. Extensive experiments
demonstrate the effectiveness of EarthMind. It achieves state-of-the-art
performance on EarthMind-Bench, surpassing GPT-4o despite being only 4B in
scale. Moreover, EarthMind outperforms existing methods on multiple public EO
benchmarks, showcasing its potential to handle both multi-granular and
multi-sensor challenges in a unified framework.

</details>


### [143] [MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal LLMs](https://arxiv.org/abs/2506.01674)
*Yipeng Du,Tiehan Fan,Kepan Nan,Rui Xie,Penghao Zhou,Xiang Li,Jian Yang,Zhenheng Yang,Ying Tai*

Main category: cs.CV

TL;DR: MotionSight提出了一种零样本方法，通过视觉提示提升多模态大语言模型（MLLMs）在细粒度视频运动理解中的表现，并发布了首个大规模数据集MotionVid-QA。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在细粒度视频运动理解中存在局限，缺乏帧间差异分析能力，且视觉提示在视频中的应用尚未充分探索。

Method: 提出MotionSight，利用物体中心视觉聚焦和运动模糊作为视觉提示，无需训练即可提升运动理解。

Result: MotionSight在开源模型中表现最佳，与商业模型竞争，并通过MotionVid-QA数据集验证了其有效性。

Conclusion: MotionSight为零样本细粒度运动理解提供了新方法，并贡献了高质量数据集，推动了该领域的发展。

Abstract: Despite advancements in Multimodal Large Language Models (MLLMs), their
proficiency in fine-grained video motion understanding remains critically
limited. They often lack inter-frame differencing and tend to average or ignore
subtle visual cues. Furthermore, while visual prompting has shown potential in
static images, its application to video's temporal complexities, particularly
for fine-grained motion understanding, remains largely unexplored. We
investigate whether inherent capability can be unlocked and boost MLLMs' motion
perception and enable distinct visual signatures tailored to decouple object
and camera motion cues. In this study, we introduce MotionSight, a novel
zero-shot method pioneering object-centric visual spotlight and motion blur as
visual prompts to effectively improve fine-grained motion understanding without
training. To convert this into valuable data assets, we curated MotionVid-QA,
the first large-scale dataset for fine-grained video motion understanding, with
hierarchical annotations including SFT and preference data, {\Theta}(40K) video
clips and {\Theta}(87K) QAs. Experiments show MotionSight achieves
state-of-the-art open-source performance and competitiveness with commercial
models. In particular, for fine-grained motion understanding we present a novel
zero-shot technique and a large-scale, high-quality dataset. All the code and
annotations will be publicly available.

</details>


### [144] [SteerPose: Simultaneous Extrinsic Camera Calibration and Matching from Articulation](https://arxiv.org/abs/2506.01691)
*Sang-Eun Lee,Ko Nishino,Shohei Nobuhara*

Main category: cs.CV

TL;DR: SteerPose是一种神经网络方法，通过旋转2D姿态进行多相机系统的校准和对应关系搜索，同时引入几何一致性损失确保有效性。


<details>
  <summary>Details</summary>
Motivation: 受人类通过心理旋转对齐多视角姿态的启发，提出一种无需人工校准的方法。

Method: 结合可微分匹配和几何一致性损失，统一框架下完成相机校准和对应关系搜索。

Result: 在多样化的野外数据集中验证了方法的有效性和鲁棒性，并成功重建新动物的3D姿态。

Conclusion: SteerPose为多相机系统提供了一种无需人工校准的自动化解决方案。

Abstract: Can freely moving humans or animals themselves serve as calibration targets
for multi-camera systems while simultaneously estimating their correspondences
across views? We humans can solve this problem by mentally rotating the
observed 2D poses and aligning them with those in the target views. Inspired by
this cognitive ability, we propose SteerPose, a neural network that performs
this rotation of 2D poses into another view. By integrating differentiable
matching, SteerPose simultaneously performs extrinsic camera calibration and
correspondence search within a single unified framework. We also introduce a
novel geometric consistency loss that explicitly ensures that the estimated
rotation and correspondences result in a valid translation estimation.
Experimental results on diverse in-the-wild datasets of humans and animals
validate the effectiveness and robustness of the proposed method. Furthermore,
we demonstrate that our method can reconstruct the 3D poses of novel animals in
multi-camera setups by leveraging off-the-shelf 2D pose estimators and our
class-agnostic model.

</details>


### [145] [Data Pruning by Information Maximization](https://arxiv.org/abs/2506.01701)
*Haoru Tan,Sitong Wu,Wei Huang,Shizhen Zhao,Xiaojuan Qi*

Main category: cs.CV

TL;DR: InfoMax是一种新型数据修剪方法，通过最大化信息内容和最小化冗余来优化核心集。


<details>
  <summary>Details</summary>
Motivation: 提高核心集的信息量，同时减少冗余样本的影响。

Method: 使用重要性评分衡量样本信息，通过相似性量化冗余，将问题建模为离散二次规划任务，并采用梯度求解器和稀疏化技术。

Result: 在图像分类、视觉语言预训练和大语言模型指令调优等任务中表现优异。

Conclusion: InfoMax是一种高效且可扩展的数据修剪方法，适用于大规模数据集。

Abstract: In this paper, we present InfoMax, a novel data pruning method, also known as
coreset selection, designed to maximize the information content of selected
samples while minimizing redundancy. By doing so, InfoMax enhances the overall
informativeness of the coreset. The information of individual samples is
measured by importance scores, which capture their influence or difficulty in
model learning. To quantify redundancy, we use pairwise sample similarities,
based on the premise that similar samples contribute similarly to the learning
process. We formalize the coreset selection problem as a discrete quadratic
programming (DQP) task, with the objective of maximizing the total information
content, represented as the sum of individual sample contributions minus the
redundancies introduced by similar samples within the coreset. To ensure
practical scalability, we introduce an efficient gradient-based solver,
complemented by sparsification techniques applied to the similarity matrix and
dataset partitioning strategies. This enables InfoMax to seamlessly scale to
datasets with millions of samples. Extensive experiments demonstrate the
superior performance of InfoMax in various data pruning tasks, including image
classification, vision-language pre-training, and instruction tuning for large
language models.

</details>


### [146] [Active Learning via Vision-Language Model Adaptation with Open Data](https://arxiv.org/abs/2506.01724)
*Tong Wang,Jiaqi Wang,Shu Kong*

Main category: cs.CV

TL;DR: 本文提出了一种名为ALOR的方法，利用公开数据和视觉语言模型（VLM）改进主动学习（AL），并通过对比调优（CT）和尾部优先采样（TFS）策略显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 减少数据标注成本并利用公开数据提升主动学习效果。

Method: 结合公开数据检索任务相关样本，对比调优（CT）和尾部优先采样（TFS）策略。

Result: ALOR方法显著优于现有方法，CT表现最佳。

Conclusion: 公开数据和CT调优结合TFS策略能有效提升主动学习性能。

Abstract: Pretrained on web-scale open data, VLMs offer powerful capabilities for
solving downstream tasks after being adapted to task-specific labeled data.
Yet, data labeling can be expensive and may demand domain expertise. Active
Learning (AL) aims to reduce this expense by strategically selecting the most
informative data for labeling and model training. Recent AL methods have
explored VLMs but have not leveraged publicly available open data, such as
VLM's pretraining data. In this work, we leverage such data by retrieving
task-relevant examples to augment the task-specific examples. As expected,
incorporating them significantly improves AL. Given that our method exploits
open-source VLM and open data, we refer to it as Active Learning with Open
Resources (ALOR). Additionally, most VLM-based AL methods use prompt tuning
(PT) for model adaptation, likely due to its ability to directly utilize
pretrained parameters and the assumption that doing so reduces the risk of
overfitting to limited labeled data. We rigorously compare popular adaptation
approaches, including linear probing (LP), finetuning (FT), and contrastive
tuning (CT). We reveal two key findings: (1) All adaptation approaches benefit
from incorporating retrieved data, and (2) CT resoundingly outperforms other
approaches across AL methods. Further analysis of retrieved data reveals a
naturally imbalanced distribution of task-relevant classes, exposing inherent
biases within the VLM. This motivates our novel Tail First Sampling (TFS)
strategy for AL, an embarrassingly simple yet effective method that prioritizes
sampling data from underrepresented classes to label. Extensive experiments
demonstrate that our final method, contrastively finetuning VLM on both
retrieved and TFS-selected labeled data, significantly outperforms existing
methods.

</details>


### [147] [VideoCap-R1: Enhancing MLLMs for Video Captioning via Structured Thinking](https://arxiv.org/abs/2506.01725)
*Desen Meng,Rui Huang,Zhilin Dai,Xinhao Li,Yifan Xu,Jun Zhang,Zhenpeng Huang,Meng Zhang,Lingshu Zhang,Yi Liu,Limin Wang*

Main category: cs.CV

TL;DR: 论文首次系统研究了基于GRPO的强化学习后训练方法，用于提升多模态大语言模型（MLLMs）在视频字幕生成中的动作描述能力，提出VideoCap-R1模型，通过结构化思维和奖励机制显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习在多模态大语言模型（MLLMs）的视频字幕生成领域研究不足，尤其是动作描述的准确性有待提升。

Method: 开发VideoCap-R1模型，采用结构化思维分析视频主体及其属性和动作，并设计两种奖励机制（LLM-free思维评分器和LLM辅助字幕评分器）进行强化学习训练。

Result: 实验表明，VideoCap-R1在多个视频字幕基准测试中显著优于基线模型（如DREAM1K事件F1提升4.4，VDC准确率提升4.2），且优于仅通过监督微调的模型。

Conclusion: GRPO强化学习方法有效提升了MLLMs在视频字幕生成中的动作描述能力，验证了结构化推理与生成任务关联的重要性。

Abstract: While recent advances in reinforcement learning have significantly enhanced
reasoning capabilities in large language models (LLMs), these techniques remain
underexplored in multi-modal LLMs for video captioning. This paper presents the
first systematic investigation of GRPO-based RL post-training for video MLLMs,
with the goal of enhancing video MLLMs' capability of describing actions in
videos. Specifically, we develop the VideoCap-R1, which is prompted to first
perform structured thinking that analyzes video subjects with their attributes
and actions before generating complete captions, supported by two specialized
reward mechanisms: a LLM-free think scorer evaluating the structured thinking
quality and a LLM-assisted caption scorer assessing the output quality. The RL
training framework effectively establishes the connection between structured
reasoning and comprehensive description generation, enabling the model to
produce captions with more accurate actions. Our experiments demonstrate that
VideoCap-R1 achieves substantial improvements over the Qwen2VL-7B baseline
using limited samples (1.5k) across multiple video caption benchmarks (DREAM1K:
+4.4 event F1, VDC: +4.2 Acc, CAREBENCH: +3.1 action F1, +6.9 object F1) while
consistently outperforming the SFT-trained counterparts, confirming GRPO's
superiority in enhancing MLLMs' captioning capabilities.

</details>


### [148] [STORM: Benchmarking Visual Rating of MLLMs with a Comprehensive Ordinal Regression Dataset](https://arxiv.org/abs/2506.01738)
*Jinhong Wang,Shuo Tong,Jian liu,Dongqi Tang,Jintai Chen,Haochao Ying,Hongxia Xu,Danny Chen,Jian Wu*

Main category: cs.CV

TL;DR: STORM是一个用于评估多模态大语言模型（MLLMs）在视觉评分任务中序数回归能力的数据集和基准，包含14个数据集和655K图像-标签对，并提出了一种粗到细的处理流程。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在视觉评分任务中表现不佳，且缺乏相关数据集和基准，因此需要STORM来填补这一空白。

Method: 收集14个序数回归数据集，提出动态考虑标签候选并提供可解释思考的粗到细处理流程。

Result: 实验证明该框架有效，并为更好的微调策略提供了启示。

Conclusion: STORM为MLLMs在视觉评分任务中的序数回归能力提供了通用且可信的评估基准。

Abstract: Visual rating is an essential capability of artificial intelligence (AI) for
multi-dimensional quantification of visual content, primarily applied in
ordinal regression (OR) tasks such as image quality assessment, facial age
estimation, and medical image grading. However, current multi-modal large
language models (MLLMs) under-perform in such visual rating ability while also
suffering the lack of relevant datasets and benchmarks. In this work, we
collect and present STORM, a data collection and benchmark for Stimulating
Trustworthy Ordinal Regression Ability of MLLMs for universal visual rating.
STORM encompasses 14 ordinal regression datasets across five common visual
rating domains, comprising 655K image-level pairs and the corresponding
carefully curated VQAs. Importantly, we also propose a coarse-to-fine
processing pipeline that dynamically considers label candidates and provides
interpretable thoughts, providing MLLMs with a general and trustworthy ordinal
thinking paradigm. This benchmark aims to evaluate the all-in-one and zero-shot
performance of MLLMs in scenarios requiring understanding of the essential
common ordinal relationships of rating labels. Extensive experiments
demonstrate the effectiveness of our framework and shed light on better
fine-tuning strategies. The STORM dataset, benchmark, and pre-trained models
are available on the following webpage to support further research in this
area. Datasets and codes are released on the project page:
https://storm-bench.github.io/.

</details>


### [149] [Efficient Egocentric Action Recognition with Multimodal Data](https://arxiv.org/abs/2506.01757)
*Marco Calzavara,Ard Kastrati,Matteo Macchini,Dushan Vasilevski,Roger Wattenhofer*

Main category: cs.CV

TL;DR: 通过分析RGB视频和3D手部姿态的采样频率对CPU使用率和动作识别性能的影响，研究发现降低RGB帧采样率并结合高频率3D手部姿态输入可显著减少CPU需求，同时保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴XR设备的普及，实现实时Egocentric Action Recognition (EAR)面临便携性、电池寿命和计算资源之间的权衡挑战。

Method: 系统分析RGB视频和3D手部姿态在不同采样频率下的性能与CPU使用率，探索多种配置以权衡准确性与计算效率。

Result: 降低RGB帧采样率并辅以高频率3D手部姿态输入，可减少3倍CPU使用率，同时几乎不影响识别性能。

Conclusion: 多模态输入策略是实现高效实时EAR的可行方法，尤其适用于资源受限的XR设备。

Abstract: The increasing availability of wearable XR devices opens new perspectives for
Egocentric Action Recognition (EAR) systems, which can provide deeper human
understanding and situation awareness. However, deploying real-time algorithms
on these devices can be challenging due to the inherent trade-offs between
portability, battery life, and computational resources. In this work, we
systematically analyze the impact of sampling frequency across different input
modalities - RGB video and 3D hand pose - on egocentric action recognition
performance and CPU usage. By exploring a range of configurations, we provide a
comprehensive characterization of the trade-offs between accuracy and
computational efficiency. Our findings reveal that reducing the sampling rate
of RGB frames, when complemented with higher-frequency 3D hand pose input, can
preserve high accuracy while significantly lowering CPU demands. Notably, we
observe up to a 3x reduction in CPU usage with minimal to no loss in
recognition performance. This highlights the potential of multimodal input
strategies as a viable approach to achieving efficient, real-time EAR on XR
devices.

</details>


### [150] [Many-for-Many: Unify the Training of Multiple Video and Image Generation and Manipulation Tasks](https://arxiv.org/abs/2506.01758)
*Tao Yang,Ruibin Li,Yangming Shi,Yuqi Zhang,Qide Dong,Haoran Cheng,Weiguo Feng,Shilei Wen,Bingyue Peng,Lei Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为many-for-many的统一框架，通过联合学习策略训练单一模型，支持多种视觉生成和操作任务，显著提升了视频生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为单一任务训练，且高质量标注数据成本高，需要一种统一且高效的解决方案。

Method: 设计轻量级适配器统一不同任务条件，采用联合图像-视频学习策略，并引入深度图作为条件。

Result: 训练了8B和2B两个版本模型，支持10多种任务，8B模型在视频生成任务中表现优异。

Conclusion: 该框架有效解决了多任务统一训练问题，显著提升了性能，具有广泛应用潜力。

Abstract: Diffusion models have shown impressive performance in many visual generation
and manipulation tasks. Many existing methods focus on training a model for a
specific task, especially, text-to-video (T2V) generation, while many other
works focus on finetuning the pretrained T2V model for image-to-video (I2V),
video-to-video (V2V), image and video manipulation tasks, etc. However,
training a strong T2V foundation model requires a large amount of high-quality
annotations, which is very costly. In addition, many existing models can
perform only one or several tasks. In this work, we introduce a unified
framework, namely many-for-many, which leverages the available training data
from many different visual generation and manipulation tasks to train a single
model for those different tasks. Specifically, we design a lightweight adapter
to unify the different conditions in different tasks, then employ a joint
image-video learning strategy to progressively train the model from scratch.
Our joint learning leads to a unified visual generation and manipulation model
with improved video generation performance. In addition, we introduce depth
maps as a condition to help our model better perceive the 3D space in visual
generation. Two versions of our model are trained with different model sizes
(8B and 2B), each of which can perform more than 10 different tasks. In
particular, our 8B model demonstrates highly competitive performance in video
generation tasks compared to open-source and even commercial engines. Our
models and source codes are available at https://github.com/leeruibin/MfM.git.

</details>


### [151] [unMORE: Unsupervised Multi-Object Segmentation via Center-Boundary Reasoning](https://arxiv.org/abs/2506.01778)
*Yafei Yang,Zihui Zhang,Bo Yang*

Main category: cs.CV

TL;DR: 论文提出了一种名为unMORE的两阶段无监督多目标分割方法，通过学习对象中心表示和多目标推理模块，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分割复杂真实世界对象时表现有限，因此需要一种更有效的方法。

Method: unMORE采用两阶段流程：第一阶段学习对象中心表示，第二阶段通过无网络推理模块发现多目标。

Result: 在6个真实世界数据集上表现优异，尤其在拥挤图像中优于所有基线方法。

Conclusion: unMORE为无监督多目标分割提供了高效解决方案，适用于复杂场景。

Abstract: We study the challenging problem of unsupervised multi-object segmentation on
single images. Existing methods, which rely on image reconstruction objectives
to learn objectness or leverage pretrained image features to group similar
pixels, often succeed only in segmenting simple synthetic objects or
discovering a limited number of real-world objects. In this paper, we introduce
unMORE, a novel two-stage pipeline designed to identify many complex objects in
real-world images. The key to our approach involves explicitly learning three
levels of carefully defined object-centric representations in the first stage.
Subsequently, our multi-object reasoning module utilizes these learned object
priors to discover multiple objects in the second stage. Notably, this
reasoning module is entirely network-free and does not require human labels.
Extensive experiments demonstrate that unMORE significantly outperforms all
existing unsupervised methods across 6 real-world benchmark datasets, including
the challenging COCO dataset, achieving state-of-the-art object segmentation
results. Remarkably, our method excels in crowded images where all baselines
collapse.

</details>


### [152] [FaceCoT: A Benchmark Dataset for Face Anti-Spoofing with Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.01783)
*Honglu Zhang,Zhiqin Fang,Ningning Zhao,Saihui Hou,Long Ma,Renwang Pei,Zhaofeng He*

Main category: cs.CV

TL;DR: 论文提出FaceCoT数据集和CEPL策略，结合视觉与语言多模态提升人脸防伪（FAS）的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统FAS依赖单一视觉模态，泛化能力有限；多模态大语言模型（MLLMs）的突破为结合视觉与语言推理提供了可能，但缺乏高质量数据集。

Method: 构建FaceCoT数据集（覆盖14种攻击类型的高质量VQA标注），开发强化学习优化的标注模型，并提出CEPL策略以利用CoT数据提升FAS性能。

Result: 实验表明，基于FaceCoT和CEPL训练的模型在多个基准数据集上优于现有方法。

Conclusion: FaceCoT和CEPL为FAS任务提供了更鲁棒和可解释的解决方案。

Abstract: Face Anti-Spoofing (FAS) typically depends on a single visual modality when
defending against presentation attacks such as print attacks, screen replays,
and 3D masks, resulting in limited generalization across devices, environments,
and attack types. Meanwhile, Multimodal Large Language Models (MLLMs) have
recently achieved breakthroughs in image-text understanding and semantic
reasoning, suggesting that integrating visual and linguistic co-inference into
FAS can substantially improve both robustness and interpretability. However,
the lack of a high-quality vision-language multimodal dataset has been a
critical bottleneck. To address this, we introduce FaceCoT (Face
Chain-of-Thought), the first large-scale Visual Question Answering (VQA)
dataset tailored for FAS. FaceCoT covers 14 spoofing attack types and enriches
model learning with high-quality CoT VQA annotations. Meanwhile, we develop a
caption model refined via reinforcement learning to expand the dataset and
enhance annotation quality. Furthermore, we introduce a CoT-Enhanced
Progressive Learning (CEPL) strategy to better leverage the CoT data and boost
model performance on FAS tasks. Extensive experiments demonstrate that models
trained with FaceCoT and CEPL outperform state-of-the-art methods on multiple
benchmark datasets.

</details>


### [153] [R2SM: Referring and Reasoning for Selective Masks](https://arxiv.org/abs/2506.01795)
*Yu-Lin Shih,Wei-En Tai,Cheng Sun,Yu-Chiang Frank Wang,Hwann-Tzong Chen*

Main category: cs.CV

TL;DR: 论文提出新任务R2SM，结合文本引导分割和用户意图驱动的掩模类型选择，挑战视觉语言模型基于自然语言提示生成模态或非模态分割掩模。


<details>
  <summary>Details</summary>
Motivation: 扩展文本引导分割任务，使其能够根据用户意图选择生成模态（可见）或非模态（完整）分割掩模，提升模型的多模态推理和意图感知能力。

Method: 构建R2SM数据集，通过增强COCOA-cls、D2SA和MUVA的标注，包含模态和非模态文本查询及对应掩模，用于模型微调和评估。

Result: R2SM任务要求模型根据提示判断生成模态或非模态掩模，例如提示明确要求完整形状时生成非模态掩模。

Conclusion: R2SM为多模态推理和意图感知分割研究提供了具有挑战性和洞察力的测试平台。

Abstract: We introduce a new task, Referring and Reasoning for Selective Masks (R2SM),
which extends text-guided segmentation by incorporating mask-type selection
driven by user intent. This task challenges vision-language models to determine
whether to generate a modal (visible) or amodal (complete) segmentation mask
based solely on natural language prompts. To support the R2SM task, we present
the R2SM dataset, constructed by augmenting annotations of COCOA-cls, D2SA, and
MUVA. The R2SM dataset consists of both modal and amodal text queries, each
paired with the corresponding ground-truth mask, enabling model finetuning and
evaluation for the ability to segment images as per user intent. Specifically,
the task requires the model to interpret whether a given prompt refers to only
the visible part of an object or to its complete shape, including occluded
regions, and then produce the appropriate segmentation. For example, if a
prompt explicitly requests the whole shape of a partially hidden object, the
model is expected to output an amodal mask that completes the occluded parts.
In contrast, prompts without explicit mention of hidden regions should generate
standard modal masks. The R2SM benchmark provides a challenging and insightful
testbed for advancing research in multimodal reasoning and intent-aware
segmentation.

</details>


### [154] [WorldExplorer: Towards Generating Fully Navigable 3D Scenes](https://arxiv.org/abs/2506.01799)
*Manuel-Andreas Schneider,Lukas Höllein,Matthias Nießner*

Main category: cs.CV

TL;DR: WorldExplorer提出了一种基于自回归视频轨迹生成的新方法，用于生成高质量、可导航的3D场景，解决了现有方法在视角扩展时的噪声和拉伸问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成3D场景时，视角受限且容易产生噪声和拉伸问题，无法实现自由探索。WorldExplorer旨在解决这些问题，生成高质量、稳定的3D场景。

Method: 通过多视角一致的360度全景图初始化场景，利用视频扩散模型迭代生成沿预定义轨迹的视频，结合场景记忆和碰撞检测机制，最终通过3D高斯泼溅优化融合所有视图。

Result: WorldExplorer生成的3D场景在较大相机运动下仍保持高质量和稳定性，首次实现了真实且无限制的探索。

Conclusion: 该方法标志着向生成沉浸式、可自由探索的虚拟3D环境迈出了重要一步。

Abstract: Generating 3D worlds from text is a highly anticipated goal in computer
vision. Existing works are limited by the degree of exploration they allow
inside of a scene, i.e., produce streched-out and noisy artifacts when moving
beyond central or panoramic perspectives. To this end, we propose
WorldExplorer, a novel method based on autoregressive video trajectory
generation, which builds fully navigable 3D scenes with consistent visual
quality across a wide range of viewpoints. We initialize our scenes by creating
multi-view consistent images corresponding to a 360 degree panorama. Then, we
expand it by leveraging video diffusion models in an iterative scene generation
pipeline. Concretely, we generate multiple videos along short, pre-defined
trajectories, that explore the scene in depth, including motion around objects.
Our novel scene memory conditions each video on the most relevant prior views,
while a collision-detection mechanism prevents degenerate results, like moving
into objects. Finally, we fuse all generated views into a unified 3D
representation via 3D Gaussian Splatting optimization. Compared to prior
approaches, WorldExplorer produces high-quality scenes that remain stable under
large camera motion, enabling for the first time realistic and unrestricted
exploration. We believe this marks a significant step toward generating
immersive and truly explorable virtual 3D environments.

</details>


### [155] [OmniV2V: Versatile Video Generation and Editing via Dynamic Content Manipulation](https://arxiv.org/abs/2506.01801)
*Sen Liang,Zhentao Yu,Zhengguang Zhou,Teng Hu,Hongmei Wang,Yi Chen,Qin Lin,Yuan Zhou,Xin Li,Qinglin Lu,Zhibo Chen*

Main category: cs.CV

TL;DR: OmniV2V是一个多功能视频生成和编辑模型，支持跨场景操作，通过统一的动态内容注入模块和视觉-文本指令模块实现高效任务处理。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型局限于单一场景，无法实现多样化视频生成和编辑。

Method: 提出统一的动态内容注入模块和基于LLaVA的视觉-文本指令模块，并构建多任务数据处理系统。

Result: OmniV2V在多项视频生成和编辑任务中表现优于或与现有最佳开源和商业模型相当。

Conclusion: OmniV2V通过统一模块和多任务数据处理系统，实现了多样化视频生成和编辑的高效性能。

Abstract: The emergence of Diffusion Transformers (DiT) has brought significant
advancements to video generation, especially in text-to-video and
image-to-video tasks. Although video generation is widely applied in various
fields, most existing models are limited to single scenarios and cannot perform
diverse video generation and editing through dynamic content manipulation. We
propose OmniV2V, a video model capable of generating and editing videos across
different scenarios based on various operations, including: object movement,
object addition, mask-guided video edit, try-on, inpainting, outpainting, human
animation, and controllable character video synthesis. We explore a unified
dynamic content manipulation injection module, which effectively integrates the
requirements of the above tasks. In addition, we design a visual-text
instruction module based on LLaVA, enabling the model to effectively understand
the correspondence between visual content and instructions. Furthermore, we
build a comprehensive multi-task data processing system. Since there is data
overlap among various tasks, this system can efficiently provide data
augmentation. Using this system, we construct a multi-type, multi-scenario
OmniV2V dataset and its corresponding OmniV2V-Test benchmark. Extensive
experiments show that OmniV2V works as well as, and sometimes better than, the
best existing open-source and commercial models for many video generation and
editing tasks.

</details>


### [156] [UMA: Ultra-detailed Human Avatars via Multi-level Surface Alignment](https://arxiv.org/abs/2506.01802)
*Heming Zhu,Guoxing Sun,Christian Theobalt,Marc Habermann*

Main category: cs.CV

TL;DR: 提出了一种基于隐式表示和2D视频点跟踪的动画化人体模型，通过潜在变形模型和级联训练策略，显著提升了渲染质量和几何精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于隐式表示的动画化人体模型在细节保留上存在不足，尤其是在高分辨率渲染时，主要原因是几何跟踪不准确。

Method: 提出潜在变形模型，利用2D视频点跟踪器指导3D变形，并通过级联训练策略生成一致的3D点轨迹。

Result: 在包含挑战性纹理和褶皱变形的数据集上，渲染质量和几何精度显著优于现有方法。

Conclusion: 该方法通过改进几何跟踪和变形模型，有效提升了动画化人体模型的细节表现和高分辨率渲染能力。

Abstract: Learning an animatable and clothed human avatar model with vivid dynamics and
photorealistic appearance from multi-view videos is an important foundational
research problem in computer graphics and vision. Fueled by recent advances in
implicit representations, the quality of the animatable avatars has achieved an
unprecedented level by attaching the implicit representation to drivable human
template meshes. However, they usually fail to preserve the highest level of
detail, particularly apparent when the virtual camera is zoomed in and when
rendering at 4K resolution and higher. We argue that this limitation stems from
inaccurate surface tracking, specifically, depth misalignment and surface drift
between character geometry and the ground truth surface, which forces the
detailed appearance model to compensate for geometric errors. To address this,
we propose a latent deformation model and supervising the 3D deformation of the
animatable character using guidance from foundational 2D video point trackers,
which offer improved robustness to shading and surface variations, and are less
prone to local minima than differentiable rendering. To mitigate the drift over
time and lack of 3D awareness of 2D point trackers, we introduce a cascaded
training strategy that generates consistent 3D point tracks by anchoring point
tracks to the rendered avatar, which ultimately supervises our avatar at the
vertex and texel level. To validate the effectiveness of our approach, we
introduce a novel dataset comprising five multi-view video sequences, each over
10 minutes in duration, captured using 40 calibrated 6K-resolution cameras,
featuring subjects dressed in clothing with challenging texture patterns and
wrinkle deformations. Our approach demonstrates significantly improved
performance in rendering quality and geometric accuracy over the prior state of
the art.

</details>


### [157] [Ridgeformer: Mutli-Stage Contrastive Training For Fine-grained Cross-Domain Fingerprint Recognition](https://arxiv.org/abs/2506.01806)
*Shubham Pandey,Bhavin Jawade,Srirangaraj Setlur*

Main category: cs.CV

TL;DR: 提出了一种基于多阶段Transformer的无接触指纹匹配方法，解决了图像模糊、对比度低等问题，显著提升了匹配精度。


<details>
  <summary>Details</summary>
Motivation: 无接触指纹识别需求增长，但面临图像模糊、对比度低、手指位置变化等挑战，亟需提升技术可靠性。

Method: 采用多阶段Transformer方法，先提取全局空间特征，再细化局部特征对齐，结合分层特征提取与匹配流程。

Result: 在HKPolyU和RidgeBase数据集上测试，优于现有方法，包括商业解决方案。

Conclusion: 该方法显著提升了无接触指纹匹配的准确性和鲁棒性，具有实际应用潜力。

Abstract: The increasing demand for hygienic and portable biometric systems has
underscored the critical need for advancements in contactless fingerprint
recognition. Despite its potential, this technology faces notable challenges,
including out-of-focus image acquisition, reduced contrast between fingerprint
ridges and valleys, variations in finger positioning, and perspective
distortion. These factors significantly hinder the accuracy and reliability of
contactless fingerprint matching. To address these issues, we propose a novel
multi-stage transformer-based contactless fingerprint matching approach that
first captures global spatial features and subsequently refines localized
feature alignment across fingerprint samples. By employing a hierarchical
feature extraction and matching pipeline, our method ensures fine-grained,
cross-sample alignment while maintaining the robustness of global feature
representation. We perform extensive evaluations on publicly available datasets
such as HKPolyU and RidgeBase under different evaluation protocols, such as
contactless-to-contact matching and contactless-to-contactless matching and
demonstrate that our proposed approach outperforms existing methods, including
COTS solutions.

</details>


### [158] [GSCodec Studio: A Modular Framework for Gaussian Splat Compression](https://arxiv.org/abs/2506.01822)
*Sicheng Li,Chengzhen Wu,Hao Li,Xiang Gao,Yiyi Liao,Lu Yu*

Main category: cs.CV

TL;DR: GSCodec Studio是一个统一的模块化框架，用于高斯溅射（GS）的重建、压缩和渲染，解决了现有方法分散的问题，并提供了高效的静态和动态GS压缩方案。


<details>
  <summary>Details</summary>
Motivation: 高斯溅射（GS）在实时渲染中表现出色，但其高存储需求限制了实际应用。现有压缩方法分散，缺乏统一框架。

Method: GSCodec Studio整合了多种3D/4D GS重建和压缩技术作为模块化组件，支持灵活组合和全面比较。

Result: 框架实现了静态和动态GS的高效压缩（Static和Dynamic GSCodec），在率失真性能上表现优异。

Conclusion: GSCodec Studio通过统一框架和开源代码推动了GS压缩研究的发展。

Abstract: 3D Gaussian Splatting and its extension to 4D dynamic scenes enable
photorealistic, real-time rendering from real-world captures, positioning
Gaussian Splats (GS) as a promising format for next-generation immersive media.
However, their high storage requirements pose significant challenges for
practical use in sharing, transmission, and storage. Despite various studies
exploring GS compression from different perspectives, these efforts remain
scattered across separate repositories, complicating benchmarking and the
integration of best practices. To address this gap, we present GSCodec Studio,
a unified and modular framework for GS reconstruction, compression, and
rendering. The framework incorporates a diverse set of 3D/4D GS reconstruction
methods and GS compression techniques as modular components, facilitating
flexible combinations and comprehensive comparisons. By integrating best
practices from community research and our own explorations, GSCodec Studio
supports the development of compact representation and compression solutions
for static and dynamic Gaussian Splats, namely our Static and Dynamic GSCodec,
achieving competitive rate-distortion performance in static and dynamic GS
compression. The code for our framework is publicly available at
https://github.com/JasonLSC/GSCodec_Studio , to advance the research on
Gaussian Splats compression.

</details>


### [159] [MoDA: Modulation Adapter for Fine-Grained Visual Grounding in Instructional MLLMs](https://arxiv.org/abs/2506.01850)
*Wayner Barrios,Andrés Villa,Juan León Alcázar,SouYoung Jin,Bernard Ghanem*

Main category: cs.CV

TL;DR: MoDA（Modulation Adapter）是一种轻量级模块，通过指令引导的调制优化预对齐视觉特征，提升多模态大语言模型（MLLMs）在复杂场景中的细粒度视觉概念理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂场景中难以准确理解细粒度视觉概念，MoDA旨在通过指令引导的调制解决这一问题。

Method: 采用两阶段训练：1）通过冻结视觉编码器和适配层对齐图像特征；2）在指令调优阶段使用MoDA模块，通过Transformer跨注意力机制生成调制掩码，优化视觉特征。

Result: 实验表明，MoDA提升了视觉定位能力，并生成更符合上下文的响应。

Conclusion: MoDA是一种通用增强方法，可有效提升基于图像的MLLMs性能。

Abstract: Recently, Multimodal Large Language Models (MLLMs) have demonstrated
impressive performance on instruction-following tasks by integrating pretrained
visual encoders with large language models (LLMs). However, existing approaches
often struggle to ground fine-grained visual concepts in complex scenes. In
this paper, we propose MoDA (Modulation Adapter), a lightweight yet effective
module designed to refine pre-aligned visual features through
instruction-guided modulation. Our approach follows the standard LLaVA training
protocol, consisting of a two-stage process: (1) aligning image features to the
LLMs input space via a frozen vision encoder and adapter layers, and (2)
refining those features using the MoDA adapter during the instructional tuning
stage. MoDA employs a Transformer-based cross-attention mechanism to generate a
modulation mask over the aligned visual tokens, thereby emphasizing
semantically relevant embedding dimensions based on the language instruction.
The modulated features are then passed to the LLM for autoregressive language
generation. Our experimental evaluation shows that MoDA improves visual
grounding and generates more contextually appropriate responses, demonstrating
its effectiveness as a general-purpose enhancement for image-based MLLMs.

</details>


### [160] [ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding](https://arxiv.org/abs/2506.01853)
*Junliang Ye,Zhengyi Wang,Ruowen Zhao,Shenghao Xie,Jun Zhu*

Main category: cs.CV

TL;DR: 论文提出ShapeLLM-Omni，一种原生3D大型语言模型，填补了多模态模型在3D内容理解与生成上的空白。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型（如ChatGPT-4o）仅限于图像和文本，而3D内容的理解与生成同样重要。

Method: 训练3D VQVAE映射3D对象到离散潜在空间，构建3D-Alpaca数据集，并在Qwen-2.5-vl-7B-Instruct模型上进行指令训练。

Result: ShapeLLM-Omni能够理解和生成3D资产与文本，为3D原生AI研究提供了基础。

Conclusion: 该研究为多模态模型扩展3D能力提供了有效尝试，推动了3D原生AI的未来发展。

Abstract: Recently, the powerful text-to-image capabilities of ChatGPT-4o have led to
growing appreciation for native multimodal large language models. However, its
multimodal capabilities remain confined to images and text. Yet beyond images,
the ability to understand and generate 3D content is equally crucial. To
address this gap, we propose ShapeLLM-Omni-a native 3D large language model
capable of understanding and generating 3D assets and text in any sequence.
First, we train a 3D vector-quantized variational autoencoder (VQVAE), which
maps 3D objects into a discrete latent space to achieve efficient and accurate
shape representation and reconstruction. Building upon the 3D-aware discrete
tokens, we innovatively construct a large-scale continuous training dataset
named 3D-Alpaca, encompassing generation, comprehension, and editing, thus
providing rich resources for future research and training. Finally, by
performing instruction-based training of the Qwen-2.5-vl-7B-Instruct model on
the 3D-Alpaca dataset. Our work provides an effective attempt at extending
multimodal models with basic 3D capabilities, which contributes to future
research in 3D-native AI. Project page:
https://github.com/JAMESYJL/ShapeLLM-Omni

</details>


### [161] [Enhancing Biomedical Multi-modal Representation Learning with Multi-scale Pre-training and Perturbed Report Discrimination](https://arxiv.org/abs/2506.01902)
*Xinliu Zhong,Kayhan Batmanghelich,Li Sun*

Main category: cs.CV

TL;DR: 提出了一种新的生物医学视觉语言模型预训练方法，通过扰动报告区分和对比学习，解决了现有方法忽视生物医学文本复杂语义的问题。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本具有复杂且领域特定的语义，现有对比学习方法常忽视这一点，导致预训练模型效果不佳。

Method: 提出扰动报告区分方法，通过文本扰动破坏语义结构，并对比注意力加权的图像子区域和子词，增强模型对多模态细粒度语义的敏感性。

Result: 在多个下游任务中表现优于基线方法，学习到更具语义意义和鲁棒性的多模态表示。

Conclusion: 该方法能有效提升生物医学视觉语言模型的预训练效果，适用于复杂语义场景。

Abstract: Vision-language models pre-trained on large scale of unlabeled biomedical
images and associated reports learn generalizable semantic representations.
These multi-modal representations can benefit various downstream tasks in the
biomedical domain. Contrastive learning is widely used to pre-train
vision-language models for general natural images and associated captions.
Despite its popularity, we found biomedical texts have complex and
domain-specific semantics that are often neglected by common contrastive
methods. To address this issue, we propose a novel method, perturbed report
discrimination, for pre-train biomedical vision-language models. First, we
curate a set of text perturbation methods that keep the same words, but disrupt
the semantic structure of the sentence. Next, we apply different types of
perturbation to reports, and use the model to distinguish the original report
from the perturbed ones given the associated image. Parallel to this, we
enhance the sensitivity of our method to higher level of granularity for both
modalities by contrasting attention-weighted image sub-regions and sub-words in
the image-text pairs. We conduct extensive experiments on multiple downstream
tasks, and our method outperforms strong baseline methods. The results
demonstrate that our approach learns more semantic meaningful and robust
multi-modal representations.

</details>


### [162] [Reinforcement Learning Tuning for VideoLLMs: Reward Design and Data Efficiency](https://arxiv.org/abs/2506.01908)
*Hongyu Li,Songhao Han,Yue Liao,Junfeng Luo,Jialin Gao,Shuicheng Yan,Si Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于强化学习调优（RLT）的后训练策略，通过双奖励机制提升多模态大语言模型（MLLMs）在视频理解任务中的语义和时间推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中视频的复杂语义和长时依赖问题，提升MLLMs在视频任务中的推理能力。

Method: 基于GRPO框架，设计双奖励机制（离散和连续信号），并提出方差感知数据选择策略优化训练样本。

Result: 在八项视频理解任务中表现优于监督微调和现有RLT基线，且训练数据需求更少。

Conclusion: 奖励设计和数据选择对提升MLLMs的视频推理能力至关重要，最新代码已更新并开源。

Abstract: Understanding real-world videos with complex semantics and long temporal
dependencies remains a fundamental challenge in computer vision. Recent
progress in multimodal large language models (MLLMs) has demonstrated strong
capabilities in vision-language tasks, while reinforcement learning tuning
(RLT) has further improved their reasoning abilities. In this work, we explore
RLT as a post-training strategy to enhance the video-specific reasoning
capabilities of MLLMs. Built upon the Group Relative Policy Optimization (GRPO)
framework, we propose a dual-reward formulation that supervises both semantic
and temporal reasoning through discrete and continuous reward signals. To
facilitate effective preference-based optimization, we introduce a
variance-aware data selection strategy based on repeated inference to identify
samples that provide informative learning signals. We evaluate our approach
across eight representative video understanding tasks, including VideoQA,
Temporal Video Grounding, and Grounded VideoQA. Our method consistently
outperforms supervised fine-tuning and existing RLT baselines, achieving
superior performance with significantly less training data. These results
underscore the importance of reward design and data selection in advancing
reasoning-centric video understanding with MLLMs. Notably, The initial code
release (two months ago) has now been expanded with updates, including
optimized reward mechanisms and additional datasets. The latest version is
available at https://github.com/appletea233/Temporal-R1 .

</details>


### [163] [Elucidating the representation of images within an unconditional diffusion model denoiser](https://arxiv.org/abs/2506.01912)
*Zahra Kadkhodaie,Stéphane Mallat,Eero Simoncelli*

Main category: cs.CV

TL;DR: 论文研究了UNet在ImageNet数据集上训练的降噪网络内部机制，发现其中间块将图像分解为稀疏通道，并提出了一种新的图像重建算法。


<details>
  <summary>Details</summary>
Motivation: 尽管生成扩散模型在图像生成上表现出色，但其内部评分网络的机制尚不明确。

Method: 通过分析UNet中间块的稀疏通道分解，提出了一种基于空间平均向量的图像重建算法。

Result: 实验表明，潜在空间中的欧氏距离与图像语义相似性相关，聚类分析揭示了图像细节与全局结构的共享特征。

Conclusion: 研究发现，仅通过降噪训练的UNet网络包含了丰富且可访问的稀疏图像表示。

Abstract: Generative diffusion models learn probability densities over diverse image
datasets by estimating the score with a neural network trained to remove noise.
Despite their remarkable success in generating high-quality images, the
internal mechanisms of the underlying score networks are not well understood.
Here, we examine a UNet trained for denoising on the ImageNet dataset, to
better understand its internal representation and computation of the score. We
show that the middle block of the UNet decomposes individual images into sparse
subsets of active channels, and that the vector of spatial averages of these
channels can provide a nonlinear representation of the underlying clean images.
We develop a novel algorithm for stochastic reconstruction of images from this
representation and demonstrate that it recovers a sample from a set of images
defined by a target image representation. We then study the properties of the
representation and demonstrate that Euclidean distances in the latent space
correspond to distances between conditional densities induced by
representations as well as semantic similarities in the image space. Applying a
clustering algorithm in the representation space yields groups of images that
share both fine details (e.g., specialized features, textured regions, small
objects), as well as global structure, but are only partially aligned with
object identities. Thus, we show for the first time that a network trained
solely on denoising contains a rich and accessible sparse representation of
images.

</details>


### [164] [MedEBench: Revisiting Text-instructed Image Editing](https://arxiv.org/abs/2506.01921)
*Minghao Liu,Zhitao He,Zhiyuan Fan,Qingyun Wang,Yi R. Fung*

Main category: cs.CV

TL;DR: MedEBench是一个用于评估文本引导医学图像编辑的基准，包含1182个临床图像-提示对，覆盖13个解剖区域和70个任务，提供评估框架、模型比较和失败分析。


<details>
  <summary>Details</summary>
Motivation: 填补文本引导图像编辑在医学影像领域缺乏标准化评估的空白，支持临床应用如手术模拟、教学材料生成和患者沟通。

Method: 提出MedEBench基准，包含临床相关的评估框架（编辑准确性、上下文保留和视觉质量），系统比较7种先进模型，并通过注意力定位分析失败模式。

Result: 揭示了常见失败模式，并通过注意力地图与ROI的交并比（IoU）识别定位错误。

Conclusion: MedEBench为开发可靠且临床有意义的医学图像编辑系统提供了坚实基础。

Abstract: Text-guided image editing has seen rapid progress in natural image domains,
but its adaptation to medical imaging remains limited and lacks standardized
evaluation. Clinically, such editing holds promise for simulating surgical
outcomes, creating personalized teaching materials, and enhancing patient
communication. To bridge this gap, we introduce \textbf{MedEBench}, a
comprehensive benchmark for evaluating text-guided medical image editing. It
consists of 1,182 clinically sourced image-prompt triplets spanning 70 tasks
across 13 anatomical regions. MedEBench offers three key contributions: (1) a
clinically relevant evaluation framework covering Editing Accuracy, Contextual
Preservation, and Visual Quality, supported by detailed descriptions of
expected change and ROI (Region of Interest) masks; (2) a systematic comparison
of seven state-of-the-art models, revealing common failure patterns; and (3) a
failure analysis protocol based on attention grounding, using IoU between
attention maps and ROIs to identify mislocalization. MedEBench provides a solid
foundation for developing and evaluating reliable, clinically meaningful
medical image editing systems.

</details>


### [165] [TaxaDiffusion: Progressively Trained Diffusion Model for Fine-Grained Species Generation](https://arxiv.org/abs/2506.01923)
*Amin Karimi Monsefi,Mridul Khurana,Rajiv Ramnath,Anuj Karpatne,Wei-Lun Chao,Cheng Zhang*

Main category: cs.CV

TL;DR: TaxaDiffusion是一种基于分类学的扩散模型训练框架，用于生成高保真度的细粒度动物图像。


<details>
  <summary>Details</summary>
Motivation: 传统方法将每个物种视为独立类别，忽略了物种间的视觉相似性。TaxaDiffusion利用分类学知识，通过层次化训练提升生成准确性。

Method: 采用分层训练策略，从粗粒度分类（如纲、目）逐步细化到物种级别，利用共享形态特征进行知识迁移。

Result: 在三个细粒度动物数据集上表现优于现有方法，生成图像具有更高的保真度。

Conclusion: TaxaDiffusion通过分类学指导的层次化训练，显著提升了细粒度动物图像的生成质量。

Abstract: We propose TaxaDiffusion, a taxonomy-informed training framework for
diffusion models to generate fine-grained animal images with high morphological
and identity accuracy. Unlike standard approaches that treat each species as an
independent category, TaxaDiffusion incorporates domain knowledge that many
species exhibit strong visual similarities, with distinctions often residing in
subtle variations of shape, pattern, and color. To exploit these relationships,
TaxaDiffusion progressively trains conditioned diffusion models across
different taxonomic levels -- starting from broad classifications such as Class
and Order, refining through Family and Genus, and ultimately distinguishing at
the Species level. This hierarchical learning strategy first captures
coarse-grained morphological traits shared by species with common ancestors,
facilitating knowledge transfer before refining fine-grained differences for
species-level distinction. As a result, TaxaDiffusion enables accurate
generation even with limited training samples per species. Extensive
experiments on three fine-grained animal datasets demonstrate that outperforms
existing approaches, achieving superior fidelity in fine-grained animal image
generation. Project page: https://amink8.github.io/TaxaDiffusion/

</details>


### [166] [E3D-Bench: A Benchmark for End-to-End 3D Geometric Foundation Models](https://arxiv.org/abs/2506.01933)
*Wenyan Cong,Yiqing Liang,Yancheng Zhang,Ziyi Yang,Yan Wang,Boris Ivanovic,Marco Pavone,Chen Chen,Zhangyang Wang,Zhiwen Fan*

Main category: cs.CV

TL;DR: 该论文提出了首个针对3D几何基础模型（GFMs）的综合基准测试，覆盖了五项核心任务，并评估了16种先进模型，揭示了其优缺点。


<details>
  <summary>Details</summary>
Motivation: 空间智能（如3D重建和感知）对机器人、航空成像等应用至关重要，但缺乏对新兴3D GFMs的系统评估。

Method: 开发了一个标准化工具包，自动化数据集处理、评估协议和指标计算，对16种GFMs进行了全面测试。

Result: 评估揭示了不同模型在任务和领域中的表现差异，为未来模型优化提供了关键见解。

Conclusion: 所有代码和数据集将公开，以推动3D空间智能研究的进一步发展。

Abstract: Spatial intelligence, encompassing 3D reconstruction, perception, and
reasoning, is fundamental to applications such as robotics, aerial imaging, and
extended reality. A key enabler is the real-time, accurate estimation of core
3D attributes (camera parameters, point clouds, depth maps, and 3D point
tracks) from unstructured or streaming imagery. Inspired by the success of
large foundation models in language and 2D vision, a new class of end-to-end 3D
geometric foundation models (GFMs) has emerged, directly predicting dense 3D
representations in a single feed-forward pass, eliminating the need for slow or
unavailable precomputed camera parameters. Since late 2023, the field has
exploded with diverse variants, but systematic evaluation is lacking. In this
work, we present the first comprehensive benchmark for 3D GFMs, covering five
core tasks: sparse-view depth estimation, video depth estimation, 3D
reconstruction, multi-view pose estimation, novel view synthesis, and spanning
both standard and challenging out-of-distribution datasets. Our standardized
toolkit automates dataset handling, evaluation protocols, and metric
computation to ensure fair, reproducible comparisons. We evaluate 16
state-of-the-art GFMs, revealing their strengths and limitations across tasks
and domains, and derive key insights to guide future model scaling and
optimization. All code, evaluation scripts, and processed data will be publicly
released to accelerate research in 3D spatial intelligence.

</details>


### [167] [Low-Rank Head Avatar Personalization with Registers](https://arxiv.org/abs/2506.01935)
*Sai Tanmay Reddy Chakkera,Aggelina Chatziagapi,Md Moniruzzaman,Chen-Ping Yu,Yi-Hsuan Tsai,Dimitris Samaras*

Main category: cs.CV

TL;DR: 提出一种新方法，通过Register Module增强LoRA，实现低秩个性化头像生成，捕捉高频面部细节。


<details>
  <summary>Details</summary>
Motivation: 通用模型难以合成独特的身份细节，现有方法（如LoRA）难以捕捉高频面部特征。

Method: 设计Register Module，应用于预训练模型的中间特征，存储和重用3D特征空间信息。

Result: 在具有独特面部细节的数据集上表现优异，定量和定性均优于现有方法。

Conclusion: 提出的方法有效捕捉未见过的面部特征，代码、模型和数据集将公开。

Abstract: We introduce a novel method for low-rank personalization of a generic model
for head avatar generation. Prior work proposes generic models that achieve
high-quality face animation by leveraging large-scale datasets of multiple
identities. However, such generic models usually fail to synthesize unique
identity-specific details, since they learn a general domain prior. To adapt to
specific subjects, we find that it is still challenging to capture
high-frequency facial details via popular solutions like low-rank adaptation
(LoRA). This motivates us to propose a specific architecture, a Register
Module, that enhances the performance of LoRA, while requiring only a small
number of parameters to adapt to an unseen identity. Our module is applied to
intermediate features of a pre-trained model, storing and re-purposing
information in a learnable 3D feature space. To demonstrate the efficacy of our
personalization method, we collect a dataset of talking videos of individuals
with distinctive facial details, such as wrinkles and tattoos. Our approach
faithfully captures unseen faces, outperforming existing methods quantitatively
and qualitatively. We will release the code, models, and dataset to the public.

</details>


### [168] [Fast and Robust Rotation Averaging with Anisotropic Coordinate Descent](https://arxiv.org/abs/2506.01940)
*Yaroslava Lochman,Carl Olsson,Christopher Zach*

Main category: cs.CV

TL;DR: 论文提出了一种结合最优性、鲁棒性和效率的各向异性旋转平均方法，通过块坐标下降法简化了求解过程，并在大规模数据集上实现了先进性能。


<details>
  <summary>Details</summary>
Motivation: 各向异性旋转平均方法在全局最优性和计算效率之间存在矛盾，本文旨在填补这一空白。

Method: 采用块坐标下降法优化标准弦距离，并扩展到各向异性情况，集成到大规模鲁棒旋转平均流程中。

Result: 在公开的结构运动数据集上实现了最先进的性能。

Conclusion: 提出的方法成功平衡了最优性、鲁棒性和效率，适用于大规模问题。

Abstract: Anisotropic rotation averaging has recently been explored as a natural
extension of respective isotropic methods. In the anisotropic formulation,
uncertainties of the estimated relative rotations -- obtained via standard
two-view optimization -- are propagated to the optimization of absolute
rotations. The resulting semidefinite relaxations are able to recover global
minima but scale poorly with the problem size. Local methods are fast and also
admit robust estimation but are sensitive to initialization. They usually
employ minimum spanning trees and therefore suffer from drift accumulation and
can get trapped in poor local minima. In this paper, we attempt to bridge the
gap between optimality, robustness and efficiency of anisotropic rotation
averaging. We analyze a family of block coordinate descent methods initially
proposed to optimize the standard chordal distances, and derive a much simpler
formulation and an anisotropic extension obtaining a fast general solver. We
integrate this solver into the extended anisotropic large-scale robust rotation
averaging pipeline. The resulting algorithm achieves state-of-the-art
performance on public structure-from-motion datasets. Project page:
https://ylochman.github.io/acd

</details>


### [169] [OD3: Optimization-free Dataset Distillation for Object Detection](https://arxiv.org/abs/2506.01942)
*Salwa K. Al Khatib,Ahmed ElHagry,Shitong Shao,Zhiqiang Shen*

Main category: cs.CV

TL;DR: OD3是一种无需优化的数据蒸馏框架，专为对象检测设计，通过候选选择与筛选两阶段流程，显著提升压缩数据集上的检测性能。


<details>
  <summary>Details</summary>
Motivation: 大规模神经网络的训练需要大量计算资源，现有数据集蒸馏方法主要针对图像分类，而复杂的目标检测任务尚未充分探索。

Method: OD3采用两阶段流程：候选选择（基于合适位置迭代放置对象实例）和候选筛选（使用预训练观察模型去除低置信度对象）。

Result: 在MS COCO和PASCAL VOC数据集上，OD3在1.0%压缩比下比现有方法提升14%以上（COCO mAP50）。

Conclusion: OD3为目标检测任务提供了一种高效的数据集蒸馏方法，显著优于现有技术。

Abstract: Training large neural networks on large-scale datasets requires substantial
computational resources, particularly for dense prediction tasks such as object
detection. Although dataset distillation (DD) has been proposed to alleviate
these demands by synthesizing compact datasets from larger ones, most existing
work focuses solely on image classification, leaving the more complex detection
setting largely unexplored. In this paper, we introduce OD3, a novel
optimization-free data distillation framework specifically designed for object
detection. Our approach involves two stages: first, a candidate selection
process in which object instances are iteratively placed in synthesized images
based on their suitable locations, and second, a candidate screening process
using a pre-trained observer model to remove low-confidence objects. We perform
our data synthesis framework on MS COCO and PASCAL VOC, two popular detection
datasets, with compression ratios ranging from 0.25% to 5%. Compared to the
prior solely existing dataset distillation method on detection and conventional
core set selection methods, OD3 delivers superior accuracy, establishes new
state-of-the-art results, surpassing prior best method by more than 14% on COCO
mAP50 at a compression ratio of 1.0%. Code and condensed datasets are available
at: https://github.com/VILA-Lab/OD3.

</details>


### [170] [Learning Video Generation for Robotic Manipulation with Collaborative Trajectory Control](https://arxiv.org/abs/2506.01943)
*Xiao Fu,Xintao Wang,Xian Liu,Jianhong Bai,Runsen Xu,Pengfei Wan,Di Zhang,Dahua Lin*

Main category: cs.CV

TL;DR: RoboMaster提出了一种新框架，通过分阶段建模多物体交互过程，解决了现有轨迹方法在复杂机器人操作中的多物体交互问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单个物体运动，难以捕捉复杂机器人操作中的多物体交互，导致视觉保真度下降。

Method: 将交互过程分解为三个阶段（交互前、交互中、交互后），分别用主导物体的特征建模，并引入外观和形状感知的潜在表示。

Result: 在Bridge V2数据集和实际评估中，RoboMaster表现优于现有方法，达到了轨迹控制视频生成的最新水平。

Conclusion: RoboMaster通过分阶段建模和潜在表示，显著提升了多物体交互的生成质量，为机器人操作提供了更精细的控制。

Abstract: Recent advances in video diffusion models have demonstrated strong potential
for generating robotic decision-making data, with trajectory conditions further
enabling fine-grained control. However, existing trajectory-based methods
primarily focus on individual object motion and struggle to capture
multi-object interaction crucial in complex robotic manipulation. This
limitation arises from multi-feature entanglement in overlapping regions, which
leads to degraded visual fidelity. To address this, we present RoboMaster, a
novel framework that models inter-object dynamics through a collaborative
trajectory formulation. Unlike prior methods that decompose objects, our core
is to decompose the interaction process into three sub-stages: pre-interaction,
interaction, and post-interaction. Each stage is modeled using the feature of
the dominant object, specifically the robotic arm in the pre- and
post-interaction phases and the manipulated object during interaction, thereby
mitigating the drawback of multi-object feature fusion present during
interaction in prior work. To further ensure subject semantic consistency
throughout the video, we incorporate appearance- and shape-aware latent
representations for objects. Extensive experiments on the challenging Bridge V2
dataset, as well as in-the-wild evaluation, demonstrate that our method
outperforms existing approaches, establishing new state-of-the-art performance
in trajectory-controlled video generation for robotic manipulation.

</details>


### [171] [MLLMs Need 3D-Aware Representation Supervision for Scene Understanding](https://arxiv.org/abs/2506.01946)
*Xiaohu Huang,Jingjing Wu,Qunyi Xie,Kai Han*

Main category: cs.CV

TL;DR: 论文提出3DRS框架，通过引入3D基础模型的监督增强MLLM的3D表示学习，提升场景理解能力。


<details>
  <summary>Details</summary>
Motivation: MLLMs在3D推理中因缺乏显式3D数据而受限，研究发现3D感知表示质量与下游任务性能正相关。

Method: 提出3DRS框架，通过3D基础模型监督对齐MLLM视觉特征，增强3D表示学习。

Result: 在多个基准测试和MLLMs中（如视觉定位、描述和问答）表现一致提升。

Conclusion: 3DRS有效提升MLLMs的3D感知能力，改善场景理解任务性能。

Abstract: Recent advances in scene understanding have leveraged multimodal large
language models (MLLMs) for 3D reasoning by capitalizing on their strong 2D
pretraining. However, the lack of explicit 3D data during MLLM pretraining
limits 3D representation capability. In this paper, we investigate the
3D-awareness of MLLMs by evaluating multi-view correspondence and reveal a
strong positive correlation between the quality of 3D-aware representation and
downstream task performance. Motivated by this, we propose 3DRS, a framework
that enhances MLLM 3D representation learning by introducing supervision from
pretrained 3D foundation models. Our approach aligns MLLM visual features with
rich 3D knowledge distilled from 3D models, effectively improving scene
understanding. Extensive experiments across multiple benchmarks and MLLMs --
including visual grounding, captioning, and question answering -- demonstrate
consistent performance gains. Project page: https://visual-ai.github.io/3drs

</details>


### [172] [IMAGHarmony: Controllable Image Editing with Consistent Object Quantity and Layout](https://arxiv.org/abs/2506.01949)
*Fei Shen,Xiaoyu Du,Yutong Gao,Jian Yu,Yushe Cao,Xing Lei,Jinhui Tang*

Main category: cs.CV

TL;DR: 论文提出了一种新任务QL-Edit，旨在解决多对象场景下的图像编辑问题，并提出了IMAGHarmony框架和PNS策略，显著提升了编辑精度和结构一致性。


<details>
  <summary>Details</summary>
Motivation: 当前图像编辑方法在多对象场景中对对象类别、数量和空间布局的控制不足，亟需改进。

Method: 提出了IMAGHarmony框架，结合和谐感知注意力（HA）和多模态语义，以及偏好引导的噪声选择（PNS）策略。

Result: 实验表明IMAGHarmony在结构对齐和语义准确性上优于现有方法。

Conclusion: IMAGHarmony为多对象图像编辑提供了有效的解决方案，并通过HarmonyBench验证了其优越性。

Abstract: Recent diffusion models have advanced image editing by enhancing visual
quality and control, supporting broad applications across creative and
personalized domains. However, current image editing largely overlooks
multi-object scenarios, where precise control over object categories, counts,
and spatial layouts remains a significant challenge. To address this, we
introduce a new task, quantity-and-layout consistent image editing (QL-Edit),
which aims to enable fine-grained control of object quantity and spatial
structure in complex scenes. We further propose IMAGHarmony, a structure-aware
framework that incorporates harmony-aware attention (HA) to integrate
multimodal semantics, explicitly modeling object counts and layouts to enhance
editing accuracy and structural consistency. In addition, we observe that
diffusion models are susceptible to initial noise and exhibit strong
preferences for specific noise patterns. Motivated by this, we present a
preference-guided noise selection (PNS) strategy that chooses semantically
aligned initial noise samples based on vision-language matching, thereby
improving generation stability and layout consistency in multi-object editing.
To support evaluation, we construct HarmonyBench, a comprehensive benchmark
covering diverse quantity and layout control scenarios. Extensive experiments
demonstrate that IMAGHarmony consistently outperforms state-of-the-art methods
in structural alignment and semantic accuracy. The code and model are available
at https://github.com/muzishen/IMAGHarmony.

</details>


### [173] [Dual-Process Image Generation](https://arxiv.org/abs/2506.01955)
*Grace Luo,Jonathan Granskog,Aleksander Holynski,Trevor Darrell*

Main category: cs.CV

TL;DR: 提出了一种双过程蒸馏方案，使前馈图像生成器能够从深思熟虑的视觉语言模型（VLM）中学习新任务。


<details>
  <summary>Details</summary>
Motivation: 现有图像生成控制方法难以学习新任务，而VLM能够通过上下文学习任务并生成正确输出。

Method: 使用VLM对生成图像评分，并通过反向传播梯度更新图像生成器的权重。

Result: 实现了多种新控制任务，如常识推理和视觉提示，用户可快速实现多模态控制。

Conclusion: 该框架为图像生成提供了灵活且高效的控制方法。

Abstract: Prior methods for controlling image generation are limited in their ability
to be taught new tasks. In contrast, vision-language models, or VLMs, can learn
tasks in-context and produce the correct outputs for a given input. We propose
a dual-process distillation scheme that allows feed-forward image generators to
learn new tasks from deliberative VLMs. Our scheme uses a VLM to rate the
generated images and backpropagates this gradient to update the weights of the
image generator. Our general framework enables a wide variety of new control
tasks through the same text-and-image based interface. We showcase a handful of
applications of this technique for different types of control signals, such as
commonsense inferences and visual prompts. With our method, users can implement
multimodal controls for properties such as color palette, line weight, horizon
position, and relative depth within a matter of minutes. Project page:
https://dual-process.github.io.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [174] [Adaptive Voxelization for Transform coding of 3D Gaussian splatting data](https://arxiv.org/abs/2506.00271)
*Chenjunjie Wang,Shashank N. Sridhara,Eduardo Pavez,Antonio Ortega,Cheng Chang*

Main category: eess.IV

TL;DR: 提出了一种基于点云变换编码工具的新型3D高斯泼溅（3DGS）压缩框架，通过自适应体素化技术高效生成多比特率压缩模型。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS压缩方法无法高效生成多比特率模型，且均匀体素化效率低。

Method: 采用自适应体素化技术，针对3DGS数据优化体素分辨率，结合高斯属性微调以减少重训练需求。

Result: 实验表明，该方法显著减少高斯数量和比特率，优于现有方法。

Conclusion: 自适应体素化和属性微调有效提升了3DGS压缩效率和质量。

Abstract: We present a novel compression framework for 3D Gaussian splatting (3DGS)
data that leverages transform coding tools originally developed for point
clouds. Contrary to existing 3DGS compression methods, our approach can produce
compressed 3DGS models at multiple bitrates in a computationally efficient way.
Point cloud voxelization is a discretization technique that point cloud codecs
use to improve coding efficiency while enabling the use of fast transform
coding algorithms. We propose an adaptive voxelization algorithm tailored to
3DGS data, to avoid the inefficiencies introduced by uniform voxelization used
in point cloud codecs. We ensure the positions of larger volume Gaussians are
represented at high resolution, as these significantly impact rendering
quality. Meanwhile, a low-resolution representation is used for dense regions
with smaller Gaussians, which have a relatively lower impact on rendering
quality. This adaptive voxelization approach significantly reduces the number
of Gaussians and the bitrate required to encode the 3DGS data. After
voxelization, many Gaussians are moved or eliminated. Thus, we propose to
fine-tune/recolor the remaining 3DGS attributes with an initialization that can
reduce the amount of retraining required. Experimental results on pre-trained
datasets show that our proposed compression framework outperforms existing
methods.

</details>


### [175] [A European Multi-Center Breast Cancer MRI Dataset](https://arxiv.org/abs/2506.00474)
*Gustav Müller-Franzes,Lorena Escudero Sánchez,Nicholas Payne,Alexandra Athanasiou,Michael Kalogeropoulos,Aitor Lopez,Alfredo Miguel Soro Busto,Julia Camps Herrero,Nika Rasoolzadeh,Tianyu Zhang,Ritse Mann,Debora Jutz,Maike Bode,Christiane Kuhl,Wouter Veldhuis,Oliver Lester Saldanha,JieFu Zhu,Jakob Nikolas Kather,Daniel Truhn,Fiona J. Gilbert*

Main category: eess.IV

TL;DR: 论文探讨了利用AI和MRI技术辅助乳腺癌早期检测的重要性，并介绍了ODELIA联盟公开的多中心数据集以支持相关AI工具开发。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌早期检测对治疗至关重要，MRI作为补充筛查工具的需求增加，但MRI解读耗时，需要自动化方法支持。

Method: 利用AI技术分析MRI数据，开发自动化检测工具。

Result: ODELIA联盟提供了公开的多中心数据集，支持AI工具的研发。

Conclusion: AI辅助MRI解读有望提高乳腺癌早期检测效率，ODELIA数据集为此提供了重要资源。

Abstract: Detecting breast cancer early is of the utmost importance to effectively
treat the millions of women afflicted by breast cancer worldwide every year.
Although mammography is the primary imaging modality for screening breast
cancer, there is an increasing interest in adding magnetic resonance imaging
(MRI) to screening programmes, particularly for women at high risk. Recent
guidelines by the European Society of Breast Imaging (EUSOBI) recommended
breast MRI as a supplemental screening tool for women with dense breast tissue.
However, acquiring and reading MRI scans requires significantly more time from
expert radiologists. This highlights the need to develop new automated methods
to detect cancer accurately using MRI and Artificial Intelligence (AI), which
have the potential to support radiologists in breast MRI interpretation and
classification and help detect cancer earlier. For this reason, the ODELIA
consortium has made this multi-centre dataset publicly available to assist in
developing AI tools for the detection of breast cancer on MRI.

</details>


### [176] [Your Demands Deserve More Bits: Referring Semantic Image Compression at Ultra-low Bitrate](https://arxiv.org/abs/2506.00526)
*Chenhao Wu,Qingbo Wu,Haoran Wei,Shuai Chen,Mingzhou He,King Ngi Ngan,Fanman Meng,Hongliang Li*

Main category: eess.IV

TL;DR: 论文提出了一种新的Referring Semantic Image Compression (RSIC)框架，通过结合全局和局部特征编码，提升用户指定内容的保真度，同时保持超高压缩比。


<details>
  <summary>Details</summary>
Motivation: 现有语义图像压缩(SIC)方法在超低比特率下表现优异，但由于粗粒度的视觉-语义对齐和随机性，可能导致重建内容与原始图像完全不同的实例，影响可靠性。

Method: RSIC框架包含三个模块：全局描述编码(GDE)、参考引导编码(RGE)和引导生成解码(GGD)，分别处理全局语义信息、局部特征和非均匀引导生成过程。

Result: 在三个数据集上的实验验证了RSIC的压缩效率和灵活性，能够平衡局部保真度、全局真实性、语义对齐和比特开销。

Conclusion: RSIC通过用户定制化压缩需求，显著提升了语义图像压缩的可靠性，同时保持了高压缩比。

Abstract: With the help of powerful generative models, Semantic Image Compression (SIC)
has achieved impressive performance at ultra-low bitrate. However, due to
coarse-grained visual-semantic alignment and inherent randomness, the
reliability of SIC is seriously concerned for reconstructing completely
different object instances, even they are semantically consistent with original
images. To tackle this issue, we propose a novel Referring Semantic Image
Compression (RSIC) framework to improve the fidelity of user-specified content
while retaining extreme compression ratios. Specifically, RSIC consists of
three modules: Global Description Encoding (GDE), Referring Guidance Encoding
(RGE), and Guided Generative Decoding (GGD). GDE and RGE encode global semantic
information and local features, respectively, while GGD handles the
non-uniformly guided generative process based on the encoded information. In
this way, our RSIC achieves flexible customized compression according to user
demands, which better balance the local fidelity, global realism, semantic
alignment, and bit overhead. Extensive experiments on three datasets verify the
compression efficiency and flexibility of the proposed method.

</details>


### [177] [Image Restoration Learning via Noisy Supervision in the Fourier Domain](https://arxiv.org/abs/2506.00564)
*Haosen Liu,Jiahao Liu,Shan Tan,Edmund Y. Lam*

Main category: eess.IV

TL;DR: 论文提出了一种在傅里叶域中处理空间相关噪声的方法，通过利用噪声的稀疏性和独立性，以及傅里叶系数的全局信息，提升了图像恢复任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理空间相关噪声时效果不佳，且仅依赖像素级损失函数提供有限监督信息。傅里叶域的特性为解决这些问题提供了可能。

Method: 利用傅里叶系数的统计特性（如高斯分布收敛性），在傅里叶域中建立噪声监督与干净目标的等价性，提出统一的学习框架。

Result: 实验验证了该方法在定量指标和感知质量上的优异表现，适用于多种图像恢复任务、网络架构和噪声模型。

Conclusion: 傅里叶域的噪声监督方法有效解决了空间相关噪声和有限监督信息的问题，具有广泛的适用性和优越性能。

Abstract: Noisy supervision refers to supervising image restoration learning with noisy
targets. It can alleviate the data collection burden and enhance the practical
applicability of deep learning techniques. However, existing methods suffer
from two key drawbacks. Firstly, they are ineffective in handling spatially
correlated noise commonly observed in practical applications such as low-light
imaging and remote sensing. Secondly, they rely on pixel-wise loss functions
that only provide limited supervision information. This work addresses these
challenges by leveraging the Fourier domain. We highlight that the Fourier
coefficients of spatially correlated noise exhibit sparsity and independence,
making them easier to handle. Additionally, Fourier coefficients contain global
information, enabling more significant supervision. Motivated by these
insights, we propose to establish noisy supervision in the Fourier domain. We
first prove that Fourier coefficients of a wide range of noise converge in
distribution to the Gaussian distribution. Exploiting this statistical
property, we establish the equivalence between using noisy targets and clean
targets in the Fourier domain. This leads to a unified learning framework
applicable to various image restoration tasks, diverse network architectures,
and different noise models. Extensive experiments validate the outstanding
performance of this framework in terms of both quantitative indices and
perceptual quality.

</details>


### [178] [TRUST -- Transformer-Driven U-Net for Sparse Target Recovery](https://arxiv.org/abs/2506.01112)
*Di An,Dylan Poppert,Jiayue Li,Mark Foster,Trac D. Tran*

Main category: eess.IV

TL;DR: 论文提出了一种名为TRUST的新型神经网络架构，结合Transformer和U-Net，用于解决未知传感算子下的稀疏信号恢复问题，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决在传感算子未知且观测数据有限的情况下，传统稀疏恢复方法难以有效恢复稀疏信号的问题。

Method: 提出TRUST模型，结合Transformer的注意力机制和U-Net的解码路径，同时学习传感算子和重建稀疏信号。

Result: TRUST在SSIM和PSNR指标上显著优于传统稀疏恢复方法和独立U-Net模型，并能有效抑制幻觉伪影。

Conclusion: TRUST通过结合全局上下文和局部细节，实现了更准确和鲁棒的稀疏信号恢复。

Abstract: In the context of inverse problems $\bf y = Ax$, sparse recovery offers a
powerful paradigm shift by enabling the stable solution of ill-posed or
underdetermined systems through the exploitation of structure, particularly
sparsity. Sparse regularization techniques via $\ell_0$- or $\ell_1$-norm
minimization encourage solutions $\bf x$ that are both consistent with
observations $\bf y$ and parsimonious in representation, often yielding
physically meaningful interpretations. In this work, we address the classical
inverse problem under the challenging condition where the sensing operator $\bf
A$ is unknown and only a limited set of observation-target pairs $\{ \bf x,\bf
y \}$ is available. We propose a novel neural architecture, TRUST, that
integrates the attention mechanism of Transformers with the decoder pathway of
a UNet to simultaneously learn the sensing operator and reconstruct the sparse
signal. The TRUST model incorporates a Transformer-based encoding branch to
capture long-range dependencies and estimate sparse support, which then guides
a U-Net-style decoder to refine reconstruction through multiscale feature
integration. The skip connections between the transformer stages and the
decoder not only enhance image quality but also enable the decoder to access
image features at different levels of abstraction. This hybrid architecture
enables more accurate and robust recovery by combining global context with
local details. Experimental results demonstrate that TRUST significantly
outperforms traditional sparse recovery methods and standalone U-Net models,
achieving superior performance in SSIM and PSNR metrics while effectively
suppressing hallucination artifacts that commonly plague deep learning-based
inverse solvers.

</details>


### [179] [Analysis of Local Methane Emissions Using Near-Simultaneous Multi-Satellite Observations: Insights from Landfills and Oil-Gas Facilities](https://arxiv.org/abs/2506.01113)
*Alvise Ferrari,Giovanni Laneve,Raul Alejandro Carvajal Tellez,Valerio Pampanoni,Simone Saquella,Rocchina Guarini*

Main category: eess.IV

TL;DR: 该研究通过多卫星平台的近同步观测，比较分析了甲烷排放的检测和量化方法，重点关注垃圾填埋场和油气设施的排放差异，并评估了传感器的性能和排放源的时空变化。


<details>
  <summary>Details</summary>
Motivation: 甲烷是一种强效温室气体，其检测和量化对缓解温室效应至关重要。研究旨在通过多卫星平台的观测数据，评估不同传感器的性能，并填补甲烷排放监测的时空空白。

Method: 利用PRISMA、EnMAP、EMIT和GHGSat等卫星平台的高光谱成像仪进行近同步观测，采用集成质量增强（IME）模型量化排放率，并通过重叠观测验证结果。

Result: 研究揭示了不同卫星系统的优势和局限性，强调了多传感器集成在填补观测空白中的关键作用。垃圾填埋场和油气设施的排放特征差异显著。

Conclusion: 多卫星平台的集成观测可提升全球甲烷监测能力，并为未来卫星设计提供指导，以改进排放量化精度。

Abstract: Methane (CH4) is a potent greenhouse gas, and its detection and
quantification are crucial for mitigating the greenhouse effect. This study
presents a comparative analysis of methane emissions observed using
near-simultaneous observations from hyperspectral imaging spectrometers hosted
aboard different satellite platforms (PRISMA, EnMAP, EMIT and GHGSat). Methane
emissions from oil and gas facilities and landfills are analyzed to evaluate
the consistency and precision of the sensors and temporal variability of the
source. Landfills, characterized by diffuse and stable emissions, and dynamic
oil and gas facilities, subject to operational variability, provide contrasting
use cases for emission monitoring. Emission rates are quantified using the
Integrated Mass Enhancement (IME) model and validated across satellites with
overlapping acquisitions. This study highlights the advantages and limitations
of each satellite system, emphasizing the critical role of multi-sensor
integration in bridging temporal and spatial observation gaps. Insights derived
here aim to enhance global methane monitoring frameworks and guide future
satellite design for improved emission quantification.

</details>


### [180] [Flexible Mixed Precision Quantization for Learned Image Compression](https://arxiv.org/abs/2506.01221)
*Md Adnan Faisal Hossain,Zhihao Duan,Fengqing Zhu*

Main category: eess.IV

TL;DR: 提出了一种灵活混合精度量化（FMPQ）方法，通过为神经网络的不同层分配不同位宽，优化资源利用，并引入自适应搜索算法降低复杂度。


<details>
  <summary>Details</summary>
Motivation: 学习图像压缩（LIC）模型的计算和存储成本高，现有固定精度量化方法资源利用不足。

Method: 使用率失真损失的分数变化作为位分配标准，提出FMPQ方法，并设计自适应搜索算法。

Result: 在相似模型大小约束下，BD-Rate性能优于其他量化方法。

Conclusion: FMPQ方法有效降低了LIC模型的计算复杂度，同时提升了性能。

Abstract: Despite its improvements in coding performance compared to traditional
codecs, Learned Image Compression (LIC) suffers from large computational costs
for storage and deployment. Model quantization offers an effective solution to
reduce the computational complexity of LIC models. However, most existing works
perform fixed-precision quantization which suffers from sub-optimal utilization
of resources due to the varying sensitivity to quantization of different layers
of a neural network. In this paper, we propose a Flexible Mixed Precision
Quantization (FMPQ) method that assigns different bit-widths to different
layers of the quantized network using the fractional change in rate-distortion
loss as the bit-assignment criterion. We also introduce an adaptive search
algorithm which reduces the time-complexity of searching for the desired
distribution of quantization bit-widths given a fixed model size. Evaluation of
our method shows improved BD-Rate performance under similar model size
constraints compared to other works on quantization of LIC models. We have made
the source code available at gitlab.com/viper-purdue/fmpq.

</details>


### [181] [Structured Pruning and Quantization for Learned Image Compression](https://arxiv.org/abs/2506.01229)
*Md Adnan Faisal Hossain,Fengqing Zhu*

Main category: eess.IV

TL;DR: 提出了一种针对学习图像压缩（LIC）模型的结构化剪枝方法，通过神经架构搜索（NAS）确定剪枝比例，减少计算成本且保持率失真性能。


<details>
  <summary>Details</summary>
Motivation: 大型深度学习模型的高计算成本限制了其实际部署，剪枝方法在计算机视觉任务中广泛应用，但在LIC领域尚未充分探索。

Method: 采用基于率失真损失的NAS方法，为网络每层计算剪枝比例，并与未剪枝模型对比。

Result: 剪枝后的模型在保持BD-Rate性能不变的同时实现了模型大小缩减，且可与量化结合进一步压缩模型。

Conclusion: 该方法有效减少了LIC模型的计算成本，同时保持了性能，为实际部署提供了可行方案。

Abstract: The high computational costs associated with large deep learning models
significantly hinder their practical deployment. Model pruning has been widely
explored in deep learning literature to reduce their computational burden, but
its application has been largely limited to computer vision tasks such as image
classification and object detection. In this work, we propose a structured
pruning method targeted for Learned Image Compression (LIC) models that aims to
reduce the computational costs associated with image compression while
maintaining the rate-distortion performance. We employ a Neural Architecture
Search (NAS) method based on the rate-distortion loss for computing the pruning
ratio for each layer of the network. We compare our pruned model with the
uncompressed LIC Model with same network architecture and show that it can
achieve model size reduction without any BD-Rate performance drop. We further
show that our pruning method can be integrated with model quantization to
achieve further model compression while maintaining similar BD-Rate
performance. We have made the source code available at
gitlab.com/viper-purdue/lic-pruning.

</details>


### [182] [Beyond Pixel Agreement: Large Language Models as Clinical Guardrails for Reliable Medical Image Segmentation](https://arxiv.org/abs/2506.01841)
*Jiaxi Sheng,Leyi Yu,Haoyue Li,Yifan Gao,Xin Gao*

Main category: eess.IV

TL;DR: 论文提出了一种基于大型语言模型（LLM）的Hierarchical Clinical Reasoner（HCR）框架，用于评估医学图像分割的临床可接受性，其表现优于传统视觉模型。


<details>
  <summary>Details</summary>
Motivation: 传统像素级评估指标无法准确反映医学图像分割的临床诊断价值，因此需要一种更可靠的零样本质量评估方法。

Method: HCR采用多阶段提示策略，引导LLM通过知识回忆、视觉特征分析、解剖推理和临床综合等步骤进行详细推理。

Result: HCR在六项医学成像任务中表现优异，分类准确率达78.12%，优于ResNet50（72.92%），并生成可解释的评估过程。

Conclusion: HCR展示了LLM在医学图像质量评估中的潜力，为AI在医学影像中的可信质量控制提供了新途径。

Abstract: Evaluating AI-generated medical image segmentations for clinical
acceptability poses a significant challenge, as traditional pixelagreement
metrics often fail to capture true diagnostic utility. This paper introduces
Hierarchical Clinical Reasoner (HCR), a novel framework that leverages Large
Language Models (LLMs) as clinical guardrails for reliable, zero-shot quality
assessment. HCR employs a structured, multistage prompting strategy that guides
LLMs through a detailed reasoning process, encompassing knowledge recall,
visual feature analysis, anatomical inference, and clinical synthesis, to
evaluate segmentations. We evaluated HCR on a diverse dataset across six
medical imaging tasks. Our results show that HCR, utilizing models like Gemini
2.5 Flash, achieved a classification accuracy of 78.12%, performing comparably
to, and in instances exceeding, dedicated vision models such as ResNet50
(72.92% accuracy) that were specifically trained for this task. The HCR
framework not only provides accurate quality classifications but also generates
interpretable, step-by-step reasoning for its assessments. This work
demonstrates the potential of LLMs, when appropriately guided, to serve as
sophisticated evaluators, offering a pathway towards more trustworthy and
clinically-aligned quality control for AI in medical imaging.

</details>


### [183] [UNSURF: Uncertainty Quantification for Cortical Surface Reconstruction of Clinical Brain MRIs](https://arxiv.org/abs/2506.00498)
*Raghav Mehta,Karthik Gopinath,Ben Glocker,Juan Eugenio Iglesias*

Main category: eess.IV

TL;DR: UNSURF是一种用于临床脑MRI扫描皮质表面重建的新型不确定性测量方法，适用于任何方向、分辨率和对比度。


<details>
  <summary>Details</summary>
Motivation: 传统的不确定性测量方法（如体素级蒙特卡洛方差）不适合建模表面放置的不确定性，因此需要一种更有效的方法。

Method: 通过比较预测的体素级符号距离函数（SDF）与拟合表面的实际SDF之间的差异，提出UNSURF。

Result: UNSURF的估计值与真实误差相关性良好，可用于自动化质量控制，并在下游阿尔茨海默病分类任务中提升性能。

Conclusion: UNSURF是一种有效的皮质表面重建不确定性测量工具，适用于临床MRI扫描的多种应用场景。

Abstract: We propose UNSURF, a novel uncertainty measure for cortical surface
reconstruction of clinical brain MRI scans of any orientation, resolution, and
contrast. It relies on the discrepancy between predicted voxel-wise signed
distance functions (SDFs) and the actual SDFs of the fitted surfaces. Our
experiments on real clinical scans show that traditional uncertainty measures,
such as voxel-wise Monte Carlo variance, are not suitable for modeling the
uncertainty of surface placement. Our results demonstrate that UNSURF estimates
correlate well with the ground truth errors and: \textit{(i)}~enable effective
automated quality control of surface reconstructions at the subject-, parcel-,
mesh node-level; and \textit{(ii)}~improve performance on a downstream
Alzheimer's disease classification task.

</details>


### [184] [MR2US-Pro: Prostate MR to Ultrasound Image Translation and Registration Based on Diffusion Models](https://arxiv.org/abs/2506.00591)
*Xudong Ma,Nantheera Anantrasirichai,Stefanos Bolomytis,Alin Achim*

Main category: eess.IV

TL;DR: 提出了一种新颖的两阶段框架，用于解决前列腺癌诊断中MRI和TRUS图像配准的挑战，包括TRUS 3D重建和跨模态配准，无需外部探头跟踪信息。


<details>
  <summary>Details</summary>
Motivation: 多模态成像（MRI和TRUS）在前列腺癌诊断中应用广泛，但由于维度和解剖表示的差异，其配准仍具挑战性。

Method: 采用两阶段方法：1）基于TRUS视图自然相关性的3D重建；2）通过伪中间模态的无监督扩散框架进行配准，结合解剖感知策略。

Result: 实验表明，该方法在完全无监督的情况下，实现了优于现有方法的配准精度和物理真实的变形。

Conclusion: 该框架为多模态图像配准提供了一种高效且无需外部信息的解决方案，具有临床应用潜力。

Abstract: The diagnosis of prostate cancer increasingly depends on multimodal imaging,
particularly magnetic resonance imaging (MRI) and transrectal ultrasound
(TRUS). However, accurate registration between these modalities remains a
fundamental challenge due to the differences in dimensionality and anatomical
representations. In this work, we present a novel framework that addresses
these challenges through a two-stage process: TRUS 3D reconstruction followed
by cross-modal registration. Unlike existing TRUS 3D reconstruction methods
that rely heavily on external probe tracking information, we propose a totally
probe-location-independent approach that leverages the natural correlation
between sagittal and transverse TRUS views. With the help of our
clustering-based feature matching method, we enable the spatial localization of
2D frames without any additional probe tracking information. For the
registration stage, we introduce an unsupervised diffusion-based framework
guided by modality translation. Unlike existing methods that translate one
modality into another, we map both MR and US into a pseudo intermediate
modality. This design enables us to customize it to retain only
registration-critical features, greatly easing registration. To further enhance
anatomical alignment, we incorporate an anatomy-aware registration strategy
that prioritizes internal structural coherence while adaptively reducing the
influence of boundary inconsistencies. Extensive validation demonstrates that
our approach outperforms state-of-the-art methods by achieving superior
registration accuracy with physically realistic deformations in a completely
unsupervised fashion.

</details>


### [185] [ABCDEFGH: An Adaptation-Based Convolutional Neural Network-CycleGAN Disease-Courses Evolution Framework Using Generative Models in Health Education](https://arxiv.org/abs/2506.00605)
*Ruiming Min,Minghao Liu*

Main category: eess.IV

TL;DR: 论文探讨了利用卷积神经网络（CNN）和CycleGAN生成合成医学图像，以解决医学教育中高质量教学材料不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现代医学教育因隐私问题和资源短缺而难以获取高质量教学材料，机器学习生成的图像数据提供了一种解决方案。

Method: 研究采用了卷积神经网络（CNN）和CycleGAN来生成合成医学图像。

Result: 成功生成了多样且可比的医学图像数据集，支持医学教育。

Conclusion: 生成模型为医学教育提供了隐私安全的图像资源，具有实际应用潜力。

Abstract: With the advancement of modern medicine and the development of technologies
such as MRI, CT, and cellular analysis, it has become increasingly critical for
clinicians to accurately interpret various diagnostic images. However, modern
medical education often faces challenges due to limited access to high-quality
teaching materials, stemming from privacy concerns and a shortage of
educational resources (Balogh et al., 2015). In this context, image data
generated by machine learning models, particularly generative models, presents
a promising solution. These models can create diverse and comparable imaging
datasets without compromising patient privacy, thereby supporting modern
medical education. In this study, we explore the use of convolutional neural
networks (CNNs) and CycleGAN (Zhu et al., 2017) for generating synthetic
medical images. The source code is available at
https://github.com/mliuby/COMP4211-Project.

</details>


### [186] [CineMA: A Foundation Model for Cine Cardiac MRI](https://arxiv.org/abs/2506.00679)
*Yunguan Fu,Weixi Yi,Charlotte Manisty,Anish N Bhuva,Thomas A Treibel,James C Moon,Matthew J Clarkson,Rhodri Huw Davies,Yipeng Hu*

Main category: eess.IV

TL;DR: CineMA是一种基于自监督学习的AI模型，用于自动化心脏磁共振（CMR）图像分析任务，减少人工标注负担，并在多项任务中表现优于传统卷积神经网络（CNN）。


<details>
  <summary>Details</summary>
Motivation: 传统CMR图像分析（如射血分数计算）耗时且主观，需要自动化解决方案。

Method: CineMA采用自监督自动编码器，通过掩码输入重建图像，并在74,916个CMR研究上进行训练，随后微调以完成多种任务。

Result: CineMA在8个数据集上的23项任务中表现优异，标签效率高于CNN，减少了标注需求。

Conclusion: CineMA为CMR分析提供了高效的基础模型，支持未来临床应用和资源民主化。

Abstract: Cardiac magnetic resonance (CMR) is a key investigation in clinical
cardiovascular medicine and has been used extensively in population research.
However, extracting clinically important measurements such as ejection fraction
for diagnosing cardiovascular diseases remains time-consuming and subjective.
We developed CineMA, a foundation AI model automating these tasks with limited
labels. CineMA is a self-supervised autoencoder model trained on 74,916 cine
CMR studies to reconstruct images from masked inputs. After fine-tuning, it was
evaluated across eight datasets on 23 tasks from four categories: ventricle and
myocardium segmentation, left and right ventricle ejection fraction
calculation, disease detection and classification, and landmark localisation.
CineMA is the first foundation model for cine CMR to match or outperform
convolutional neural networks (CNNs). CineMA demonstrated greater label
efficiency than CNNs, achieving comparable or better performance with fewer
annotations. This reduces the burden of clinician labelling and supports
replacing task-specific training with fine-tuning foundation models in future
cardiac imaging applications. Models and code for pre-training and fine-tuning
are available at https://github.com/mathpluscode/CineMA, democratising access
to high-performance models that otherwise require substantial computational
resources, promoting reproducibility and accelerating clinical translation.

</details>


### [187] [NTIRE 2025 the 2nd Restore Any Image Model (RAIM) in the Wild Challenge](https://arxiv.org/abs/2506.01394)
*Jie Liang,Radu Timofte,Qiaosi Yi,Zhengqiang Zhang,Shuaizheng Liu,Lingchen Sun,Rongyuan Wu,Xindong Zhang,Hui Zeng,Lei Zhang*

Main category: eess.IV

TL;DR: NTIRE 2025挑战赛聚焦于真实世界图像修复，分为两个赛道：低光联合去噪与去马赛克（JDD）和图像细节增强/生成，吸引了大量参与者并推动了技术进步。


<details>
  <summary>Details</summary>
Motivation: 为真实世界图像修复建立新基准，解决复杂未知退化问题，同时评估感知质量和保真度。

Method: 挑战赛分为两个赛道，每个赛道包含两个子任务：基于配对数据的定量评估和基于非配对数据的主观质量评估。

Result: 吸引了300名注册者，51个团队提交600多份结果，顶级方法获得专家一致认可并推动了领域发展。

Conclusion: NTIRE 2025挑战赛成功推动了图像修复技术的进步，并为未来研究提供了宝贵数据和基准。

Abstract: In this paper, we present a comprehensive overview of the NTIRE 2025
challenge on the 2nd Restore Any Image Model (RAIM) in the Wild. This challenge
established a new benchmark for real-world image restoration, featuring diverse
scenarios with and without reference ground truth. Participants were tasked
with restoring real-captured images suffering from complex and unknown
degradations, where both perceptual quality and fidelity were critically
evaluated. The challenge comprised two tracks: (1) the low-light joint
denoising and demosaicing (JDD) task, and (2) the image detail
enhancement/generation task. Each track included two sub-tasks. The first
sub-task involved paired data with available ground truth, enabling
quantitative evaluation. The second sub-task dealt with real-world yet unpaired
images, emphasizing restoration efficiency and subjective quality assessed
through a comprehensive user study. In total, the challenge attracted nearly
300 registrations, with 51 teams submitting more than 600 results. The
top-performing methods advanced the state of the art in image restoration and
received unanimous recognition from all 20+ expert judges. The datasets used in
Track 1 and Track 2 are available at
https://drive.google.com/drive/folders/1Mgqve-yNcE26IIieI8lMIf-25VvZRs_J and
https://drive.google.com/drive/folders/1UB7nnzLwqDZOwDmD9aT8J0KVg2ag4Qae,
respectively. The official challenge pages for Track 1 and Track 2 can be found
at https://codalab.lisn.upsaclay.fr/competitions/21334#learn_the_details and
https://codalab.lisn.upsaclay.fr/competitions/21623#learn_the_details.

</details>


### [188] [RAW Image Reconstruction from RGB on Smartphones. NTIRE 2025 Challenge Report](https://arxiv.org/abs/2506.01947)
*Marcos V. Conde,Radu Timofte,Radu Berdan,Beril Besbinar,Daisuke Iso,Pengzhou Ji,Xiong Dun,Zeying Fan,Chen Wu,Zhansheng Wang,Pengbo Zhang,Jiazi Huang,Qinglin Liu,Wei Yu,Shengping Zhang,Xiangyang Ji,Kyungsik Kim,Minkyung Kim,Hwalmin Lee,Hekun Ma,Huan Zheng,Yanyan Wei,Zhao Zhang,Jing Fang,Meilin Gao,Xiang Yu,Shangbin Xie,Mengyuan Sun,Huanjing Yue,Jingyu Yang Huize Cheng,Shaomeng Zhang,Zhaoyang Zhang,Haoxiang Liang*

Main category: eess.IV

TL;DR: 论文探讨了从sRGB图像重建RAW传感器图像的挑战（Reverse ISP），旨在无需元数据的情况下恢复智能手机的RAW图像，并提出了高效模型。


<details>
  <summary>Details</summary>
Motivation: 由于RAW图像数据集稀缺且昂贵，而sRGB数据集丰富且公开，因此需要从sRGB图像生成逼真的RAW图像。

Method: 通过挑战赛形式，收集了150多名参与者提交的高效模型，用于从sRGB图像逆向生成RAW图像。

Result: 提出的方法和基准测试确立了生成逼真RAW数据的最新技术。

Conclusion: 该研究为RAW图像重建提供了新的解决方案，并推动了相关技术的发展。

Abstract: Numerous low-level vision tasks operate in the RAW domain due to its linear
properties, bit depth, and sensor designs. Despite this, RAW image datasets are
scarce and more expensive to collect than the already large and public sRGB
datasets. For this reason, many approaches try to generate realistic RAW images
using sensor information and sRGB images. This paper covers the second
challenge on RAW Reconstruction from sRGB (Reverse ISP). We aim to recover RAW
sensor images from smartphones given the corresponding sRGB images without
metadata and, by doing this, ``reverse" the ISP transformation. Over 150
participants joined this NTIRE 2025 challenge and submitted efficient models.
The proposed methods and benchmark establish the state-of-the-art for
generating realistic RAW data.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [189] [MotionPersona: Characteristics-aware Locomotion Control](https://arxiv.org/abs/2506.00173)
*Mingyi Shi,Wei Liu,Jidong Mei,Wangpok Tse,Rui Chen,Xuelin Chen,Taku Komura*

Main category: cs.GR

TL;DR: MotionPersona是一种实时角色控制器，通过用户定义的属性（如身体特征、心理状态和人口统计信息）生成符合特征的运动动画。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习控制器通常生成单一角色的同质动画，而MotionPersona旨在模拟真实世界中不同特征对人类运动的影响。

Method: 采用基于SMPLX参数、文本提示和用户控制信号的条件块自回归运动扩散模型，并构建了涵盖多种运动类型和角色特征的数据集。

Result: MotionPersona能够实时生成忠实反映用户指定特征的运动（如老年人的蹒跚步态），并在实验中表现出优于现有方法的运动质量和多样性。

Conclusion: MotionPersona是首个能够结合用户特征和实时控制输入生成多样化运动的方法，并通过少样本技术进一步扩展了定制能力。

Abstract: We present MotionPersona, a novel real-time character controller that allows
users to characterize a character by specifying attributes such as physical
traits, mental states, and demographics, and projects these properties into the
generated motions for animating the character. In contrast to existing deep
learning-based controllers, which typically produce homogeneous animations
tailored to a single, predefined character, MotionPersona accounts for the
impact of various traits on human motion as observed in the real world. To
achieve this, we develop a block autoregressive motion diffusion model
conditioned on SMPLX parameters, textual prompts, and user-defined locomotion
control signals. We also curate a comprehensive dataset featuring a wide range
of locomotion types and actor traits to enable the training of this
characteristic-aware controller. Unlike prior work, MotionPersona is the first
method capable of generating motion that faithfully reflects user-specified
characteristics (e.g., an elderly person's shuffling gait) while responding in
real time to dynamic control inputs. Additionally, we introduce a few-shot
characterization technique as a complementary conditioning mechanism, enabling
customization via short motion clips when language prompts fall short. Through
extensive experiments, we demonstrate that MotionPersona outperforms existing
methods in characteristics-aware locomotion control, achieving superior motion
quality and diversity. Results, code, and demo can be found at:
https://motionpersona25.github.io/.

</details>


### [190] [Power-Linear Polar Directional Fields](https://arxiv.org/abs/2506.00222)
*Jiabao Brad Wang,Amir Vaxman*

Main category: cs.GR

TL;DR: 提出了一种新的网格方向场设计方法，支持在任意位置指定奇点，采用分段幂线性表示，提供精确的拓扑控制。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在粗糙或不均匀网格上产生的伪影问题，同时支持任意奇点指数和场对称性。

Method: 使用分段幂线性表示相位和尺度，确保方向场的平滑性和拓扑精确性。

Result: 生成的场平滑且适应性强，适用于不同拓扑和三角形质量的网格。

Conclusion: 该方法有效解决了网格方向场设计中的伪影问题，提供了更高的灵活性和精确性。

Abstract: We introduce a novel method for directional-field design on meshes, enabling
users to specify singularities at any location on a mesh. Our method uses a
piecewise power-linear representation for phase and scale, offering precise
control over field topology. The resulting fields are smooth and accommodate
any singularity index and field symmetry. With this representation, we mitigate
the artifacts caused by coarse or uneven meshes. We showcase our approach on
meshes with diverse topologies and triangle qualities.

</details>


### [191] [Pro3D-Editor : A Progressive-Views Perspective for Consistent and Precise 3D Editing](https://arxiv.org/abs/2506.00512)
*Yang Zheng,Mengqi Huang,Nan Chen,Zhendong Mao*

Main category: cs.GR

TL;DR: 论文提出了一种基于渐进视图范式的3D编辑方法Pro3D-Editor，通过动态采样主视图、传播编辑语义到关键视图，最终优化全视图，实现了更准确和一致的3D编辑。


<details>
  <summary>Details</summary>
Motivation: 现有3D编辑方法在多视图编辑中存在不一致性问题，忽视了视图间的相互依赖性。本文旨在通过渐进视图范式解决这一问题。

Method: Pro3D-Editor框架包括动态采样主视图的主视图采样器、通过MoVE-LoRA传播语义的关键视图渲染器，以及基于多视图编辑的全视图优化器。

Result: 实验表明，该方法在编辑准确性和空间一致性上优于现有方法。

Conclusion: 渐进视图范式为3D编辑提供了一种更一致和高效的解决方案。

Abstract: Text-guided 3D editing aims to precisely edit semantically relevant local 3D
regions, which has significant potential for various practical applications
ranging from 3D games to film production. Existing methods typically follow a
view-indiscriminate paradigm: editing 2D views indiscriminately and projecting
them back into 3D space. However, they overlook the different cross-view
interdependencies, resulting in inconsistent multi-view editing. In this study,
we argue that ideal consistent 3D editing can be achieved through a
\textit{progressive-views paradigm}, which propagates editing semantics from
the editing-salient view to other editing-sparse views. Specifically, we
propose \textit{Pro3D-Editor}, a novel framework, which mainly includes
Primary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view
Sampler dynamically samples and edits the most editing-salient view as the
primary view. Key-view Render accurately propagates editing semantics from the
primary view to other key views through its Mixture-of-View-Experts Low-Rank
Adaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based
on the edited multi-views. Extensive experiments demonstrate that our method
outperforms existing methods in editing accuracy and spatial consistency.

</details>


### [192] [Neural Path Guiding with Distribution Factorization](https://arxiv.org/abs/2506.00839)
*Pedro Figueiredo,Qihao He,Nima Khademi Kalantari*

Main category: cs.GR

TL;DR: 提出一种神经路径引导方法，用于改进蒙特卡洛积分在渲染中的应用，通过分解2D方向分布为两个1D概率分布函数（PDF），并利用神经网络建模，实现高效且表达力强的分布表示。


<details>
  <summary>Details</summary>
Motivation: 现有神经方法在分布表示上无法同时兼顾速度和表达力，因此需要一种更优的解决方案。

Method: 将2D方向分布分解为两个1D PDF，用神经网络建模离散坐标上的分布，并通过插值实现任意位置的评估和采样；训练时最大化学习分布与目标分布的相似性，并使用额外网络缓存入射辐射以减少梯度方差。

Result: 实验表明，该方法在复杂光照场景中优于现有方法。

Conclusion: 提出的方法在表达力和速度上取得平衡，适用于复杂光线传输场景。

Abstract: In this paper, we present a neural path guiding method to aid with Monte
Carlo (MC) integration in rendering. Existing neural methods utilize
distribution representations that are either fast or expressive, but not both.
We propose a simple, but effective, representation that is sufficiently
expressive and reasonably fast. Specifically, we break down the 2D distribution
over the directional domain into two 1D probability distribution functions
(PDF). We propose to model each 1D PDF using a neural network that estimates
the distribution at a set of discrete coordinates. The PDF at an arbitrary
location can then be evaluated and sampled through interpolation. To train the
network, we maximize the similarity of the learned and target distributions. To
reduce the variance of the gradient during optimizations and estimate the
normalization factor, we propose to cache the incoming radiance using an
additional network. Through extensive experiments, we demonstrate that our
approach is better than the existing methods, particularly in challenging
scenes with complex light transport.

</details>


### [193] [Hybridizing Expressive Rendering: Stroke-Based Rendering with Classic and Neural Methods](https://arxiv.org/abs/2506.00870)
*Kapil Dev*

Main category: cs.GR

TL;DR: 论文分析了传统非真实感渲染（NPR）与基于深度学习的NPR技术在笔触渲染（SBR）上的异同，探讨了质量与艺术控制的权衡，并提出了一种结合两者的框架。


<details>
  <summary>Details</summary>
Motivation: 研究传统NPR与深度学习NPR技术的异同，探索如何结合两者以实现更具表现力的渲染效果。

Method: 分析传统NPR（如边缘检测、卡通着色）与神经网络NPR在笔触渲染上的优缺点，提出结合框架。

Result: 揭示了两种技术的优势与局限，展示了结合框架的潜力。

Conclusion: 结合传统与深度学习NPR技术可以拓展表现力，为艺术渲染提供新可能。

Abstract: Non-Photorealistic Rendering (NPR) has long been used to create artistic
visualizations that prioritize style over realism, enabling the depiction of a
wide range of aesthetic effects, from hand-drawn sketches to painterly
renderings. While classical NPR methods, such as edge detection, toon shading,
and geometric abstraction, have been well-established in both research and
practice, with a particular focus on stroke-based rendering, the recent rise of
deep learning represents a paradigm shift. We analyze the similarities and
differences between classical and neural network based NPR techniques, focusing
on stroke-based rendering (SBR), highlighting their strengths and limitations.
We discuss trade offs in quality and artistic control between these paradigms,
propose a framework where these approaches can be combined for new
possibilities in expressive rendering.

</details>


### [194] [LensCraft: Your Professional Virtual Cinematographer](https://arxiv.org/abs/2506.00988)
*Zahra Dehghanian,Morteza Abolghasemi,Hossein Azizinaghsh,Amir Vahedi,Hamid Beigy,Hamid R. Rabiee*

Main category: cs.GR

TL;DR: LensCraft 提出了一种数据驱动的方法，结合电影摄影原则，解决了自动化拍摄系统中机械执行与创意意图之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前自动化拍摄系统忽略主体的方向和体积，限制了空间感知能力，LensCraft 旨在模仿专业摄影师的专业知识。

Method: 结合专业模拟框架生成高保真训练数据，使用先进的神经模型，支持多种输入模态（如文本提示、轨迹、关键点等）。

Result: 在静态和动态场景中表现出前所未有的准确性和一致性，计算复杂度低且推理速度快。

Conclusion: LensCraft 为智能相机系统设定了新基准，提供了公开的数据集、代码和模型。

Abstract: Digital creators, from indie filmmakers to animation studios, face a
persistent bottleneck: translating their creative vision into precise camera
movements. Despite significant progress in computer vision and artificial
intelligence, current automated filming systems struggle with a fundamental
trade-off between mechanical execution and creative intent. Crucially, almost
all previous works simplify the subject to a single point-ignoring its
orientation and true volume-severely limiting spatial awareness during filming.
LensCraft solves this problem by mimicking the expertise of a professional
cinematographer, using a data-driven approach that combines cinematographic
principles with the flexibility to adapt to dynamic scenes in real time. Our
solution combines a specialized simulation framework for generating
high-fidelity training data with an advanced neural model that is faithful to
the script while being aware of the volume and dynamic behavior of the subject.
Additionally, our approach allows for flexible control via various input
modalities, including text prompts, subject trajectory and volume, key points,
or a full camera trajectory, offering creators a versatile tool to guide camera
movements in line with their vision. Leveraging a lightweight real time
architecture, LensCraft achieves markedly lower computational complexity and
faster inference while maintaining high output quality. Extensive evaluation
across static and dynamic scenarios reveals unprecedented accuracy and
coherence, setting a new benchmark for intelligent camera systems compared to
state-of-the-art models. Extended results, the complete dataset, simulation
environment, trained model weights, and source code are publicly accessible on
LensCraft Webpage.

</details>


### [195] [TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans](https://arxiv.org/abs/2506.01077)
*Yueqian Guo,Tianzhao Li,Xin Lyu,Jiehaolin Chen,Zhaohan Wang,Sirui Xiao,Yurun Chen,Yezi He,Helin Li,Fan Zhang*

Main category: cs.GR

TL;DR: TRiMM是一种基于Transformer的多模态框架，用于实时3D手势生成，解决了现有方法在实时合成和长文本理解上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实时合成和长文本理解方面表现不佳，限制了LLM驱动的数字人类的应用。

Method: TRiMM结合了跨模态注意力机制、长上下文自回归模型和大规模手势匹配系统，并通过轻量级管道在Unreal Engine中实现。

Result: 在消费级GPU上实现120 fps的实时推理，每句延迟0.15秒，并在ZEGGS和BEAT数据集上表现优于现有方法。

Conclusion: TRiMM在保证手势质量的同时提升了生成速度，使LLM驱动的数字人类能够实时响应语音并生成手势。

Abstract: Large Language Model (LLM)-driven digital humans have sparked a series of
recent studies on co-speech gesture generation systems. However, existing
approaches struggle with real-time synthesis and long-text comprehension. This
paper introduces Transformer-Based Rich Motion Matching (TRiMM), a novel
multi-modal framework for real-time 3D gesture generation. Our method
incorporates three modules: 1) a cross-modal attention mechanism to achieve
precise temporal alignment between speech and gestures; 2) a long-context
autoregressive model with a sliding window mechanism for effective sequence
modeling; 3) a large-scale gesture matching system that constructs an atomic
action library and enables real-time retrieval. Additionally, we develop a
lightweight pipeline implemented in the Unreal Engine for experimentation. Our
approach achieves real-time inference at 120 fps and maintains a per-sentence
latency of 0.15 seconds on consumer-grade GPUs (Geforce RTX3060). Extensive
subjective and objective evaluations on the ZEGGS, and BEAT datasets
demonstrate that our model outperforms current state-of-the-art methods. TRiMM
enhances the speed of co-speech gesture generation while ensuring gesture
quality, enabling LLM-driven digital humans to respond to speech in real time
and synthesize corresponding gestures. Our code is available at
https://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching

</details>


### [196] [PromptVFX: Text-Driven Fields for Open-World 3D Gaussian Animation](https://arxiv.org/abs/2506.01091)
*Mert Kiray,Paul Uhlenbruck,Nassir Navab,Benjamin Busam*

Main category: cs.GR

TL;DR: 论文提出了一种基于文本驱动的4D流场预测框架，用于实时生成3D动画效果，减少了传统方法的时间和专业要求。


<details>
  <summary>Details</summary>
Motivation: 现代影视、游戏和AR/VR中，视觉特效（VFX）对沉浸感至关重要，但传统3D动画制作需要专业技能且耗时。现有生成方法计算量大、速度慢。

Method: 将3D动画重构为场预测任务，利用大语言模型（LLMs）和视觉语言模型（VLMs）生成函数，通过文本提示实时更新3D高斯模型的颜色、透明度和位置。

Result: 实验表明，简单的文本指令即可生成动态VFX，显著减少了手动建模和物理模拟的工作量。

Conclusion: 该方法为语言驱动的3D内容创作提供了快速、易用的途径，有望进一步普及VFX制作。

Abstract: Visual effects (VFX) are key to immersion in modern films, games, and AR/VR.
Creating 3D effects requires specialized expertise and training in 3D animation
software and can be time consuming. Generative solutions typically rely on
computationally intense methods such as diffusion models which can be slow at
4D inference. We reformulate 3D animation as a field prediction task and
introduce a text-driven framework that infers a time-varying 4D flow field
acting on 3D Gaussians. By leveraging large language models (LLMs) and
vision-language models (VLMs) for function generation, our approach interprets
arbitrary prompts (e.g., "make the vase glow orange, then explode") and
instantly updates color, opacity, and positions of 3D Gaussians in real time.
This design avoids overheads such as mesh extraction, manual or physics-based
simulations and allows both novice and expert users to animate volumetric
scenes with minimal effort on a consumer device even in a web browser.
Experimental results show that simple textual instructions suffice to generate
compelling time-varying VFX, reducing the manual effort typically required for
rigging or advanced modeling. We thus present a fast and accessible pathway to
language-driven 3D content creation that can pave the way to democratize VFX
further.

</details>


### [197] [WishGI: Lightweight Static Global Illumination Baking via Spherical Harmonics Fitting](https://arxiv.org/abs/2506.01288)
*Junke Zhu,Zehan Wu,Qixing Zhang,Cheng Liao,Zhangjin Huang*

Main category: cs.GR

TL;DR: 提出了一种全局光照重建方法，通过减少片段着色器采样和避免额外渲染通道，适用于低端平台。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量纹理存储和像素级采样，导致性能开销大，难以在低端平台上运行。

Method: 采用球谐函数拟合烘焙光照信息，并提出逆探针分布方法为每个网格生成独特的探针关联。

Result: 仅需主流行业技术约5%的内存，即可实现高质量的全局光照效果。

Conclusion: 该方法在低端平台上实现了高性能和高质量的全局光照效果。

Abstract: Global illumination combines direct and indirect lighting to create realistic
lighting effects, bringing virtual scenes closer to reality. Static global
illumination is a crucial component of virtual scene rendering, leveraging
precomputation and baking techniques to significantly reduce runtime
computational costs. Unfortunately, many existing works prioritize visual
quality by relying on extensive texture storage and massive pixel-level texture
sampling, leading to large performance overhead. In this paper, we introduce an
illumination reconstruction method that effectively reduces sampling in
fragment shader and avoids additional render passes, making it well-suited for
low-end platforms. To achieve high-quality global illumination with reduced
memory usage, we adopt a spherical harmonics fitting approach for baking
effective illumination information and propose an inverse probe distribution
method that generates unique probe associations for each mesh. This
association, which can be generated offline in the local space, ensures
consistent lighting quality across all instances of the same mesh. As a
consequence, our method delivers highly competitive lighting effects while
using only approximately 5% of the memory required by mainstream industry
techniques.

</details>


### [198] [Silence is Golden: Leveraging Adversarial Examples to Nullify Audio Control in LDM-based Talking-Head Generation](https://arxiv.org/abs/2506.01591)
*Yuan Gan,Jiaxu Miao,Yunze Wang,Yi Yang*

Main category: cs.GR

TL;DR: 论文提出了一种名为Silencer的两阶段方法，用于主动保护肖像隐私，防止基于LDM的说话头动画技术被滥用。


<details>
  <summary>Details</summary>
Motivation: 由于LDM技术生成的说话头动画高度逼真，可能被滥用于诈骗、政治操纵和虚假信息传播，因此需要解决这一伦理问题。

Method: Silencer采用两阶段方法：1）提出无效化损失以忽略音频控制；2）应用抗净化损失优化潜在特征以生成鲁棒扰动。

Result: 实验证明Silencer能有效保护肖像隐私。

Conclusion: 该研究旨在提高AI安全社区对说话头生成技术伦理问题的关注。

Abstract: Advances in talking-head animation based on Latent Diffusion Models (LDM)
enable the creation of highly realistic, synchronized videos. These fabricated
videos are indistinguishable from real ones, increasing the risk of potential
misuse for scams, political manipulation, and misinformation. Hence, addressing
these ethical concerns has become a pressing issue in AI security. Recent
proactive defense studies focused on countering LDM-based models by adding
perturbations to portraits. However, these methods are ineffective at
protecting reference portraits from advanced image-to-video animation. The
limitations are twofold: 1) they fail to prevent images from being manipulated
by audio signals, and 2) diffusion-based purification techniques can
effectively eliminate protective perturbations. To address these challenges, we
propose Silencer, a two-stage method designed to proactively protect the
privacy of portraits. First, a nullifying loss is proposed to ignore audio
control in talking-head generation. Second, we apply anti-purification loss in
LDM to optimize the inverted latent feature to generate robust perturbations.
Extensive experiments demonstrate the effectiveness of Silencer in proactively
protecting portrait privacy. We hope this work will raise awareness among the
AI security community regarding critical ethical issues related to talking-head
generation techniques. Code: https://github.com/yuangan/Silencer.

</details>


### [199] [Image Generation from Contextually-Contradictory Prompts](https://arxiv.org/abs/2506.01929)
*Saar Huberman,Or Patashnik,Omer Dahary,Ron Mokady,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: 提出了一种阶段感知的提示分解框架，通过代理提示引导去噪过程，解决文本到图像扩散模型中的上下文矛盾问题。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型在生成高质量图像时，常因提示中的概念组合与学习先验矛盾而失败。

Method: 使用大语言模型分析目标提示，生成代理提示序列，确保语义连贯并匹配去噪阶段。

Result: 实验表明，该方法显著提高了图像与文本提示的对齐度。

Conclusion: 通过阶段感知提示分解，实现了对上下文矛盾的精细语义控制，提升了生成准确性。

Abstract: Text-to-image diffusion models excel at generating high-quality, diverse
images from natural language prompts. However, they often fail to produce
semantically accurate results when the prompt contains concept combinations
that contradict their learned priors. We define this failure mode as contextual
contradiction, where one concept implicitly negates another due to entangled
associations learned during training. To address this, we propose a stage-aware
prompt decomposition framework that guides the denoising process using a
sequence of proxy prompts. Each proxy prompt is constructed to match the
semantic content expected to emerge at a specific stage of denoising, while
ensuring contextual coherence. To construct these proxy prompts, we leverage a
large language model (LLM) to analyze the target prompt, identify
contradictions, and generate alternative expressions that preserve the original
intent while resolving contextual conflicts. By aligning prompt information
with the denoising progression, our method enables fine-grained semantic
control and accurate image generation in the presence of contextual
contradictions. Experiments across a variety of challenging prompts show
substantial improvements in alignment to the textual prompt.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [200] [DiffPINN: Generative diffusion-initialized physics-informed neural networks for accelerating seismic wavefield representation](https://arxiv.org/abs/2506.00471)
*Shijun Cheng,Tariq Alkhalifah*

Main category: physics.geo-ph

TL;DR: 提出了一种基于潜在扩散的策略，用于快速初始化物理信息神经网络（PINNs），以解决传统方法在不同速度模型中需要耗时重新训练的问题。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在不同速度模型中需要重新训练且收敛慢，影响了其在波场建模中的效率。

Method: 通过训练多个PINN生成参数向量，使用自编码器学习潜在表示，并训练条件扩散模型生成新速度模型对应的PINN参数。

Result: 实验表明该方法显著加速训练，并在分布内和分布外速度场景中保持高精度。

Conclusion: 该方法为PINNs在不同速度模型中的快速初始化提供了有效解决方案。

Abstract: Physics-informed neural networks (PINNs) offer a powerful framework for
seismic wavefield modeling, yet they typically require time-consuming
retraining when applied to different velocity models. Moreover, their training
can suffer from slow convergence due to the complexity of of the wavefield
solution. To address these challenges, we introduce a latent diffusion-based
strategy for rapid and effective PINN initialization. First, we train multiple
PINNs to represent frequency-domain scattered wavefields for various velocity
models, then flatten each trained network's parameters into a one-dimensional
vector, creating a comprehensive parameter dataset. Next, we employ an
autoencoder to learn latent representations of these parameter vectors,
capturing essential patterns across diverse PINN's parameters. We then train a
conditional diffusion model to store the distribution of these latent vectors,
with the corresponding velocity models serving as conditions. Once trained,
this diffusion model can generate latent vectors corresponding to new velocity
models, which are subsequently decoded by the autoencoder into complete PINN
parameters. Experimental results indicate that our method significantly
accelerates training and maintains high accuracy across in-distribution and
out-of-distribution velocity scenarios.

</details>


### [201] [Generalized nearest-neighbor distance and Hawkes point process modeling applied to mining induced seismicity](https://arxiv.org/abs/2506.00768)
*Mohammadamin Sedghizadeh,Robert Shcherbakov,Matthew van den Berghe*

Main category: physics.geo-ph

TL;DR: 论文提出了一种广义化的最近邻距离（NND）方法，结合任意分布函数分析地震活动的频率-震级统计特性，并通过霍克斯过程建模采矿地震率，揭示了地震活动主要由外部因素驱动。


<details>
  <summary>Details</summary>
Motivation: 研究采矿等资源开采活动引发的地震活动率与聚类特性，以理解其物理和统计特征并评估相关风险。

Method: 广义化NND方法，结合双截断帕累托分布，分析地震聚类特性；应用霍克斯过程建模地震率。

Result: 地震活动主要由外部因素驱动，缺乏显著的事件间触发；通过萨斯喀彻温省钾盐矿案例验证了方法的有效性。

Conclusion: 广义NND方法和霍克斯过程为采矿环境中的诱发地震活动提供了新的分析工具，揭示了其聚类特性和时间变化规律。

Abstract: Modeling seismic activity rates and clustering plays an important role in
studies of induced seismicity associated with mining and other resource
extraction operations. This is critical for understanding the physical and
statistical characteristics of seismicity and assessing the associated hazard.
In this work, we introduce the generalization of the Nearest-Neighbor Distance
(NND) method by incorporating an arbitrary distribution function for the
frequency-magnitude statistics of seismic events. Operating within a rescaled
hyperspace that includes spatial, temporal, and magnitude domains, the NND
method provides an effective framework for examining seismic clustering. By
integrating a mixture of the two tapered Pareto distributions, the generalized
NND approach accommodates deviations from standard frequency-magnitude scaling
when studying the clustering properties of seismicity. In addition, the
application of the temporal Hawkes process to model the mining seismicity rate
reveals that the seismicity is primarily driven by external factors and lacks
pronounced interevent triggering. A case study from a potash mine in
Saskatchewan is presented to illustrate the application of the generalized NND
method and the Hawkes process to estimate the clustering properties and
occurrence rates of induced microseismicity. The implications of observed
temporal variations and clustering behavior are discussed, providing insights
into the nature of induced seismicity within mining environments.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [202] [Electronic Temperature-Driven Phase Stability and Structural Evolution of Iron at High Pressure](https://arxiv.org/abs/2506.01809)
*S. Azadi,S. M. Vinko*

Main category: cond-mat.mtrl-sci

TL;DR: 通过有限温度密度泛函理论研究了压缩铁的相图，预测了电子熵和温度驱动的固-固相变。


<details>
  <summary>Details</summary>
Motivation: 研究高压高温下铁的相变行为，特别是电子熵和温度对相变的影响。

Method: 使用有限温度密度泛函理论和密度泛函微扰理论，分析20至300 GPa压力范围和最高3 eV电子温度下的相图。

Result: 预测了hcp到bcc的相变，压力超过200 GPa时相变依赖于电子温度，实验已观察到bcc相的稳定性。

Conclusion: 电子熵和温度在高压下对铁的相变有显著影响，实验结果支持理论预测。

Abstract: We present Gibbs free-energy phase diagrams for compressed iron within a
pressure range of 20 to 300 GPa and electronic temperature up to 3 eV obtained
using finite-temperature density functional and density functional perturbation
theories. Our results for bcc, fcc, and hcp phases predict solid-solid phase
transitions in iron driven purely by electronic entropy and temperature. We
found a phase transition from hcp to bcc at pressures above 200 GPa, which
depends on the electronic temperature. An experimental observation of the
stability of the bcc phase above 200 GPa by X-ray Free Electron Laser has
recently been reported.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [203] [GaussianFusion: Gaussian-Based Multi-Sensor Fusion for End-to-End Autonomous Driving](https://arxiv.org/abs/2506.00034)
*Shuai Liu,Quanmin Liang,Zefeng Li,Boyang Li,Kai Huang*

Main category: cs.RO

TL;DR: GaussianFusion提出了一种基于高斯分布的多传感器融合框架，用于端到端自动驾驶，通过高斯表示聚合多模态信息，提高了性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如基于注意力的扁平融合或鸟瞰图融合）存在可解释性差或计算密集的问题，需要更高效且直观的融合方法。

Method: 使用2D高斯分布作为中间载体，初始化并逐步优化高斯参数，结合显式和隐式特征，设计级联规划头优化轨迹预测。

Result: 在NAVSIM和Bench2Drive基准测试中验证了框架的有效性和鲁棒性。

Conclusion: GaussianFusion提供了一种高效且直观的多传感器融合方法，显著提升了自动驾驶系统的性能。

Abstract: Multi-sensor fusion is crucial for improving the performance and robustness
of end-to-end autonomous driving systems. Existing methods predominantly adopt
either attention-based flatten fusion or bird's eye view fusion through
geometric transformations. However, these approaches often suffer from limited
interpretability or dense computational overhead. In this paper, we introduce
GaussianFusion, a Gaussian-based multi-sensor fusion framework for end-to-end
autonomous driving. Our method employs intuitive and compact Gaussian
representations as intermediate carriers to aggregate information from diverse
sensors. Specifically, we initialize a set of 2D Gaussians uniformly across the
driving scene, where each Gaussian is parameterized by physical attributes and
equipped with explicit and implicit features. These Gaussians are progressively
refined by integrating multi-modal features. The explicit features capture rich
semantic and spatial information about the traffic scene, while the implicit
features provide complementary cues beneficial for trajectory planning. To
fully exploit rich spatial and semantic information in Gaussians, we design a
cascade planning head that iteratively refines trajectory predictions through
interactions with Gaussians. Extensive experiments on the NAVSIM and
Bench2Drive benchmarks demonstrate the effectiveness and robustness of the
proposed GaussianFusion framework. The source code will be released at
https://github.com/Say2L/GaussianFusion.

</details>


### [204] [From Motion to Behavior: Hierarchical Modeling of Humanoid Generative Behavior Control](https://arxiv.org/abs/2506.00043)
*Jusheng Zhang,Jinzhou Tang,Sidi Liu,Mingyan Li,Sheng Zhang,Jian Wang,Keze Wang*

Main category: cs.RO

TL;DR: 论文提出了一种基于认知科学的统一框架GBC，通过结合大语言模型（LLMs）生成的分层行为计划，建模多样的人类运动。实验表明，GBC能生成更长、更多样且高质量的运动。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注低层次短周期运动或高层次动作规划，忽视了人类活动的分层目标导向特性。论文旨在填补这一空白。

Method: 提出Generative Behavior Control（GBC）框架，结合任务和运动规划，并利用LLMs指导运动生成。同时提出GBC-100K数据集，包含分层语义和运动计划。

Result: GBC能生成比现有方法长10倍、更多样且高质量的运动。

Conclusion: GBC为人类行为建模的未来研究奠定了基础，数据集和代码将公开。

Abstract: Human motion generative modeling or synthesis aims to characterize
complicated human motions of daily activities in diverse real-world
environments. However, current research predominantly focuses on either
low-level, short-period motions or high-level action planning, without taking
into account the hierarchical goal-oriented nature of human activities. In this
work, we take a step forward from human motion generation to human behavior
modeling, which is inspired by cognitive science. We present a unified
framework, dubbed Generative Behavior Control (GBC), to model diverse human
motions driven by various high-level intentions by aligning motions with
hierarchical behavior plans generated by large language models (LLMs). Our
insight is that human motions can be jointly controlled by task and motion
planning in robotics, but guided by LLMs to achieve improved motion diversity
and physical fidelity. Meanwhile, to overcome the limitations of existing
benchmarks, i.e., lack of behavioral plans, we propose GBC-100K dataset
annotated with a hierarchical granularity of semantic and motion plans driven
by target goals. Our experiments demonstrate that GBC can generate more diverse
and purposeful high-quality human motions with 10* longer horizons compared
with existing methods when trained on GBC-100K, laying a foundation for future
research on behavioral modeling of human motions. Our dataset and source code
will be made publicly available.

</details>


### [205] [Understanding while Exploring: Semantics-driven Active Mapping](https://arxiv.org/abs/2506.00225)
*Liyan Chen,Huangying Zhan,Hairong Yin,Yi Xu,Philippos Mordohai*

Main category: cs.RO

TL;DR: ActiveSGM是一个主动语义映射框架，通过预测潜在观测的信息量来提升机器人探索未知环境的效率。


<details>
  <summary>Details</summary>
Motivation: 在未知环境中实现有效的机器人自主性需要主动探索和对几何与语义的精确理解。

Method: 基于3D高斯散射（3DGS）映射框架，结合语义和几何不确定性量化以及稀疏语义表示，指导机器人选择最优视角。

Result: 在Replica和Matterport3D数据集上的实验表明，ActiveSGM显著提升了映射的完整性、准确性和对噪声语义数据的鲁棒性。

Conclusion: ActiveSGM通过优化视角选择，支持更自适应的场景探索，提升了主动语义映射的效果。

Abstract: Effective robotic autonomy in unknown environments demands proactive
exploration and precise understanding of both geometry and semantics. In this
paper, we propose ActiveSGM, an active semantic mapping framework designed to
predict the informativeness of potential observations before execution. Built
upon a 3D Gaussian Splatting (3DGS) mapping backbone, our approach employs
semantic and geometric uncertainty quantification, coupled with a sparse
semantic representation, to guide exploration. By enabling robots to
strategically select the most beneficial viewpoints, ActiveSGM efficiently
enhances mapping completeness, accuracy, and robustness to noisy semantic data,
ultimately supporting more adaptive scene exploration. Our experiments on the
Replica and Matterport3D datasets highlight the effectiveness of ActiveSGM in
active semantic mapping tasks.

</details>


### [206] [Using Diffusion Ensembles to Estimate Uncertainty for End-to-End Autonomous Driving](https://arxiv.org/abs/2506.00560)
*Florian Wintel,Sigmund H. Høeg,Gabriel Kiss,Frank Lindseth*

Main category: cs.RO

TL;DR: EnDfuser是一种端到端自动驾驶系统，利用扩散模型作为轨迹规划器，通过集成扩散生成候选轨迹分布，提高驾驶决策的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统在规划中未充分考虑不确定性或使用不具泛化能力的专用表示，EnDfuser旨在解决这一问题。

Method: 结合注意力池化和轨迹规划，使用扩散变换器模块，通过集成扩散从单个感知帧生成128条候选轨迹。

Result: 在CARLA的Longest6基准测试中取得70.1的驾驶分数，同时保持推理速度。

Conclusion: 集成扩散作为传统点估计轨迹规划模块的替代，能够通过建模后验轨迹分布的不确定性提高驾驶安全性。

Abstract: End-to-end planning systems for autonomous driving are improving rapidly,
especially in closed-loop simulation environments like CARLA. Many such driving
systems either do not consider uncertainty as part of the plan itself, or
obtain it by using specialized representations that do not generalize. In this
paper, we propose EnDfuser, an end-to-end driving system that uses a diffusion
model as the trajectory planner. EnDfuser effectively leverages complex
perception information like fused camera and LiDAR features, through combining
attention pooling and trajectory planning into a single diffusion transformer
module. Instead of committing to a single plan, EnDfuser produces a
distribution of candidate trajectories (128 for our case) from a single
perception frame through ensemble diffusion. By observing the full set of
candidate trajectories, EnDfuser provides interpretability for uncertain,
multi-modal future trajectory spaces, where there are multiple plausible
options. EnDfuser achieves a competitive driving score of 70.1 on the Longest6
benchmark in CARLA with minimal concessions on inference speed. Our findings
suggest that ensemble diffusion, used as a drop-in replacement for traditional
point-estimate trajectory planning modules, can help improve the safety of
driving decisions by modeling the uncertainty of the posterior trajectory
distribution.

</details>


### [207] [OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation](https://arxiv.org/abs/2506.01196)
*Ishika Singh,Ankit Goyal,Stan Birchfield,Dieter Fox,Animesh Garg,Valts Blukis*

Main category: cs.RO

TL;DR: OG-VLA结合了视觉语言动作模型（VLA）的泛化能力和3D感知策略的鲁棒性，通过多视角RGBD观测和自然语言指令生成机器人动作。


<details>
  <summary>Details</summary>
Motivation: 解决3D感知策略在未见指令、场景和物体上泛化能力不足的问题，同时弥补VLA对相机和机器人姿态变化的敏感性。

Method: 将多视角观测投影为点云并渲染为标准正交视图，利用视觉骨干网络、大语言模型和图像扩散模型生成末端执行器的位置和方向。

Result: 在Arnold和Colosseum基准测试中，OG-VLA在未见环境中的泛化能力提升了40%以上，同时在已知场景中保持鲁棒性。

Conclusion: OG-VLA通过结合语言和视觉先验知识，显著提升了机器人动作生成的泛化能力和鲁棒性。

Abstract: We introduce OG-VLA, a novel architecture and learning framework that
combines the generalization strengths of Vision Language Action models (VLAs)
with the robustness of 3D-aware policies. We address the challenge of mapping
natural language instructions and multi-view RGBD observations to quasi-static
robot actions. 3D-aware robot policies achieve state-of-the-art performance on
precise robot manipulation tasks, but struggle with generalization to unseen
instructions, scenes, and objects. On the other hand, VLAs excel at
generalizing across instructions and scenes, but can be sensitive to camera and
robot pose variations. We leverage prior knowledge embedded in language and
vision foundation models to improve generalization of 3D-aware keyframe
policies. OG-VLA projects input observations from diverse views into a point
cloud which is then rendered from canonical orthographic views, ensuring input
view invariance and consistency between input and output spaces. These
canonical views are processed with a vision backbone, a Large Language Model
(LLM), and an image diffusion model to generate images that encode the next
position and orientation of the end-effector on the input scene. Evaluations on
the Arnold and Colosseum benchmarks demonstrate state-of-the-art generalization
to unseen environments, with over 40% relative improvements while maintaining
robust performance in seen settings. We also show real-world adaption in 3 to 5
demonstrations along with strong generalization. Videos and resources at
https://og-vla.github.io/

</details>


### [208] [Sparse Imagination for Efficient Visual World Model Planning](https://arxiv.org/abs/2506.01392)
*Junha Chun,Youngjoon Jeong,Taesup Kim*

Main category: cs.RO

TL;DR: 提出了一种稀疏想象的视觉世界模型规划方法，通过减少前向预测中的令牌数量来提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 世界模型在复杂环境中的决策能力强大，但高计算资源需求限制了实时应用，尤其在机器人领域。

Method: 基于稀疏训练的视觉世界模型，采用随机分组注意力策略，动态调整处理的令牌数量以适应计算资源。

Result: 稀疏想象显著加速了规划过程，同时保持了高控制精度，实验证明了其高效性。

Conclusion: 该方法为世界模型在实时决策场景中的部署提供了可行路径。

Abstract: World model based planning has significantly improved decision-making in
complex environments by enabling agents to simulate future states and make
informed choices. However, ensuring the prediction accuracy of world models
often demands substantial computational resources, posing a major challenge for
real-time applications. This computational burden is particularly restrictive
in robotics, where resources are severely constrained. To address this
limitation, we propose a Sparse Imagination for Efficient Visual World Model
Planning, which enhances computational efficiency by reducing the number of
tokens processed during forward prediction. Our method leverages a sparsely
trained vision-based world model based on transformers with randomized grouped
attention strategy, allowing the model to adaptively adjust the number of
tokens processed based on the computational resource. By enabling sparse
imagination (rollout), our approach significantly accelerates planning while
maintaining high control fidelity. Experimental results demonstrate that sparse
imagination preserves task performance while dramatically improving inference
efficiency, paving the way for the deployment of world models in real-time
decision-making scenarios.

</details>


### [209] [SEMNAV: A Semantic Segmentation-Driven Approach to Visual Semantic Navigation](https://arxiv.org/abs/2506.01418)
*Rafael Flor-Rodríguez,Carlos Gutiérrez-Álvarez,Francisco Javier Acevedo-Rodríguez,Sergio Lafuente-Arroyo,Roberto J. López-Sastre*

Main category: cs.RO

TL;DR: 提出SEMNAV方法，利用语义分割增强视觉导航能力，解决仿真到现实的领域适应问题，并在仿真和真实环境中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语义导航模型依赖仿真环境的RGB数据，泛化能力受限，需解决领域适应问题。

Method: 提出SEMNAV方法，以语义分割为主要视觉输入，结合高层次语义信息，提升导航策略的鲁棒性。

Result: 在仿真和真实环境中均表现优异，成功率高，有效缩小仿真与现实的差距。

Conclusion: SEMNAV通过语义分割显著提升导航性能，是实用视觉语义导航的有力解决方案。

Abstract: Visual Semantic Navigation (VSN) is a fundamental problem in robotics, where
an agent must navigate toward a target object in an unknown environment, mainly
using visual information. Most state-of-the-art VSN models are trained in
simulation environments, where rendered scenes of the real world are used, at
best. These approaches typically rely on raw RGB data from the virtual scenes,
which limits their ability to generalize to real-world environments due to
domain adaptation issues. To tackle this problem, in this work, we propose
SEMNAV, a novel approach that leverages semantic segmentation as the main
visual input representation of the environment to enhance the agent's
perception and decision-making capabilities. By explicitly incorporating
high-level semantic information, our model learns robust navigation policies
that improve generalization across unseen environments, both in simulated and
real world settings. We also introduce a newly curated dataset, i.e. the SEMNAV
dataset, designed for training semantic segmentation-aware navigation models
like SEMNAV. Our approach is evaluated extensively in both simulated
environments and with real-world robotic platforms. Experimental results
demonstrate that SEMNAV outperforms existing state-of-the-art VSN models,
achieving higher success rates in the Habitat 2.0 simulation environment, using
the HM3D dataset. Furthermore, our real-world experiments highlight the
effectiveness of semantic segmentation in mitigating the sim-to-real gap,
making our model a promising solution for practical VSN-based robotic
applications. We release SEMNAV dataset, code and trained models at
https://github.com/gramuah/semnav

</details>


### [210] [FreqPolicy: Frequency Autoregressive Visuomotor Policy with Continuous Tokens](https://arxiv.org/abs/2506.01583)
*Yiming Zhong,Yumeng Liu,Chuyang Xiao,Zemin Yang,Youzhuo Wang,Yufei Zhu,Ye Shi,Yujing Sun,Xinge Zhu,Yuexin Ma*

Main category: cs.RO

TL;DR: 提出了一种基于频域表示的新型视觉运动策略学习方法，通过分层建模频率分量和连续潜在表示，显著提升了机器人操作的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动作表示和网络架构上存在固有局限，频域表示能更有效地捕捉动作的结构化特性（低频全局运动，高频局部细节），且不同任务需要不同频带的建模精度。

Method: 提出了一种渐进式建模分层频率分量的范式，并引入连续潜在表示以保持动作空间的平滑性和连续性。

Result: 在多种2D和3D机器人操作基准测试中，该方法在精度和效率上均优于现有方法。

Conclusion: 频域自回归框架与连续潜在表示的结合为广义机器人操作提供了潜力。

Abstract: Learning effective visuomotor policies for robotic manipulation is
challenging, as it requires generating precise actions while maintaining
computational efficiency. Existing methods remain unsatisfactory due to
inherent limitations in the essential action representation and the basic
network architectures. We observe that representing actions in the frequency
domain captures the structured nature of motion more effectively: low-frequency
components reflect global movement patterns, while high-frequency components
encode fine local details. Additionally, robotic manipulation tasks of varying
complexity demand different levels of modeling precision across these frequency
bands. Motivated by this, we propose a novel paradigm for visuomotor policy
learning that progressively models hierarchical frequency components. To
further enhance precision, we introduce continuous latent representations that
maintain smoothness and continuity in the action space. Extensive experiments
across diverse 2D and 3D robotic manipulation benchmarks demonstrate that our
approach outperforms existing methods in both accuracy and efficiency,
showcasing the potential of a frequency-domain autoregressive framework with
continuous tokens for generalized robotic manipulation.

</details>


### [211] [WoMAP: World Models For Embodied Open-Vocabulary Object Localization](https://arxiv.org/abs/2506.01600)
*Tenny Yin,Zhiting Mei,Tao Sun,Lihan Zha,Emily Zhou,Jeremy Bao,Miyu Yamane,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.RO

TL;DR: WoMAP是一种用于训练开放词汇对象定位策略的方法，通过高斯泼溅技术和潜在世界模型，显著提升了零样本任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在泛化性和物理动作生成上的不足，如模仿学习方法的泛化性差和视觉语言模型（VLM）的动作不实际。

Method: 使用高斯泼溅技术实现真实到模拟再到真实的数据生成，结合开放词汇对象检测器的密集奖励信号和潜在世界模型进行动态和奖励预测。

Result: 在零样本对象定位任务中，WoMAP的成功率比VLM和扩散策略基线分别高出9倍和2倍，并在TidyBot上展示了强大的泛化和模拟到真实的迁移能力。

Conclusion: WoMAP通过创新的数据生成和奖励信号设计，显著提升了对象定位任务的性能，具有广泛的适用性。

Abstract: Language-instructed active object localization is a critical challenge for
robots, requiring efficient exploration of partially observable environments.
However, state-of-the-art approaches either struggle to generalize beyond
demonstration datasets (e.g., imitation learning methods) or fail to generate
physically grounded actions (e.g., VLMs). To address these limitations, we
introduce WoMAP (World Models for Active Perception): a recipe for training
open-vocabulary object localization policies that: (i) uses a Gaussian
Splatting-based real-to-sim-to-real pipeline for scalable data generation
without the need for expert demonstrations, (ii) distills dense rewards signals
from open-vocabulary object detectors, and (iii) leverages a latent world model
for dynamics and rewards prediction to ground high-level action proposals at
inference time. Rigorous simulation and hardware experiments demonstrate
WoMAP's superior performance in a broad range of zero-shot object localization
tasks, with more than 9x and 2x higher success rates compared to VLM and
diffusion policy baselines, respectively. Further, we show that WoMAP achieves
strong generalization and sim-to-real transfer on a TidyBot.

</details>


### [212] [DualMap: Online Open-Vocabulary Semantic Mapping for Natural Language Navigation in Dynamic Changing Scenes](https://arxiv.org/abs/2506.01950)
*Jiajun Jiang,Yiming Zhu,Zirui Wu,Jie Song*

Main category: cs.RO

TL;DR: DualMap是一种在线开放词汇映射系统，通过自然语言查询帮助机器人理解和导航动态变化的环境。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中机器人导航的语义理解和适应性需求。

Method: 采用混合分割前端和对象级状态检查，避免昂贵的3D对象合并；使用双地图表示（全局抽象地图和局部具体地图）。

Result: 在3D开放词汇分割、高效场景映射和在线语言引导导航方面表现优异。

Conclusion: DualMap在动态环境导航中实现了高效和适应性。

Abstract: We introduce DualMap, an online open-vocabulary mapping system that enables
robots to understand and navigate dynamically changing environments through
natural language queries. Designed for efficient semantic mapping and
adaptability to changing environments, DualMap meets the essential requirements
for real-world robot navigation applications. Our proposed hybrid segmentation
frontend and object-level status check eliminate the costly 3D object merging
required by prior methods, enabling efficient online scene mapping. The
dual-map representation combines a global abstract map for high-level candidate
selection with a local concrete map for precise goal-reaching, effectively
managing and updating dynamic changes in the environment. Through extensive
experiments in both simulation and real-world scenarios, we demonstrate
state-of-the-art performance in 3D open-vocabulary segmentation, efficient
scene mapping, and online language-guided navigation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [213] [Enabling Chatbots with Eyes and Ears: An Immersive Multimodal Conversation System for Dynamic Interactions](https://arxiv.org/abs/2506.00421)
*Jihyoung Jang,Minwook Bae,Minji Kim,Dilek Hakkani-Tur,Hyounghun Kim*

Main category: cs.CL

TL;DR: 该论文提出了一种新型多模态对话模型，旨在解决现有聊天机器人在多模态交互中忽视听觉信息的问题，并引入了一个新的数据集$M^3C$。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注视觉任务，忽视听觉信息，且多模态交互多为静态或任务受限，无法实现动态自然对话。

Method: 提出了一种多模态对话模型，结合多模态记忆检索，并在$M^3C$数据集上进行训练。

Result: 模型能够处理视觉和听觉输入，在复杂场景中实现长期多说话者对话，人类评估显示其性能优越。

Conclusion: 该模型在多模态对话中表现出色，为未来高级多模态对话代理提供了潜力。

Abstract: As chatbots continue to evolve toward human-like, real-world, interactions,
multimodality remains an active area of research and exploration. So far,
efforts to integrate multimodality into chatbots have primarily focused on
image-centric tasks, such as visual dialogue and image-based instructions,
placing emphasis on the "eyes" of human perception while neglecting the "ears",
namely auditory aspects. Moreover, these studies often center around static
interactions that focus on discussing the modality rather than naturally
incorporating it into the conversation, which limits the richness of
simultaneous, dynamic engagement. Furthermore, while multimodality has been
explored in multi-party and multi-session conversations, task-specific
constraints have hindered its seamless integration into dynamic, natural
conversations. To address these challenges, this study aims to equip chatbots
with "eyes and ears" capable of more immersive interactions with humans. As
part of this effort, we introduce a new multimodal conversation dataset,
Multimodal Multi-Session Multi-Party Conversation ($M^3C$), and propose a novel
multimodal conversation model featuring multimodal memory retrieval. Our model,
trained on the $M^3C$, demonstrates the ability to seamlessly engage in
long-term conversations with multiple speakers in complex, real-world-like
settings, effectively processing visual and auditory inputs to understand and
respond appropriately. Human evaluations highlight the model's strong
performance in maintaining coherent and dynamic interactions, demonstrating its
potential for advanced multimodal conversational agents.

</details>


### [214] [EffiVLM-BENCH: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Vision-Language Models](https://arxiv.org/abs/2506.00479)
*Zekun Wang,Minghua Ma,Zexin Wang,Rongchuan Mu,Liping Shan,Ming Liu,Bing Qin*

Main category: cs.CL

TL;DR: 本文系统评估了大型视觉语言模型（LVLM）的主流加速技术，提出了EffiVLM-Bench框架，用于全面评估性能、泛化性和忠诚度，并开源了代码。


<details>
  <summary>Details</summary>
Motivation: 尽管LVLM取得了显著成功，但其高计算需求限制了实际部署，现有方法缺乏全面的评估。

Method: 将加速技术分为令牌和参数压缩两类，通过EffiVLM-Bench框架进行系统评估。

Result: 实验和分析提供了加速LVLM的最优策略，并开源了代码。

Conclusion: EffiVLM-Bench为未来研究提供了工具和见解。

Abstract: Large Vision-Language Models (LVLMs) have achieved remarkable success, yet
their significant computational demands hinder practical deployment. While
efforts to improve LVLM efficiency are growing, existing methods lack
comprehensive evaluation across diverse backbones, benchmarks, and metrics. In
this work, we systematically evaluate mainstream acceleration techniques for
LVLMs, categorized into token and parameter compression. We introduce
EffiVLM-Bench, a unified framework for assessing not only absolute performance
but also generalization and loyalty, while exploring Pareto-optimal trade-offs.
Our extensive experiments and in-depth analyses offer insights into optimal
strategies for accelerating LVLMs. We open-source code and recipes for
EffiVLM-Bench to foster future research.

</details>


### [215] [Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural Understanding and Transcreation](https://arxiv.org/abs/2506.01565)
*Li Zhou,Lutong Yu,Dongchu Xie,Shaohuan Cheng,Wenyan Li,Haizhou Li*

Main category: cs.CL

TL;DR: 论文提出了Hanfu-Bench数据集，用于研究文化视觉理解和图像再创作，填补了现有视觉语言模型在文化时间维度研究的空白。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型研究主要关注地理多样性，忽视了文化的时间维度。

Method: 通过Hanfu-Bench数据集，设计文化视觉理解和图像再创作两项任务，评估模型表现。

Result: 封闭视觉语言模型表现接近非专家，但落后专家10%；开放模型表现更差。再创作任务中最佳模型成功率仅42%。

Conclusion: Hanfu-Bench揭示了文化时间维度理解和创意适应的重要挑战。

Abstract: Culture is a rich and dynamic domain that evolves across both geography and
time. However, existing studies on cultural understanding with vision-language
models (VLMs) primarily emphasize geographic diversity, often overlooking the
critical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a
novel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning
ancient Chinese dynasties, serves as a representative cultural heritage that
reflects the profound temporal aspects of Chinese culture while remaining
highly popular in Chinese contemporary society. Hanfu-Bench comprises two core
tasks: cultural visual understanding and cultural image transcreation.The
former task examines temporal-cultural feature recognition based on single- or
multi-image inputs through multiple-choice visual question answering, while the
latter focuses on transforming traditional attire into modern designs through
cultural element inheritance and modern context adaptation. Our evaluation
shows that closed VLMs perform comparably to non-experts on visual cutural
understanding but fall short by 10\% to human experts, while open VLMs lags
further behind non-experts. For the transcreation task, multi-faceted human
evaluation indicates that the best-performing model achieves a success rate of
only 42\%. Our benchmark provides an essential testbed, revealing significant
challenges in this new direction of temporal cultural understanding and
creative adaptation.

</details>


### [216] [Is Extending Modality The Right Path Towards Omni-Modality?](https://arxiv.org/abs/2506.01872)
*Tinghui Zhu,Kai Zhang,Muhao Chen,Yu Su*

Main category: cs.CL

TL;DR: 论文研究了扩展模态对多模态模型的影响，探讨了模态扩展是否损害语言能力、模型合并是否能实现全模态，以及全模态扩展是否优于顺序扩展。


<details>
  <summary>Details</summary>
Motivation: 现有开源模型在多模态输入处理和泛化能力上表现不足，研究旨在探索实现真正全模态的可行性。

Method: 通过实验分析模态扩展对语言能力的影响，评估模型合并的效果，并比较全模态扩展与顺序扩展的优劣。

Result: 实验提供了关于当前方法实现全模态的可行性的深入见解。

Conclusion: 研究为未来实现真正全模态的模型提供了重要参考。

Abstract: Omni-modal language models (OLMs) aim to integrate and reason over diverse
input modalities--such as text, images, video, and audio--while maintaining
strong language capabilities. Despite recent advancements, existing models,
especially open-source ones, remain far from true omni-modality, struggling to
generalize beyond the specific modality pairs they are trained on or to achieve
strong performance when processing multi-modal inputs. We study the effect of
extending modality, the dominant technique for training multimodal models,
where an off-the-shelf language model is fine-tuned on target-domain and
language data. Specifically, we investigate three key questions: (1) Does
modality extension compromise core language abilities? (2) Can model merging
effectively integrate independently fine-tuned modality-specific models to
achieve omni-modality? (3) Does omni-modality extension lead to better
knowledge sharing and generalization compared to sequential extension? Through
extensive experiments, we analyze these trade-offs and provide insights into
the feasibility of achieving true omni-modality using current approaches.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [217] [3D Gaussian Splat Vulnerabilities](https://arxiv.org/abs/2506.00280)
*Matthew Hull,Haoyang Yang,Pratham Mehta,Mansi Phute,Aeree Cho,Haoran Wang,Matthew Lau,Wenke Lee,Willian T. Lunardi,Martin Andreoni,Polo Chau*

Main category: cs.CR

TL;DR: 论文介绍了CLOAK和DAGGER两种针对3D高斯溅射（3DGS）的攻击方法，利用视角依赖的外观和直接扰动3D高斯，揭示了3DGS在安全关键应用中的潜在威胁。


<details>
  <summary>Details</summary>
Motivation: 随着3DGS在安全关键应用中的普及，研究其潜在攻击方式以防范风险变得至关重要。

Method: CLOAK通过视角依赖的高斯外观嵌入对抗内容；DAGGER则直接扰动3D高斯，欺骗多阶段目标检测器。

Result: 攻击成功展示了3DGS的未探索漏洞，威胁到自动驾驶等安全关键应用。

Conclusion: 3DGS存在新的安全威胁，需进一步研究防御措施。

Abstract: With 3D Gaussian Splatting (3DGS) being increasingly used in safety-critical
applications, how can an adversary manipulate the scene to cause harm? We
introduce CLOAK, the first attack that leverages view-dependent Gaussian
appearances - colors and textures that change with viewing angle - to embed
adversarial content visible only from specific viewpoints. We further
demonstrate DAGGER, a targeted adversarial attack directly perturbing 3D
Gaussians without access to underlying training data, deceiving multi-stage
object detectors e.g., Faster R-CNN, through established methods such as
projected gradient descent. These attacks highlight underexplored
vulnerabilities in 3DGS, introducing a new potential threat to robotic learning
for autonomous navigation and other safety-critical 3DGS applications.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [218] [Applying Vision Transformers on Spectral Analysis of Astronomical Objects](https://arxiv.org/abs/2506.00294)
*Luis Felipe Strano Moraes,Ignacio Becker,Pavlos Protopapas,Guillermo Cabrera-Vives*

Main category: astro-ph.IM

TL;DR: 将预训练的视觉Transformer（ViT）应用于天文光谱数据，通过将一维光谱转换为二维图像表示，利用空间自注意力捕捉局部和全局特征。模型在SDSS和LAMOST数据上微调，在恒星分类和红移估计任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 探索预训练视觉模型在天文光谱数据分析中的潜力，填补ViT在大规模真实光谱数据应用中的空白。

Method: 将一维光谱转换为二维图像，微调ImageNet预训练的ViT模型，使用SDSS和LAMOST数据训练。

Result: 在恒星分类和红移估计任务中表现优于支持向量机和随机森林，R²值与AstroCLIP相当。

Conclusion: 预训练视觉模型在天文光谱数据分析中具有显著效果，ViT首次成功应用于大规模真实光谱数据。

Abstract: We apply pre-trained Vision Transformers (ViTs), originally developed for
image recognition, to the analysis of astronomical spectral data. By converting
traditional one-dimensional spectra into two-dimensional image representations,
we enable ViTs to capture both local and global spectral features through
spatial self-attention. We fine-tune a ViT pretrained on ImageNet using
millions of spectra from the SDSS and LAMOST surveys, represented as spectral
plots. Our model is evaluated on key tasks including stellar object
classification and redshift ($z$) estimation, where it demonstrates strong
performance and scalability. We achieve classification accuracy higher than
Support Vector Machines and Random Forests, and attain $R^2$ values comparable
to AstroCLIP's spectrum encoder, even when generalizing across diverse object
types. These results demonstrate the effectiveness of using pretrained vision
models for spectroscopic data analysis. To our knowledge, this is the first
application of ViTs to large-scale, which also leverages real spectroscopic
data and does not rely on synthetic inputs.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [219] [Vid2Coach: Transforming How-To Videos into Task Assistants](https://arxiv.org/abs/2506.00717)
*Mina Huh,Zihui Xue,Ujjaini Das,Kumar Ashutosh,Kristen Grauman,Amy Pavel*

Main category: cs.HC

TL;DR: Vid2Coach是一个将教学视频转化为可穿戴设备辅助工具的系统，帮助盲人和低视力人群通过详细描述和非视觉替代方法完成任务。


<details>
  <summary>Details</summary>
Motivation: 盲人和低视力人群难以通过视觉对比学习视频内容，需要辅助工具提供支持。

Method: 系统通过视频生成详细指令，结合非视觉替代方法，并利用智能眼镜的摄像头监控进度，提供反馈。

Result: 使用Vid2Coach的参与者完成任务时错误率降低了58.5%，并表示希望在日常中使用该系统。

Conclusion: Vid2Coach展示了AI视觉辅助的潜力，能够增强而非替代非视觉专业知识。

Abstract: People use videos to learn new recipes, exercises, and crafts. Such videos
remain difficult for blind and low vision (BLV) people to follow as they rely
on visual comparison. Our observations of visual rehabilitation therapists
(VRTs) guiding BLV people to follow how-to videos revealed that VRTs provide
both proactive and responsive support including detailed descriptions,
non-visual workarounds, and progress feedback. We propose Vid2Coach, a system
that transforms how-to videos into wearable camera-based assistants that
provide accessible instructions and mixed-initiative feedback. From the video,
Vid2Coach generates accessible instructions by augmenting narrated instructions
with demonstration details and completion criteria for each step. It then uses
retrieval-augmented-generation to extract relevant non-visual workarounds from
BLV-specific resources. Vid2Coach then monitors user progress with a camera
embedded in commercial smart glasses to provide context-aware instructions,
proactive feedback, and answers to user questions. BLV participants (N=8) using
Vid2Coach completed cooking tasks with 58.5\% fewer errors than when using
their typical workflow and wanted to use Vid2Coach in their daily lives.
Vid2Coach demonstrates an opportunity for AI visual assistance that strengthens
rather than replaces non-visual expertise.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [220] [GeoChain: Multimodal Chain-of-Thought for Geographic Reasoning](https://arxiv.org/abs/2506.00785)
*Sahiti Yerramilli,Nilay Pande,Rynaa Grover,Jayant Sravan Tamarapalli*

Main category: cs.AI

TL;DR: GeoChain是一个用于评估多模态大语言模型（MLLMs）逐步地理推理能力的大规模基准，包含1.46百万张街景图像和30百万个问答对。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在地理推理方面存在视觉定位不准、推理不稳定等问题，GeoChain旨在提供一个诊断工具以推动改进。

Method: 利用Mapillary街景图像，构建21步链式推理问题序列，涵盖视觉、空间、文化和精确定位四类，并标注难度。

Result: 测试显示MLLMs在视觉定位和复杂推理中表现不佳，尤其是随着难度增加。

Conclusion: GeoChain为MLLMs的复杂地理推理能力提供了重要诊断方法，有助于未来研究。

Abstract: This paper introduces GeoChain, a large-scale benchmark for evaluating
step-by-step geographic reasoning in multimodal large language models (MLLMs).
Leveraging 1.46 million Mapillary street-level images, GeoChain pairs each
image with a 21-step chain-of-thought (CoT) question sequence (over 30 million
Q&A pairs). These sequences guide models from coarse attributes to fine-grained
localization across four reasoning categories - visual, spatial, cultural, and
precise geolocation - annotated by difficulty. Images are also enriched with
semantic segmentation (150 classes) and a visual locatability score. Our
benchmarking of contemporary MLLMs (GPT-4.1 variants, Claude 3.7, Gemini 2.5
variants) on a diverse 2,088-image subset reveals consistent challenges: models
frequently exhibit weaknesses in visual grounding, display erratic reasoning,
and struggle to achieve accurate localization, especially as the reasoning
complexity escalates. GeoChain offers a robust diagnostic methodology, critical
for fostering significant advancements in complex geographic reasoning within
MLLMs.

</details>


### [221] [SynPO: Synergizing Descriptiveness and Preference Optimization for Video Detailed Captioning](https://arxiv.org/abs/2506.00835)
*Jisheng Dang,Yizhou Zhang,Hao Ye,Teng Wang,Siming Chen,Huicheng Zheng,Yulan Guo,Jianhuang Lai,Bin Hu*

Main category: cs.AI

TL;DR: 论文提出了一种基于偏好学习的细粒度视频描述方法SynPO，通过优化偏好对构建和训练效率，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉视频的细微动态和丰富细节信息，直接偏好优化（DPO）存在局限性。

Method: 提出偏好对构建流程和Synergistic Preference Optimization（SynPO）方法，避免负面偏好主导优化并保留语言能力。

Result: 在视频描述和NLP任务中，SynPO均优于DPO变体，训练效率提升20%。

Conclusion: SynPO在性能和效率上均表现出色，为细粒度视频描述提供了有效解决方案。

Abstract: Fine-grained video captioning aims to generate detailed, temporally coherent
descriptions of video content. However, existing methods struggle to capture
subtle video dynamics and rich detailed information. In this paper, we leverage
preference learning to enhance the performance of vision-language models in
fine-grained video captioning, while mitigating several limitations inherent to
direct preference optimization (DPO). First, we propose a pipeline for
constructing preference pairs that leverages the intrinsic properties of VLMs
along with partial assistance from large language models, achieving an optimal
balance between cost and data quality. Second, we propose Synergistic
Preference Optimization (SynPO), a novel optimization method offering
significant advantages over DPO and its variants. SynPO prevents negative
preferences from dominating the optimization, explicitly preserves the model's
language capability to avoid deviation of the optimization objective, and
improves training efficiency by eliminating the need for the reference model.
We extensively evaluate SynPO not only on video captioning benchmarks (e.g.,
VDC, VDD, VATEX) but also across well-established NLP tasks, including general
language understanding and preference evaluation, using diverse pretrained
models. Results demonstrate that SynPO consistently outperforms DPO variants
while achieving 20\% improvement in training efficiency. Code is available at
https://github.com/longmalongma/SynPO

</details>


### [222] [Speaking Beyond Language: A Large-Scale Multimodal Dataset for Learning Nonverbal Cues from Video-Grounded Dialogues](https://arxiv.org/abs/2506.00958)
*Youngmin Kim,Jiwan Chung,Jisoo Kim,Sunghyun Lee,Sangkyu Lee,Junhyeok Kim,Cheoljong Yang,Youngjae Yu*

Main category: cs.AI

TL;DR: MARS是一种多模态语言模型，结合文本和非语言线索（如面部表情和肢体语言）以提升对话AI的沉浸感。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）未能有效整合非语言元素，限制了对话体验的沉浸感。

Method: 利用VENUS数据集（包含标注视频、文本、面部表情和肢体语言），通过下一词预测目标训练MARS，实现多模态理解和生成。

Result: MARS成功生成与对话输入对应的文本和非语言内容，VENUS数据集验证了其规模大且高效。

Conclusion: MARS填补了对话AI中非语言交流的空白，为多模态交互提供了新方向。

Abstract: Nonverbal communication is integral to human interaction, with gestures,
facial expressions, and body language conveying critical aspects of intent and
emotion. However, existing large language models (LLMs) fail to effectively
incorporate these nonverbal elements, limiting their capacity to create fully
immersive conversational experiences. We introduce MARS, a multimodal language
model designed to understand and generate nonverbal cues alongside text,
bridging this gap in conversational AI. Our key innovation is VENUS, a
large-scale dataset comprising annotated videos with time-aligned text, facial
expressions, and body language. Leveraging VENUS, we train MARS with a
next-token prediction objective, combining text with vector-quantized nonverbal
representations to achieve multimodal understanding and generation within a
unified framework. Based on various analyses of the VENUS datasets, we validate
its substantial scale and high effectiveness. Our quantitative and qualitative
results demonstrate that MARS successfully generates text and nonverbal
languages, corresponding to conversational input.

</details>


### [223] [EgoBrain: Synergizing Minds and Eyes For Human Action Understanding](https://arxiv.org/abs/2506.01353)
*Nie Lin,Yansen Wang,Dongqi Han,Weibang Jiang,Jingyuan Li,Ryosuke Furuta,Yoichi Sato,Dongsheng Li*

Main category: cs.AI

TL;DR: EgoBrain是一个大规模、时间对齐的多模态数据集，结合了脑电图（EEG）和第一人称视频，用于人类行为分析。通过多模态学习框架，实现了66.70%的动作识别准确率。


<details>
  <summary>Details</summary>
Motivation: 结合脑机接口（BCI）和人工智能（AI），特别是多模态AI模型，以解码人类认知和行为。

Method: 开发了EgoBrain数据集，包含61小时的同步EEG和第一人称视频数据，并提出多模态学习框架融合EEG和视觉信息。

Result: 在跨主体和跨环境挑战中验证，动作识别准确率达到66.70%。

Conclusion: EgoBrain为多模态脑机接口提供了统一框架，并公开数据以促进认知计算的开放科学。

Abstract: The integration of brain-computer interfaces (BCIs), in particular
electroencephalography (EEG), with artificial intelligence (AI) has shown
tremendous promise in decoding human cognition and behavior from neural
signals. In particular, the rise of multimodal AI models have brought new
possibilities that have never been imagined before. Here, we present EgoBrain
--the world's first large-scale, temporally aligned multimodal dataset that
synchronizes egocentric vision and EEG of human brain over extended periods of
time, establishing a new paradigm for human-centered behavior analysis. This
dataset comprises 61 hours of synchronized 32-channel EEG recordings and
first-person video from 40 participants engaged in 29 categories of daily
activities. We then developed a muiltimodal learning framework to fuse EEG and
vision for action understanding, validated across both cross-subject and
cross-environment challenges, achieving an action recognition accuracy of
66.70%. EgoBrain paves the way for a unified framework for brain-computer
interface with multiple modalities. All data, tools, and acquisition protocols
are openly shared to foster open science in cognitive computing.

</details>


### [224] [AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.01391)
*Zhong Zhang,Yaxi Lu,Yikun Fu,Yupeng Huo,Shenzhi Yang,Yesai Wu,Han Si,Xin Cong,Haotian Chen,Yankai Lin,Jie Xie,Wei Zhou,Wang Xu,Yuanheng Zhang,Zhou Su,Zhongwu Zhai,Xiaoming Liu,Yudong Mei,Jianming Xu,Hongyan Tian,Chongyi Wang,Chi Chen,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: AgentCPM-GUI是一个8B参数的GUI代理，通过改进的训练流程和紧凑的动作空间，在移动设备上实现了高效交互，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有GUI代理训练数据噪声大、语义多样性不足、泛化能力差以及非英语界面支持不足的问题。

Method: 采用基于感知的预训练、高质量中英文轨迹的监督微调以及GRPO强化微调，并设计紧凑动作空间。

Result: 在五个公共基准和新中文基准CAGUI上达到96.9% Type-Match和91.3% Exact-Match。

Conclusion: AgentCPM-GUI在移动GUI交互中表现出色，并公开了代码、模型和评估数据以促进研究。

Abstract: The recent progress of large language model agents has opened new
possibilities for automating tasks through graphical user interfaces (GUIs),
especially in mobile environments where intelligent interaction can greatly
enhance usability. However, practical deployment of such agents remains
constrained by several key challenges. Existing training data is often noisy
and lack semantic diversity, which hinders the learning of precise grounding
and planning. Models trained purely by imitation tend to overfit to seen
interface patterns and fail to generalize in unfamiliar scenarios. Moreover,
most prior work focuses on English interfaces while overlooks the growing
diversity of non-English applications such as those in the Chinese mobile
ecosystem. In this work, we present AgentCPM-GUI, an 8B-parameter GUI agent
built for robust and efficient on-device GUI interaction. Our training pipeline
includes grounding-aware pre-training to enhance perception, supervised
fine-tuning on high-quality Chinese and English trajectories to imitate
human-like actions, and reinforcement fine-tuning with GRPO to improve
reasoning capability. We also introduce a compact action space that reduces
output length and supports low-latency execution on mobile devices.
AgentCPM-GUI achieves state-of-the-art performance on five public benchmarks
and a new Chinese GUI benchmark called CAGUI, reaching $96.9\%$ Type-Match and
$91.3\%$ Exact-Match. To facilitate reproducibility and further research, we
publicly release all code, model checkpoint, and evaluation data.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [225] [Multiverse Through Deepfakes: The MultiFakeVerse Dataset of Person-Centric Visual and Conceptual Manipulations](https://arxiv.org/abs/2506.00868)
*Parul Gupta,Shreya Ghosh,Tom Gedeon,Thanh-Toan Do,Abhinav Dhall*

Main category: cs.MM

TL;DR: 论文提出了MultiFakeVerse，一个基于视觉语言模型生成的大规模人物中心化深度伪造数据集，填补了现有数据集的不足。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造数据集缺乏大规模且具备语义推理能力的数据集，特别是针对人物中心化、上下文和场景操纵的需求。

Method: 利用视觉语言模型（VLM）生成845,286张图像，通过语义驱动的指令实现人物和场景的上下文感知修改。

Result: 实验表明，现有深度伪造检测模型和人类观察者难以识别这些细微但有意义的操纵。

Conclusion: MultiFakeVerse为深度伪造检测研究提供了新的挑战和资源。

Abstract: The rapid advancement of GenAI technology over the past few years has
significantly contributed towards highly realistic deepfake content generation.
Despite ongoing efforts, the research community still lacks a large-scale and
reasoning capability driven deepfake benchmark dataset specifically tailored
for person-centric object, context and scene manipulations. In this paper, we
address this gap by introducing MultiFakeVerse, a large scale person-centric
deepfake dataset, comprising 845,286 images generated through manipulation
suggestions and image manipulations both derived from vision-language models
(VLM). The VLM instructions were specifically targeted towards modifications to
individuals or contextual elements of a scene that influence human perception
of importance, intent, or narrative. This VLM-driven approach enables semantic,
context-aware alterations such as modifying actions, scenes, and human-object
interactions rather than synthetic or low-level identity swaps and
region-specific edits that are common in existing datasets. Our experiments
reveal that current state-of-the-art deepfake detection models and human
observers struggle to detect these subtle yet meaningful manipulations. The
code and dataset are available on
\href{https://github.com/Parul-Gupta/MultiFakeVerse}{GitHub}.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [226] [ProtInvTree: Deliberate Protein Inverse Folding with Reward-guided Tree Search](https://arxiv.org/abs/2506.00925)
*Mengdi Liu,Xiaoxue Cheng,Zhangyang Gao,Hong Chang,Cheng Tan,Shiguang Shan,Xilin Chen*

Main category: q-bio.BM

TL;DR: ProtInvTree是一种基于树搜索的蛋白质逆折叠生成模型，能够设计多样且结构一致的序列。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习方法忽视蛋白质逆折叠问题的一对多特性（多个序列对应同一结构）的局限性。

Method: 提出ProtInvTree框架，采用奖励引导的树搜索策略，结合两阶段动作机制（位置选择和残基生成）和跳跃去噪策略。

Result: 在多个基准测试中优于现有方法，生成结构一致且多样的序列。

Conclusion: ProtInvTree为蛋白质逆折叠提供了一种高效且灵活的解决方案。

Abstract: Designing protein sequences that fold into a target 3D structure, known as
protein inverse folding, is a fundamental challenge in protein engineering.
While recent deep learning methods have achieved impressive performance by
recovering native sequences, they often overlook the one-to-many nature of the
problem: multiple diverse sequences can fold into the same structure. This
motivates the need for a generative model capable of designing diverse
sequences while preserving structural consistency. To address this trade-off,
we introduce ProtInvTree, the first reward-guided tree-search framework for
protein inverse folding. ProtInvTree reformulates sequence generation as a
deliberate, step-wise decision-making process, enabling the exploration of
multiple design paths and exploitation of promising candidates through
self-evaluation, lookahead, and backtracking. We propose a two-stage
focus-and-grounding action mechanism that decouples position selection and
residue generation. To efficiently evaluate intermediate states, we introduce a
jumpy denoising strategy that avoids full rollouts. Built upon pretrained
protein language models, ProtInvTree supports flexible test-time scaling by
expanding the search depth and breadth without retraining. Empirically,
ProtInvTree outperforms state-of-the-art baselines across multiple benchmarks,
generating structurally consistent yet diverse sequences, including those far
from the native ground truth.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [227] [Learning Sparsity for Effective and Efficient Music Performance Question Answering](https://arxiv.org/abs/2506.01319)
*Xingjian Diao,Tianzhen Yang,Chunhui Zhang,Weiyi Wu,Ming Cheng,Jiang Gui*

Main category: cs.SD

TL;DR: 论文提出了一种名为Sparsify的稀疏学习框架，用于解决音乐表演音频-视觉问答（Music AVQA）中的信息冗余和效率问题，通过三种稀疏化策略提升了性能并减少了训练时间。


<details>
  <summary>Details</summary>
Motivation: 音乐表演的密集连续音频和视听整合特性为多模态场景理解带来挑战，现有方法存在信息冗余和效率低下的问题。

Method: 提出Sparsify框架，集成三种稀疏化策略，并设计关键子集选择算法以提高数据效率。

Result: 在Music AVQA数据集上达到最优性能，训练时间减少28.32%，同时使用25%的训练数据仍能保持70-80%的性能。

Conclusion: Sparsify框架有效提升了Music AVQA任务的效率和性能，为多模态学习提供了新思路。

Abstract: Music performances, characterized by dense and continuous audio as well as
seamless audio-visual integration, present unique challenges for multimodal
scene understanding and reasoning. Recent Music Performance Audio-Visual
Question Answering (Music AVQA) datasets have been proposed to reflect these
challenges, highlighting the continued need for more effective integration of
audio-visual representations in complex question answering. However, existing
Music AVQA methods often rely on dense and unoptimized representations, leading
to inefficiencies in the isolation of key information, the reduction of
redundancy, and the prioritization of critical samples. To address these
challenges, we introduce Sparsify, a sparse learning framework specifically
designed for Music AVQA. It integrates three sparsification strategies into an
end-to-end pipeline and achieves state-of-the-art performance on the Music AVQA
datasets. In addition, it reduces training time by 28.32% compared to its fully
trained dense counterpart while maintaining accuracy, demonstrating clear
efficiency gains. To further improve data efficiency, we propose a key-subset
selection algorithm that selects and uses approximately 25% of MUSIC-AVQA v2.0
training data and retains 70-80% of full-data performance across models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [228] [PerFormer: A Permutation Based Vision Transformer for Remaining Useful Life Prediction](https://arxiv.org/abs/2506.00259)
*Zhengyang Fan,Wanru Li,Kuo-chu Chang,Ting Yuan*

Main category: cs.LG

TL;DR: 论文提出了一种基于排列的视觉Transformer方法（PerFormer），用于提升退化系统剩余使用寿命（RUL）预测的准确性，通过将多变量时间序列数据转换为类似图像的空间特征，解决了ViT直接应用于时间序列数据的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着Vision Transformer（ViT）在计算机视觉任务中展现出优于CNN的性能，研究者希望探索其在RUL预测中的潜力，但时间序列数据的空间信息模糊性带来了挑战。

Method: 提出PerFormer方法，通过排列多变量时间序列数据模拟图像的空间特征，并设计了一种新的排列损失函数来生成所需的排列矩阵。

Result: 在NASA的C-MAPSS数据集上，PerFormer在RUL预测中表现优于基于CNN、RNN和其他Transformer模型的现有方法。

Conclusion: PerFormer展示了在PHM应用中的高效性和潜力，为RUL预测提供了一种新的解决方案。

Abstract: Accurately estimating the remaining useful life (RUL) for degradation systems
is crucial in modern prognostic and health management (PHM). Convolutional
Neural Networks (CNNs), initially developed for tasks like image and video
recognition, have proven highly effectively in RUL prediction, demonstrating
remarkable performance. However, with the emergence of the Vision Transformer
(ViT), a Transformer model tailored for computer vision tasks such as image
classification, and its demonstrated superiority over CNNs, there is a natural
inclination to explore its potential in enhancing RUL prediction accuracy.
Nonetheless, applying ViT directly to multivariate sensor data for RUL
prediction poses challenges, primarily due to the ambiguous nature of spatial
information in time series data. To address this issue, we introduce the
PerFormer, a permutation-based vision transformer approach designed to permute
multivariate time series data, mimicking spatial characteristics akin to image
data, thereby making it suitable for ViT. To generate the desired permutation
matrix, we introduce a novel permutation loss function aimed at guiding the
convergence of any matrix towards a permutation matrix. Our experiments on
NASA's C-MAPSS dataset demonstrate the PerFormer's superior performance in RUL
prediction compared to state-of-the-art methods employing CNNs, Recurrent
Neural Networks (RNNs), and various Transformer models. This underscores its
effectiveness and potential in PHM applications.

</details>


### [229] [Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation](https://arxiv.org/abs/2506.00329)
*Muhammad Adnan,Nithesh Kurella,Akhil Arunkumar,Prashant J. Nair*

Main category: cs.LG

TL;DR: Foresight是一种自适应层重用技术，通过动态识别和重用DiT块输出来减少计算冗余，提升视频生成效率，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformers（DiTs）在视频生成中计算成本高，静态缓存方法无法适应生成动态，导致速度与质量的次优权衡。

Method: 提出Foresight技术，动态重用DiT块输出，适应生成参数（如分辨率和去噪计划）以优化效率。

Result: 在OpenSora、Latte和CogVideoX上，Foresight实现了最高1.63倍的端到端加速，同时保持视频质量。

Conclusion: Foresight通过自适应层重用显著提升了视频生成的效率，且不影响性能。

Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art results in
text-to-image, text-to-video generation, and editing. However, their large
model size and the quadratic cost of spatial-temporal attention over multiple
denoising steps make video generation computationally expensive. Static caching
mitigates this by reusing features across fixed steps but fails to adapt to
generation dynamics, leading to suboptimal trade-offs between speed and
quality.
  We propose Foresight, an adaptive layer-reuse technique that reduces
computational redundancy across denoising steps while preserving baseline
performance. Foresight dynamically identifies and reuses DiT block outputs for
all layers across steps, adapting to generation parameters such as resolution
and denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and
CogVideoX, Foresight achieves up to 1.63x end-to-end speedup, while maintaining
video quality. The source code of Foresight is available at
\texttt{https://github.com/STAR-Laboratory/foresight}.

</details>


### [230] [SST: Self-training with Self-adaptive Thresholding for Semi-supervised Learning](https://arxiv.org/abs/2506.00467)
*Shuai Zhao,Heyan Huang,Xinge Li,Xiaokang Chen,Rui Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为SST的半监督学习框架，通过自适应的阈值机制（SAT）高效选择高质量伪标签，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景中获取高质量标注数据成本高且耗时，现有半监督学习方法依赖固定阈值或频繁更新阈值，效率低且可能不必要。

Method: 提出SST框架，引入自适应的SAT机制，根据模型学习进度动态调整类别特定阈值，确保选择高质量伪标签。

Result: SST在ImageNet-1K基准测试中表现优异，仅用1%/10%标注数据即达到80.7%/84.9%的Top-1准确率，优于全监督方法。

Conclusion: SST通过自适应阈值机制高效解决了伪标签质量问题，显著提升了半监督学习的性能和效率。

Abstract: Neural networks have demonstrated exceptional performance in supervised
learning, benefiting from abundant high-quality annotated data. However,
obtaining such data in real-world scenarios is costly and labor-intensive.
Semi-supervised learning (SSL) offers a solution to this problem. Recent
studies, such as Semi-ViT and Noisy Student, which employ consistency
regularization or pseudo-labeling, have demonstrated significant achievements.
However, they still face challenges, particularly in accurately selecting
sufficient high-quality pseudo-labels due to their reliance on fixed
thresholds. Recent methods such as FlexMatch and FreeMatch have introduced
flexible or self-adaptive thresholding techniques, greatly advancing SSL
research. Nonetheless, their process of updating thresholds at each iteration
is deemed time-consuming, computationally intensive, and potentially
unnecessary. To address these issues, we propose Self-training with
Self-adaptive Thresholding (SST), a novel, effective, and efficient SSL
framework. SST introduces an innovative Self-Adaptive Thresholding (SAT)
mechanism that adaptively adjusts class-specific thresholds based on the
model's learning progress. SAT ensures the selection of high-quality
pseudo-labeled data, mitigating the risks of inaccurate pseudo-labels and
confirmation bias. Extensive experiments demonstrate that SST achieves
state-of-the-art performance with remarkable efficiency, generalization, and
scalability across various architectures and datasets. Semi-SST-ViT-Huge
achieves the best results on competitive ImageNet-1K SSL benchmarks, with 80.7%
/ 84.9% Top-1 accuracy using only 1% / 10% labeled data. Compared to the
fully-supervised DeiT-III-ViT-Huge, which achieves 84.8% Top-1 accuracy using
100% labeled data, our method demonstrates superior performance using only 10%
labeled data.

</details>


### [231] [Flashbacks to Harmonize Stability and Plasticity in Continual Learning](https://arxiv.org/abs/2506.00477)
*Leila Mahmoodi,Peyman Moghadam,Munawar Hayat,Christian Simon,Mehrtash Harandi*

Main category: cs.LG

TL;DR: Flashback Learning (FL) 是一种新颖的持续学习方法，通过双向正则化平衡模型的稳定性和可塑性，显著提升了分类任务的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中模型在保留旧知识的同时学习新知识的平衡问题。

Method: 采用两阶段训练过程，结合两种知识库（增强可塑性和稳定性）进行双向正则化。

Result: 在标准图像分类基准上，FL 平均提升准确率 4.91%（类增量）和 3.51%（任务增量），并在 ImageNet 上优于现有方法。

Conclusion: FL 有效平衡了稳定性和可塑性，显著提升了持续学习性能。

Abstract: We introduce Flashback Learning (FL), a novel method designed to harmonize
the stability and plasticity of models in Continual Learning (CL). Unlike prior
approaches that primarily focus on regularizing model updates to preserve old
information while learning new concepts, FL explicitly balances this trade-off
through a bidirectional form of regularization. This approach effectively
guides the model to swiftly incorporate new knowledge while actively retaining
its old knowledge. FL operates through a two-phase training process and can be
seamlessly integrated into various CL methods, including replay, parameter
regularization, distillation, and dynamic architecture techniques. In designing
FL, we use two distinct knowledge bases: one to enhance plasticity and another
to improve stability. FL ensures a more balanced model by utilizing both
knowledge bases to regularize model updates. Theoretically, we analyze how the
FL mechanism enhances the stability-plasticity balance. Empirically, FL
demonstrates tangible improvements over baseline methods within the same
training budget. By integrating FL into at least one representative baseline
from each CL category, we observed an average accuracy improvement of up to
4.91% in Class-Incremental and 3.51% in Task-Incremental settings on standard
image classification benchmarks. Additionally, measurements of the
stability-to-plasticity ratio confirm that FL effectively enhances this
balance. FL also outperforms state-of-the-art CL methods on more challenging
datasets like ImageNet.

</details>


### [232] [Dynamic Domain Adaptation-Driven Physics-Informed Graph Representation Learning for AC-OPF](https://arxiv.org/abs/2506.00478)
*Hongjie Zhu,Zezheng Zhang,Zeyu Zhang,Yu Bai,Shimin Wen,Huazhang Wang,Daji Ergu,Ying Cai,Yang Zhao*

Main category: cs.LG

TL;DR: DDA-PIGCN是一种结合时空特征的图卷积网络方法，用于解决AC-OPF中的约束建模问题，显著提升了性能和约束满足率。


<details>
  <summary>Details</summary>
Motivation: 当前AC-OPF求解器在约束建模和时空信息整合方面存在不足，限制了知识表示的多样性和性能。

Method: 提出DDA-PIGCN方法，通过多层硬物理约束和动态域适应学习机制，结合时空特征优化约束验证。

Result: 在多个IEEE标准测试案例中，DDA-PIGCN表现出色，MAE低至0.0011-0.0624，约束满足率达99.6%-100%。

Conclusion: DDA-PIGCN是一种高效可靠的AC-OPF求解器，能够有效整合时空信息并优化约束建模。

Abstract: Alternating Current Optimal Power Flow (AC-OPF) aims to optimize generator
power outputs by utilizing the non-linear relationships between voltage
magnitudes and phase angles in a power system. However, current AC-OPF solvers
struggle to effectively represent the complex relationship between variable
distributions in the constraint space and their corresponding optimal
solutions. This limitation in constraint modeling restricts the system's
ability to develop diverse knowledge representations. Additionally, modeling
the power grid solely based on spatial topology further limits the integration
of additional prior knowledge, such as temporal information. To overcome these
challenges, we propose DDA-PIGCN (Dynamic Domain Adaptation-Driven
Physics-Informed Graph Convolutional Network), a new method designed to address
constraint-related issues and build a graph-based learning framework that
incorporates spatiotemporal features. DDA-PIGCN improves consistency
optimization for features with varying long-range dependencies by applying
multi-layer, hard physics-informed constraints. It also uses a dynamic domain
adaptation learning mechanism that iteratively updates and refines key state
variables under predefined constraints, enabling precise constraint
verification. Moreover, it captures spatiotemporal dependencies between
generators and loads by leveraging the physical structure of the power grid,
allowing for deep integration of topological information across time and space.
Extensive comparative and ablation studies show that DDA-PIGCN delivers strong
performance across several IEEE standard test cases (such as case9, case30, and
case300), achieving mean absolute errors (MAE) from 0.0011 to 0.0624 and
constraint satisfaction rates between 99.6% and 100%, establishing it as a
reliable and efficient AC-OPF solver.

</details>


### [233] [MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning](https://arxiv.org/abs/2506.00555)
*Peng Xia,Jinglu Wang,Yibo Peng,Kaide Zeng,Xian Wu,Xiangru Tang,Hongtu Zhu,Yun Li,Shujie Liu,Yan Lu,Huaxiu Yao*

Main category: cs.LG

TL;DR: MMedAgent-RL 是一个基于强化学习的多智能体框架，通过动态协作提升医疗多模态诊断任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的单智能体模型在跨医疗专业泛化能力上表现不足，而静态多智能体协作框架缺乏灵活性和适应性。

Method: 提出 MMedAgent-RL，通过强化学习训练两个 GP 智能体（分诊医生和主治医生），并引入课程学习策略以平衡专家意见。

Result: 在五个医疗 VQA 基准测试中，MMedAgent-RL 性能优于现有开源和专有模型，平均提升 18.4%。

Conclusion: MMedAgent-RL 不仅性能优越，还展现出类人推理模式，为医疗多智能体协作提供了新思路。

Abstract: Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential
in multimodal diagnostic tasks. However, existing single-agent models struggle
to generalize across diverse medical specialties, limiting their performance.
Recent efforts introduce multi-agent collaboration frameworks inspired by
clinical workflows, where general practitioners (GPs) and specialists interact
in a fixed sequence. Despite improvements, these static pipelines lack
flexibility and adaptability in reasoning. To address this, we propose
MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that
enables dynamic, optimized collaboration among medical agents. Specifically, we
train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to
assign patients to appropriate specialties, while the attending physician
integrates the judgments from multi-specialists and its own knowledge to make
final decisions. To address the inconsistency in specialist outputs, we
introduce a curriculum learning (CL)-guided RL strategy that progressively
teaches the attending physician to balance between imitating specialists and
correcting their mistakes. Experiments on five medical VQA benchmarks
demonstrate that MMedAgent-RL not only outperforms both open-source and
proprietary Med-LVLMs, but also exhibits human-like reasoning patterns.
Notably, it achieves an average performance gain of 18.4% over supervised
fine-tuning baselines.

</details>


### [234] [QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training](https://arxiv.org/abs/2506.00711)
*Wei Dai,Peilin Chen,Chanakya Ekbote,Paul Pu Liang*

Main category: cs.LG

TL;DR: QoQ-Med-7B/32B是首个开放通用的临床基础模型，能够跨医学图像、时间序列信号和文本报告进行联合推理，通过DRPO训练显著提升诊断性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态语言模型在临床领域中的局限性，如视觉中心化和跨专业泛化能力不足。

Method: 采用Domain-aware Relative Policy Optimization (DRPO)训练方法，根据领域稀有性和模态难度分层缩放奖励，缓解数据分布不均导致的性能不平衡。

Result: DRPO训练使诊断性能平均提升43%（宏F1），在分割任务中IoU比开放模型高10倍，性能接近OpenAI o4-mini。

Conclusion: QoQ-Med通过DRPO和多模态联合推理显著提升临床决策能力，并开源模型权重、训练管道和推理轨迹以促进研究。

Abstract: Clinical decision-making routinely demands reasoning over heterogeneous data,
yet existing multimodal language models (MLLMs) remain largely vision-centric
and fail to generalize across clinical specialties. To bridge this gap, we
introduce QoQ-Med-7B/32B, the first open generalist clinical foundation model
that jointly reasons across medical images, time-series signals, and text
reports. QoQ-Med is trained with Domain-aware Relative Policy Optimization
(DRPO), a novel reinforcement-learning objective that hierarchically scales
normalized rewards according to domain rarity and modality difficulty,
mitigating performance imbalance caused by skewed clinical data distributions.
Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains,
we show that DRPO training boosts diagnostic performance by 43% in macro-F1 on
average across all visual domains as compared to other critic-free training
methods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation
data, it is able to highlight salient regions related to the diagnosis, with an
IoU 10x higher than open models while reaching the performance of OpenAI
o4-mini. To foster reproducibility and downstream research, we release (i) the
full model weights, (ii) the modular training pipeline, and (iii) all
intermediate reasoning traces at https://github.com/DDVD233/QoQ_Med.

</details>


### [235] [Adaptive Plane Reformatting for 4D Flow MRI using Deep Reinforcement Learning](https://arxiv.org/abs/2506.00727)
*Javier Bisbal,Julio Sotelo,Maria I Valdés,Pablo Irarrazaval,Marcelo E Andia,Julio García,José Rodriguez-Palomarez,Francesca Raimondi,Cristián Tejos,Sergio Uribe*

Main category: cs.LG

TL;DR: 论文提出了一种基于灵活坐标系的深度强化学习方法，用于医学图像中的平面重格式化任务，显著提高了准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前深度强化学习方法在测试数据与训练数据位置和方向一致时表现良好，但缺乏灵活性。本文旨在解决这一问题。

Method: 采用异步优势演员评论家（A3C）算法，结合灵活坐标系，实现对任意位置和方向体积的导航。

Result: 在4D流MRI中，角度和距离误差分别为6.32±4.15°和3.40±2.75毫米，与专家结果统计等效（p=0.21）。

Conclusion: 该方法灵活且适应性强，适用于4D流MRI以外的其他医学影像应用。

Abstract: Deep reinforcement learning (DRL) algorithms have shown robust results in
plane reformatting tasks. In these methods, an agent sequentially adjusts the
position and orientation of an initial plane towards an objective location.
This process allows accurate plane reformatting, without the need for detailed
landmarks, which makes it suitable for images with limited contrast and
resolution, such as 4D flow MRI. However, current DRL methods require the test
dataset to be in the same position and orientation as the training dataset. In
this paper, we present a novel technique that utilizes a flexible coordinate
system based on the current state, enabling navigation in volumes at any
position or orientation. We adopted the Asynchronous Advantage Actor Critic
(A3C) algorithm for reinforcement learning, outperforming Deep Q Network (DQN).
Experimental results in 4D flow MRI demonstrate improved accuracy in plane
reformatting angular and distance errors (6.32 +- 4.15 {\deg} and 3.40 +- 2.75
mm), as well as statistically equivalent flow measurements determined by a
plane reformatting process done by an expert (p=0.21). The method's flexibility
and adaptability make it a promising candidate for other medical imaging
applications beyond 4D flow MRI.

</details>


### [236] [Understanding Model Reprogramming for CLIP via Decoupling Visual Prompts](https://arxiv.org/abs/2506.01000)
*Chengyi Cai,Zesheng Ye,Lei Feng,Jianzhong Qi,Feng Liu*

Main category: cs.LG

TL;DR: 论文提出了一种解耦与重加权框架（DVP），通过分组优化视觉提示（DVP-cse/DVP-cls）并利用概率重加权矩阵（PRM）整合输出，以提升CLIP模型在下游任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉重编程（VR）方法因学习能力有限，可能无法捕捉描述的多样性或偏向非信息性属性，影响分类效果。

Method: 提出解耦视觉提示（DVP），按显式原因或聚类分组优化提示，并通过PRM矩阵加权整合输出。

Result: 在11个下游数据集上平均表现优于基线，且DVP-PRM组合提供了分类决策的可解释性。

Conclusion: DVP框架通过解耦与重加权提升了模型性能，并提供了对重编程过程的可理解性。

Abstract: Model reprogramming adapts pretrained models to downstream tasks by modifying
only the input and output spaces. Visual reprogramming (VR) is one instance for
vision tasks that adds a trainable noise pattern (i.e., a visual prompt) to
input images to facilitate downstream classification. The existing VR
approaches for CLIP train a single visual prompt using all descriptions of
different downstream classes. However, the limited learning capacity may result
in (1) a failure to capture diverse aspects of the descriptions (e.g., shape,
color, and texture), and (2) a possible bias toward less informative attributes
that do not help distinguish between classes. In this paper, we introduce a
decoupling-and-reweighting framework. Our decoupled visual prompts (DVP) are
optimized using descriptions grouped by explicit causes (DVP-cse) or
unsupervised clusters (DVP-cls). Then, we integrate the outputs of these visual
prompts with a probabilistic reweighting matrix (PRM) that measures their
contributions to each downstream class. Theoretically, DVP lowers the empirical
risk bound. Experimentally, DVP outperforms baselines on average across 11
downstream datasets. Notably, the DVP-PRM integration enables insights into how
individual visual prompts influence classification decisions, providing a
probabilistic framework for understanding reprogramming. Our code is available
at https://github.com/tmlr-group/DecoupledVP.

</details>


### [237] [$Ψ$-Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models](https://arxiv.org/abs/2506.01320)
*Taehoon Yoon,Yunhong Min,Kyeongmin Yeo,Minhyuk Sung*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce $\Psi$-Sampler, an SMC-based framework incorporating pCNL-based
initial particle sampling for effective inference-time reward alignment with a
score-based generative model. Inference-time reward alignment with score-based
generative models has recently gained significant traction, following a broader
paradigm shift from pre-training to post-training optimization. At the core of
this trend is the application of Sequential Monte Carlo (SMC) to the denoising
process. However, existing methods typically initialize particles from the
Gaussian prior, which inadequately captures reward-relevant regions and results
in reduced sampling efficiency. We demonstrate that initializing from the
reward-aware posterior significantly improves alignment performance. To enable
posterior sampling in high-dimensional latent spaces, we introduce the
preconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines
dimension-robust proposals with gradient-informed dynamics. This approach
enables efficient and scalable posterior sampling and consistently improves
performance across various reward alignment tasks, including layout-to-image
generation, quantity-aware generation, and aesthetic-preference generation, as
demonstrated in our experiments.

</details>


### [238] [Variance-Based Defense Against Blended Backdoor Attacks](https://arxiv.org/abs/2506.01444)
*Sujeevan Aseervatham,Achraf Kerzazi,Younès Bennani*

Main category: cs.LG

TL;DR: 论文提出了一种新的防御方法，用于检测和清除AI模型中的后门攻击，无需依赖干净数据集。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法依赖干净数据集计算统计异常，但在现实场景中可能不可行。

Method: 训练模型检测中毒类别，提取攻击触发器的关键部分，并识别中毒实例。

Result: 实验证明该方法在知名图像数据集上有效，优于SCAn、ABL和AGPD三种先进算法。

Conclusion: 新方法提高了防御后门攻击的可行性，并增强了可解释性。

Abstract: Backdoor attacks represent a subtle yet effective class of cyberattacks
targeting AI models, primarily due to their stealthy nature. The model behaves
normally on clean data but exhibits malicious behavior only when the attacker
embeds a specific trigger into the input. This attack is performed during the
training phase, where the adversary corrupts a small subset of the training
data by embedding a pattern and modifying the labels to a chosen target. The
objective is to make the model associate the pattern with the target label
while maintaining normal performance on unaltered data. Several defense
mechanisms have been proposed to sanitize training data-sets. However, these
methods often rely on the availability of a clean dataset to compute
statistical anomalies, which may not always be feasible in real-world scenarios
where datasets can be unavailable or compromised. To address this limitation,
we propose a novel defense method that trains a model on the given dataset,
detects poisoned classes, and extracts the critical part of the attack trigger
before identifying the poisoned instances. This approach enhances
explainability by explicitly revealing the harmful part of the trigger. The
effectiveness of our method is demonstrated through experimental evaluations on
well-known image datasets and comparative analysis against three
state-of-the-art algorithms: SCAn, ABL, and AGPD.

</details>


### [239] [Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability](https://arxiv.org/abs/2506.01789)
*Genta Indra Winata,David Anugraha,Emmy Liu,Alham Fikri Aji,Shou-Yi Hung,Aditya Parashar,Patrick Amadeus Irawan,Ruochen Zhang,Zheng-Xin Yong,Jan Christian Blaise Cruz,Niklas Muennighoff,Seungone Kim,Hanyang Zhao,Sudipta Kar,Kezia Erina Suryoraharjo,M. Farid Adilazuarda,En-Shiun Annie Lee,Ayu Purwarianti,Derry Tanti Wijaya,Monojit Choudhury*

Main category: cs.LG

TL;DR: 论文提出DataRubrics框架，通过系统化、基于量表的评估指标改进数据集质量评估，并探索合成数据生成方法。


<details>
  <summary>Details</summary>
Motivation: 当前数据集论文缺乏原创性、多样性和严格的质量控制，且透明度不足，现有工具和方法未能有效解决这些问题。

Method: 提出DataRubrics框架，结合LLM评估方法，提供可重复、可扩展的数据集质量评估方案。

Result: DataRubrics为作者和评审提供了标准化、可操作的数据集质量评估工具，支持更高效的数据中心研究。

Conclusion: 论文呼吁在数据集评审过程中采用系统化评估指标，并开源代码以支持LLM评估的复现。

Abstract: High-quality datasets are fundamental to training and evaluating machine
learning models, yet their creation-especially with accurate human
annotations-remains a significant challenge. Many dataset paper submissions
lack originality, diversity, or rigorous quality control, and these
shortcomings are often overlooked during peer review. Submissions also
frequently omit essential details about dataset construction and properties.
While existing tools such as datasheets aim to promote transparency, they are
largely descriptive and do not provide standardized, measurable methods for
evaluating data quality. Similarly, metadata requirements at conferences
promote accountability but are inconsistently enforced. To address these
limitations, this position paper advocates for the integration of systematic,
rubric-based evaluation metrics into the dataset review process-particularly as
submission volumes continue to grow. We also explore scalable, cost-effective
methods for synthetic data generation, including dedicated tools and
LLM-as-a-judge approaches, to support more efficient evaluation. As a call to
action, we introduce DataRubrics, a structured framework for assessing the
quality of both human- and model-generated datasets. Leveraging recent advances
in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and
actionable solution for dataset quality assessment, enabling both authors and
reviewers to uphold higher standards in data-centric research. We also release
code to support reproducibility of LLM-based evaluations at
https://github.com/datarubrics/datarubrics.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [240] [Transport Network, Graph, and Air Pollution](https://arxiv.org/abs/2506.01164)
*Nan Xu*

Main category: physics.soc-ph

TL;DR: 研究通过分析全球城市的30万张图像，发现了与污染相关的交通网络几何模式，并提出12个指数研究网络与污染的关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能全面分析交通网络的几何与拓扑特征对空气污染的影响。

Method: 通过图像解析和12个指数分析交通网络的几何模式与污染相关性。

Result: 提出改善连通性、平衡道路类型和避免极端聚类系数等策略以减少污染。

Conclusion: 研究为城市规划提供了基础设施对污染影响的独立分析，有助于更高效地减少污染。

Abstract: Air pollution can be studied in the urban structure regulated by transport
networks. Transport networks can be studied as geometric and topological graph
characteristics through designed models. Current studies do not offer a
comprehensive view as limited models with insufficient features are examined.
Our study finds geometric patterns of pollution-indicated transport networks
through 0.3 million image interpretations of global cities. These are then
described as part of 12 indices to investigate the network-pollution
correlation. Strategies such as improved connectivity, more balanced road types
and the avoidance of extreme clustering coefficient are identified as
beneficial for alleviated pollution. As a graph-only study, it informs superior
urban planning by separating the impact of permanent infrastructure from that
of derived development for a more focused and efficient effort toward pollution
reduction.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [241] [Controlled Spherulitic Crystal Growth from Salt Mixtures: A Universal Mechanism for Complex Crystal Self-Assembly](https://arxiv.org/abs/2506.01163)
*Tess Heeremans,Simon Lépinay,Romane Le Dizès Castell,Isa Yusuf,Paul Kolpakov,Daniel Bonn,Michael Steiger,Noushine Shahidzadeh*

Main category: cond-mat.soft

TL;DR: 研究揭示了二价金属离子如何诱导硫酸钠球晶生长，并首次量化了球晶生长的过饱和度和动力学条件。


<details>
  <summary>Details</summary>
Motivation: 球晶的结晶动力学及其生长条件尚不明确，限制了其在多个领域的应用。

Method: 通过蒸发硫酸盐混合水溶液，在室温下诱导硫酸钠球晶生长，并量化过饱和度和生长动力学。

Result: 高过饱和度和高粘度溶液（约111 Pa·s）下，非经典成核过程诱导球晶生长，且球晶形态随过饱和度变化而变化。

Conclusion: 研究阐明了球晶形成的条件，并提供了调控其形态的实用策略。

Abstract: Spherulites are complex polycrystalline structures that form through the
self-assembly of small aggregated nanocrystals starting from a central point
and growing radially outward. Despite their wide prevalence and relevance to
fields ranging from geology to medicine, the dynamics of spherulitic
crystallization and the conditions required for such growth remain
ill-understood. Here, we report on the conditions to induce controlled
spherulitic growth of sodium sulfate from evaporating aqueous solutions of
sulfate salt mixtures at room temperature. We reveal that introducing divalent
metal ions in the solution cause spherulitic growth of sodium sulfate. For the
first time, we quantify the supersaturation at the onset of spherulitic growth
from salt mixtures and determine the growth kinetics. Our results show that the
nonclassical nucleation process induces the growth of sodium sulfate
spherulites at high supersaturation in highly viscous solutions. The latter
reaches approximately 111 Pa$\cdot$s, triggered by the divalent ions, at the
onset of spherulite precipitation leading to a diffusion limited growth. We
also show that spherulites, which are metastable structures formed under
out-of-equilibrium conditions, can evolve into other shapes when
supersaturation decreases as growth continues at different evaporation rates.
These findings shed light on the conditions under which spherulites form and
offer practical strategies for tuning their morphology.

</details>
