<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 118]
- [eess.IV](#eess.IV) [Total: 14]
- [cs.GR](#cs.GR) [Total: 4]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 5]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 9]
- [stat.ML](#stat.ML) [Total: 3]
- [cs.CR](#cs.CR) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.CL](#cs.CL) [Total: 3]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning](https://arxiv.org/abs/2505.23883)
*Jianyang Gu,Samuel Stevens,Elizabeth G Campolongo,Matthew J Thompson,Net Zhang,Jiaman Wu,Andrei Kopanev,Zheda Mai,Alexander E. White,James Balhoff,Wasila Dahdul,Daniel Rubenstein,Hilmar Lapp,Tanya Berger-Wolf,Wei-Lun Chao,Yu Su*

Main category: cs.CV

TL;DR: 论文研究了大规模对比视觉语言训练中生物视觉模型的涌现行为，通过训练BioCLIP 2模型在TreeOfLife-200M数据集上，发现其学习到的嵌入空间具有生态和功能意义。


<details>
  <summary>Details</summary>
Motivation: 探索大规模训练的生物视觉模型是否会出现超越初始训练目标的新能力，并验证其在生物视觉任务中的表现。

Method: 构建TreeOfLife-200M数据集（2.14亿张生物图像），训练BioCLIP 2模型进行物种区分，并分析其嵌入空间的涌现特性。

Result: BioCLIP 2在栖息地分类和性状预测等任务中表现优异，嵌入空间在物种间和物种内均呈现有意义的分布。

Conclusion: 大规模训练数据能促进涌现特性的显著提升，形成具有生物学意义的嵌入空间。

Abstract: Foundation models trained at scale exhibit remarkable emergent behaviors,
learning new capabilities beyond their initial training objectives. We find
such emergent behaviors in biological vision models via large-scale contrastive
vision-language training. To achieve this, we first curate TreeOfLife-200M,
comprising 214 million images of living organisms, the largest and most diverse
biological organism image dataset to date. We then train BioCLIP 2 on
TreeOfLife-200M to distinguish different species. Despite the narrow training
objective, BioCLIP 2 yields extraordinary accuracy when applied to various
biological visual tasks such as habitat classification and trait prediction. We
identify emergent properties in the learned embedding space of BioCLIP 2. At
the inter-species level, the embedding distribution of different species aligns
closely with functional and ecological meanings (e.g., beak sizes and
habitats). At the intra-species level, instead of being diminished, the
intra-species variations (e.g., life stages and sexes) are preserved and better
separated in subspaces orthogonal to inter-species distinctions. We provide
formal proof and analyses to explain why hierarchical supervision and
contrastive objectives encourage these emergent properties. Crucially, our
results reveal that these properties become increasingly significant with
larger-scale training data, leading to a biologically meaningful embedding
space.

</details>


### [2] [Generating Fit Check Videos with a Handheld Camera](https://arxiv.org/abs/2505.23886)
*Bowei Chen,Brian Curless,Ira Kemelmacher-Shlizerman,Steven M. Seitz*

Main category: cs.CV

TL;DR: 提出了一种使用手持移动设备捕捉全身视频的便捷方法，通过两张静态照片和IMU运动参考生成逼真视频。


<details>
  <summary>Details</summary>
Motivation: 传统全身视频捕捉需要固定摄像头和精心构图，操作复杂。

Method: 输入两张静态照片和IMU运动参考，采用视频扩散模型和参数无关帧生成策略，结合多参考注意力机制。

Result: 实现了逼真的视频合成，支持新场景渲染，光照和阴影一致。

Conclusion: 该方法简化了全身视频捕捉流程，提升了生成视频的真实感。

Abstract: Self-captured full-body videos are popular, but most deployments require
mounted cameras, carefully-framed shots, and repeated practice. We propose a
more convenient solution that enables full-body video capture using handheld
mobile devices. Our approach takes as input two static photos (front and back)
of you in a mirror, along with an IMU motion reference that you perform while
holding your mobile phone, and synthesizes a realistic video of you performing
a similar target motion. We enable rendering into a new scene, with consistent
illumination and shadows. We propose a novel video diffusion-based model to
achieve this. Specifically, we propose a parameter-free frame generation
strategy, as well as a multi-reference attention mechanism, that effectively
integrate appearance information from both the front and back selfies into the
video diffusion model. Additionally, we introduce an image-based fine-tuning
strategy to enhance frame sharpness and improve the generation of shadows and
reflections, achieving a more realistic human-scene composition.

</details>


### [3] [Cora: Correspondence-aware image editing using few step diffusion](https://arxiv.org/abs/2505.23907)
*Amirhossein Almohammadi,Aryan Mikaeili,Sauradip Nag,Negar Hassanpour,Andrea Tagliasacchi,Ali Mahdavi-Amiri*

Main category: cs.CV

TL;DR: Cora是一种新型图像编辑框架，通过引入对应感知噪声校正和插值注意力图，解决了现有方法在结构修改和纹理保留上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法在处理非刚性变形、对象修改或内容生成等结构变化时，常产生无关纹理或无法保留关键属性（如姿势）。

Method: Cora利用语义对应关系对齐源图像和目标图像的纹理和结构，通过对应感知噪声校正和插值注意力图实现精确纹理转移和新内容生成。

Result: 实验表明，Cora在保持结构、纹理和身份方面表现优异，适用于姿势变化、对象添加和纹理细化等多种编辑任务。

Conclusion: Cora在用户研究中表现优于现有方法，提供了更好的编辑效果。

Abstract: Image editing is an important task in computer graphics, vision, and VFX,
with recent diffusion-based methods achieving fast and high-quality results.
However, edits requiring significant structural changes, such as non-rigid
deformations, object modifications, or content generation, remain challenging.
Existing few step editing approaches produce artifacts such as irrelevant
texture or struggle to preserve key attributes of the source image (e.g.,
pose). We introduce Cora, a novel editing framework that addresses these
limitations by introducing correspondence-aware noise correction and
interpolated attention maps. Our method aligns textures and structures between
the source and target images through semantic correspondence, enabling accurate
texture transfer while generating new content when necessary. Cora offers
control over the balance between content generation and preservation. Extensive
experiments demonstrate that, quantitatively and qualitatively, Cora excels in
maintaining structure, textures, and identity across diverse edits, including
pose changes, object addition, and texture refinements. User studies confirm
that Cora delivers superior results, outperforming alternatives.

</details>


### [4] [Representational Difference Explanations](https://arxiv.org/abs/2505.23917)
*Neehar Kondapaneni,Oisin Mac Aodha,Pietro Perona*

Main category: cs.CV

TL;DR: 提出了一种名为RDX的方法，用于发现和可视化两种学习表示之间的差异，支持更直接和可解释的模型比较。


<details>
  <summary>Details</summary>
Motivation: 当前的可解释AI（XAI）方法在模型比较方面表现不佳，而比较是科学分析的基础。

Method: 提出Representational Differences Explanations（RDX）方法，通过比较已知概念差异的模型进行验证。

Result: RDX在ImageNet和iNaturalist数据集上成功揭示了有意义的表示差异和数据的微妙模式。

Conclusion: RDX填补了机器学习中模型比较工具的空白，提供了一种有效且可解释的对比方法。

Abstract: We propose a method for discovering and visualizing the differences between
two learned representations, enabling more direct and interpretable model
comparisons. We validate our method, which we call Representational Differences
Explanations (RDX), by using it to compare models with known conceptual
differences and demonstrate that it recovers meaningful distinctions where
existing explainable AI (XAI) techniques fail. Applied to state-of-the-art
models on challenging subsets of the ImageNet and iNaturalist datasets, RDX
reveals both insightful representational differences and subtle patterns in the
data. Although comparison is a cornerstone of scientific analysis, current
tools in machine learning, namely post hoc XAI methods, struggle to support
model comparison effectively. Our work addresses this gap by introducing an
effective and explainable tool for contrasting model representations.

</details>


### [5] [ScaleLong: A Multi-Timescale Benchmark for Long Video Understanding](https://arxiv.org/abs/2505.23922)
*David Ma,Huaqing Yuan,Xingjian Wang,Qianbo Zang,Tianci Liu,Xinyang He,Yanbin Wei,Jiawei Guo,Ni Jiahui,Zhenzhu Yang,Meng Cao,Shanghaoran Quan,Yizhi Li,Wangchunshu Zhou,Jiaheng Liu,Wenhao Huang,Ge Zhang,Shiwen Ni,Xiaojie Jin*

Main category: cs.CV

TL;DR: ScaleLong是一个新的长视频理解基准测试，通过在同一视频内容中嵌入针对四个层次时间尺度的问题，直接比较模型在不同时间尺度上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试要么忽略多尺度设计，要么将问题分散在不同视频中，无法直接比较模型在同一内容上的表现。

Method: ScaleLong包含269个长视频，每个视频设计4-8个问题，覆盖四个时间尺度（秒、十秒、分钟、小时）。

Result: 评估23个MLLM显示性能呈U型曲线，最短和最长时间尺度表现较好，中间尺度较差。视觉标记容量的增加能提升所有时间尺度的推理能力。

Conclusion: ScaleLong为长视频理解提供了一个细粒度的多时间尺度基准测试，推动了MLLM能力的发展。

Abstract: Although long-video understanding demands that models capture hierarchical
temporal information -- from clip (seconds) and shot (tens of seconds) to event
(minutes) and story (hours) -- existing benchmarks either neglect this
multi-scale design or scatter scale-specific questions across different videos,
preventing direct comparison of model performance across timescales on the same
content. To address this, we introduce ScaleLong, the first benchmark to
disentangle these factors by embedding questions targeting four hierarchical
timescales -- clip (seconds), shot (tens of seconds), event (minutes), and
story (hours) -- all within the same video content. This within-content
multi-timescale questioning design enables direct comparison of model
performance across timescales on identical videos. ScaleLong features 269 long
videos (avg.\ 86\,min) from 5 main categories and 36 sub-categories, with 4--8
carefully designed questions, including at least one question for each
timescale. Evaluating 23 MLLMs reveals a U-shaped performance curve, with
higher accuracy at the shortest and longest timescales and a dip at
intermediate levels. Furthermore, ablation studies show that increased visual
token capacity consistently enhances reasoning across all timescales. ScaleLong
offers a fine-grained, multi-timescale benchmark for advancing MLLM
capabilities in long-video understanding. The code and dataset are available
https://github.com/multimodal-art-projection/ScaleLong.

</details>


### [6] [Point-MoE: Towards Cross-Domain Generalization in 3D Semantic Segmentation via Mixture-of-Experts](https://arxiv.org/abs/2505.23926)
*Xuweiyi Chen,Wentao Zhou,Aruni RoyChowdhury,Zezhou Cheng*

Main category: cs.CV

TL;DR: Point-MoE提出了一种Mixture-of-Experts架构，用于解决3D点云理解中的跨域泛化问题，无需域标签即可自动分配专家模块。


<details>
  <summary>Details</summary>
Motivation: 3D点云数据因来源多样（如不同传感器和场景）导致域异质性，传统方法难以实现跨域统一建模。

Method: 采用Point-MoE架构，结合top-k路由策略，自动分配专家模块处理不同域数据。

Result: Point-MoE在混合域数据上表现优于传统方法，并能泛化到未见过的域。

Conclusion: Point-MoE为3D理解提供了一种无需人工干预的可扩展解决方案。

Abstract: While scaling laws have transformed natural language processing and computer
vision, 3D point cloud understanding has yet to reach that stage. This can be
attributed to both the comparatively smaller scale of 3D datasets, as well as
the disparate sources of the data itself. Point clouds are captured by diverse
sensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor,
outdoor), each introducing unique scanning patterns, sampling densities, and
semantic biases. Such domain heterogeneity poses a major barrier towards
training unified models at scale, especially under the realistic constraint
that domain labels are typically inaccessible at inference time. In this work,
we propose Point-MoE, a Mixture-of-Experts architecture designed to enable
large-scale, cross-domain generalization in 3D perception. We show that
standard point cloud backbones degrade significantly in performance when
trained on mixed-domain data, whereas Point-MoE with a simple top-k routing
strategy can automatically specialize experts, even without access to domain
labels. Our experiments demonstrate that Point-MoE not only outperforms strong
multi-domain baselines but also generalizes better to unseen domains. This work
highlights a scalable path forward for 3D understanding: letting the model
discover structure in diverse 3D data, rather than imposing it via manual
curation or domain supervision.

</details>


### [7] [Leveraging Auxiliary Information in Text-to-Video Retrieval: A Review](https://arxiv.org/abs/2505.23952)
*Adriano Fragomeni,Dima Damen,Michael Wray*

Main category: cs.CV

TL;DR: 本文综述了81篇关于文本到视频检索（T2V）的研究论文，重点分析了利用辅助信息（如视觉属性、时空上下文和文本描述）提升检索性能的方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅依赖视频和文本模态的对齐计算相似性，而辅助信息可以弥补模态间的语义鸿沟，提升检索性能。

Method: 综述分析了81篇论文的方法论，包括辅助信息的提取和应用方式。

Result: 总结了在基准数据集上的最新成果，并讨论了可用数据集及其辅助信息。

Conclusion: 提出了未来研究方向，重点是利用辅助信息进一步优化检索性能。

Abstract: Text-to-Video (T2V) retrieval aims to identify the most relevant item from a
gallery of videos based on a user's text query. Traditional methods rely solely
on aligning video and text modalities to compute the similarity and retrieve
relevant items. However, recent advancements emphasise incorporating auxiliary
information extracted from video and text modalities to improve retrieval
performance and bridge the semantic gap between these modalities. Auxiliary
information can include visual attributes, such as objects; temporal and
spatial context; and textual descriptions, such as speech and rephrased
captions. This survey comprehensively reviews 81 research papers on
Text-to-Video retrieval that utilise such auxiliary information. It provides a
detailed analysis of their methodologies; highlights state-of-the-art results
on benchmark datasets; and discusses available datasets and their auxiliary
information. Additionally, it proposes promising directions for future
research, focusing on different ways to further enhance retrieval performance
using this information.

</details>


### [8] [MangoLeafViT: Leveraging Lightweight Vision Transformer with Runtime Augmentation for Efficient Mango Leaf Disease Classification](https://arxiv.org/abs/2505.23961)
*Rafi Hassan Chowdhury,Sabbir Ahmed*

Main category: cs.CV

TL;DR: 提出了一种轻量级视觉Transformer管道，用于芒果叶病分类，计算高效且兼容低端设备，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 芒果叶病对南亚国家农业供应链造成重大经济损失，现有深度学习方法计算效率不足，需设计更高效解决方案。

Method: 采用轻量级视觉Transformer结合自注意力机制，利用全局注意力捕捉疾病模式，并引入运行时增强提升性能。

Result: 在MangoLeafBD数据集上达到99.43%准确率，模型大小、参数数量和FLOPs均优于现有方法。

Conclusion: 该方法为芒果叶病分类提供了高效且高性能的解决方案，适用于低端设备。

Abstract: Ensuring food safety is critical due to its profound impact on public health,
economic stability, and global supply chains. Cultivation of Mango, a major
agricultural product in several South Asian countries, faces high financial
losses due to different diseases, affecting various aspects of the entire
supply chain. While deep learning-based methods have been explored for mango
leaf disease classification, there remains a gap in designing solutions that
are computationally efficient and compatible with low-end devices. In this
work, we propose a lightweight Vision Transformer-based pipeline with a
self-attention mechanism to classify mango leaf diseases, achieving
state-of-the-art performance with minimal computational overhead. Our approach
leverages global attention to capture intricate patterns among disease types
and incorporates runtime augmentation for enhanced performance. Evaluation on
the MangoLeafBD dataset demonstrates a 99.43% accuracy, outperforming existing
methods in terms of model size, parameter count, and FLOPs count.

</details>


### [9] [VisualSphinx: Large-Scale Synthetic Vision Logic Puzzles for RL](https://arxiv.org/abs/2505.23977)
*Yichen Feng,Zhangchen Xu,Fengqing Jiang,Yuetai Li,Bhaskar Ramasubramanian,Luyao Niu,Bill Yuchen Lin,Radha Poovendran*

Main category: cs.CV

TL;DR: VisualSphinx是一个大规模合成的视觉逻辑推理训练数据集，旨在提升视觉语言模型（VLM）的逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前VLM缺乏大规模且结构化的训练数据，限制了其在多模态推理任务中的表现。

Method: 提出规则到图像的合成流程，从种子问题中提取并扩展谜题规则，生成用于样本组装的图像合成代码。

Result: 实验表明，使用VisualSphinx训练的VLM在逻辑推理任务中表现更优，且对其他推理任务（如代数、算术和几何推理）也有帮助。

Conclusion: VisualSphinx填补了VLM训练数据的空白，显著提升了模型的逻辑推理能力。

Abstract: Vision language models (VLMs) are expected to perform effective multimodal
reasoning and make logically coherent decisions, which is critical to tasks
such as diagram understanding and spatial problem solving. However, current VLM
reasoning lacks large-scale and well-structured training datasets. To bridge
this gap, we propose VisualSphinx, a first-of-its-kind large-scale synthetic
visual logical reasoning training data. To tackle the challenge of image
synthesis with grounding answers, we propose a rule-to-image synthesis
pipeline, which extracts and expands puzzle rules from seed questions and
generates the code of grounding synthesis image synthesis for puzzle sample
assembly. Experiments demonstrate that VLM trained using GRPO on VisualSphinx
benefit from logical coherence and readability of our dataset and exhibit
improved performance on logical reasoning tasks. The enhanced reasoning
capabilities developed from VisualSphinx also benefit other reasoning tasks
such as algebraic reasoning, arithmetic reasoning and geometry reasoning.

</details>


### [10] [DeepTopoNet: A Framework for Subglacial Topography Estimation on the Greenland Ice Sheets](https://arxiv.org/abs/2505.23980)
*Bayu Adhi Tama,Mansa Krishna,Homayra Alam,Mostafa Cham,Omar Faruque,Gong Cheng,Jianwu Wang,Mathieu Morlighem,Vandana Janeja*

Main category: cs.CV

TL;DR: 论文提出了一种名为DeepTopoNet的深度学习框架，通过动态损失平衡机制整合雷达和BedMachine数据，以高精度重建格陵兰冰下地形。


<details>
  <summary>Details</summary>
Motivation: 格陵兰冰下地形的准确理解对预测冰盖质量损失及其对全球海平面上升的贡献至关重要，但观测数据稀疏且复杂，增加了模型预测的不确定性。

Method: 采用动态损失平衡机制，结合雷达和BedMachine数据，利用CNN架构进行亚网格尺度预测，并通过梯度特征和趋势面特征提升性能。

Result: 在Upernavik Isstrøm区域测试中，模型表现出高精度，优于基线方法。

Conclusion: 深度学习能有效填补观测空白，为冰下地形推断提供可扩展的高效解决方案。

Abstract: Understanding Greenland's subglacial topography is critical for projecting
the future mass loss of the ice sheet and its contribution to global sea-level
rise. However, the complex and sparse nature of observational data,
particularly information about the bed topography under the ice sheet,
significantly increases the uncertainty in model projections. Bed topography is
traditionally measured by airborne ice-penetrating radar that measures the ice
thickness directly underneath the aircraft, leaving data gap of tens of
kilometers in between flight lines. This study introduces a deep learning
framework, which we call as DeepTopoNet, that integrates radar-derived ice
thickness observations and BedMachine Greenland data through a novel dynamic
loss-balancing mechanism. Among all efforts to reconstruct bed topography,
BedMachine has emerged as one of the most widely used datasets, combining mass
conservation principles and ice thickness measurements to generate
high-resolution bed elevation estimates. The proposed loss function adaptively
adjusts the weighting between radar and BedMachine data, ensuring robustness in
areas with limited radar coverage while leveraging the high spatial resolution
of BedMachine predictions i.e. bed estimates. Our approach incorporates
gradient-based and trend surface features to enhance model performance and
utilizes a CNN architecture designed for subgrid-scale predictions. By
systematically testing on the Upernavik Isstr{\o}m) region, the model achieves
high accuracy, outperforming baseline methods in reconstructing subglacial
terrain. This work demonstrates the potential of deep learning in bridging
observational gaps, providing a scalable and efficient solution to inferring
subglacial topography.

</details>


### [11] [DGIQA: Depth-guided Feature Attention and Refinement for Generalizable Image Quality Assessment](https://arxiv.org/abs/2505.24002)
*Vaishnav Ramesh,Junliang Liu,Haining Wang,Md Jahidul Islam*

Main category: cs.CV

TL;DR: 论文提出了一种结合深度引导跨注意力机制（Depth-CAR）和Transformer-CNN桥接（TCB）的新方法DGIQA，用于无参考图像质量评估（NR-IQA），在合成和真实数据集上均达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决NR-IQA中因缺乏对人类主观感知的客观泛化能力而难以应对未知自然失真的问题。

Method: 通过Depth-CAR机制提取场景深度和空间特征，结合TCB融合Transformer的全局上下文依赖和CNN的局部空间特征，实现多模态注意力投影。

Result: DGIQA在合成和真实数据集上表现优异，尤其在跨数据集评估和自然失真（如低光、雾霾、镜头光晕）评估中超越SOTA模型。

Conclusion: DGIQA通过深度引导和跨模态特征融合，显著提升了NR-IQA的性能和泛化能力。

Abstract: A long-held challenge in no-reference image quality assessment (NR-IQA)
learning from human subjective perception is the lack of objective
generalization to unseen natural distortions. To address this, we integrate a
novel Depth-Guided cross-attention and refinement (Depth-CAR) mechanism, which
distills scene depth and spatial features into a structure-aware representation
for improved NR-IQA. This brings in the knowledge of object saliency and
relative contrast of the scene for more discriminative feature learning.
Additionally, we introduce the idea of TCB (Transformer-CNN Bridge) to fuse
high-level global contextual dependencies from a transformer backbone with
local spatial features captured by a set of hierarchical CNN (convolutional
neural network) layers. We implement TCB and Depth-CAR as multimodal
attention-based projection functions to select the most informative features,
which also improve training time and inference efficiency. Experimental results
demonstrate that our proposed DGIQA model achieves state-of-the-art (SOTA)
performance on both synthetic and authentic benchmark datasets. More
importantly, DGIQA outperforms SOTA models on cross-dataset evaluations as well
as in assessing natural image distortions such as low-light effects, hazy
conditions, and lens flares.

</details>


### [12] [Preemptive Hallucination Reduction: An Input-Level Approach for Multimodal Language Model](https://arxiv.org/abs/2505.24007)
*Nokimul Hasan Arif,Shadman Rabby,Md Hefzul Hossain Papon,Sabbir Ahmed*

Main category: cs.CV

TL;DR: 论文提出了一种基于集成学习的预处理框架，通过自适应选择输入过滤方法，显著减少大型语言模型（LLM）的视觉幻觉问题，无需修改模型架构或训练流程。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在视觉输入不一致时产生的幻觉问题影响了其可靠性，当前研究多关注事后修正或微调策略，而预处理技术的研究较少。

Method: 提出了一种基于噪声减少（NR）、边缘增强（EE）或原始输入（org）的自适应预处理框架，根据问题类型选择最佳过滤方法。

Result: 在`HaloQuest`数据集上，该方法将幻觉率降低了44.3%，通过SelfCheckGPT的自然语言推理（NLI）评分验证。

Conclusion: 研究表明，智能输入预处理能显著提升LLM的准确性，为更可靠的多模态系统提供了新思路。

Abstract: Visual hallucinations in Large Language Models (LLMs), where the model
generates responses that are inconsistent with the visual input, pose a
significant challenge to their reliability, particularly in contexts where
precise and trustworthy outputs are critical. Current research largely
emphasizes post-hoc correction or model-specific fine-tuning strategies, with
limited exploration of preprocessing techniques to address hallucination issues
at the input stage. This study presents a novel ensemble-based preprocessing
framework that adaptively selects the most appropriate filtering approach --
noise reduced (NR), edge enhanced (EE), or unaltered input (org) based on the
type of question posed, resulting into reduced hallucination without requiring
any modifications to the underlying model architecture or training pipeline.
Evaluated on the `HaloQuest' dataset -- a benchmark designed to test multimodal
reasoning on visually complex inputs, our method achieves a 44.3% reduction in
hallucination rates, as measured by Natural Language Inference (NLI) scores
using SelfCheckGPT. This demonstrates that intelligent input conditioning alone
can significantly enhance factual grounding in LLM responses. The findings
highlight the importance of adaptive preprocessing techniques in mitigating
hallucinations, paving the way for more reliable multimodal systems capable of
addressing real-world challenges.

</details>


### [13] [Multi-Group Proportional Representation for Text-to-Image Models](https://arxiv.org/abs/2505.24023)
*Sangwon Jung,Alex Oesterling,Claudio Mayrink Verdun,Sajani Vithana,Taesup Moon,Flavio P. Calmon*

Main category: cs.CV

TL;DR: 本文提出了一种新框架MPR，用于衡量T2I模型生成的图像中交叉群体的代表性，并通过优化算法改善模型的平衡生成。


<details>
  <summary>Details</summary>
Motivation: T2I模型可能无法公平代表不同人口群体，甚至传播刻板印象或忽视少数群体，目前缺乏系统方法来衡量和控制这种代表性危害。

Method: 应用MPR指标衡量交叉群体的代表性，并开发算法优化T2I模型以平衡生成。

Result: 实验表明MPR能有效衡量代表性，优化后的模型在保持生成质量的同时更平衡地生成图像。

Conclusion: MPR为T2I模型的公平性提供了量化工具，并展示了优化潜力。

Abstract: Text-to-image (T2I) generative models can create vivid, realistic images from
textual descriptions. As these models proliferate, they expose new concerns
about their ability to represent diverse demographic groups, propagate
stereotypes, and efface minority populations. Despite growing attention to the
"safe" and "responsible" design of artificial intelligence (AI), there is no
established methodology to systematically measure and control representational
harms in image generation. This paper introduces a novel framework to measure
the representation of intersectional groups in images generated by T2I models
by applying the Multi-Group Proportional Representation (MPR) metric. MPR
evaluates the worst-case deviation of representation statistics across given
population groups in images produced by a generative model, allowing for
flexible and context-specific measurements based on user requirements. We also
develop an algorithm to optimize T2I models for this metric. Through
experiments, we demonstrate that MPR can effectively measure representation
statistics across multiple intersectional groups and, when used as a training
objective, can guide models toward a more balanced generation across
demographic groups while maintaining generation quality.

</details>


### [14] [DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models](https://arxiv.org/abs/2505.24025)
*Chenbin Pan,Wenbin He,Zhengzhong Tu,Liu Ren*

Main category: cs.CV

TL;DR: DINO-R1首次通过强化学习提升视觉基础模型的上下文推理能力，提出GRQO训练策略，显著优于传统微调方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型（如DINO系列）缺乏上下文推理能力，而大语言模型（如DeepSeek-R1）已通过强化学习取得显著成功。

Method: 提出GRQO训练策略，结合KL正则化稳定训练，并在Grounding-DINO基础上集成视觉提示编码器和查询选择机制。

Result: 在COCO、LVIS和ODinW数据集上，DINO-R1显著优于监督微调基线，泛化能力强。

Conclusion: DINO-R1通过强化学习成功提升了视觉基础模型的推理能力，为视觉任务提供了新思路。

Abstract: The recent explosive interest in the reasoning capabilities of large language
models, such as DeepSeek-R1, has demonstrated remarkable success through
reinforcement learning-based fine-tuning frameworks, exemplified by methods
like Group Relative Policy Optimization (GRPO). However, such reasoning
abilities remain underexplored and notably absent in vision foundation models,
including representation models like the DINO series. In this work, we propose
\textbf{DINO-R1}, the first such attempt to incentivize visual in-context
reasoning capabilities of vision foundation models using reinforcement
learning. Specifically, DINO-R1 introduces \textbf{Group Relative Query
Optimization (GRQO)}, a novel reinforcement-style training strategy explicitly
designed for query-based representation models, which computes query-level
rewards based on group-normalized alignment quality. We also apply
KL-regularization to stabilize the objectness distribution to reduce the
training instability. This joint optimization enables dense and expressive
supervision across queries while mitigating overfitting and distributional
drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models
that integrate a visual prompt encoder and a visual-guided query selection
mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that
DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving
strong generalization in both open-vocabulary and closed-set visual prompting
scenarios.

</details>


### [15] [MaskAdapt: Unsupervised Geometry-Aware Domain Adaptation Using Multimodal Contextual Learning and RGB-Depth Masking](https://arxiv.org/abs/2505.24026)
*Numair Nadeem,Muhammad Hamza Asad,Saeed Anwar,Abdul Bais*

Main category: cs.CV

TL;DR: MaskAdapt通过结合RGB图像和深度数据，提出了一种新的无监督域适应方法，显著提高了作物和杂草的语义分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖像素级标注且难以适应新领域的域偏移，尤其是在作物和杂草视觉混合的情况下。

Method: MaskAdapt整合RGB图像和深度梯度特征，通过跨注意力机制优化特征表示，并采用几何感知掩码策略增强空间上下文学习。

Result: 在真实农业数据集上，MaskAdapt优于现有SOTA方法，显著提高了mIOU。

Conclusion: MaskAdapt通过多模态上下文学习和几何感知掩码，有效解决了域适应中的遮挡和视觉混合问题。

Abstract: Semantic segmentation of crops and weeds is crucial for site-specific farm
management; however, most existing methods depend on labor intensive
pixel-level annotations. A further challenge arises when models trained on one
field (source domain) fail to generalize to new fields (target domain) due to
domain shifts, such as variations in lighting, camera setups, soil composition,
and crop growth stages. Unsupervised Domain Adaptation (UDA) addresses this by
enabling adaptation without target-domain labels, but current UDA methods
struggle with occlusions and visual blending between crops and weeds, leading
to misclassifications in real-world conditions. To overcome these limitations,
we introduce MaskAdapt, a novel approach that enhances segmentation accuracy
through multimodal contextual learning by integrating RGB images with features
derived from depth data. By computing depth gradients from depth maps, our
method captures spatial transitions that help resolve texture ambiguities.
These gradients, through a cross-attention mechanism, refines RGB feature
representations, resulting in sharper boundary delineation. In addition, we
propose a geometry-aware masking strategy that applies horizontal, vertical,
and stochastic masks during training. This encourages the model to focus on the
broader spatial context for robust visual recognition. Evaluations on real
agricultural datasets demonstrate that MaskAdapt consistently outperforms
existing State-of-the-Art (SOTA) UDA methods, achieving improved segmentation
mean Intersection over Union (mIOU) across diverse field conditions.

</details>


### [16] [SIM: A mapping framework for built environment auditing based on street view imagery](https://arxiv.org/abs/2505.24076)
*Huan Ning,Zhenlong Li,Manzhu Yu,Wenpeng Yin*

Main category: cs.CV

TL;DR: 本文提出了一种基于街景图像的开源映射框架，用于提升建筑环境审计的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的建筑环境审计方法耗时且成本高，而现有的街景图像映射工具和方法尚未成熟，缺乏通用框架。

Method: 开发了一个开源框架，提供三种管道：地面物体宽度测量、已知尺寸物体的3D定位和直径测量。

Result: 通过道路宽度测量、停车标志定位和街道树木直径测量三个案例展示了框架的实用性。

Conclusion: 该框架为研究人员和城市规划者提供了自动测量和映射工具，显著提升了审计效率和准确性。

Abstract: Built environment auditing refers to the systematic documentation and
assessment of urban and rural spaces' physical, social, and environmental
characteristics, such as walkability, road conditions, and traffic lights. It
is used to collect data for the evaluation of how built environments impact
human behavior, health, mobility, and overall urban functionality.
Traditionally, built environment audits were conducted using field surveys and
manual observations, which were time-consuming and costly. The emerging street
view imagery, e.g., Google Street View, has become a widely used data source
for conducting built environment audits remotely. Deep learning and computer
vision techniques can extract and classify objects from street images to
enhance auditing productivity. Before meaningful analysis, the detected objects
need to be geospatially mapped for accurate documentation. However, the mapping
methods and tools based on street images are underexplored, and there are no
universal frameworks or solutions yet, imposing difficulties in auditing the
street objects. In this study, we introduced an open source street view mapping
framework, providing three pipelines to map and measure: 1) width measurement
for ground objects, such as roads; 2) 3D localization for objects with a known
dimension (e.g., doors and stop signs); and 3) diameter measurements (e.g.,
street trees). These pipelines can help researchers, urban planners, and other
professionals automatically measure and map target objects, promoting built
environment auditing productivity and accuracy. Three case studies, including
road width measurement, stop sign localization, and street tree diameter
measurement, are provided in this paper to showcase pipeline usage.

</details>


### [17] [ComposeAnything: Composite Object Priors for Text-to-Image Generation](https://arxiv.org/abs/2505.24086)
*Zeeshan Khan,Shizhe Chen,Cordelia Schmid*

Main category: cs.CV

TL;DR: ComposeAnything 是一个新框架，通过结合 LLM 的推理能力和 2.5D 语义布局，提升文本到图像生成的质量和一致性，无需重新训练现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有 T2I 模型在复杂对象排列和 3D 定位上的不足，同时保持生成质量和一致性。

Method: 利用 LLM 生成 2.5D 语义布局（含深度信息），作为扩散模型的先验，通过对象先验强化和空间控制去噪生成图像。

Result: 在 T2I-CompBench 和 NSR-1K 基准测试中表现优于现有方法，生成图像质量高且忠实于文本描述。

Conclusion: ComposeAnything 显著提升了复杂文本到图像生成的能力，尤其在 3D 定位和对象排列方面。

Abstract: Generating images from text involving complex and novel object arrangements
remains a significant challenge for current text-to-image (T2I) models.
Although prior layout-based methods improve object arrangements using spatial
constraints with 2D layouts, they often struggle to capture 3D positioning and
sacrifice quality and coherence. In this work, we introduce ComposeAnything, a
novel framework for improving compositional image generation without retraining
existing T2I models. Our approach first leverages the chain-of-thought
reasoning abilities of LLMs to produce 2.5D semantic layouts from text,
consisting of 2D object bounding boxes enriched with depth information and
detailed captions. Based on this layout, we generate a spatial and depth aware
coarse composite of objects that captures the intended composition, serving as
a strong and interpretable prior that replaces stochastic noise initialization
in diffusion-based T2I models. This prior guides the denoising process through
object prior reinforcement and spatial-controlled denoising, enabling seamless
generation of compositional objects and coherent backgrounds, while allowing
refinement of inaccurate priors. ComposeAnything outperforms state-of-the-art
methods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D
spatial arrangements, high object counts, and surreal compositions. Human
evaluations further demonstrate that our model generates high-quality images
with compositions that faithfully reflect the text.

</details>


### [18] [Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors](https://arxiv.org/abs/2505.24103)
*Peiran Xu,Yadong Mu*

Main category: cs.CV

TL;DR: 论文提出了一种弱监督功能定位方法，通过伪标签训练模型，结合三种改进技术，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法基于类激活图，不适合定位动作和功能，需改进。

Method: 利用基础模型生成伪标签，引入标签细化、细粒度特征对齐和轻量推理模块。

Result: 实验表明模型性能显著优于现有方法。

Conclusion: 提出的方法有效解决了对象与动作之间的鸿沟，代码已开源。

Abstract: In this work, we focus on the task of weakly supervised affordance grounding,
where a model is trained to identify affordance regions on objects using
human-object interaction images and egocentric object images without dense
labels. Previous works are mostly built upon class activation maps, which are
effective for semantic segmentation but may not be suitable for locating
actions and functions. Leveraging recent advanced foundation models, we develop
a supervised training pipeline based on pseudo labels. The pseudo labels are
generated from an off-the-shelf part segmentation model, guided by a mapping
from affordance to part names. Furthermore, we introduce three key enhancements
to the baseline model: a label refining stage, a fine-grained feature alignment
process, and a lightweight reasoning module. These techniques harness the
semantic knowledge of static objects embedded in off-the-shelf foundation
models to improve affordance learning, effectively bridging the gap between
objects and actions. Extensive experiments demonstrate that the performance of
the proposed model has achieved a breakthrough improvement over existing
methods. Our codes are available at https://github.com/woyut/WSAG-PLSP .

</details>


### [19] [Federated Foundation Model for GI Endoscopy Images](https://arxiv.org/abs/2505.24108)
*Alina Devkota,Annahita Amireskandari,Joel Palko,Shyam Thakkar,Donald Adjeroh,Xiajun Jiang,Binod Bhattarai,Prashnna K. Gyawali*

Main category: cs.CV

TL;DR: 提出了一种基于联邦学习（FL）的框架，用于训练胃肠内窥镜成像的基础模型，解决了医疗数据隐私问题，并在分类、检测和分割任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 医疗数据隐私限制导致基础模型训练困难，而联邦学习可以保护数据隐私的同时实现模型共享。

Method: 采用联邦学习框架，评估多种FL算法在无任务特定标签的情况下训练基础模型的效果，并在同质和异质环境下进行实验。

Result: 训练的基础模型在分类、检测和分割任务中均表现优异，验证了方法的有效性。

Conclusion: 联邦学习为医疗基础模型训练提供了一种隐私保护的可行方案，具有广泛应用潜力。

Abstract: Gastrointestinal (GI) endoscopy is essential in identifying GI tract
abnormalities in order to detect diseases in their early stages and improve
patient outcomes. Although deep learning has shown success in supporting GI
diagnostics and decision-making, these models require curated datasets with
labels that are expensive to acquire. Foundation models offer a promising
solution by learning general-purpose representations, which can be finetuned
for specific tasks, overcoming data scarcity. Developing foundation models for
medical imaging holds significant potential, but the sensitive and protected
nature of medical data presents unique challenges. Foundation model training
typically requires extensive datasets, and while hospitals generate large
volumes of data, privacy restrictions prevent direct data sharing, making
foundation model training infeasible in most scenarios. In this work, we
propose a FL framework for training foundation models for gastroendoscopy
imaging, enabling data to remain within local hospital environments while
contributing to a shared model. We explore several established FL algorithms,
assessing their suitability for training foundation models without relying on
task-specific labels, conducting experiments in both homogeneous and
heterogeneous settings. We evaluate the trained foundation model on three
critical downstream tasks--classification, detection, and segmentation--and
demonstrate that it achieves improved performance across all tasks,
highlighting the effectiveness of our approach in a federated,
privacy-preserving setting.

</details>


### [20] [CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs](https://arxiv.org/abs/2505.24120)
*Ai Jian,Weijie Qiu,Xiaokun Wang,Peiyu Wang,Yunzhuo Hao,Jiangbo Pei,Yichen Wei,Yi Peng,Xuchen Song*

Main category: cs.CV

TL;DR: 论文介绍了CSVQA，一个用于评估视觉语言模型（VLMs）在科学推理能力的多模态基准测试，填补了现有测试缺乏科学背景的空白。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估通用图像理解或文本驱动推理，缺乏需要领域知识和视觉证据分析的科学推理评估。

Method: 提出CSVQA基准，包含1,378个跨STEM学科的问题-答案对，要求领域知识、视觉证据整合和高阶推理。

Result: 对15个VLMs的评估显示性能差距显著，最优模型准确率仅49.6%。

Conclusion: CSVQA揭示了VLMs在科学推理上的不足，呼吁进一步研究提升其能力。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable progress in
multimodal understanding, yet their capabilities for scientific reasoning
remains inadequately assessed. Current multimodal benchmarks predominantly
evaluate generic image comprehension or text-driven reasoning, lacking
authentic scientific contexts that require domain-specific knowledge
integration with visual evidence analysis. To fill this gap, we present CSVQA,
a diagnostic multimodal benchmark specifically designed for evaluating
scientific reasoning through domain-grounded visual question answering.Our
benchmark features 1,378 carefully constructed question-answer pairs spanning
diverse STEM disciplines, each demanding domain knowledge, integration of
visual evidence, and higher-order reasoning. Compared to prior multimodal
benchmarks, CSVQA places greater emphasis on real-world scientific content and
complex reasoning.We additionally propose a rigorous evaluation protocol to
systematically assess whether model predictions are substantiated by valid
intermediate reasoning steps based on curated explanations. Our comprehensive
evaluation of 15 VLMs on this benchmark reveals notable performance
disparities, as even the top-ranked proprietary model attains only 49.6\%
accuracy.This empirical evidence underscores the pressing need for advancing
scientific reasoning capabilities in VLMs. Our CSVQA is released at
https://huggingface.co/datasets/Skywork/CSVQA.

</details>


### [21] [S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Modelwith Spatio-Temporal Visual Representation](https://arxiv.org/abs/2505.24139)
*Yichen Xie,Runsheng Xu,Tong He,Jyh-Jing Hwang,Katie Luo,Jingwei Ji,Hubert Lin,Letian Chen,Yiren Lu,Zhaoqi Leng,Dragomir Anguelov,Mingxing Tan*

Main category: cs.CV

TL;DR: S4-Driver是一种基于多模态大语言模型的自监督运动规划算法，通过稀疏体积策略将2D视觉表示转换为3D空间，无需微调视觉编码器，在nuScenes和Waymo数据集上表现优于现有监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于MLLM的端到端运动规划方法通常在2D图像空间预训练，而自动驾驶规划是在3D空间进行，导致性能不足。S4-Driver旨在填补这一输入表示空间的差距。

Method: 提出S4-Driver算法，利用稀疏体积策略将MLLM的视觉表示从透视视图无缝转换到3D空间，聚合多视角和多帧视觉输入。

Result: 在nuScenes和Waymo数据集上，S4-Driver表现优于现有监督方法，且无需人工标注，具有良好扩展性。

Conclusion: S4-Driver通过自监督学习和3D空间表示优化，显著提升了自动驾驶运动规划的性能和扩展性。

Abstract: The latest advancements in multi-modal large language models (MLLMs) have
spurred a strong renewed interest in end-to-end motion planning approaches for
autonomous driving. Many end-to-end approaches rely on human annotations to
learn intermediate perception and prediction tasks, while purely
self-supervised approaches--which directly learn from sensor inputs to generate
planning trajectories without human annotations often underperform the state of
the art. We observe a key gap in the input representation space: end-to-end
approaches built on MLLMs are often pretrained with reasoning tasks in 2D image
space rather than the native 3D space in which autonomous vehicles plan. To
this end, we propose S4-Driver, a scalable self-supervised motion planning
algorithm with spatio-temporal visual representation, based on the popular PaLI
multimodal large language model. S4-Driver uses a novel sparse volume strategy
to seamlessly transform the strong visual representation of MLLMs from
perspective view to 3D space without the need to finetune the vision encoder.
This representation aggregates multi-view and multi-frame visual inputs and
enables better prediction of planning trajectories in 3D space. To validate our
method, we run experiments on both nuScenes and Waymo Open Motion Dataset (with
in-house camera data). Results show that S4-Driver performs favorably against
existing supervised multi-task approaches while requiring no human annotations.
It also demonstrates great scalability when pretrained on large volumes of
unannotated driving logs.

</details>


### [22] [The Butterfly Effect in Pathology: Exploring Security in Pathology Foundation Models](https://arxiv.org/abs/2505.24141)
*Jiashuai Liu,Yingjia Shang,Yingkang Zhan,Di Zhang,Yi Niu,Dong Wei,Xian Wu,Zeyu Gao,Chen Li,Yefeng Zheng*

Main category: cs.CV

TL;DR: 本文首次系统研究了病理学基础模型在全切片图像（WSI）分析中对对抗攻击的脆弱性，提出了一种无标签攻击框架，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着病理学基础模型在研究和临床决策支持系统中的广泛应用，其安全性问题日益突出，但对抗攻击的脆弱性尚未得到充分研究。

Method: 提出了一种基于“局部扰动具有全局影响”原则的无标签攻击框架，并改进了四种经典白盒攻击方法，重新定义了基于WSI特性的扰动预算。

Result: 实验表明，仅对每张切片0.1%的区块添加不可察觉的噪声，即可导致下游任务准确率下降高达20%。

Conclusion: 研究为病理学基础模型的对抗鲁棒性和可靠部署奠定了基础，并探讨了潜在防御策略。

Abstract: With the widespread adoption of pathology foundation models in both research
and clinical decision support systems, exploring their security has become a
critical concern. However, despite their growing impact, the vulnerability of
these models to adversarial attacks remains largely unexplored. In this work,
we present the first systematic investigation into the security of pathology
foundation models for whole slide image~(WSI) analysis against adversarial
attacks. Specifically, we introduce the principle of \textit{local perturbation
with global impact} and propose a label-free attack framework that operates
without requiring access to downstream task labels. Under this attack
framework, we revise four classical white-box attack methods and redefine the
perturbation budget based on the characteristics of WSI. We conduct
comprehensive experiments on three representative pathology foundation models
across five datasets and six downstream tasks. Despite modifying only 0.1\% of
patches per slide with imperceptible noise, our attack leads to downstream
accuracy degradation that can reach up to 20\% in the worst cases. Furthermore,
we analyze key factors that influence attack success, explore the relationship
between patch-level vulnerability and semantic content, and conduct a
preliminary investigation into potential defence strategies. These findings lay
the groundwork for future research on the adversarial robustness and reliable
deployment of pathology foundation models. Our code is publicly available at:
https://github.com/Jiashuai-Liu-hmos/Attack-WSI-pathology-foundation-models.

</details>


### [23] [Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction](https://arxiv.org/abs/2505.24156)
*Chenyou Fan,Fangzheng Yan,Chenjia Bai,Jiepeng Wang,Chi Zhang,Zhen Wang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出了一种新的双手机器人操作策略，通过微调文本到视频模型和训练轻量级扩散策略，解决了双手机器人操作中的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 双手机器人操作由于动作空间大和需要协调双臂运动，泛化能力差。现有方法依赖视觉-语言-动作模型，但单臂数据或预训练模型的知识迁移效果不佳。

Method: 采用两阶段范式：微调独立的文本到光流和光流到视频模型，利用光流作为中间变量表示图像间的细微运动，减少语言模糊性。

Result: 在仿真和真实世界实验中，方法显著减少了机器人数据需求并提高了操作效果。

Conclusion: 该方法通过光流中间表示和两阶段模型，有效提升了双手机器人操作的泛化能力。

Abstract: Learning a generalizable bimanual manipulation policy is extremely
challenging for embodied agents due to the large action space and the need for
coordinated arm movements. Existing approaches rely on Vision-Language-Action
(VLA) models to acquire bimanual policies. However, transferring knowledge from
single-arm datasets or pre-trained VLA models often fails to generalize
effectively, primarily due to the scarcity of bimanual data and the fundamental
differences between single-arm and bimanual manipulation. In this paper, we
propose a novel bimanual foundation policy by fine-tuning the leading
text-to-video models to predict robot trajectories and training a lightweight
diffusion policy for action generation. Given the lack of embodied knowledge in
text-to-video models, we introduce a two-stage paradigm that fine-tunes
independent text-to-flow and flow-to-video models derived from a pre-trained
text-to-video model. Specifically, optical flow serves as an intermediate
variable, providing a concise representation of subtle movements between
images. The text-to-flow model predicts optical flow to concretize the intent
of language instructions, and the flow-to-video model leverages this flow for
fine-grained video prediction. Our method mitigates the ambiguity of language
in single-stage text-to-video prediction and significantly reduces the
robot-data requirement by avoiding direct use of low-level actions. In
experiments, we collect high-quality manipulation data for real dual-arm robot,
and the results of simulation and real-world experiments demonstrate the
effectiveness of our method.

</details>


### [24] [Threading Keyframe with Narratives: MLLMs as Strong Long Video Comprehenders](https://arxiv.org/abs/2505.24158)
*Bo Fang,Wenhao Wu,Qiangqiang Wu,Yuxin Song,Antoni B. Chan*

Main category: cs.CV

TL;DR: 提出Nar-KFC模块，通过关键帧选择和文本叙事插入解决长视频理解中的视觉标记过多与语言模型上下文限制问题。


<details>
  <summary>Details</summary>
Motivation: 长视频理解中，视觉标记过多与语言模型上下文限制的矛盾导致传统方法效率低下或内容不相关。

Method: 1. 将关键帧选择建模为整数二次规划问题，优化查询相关性和帧多样性；2. 使用非关键帧生成文本叙事，插入关键帧间以保持时序连贯性。

Result: 在多个长视频基准测试中，Nar-KFC显著提升了多模态大语言模型的性能。

Conclusion: Nar-KFC是一种高效且内容感知的长视频压缩策略，有效结合视觉与文本模态。

Abstract: Employing Multimodal Large Language Models (MLLMs) for long video
understanding remains a challenging problem due to the dilemma between the
substantial number of video frames (i.e., visual tokens) versus the limited
context length of language models. Traditional uniform sampling often leads to
selection of irrelevant content, while post-training MLLMs on thousands of
frames imposes a substantial computational burden. In this paper, we propose
threading keyframes with narratives (Nar-KFC), a plug-and-play module to
facilitate effective and efficient long video perception. Nar-KFC generally
involves two collaborative steps. First, we formulate the keyframe selection
process as an integer quadratic programming problem, jointly optimizing
query-relevance and frame-diversity. To avoid its computational complexity, a
customized greedy search strategy is designed as an efficient alternative.
Second, to mitigate the temporal discontinuity caused by sparse keyframe
sampling, we further introduce interleaved textual narratives generated from
non-keyframes using off-the-shelf captioners. These narratives are inserted
between keyframes based on their true temporal order, forming a coherent and
compact representation. Nar-KFC thus serves as a temporal- and content-aware
compression strategy that complements visual and textual modalities.
Experimental results on multiple long-video benchmarks demonstrate that Nar-KFC
significantly improves the performance of popular MLLMs. Code will be made
publicly available.

</details>


### [25] [Training-free zero-shot 3D symmetry detection with visual features back-projected to geometry](https://arxiv.org/abs/2505.24162)
*Isaac Aguirre,Ivan Sipiran*

Main category: cs.CV

TL;DR: 提出了一种无需训练的零样本3D对称性检测方法，利用DINOv2等基础视觉模型提取特征，并通过反向投影和算法识别反射对称平面，性能优于传统几何方法和基于学习的方法。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用基础视觉模型解决复杂的3D几何问题（如对称性检测），而无需依赖训练数据。

Method: 从3D物体的渲染视图中提取视觉特征，反向投影到原始几何上，利用特征的对称不变性设计算法识别反射对称平面。

Result: 在ShapeNet子集上的实验表明，该方法优于传统几何方法和基于学习的方法，且无需训练数据。

Conclusion: 基础视觉模型能够有效解决复杂的3D几何问题，为对称性检测提供了新的思路。

Abstract: We present a simple yet effective training-free approach for zero-shot 3D
symmetry detection that leverages visual features from foundation vision models
such as DINOv2. Our method extracts features from rendered views of 3D objects
and backprojects them onto the original geometry. We demonstrate the symmetric
invariance of these features and use them to identify reflection-symmetry
planes through a proposed algorithm. Experiments on a subset of ShapeNet
demonstrate that our approach outperforms both traditional geometric methods
and learning-based approaches without requiring any training data. Our work
demonstrates how foundation vision models can help in solving complex 3D
geometric problems such as symmetry detection.

</details>


### [26] [Pretraining Deformable Image Registration Networks with Random Images](https://arxiv.org/abs/2505.24167)
*Junyu Chen,Shuwen Wei,Yihao Liu,Aaron Carass,Yong Du*

Main category: cs.CV

TL;DR: 使用随机图像配准作为预训练任务，提升医学图像配准的准确性、减少领域数据需求并加速下游训练。


<details>
  <summary>Details</summary>
Motivation: 探索无需医学图像的深度学习方法，通过随机图像配准预训练模型，提升泛化能力和计算效率。

Method: 提出以随机图像配准为代理任务，预训练基础模型，用于医学图像配准。

Result: 预训练策略提高了配准精度，减少了对领域数据的需求，并加速了下游训练的收敛。

Conclusion: 随机图像配准预训练是一种有效的策略，可提升医学图像配准的性能和效率。

Abstract: Recent advances in deep learning-based medical image registration have shown
that training deep neural networks~(DNNs) does not necessarily require medical
images. Previous work showed that DNNs trained on randomly generated images
with carefully designed noise and contrast properties can still generalize well
to unseen medical data. Building on this insight, we propose using registration
between random images as a proxy task for pretraining a foundation model for
image registration. Empirical results show that our pretraining strategy
improves registration accuracy, reduces the amount of domain-specific data
needed to achieve competitive performance, and accelerates convergence during
downstream training, thereby enhancing computational efficiency.

</details>


### [27] [DrVD-Bench: Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis?](https://arxiv.org/abs/2505.24173)
*Tianhong Zhou,Yin Xu,Yingtao Zhu,Chuxi Xiao,Haiyang Bian,Lei Wei,Xuegong Zhang*

Main category: cs.CV

TL;DR: DrVD-Bench是一个多模态临床视觉推理基准，旨在评估视觉语言模型（VLMs）是否真正具备类似临床医生的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能系统评估VLMs是否真正模拟人类临床推理，还是仅模仿表面模式。

Method: DrVD-Bench包含三个模块（视觉证据理解、推理轨迹评估和报告生成评估），涵盖7,789个图像-问题对，覆盖20种任务类型和17种诊断类别。

Result: 评估19种VLMs发现，随着推理复杂性增加，性能显著下降，部分模型开始展现类似人类的推理痕迹，但仍依赖表面相关性。

Conclusion: DrVD-Bench为开发可信赖的临床VLMs提供了严格的评估框架。

Abstract: Vision-language models (VLMs) exhibit strong zero-shot generalization on
natural images and show early promise in interpretable medical image analysis.
However, existing benchmarks do not systematically evaluate whether these
models truly reason like human clinicians or merely imitate superficial
patterns. To address this gap, we propose DrVD-Bench, the first multimodal
benchmark for clinical visual reasoning. DrVD-Bench consists of three modules:
Visual Evidence Comprehension, Reasoning Trajectory Assessment, and Report
Generation Evaluation, comprising a total of 7,789 image-question pairs. Our
benchmark covers 20 task types, 17 diagnostic categories, and five imaging
modalities-CT, MRI, ultrasound, radiography, and pathology. DrVD-Bench is
explicitly structured to reflect the clinical reasoning workflow from modality
recognition to lesion identification and diagnosis. We benchmark 19 VLMs,
including general-purpose and medical-specific, open-source and proprietary
models, and observe that performance drops sharply as reasoning complexity
increases. While some models begin to exhibit traces of human-like reasoning,
they often still rely on shortcut correlations rather than grounded visual
understanding. DrVD-Bench offers a rigorous and structured evaluation framework
to guide the development of clinically trustworthy VLMs.

</details>


### [28] [Seeing is Not Reasoning: MVPBench for Graph-based Evaluation of Multi-path Visual Physical CoT](https://arxiv.org/abs/2505.24182)
*Zhuobai Dong,Junchao Yi,Ziyuan Zheng,Haochen Han,Xiangxi Zheng,Alex Jinpeng Wang,Fangming Liu,Linjie Li*

Main category: cs.CV

TL;DR: MVPBench 是一个新的基准测试，用于评估多模态大语言模型（MLLMs）在视觉物理推理中的表现，发现即使是先进模型如 GPT-4o 也表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前 MLLMs 在视觉物理推理（如运动规律、空间关系和因果关系）方面表现较差，缺乏连贯的推理链和视觉证据支持。

Method: 引入 MVPBench 基准测试，通过视觉链式推理（CoT）和多图像输入评估模型，并设计图式 CoT 一致性指标验证推理逻辑。

Result: 实验显示，即使是先进 MLLMs 在视觉推理准确性和图像-文本对齐方面表现不佳，且 RL 微调可能损害空间推理能力。

Conclusion: 需要重新思考当前 MLLMs 的微调方法，以提升其在物理推理任务中的表现。

Abstract: Understanding the physical world - governed by laws of motion, spatial
relations, and causality - poses a fundamental challenge for multimodal large
language models (MLLMs). While recent advances such as OpenAI o3 and GPT-4o
demonstrate impressive perceptual and reasoning capabilities, our investigation
reveals these models struggle profoundly with visual physical reasoning,
failing to grasp basic physical laws, spatial interactions, and causal effects
in complex scenes. More importantly, they often fail to follow coherent
reasoning chains grounded in visual evidence, especially when multiple steps
are needed to arrive at the correct answer. To rigorously evaluate this
capability, we introduce MVPBench, a curated benchmark designed to rigorously
evaluate visual physical reasoning through the lens of visual chain-of-thought
(CoT). Each example features interleaved multi-image inputs and demands not
only the correct final answer but also a coherent, step-by-step reasoning path
grounded in evolving visual cues. This setup mirrors how humans reason through
real-world physical processes over time. To ensure fine-grained evaluation, we
introduce a graph-based CoT consistency metric that verifies whether the
reasoning path of model adheres to valid physical logic. Additionally, we
minimize shortcut exploitation from text priors, encouraging models to rely on
visual understanding. Experimental results reveal a concerning trend: even
cutting-edge MLLMs exhibit poor visual reasoning accuracy and weak image-text
alignment in physical domains. Surprisingly, RL-based post-training alignment -
commonly believed to improve visual reasoning performance - often harms spatial
reasoning, suggesting a need to rethink current fine-tuning practices.

</details>


### [29] [Boosting All-in-One Image Restoration via Self-Improved Privilege Learning](https://arxiv.org/abs/2505.24207)
*Gang Wu,Junjun Jiang,Kui Jiang,Xianming Liu*

Main category: cs.CV

TL;DR: SIPL通过自改进特权学习范式，利用伪特权信号在推理阶段实现自我优化，显著提升图像恢复性能。


<details>
  <summary>Details</summary>
Motivation: 解决统一图像恢复模型在多样化和混合退化任务中的优化不稳定性和任务间冲突问题。

Method: 提出SIPL范式，通过Proxy Fusion模块和特权字典在训练和推理阶段利用特权信息进行自我改进。

Result: 在多种图像恢复基准测试中显著提升性能，如PSNR提高+4.58 dB。

Conclusion: SIPL是一种高效且广泛适用的方法，可无缝集成到多种骨干架构中。

Abstract: Unified image restoration models for diverse and mixed degradations often
suffer from unstable optimization dynamics and inter-task conflicts. This paper
introduces Self-Improved Privilege Learning (SIPL), a novel paradigm that
overcomes these limitations by innovatively extending the utility of privileged
information (PI) beyond training into the inference stage. Unlike conventional
Privilege Learning, where ground-truth-derived guidance is typically discarded
after training, SIPL empowers the model to leverage its own preliminary outputs
as pseudo-privileged signals for iterative self-refinement at test time.
Central to SIPL is Proxy Fusion, a lightweight module incorporating a learnable
Privileged Dictionary. During training, this dictionary distills essential
high-frequency and structural priors from privileged feature representations.
Critically, at inference, the same learned dictionary then interacts with
features derived from the model's initial restoration, facilitating a
self-correction loop. SIPL can be seamlessly integrated into various backbone
architectures, offering substantial performance improvements with minimal
computational overhead. Extensive experiments demonstrate that SIPL
significantly advances the state-of-the-art on diverse all-in-one image
restoration benchmarks. For instance, when integrated with the PromptIR model,
SIPL achieves remarkable PSNR improvements of +4.58 dB on composite degradation
tasks and +1.28 dB on diverse five-task benchmarks, underscoring its
effectiveness and broad applicability. Codes are available at our project page
https://github.com/Aitical/SIPL.

</details>


### [30] [STORK: Improving the Fidelity of Mid-NFE Sampling for Diffusion and Flow Matching Models](https://arxiv.org/abs/2505.24210)
*Zheng Tan,Weizhen Wang,Andrea L. Bertozzi,Ernest K. Ryu*

Main category: cs.CV

TL;DR: 提出了一种名为STORK的新型训练自由、结构无关的DM ODE求解器，适用于中NFE范围（20-50），提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在高保真图像和视频生成中表现优异，但中NFE范围（20-50）的研究不足，而实际应用（如Stable Diffusion 3.5）常在此范围内运行。

Method: 基于刚性ODE求解器和泰勒展开适应，提出STORK方法，适用于任何DM采样，包括噪声和流匹配模型。

Result: 在20-50 NFE范围内，STORK在无条件像素级生成和条件潜在空间生成任务中均提升了生成质量（通过FID分数衡量）。

Conclusion: STORK是一种高效且通用的DM ODE求解器，适用于中NFE范围，显著提升生成质量。

Abstract: Diffusion models (DMs) have demonstrated remarkable performance in
high-fidelity image and video generation. Because high-quality generations with
DMs typically require a large number of function evaluations (NFEs), resulting
in slow sampling, there has been extensive research successfully reducing the
NFE to a small range (<10) while maintaining acceptable image quality. However,
many practical applications, such as those involving Stable Diffusion 3.5,
FLUX, and SANA, commonly operate in the mid-NFE regime (20-50 NFE) to achieve
superior results, and, despite the practical relevance, research on the
effective sampling within this mid-NFE regime remains underexplored. In this
work, we propose a novel, training-free, and structure-independent DM ODE
solver called the Stabilized Taylor Orthogonal Runge--Kutta (STORK) method,
based on a class of stiff ODE solvers with a Taylor expansion adaptation.
Unlike prior work such as DPM-Solver, which is dependent on the semi-linear
structure of the DM ODE, STORK is applicable to any DM sampling, including
noise-based and flow matching-based models. Within the 20-50 NFE range, STORK
achieves improved generation quality, as measured by FID scores, across
unconditional pixel-level generation and conditional latent-space generation
tasks using models like Stable Diffusion 3.5 and SANA. Code is available at
https://github.com/ZT220501/STORK.

</details>


### [31] [Benchmarking Foundation Models for Zero-Shot Biometric Tasks](https://arxiv.org/abs/2505.24214)
*Redwan Sony,Parisa Farmanifard,Hamzeh Alzwairy,Nitish Shukla,Arun Ross*

Main category: cs.CV

TL;DR: 论文评估了41种视觉语言模型和多模态大语言模型在六种生物识别任务中的零样本和少样本性能，展示了它们在无需微调的情况下取得的高准确率。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型（如VLMs和MLLMs）在生物识别任务中的潜力，填补这一领域的未充分研究空白。

Method: 通过构建一个综合基准，评估模型在六种生物识别任务中的表现，包括人脸验证、虹膜识别等，并测试其零样本和少样本性能。

Result: 模型在多种任务中表现优异，例如人脸验证的TMR达到96.77%（FMR为1%），虹膜识别的TMR为97.55%（FMR为1%）。

Conclusion: 预训练模型在生物识别任务中表现出色，为实现通用人工智能的长期目标提供了支持。

Abstract: The advent of foundation models, particularly Vision-Language Models (VLMs)
and Multi-modal Large Language Models (MLLMs), has redefined the frontiers of
artificial intelligence, enabling remarkable generalization across diverse
tasks with minimal or no supervision. Yet, their potential in biometric
recognition and analysis remains relatively underexplored. In this work, we
introduce a comprehensive benchmark that evaluates the zero-shot and few-shot
performance of state-of-the-art publicly available VLMs and MLLMs across six
biometric tasks spanning the face and iris modalities: face verification, soft
biometric attribute prediction (gender and race), iris recognition,
presentation attack detection (PAD), and face manipulation detection (morphs
and deepfakes). A total of 41 VLMs were used in this evaluation. Experiments
show that embeddings from these foundation models can be used for diverse
biometric tasks with varying degrees of success. For example, in the case of
face verification, a True Match Rate (TMR) of 96.77 percent was obtained at a
False Match Rate (FMR) of 1 percent on the Labeled Face in the Wild (LFW)
dataset, without any fine-tuning. In the case of iris recognition, the TMR at 1
percent FMR on the IITD-R-Full dataset was 97.55 percent without any
fine-tuning. Further, we show that applying a simple classifier head to these
embeddings can help perform DeepFake detection for faces, Presentation Attack
Detection (PAD) for irides, and extract soft biometric attributes like gender
and ethnicity from faces with reasonably high accuracy. This work reiterates
the potential of pretrained models in achieving the long-term vision of
Artificial General Intelligence.

</details>


### [32] [Shuffle PatchMix Augmentation with Confidence-Margin Weighted Pseudo-Labels for Enhanced Source-Free Domain Adaptation](https://arxiv.org/abs/2505.24216)
*Prasanna Reddy Pulakurthi,Majid Rabbani,Jamison Heard,Sohail Dianat,Celso M. de Melo,Raghuveer Rao*

Main category: cs.CV

TL;DR: 本文提出了一种新的源自由域适应（SFDA）方法，结合了Shuffle PatchMix（SPM）增强技术和伪标签重加权策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决源自由域适应中因缺乏源数据而导致的过拟合和伪标签噪声问题。

Method: 引入SPM增强技术（打乱和混合图像块）和伪标签重加权策略，优先选择可靠的伪标签。

Result: 在PACS、VisDA-C和DomainNet-126三个基准上达到最优性能，PACS上单目标和多目标分别提升7.3%和7.2%。

Conclusion: SPM和伪标签重加权策略为SFDA设定了新的性能基准。

Abstract: This work investigates Source-Free Domain Adaptation (SFDA), where a model
adapts to a target domain without access to source data. A new augmentation
technique, Shuffle PatchMix (SPM), and a novel reweighting strategy are
introduced to enhance performance. SPM shuffles and blends image patches to
generate diverse and challenging augmentations, while the reweighting strategy
prioritizes reliable pseudo-labels to mitigate label noise. These techniques
are particularly effective on smaller datasets like PACS, where overfitting and
pseudo-label noise pose greater risks. State-of-the-art results are achieved on
three major benchmarks: PACS, VisDA-C, and DomainNet-126. Notably, on PACS,
improvements of 7.3% (79.4% to 86.7%) and 7.2% are observed in single-target
and multi-target settings, respectively, while gains of 2.8% and 0.7% are
attained on DomainNet-126 and VisDA-C. This combination of advanced
augmentation and robust pseudo-label reweighting establishes a new benchmark
for SFDA. The code is available at: https://github.com/PrasannaPulakurthi/SPM

</details>


### [33] [Unleashing High-Quality Image Generation in Diffusion Sampling Using Second-Order Levenberg-Marquardt-Langevin](https://arxiv.org/abs/2505.24222)
*Fangyikang Wang,Hubery Yin,Lei Qian,Yinan Li,Shaobin Zhuang,Huminhao Zhu,Yilin Zhang,Yanlong Tang,Chao Zhang,Hanbin Zhao,Hui Qian,Chen Li*

Main category: cs.CV

TL;DR: 提出了一种名为LML的新方法，通过低秩近似和阻尼机制高效利用扩散Hessian几何，显著提升扩散模型生成图像的质量，且计算开销极小。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型采样方法主要依赖一阶Langevin动力学，而利用二阶Hessian几何虽能提升质量，但高维计算成本过高。本文旨在解决这一问题。

Method: 引入Levenberg-Marquardt-Langevin（LML）方法，通过低秩近似扩散Hessian几何和阻尼机制，避免显式二次复杂度计算。

Result: 实验表明，LML方法在多个预训练扩散模型上显著提升图像生成质量，且计算开销可忽略。

Conclusion: LML方法通过高效近似Hessian几何，为扩散模型提供了一种高质量、低成本的采样方案。

Abstract: The diffusion models (DMs) have demonstrated the remarkable capability of
generating images via learning the noised score function of data distribution.
Current DM sampling techniques typically rely on first-order Langevin dynamics
at each noise level, with efforts concentrated on refining inter-level
denoising strategies. While leveraging additional second-order Hessian geometry
to enhance the sampling quality of Langevin is a common practice in Markov
chain Monte Carlo (MCMC), the naive attempts to utilize Hessian geometry in
high-dimensional DMs lead to quadratic-complexity computational costs,
rendering them non-scalable. In this work, we introduce a novel
Levenberg-Marquardt-Langevin (LML) method that approximates the diffusion
Hessian geometry in a training-free manner, drawing inspiration from the
celebrated Levenberg-Marquardt optimization algorithm. Our approach introduces
two key innovations: (1) A low-rank approximation of the diffusion Hessian,
leveraging the DMs' inherent structure and circumventing explicit
quadratic-complexity computations; (2) A damping mechanism to stabilize the
approximated Hessian. This LML approximated Hessian geometry enables the
diffusion sampling to execute more accurate steps and improve the image
generation quality. We further conduct a theoretical analysis to substantiate
the approximation error bound of low-rank approximation and the convergence
property of the damping mechanism. Extensive experiments across multiple
pretrained DMs validate that the LML method significantly improves image
generation quality, with negligible computational overhead.

</details>


### [34] [Reasoning Can Hurt the Inductive Abilities of Large Language Models](https://arxiv.org/abs/2505.24225)
*Haibo Jin,Peiyan Zhang,Man Luo,Haohan Wang*

Main category: cs.CV

TL;DR: 研究发现，链式思维（CoT）提示可能降低大型语言模型（LLM）的归纳推理能力，而非增强。通过理论和实验分析，提出了结构化干预方法以改进推理准确性。


<details>
  <summary>Details</summary>
Motivation: 探究链式思维（CoT）提示是否真正提升大型语言模型的归纳推理能力，并揭示其潜在问题。

Method: 设计了四种基于游戏的诊断任务（如国际象棋、德州扑克等），对比了使用和不使用CoT的模型表现，并提出了理论框架分析失败模式。

Result: 发现CoT可能通过错误的子任务分解、解决或答案总结放大错误，导致推理性能下降。

Conclusion: 有效的CoT推理不仅需要更多步骤，还需确保步骤结构合理；提出的结构化干预方法无需重新训练即可提升准确性。

Abstract: Large Language Models (LLMs) have shown remarkable progress across domains,
yet their ability to perform inductive reasoning - inferring latent rules from
sparse examples - remains limited. It is often assumed that chain-of-thought
(CoT) prompting, as used in Large Reasoning Models (LRMs), enhances such
reasoning. We investigate this assumption with creating four controlled,
diagnostic game-based tasks - chess, Texas Hold'em, dice games, and blackjack -
with hidden human-defined rules. We find that CoT reasoning can degrade
inductive performance, with LRMs often underperforming their non-reasoning
counterparts.
  To explain this, we present a theoretical framework that reveals how
reasoning steps can amplify error through three failure modes: incorrect
sub-task decomposition, incorrect sub-task solving, and incorrect final answer
summarization. Based on our theoretical and empirical analysis, we introduce
structured interventions that adapt CoT generation according to our identified
failure types. These interventions improve inductive accuracy without
retraining. Our findings suggest that effective (CoT) reasoning depends not
only on taking more steps but also on ensuring those steps are well-structured.

</details>


### [35] [Light as Deception: GPT-driven Natural Relighting Against Vision-Language Pre-training Models](https://arxiv.org/abs/2505.24227)
*Ying Yang,Jie Zhang,Xiao Lv,Di Lin,Tao Xiang,Qing Guo*

Main category: cs.CV

TL;DR: LightD是一个通过语义引导的重新光照生成自然对抗样本的框架，适用于视觉与语言预训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法在适应视觉与语言预训练模型时存在优化空间受限、攻击效果差或生成不自然的问题。

Method: LightD利用ChatGPT提出上下文感知的初始光照参数，结合预训练的重新光照模型（IC-light）进行多样化光照调整，并通过梯度优化增强攻击效果。

Result: LightD在图像描述和视觉问答等任务中表现出色，优于现有方法。

Conclusion: LightD通过语义引导的重新光照，有效生成了自然的对抗样本，提升了攻击效果和视觉自然性。

Abstract: While adversarial attacks on vision-and-language pretraining (VLP) models
have been explored, generating natural adversarial samples crafted through
realistic and semantically meaningful perturbations remains an open challenge.
Existing methods, primarily designed for classification tasks, struggle when
adapted to VLP models due to their restricted optimization spaces, leading to
ineffective attacks or unnatural artifacts. To address this, we propose
\textbf{LightD}, a novel framework that generates natural adversarial samples
for VLP models via semantically guided relighting. Specifically, LightD
leverages ChatGPT to propose context-aware initial lighting parameters and
integrates a pretrained relighting model (IC-light) to enable diverse lighting
adjustments. LightD expands the optimization space while ensuring perturbations
align with scene semantics. Additionally, gradient-based optimization is
applied to the reference lighting image to further enhance attack effectiveness
while maintaining visual naturalness. The effectiveness and superiority of the
proposed LightD have been demonstrated across various VLP models in tasks such
as image captioning and visual question answering.

</details>


### [36] [From Hallucinations to Jailbreaks: Rethinking the Vulnerability of Large Foundation Models](https://arxiv.org/abs/2505.24232)
*Haibo Jin,Peiyan Zhang,Peiran Wang,Man Luo,Haohan Wang*

Main category: cs.CV

TL;DR: 论文提出了一个统一的理论框架，将越狱攻击和幻觉问题分别建模为令牌级和注意力级优化，并验证了二者的损失函数和梯度行为的一致性。


<details>
  <summary>Details</summary>
Motivation: 大型基础模型（LFMs）存在幻觉和越狱攻击两种漏洞，通常孤立研究，但防御措施对二者的影响表明它们可能存在深层联系。

Method: 提出统一框架，将越狱攻击建模为令牌级优化，幻觉为注意力级优化，验证损失函数收敛和梯度一致性。

Result: 在LLaVA-1.5和MiniGPT-4上验证了优化趋势和梯度一致性，并发现缓解幻觉的技术可降低越狱成功率，反之亦然。

Conclusion: 揭示了LFMs的共享失败模式，建议鲁棒性策略需同时解决两种漏洞。

Abstract: Large foundation models (LFMs) are susceptible to two distinct
vulnerabilities: hallucinations and jailbreak attacks. While typically studied
in isolation, we observe that defenses targeting one often affect the other,
hinting at a deeper connection.
  We propose a unified theoretical framework that models jailbreaks as
token-level optimization and hallucinations as attention-level optimization.
Within this framework, we establish two key propositions: (1) \textit{Similar
Loss Convergence} - the loss functions for both vulnerabilities converge
similarly when optimizing for target-specific outputs; and (2) \textit{Gradient
Consistency in Attention Redistribution} - both exhibit consistent gradient
behavior driven by shared attention dynamics.
  We validate these propositions empirically on LLaVA-1.5 and MiniGPT-4,
showing consistent optimization trends and aligned gradients. Leveraging this
connection, we demonstrate that mitigation techniques for hallucinations can
reduce jailbreak success rates, and vice versa. Our findings reveal a shared
failure mode in LFMs and suggest that robustness strategies should jointly
address both vulnerabilities.

</details>


### [37] [MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM](https://arxiv.org/abs/2505.24238)
*Bowen Dong,Minheng Ni,Zitong Huang,Guanglei Yang,Wangmeng Zuo,Lei Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种新的基准{\dataset}，用于区分多模态大语言模型（MLLMs）中的感知诱导幻觉和推理诱导幻觉，并提出了{\method}方法以减少推理幻觉。


<details>
  <summary>Details</summary>
Motivation: 多模态幻觉限制了MLLMs的正确性，但现有基准无法区分感知和推理诱导的幻觉，阻碍了对MLLMs推理失败的诊断。

Method: 提出了{\dataset}基准，通过构建问题隔离推理幻觉，并引入多粒度评估指标。同时提出了{\method}方法，结合课程强化微调和协作提示推理以减少推理复杂度。

Result: 研究发现模型规模、数据规模和训练阶段显著影响幻觉程度；当前MLLMs在空间幻觉上表现不佳；问题类型与幻觉模式相关。{\method}有效减少了逻辑幻觉。

Conclusion: {\dataset}和{\method}为诊断和缓解MLLMs的推理幻觉提供了有效工具，揭示了改进方向。

Abstract: Multimodal hallucination in multimodal large language models (MLLMs)
restricts the correctness of MLLMs. However, multimodal hallucinations are
multi-sourced and arise from diverse causes. Existing benchmarks fail to
adequately distinguish between perception-induced hallucinations and
reasoning-induced hallucinations. This failure constitutes a significant issue
and hinders the diagnosis of multimodal reasoning failures within MLLMs. To
address this, we propose the {\dataset} benchmark, which isolates reasoning
hallucinations by constructing questions where input images are correctly
perceived by MLLMs yet reasoning errors persist. {\dataset} introduces
multi-granular evaluation metrics: accuracy, factuality, and LLMs hallucination
score for hallucination quantification. Our analysis reveals that (1) the model
scale, data scale, and training stages significantly affect the degree of
logical, fabrication, and factual hallucinations; (2) current MLLMs show no
effective improvement on spatial hallucinations caused by misinterpreted
spatial relationships, indicating their limited visual reasoning capabilities;
and (3) question types correlate with distinct hallucination patterns,
highlighting targeted challenges and potential mitigation strategies. To
address these challenges, we propose {\method}, a method that combines
curriculum reinforcement fine-tuning to encourage models to generate
logic-consistent reasoning chains by stepwise reducing learning difficulty, and
collaborative hint inference to reduce reasoning complexity. {\method}
establishes a baseline on {\dataset}, and reduces the logical hallucinations in
original base models.

</details>


### [38] [LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework](https://arxiv.org/abs/2505.24245)
*Xin Kang,Zihan Zheng,Lei Chu,Yue Gao,Jiahao Li,Hao Pan,Xuejin Chen,Yan Lu*

Main category: cs.CV

TL;DR: LTM3D是一个结合扩散模型和自回归模型的3D形状生成框架，通过条件分布建模和前缀学习提升生成效果，支持多种3D表示形式。


<details>
  <summary>Details</summary>
Motivation: 结合扩散模型和自回归模型的优势，解决3D形状生成中的挑战，提升生成质量和灵活性。

Method: 采用条件分布建模主干、前缀学习和潜在令牌重建模块，结合重建引导采样减少不确定性。

Result: 在图像和文本条件生成任务中表现优于现有方法，生成形状的提示保真度和结构准确性更高。

Conclusion: LTM3D为多模态、多表示3D生成提供了一个通用且高效的框架。

Abstract: We present LTM3D, a Latent Token space Modeling framework for conditional 3D
shape generation that integrates the strengths of diffusion and auto-regressive
(AR) models. While diffusion-based methods effectively model continuous latent
spaces and AR models excel at capturing inter-token dependencies, combining
these paradigms for 3D shape generation remains a challenge. To address this,
LTM3D features a Conditional Distribution Modeling backbone, leveraging a
masked autoencoder and a diffusion model to enhance token dependency learning.
Additionally, we introduce Prefix Learning, which aligns condition tokens with
shape latent tokens during generation, improving flexibility across modalities.
We further propose a Latent Token Reconstruction module with
Reconstruction-Guided Sampling to reduce uncertainty and enhance structural
fidelity in generated shapes. Our approach operates in token space, enabling
support for multiple 3D representations, including signed distance fields,
point clouds, meshes, and 3D Gaussian Splatting. Extensive experiments on
image- and text-conditioned shape generation tasks demonstrate that LTM3D
outperforms existing methods in prompt fidelity and structural accuracy while
offering a generalizable framework for multi-modal, multi-representation 3D
generation.

</details>


### [39] [50 Years of Automated Face Recognition](https://arxiv.org/abs/2505.24247)
*Minchul Kim,Anil Jain,Xiaoming Liu*

Main category: cs.CV

TL;DR: 本文回顾了50年来人脸识别技术的发展，从早期几何统计方法到现代深度学习模型，分析了关键创新点、数据集影响及当前挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 总结人脸识别技术的演进历程，探讨其关键技术进步及未来研究方向。

Method: 通过历史回顾和技术分析，梳理了从几何统计方法到深度学习模型的演进，重点研究了数据集、损失函数、神经网络设计和特征融合的创新。

Result: 现代人脸识别系统在NIST FRVT评估中达到0.13%的误识率，展示了高性能，但仍存在挑战。

Conclusion: 尽管技术取得显著进展，仍需解决可扩展性、多模态融合、合成身份生成和可解释系统等开放问题。

Abstract: Over the past 50 years, automated face recognition has evolved from
rudimentary, handcrafted systems into sophisticated deep learning models that
rival and often surpass human performance. This paper chronicles the history
and technological progression of FR, from early geometric and statistical
methods to modern deep neural architectures leveraging massive real and
AI-generated datasets. We examine key innovations that have shaped the field,
including developments in dataset, loss function, neural network design and
feature fusion. We also analyze how the scale and diversity of training data
influence model generalization, drawing connections between dataset growth and
benchmark improvements. Recent advances have achieved remarkable milestones:
state-of-the-art face verification systems now report False Negative
Identification Rates of 0.13% against a 12.4 million gallery in NIST FRVT
evaluations for 1:N visa-to-border matching. While recent advances have enabled
remarkable accuracy in high- and low-quality face scenarios, numerous
challenges persist. While remarkable progress has been achieved, several open
research problems remain. We outline critical challenges and promising
directions for future face recognition research, including scalability,
multi-modal fusion, synthetic identity generation, and explainable systems.

</details>


### [40] [Harnessing Foundation Models for Robust and Generalizable 6-DOF Bronchoscopy Localization](https://arxiv.org/abs/2505.24249)
*Qingyao Tian,Huai Liao,Xinyan Huang,Bingyu Yang,Hongbin Liu*

Main category: cs.CV

TL;DR: PANSv2是一个通用且鲁棒的支气管镜定位框架，通过整合深度估计、标志点检测和中心线约束，解决了现有方法在泛化和视觉退化下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在泛化性和视觉退化下表现不佳，PANSv2旨在解决这些问题。

Method: PANSv2结合深度估计、标志点检测和中心线约束，利用EndoOmni和EndoMamba模型增强泛化能力，并引入自动重新初始化模块提高鲁棒性。

Result: 在10个患者案例的数据集上，PANSv2的跟踪成功率最高，SR-5指标提升了18.1%。

Conclusion: PANSv2展示了在临床实际应用中的潜力。

Abstract: Vision-based 6-DOF bronchoscopy localization offers a promising solution for
accurate and cost-effective interventional guidance. However, existing methods
struggle with 1) limited generalization across patient cases due to scarce
labeled data, and 2) poor robustness under visual degradation, as bronchoscopy
procedures frequently involve artifacts such as occlusions and motion blur that
impair visual information. To address these challenges, we propose PANSv2, a
generalizable and robust bronchoscopy localization framework. Motivated by PANS
that leverages multiple visual cues for pose likelihood measurement, PANSv2
integrates depth estimation, landmark detection, and centerline constraints
into a unified pose optimization framework that evaluates pose probability and
solves for the optimal bronchoscope pose. To further enhance generalization
capabilities, we leverage the endoscopic foundation model EndoOmni for depth
estimation and the video foundation model EndoMamba for landmark detection,
incorporating both spatial and temporal analyses. Pretrained on diverse
endoscopic datasets, these models provide stable and transferable visual
representations, enabling reliable performance across varied bronchoscopy
scenarios. Additionally, to improve robustness to visual degradation, we
introduce an automatic re-initialization module that detects tracking failures
and re-establishes pose using landmark detections once clear views are
available. Experimental results on bronchoscopy dataset encompassing 10 patient
cases show that PANSv2 achieves the highest tracking success rate, with an
18.1% improvement in SR-5 (percentage of absolute trajectory error under 5 mm)
compared to existing methods, showing potential towards real clinical usage.

</details>


### [41] [Interactive Video Generation via Domain Adaptation](https://arxiv.org/abs/2505.24253)
*Ishaan Rawal,Suryansh Kumar*

Main category: cs.CV

TL;DR: 本文提出了一种改进交互式视频生成（IVG）的方法，通过掩码归一化和时间内在扩散先验解决现有技术中的感知质量下降和轨迹控制问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本条件的扩散模型在视频生成中表现优异，但在交互式控制（如物体轨迹）时存在感知质量下降的问题。

Method: 提出掩码归一化以解决注意力掩码导致的内部协变量偏移，并引入时间内在扩散先验以对齐初始噪声与IVG条件。

Result: 实验表明，该方法在感知质量和轨迹控制上优于现有技术。

Conclusion: 掩码归一化和时间内在扩散先验有效提升了交互式视频生成的性能。

Abstract: Text-conditioned diffusion models have emerged as powerful tools for
high-quality video generation. However, enabling Interactive Video Generation
(IVG), where users control motion elements such as object trajectory, remains
challenging. Recent training-free approaches introduce attention masking to
guide trajectory, but this often degrades perceptual quality. We identify two
key failure modes in these methods, both of which we interpret as domain shift
problems, and propose solutions inspired by domain adaptation. First, we
attribute the perceptual degradation to internal covariate shift induced by
attention masking, as pretrained models are not trained to handle masked
attention. To address this, we propose mask normalization, a pre-normalization
layer designed to mitigate this shift via distribution matching. Second, we
address initialization gap, where the randomly sampled initial noise does not
align with IVG conditioning, by introducing a temporal intrinsic diffusion
prior that enforces spatio-temporal consistency at each denoising step.
Extensive qualitative and quantitative evaluations demonstrate that mask
normalization and temporal intrinsic denoising improve both perceptual quality
and trajectory control over the existing state-of-the-art IVG techniques.

</details>


### [42] [Out of Sight, Not Out of Context? Egocentric Spatial Reasoning in VLMs Across Disjoint Frames](https://arxiv.org/abs/2505.24257)
*Sahithya Ravi,Gabriel Sarch,Vibhav Vineet,Andrew D. Wilson,Balasaravanan Thoravi Kumaravel*

Main category: cs.CV

TL;DR: Disjoint-3DQA是一个评估视觉语言模型（VLMs）在非共视帧中对物体空间关系推理能力的基准测试，结果显示当前模型与人类表现差距较大，尤其在时间跨度增加时性能下降明显。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决基于自我中心视频的AI助手在跨时间空间推理上的能力不足问题，尤其是物体对不在同一帧时的空间关系理解。

Method: 通过Disjoint-3DQA基准测试，评估七种先进VLMs在非共视物体对问题上的表现，并分析不同输入（如轨迹、鸟瞰图、3D坐标）对性能的影响。

Result: 模型表现较人类低28%，时间跨度增加时性能下降更显著（60%到30%）。提供3D坐标可显著提升性能（20%），而轨迹或鸟瞰图改善有限。

Conclusion: Disjoint-3DQA揭示了VLMs在多帧视觉信号中构建和维护3D场景表示的核心瓶颈，为未来视觉、语言与具身AI结合的研究提供了明确挑战。

Abstract: An embodied AI assistant operating on egocentric video must integrate spatial
cues across time - for instance, determining where an object A, glimpsed a few
moments ago lies relative to an object B encountered later. We introduce
Disjoint-3DQA , a generative QA benchmark that evaluates this ability of VLMs
by posing questions about object pairs that are not co-visible in the same
frame. We evaluated seven state-of-the-art VLMs and found that models lag
behind human performance by 28%, with steeper declines in accuracy (60% to 30
%) as the temporal gap widens. Our analysis further reveals that providing
trajectories or bird's-eye-view projections to VLMs results in only marginal
improvements, whereas providing oracle 3D coordinates leads to a substantial
20% performance increase. This highlights a core bottleneck of multi-frame VLMs
in constructing and maintaining 3D scene representations over time from visual
signals. Disjoint-3DQA therefore sets a clear, measurable challenge for
long-horizon spatial reasoning and aims to catalyze future research at the
intersection of vision, language, and embodied AI.

</details>


### [43] [LLM-powered Query Expansion for Enhancing Boundary Prediction in Language-driven Action Localization](https://arxiv.org/abs/2505.24282)
*Zirui Shang,Xinxiao Wu,Shuo Yang*

Main category: cs.CV

TL;DR: 论文提出通过LLMs扩展语言查询以提供更详细的边界线索，并通过建模边界概率分数来减少边界不确定性对训练的影响。


<details>
  <summary>Details</summary>
Motivation: 语言查询通常缺乏动作边界的具体细节，导致边界标注的主观性和训练数据的不确定性。

Method: 1. 使用LLMs生成动作边界的文本描述以扩展查询；2. 建模边界概率分数，结合语义相似性和时间距离。

Result: 在多个数据集上验证了方法的有效性。

Conclusion: 该方法可无缝集成到现有模型中，提升语言驱动动作定位的稳定性和准确性。

Abstract: Language-driven action localization in videos requires not only semantic
alignment between language query and video segment, but also prediction of
action boundaries.
  However, the language query primarily describes the main content of an action
and usually lacks specific details of action start and end boundaries, which
increases the subjectivity of manual boundary annotation and leads to boundary
uncertainty in training data.
  In this paper, on one hand, we propose to expand the original query by
generating textual descriptions of the action start and end boundaries through
LLMs, which can provide more detailed boundary cues for localization and thus
reduce the impact of boundary uncertainty.
  On the other hand, to enhance the tolerance to boundary uncertainty during
training, we propose to model probability scores of action boundaries by
calculating the semantic similarities between frames and the expanded query as
well as the temporal distances between frames and the annotated boundary
frames. They can provide more consistent boundary supervision, thus improving
the stability of training.
  Our method is model-agnostic and can be seamlessly and easily integrated into
any existing models of language-driven action localization in an off-the-shelf
manner. Experimental results on several datasets demonstrate the effectiveness
of our method.

</details>


### [44] [EgoExOR: An Ego-Exo-Centric Operating Room Dataset for Surgical Activity Understanding](https://arxiv.org/abs/2505.24287)
*Ege Özsoy,Arda Mamur,Felix Tristram,Chantal Pellegrini,Magdalena Wysocki,Benjamin Busam,Nassir Navab*

Main category: cs.CV

TL;DR: EgoExOR是首个结合第一人称和第三人称视角的手术室数据集，提供多模态数据支持临床感知任务。


<details>
  <summary>Details</summary>
Motivation: 手术室需要精确协调，现有数据集视角单一，无法全面支持临床感知需求。

Method: 引入EgoExOR数据集，整合穿戴设备和RGB-D相机的多模态数据，并标注详细场景图。

Result: 评估了两种先进模型的性能，并提供了新的基线模型。

Conclusion: EgoExOR为手术室感知提供了新的多模态资源基础。

Abstract: Operating rooms (ORs) demand precise coordination among surgeons, nurses, and
equipment in a fast-paced, occlusion-heavy environment, necessitating advanced
perception models to enhance safety and efficiency. Existing datasets either
provide partial egocentric views or sparse exocentric multi-view context, but
do not explore the comprehensive combination of both. We introduce EgoExOR, the
first OR dataset and accompanying benchmark to fuse first-person and
third-person perspectives. Spanning 94 minutes (84,553 frames at 15 FPS) of two
emulated spine procedures, Ultrasound-Guided Needle Insertion and Minimally
Invasive Spine Surgery, EgoExOR integrates egocentric data (RGB, gaze, hand
tracking, audio) from wearable glasses, exocentric RGB and depth from RGB-D
cameras, and ultrasound imagery. Its detailed scene graph annotations, covering
36 entities and 22 relations (568,235 triplets), enable robust modeling of
clinical interactions, supporting tasks like action recognition and
human-centric perception. We evaluate the surgical scene graph generation
performance of two adapted state-of-the-art models and offer a new baseline
that explicitly leverages EgoExOR's multimodal and multi-perspective signals.
This new dataset and benchmark set a new foundation for OR perception, offering
a rich, multimodal resource for next-generation clinical perception.

</details>


### [45] [Category-aware EEG image generation based on wavelet transform and contrast semantic loss](https://arxiv.org/abs/2505.24301)
*Enshang Zhang,Zhicheng Zhang,Takashi Hanakawa*

Main category: cs.CV

TL;DR: 本文提出了一种基于Transformer的EEG信号编码器，结合DWT和门控机制，通过预训练的扩散模型将EEG信号特征重建为视觉刺激。实验表明，该模型在语义对齐和分类准确性上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 实现脑机接口的关键步骤是从EEG信号中重建视觉刺激，现有方法在语义层面的定量分析存在局限性。

Method: 提出了一种结合DWT和门控机制的Transformer编码器，利用特征对齐和类别感知融合损失提取EEG信号特征，并通过预训练的扩散模型重建视觉刺激。

Result: 在THINGS-EEG数据集上，模型在EEG到图像生成和分类任务中表现优异，单被试最高分类准确率达43%。

Conclusion: 该模型显著提升了语义对齐和分类准确性，为脑机接口的视觉刺激重建提供了有效解决方案。

Abstract: Reconstructing visual stimuli from EEG signals is a crucial step in realizing
brain-computer interfaces. In this paper, we propose a transformer-based EEG
signal encoder integrating the Discrete Wavelet Transform (DWT) and the gating
mechanism. Guided by the feature alignment and category-aware fusion losses,
this encoder is used to extract features related to visual stimuli from EEG
signals. Subsequently, with the aid of a pre-trained diffusion model, these
features are reconstructed into visual stimuli. To verify the effectiveness of
the model, we conducted EEG-to-image generation and classification tasks using
the THINGS-EEG dataset. To address the limitations of quantitative analysis at
the semantic level, we combined WordNet-based classification and semantic
similarity metrics to propose a novel semantic-based score, emphasizing the
ability of our model to transfer neural activities into visual representations.
Experimental results show that our model significantly improves semantic
alignment and classification accuracy, which achieves a maximum single-subject
accuracy of 43\%, outperforming other state-of-the-art methods. The source code
and supplementary material is available at
https://github.com/zes0v0inn/DWT_EEG_Reconstruction/tree/main.

</details>


### [46] [Progressive Class-level Distillation](https://arxiv.org/abs/2505.24310)
*Jiayan Li,Jun Li,Zhourui Zhang,Jianhua Xu*

Main category: cs.CV

TL;DR: 提出了一种名为渐进式类别级蒸馏（PCD）的新方法，通过分阶段蒸馏解决传统方法中低概率类别知识传递不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法中，高置信度类别主导蒸馏过程，导致低概率类别的判别信息被忽视。

Method: PCD方法通过排名教师-学生logits差异确定蒸馏优先级，分阶段进行双向蒸馏，结合细到粗和粗到细的学习。

Result: 在公开基准数据集上的实验表明，PCD在分类和检测任务上优于现有方法。

Conclusion: PCD通过分阶段蒸馏实现了更全面的知识传递，提升了学生模型的性能。

Abstract: In knowledge distillation (KD), logit distillation (LD) aims to transfer
class-level knowledge from a more powerful teacher network to a small student
model via accurate teacher-student alignment at the logits level. Since
high-confidence object classes usually dominate the distillation process,
low-probability classes which also contain discriminating information are
downplayed in conventional methods, leading to insufficient knowledge transfer.
To address this issue, we propose a simple yet effective LD method termed
Progressive Class-level Distillation (PCD). In contrast to existing methods
which perform all-class ensemble distillation, our PCD approach performs
stage-wise distillation for step-by-step knowledge transfer. More specifically,
we perform ranking on teacher-student logits difference for identifying
distillation priority from scratch, and subsequently divide the entire LD
process into multiple stages. Next, bidirectional stage-wise distillation
incorporating fine-to-coarse progressive learning and reverse coarse-to-fine
refinement is conducted, allowing comprehensive knowledge transfer via
sufficient logits alignment within separate class groups in different
distillation stages. Extension experiments on public benchmarking datasets
demonstrate the superiority of our method compared to state-of-the-arts for
both classification and detection tasks.

</details>


### [47] [InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing](https://arxiv.org/abs/2505.24315)
*Jinlu Zhang,Yixin Chen,Zan Wang,Jie Yang,Yizhou Wang,Siyuan Huang*

Main category: cs.CV

TL;DR: 提出了一种零样本3D人-物交互生成框架，利用预训练模型解决开放集对象的挑战，无需特定数据集训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从文本生成新颖人-物交互（HOI）时面临精确关系推理、开放集对象适配性和详细姿态合成的挑战。

Method: 结合大型语言模型（LLMs）推理人-物关系，利用预训练2D扩散模型解析对象，并通过多视角SDS生成初始姿态，最后优化实现精细交互。

Result: 实验表明，该方法在交互细节和开放集对象处理上优于现有工作。

Conclusion: 该框架为零样本3D HOI生成提供了有效解决方案，尤其在开放集对象和精细交互方面表现突出。

Abstract: Recent advances in 3D human-aware generation have made significant progress.
However, existing methods still struggle with generating novel Human Object
Interaction (HOI) from text, particularly for open-set objects. We identify
three main challenges of this task: precise human-object relation reasoning,
affordance parsing for any object, and detailed human interaction pose
synthesis aligning description and object geometry. In this work, we propose a
novel zero-shot 3D HOI generation framework without training on specific
datasets, leveraging the knowledge from large-scale pre-trained models.
Specifically, the human-object relations are inferred from large language
models (LLMs) to initialize object properties and guide the optimization
process. Then we utilize a pre-trained 2D image diffusion model to parse unseen
objects and extract contact points, avoiding the limitations imposed by
existing 3D asset knowledge. The initial human pose is generated by sampling
multiple hypotheses through multi-view SDS based on the input text and object
geometry. Finally, we introduce a detailed optimization to generate
fine-grained, precise, and natural interaction, enforcing realistic 3D contact
between the 3D object and the involved body parts, including hands in grasping.
This is achieved by distilling human-level feedback from LLMs to capture
detailed human-object relations from the text instruction. Extensive
experiments validate the effectiveness of our approach compared to prior works,
particularly in terms of the fine-grained nature of interactions and the
ability to handle open-set 3D objects.

</details>


### [48] [STAR-Net: An Interpretable Model-Aided Network for Remote Sensing Image Denoising](https://arxiv.org/abs/2505.24327)
*Jingjing Liu,Jiashun Jin,Xianchao Xiu,Jianhua Zhang,Wanquan Liu*

Main category: cs.CV

TL;DR: 提出了一种名为STAR-Net的新型遥感图像去噪方法，结合低秩先验和非局部自相似性，并通过ADMM引导的深度展开网络自动学习参数，解决了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法缺乏物理模型集成和可解释性，且对非局部自相似性关注不足，参数调优繁琐。

Method: 提出STAR-Net及其稀疏变体STAR-Net-S，利用低秩先验捕获非局部自相似性，并通过ADMM引导的深度展开网络自动学习参数。

Result: 在合成和真实数据集上，STAR-Net和STAR-Net-S优于现有最先进的去噪方法。

Conclusion: STAR-Net成功结合了模型驱动和数据驱动方法的优势，解决了传统方法的不足，表现出色。

Abstract: Remote sensing image (RSI) denoising is an important topic in the field of
remote sensing. Despite the impressive denoising performance of RSI denoising
methods, most current deep learning-based approaches function as black boxes
and lack integration with physical information models, leading to limited
interpretability. Additionally, many methods may struggle with insufficient
attention to non-local self-similarity in RSI and require tedious tuning of
regularization parameters to achieve optimal performance, particularly in
conventional iterative optimization approaches. In this paper, we first propose
a novel RSI denoising method named sparse tensor-aided representation network
(STAR-Net), which leverages a low-rank prior to effectively capture the
non-local self-similarity within RSI. Furthermore, we extend STAR-Net to a
sparse variant called STAR-Net-S to deal with the interference caused by
non-Gaussian noise in original RSI for the purpose of improving robustness.
Different from conventional iterative optimization, we develop an alternating
direction method of multipliers (ADMM)-guided deep unrolling network, in which
all regularization parameters can be automatically learned, thus inheriting the
advantages of both model-based and deep learning-based approaches and
successfully addressing the above-mentioned shortcomings. Comprehensive
experiments on synthetic and real-world datasets demonstrate that STAR-Net and
STAR-Net-S outperform state-of-the-art RSI denoising methods.

</details>


### [49] [DisTime: Distribution-based Time Representation for Video Large Language Models](https://arxiv.org/abs/2505.24329)
*Yingsen Zeng,Zepeng Huang,Yujie Zhong,Chengjian Feng,Jie Hu,Lin Ma,Yang Liu*

Main category: cs.CV

TL;DR: DisTime框架通过连续时间嵌入和分布解码器提升Video-LLMs的时间定位能力，并构建了大规模数据集InternVid-TG，实验表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有Video-LLMs在时间定位上存在离散表示和数据集不足的问题，需要更精确的时间理解方法。

Method: DisTime采用可学习token构建连续时间嵌入空间，结合分布解码器和编码器，并通过自动化标注生成数据集InternVid-TG。

Result: DisTime在时间敏感任务中表现最佳，同时保持视频问答任务的竞争力。

Conclusion: DisTime有效解决了时间定位问题，并通过新数据集推动了领域发展。

Abstract: Despite advances in general video understanding, Video Large Language Models
(Video-LLMs) face challenges in precise temporal localization due to discrete
time representations and limited temporally aware datasets. Existing methods
for temporal expression either conflate time with text-based numerical values,
add a series of dedicated temporal tokens, or regress time using specialized
temporal grounding heads. To address these issues, we introduce DisTime, a
lightweight framework designed to enhance temporal comprehension in Video-LLMs.
DisTime employs a learnable token to create a continuous temporal embedding
space and incorporates a Distribution-based Time Decoder that generates
temporal probability distributions, effectively mitigating boundary ambiguities
and maintaining temporal continuity. Additionally, the Distribution-based Time
Encoder re-encodes timestamps to provide time markers for Video-LLMs. To
overcome temporal granularity limitations in existing datasets, we propose an
automated annotation paradigm that combines the captioning capabilities of
Video-LLMs with the localization expertise of dedicated temporal models. This
leads to the creation of InternVid-TG, a substantial dataset with 1.25M
temporally grounded events across 179k videos, surpassing ActivityNet-Caption
by 55 times. Extensive experiments demonstrate that DisTime achieves
state-of-the-art performance across benchmarks in three time-sensitive tasks
while maintaining competitive performance in Video QA tasks. Code and data are
released at https://github.com/josephzpng/DisTime.

</details>


### [50] [KairosAD: A SAM-Based Model for Industrial Anomaly Detection on Embedded Devices](https://arxiv.org/abs/2505.24334)
*Uzair Khan,Franco Fummi,Luigi Capogrosso*

Main category: cs.CV

TL;DR: KairosAD是一种基于MobileSAM的新型监督学习方法，用于图像异常检测，适用于资源受限的嵌入式设备，参数减少78%，推理速度快4倍，性能与主流模型相当。


<details>
  <summary>Details</summary>
Motivation: 解决现有异常检测模型在资源受限的嵌入式设备上部署不切实际的问题。

Method: 利用Mobile Segment Anything Model (MobileSAM)进行图像异常检测。

Result: 在MVTec-AD和ViSA数据集上表现优异，参数减少78%，推理速度快4倍，AUROC性能相当。

Conclusion: KairosAD成功部署于嵌入式设备，并在实际生产线上测试，代码开源。

Abstract: In the era of intelligent manufacturing, anomaly detection has become
essential for maintaining quality control on modern production lines. However,
while many existing models show promising performance, they are often too
large, computationally demanding, and impractical to deploy on
resource-constrained embedded devices that can be easily installed on the
production lines of Small and Medium Enterprises (SMEs). To bridge this gap, we
present KairosAD, a novel supervised approach that uses the power of the Mobile
Segment Anything Model (MobileSAM) for image-based anomaly detection. KairosAD
has been evaluated on the two well-known industrial anomaly detection datasets,
i.e., MVTec-AD and ViSA. The results show that KairosAD requires 78% fewer
parameters and boasts a 4x faster inference time compared to the leading
state-of-the-art model, while maintaining comparable AUROC performance. We
deployed KairosAD on two embedded devices, the NVIDIA Jetson NX, and the NVIDIA
Jetson AGX. Finally, KairosAD was successfully installed and tested on the real
production line of the Industrial Computer Engineering Laboratory (ICE Lab) at
the University of Verona. The code is available at
https://github.com/intelligolabs/KairosAD.

</details>


### [51] [GeoVision Labeler: Zero-Shot Geospatial Classification with Vision and Language Models](https://arxiv.org/abs/2505.24340)
*Gilles Quentin Hacheme,Girmaw Abebe Tadesse,Caleb Robinson,Akram Zaytar,Rahul Dodhia,Juan M. Lavista Ferres*

Main category: cs.CV

TL;DR: GeoVision Labeler (GVL) 是一个严格零样本分类框架，通过视觉大语言模型生成图像描述，再通过常规大语言模型映射到用户定义类别，实现了灵活且可解释的分类。


<details>
  <summary>Details</summary>
Motivation: 解决地理空间图像分类中标注数据稀缺的问题，特别是灾害响应和土地利用监测等应用场景。

Method: GVL 结合视觉大语言模型（vLLM）生成图像描述，再通过常规大语言模型（LLM）映射到用户定义类别，支持多级分类任务。

Result: 在 SpaceNet v7 上零样本准确率达 93.2%，在多类任务中通过递归聚类和分层分类实现竞争性表现。

Conclusion: GVL 是一个模块化、可解释的零样本分类框架，适用于多种地理空间应用，已开源以促进实际应用。

Abstract: Classifying geospatial imagery remains a major bottleneck for applications
such as disaster response and land-use monitoring-particularly in regions where
annotated data is scarce or unavailable. Existing tools (e.g., RS-CLIP) that
claim zero-shot classification capabilities for satellite imagery nonetheless
rely on task-specific pretraining and adaptation to reach competitive
performance. We introduce GeoVision Labeler (GVL), a strictly zero-shot
classification framework: a vision Large Language Model (vLLM) generates rich,
human-readable image descriptions, which are then mapped to user-defined
classes by a conventional Large Language Model (LLM). This modular, and
interpretable pipeline enables flexible image classification for a large range
of use cases. We evaluated GVL across three benchmarks-SpaceNet v7, UC Merced,
and RESISC45. It achieves up to 93.2% zero-shot accuracy on the binary
Buildings vs. No Buildings task on SpaceNet v7. For complex multi-class
classification tasks (UC Merced, RESISC45), we implemented a recursive
LLM-driven clustering to form meta-classes at successive depths, followed by
hierarchical classification-first resolving coarse groups, then finer
distinctions-to deliver competitive zero-shot performance. GVL is open-sourced
at https://github.com/microsoft/geo-vision-labeler to catalyze adoption in
real-world geospatial workflows.

</details>


### [52] [KEVER^2: Knowledge-Enhanced Visual Emotion Reasoning and Retrieval](https://arxiv.org/abs/2505.24342)
*Fanhang Man,Xiaoyue Chen,Huandong Wang,Baining Zhao,Han Li,Xinlei Chen,Yong Li*

Main category: cs.CV

TL;DR: K-EVER²是一个知识增强的视觉情感推理与检索框架，通过整合外部情感知识和多模态对齐，无需手工标签即可实现鲁棒的情感预测。


<details>
  <summary>Details</summary>
Motivation: 图像中的情感线索抽象、重叠且复杂，现有视觉语言模型在情感语义对齐和情感推理一致性方面存在不足。

Method: 提出K-EVER²框架，结构化视觉情感线索并整合外部情感知识，通过多模态对齐实现无监督情感预测。

Result: 在三个基准测试中，K-EVER²显著优于基线模型，最高提升19%的特定情感准确率和12.3%的平均准确率。

Conclusion: K-EVER²为视觉内容的情感理解提供了可扩展且通用的解决方案。

Abstract: Understanding what emotions images evoke in their viewers is a foundational
goal in human-centric visual computing. While recent advances in
vision-language models (VLMs) have shown promise for visual emotion analysis
(VEA), several key challenges remain unresolved. Emotional cues in images are
often abstract, overlapping, and entangled, making them difficult to model and
interpret. Moreover, VLMs struggle to align these complex visual patterns with
emotional semantics due to limited supervision and sparse emotional grounding.
Finally, existing approaches lack structured affective knowledge to resolve
ambiguity and ensure consistent emotional reasoning across diverse visual
domains.
  To address these limitations, we propose \textbf{K-EVER\textsuperscript{2}},
a knowledge-enhanced framework for emotion reasoning and retrieval. Our
approach introduces a semantically structured formulation of visual emotion
cues and integrates external affective knowledge through multimodal alignment.
Without relying on handcrafted labels or direct emotion supervision,
K-EVER\textsuperscript{2} achieves robust and interpretable emotion predictions
across heterogeneous image types.
  We validate our framework on three representative benchmarks, Emotion6,
EmoSet, and M-Disaster, covering social media imagery, human-centric scenes,
and disaster contexts. K-EVER\textsuperscript{2} consistently outperforms
strong CNN and VLM baselines, achieving up to a \textbf{19\% accuracy gain} for
specific emotions and a \textbf{12.3\% average accuracy gain} across all
emotion categories. Our results demonstrate a scalable and generalizable
solution for advancing emotional understanding of visual content.

</details>


### [53] [VUDG: A Dataset for Video Understanding Domain Generalization](https://arxiv.org/abs/2505.24346)
*Ziyi Wang,Zhi Gao,Boxuan Yu,Zirui Dai,Yuxiang Song,Qingyuan Lu,Jin Chen,Xinxiao Wu*

Main category: cs.CV

TL;DR: 该论文提出了一个专门用于评估视频理解领域泛化（DG）性能的新数据集VUDG，包含11个不同领域的视频，覆盖三种域偏移类型，并通过多专家渐进标注框架进行标注。实验表明，现有模型在域偏移下性能下降，凸显了VUDG的挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解研究通常忽略真实世界中的域偏移问题，导致领域泛化（DG）在视频理解中未被充分探索。

Method: 提出VUDG数据集，包含11个领域的视频，覆盖三种域偏移类型，并通过多专家渐进标注框架标注视频。

Result: 实验显示，大多数模型（包括先进的LVLMs）在域偏移下性能下降，揭示了当前模型对数据分布偏移的鲁棒性差异。

Conclusion: VUDG为未来领域泛化视频理解研究提供了宝贵资源。

Abstract: Video understanding has made remarkable progress in recent years, largely
driven by advances in deep models and the availability of large-scale annotated
datasets. However, existing works typically ignore the inherent domain shifts
encountered in real-world video applications, leaving domain generalization
(DG) in video understanding underexplored. Hence, we propose Video
Understanding Domain Generalization (VUDG), a novel dataset designed
specifically for evaluating the DG performance in video understanding. VUDG
contains videos from 11 distinct domains that cover three types of domain
shifts, and maintains semantic similarity across different domains to ensure
fair and meaningful evaluation. We propose a multi-expert progressive
annotation framework to annotate each video with both multiple-choice and
open-ended question-answer pairs. Extensive experiments on 9 representative
large video-language models (LVLMs) and several traditional video question
answering methods show that most models (including state-of-the-art LVLMs)
suffer performance degradation under domain shifts. These results highlight the
challenges posed by VUDG and the difference in the robustness of current models
to data distribution shifts. We believe VUDG provides a valuable resource for
prompting future research in domain generalization video understanding.

</details>


### [54] [Revisiting Cross-Modal Knowledge Distillation: A Disentanglement Approach for RGBD Semantic Segmentation](https://arxiv.org/abs/2505.24361)
*Roger Ferrod,Cássio F. Dantas,Luigi Di Caro,Dino Ienco*

Main category: cs.CV

TL;DR: CroDiNo-KD提出了一种新的跨模态知识蒸馏框架，通过解耦表示、对比学习和数据增强，解决了传统RGBD语义分割中模态不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 解决RGBD数据在训练和推理阶段模态不匹配的问题，并改进传统跨模态知识蒸馏框架的局限性。

Method: 利用解耦表示、对比学习和解耦数据增强，同时学习单模态RGB和深度模型。

Result: 在三个RGBD数据集上验证了CroDiNo-KD的优越性，优于现有跨模态知识蒸馏方法。

Conclusion: CroDiNo-KD为多模态到单模态的知识蒸馏提供了新思路，挑战了传统的师生范式。

Abstract: Multi-modal RGB and Depth (RGBD) data are predominant in many domains such as
robotics, autonomous driving and remote sensing. The combination of these
multi-modal data enhances environmental perception by providing 3D spatial
context, which is absent in standard RGB images. Although RGBD multi-modal data
can be available to train computer vision models, accessing all sensor
modalities during the inference stage may be infeasible due to sensor failures
or resource constraints, leading to a mismatch between data modalities
available during training and inference. Traditional Cross-Modal Knowledge
Distillation (CMKD) frameworks, developed to address this task, are typically
based on a teacher/student paradigm, where a multi-modal teacher distills
knowledge into a single-modality student model. However, these approaches face
challenges in teacher architecture choices and distillation process selection,
thus limiting their adoption in real-world scenarios. To overcome these issues,
we introduce CroDiNo-KD (Cross-Modal Disentanglement: a New Outlook on
Knowledge Distillation), a novel cross-modal knowledge distillation framework
for RGBD semantic segmentation. Our approach simultaneously learns
single-modality RGB and Depth models by exploiting disentanglement
representation, contrastive learning and decoupled data augmentation with the
aim to structure the internal manifolds of neural network models through
interaction and collaboration. We evaluated CroDiNo-KD on three RGBD datasets
across diverse domains, considering recent CMKD frameworks as competitors. Our
findings illustrate the quality of CroDiNo-KD, and they suggest reconsidering
the conventional teacher/student paradigm to distill information from
multi-modal data to single-modality neural networks.

</details>


### [55] [Grid-LOGAT: Grid Based Local and Global Area Transcription for Video Question Answering](https://arxiv.org/abs/2505.24371)
*Md Intisar Chowdhury,Kittinun Aukkapinyo,Hiroshi Fujimura,Joo Ann Woo,Wasu Wasusatein,Fadoua Ghourabi*

Main category: cs.CV

TL;DR: 提出了一种基于网格的局部和全局区域转录系统（Grid-LoGAT），用于视频问答（VideoQA），通过视觉语言模型（VLM）提取视频帧文本，再通过大语言模型（LLM）生成答案，提升了转录质量和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 解决视频问答中文本转录质量不足和隐私保护问题。

Method: 分两阶段：1. 使用VLM提取视频帧文本；2. 使用LLM处理问题生成答案。采用网格化视觉提示提升转录质量。

Result: 在NExT-QA和STAR-QA数据集上分别达到65.9%和50.11%的准确率，优于同类方法。网格版本在定位问题上比非网格版本高出24分。

Conclusion: Grid-LoGAT在视频问答中表现出色，尤其在转录质量和隐私保护方面具有优势。

Abstract: In this paper, we propose a Grid-based Local and Global Area Transcription
(Grid-LoGAT) system for Video Question Answering (VideoQA). The system operates
in two phases. First, extracting text transcripts from video frames using a
Vision-Language Model (VLM). Next, processing questions using these transcripts
to generate answers through a Large Language Model (LLM). This design ensures
image privacy by deploying the VLM on edge devices and the LLM in the cloud. To
improve transcript quality, we propose grid-based visual prompting, which
extracts intricate local details from each grid cell and integrates them with
global information. Evaluation results show that Grid-LoGAT, using the
open-source VLM (LLaVA-1.6-7B) and LLM (Llama-3.1-8B), outperforms
state-of-the-art methods with similar baseline models on NExT-QA and STAR-QA
datasets with an accuracy of 65.9% and 50.11% respectively. Additionally, our
method surpasses the non-grid version by 24 points on localization-based
questions we created using NExT-QA.

</details>


### [56] [D2AF: A Dual-Driven Annotation and Filtering Framework for Visual Grounding](https://arxiv.org/abs/2505.24372)
*Yichi Zhang,Gongwei Chen,Jun Zhu,Jia Wan*

Main category: cs.CV

TL;DR: D2AF是一个仅使用输入图像的视觉定位标注框架，通过多模态大模型和对象检测模型生成区域-文本对，提升数据量和多样性，并通过过滤方法优化数据质量，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉定位任务中手动标注成本高、数据集规模受限的问题，同时提升数据的多样性和数量。

Method: 提出D2AF框架，结合多模态大模型和对象检测模型，采用双驱动标注策略生成区域-文本对，并设计一致性及分布感知过滤方法优化数据质量。

Result: 实验表明，增加数据量提升模型性能，但改进程度取决于伪标签对原始数据分布的扩展效果。D2AF在三个视觉定位任务中取得最优性能。

Conclusion: D2AF通过自动生成高质量标注数据，显著提升视觉定位任务的性能，为大规模数据需求提供了可行解决方案。

Abstract: Visual Grounding is a task that aims to localize a target region in an image
based on a free-form natural language description. With the rise of Transformer
architectures, there is an increasing need for larger datasets to boost
performance. However, the high cost of manual annotation poses a challenge,
hindering the scale of data and the ability of large models to enhance their
effectiveness. Previous pseudo label generation methods heavily rely on
human-labeled captions of the original dataset, limiting scalability and
diversity. To address this, we propose D2AF, a robust annotation framework for
visual grounding using only input images. This approach overcomes dataset size
limitations and enriches both the quantity and diversity of referring
expressions. Our approach leverages multimodal large models and object
detection models. By implementing dual-driven annotation strategies, we
effectively generate detailed region-text pairs using both closed-set and
open-set approaches. We further conduct an in-depth analysis of data quantity
and data distribution. Our findings demonstrate that increasing data volume
enhances model performance. However, the degree of improvement depends on how
well the pseudo labels broaden the original data distribution. Based on these
insights, we propose a consistency and distribution aware filtering method to
further improve data quality by effectively removing erroneous and redundant
data. This approach effectively eliminates noisy data, leading to improved
performance. Experiments on three visual grounding tasks demonstrate that our
method significantly improves the performance of existing models and achieves
state-of-the-art results.

</details>


### [57] [Spatiotemporal Analysis of Forest Machine Operations Using 3D Video Classification](https://arxiv.org/abs/2505.24375)
*Maciej Wielgosz,Simon Berg,Heikki Korpunen,Stephan Hoffmann*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的框架，用于从行车记录仪视频中分类林业操作，使用3D ResNet-50架构，验证F1分数为0.88，精度为0.90。


<details>
  <summary>Details</summary>
Motivation: 减少传统时间研究中的手动工作量，为林业操作监控和效率分析提供可扩展的解决方案。

Method: 采用3D ResNet-50架构，结合PyTorchVideo实现，使用手动标注的数据集进行训练，并应用标准预处理和数据增强技术。

Result: 模型表现良好，验证F1分数为0.88，精度为0.90，但存在过拟合问题。

Conclusion: 该方法在林业环境中具有潜力，未来计划包括数据集扩展、增强正则化和嵌入式系统部署。

Abstract: This paper presents a deep learning-based framework for classifying forestry
operations from dashcam video footage. Focusing on four key work elements -
crane-out, cutting-and-to-processing, driving, and processing - the approach
employs a 3D ResNet-50 architecture implemented with PyTorchVideo. Trained on a
manually annotated dataset of field recordings, the model achieves strong
performance, with a validation F1 score of 0.88 and precision of 0.90. These
results underscore the effectiveness of spatiotemporal convolutional networks
for capturing both motion patterns and appearance in real-world forestry
environments.
  The system integrates standard preprocessing and augmentation techniques to
improve generalization, but overfitting is evident, highlighting the need for
more training data and better class balance. Despite these challenges, the
method demonstrates clear potential for reducing the manual workload associated
with traditional time studies, offering a scalable solution for operational
monitoring and efficiency analysis in forestry.
  This work contributes to the growing application of AI in natural resource
management and sets the foundation for future systems capable of real-time
activity recognition in forest machinery. Planned improvements include dataset
expansion, enhanced regularization, and deployment trials on embedded systems
for in-field use.

</details>


### [58] [SASP: Strip-Aware Spatial Perception for Fine-Grained Bird Image Classification](https://arxiv.org/abs/2505.24380)
*Zheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于条带感知空间感知的细粒度鸟类分类框架，通过捕捉图像中的长距离空间依赖关系，提升模型的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 细粒度鸟类图像分类（FBIC）在生态监测和物种识别中具有重要意义，但由于鸟类尺寸、背景干扰和姿态变化等因素，传统方法难以稳定提取判别性特征。

Method: 提出了一种结合扩展感知聚合器（EPA）和通道语义编织（CSW）的框架，通过水平和垂直空间方向的信息聚合以及通道维度的自适应融合，增强特征提取能力。

Result: 在CUB-200-2011数据集上的实验表明，该框架在保持架构效率的同时显著提升了性能。

Conclusion: 该方法通过长距离空间依赖关系的建模，有效解决了细粒度鸟类分类中的挑战，具有较高的实用价值。

Abstract: Fine-grained bird image classification (FBIC) is not only of great
significance for ecological monitoring and species identification, but also
holds broad research value in the fields of image recognition and fine-grained
visual modeling. Compared with general image classification tasks, FBIC poses
more formidable challenges: 1) the differences in species size and imaging
distance result in the varying sizes of birds presented in the images; 2)
complex natural habitats often introduce strong background interference; 3) and
highly flexible poses such as flying, perching, or foraging result in
substantial intra-class variability. These factors collectively make it
difficult for traditional methods to stably extract discriminative features,
thereby limiting the generalizability and interpretability of models in
real-world applications. To address these challenges, this paper proposes a
fine-grained bird classification framework based on strip-aware spatial
perception, which aims to capture long-range spatial dependencies across entire
rows or columns in bird images, thereby enhancing the model's robustness and
interpretability. The proposed method incorporates two novel modules:
extensional perception aggregator (EPA) and channel semantic weaving (CSW).
Specifically, EPA integrates local texture details with global structural cues
by aggregating information across horizontal and vertical spatial directions.
CSW further refines the semantic representations by adaptively fusing
long-range and short-range information along the channel dimension. Built upon
a ResNet-50 backbone, the model enables jump-wise connection of extended
structural features across the spatial domain. Experimental results on the
CUB-200-2011 dataset demonstrate that our framework achieves significant
performance improvements while maintaining architectural efficiency.

</details>


### [59] [Leadership Assessment in Pediatric Intensive Care Unit Team Training](https://arxiv.org/abs/2505.24389)
*Liangyang Ouyang,Yuki Sakai,Ryosuke Furuta,Hisataka Nozawa,Hikoro Matsui,Yoichi Sato*

Main category: cs.CV

TL;DR: 本文提出了一种基于自我中心视觉的自动化分析框架，用于评估PICU团队的领导技能，通过行为线索（如注视对象、眼神接触和对话模式）实现。


<details>
  <summary>Details</summary>
Motivation: 评估PICU团队的领导技能对培训至关重要，但传统方法依赖人工观察，效率低且主观性强。

Method: 使用Aria Glasses记录视频、音频、注视和头部运动数据，结合REMoDNaV、SAM、YOLO和ChatGPT进行自动化分析。

Result: 实验显示领导技能与行为指标（如注视时间、转换模式和直接指令）显著相关。

Conclusion: 该框架能有效解决PICU团队技能评估问题。

Abstract: This paper addresses the task of assessing PICU team's leadership skills by
developing an automated analysis framework based on egocentric vision. We
identify key behavioral cues, including fixation object, eye contact, and
conversation patterns, as essential indicators of leadership assessment. In
order to capture these multimodal signals, we employ Aria Glasses to record
egocentric video, audio, gaze, and head movement data. We collect one-hour
videos of four simulated sessions involving doctors with different roles and
levels. To automate data processing, we propose a method leveraging REMoDNaV,
SAM, YOLO, and ChatGPT for fixation object detection, eye contact detection,
and conversation classification. In the experiments, significant correlations
are observed between leadership skills and behavioral metrics, i.e., the output
of our proposed methods, such as fixation time, transition patterns, and direct
orders in speech. These results indicate that our proposed data collection and
analysis framework can effectively solve skill assessment for training PICU
teams.

</details>


### [60] [S3CE-Net: Spike-guided Spatiotemporal Semantic Coupling and Expansion Network for Long Sequence Event Re-Identification](https://arxiv.org/abs/2505.24401)
*Xianheng Ma,Hongchen Tan,Xiuping Liu,Yi Zhang,Huasheng Wang,Jiang Liu,Ying Chen,Hantao Liu*

Main category: cs.CV

TL;DR: 提出了一种基于脉冲神经网络的低参数高效模型S3CE-Net，用于长序列事件行人重识别任务，通过时空注意力机制和特征采样策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 利用事件相机优势（如抗强光、高时间分辨率）解决长序列事件行人重识别问题。

Method: 基于脉冲神经网络构建S3CE-Net，引入时空注意力机制（SSAM）和时空特征采样策略（STFS）。

Result: 在多个主流数据集上表现优异。

Conclusion: S3CE-Net是一种高效且参数少的模型，适用于长序列事件行人重识别任务。

Abstract: In this paper, we leverage the advantages of event cameras to resist harsh
lighting conditions, reduce background interference, achieve high time
resolution, and protect facial information to study the long-sequence
event-based person re-identification (Re-ID) task. To this end, we propose a
simple and efficient long-sequence event Re-ID model, namely the Spike-guided
Spatiotemporal Semantic Coupling and Expansion Network (S3CE-Net). To better
handle asynchronous event data, we build S3CE-Net based on spiking neural
networks (SNNs). The S3CE-Net incorporates the Spike-guided Spatial-temporal
Attention Mechanism (SSAM) and the Spatiotemporal Feature Sampling Strategy
(STFS). The SSAM is designed to carry out semantic interaction and association
in both spatial and temporal dimensions, leveraging the capabilities of SNNs.
The STFS involves sampling spatial feature subsequences and temporal feature
subsequences from the spatiotemporal dimensions, driving the Re-ID model to
perceive broader and more robust effective semantics. Notably, the STFS
introduces no additional parameters and is only utilized during the training
stage. Therefore, S3CE-Net is a low-parameter and high-efficiency model for
long-sequence event-based person Re-ID. Extensive experiments have verified
that our S3CE-Net achieves outstanding performance on many mainstream
long-sequence event-based person Re-ID datasets. Code is available
at:https://github.com/Mhsunshine/SC3E_Net.

</details>


### [61] [Leveraging Intermediate Features of Vision Transformer for Face Anti-Spoofing](https://arxiv.org/abs/2505.24402)
*Mika Feng,Koichi Ito,Takafumi Aoki,Tetsushi Ohki,Masakatsu Nishigaki*

Main category: cs.CV

TL;DR: 提出了一种基于Vision Transformer（ViT）的欺骗攻击检测方法，利用ViT的中间特征平衡局部和全局特征，并通过两种数据增强方法提高检测精度。


<details>
  <summary>Details</summary>
Motivation: 恶意用户可能通过照片欺骗人脸识别系统，因此需要检测欺骗攻击以确保系统安全。

Method: 使用ViT的中间特征计算训练损失和推理得分，并引入两种数据增强方法：防欺骗数据增强和分块数据增强。

Result: 在OULU-NPU和SiW数据集上的实验证明了该方法的有效性。

Conclusion: 提出的方法能有效检测欺骗攻击，提高人脸识别系统的安全性。

Abstract: Face recognition systems are designed to be robust against changes in head
pose, illumination, and blurring during image capture. If a malicious person
presents a face photo of the registered user, they may bypass the
authentication process illegally. Such spoofing attacks need to be detected
before face recognition. In this paper, we propose a spoofing attack detection
method based on Vision Transformer (ViT) to detect minute differences between
live and spoofed face images. The proposed method utilizes the intermediate
features of ViT, which have a good balance between local and global features
that are important for spoofing attack detection, for calculating loss in
training and score in inference. The proposed method also introduces two data
augmentation methods: face anti-spoofing data augmentation and patch-wise data
augmentation, to improve the accuracy of spoofing attack detection. We
demonstrate the effectiveness of the proposed method through experiments using
the OULU-NPU and SiW datasets.

</details>


### [62] [PCIE_Interaction Solution for Ego4D Social Interaction Challenge](https://arxiv.org/abs/2505.24404)
*Kanokphan Lertniphonphan,Feng Chen,Junda Xu,Fengbu Lan,Jun Xie,Tao Zhang,Zhepeng Wang*

Main category: cs.CV

TL;DR: 团队提出了PCIE_Interaction解决方案，用于CVPR 2025的Ego4D社交互动挑战，针对LAM和TTM任务，分别采用面部质量增强与多模态融合方法，取得了0.81和0.71的mAP成绩。


<details>
  <summary>Details</summary>
Motivation: 解决Ego4D社交互动挑战中LAM和TTM任务的准确检测问题，提升社交互动的识别能力。

Method: LAM任务使用面部质量增强和集成方法；TTM任务通过视觉质量评分加权融合音频和视觉线索。

Result: 在LAM和TTM任务中分别取得0.81和0.71的mAP成绩。

Conclusion: PCIE_Interaction方法在社交互动检测中表现优异，代码已开源。

Abstract: This report presents our team's PCIE_Interaction solution for the Ego4D
Social Interaction Challenge at CVPR 2025, addressing both Looking At Me (LAM)
and Talking To Me (TTM) tasks. The challenge requires accurate detection of
social interactions between subjects and the camera wearer, with LAM relying
exclusively on face crop sequences and TTM combining speaker face crops with
synchronized audio segments. In the LAM track, we employ face quality
enhancement and ensemble methods. For the TTM task, we extend visual
interaction analysis by fusing audio and visual cues, weighted by a visual
quality score. Our approach achieved 0.81 and 0.71 mean average precision (mAP)
on the LAM and TTM challenges leader board. Code is available at
https://github.com/KanokphanL/PCIE_Ego4D_Social_Interaction

</details>


### [63] [IRBridge: Solving Image Restoration Bridge with Pre-trained Generative Diffusion Models](https://arxiv.org/abs/2505.24406)
*Hanting Wang,Tao Jin,Wang Lin,Shulei Wang,Hai Huang,Shengpeng Ji,Zhou Zhao*

Main category: cs.CV

TL;DR: 提出IRBridge框架，利用预训练生成模型改进图像修复任务，避免为每种退化类型单独训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有图像修复方法需为每种退化类型单独训练模型，计算成本高且性能有限。

Method: 提出过渡方程，桥接两个扩散过程，并引入IRBridge框架直接利用生成模型。

Result: 在六种图像修复任务中验证了IRBridge的鲁棒性和泛化性能提升。

Conclusion: IRBridge高效整合生成先验，为图像修复提供更灵活的方法。

Abstract: Bridge models in image restoration construct a diffusion process from
degraded to clear images. However, existing methods typically require training
a bridge model from scratch for each specific type of degradation, resulting in
high computational costs and limited performance. This work aims to efficiently
leverage pretrained generative priors within existing image restoration bridges
to eliminate this requirement. The main challenge is that standard generative
models are typically designed for a diffusion process that starts from pure
noise, while restoration tasks begin with a low-quality image, resulting in a
mismatch in the state distributions between the two processes. To address this
challenge, we propose a transition equation that bridges two diffusion
processes with the same endpoint distribution. Based on this, we introduce the
IRBridge framework, which enables the direct utilization of generative models
within image restoration bridges, offering a more flexible and adaptable
approach to image restoration. Extensive experiments on six image restoration
tasks demonstrate that IRBridge efficiently integrates generative priors,
resulting in improved robustness and generalization performance. Code will be
available at GitHub.

</details>


### [64] [PCIE_Pose Solution for EgoExo4D Pose and Proficiency Estimation Challenge](https://arxiv.org/abs/2505.24411)
*Feng Chen,Kanokphan Lertniphonphan,Qiancheng Yan,Xiaohui Fan,Jun Xie,Tao Zhang,Zhepeng Wang*

Main category: cs.CV

TL;DR: 团队提出HP-ViT+和时空特征融合方法，在CVPR2025挑战赛中取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 解决RGB视频中手部和身体姿态估计的复杂问题，如细微动作和遮挡。

Method: 结合Vision Transformer和CNN的HP-ViT+架构，以及多模态时空特征融合策略。

Result: 手部姿态挑战赛PA-MPJPE 8.31，身体姿态挑战赛MPJPE 11.25，熟练度估计任务准确率0.53。

Conclusion: 方法在多个任务中表现优异，验证了技术的有效性。

Abstract: This report introduces our team's (PCIE_EgoPose) solutions for the EgoExo4D
Pose and Proficiency Estimation Challenges at CVPR2025. Focused on the
intricate task of estimating 21 3D hand joints from RGB egocentric videos,
which are complicated by subtle movements and frequent occlusions, we developed
the Hand Pose Vision Transformer (HP-ViT+). This architecture synergizes a
Vision Transformer and a CNN backbone, using weighted fusion to refine the hand
pose predictions. For the EgoExo4D Body Pose Challenge, we adopted a multimodal
spatio-temporal feature integration strategy to address the complexities of
body pose estimation across dynamic contexts. Our methods achieved remarkable
performance: 8.31 PA-MPJPE in the Hand Pose Challenge and 11.25 MPJPE in the
Body Pose Challenge, securing championship titles in both competitions. We
extended our pose estimation solutions to the Proficiency Estimation task,
applying core technologies such as transformer-based architectures. This
extension enabled us to achieve a top-1 accuracy of 0.53, a SOTA result, in the
Demonstrator Proficiency Estimation competition.

</details>


### [65] [EasyText: Controllable Diffusion Transformer for Multilingual Text Rendering](https://arxiv.org/abs/2505.24417)
*Runnan Lu,Yuxuan Zhang,Jiaming Liu,Haofan Wang,Yiren Song*

Main category: cs.CV

TL;DR: EasyText是一个基于DiT的多语言文本渲染框架，通过字符定位编码和位置编码插值技术实现可控且精确的文本生成。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在单语言文本生成上取得进展，但多语言文本生成仍具挑战性。

Method: 采用DiT框架，结合字符定位编码和位置编码插值技术，并使用大规模合成数据集进行预训练和微调。

Result: 实验证明EasyText在多语言文本渲染、视觉质量和布局感知文本集成方面表现优越。

Conclusion: EasyText为多语言文本生成提供了高效且先进的解决方案。

Abstract: Generating accurate multilingual text with diffusion models has long been
desired but remains challenging. Recent methods have made progress in rendering
text in a single language, but rendering arbitrary languages is still an
unexplored area. This paper introduces EasyText, a text rendering framework
based on DiT (Diffusion Transformer), which connects denoising latents with
multilingual character tokens encoded as character tokens. We propose character
positioning encoding and position encoding interpolation techniques to achieve
controllable and precise text rendering. Additionally, we construct a
large-scale synthetic text image dataset with 1 million multilingual image-text
annotations as well as a high-quality dataset of 20K annotated images, which
are used for pretraining and fine-tuning respectively. Extensive experiments
and evaluations demonstrate the effectiveness and advancement of our approach
in multilingual text rendering, visual quality, and layout-aware text
integration.

</details>


### [66] [Bridging 3D Anomaly Localization and Repair via High-Quality Continuous Geometric Representation](https://arxiv.org/abs/2505.24431)
*Bozhong Zheng,Jinye Gan,Xiaohao Xu,Wenqiao Li,Xiaonan Huang,Na Ni,Yingna Wu*

Main category: cs.CV

TL;DR: PASDF是一种基于连续符号距离场（SDF）的3D点云异常检测与修复框架，通过姿态对齐模块和SDF网络实现高精度异常定位与修复。


<details>
  <summary>Details</summary>
Motivation: 现有基于离散体素化或投影的方法在几何保真度上存在问题，限制了细粒度异常定位能力。

Method: PASDF结合姿态对齐模块和SDF网络，学习连续、姿态不变的形状表示，并通过异常感知评分模块实现像素级定位。

Result: 在Real3D-AD和Anomaly-ShapeNet上分别达到80.2%和90.0%的AUROC分数。

Conclusion: 连续几何表示在3D异常检测和修复中具有显著优势，PASDF框架为实际应用提供了有效解决方案。

Abstract: 3D point cloud anomaly detection is essential for robust vision systems but
is challenged by pose variations and complex geometric anomalies. Existing
patch-based methods often suffer from geometric fidelity issues due to discrete
voxelization or projection-based representations, limiting fine-grained anomaly
localization. We introduce Pose-Aware Signed Distance Field (PASDF), a novel
framework that integrates 3D anomaly detection and repair by learning a
continuous, pose-invariant shape representation. PASDF leverages a Pose
Alignment Module for canonicalization and a SDF Network to dynamically
incorporate pose, enabling implicit learning of high-fidelity anomaly repair
templates from the continuous SDF. This facilitates precise pixel-level anomaly
localization through an Anomaly-Aware Scoring Module. Crucially, the continuous
3D representation in PASDF extends beyond detection, facilitating in-situ
anomaly repair. Experiments on Real3D-AD and Anomaly-ShapeNet demonstrate
state-of-the-art performance, achieving high object-level AUROC scores of 80.2%
and 90.0%, respectively. These results highlight the effectiveness of
continuous geometric representations in advancing 3D anomaly detection and
facilitating practical anomaly region repair. The code is available at
https://github.com/ZZZBBBZZZ/PASDF to support further research.

</details>


### [67] [SORCE: Small Object Retrieval in Complex Environments](https://arxiv.org/abs/2505.24441)
*Chunxu Liu,Chi Xie,Xiaxu Chen,Wei Li,Feng Zhu,Rui Zhao,Limin Wang*

Main category: cs.CV

TL;DR: 论文提出了一种新的子领域SORCE（复杂环境中的小物体检索），并构建了基准数据集SORCE-1K。通过多嵌入表示和区域提示（ReP）方法，显著提升了小物体检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像检索（T2IR）方法主要关注整体图像语义或显著前景物体，忽视了复杂环境中不显眼的小物体。这些小物体在实际应用中至关重要。

Method: 提出SORCE-1K基准数据集，并利用多模态大语言模型（MLLM）和区域提示（ReP）生成多嵌入表示，以捕捉小物体特征。

Result: 实验表明，多嵌入方法显著优于现有T2IR方法，验证了SORCE-1K的有效性。

Conclusion: 多嵌入表示和文本定制的MLLM特征为解决小物体检索任务提供了潜力。

Abstract: Text-to-Image Retrieval (T2IR) is a highly valuable task that aims to match a
given textual query to images in a gallery. Existing benchmarks primarily focus
on textual queries describing overall image semantics or foreground salient
objects, possibly overlooking inconspicuous small objects, especially in
complex environments. Such small object retrieval is crucial, as in real-world
applications, the targets of interest are not always prominent in the image.
Thus, we introduce SORCE (Small Object Retrieval in Complex Environments), a
new subfield of T2IR, focusing on retrieving small objects in complex images
with textual queries. We propose a new benchmark, SORCE-1K, consisting of
images with complex environments and textual queries describing less
conspicuous small objects with minimal contextual cues from other salient
objects. Preliminary analysis on SORCE-1K finds that existing T2IR methods
struggle to capture small objects and encode all the semantics into a single
embedding, leading to poor retrieval performance on SORCE-1K. Therefore, we
propose to represent each image with multiple distinctive embeddings. We
leverage Multimodal Large Language Models (MLLMs) to extract multiple
embeddings for each image instructed by a set of Regional Prompts (ReP).
Experimental results show that our multi-embedding approach through MLLM and
ReP significantly outperforms existing T2IR methods on SORCE-1K. Our
experiments validate the effectiveness of SORCE-1K for benchmarking SORCE
performances, highlighting the potential of multi-embedding representation and
text-customized MLLM features for addressing this task.

</details>


### [68] [Diversify and Conquer: Open-set Disagreement for Robust Semi-supervised Learning with Outliers](https://arxiv.org/abs/2505.24443)
*Heejo Kong,Sung-Jin Kim,Gunho Jung,Seong-Whan Lee*

Main category: cs.CV

TL;DR: 论文提出了一种名为DAC的新框架，通过利用多个模型的预测差异来增强半监督学习在开放集场景中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统半监督学习假设标记和未标记数据共享相同的类别分布，但实际中未标记数据常包含未知类别的异常值，导致模型性能下降。

Method: DAC通过训练多个具有不同偏见的模型，利用它们在未标记数据上的预测差异来检测异常值。

Result: 该方法在标记数据不足时仍能有效识别异常值，性能优于现有方法。

Conclusion: DAC通过多模型预测差异提高了开放集半监督学习的鲁棒性。

Abstract: Conventional semi-supervised learning (SSL) ideally assumes that labeled and
unlabeled data share an identical class distribution, however in practice, this
assumption is easily violated, as unlabeled data often includes unknown class
data, i.e., outliers. The outliers are treated as noise, considerably degrading
the performance of SSL models. To address this drawback, we propose a novel
framework, Diversify and Conquer (DAC), to enhance SSL robustness in the
context of open-set semi-supervised learning. In particular, we note that
existing open-set SSL methods rely on prediction discrepancies between inliers
and outliers from a single model trained on labeled data. This approach can be
easily failed when the labeled data is insufficient, leading to performance
degradation that is worse than naive SSL that do not account for outliers. In
contrast, our approach exploits prediction disagreements among multiple models
that are differently biased towards the unlabeled distribution. By leveraging
the discrepancies arising from training on unlabeled data, our method enables
robust outlier detection even when the labeled data is underspecified. Our key
contribution is constructing a collection of differently biased models through
a single training process. By encouraging divergent heads to be differently
biased towards outliers while making consistent predictions for inliers, we
exploit the disagreement among these heads as a measure to identify unknown
concepts. Our code is available at https://github.com/heejokong/DivCon.

</details>


### [69] [SA-Person: Text-Based Person Retrieval with Scene-aware Re-ranking](https://arxiv.org/abs/2505.24466)
*Yingjia Xu,Jinlin Wu,Zhen Chen,Daming Gao,Yang Yang,Zhen Lei,Min Cao*

Main category: cs.CV

TL;DR: 论文提出了一种基于文本的人物检索方法SA-Person，通过两阶段框架结合外观和场景信息，提升了复杂场景下的检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注外观跨模态检索，忽略了场景中的上下文信息，而这些信息可以提供有价值的补充。

Method: 提出两阶段检索框架：第一阶段通过外观对齐文本与行人区域；第二阶段引入SceneRanker，利用多模态大语言模型联合推理外观和全局场景上下文。

Result: 在SCENEPERSON-13W数据集上的实验验证了框架在复杂场景检索中的有效性。

Conclusion: SA-Person框架通过结合外观和场景信息，显著提升了文本检索的准确性，代码和数据集将公开。

Abstract: Text-based person retrieval aims to identify a target individual from a
gallery of images based on a natural language description. It presents a
significant challenge due to the complexity of real-world scenes and the
ambiguity of appearance-related descriptions. Existing methods primarily
emphasize appearance-based cross-modal retrieval, often neglecting the
contextual information embedded within the scene, which can offer valuable
complementary insights for retrieval. To address this, we introduce
SCENEPERSON-13W, a large-scale dataset featuring over 100,000 scenes with rich
annotations covering both pedestrian appearance and environmental cues. Based
on this, we propose SA-Person, a two-stage retrieval framework. In the first
stage, it performs discriminative appearance grounding by aligning textual cues
with pedestrian-specific regions. In the second stage, it introduces
SceneRanker, a training-free, scene-aware re-ranking method leveraging
multimodal large language models to jointly reason over pedestrian appearance
and the global scene context. Experiments on SCENEPERSON-13W validate the
effectiveness of our framework in challenging scene-level retrieval scenarios.
The code and dataset will be made publicly available.

</details>


### [70] [SPPSFormer: High-quality Superpoint-based Transformer for Roof Plane Instance Segmentation from Point Clouds](https://arxiv.org/abs/2505.24475)
*Cheng Zeng,Xiatian Qi,Chi Chen,Kai Sun,Wangle Zhang,Yuxuan Liu,Yan Meng,Bisheng Yang*

Main category: cs.CV

TL;DR: 本文提出了一种用于点云屋顶平面实例分割的两阶段超点生成方法，结合多维手工特征和改进的解码器，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有超点Transformer在点云屋顶分割中性能有限，主要由于低质量超点。本文旨在解决这一问题。

Method: 提出两阶段超点生成过程，结合多维手工特征，并设计结合Kolmogorov-Arnold网络与Transformer模块的解码器。

Result: 方法在多个数据集上达到最优性能，且对标注边界不敏感，减少了标注负担。

Conclusion: 点云密度、均匀性和精度对分割性能影响显著，需在数据增强中考虑这些因素以提高模型鲁棒性。

Abstract: Transformers have been seldom employed in point cloud roof plane instance
segmentation, which is the focus of this study, and existing superpoint
Transformers suffer from limited performance due to the use of low-quality
superpoints. To address this challenge, we establish two criteria that
high-quality superpoints for Transformers should satisfy and introduce a
corresponding two-stage superpoint generation process. The superpoints
generated by our method not only have accurate boundaries, but also exhibit
consistent geometric sizes and shapes, both of which greatly benefit the
feature learning of superpoint Transformers. To compensate for the limitations
of deep learning features when the training set size is limited, we incorporate
multidimensional handcrafted features into the model. Additionally, we design a
decoder that combines a Kolmogorov-Arnold Network with a Transformer module to
improve instance prediction and mask extraction. Finally, our network's
predictions are refined using traditional algorithm-based postprocessing. For
evaluation, we annotated a real-world dataset and corrected annotation errors
in the existing RoofN3D dataset. Experimental results show that our method
achieves state-of-the-art performance on our dataset, as well as both the
original and reannotated RoofN3D datasets. Moreover, our model is not sensitive
to plane boundary annotations during training, significantly reducing the
annotation burden. Through comprehensive experiments, we also identified key
factors influencing roof plane segmentation performance: in addition to roof
types, variations in point cloud density, density uniformity, and 3D point
precision have a considerable impact. These findings underscore the importance
of incorporating data augmentation strategies that account for point cloud
quality to enhance model robustness under diverse and challenging conditions.

</details>


### [71] [Period-LLM: Extending the Periodic Capability of Multimodal Large Language Model](https://arxiv.org/abs/2505.24476)
*Yuting Zhang,Hao Lu,Qingyong Hu,Yin Wang,Kaishen Yuan,Xin Liu,Kaishun Wu*

Main category: cs.CV

TL;DR: Period-LLM是一种多模态大语言模型，旨在提升周期性任务的性能，并通过逐步任务难度和优化策略解决现有模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 周期性现象广泛存在于自然过程中，但现有MLLMs在时间建模和周期冲突方面表现不足。

Method: 采用“从易到难”的范式，结合“抵抗逻辑遗忘”优化策略，逐步构建模型的周期性推理能力。

Result: 实验证明Period-LLM在周期性任务中优于现有MLLMs。

Conclusion: Period-LLM为跨模态周期性任务提供了有效解决方案，并开源了代码。

Abstract: Periodic or quasi-periodic phenomena reveal intrinsic characteristics in
various natural processes, such as weather patterns, movement behaviors,
traffic flows, and biological signals. Given that these phenomena span multiple
modalities, the capabilities of Multimodal Large Language Models (MLLMs) offer
promising potential to effectively capture and understand their complex nature.
However, current MLLMs struggle with periodic tasks due to limitations in: 1)
lack of temporal modelling and 2) conflict between short and long periods. This
paper introduces Period-LLM, a multimodal large language model designed to
enhance the performance of periodic tasks across various modalities, and
constructs a benchmark of various difficulty for evaluating the cross-modal
periodic capabilities of large models. Specially, We adopt an "Easy to Hard
Generalization" paradigm, starting with relatively simple text-based tasks and
progressing to more complex visual and multimodal tasks, ensuring that the
model gradually builds robust periodic reasoning capabilities. Additionally, we
propose a "Resisting Logical Oblivion" optimization strategy to maintain
periodic reasoning abilities during semantic alignment. Extensive experiments
demonstrate the superiority of the proposed Period-LLM over existing MLLMs in
periodic tasks. The code is available at
https://github.com/keke-nice/Period-LLM.

</details>


### [72] [ACM-UNet: Adaptive Integration of CNNs and Mamba for Efficient Medical Image Segmentation](https://arxiv.org/abs/2505.24481)
*Jing Huang,Yongkang Zhao,Yuhan Li,Zhitao Dai,Cheng Chen,Qiying Lai*

Main category: cs.CV

TL;DR: ACM-UNet是一个通用的医学图像分割框架，通过轻量级适配器机制整合预训练的CNN和Mamba模型，解决了结构不匹配问题，并在Synapse和ACDC基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以充分利用预训练视觉主干（如ResNet、ViT、VMamba）的结构优势，导致性能受限。

Method: 提出ACM-UNet框架，结合轻量级适配器机制整合CNN和Mamba模型，并在解码器中引入分层多尺度小波变换模块。

Result: 在Synapse数据集上达到85.12% Dice Score和13.89mm HD95，计算效率高（17.93G FLOPs）。

Conclusion: ACM-UNet通过简单设计有效结合局部和全局特征建模，性能优越且计算高效。

Abstract: The U-shaped encoder-decoder architecture with skip connections has become a
prevailing paradigm in medical image segmentation due to its simplicity and
effectiveness. While many recent works aim to improve this framework by
designing more powerful encoders and decoders, employing advanced convolutional
neural networks (CNNs) for local feature extraction, Transformers or state
space models (SSMs) such as Mamba for global context modeling, or hybrid
combinations of both, these methods often struggle to fully utilize pretrained
vision backbones (e.g., ResNet, ViT, VMamba) due to structural mismatches. To
bridge this gap, we introduce ACM-UNet, a general-purpose segmentation
framework that retains a simple UNet-like design while effectively
incorporating pretrained CNNs and Mamba models through a lightweight adapter
mechanism. This adapter resolves architectural incompatibilities and enables
the model to harness the complementary strengths of CNNs and SSMs-namely,
fine-grained local detail extraction and long-range dependency modeling.
Additionally, we propose a hierarchical multi-scale wavelet transform module in
the decoder to enhance feature fusion and reconstruction fidelity. Extensive
experiments on the Synapse and ACDC benchmarks demonstrate that ACM-UNet
achieves state-of-the-art performance while remaining computationally
efficient. Notably, it reaches 85.12% Dice Score and 13.89mm HD95 on the
Synapse dataset with 17.93G FLOPs, showcasing its effectiveness and
scalability. Code is available at: https://github.com/zyklcode/ACM-UNet.

</details>


### [73] [Deformable Attention Mechanisms Applied to Object Detection, case of Remote Sensing](https://arxiv.org/abs/2505.24489)
*Anasse Boutayeb,Iyad Lahsen-cherif,Ahmed El Khadimi*

Main category: cs.CV

TL;DR: 论文提出了一种基于Deformable-DETR模型的目标检测方法，应用于光学和SAR遥感图像，性能优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 遥感图像中目标检测的重要性，以及基于Transformer的DL模型在此任务中的潜力。

Method: 使用Deformable-DETR模型，结合光学（Pleiades Aircraft）和SAR（SSDD）数据集进行10折分层验证。

Result: 光学数据集F1得分95.12%，SAR数据集94.54%，优于CNN和Transformer模型。

Conclusion: Deformable-DETR在遥感目标检测中表现优异，具有实际应用价值。

Abstract: Object detection has recently seen an interesting trend in terms of the most
innovative research work, this task being of particular importance in the field
of remote sensing, given the consistency of these images in terms of
geographical coverage and the objects present. Furthermore, Deep Learning (DL)
models, in particular those based on Transformers, are especially relevant for
visual computing tasks in general, and target detection in particular. Thus,
the present work proposes an application of Deformable-DETR model, a specific
architecture using deformable attention mechanisms, on remote sensing images in
two different modes, especially optical and Synthetic Aperture Radar (SAR). To
achieve this objective, two datasets are used, one optical, which is Pleiades
Aircraft dataset, and the other SAR, in particular SAR Ship Detection Dataset
(SSDD). The results of a 10-fold stratified validation showed that the proposed
model performed particularly well, obtaining an F1 score of 95.12% for the
optical dataset and 94.54% for SSDD, while comparing these results with several
models detections, especially those based on CNNs and transformers, as well as
those specifically designed to detect different object classes in remote
sensing images.

</details>


### [74] [Reason-SVG: Hybrid Reward RL for Aha-Moments in Vector Graphics Generation](https://arxiv.org/abs/2505.24499)
*Ximing Xing,Yandong Guan,Jing Zhang,Dong Xu,Qian Yu*

Main category: cs.CV

TL;DR: Reason-SVG通过“Drawing-with-Thought”范式，结合监督微调和强化学习，显著提升了LLM生成高质量SVG的能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在生成SVG时缺乏结构有效性、语义忠实性和视觉连贯性，Reason-SVG旨在解决这一问题。

Method: 采用两阶段训练策略：监督微调激活基础推理能力，强化学习（GRPO）优化生成过程，并设计混合奖励函数评估生成质量。

Result: Reason-SVG显著提升了LLM生成准确且视觉吸引人的SVG的能力。

Conclusion: Reason-SVG通过创新方法解决了LLM在SVG生成中的不足，为设计领域提供了新思路。

Abstract: Generating high-quality Scalable Vector Graphics (SVGs) is challenging for
Large Language Models (LLMs), as it requires advanced reasoning for structural
validity, semantic faithfulness, and visual coherence -- capabilities in which
current LLMs often fall short. In this work, we introduce Reason-SVG, a novel
framework designed to enhance LLM reasoning for SVG generation. Reason-SVG
pioneers the "Drawing-with-Thought" (DwT) paradigm, in which models generate
both SVG code and explicit design rationales, mimicking the human creative
process. Reason-SVG adopts a two-stage training strategy: First, Supervised
Fine-Tuning (SFT) trains the LLM on the DwT paradigm to activate foundational
reasoning abilities. Second, Reinforcement Learning (RL), utilizing Group
Relative Policy Optimization (GRPO), empowers the model to generate both DwT
and SVGs rationales through refined, reward-driven reasoning. To facilitate
reasoning-driven SVG generation, we design a Hybrid Reward function that
evaluates the presence and utility of DwT reasoning, along with structural
validity, semantic alignment, and visual quality. We also introduce the
SVGX-DwT-10k dataset, a high-quality corpus of 10,000 SVG-DwT pairs, where each
SVG code is generated based on explicit DwT reasoning. By integrating DwT, SFT,
and Hybrid Reward-guided RL, Reason-SVG significantly improves LLM performance
in generating accurate and visually compelling SVGs, potentially fostering "Aha
moments" in design.

</details>


### [75] [un$^2$CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP](https://arxiv.org/abs/2505.24517)
*Yinqi Li,Jiahe Zhao,Hong Chang,Ruibing Hou,Shiguang Shan,Xilin Chen*

Main category: cs.CV

TL;DR: 本文提出了一种改进CLIP模型的方法un$^2$CLIP，通过结合生成模型unCLIP的能力，提升CLIP在视觉细节捕捉和多模态任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP模型在区分图像细节和密集预测任务中表现不佳，因此需要改进以更好地捕捉视觉细节。

Method: 利用生成模型unCLIP（通过CLIP图像嵌入生成图像）的能力，提出un$^2$CLIP方法，将unCLIP的视觉细节捕捉能力融入CLIP模型。

Result: 实验表明，un$^2$CLIP在多个任务（如MMVP-VLM基准、开放词汇分割任务等）中显著优于原始CLIP及其他改进方法。

Conclusion: un$^2$CLIP成功提升了CLIP模型的视觉细节捕捉能力，同时保持了其与文本编码器的对齐性。

Abstract: Contrastive Language-Image Pre-training (CLIP) has become a foundation model
and has been applied to various vision and multimodal tasks. However, recent
works indicate that CLIP falls short in distinguishing detailed differences in
images and shows suboptimal performance on dense-prediction and vision-centric
multimodal tasks. Therefore, this work focuses on improving existing CLIP
models, aiming to capture as many visual details in images as possible. We find
that a specific type of generative models, unCLIP, provides a suitable
framework for achieving our goal. Specifically, unCLIP trains an image
generator conditioned on the CLIP image embedding. In other words, it inverts
the CLIP image encoder. Compared to discriminative models like CLIP, generative
models are better at capturing image details because they are trained to learn
the data distribution of images. Additionally, the conditional input space of
unCLIP aligns with CLIP's original image-text embedding space. Therefore, we
propose to invert unCLIP (dubbed un$^2$CLIP) to improve the CLIP model. In this
way, the improved image encoder can gain unCLIP's visual detail capturing
ability while preserving its alignment with the original text encoder
simultaneously. We evaluate our improved CLIP across various tasks to which
CLIP has been applied, including the challenging MMVP-VLM benchmark, the
dense-prediction open-vocabulary segmentation task, and multimodal large
language model tasks. Experiments show that un$^2$CLIP significantly improves
the original CLIP and previous CLIP improvement methods. Code and models will
be available at https://github.com/LiYinqi/un2CLIP.

</details>


### [76] [AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders](https://arxiv.org/abs/2505.24519)
*Yuqi Zhang,Yuchun Miao,Zuchao Li,Liang Ding*

Main category: cs.CV

TL;DR: AMIA是一种轻量级的防御方法，通过自动屏蔽无关图像块和联合意图分析，显著提升大型视觉语言模型的安全性，同时保持实用性。


<details>
  <summary>Details</summary>
Motivation: 针对大型视觉语言模型（LVLMs）易受对抗性攻击的问题，提出一种无需重新训练的轻量级防御方案。

Method: 1. 自动屏蔽与文本无关的图像块以破坏对抗性扰动；2. 联合意图分析以在生成响应前发现并缓解潜在有害意图。

Result: 防御成功率从平均52.4%提升至81.7%，实用性仅下降2%，推理开销较低。

Conclusion: 屏蔽和意图分析是实现安全性与实用性平衡的关键组件。

Abstract: We introduce AMIA, a lightweight, inference-only defense for Large
Vision-Language Models (LVLMs) that (1) Automatically Masks a small set of
text-irrelevant image patches to disrupt adversarial perturbations, and (2)
conducts joint Intention Analysis to uncover and mitigate hidden harmful
intents before response generation. Without any retraining, AMIA improves
defense success rates across diverse LVLMs and jailbreak benchmarks from an
average of 52.4% to 81.7%, preserves general utility with only a 2% average
accuracy drop, and incurs only modest inference overhead. Ablation confirms
both masking and intention analysis are essential for a robust safety-utility
trade-off.

</details>


### [77] [UniGeo: Taming Video Diffusion for Unified Consistent Geometry Estimation](https://arxiv.org/abs/2505.24521)
*Yang-Tian Sun,Xin Yu,Zehuan Huang,Yi-Hua Huang,Yuan-Chen Guo,Ziyi Yang,Yan-Pei Cao,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 利用扩散模型先验辅助单目几何估计（如深度和法线）的方法因其强泛化能力受到关注。本文通过适当设计和微调，利用视频生成模型的固有一致性实现一致的几何估计。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注单帧相机坐标系内的几何属性估计，忽略了扩散模型确定帧间对应关系的能力。

Method: 1) 选择全局坐标系中与视频帧共享对应关系的几何属性作为预测目标；2) 提出重用位置编码的高效条件方法；3) 通过联合训练共享对应关系的多几何属性提升性能。

Result: 在预测视频全局几何属性方面表现优异，可直接应用于重建任务，且仅需静态视频数据训练即可泛化到动态场景。

Conclusion: 通过利用扩散模型的固有一致性，本文方法在几何估计中实现了高效和泛化能力。

Abstract: Recently, methods leveraging diffusion model priors to assist monocular
geometric estimation (e.g., depth and normal) have gained significant attention
due to their strong generalization ability. However, most existing works focus
on estimating geometric properties within the camera coordinate system of
individual video frames, neglecting the inherent ability of diffusion models to
determine inter-frame correspondence. In this work, we demonstrate that,
through appropriate design and fine-tuning, the intrinsic consistency of video
generation models can be effectively harnessed for consistent geometric
estimation. Specifically, we 1) select geometric attributes in the global
coordinate system that share the same correspondence with video frames as the
prediction targets, 2) introduce a novel and efficient conditioning method by
reusing positional encodings, and 3) enhance performance through joint training
on multiple geometric attributes that share the same correspondence. Our
results achieve superior performance in predicting global geometric attributes
in videos and can be directly applied to reconstruction tasks. Even when
trained solely on static video data, our approach exhibits the potential to
generalize to dynamic video scenes.

</details>


### [78] [Optimal Density Functions for Weighted Convolution in Learning Models](https://arxiv.org/abs/2505.24527)
*Simone Cammarasana,Giuseppe Patanè*

Main category: cs.CV

TL;DR: 论文提出了一种加权卷积方法，通过最优密度函数调整邻域像素的贡献，显著提升了卷积神经网络的精度。


<details>
  <summary>Details</summary>
Motivation: 传统卷积对所有邻域像素平等处理，而加权卷积通过距离优化像素贡献，以提高近似精度。

Method: 提出加权卷积框架，分离卷积核权重优化（随机梯度下降）和密度函数优化（DIRECT-L）。

Result: 实验显示，加权卷积在图像去噪任务中损失减少53%，测试精度提升，但执行时间增加11%。

Conclusion: 加权卷积在多种超参数下表现稳健，未来将应用于2D和3D图像卷积学习问题。

Abstract: The paper introduces the weighted convolution, a novel approach to the
convolution for signals defined on regular grids (e.g., 2D images) through the
application of an optimal density function to scale the contribution of
neighbouring pixels based on their distance from the central pixel. This choice
differs from the traditional uniform convolution, which treats all neighbouring
pixels equally. Our weighted convolution can be applied to convolutional neural
network problems to improve the approximation accuracy. Given a convolutional
network, we define a framework to compute the optimal density function through
a minimisation model. The framework separates the optimisation of the
convolutional kernel weights (using stochastic gradient descent) from the
optimisation of the density function (using DIRECT-L). Experimental results on
a learning model for an image-to-image task (e.g., image denoising) show that
the weighted convolution significantly reduces the loss (up to 53% improvement)
and increases the test accuracy compared to standard convolution. While this
method increases execution time by 11%, it is robust across several
hyperparameters of the learning model. Future work will apply the weighted
convolution to real-case 2D and 3D image convolutional learning problems.

</details>


### [79] [Geospatial Foundation Models to Enable Progress on Sustainable Development Goals](https://arxiv.org/abs/2505.24528)
*Pedram Ghamisi,Weikang Yu,Xiaokang Zhang,Aldino Rizaldy,Jian Wang,Chufeng Zhou,Richard Gloaguen,Gustau Camps-Valls*

Main category: cs.CV

TL;DR: SustainFM是一个基于17个可持续发展目标的基准测试框架，用于评估地理空间基础模型（FMs）的实际效用及其与全球可持续发展目标的一致性。研究发现FMs在多任务中表现优于传统方法，但需综合考虑准确性、可迁移性、泛化能力和能源效率。


<details>
  <summary>Details</summary>
Motivation: 地理空间基础模型（FMs）在自然语言处理和计算机视觉领域取得突破，但其在可持续发展目标中的实际应用和效用尚未充分探索。

Method: 通过SustainFM框架，对FMs在多样化的地理空间任务中进行评估，包括资产财富预测和环境灾害检测等。

Result: FMs在多任务中表现优于传统方法，但其评估需包括准确性、可迁移性、泛化能力和能源效率。FMs为可持续发展目标提供了可扩展的解决方案。

Conclusion: 研究呼吁从以模型为中心的开发转向以影响力为导向的部署，并强调能源效率、领域适应性及伦理考量等指标的重要性。

Abstract: Foundation Models (FMs) are large-scale, pre-trained AI systems that have
revolutionized natural language processing and computer vision, and are now
advancing geospatial analysis and Earth Observation (EO). They promise improved
generalization across tasks, scalability, and efficient adaptation with minimal
labeled data. However, despite the rapid proliferation of geospatial FMs, their
real-world utility and alignment with global sustainability goals remain
underexplored. We introduce SustainFM, a comprehensive benchmarking framework
grounded in the 17 Sustainable Development Goals with extremely diverse tasks
ranging from asset wealth prediction to environmental hazard detection. This
study provides a rigorous, interdisciplinary assessment of geospatial FMs and
offers critical insights into their role in attaining sustainability goals. Our
findings show: (1) While not universally superior, FMs often outperform
traditional approaches across diverse tasks and datasets. (2) Evaluating FMs
should go beyond accuracy to include transferability, generalization, and
energy efficiency as key criteria for their responsible use. (3) FMs enable
scalable, SDG-grounded solutions, offering broad utility for tackling complex
sustainability challenges. Critically, we advocate for a paradigm shift from
model-centric development to impact-driven deployment, and emphasize metrics
such as energy efficiency, robustness to domain shifts, and ethical
considerations.

</details>


### [80] [Mixpert: Mitigating Multimodal Learning Conflicts with Efficient Mixture-of-Vision-Experts](https://arxiv.org/abs/2505.24541)
*Xin He,Xumeng Han,Longhui Wei,Lingxi Xie,Qi Tian*

Main category: cs.CV

TL;DR: Mixpert是一种高效的混合视觉专家架构，通过动态路由机制分配任务到合适的专家，解决单视觉编码器在多任务学习中的冲突，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 单视觉编码器难以处理多样化任务领域，导致冲突；现有方法通过多编码器增强感知但增加了复杂性。

Method: 提出Mixpert架构，结合单编码器的联合学习优势和多专家范式，设计动态路由机制。

Result: 实验显示Mixpert在多任务学习中显著提升性能，计算成本低。

Conclusion: Mixpert高效解决多任务冲突，适用于任何MLLM，性能优越。

Abstract: Multimodal large language models (MLLMs) require a nuanced interpretation of
complex image information, typically leveraging a vision encoder to perceive
various visual scenarios. However, relying solely on a single vision encoder to
handle diverse task domains proves difficult and inevitably leads to conflicts.
Recent work enhances data perception by directly integrating multiple
domain-specific vision encoders, yet this structure adds complexity and limits
the potential for joint optimization. In this paper, we introduce Mixpert, an
efficient mixture-of-vision-experts architecture that inherits the joint
learning advantages from a single vision encoder while being restructured into
a multi-expert paradigm for task-specific fine-tuning across different visual
tasks. Additionally, we design a dynamic routing mechanism that allocates input
images to the most suitable visual expert. Mixpert effectively alleviates
domain conflicts encountered by a single vision encoder in multi-task learning
with minimal additional computational cost, making it more efficient than
multiple encoders. Furthermore, Mixpert integrates seamlessly into any MLLM,
with experimental results demonstrating substantial performance gains across
various tasks.

</details>


### [81] [Optimal Weighted Convolution for Classification and Denosing](https://arxiv.org/abs/2505.24558)
*Simone Cammarasana,Giuseppe Patanè*

Main category: cs.CV

TL;DR: 提出了一种新型加权卷积算子，通过整合空间密度函数增强传统CNN，提升特征提取能力，并在图像分类和去噪任务中表现优于标准卷积。


<details>
  <summary>Details</summary>
Motivation: 传统卷积操作对所有邻域像素赋予相同权重，限制了空间特征的精确表征。通过引入加权卷积，可以更灵活地处理不同位置的像素，提升模型性能。

Method: 设计了一种加权卷积算子，通过预计算密度函数实现高效执行，保持参数数量不变且兼容现有CNN架构。

Result: 在CIFAR-100和DIV2K数据集上，加权卷积显著提升了性能（如VGG准确率从56.89%提升至66.94%，DnCNN的PSNR从20.17提升至22.63）。

Conclusion: 加权卷积是一种高效且通用的改进方法，适用于多种任务和数据类型，代码已开源。

Abstract: We introduce a novel weighted convolution operator that enhances traditional
convolutional neural networks (CNNs) by integrating a spatial density function
into the convolution operator. This extension enables the network to
differentially weight neighbouring pixels based on their relative position to
the reference pixel, improving spatial characterisation and feature extraction.
The proposed operator maintains the same number of trainable parameters and is
fully compatible with existing CNN architectures. Although developed for 2D
image data, the framework is generalisable to signals on regular grids of
arbitrary dimensions, such as 3D volumetric data or 1D time series. We propose
an efficient implementation of the weighted convolution by pre-computing the
density function and achieving execution times comparable to standard
convolution layers. We evaluate our method on two deep learning tasks: image
classification using the CIFAR-100 dataset [KH+09] and image denoising using
the DIV2K dataset [AT17]. Experimental results with state-of-the-art
classification (e.g., VGG [SZ15], ResNet [HZRS16]) and denoising (e.g., DnCNN
[ZZC+17], NAFNet [CCZS22]) methods show that the weighted convolution improves
performance with respect to standard convolution across different quantitative
metrics. For example, VGG achieves an accuracy of 66.94% with weighted
convolution versus 56.89% with standard convolution on the classification
problem, while DnCNN improves the PSNR value from 20.17 to 22.63 on the
denoising problem. All models were trained on the CINECA Leonardo cluster to
reduce the execution time and improve the tuning of the density function
values. The PyTorch implementation of the weighted convolution is publicly
available at: https://github.com/cammarasana123/weightedConvolution2.0.

</details>


### [82] [Unleashing the Power of Intermediate Domains for Mixed Domain Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2505.24567)
*Qinghe Ma,Jian Zhang,Lei Qi,Qian Yu,Yinghuan Shi,Yang Gao*

Main category: cs.CV

TL;DR: 论文提出了一种名为MiDSS的新场景，结合了半监督学习和领域适应，以解决医学图像分割中标注不足和领域偏移的问题。提出的UST-RUN框架通过中间域信息促进知识迁移，实验显示其性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割中标注不足和领域偏移问题并存，传统方法无法同时解决，因此提出了MiDSS场景和UST-RUN框架。

Method: UST-RUN框架包括：1) 统一复制粘贴(UCP)构建中间域；2) 对称引导训练(SymGD)利用伪标签监督；3) 训练过程感知的随机幅度混合(TP-RAM)逐步引入风格转换。

Result: 在四个公开数据集上的实验表明，UST-RUN表现优异，尤其在Prostate数据集上Dice分数提升了12.94%。

Conclusion: UST-RUN有效解决了MiDSS场景中的挑战，为医学图像分割提供了新思路。

Abstract: Both limited annotation and domain shift are prevalent challenges in medical
image segmentation. Traditional semi-supervised segmentation and unsupervised
domain adaptation methods address one of these issues separately. However, the
coexistence of limited annotation and domain shift is quite common, which
motivates us to introduce a novel and challenging scenario: Mixed Domain
Semi-supervised medical image Segmentation (MiDSS), where limited labeled data
from a single domain and a large amount of unlabeled data from multiple
domains. To tackle this issue, we propose the UST-RUN framework, which fully
leverages intermediate domain information to facilitate knowledge transfer. We
employ Unified Copy-paste (UCP) to construct intermediate domains, and propose
a Symmetric GuiDance training strategy (SymGD) to supervise unlabeled data by
merging pseudo-labels from intermediate samples. Subsequently, we introduce a
Training Process aware Random Amplitude MixUp (TP-RAM) to progressively
incorporate style-transition components into intermediate samples. To generate
more diverse intermediate samples, we further select reliable samples with
high-quality pseudo-labels, which are then mixed with other unlabeled data.
Additionally, we generate sophisticated intermediate samples with high-quality
pseudo-labels for unreliable samples, ensuring effective knowledge transfer for
them. Extensive experiments on four public datasets demonstrate the superiority
of UST-RUN. Notably, UST-RUN achieves a 12.94% improvement in Dice score on the
Prostate dataset. Our code is available at https://github.com/MQinghe/UST-RUN

</details>


### [83] [SARD: A Large-Scale Synthetic Arabic OCR Dataset for Book-Style Text Recognition](https://arxiv.org/abs/2505.24600)
*Omer Nacar,Yasser Al-Habashi,Serry Sibaee,Adel Ammar,Wadii Boulila*

Main category: cs.CV

TL;DR: SARD是一个大规模合成的阿拉伯语OCR数据集，旨在弥补现有数据集的不足，提供干净、可控的训练环境。


<details>
  <summary>Details</summary>
Motivation: 现有阿拉伯语OCR数据集规模小、多样性不足，无法模拟真实书籍布局，限制了现代OCR模型的训练。

Method: 通过合成生成843,622张文档图像，包含6.9亿单词，覆盖10种阿拉伯字体，确保多样性和可控性。

Result: SARD为OCR模型提供了无噪声的训练环境，并展示了其在传统和深度学习模型上的基准结果。

Conclusion: SARD是开发强大阿拉伯语OCR和视觉语言模型的宝贵资源。

Abstract: Arabic Optical Character Recognition (OCR) is essential for converting vast
amounts of Arabic print media into digital formats. However, training modern
OCR models, especially powerful vision-language models, is hampered by the lack
of large, diverse, and well-structured datasets that mimic real-world book
layouts. Existing Arabic OCR datasets often focus on isolated words or lines or
are limited in scale, typographic variety, or structural complexity found in
books. To address this significant gap, we introduce SARD (Large-Scale
Synthetic Arabic OCR Dataset). SARD is a massive, synthetically generated
dataset specifically designed to simulate book-style documents. It comprises
843,622 document images containing 690 million words, rendered across ten
distinct Arabic fonts to ensure broad typographic coverage. Unlike datasets
derived from scanned documents, SARD is free from real-world noise and
distortions, offering a clean and controlled environment for model training.
Its synthetic nature provides unparalleled scalability and allows for precise
control over layout and content variation. We detail the dataset's composition
and generation process and provide benchmark results for several OCR models,
including traditional and deep learning approaches, highlighting the challenges
and opportunities presented by this dataset. SARD serves as a valuable resource
for developing and evaluating robust OCR and vision-language models capable of
processing diverse Arabic book-style texts.

</details>


### [84] [GARLIC: GAussian Representation LearnIng for spaCe partitioning](https://arxiv.org/abs/2505.24608)
*Panagiotis Rigas,Panagiotis Drivas,Charalambos Tzamos,Ioannis Chamodrakas,George Ioannakis,Leonidas J. Guibas,Ioannis Z. Emiris*

Main category: cs.CV

TL;DR: GARLIC是一种基于高维高斯分布的新型索引结构，用于高效学习和搜索高维向量空间，结合了高斯渲染技术和信息论优化，在快速构建和高召回率方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的高维向量空间索引方法在效率和准确性上存在不足，GARLIC旨在通过高斯表示学习和动态优化解决这一问题。

Method: 利用高斯渲染技术，通过信息论目标优化高斯参数，并通过分裂和克隆操作逐步细化表示，支持高维数据。

Result: 在SIFT1M数据集上构建时间低于5分钟，Recall10@10达到50%；在Fashion-MNIST上比Faiss-IVF减少一半的查询次数；分类任务中比其他多数投票方法准确率高15%。

Conclusion: GARLIC在速度和准确性上均表现优异，适用于需要高效检索和分类的应用场景。

Abstract: We introduce GARLIC (GAussian Representation LearnIng for spaCe
partitioning), a novel indexing structure based on \(N\)-dimensional Gaussians
for efficiently learning high-dimensional vector spaces. Our approach is
inspired from Gaussian splatting techniques, typically used in 3D rendering,
which we adapt for high-dimensional search and classification. We optimize
Gaussian parameters using information-theoretic objectives that balance
coverage, assignment confidence, and structural and semantic consistency. A key
contribution is to progressively refine the representation through split and
clone operations, handling hundreds of dimensions, thus handling varying data
densities. GARLIC offers the fast building times of traditional space
partitioning methods (e.g., under \(\sim5\) min build time for SIFT1M) while
achieving \(\sim50\%\) Recall10@10 in low-candidate regimes. Experimental
results on standard benchmarks demonstrate our method's consistency in (a)
\(k\)-NN retrieval, outperforming methods, such as Faiss-IVF, in fast-recall by
using about half their probes for the same Recall10@10 in Fashion-MNIST, and
(b) in classification tasks, beating by \(\sim15\%\) accuracy other majority
voting methods. Further, we show strong generalization capabilities,
maintaining high accuracy even with downsampled training data: using just
\(1\%\) of the training data returns \(\sim 45\%\) Recall@1, thus making GARLIC
quite powerful for applications requiring both speed and accuracy.

</details>


### [85] [Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors](https://arxiv.org/abs/2505.24625)
*Duo Zheng,Shijia Huang,Yanyang Li,Liwei Wang*

Main category: cs.CV

TL;DR: 提出了一种直接从视频数据理解3D场景的新方法VG LLM，无需额外3D输入，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提升多模态大语言模型（MLLMs）直接从视频数据理解3D空间的能力，避免依赖额外的3D输入数据。

Method: 使用3D视觉几何编码器从视频序列提取3D先验信息，并与视觉标记结合输入MLLM。

Result: 在3D场景理解和空间推理任务中表现优异，4B模型在VSI-Bench评测中超越Gemini-1.5-Pro。

Conclusion: VG LLM方法高效且无需额外3D数据，为3D场景理解提供了新思路。

Abstract: Previous research has investigated the application of Multimodal Large
Language Models (MLLMs) in understanding 3D scenes by interpreting them as
videos. These approaches generally depend on comprehensive 3D data inputs, such
as point clouds or reconstructed Bird's-Eye View (BEV) maps. In our research,
we advance this field by enhancing the capability of MLLMs to understand and
reason in 3D spaces directly from video data, without the need for additional
3D input. We propose a novel and efficient method, the Video-3D Geometry Large
Language Model (VG LLM). Our approach employs a 3D visual geometry encoder that
extracts 3D prior information from video sequences. This information is
integrated with visual tokens and fed into the MLLM. Extensive experiments have
shown that our method has achieved substantial improvements in various tasks
related to 3D scene understanding and spatial reasoning, all directly learned
from video sources. Impressively, our 4B model, which does not rely on explicit
3D data inputs, achieves competitive results compared to existing
state-of-the-art methods, and even surpasses the Gemini-1.5-Pro in the
VSI-Bench evaluations.

</details>


### [86] [NUC-Net: Non-uniform Cylindrical Partition Network for Efficient LiDAR Semantic Segmentation](https://arxiv.org/abs/2505.24634)
*Xuzhi Wang,Wei Feng,Lingdong Kong,Liang Wan*

Main category: cs.CV

TL;DR: NUC-Net提出了一种非均匀圆柱分割网络，解决了LiDAR语义分割中均匀分割方法的高计算成本和内存消耗问题，并在性能和效率上显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于体素的LiDAR语义分割方法存在高计算成本和内存消耗，且无法有效处理点云分布不均的问题。

Method: 提出非均匀圆柱分割（API方法）和非均匀多尺度聚合方法，优化体素表示和上下文信息。

Result: 在SemanticKITTI和nuScenes数据集上达到SOTA性能，训练速度提升4倍，GPU内存减少2倍，推理速度提升3倍。

Conclusion: NUC-Net是一种高效且通用的LiDAR语义分割方法，显著提升了准确性和效率，并提供了理论分析支持。

Abstract: LiDAR semantic segmentation plays a vital role in autonomous driving.
Existing voxel-based methods for LiDAR semantic segmentation apply uniform
partition to the 3D LiDAR point cloud to form a structured representation based
on cartesian/cylindrical coordinates. Although these methods show impressive
performance, the drawback of existing voxel-based methods remains in two
aspects: (1) it requires a large enough input voxel resolution, which brings a
large amount of computation cost and memory consumption. (2) it does not well
handle the unbalanced point distribution of LiDAR point cloud. In this paper,
we propose a non-uniform cylindrical partition network named NUC-Net to tackle
the above challenges. Specifically, we propose the Arithmetic Progression of
Interval (API) method to non-uniformly partition the radial axis and generate
the voxel representation which is representative and efficient. Moreover, we
propose a non-uniform multi-scale aggregation method to improve contextual
information. Our method achieves state-of-the-art performance on SemanticKITTI
and nuScenes datasets with much faster speed and much less training time. And
our method can be a general component for LiDAR semantic segmentation, which
significantly improves both the accuracy and efficiency of the uniform
counterpart by $4 \times$ training faster and $2 \times$ GPU memory reduction
and $3 \times$ inference speedup. We further provide theoretical analysis
towards understanding why NUC is effective and how point distribution affects
performance. Code is available at
\href{https://github.com/alanWXZ/NUC-Net}{https://github.com/alanWXZ/NUC-Net}.

</details>


### [87] [Category-Level 6D Object Pose Estimation in Agricultural Settings Using a Lattice-Deformation Framework and Diffusion-Augmented Synthetic Data](https://arxiv.org/abs/2505.24636)
*Marios Glytsos,Panagiotis P. Filntisis,George Retsinas,Petros Maragos*

Main category: cs.CV

TL;DR: PLANTPose是一个基于RGB输入的类别级6D姿态估计框架，通过预测变形参数适应未见实例，无需实例特定数据。


<details>
  <summary>Details</summary>
Motivation: 解决农业中水果和蔬菜因形状、大小和纹理差异导致的6D姿态估计难题，避免依赖CAD模型或深度传感器。

Method: 利用基网格预测6D姿态和变形参数，结合Stable Diffusion增强合成训练图像的真实性。

Result: 在包含不同形状、大小和成熟度的香蕉基准测试中表现优异，显著优于MegaPose。

Conclusion: PLANTPose能有效处理类别内大变化，实现高精度6D姿态估计，适用于实际农业场景。

Abstract: Accurate 6D object pose estimation is essential for robotic grasping and
manipulation, particularly in agriculture, where fruits and vegetables exhibit
high intra-class variability in shape, size, and texture. The vast majority of
existing methods rely on instance-specific CAD models or require depth sensors
to resolve geometric ambiguities, making them impractical for real-world
agricultural applications. In this work, we introduce PLANTPose, a novel
framework for category-level 6D pose estimation that operates purely on RGB
input. PLANTPose predicts both the 6D pose and deformation parameters relative
to a base mesh, allowing a single category-level CAD model to adapt to unseen
instances. This enables accurate pose estimation across varying shapes without
relying on instance-specific data. To enhance realism and improve
generalization, we also leverage Stable Diffusion to refine synthetic training
images with realistic texturing, mimicking variations due to ripeness and
environmental factors and bridging the domain gap between synthetic data and
the real world. Our evaluations on a challenging benchmark that includes
bananas of various shapes, sizes, and ripeness status demonstrate the
effectiveness of our framework in handling large intraclass variations while
maintaining accurate 6D pose predictions, significantly outperforming the
state-of-the-art RGB-based approach MegaPose.

</details>


### [88] [Cloud Optical Thickness Retrievals Using Angle Invariant Attention Based Deep Learning Models](https://arxiv.org/abs/2505.24638)
*Zahid Hassan Tushar,Adeleke Ademakinwa,Jianwu Wang,Zhibo Zhang,Sanjay Purushotham*

Main category: cs.CV

TL;DR: 论文提出了一种名为CAAC的新型角度不变、基于注意力的深度学习模型，用于更准确地估计云光学厚度（COT），显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统独立像素近似（IPA）方法因简化假设引入偏差，而现有深度学习模型对辐射强度变化和角度敏感，导致COT估计不准确。

Method: 提出Cloud-Attention-Net with Angle Coding（CAAC），利用注意力机制和角度嵌入考虑卫星视角几何和3D辐射传输效应，并通过多角度训练策略实现角度不变性。

Result: CAAC显著优于现有深度学习模型，将云属性检索误差降低至少九倍。

Conclusion: CAAC通过结合注意力机制和多角度训练，解决了现有方法的局限性，为COT估计提供了更准确的解决方案。

Abstract: Cloud Optical Thickness (COT) is a critical cloud property influencing
Earth's climate, weather, and radiation budget. Satellite radiance measurements
enable global COT retrieval, but challenges like 3D cloud effects, viewing
angles, and atmospheric interference must be addressed to ensure accurate
estimation. Traditionally, the Independent Pixel Approximation (IPA) method,
which treats individual pixels independently, has been used for COT estimation.
However, IPA introduces significant bias due to its simplified assumptions.
Recently, deep learning-based models have shown improved performance over IPA
but lack robustness, as they are sensitive to variations in radiance intensity,
distortions, and cloud shadows. These models also introduce substantial errors
in COT estimation under different solar and viewing zenith angles. To address
these challenges, we propose a novel angle-invariant, attention-based deep
model called Cloud-Attention-Net with Angle Coding (CAAC). Our model leverages
attention mechanisms and angle embeddings to account for satellite viewing
geometry and 3D radiative transfer effects, enabling more accurate retrieval of
COT. Additionally, our multi-angle training strategy ensures angle invariance.
Through comprehensive experiments, we demonstrate that CAAC significantly
outperforms existing state-of-the-art deep learning models, reducing cloud
property retrieval errors by at least a factor of nine.

</details>


### [89] [A Cross Branch Fusion-Based Contrastive Learning Framework for Point Cloud Self-supervised Learning](https://arxiv.org/abs/2505.24641)
*Chengzhi Wu,Qianliang Huang,Kun Jin,Julius Pfrommer,Jürgen Beyerer*

Main category: cs.CV

TL;DR: 提出了一种基于对比跨分支注意力的点云无监督学习框架PoCCA，通过在损失端之前引入子分支实现信息交换，提升了点云表示学习的效果。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习框架仅在损失端对学习到的特征进行对比操作，缺乏分支间的信息交换，限制了表示学习的效果。

Method: 提出PoCCA框架，通过引入子分支实现不同分支间的信息交换，适用于点云数据的无监督学习。

Result: 实验表明，在不使用额外训练数据的情况下，PoCCA学习的表示在下游任务中达到最优性能。

Conclusion: PoCCA通过分支间信息交换提升了点云表示学习的效果，为无监督学习提供了新思路。

Abstract: Contrastive learning is an essential method in self-supervised learning. It
primarily employs a multi-branch strategy to compare latent representations
obtained from different branches and train the encoder. In the case of
multi-modal input, diverse modalities of the same object are fed into distinct
branches. When using single-modal data, the same input undergoes various
augmentations before being fed into different branches. However, all existing
contrastive learning frameworks have so far only performed contrastive
operations on the learned features at the final loss end, with no information
exchange between different branches prior to this stage. In this paper, for
point cloud unsupervised learning without the use of extra training data, we
propose a Contrastive Cross-branch Attention-based framework for Point cloud
data (termed PoCCA), to learn rich 3D point cloud representations. By
introducing sub-branches, PoCCA allows information exchange between different
branches before the loss end. Experimental results demonstrate that in the case
of using no extra training data, the representations learned with our
self-supervised model achieve state-of-the-art performances when used for
downstream tasks on point clouds.

</details>


### [90] [BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models](https://arxiv.org/abs/2505.24649)
*Huu-Thien Tran,Thanh-Dat Truong,Khoa Luu*

Main category: cs.CV

TL;DR: 论文提出了一种新的双射最大似然学习（BIMA）方法，用于减少视觉语言模型中的幻觉问题，取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在多个领域广泛应用，但其不可解释性导致幻觉问题（模型生成与视觉内容不符的响应），亟需解决。

Method: 采用归一化流理论，提出BIMA方法，优化解码过程以减少幻觉。

Result: BIMA在POPE基准测试中平均F1得分为85.06%，CHAIRS和CHAIRI分别降低7.6%和2.6%。

Conclusion: BIMA是首批通过双射方法减少视觉语言模型幻觉的研究之一，效果显著。

Abstract: Large vision-language models have become widely adopted to advance in various
domains. However, developing a trustworthy system with minimal interpretable
characteristics of large-scale models presents a significant challenge. One of
the most prevalent terms associated with the fallacy functions caused by these
systems is hallucination, where the language model generates a response that
does not correspond to the visual content. To mitigate this problem, several
approaches have been developed, and one prominent direction is to ameliorate
the decoding process. In this paper, we propose a new Bijective Maximum
Likelihood Learning (BIMA) approach to hallucination mitigation using
normalizing flow theories. The proposed BIMA method can efficiently mitigate
the hallucination problem in prevailing vision-language models, resulting in
significant improvements. Notably, BIMA achieves the average F1 score of 85.06%
on POPE benchmark and remarkably reduce CHAIRS and CHAIRI by 7.6% and 2.6%,
respectively. To the best of our knowledge, this is one of the first studies
that contemplates the bijection means to reduce hallucination induced by large
vision-language models.

</details>


### [91] [Decoupled Competitive Framework for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2505.24667)
*Jiahe Chen,Jiahe Ying,Shen Wang,Jianwei Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种解耦竞争框架（DCF），用于解决半监督医学图像分割中Mean Teacher和Dual Students结构的局限性，通过动态解耦和竞争机制提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决半监督医学图像分割中Mean Teacher结构的过耦合问题和Dual Students结构的认知偏差问题，以突破性能瓶颈。

Method: 提出DCF框架，采用竞争机制动态更新EMA，并促进学生间精确知识的交换。

Result: 在多个公开数据集上验证了DCF的优越性，性能超越现有方法。

Conclusion: DCF通过解耦和竞争机制有效提升了半监督医学图像分割的性能，具有实际应用潜力。

Abstract: Confronting the critical challenge of insufficiently annotated samples in
medical domain, semi-supervised medical image segmentation (SSMIS) emerges as a
promising solution. Specifically, most methodologies following the Mean Teacher
(MT) or Dual Students (DS) architecture have achieved commendable results.
However, to date, these approaches face a performance bottleneck due to two
inherent limitations, \textit{e.g.}, the over-coupling problem within MT
structure owing to the employment of exponential moving average (EMA)
mechanism, as well as the severe cognitive bias between two students of DS
structure, both of which potentially lead to reduced efficacy, or even model
collapse eventually. To mitigate these issues, a Decoupled Competitive
Framework (DCF) is elaborated in this work, which utilizes a straightforward
competition mechanism for the update of EMA, effectively decoupling students
and teachers in a dynamical manner. In addition, the seamless exchange of
invaluable and precise insights is facilitated among students, guaranteeing a
better learning paradigm. The DCF introduced undergoes rigorous validation on
three publicly accessible datasets, which encompass both 2D and 3D datasets.
The results demonstrate the superiority of our method over previous
cutting-edge competitors. Code will be available at
https://github.com/JiaheChen2002/DCF.

</details>


### [92] [6D Pose Estimation on Point Cloud Data through Prior Knowledge Integration: A Case Study in Autonomous Disassembly](https://arxiv.org/abs/2505.24669)
*Chengzhi Wu,Hao Fu,Jan-Philipp Kaiser,Erik Tabuchi Barczak,Julius Pfrommer,Gisela Lanza,Michael Heizmann,Jürgen Beyerer*

Main category: cs.CV

TL;DR: 论文提出了一种多阶段管道，用于在制造环境中准确估计螺栓的6D姿态，解决了遮挡和单视角数据限制的问题。


<details>
  <summary>Details</summary>
Motivation: 在制造领域，利用先验知识改进6D姿态估计，以支持自动化拆卸流程。

Method: 设计了一个多阶段管道，结合先验知识，从遮挡和单视角数据中获取完整的螺栓6D信息。

Result: 管道成功捕获了所有螺栓的6D姿态信息，验证了先验知识在复杂任务中的有效性。

Conclusion: 该方法不仅推动了6D姿态估计领域的发展，还展示了领域知识在制造自动化中的实际应用潜力。

Abstract: The accurate estimation of 6D pose remains a challenging task within the
computer vision domain, even when utilizing 3D point cloud data. Conversely, in
the manufacturing domain, instances arise where leveraging prior knowledge can
yield advancements in this endeavor. This study focuses on the disassembly of
starter motors to augment the engineering of product life cycles. A pivotal
objective in this context involves the identification and 6D pose estimation of
bolts affixed to the motors, facilitating automated disassembly within the
manufacturing workflow. Complicating matters, the presence of occlusions and
the limitations of single-view data acquisition, notably when motors are placed
in a clamping system, obscure certain portions and render some bolts
imperceptible. Consequently, the development of a comprehensive pipeline
capable of acquiring complete bolt information is imperative to avoid oversight
in bolt detection. In this paper, employing the task of bolt detection within
the scope of our project as a pertinent use case, we introduce a meticulously
devised pipeline. This multi-stage pipeline effectively captures the 6D
information with regard to all bolts on the motor, thereby showcasing the
effective utilization of prior knowledge in handling this challenging task. The
proposed methodology not only contributes to the field of 6D pose estimation
but also underscores the viability of integrating domain-specific insights to
tackle complex problems in manufacturing and automation.

</details>


### [93] [Beyond FACS: Data-driven Facial Expression Dictionaries, with Application to Predicting Autism](https://arxiv.org/abs/2505.24679)
*Evangelos Sariyanidi,Lisa Yankowitz,Robert T. Schultz,John D. Herrington,Birkan Tunc,Jeffrey Cohn*

Main category: cs.CV

TL;DR: 论文提出了一种名为Facial Basis的新编码系统，替代传统的FACS，用于全面捕捉面部行为，克服了FACS的三大局限性，并在自闭症诊断中表现优于常用AU检测器。


<details>
  <summary>Details</summary>
Motivation: FACS编码过程繁琐且成本高，且检测精度不足，无法全面表征面部表情，因此需要一种新的编码系统。

Method: 提出完全无监督的Facial Basis编码系统，基于数据驱动，捕捉局部化和可解释的3D面部运动。

Result: Facial Basis在自闭症诊断预测中优于常用AU检测器，并能全面重建所有可观察的面部运动。

Conclusion: Facial Basis是首个替代FACS的系统，能更全面地编码面部行为，且开源实现可供使用。

Abstract: The Facial Action Coding System (FACS) has been used by numerous studies to
investigate the links between facial behavior and mental health. The laborious
and costly process of FACS coding has motivated the development of machine
learning frameworks for Action Unit (AU) detection. Despite intense efforts
spanning three decades, the detection accuracy for many AUs is considered to be
below the threshold needed for behavioral research. Also, many AUs are excluded
altogether, making it impossible to fulfill the ultimate goal of FACS-the
representation of any facial expression in its entirety. This paper considers
an alternative approach. Instead of creating automated tools that mimic FACS
experts, we propose to use a new coding system that mimics the key properties
of FACS. Specifically, we construct a data-driven coding system called the
Facial Basis, which contains units that correspond to localized and
interpretable 3D facial movements, and overcomes three structural limitations
of automated FACS coding. First, the proposed method is completely
unsupervised, bypassing costly, laborious and variable manual annotation.
Second, Facial Basis reconstructs all observable movement, rather than relying
on a limited repertoire of recognizable movements (as in automated FACS).
Finally, the Facial Basis units are additive, whereas AUs may fail detection
when they appear in a non-additive combination. The proposed method outperforms
the most frequently used AU detector in predicting autism diagnosis from
in-person and remote conversations, highlighting the importance of encoding
facial behavior comprehensively. To our knowledge, Facial Basis is the first
alternative to FACS for deconstructing facial expressions in videos into
localized movements. We provide an open source implementation of the method at
github.com/sariyanidi/FacialBasis.

</details>


### [94] [Learning reusable concepts across different egocentric video understanding tasks](https://arxiv.org/abs/2505.24690)
*Simone Alberto Peirone,Francesca Pistilli,Antonio Alliegro,Tatiana Tommasi,Giuseppe Averta*

Main category: cs.CV

TL;DR: Hier-EgoPack是一个统一框架，旨在通过多任务视角提升自主系统的整体感知能力。


<details>
  <summary>Details</summary>
Motivation: 人类对视频流的理解是多方面的，自主系统需要类似的多任务协同学习能力以实现全面感知。

Method: 提出Hier-EgoPack框架，通过任务视角的集合为下游任务提供额外信息，形成可迁移的技能库。

Result: 框架能够为机器人提供可携带和按需使用的技能背包。

Conclusion: Hier-EgoPack通过任务协同学习，增强了自主系统的多任务感知能力。

Abstract: Our comprehension of video streams depicting human activities is naturally
multifaceted: in just a few moments, we can grasp what is happening, identify
the relevance and interactions of objects in the scene, and forecast what will
happen soon, everything all at once. To endow autonomous systems with such
holistic perception, learning how to correlate concepts, abstract knowledge
across diverse tasks, and leverage tasks synergies when learning novel skills
is essential. In this paper, we introduce Hier-EgoPack, a unified framework
able to create a collection of task perspectives that can be carried across
downstream tasks and used as a potential source of additional insights, as a
backpack of skills that a robot can carry around and use when needed.

</details>


### [95] [Conformal Prediction for Zero-Shot Models](https://arxiv.org/abs/2505.24693)
*Julio Silva-Rodríguez,Ismail Ben Ayed,Jose Dolz*

Main category: cs.CV

TL;DR: 本文研究了CLIP模型在分形预测范式下的可靠性，提出了一种名为Conf-OT的迁移学习方法，通过最优传输解决预训练与任务间的领域差异，显著提升了集合效率。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在判别任务上表现优异，但其可靠性和不确定性研究不足，尤其是在领域漂移的情况下。

Method: 提出Conf-OT方法，通过最优传输在标定集和查询集上进行转导学习，无需额外数据分割即可缓解领域差异。

Result: 在15个数据集和三种非一致性评分上，Conf-OT将集合效率提升了20%，速度比传统方法快15倍。

Conclusion: Conf-OT有效解决了领域漂移问题，提升了分形预测的效率，同时保持了覆盖保证。

Abstract: Vision-language models pre-trained at large scale have shown unprecedented
adaptability and generalization to downstream tasks. Although its
discriminative potential has been widely explored, its reliability and
uncertainty are still overlooked. In this work, we investigate the capabilities
of CLIP models under the split conformal prediction paradigm, which provides
theoretical guarantees to black-box models based on a small, labeled
calibration set. In contrast to the main body of literature on conformal
predictors in vision classifiers, foundation models exhibit a particular
characteristic: they are pre-trained on a one-time basis on an inaccessible
source domain, different from the transferred task. This domain drift
negatively affects the efficiency of the conformal sets and poses additional
challenges. To alleviate this issue, we propose Conf-OT, a transfer learning
setting that operates transductive over the combined calibration and query
sets. Solving an optimal transport problem, the proposed method bridges the
domain gap between pre-training and adaptation without requiring additional
data splits but still maintaining coverage guarantees. We comprehensively
explore this conformal prediction strategy on a broad span of 15 datasets and
three non-conformity scores. Conf-OT provides consistent relative improvements
of up to 20% on set efficiency while being 15 times faster than popular
transductive approaches.

</details>


### [96] [RT-X Net: RGB-Thermal cross attention network for Low-Light Image Enhancement](https://arxiv.org/abs/2505.24705)
*Raman Jha,Adithya Lenka,Mani Ramanagopal,Aswin Sankaranarayanan,Kaushik Mitra*

Main category: cs.CV

TL;DR: RT-X Net是一种跨注意力网络，融合RGB和热成像图像以提升夜间图像质量，并引入V-TIEE数据集支持研究。


<details>
  <summary>Details</summary>
Motivation: 夜间图像因高噪声和强光源导致质量下降，热成像图像能提供互补信息。

Method: 采用自注意力网络提取特征，跨注意力机制融合RGB和热成像图像。

Result: 在LLVIP和V-TIEE数据集上，RT-X Net优于现有方法。

Conclusion: RT-X Net有效提升夜间图像质量，V-TIEE数据集为研究提供支持。

Abstract: In nighttime conditions, high noise levels and bright illumination sources
degrade image quality, making low-light image enhancement challenging. Thermal
images provide complementary information, offering richer textures and
structural details. We propose RT-X Net, a cross-attention network that fuses
RGB and thermal images for nighttime image enhancement. We leverage
self-attention networks for feature extraction and a cross-attention mechanism
for fusion to effectively integrate information from both modalities. To
support research in this domain, we introduce the Visible-Thermal Image
Enhancement Evaluation (V-TIEE) dataset, comprising 50 co-located visible and
thermal images captured under diverse nighttime conditions. Extensive
evaluations on the publicly available LLVIP dataset and our V-TIEE dataset
demonstrate that RT-X Net outperforms state-of-the-art methods in low-light
image enhancement. The code and the V-TIEE can be found here
https://github.com/jhakrraman/rt-xnet.

</details>


### [97] [Reinforcing Video Reasoning with Focused Thinking](https://arxiv.org/abs/2505.24718)
*Jisheng Dang,Jingze Wu,Teng Wang,Xuanhui Lin,Nannan Zhu,Hongbo Chen,Wei-Shi Zheng,Meng Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: TW-GRPO通过令牌加权机制和密集奖励粒度改进了GRPO，提升了多模态大语言模型在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在推理链冗长和二元奖励无法区分部分正确答案的问题，影响了学习效率和性能。

Method: 提出令牌加权机制以筛选高信息密度令牌，并采用多选择QA任务和软奖励机制，同时引入问题-答案反转的数据增强策略。

Result: 在多个视频推理和理解基准测试中表现优异，如CLEVRER上准确率提升18.8%。

Conclusion: TW-GRPO通过优化推理聚焦和奖励设计，显著提升了模型性能，为复杂推理任务提供了新思路。

Abstract: Recent advancements in reinforcement learning, particularly through Group
Relative Policy Optimization (GRPO), have significantly improved multimodal
large language models for complex reasoning tasks. However, two critical
limitations persist: 1) they often produce unfocused, verbose reasoning chains
that obscure salient spatiotemporal cues and 2) binary rewarding fails to
account for partially correct answers, resulting in high reward variance and
inefficient learning. In this paper, we propose TW-GRPO, a novel framework that
enhances visual reasoning with focused thinking and dense reward granularity.
Specifically, we employs a token weighting mechanism that prioritizes tokens
with high informational density (estimated by intra-group variance),
suppressing redundant tokens like generic reasoning prefixes. Furthermore, we
reformulate RL training by shifting from single-choice to multi-choice QA
tasks, where soft rewards enable finer-grained gradient estimation by
distinguishing partial correctness. Additionally, we propose question-answer
inversion, a data augmentation strategy to generate diverse multi-choice
samples from existing benchmarks. Experiments demonstrate state-of-the-art
performance on several video reasoning and general understanding benchmarks.
Notably, TW-GRPO achieves 50.4\% accuracy on CLEVRER (18.8\% improvement over
Video-R1) and 65.8\% on MMVU. Our codes are available at
\href{https://github.com/longmalongma/TW-GRPO}{https://github.com/longmalongma/TW-GRPO}.

</details>


### [98] [DreamDance: Animating Character Art via Inpainting Stable Gaussian Worlds](https://arxiv.org/abs/2505.24733)
*Jiaxu Zhang,Xianfang Zeng,Xin Chen,Wei Zuo,Gang Yu,Guosheng Lin,Zhigang Tu*

Main category: cs.CV

TL;DR: DreamDance是一个新颖的角色动画框架，通过相机轨迹生成稳定的角色和场景动画，分为场景修复和视频修复两步。


<details>
  <summary>Details</summary>
Motivation: 解决角色动画中相机轨迹与动态角色结合的挑战，生成高质量且一致的动画。

Method: 采用两步修复法：相机感知场景修复生成背景视频，姿态感知视频修复注入动态角色并优化背景。

Result: 实验证明DreamDance能生成高质量、一致的动画，具有显著的相机动态效果。

Conclusion: DreamDance通过创新的两步修复法，有效解决了角色动画中的动态与一致性难题。

Abstract: This paper presents DreamDance, a novel character art animation framework
capable of producing stable, consistent character and scene motion conditioned
on precise camera trajectories. To achieve this, we re-formulate the animation
task as two inpainting-based steps: Camera-aware Scene Inpainting and
Pose-aware Video Inpainting. The first step leverages a pre-trained image
inpainting model to generate multi-view scene images from the reference art and
optimizes a stable large-scale Gaussian field, which enables coarse background
video rendering with camera trajectories. However, the rendered video is rough
and only conveys scene motion. To resolve this, the second step trains a
pose-aware video inpainting model that injects the dynamic character into the
scene video while enhancing background quality. Specifically, this model is a
DiT-based video generation model with a gating strategy that adaptively
integrates the character's appearance and pose information into the base
background video. Through extensive experiments, we demonstrate the
effectiveness and generalizability of DreamDance, producing high-quality and
consistent character animations with remarkable camera dynamics.

</details>


### [99] [Tackling View-Dependent Semantics in 3D Language Gaussian Splatting](https://arxiv.org/abs/2505.24746)
*Jiazhong Cen,Xudong Zhou,Jiemin Fang,Changsong Wen,Lingxi Xie,Xiaopeng Zhang,Wei Shen,Qi Tian*

Main category: cs.CV

TL;DR: LaGa提出了一种方法，通过分解3D场景为对象并构建视角聚合的语义表示，解决了2D与3D语义理解之间的视角依赖性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法将2D语义特征简单投影到3D高斯上，忽略了3D对象在不同视角下可能展现不同的语义（视角依赖语义）。

Method: LaGa通过分解3D场景为对象，聚类语义描述符，并基于多视角语义重新加权，构建视角聚合的语义表示。

Result: 在LERF-OVS数据集上，LaGa比之前的最佳方法提升了18.7%的mIoU。

Conclusion: LaGa有效捕捉了视角依赖语义的关键信息，实现了对3D场景更全面的理解。

Abstract: Recent advancements in 3D Gaussian Splatting (3D-GS) enable high-quality 3D
scene reconstruction from RGB images. Many studies extend this paradigm for
language-driven open-vocabulary scene understanding. However, most of them
simply project 2D semantic features onto 3D Gaussians and overlook a
fundamental gap between 2D and 3D understanding: a 3D object may exhibit
various semantics from different viewpoints--a phenomenon we term
view-dependent semantics. To address this challenge, we propose LaGa (Language
Gaussians), which establishes cross-view semantic connections by decomposing
the 3D scene into objects. Then, it constructs view-aggregated semantic
representations by clustering semantic descriptors and reweighting them based
on multi-view semantics. Extensive experiments demonstrate that LaGa
effectively captures key information from view-dependent semantics, enabling a
more comprehensive understanding of 3D scenes. Notably, under the same
settings, LaGa achieves a significant improvement of +18.7% mIoU over the
previous SOTA on the LERF-OVS dataset. Our code is available at:
https://github.com/SJTU-DeepVisionLab/LaGa.

</details>


### [100] [Draw ALL Your Imagine: A Holistic Benchmark and Agent Framework for Complex Instruction-based Image Generation](https://arxiv.org/abs/2505.24787)
*Yucheng Zhou,Jiahao Yuan,Qianning Wang*

Main category: cs.CV

TL;DR: 论文介绍了LongBench-T2I，一个专门评估文本到图像生成模型在复杂指令下表现的基准，并提出Plan2Gen框架以提升复杂指令驱动的图像生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型在复杂指令（涉及多对象、属性和空间关系）下表现不佳，且现有评估基准无法全面捕捉这些复杂需求。

Method: 提出LongBench-T2I基准（包含500个复杂提示）和Plan2Gen框架（利用大语言模型分解复杂指令，指导生成过程）。

Result: LongBench-T2I提供了多维度评估工具，Plan2Gen框架无需额外训练即可提升复杂指令下的生成效果。

Conclusion: LongBench-T2I填补了复杂指令评估的空白，Plan2Gen框架为提升文本到图像生成模型的复杂指令处理能力提供了新思路。

Abstract: Recent advancements in text-to-image (T2I) generation have enabled models to
produce high-quality images from textual descriptions. However, these models
often struggle with complex instructions involving multiple objects,
attributes, and spatial relationships. Existing benchmarks for evaluating T2I
models primarily focus on general text-image alignment and fail to capture the
nuanced requirements of complex, multi-faceted prompts. Given this gap, we
introduce LongBench-T2I, a comprehensive benchmark specifically designed to
evaluate T2I models under complex instructions. LongBench-T2I consists of 500
intricately designed prompts spanning nine diverse visual evaluation
dimensions, enabling a thorough assessment of a model's ability to follow
complex instructions. Beyond benchmarking, we propose an agent framework
(Plan2Gen) that facilitates complex instruction-driven image generation without
requiring additional model training. This framework integrates seamlessly with
existing T2I models, using large language models to interpret and decompose
complex prompts, thereby guiding the generation process more effectively. As
existing evaluation metrics, such as CLIPScore, fail to adequately capture the
nuances of complex instructions, we introduce an evaluation toolkit that
automates the quality assessment of generated images using a set of
multi-dimensional metrics. The data and code are released at
https://github.com/yczhou001/LongBench-T2I.

</details>


### [101] [Lightweight Relational Embedding in Task-Interpolated Few-Shot Networks for Enhanced Gastrointestinal Disease Classification](https://arxiv.org/abs/2505.24792)
*Xinliu Zhong,Leo Hwa Liang,Angela S. Koh,Yeo Si Yong*

Main category: cs.CV

TL;DR: 提出一种基于Few-Shot Learning的深度学习网络，用于优化结肠镜图像分析，提高结直肠癌检测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统结肠镜检查具有侵入性且依赖高质量图像，而现有方法在图像区分和诊断准确性上存在局限。

Method: 结合Few-Shot Learning架构，包括特征提取器、任务插值、关系嵌入和注意力机制，以增强模型对未见图像模式的适应能力。

Result: 在Kvasir数据集上表现优异，准确率达90.1%，F1分数0.891，超越现有方法。

Conclusion: 该模型为优化结直肠癌检测提供了高效解决方案，减少侵入性检查的风险。

Abstract: Traditional diagnostic methods like colonoscopy are invasive yet critical
tools necessary for accurately diagnosing colorectal cancer (CRC). Detection of
CRC at early stages is crucial for increasing patient survival rates. However,
colonoscopy is dependent on obtaining adequate and high-quality endoscopic
images. Prolonged invasive procedures are inherently risky for patients, while
suboptimal or insufficient images hamper diagnostic accuracy. These images,
typically derived from video frames, often exhibit similar patterns, posing
challenges in discrimination. To overcome these challenges, we propose a novel
Deep Learning network built on a Few-Shot Learning architecture, which includes
a tailored feature extractor, task interpolation, relational embedding, and a
bi-level routing attention mechanism. The Few-Shot Learning paradigm enables
our model to rapidly adapt to unseen fine-grained endoscopic image patterns,
and the task interpolation augments the insufficient images artificially from
varied instrument viewpoints. Our relational embedding approach discerns
critical intra-image features and captures inter-image transitions between
consecutive endoscopic frames, overcoming the limitations of Convolutional
Neural Networks (CNNs). The integration of a light-weight attention mechanism
ensures a concentrated analysis of pertinent image regions. By training on
diverse datasets, the model's generalizability and robustness are notably
improved for handling endoscopic images. Evaluated on Kvasir dataset, our model
demonstrated superior performance, achieving an accuracy of 90.1\%, precision
of 0.845, recall of 0.942, and an F1 score of 0.891. This surpasses current
state-of-the-art methods, presenting a promising solution to the challenges of
invasive colonoscopy by optimizing CRC detection through advanced image
analysis.

</details>


### [102] [CL-LoRA: Continual Low-Rank Adaptation for Rehearsal-Free Class-Incremental Learning](https://arxiv.org/abs/2505.24816)
*Jiangpeng He,Zhihao Duan,Fengqing Zhu*

Main category: cs.CV

TL;DR: CL-LoRA提出了一种双适配器架构，结合任务共享适配器和任务特定适配器，以解决CIL中参数冗余和跨任务知识共享不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于适配器的方法在CIL中为每个新任务创建新适配器，导致参数冗余且未能利用跨任务共享知识。

Method: CL-LoRA采用双适配器架构：任务共享适配器学习跨任务知识，任务特定适配器捕获新任务独特特征；共享适配器使用随机正交矩阵和知识蒸馏，任务特定适配器引入可学习块权重。

Result: CL-LoRA在多个基准测试中表现优异，同时减少了训练和推理计算量。

Conclusion: CL-LoRA为基于预训练模型的持续学习提供了更高效和可扩展的范式。

Abstract: Class-Incremental Learning (CIL) aims to learn new classes sequentially while
retaining the knowledge of previously learned classes. Recently, pre-trained
models (PTMs) combined with parameter-efficient fine-tuning (PEFT) have shown
remarkable performance in rehearsal-free CIL without requiring exemplars from
previous tasks. However, existing adapter-based methods, which incorporate
lightweight learnable modules into PTMs for CIL, create new adapters for each
new task, leading to both parameter redundancy and failure to leverage shared
knowledge across tasks. In this work, we propose ContinuaL Low-Rank Adaptation
(CL-LoRA), which introduces a novel dual-adapter architecture combining
\textbf{task-shared adapters} to learn cross-task knowledge and
\textbf{task-specific adapters} to capture unique features of each new task.
Specifically, the shared adapters utilize random orthogonal matrices and
leverage knowledge distillation with gradient reassignment to preserve
essential shared knowledge. In addition, we introduce learnable block-wise
weights for task-specific adapters, which mitigate inter-task interference
while maintaining the model's plasticity. We demonstrate CL-LoRA consistently
achieves promising performance under multiple benchmarks with reduced training
and inference computation, establishing a more efficient and scalable paradigm
for continual learning with pre-trained models.

</details>


### [103] [Segmenting France Across Four Centuries](https://arxiv.org/abs/2505.24824)
*Marta López-Rauhut,Hongyu Zhou,Mathieu Aubry,Loic Landrieu*

Main category: cs.CV

TL;DR: 论文介绍了一个新的历史地图数据集，用于大规模、长期的土地利用和土地覆盖演变分析，并评估了三种分割方法的性能。


<details>
  <summary>Details</summary>
Motivation: 历史地图提供了对过去几个世纪领土演变的宝贵视角，但现有数据集通常局限于单一地图类型或时期，且标注成本高。本文旨在解决这些问题。

Method: 提出了一个包含18、19和20世纪法国地图的新数据集，提供现代和历史标注。评估了三种分割方法：全监督模型和两种弱监督模型。

Result: 数据集展示了分割任务的复杂性（如风格不一致性和景观变化）。弱监督模型通过现代标注或图像翻译方法表现良好。

Conclusion: 这些方法支持长期环境监测，为景观演变提供了洞察。数据集和代码已公开。

Abstract: Historical maps offer an invaluable perspective into territory evolution
across past centuries--long before satellite or remote sensing technologies
existed. Deep learning methods have shown promising results in segmenting
historical maps, but publicly available datasets typically focus on a single
map type or period, require extensive and costly annotations, and are not
suited for nationwide, long-term analyses. In this paper, we introduce a new
dataset of historical maps tailored for analyzing large-scale, long-term land
use and land cover evolution with limited annotations. Spanning metropolitan
France (548,305 km^2), our dataset contains three map collections from the
18th, 19th, and 20th centuries. We provide both comprehensive modern labels and
22,878 km^2 of manually annotated historical labels for the 18th and 19th
century maps. Our dataset illustrates the complexity of the segmentation task,
featuring stylistic inconsistencies, interpretive ambiguities, and significant
landscape changes (e.g., marshlands disappearing in favor of forests). We
assess the difficulty of these challenges by benchmarking three approaches: a
fully-supervised model trained with historical labels, and two
weakly-supervised models that rely only on modern annotations. The latter
either use the modern labels directly or first perform image-to-image
translation to address the stylistic gap between historical and contemporary
maps. Finally, we discuss how these methods can support long-term environment
monitoring, offering insights into centuries of landscape transformation. Our
official project repository is publicly available at
https://github.com/Archiel19/FRAx4.git.

</details>


### [104] [Zero-Shot Chinese Character Recognition with Hierarchical Multi-Granularity Image-Text Aligning](https://arxiv.org/abs/2505.24837)
*Yinglian Zhu,Haiyang Yu,Qizao Wang,Wei Lu,Xiangyang Xue,Bin Li*

Main category: cs.CV

TL;DR: 提出了一种基于对比范式的层次化多粒度图文对齐框架（Hi-GITA），通过多粒度编码器和融合模块，显著提升了中文零样本字符识别的性能。


<details>
  <summary>Details</summary>
Motivation: 现有中文字符识别方法通常基于自回归或编辑距离后处理，且依赖单层字符表示，未能充分利用中文字符的细粒度语义信息。

Method: 设计了图像和文本的多粒度编码器，分别提取字符图像的层次化表示和文本的笔画、部首序列表示，并引入多粒度融合模块和细粒度解耦图文对比损失。

Result: 在零样本中文字符识别任务中，Hi-GITA显著优于现有方法，例如在手写字符和部首零样本设置下准确率提升约20%。

Conclusion: Hi-GITA通过多层次语义对齐，有效提升了中文字符识别的性能，尤其在零样本场景下表现突出。

Abstract: Chinese Character Recognition (CCR) is a fundamental technology for
intelligent document processing. Unlike Latin characters, Chinese characters
exhibit unique spatial structures and compositional rules, allowing for the use
of fine-grained semantic information in representation. However, existing
approaches are usually based on auto-regressive as well as edit distance
post-process and typically rely on a single-level character representation. In
this paper, we propose a Hierarchical Multi-Granularity Image-Text Aligning
(Hi-GITA) framework based on a contrastive paradigm. To leverage the abundant
fine-grained semantic information of Chinese characters, we propose
multi-granularity encoders on both image and text sides. Specifically, the
Image Multi-Granularity Encoder extracts hierarchical image representations
from character images, capturing semantic cues from localized strokes to
holistic structures. The Text Multi-Granularity Encoder extracts stroke and
radical sequence representations at different levels of granularity. To better
capture the relationships between strokes and radicals, we introduce
Multi-Granularity Fusion Modules on the image and text sides, respectively.
Furthermore, to effectively bridge the two modalities, we further introduce a
Fine-Grained Decoupled Image-Text Contrastive loss, which aligns image and text
representations across multiple granularities. Extensive experiments
demonstrate that our proposed Hi-GITA significantly outperforms existing
zero-shot CCR methods. For instance, it brings about 20% accuracy improvement
in handwritten character and radical zero-shot settings. Code and models will
be released soon.

</details>


### [105] [VideoCAD: A Large-Scale Video Dataset for Learning UI Interactions and 3D Reasoning from CAD Software](https://arxiv.org/abs/2505.24838)
*Brandon Man,Ghadi Nehme,Md Ferdous Alam,Faez Ahmed*

Main category: cs.CV

TL;DR: VideoCAD是一个大规模合成数据集，用于学习CAD操作的UI交互，支持长时程和高复杂度任务，并提出了VideoCADFormer模型和VQA基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有AI驱动的UI代理数据集和方法难以满足专业工程工具的需求，尤其是CAD这种高复杂度、长时程的任务。

Method: 通过自动化框架生成41K标注视频，构建VideoCAD数据集，并提出VideoCADFormer模型学习CAD交互。

Result: VideoCADFormer优于基线模型，同时VQA基准测试揭示了视频UI理解中的关键挑战。

Conclusion: VideoCAD为工程UI交互学习提供了新基准，并展示了当前视频理解技术的局限性。

Abstract: Computer-Aided Design (CAD) is a time-consuming and complex process,
requiring precise, long-horizon user interactions with intricate 3D interfaces.
While recent advances in AI-driven user interface (UI) agents show promise,
most existing datasets and methods focus on short, low-complexity tasks in
mobile or web applications, failing to capture the demands of professional
engineering tools. In this work, we introduce VideoCAD, the first attempt at
engineering UI interaction learning for precision tasks. Specifically, VideoCAD
is a large-scale synthetic dataset consisting of over 41K annotated video
recordings of CAD operations, generated using an automated framework for
collecting high-fidelity UI action data from human-made CAD designs. Compared
to existing datasets, VideoCAD offers an order of magnitude higher complexity
in UI interaction learning for real-world engineering tasks, having up to a 20x
longer time horizon than other datasets. We show two important downstream
applications of VideoCAD: learning UI interactions from professional precision
3D CAD tools and a visual question-answering (VQA) benchmark designed to
evaluate multimodal large language models' (LLM) spatial reasoning and video
understanding abilities. To learn the UI interactions, we propose
VideoCADFormer - a state-of-the-art model in learning CAD interactions directly
from video, which outperforms multiple behavior cloning baselines. Both
VideoCADFormer and the VQA benchmark derived from VideoCAD reveal key
challenges in the current state of video-based UI understanding, including the
need for precise action grounding, multi-modal and spatial reasoning, and
long-horizon dependencies.

</details>


### [106] [Vision LLMs Are Bad at Hierarchical Visual Understanding, and LLMs Are the Bottleneck](https://arxiv.org/abs/2505.24840)
*Yuwen Tan,Yuan Qing,Boqing Gong*

Main category: cs.CV

TL;DR: 研究发现，许多先进的大语言模型（LLMs）缺乏对视觉世界的层次知识，甚至不了解基本的生物分类学，这限制了视觉LLMs的层次理解能力。通过构建约100万个四选一视觉问答任务，验证了这一点，并发现微调视觉LLMs后，LLMs的层次一致性提升更多。


<details>
  <summary>Details</summary>
Motivation: 揭示LLMs在视觉层次理解上的不足，并探讨其对视觉LLMs的限制。

Method: 使用约100万个四选一视觉问答任务，基于六个分类学和四个图像数据集进行分析，并对视觉LLMs进行微调。

Result: LLMs缺乏层次知识，成为视觉LLMs的瓶颈；微调后LLMs的层次一致性提升更显著。

Conclusion: 视觉LLMs的完全层次理解需以LLMs具备相应的分类学知识为前提。

Abstract: This paper reveals that many state-of-the-art large language models (LLMs)
lack hierarchical knowledge about our visual world, unaware of even
well-established biology taxonomies. This shortcoming makes LLMs a bottleneck
for vision LLMs' hierarchical visual understanding (e.g., recognizing Anemone
Fish but not Vertebrate). We arrive at these findings using about one million
four-choice visual question answering (VQA) tasks constructed from six
taxonomies and four image datasets. Interestingly, finetuning a vision LLM
using our VQA tasks reaffirms LLMs' bottleneck effect to some extent because
the VQA tasks improve the LLM's hierarchical consistency more than the vision
LLM's. We conjecture that one cannot make vision LLMs understand visual
concepts fully hierarchical until LLMs possess corresponding taxonomy
knowledge.

</details>


### [107] [Reading Recognition in the Wild](https://arxiv.org/abs/2505.24848)
*Charig Yang,Samiul Alam,Shakhrul Iman Siam,Michael J. Proulx,Lambert Mathias,Kiran Somasundaram,Luis Pesqueira,James Fort,Sheroze Sheriffdeen,Omkar Parkhi,Carl Ren,Mi Zhang,Yuning Chai,Richard Newcombe,Hyo Jin Kim*

Main category: cs.CV

TL;DR: 论文提出了一种新的阅读识别任务，用于判断用户是否在阅读，并发布了首个大规模多模态数据集“Reading in the Wild”，包含100小时的阅读和非阅读视频。通过结合RGB、眼动和头部姿态三种模态，提出了一种灵活的Transformer模型，验证了这些模态的相关性和互补性。


<details>
  <summary>Details</summary>
Motivation: 为了实现智能眼镜中的情景化AI，需要记录用户与世界的互动，包括阅读行为。当前的研究受限于小规模和约束环境，因此需要更真实、多样化的数据集和方法。

Method: 提出了一个多模态Transformer模型，利用RGB、眼动和头部姿态三种模态（单独或组合）进行阅读识别。

Result: 验证了三种模态的相关性和互补性，并展示了数据集在分类阅读类型方面的实用性。

Conclusion: 该研究扩展了当前阅读理解研究的规模和多样性，为智能眼镜中的情景化AI提供了支持。

Abstract: To enable egocentric contextual AI in always-on smart glasses, it is crucial
to be able to keep a record of the user's interactions with the world,
including during reading. In this paper, we introduce a new task of reading
recognition to determine when the user is reading. We first introduce the
first-of-its-kind large-scale multimodal Reading in the Wild dataset,
containing 100 hours of reading and non-reading videos in diverse and realistic
scenarios. We then identify three modalities (egocentric RGB, eye gaze, head
pose) that can be used to solve the task, and present a flexible transformer
model that performs the task using these modalities, either individually or
combined. We show that these modalities are relevant and complementary to the
task, and investigate how to efficiently and effectively encode each modality.
Additionally, we show the usefulness of this dataset towards classifying types
of reading, extending current reading understanding studies conducted in
constrained settings to larger scale, diversity and realism. Code, model, and
data will be public.

</details>


### [108] [ViStoryBench: Comprehensive Benchmark Suite for Story Visualization](https://arxiv.org/abs/2505.24862)
*Cailin Zhuang,Ailin Huang,Wei Cheng,Jingwei Wu,Yaoqi Hu,Jiaqi Liao,Zhewei Huang,Hongyuan Wang,Xinyao Liao,Weiwei Cai,Hengyuan Xu,Xuanyang Zhang,Xianfang Zeng,Gang Yu,Chi Zhang*

Main category: cs.CV

TL;DR: 论文提出了一个名为ViStoryBench的综合性评估基准，用于提升故事可视化框架在现实场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 为了进一步改进生成模型在故事可视化中的表现，尤其是在多样化的叙事和视觉风格下，需要一个全面的评估工具。

Method: 收集了包含多种故事类型和艺术风格的数据集，设计了涵盖不同情节和视觉美学的评估维度，并引入了多种评价指标。

Result: ViStoryBench能够全面评估模型在多方面的表现，帮助识别模型的优缺点。

Conclusion: 该基准为研究者提供了一个结构化和多方面的框架，促进了对模型的针对性改进。

Abstract: Story visualization, which aims to generate a sequence of visually coherent
images aligning with a given narrative and reference images, has seen
significant progress with recent advancements in generative models. To further
enhance the performance of story visualization frameworks in real-world
scenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We
collect a diverse dataset encompassing various story types and artistic styles,
ensuring models are evaluated across multiple dimensions such as different
plots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D
renderings). ViStoryBench is carefully curated to balance narrative structures
and visual elements, featuring stories with single and multiple protagonists to
test models' ability to maintain character consistency. Additionally, it
includes complex plots and intricate world-building to challenge models in
generating accurate visuals. To ensure comprehensive comparisons, our benchmark
incorporates a wide range of evaluation metrics assessing critical aspects.
This structured and multifaceted framework enables researchers to thoroughly
identify both the strengths and weaknesses of different models, fostering
targeted improvements.

</details>


### [109] [TalkingHeadBench: A Multi-Modal Benchmark & Analysis of Talking-Head DeepFake Detection](https://arxiv.org/abs/2505.24866)
*Xinqi Xiong,Prakrut Patel,Qingyuan Fan,Amisha Wadhwa,Sarathy Selvam,Xiao Guo,Luchao Qi,Xiaoming Liu,Roni Sengupta*

Main category: cs.CV

TL;DR: 论文介绍了TalkingHeadBench，一个用于评估最先进深度伪造检测器的多模型多生成器基准和数据集，旨在解决现有基准的不足。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的快速发展对媒体、政治和金融等领域构成重大风险，但现有检测基准未能反映最新进展，缺乏对模型鲁棒性和泛化能力的评估。

Method: 提出TalkingHeadBench，包含由领先学术和商业模型合成的深度伪造视频，并设计协议评估身份和生成器特性分布变化下的泛化能力。

Result: 对多种检测方法（如CNN、视觉Transformer和时间模型）进行基准测试，分析其鲁棒性和泛化能力，并通过Grad-CAM可视化揭示常见失败模式和检测器偏差。

Conclusion: TalkingHeadBench旨在加速研究，以应对快速发展的生成技术，推动更鲁棒和可泛化的检测模型的发展。

Abstract: The rapid advancement of talking-head deepfake generation fueled by advanced
generative models has elevated the realism of synthetic videos to a level that
poses substantial risks in domains such as media, politics, and finance.
However, current benchmarks for deepfake talking-head detection fail to reflect
this progress, relying on outdated generators and offering limited insight into
model robustness and generalization. We introduce TalkingHeadBench, a
comprehensive multi-model multi-generator benchmark and curated dataset
designed to evaluate the performance of state-of-the-art detectors on the most
advanced generators. Our dataset includes deepfakes synthesized by leading
academic and commercial models and features carefully constructed protocols to
assess generalization under distribution shifts in identity and generator
characteristics. We benchmark a diverse set of existing detection methods,
including CNNs, vision transformers, and temporal models, and analyze their
robustness and generalization capabilities. In addition, we provide error
analysis using Grad-CAM visualizations to expose common failure modes and
detector biases. TalkingHeadBench is hosted on
https://huggingface.co/datasets/luchaoqi/TalkingHeadBench with open access to
all data splits and protocols. Our benchmark aims to accelerate research
towards more robust and generalizable detection models in the face of rapidly
evolving generative techniques.

</details>


### [110] [Time Blindness: Why Video-Language Models Can't See What Humans Can?](https://arxiv.org/abs/2505.24867)
*Ujjwal Upadhyay,Mukul Ranjan,Zhiqiang Shen,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: 论文提出了SpookyBench基准测试，揭示了当前视觉语言模型（VLMs）在纯时间序列任务中的局限性，与人类表现形成鲜明对比。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在空间信息被遮挡时无法捕捉纯时间模式，而人类却能高效识别。

Method: 通过创建SpookyBench基准测试，评估模型在噪声帧序列中的表现，并与人类表现对比。

Result: 人类准确率超过98%，而最先进的VLMs准确率为0%，显示模型对时间线索的提取能力不足。

Conclusion: 需开发新架构或训练范式以解耦空间和时间处理，SpookyBench将推动时间模式识别研究。

Abstract: Recent advances in vision-language models (VLMs) have made impressive strides
in understanding spatio-temporal relationships in videos. However, when spatial
information is obscured, these models struggle to capture purely temporal
patterns. We introduce $\textbf{SpookyBench}$, a benchmark where information is
encoded solely in temporal sequences of noise-like frames, mirroring natural
phenomena from biological signaling to covert communication. Interestingly,
while humans can recognize shapes, text, and patterns in these sequences with
over 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance
gap highlights a critical limitation: an over-reliance on frame-level spatial
features and an inability to extract meaning from temporal cues. Furthermore,
when trained in data sets with low spatial signal-to-noise ratios (SNR),
temporal understanding of models degrades more rapidly than human perception,
especially in tasks requiring fine-grained temporal reasoning. Overcoming this
limitation will require novel architectures or training paradigms that decouple
spatial dependencies from temporal processing. Our systematic analysis shows
that this issue persists across model scales and architectures. We release
SpookyBench to catalyze research in temporal pattern recognition and bridge the
gap between human and machine video understanding. Dataset and code has been
made available on our project website: https://timeblindness.github.io/.

</details>


### [111] [SiLVR: A Simple Language-based Video Reasoning Framework](https://arxiv.org/abs/2505.24869)
*Ce Zhang,Yan-Bo Lin,Ziyang Wang,Mohit Bansal,Gedas Bertasius*

Main category: cs.CV

TL;DR: SiLVR是一个简单的基于语言的视频推理框架，通过两阶段分解复杂视频理解任务，利用多感官输入和自适应令牌减少方案，在多个视频语言任务中取得最佳结果。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型（MLLMs）在复杂视频语言任务中的推理能力显著落后，SiLVR旨在解决这一问题。

Method: SiLVR将视频理解分为两阶段：1) 将原始视频转换为基于语言的表示；2) 将语言描述输入强大的推理LLM。采用自适应令牌减少方案处理长上下文多感官输入。

Result: SiLVR在Video-MME、Video-MMMU、Video-MMLU、CGBench和EgoLife等任务中取得最佳报告结果。

Conclusion: 研究表明，尽管未针对视频进行专门训练，强大的推理LLM仍能有效聚合多感官输入信息，完成复杂视频推理任务。

Abstract: Recent advances in test-time optimization have led to remarkable reasoning
capabilities in Large Language Models (LLMs), enabling them to solve highly
complex problems in math and coding. However, the reasoning capabilities of
multimodal LLMs (MLLMs) still significantly lag, especially for complex
video-language tasks. To address this issue, we present SiLVR, a Simple
Language-based Video Reasoning framework that decomposes complex video
understanding into two stages. In the first stage, SiLVR transforms raw video
into language-based representations using multisensory inputs, such as short
clip captions and audio/speech subtitles. In the second stage, language
descriptions are fed into a powerful reasoning LLM to solve complex
video-language understanding tasks. To handle long-context multisensory inputs,
we use an adaptive token reduction scheme, which dynamically determines the
temporal granularity with which to sample the tokens. Our simple, modular, and
training-free video reasoning framework achieves the best-reported results on
Video-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife.
Furthermore, our empirical study focused on video reasoning capabilities shows
that, despite not being explicitly trained on video, strong reasoning LLMs can
effectively aggregate multisensory input information from video, speech, and
audio for complex temporal, causal, long-context, and knowledge acquisition
reasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR.

</details>


### [112] [GenSpace: Benchmarking Spatially-Aware Image Generation](https://arxiv.org/abs/2505.24870)
*Zehan Wang,Jiayang Xu,Ziang Zhang,Tianyu Pan,Chao Du,Hengshuang Zhao,Zhou Zhao*

Main category: cs.CV

TL;DR: GenSpace是一个新的基准和评估流程，用于评估图像生成模型的空间感知能力，发现现有模型在3D空间细节上存在不足。


<details>
  <summary>Details</summary>
Motivation: 研究AI图像生成模型是否具备类似人类的空间感知能力，以生成更准确的3D场景。

Method: 提出专门的评估流程和指标，通过多视觉基础模型重建3D场景几何，提供更精确的空间忠实度评估。

Result: AI模型在视觉吸引力上表现良好，但在对象放置、关系和测量等3D细节上表现不佳。

Conclusion: 总结了当前图像生成模型的三大空间感知局限，并提出了改进方向。

Abstract: Humans can intuitively compose and arrange scenes in the 3D space for
photography. However, can advanced AI image generators plan scenes with similar
3D spatial awareness when creating images from text or image prompts? We
present GenSpace, a novel benchmark and evaluation pipeline to comprehensively
assess the spatial awareness of current image generation models. Furthermore,
standard evaluations using general Vision-Language Models (VLMs) frequently
fail to capture the detailed spatial errors. To handle this challenge, we
propose a specialized evaluation pipeline and metric, which reconstructs 3D
scene geometry using multiple visual foundation models and provides a more
accurate and human-aligned metric of spatial faithfulness. Our findings show
that while AI models create visually appealing images and can follow general
instructions, they struggle with specific 3D details like object placement,
relationships, and measurements. We summarize three core limitations in the
spatial perception of current state-of-the-art image generation models: 1)
Object Perspective Understanding, 2) Egocentric-Allocentric Transformation and
3) Metric Measurement Adherence, highlighting possible directions for improving
spatial intelligence in image generation.

</details>


### [113] [MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning](https://arxiv.org/abs/2505.24871)
*Yiqing Liang,Jielin Qiu,Wenhao Ding,Zuxin Liu,James Tompkin,Mengdi Xu,Mengzhou Xia,Zhengzhong Tu,Laixi Shi,Jiacheng Zhu*

Main category: cs.CV

TL;DR: 论文提出了一种多模态LLM的RLVR后训练框架，通过优化数据集混合策略提升模型的泛化和推理能力。


<details>
  <summary>Details</summary>
Motivation: 将RLVR应用于多模态LLM（MLLM）面临数据集冲突和任务多样性的挑战，需要解决最优数据混合问题。

Method: 开发了多模态RLVR框架，提出数据混合策略预测RL微调结果并优化混合比例。

Result: 多域RLVR训练结合混合策略显著提升MLLM的泛化能力，最佳混合使模型在分布外基准上平均提升5.24%。

Conclusion: 优化的数据混合策略在多模态RLVR训练中至关重要，显著提升模型性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for post-training large language models (LLMs), achieving
state-of-the-art performance on tasks with structured, verifiable answers.
Applying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but
is complicated by the broader, heterogeneous nature of vision-language tasks
that demand nuanced visual, logical, and spatial capabilities. As such,
training MLLMs using RLVR on multiple datasets could be beneficial but creates
challenges with conflicting objectives from interaction among diverse datasets,
highlighting the need for optimal dataset mixture strategies to improve
generalization and reasoning. We introduce a systematic post-training framework
for Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation
and benchmark implementation. Specifically, (1) We developed a multimodal RLVR
framework for multi-dataset post-training by curating a dataset that contains
different verifiable vision-language problems and enabling multi-domain online
RL learning with different verifiable rewards; (2) We proposed a data mixture
strategy that learns to predict the RL fine-tuning outcome from the data
mixture distribution, and consequently optimizes the best mixture.
Comprehensive experiments showcase that multi-domain RLVR training, when
combined with mixture prediction strategies, can significantly boost MLLM
general reasoning capacities. Our best mixture improves the post-trained
model's accuracy on out-of-distribution benchmarks by an average of 5.24%
compared to the same model post-trained with uniform data mixture, and by a
total of 20.74% compared to the pre-finetuning baseline.

</details>


### [114] [ProxyThinker: Test-Time Guidance through Small Visual Reasoners](https://arxiv.org/abs/2505.24872)
*Zilin Xiao,Jaywon Koo,Siru Ouyang,Jefferson Hernandez,Yu Meng,Vicente Ordonez*

Main category: cs.CV

TL;DR: ProxyThinker是一种无需训练的推理时技术，通过调整解码动态，使大模型继承小模型的视觉推理能力，显著提升性能并加速推理。


<details>
  <summary>Details</summary>
Motivation: 尽管强化微调（RFT）能提升视觉推理能力，但其计算成本高，难以扩展模型规模。

Method: 通过从RFT推理器中减去基础模型的输出分布，ProxyThinker修改解码动态，激发慢速推理行为。

Result: ProxyThinker在空间、数学和多学科推理任务中显著提升性能，推理速度提升38倍。

Conclusion: ProxyThinker为大规模视觉语言模型的实用部署提供了高效解决方案。

Abstract: Recent advancements in reinforcement learning with verifiable rewards have
pushed the boundaries of the visual reasoning capabilities in large
vision-language models (LVLMs). However, training LVLMs with reinforcement
fine-tuning (RFT) is computationally expensive, posing a significant challenge
to scaling model size. In this work, we propose ProxyThinker, an inference-time
technique that enables large models to inherit the visual reasoning
capabilities from small, slow-thinking visual reasoners without any training.
By subtracting the output distributions of base models from those of RFT
reasoners, ProxyThinker modifies the decoding dynamics and successfully elicits
the slow-thinking reasoning demonstrated by the emerged sophisticated behaviors
such as self-verification and self-correction. ProxyThinker consistently boosts
performance on challenging visual benchmarks on spatial, mathematical, and
multi-disciplinary reasoning, enabling untuned base models to compete with the
performance of their full-scale RFT counterparts. Furthermore, our
implementation efficiently coordinates multiple language models with
parallelism techniques and achieves up to 38 $\times$ faster inference compared
to previous decoding-time methods, paving the way for the practical deployment
of ProxyThinker. Code is available at
https://github.com/MrZilinXiao/ProxyThinker.

</details>


### [115] [MiniMax-Remover: Taming Bad Noise Helps Video Object Removal](https://arxiv.org/abs/2505.24873)
*Bojia Zi,Weixuan Peng,Xianbiao Qi,Jianan Wang,Shihao Zhao,Rong Xiao,Kam-Fai Wong*

Main category: cs.CV

TL;DR: MiniMax-Remover是一种两阶段视频对象移除方法，通过简化模型架构和优化策略，显著提高了编辑质量和推理速度。


<details>
  <summary>Details</summary>
Motivation: 视频对象移除是视频编辑的关键子任务，但现有方法存在幻觉对象、视觉伪影和计算成本高的问题。

Method: 第一阶段移除文本输入和交叉注意力层，简化模型；第二阶段通过最小最大化优化策略进一步优化编辑质量和推理速度。

Result: 该方法仅需6次采样步骤且不依赖CFG，实现了最先进的视频对象移除效果。

Conclusion: MiniMax-Remover在效率和效果上均优于现有方法。

Abstract: Recent advances in video diffusion models have driven rapid progress in video
editing techniques. However, video object removal, a critical subtask of video
editing, remains challenging due to issues such as hallucinated objects and
visual artifacts. Furthermore, existing methods often rely on computationally
expensive sampling procedures and classifier-free guidance (CFG), resulting in
slow inference. To address these limitations, we propose MiniMax-Remover, a
novel two-stage video object removal approach. Motivated by the observation
that text condition is not best suited for this task, we simplify the
pretrained video generation model by removing textual input and cross-attention
layers, resulting in a more lightweight and efficient model architecture in the
first stage. In the second stage, we distilled our remover on successful videos
produced by the stage-1 model and curated by human annotators, using a minimax
optimization strategy to further improve editing quality and inference speed.
Specifically, the inner maximization identifies adversarial input noise ("bad
noise") that makes failure removals, while the outer minimization step trains
the model to generate high-quality removal results even under such challenging
conditions. As a result, our method achieves a state-of-the-art video object
removal results with as few as 6 sampling steps and doesn't rely on CFG,
significantly improving inference efficiency. Extensive experiments demonstrate
the effectiveness and superiority of MiniMax-Remover compared to existing
methods. Codes and Videos are available at: https://minimax-remover.github.io.

</details>


### [116] [ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL](https://arxiv.org/abs/2505.24875)
*Yu Zhang,Yunqi Li,Yifan Yang,Rui Wang,Yuqing Yang,Dai Qi,Jianmin Bao,Dongdong Chen,Chong Luo,Lili Qiu*

Main category: cs.CV

TL;DR: ReasonGen-R1是一个两阶段框架，结合了链式思维推理和强化学习，通过文本推理生成高质量图像。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思维推理和强化学习在NLP领域取得突破，但在生成视觉模型中的应用尚未充分探索。

Method: 首先通过监督微调赋予自回归图像生成器文本推理能力，然后使用Group Relative Policy Optimization优化输出。

Result: 在GenEval、DPG和T2I基准测试中，ReasonGen-R1表现优于基线模型和现有最佳模型。

Conclusion: ReasonGen-R1展示了文本推理与视觉生成的结合潜力，为未来研究提供了新方向。

Abstract: Although chain-of-thought reasoning and reinforcement learning (RL) have
driven breakthroughs in NLP, their integration into generative vision models
remains underexplored. We introduce ReasonGen-R1, a two-stage framework that
first imbues an autoregressive image generator with explicit text-based
"thinking" skills via supervised fine-tuning on a newly generated reasoning
dataset of written rationales, and then refines its outputs using Group
Relative Policy Optimization. To enable the model to reason through text before
generating images, We automatically generate and release a corpus of model
crafted rationales paired with visual prompts, enabling controlled planning of
object layouts, styles, and scene compositions. Our GRPO algorithm uses reward
signals from a pretrained vision language model to assess overall visual
quality, optimizing the policy in each update. Evaluations on GenEval, DPG, and
the T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong
baselines and prior state-of-the-art models. More: aka.ms/reasongen.

</details>


### [117] [Agent-X: Evaluating Deep Multimodal Reasoning in Vision-Centric Agentic Tasks](https://arxiv.org/abs/2505.24876)
*Tajamul Ashraf,Amal Saqib,Hanan Ghani,Muhra AlMahri,Yuhao Li,Noor Ahsan,Umair Nawaz,Jean Lahoud,Hisham Cholakkal,Mubarak Shah,Philip Torr,Fahad Shahbaz Khan,Rao Muhammad Anwer,Salman Khan*

Main category: cs.CV

TL;DR: Agent-X是一个大规模基准测试，用于评估视觉中心代理在真实世界多模态环境中的多步深度推理能力，揭示了当前模型在多步视觉任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常评估代理在完全合成的单轮查询、有限视觉模态下的表现，缺乏评估真实世界中多步推理质量的框架。

Method: 引入Agent-X基准测试，包含828个任务，涵盖多种视觉环境和多模态输入，并提出细粒度、步骤级评估框架。

Result: 即使表现最佳的模型（如GPT、Gemini和Qwen家族）在多步视觉任务中的成功率也低于50%。

Conclusion: Agent-X揭示了当前LMM在推理和工具使用能力上的瓶颈，为未来视觉中心代理推理模型的研究指明了方向。

Abstract: Deep reasoning is fundamental for solving complex tasks, especially in
vision-centric scenarios that demand sequential, multimodal understanding.
However, existing benchmarks typically evaluate agents with fully synthetic,
single-turn queries, limited visual modalities, and lack a framework to assess
reasoning quality over multiple steps as required in real-world settings. To
address this, we introduce Agent-X, a large-scale benchmark for evaluating
vision-centric agents multi-step and deep reasoning capabilities in real-world,
multimodal settings. Agent- X features 828 agentic tasks with authentic visual
contexts, including images, multi-image comparisons, videos, and instructional
text. These tasks span six major agentic environments: general visual
reasoning, web browsing, security and surveillance, autonomous driving, sports,
and math reasoning. Our benchmark requires agents to integrate tool use with
explicit, stepwise decision-making in these diverse settings. In addition, we
propose a fine-grained, step-level evaluation framework that assesses the
correctness and logical coherence of each reasoning step and the effectiveness
of tool usage throughout the task. Our results reveal that even the
best-performing models, including GPT, Gemini, and Qwen families, struggle to
solve multi-step vision tasks, achieving less than 50% full-chain success.
These findings highlight key bottlenecks in current LMM reasoning and tool-use
capabilities and identify future research directions in vision-centric agentic
reasoning models. Our data and code are publicly available at
https://github.com/mbzuai-oryx/Agent-X

</details>


### [118] [AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion](https://arxiv.org/abs/2505.24877)
*Yangyi Huang,Ye Yuan,Xueting Li,Jan Kautz,Umar Iqbal*

Main category: cs.CV

TL;DR: AdaHuman是一个新框架，从单张图像生成高保真、可动画的3D虚拟形象，解决了现有方法在细节和动画适应性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以生成高细节、适合动画的3D虚拟形象，限制了实际应用。

Method: 结合了姿势条件3D联合扩散模型和多视角图像合成，以及3D高斯溅射（3DGS）细化模块，提升局部细节并整合为完整模型。

Result: AdaHuman在公开基准测试和实际图像中显著优于现有方法，生成高保真、标准化的A姿势虚拟形象。

Conclusion: AdaHuman为3D虚拟形象生成提供了高效、高质量的解决方案，支持动画和运动输入。

Abstract: Existing methods for image-to-3D avatar generation struggle to produce highly
detailed, animation-ready avatars suitable for real-world applications. We
introduce AdaHuman, a novel framework that generates high-fidelity animatable
3D avatars from a single in-the-wild image. AdaHuman incorporates two key
innovations: (1) A pose-conditioned 3D joint diffusion model that synthesizes
consistent multi-view images in arbitrary poses alongside corresponding 3D
Gaussian Splats (3DGS) reconstruction at each diffusion step; (2) A
compositional 3DGS refinement module that enhances the details of local body
parts through image-to-image refinement and seamlessly integrates them using a
novel crop-aware camera ray map, producing a cohesive detailed 3D avatar. These
components allow AdaHuman to generate highly realistic standardized A-pose
avatars with minimal self-occlusion, enabling rigging and animation with any
input motion. Extensive evaluation on public benchmarks and in-the-wild images
demonstrates that AdaHuman significantly outperforms state-of-the-art methods
in both avatar reconstruction and reposing. Code and models will be publicly
available for research purposes.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [119] [Parameter-Free Bio-Inspired Channel Attention for Enhanced Cardiac MRI Reconstruction](https://arxiv.org/abs/2505.23872)
*Anam Hashmi,Julia Dietlmeier,Kathleen M. Curran,Noel E. O'Connor*

Main category: eess.IV

TL;DR: 论文提出了一种基于生态学原理的非线性注意力架构，用于心脏MRI重建，其参数自由模块优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有注意力模块缺乏理论支持，研究试图通过生态学原理填补这一空白。

Method: 利用描述单物种种群增长的非线性生态差异方程，设计参数自由的注意力模块。

Result: 提出的模块在心脏MRI重建任务中超越了当前最先进的参数自由方法。

Conclusion: 生态学原理可以指导开发高效且有效的注意力机制。

Abstract: Attention is a fundamental component of the human visual recognition system.
The inclusion of attention in a convolutional neural network amplifies relevant
visual features and suppresses the less important ones. Integrating attention
mechanisms into convolutional neural networks enhances model performance and
interpretability. Spatial and channel attention mechanisms have shown
significant advantages across many downstream tasks in medical imaging. While
existing attention modules have proven to be effective, their design often
lacks a robust theoretical underpinning. In this study, we address this gap by
proposing a non-linear attention architecture for cardiac MRI reconstruction
and hypothesize that insights from ecological principles can guide the
development of effective and efficient attention mechanisms. Specifically, we
investigate a non-linear ecological difference equation that describes
single-species population growth to devise a parameter-free attention module
surpassing current state-of-the-art parameter-free methods.

</details>


### [120] [Estimating Head Motion in Structural MRI Using a Deep Neural Network Trained on Synthetic Artifacts](https://arxiv.org/abs/2505.23916)
*Charles Bricout,Samira Ebrahimi Kahou,Sylvain Bouix*

Main category: eess.IV

TL;DR: 该论文提出了一种基于3D卷积神经网络的自动化方法，用于评估MRI中的运动伪影严重程度，无需依赖专业硬件或噪声数据。


<details>
  <summary>Details</summary>
Motivation: MRI中的运动伪影会影响神经解剖学指标的准确性，现有方法依赖硬件或噪声数据，缺乏客观性。

Method: 使用合成损坏的3D体积训练卷积神经网络，验证时采用独立数据集和人工评分。

Result: 在独立验证中，模型与人工评分的相关性为R²=0.65，并在12/15数据集中显著相关。预测结果与年龄相关。

Conclusion: 该方法可跨扫描仪和协议推广，为结构MRI研究提供客观、可扩展的运动评估。

Abstract: Motion-related artifacts are inevitable in Magnetic Resonance Imaging (MRI)
and can bias automated neuroanatomical metrics such as cortical thickness.
Manual review cannot objectively quantify motion in anatomical scans, and
existing automated approaches often require specialized hardware or rely on
unbalanced noisy training data. Here, we train a 3D convolutional neural
network to estimate motion severity using only synthetically corrupted volumes.
We validate our method with one held-out site from our training cohort and with
14 fully independent datasets, including one with manual ratings, achieving a
representative $R^2 = 0.65$ versus manual labels and significant
thickness-motion correlations in 12/15 datasets. Furthermore, our predicted
motion correlates with subject age in line with prior studies. Our approach
generalizes across scanner brands and protocols, enabling objective, scalable
motion assessment in structural MRI studies without prospective motion
correction.

</details>


### [121] [Improved Accuracy in Pelvic Tumor Resections Using a Real-Time Vision-Guided Surgical System](https://arxiv.org/abs/2505.23984)
*Vahid Danesh,Paul Arauz,Maede Boroji,Andrew Zhu,Mia Cottone,Elaine Gould,Fazel A. Khan,Imin Kao*

Main category: eess.IV

TL;DR: 研究评估了一种结合实时视觉引导和模块化夹具的系统，显著提高了骨盆骨肿瘤切除的准确性。


<details>
  <summary>Details</summary>
Motivation: 骨盆骨肿瘤切除因复杂的三维解剖结构和有限的手术可视化而极具挑战性，现有导航系统和个性化器械存在高成本、辐射暴露等问题。

Method: 开发并验证了一种结合实时视觉引导、模块化切割夹具和光学追踪的系统，与传统徒手方法进行对比。

Result: 视觉引导系统显著减少了距离和角度偏差，所有样本误差均小于3毫米。

Conclusion: 该系统在提高准确性的同时保持了工作流效率，是一种经济高效的解决方案。

Abstract: Pelvic bone tumor resections remain significantly challenging due to complex
three-dimensional anatomy and limited surgical visualization. Current
navigation systems and patient-specific instruments, while accurate, present
limitations including high costs, radiation exposure, workflow disruption, long
production time, and lack of reusability. This study evaluates a real-time
vision-guided surgical system combined with modular jigs to improve accuracy in
pelvic bone tumor resections. A vision-guided surgical system combined with
modular cutting jigs and real-time optical tracking was developed and
validated. Five female pelvis sawbones were used, with each hemipelvis randomly
assigned to either the vision-guided and modular jig system or traditional
freehand method. A total of twenty resection planes were analyzed for each
method. Accuracy was assessed by measuring distance and angular deviations from
the planned resection planes. The vision-guided and modular jig system
significantly improved resection accuracy compared to the freehand method,
reducing the mean distance deviation from 2.07 $\pm$ 1.71 mm to 1.01 $\pm$ 0.78
mm (p=0.0193). In particular, all specimens resected using the vision-guided
system exhibited errors of less than 3 mm. Angular deviations also showed
significant improvements with roll angle deviation reduced from 15.36 $\pm$
17.57$^\circ$ to 4.21 $\pm$ 3.46$^\circ$ (p=0.0275), and pitch angle deviation
decreased from 6.17 $\pm$ 4.58$^\circ$ to 1.84 $\pm$ 1.48$^\circ$ (p<0.001).
The proposed vision-guided and modular jig system significantly improves the
accuracy of pelvic bone tumor resections while maintaining workflow efficiency.
This cost-effective solution provides real-time guidance without the need for
referencing external monitors, potentially improving surgical outcomes in
complex pelvic bone tumor cases.

</details>


### [122] [Semantics-Guided Generative Image Compression](https://arxiv.org/abs/2505.24015)
*Cheng-Lin Wu,Hyomin Choi,Ivan V. Bajić*

Main category: eess.IV

TL;DR: 本文提出了一种改进的多模态图像语义压缩（MISC）方法，通过引入语义分割指导和内容自适应扩散技术，显著提升了生成图像的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 随着多模态生成AI的发展，如何在极低比特率下实现高质量图像压缩成为研究重点。

Method: 在MISC框架中引入语义分割指导和内容自适应扩散技术，优化生成解码器并减少扩散步骤。

Result: 新方法显著提升了PSNR和感知指标，同时将编解码时间减少36%以上，优于主流编解码器。

Conclusion: 提出的压缩框架在质量和效率上均取得显著改进，为图像压缩领域提供了新思路。

Abstract: Advancements in text-to-image generative AI with large multimodal models are
spreading into the field of image compression, creating high-quality
representation of images at extremely low bit rates. This work introduces novel
components to the existing multimodal image semantic compression (MISC)
approach, enhancing the quality of the generated images in terms of PSNR and
perceptual metrics. The new components include semantic segmentation guidance
for the generative decoder, as well as content-adaptive diffusion, which
controls the number of diffusion steps based on image characteristics. The
results show that our newly introduced methods significantly improve the
baseline MISC model while also decreasing the complexity. As a result, both the
encoding and decoding time are reduced by more than 36%. Moreover, the proposed
compression framework outperforms mainstream codecs in terms of perceptual
similarity and quality. The code and visual examples are available.

</details>


### [123] [Sparsity-Driven Parallel Imaging Consistency for Improved Self-Supervised MRI Reconstruction](https://arxiv.org/abs/2505.24136)
*Yaşar Utku Alçalar,Mehmet Akçakaya*

Main category: eess.IV

TL;DR: 提出了一种通过扰动训练物理驱动深度学习模型的新方法，以解决高加速率下自监督学习引入的伪影问题。


<details>
  <summary>Details</summary>
Motivation: 在高加速率MRI扫描中，自监督学习常导致伪影，影响图像质量。

Method: 通过精心设计的扰动训练模型，引入一致性项评估模型在稀疏域中预测扰动的能力。

Result: 在fastMRI膝部和脑部数据集上，新方法有效减少了伪影和噪声放大，优于现有自监督方法。

Conclusion: 所提出的训练策略在高加速率MRI重建中表现出色，提升了图像质量和可靠性。

Abstract: Physics-driven deep learning (PD-DL) models have proven to be a powerful
approach for improved reconstruction of rapid MRI scans. In order to train
these models in scenarios where fully-sampled reference data is unavailable,
self-supervised learning has gained prominence. However, its application at
high acceleration rates frequently introduces artifacts, compromising image
fidelity. To mitigate this shortcoming, we propose a novel way to train PD-DL
networks via carefully-designed perturbations. In particular, we enhance the
k-space masking idea of conventional self-supervised learning with a novel
consistency term that assesses the model's ability to accurately predict the
added perturbations in a sparse domain, leading to more reliable and
artifact-free reconstructions. The results obtained from the fastMRI knee and
brain datasets show that the proposed training strategy effectively reduces
aliasing artifacts and mitigates noise amplification at high acceleration
rates, outperforming state-of-the-art self-supervised methods both visually and
quantitatively.

</details>


### [124] [Beyond the LUMIR challenge: The pathway to foundational registration models](https://arxiv.org/abs/2505.24160)
*Junyu Chen,Shuwen Wei,Joel Honkamaa,Pekka Marttinen,Hang Zhang,Min Liu,Yichao Zhou,Zuopeng Tan,Zhuoyuan Wang,Yi Wang,Hongchao Zhou,Shunbo Hu,Yi Zhang,Qian Tao,Lukas Förner,Thomas Wendler,Bailiang Jian,Benedikt Wiestler,Tim Hable,Jin Kim,Dan Ruan,Frederic Madesta,Thilo Sentker,Wiebke Heyer,Lianrui Zuo,Yuwei Dai,Jing Wu,Jerry L. Prince,Harrison Bai,Yong Du,Yihao Liu,Alessa Hering,Reuben Dorent,Lasse Hansen,Mattias P. Heinrich,Aaron Carass*

Main category: eess.IV

TL;DR: LUMIR挑战是一个无监督脑MRI图像配准的新基准，旨在通过自监督学习推动生物合理的变形建模，并在多领域任务中评估性能。


<details>
  <summary>Details</summary>
Motivation: 推动无监督脑MRI图像配准的发展，减少对解剖标签的依赖，并通过多领域任务验证模型的泛化能力。

Method: 提供4000多张预处理T1加权脑MRI图像用于训练，引入零样本泛化任务，评估模型在不同模态、疾病、协议和物种上的表现。

Result: 深度学习方法在配准精度和解剖合理性上表现优异，优于优化方法，但对域外对比度适应性较差。

Conclusion: LUMIR挑战成功推动了无监督配准技术的发展，展示了深度学习在多领域任务中的潜力。

Abstract: Medical image challenges have played a transformative role in advancing the
field, catalyzing algorithmic innovation and establishing new performance
standards across diverse clinical applications. Image registration, a
foundational task in neuroimaging pipelines, has similarly benefited from the
Learn2Reg initiative. Building on this foundation, we introduce the Large-scale
Unsupervised Brain MRI Image Registration (LUMIR) challenge, a next-generation
benchmark designed to assess and advance unsupervised brain MRI registration.
Distinct from prior challenges that leveraged anatomical label maps for
supervision, LUMIR removes this dependency by providing over 4,000 preprocessed
T1-weighted brain MRIs for training without any label maps, encouraging
biologically plausible deformation modeling through self-supervision. In
addition to evaluating performance on 590 held-out test subjects, LUMIR
introduces a rigorous suite of zero-shot generalization tasks, spanning
out-of-domain imaging modalities (e.g., FLAIR, T2-weighted, T2*-weighted),
disease populations (e.g., Alzheimer's disease), acquisition protocols (e.g.,
9.4T MRI), and species (e.g., macaque brains). A total of 1,158 subjects and
over 4,000 image pairs were included for evaluation. Performance was assessed
using both segmentation-based metrics (Dice coefficient, 95th percentile
Hausdorff distance) and landmark-based registration accuracy (target
registration error). Across both in-domain and zero-shot tasks, deep
learning-based methods consistently achieved state-of-the-art accuracy while
producing anatomically plausible deformation fields. The top-performing deep
learning-based models demonstrated diffeomorphic properties and inverse
consistency, outperforming several leading optimization-based methods, and
showing strong robustness to most domain shifts, the exception being a drop in
performance on out-of-domain contrasts.

</details>


### [125] [Deep learning-derived arterial input function](https://arxiv.org/abs/2505.24166)
*Junyu Chen,Zirui Jiang,Jennifer M. Coughlin,Martin G. Pomper,Yong Du*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的非侵入性动脉输入函数（DLIF）估计方法，用于动态PET成像，避免了传统方法中需要血液采样的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统动态PET成像中，准确的动脉输入函数（AIF）需要侵入性血液采样，限制了其应用。本研究旨在开发一种无需采样的替代方法。

Method: 利用深度学习框架（DLIF）直接从动态PET图像序列中估计代谢校正的AIF，结合基础函数引入先验知识。

Result: DLIF在验证数据中表现出高准确性和鲁棒性，生成的参数图与真实测量结果一致。

Conclusion: DLIF为动态PET成像提供了一种快速、准确且完全非侵入的AIF估计方法，具有潜在临床应用价值。

Abstract: Dynamic positron emission tomography (PET) imaging combined with radiotracer
kinetic modeling is a powerful technique for visualizing biological processes
in the brain, offering valuable insights into brain functions and neurological
disorders such as Alzheimer's and Parkinson's diseases. Accurate kinetic
modeling relies heavily on the use of a metabolite-corrected arterial input
function (AIF), which typically requires invasive and labor-intensive arterial
blood sampling. While alternative non-invasive approaches have been proposed,
they often compromise accuracy or still necessitate at least one invasive blood
sampling. In this study, we present the deep learning-derived arterial input
function (DLIF), a deep learning framework capable of estimating a
metabolite-corrected AIF directly from dynamic PET image sequences without any
blood sampling. We validated DLIF using existing dynamic PET patient data. We
compared DLIF and resulting parametric maps against ground truth measurements.
Our evaluation shows that DLIF achieves accurate and robust AIF estimation. By
leveraging deep learning's ability to capture complex temporal dynamics and
incorporating prior knowledge of typical AIF shapes through basis functions,
DLIF provides a rapid, accurate, and entirely non-invasive alternative to
traditional AIF measurement methods.

</details>


### [126] [A Novel Coronary Artery Registration Method Based on Super-pixel Particle Swarm Optimization](https://arxiv.org/abs/2505.24351)
*Peng Qi,Wenxi Qu,Tianliang Yao,Haonan Ma,Dylan Wintle,Yinyi Lai,Giorgos Papanastasiou,Chengjia Wang*

Main category: eess.IV

TL;DR: 提出了一种基于群体优化算法的多模态冠状动脉图像配准方法，显著提升了PCI手术中XRA和CTA图像的配准精度。


<details>
  <summary>Details</summary>
Motivation: 利用CTA提供的3D血管解剖信息改进PCI手术，但需要解决2D XRA与3D CTA图像配准的挑战。

Method: 采用预处理和基于Steger及超像素粒子群优化算法的配准模块。

Result: 在28对XRA和CTA图像上测试，表现优于四种现有方法。

Conclusion: 该方法显著提升了配准效果，有望改善冠状动脉疾病的临床治疗结果。

Abstract: Percutaneous Coronary Intervention (PCI) is a minimally invasive procedure
that improves coronary blood flow and treats coronary artery disease. Although
PCI typically requires 2D X-ray angiography (XRA) to guide catheter placement
at real-time, computed tomography angiography (CTA) may substantially improve
PCI by providing precise information of 3D vascular anatomy and status. To
leverage real-time XRA and detailed 3D CTA anatomy for PCI, accurate multimodal
image registration of XRA and CTA is required, to guide the procedure and avoid
complications. This is a challenging process as it requires registration of
images from different geometrical modalities (2D -> 3D and vice versa), with
variations in contrast and noise levels. In this paper, we propose a novel
multimodal coronary artery image registration method based on a swarm
optimization algorithm, which effectively addresses challenges such as large
deformations, low contrast, and noise across these imaging modalities. Our
algorithm consists of two main modules: 1) preprocessing of XRA and CTA images
separately, and 2) a registration module based on feature extraction using the
Steger and Superpixel Particle Swarm Optimization algorithms. Our technique was
evaluated on a pilot dataset of 28 pairs of XRA and CTA images from 10 patients
who underwent PCI. The algorithm was compared with four state-of-the-art (SOTA)
methods in terms of registration accuracy, robustness, and efficiency. Our
method outperformed the selected SOTA baselines in all aspects. Experimental
results demonstrate the significant effectiveness of our algorithm, surpassing
the previous benchmarks and proposes a novel clinical approach that can
potentially have merit for improving patient outcomes in coronary artery
disease.

</details>


### [127] [Efficient RAW Image Deblurring with Adaptive Frequency Modulation](https://arxiv.org/abs/2505.24407)
*Wenlong Jiao,Binglong Li,Wei Shang,Ping Wang,Dongwei Ren*

Main category: eess.IV

TL;DR: FrENet是一个专为RAW图像去模糊设计的频率增强网络，通过自适应频率位置调制模块和频率域跳跃连接，显著提升了去模糊效果，并在计算效率上表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管RAW图像具有更好的恢复潜力，但现有深度学习方法主要关注sRGB图像，且RAW图像去模糊面临频率依赖模糊和计算效率的挑战。

Method: 提出FrENet框架，直接在频率域操作，引入自适应频率位置调制模块和频率域跳跃连接。

Result: FrENet在RAW图像去模糊上优于现有方法，恢复质量更高且计算效率更高，还能扩展到sRGB图像。

Conclusion: FrENet为RAW图像去模糊提供了高效且高质量的解决方案，并展示了在sRGB图像上的扩展潜力。

Abstract: Image deblurring plays a crucial role in enhancing visual clarity across
various applications. Although most deep learning approaches primarily focus on
sRGB images, which inherently lose critical information during the image signal
processing pipeline, RAW images, being unprocessed and linear, possess superior
restoration potential but remain underexplored. Deblurring RAW images presents
unique challenges, particularly in handling frequency-dependent blur while
maintaining computational efficiency. To address these issues, we propose
Frequency Enhanced Network (FrENet), a framework specifically designed for
RAW-to-RAW deblurring that operates directly in the frequency domain. We
introduce a novel Adaptive Frequency Positional Modulation module, which
dynamically adjusts frequency components according to their spectral positions,
thereby enabling precise control over the deblurring process. Additionally,
frequency domain skip connections are adopted to further preserve
high-frequency details. Experimental results demonstrate that FrENet surpasses
state-of-the-art deblurring methods in RAW image deblurring, achieving
significantly better restoration quality while maintaining high efficiency in
terms of reduced MACs. Furthermore, FrENet's adaptability enables it to be
extended to sRGB images, where it delivers comparable or superior performance
compared to methods specifically designed for sRGB data. The code will be
available at https://github.com/WenlongJiao/FrENet .

</details>


### [128] [pyMEAL: A Multi-Encoder Augmentation-Aware Learning for Robust and Generalizable Medical Image Translation](https://arxiv.org/abs/2505.24421)
*Abdul-mojeed Olabisi Ilyas,Adeleke Maradesa,Jamal Banzi,Jianpan Huang,Henry K. F. Mak,Kannie W. Y. Chan*

Main category: eess.IV

TL;DR: MEAL框架通过多编码器和融合策略解决3D医学影像数据稀缺和协议差异问题，在CT-MRI转换中表现优异。


<details>
  <summary>Details</summary>
Motivation: 临床AI医学影像面临患者差异、图像伪影和模型泛化不足的挑战，传统增强方法无法处理数据量和协议差异。

Method: 提出MEAL框架，利用四种增强变体和三种融合策略（CC、FL、BD）构建多编码器模型，保留增强感知特征。

Result: MEAL-BD在CT-MRI转换中表现最佳，PSNR和SSIM评分高于其他方法，适用于几何变换和非增强输入。

Conclusion: MEAL通过增强多样性支持协议不变学习，提升医学影像的临床可靠性。

Abstract: Medical imaging is critical for diagnostics, but clinical adoption of
advanced AI-driven imaging faces challenges due to patient variability, image
artifacts, and limited model generalization. While deep learning has
transformed image analysis, 3D medical imaging still suffers from data scarcity
and inconsistencies due to acquisition protocols, scanner differences, and
patient motion. Traditional augmentation uses a single pipeline for all
transformations, disregarding the unique traits of each augmentation and
struggling with large data volumes.
  To address these challenges, we propose a Multi-encoder Augmentation-Aware
Learning (MEAL) framework that leverages four distinct augmentation variants
processed through dedicated encoders. Three fusion strategies such as
concatenation (CC), fusion layer (FL), and adaptive controller block (BD) are
integrated to build multi-encoder models that combine augmentation-specific
features before decoding. MEAL-BD uniquely preserves augmentation-aware
representations, enabling robust, protocol-invariant feature learning.
  As demonstrated in a Computed Tomography (CT)-to-T1-weighted Magnetic
Resonance Imaging (MRI) translation study, MEAL-BD consistently achieved the
best performance on both unseen- and predefined-test data. On both geometric
transformations (like rotations and flips) and non-augmented inputs, MEAL-BD
outperformed other competing methods, achieving higher mean peak
signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM)
scores. These results establish MEAL as a reliable framework for preserving
structural fidelity and generalizing across clinically relevant variability. By
reframing augmentation as a source of diverse, generalizable features, MEAL
supports robust, protocol-invariant learning, advancing clinically reliable
medical imaging solutions.

</details>


### [129] [Model-Guided Network with Cluster-Based Operators for Spatio-Spectral Super-Resolution](https://arxiv.org/abs/2505.24605)
*Ivan Pereira-Sánchez,Julia Navarro,Ana Belén Petro,Joan Duran*

Main category: eess.IV

TL;DR: 提出了一种端到端的模型驱动框架，用于联合空间-光谱超分辨率重建高分辨率高光谱图像。


<details>
  <summary>Details</summary>
Motivation: 解决从低分辨率多光谱观测中重建高分辨率高光谱图像的问题，尤其是联合空间-光谱超分辨率这一相对未被充分探索的领域。

Method: 将问题分解为空间超分辨率、光谱超分辨率和融合任务，采用变分方法展开，并用可学习模块替代梯度迭代中的算子。设计基于经典反向投影算法的上采样算子、可学习的聚类上/下采样算子，以及低频估计和高频注入模块进行融合。

Result: 在多个数据集和采样因子上的广泛评估证明了该方法的有效性。

Conclusion: 提出的框架在联合空间-光谱超分辨率任务中表现优异，代码将开源。

Abstract: This paper addresses the problem of reconstructing a high-resolution
hyperspectral image from a low-resolution multispectral observation. While
spatial super-resolution and spectral super-resolution have been extensively
studied, joint spatio-spectral super-resolution remains relatively explored. We
propose an end-to-end model-driven framework that explicitly decomposes the
joint spatio-spectral super-resolution problem into spatial super-resolution,
spectral super-resolution and fusion tasks. Each sub-task is addressed by
unfolding a variational-based approach, where the operators involved in the
proximal gradient iterative scheme are replaced with tailored learnable
modules. In particular, we design an upsampling operator for spatial
super-resolution based on classical back-projection algorithms, adapted to
handle arbitrary scaling factors. Spectral reconstruction is performed using
learnable cluster-based upsampling and downsampling operators. For image
fusion, we integrate low-frequency estimation and high-frequency injection
modules to combine the spatial and spectral information from spatial
super-resolution and spectral super-resolution outputs. Additionally, we
introduce an efficient nonlocal post-processing step that leverages image
self-similarity by combining a multi-head attention mechanism with residual
connections. Extensive evaluations on several datasets and sampling factors
demonstrate the effectiveness of our approach. The source code will be
available at https://github.com/TAMI-UIB/JSSUNet

</details>


### [130] [TumorGen: Boundary-Aware Tumor-Mask Synthesis with Rectified Flow Matching](https://arxiv.org/abs/2505.24687)
*Shengyuan Liu,Wenting Chen,Boyun Zheng,Wentao Pan,Xiang Li,Yixuan Yuan*

Main category: eess.IV

TL;DR: TumorGen提出了一种高效的三维肿瘤合成方法，通过边界感知伪掩码生成、空间约束向量场估计器和VAE引导的掩码细化器，显著提升了计算效率和边界真实性。


<details>
  <summary>Details</summary>
Motivation: 解决现有肿瘤数据合成方法中肿瘤多样性受限、计算效率低以及边界过渡不自然的问题。

Method: 采用边界感知伪掩码生成模块、空间约束向量场估计器和VAE引导的掩码细化器，结合修正流匹配技术。

Result: 实验表明，TumorGen在计算效率和真实性上优于现有方法，为AI驱动的癌症诊断提供了有力工具。

Conclusion: TumorGen通过高效的三维肿瘤合成方法，显著提升了肿瘤数据的多样性和边界真实性，具有重要的临床应用价值。

Abstract: Tumor data synthesis offers a promising solution to the shortage of annotated
medical datasets. However, current approaches either limit tumor diversity by
using predefined masks or employ computationally expensive two-stage processes
with multiple denoising steps, causing computational inefficiency.
Additionally, these methods typically rely on binary masks that fail to capture
the gradual transitions characteristic of tumor boundaries. We present
TumorGen, a novel Boundary-Aware Tumor-Mask Synthesis with Rectified Flow
Matching for efficient 3D tumor synthesis with three key components: a
Boundary-Aware Pseudo Mask Generation module that replaces strict binary masks
with flexible bounding boxes; a Spatial-Constraint Vector Field Estimator that
simultaneously synthesizes tumor latents and masks using rectified flow
matching to ensure computational efficiency; and a VAE-guided mask refiner that
enhances boundary realism. TumorGen significantly improves computational
efficiency by requiring fewer sampling steps while maintaining pathological
accuracy through coarse and fine-grained spatial constraints. Experimental
results demonstrate TumorGen's superior performance over existing tumor
synthesis methods in both efficiency and realism, offering a valuable
contribution to AI-driven cancer diagnostics.

</details>


### [131] [Contrast-Invariant Self-supervised Segmentation for Quantitative Placental MRI](https://arxiv.org/abs/2505.24739)
*Xinliu Zhong,Ruiying Liu,Emily S. Nichols,Xuzhe Zhang,Andrew F. Laine,Emma G. Duerden,Yun Wang*

Main category: eess.IV

TL;DR: 提出了一种基于多回波T2*加权MRI的胎盘分割框架，通过自监督预训练和无监督域适应解决边界对比弱、缺乏标注和运动伪影问题。


<details>
  <summary>Details</summary>
Motivation: 胎盘分割在T2*加权成像中面临边界对比弱、缺乏标注和运动伪影的挑战，需要一种鲁棒的分割方法。

Method: 结合掩码自编码（MAE）进行自监督预训练，掩码伪标签（MPL）实现无监督域适应，并通过全局-局部协作对齐特征。

Result: 在临床数据集上验证，方法在多回波间泛化能力强，优于单回波和简单融合基线。

Conclusion: 首次系统利用多回波T2*加权MRI进行胎盘分割，为定量分析提供了有效工具。

Abstract: Accurate placental segmentation is essential for quantitative analysis of the
placenta. However, this task is particularly challenging in T2*-weighted
placental imaging due to: (1) weak and inconsistent boundary contrast across
individual echoes; (2) the absence of manual ground truth annotations for all
echo times; and (3) motion artifacts across echoes caused by fetal and maternal
movement. In this work, we propose a contrast-augmented segmentation framework
that leverages complementary information across multi-echo T2*-weighted MRI to
learn robust, contrast-invariant representations. Our method integrates: (i)
masked autoencoding (MAE) for self-supervised pretraining on unlabeled
multi-echo slices; (ii) masked pseudo-labeling (MPL) for unsupervised domain
adaptation across echo times; and (iii) global-local collaboration to align
fine-grained features with global anatomical context. We further introduce a
semantic matching loss to encourage representation consistency across echoes of
the same subject. Experiments on a clinical multi-echo placental MRI dataset
demonstrate that our approach generalizes effectively across echo times and
outperforms both single-echo and naive fusion baselines. To our knowledge, this
is the first work to systematically exploit multi-echo T2*-weighted MRI for
placental segmentation.

</details>


### [132] [Beyond Pretty Pictures: Combined Single- and Multi-Image Super-resolution for Sentinel-2 Images](https://arxiv.org/abs/2505.24799)
*Aditya Retnanto,Son Le,Sebastian Mueller,Armin Leitner,Konrad Schindler,Yohan Iddawela,Michael Riffler*

Main category: eess.IV

TL;DR: SEN4X是一种混合超分辨率架构，结合单图像和多图像技术，将Sentinel-2图像分辨率提升至2.5米，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Sentinel-2卫星图像分辨率较低，无法捕捉小尺度特征（如房屋、街道等），需要超分辨率技术提升其分辨率。

Method: 结合Sentinel-2的多次采集数据（时间过采样）和高分辨率Pléiades Neo数据的学习先验，构建混合超分辨率架构SEN4X。

Result: 在越南河内的城市土地覆盖分类测试中，SEN4X显著优于现有超分辨率基线方法。

Conclusion: SEN4X成功提升了Sentinel-2图像的分辨率，为小尺度特征识别提供了更高质量的数据。

Abstract: Super-resolution aims to increase the resolution of satellite images by
reconstructing high-frequency details, which go beyond na\"ive upsampling. This
has particular relevance for Earth observation missions like Sentinel-2, which
offer frequent, regular coverage at no cost; but at coarse resolution. Its
pixel footprint is too large to capture small features like houses, streets, or
hedge rows. To address this, we present SEN4X, a hybrid super-resolution
architecture that combines the advantages of single-image and multi-image
techniques. It combines temporal oversampling from repeated Sentinel-2
acquisitions with a learned prior from high-resolution Pl\'eiades Neo data. In
doing so, SEN4X upgrades Sentinel-2 imagery to 2.5 m ground sampling distance.
We test the super-resolved images on urban land-cover classification in Hanoi,
Vietnam. We find that they lead to a significant performance improvement over
state-of-the-art super-resolution baselines.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [133] [Force-Dual Modes: Subspace Design from Stochastic Forces](https://arxiv.org/abs/2505.23969)
*Otman Benchekroun,Eitan Grinspun,Maurizio Chiaramonte,Philip Allen Etter*

Main category: cs.GR

TL;DR: 提出了一种基于力分布构建降阶模型（ROM）子空间的方法，适用于动态模拟中的约束、接触和肌肉骨骼驱动等场景。


<details>
  <summary>Details</summary>
Motivation: 在动态模拟中，如何选择最优子空间是一个挑战。本文旨在通过力分布构建适应性强且高效模拟的子空间。

Method: 采用统计视角，将用户设计的力分布通过线性化模拟转化为位移分布，并拟合低秩高斯模型构建子空间。

Result: 该方法能生成适应物理材料特性和任意力分布的子空间，优于传统线性模态分析和格林函数子空间。

Conclusion: 提出的框架为动态模拟提供了灵活且高效的子空间构建方法，适用于多种场景交互。

Abstract: Designing subspaces for Reduced Order Modeling (ROM) is crucial for
accelerating finite element simulations in graphics and engineering.
Unfortunately, it's not always clear which subspace is optimal for arbitrary
dynamic simulation. We propose to construct simulation subspaces from force
distributions, allowing us to tailor such subspaces to common scene
interactions involving constraint penalties, handles-based control, contact and
musculoskeletal actuation. To achieve this we adopt a statistical perspective
on Reduced Order Modelling, which allows us to push such user-designed force
distributions through a linearized simulation to obtain a dual distribution on
displacements. To construct our subspace, we then fit a low-rank Gaussian model
to this displacement distribution, which we show generalizes Linear Modal
Analysis subspaces for uncorrelated unit variance force distributions, as well
as Green's Function subspaces for low rank force distributions. We show our
framework allows for the construction of subspaces that are optimal both with
respect to physical material properties, as well as arbitrary force
distributions as observed in handle-based, contact, and musculoskeletal scene
interactions.

</details>


### [134] [3DGEER: Exact and Efficient Volumetric Rendering with 3D Gaussians](https://arxiv.org/abs/2505.24053)
*Zixun Huang,Cho-Ying Wu,Yuliang Guo,Xinyu Huang,Liu Ren*

Main category: cs.GR

TL;DR: 3DGEER是一种精确且高效的体积高斯渲染方法，解决了3D高斯投影的近似误差问题，支持任意相机模型，并在实时神经渲染中达到新水平。


<details>
  <summary>Details</summary>
Motivation: 3D高斯投影（3DGS）在平衡渲染质量和效率方面取得进展，但其近似方法在大视场角（FoV）下限制了渲染质量。现有方法未能同时实现精确性和高效性。

Method: 从基本原理出发，推导了3D高斯分布沿射线的密度积分闭式表达式，提出粒子边界视锥（PBF）和双极等角投影（BEAP）表示，以加速射线关联和优化。

Result: 实验表明，3DGEER在多种相机模型下优于现有方法，实现了实时神经渲染的新突破。

Conclusion: 3DGEER通过精确的数学推导和高效计算，解决了3DGS的局限性，为实时神经渲染提供了新的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) marks a significant milestone in balancing the
quality and efficiency of differentiable rendering. However, its high
efficiency stems from an approximation of projecting 3D Gaussians onto the
image plane as 2D Gaussians, which inherently limits rendering
quality--particularly under large Field-of-View (FoV) camera inputs. While
several recent works have extended 3DGS to mitigate these approximation errors,
none have successfully achieved both exactness and high efficiency
simultaneously. In this work, we introduce 3DGEER, an Exact and Efficient
Volumetric Gaussian Rendering method. Starting from first principles, we derive
a closed-form expression for the density integral along a ray traversing a 3D
Gaussian distribution. This formulation enables precise forward rendering with
arbitrary camera models and supports gradient-based optimization of 3D Gaussian
parameters. To ensure both exactness and real-time performance, we propose an
efficient method for computing a tight Particle Bounding Frustum (PBF) for each
3D Gaussian, enabling accurate and efficient ray-Gaussian association. We also
introduce a novel Bipolar Equiangular Projection (BEAP) representation to
accelerate ray association under generic camera models. BEAP further provides a
more uniform ray sampling strategy to apply supervision, which empirically
improves reconstruction quality. Experiments on multiple pinhole and fisheye
datasets show that our method consistently outperforms prior methods,
establishing a new state-of-the-art in real-time neural rendering.

</details>


### [135] [Minimizing Ray Tracing Memory Traffic through Quantized Structures and Ray Stream Tracing](https://arxiv.org/abs/2505.24653)
*Moritz Grauer,Johannes Hanika,Carsten Dachsbacher*

Main category: cs.GR

TL;DR: 提出了一种内存高效的射线追踪方法，结合压缩数据结构和射线流技术，显著减少内存流量。


<details>
  <summary>Details</summary>
Motivation: 内存带宽限制是射线追踪性能的主要瓶颈，尤其是在场景复杂性增加时。

Method: 采用压缩BVH和三角形表示，结合射线流追踪，减少内存流量；使用定点算术避免几何空洞。

Result: 内存流量减少至传统方法的18%，适用于带宽受限的硬件环境。

Conclusion: 该方法有效解决了内存带宽和数值精度问题，适用于现代射线追踪应用。

Abstract: Memory bandwidth constraints continue to be a significant limiting factor in
ray tracing performance, particularly as scene complexity grows and
computational capabilities outpace memory access speeds. This paper presents a
memory-efficient ray tracing methodology that integrates compressed data
structures with ray stream techniques to reduce memory traffic. The approach
implements compressed BVH and triangle representations to minimize acceleration
structure size in combination with ray stream tracing to reduce traversal stack
memory traffic. The technique employs fixed-point arithmetic for intersection
tests for prospective hardware with tailored integer operations. Despite using
reduced precision, geometric holes are avoided by leveraging fixed-point
arithmetic instead of encountering the floating-point rounding errors common in
traditional approaches. Quantitative analysis demonstrates significant memory
traffic reduction across various scene complexities and BVH configurations. The
presented 8-wide BVH ray stream implementation reduces memory traffic to only
18% of traditional approaches by using 8-bit quantization for box and triangle
coordinates and directly ray tracing these quantized structures. These
reductions are especially beneficial for bandwidth-constrained hardware
environments such as mobile devices. This integrated approach addresses both
memory bandwidth limitations and numerical precision challenges inherent to
modern ray tracing applications.

</details>


### [136] [TC-GS: A Faster Gaussian Splatting Module Utilizing Tensor Cores](https://arxiv.org/abs/2505.24796)
*Zimu Liao,Jifeng Ding,Rong Fu,Siwei Cui,Ruixuan Gong,Li Wang,Boni Hu,Yi Wang,Hengjie Li,XIngcheng Zhang,Hui Wang*

Main category: cs.GR

TL;DR: TC-GS通过将alpha计算映射到矩阵乘法，利用Tensor Core加速3D高斯溅射渲染，提供2.18倍额外加速，总加速达5.6倍。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS渲染中，alpha混合是时间瓶颈，且Tensor Core未被充分利用。

Method: 提出TC-GS模块，将alpha计算映射为矩阵乘法，并引入全局到局部坐标变换以减少精度误差。

Result: 实验显示TC-GS在保持渲染质量的同时，提供额外2.18倍加速，总加速达5.6倍。

Conclusion: TC-GS是一种通用模块，可无缝集成现有3DGS优化框架，显著提升渲染效率。

Abstract: 3D Gaussian Splatting (3DGS) renders pixels by rasterizing Gaussian
primitives, where conditional alpha-blending dominates the time cost in the
rendering pipeline. This paper proposes TC-GS, an algorithm-independent
universal module that expands Tensor Core (TCU) applicability for 3DGS, leading
to substantial speedups and seamless integration into existing 3DGS
optimization frameworks. The key innovation lies in mapping alpha computation
to matrix multiplication, fully utilizing otherwise idle TCUs in existing 3DGS
implementations. TC-GS provides plug-and-play acceleration for existing
top-tier acceleration algorithms tightly coupled with rendering pipeline
designs, like Gaussian compression and redundancy elimination algorithms.
Additionally, we introduce a global-to-local coordinate transformation to
mitigate rounding errors from quadratic terms of pixel coordinates caused by
Tensor Core half-precision computation. Extensive experiments demonstrate that
our method maintains rendering quality while providing an additional 2.18x
speedup over existing Gaussian acceleration algorithms, thus reaching up to a
total 5.6x acceleration. The code is currently available at anonymous
\href{https://github.com/TensorCore3DGS/3DGSTensorCore}

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [137] [Real-time processing of distributed acoustic sensing data for earthquake monitoring operations](https://arxiv.org/abs/2505.24077)
*Ettore Biondi,Gabrielle Tepp,Ellen Yu,Jessie K. Saunders,Victor Yartsev,Michael Black,Michael Watkins,Aparna Bhaskaran,Rayomand Bhadha,Zhongwen Zhan,Allen L. Husker*

Main category: physics.geo-ph

TL;DR: 介绍了一个模块化软件框架，用于将分布式声学传感（DAS）数据集成到地震监测系统中。


<details>
  <summary>Details</summary>
Motivation: 利用DAS的高空间采样密度，提升地震监测系统的实时性和准确性。

Method: 基于ANSS和SCSN的基础设施，采用标准化数据格式和机器学习方法实现DAS数据的实时流处理和旅行时间拾取。

Result: 成功将100公里长的DAS阵列数据集成到AQMS中，验证了框架的可行性。

Conclusion: 该框架为DAS在地震监测中的常规应用奠定了基础，并拓展了光纤网络在实时灾害评估中的潜力。

Abstract: We introduce a modular software framework designed to integrate distributed
acoustic sensing (DAS) data into operational earthquake monitoring systems.
Building on the infrastructure of the Advanced National Seismic System (ANSS)
and the Southern California Seismic Network (SCSN), which employs the ANSS
Quake Monitoring Software (AQMS), our solution supports real-time DAS waveform
streaming and machine-learning-based traveltime picking to leverage the dense
spatial sampling of DAS arrays. To enable seamless compatibility with the AQMS,
our approach uses standardized seismic data formats to incorporate
predetermined DAS channels. We demonstrate the integration of data from a
100-km-long DAS array deployed in Ridgecrest, California, and provide a
detailed description of the software components and deployment strategy. This
work represents a step toward incorporating DAS into routine seismic monitoring
and opens new possibilities for real-time hazard assessment using fiber-optic
networks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [138] [A Benchmark Reference for ESP32-CAM Module](https://arxiv.org/abs/2505.24081)
*Sayed T. Nowroz,Nermeen M. Saleh,Siam Shakur,Sean Banerjee,Fathi Amsaad*

Main category: cs.RO

TL;DR: 研究分析了ESP32-CAM模块在不同电压条件下的实时视频流性能，修复了官方驱动中的帧率记录错误，并提供了关键性能指标。


<details>
  <summary>Details</summary>
Motivation: ESP32-CAM模块虽广泛使用，但其性能指标缺乏全面文档记录，本研究填补了这一空白。

Method: 通过HTTP WiFi连接，测试OV2640图像传感器在所有支持分辨率下的实时视频流，记录六小时日志，并分析五种不同电压条件下的性能。

Result: 修复了帧率记录错误，提供了瞬时和平均帧率、总流数据、传输计数及芯片温度等关键指标，评估了不同电压对模块可靠性的影响。

Conclusion: 研究为ESP32-CAM的性能评估提供了全面数据，有助于开发者和用户更好地理解其实际表现。

Abstract: The ESP32-CAM is one of the most widely adopted open-source modules for
prototyping embedded vision applications. Since its release in 2019, it has
gained popularity among both hobbyists and professional developers due to its
affordability, versatility, and integrated wireless capabilities. Despite its
widespread use, comprehensive documentation of the performance metrics remains
limited. This study addresses this gap by collecting and analyzing over six
hours of real-time video streaming logs across all supported resolutions of the
OV2640 image sensor, tested under five distinct voltage conditions via an
HTTP-based WiFi connection. A long standing bug in the official Arduino ESP32
driver, responsible for inaccurate frame rate logging, was fixed. The resulting
analysis includes key performance metrics such as instantaneous and average
frame rate, total streamed data, transmission count, and internal chip
temperature. The influence of varying power levels was evaluated to assess the
reliability of the module.

</details>


### [139] [SR3D: Unleashing Single-view 3D Reconstruction for Transparent and Specular Object Grasping](https://arxiv.org/abs/2505.24305)
*Mingxu Zhang,Xiaoqi Li,Jiahui Xu,Kaichen Zhou,Hojin Bae,Yan Shen,Chuyan Xiong,Jiaming Liu,Hao Dong*

Main category: cs.RO

TL;DR: SR3D是一种无需训练的单视图3D重建框架，用于透明和镜面物体的机器人抓取，通过RGB和深度图像生成3D网格并精确定位物体。


<details>
  <summary>Details</summary>
Motivation: 透明和镜面材料因深度感知限制难以抓取，现有方法复杂或信息利用不足。

Method: 利用单视图RGB和深度图像，通过外部视觉模型生成3D网格，结合2D/3D语义和几何信息精确定位物体。

Result: 在仿真和现实实验中验证了SR3D的重建有效性。

Conclusion: SR3D为透明和镜面物体的抓取提供了高效解决方案。

Abstract: Recent advancements in 3D robotic manipulation have improved grasping of
everyday objects, but transparent and specular materials remain challenging due
to depth sensing limitations. While several 3D reconstruction and depth
completion approaches address these challenges, they suffer from setup
complexity or limited observation information utilization. To address this,
leveraging the power of single view 3D object reconstruction approaches, we
propose a training free framework SR3D that enables robotic grasping of
transparent and specular objects from a single view observation. Specifically,
given single view RGB and depth images, SR3D first uses the external visual
models to generate 3D reconstructed object mesh based on RGB image. Then, the
key idea is to determine the 3D object's pose and scale to accurately localize
the reconstructed object back into its original depth corrupted 3D scene.
Therefore, we propose view matching and keypoint matching mechanisms,which
leverage both the 2D and 3D's inherent semantic and geometric information in
the observation to determine the object's 3D state within the scene, thereby
reconstructing an accurate 3D depth map for effective grasp detection.
Experiments in both simulation and real world show the reconstruction
effectiveness of SR3D.

</details>


### [140] [Black-box Adversarial Attacks on CNN-based SLAM Algorithms](https://arxiv.org/abs/2505.24654)
*Maria Rafaela Gkeka,Bowen Sun,Evgenia Smirni,Christos D. Antonopoulos,Spyros Lalis,Nikolaos Bellas*

Main category: cs.RO

TL;DR: 论文研究了针对基于CNN的SLAM系统的对抗攻击，发现即使是中等规模的攻击也会导致76%的帧跟踪失败，且攻击深度输入比RGB输入更具破坏性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在SLAM任务中取得了显著进展，但其对对抗攻击的脆弱性限制了其在自主导航等应用中的可靠部署。目前缺乏针对CNN-SLAM系统的对抗攻击的全面研究。

Method: 研究采用黑盒对抗扰动方法，针对GCN-SLAM算法的RGB输入图像进行攻击，并在TUM数据集上进行实验。

Result: 实验结果显示，中等规模的对抗攻击可导致76%的帧跟踪失败，且攻击深度输入对SLAM系统的破坏性更强。

Conclusion: 论文揭示了CNN-SLAM系统对对抗攻击的脆弱性，强调了在部署此类系统时需考虑安全性问题。

Abstract: Continuous advancements in deep learning have led to significant progress in
feature detection, resulting in enhanced accuracy in tasks like Simultaneous
Localization and Mapping (SLAM). Nevertheless, the vulnerability of deep neural
networks to adversarial attacks remains a challenge for their reliable
deployment in applications, such as navigation of autonomous agents. Even
though CNN-based SLAM algorithms are a growing area of research there is a
notable absence of a comprehensive presentation and examination of adversarial
attacks targeting CNN-based feature detectors, as part of a SLAM system. Our
work introduces black-box adversarial perturbations applied to the RGB images
fed into the GCN-SLAM algorithm. Our findings on the TUM dataset [30] reveal
that even attacks of moderate scale can lead to tracking failure in as many as
76% of the frames. Moreover, our experiments highlight the catastrophic impact
of attacking depth instead of RGB input images on the SLAM system.

</details>


### [141] [DiG-Net: Enhancing Quality of Life through Hyper-Range Dynamic Gesture Recognition in Assistive Robotics](https://arxiv.org/abs/2505.24786)
*Eran Bamani Beeri,Eden Nissinman,Avishai Sintov*

Main category: cs.RO

TL;DR: 提出了一种名为DiG-Net的新方法，用于远距离动态手势识别，显著提升了辅助机器人交互的可用性。


<details>
  <summary>Details</summary>
Motivation: 当前手势识别方法局限于短距离交互，限制了其在需要远距离辅助通信的场景中的应用。

Method: 结合了Depth-Conditioned Deformable Alignment (DADA)块和时空图模块，并引入了Radiometric Spatio-Temporal Depth Attenuation Loss (RSTDAL)以增强模型鲁棒性。

Result: 在多样化数据集上实现了97.3%的识别准确率，显著优于现有方法。

Conclusion: DiG-Net通过远距离手势识别，显著提升了辅助机器人在家庭医疗、工业安全和远程协助等场景中的可用性。

Abstract: Dynamic hand gestures play a pivotal role in assistive human-robot
interaction (HRI), facilitating intuitive, non-verbal communication,
particularly for individuals with mobility constraints or those operating
robots remotely. Current gesture recognition methods are mostly limited to
short-range interactions, reducing their utility in scenarios demanding robust
assistive communication from afar. In this paper, we introduce a novel approach
designed specifically for assistive robotics, enabling dynamic gesture
recognition at extended distances of up to 30 meters, thereby significantly
improving accessibility and quality of life. Our proposed Distance-aware
Gesture Network (DiG-Net) effectively combines Depth-Conditioned Deformable
Alignment (DADA) blocks with Spatio-Temporal Graph modules, enabling robust
processing and classification of gesture sequences captured under challenging
conditions, including significant physical attenuation, reduced resolution, and
dynamic gesture variations commonly experienced in real-world assistive
environments. We further introduce the Radiometric Spatio-Temporal Depth
Attenuation Loss (RSTDAL), shown to enhance learning and strengthen model
robustness across varying distances. Our model demonstrates significant
performance improvement over state-of-the-art gesture recognition frameworks,
achieving a recognition accuracy of 97.3% on a diverse dataset with challenging
hyper-range gestures. By effectively interpreting gestures from considerable
distances, DiG-Net significantly enhances the usability of assistive robots in
home healthcare, industrial safety, and remote assistance scenarios, enabling
seamless and intuitive interactions for users regardless of physical
limitations

</details>


### [142] [Bi-Manual Joint Camera Calibration and Scene Representation](https://arxiv.org/abs/2505.24819)
*Haozhan Tang,Tianyi Zhang,Matthew Johnson-Roberson,Weiming Zhi*

Main category: cs.RO

TL;DR: Bi-JCR框架通过3D基础模型实现多机器人相机标定，无需标记，同时构建共享工作空间的统一3D表示。


<details>
  <summary>Details</summary>
Motivation: 传统多相机标定过程繁琐，需依赖标记物，Bi-JCR旨在简化这一过程并支持后续任务。

Method: 利用3D基础模型实现无标记多视角对应，联合估计相机外参、机械臂间位姿及统一3D表示。

Result: 实验验证了Bi-JCR在多种桌面环境中的鲁棒性，并展示其在下游任务中的适用性。

Conclusion: Bi-JCR为多机器人协作提供了一种高效、无标记的标定与表示解决方案。

Abstract: Robot manipulation, especially bimanual manipulation, often requires setting
up multiple cameras on multiple robot manipulators. Before robot manipulators
can generate motion or even build representations of their environments, the
cameras rigidly mounted to the robot need to be calibrated. Camera calibration
is a cumbersome process involving collecting a set of images, with each
capturing a pre-determined marker. In this work, we introduce the Bi-Manual
Joint Calibration and Representation Framework (Bi-JCR). Bi-JCR enables
multiple robot manipulators, each with cameras mounted, to circumvent taking
images of calibration markers. By leveraging 3D foundation models for dense,
marker-free multi-view correspondence, Bi-JCR jointly estimates: (i) the
extrinsic transformation from each camera to its end-effector, (ii) the
inter-arm relative poses between manipulators, and (iii) a unified,
scale-consistent 3D representation of the shared workspace, all from the same
captured RGB image sets. The representation, jointly constructed from images
captured by cameras on both manipulators, lives in a common coordinate frame
and supports collision checking and semantic segmentation to facilitate
downstream bimanual coordination tasks. We empirically evaluate the robustness
of Bi-JCR on a variety of tabletop environments, and demonstrate its
applicability on a variety of downstream tasks.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [143] [Digital twins enable full-reference quality assessment of photoacoustic image reconstructions](https://arxiv.org/abs/2505.24514)
*Janek Gröhl,Leonid Kunyansky,Jenni Poimala,Thomas R. Else,Francesca Di Cecio,Sarah E. Bohndiek,Ben T. Cox,Andreas Hauptmann*

Main category: physics.med-ph

TL;DR: 该论文提出了一种利用数字孪生体定量评估光声图像重建算法质量的方法，并首次在实验数据上测试了一种基于傅里叶变换的重建算法。


<details>
  <summary>Details</summary>
Motivation: 光声图像重建算法的定量比较缺乏理想参考图像，数字孪生体可减少仿真与实际数据的差距。

Method: 使用数字孪生体框架比较多种重建算法，并测试基于傅里叶变换的算法。

Result: 数字孪生体有效评估了数值模型的准确性，傅里叶变换算法在计算成本更低的情况下与迭代时间反演结果相当。

Conclusion: 数字孪生体为光声图像重建算法的定量评估提供了有效工具，傅里叶变换算法具有实际应用潜力。

Abstract: Quantitative comparison of the quality of photoacoustic image reconstruction
algorithms remains a major challenge. No-reference image quality measures are
often inadequate, but full-reference measures require access to an ideal
reference image. While the ground truth is known in simulations, it is unknown
in vivo, or in phantom studies, as the reference depends on both the phantom
properties and the imaging system. We tackle this problem by using numerical
digital twins of tissue-mimicking phantoms and the imaging system to perform a
quantitative calibration to reduce the simulation gap. The contributions of
this paper are two-fold: First, we use this digital-twin framework to compare
multiple state-of-the-art reconstruction algorithms. Second, among these is a
Fourier transform-based reconstruction algorithm for circular detection
geometries, which we test on experimental data for the first time. Our results
demonstrate the usefulness of digital phantom twins by enabling assessment of
the accuracy of the numerical forward model and enabling comparison of image
reconstruction schemes with full-reference image quality assessment. We show
that the Fourier transform-based algorithm yields results comparable to those
of iterative time reversal, but at a lower computational cost. All data and
code are publicly available on Zenodo: https://doi.org/10.5281/zenodo.15388429.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [144] [Test-Time Training Done Right](https://arxiv.org/abs/2505.23884)
*Tianyuan Zhang,Sai Bi,Yicong Hong,Kai Zhang,Fujun Luan,Songlin Yang,Kalyan Sunkavalli,William T. Freeman,Hao Tan*

Main category: cs.LG

TL;DR: LaCT（大块测试时训练）通过使用极大的块更新（2K到1M令牌）显著提高了硬件利用率和状态容量，适用于多模态任务。


<details>
  <summary>Details</summary>
Motivation: 现有TTT方法在长上下文数据处理上效率低下，硬件利用率低，且不适用于非序列数据。

Method: 采用极大块更新策略（LaCT），提升非线性状态容量，并支持复杂优化器。

Result: 在多种任务中验证有效，包括14B参数AR视频扩散模型和100万令牌上下文长度的新视图合成。

Conclusion: LaCT为长上下文建模和测试时训练提供了高效解决方案，有望推动相关研究。

Abstract: Test-Time Training (TTT) models context dependencies by adapting part of the
model's weights (referred to as fast weights) during inference. This fast
weight, akin to recurrent states in RNNs, stores temporary memories of past
tokens in the current sequence. Existing TTT methods struggled to show
effectiveness in handling long-context data, due to their inefficiency on
modern GPUs. The TTT layers in many of these approaches operate with extremely
low FLOPs utilization (often <5%) because they deliberately apply small online
minibatch sizes (e.g., updating fast weights every 16 or 64 tokens). Moreover,
a small minibatch implies fine-grained block-wise causal dependencies in the
data, unsuitable for data beyond 1D ordered sequences, like sets or
N-dimensional grids such as images or videos. In contrast, we pursue the
opposite direction by using an extremely large chunk update, ranging from 2K to
1M tokens across tasks of varying modalities, which we refer to as Large Chunk
Test-Time Training (LaCT). It improves hardware utilization by orders of
magnitude, and more importantly, facilitates scaling of nonlinear state size
(up to 40% of model parameters), hence substantially improving state capacity,
all without requiring cumbersome and error-prone kernel implementations. It
also allows easy integration of sophisticated optimizers, e.g. Muon for online
updates. We validate our approach across diverse modalities and tasks,
including novel view synthesis with image set, language models, and
auto-regressive video diffusion. Our approach can scale up to 14B-parameter AR
video diffusion model on sequences up to 56K tokens. In our longest sequence
experiment, we perform novel view synthesis with 1 million context length. We
hope this work will inspire and accelerate new research in the field of
long-context modeling and test-time training. Website:
https://tianyuanzhang.com/projects/ttt-done-right

</details>


### [145] [Vision Language Models are Biased](https://arxiv.org/abs/2505.23941)
*An Vo,Khai-Nguyen Nguyen,Mohammad Reza Taesiri,Vy Tuong Dang,Anh Totti Nguyen,Daeyoung Kim*

Main category: cs.LG

TL;DR: 研究发现视觉语言模型（VLMs）在计数和识别任务中因先验知识而产生严重偏差，准确率仅为17.05%。


<details>
  <summary>Details</summary>
Motivation: 探讨先验知识如何影响视觉语言模型在标准视觉任务中的表现。

Method: 测试VLMs在7个领域的计数任务中表现，并分析文本提示对其准确率的影响。

Result: VLMs表现严重偏差，准确率低，且文本提示进一步降低其表现。

Conclusion: 研究揭示了VLMs的偏差问题，并提供了测试框架。

Abstract: Large language models (LLMs) memorize a vast amount of prior knowledge from
the Internet that help them on downstream tasks but also may notoriously sway
their outputs towards wrong or biased answers. In this work, we test how the
knowledge about popular subjects hurt the accuracy of vision language models
(VLMs) on standard, objective visual tasks of counting and identification. We
find that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a
fourth stripe has been added to a 3-stripe Adidas logo) scoring an average of
17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo)
across 7 diverse domains from animals, logos, chess, board games, optical
illusions, to patterned grids. Insert text (e.g., "Adidas") describing the
subject name into the counterfactual image further decreases VLM accuracy. The
biases in VLMs are so strong that instructing them to double-check their
results or rely exclusively on image details to answer improves counting
accuracy by only +2 points, on average. Our work presents an interesting
failure mode in VLMs and an automated framework for testing VLM biases. Code
and data are available at: vlmsarebiased.github.io.

</details>


### [146] [From Images to Signals: Are Large Vision Models Useful for Time Series Analysis?](https://arxiv.org/abs/2505.24030)
*Ziming Zhao,ChengAo Shen,Hanghang Tong,Dongjin Song,Zhigang Deng,Qingsong Wen,Jingchao Ni*

Main category: cs.LG

TL;DR: 本文探讨了大型视觉模型（LVMs）在时间序列分析中的有效性，通过实验发现LVMs在分类任务中表现良好，但在预测任务中存在局限性。


<details>
  <summary>Details</summary>
Motivation: 研究LVMs是否真正适用于时间序列分析，填补该领域的研究空白。

Method: 设计了首个系统性研究，涵盖4种LVMs、8种成像方法、18个数据集和26个基线模型，覆盖分类和预测任务。

Result: LVMs在时间序列分类中有效，但在预测任务中表现受限，且对特定LVMs和成像方法有偏好。

Conclusion: LVMs在时间序列分析中有潜力，但需进一步研究以克服预测任务的局限性。

Abstract: Transformer-based models have gained increasing attention in time series
research, driving interest in Large Language Models (LLMs) and foundation
models for time series analysis. As the field moves toward multi-modality,
Large Vision Models (LVMs) are emerging as a promising direction. In the past,
the effectiveness of Transformer and LLMs in time series has been debated. When
it comes to LVMs, a similar question arises: are LVMs truely useful for time
series analysis? To address it, we design and conduct the first principled
study involving 4 LVMs, 8 imaging methods, 18 datasets and 26 baselines across
both high-level (classification) and low-level (forecasting) tasks, with
extensive ablation analysis. Our findings indicate LVMs are indeed useful for
time series classification but face challenges in forecasting. Although
effective, the contemporary best LVM forecasters are limited to specific types
of LVMs and imaging methods, exhibit a bias toward forecasting periods, and
have limited ability to utilize long look-back windows. We hope our findings
could serve as a cornerstone for future research on LVM- and multimodal-based
solutions to different time series tasks.

</details>


### [147] [Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting](https://arxiv.org/abs/2505.24088)
*Chen Huang,Skyler Seto,Hadi Pouransari,Mehrdad Farajtabar,Raviteja Vemulapalli,Fartash Faghri,Oncel Tuzel,Barry-John Theobald,Josh Susskind*

Main category: cs.LG

TL;DR: Proxy-FDA是一种新的正则化方法，通过特征分布对齐和动态生成代理来减少微调过程中的概念遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 微调视觉基础模型时，常导致其他任务的概念遗忘问题，现有方法通过点对点匹配权重或特征来保留知识，但忽略了特征邻域结构的重要性。

Method: 提出Proxy-FDA方法，通过特征分布对齐（使用最近邻图）和动态生成代理来显式保留特征空间中的结构知识。

Result: 实验表明，Proxy-FDA显著减少了微调中的概念遗忘，并发现遗忘与分布距离度量（而非L2距离）有强相关性。

Conclusion: Proxy-FDA在多种微调设置和任务中表现优异，验证了其有效性。

Abstract: Vision foundation models pre-trained on massive data encode rich
representations of real-world concepts, which can be adapted to downstream
tasks by fine-tuning. However, fine-tuning foundation models on one task often
leads to the issue of concept forgetting on other tasks. Recent methods of
robust fine-tuning aim to mitigate forgetting of prior knowledge without
affecting the fine-tuning performance. Knowledge is often preserved by matching
the original and fine-tuned model weights or feature pairs. However, such
point-wise matching can be too strong, without explicit awareness of the
feature neighborhood structures that encode rich knowledge as well. We propose
a novel regularization method Proxy-FDA that explicitly preserves the
structural knowledge in feature space. Proxy-FDA performs Feature Distribution
Alignment (using nearest neighbor graphs) between the pre-trained and
fine-tuned feature spaces, and the alignment is further improved by informative
proxies that are generated dynamically to increase data diversity. Experiments
show that Proxy-FDA significantly reduces concept forgetting during
fine-tuning, and we find a strong correlation between forgetting and a
distributional distance metric (in comparison to L2 distance). We further
demonstrate Proxy-FDA's benefits in various fine-tuning settings (end-to-end,
few-shot and continual tuning) and across different tasks like image
classification, captioning and VQA.

</details>


### [148] [Towards Unified Modeling in Federated Multi-Task Learning via Subspace Decoupling](https://arxiv.org/abs/2505.24185)
*Yipan Wei,Yuchen Zou,Yapeng Li,Bo Du*

Main category: cs.LG

TL;DR: FedDEA是一种针对多任务联邦学习的聚合方法，通过动态识别任务相关维度并优化更新结构，有效减少任务间干扰，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦多任务学习方法无法有效聚合异构任务，导致在任务目标、标签空间和优化路径差异大的场景中效果不佳。

Method: 提出FedDEA，基于本地更新的响应强度动态识别任务相关维度，并通过重新缩放增强优化效果，实现任务级解耦聚合。

Result: 实验表明，FedDEA能轻松集成到主流联邦优化算法中，在NYUD-V2和PASCAL-Context数据集上显著提升性能。

Conclusion: FedDEA在高度异构任务场景下表现出鲁棒性和泛化能力，无需任务标签或架构修改，适用性广泛。

Abstract: Federated Multi-Task Learning (FMTL) enables multiple clients performing
heterogeneous tasks without exchanging their local data, offering broad
potential for privacy preserving multi-task collaboration. However, most
existing methods focus on building personalized models for each client and
unable to support the aggregation of multiple heterogeneous tasks into a
unified model. As a result, in real-world scenarios where task objectives,
label spaces, and optimization paths vary significantly, conventional FMTL
methods struggle to achieve effective joint training. To address this
challenge, we propose FedDEA (Federated Decoupled Aggregation), an
update-structure-aware aggregation method specifically designed for multi-task
model integration. Our method dynamically identifies task-relevant dimensions
based on the response strength of local updates and enhances their optimization
effectiveness through rescaling. This mechanism effectively suppresses
cross-task interference and enables task-level decoupled aggregation within a
unified global model. FedDEA does not rely on task labels or architectural
modifications, making it broadly applicable and deployment-friendly.
Experimental results demonstrate that it can be easily integrated into various
mainstream federated optimization algorithms and consistently delivers
significant overall performance improvements on widely used NYUD-V2 and
PASCAL-Context. These results validate the robustness and generalization
capabilities of FedDEA under highly heterogeneous task settings.

</details>


### [149] [Provably Improving Generalization of Few-Shot Models with Synthetic Data](https://arxiv.org/abs/2505.24190)
*Lan-Cuong Nguyen,Quan Nguyen-Tri,Bang Tran Khanh,Dung D. Le,Long Tran-Thanh,Khoat Than*

Main category: cs.LG

TL;DR: 论文提出了一种理论框架，用于量化合成数据与真实数据分布差异对图像分类的影响，并提出了一种基于原型的算法，显著提升了小样本分类性能。


<details>
  <summary>Details</summary>
Motivation: 小样本图像分类因标记数据稀缺而具有挑战性，合成数据虽能缓解此问题，但分布差异会导致性能下降。

Method: 开发了理论框架量化分布差异，并提出基于原型学习的算法，优化数据划分和模型训练。

Result: 实验表明，该方法在多个数据集上优于现有技术。

Conclusion: 提出的框架和算法有效缩小了真实与合成数据的分布差距，提升了小样本分类性能。

Abstract: Few-shot image classification remains challenging due to the scarcity of
labeled training examples. Augmenting them with synthetic data has emerged as a
promising way to alleviate this issue, but models trained on synthetic samples
often face performance degradation due to the inherent gap between real and
synthetic distributions. To address this limitation, we develop a theoretical
framework that quantifies the impact of such distribution discrepancies on
supervised learning, specifically in the context of image classification. More
importantly, our framework suggests practical ways to generate good synthetic
samples and to train a predictor with high generalization ability. Building
upon this framework, we propose a novel theoretical-based algorithm that
integrates prototype learning to optimize both data partitioning and model
training, effectively bridging the gap between real few-shot data and synthetic
data. Extensive experiments results show that our approach demonstrates
superior performance compared to state-of-the-art methods, outperforming them
across multiple datasets.

</details>


### [150] [Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning](https://arxiv.org/abs/2505.24424)
*Amit Peleg,Naman Deep Singh,Matthias Hein*

Main category: cs.LG

TL;DR: CLIC是一种基于多图像及其关联标题的新型微调方法，旨在提升CLIP模型的组合推理能力，同时在检索性能上取得一致提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（如CLIP）在组合推理能力上表现不足，且改进组合性的尝试往往忽视了语义理解，甚至导致检索性能下降。

Method: 提出CLIC方法，通过结合多图像及其关联标题进行微调，提升模型的组合推理能力。

Result: CLIC在多种架构和预训练CLIP模型上均提升了组合性（包括词汇和语义理解），并在检索性能上取得一致提升。

Conclusion: CLIC是当前SugarCrepe++基准上组合性最佳的CLIP模型，且能显著提升检索性能。

Abstract: Vision-language models like CLIP have demonstrated remarkable zero-shot
capabilities in classification and retrieval. However, these models often
struggle with compositional reasoning - the ability to understand the
relationships between concepts. A recent benchmark, SugarCrepe++, reveals that
previous works on improving compositionality have mainly improved lexical
sensitivity but neglected semantic understanding. In addition, downstream
retrieval performance often deteriorates, although one would expect that
improving compositionality should enhance retrieval. In this work, we introduce
CLIC (Compositionally-aware Learning in CLIP), a fine-tuning method based on a
novel training technique combining multiple images and their associated
captions. CLIC improves compositionality across architectures as well as
differently pre-trained CLIP models, both in terms of lexical and semantic
understanding, and achieves consistent gains in retrieval performance. This
even applies to the recent CLIPS, which achieves SOTA retrieval performance.
Nevertheless, the short fine-tuning with CLIC leads to an improvement in
retrieval and to the best compositional CLIP model on SugarCrepe++. All our
models and code are available at https://clic-compositional-clip.github.io

</details>


### [151] [Graph Flow Matching: Enhancing Image Generation with Neighbor-Aware Flow Fields](https://arxiv.org/abs/2505.24434)
*Md Shahriar Rahim Siddiqui,Moshe Eliasof,Eldad Haber*

Main category: cs.LG

TL;DR: 论文提出了一种名为Graph Flow Matching（GFM）的方法，通过引入图神经网络模块增强流匹配模型的性能，从而提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的流匹配网络通常独立预测每个点的速度，忽略了相邻点之间的相关性，可能导致生成质量受限。

Method: GFM将学习到的速度分解为反应项（标准流匹配网络）和扩散项（通过图神经网络聚合邻居信息），以低成本引入局部上下文信息。

Result: 在多个图像生成基准测试中，GFM显著提升了FID和召回率。

Conclusion: GFM是一种轻量级且有效的模块化增强方法，适用于现有流匹配架构。

Abstract: Flow matching casts sample generation as learning a continuous-time velocity
field that transports noise to data. Existing flow matching networks typically
predict each point's velocity independently, considering only its location and
time along its flow trajectory, and ignoring neighboring points. However, this
pointwise approach may overlook correlations between points along the
generation trajectory that could enhance velocity predictions, thereby
improving downstream generation quality. To address this, we propose Graph Flow
Matching (GFM), a lightweight enhancement that decomposes the learned velocity
into a reaction term -- any standard flow matching network -- and a diffusion
term that aggregates neighbor information via a graph neural module. This
reaction-diffusion formulation retains the scalability of deep flow models
while enriching velocity predictions with local context, all at minimal
additional computational cost. Operating in the latent space of a pretrained
variational autoencoder, GFM consistently improves Fr\'echet Inception Distance
(FID) and recall across five image generation benchmarks (LSUN Church, LSUN
Bedroom, FFHQ, AFHQ-Cat, and CelebA-HQ at $256\times256$), demonstrating its
effectiveness as a modular enhancement to existing flow matching architectures.

</details>


### [152] [Hyperbolic Dataset Distillation](https://arxiv.org/abs/2505.24623)
*Wenyuan Li,Guang Li,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.LG

TL;DR: 论文提出了一种基于双曲空间的数据集蒸馏方法HDD，通过将数据嵌入双曲空间并优化其中心点的双曲距离，显著提升了数据集蒸馏的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有分布匹配方法在欧几里得空间中忽略数据复杂几何和层次关系的问题。

Method: 将浅层网络提取的特征嵌入洛伦兹双曲空间，通过优化合成数据与原始数据的双曲距离来整合层次结构。

Result: HDD在保持模型性能的同时，仅需20%的核心集即可实现高效训练，且显著提升训练稳定性。

Conclusion: HDD是一种高效且兼容性强的数据集蒸馏方法，适用于多种数据集。

Abstract: To address the computational and storage challenges posed by large-scale
datasets in deep learning, dataset distillation has been proposed to synthesize
a compact dataset that replaces the original while maintaining comparable model
performance. Unlike optimization-based approaches that require costly bi-level
optimization, distribution matching (DM) methods improve efficiency by aligning
the distributions of synthetic and original data, thereby eliminating nested
optimization. DM achieves high computational efficiency and has emerged as a
promising solution. However, existing DM methods, constrained to Euclidean
space, treat data as independent and identically distributed points,
overlooking complex geometric and hierarchical relationships. To overcome this
limitation, we propose a novel hyperbolic dataset distillation method, termed
HDD. Hyperbolic space, characterized by negative curvature and exponential
volume growth with distance, naturally models hierarchical and tree-like
structures. HDD embeds features extracted by a shallow network into the Lorentz
hyperbolic space, where the discrepancy between synthetic and original data is
measured by the hyperbolic (geodesic) distance between their centroids. By
optimizing this distance, the hierarchical structure is explicitly integrated
into the distillation process, guiding synthetic samples to gravitate towards
the root-centric regions of the original data distribution while preserving
their underlying geometric characteristics. Furthermore, we find that pruning
in hyperbolic space requires only 20% of the distilled core set to retain model
performance, while significantly improving training stability. Notably, HDD is
seamlessly compatible with most existing DM methods, and extensive experiments
on different datasets validate its effectiveness.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [153] [Conformal Object Detection by Sequential Risk Control](https://arxiv.org/abs/2505.24038)
*Léo Andéol,Luca Mossina,Adrien Mazoyer,Sébastien Gerchinovitz*

Main category: stat.ML

TL;DR: 论文提出了一种基于Conformal Prediction的Conformal Object Detection (COD)方法，通过Sequential Conformal Risk Control (SeqCRC)为对象检测模型提供统计保证，解决了神经网络可靠性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前对象检测模型在关键应用中的部署受到神经网络可靠性不足和模型结构复杂性的限制。

Method: 提出SeqCRC方法，扩展Conformal Risk Control (CRC)到两个顺序任务，并设计适用于不同应用和认证需求的损失函数和预测集。

Result: 通过实验验证了方法的有效性，并提供了基准测试，展示了方法的权衡和实践影响。

Conclusion: SeqCRC为对象检测提供了统计保证，解决了可靠性问题，并提供了实用的工具包和实验结果。

Abstract: Recent advances in object detectors have led to their adoption for industrial
uses. However, their deployment in critical applications is hindered by the
inherent lack of reliability of neural networks and the complex structure of
object detection models. To address these challenges, we turn to Conformal
Prediction, a post-hoc procedure which offers statistical guarantees that are
valid for any dataset size, without requiring prior knowledge on the model or
data distribution. Our contribution is manifold: first, we formally define the
problem of Conformal Object Detection (COD) and introduce a novel method,
Sequential Conformal Risk Control (SeqCRC), that extends the statistical
guarantees of Conformal Risk Control (CRC) to two sequential tasks with two
parameters, as required in the COD setting. Then, we propose loss functions and
prediction sets suited to applying CRC to different applications and
certification requirements. Finally, we present a conformal toolkit, enabling
replication and further exploration of our methods. Using this toolkit, we
perform extensive experiments, yielding a benchmark that validates the
investigated methods and emphasizes trade-offs and other practical
consequences.

</details>


### [154] [A Mathematical Perspective On Contrastive Learning](https://arxiv.org/abs/2505.24134)
*Ricardo Baptista,Andrew M. Stuart,Son Tran*

Main category: stat.ML

TL;DR: 本文提出了一种基于概率视角的多模态对比学习框架，将其视为条件概率分布的优化问题，并探讨了损失函数和度量方法的扩展。


<details>
  <summary>Details</summary>
Motivation: 传统多模态对比学习通常关注模态间的表示对齐，但缺乏概率解释。本文旨在通过概率框架重新定义对比学习，以支持跨模态检索、分类和生成任务。

Method: 将对比学习视为条件概率分布的优化问题，引入新的概率损失函数和度量方法，并在多元高斯设置下研究其性质。

Result: 提出了新的对比学习算法变体，适用于特定任务（如模式搜索和生成），并通过实验验证了框架的有效性。

Conclusion: 概率视角为多模态对比学习提供了更灵活的理论基础，支持更广泛的任务和应用。

Abstract: Multimodal contrastive learning is a methodology for linking different data
modalities; the canonical example is linking image and text data. The
methodology is typically framed as the identification of a set of encoders, one
for each modality, that align representations within a common latent space. In
this work, we focus on the bimodal setting and interpret contrastive learning
as the optimization of (parameterized) encoders that define conditional
probability distributions, for each modality conditioned on the other,
consistent with the available data. This provides a framework for multimodal
algorithms such as crossmodal retrieval, which identifies the mode of one of
these conditional distributions, and crossmodal classification, which is
similar to retrieval but includes a fine-tuning step to make it task specific.
  The framework we adopt also gives rise to crossmodal generative models. This
probabilistic perspective suggests two natural generalizations of contrastive
learning: the introduction of novel probabilistic loss functions, and the use
of alternative metrics for measuring alignment in the common latent space. We
study these generalizations of the classical approach in the multivariate
Gaussian setting. In this context we view the latent space identification as a
low-rank matrix approximation problem. This allows us to characterize the
capabilities of loss functions and alignment metrics to approximate natural
statistics, such as conditional means and covariances; doing so yields novel
variants on contrastive learning algorithms for specific mode-seeking and for
generative tasks. The framework we introduce is also studied through numerical
experiments on multivariate Gaussians, the labeled MNIST dataset, and on a data
assimilation application arising in oceanography.

</details>


### [155] [Efficient Estimation of Regularized Tyler's M-Estimator Using Approximate LOOCV](https://arxiv.org/abs/2505.24781)
*Karim Abou-Moustafa*

Main category: stat.ML

TL;DR: 论文提出了一种高效估计正则化参数α的方法，通过近似留一交叉验证（LOOCV）对数似然损失，显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决正则化Tyler M估计器（RTME）中正则化参数α估计的高计算成本问题。

Method: 提出一种近似LOOCV对数似然损失的快速计算方法，避免重复调用RTME过程。

Result: 在合成和真实高维数据上验证了方法的效率和准确性，优于现有方法。

Conclusion: 该方法高效且准确，适用于高维数据中的正则化参数估计。

Abstract: We consider the problem of estimating a regularization parameter, or a
shrinkage coefficient $\alpha \in (0,1)$ for Regularized Tyler's M-estimator
(RTME). In particular, we propose to estimate an optimal shrinkage coefficient
by setting $\alpha$ as the solution to a suitably chosen objective function;
namely the leave-one-out cross-validated (LOOCV) log-likelihood loss. Since
LOOCV is computationally prohibitive even for moderate sample size $n$, we
propose a computationally efficient approximation for the LOOCV log-likelihood
loss that eliminates the need for invoking the RTME procedure $n$ times for
each sample left out during the LOOCV procedure. This approximation yields an
$O(n)$ reduction in the running time complexity for the LOOCV procedure, which
results in a significant speedup for computing the LOOCV estimate. We
demonstrate the efficiency and accuracy of the proposed approach on synthetic
high-dimensional data sampled from heavy-tailed elliptical distributions, as
well as on real high-dimensional datasets for object recognition, face
recognition, and handwritten digit's recognition. Our experiments show that the
proposed approach is efficient and consistently more accurate than other
methods in the literature for shrinkage coefficient estimation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [156] [PatchDEMUX: A Certifiably Robust Framework for Multi-label Classifiers Against Adversarial Patches](https://arxiv.org/abs/2505.24703)
*Dennis Jacob,Chong Xiang,Prateek Mittal*

Main category: cs.CR

TL;DR: PatchDEMUX是一个可证明鲁棒的多标签分类框架，通过将多标签任务分解为多个二分类问题，扩展了单标签分类的防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有的可证明防御方法主要针对单标签分类，而多标签分类的防御研究较少，PatchDEMUX填补了这一空白。

Method: 将多标签分类任务分解为多个独立的二分类问题，并基于单标签防御方法（如PatchCleanser）扩展，同时针对单补丁攻击提出更紧的鲁棒性边界。

Result: 在MS-COCO和PASCAL VOC数据集上实现了显著的鲁棒性，同时保持了较高的干净性能。

Conclusion: PatchDEMUX为多标签分类提供了可证明的鲁棒防御，扩展了现有单标签防御方法的适用性。

Abstract: Deep learning techniques have enabled vast improvements in computer vision
technologies. Nevertheless, these models are vulnerable to adversarial patch
attacks which catastrophically impair performance. The physically realizable
nature of these attacks calls for certifiable defenses, which feature provable
guarantees on robustness. While certifiable defenses have been successfully
applied to single-label classification, limited work has been done for
multi-label classification. In this work, we present PatchDEMUX, a certifiably
robust framework for multi-label classifiers against adversarial patches. Our
approach is a generalizable method which can extend any existing certifiable
defense for single-label classification; this is done by considering the
multi-label classification task as a series of isolated binary classification
problems to provably guarantee robustness. Furthermore, in the scenario where
an attacker is limited to a single patch we propose an additional certification
procedure that can provide tighter robustness bounds. Using the current
state-of-the-art (SOTA) single-label certifiable defense PatchCleanser as a
backbone, we find that PatchDEMUX can achieve non-trivial robustness on the
MS-COCO and PASCAL VOC datasets while maintaining high clean performance

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [157] [Ultrafast High-Flux Single-Photon LiDAR Simulator via Neural Mapping](https://arxiv.org/abs/2505.23992)
*Weijian Zhang,Hashan K. Weerasooriya,Stanley Chan*

Main category: eess.SP

TL;DR: 提出了一种基于学习的框架，通过自动编码器（AE）建模光子计数并直接预测光子注册概率密度函数（PDF），显著加速单光子LiDAR（SPL）的仿真过程。


<details>
  <summary>Details</summary>
Motivation: 在单光子LiDAR（SPL）中，硬件死区时间会显著扭曲光子测量，传统方法计算量大且效率低。

Method: 使用自动编码器（AE）建模光子计数并预测光子注册概率密度函数（PDF）。

Result: 方法在估计注册光子总数及其时间分布方面具有高准确性，同时大幅减少仿真时间。

Conclusion: 该方法为高流量条件下的数据密集型成像任务提供了快速且准确的SPL仿真解决方案。

Abstract: Efficient simulation of photon registrations in single-photon LiDAR (SPL) is
essential for applications such as depth estimation under high-flux conditions,
where hardware dead time significantly distorts photon measurements. However,
the conventional wisdom is computationally intensive due to their inherently
sequential, photon-by-photon processing. In this paper, we propose a
learning-based framework that accelerates the simulation process by modeling
the photon count and directly predicting the photon registration probability
density function (PDF) using an autoencoder (AE). Our method achieves high
accuracy in estimating both the total number of registered photons and their
temporal distribution, while substantially reducing simulation time. Extensive
experiments validate the effectiveness and efficiency of our approach,
highlighting its potential to enable fast and accurate SPL simulations for
data-intensive imaging tasks in the high-flux regime.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [158] [My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals](https://arxiv.org/abs/2505.23798)
*Jian Lan,Yifei Fu,Udo Schlegel,Gengyuan Zhang,Tanveer Hannan,Haokun Chen,Thomas Seidl*

Main category: cs.CL

TL;DR: 该论文研究了大型视觉语言模型（VLMs）中的社会偏见问题，提出了一种无需训练、模型无关的后处理方法，通过调整隐藏层残差来减少偏见。


<details>
  <summary>Details</summary>
Motivation: 社会偏见是VLMs中的关键问题，可能导致对某些社会群体的不公平和伦理问题。目前尚不清楚VLMs在生成响应中产生社会偏见的程度。

Method: 首先评估了四种最先进的VLMs在PAIRS和SocialCounterfactuals数据集上的表现，发现模型存在性别和种族偏见。随后提出了一种后处理方法，通过调整隐藏层残差来减少偏见。

Result: 研究发现VLMs的隐藏层在公平性水平上存在显著波动，某些残差对公平性有积极影响，而另一些则增加偏见。提出的后处理方法优于竞争性训练策略，显著提高了公平性和置信度可靠性。

Conclusion: 通过调整隐藏层残差的后处理方法能有效减少VLMs的社会偏见，提高公平性和置信度可靠性。

Abstract: Social bias is a critical issue in large vision-language models (VLMs), where
fairness- and ethics-related problems harm certain groups of people in society.
It is unknown to what extent VLMs yield social bias in generative responses. In
this study, we focus on evaluating and mitigating social bias on both the
model's response and probability distribution. To do so, we first evaluate four
state-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets with the
multiple-choice selection task. Surprisingly, we find that models suffer from
generating gender-biased or race-biased responses. We also observe that models
are prone to stating their responses are fair, but indeed having mis-calibrated
confidence levels towards particular social groups. While investigating why
VLMs are unfair in this study, we observe that VLMs' hidden layers exhibit
substantial fluctuations in fairness levels. Meanwhile, residuals in each layer
show mixed effects on fairness, with some contributing positively while some
lead to increased bias. Based on these findings, we propose a post-hoc method
for the inference stage to mitigate social bias, which is training-free and
model-agnostic. We achieve this by ablating bias-associated residuals while
amplifying fairness-associated residuals on model hidden layers during
inference. We demonstrate that our post-hoc method outperforms the competing
training strategies, helping VLMs have fairer responses and more reliable
confidence levels.

</details>


### [159] [Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models](https://arxiv.org/abs/2505.24164)
*Shilin Xu,Yanwei Li,Rui Yang,Tao Zhang,Yueyi Sun,Wei Chow,Linfeng Li,Hang Song,Qi Xu,Yunhai Tong,Xiangtai Li,Hao Fei*

Main category: cs.CL

TL;DR: 论文提出Mixed-R1框架，通过混合奖励函数设计（Mixed-Reward）和混合后训练数据集（Mixed-45K），解决多源MLLM任务的稳定强化学习问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究多专注于单一任务（如数学问题或图表分析），缺乏多源MLLM任务的统一解决方案。

Method: 设计数据引擎构建Mixed-45K数据集，提出包含四种奖励函数的Mixed-Reward设计，并引入新的开放奖励BMAS。

Result: 实验证明方法在多种MLLM（如Qwen2.5-VL和Intern-VL）上有效。

Conclusion: Mixed-R1为多源MLLM任务提供了一种统一的强化学习解决方案。

Abstract: Recent works on large language models (LLMs) have successfully demonstrated
the emergence of reasoning capabilities via reinforcement learning (RL).
Although recent efforts leverage group relative policy optimization (GRPO) for
MLLMs post-training, they constantly explore one specific aspect, such as
grounding tasks, math problems, or chart analysis. There are no works that can
leverage multi-source MLLM tasks for stable reinforcement learning. In this
work, we present a unified perspective to solve this problem. We present
Mixed-R1, a unified yet straightforward framework that contains a mixed reward
function design (Mixed-Reward) and a mixed post-training dataset (Mixed-45K).
We first design a data engine to select high-quality examples to build the
Mixed-45K post-training dataset. Then, we present a Mixed-Reward design, which
contains various reward functions for various MLLM tasks. In particular, it has
four different reward functions: matching reward for binary answer or
multiple-choice problems, chart reward for chart-aware datasets, IoU reward for
grounding problems, and open-ended reward for long-form text responses such as
caption datasets. To handle the various long-form text content, we propose a
new open-ended reward named Bidirectional Max-Average Similarity (BMAS) by
leveraging tokenizer embedding matching between the generated response and the
ground truth. Extensive experiments show the effectiveness of our proposed
method on various MLLMs, including Qwen2.5-VL and Intern-VL on various sizes.
Our dataset and model are available at https://github.com/xushilin1/mixed-r1.

</details>


### [160] [LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text](https://arxiv.org/abs/2505.24826)
*Li yunhan,Wu gengshen*

Main category: cs.CL

TL;DR: 论文提出了一种评估法律文本质量的方法，分析了49个大型语言模型，发现模型质量在140亿参数后趋于稳定，推理模型表现优于基础架构，并发布了Qwen3系列作为性价比最优选择。


<details>
  <summary>Details</summary>
Motivation: 当前法律领域的大型语言模型评估主要关注事实准确性，而忽视了清晰度、连贯性和术语等语言质量方面。

Method: 1. 开发回归模型评估法律文本质量；2. 创建专门的法律问题集；3. 使用该框架分析49个LLM。

Result: 1. 模型质量在140亿参数后趋于稳定；2. 工程选择（如量化和上下文长度）影响可忽略；3. 推理模型表现优于基础架构。

Conclusion: 研究建立了法律LLM的标准化评估协议，揭示了当前训练数据优化方法的局限性，并发布了Qwen3系列作为性价比最优选择。

Abstract: As large language models (LLMs) are increasingly used in legal applications,
current evaluation benchmarks tend to focus mainly on factual accuracy while
largely neglecting important linguistic quality aspects such as clarity,
coherence, and terminology. To address this gap, we propose three steps: First,
we develop a regression model to evaluate the quality of legal texts based on
clarity, coherence, and terminology. Second, we create a specialized set of
legal questions. Third, we analyze 49 LLMs using this evaluation framework.
  Our analysis identifies three key findings: First, model quality levels off
at 14 billion parameters, with only a marginal improvement of $2.7\%$ noted at
72 billion parameters. Second, engineering choices such as quantization and
context length have a negligible impact, as indicated by statistical
significance thresholds above 0.016. Third, reasoning models consistently
outperform base architectures. A significant outcome of our research is the
release of a ranking list and Pareto analysis, which highlight the Qwen3 series
as the optimal choice for cost-performance tradeoffs. This work not only
establishes standardized evaluation protocols for legal LLMs but also uncovers
fundamental limitations in current training data refinement approaches. Code
and models are available at: https://github.com/lyxx3rd/LegalEval-Q.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [161] [Exploring Domain Wall Pinning in Ferroelectrics via Automated High Throughput AFM](https://arxiv.org/abs/2505.24062)
*Kamyar Barakati,Yu Liu,Hiroshi Funakubo,Sergei V. Kalinin*

Main category: cond-mat.mtrl-sci

TL;DR: 论文研究了铁电材料中畴壁动力学的空间依赖性，通过机器学习控制的自动化压电力显微镜技术，量化了电场驱动的极应变畴结构动态行为。


<details>
  <summary>Details</summary>
Motivation: 畴壁动力学受局部微结构影响显著，传统方法效率低，需自动化高分辨率研究。

Method: 使用ML控制的自动化压电力显微镜技术，分析1500个开关事件，研究畴壁位移与电场参数及局部铁电-铁弹构型的关系。

Result: 发现畴壁位移不仅依赖电场参数，还与局部畴构型相关，如双晶界在特定偏压下保持稳定。

Conclusion: 通过自动化工作流程和统计分析，建立了畴构型与脉冲参数的预测映射，为铁电存储器设计奠定基础。

Abstract: Domain-wall dynamics in ferroelectric materials are strongly
position-dependent since each polar interface is locked into a unique local
microstructure. This necessitates spatially resolved studies of the
wall-pinning using scanning-probe microscopy techniques. The pinning centers
and preexisting domain walls are usually sparse within image plane, precluding
the use of dense hyperspectral imaging modes and requiring time-consuming human
experimentation. Here, a large area epitaxial PbTiO$_3$ film on cubic KTaO$_3$
were investigated to quantify the electric field driven dynamics of the
polar-strain domain structures using ML-controlled automated Piezoresponse
Force Microscopy. Analysis of 1500 switching events reveals that domain wall
displacement depends not only on field parameters but also on the local
ferroelectric-ferroelastic configuration. For example, twin boundaries in
polydomains regions like a$_1^-$/$c^+$ $\parallel$ a$_2^-$/$c^-$ stay pinned up
to a certain level of bias magnitude and change only marginally as the bias
increases from 20V to 30V, whereas single variant boundaries like a$_2^+$/$c^+$
$\parallel$ a$_2^-$/$c^-$ stack are already activated at 20V. These statistics
on the possible ferroelectric and ferroelastic wall orientations, together with
the automated, high-throughput AFM workflow, can be distilled into a predictive
map that links domain configurations to pulse parameters. This
microstructure-specific rule set forms the foundation for designing
ferroelectric memories.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [162] [Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and Benchmarking Multimodal LLM Agents](https://arxiv.org/abs/2505.24878)
*Yaxin Luo,Zhaoyi Li,Jiacheng Liu,Jiacheng Cui,Xiaohan Zhao,Zhiqiang Shen*

Main category: cs.AI

TL;DR: Open CaptchaWorld是一个基于网络的基准测试平台，用于评估多模态LLM代理在动态CAPTCHA任务中的表现，发现其性能远低于人类。


<details>
  <summary>Details</summary>
Motivation: CAPTCHAs是网络代理在现实应用中的瓶颈，而现有多模态LLM代理在动态交互任务中的能力尚未充分测试。

Method: 开发了包含20种CAPTCHA类型、共225个谜题的Open CaptchaWorld平台，并提出CAPTCHA Reasoning Depth指标量化解决步骤。

Result: 人类表现接近完美（93.3%），而最佳MLLM代理成功率仅40.0%，显著落后。

Conclusion: Open CaptchaWorld揭示了当前多模态代理的局限性，为开发更强大的推理系统提供了方向。

Abstract: CAPTCHAs have been a critical bottleneck for deploying web agents in
real-world applications, often blocking them from completing end-to-end
automation tasks. While modern multimodal LLM agents have demonstrated
impressive performance in static perception tasks, their ability to handle
interactive, multi-step reasoning challenges like CAPTCHAs is largely untested.
To address this gap, we introduce Open CaptchaWorld, the first web-based
benchmark and platform specifically designed to evaluate the visual reasoning
and interaction capabilities of MLLM-powered agents through diverse and dynamic
CAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225
CAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,
which quantifies the number of cognitive and motor steps required to solve each
puzzle. Experimental results show that humans consistently achieve near-perfect
scores, state-of-the-art MLLM agents struggle significantly, with success rates
at most 40.0% by Browser-Use Openai-o3, far below human-level performance,
93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing
the limits of current multimodal agents and guiding the development of more
robust multimodal reasoning systems. Code and Data are available at this https
URL.

</details>
