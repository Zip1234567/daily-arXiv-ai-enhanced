<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 117]
- [eess.IV](#eess.IV) [Total: 22]
- [cs.GR](#cs.GR) [Total: 8]
- [physics.geo-ph](#physics.geo-ph) [Total: 7]
- [cs.AI](#cs.AI) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.CG](#cs.CG) [Total: 2]
- [astro-ph.EP](#astro-ph.EP) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [physics.ins-det](#physics.ins-det) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Strong View-Free Baseline Approach for Single-View Image Guided Point Cloud Completion](https://arxiv.org/abs/2506.15747)
*Fangzhou Lin,Zilin Dai,Rigved Sanku,Songlin Hou,Kazunori D Yamada,Haichong K. Zhang,Ziming Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于注意力机制的多分支编码器-解码器网络，用于单视图图像引导点云补全任务，仅使用部分点云输入，无需图像引导。实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索图像引导在点云补全任务中的必要性，并提出一种无需图像输入的高效方法。

Method: 采用注意力机制的多分支编码器-解码器网络，结合跨注意力和自注意力层，通过层次化自融合机制整合信息。

Result: 在ShapeNet-ViPC数据集上的实验表明，该方法优于现有单视图图像引导方法。

Conclusion: 该方法为多模态学习在点云补全任务中的应用提供了新思路，代码已开源。

Abstract: The single-view image guided point cloud completion (SVIPC) task aims to
reconstruct a complete point cloud from a partial input with the help of a
single-view image. While previous works have demonstrated the effectiveness of
this multimodal approach, the fundamental necessity of image guidance remains
largely unexamined. To explore this, we propose a strong baseline approach for
SVIPC based on an attention-based multi-branch encoder-decoder network that
only takes partial point clouds as input, view-free. Our hierarchical
self-fusion mechanism, driven by cross-attention and self-attention layers,
effectively integrates information across multiple streams, enriching feature
representations and strengthening the networks ability to capture geometric
structures. Extensive experiments and ablation studies on the ShapeNet-ViPC
dataset demonstrate that our view-free framework performs superiorly to
state-of-the-art SVIPC methods. We hope our findings provide new insights into
the development of multimodal learning in SVIPC. Our demo code will be
available at https://github.com/Zhang-VISLab.

</details>


### [2] [VLMInferSlow: Evaluating the Efficiency Robustness of Large Vision-Language Models as a Service](https://arxiv.org/abs/2506.15755)
*Xiasi Wang,Tianliang Yao,Simin Chen,Runqi Wang,Lei YE,Kuofeng Gao,Yi Huang,Yuan Yao*

Main category: cs.CV

TL;DR: 提出了一种名为VLMInferSlow的新方法，用于在现实黑盒设置中评估视觉语言模型（VLM）的效率鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注VLM的准确性，而效率鲁棒性在实时应用和高推理开销的背景下至关重要。然而，以往研究在评估效率鲁棒性时依赖不切实际的假设（如需要模型架构和参数），这在ML-as-a-service场景中不适用。

Method: VLMInferSlow结合了针对VLM推理的细粒度效率建模，并利用零阶优化搜索对抗样本。

Result: 实验表明，VLMInferSlow生成的对抗图像具有不可察觉的扰动，可将计算成本增加高达128.47%。

Conclusion: 该研究旨在提高社区对VLM效率鲁棒性的关注。

Abstract: Vision-Language Models (VLMs) have demonstrated great potential in real-world
applications. While existing research primarily focuses on improving their
accuracy, the efficiency remains underexplored. Given the real-time demands of
many applications and the high inference overhead of VLMs, efficiency
robustness is a critical issue. However, previous studies evaluate efficiency
robustness under unrealistic assumptions, requiring access to the model
architecture and parameters -- an impractical scenario in ML-as-a-service
settings, where VLMs are deployed via inference APIs. To address this gap, we
propose VLMInferSlow, a novel approach for evaluating VLM efficiency robustness
in a realistic black-box setting. VLMInferSlow incorporates fine-grained
efficiency modeling tailored to VLM inference and leverages zero-order
optimization to search for adversarial examples. Experimental results show that
VLMInferSlow generates adversarial images with imperceptible perturbations,
increasing the computational cost by up to 128.47%. We hope this research
raises the community's awareness about the efficiency robustness of VLMs.

</details>


### [3] [Weakly-supervised VLM-guided Partial Contrastive Learning for Visual Language Navigation](https://arxiv.org/abs/2506.15757)
*Ruoyu Wang,Tong Yu,Junda Wu,Yao Liu,Julian McAuley,Lina Yao*

Main category: cs.CV

TL;DR: 论文提出了一种弱监督部分对比学习（WPCL）方法，用于提升视觉语言导航（VLN）任务中代理的动态视角物体识别能力，无需微调视觉语言模型（VLM）。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练模型，但动态视角和领域知识不足限制了性能，且微调计算成本高。

Method: 提出WPCL方法，有效整合预训练VLM知识，提升代理的环境感知能力，同时保持计算效率。

Result: 实验表明，WPCL在多个基准测试中优于基线方法，验证了其有效性、鲁棒性和泛化性。

Conclusion: WPCL为VLN任务提供了一种高效且性能优越的解决方案。

Abstract: Visual Language Navigation (VLN) is a fundamental task within the field of
Embodied AI, focusing on the ability of agents to navigate complex environments
based on natural language instructions. Despite the progress made by existing
methods, these methods often present some common challenges. First, they rely
on pre-trained backbone models for visual perception, which struggle with the
dynamic viewpoints in VLN scenarios. Second, the performance is limited when
using pre-trained LLMs or VLMs without fine-tuning, due to the absence of VLN
domain knowledge. Third, while fine-tuning LLMs and VLMs can improve results,
their computational costs are higher than those without fine-tuning. To address
these limitations, we propose Weakly-supervised Partial Contrastive Learning
(WPCL), a method that enhances an agent's ability to identify objects from
dynamic viewpoints in VLN scenarios by effectively integrating pre-trained VLM
knowledge into the perception process, without requiring VLM fine-tuning. Our
method enhances the agent's ability to interpret and respond to environmental
cues while ensuring computational efficiency. Experimental results have shown
that our method outperforms the baseline methods on multiple benchmarks, which
validate the effectiveness, robustness and generalizability of our method.

</details>


### [4] [Implicit 3D scene reconstruction using deep learning towards efficient collision understanding in autonomous driving](https://arxiv.org/abs/2506.15806)
*Akarshani Ramanayake,Nihal Kodikara*

Main category: cs.CV

TL;DR: 论文提出了一种基于学习的方法，利用LiDAR数据和深度神经网络构建静态SDF地图，以解决密集交通环境中3D场景重建的边界精度问题。


<details>
  <summary>Details</summary>
Motivation: 当前技术在密集城市交通环境中难以实现精确导航，而3D场景重建的边界精度问题尚未完全解决。

Method: 使用LiDAR数据和深度神经网络构建静态SDF地图，替代传统的多边形表示方法。

Result: 初步结果表明，该方法能显著提升碰撞检测性能，尤其是在拥挤和动态环境中。

Conclusion: 该方法为密集交通环境中的3D场景重建提供了更高效的解决方案。

Abstract: In crowded urban environments where traffic is dense, current technologies
struggle to oversee tight navigation, but surface-level understanding allows
autonomous vehicles to safely assess proximity to surrounding obstacles. 3D or
2D scene mapping of the surrounding objects is an essential task in addressing
the above problem. Despite its importance in dense vehicle traffic conditions,
3D scene reconstruction of object shapes with higher boundary level accuracy is
not yet entirely considered in current literature. The sign distance function
represents any shape through parameters that calculate the distance from any
point in space to the closest obstacle surface, making it more efficient in
terms of storage. In recent studies, researchers have started to formulate
problems with Implicit 3D reconstruction methods in the autonomous driving
domain, highlighting the possibility of using sign distance function to map
obstacles effectively. This research addresses this gap by developing a
learning-based 3D scene reconstruction methodology that leverages LiDAR data
and a deep neural network to build a the static Signed Distance Function (SDF)
maps. Unlike traditional polygonal representations, this approach has the
potential to map 3D obstacle shapes with more boundary-level details. Our
preliminary results demonstrate that this method would significantly enhance
collision detection performance, particularly in congested and dynamic
environments.

</details>


### [5] [ADAM-Dehaze: Adaptive Density-Aware Multi-Stage Dehazing for Improved Object Detection in Foggy Conditions](https://arxiv.org/abs/2506.15837)
*Fatmah AlHindaassi,Mohammed Talha Alam,Fakhri Karray*

Main category: cs.CV

TL;DR: ADAM-Dehaze是一种自适应、密度感知的去雾框架，通过动态路由和自适应损失优化图像恢复和物体检测，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 雾天气严重影响自动驾驶等安全关键应用的视觉信息，需要一种能够适应不同雾密度的去雾方法。

Method: 使用HDEN网络分类雾密度，动态路由到三个CORUN分支，结合自适应损失平衡物理模型和感知保真度。

Result: 在Cityscapes和RTTS基准测试中，PSNR提升2.1 dB，FADE降低30%，物体检测mAP提升13点，推理时间减少20%。

Conclusion: ADAM-Dehaze证明了密度特定处理和与下游视觉任务无缝集成的重要性。

Abstract: Adverse weather conditions, particularly fog, pose a significant challenge to
autonomous vehicles, surveillance systems, and other safety-critical
applications by severely degrading visual information. We introduce
ADAM-Dehaze, an adaptive, density-aware dehazing framework that jointly
optimizes image restoration and object detection under varying fog intensities.
A lightweight Haze Density Estimation Network (HDEN) classifies each input as
light, medium, or heavy fog. Based on this score, the system dynamically routes
the image through one of three CORUN branches: Light, Medium, or Complex, each
tailored to its haze regime. A novel adaptive loss balances physical-model
coherence and perceptual fidelity, ensuring both accurate defogging and
preservation of fine details. On Cityscapes and the real-world RTTS benchmark,
ADAM-Dehaze improves PSNR by up to 2.1 dB, reduces FADE by 30 percent, and
increases object detection mAP by up to 13 points, while cutting inference time
by 20 percent. These results highlight the importance of intensity-specific
processing and seamless integration with downstream vision tasks. Code
available at: https://github.com/talha-alam/ADAM-Dehaze.

</details>


### [6] [EchoShot: Multi-Shot Portrait Video Generation](https://arxiv.org/abs/2506.15838)
*Jiahao Wang,Hualian Sheng,Sijia Cai,Weizhan Zhang,Caixia Yan,Yachuang Feng,Bing Deng,Jieping Ye*

Main category: cs.CV

TL;DR: EchoShot是一个基于视频扩散模型的多镜头肖像定制框架，通过创新的位置嵌入机制和高质量数据集，实现了身份一致性和内容可控性。


<details>
  <summary>Details</summary>
Motivation: 现实应用需要多镜头生成且保持身份一致性和内容可控性，而现有方法仅限于单镜头生成。

Method: 提出EchoShot框架，采用镜头感知位置嵌入机制，构建大规模数据集PortraitGala，并扩展支持参考图像生成和长视频合成。

Result: EchoShot在多镜头肖像视频生成中表现出卓越的身份一致性和属性级可控性。

Conclusion: EchoShot为通用多镜头视频建模提供了潜在的基础范式。

Abstract: Video diffusion models substantially boost the productivity of artistic
workflows with high-quality portrait video generative capacity. However,
prevailing pipelines are primarily constrained to single-shot creation, while
real-world applications urge for multiple shots with identity consistency and
flexible content controllability. In this work, we propose EchoShot, a native
and scalable multi-shot framework for portrait customization built upon a
foundation video diffusion model. To start with, we propose shot-aware position
embedding mechanisms within video diffusion transformer architecture to model
inter-shot variations and establish intricate correspondence between multi-shot
visual content and their textual descriptions. This simple yet effective design
enables direct training on multi-shot video data without introducing additional
computational overhead. To facilitate model training within multi-shot
scenario, we construct PortraitGala, a large-scale and high-fidelity
human-centric video dataset featuring cross-shot identity consistency and
fine-grained captions such as facial attributes, outfits, and dynamic motions.
To further enhance applicability, we extend EchoShot to perform reference
image-based personalized multi-shot generation and long video synthesis with
infinite shot counts. Extensive evaluations demonstrate that EchoShot achieves
superior identity consistency as well as attribute-level controllability in
multi-shot portrait video generation. Notably, the proposed framework
demonstrates potential as a foundational paradigm for general multi-shot video
modeling.

</details>


### [7] [Assessing the impact of Binarization for Writer Identification in Greek Papyrus](https://arxiv.org/abs/2506.15852)
*Dominic Akt,Marco Peer,Florian Kleber*

Main category: cs.CV

TL;DR: 本文研究了希腊纸草文献的作者识别任务，重点探讨了图像二值化预处理对识别性能的影响，比较了传统方法与深度学习方法，并评估了数据增强和二值化质量对结果的作用。


<details>
  <summary>Details</summary>
Motivation: 希腊纸草文献的背景通常不均匀、碎片化且变色，传统二值化方法难以处理，影响后续作者识别性能。

Method: 比较传统二值化方法与深度学习模型，引入自定义数据增强技术，并在DIBCO 2019数据集上系统评估。

Result: 数据增强对深度学习方法有显著影响，二值化效果与作者识别性能密切相关。

Conclusion: 二值化质量对希腊纸草文献的作者识别至关重要，深度学习方法结合数据增强表现更优。

Abstract: This paper tackles the task of writer identification for Greek papyri. A
common preprocessing step in writer identification pipelines is image
binarization, which prevents the model from learning background features. This
is challenging in historical documents, in our case Greek papyri, as background
is often non-uniform, fragmented, and discolored with visible fiber structures.
We compare traditional binarization methods to state-of-the-art Deep Learning
(DL) models, evaluating the impact of binarization quality on subsequent writer
identification performance. DL models are trained with and without a custom
data augmentation technique, as well as different model selection criteria are
applied. The performance of these binarization methods, is then systematically
evaluated on the DIBCO 2019 dataset. The impact of binarization on writer
identification is subsequently evaluated using a state-of-the-art approach for
writer identification. The results of this analysis highlight the influence of
data augmentation for DL methods. Furthermore, findings indicate a strong
correlation between binarization effectiveness on papyri documents of DIBCO
2019 and downstream writer identification performance.

</details>


### [8] [Privacy-Preserving in Connected and Autonomous Vehicles Through Vision to Text Transformation](https://arxiv.org/abs/2506.15854)
*Abdolazim Rezaei,Mehdi Sookhak,Ahmad Patooghy*

Main category: cs.CV

TL;DR: 论文提出了一种基于反馈强化学习和视觉语言模型的新框架，用于保护AI摄像头捕获的隐私敏感数据，将图像转换为语义等效的文本描述。


<details>
  <summary>Details</summary>
Motivation: 传统隐私保护技术（如模糊处理）无法完全防止个人追踪，隐私风险仍然存在。

Method: 采用分层强化学习策略迭代优化生成的文本描述，结合视觉语言模型保留场景语义信息。

Result: 相比现有方法，隐私保护和文本质量显著提升，独特词数量增加约77%，细节密度提高约50%。

Conclusion: 该框架在保护隐私的同时保留了场景信息，为CAV系统中的隐私保护提供了有效解决方案。

Abstract: Connected and Autonomous Vehicles (CAVs) rely on a range of devices that
often process privacy-sensitive data. Among these, roadside units play a
critical role particularly through the use of AI-equipped (AIE) cameras for
applications such as violation detection. However, the privacy risks associated
with captured imagery remain a major concern, as such data can be misused for
identity theft, profiling, or unauthorized commercial purposes. While
traditional techniques such as face blurring and obfuscation have been applied
to mitigate privacy risks, individual privacy remains at risk, as individuals
can still be tracked using other features such as their clothing. This paper
introduces a novel privacy-preserving framework that leverages feedback-based
reinforcement learning (RL) and vision-language models (VLMs) to protect
sensitive visual information captured by AIE cameras. The main idea is to
convert images into semantically equivalent textual descriptions, ensuring that
scene-relevant information is retained while visual privacy is preserved. A
hierarchical RL strategy is employed to iteratively refine the generated text,
enhancing both semantic accuracy and privacy. Evaluation results demonstrate
significant improvements in both privacy protection and textual quality, with
the Unique Word Count increasing by approximately 77\% and Detail Density by
around 50\% compared to existing approaches.

</details>


### [9] [Visual symbolic mechanisms: Emergent symbol processing in vision language models](https://arxiv.org/abs/2506.15871)
*Rim Assouel,Declan Campbell,Taylor Webb*

Main category: cs.CV

TL;DR: 论文探讨了视觉语言模型（VLMs）如何通过空间索引机制解决特征绑定问题，并揭示了其失败原因。


<details>
  <summary>Details</summary>
Motivation: 研究VLMs是否采用类似语言模型的符号化机制解决特征绑定问题，以解释其在绑定任务中的持续失败。

Method: 识别VLMs中支持绑定的内容无关空间索引机制，并分析绑定错误与这些机制的关系。

Result: 发现VLMs通过空间索引机制实现绑定，绑定错误源于这些机制的失效。

Conclusion: 研究揭示了VLMs的符号化处理机制，为解决其绑定失败问题提供了方向。

Abstract: To accurately process a visual scene, observers must bind features together
to represent individual objects. This capacity is necessary, for instance, to
distinguish an image containing a red square and a blue circle from an image
containing a blue square and a red circle. Recent work has found that language
models solve this 'binding problem' via a set of symbol-like,
content-independent indices, but it is unclear whether similar mechanisms are
employed by vision language models (VLMs). This question is especially
relevant, given the persistent failures of VLMs on tasks that require binding.
Here, we identify a set of emergent symbolic mechanisms that support binding in
VLMs via a content-independent, spatial indexing scheme. Moreover, we find that
binding errors can be traced directly to failures in these mechanisms. Taken
together, these results shed light on the mechanisms that support symbol-like
processing in VLMs, and suggest possible avenues for addressing the persistent
binding failures exhibited by these models.

</details>


### [10] [Pediatric Pancreas Segmentation from MRI Scans with Deep Learning](https://arxiv.org/abs/2506.15908)
*Elif Keles,Merve Yazol,Gorkem Durak,Ziliang Hong,Halil Ertugrul Aktas,Zheyuan Zhang,Linkai Peng,Onkar Susladkar,Necati Guzelyel,Oznur Leman Boyunaga,Cemal Yazici,Mark Lowe,Aliye Uc,Ulas Bagci*

Main category: cs.CV

TL;DR: PanSegNet是一种深度学习算法，用于儿童胰腺MRI分割，在健康和患病状态下表现优异。


<details>
  <summary>Details</summary>
Motivation: 评估和验证PanSegNet在儿童急性胰腺炎、慢性胰腺炎和健康对照中的胰腺分割效果。

Method: 回顾性收集84例MRI扫描，由放射科医生手动分割胰腺，PanSegNet生成的分割结果通过DSC和HD95评估。

Result: PanSegNet在健康儿童中DSC为88%，在患病儿童中为80-81%，HD95值显示较高精度，观察者间一致性良好。

Conclusion: PanSegNet是首个经过验证的胰腺MRI分割深度学习工具，性能达到专家水平，工具和数据集已开源。

Abstract: Objective: Our study aimed to evaluate and validate PanSegNet, a deep
learning (DL) algorithm for pediatric pancreas segmentation on MRI in children
with acute pancreatitis (AP), chronic pancreatitis (CP), and healthy controls.
Methods: With IRB approval, we retrospectively collected 84 MRI scans (1.5T/3T
Siemens Aera/Verio) from children aged 2-19 years at Gazi University
(2015-2024). The dataset includes healthy children as well as patients
diagnosed with AP or CP based on clinical criteria. Pediatric and general
radiologists manually segmented the pancreas, then confirmed by a senior
pediatric radiologist. PanSegNet-generated segmentations were assessed using
Dice Similarity Coefficient (DSC) and 95th percentile Hausdorff distance
(HD95). Cohen's kappa measured observer agreement. Results: Pancreas MRI T2W
scans were obtained from 42 children with AP/CP (mean age: 11.73 +/- 3.9 years)
and 42 healthy children (mean age: 11.19 +/- 4.88 years). PanSegNet achieved
DSC scores of 88% (controls), 81% (AP), and 80% (CP), with HD95 values of 3.98
mm (controls), 9.85 mm (AP), and 15.67 mm (CP). Inter-observer kappa was 0.86
(controls), 0.82 (pancreatitis), and intra-observer agreement reached 0.88 and
0.81. Strong agreement was observed between automated and manual volumes (R^2 =
0.85 in controls, 0.77 in diseased), demonstrating clinical reliability.
Conclusion: PanSegNet represents the first validated deep learning solution for
pancreatic MRI segmentation, achieving expert-level performance across healthy
and diseased states. This tool, algorithm, along with our annotated dataset,
are freely available on GitHub and OSF, advancing accessible, radiation-free
pediatric pancreatic imaging and fostering collaborative research in this
underserved domain.

</details>


### [11] [MoiréXNet: Adaptive Multi-Scale Demoiréing with Linear Attention Test-Time Training and Truncated Flow Matching Prior](https://arxiv.org/abs/2506.15929)
*Liangyan Li,Yimo Ning,Kevin Le,Wei Dong,Yunzhe Li,Jun Chen,Xiaohong Liu*

Main category: cs.CV

TL;DR: 提出了一种结合MAP估计和深度学习的图像/视频去摩尔纹框架，解决了非线性退化问题，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法完全去除摩尔纹或导致过度平滑，模型能力受限且训练数据不足。

Method: 结合监督学习模型（带线性注意力TTT模块）和TFMP先验，高效学习非线性映射并细化输出。

Result: 框架结合了线性注意力的计算效率和生成模型的细化能力，提升了恢复性能。

Conclusion: 提出的混合框架在去摩尔纹任务中表现优异，解决了非线性退化问题。

Abstract: This paper introduces a novel framework for image and video demoir\'eing by
integrating Maximum A Posteriori (MAP) estimation with advanced deep learning
techniques. Demoir\'eing addresses inherently nonlinear degradation processes,
which pose significant challenges for existing methods.
  Traditional supervised learning approaches either fail to remove moir\'e
patterns completely or produce overly smooth results. This stems from
constrained model capacity and scarce training data, which inadequately
represent the clean image distribution and hinder accurate reconstruction of
ground-truth images. While generative models excel in image restoration for
linear degradations, they struggle with nonlinear cases such as demoir\'eing
and often introduce artifacts.
  To address these limitations, we propose a hybrid MAP-based framework that
integrates two complementary components. The first is a supervised learning
model enhanced with efficient linear attention Test-Time Training (TTT)
modules, which directly learn nonlinear mappings for RAW-to-sRGB demoir\'eing.
The second is a Truncated Flow Matching Prior (TFMP) that further refines the
outputs by aligning them with the clean image distribution, effectively
restoring high-frequency details and suppressing artifacts. These two
components combine the computational efficiency of linear attention with the
refinement abilities of generative models, resulting in improved restoration
performance.

</details>


### [12] [Beyond Audio and Pose: A General-Purpose Framework for Video Synchronization](https://arxiv.org/abs/2506.15937)
*Yosub Shin,Igor Molybog*

Main category: cs.CV

TL;DR: VideoSync是一个独立于特定特征提取方法的视频同步框架，适用于多种内容类型，并在公平实验条件下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频同步方法依赖音频或特定视觉事件，限制了在信号不可靠或缺失场景的适用性，且缺乏通用和可复现的基准。

Method: 提出VideoSync框架，不依赖特定特征提取方法，并在新构建的数据集上评估，提供数据集创建方法和代码。

Result: VideoSync在公平条件下优于现有方法（如SeSyn-Net），并发现CNN模型在同步偏移预测中最有效。

Conclusion: VideoSync提升了视频同步的通用性和鲁棒性，适用于更广泛的现实应用。

Abstract: Video synchronization-aligning multiple video streams capturing the same
event from different angles-is crucial for applications such as reality TV show
production, sports analysis, surveillance, and autonomous systems. Prior work
has heavily relied on audio cues or specific visual events, limiting
applicability in diverse settings where such signals may be unreliable or
absent. Additionally, existing benchmarks for video synchronization lack
generality and reproducibility, restricting progress in the field. In this
work, we introduce VideoSync, a video synchronization framework that operates
independently of specific feature extraction methods, such as human pose
estimation, enabling broader applicability across different content types. We
evaluate our system on newly composed datasets covering single-human,
multi-human, and non-human scenarios, providing both the methodology and code
for dataset creation to establish reproducible benchmarks. Our analysis reveals
biases in prior SOTA work, particularly in SeSyn-Net's preprocessing pipeline,
leading to inflated performance claims. We correct these biases and propose a
more rigorous evaluation framework, demonstrating that VideoSync outperforms
existing approaches, including SeSyn-Net, under fair experimental conditions.
Additionally, we explore various synchronization offset prediction methods,
identifying a convolutional neural network (CNN)-based model as the most
effective. Our findings advance video synchronization beyond domain-specific
constraints, making it more generalizable and robust for real-world
applications.

</details>


### [13] [Dense 3D Displacement Estimation for Landslide Monitoring via Fusion of TLS Point Clouds and Embedded RGB Images](https://arxiv.org/abs/2506.16265)
*Zhaoyi Wang,Jemil Avers Butt,Shengyu Huang,Tomislav Medic,Andreas Wieser*

Main category: cs.CV

TL;DR: 提出了一种基于点云和RGB图像融合的分层分区方法，用于估计密集的3D位移矢量场，提高了滑坡监测的空间覆盖率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅依赖几何或辐射信息，导致稀疏或非3D位移估计，无法满足滑坡监测的需求。

Method: 通过结合3D点云和配准的RGB图像，构建基于几何和图像特征的匹配，并通过几何一致性检查和刚性变换估计进行优化。

Result: 在两个真实滑坡数据集上，空间覆盖率分别达到79%和97%，位移幅度偏差低于扫描分辨率，优于现有方法F2S3。

Conclusion: 该方法为基于TLS的滑坡监测提供了实用且适应性强的解决方案，并可扩展至其他点云和监测任务。

Abstract: Landslide monitoring is essential for understanding geohazards and mitigating
associated risks. However, existing point cloud-based methods typically rely on
either geometric or radiometric information and often yield sparse or non-3D
displacement estimates. In this paper, we propose a hierarchical
partition-based coarse-to-fine approach that fuses 3D point clouds and
co-registered RGB images to estimate dense 3D displacement vector fields. We
construct patch-level matches using both 3D geometry and 2D image features.
These matches are refined via geometric consistency checks, followed by rigid
transformation estimation per match. Experimental results on two real-world
landslide datasets demonstrate that our method produces 3D displacement
estimates with high spatial coverage (79% and 97%) and high accuracy.
Deviations in displacement magnitude with respect to external measurements
(total station or GNSS observations) are 0.15 m and 0.25 m on the two datasets,
respectively, and only 0.07 m and 0.20 m compared to manually derived
references. These values are below the average scan resolutions (0.08 m and
0.30 m). Our method outperforms the state-of-the-art method F2S3 in spatial
coverage while maintaining comparable accuracy. Our approach offers a practical
and adaptable solution for TLS-based landslide monitoring and is extensible to
other types of point clouds and monitoring tasks. Our example data and source
code are publicly available at https://github.com/zhaoyiww/fusion4landslide.

</details>


### [14] [Polyline Path Masked Attention for Vision Transformer](https://arxiv.org/abs/2506.15940)
*Zhongchen Zhao,Chaodong Xiao,Hui Lin,Qi Xie,Lei Zhang,Deyu Meng*

Main category: cs.CV

TL;DR: 提出了一种结合ViTs自注意力机制和Mamba2结构化掩码的PPMA方法，通过改进的2D折线路径扫描策略增强空间邻接关系建模，在多项视觉任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 结合ViTs的全局依赖建模能力和Mamba2的空间邻接先验建模能力，以提升视觉任务的性能。

Method: 改进Mamba2的结构化掩码为2D折线路径掩码，并将其嵌入ViTs的自注意力机制中，实现空间邻接先验的显式建模。

Result: 在图像分类、目标检测和分割任务中超越现有方法，例如在ADE20K分割任务中mIoU提升0.3%-1.3%。

Conclusion: PPMA方法有效结合了两种架构的优势，显著提升了视觉任务的性能。

Abstract: Global dependency modeling and spatial position modeling are two core issues
of the foundational architecture design in current deep learning frameworks.
Recently, Vision Transformers (ViTs) have achieved remarkable success in
computer vision, leveraging the powerful global dependency modeling capability
of the self-attention mechanism. Furthermore, Mamba2 has demonstrated its
significant potential in natural language processing tasks by explicitly
modeling the spatial adjacency prior through the structured mask. In this
paper, we propose Polyline Path Masked Attention (PPMA) that integrates the
self-attention mechanism of ViTs with an enhanced structured mask of Mamba2,
harnessing the complementary strengths of both architectures. Specifically, we
first ameliorate the traditional structured mask of Mamba2 by introducing a 2D
polyline path scanning strategy and derive its corresponding structured mask,
polyline path mask, which better preserves the adjacency relationships among
image tokens. Notably, we conduct a thorough theoretical analysis on the
structural characteristics of the proposed polyline path mask and design an
efficient algorithm for the computation of the polyline path mask. Next, we
embed the polyline path mask into the self-attention mechanism of ViTs,
enabling explicit modeling of spatial adjacency prior. Extensive experiments on
standard benchmarks, including image classification, object detection, and
segmentation, demonstrate that our model outperforms previous state-of-the-art
approaches based on both state-space models and Transformers. For example, our
proposed PPMA-T/S/B models achieve 48.7%/51.1%/52.3% mIoU on the ADE20K
semantic segmentation task, surpassing RMT-T/S/B by 0.7%/1.3%/0.3%,
respectively. Code is available at https://github.com/zhongchenzhao/PPMA.

</details>


### [15] [Heterogeneous-Modal Unsupervised Domain Adaptation via Latent Space Bridging](https://arxiv.org/abs/2506.15971)
*Jiawen Yang,Shuhao Chen,Yucong Duan,Ke Tang,Yu Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为HMUDA的新设置，用于解决不同模态间的无监督域适应问题，并提出LSB框架，通过双分支结构和损失函数实现模态对齐和域对齐。


<details>
  <summary>Details</summary>
Motivation: 解决源域和目标域属于完全不同模态时无监督域适应方法的局限性。

Method: 提出LSB框架，采用双分支结构，结合特征一致性损失和域对齐损失。

Result: 在六个基准数据集上实验表明，LSB达到了最先进的性能。

Conclusion: LSB框架成功实现了不同模态间的知识迁移，并在语义分割任务中表现出色。

Abstract: Unsupervised domain adaptation (UDA) methods effectively bridge domain gaps
but become struggled when the source and target domains belong to entirely
distinct modalities. To address this limitation, we propose a novel setting
called Heterogeneous-Modal Unsupervised Domain Adaptation (HMUDA), which
enables knowledge transfer between completely different modalities by
leveraging a bridge domain containing unlabeled samples from both modalities.
To learn under the HMUDA setting, we propose Latent Space Bridging (LSB), a
specialized framework designed for the semantic segmentation task.
Specifically, LSB utilizes a dual-branch architecture, incorporating a feature
consistency loss to align representations across modalities and a domain
alignment loss to reduce discrepancies between class centroids across domains.
Extensive experiments conducted on six benchmark datasets demonstrate that LSB
achieves state-of-the-art performance.

</details>


### [16] [LBMamba: Locally Bi-directional Mamba](https://arxiv.org/abs/2506.15976)
*Jingwei Zhang,Xi Han,Hong Qin,Mahdi S. Hosseini,Dimitris Samaras*

Main category: cs.CV

TL;DR: LBMamba是一种局部双向状态空间模型（SSM）块，通过在正向选择性扫描中嵌入轻量级局部反向扫描，避免了传统双向扫描的计算开销，提升了效率。基于LBMamba的LBVim视觉骨干网络在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决Mamba模型因单向扫描导致的信息缺失问题，同时避免传统双向扫描带来的计算负担。

Method: 提出LBMamba，在正向选择性扫描中嵌入局部反向扫描；构建LBVim网络，通过交替扫描方向恢复全局感受野。

Result: 在ImageNet-1K、ADE20K、COCO等数据集上性能显著提升；在病理图像分类任务中AUC、F1和准确率均有提高。

Conclusion: LBMamba和LBVim在保持高效的同时显著提升了性能，适用于多种视觉任务。

Abstract: Mamba, a State Space Model (SSM) that accelerates training by recasting
recurrence as a parallel selective scan, has recently emerged as a
linearly-scaling, efficient alternative to self-attention. Because of its
unidirectional nature, each state in Mamba only has information of its previous
states and is blind to states after. Current Mamba-based computer-vision
methods typically overcome this limitation by augmenting Mamba's global forward
scan with a global backward scan, forming a bi-directional scan that restores a
full receptive field. However, this operation doubles the computational load,
eroding much of the efficiency advantage that originally Mamba have. To
eliminate this extra scans, we introduce LBMamba, a locally bi-directional SSM
block that embeds a lightweight locally backward scan inside the forward
selective scan and executes it entirely in per-thread registers. Building on
LBMamba, we present LBVim, a scalable vision backbone that alternates scan
directions every two layers to recover a global receptive field without extra
backward sweeps. We validate the versatility of our approach on both natural
images and whole slide images (WSIs). We show that our LBVim constantly offers
a superior performance-throughput trade-off. That is under the same throughput,
LBVim achieves 0.8% to 1.6% higher top-1 accuracy on the ImageNet-1K
classification dataset, 0.6% to 2.7% higher mIoU on the ADE20K semantic
segmentation dataset, 0.9% higher APb and 1.1% higher APm on the COCO detection
dataset. We also integrate LBMamba into the SOTA pathology multiple instance
learning (MIL) approach, MambaMIL, which uses single directional scan.
Experiments on 3 public WSI classification datasets for show that our method
achieves a relative improvement of up to 3.06% better AUC, 3.39% better F1,
1.67% better accuracy.

</details>


### [17] [Towards Classifying Histopathological Microscope Images as Time Series Data](https://arxiv.org/abs/2506.15977)
*Sungrae Hong,Hyeongmin Park,Youngsin Ko,Sol Lee,Bryan Wong,Mun Yong Yi*

Main category: cs.CV

TL;DR: 论文提出了一种将显微镜图像作为时间序列数据分类的新方法，解决了手动获取和弱标签带来的挑战，并通过动态时间规整和注意力池化实现了稳定可靠的性能。


<details>
  <summary>Details</summary>
Motivation: 显微镜病理图像是癌症诊断的前线数据，但深度学习社区对其关注不足。论文旨在通过新方法提升其分类性能。

Method: 利用动态时间规整（DTW）将不同长度的图像序列拟合为固定长度目标，并结合注意力池化预测类别。

Result: 通过与多种基线比较和消融实验验证了方法的有效性，展示了稳定可靠的性能。

Conclusion: 该方法不仅将显微镜图像纳入医学图像分析，还将其性能提升至可信水平。

Abstract: As the frontline data for cancer diagnosis, microscopic pathology images are
fundamental for providing patients with rapid and accurate treatment. However,
despite their practical value, the deep learning community has largely
overlooked their usage. This paper proposes a novel approach to classifying
microscopy images as time series data, addressing the unique challenges posed
by their manual acquisition and weakly labeled nature. The proposed method fits
image sequences of varying lengths to a fixed-length target by leveraging
Dynamic Time-series Warping (DTW). Attention-based pooling is employed to
predict the class of the case simultaneously. We demonstrate the effectiveness
of our approach by comparing performance with various baselines and showcasing
the benefits of using various inference strategies in achieving stable and
reliable results. Ablation studies further validate the contribution of each
component. Our approach contributes to medical image analysis by not only
embracing microscopic images but also lifting them to a trustworthy level of
performance.

</details>


### [18] [Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization](https://arxiv.org/abs/2506.15980)
*Cong Wang,Zexuan Deng,Zhiwei Jiang,Fei Shen,Yafeng Yin,Shiwei Gan,Zifeng Cheng,Shiping Ge,Qing Gu*

Main category: cs.CV

TL;DR: SignViP是一个新的手语视频生成框架，通过多细粒度条件提升生成质量，采用离散标记化和扩散模型实现高效生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一粗粒度条件（如骨架序列），限制了生成视频的自然性和表现力。

Method: SignViP包含三个核心组件：1）联合训练的视频扩散模型和多条件编码器；2）FSQ自动编码器压缩嵌入为离散标记；3）多条件标记翻译器将文本翻译为离散标记。

Result: 实验表明SignViP在视频质量、时间一致性和语义保真度上达到最优性能。

Conclusion: SignViP通过多细粒度条件和离散标记化显著提升了手语视频生成的质量。

Abstract: Sign Language Video Generation (SLVG) seeks to generate identity-preserving
sign language videos from spoken language texts. Existing methods primarily
rely on the single coarse condition (\eg, skeleton sequences) as the
intermediary to bridge the translation model and the video generation model,
which limits both the naturalness and expressiveness of the generated videos.
To overcome these limitations, we propose SignViP, a novel SLVG framework that
incorporates multiple fine-grained conditions for improved generation fidelity.
Rather than directly translating error-prone high-dimensional conditions,
SignViP adopts a discrete tokenization paradigm to integrate and represent
fine-grained conditions (\ie, fine-grained poses and 3D hands). SignViP
contains three core components. (1) Sign Video Diffusion Model is jointly
trained with a multi-condition encoder to learn continuous embeddings that
encapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization
(FSQ) Autoencoder is further trained to compress and quantize these embeddings
into discrete tokens for compact representation of the conditions. (3)
Multi-Condition Token Translator is trained to translate spoken language text
to discrete multi-condition tokens. During inference, Multi-Condition Token
Translator first translates the spoken language text into discrete
multi-condition tokens. These tokens are then decoded to continuous embeddings
by FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion
Model to guide video generation. Experimental results show that SignViP
achieves state-of-the-art performance across metrics, including video quality,
temporal coherence, and semantic fidelity. The code is available at
https://github.com/umnooob/signvip/.

</details>


### [19] [Adversarial Attacks and Detection in Visual Place Recognition for Safer Robot Navigation](https://arxiv.org/abs/2506.15988)
*Connor Malone,Owen Claxton,Iman Shames,Michael Milford*

Main category: cs.CV

TL;DR: 论文分析了视觉地点识别（VPR）系统对对抗攻击的脆弱性，提出了四种常见攻击和四种VPR专用攻击的影响，并展示了通过引入对抗攻击检测器（AAD）和主动导航决策的闭环系统如何提升性能。实验表明，AAD能显著降低定位误差。


<details>
  <summary>Details</summary>
Motivation: VPR系统在对抗攻击下表现脆弱，可能导致机器人导航灾难性后果，因此需要研究其防御机制。

Method: 分析了四种常见攻击和四种VPR专用攻击，提出闭环系统框架，结合AAD和主动导航决策，并通过实验验证其效果。

Result: 实验显示，AAD能显著改善性能，例如将平均沿轨定位误差降低约50%，且检测率仅需75%真阳性和25%假阳性。

Conclusion: 研究表明AAD对提升VPR系统的鲁棒性至关重要，为实际系统设计提供了定量要求。

Abstract: Stand-alone Visual Place Recognition (VPR) systems have little defence
against a well-designed adversarial attack, which can lead to disastrous
consequences when deployed for robot navigation. This paper extensively
analyzes the effect of four adversarial attacks common in other perception
tasks and four novel VPR-specific attacks on VPR localization performance. We
then propose how to close the loop between VPR, an Adversarial Attack Detector
(AAD), and active navigation decisions by demonstrating the performance benefit
of simulated AADs in a novel experiment paradigm -- which we detail for the
robotics community to use as a system framework. In the proposed experiment
paradigm, we see the addition of AADs across a range of detection accuracies
can improve performance over baseline; demonstrating a significant improvement
-- such as a ~50% reduction in the mean along-track localization error -- can
be achieved with True Positive and False Positive detection rates of only 75%
and up to 25% respectively. We examine a variety of metrics including:
Along-Track Error, Percentage of Time Attacked, Percentage of Time in an
`Unsafe' State, and Longest Continuous Time Under Attack. Expanding further on
these results, we provide the first investigation into the efficacy of the Fast
Gradient Sign Method (FGSM) adversarial attack for VPR. The analysis in this
work highlights the need for AADs in real-world systems for trustworthy
navigation, and informs quantitative requirements for system design.

</details>


### [20] [DIGMAPPER: A Modular System for Automated Geologic Map Digitization](https://arxiv.org/abs/2506.16006)
*Weiwei Duan,Michael P. Gerlek,Steven N. Minton,Craig A. Knoblock,Fandel Lin,Theresa Chen,Leeje Jang,Sofia Kirsanova,Zekun Li,Yijun Lin,Yao-Yi Chiang*

Main category: cs.CV

TL;DR: DIGMAPPER是一个自动化地质地图数字化的系统，通过深度学习模型和创新的技术解决了传统数字化方法的效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统地质地图数字化工作耗时耗力，而地质信息对可再生能源、电动汽车和国家安全至关重要，因此需要一种高效自动化的解决方案。

Method: DIGMAPPER采用模块化、可扩展的架构，集成了深度学习模型进行地图布局分析、特征提取和地理配准，并利用上下文学习、合成数据生成和基于Transformer的模型解决数据不足和复杂视觉内容的挑战。

Result: 在DARPA-USGS数据集上评估显示，系统在多类特征提取和地理配准方面表现优异，显著提升了地质数据生成的效率。

Conclusion: DIGMAPPER已在美国地质调查局部署，为全国性关键矿产评估和更广泛的地球科学应用提供了高效支持。

Abstract: Historical geologic maps contain rich geospatial information, such as rock
units, faults, folds, and bedding planes, that is critical for assessing
mineral resources essential to renewable energy, electric vehicles, and
national security. However, digitizing maps remains a labor-intensive and
time-consuming task. We present DIGMAPPER, a modular, scalable system developed
in collaboration with the United States Geological Survey (USGS) to automate
the digitization of geologic maps. DIGMAPPER features a fully dockerized,
workflow-orchestrated architecture that integrates state-of-the-art deep
learning models for map layout analysis, feature extraction, and
georeferencing. To overcome challenges such as limited training data and
complex visual content, our system employs innovative techniques, including
in-context learning with large language models, synthetic data generation, and
transformer-based models. Evaluations on over 100 annotated maps from the
DARPA-USGS dataset demonstrate high accuracy across polygon, line, and point
feature extraction, and reliable georeferencing performance. Deployed at USGS,
DIGMAPPER significantly accelerates the creation of analysis-ready geospatial
datasets, supporting national-scale critical mineral assessments and broader
geoscientific applications.

</details>


### [21] [EndoMUST: Monocular Depth Estimation for Robotic Endoscopy via End-to-end Multi-step Self-supervised Training](https://arxiv.org/abs/2506.16017)
*Liangjing Shao,Linxin Bai,Chenkang Du,Xinrong Chen*

Main category: cs.CV

TL;DR: 提出了一种多步微调框架，用于内窥镜场景中的自监督深度估计，解决了光照变化和纹理稀疏问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 内窥镜场景中光照变化和纹理稀疏导致深度估计困难，现有方法在多模块训练策略上仍有不足。

Method: 采用三步训练策略：光流注册、多尺度图像分解和多变换对齐，每步仅训练相关网络以减少干扰。

Result: 在SCARED和Hamlyn数据集上实现了4%~10%的误差降低，达到SOTA性能。

Conclusion: 多步微调框架有效提升了内窥镜场景中的自监督深度估计性能。

Abstract: Monocular depth estimation and ego-motion estimation are significant tasks
for scene perception and navigation in stable, accurate and efficient
robot-assisted endoscopy. To tackle lighting variations and sparse textures in
endoscopic scenes, multiple techniques including optical flow, appearance flow
and intrinsic image decomposition have been introduced into the existing
methods. However, the effective training strategy for multiple modules are
still critical to deal with both illumination issues and information
interference for self-supervised depth estimation in endoscopy. Therefore, a
novel framework with multistep efficient finetuning is proposed in this work.
In each epoch of end-to-end training, the process is divided into three steps,
including optical flow registration, multiscale image decomposition and
multiple transformation alignments. At each step, only the related networks are
trained without interference of irrelevant information. Based on
parameter-efficient finetuning on the foundation model, the proposed method
achieves state-of-the-art performance on self-supervised depth estimation on
SCARED dataset and zero-shot depth estimation on Hamlyn dataset, with
4\%$\sim$10\% lower error. The evaluation code of this work has been published
on https://github.com/BaymaxShao/EndoMUST.

</details>


### [22] [PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models](https://arxiv.org/abs/2506.16054)
*Tianchen Zhao,Ke Hong,Xinhao Yang,Xuefeng Xiao,Huixia Li,Feng Ling,Ruiqi Xie,Siqi Chen,Hongyu Zhu,Yichong Zhang,Yu Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为PARO的技术，通过重新组织注意力模式来降低视觉生成中的计算和内存成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉生成中注意力机制的高计算和内存成本问题，尤其是在高分辨率图像或多帧视频生成中。

Method: 设计了一种名为PARO的技术，通过重新组织注意力模式为硬件友好的块状模式，简化稀疏化和量化。

Result: PAROAttention在低密度（20%-30%）和低比特宽度（INT8/INT4）下实现了无损性能，并显著加速了端到端延迟（1.9x至2.7x）。

Conclusion: PARO技术通过重新组织注意力模式，有效解决了视觉生成中的计算和内存问题，同时保持了高性能。

Abstract: In visual generation, the quadratic complexity of attention mechanisms
results in high memory and computational costs, especially for longer token
sequences required in high-resolution image or multi-frame video generation. To
address this, prior research has explored techniques such as sparsification and
quantization. However, these techniques face significant challenges under low
density and reduced bitwidths. Through systematic analysis, we identify that
the core difficulty stems from the dispersed and irregular characteristics of
visual attention patterns. Therefore, instead of introducing specialized
sparsification and quantization design to accommodate such patterns, we propose
an alternative strategy: *reorganizing* the attention pattern to alleviate the
challenges. Inspired by the local aggregation nature of visual feature
extraction, we design a novel **Pattern-Aware token ReOrdering (PARO)**
technique, which unifies the diverse attention patterns into a
hardware-friendly block-wise pattern. This unification substantially simplifies
and enhances both sparsification and quantization. We evaluate the
performance-efficiency trade-offs of various design choices and finalize a
methodology tailored for the unified pattern. Our approach, **PAROAttention**,
achieves video and image generation with lossless metrics, and nearly identical
results from full-precision (FP) baselines, while operating at notably lower
density (~20%-30%) and bitwidth (**INT8/INT4**), achieving a **1.9x** to
**2.7x** end-to-end latency speedup.

</details>


### [23] [Stepping Out of Similar Semantic Space for Open-Vocabulary Segmentation](https://arxiv.org/abs/2506.16058)
*Yong Liu,SongLi Wu,Sule Bai,Jiahao Wang,Yitong Wang,Yansong Tang*

Main category: cs.CV

TL;DR: 论文提出了一种新的基准测试OpenBench，用于更准确地评估开放词汇分割模型的性能，并提出了OVSNet方法以提升分割效果。


<details>
  <summary>Details</summary>
Motivation: 现有测试集无法充分衡量模型对开放词汇概念的理解能力，因为其语义空间与训练空间高度相似。

Method: 提出OVSNet方法，通过异构特征融合和训练空间的无成本扩展提升分割性能。

Result: OVSNet在现有数据集和OpenBench上均取得最优结果。

Conclusion: OpenBench和OVSNet的有效性通过实验和分析得到验证。

Abstract: Open-vocabulary segmentation aims to achieve segmentation of arbitrary
categories given unlimited text inputs as guidance. To achieve this, recent
works have focused on developing various technical routes to exploit the
potential of large-scale pre-trained vision-language models and have made
significant progress on existing benchmarks. However, we find that existing
test sets are limited in measuring the models' comprehension of
``open-vocabulary" concepts, as their semantic space closely resembles the
training space, even with many overlapping categories. To this end, we present
a new benchmark named OpenBench that differs significantly from the training
semantics. It is designed to better assess the model's ability to understand
and segment a wide range of real-world concepts. When testing existing methods
on OpenBench, we find that their performance diverges from the conclusions
drawn on existing test sets. In addition, we propose a method named OVSNet to
improve the segmentation performance for diverse and open scenarios. Through
elaborate fusion of heterogeneous features and cost-free expansion of the
training space, OVSNet achieves state-of-the-art results on both existing
datasets and our proposed OpenBench. Corresponding analysis demonstrate the
soundness and effectiveness of our proposed benchmark and method.

</details>


### [24] [STAR-Pose: Efficient Low-Resolution Video Human Pose Estimation via Spatial-Temporal Adaptive Super-Resolution](https://arxiv.org/abs/2506.16061)
*Yucheng Jin,Jinyan Chen,Ziyue He,Baojun Han,Furan An*

Main category: cs.CV

TL;DR: STAR-Pose是一个针对低分辨率视频中人体姿态估计的时空自适应超分辨率框架，通过改进的Transformer和自适应融合模块，显著提升了性能并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 低分辨率视频中的人体姿态估计是一个挑战，传统方法要么需要高质量输入，要么计算成本高，限制了在资源受限环境中的部署。

Method: 提出STAR-Pose框架，结合时空Transformer和自适应融合模块，采用LeakyReLU改进的线性注意力机制，并设计任务导向的复合损失函数。

Result: 在多个主流视频数据集上，STAR-Pose在极低分辨率（64x48）条件下mAP提升5.2%，推理速度比级联方法快2.8x至4.4x。

Conclusion: STAR-Pose在低分辨率视频中实现了高效且高性能的人体姿态估计，为资源受限环境提供了可行的解决方案。

Abstract: Human pose estimation in low-resolution videos presents a fundamental
challenge in computer vision. Conventional methods either assume high-quality
inputs or employ computationally expensive cascaded processing, which limits
their deployment in resource-constrained environments. We propose STAR-Pose, a
spatial-temporal adaptive super-resolution framework specifically designed for
video-based human pose estimation. Our method features a novel spatial-temporal
Transformer with LeakyReLU-modified linear attention, which efficiently
captures long-range temporal dependencies. Moreover, it is complemented by an
adaptive fusion module that integrates parallel CNN branch for local texture
enhancement. We also design a pose-aware compound loss to achieve task-oriented
super-resolution. This loss guides the network to reconstruct structural
features that are most beneficial for keypoint localization, rather than
optimizing purely for visual quality. Extensive experiments on several
mainstream video HPE datasets demonstrate that STAR-Pose outperforms existing
approaches. It achieves up to 5.2% mAP improvement under extremely
low-resolution (64x48) conditions while delivering 2.8x to 4.4x faster
inference than cascaded approaches.

</details>


### [25] [TD3Net: A Temporal Densely Connected Multi-Dilated Convolutional Network for Lipreading](https://arxiv.org/abs/2506.16073)
*Byung Hoon Lee,Wooseok Shin,Sung Won Han*

Main category: cs.CV

TL;DR: 论文提出TD3Net，一种结合密集跳跃连接和多扩张卷积的后端架构，用于词级唇读任务，解决了现有方法因感受野盲点导致的信息丢失问题。


<details>
  <summary>Details</summary>
Motivation: 现有唇读方法中，TCNs虽广泛使用，但因感受野盲点和信息丢失问题，性能受限。

Method: 提出TD3Net，结合密集跳跃连接和多扩张卷积，覆盖更广且无盲点的感受野。

Result: 在LRW和LRW-1000数据集上，TD3Net性能与现有方法相当，但参数更少、计算量更低。

Conclusion: TD3Net能有效利用多样时序特征并保持连续性，在唇读系统中具有显著优势。

Abstract: The word-level lipreading approach typically employs a two-stage framework
with separate frontend and backend architectures to model dynamic lip
movements. Each component has been extensively studied, and in the backend
architecture, temporal convolutional networks (TCNs) have been widely adopted
in state-of-the-art methods. Recently, dense skip connections have been
introduced in TCNs to mitigate the limited density of the receptive field,
thereby improving the modeling of complex temporal representations. However,
their performance remains constrained owing to potential information loss
regarding the continuous nature of lip movements, caused by blind spots in the
receptive field. To address this limitation, we propose TD3Net, a temporal
densely connected multi-dilated convolutional network that combines dense skip
connections and multi-dilated temporal convolutions as the backend
architecture. TD3Net covers a wide and dense receptive field without blind
spots by applying different dilation factors to skip-connected features.
Experimental results on a word-level lipreading task using two large publicly
available datasets, Lip Reading in the Wild (LRW) and LRW-1000, indicate that
the proposed method achieves performance comparable to state-of-the-art
methods. It achieved higher accuracy with fewer parameters and lower
floating-point operations compared to existing TCN-based backend architectures.
Moreover, visualization results suggest that our approach effectively utilizes
diverse temporal features while preserving temporal continuity, presenting
notable advantages in lipreading systems. The code is available at our GitHub
repository:
https://github.com/Leebh-kor/TD3Net-A-Temporal-Densely-Connected-Multi-dilated-Convolutional-Network-for-Lipreading

</details>


### [26] [PR-DETR: Injecting Position and Relation Prior for Dense Video Captioning](https://arxiv.org/abs/2506.16082)
*Yizhe Li,Sanping Zhou,Zheng Qin,Le Wang*

Main category: cs.CV

TL;DR: PR-DETR是一种新的密集视频描述框架，通过显式位置和关系先验改进检测变换器，提升事件定位和描述质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法基于变换器隐式学习事件位置和语义，需要大量训练数据且性能受限。

Method: 引入位置锚定查询和事件关系编码器，显式提供位置和关系先验。

Result: 在ActivityNet Captions和YouCook2数据集上表现优异。

Conclusion: 显式先验注入显著提升了密集视频描述的性能。

Abstract: Dense video captioning is a challenging task that aims to localize and
caption multiple events in an untrimmed video. Recent studies mainly follow the
transformer-based architecture to jointly perform the two sub-tasks, i.e.,
event localization and caption generation, in an end-to-end manner. Based on
the general philosophy of detection transformer, these methods implicitly learn
the event locations and event semantics, which requires a large amount of
training data and limits the model's performance in practice. In this paper, we
propose a novel dense video captioning framework, named PR-DETR, which injects
the explicit position and relation prior into the detection transformer to
improve the localization accuracy and caption quality, simultaneously. On the
one hand, we first generate a set of position-anchored queries to provide the
scene-specific position and semantic information about potential events as
position prior, which serves as the initial event search regions to eliminate
the implausible event proposals. On the other hand, we further design an event
relation encoder to explicitly calculate the relationship between event
boundaries as relation prior to guide the event interaction to improve the
semantic coherence of the captions. Extensive ablation studies are conducted to
verify the effectiveness of the position and relation prior. Experimental
results also show the competitive performance of our method on ActivityNet
Captions and YouCook2 datasets.

</details>


### [27] [AutoV: Learning to Retrieve Visual Prompt for Large Vision-Language Models](https://arxiv.org/abs/2506.16112)
*Yuan Zhang,Chun-Kai Fan,Tao Huang,Ming Lu,Sicheng Yu,Junwen Pan,Kuan Cheng,Qi She,Shanghang Zhang*

Main category: cs.CV

TL;DR: AutoV是一种自动选择最优视觉提示的方法，用于提升大型视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉提示方法依赖人工设计，效果有限且耗时，AutoV旨在自动优化视觉提示选择。

Method: 通过数据收集和标注管道评估多种视觉提示，利用预训练模型排名并训练AutoV自动选择最优提示。

Result: AutoV显著提升多种LVLM的性能，例如LLaVA-OV和Qwen2.5-VL在多个任务中准确率分别提高1.7%和1.9%。

Conclusion: AutoV是一种高效的视觉提示优化方法，为LVLM的性能提升提供了新思路。

Abstract: Inspired by text prompts in large language models (LLMs), visual prompts have
been explored to enhance the reasoning capabilities of large vision-language
models (LVLMs). Current methods design heuristic visual prompts, such as
overlaying a text-query-guided attention heatmap on the original input image.
However, designing effective prompts manually is challenging and
time-consuming, and it often fails to explore the benefits of different visual
prompts, leading to sub-optimal performance. To this end, we propose
\textbf{AutoV} that learns to automatically select the optimal visual prompt
from various candidates based on given textual queries and the input image. To
train AutoV, we developed an automatic data collection and labeling pipeline
that evaluates various visual prompts with a pre-trained LVLM. We input a set
of visual prompts into the LVLM and rank them according to the prediction
losses generated by the model. Using the ranking as a supervision signal, we
train AutoV to automatically choose the optimal visual prompt from various
visual prompts for LVLMs. Experimental results indicate that AutoV enhances the
performance of various LVLMs across multiple popular image understanding tasks.
For instance, LLaVA-OV with AutoV achieves $\textbf{1.7}\%$ accuracy gain on
LLaVA$^{\text{Wild}}$, and AutoV boosts Qwen2.5-VL by $\textbf{1.9}\%$ on MMMU,
highlighting its potential as an optimal visual prompting method for LVLMs.

</details>


### [28] [FastInit: Fast Noise Initialization for Temporally Consistent Video Generation](https://arxiv.org/abs/2506.16119)
*Chengyu Bai,Yuming Li,Zhongyu Zhao,Jintao Chen,Peidong Jia,Qi She,Ming Lu,Shanghang Zhang*

Main category: cs.CV

TL;DR: FastInit提出了一种快速噪声初始化方法，通过单次前向传播生成优化的噪声，显著提高了视频生成的效率和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如FreeInit）通过迭代优化噪声实现时间一致性，但计算成本高，FastInit旨在解决这一问题。

Method: FastInit训练了一个视频噪声预测网络（VNPNet），输入随机噪声和文本提示，单次生成优化的噪声。

Result: 实验表明，FastInit在多种文本到视频模型中均能提升生成视频的质量和时间一致性。

Conclusion: FastInit是一种高效且实用的视频生成优化方法，可直接应用于推理阶段。

Abstract: Video generation has made significant strides with the development of
diffusion models; however, achieving high temporal consistency remains a
challenging task. Recently, FreeInit identified a training-inference gap and
introduced a method to iteratively refine the initial noise during inference.
However, iterative refinement significantly increases the computational cost
associated with video generation. In this paper, we introduce FastInit, a fast
noise initialization method that eliminates the need for iterative refinement.
FastInit learns a Video Noise Prediction Network (VNPNet) that takes random
noise and a text prompt as input, generating refined noise in a single forward
pass. Therefore, FastInit greatly enhances the efficiency of video generation
while achieving high temporal consistency across frames. To train the VNPNet,
we create a large-scale dataset consisting of pairs of text prompts, random
noise, and refined noise. Extensive experiments with various text-to-video
models show that our method consistently improves the quality and temporal
consistency of the generated videos. FastInit not only provides a substantial
improvement in video generation but also offers a practical solution that can
be applied directly during inference. The code and dataset will be released.

</details>


### [29] [Neurosymbolic Object-Centric Learning with Distant Supervision](https://arxiv.org/abs/2506.16129)
*Stefano Colamonaco,David Debot,Giuseppe Marra*

Main category: cs.CV

TL;DR: 提出了一种直接从原始非结构化感知数据中学习对象中心表示的方法，结合感知模块和符号推理层，通过概率逻辑编程实现推理，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有系统依赖对象级监督或预定义的对象分解，限制了模型的泛化能力。

Method: 提出DeepObjectLog模型，整合感知模块和基于概率逻辑编程的符号推理层，通过概率逻辑推理引导对象发现。

Result: 在多种泛化场景（如未见对象组合、任务和对象数量）中表现优于神经和神经符号基线。

Conclusion: 该方法通过神经符号结合，直接从原始数据学习对象表示，显著提升了泛化能力。

Abstract: Relational learning enables models to generalize across structured domains by
reasoning over objects and their interactions. While recent advances in
neurosymbolic reasoning and object-centric learning bring us closer to this
goal, existing systems rely either on object-level supervision or on a
predefined decomposition of the input into objects. In this work, we propose a
neurosymbolic formulation for learning object-centric representations directly
from raw unstructured perceptual data and using only distant supervision. We
instantiate this approach in DeepObjectLog, a neurosymbolic model that
integrates a perceptual module, which extracts relevant object representations,
with a symbolic reasoning layer based on probabilistic logic programming. By
enabling sound probabilistic logical inference, the symbolic component
introduces a novel learning signal that further guides the discovery of
meaningful objects in the input. We evaluate our model across a diverse range
of generalization settings, including unseen object compositions, unseen tasks,
and unseen number of objects. Experimental results show that our method
outperforms neural and neurosymbolic baselines across the tested settings.

</details>


### [30] [GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal Reasoning](https://arxiv.org/abs/2506.16141)
*Yi Chen,Yuying Ge,Rui Wang,Yixiao Ge,Junhao Cheng,Ying Shan,Xihui Liu*

Main category: cs.CV

TL;DR: 论文提出了GRPO-CARE，一种一致性感知的强化学习框架，用于提升多模态大语言模型（MLLMs）的推理一致性和答案准确性，并通过SEED-Bench-R1基准验证其效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如GRPO）在提升答案准确性的同时，可能降低推理步骤与答案的逻辑一致性，且缺乏对MLLMs后训练方法的严格评估。

Method: GRPO-CARE通过双层级奖励机制（基础奖励和自适应一致性奖励）优化答案正确性和推理一致性，并替换KL惩罚。

Result: GRPO-CARE在SEED-Bench-R1上表现优于标准GRPO，性能提升6.7%，一致性提高24.5%，并展示了强迁移能力。

Conclusion: GRPO-CARE为MLLMs的后训练提供了通用框架，提升了模型的解释性和鲁棒性。

Abstract: Recent reinforcement learning approaches, such as outcome-supervised GRPO,
have advanced Chain-of-Thought reasoning in large language models (LLMs), yet
their adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack
of rigorous evaluation for MLLM post-training methods, we introduce
SEED-Bench-R1, a benchmark with complex real-world videos requiring balanced
perception and reasoning. It offers a large training set and evaluates
generalization across three escalating challenges: in-distribution,
cross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1,
we find that standard GRPO, while improving answer accuracy, often reduces
logical coherence between reasoning steps and answers, with only a 57.9%
consistency rate. This stems from reward signals focusing solely on final
answers, encouraging shortcuts, and strict KL penalties limiting exploration.To
address this, we propose GRPO-CARE, a consistency-aware RL framework optimizing
both answer correctness and reasoning coherence without explicit supervision.
GRPO-CARE introduces a two-tiered reward: (1) a base reward for answer
correctness, and (2) an adaptive consistency bonus, computed by comparing the
model's reasoning-to-answer likelihood (via a slowly-evolving reference model)
against group peers.This dual mechanism amplifies rewards for reasoning paths
that are both correct and logically consistent. Replacing KL penalties with
this adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1,
achieving a 6.7% performance gain on the hardest evaluation level and a 24.5%
improvement in consistency. It also shows strong transferability, improving
model performance across diverse video understanding benchmarks. Our work
contributes a systematically designed benchmark and a generalizable
post-training framework, advancing the development of more interpretable and
robust MLLMs.

</details>


### [31] [MBA: Multimodal Bidirectional Attack for Referring Expression Segmentation Models](https://arxiv.org/abs/2506.16157)
*Xingbai Chen,Tingchao Fu,Renyang Liu,Wei Zhou,Chao Yi*

Main category: cs.CV

TL;DR: 本文提出了一种针对Referring Expression Segmentation (RES)模型的多模态双向攻击方法，通过联合优化图像和文本模态生成对抗样本，提升了跨文本迁移能力。


<details>
  <summary>Details</summary>
Motivation: 尽管RES模型在基于自然语言描述的图像分割中表现出色，但其对抗样本的鲁棒性尚未充分研究。现有攻击方法在RES模型上表现不佳，且实际场景中用户会使用多样化的文本输入，需要对抗样本具备跨文本泛化能力。

Method: 提出多模态双向攻击策略，结合可学习的代理文本嵌入扰动，同时优化图像和文本模态，使对抗样本适应更具挑战性的文本嵌入。

Result: 在多个RES模型和基准数据集上的实验表明，该方法优于现有方法，显著提升了对抗样本的跨文本迁移能力。

Conclusion: 该方法有效解决了RES模型在多模态对抗攻击中的挑战，为提升模型鲁棒性提供了新思路。

Abstract: Referring Expression Segmentation (RES) enables precise object segmentation
in images based on natural language descriptions, offering high flexibility and
broad applicability in real-world vision tasks. Despite its impressive
performance, the robustness of RES models against adversarial examples remains
largely unexplored. While prior adversarial attack methods have explored
adversarial robustness on conventional segmentation models, they perform poorly
when directly applied to RES, failing to expose vulnerabilities in its
multimodal structure. Moreover, in practical open-world scenarios, users
typically issue multiple, diverse referring expressions to interact with the
same image, highlighting the need for adversarial examples that generalize
across varied textual inputs. To address these multimodal challenges, we
propose a novel adversarial attack strategy termed \textbf{Multimodal
Bidirectional Attack}, tailored for RES models. Our method introduces learnable
proxy textual embedding perturbation and jointly performs visual-aligned
optimization on the image modality and textual-adversarial optimization on the
textual modality during attack generation. This dual optimization framework
encourages adversarial images to actively adapt to more challenging text
embedding during optimization, thereby enhancing their cross-text
transferability, which refers to the ability of adversarial examples to remain
effective under a variety of unseen or semantically diverse textual inputs.
Extensive experiments conducted on multiple RES models and benchmark datasets
demonstrate the superior effectiveness of our method compared to existing
methods.

</details>


### [32] [Co-Speech Gesture and Facial Expression Generation for Non-Photorealistic 3D Characters](https://arxiv.org/abs/2506.16159)
*Taisei Omine,Naoyuki Kawabata,Fuminori Homma*

Main category: cs.CV

TL;DR: 研究提出了一种针对非写实角色（如动漫）的情感表达方法，利用漫画中的表情数据和对话语义手势，显著优于现有研究。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注写实化虚拟角色，无法满足非写实角色的情感表达需求。

Method: 从漫画中提取表情数据，结合对话语义手势，设计非写实角色的情感表达方法。

Result: 用户研究表明，该方法在多个方面显著优于现有研究。

Conclusion: 该方法为非写实角色的情感表达提供了有效解决方案。

Abstract: With the advancement of conversational AI, research on bodily expressions,
including gestures and facial expressions, has also progressed. However, many
existing studies focus on photorealistic avatars, making them unsuitable for
non-photorealistic characters, such as those found in anime. This study
proposes methods for expressing emotions, including exaggerated expressions
unique to non-photorealistic characters, by utilizing expression data extracted
from comics and dialogue-specific semantic gestures. A user study demonstrated
significant improvements across multiple aspects when compared to existing
research.

</details>


### [33] [Align the GAP: Prior-based Unified Multi-Task Remote Physiological Measurement Framework For Domain Generalization and Personalization](https://arxiv.org/abs/2506.16160)
*Jiyao Wang,Xiao Yang,Hao Lu,Dengbo He,Kaishun Wu*

Main category: cs.CV

TL;DR: 提出了一种统一框架（GAP），结合多源语义域泛化（MSSDG）和测试时个性化适应（TTPA），通过先验知识和观察分解面部视频信息，实现多任务远程生理测量的泛化和个性化。


<details>
  <summary>Details</summary>
Motivation: 解决多任务远程生理测量中部分标注和环境噪声导致的泛化性不足问题，并探索测试时个性化适应的需求。

Method: 将面部视频信息分解为不变语义、个体偏差和噪声，利用先验知识和观察设计多模块框架，同时处理MSSDG和TTPA。

Result: 在六个公开数据集和新引入的驾驶数据集上验证了框架的有效性，代码和数据集将公开。

Conclusion: GAP框架通过统一方法解决了泛化和个性化问题，适用于多任务远程生理测量。

Abstract: Multi-source synsemantic domain generalization (MSSDG) for multi-task remote
physiological measurement seeks to enhance the generalizability of these
metrics and attracts increasing attention. However, challenges like partial
labeling and environmental noise may disrupt task-specific accuracy. Meanwhile,
given that real-time adaptation is necessary for personalized products, the
test-time personalized adaptation (TTPA) after MSSDG is also worth exploring,
while the gap between previous generalization and personalization methods is
significant and hard to fuse. Thus, we proposed a unified framework for
MSSD\textbf{G} and TTP\textbf{A} employing \textbf{P}riors (\textbf{GAP}) in
biometrics and remote photoplethysmography (rPPG). We first disentangled
information from face videos into invariant semantics, individual bias, and
noise. Then, multiple modules incorporating priors and our observations were
applied in different stages and for different facial information. Then, based
on the different principles of achieving generalization and personalization,
our framework could simultaneously address MSSDG and TTPA under multi-task
remote physiological estimation with minimal adjustments. We expanded the MSSDG
benchmark to the TTPA protocol on six publicly available datasets and
introduced a new real-world driving dataset with complete labeling. Extensive
experiments that validated our approach, and the codes along with the new
dataset will be released.

</details>


### [34] [Learning Multi-scale Spatial-frequency Features for Image Denoising](https://arxiv.org/abs/2506.16307)
*Xu Zhao,Chen Zhao,Xiantao Hu,Hongliang Zhang,Ying Tai,Jian Yang*

Main category: cs.CV

TL;DR: 提出了一种新型多尺度自适应双域网络（MADNet），用于图像去噪，通过自适应空间频率学习单元（ASFU）和全局特征融合块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定单输入单输出Unet架构，忽略多尺度像素级表示，且对高、低频噪声处理不足。

Method: 使用图像金字塔输入，设计ASFU单元分离高、低频信息，并在跳跃连接中引入全局特征融合块。

Result: 在合成和真实噪声图像数据集上验证了MADNet优于当前最先进去噪方法。

Conclusion: MADNet通过多尺度自适应双域设计，显著提升了图像去噪性能。

Abstract: Recent advancements in multi-scale architectures have demonstrated
exceptional performance in image denoising tasks. However, existing
architectures mainly depends on a fixed single-input single-output Unet
architecture, ignoring the multi-scale representations of pixel level. In
addition, previous methods treat the frequency domain uniformly, ignoring the
different characteristics of high-frequency and low-frequency noise. In this
paper, we propose a novel multi-scale adaptive dual-domain network (MADNet) for
image denoising. We use image pyramid inputs to restore noise-free results from
low-resolution images. In order to realize the interaction of high-frequency
and low-frequency information, we design an adaptive spatial-frequency learning
unit (ASFU), where a learnable mask is used to separate the information into
high-frequency and low-frequency components. In the skip connections, we design
a global feature fusion block to enhance the features at different scales.
Extensive experiments on both synthetic and real noisy image datasets verify
the effectiveness of MADNet compared with current state-of-the-art denoising
approaches.

</details>


### [35] [Integrating Generative Adversarial Networks and Convolutional Neural Networks for Enhanced Traffic Accidents Detection and Analysis](https://arxiv.org/abs/2506.16186)
*Zhenghao Xi,Xiang Liu,Yaqi Liu,Yitong Cai,Yangyu Zheng*

Main category: cs.CV

TL;DR: 该研究利用深度学习技术（GANs和CNN）解决交通事故检测中的数据不足问题，通过合成数据和模型训练，实现了高精度的实时事故检测。


<details>
  <summary>Details</summary>
Motivation: 全球交通事故数量上升，亟需智能、高效的自动化事故检测系统以提升交通安全。

Method: 结合GANs生成合成数据，使用CNN、FTCNN和VIT模型训练，并对视频帧进行预处理（调整大小、增强和归一化）。

Result: FTCNN和VIT模型的准确率分别为94%和95%，CNN为88%，验证了框架的高效性和实时检测能力。

Conclusion: 该框架为智能监控系统奠定了基础，适用于实时交通监测和智慧城市应用。

Abstract: Accident detection using Closed Circuit Television (CCTV) footage is one of
the most imperative features for enhancing transport safety and efficient
traffic control. To this end, this research addresses the issues of supervised
monitoring and data deficiency in accident detection systems by adapting
excellent deep learning technologies. The motivation arises from rising
statistics in the number of car accidents worldwide; this calls for innovation
and the establishment of a smart, efficient and automated way of identifying
accidents and calling for help to save lives. Addressing the problem of the
scarcity of data, the presented framework joins Generative Adversarial Networks
(GANs) for synthesizing data and Convolutional Neural Networks (CNN) for model
training. Video frames for accidents and non-accidents are collected from
YouTube videos, and we perform resizing, image enhancement and image
normalisation pixel range adjustments. Three models are used: CNN, Fine-tuned
Convolutional Neural Network (FTCNN) and Vision Transformer (VIT) worked best
for detecting accidents from CCTV, obtaining an accuracy rate of 94% and 95%,
while the CNN model obtained 88%. Such results show that the proposed framework
suits traffic safety applications due to its high real-time accident detection
capabilities and broad-scale applicability. This work lays the foundation for
intelligent surveillance systems in the future for real-time traffic
monitoring, smart city framework, and integration of intelligent surveillance
systems into emergency management systems.

</details>


### [36] [Efficient Transformations in Deep Learning Convolutional Neural Networks](https://arxiv.org/abs/2506.16418)
*Berk Yilmaz,Daniel Fidel Harvey,Prajit Dhuri*

Main category: cs.CV

TL;DR: 研究探讨了在ResNet50 CNN模型中集成FFT、WHT和DCT信号处理变换对图像分类的影响，发现WHT显著降低能耗并提高准确率。


<details>
  <summary>Details</summary>
Motivation: 评估计算效率、能耗和分类精度之间的权衡，为能源受限的CNN应用提供高效解决方案。

Method: 在ResNet50的早期和后期卷积层中集成WHT，使用CIFAR-100数据集进行实验。

Result: WHT版本模型测试准确率提升至79%，能耗降至39 kJ，显著优于基线模型。

Conclusion: WHT是一种高效且有效的信号处理变换，适用于能源受限的CNN应用。

Abstract: This study investigates the integration of signal processing transformations
-- Fast Fourier Transform (FFT), Walsh-Hadamard Transform (WHT), and Discrete
Cosine Transform (DCT) -- within the ResNet50 convolutional neural network
(CNN) model for image classification. The primary objective is to assess the
trade-offs between computational efficiency, energy consumption, and
classification accuracy during training and inference. Using the CIFAR-100
dataset (100 classes, 60,000 images), experiments demonstrated that
incorporating WHT significantly reduced energy consumption while improving
accuracy. Specifically, a baseline ResNet50 model achieved a testing accuracy
of 66%, consuming an average of 25,606 kJ per model. In contrast, a modified
ResNet50 incorporating WHT in the early convolutional layers achieved 74%
accuracy, and an enhanced version with WHT applied to both early and late
layers achieved 79% accuracy, with an average energy consumption of only 39 kJ
per model. These results demonstrate the potential of WHT as a highly efficient
and effective approach for energy-constrained CNN applications.

</details>


### [37] [VideoGAN-based Trajectory Proposal for Automated Vehicles](https://arxiv.org/abs/2506.16209)
*Annajoyce Mariani,Kira Maag,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 论文提出了一种基于GAN的方法，通过鸟瞰视角视频生成逼真的交通轨迹，解决了传统方法难以捕捉复杂多模态分布的问题。


<details>
  <summary>Details</summary>
Motivation: 提高道路车辆自动化程度需要生成逼真的轨迹选项，但现有方法难以有效捕捉复杂多模态的未来轨迹分布。

Method: 使用低分辨率鸟瞰视角占用网格视频训练视频生成模型，通过GAN架构快速生成交通场景视频，并从中提取抽象轨迹数据。

Result: 在100 GPU小时内完成训练，推理时间低于20毫秒，生成的轨迹在空间和动态参数上与真实数据分布对齐。

Conclusion: GAN方法能够高效生成统计准确的交通轨迹，适用于提高车辆自动化程度。

Abstract: Being able to generate realistic trajectory options is at the core of
increasing the degree of automation of road vehicles. While model-driven,
rule-based, and classical learning-based methods are widely used to tackle
these tasks at present, they can struggle to effectively capture the complex,
multimodal distributions of future trajectories. In this paper we investigate
whether a generative adversarial network (GAN) trained on videos of bird's-eye
view (BEV) traffic scenarios can generate statistically accurate trajectories
that correctly capture spatial relationships between the agents. To this end,
we propose a pipeline that uses low-resolution BEV occupancy grid videos as
training data for a video generative model. From the generated videos of
traffic scenarios we extract abstract trajectory data using single-frame object
detection and frame-to-frame object matching. We particularly choose a GAN
architecture for the fast training and inference times with respect to
diffusion models. We obtain our best results within 100 GPU hours of training,
with inference times under 20\,ms. We demonstrate the physical realism of the
proposed trajectories in terms of distribution alignment of spatial and dynamic
parameters with respect to the ground truth videos from the Waymo Open Motion
Dataset.

</details>


### [38] [MetaQAP -- A Meta-Learning Approach for Quality-Aware Pretraining in Image Quality Assessment](https://arxiv.org/abs/2506.16601)
*Muhammad Azeem Aslam,Muhammad Hamza,Nisar Ahmed,Gulshan Saleem,Zhu Shuangtong,Hu Hongfei,Xu Wei,Saba Aslam,Wang Jun*

Main category: cs.CV

TL;DR: MetaQAP是一种新型无参考图像质量评估模型，通过质量感知预训练和元学习解决IQA的挑战，在多个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 图像质量评估（IQA）因人类感知的主观性和真实世界图像失真的复杂性而具有挑战性，需要一种更有效的模型。

Method: MetaQAP结合了质量感知预训练的CNN、质量感知损失函数和元学习器，形成集成模型。

Result: 在LiveCD、KonIQ-10K和BIQ2021数据集上，PLCC和SROCC得分分别为0.9885/0.9812、0.9702/0.9658和0.884/0.8765，优于现有方法。

Conclusion: MetaQAP不仅解决了真实失真的复杂性，还为IQA应用提供了鲁棒且通用的框架，推动了无参考IQA的进展。

Abstract: Image Quality Assessment (IQA) is a critical task in a wide range of
applications but remains challenging due to the subjective nature of human
perception and the complexity of real-world image distortions. This study
proposes MetaQAP, a novel no-reference IQA model designed to address these
challenges by leveraging quality-aware pre-training and meta-learning. The
model performs three key contributions: pre-training Convolutional Neural
Networks (CNNs) on a quality-aware dataset, implementing a quality-aware loss
function to optimize predictions, and integrating a meta-learner to form an
ensemble model that effectively combines predictions from multiple base models.
Experimental evaluations were conducted on three benchmark datasets: LiveCD,
KonIQ-10K, and BIQ2021. The proposed MetaQAP model achieved exceptional
performance with Pearson Linear Correlation Coefficient (PLCC) and Spearman
Rank Order Correlation Coefficient (SROCC) scores of 0.9885/0.9812 on LiveCD,
0.9702/0.9658 on KonIQ-10K, and 0.884/0.8765 on BIQ2021, outperforming existing
IQA methods. Cross-dataset evaluations further demonstrated the
generalizability of the model, with PLCC and SROCC scores ranging from 0.6721
to 0.8023 and 0.6515 to 0.7805, respectively, across diverse datasets. The
ablation study confirmed the significance of each model component, revealing
substantial performance degradation when critical elements such as the
meta-learner or quality-aware loss function were omitted. MetaQAP not only
addresses the complexities of authentic distortions but also establishes a
robust and generalizable framework for practical IQA applications. By advancing
the state-of-the-art in no-reference IQA, this research provides valuable
insights and methodologies for future improvements and extensions in the field.

</details>


### [39] [FOCoOp: Enhancing Out-of-Distribution Robustness in Federated Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2506.16218)
*Xinting Liao,Weiming Liu,Jiaming Qian,Pengyang Zhou,Jiahe Xu,Wenjie Wang,Chaochao Chen,Xiaolin Zheng,Tat-Seng Chua*

Main category: cs.CV

TL;DR: FOCoOp是一种联邦OOD感知上下文优化框架，通过全局、局部和OOD提示解决联邦提示学习中性能与鲁棒性的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦提示学习方法在分布外（OOD）偏移时性能与鲁棒性难以兼顾，且客户端数据异构性加剧了这一挑战。

Method: FOCoOp利用三类提示（全局、局部、OOD）实现类别和分布级别的分离，并通过双层分布鲁棒优化适应OOD偏移，同时使用半不平衡最优传输校准提示。

Result: 实验表明FOCoOp能有效捕捉分布式异构分布并提升OOD偏移的鲁棒性。

Conclusion: FOCoOp填补了联邦提示学习在OOD场景下的性能与鲁棒性缺口，适用于现实场景。

Abstract: Federated prompt learning (FPL) for vision-language models is a powerful
approach to collaboratively adapt models across distributed clients while
preserving data privacy. However, existing FPL approaches suffer from a
trade-off between performance and robustness, particularly in
out-of-distribution (OOD) shifts, limiting their reliability in real-world
scenarios. The inherent in-distribution (ID) data heterogeneity among different
clients makes it more challenging to maintain this trade-off. To fill this gap,
we introduce a Federated OOD-aware Context Optimization (FOCoOp) framework,
which captures diverse distributions among clients using ID global prompts,
local prompts, and OOD prompts. Specifically, FOCoOp leverages three sets of
prompts to create both class-level and distribution-level separations, which
adapt to OOD shifts through bi-level distributionally robust optimization.
Additionally, FOCoOp improves the discrimination consistency among clients,
i.e., calibrating global prompts, seemingly OOD prompts, and OOD prompts by
semi-unbalanced optimal transport. The extensive experiments on real-world
datasets demonstrate that FOCoOp effectively captures decentralized
heterogeneous distributions and enhances robustness of different OOD shifts.
The project is available at GitHub.

</details>


### [40] [3DeepRep: 3D Deep Low-rank Tensor Representation for Hyperspectral Image Inpainting](https://arxiv.org/abs/2506.16735)
*Yunshan Li,Wenwu Gong,Qianqian Wang,Chao Wang,Lili Yang*

Main category: cs.CV

TL;DR: 提出了一种新的3方向深度低秩张量表示（3DeepRep）模型，通过在所有三个HSI张量模式上执行深度非线性变换，显著提升了HSI修复性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅限制变换在光谱模式上，忽略了其他张量模式的低秩特性，因此需要一种更全面的方法。

Method: 提出3DeepRep模型，通过在三个方向上最小化核范数，并结合可学习的聚合模块进行结果融合。

Result: 在真实HSI数据集上的实验表明，该方法在定性和定量上均优于现有技术。

Conclusion: 3DeepRep模型通过多方向深度变换和核范数优化，显著提升了HSI修复效果。

Abstract: Recent approaches based on transform-based tensor nuclear norm (TNN) have
demonstrated notable effectiveness in hyperspectral image (HSI) inpainting by
leveraging low-rank structures in latent representations. Recent developments
incorporate deep transforms to improve low-rank tensor representation; however,
existing approaches typically restrict the transform to the spectral mode,
neglecting low-rank properties along other tensor modes. In this paper, we
propose a novel 3-directional deep low-rank tensor representation (3DeepRep)
model, which performs deep nonlinear transforms along all three modes of the
HSI tensor. To enforce low-rankness, the model minimizes the nuclear norms of
mode-i frontal slices in the corresponding latent space for each direction
(i=1,2,3), forming a 3-directional TNN regularization. The outputs from the
three directional branches are subsequently fused via a learnable aggregation
module to produce the final result. An efficient gradient-based optimization
algorithm is developed to solve the model in a self-supervised manner.
Extensive experiments on real-world HSI datasets demonstrate that the proposed
method achieves superior inpainting performance compared to existing
state-of-the-art techniques, both qualitatively and quantitatively.

</details>


### [41] [R3eVision: A Survey on Robust Rendering, Restoration, and Enhancement for 3D Low-Level Vision](https://arxiv.org/abs/2506.16262)
*Weeyoung Kwon,Jeahun Sung,Minkyu Jeon,Chanho Eom,Jihyong Oh*

Main category: cs.CV

TL;DR: 该论文综述了3D低层视觉（3D LLV）领域，探讨了如何在退化条件下实现鲁棒的3D渲染、恢复和增强。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法（如NeRF和3DGS）通常假设输入为干净高分辨率图像，而现实场景中存在噪声、模糊、低分辨率等问题，限制了其鲁棒性。

Method: 论文通过形式化退化感知渲染问题，分类总结了将低层视觉任务（如超分辨率、去模糊）整合到神经渲染框架中的方法。

Result: 展示了这些方法在恶劣条件下实现高保真3D重建的能力，并讨论了其在自动驾驶、AR/VR等领域的应用。

Conclusion: 3D LLV是提升现实场景中3D内容生成和重建鲁棒性的重要方向。

Abstract: Neural rendering methods such as Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have achieved significant progress in photorealistic
3D scene reconstruction and novel view synthesis. However, most existing models
assume clean and high-resolution (HR) multi-view inputs, which limits their
robustness under real-world degradations such as noise, blur, low-resolution
(LR), and weather-induced artifacts. To address these limitations, the emerging
field of 3D Low-Level Vision (3D LLV) extends classical 2D Low-Level Vision
tasks including super-resolution (SR), deblurring, weather degradation removal,
restoration, and enhancement into the 3D spatial domain. This survey, referred
to as R\textsuperscript{3}eVision, provides a comprehensive overview of robust
rendering, restoration, and enhancement for 3D LLV by formalizing the
degradation-aware rendering problem and identifying key challenges related to
spatio-temporal consistency and ill-posed optimization. Recent methods that
integrate LLV into neural rendering frameworks are categorized to illustrate
how they enable high-fidelity 3D reconstruction under adverse conditions.
Application domains such as autonomous driving, AR/VR, and robotics are also
discussed, where reliable 3D perception from degraded inputs is critical. By
reviewing representative methods, datasets, and evaluation protocols, this work
positions 3D LLV as a fundamental direction for robust 3D content generation
and scene-level reconstruction in real-world environments.

</details>


### [42] [Reversing Flow for Image Restoration](https://arxiv.org/abs/2506.16961)
*Haina Qin,Wenyang Luo,Libin Wang,Dandan Zheng,Jingdong Chen,Ming Yang,Bing Li,Weiming Hu*

Main category: cs.CV

TL;DR: ResFlow提出了一种新的图像恢复框架，通过确定性路径建模退化过程，显著提升了性能和速度。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型（如扩散和基于分数的模型）将退化过程视为随机变换，导致效率低下和复杂性增加。

Method: ResFlow采用连续归一化流建模确定性退化路径，并通过辅助过程消除HQ预测的不确定性，实现可逆建模。

Result: ResFlow在少于4个采样步骤内完成任务，并在多个图像恢复基准上达到最先进水平。

Conclusion: ResFlow为实际应用提供了一种高效且实用的图像恢复解决方案。

Abstract: Image restoration aims to recover high-quality (HQ) images from degraded
low-quality (LQ) ones by reversing the effects of degradation. Existing
generative models for image restoration, including diffusion and score-based
models, often treat the degradation process as a stochastic transformation,
which introduces inefficiency and complexity. In this work, we propose ResFlow,
a novel image restoration framework that models the degradation process as a
deterministic path using continuous normalizing flows. ResFlow augments the
degradation process with an auxiliary process that disambiguates the
uncertainty in HQ prediction to enable reversible modeling of the degradation
process. ResFlow adopts entropy-preserving flow paths and learns the augmented
degradation flow by matching the velocity field. ResFlow significantly improves
the performance and speed of image restoration, completing the task in fewer
than four sampling steps. Extensive experiments demonstrate that ResFlow
achieves state-of-the-art results across various image restoration benchmarks,
offering a practical and efficient solution for real-world applications.

</details>


### [43] [Unsupervised Image Super-Resolution Reconstruction Based on Real-World Degradation Patterns](https://arxiv.org/abs/2506.17027)
*Yiyang Tie,Hong Zhu,Yunyun Luo,Jing Shi*

Main category: cs.CV

TL;DR: 提出了一种TripleGAN框架，通过两个GAN组件分别处理模糊特性和其他退化模式，第三个GAN用于重建真实低分辨率图像，显著提升了超分辨率重建的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在模拟真实世界退化模式时，无法同时捕捉模糊、噪声和隐式退化（如色域偏移），且合成数据与真实数据之间存在显著的退化域差距。

Method: 设计了TripleGAN框架：FirstGAN缩小模糊特性的域差距，SecondGAN学习目标域的模糊特性和其他退化模式，ThirdGAN基于伪真实数据重建真实低分辨率图像。

Result: 在RealSR和DRealSR数据集上的实验表明，该方法在定量指标上具有明显优势，且能保持重建图像的锐度，避免过度平滑。

Conclusion: TripleGAN框架能有效学习真实世界的退化模式，并生成对齐的退化数据集，从而显著提升超分辨率重建的质量。

Abstract: The training of real-world super-resolution reconstruction models heavily
relies on datasets that reflect real-world degradation patterns. Extracting and
modeling degradation patterns for super-resolution reconstruction using only
real-world low-resolution (LR) images remains a challenging task. When
synthesizing datasets to simulate real-world degradation, relying solely on
degradation extraction methods fails to capture both blur and diverse noise
characteristics across varying LR distributions, as well as more implicit
degradations such as color gamut shifts. Conversely, domain translation alone
cannot accurately approximate real-world blur characteristics due to the
significant degradation domain gap between synthetic and real data. To address
these challenges, we propose a novel TripleGAN framework comprising two
strategically designed components: The FirstGAN primarily focuses on narrowing
the domain gap in blur characteristics, while the SecondGAN performs
domain-specific translation to approximate target-domain blur properties and
learn additional degradation patterns. The ThirdGAN is trained on pseudo-real
data generated by the FirstGAN and SecondGAN to reconstruct real-world LR
images. Extensive experiments on the RealSR and DRealSR datasets demonstrate
that our method exhibits clear advantages in quantitative metrics while
maintaining sharp reconstructions without over-smoothing artifacts. The
proposed framework effectively learns real-world degradation patterns from LR
observations and synthesizes aligned datasets with corresponding degradation
characteristics, thereby enabling the trained network to achieve superior
performance in reconstructing high-quality SR images from real-world LR inputs.

</details>


### [44] [Fine-grained Image Retrieval via Dual-Vision Adaptation](https://arxiv.org/abs/2506.16273)
*Xin Jiang,Meiqi Cao,Hao Tang,Fei Shen,Zechao Li*

Main category: cs.CV

TL;DR: 论文提出了一种双视觉适应（DVA）方法，用于细粒度图像检索（FGIR），通过样本和特征适应协作指导预训练模型完成任务，提升了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前FGIR方法容易过拟合训练数据，忽视预训练知识，泛化能力不足。

Method: 设计了对象感知适应和上下文适应，分别调整输入样本和特征，同时通过知识蒸馏机制平衡检索效率和性能。

Result: DVA在多个数据集上表现优异，且参数较少。

Conclusion: DVA方法有效提升了FGIR的泛化能力和性能。

Abstract: Fine-Grained Image Retrieval~(FGIR) faces challenges in learning
discriminative visual representations to retrieve images with similar
fine-grained features. Current leading FGIR solutions typically follow two
regimes: enforce pairwise similarity constraints in the semantic embedding
space, or incorporate a localization sub-network to fine-tune the entire model.
However, such two regimes tend to overfit the training data while forgetting
the knowledge gained from large-scale pre-training, thus reducing their
generalization ability. In this paper, we propose a Dual-Vision Adaptation
(DVA) approach for FGIR, which guides the frozen pre-trained model to perform
FGIR through collaborative sample and feature adaptation. Specifically, we
design Object-Perceptual Adaptation, which modifies input samples to help the
pre-trained model perceive critical objects and elements within objects that
are helpful for category prediction. Meanwhile, we propose In-Context
Adaptation, which introduces a small set of parameters for feature adaptation
without modifying the pre-trained parameters. This makes the FGIR task using
these adjusted features closer to the task solved during the pre-training.
Additionally, to balance retrieval efficiency and performance, we propose
Discrimination Perception Transfer to transfer the discriminative knowledge in
the object-perceptual adaptation to the image encoder using the knowledge
distillation mechanism. Extensive experiments show that DVA has fewer learnable
parameters and performs well on three in-distribution and three
out-of-distribution fine-grained datasets.

</details>


### [45] [SycnMapV2: Robust and Adaptive Unsupervised Segmentation](https://arxiv.org/abs/2506.16297)
*Heng Zhang,Zikang Wan,Danilo Vasconcellos Vargas*

Main category: cs.CV

TL;DR: SyncMapV2是一种无监督分割算法，具有卓越的鲁棒性，能在噪声、天气和模糊等干扰下保持高精度，且无需重新初始化。


<details>
  <summary>Details</summary>
Motivation: 人类视觉在无明确训练的情况下仍能有效分割视觉线索，而现有AI算法在类似条件下表现不佳。SyncMapV2旨在解决这一问题。

Method: 基于自组织动力学方程和随机网络概念，SyncMapV2无需鲁棒训练、监督或损失函数，并能在线适应输入。

Result: 在数字干扰下，SyncMapV2的mIoU仅下降0.01%，远优于现有方法（下降23.8%）。在噪声、天气和模糊干扰下也表现优异。

Conclusion: SyncMapV2首次实现了在线自适应无监督分割，为未来鲁棒自适应智能的发展奠定了基础。

Abstract: Human vision excels at segmenting visual cues without the need for explicit
training, and it remains remarkably robust even as noise severity increases. In
contrast, existing AI algorithms struggle to maintain accuracy under similar
conditions. Here, we present SyncMapV2, the first to solve unsupervised
segmentation with state-of-the-art robustness. SyncMapV2 exhibits a minimal
drop in mIoU, only 0.01%, under digital corruption, compared to a 23.8% drop
observed in SOTA methods.This superior performance extends across various types
of corruption: noise (7.3% vs. 37.7%), weather (7.5% vs. 33.8%), and blur (7.0%
vs. 29.5%). Notably, SyncMapV2 accomplishes this without any robust training,
supervision, or loss functions. It is based on a learning paradigm that uses
self-organizing dynamical equations combined with concepts from random
networks. Moreover,unlike conventional methods that require re-initialization
for each new input, SyncMapV2 adapts online, mimicking the continuous
adaptability of human vision. Thus, we go beyond the accurate and robust
results, and present the first algorithm that can do all the above online,
adapting to input rather than re-initializing. In adaptability tests, SyncMapV2
demonstrates near-zero performance degradation, which motivates and fosters a
new generation of robust and adaptive intelligence in the near future.

</details>


### [46] [Segment Anything for Satellite Imagery: A Strong Baseline and a Regional Dataset for Automatic Field Delineation](https://arxiv.org/abs/2506.16318)
*Carmelo Scribano,Elena Govi,Paolo bertellini,Simone Parisi,Giorgia Franchini,Marko Bertogna*

Main category: cs.CV

TL;DR: 提出了一种基于Segment Anything Model（SAM）的农田边界自动提取方法，通过微调策略适应任务，并补充了新的区域数据集ERAS。


<details>
  <summary>Details</summary>
Motivation: 高分辨率卫星图像自动提取农田边界可避免昂贵的地面调查，提高农业操作效率。

Method: 基于SAM的农田边界提取流程，引入微调策略，并补充区域数据集ERAS。

Result: 实验验证了分割准确性和泛化能力，为自动化农田边界提取提供了稳健基准。

Conclusion: 方法有效，ERAS数据集已公开。

Abstract: Accurate mapping of agricultural field boundaries is essential for the
efficient operation of agriculture. Automatic extraction from high-resolution
satellite imagery, supported by computer vision techniques, can avoid costly
ground surveys. In this paper, we present a pipeline for field delineation
based on the Segment Anything Model (SAM), introducing a fine-tuning strategy
to adapt SAM to this task. In addition to using published datasets, we describe
a method for acquiring a complementary regional dataset that covers areas
beyond current sources. Extensive experiments assess segmentation accuracy and
evaluate the generalization capabilities. Our approach provides a robust
baseline for automated field delineation. The new regional dataset, known as
ERAS, is now publicly available.

</details>


### [47] [RealDriveSim: A Realistic Multi-Modal Multi-Task Synthetic Dataset for Autonomous Driving](https://arxiv.org/abs/2506.16319)
*Arpit Jadon,Haoran Wang,Phillip Thomas,Michael Stanley,S. Nathaniel Cibik,Rachel Laurat,Omar Maher,Lukas Hoyer,Ozan Unal,Dengxin Dai*

Main category: cs.CV

TL;DR: RealDriveSim是一个多模态合成数据集，用于自动驾驶，支持2D计算机视觉和LiDAR应用，提供64类精细标注，显著优于现有合成基准。


<details>
  <summary>Details</summary>
Motivation: 随着感知模型的发展，大规模数据集需求增加，但数据标注成本高昂，合成数据集成为低成本提升模型性能的解决方案。然而，现有合成数据集在范围、真实性和任务适用性上有限。

Method: 提出RealDriveSim，一个真实的多模态合成数据集，支持2D计算机视觉和LiDAR应用，并提供64类精细标注。

Result: 在多种应用和领域中广泛评估，展示了优于现有合成基准的先进性能。

Conclusion: RealDriveSim为自动驾驶提供了高质量、多模态的合成数据集，解决了现有合成数据集的局限性。

Abstract: As perception models continue to develop, the need for large-scale datasets
increases. However, data annotation remains far too expensive to effectively
scale and meet the demand. Synthetic datasets provide a solution to boost model
performance with substantially reduced costs. However, current synthetic
datasets remain limited in their scope, realism, and are designed for specific
tasks and applications. In this work, we present RealDriveSim, a realistic
multi-modal synthetic dataset for autonomous driving that not only supports
popular 2D computer vision applications but also their LiDAR counterparts,
providing fine-grained annotations for up to 64 classes. We extensively
evaluate our dataset for a wide range of applications and domains,
demonstrating state-of-the-art results compared to existing synthetic
benchmarks. The dataset is publicly available at
https://realdrivesim.github.io/.

</details>


### [48] [Reliable Few-shot Learning under Dual Noises](https://arxiv.org/abs/2506.16330)
*Ji Zhang,Jingkuan Song,Lianli Gao,Nicu Sebe,Heng Tao Shen*

Main category: cs.CV

TL;DR: DETA++提出了一种用于可靠少样本学习的方法，通过去噪任务适应和噪声鲁棒性策略解决ID和OOD噪声问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在开放世界中可能因ID和OOD噪声而失败，DETA++旨在解决这一问题。

Method: 使用CoRA模块计算支持样本权重，提出干净原型损失和噪声熵最大化损失，并利用LocalNCC和IntraSwap策略增强鲁棒性。

Result: 实验证明DETA++在噪声环境下具有有效性和灵活性。

Conclusion: DETA++通过去噪和鲁棒性策略显著提升了少样本学习的可靠性。

Abstract: Recent advances in model pre-training give rise to task adaptation-based
few-shot learning (FSL), where the goal is to adapt a pre-trained task-agnostic
model for capturing task-specific knowledge with a few-labeled support samples
of the target task.Nevertheless, existing approaches may still fail in the open
world due to the inevitable in-distribution (ID) and out-of-distribution (OOD)
noise from both support and query samples of the target task. With limited
support samples available, i) the adverse effect of the dual noises can be
severely amplified during task adaptation, and ii) the adapted model can
produce unreliable predictions on query samples in the presence of the dual
noises. In this work, we propose DEnoised Task Adaptation (DETA++) for reliable
FSL. DETA++ uses a Contrastive Relevance Aggregation (CoRA) module to calculate
image and region weights for support samples, based on which a clean prototype
loss and a noise entropy maximization loss are proposed to achieve noise-robust
task adaptation. Additionally,DETA++ employs a memory bank to store and refine
clean regions for each inner-task class, based on which a Local Nearest
Centroid Classifier (LocalNCC) is devised to yield noise-robust predictions on
query samples. Moreover, DETA++ utilizes an Intra-class Region Swapping
(IntraSwap) strategy to rectify ID class prototypes during task adaptation,
enhancing the model's robustness to the dual noises. Extensive experiments
demonstrate the effectiveness and flexibility of DETA++.

</details>


### [49] [Transparency Techniques for Neural Networks trained on Writer Identification and Writer Verification](https://arxiv.org/abs/2506.16331)
*Viktoria Pundy,Marco Peer,Florian Kleber*

Main category: cs.CV

TL;DR: 论文研究了神经网络在笔迹识别和验证中的透明度问题，首次应用了两种透明度技术，评估结果显示像素级显著性图优于点特定显著性图。


<details>
  <summary>Details</summary>
Motivation: 提高神经网络在笔迹识别和验证中的透明度和可靠性，为法医专家提供支持。

Method: 应用像素级和点特定显著性图技术，并通过删除和插入评分指标进行评估。

Result: 像素级显著性图表现优于点特定显著性图，适合法医专家使用。

Conclusion: 像素级显著性图在支持法医专家方面更具潜力，未来可进一步探索其应用。

Abstract: Neural Networks are the state of the art for many tasks in the computer
vision domain, including Writer Identification (WI) and Writer Verification
(WV). The transparency of these "black box" systems is important for
improvements of performance and reliability. For this work, two transparency
techniques are applied to neural networks trained on WI and WV for the first
time in this domain. The first technique provides pixel-level saliency maps,
while the point-specific saliency maps of the second technique provide
information on similarities between two images. The transparency techniques are
evaluated using deletion and insertion score metrics. The goal is to support
forensic experts with information on similarities in handwritten text and to
explore the characteristics selected by a neural network for the identification
process. For the qualitative evaluation, the highlights of the maps are
compared to the areas forensic experts consider during the identification
process. The evaluation results show that the pixel-wise saliency maps
outperform the point-specific saliency maps and are suitable for the support of
forensic experts.

</details>


### [50] [MambaHash: Visual State Space Deep Hashing Model for Large-Scale Image Retrieval](https://arxiv.org/abs/2506.16353)
*Chao He,Hongxi Wei*

Main category: cs.CV

TL;DR: MambaHash是一种基于视觉状态空间的深度哈希模型，利用Mamba操作和通道交互注意力模块提升图像检索性能。


<details>
  <summary>Details</summary>
Motivation: 探索Mamba在大规模图像检索任务中的适用性，并提升检索效率与性能。

Method: 提出分阶段的主干网络，引入分组Mamba操作和多方向扫描，结合通道交互注意力模块和自适应特征增强模块。

Result: 在CIFAR-10、NUS-WIDE和IMAGENET数据集上表现优于现有深度哈希方法。

Conclusion: MambaHash在效率和性能上均表现出色，适用于大规模图像检索任务。

Abstract: Deep image hashing aims to enable effective large-scale image retrieval by
mapping the input images into simple binary hash codes through deep neural
networks. More recently, Vision Mamba with linear time complexity has attracted
extensive attention from researchers by achieving outstanding performance on
various computer tasks. Nevertheless, the suitability of Mamba for large-scale
image retrieval tasks still needs to be explored. Towards this end, we propose
a visual state space hashing model, called MambaHash. Concretely, we propose a
backbone network with stage-wise architecture, in which grouped Mamba operation
is introduced to model local and global information by utilizing Mamba to
perform multi-directional scanning along different groups of the channel.
Subsequently, the proposed channel interaction attention module is used to
enhance information communication across channels. Finally, we meticulously
design an adaptive feature enhancement module to increase feature diversity and
enhance the visual representation capability of the model. We have conducted
comprehensive experiments on three widely used datasets: CIFAR-10, NUS-WIDE and
IMAGENET. The experimental results demonstrate that compared with the
state-of-the-art deep hashing methods, our proposed MambaHash has well
efficiency and superior performance to effectively accomplish large-scale image
retrieval tasks. Source code is available
https://github.com/shuaichaochao/MambaHash.git

</details>


### [51] [Prompt-based Dynamic Token Pruning to Guide Transformer Attention in Efficient Segmentation](https://arxiv.org/abs/2506.16369)
*Pallabi Dutta,Anubhab Maity,Sushmita Mitra*

Main category: cs.CV

TL;DR: 提出了一种自适应提示引导的剪枝方法，减少ViT处理无关标记的计算负担，提升医学图像分割效率。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers（ViTs）在处理大量标记时计算需求高，限制了其在医学图像分析中的实际应用。

Method: 通过提示引导的空间先验对标记进行相关性排序，剪枝低相关标记，仅保留相关标记进行后续处理。

Result: 实验显示标记减少35-55%，计算成本降低，同时保持分割精度。

Conclusion: 该方法提升了计算效率，适用于资源受限环境，促进实时诊断。

Abstract: The high computational demands of Vision Transformers (ViTs), in processing a
huge number of tokens, often constrain their practical application in analyzing
medical images. This research proposes an adaptive prompt-guided pruning method
to selectively reduce the processing of irrelevant tokens in the segmentation
pipeline. The prompt-based spatial prior helps to rank the tokens according to
their relevance. Tokens with low-relevance scores are down-weighted, ensuring
that only the relevant ones are propagated for processing across subsequent
stages. This data-driven pruning strategy facilitates end-to-end training,
maintains gradient flow, and improves segmentation accuracy by focusing
computational resources on essential regions. The proposed framework is
integrated with several state-of-the-art models to facilitate the elimination
of irrelevant tokens; thereby, enhancing computational efficiency while
preserving segmentation accuracy. The experimental results show a reduction of
$\sim$ 35-55\% tokens; thus reducing the computational costs relative to the
baselines. Cost-effective medical image processing, using our framework,
facilitates real-time diagnosis by expanding its applicability in
resource-constrained environments.

</details>


### [52] [AGC-Drive: A Large-Scale Dataset for Real-World Aerial-Ground Collaboration in Driving Scenarios](https://arxiv.org/abs/2506.16371)
*Yunhao Hou,Bochao Zou,Min Zhang,Ran Chen,Shangdong Yang,Yanmei Zhang,Junbao Zhuo,Siheng Chen,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: AGC-Drive是首个大规模真实世界空中-地面协作3D感知数据集，填补了无人机视角在协作感知中的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注车辆间或车辆与基础设施的协作感知，而无人机提供的动态俯视视角能显著缓解遮挡问题，但缺乏高质量数据集支持。

Method: 通过两辆配备多摄像头和LiDAR的车辆及一架搭载摄像头和LiDAR的无人机，采集多视角多智能体感知数据，覆盖14种驾驶场景。

Result: 数据集包含120K LiDAR帧和440K图像，标注了13类物体的3D边界框，并提供两种协作感知任务的基准。

Conclusion: AGC-Drive为空中-地面协作感知研究提供了重要资源，并开源了工具包以支持进一步研究。

Abstract: By sharing information across multiple agents, collaborative perception helps
autonomous vehicles mitigate occlusions and improve overall perception
accuracy. While most previous work focus on vehicle-to-vehicle and
vehicle-to-infrastructure collaboration, with limited attention to aerial
perspectives provided by UAVs, which uniquely offer dynamic, top-down views to
alleviate occlusions and monitor large-scale interactive environments. A major
reason for this is the lack of high-quality datasets for aerial-ground
collaborative scenarios. To bridge this gap, we present AGC-Drive, the first
large-scale real-world dataset for Aerial-Ground Cooperative 3D perception. The
data collection platform consists of two vehicles, each equipped with five
cameras and one LiDAR sensor, and one UAV carrying a forward-facing camera and
a LiDAR sensor, enabling comprehensive multi-view and multi-agent perception.
Consisting of approximately 120K LiDAR frames and 440K images, the dataset
covers 14 diverse real-world driving scenarios, including urban roundabouts,
highway tunnels, and on/off ramps. Notably, 19.5% of the data comprises dynamic
interaction events, including vehicle cut-ins, cut-outs, and frequent lane
changes. AGC-Drive contains 400 scenes, each with approximately 100 frames and
fully annotated 3D bounding boxes covering 13 object categories. We provide
benchmarks for two 3D perception tasks: vehicle-to-vehicle collaborative
perception and vehicle-to-UAV collaborative perception. Additionally, we
release an open-source toolkit, including spatiotemporal alignment verification
tools, multi-agent visualization systems, and collaborative annotation
utilities. The dataset and code are available at
https://github.com/PercepX/AGC-Drive.

</details>


### [53] [CLIP-MG: Guiding Semantic Attention with Skeletal Pose Features and RGB Data for Micro-Gesture Recognition on the iMiGUE Dataset](https://arxiv.org/abs/2506.16385)
*Santosh Patapati,Trisanth Srinivasan,Amith Adiraju*

Main category: cs.CV

TL;DR: 论文提出了一种基于CLIP的架构CLIP-MG，用于微手势识别，通过结合姿态信息和多模态融合机制，在iMiGUE数据集上取得了61.82%的Top-1准确率。


<details>
  <summary>Details</summary>
Motivation: 微手势因其细微、不自主的特性以及低幅度运动，是情感计算中的一项挑战性任务。

Method: 采用Pose-Guided Semantics-Aware CLIP-based架构（CLIP-MG），通过姿态引导的语义查询生成和门控多模态融合机制，将人体姿态信息整合到识别流程中。

Result: 模型在iMiGUE数据集上的Top-1准确率为61.82%。

Conclusion: 该方法展示了CLIP等视觉语言模型在微手势识别中的潜力，但也表明完全适应此类任务仍具挑战性。

Abstract: Micro-gesture recognition is a challenging task in affective computing due to
the subtle, involuntary nature of the gestures and their low movement
amplitude. In this paper, we introduce a Pose-Guided Semantics-Aware CLIP-based
architecture, or CLIP for Micro-Gesture recognition (CLIP-MG), a modified CLIP
model tailored for micro-gesture classification on the iMiGUE dataset. CLIP-MG
integrates human pose (skeleton) information into the CLIP-based recognition
pipeline through pose-guided semantic query generation and a gated multi-modal
fusion mechanism. The proposed model achieves a Top-1 accuracy of 61.82%. These
results demonstrate both the potential of our approach and the remaining
difficulty in fully adapting vision-language models like CLIP for micro-gesture
recognition.

</details>


### [54] [HyperPath: Knowledge-Guided Hyperbolic Semantic Hierarchy Modeling for WSI Analysis](https://arxiv.org/abs/2506.16398)
*Peixiang Huang,Yanyan Huang,Weiqin Zhao,Junjun He,Lequan Yu*

Main category: cs.CV

TL;DR: 论文提出了一种名为HyperPath的新方法，利用双曲空间建模WSI的语义层次结构，结合视觉和文本特征，通过几何感知方法提升WSI分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有的MIL方法主要依赖欧几里得嵌入，难以充分捕捉WSI的语义层次结构，因此需要一种更有效的方法来建模这种层次关系。

Method: 提出HyperPath方法，将视觉和文本特征映射到双曲空间，设计角度模态对齐损失和语义层次一致性损失，利用测地距离进行分类。

Result: 实验表明，HyperPath在多个任务上优于现有方法，验证了双曲嵌入在WSI分析中的潜力。

Conclusion: HyperPath通过双曲空间建模语义层次，显著提升了WSI分类性能，为病理图像分析提供了新思路。

Abstract: Pathology is essential for cancer diagnosis, with multiple instance learning
(MIL) widely used for whole slide image (WSI) analysis. WSIs exhibit a natural
hierarchy -- patches, regions, and slides -- with distinct semantic
associations. While some methods attempt to leverage this hierarchy for
improved representation, they predominantly rely on Euclidean embeddings, which
struggle to fully capture semantic hierarchies. To address this limitation, we
propose HyperPath, a novel method that integrates knowledge from textual
descriptions to guide the modeling of semantic hierarchies of WSIs in
hyperbolic space, thereby enhancing WSI classification. Our approach adapts
both visual and textual features extracted by pathology vision-language
foundation models to the hyperbolic space. We design an Angular Modality
Alignment Loss to ensure robust cross-modal alignment, while a Semantic
Hierarchy Consistency Loss further refines feature hierarchies through
entailment and contradiction relationships and thus enhance semantic coherence.
The classification is performed with geodesic distance, which measures the
similarity between entities in the hyperbolic semantic hierarchy. This
eliminates the need for linear classifiers and enables a geometry-aware
approach to WSI analysis. Extensive experiments show that our method achieves
superior performance across tasks compared to existing methods, highlighting
the potential of hyperbolic embeddings for WSI analysis.

</details>


### [55] [Robustness Evaluation of OCR-based Visual Document Understanding under Multi-Modal Adversarial Attacks](https://arxiv.org/abs/2506.16407)
*Dong Nguyen Tien,Dung D. Le*

Main category: cs.CV

TL;DR: 论文提出了首个统一框架，用于生成和评估基于OCR的VDU模型的多模态对抗攻击，涵盖六种梯度布局攻击场景。实验表明，行级攻击和复合扰动（BBox + Pixel + Text）对性能影响最大。


<details>
  <summary>Details</summary>
Motivation: 探索VDU系统在现实对抗扰动下的鲁棒性，填补现有研究的不足。

Method: 采用梯度布局攻击方法，包括OCR边界框、像素和文本的扰动，约束布局扰动预算以保持合理性。

Result: 在四个数据集和六种模型上，行级攻击和复合扰动导致性能显著下降，PGD-based BBox扰动优于随机基线。

Conclusion: 布局预算、文本修改和对抗可转移性对攻击效果有显著影响。

Abstract: Visual Document Understanding (VDU) systems have achieved strong performance
in information extraction by integrating textual, layout, and visual signals.
However, their robustness under realistic adversarial perturbations remains
insufficiently explored. We introduce the first unified framework for
generating and evaluating multi-modal adversarial attacks on OCR-based VDU
models. Our method covers six gradient-based layout attack scenarios,
incorporating manipulations of OCR bounding boxes, pixels, and texts across
both word and line granularities, with constraints on layout perturbation
budget (e.g., IoU >= 0.6) to preserve plausibility.
  Experimental results across four datasets (FUNSD, CORD, SROIE, DocVQA) and
six model families demonstrate that line-level attacks and compound
perturbations (BBox + Pixel + Text) yield the most severe performance
degradation. Projected Gradient Descent (PGD)-based BBox perturbations
outperform random-shift baselines in all investigated models. Ablation studies
further validate the impact of layout budget, text modification, and
adversarial transferability.

</details>


### [56] [Structured Semantic 3D Reconstruction (S23DR) Challenge 2025 -- Winning solution](https://arxiv.org/abs/2506.16421)
*Jan Skvrna,Lukas Neumann*

Main category: cs.CV

TL;DR: 本文介绍了S23DR Challenge 2025的获胜方案，通过3D深度学习从稀疏点云和语义分割预测房屋的3D屋顶线框。


<details>
  <summary>Details</summary>
Motivation: 解决从稀疏点云和语义分割中预测房屋3D屋顶线框的挑战。

Method: 直接在3D空间中操作，首先利用Gestalt分割从COLMAP点云识别顶点候选，然后使用两个PointNet-like模型：一个用于通过分析局部立方块来细化和分类候选顶点，另一个用于通过处理连接顶点对的圆柱区域来预测边。

Result: 在私有排行榜上以0.43的混合结构分数（HSS）获胜。

Conclusion: 两阶段的3D深度学习方法在预测3D屋顶线框任务中表现出色。

Abstract: This paper presents the winning solution for the S23DR Challenge 2025, which
involves predicting a house's 3D roof wireframe from a sparse point cloud and
semantic segmentations. Our method operates directly in 3D, first identifying
vertex candidates from the COLMAP point cloud using Gestalt segmentations. We
then employ two PointNet-like models: one to refine and classify these
candidates by analyzing local cubic patches, and a second to predict edges by
processing the cylindrical regions connecting vertex pairs. This two-stage, 3D
deep learning approach achieved a winning Hybrid Structure Score (HSS) of 0.43
on the private leaderboard.

</details>


### [57] [How Far Can Off-the-Shelf Multimodal Large Language Models Go in Online Episodic Memory Question Answering?](https://arxiv.org/abs/2506.16450)
*Giuseppe Lando,Rosario Forte,Giovanni Maria Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: 研究探讨了现成的多模态大语言模型（MLLMs）能否在不额外训练的情况下处理在线情景记忆视频问答（OEM-VQA），通过轻量级文本记忆实现高效存储与推理。


<details>
  <summary>Details</summary>
Motivation: 探索现成MLLMs在OEM-VQA任务中的潜力，避免额外训练成本，同时实现高效存储。

Method: 将流式视频转换为轻量级文本记忆（每分钟仅几KB），通过MLLM描述模块和LLM推理模块回答问题。

Result: 在QAEgo4D-Closed基准上达到56.0%准确率，存储效率比现有系统高10^4/10^5倍。

Conclusion: 该方法展示了现成MLLMs在OEM-VQA任务中的可行性，为未来研究提供了改进方向。

Abstract: We investigate whether off-the-shelf Multimodal Large Language Models (MLLMs)
can tackle Online Episodic-Memory Video Question Answering (OEM-VQA) without
additional training. Our pipeline converts a streaming egocentric video into a
lightweight textual memory, only a few kilobytes per minute, via an MLLM
descriptor module, and answers multiple-choice questions by querying this
memory with an LLM reasoner module. On the QAEgo4D-Closed benchmark, our best
configuration attains 56.0% accuracy with 3.6 kB per minute storage, matching
the performance of dedicated state-of-the-art systems while being 10**4/10**5
times more memory-efficient. Extensive ablations provides insights into the
role of each component and design choice, and highlight directions of
improvement for future research.

</details>


### [58] [Spotting tell-tale visual artifacts in face swapping videos: strengths and pitfalls of CNN detectors](https://arxiv.org/abs/2506.16497)
*Riccardo Ziglio,Cecilia Pasquini,Silvio Ranise*

Main category: cs.CV

TL;DR: 论文探讨了基于CNN的模型在视频换脸检测中的表现，发现其在同源数据上表现优异，但在跨数据集泛化时难以捕捉遮挡线索，需专门策略。


<details>
  <summary>Details</summary>
Motivation: 视频换脸技术日益成熟，威胁远程视频通信安全，需研究其检测方法。

Method: 通过两个数据集（包括新收集的数据集）测试CNN模型的性能，并分析其对不同来源和换脸算法的泛化能力。

Result: CNN在同源数据上表现优异，但在跨数据集时难以捕捉遮挡线索。

Conclusion: 需开发专门策略以检测换脸中的遮挡线索。

Abstract: Face swapping manipulations in video streams represents an increasing threat
in remote video communications, due to advances
  in automated and real-time tools. Recent literature proposes to characterize
and exploit visual artifacts introduced in video frames
  by swapping algorithms when dealing with challenging physical scenes, such as
face occlusions. This paper investigates the
  effectiveness of this approach by benchmarking CNN-based data-driven models
on two data corpora (including a newly collected
  one) and analyzing generalization capabilities with respect to different
acquisition sources and swapping algorithms. The results
  confirm excellent performance of general-purpose CNN architectures when
operating within the same data source, but a significant
  difficulty in robustly characterizing occlusion-based visual cues across
datasets. This highlights the need for specialized detection
  strategies to deal with such artifacts.

</details>


### [59] [Hunyuan3D 2.5: Towards High-Fidelity 3D Assets Generation with Ultimate Details](https://arxiv.org/abs/2506.16504)
*Zeqiang Lai,Yunfei Zhao,Haolin Liu,Zibo Zhao,Qingxiang Lin,Huiwen Shi,Xianghui Yang,Mingxin Yang,Shuhui Yang,Yifei Feng,Sheng Zhang,Xin Huang,Di Luo,Fan Yang,Fang Yang,Lifu Wang,Sicong Liu,Yixuan Tang,Yulin Cai,Zebin He,Tian Liu,Yuhong Liu,Jie Jiang,Linus,Jingwei Huang,Chunchao Guo*

Main category: cs.CV

TL;DR: Hunyuan3D 2.5是一套强大的3D扩散模型，通过两阶段流程显著提升了形状和纹理生成的质量。


<details>
  <summary>Details</summary>
Motivation: 旨在生成高保真且细节丰富的3D资产，缩小生成与手工制作3D形状之间的差距。

Method: 采用两阶段流程，引入新的形状基础模型LATTICE（10B参数），并升级纹理生成技术，支持基于物理的渲染（PBR）。

Result: 在形状和端到端纹理生成方面显著优于之前的方法。

Conclusion: Hunyuan3D 2.5在3D资产生成领域取得了重要进展。

Abstract: In this report, we present Hunyuan3D 2.5, a robust suite of 3D diffusion
models aimed at generating high-fidelity and detailed textured 3D assets.
Hunyuan3D 2.5 follows two-stages pipeline of its previous version Hunyuan3D
2.0, while demonstrating substantial advancements in both shape and texture
generation. In terms of shape generation, we introduce a new shape foundation
model -- LATTICE, which is trained with scaled high-quality datasets,
model-size, and compute. Our largest model reaches 10B parameters and generates
sharp and detailed 3D shape with precise image-3D following while keeping mesh
surface clean and smooth, significantly closing the gap between generated and
handcrafted 3D shapes. In terms of texture generation, it is upgraded with
phyiscal-based rendering (PBR) via a novel multi-view architecture extended
from Hunyuan3D 2.0 Paint model. Our extensive evaluation shows that Hunyuan3D
2.5 significantly outperforms previous methods in both shape and end-to-end
texture generation.

</details>


### [60] [How Hard Is Snow? A Paired Domain Adaptation Dataset for Clear and Snowy Weather: CADC+](https://arxiv.org/abs/2506.16531)
*Mei Qi Tang,Sean Sedwards,Chengjie Huang,Krzysztof Czarnecki*

Main category: cs.CV

TL;DR: CADC+是一个针对冬季自动驾驶的配对天气域适应数据集，解决了现有数据集中雪天和晴天数据不足或合成数据不真实的问题。


<details>
  <summary>Details</summary>
Motivation: 研究雪天对3D物体检测性能的影响，但现有数据集缺乏足够的雪天和晴天配对数据，且合成数据不真实。

Method: 通过扩展CADC数据集，创建CADC+，将雪天和晴天数据配对，最小化非雪因素引起的域偏移。

Result: 初步结果显示，雪天既引入噪声，又形成独特的数据域，增加了不确定性。

Conclusion: CADC+为研究雪天对3D物体检测的影响提供了更真实的数据基础。

Abstract: The impact of snowfall on 3D object detection performance remains
underexplored. Conducting such an evaluation requires a dataset with sufficient
labelled data from both weather conditions, ideally captured in the same
driving environment. Current driving datasets with LiDAR point clouds either do
not provide enough labelled data in both snowy and clear weather conditions, or
rely on de-snowing methods to generate synthetic clear weather. Synthetic data
often lacks realism and introduces an additional domain shift that confounds
accurate evaluations. To address these challenges, we present CADC+, the first
paired weather domain adaptation dataset for autonomous driving in winter
conditions. CADC+ extends the Canadian Adverse Driving Conditions dataset
(CADC) using clear weather data that was recorded on the same roads and in the
same period as CADC. To create CADC+, we pair each CADC sequence with a clear
weather sequence that matches the snowy sequence as closely as possible. CADC+
thus minimizes the domain shift resulting from factors unrelated to the
presence of snow. We also present some preliminary results using CADC+ to
evaluate the effect of snow on 3D object detection performance. We observe that
snow introduces a combination of aleatoric and epistemic uncertainties, acting
as both noise and a distinct data domain.

</details>


### [61] [From Semantic To Instance: A Semi-Self-Supervised Learning Approach](https://arxiv.org/abs/2506.16563)
*Keyhan Najafian,Farhad Maleki,Lingling Jin,Ian Stavness*

Main category: cs.CV

TL;DR: 提出了一种半自监督学习方法GLMask，用于实例分割，减少对大量手动标注的依赖，在农业和通用数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决农业图像中密集、遮挡物体实例分割标注成本高的问题，推动深度学习在该领域的应用。

Method: 设计GLMask图像-掩码表示，关注形状、纹理和模式，减少对颜色特征的依赖；通过生成语义分割并转换为实例分割。

Result: 在小麦头实例分割任务中达到98.5% mAP@50，在COCO数据集上性能提升12.6%。

Conclusion: GLMask方法高效且通用，适用于农业及其他类似数据特征的领域。

Abstract: Instance segmentation is essential for applications such as automated
monitoring of plant health, growth, and yield. However, extensive effort is
required to create large-scale datasets with pixel-level annotations of each
object instance for developing instance segmentation models that restrict the
use of deep learning in these areas. This challenge is more significant in
images with densely packed, self-occluded objects, which are common in
agriculture. To address this challenge, we propose a semi-self-supervised
learning approach that requires minimal manual annotation to develop a
high-performing instance segmentation model. We design GLMask, an image-mask
representation for the model to focus on shape, texture, and pattern while
minimizing its dependence on color features. We develop a pipeline to generate
semantic segmentation and then transform it into instance-level segmentation.
The proposed approach substantially outperforms the conventional instance
segmentation models, establishing a state-of-the-art wheat head instance
segmentation model with mAP@50 of 98.5%. Additionally, we assessed the proposed
methodology on the general-purpose Microsoft COCO dataset, achieving a
significant performance improvement of over 12.6% mAP@50. This highlights that
the utility of our proposed approach extends beyond precision agriculture and
applies to other domains, specifically those with similar data characteristics.

</details>


### [62] [SafeTriage: Facial Video De-identification for Privacy-Preserving Stroke Triage](https://arxiv.org/abs/2506.16578)
*Tongan Cai,Haomiao Ni,Wenchao Ma,Yuan Xue,Qian Ma,Rachel Leicht,Kelvin Wong,John Volpi,Stephen T. C. Wong,James Z. Wang,Sharon X. Huang*

Main category: cs.CV

TL;DR: SafeTriage是一种新方法，通过去识别化患者面部视频保留关键运动特征，用于AI中风分诊，同时解决隐私问题。


<details>
  <summary>Details</summary>
Motivation: 解决AI模型依赖真实患者数据带来的隐私和伦理问题，同时保留诊断相关的面部动态特征。

Method: 利用预训练视频运动转移模型将真实患者面部运动映射到合成身份上，并通过条件生成模型调整输入空间以确保准确运动转移。

Result: 合成视频有效保留了中风相关的面部模式，同时提供强大的隐私保护，维持诊断准确性。

Conclusion: SafeTriage为神经疾病数据共享和AI临床分析提供了安全且伦理的基础。

Abstract: Effective stroke triage in emergency settings often relies on clinicians'
ability to identify subtle abnormalities in facial muscle coordination. While
recent AI models have shown promise in detecting such patterns from patient
facial videos, their reliance on real patient data raises significant ethical
and privacy challenges -- especially when training robust and generalizable
models across institutions. To address these concerns, we propose SafeTriage, a
novel method designed to de-identify patient facial videos while preserving
essential motion cues crucial for stroke diagnosis. SafeTriage leverages a
pretrained video motion transfer (VMT) model to map the motion characteristics
of real patient faces onto synthetic identities. This approach retains
diagnostically relevant facial dynamics without revealing the patients'
identities. To mitigate the distribution shift between normal population
pre-training videos and patient population test videos, we introduce a
conditional generative model for visual prompt tuning, which adapts the input
space of the VMT model to ensure accurate motion transfer without needing to
fine-tune the VMT model backbone. Comprehensive evaluation, including
quantitative metrics and clinical expert assessments, demonstrates that
SafeTriage-produced synthetic videos effectively preserve stroke-relevant
facial patterns, enabling reliable AI-based triage. Our evaluations also show
that SafeTriage provides robust privacy protection while maintaining diagnostic
accuracy, offering a secure and ethically sound foundation for data sharing and
AI-driven clinical analysis in neurological disorders.

</details>


### [63] [Spatially-Aware Evaluation of Segmentation Uncertainty](https://arxiv.org/abs/2506.16589)
*Tal Zeevi,Eléonore V. Lieffrig,Lawrence H. Staib,John A. Onofrey*

Main category: cs.CV

TL;DR: 论文提出三种空间感知的指标，结合结构和边界信息，用于评估医学图像分割中的不确定性，优于传统独立像素评估方法。


<details>
  <summary>Details</summary>
Motivation: 传统不确定性评估指标忽略空间上下文和解剖结构，无法区分不同模式的不确定性（如分散与边界对齐）。

Method: 提出三种结合结构和边界信息的空间感知指标，并在前列腺分区分割挑战数据上进行验证。

Result: 新指标能更好地区分有意义和虚假的不确定性模式，并与临床重要因素更一致。

Conclusion: 空间感知指标在医学图像分割中能更有效地评估不确定性。

Abstract: Uncertainty maps highlight unreliable regions in segmentation predictions.
However, most uncertainty evaluation metrics treat voxels independently,
ignoring spatial context and anatomical structure. As a result, they may assign
identical scores to qualitatively distinct patterns (e.g., scattered vs.
boundary-aligned uncertainty). We propose three spatially aware metrics that
incorporate structural and boundary information and conduct a thorough
validation on medical imaging data from the prostate zonal segmentation
challenge within the Medical Segmentation Decathlon. Our results demonstrate
improved alignment with clinically important factors and better discrimination
between meaningful and spurious uncertainty patterns.

</details>


### [64] [Leveraging CNN and IoT for Effective E-Waste Management](https://arxiv.org/abs/2506.16647)
*Ajesh Thangaraj Nadar,Gabriel Nixon Raj,Soham Chandane,Sushant Bhat*

Main category: cs.CV

TL;DR: 论文提出了一种结合物联网和轻量级CNN分类的电子废物识别系统，通过视觉和重量属性自动化分类，提升回收效率。


<details>
  <summary>Details</summary>
Motivation: 现代电子设备的普及导致电子废物激增，不当处理带来环境和健康风险，需高效分类和回收方法。

Method: 集成摄像头和数字秤，利用轻量级CNN分类管道，基于视觉和重量属性自动化分类电子废物。

Result: 系统能实时检测电路板、传感器等组件，优化智能回收流程，提高废物处理效率。

Conclusion: 该IoT和CNN结合的系统为电子废物分类和回收提供了高效解决方案。

Abstract: The increasing proliferation of electronic devices in the modern era has led
to a significant surge in electronic waste (e-waste). Improper disposal and
insufficient recycling of e-waste pose serious environmental and health risks.
This paper proposes an IoT-enabled system combined with a lightweight CNN-based
classification pipeline to enhance the identification, categorization, and
routing of e-waste materials. By integrating a camera system and a digital
weighing scale, the framework automates the classification of electronic items
based on visual and weight-based attributes. The system demonstrates how
real-time detection of e-waste components such as circuit boards, sensors, and
wires can facilitate smart recycling workflows and improve overall waste
processing efficiency.

</details>


### [65] [A Comparative Analysis of Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) as Dimensionality Reduction Techniques](https://arxiv.org/abs/2506.16663)
*Michael Gyimadu,Gregory Bell*

Main category: cs.CV

TL;DR: 本文对PCA和SVD两种线性降维技术进行了理论比较，分析了它们的可解释性、数值稳定性及适用场景，并提出了选择指南。


<details>
  <summary>Details</summary>
Motivation: 高维图像数据常需降维处理，PCA和SVD是常用方法，但缺乏理论比较和选择依据。

Method: 从基本原理推导PCA和SVD算法，评估其特性，并结合文献提出选择建议。

Result: 提出了基于理论分析的PCA与SVD选择指南，无需依赖实验验证。

Conclusion: 总结了两种方法的优缺点，并指出未来实验研究的局限性与方向。

Abstract: High-dimensional image data often require dimensionality reduction before
further analysis. This paper provides a purely analytical comparison of two
linear techniques-Principal Component Analysis (PCA) and Singular Value
Decomposition (SVD). After the derivation of each algorithm from first
principles, we assess their interpretability, numerical stability, and
suitability for differing matrix shapes. building on classical and recent
numerical literature, We synthesize rule-of-thumb guidelines for choosing one
out of the two algorithms without empirical benchmarking, building on classical
and recent numerical literature. Limitations and directions for future
experimental work are outlined at the end.

</details>


### [66] [Extracting Multimodal Learngene in CLIP: Unveiling the Multimodal Generalizable Knowledge](https://arxiv.org/abs/2506.16673)
*Ruiming Chen,Junming Yang,Shiyu Xia,Xu Yang,Jing Wang,Xin Geng*

Main category: cs.CV

TL;DR: MM-LG是一种从CLIP中提取多模态通用知识的新框架，通过加权和方式提取多模态和单模态知识，用于初始化不同规模和模态的模型，显著减少参数存储和预训练成本。


<details>
  <summary>Details</summary>
Motivation: 解决现有Learngene方法在多模态场景中无法处理通用知识的问题，同时降低CLIP的预训练成本。

Method: 使用多模态和单模态块提取通用知识，并以加权和方式组合，用于初始化不同规模和模态的模型。

Result: 在多个数据集上性能优于现有Learngene方法（如Oxford-IIIT PET提升3.1%），且仅需25%参数存储和减少2.8倍预训练成本。

Conclusion: MM-LG是一种高效的多模态通用知识提取框架，适用于多样化下游任务。

Abstract: CLIP (Contrastive Language-Image Pre-training) has attracted widespread
attention for its multimodal generalizable knowledge, which is significant for
downstream tasks. However, the computational overhead of a large number of
parameters and large-scale pre-training poses challenges of pre-training a
different scale of CLIP. Learngene extracts the generalizable components termed
as learngene from an ancestry model and initializes diverse descendant models
with it. Previous Learngene paradigms fail to handle the generalizable
knowledge in multimodal scenarios. In this paper, we put forward the idea of
utilizing a multimodal block to extract the multimodal generalizable knowledge,
which inspires us to propose MM-LG (Multimodal Learngene), a novel framework
designed to extract and leverage generalizable components from CLIP.
Specifically, we first establish multimodal and unimodal blocks to extract the
multimodal and unimodal generalizable knowledge in a weighted-sum manner.
Subsequently, we employ these components to numerically initialize descendant
models of varying scales and modalities. Extensive experiments demonstrate
MM-LG's effectiveness, which achieves performance gains over existing learngene
approaches (e.g.,+3.1% on Oxford-IIIT PET and +4.13% on Flickr30k) and
comparable or superior results to the pre-training and fine-tuning paradigm
(e.g.,+1.9% on Oxford-IIIT PET and +3.65% on Flickr30k). Notably, MM-LG
requires only around 25% of the parameter storage while reducing around 2.8
times pre-training costs for diverse model scales compared to the pre-training
and fine-tuning paradigm, making it particularly suitable for efficient
deployment across diverse downstream tasks.

</details>


### [67] [How to Train your Text-to-Image Model: Evaluating Design Choices for Synthetic Training Captions](https://arxiv.org/abs/2506.16679)
*Manuel Brack,Sudeep Katakol,Felix Friedrich,Patrick Schramowski,Hareesh Ravi,Kristian Kersting,Ajinkya Kale*

Main category: cs.CV

TL;DR: 研究探讨了合成标题策略对文本到图像模型性能的影响，发现密集高质量标题提升文本对齐但可能牺牲美学和多样性，而随机长度标题则能平衡美学和对齐。


<details>
  <summary>Details</summary>
Motivation: 由于网络抓取数据集的噪声和不一致性，合成标题成为训练文本到图像模型的关键，但缺乏对其设计选择的系统研究。

Method: 系统研究了不同合成标题策略对模型性能的影响，包括密集高质量标题和随机长度标题。

Result: 密集高质量标题提升文本对齐但可能影响美学和多样性；随机长度标题平衡美学和对齐且不损害多样性。标题分布变化还会显著影响模型输出偏差。

Conclusion: 标题设计对模型性能至关重要，研究为文本到图像生成提供了更有效的训练数据策略。

Abstract: Training data is at the core of any successful text-to-image models. The
quality and descriptiveness of image text are crucial to a model's performance.
Given the noisiness and inconsistency in web-scraped datasets, recent works
shifted towards synthetic training captions. While this setup is generally
believed to produce more capable models, current literature does not provide
any insights into its design choices. This study closes this gap by
systematically investigating how different synthetic captioning strategies
impact the downstream performance of text-to-image models. Our experiments
demonstrate that dense, high-quality captions enhance text alignment but may
introduce trade-offs in output aesthetics and diversity. Conversely, captions
of randomized lengths yield balanced improvements across aesthetics and
alignment without compromising sample diversity. We also demonstrate that
varying caption distributions introduce significant shifts in the output bias
of a trained model. Our findings underscore the importance of caption design in
achieving optimal model performance and provide practical insights for more
effective training data strategies in text-to-image generation.

</details>


### [68] [DepthVanish: Optimizing Adversarial Interval Structures for Stereo-Depth-Invisible Patches](https://arxiv.org/abs/2506.16690)
*Yun Xing,Yue Cao,Nhat Chung,Jie Zhang,Ivor Tsang,Ming-Ming Cheng,Yang Liu,Lei Ma,Qing Guo*

Main category: cs.CV

TL;DR: 论文提出了一种通过条纹结构增强对抗性贴片攻击效果的新方法，显著提升了在物理世界中攻击立体深度估计系统的实用性。


<details>
  <summary>Details</summary>
Motivation: 立体深度估计在自动驾驶和机器人技术中至关重要，但对抗性攻击可以揭示其潜在漏洞。然而，现有方法在物理世界中效果不佳，限制了实际应用。

Method: 研究发现，通过在重复纹理中引入规则间隔（条纹结构），可以显著提升攻击效果。论文提出了一种联合优化条纹结构和纹理元素的新型对抗性贴片生成方法。

Result: 生成的对抗性贴片能够成功攻击先进的立体深度估计方法（如RAFT-Stereo和STTR），并在实际场景中对商用RGB-D相机（如Intel RealSense）有效。

Conclusion: 该研究为立体系统的安全评估提供了实用的对抗性攻击工具，证明了条纹结构在物理世界中的有效性。

Abstract: Stereo Depth estimation is a critical task in autonomous driving and
robotics, where inaccuracies (such as misidentifying nearby objects as distant)
can lead to dangerous situations. Adversarial attacks against stereo depth
estimation can help reveal vulnerabilities before deployment. Previous work has
shown that repeating optimized textures can effectively mislead stereo depth
estimation in digital settings. However, our research reveals that these
naively repeated texture structures perform poorly in physical-world
implementations, i.e., when deployed as patches, limiting their practical
utility for testing stereo depth estimation systems. In this work, for the
first time, we discover that introducing regular intervals between repeated
textures, creating a striped structure, significantly enhances the patch attack
effectiveness. Through extensive experimentation, we analyze how variations of
this novel structure influence the performance. Based on these insights, we
develop a novel stereo depth attack that jointly optimizes both the striped
structure and texture elements. Our generated adversarial patches can be
inserted into any scenes and successfully attack state-of-the-art stereo depth
estimation methods, i.e., RAFT-Stereo and STTR. Most critically, our patch can
also attack commercial RGB-D cameras (Intel RealSense) in real-world
conditions, demonstrating their practical relevance for security assessment of
stereo systems.

</details>


### [69] [LaVi: Efficient Large Vision-Language Models via Internal Feature Modulation](https://arxiv.org/abs/2506.16691)
*Tongtian Yue,Longteng Guo,Yepeng Tang,Zijia Zhao,Xinxin Zhu,Hua Huang,Jing Liu*

Main category: cs.CV

TL;DR: LaVi提出了一种新型大型视觉语言模型，通过内部特征调制实现高效视觉语言融合，显著提升了计算效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在视觉语言融合上效率低下，限制了其扩展性和实用性。

Method: LaVi通过轻量级自适应变换，将视觉条件化的增量注入层归一化的仿射参数中，直接调节语言隐藏状态，避免了长上下文扩展。

Result: 在15个图像和视频基准测试中，LaVi实现了最先进的多模态性能，计算量减少94.0%，推理速度提升3.1倍，内存使用减半。

Conclusion: LaVi是一种可扩展且实用的实时多模态推理解决方案。

Abstract: Despite the impressive advancements of Large Vision-Language Models (LVLMs),
existing approaches suffer from a fundamental bottleneck: inefficient
visual-language integration. Current methods either disrupt the model's
inherent structure or introduce severe long-context computational burden,
severely limiting scalability and efficiency. In this paper, we rethink
multimodal integration and present LaVi, a novel LVLM that enables seamless and
efficient vision-language fusion through internal feature modulation within the
Large Language Models (LLMs). Unlike dominant LVLMs that rely on visual token
concatenation, LaVi bypasses long-context expansion by introducing a
lightweight and adaptive transformation, which incorporates visual context by
injecting token-wise vision-conditioned deltas into the affine parameters of
layer normalization. This mechanism directly modulates linguistic hidden states
based on visual input, ensuring precise vision-language alignment while
preserving the LLM's linguistic priors and drastically reducing computational
costs. Extensive evaluations across 15 image and video benchmarks demonstrate
that LaVi not only achieves state-of-the-art multimodal performance but also
dramatically enhances efficiency. Compared to LLaVA-OV-7B, LaVi reduces FLOPs
by 94.0%, improves inference speed by 3.1 times, and cuts memory usage in half
- establishing LaVi as a scalable and practical solution for real-time
multimodal reasoning. The code and models will be released soon.

</details>


### [70] [Language-driven Description Generation and Common Sense Reasoning for Video Action Recognition](https://arxiv.org/abs/2506.16701)
*Xiaodan Hu,Chuhang Zou,Suchen Wang,Jaechul Kim,Narendra Ahuja*

Main category: cs.CV

TL;DR: 论文提出了一种结合语言驱动常识先验的框架，用于识别遮挡严重的单目视频动作序列。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用语言模型中的常识先验（如场景上下文），而这些先验对理解复杂视频动作至关重要。

Method: 提出三个模块：视频上下文总结、描述生成和多模态动作识别头，结合视觉和文本线索。

Result: 在Action Genome和Charades数据集上验证了方法的有效性。

Conclusion: 通过引入语言驱动的常识先验，显著提升了复杂视频动作识别的性能。

Abstract: Recent video action recognition methods have shown excellent performance by
adapting large-scale pre-trained language-image models to the video domain.
However, language models contain rich common sense priors - the scene contexts
that humans use to constitute an understanding of objects, human-object
interactions, and activities - that have not been fully exploited. In this
paper, we introduce a framework incorporating language-driven common sense
priors to identify cluttered video action sequences from monocular views that
are often heavily occluded. We propose: (1) A video context summary component
that generates candidate objects, activities, and the interactions between
objects and activities; (2) A description generation module that describes the
current scene given the context and infers subsequent activities, through
auxiliary prompts and common sense reasoning; (3) A multi-modal activity
recognition head that combines visual and textual cues to recognize video
actions. We demonstrate the effectiveness of our approach on the challenging
Action Genome and Charades datasets.

</details>


### [71] [Few-Shot Generalized Category Discovery With Retrieval-Guided Decision Boundary Enhancement](https://arxiv.org/abs/2506.16728)
*Yunhan Ren,Feng Luo,Siyu Huang*

Main category: cs.CV

TL;DR: 本文提出了Few-shot Generalized Category Discovery (FSGCD)任务，旨在在已知信息稀缺的条件下提升GCD任务的性能。通过决策边界增强框架和基于相似性的检索，该方法在六个公开GCD基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有GCD模型在有限标注样本和少量已知类别下的性能尚未充分研究，FSGCD任务旨在填补这一空白。

Method: 提出决策边界增强框架，包括预训练模块和两阶段检索优化策略，利用相似性检索提升决策边界。

Result: 在六个公开GCD基准测试中，该方法优于现有方法。

Conclusion: FSGCD框架通过决策边界增强和相似性检索，显著提升了GCD任务在信息稀缺条件下的性能。

Abstract: While existing Generalized Category Discovery (GCD) models have achieved
significant success, their performance with limited labeled samples and a small
number of known categories remains largely unexplored. In this work, we
introduce the task of Few-shot Generalized Category Discovery (FSGCD), aiming
to achieve competitive performance in GCD tasks under conditions of known
information scarcity. To tackle this challenge, we propose a decision boundary
enhancement framework with affinity-based retrieval. Our framework is designed
to learn the decision boundaries of known categories and transfer these
boundaries to unknown categories. First, we use a decision boundary
pre-training module to mitigate the overfitting of pre-trained information on
known category boundaries and improve the learning of these decision boundaries
using labeled samples. Second, we implement a two-stage retrieval-guided
decision boundary optimization strategy. Specifically, this strategy further
enhances the severely limited known boundaries by using affinity-retrieved
pseudo-labeled samples. Then, these refined boundaries are applied to unknown
clusters via guidance from affinity-based feature retrieval. Experimental
results demonstrate that our proposed method outperforms existing methods on
six public GCD benchmarks under the FSGCD setting. The codes are available at:
https://github.com/Ryh1218/FSGCD

</details>


### [72] [TeSG: Textual Semantic Guidance for Infrared and Visible Image Fusion](https://arxiv.org/abs/2506.16730)
*Mingrui Zhu,Xiru Chen,Xin Wei,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: 论文提出了一种基于文本语义引导的红外与可见光图像融合方法（TeSG），通过多层次语义信息优化下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导的红外与可见光图像融合方法对文本语义信息的利用不足，影响了融合效果和下游任务表现。

Method: TeSG包含语义信息生成器（SIG）、掩码引导交叉注意力模块（MGCA）和文本驱动注意力融合模块（TDAF），分别生成语义信息、初步融合视觉特征并优化融合过程。

Result: 实验表明，TeSG在下游任务（如检测和分割）中优于现有方法。

Conclusion: TeSG通过多层次语义引导，显著提升了红外与可见光图像融合的效果和下游任务性能。

Abstract: Infrared and visible image fusion (IVF) aims to combine complementary
information from both image modalities, producing more informative and
comprehensive outputs. Recently, text-guided IVF has shown great potential due
to its flexibility and versatility. However, the effective integration and
utilization of textual semantic information remains insufficiently studied. To
tackle these challenges, we introduce textual semantics at two levels: the mask
semantic level and the text semantic level, both derived from textual
descriptions extracted by large Vision-Language Models (VLMs). Building on
this, we propose Textual Semantic Guidance for infrared and visible image
fusion, termed TeSG, which guides the image synthesis process in a way that is
optimized for downstream tasks such as detection and segmentation.
Specifically, TeSG consists of three core components: a Semantic Information
Generator (SIG), a Mask-Guided Cross-Attention (MGCA) module, and a Text-Driven
Attentional Fusion (TDAF) module. The SIG generates mask and text semantics
based on textual descriptions. The MGCA module performs initial attention-based
fusion of visual features from both infrared and visible images, guided by mask
semantics. Finally, the TDAF module refines the fusion process with gated
attention driven by text semantics. Extensive experiments demonstrate the
competitiveness of our approach, particularly in terms of performance on
downstream tasks, compared to existing state-of-the-art methods.

</details>


### [73] [Cross-modal Offset-guided Dynamic Alignment and Fusion for Weakly Aligned UAV Object Detection](https://arxiv.org/abs/2506.16737)
*Liu Zongzhen,Luo Hui,Wang Zhixing,Wei Yuxing,Zuo Haorui,Zhang Jianlin*

Main category: cs.CV

TL;DR: 论文提出了一种名为CoDAF的统一框架，通过联合解决无人机目标检测中的语义不一致和模态冲突问题，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 无人机目标检测在环境监测和城市安全中至关重要，但多模态图像的空间错位导致语义不一致和模态冲突，现有方法未能有效解决。

Method: CoDAF框架包含两个模块：OSA模块通过可变形卷积和共享语义空间精确对齐特征；DAFM模块通过门控机制和双注意力自适应融合模态特征。

Result: 在DroneVehicle数据集上，CoDAF实现了78.6%的mAP，验证了其有效性。

Conclusion: CoDAF通过统一设计解决了弱对齐问题，为无人机目标检测提供了鲁棒的解决方案。

Abstract: Unmanned aerial vehicle (UAV) object detection plays a vital role in
applications such as environmental monitoring and urban security. To improve
robustness, recent studies have explored multimodal detection by fusing visible
(RGB) and infrared (IR) imagery. However, due to UAV platform motion and
asynchronous imaging, spatial misalignment frequently occurs between
modalities, leading to weak alignment. This introduces two major challenges:
semantic inconsistency at corresponding spatial locations and modality conflict
during feature fusion. Existing methods often address these issues in
isolation, limiting their effectiveness. In this paper, we propose Cross-modal
Offset-guided Dynamic Alignment and Fusion (CoDAF), a unified framework that
jointly tackles both challenges in weakly aligned UAV-based object detection.
CoDAF comprises two novel modules: the Offset-guided Semantic Alignment (OSA),
which estimates attention-based spatial offsets and uses deformable convolution
guided by a shared semantic space to align features more precisely; and the
Dynamic Attention-guided Fusion Module (DAFM), which adaptively balances
modality contributions through gating and refines fused features via
spatial-channel dual attention. By integrating alignment and fusion in a
unified design, CoDAF enables robust UAV object detection. Experiments on
standard benchmarks validate the effectiveness of our approach, with CoDAF
achieving a mAP of 78.6% on the DroneVehicle dataset.

</details>


### [74] [Uncertainty-Aware Variational Information Pursuit for Interpretable Medical Image Analysis](https://arxiv.org/abs/2506.16742)
*Md Nahiduzzaman,Ruwan Tennakoon,Steven Korevaar,Zongyuan Ge,Alireza Bab-Hadiashar*

Main category: cs.CV

TL;DR: 论文提出了一种不确定性感知的变分信息追踪（UAV-IP）框架，用于提升医学影像AI系统的解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有V-IP方法忽略了查询-答案生成中的实例级不确定性，影响了模型的可靠性和临床决策支持效果。

Method: UAV-IP通过将不确定性量化整合到V-IP过程中，解决了模型限制和专家响应变异性带来的不确定性。

Result: 在四个医学影像数据集上的实验表明，UAV-IP的AUC平均提升了3.2%，同时生成的解释更简洁20%，且信息量未减少。

Conclusion: 不确定性感知的推理对设计可解释且可靠的医学决策模型至关重要。

Abstract: In medical imaging, AI decision-support systems must balance accuracy and
interpretability to build user trust and support effective clinical
decision-making. Recently, Variational Information Pursuit (V-IP) and its
variants have emerged as interpretable-by-design modeling techniques, aiming to
explain AI decisions in terms of human-understandable, clinically relevant
concepts. However, existing V-IP methods overlook instance-level uncertainties
in query-answer generation, which can arise from model limitations (epistemic
uncertainty) or variability in expert responses (aleatoric uncertainty).
  This paper introduces Uncertainty-Aware V-IP (UAV-IP), a novel framework that
integrates uncertainty quantification into the V-IP process. We evaluate UAV-IP
across four medical imaging datasets, PH2, Derm7pt, BrEaST, and SkinCon,
demonstrating an average AUC improvement of approximately 3.2% while generating
20% more concise explanations compared to baseline V-IP, without sacrificing
informativeness. These findings highlight the importance of uncertainty-aware
reasoning in interpretable by design models for robust and reliable medical
decision-making.

</details>


### [75] [Noise-Informed Diffusion-Generated Image Detection with Anomaly Attention](https://arxiv.org/abs/2506.16743)
*Weinan Guan,Wei Wang,Bo Peng,Ziwen He,Jing Dong,Haonan Cheng*

Main category: cs.CV

TL;DR: 论文提出了一种基于噪声感知自注意力模块（NASA）的检测方法，用于识别扩散模型生成的图像，并在未见过的扩散模型上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型生成图像质量的提升，信息安全隐患增加，需要一种能泛化到未见扩散模型的检测方法。

Method: 通过分析图像噪声模式，引入NASA模块并结合Swin Transformer，提出NASA-Swin架构，同时采用跨模态融合和通道掩码策略。

Result: 实验表明，该方法在检测扩散生成图像上表现优异，尤其在未见过的生成方法上达到SOTA性能。

Conclusion: NASA-Swin通过噪声感知和跨模态学习，显著提升了扩散生成图像的检测能力。

Abstract: With the rapid development of image generation technologies, especially the
advancement of Diffusion Models, the quality of synthesized images has
significantly improved, raising concerns among researchers about information
security. To mitigate the malicious abuse of diffusion models,
diffusion-generated image detection has proven to be an effective
countermeasure.However, a key challenge for forgery detection is generalising
to diffusion models not seen during training. In this paper, we address this
problem by focusing on image noise. We observe that images from different
diffusion models share similar noise patterns, distinct from genuine images.
Building upon this insight, we introduce a novel Noise-Aware Self-Attention
(NASA) module that focuses on noise regions to capture anomalous patterns. To
implement a SOTA detection model, we incorporate NASA into Swin Transformer,
forming an novel detection architecture NASA-Swin. Additionally, we employ a
cross-modality fusion embedding to combine RGB and noise images, along with a
channel mask strategy to enhance feature learning from both modalities.
Extensive experiments demonstrate the effectiveness of our approach in
enhancing detection capabilities for diffusion-generated images. When
encountering unseen generation methods, our approach achieves the
state-of-the-art performance.Our code is available at
https://github.com/WeinanGuan/NASA-Swin.

</details>


### [76] [Class Agnostic Instance-level Descriptor for Visual Instance Search](https://arxiv.org/abs/2506.16745)
*Qi-Ying Sun,Wan-Lei Zhao,Yi-Bo Miao,Chong-Wah Ngo*

Main category: cs.CV

TL;DR: 提出了一种基于自监督ViT的分层特征子集检测方法，用于图像中的实例级区域发现，解决了对象嵌入和遮挡问题，显著提升了实例搜索性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度特征在实例级图像检索中表现不佳，监督或弱监督方法对未知类别效果差，需一种更有效的实例级特征表示方法。

Method: 利用自监督ViT输出的特征集，通过分层方式检测紧凑特征子集，生成多语义尺度的实例区域层次结构。

Result: 在三个实例搜索基准测试中显著优于现有方法，对已知和未知类别均有效。

Conclusion: 分层特征子集方法能有效解决实例搜索中的嵌入和遮挡问题，提升检索性能。

Abstract: Despite the great success of the deep features in content-based image
retrieval, the visual instance search remains challenging due to the lack of
effective instance level feature representation. Supervised or weakly
supervised object detection methods are not among the options due to their poor
performance on the unknown object categories. In this paper, based on the
feature set output from self-supervised ViT, the instance level region
discovery is modeled as detecting the compact feature subsets in a hierarchical
fashion. The hierarchical decomposition results in a hierarchy of feature
subsets. The non-leaf nodes and leaf nodes on the hierarchy correspond to the
various instance regions in an image of different semantic scales. The
hierarchical decomposition well addresses the problem of object embedding and
occlusions, which are widely observed in the real scenarios. The features
derived from the nodes on the hierarchy make up a comprehensive representation
for the latent instances in the image. Our instance-level descriptor remains
effective on both the known and unknown object categories. Empirical studies on
three instance search benchmarks show that it outperforms state-of-the-art
methods considerably.

</details>


### [77] [Infrared and Visible Image Fusion Based on Implicit Neural Representations](https://arxiv.org/abs/2506.16773)
*Shuchen Sun,Ligen Shi,Chang Liu,Lina Wu,Jun Qiu*

Main category: cs.CV

TL;DR: 本文提出了一种基于隐式神经表示（INR）的红外与可见光图像融合方法INRFuse，通过神经网络参数化连续函数实现多模态信息融合，无需训练数据集即可生成高质量融合图像。


<details>
  <summary>Details</summary>
Motivation: 红外与可见光图像融合旨在结合两种模态的优势，生成信息丰富且满足视觉或计算需求的图像。传统方法依赖离散像素或显式特征，限制了融合效果。

Method: 利用归一化空间坐标作为输入，通过多层感知机自适应融合两种模态的特征，设计多损失函数联合优化融合图像与原图的相似性，保留红外图像的热辐射信息和可见光图像的纹理细节。

Result: 实验表明，INRFuse在主观视觉质量和客观评价指标上均优于现有方法，生成的融合图像结构清晰、细节自然、信息丰富，且支持不同分辨率图像的直接融合及超分辨率重建。

Conclusion: INRFuse通过隐式神经表示突破了传统方法的限制，实现了高质量、分辨率无关的图像融合，具有广泛的应用潜力。

Abstract: Infrared and visible light image fusion aims to combine the strengths of both
modalities to generate images that are rich in information and fulfill visual
or computational requirements. This paper proposes an image fusion method based
on Implicit Neural Representations (INR), referred to as INRFuse. This method
parameterizes a continuous function through a neural network to implicitly
represent the multimodal information of the image, breaking through the
traditional reliance on discrete pixels or explicit features. The normalized
spatial coordinates of the infrared and visible light images serve as inputs,
and multi-layer perceptrons is utilized to adaptively fuse the features of both
modalities, resulting in the output of the fused image. By designing multiple
loss functions, the method jointly optimizes the similarity between the fused
image and the original images, effectively preserving the thermal radiation
information of the infrared image while maintaining the texture details of the
visible light image. Furthermore, the resolution-independent characteristic of
INR allows for the direct fusion of images with varying resolutions and
achieves super-resolution reconstruction through high-density coordinate
queries. Experimental results indicate that INRFuse outperforms existing
methods in both subjective visual quality and objective evaluation metrics,
producing fused images with clear structures, natural details, and rich
information without the necessity for a training dataset.

</details>


### [78] [PQCAD-DM: Progressive Quantization and Calibration-Assisted Distillation for Extremely Efficient Diffusion Model](https://arxiv.org/abs/2506.16776)
*Beomseok Ko,Hyeryung Jang*

Main category: cs.CV

TL;DR: PQCAD-DM是一种结合渐进量化（PQ）和校准辅助蒸馏（CAD）的混合压缩框架，旨在解决扩散模型的计算和资源密集型问题，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成中表现出色，但因其依赖迭代马尔可夫链过程而计算和资源密集，导致误差累积，限制了简单压缩技术的有效性。

Method: PQ采用两阶段量化，通过动量机制指导自适应位宽转换，减少低精度下的权重扰动；CAD利用全精度校准数据集进行蒸馏，使量化学生模型能匹配全精度性能。

Result: PQCAD-DM在计算效率和生成质量之间取得平衡，推理时间减半且性能保持竞争力，实验证明其优于固定位量化方法。

Conclusion: PQCAD-DM通过混合压缩框架显著提升了扩散模型的效率和性能，适用于多样化数据集。

Abstract: Diffusion models excel in image generation but are computational and
resource-intensive due to their reliance on iterative Markov chain processes,
leading to error accumulation and limiting the effectiveness of naive
compression techniques. In this paper, we propose PQCAD-DM, a novel hybrid
compression framework combining Progressive Quantization (PQ) and
Calibration-Assisted Distillation (CAD) to address these challenges. PQ employs
a two-stage quantization with adaptive bit-width transitions guided by a
momentum-based mechanism, reducing excessive weight perturbations in
low-precision. CAD leverages full-precision calibration datasets during
distillation, enabling the student to match full-precision performance even
with a quantized teacher. As a result, PQCAD-DM achieves a balance between
computational efficiency and generative quality, halving inference time while
maintaining competitive performance. Extensive experiments validate PQCAD-DM's
superior generative capabilities and efficiency across diverse datasets,
outperforming fixed-bit quantization methods.

</details>


### [79] [TextBraTS: Text-Guided Volumetric Brain Tumor Segmentation with Innovative Dataset Development and Fusion Module Exploration](https://arxiv.org/abs/2506.16784)
*Xiaoyu Shi,Rahul Kumar Jain,Yinhao Li,Ruibo Hou,Jingliang Cheng,Jie Bai,Guohua Zhao,Lanfen Lin,Rui Xu,Yen-wei Chen*

Main category: cs.CV

TL;DR: 论文介绍了首个公开的多模态数据集TextBraTS，结合MRI图像和文本注释，并提出了一种基于文本引导的医学图像分割方法，显著提高了脑肿瘤分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有脑肿瘤分析领域缺乏结合放射学图像和文本注释的综合数据集，限制了多模态方法的研究。

Method: 提出了TextBraTS数据集和一种基于顺序交叉注意力的文本引导体积医学图像分割框架。

Result: 实验表明，该方法显著提高了脑肿瘤分割的准确性，并提供了有效的多模态集成技术见解。

Conclusion: TextBraTS数据集和提出的方法填补了领域空白，为多模态医学图像分析提供了新工具。

Abstract: Deep learning has demonstrated remarkable success in medical image
segmentation and computer-aided diagnosis. In particular, numerous advanced
methods have achieved state-of-the-art performance in brain tumor segmentation
from MRI scans. While recent studies in other medical imaging domains have
revealed that integrating textual reports with visual data can enhance
segmentation accuracy, the field of brain tumor analysis lacks a comprehensive
dataset that combines radiological images with corresponding textual
annotations. This limitation has hindered the exploration of multimodal
approaches that leverage both imaging and textual data.
  To bridge this critical gap, we introduce the TextBraTS dataset, the first
publicly available volume-level multimodal dataset that contains paired MRI
volumes and rich textual annotations, derived from the widely adopted BraTS2020
benchmark. Building upon this novel dataset, we propose a novel baseline
framework and sequential cross-attention method for text-guided volumetric
medical image segmentation. Through extensive experiments with various
text-image fusion strategies and templated text formulations, our approach
demonstrates significant improvements in brain tumor segmentation accuracy,
offering valuable insights into effective multimodal integration techniques.
  Our dataset, implementation code, and pre-trained models are publicly
available at https://github.com/Jupitern52/TextBraTS.

</details>


### [80] [RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution with Vision-Language Chain-of-Thought](https://arxiv.org/abs/2506.16796)
*Junbo Qiao,Miaomiao Cai,Wei Li,Yutong Liu,Xudong Huang,Gaoqi He,Jiao Xie,Jie Hu,Xinghao Chen,Shaohui Lin*

Main category: cs.CV

TL;DR: RealSR-R1提出了一种结合视觉与语言推理的VLCoT框架，通过GRPO优化方法提升真实世界图像超分辨率任务的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在理解退化图像内容时表现不佳，导致重建结果低保真且不自然。

Method: 提出VLCoT框架，模拟人类处理退化图像的过程，并引入GRPO优化方法，设计四种奖励函数。

Result: 实验表明，RealSR-R1能生成更真实的细节并准确理解图像内容，尤其在语义丰富或严重退化的场景中表现突出。

Conclusion: VLCoT-GRPO框架有效提升了真实世界图像超分辨率任务的效果。

Abstract: Real-World Image Super-Resolution is one of the most challenging task in
image restoration. However, existing methods struggle with an accurate
understanding of degraded image content, leading to reconstructed results that
are both low-fidelity and unnatural. We present RealSR-R1 in this work, which
empowers the RealSR models with understanding and reasoning capabilities.
Inspired by the success of Chain of Thought (CoT) in large language models
(LLMs), we simulate the human process of handling degraded images and propose
the VLCoT framework, which integrates vision and language reasoning. The
framework aims to precisely restore image details by progressively generating
more comprehensive text and higher-resolution images. To overcome the challenge
of traditional supervised learning CoT failing to generalize to real-world
scenarios, we introduce, for the first time, Group Relative Policy Optimization
(GRPO) into the Real-World Image Super-Resolution task. We propose VLCoT-GRPO
as a solution, which designs four reward functions: (1) Format reward, used to
standardize the CoT process; (2) Degradation reward, to incentivize accurate
degradation estimation; (3) Understanding reward, to ensure the accuracy of the
generated content; and (4) Generation reward, where we propose using a visual
expert model to evaluate the quality of generated images, encouraging the model
to generate more realistic images. Extensive experiments demonstrate that our
proposed RealSR-R1 can generate realistic details and accurately understand
image content, particularly in semantically rich scenes or images with severe
degradation.

</details>


### [81] [Seeing What Matters: Generalizable AI-generated Video Detection with Forensic-Oriented Augmentation](https://arxiv.org/abs/2506.16802)
*Riccardo Corvi,Davide Cozzolino,Ekta Prashnani,Shalini De Mello,Koki Nagano,Luisa Verdoliva*

Main category: cs.CV

TL;DR: 提出了一种新方法，通过引导检测器关注生成视频的低级特征而非高级语义缺陷，提升AI生成视频检测器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频伪造检测器泛化能力差，难以适应真实场景。

Method: 研究不同生成架构，识别共享特征；引入基于小波分解的数据增强策略，引导模型关注更相关的伪造线索。

Result: 新方法显著提升了检测器的泛化能力，即使仅用单一生成模型训练，也能在多种模型生成的视频上表现优异。

Conclusion: 该方法简单有效，无需复杂算法或大数据集，显著优于现有技术。

Abstract: Synthetic video generation is progressing very rapidly. The latest models can
produce very realistic high-resolution videos that are virtually
indistinguishable from real ones. Although several video forensic detectors
have been recently proposed, they often exhibit poor generalization, which
limits their applicability in a real-world scenario. Our key insight to
overcome this issue is to guide the detector towards seeing what really
matters. In fact, a well-designed forensic classifier should focus on
identifying intrinsic low-level artifacts introduced by a generative
architecture rather than relying on high-level semantic flaws that characterize
a specific model. In this work, first, we study different generative
architectures, searching and identifying discriminative features that are
unbiased, robust to impairments, and shared across models. Then, we introduce a
novel forensic-oriented data augmentation strategy based on the wavelet
decomposition and replace specific frequency-related bands to drive the model
to exploit more relevant forensic cues. Our novel training paradigm improves
the generalizability of AI-generated video detectors, without the need for
complex algorithms and large datasets that include multiple synthetic
generators. To evaluate our approach, we train the detector using data from a
single generative model and test it against videos produced by a wide range of
other models. Despite its simplicity, our method achieves a significant
accuracy improvement over state-of-the-art detectors and obtains excellent
results even on very recent generative models, such as NOVA and FLUX. Code and
data will be made publicly available.

</details>


### [82] [Co-VisiON: Co-Visibility ReasONing on Sparse Image Sets of Indoor Scenes](https://arxiv.org/abs/2506.16805)
*Chao Chen,Nobel Dang,Juexiao Zhang,Wenkai Sun,Pengfei Zheng,Xuhang He,Yimeng Ye,Taarun Srinivas,Chen Feng*

Main category: cs.CV

TL;DR: 论文提出了Co-VisiON基准测试，用于评估稀疏图像集中的共视性推理能力，发现现有视觉模型在此任务上表现不佳，尤其是与人类相比。提出的多视图基线模型Covis表现最佳，但仍需进一步改进。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉学习取得了显著进展，但现有模型在稀疏条件下的共视性推理能力是否达到人类水平尚不明确。

Method: 引入Co-VisiON基准测试，评估1000多个室内场景的稀疏图像集的共视性推理能力，并提出多视图基线模型Covis。

Result: 现有视觉模型在稀疏条件下表现不佳，专有视觉语言模型表现最佳，但所有模型均远低于人类水平。Covis在纯视觉模型中表现最优。

Conclusion: 共视性推理需要高层次的多视图空间理解，而非低层次特征匹配。Co-VisiON基准和Covis模型为未来研究提供了方向。

Abstract: Humans exhibit a remarkable ability to recognize co-visibility-the
overlapping regions visible in multiple images-even when these images are
sparsely distributed across a complex scene. This capability is foundational in
3D vision and robotic perception. Despite significant progress in vision
learning, it remains unclear whether current vision models have reached
human-level proficiency in co-visibility analysis. In this work, we introduce
the Co-Visibility reasONing (Co-VisiON) benchmark, designed to directly
evaluate co-visibility reasoning on sparse image sets across over 1000 indoor
scenarios. Our experiments reveal that while co-visibility is typically treated
as a low-level feature matching task, it poses a significant challenge for
existing vision models under sparse conditions. Notably, a proprietary
vision-language model outperforms all purely vision-based approaches, with all
models lagging substantially behind human performance. This gap underscores the
need for more than basic pairwise vision processing-it calls for a
comprehensive spatial understanding through high-level reasoning across
multiple views. Inspired by human visual cognition, we propose a novel
multi-view baseline, Covis, which achieves top performance among pure vision
models and narrows the gap to the proprietary VLM. We hope our benchmark and
findings will spur further advancements in developing vision models capable of
robust, high-level reasoning in challenging, sparse environments. Our dataset
and source code can be found at: https://ai4ce.github.io/CoVISION

</details>


### [83] [FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation](https://arxiv.org/abs/2506.16806)
*Fan Yang,Yousong Zhu,Xin Li,Yufei Zhan,Hongyin Zhao,Shurong Zheng,Yaowei Wang,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: FOCUS是一个统一的LVLM模型，整合了分割感知和可控对象生成，通过端到端框架实现视觉理解和编辑的统一。


<details>
  <summary>Details</summary>
Motivation: 当前方法将视觉理解和编辑分离，依赖多个独立模型，FOCUS旨在填补这一空白。

Method: 采用双分支视觉编码器捕获全局语义和局部细节，结合MoVQGAN视觉分词器和多阶段训练管道。

Result: 在多项任务中表现优异，包括多模态理解、参考分割和可控图像生成。

Conclusion: FOCUS通过联合优化视觉感知和生成能力，实现了高效的分割感知与视觉合成的统一。

Abstract: Recent Large Vision Language Models (LVLMs) demonstrate promising
capabilities in unifying visual understanding and generative modeling, enabling
both accurate content understanding and flexible editing. However, current
approaches treat "what to see" and "how to edit" separately: they either
perform isolated object segmentation or utilize segmentation masks merely as
conditional prompts for local edit generation tasks, often relying on multiple
disjointed models. To bridge these gaps, we introduce FOCUS, a unified LVLM
that integrates segmentation-aware perception and controllable object-centric
generation within an end-to-end framework. FOCUS employs a dual-branch visual
encoder to simultaneously capture global semantic context and fine-grained
spatial details. In addition, we leverage a MoVQGAN-based visual tokenizer to
produce discrete visual tokens that enhance generation quality. To enable
accurate and controllable image editing, we propose a progressive multi-stage
training pipeline, where segmentation masks are jointly optimized and used as
spatial condition prompts to guide the diffusion decoder. This strategy aligns
visual encoding, segmentation, and generation modules, effectively bridging
segmentation-aware perception with fine-grained visual synthesis. Extensive
experiments across three core tasks, including multimodal understanding,
referring segmentation accuracy, and controllable image generation, demonstrate
that FOCUS achieves strong performance by jointly optimizing visual perception
and generative capabilities.

</details>


### [84] [Loupe: A Generalizable and Adaptive Framework for Image Forgery Detection](https://arxiv.org/abs/2506.16819)
*Yuchu Jiang,Jiaming Chu,Jian Zhao,Xin Zhang,Xu Yang,Lei Jin,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: Loupe是一个轻量级框架，用于联合深度伪造检测和定位，通过整合补丁感知分类器和分割模块，实现了全局分类和细粒度掩码预测，并在测试时引入伪标签引导的适应机制。


<details>
  <summary>Details</summary>
Motivation: 生成模型的普及引发了视觉内容伪造的严重问题，现有方法在泛化性和架构复杂性上存在局限。

Method: Loupe结合补丁感知分类器和条件查询的分割模块，引入伪标签引导的测试时适应机制。

Result: 在DDL数据集上表现优异，IJCAI 2025挑战赛中得分0.846，验证了补丁级融合和条件查询设计的有效性。

Conclusion: Loupe在分类准确性和空间定位上表现卓越，适用于多样伪造模式。

Abstract: The proliferation of generative models has raised serious concerns about
visual content forgery. Existing deepfake detection methods primarily target
either image-level classification or pixel-wise localization. While some
achieve high accuracy, they often suffer from limited generalization across
manipulation types or rely on complex architectures. In this paper, we propose
Loupe, a lightweight yet effective framework for joint deepfake detection and
localization. Loupe integrates a patch-aware classifier and a segmentation
module with conditional queries, allowing simultaneous global authenticity
classification and fine-grained mask prediction. To enhance robustness against
distribution shifts of test set, Loupe introduces a pseudo-label-guided
test-time adaptation mechanism by leveraging patch-level predictions to
supervise the segmentation head. Extensive experiments on the DDL dataset
demonstrate that Loupe achieves state-of-the-art performance, securing the
first place in the IJCAI 2025 Deepfake Detection and Localization Challenge
with an overall score of 0.846. Our results validate the effectiveness of the
proposed patch-level fusion and conditional query design in improving both
classification accuracy and spatial localization under diverse forgery
patterns. The code is available at https://github.com/Kamichanw/Loupe.

</details>


### [85] [Self-supervised Feature Extraction for Enhanced Ball Detection on Soccer Robots](https://arxiv.org/abs/2506.16821)
*Can Lin,Daniele Affinita,Marco E. P. Zimmatore,Daniele Nardi,Domenico D. Bloisi,Vincenzo Suriani*

Main category: cs.CV

TL;DR: 提出了一种自监督学习框架，用于提升足球机器人在动态环境中的球检测性能，避免了传统方法的高标注成本。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法需要大量人工标注，成本高且耗时，因此需要一种无需标注的自适应特征提取方法。

Method: 利用预训练模型生成伪标签，通过自监督任务（如着色、边缘检测和三元组损失）学习特征，并结合MAML策略快速适应新场景。

Result: 实验表明，该方法在准确性、F1分数和IoU上优于基线模型，且收敛更快。

Conclusion: 提出的自监督框架有效提升了球检测性能，同时减少了标注需求。

Abstract: Robust and accurate ball detection is a critical component for autonomous
humanoid soccer robots, particularly in dynamic and challenging environments
such as RoboCup outdoor fields. However, traditional supervised approaches
require extensive manual annotation, which is costly and time-intensive. To
overcome this problem, we present a self-supervised learning framework for
domain-adaptive feature extraction to enhance ball detection performance. The
proposed approach leverages a general-purpose pretrained model to generate
pseudo-labels, which are then used in a suite of self-supervised pretext tasks
-- including colorization, edge detection, and triplet loss -- to learn robust
visual features without relying on manual annotations. Additionally, a
model-agnostic meta-learning (MAML) strategy is incorporated to ensure rapid
adaptation to new deployment scenarios with minimal supervision. A new dataset
comprising 10,000 labeled images from outdoor RoboCup SPL matches is
introduced, used to validate the method, and made available to the community.
Experimental results demonstrate that the proposed pipeline outperforms
baseline models in terms of accuracy, F1 score, and IoU, while also exhibiting
faster convergence.

</details>


### [86] [AnyTraverse: An off-road traversability framework with VLM and human operator in the loop](https://arxiv.org/abs/2506.16826)
*Sattwik Sahu,Agamdeep Singh,Karthik Nambiar,Srikanth Saripalli,P. B. Sujit*

Main category: cs.CV

TL;DR: AnyTraverse是一个结合自然语言提示和人工辅助的框架，用于分割可导航区域，适用于多种机器人类型，减少人工监督需求。


<details>
  <summary>Details</summary>
Motivation: 当前框架在非结构化环境中适应性不足，无法应对场景变化和不同机器人类型的需求。

Method: 结合自然语言提示和人工辅助，仅在遇到未知场景或类别时调用操作员，采用零样本学习避免数据收集和重新训练。

Result: 在多个数据集和机器人平台上验证，性能优于GA-NAV和Off-seg，提供车辆无关的解决方案。

Conclusion: AnyTraverse在自动化与人工监督之间取得平衡，适用于多样化户外场景。

Abstract: Off-road traversability segmentation enables autonomous navigation with
applications in search-and-rescue, military operations, wildlife exploration,
and agriculture. Current frameworks struggle due to significant variations in
unstructured environments and uncertain scene changes, and are not adaptive to
be used for different robot types. We present AnyTraverse, a framework
combining natural language-based prompts with human-operator assistance to
determine navigable regions for diverse robotic vehicles. The system segments
scenes for a given set of prompts and calls the operator only when encountering
previously unexplored scenery or unknown class not part of the prompt in its
region-of-interest, thus reducing active supervision load while adapting to
varying outdoor scenes. Our zero-shot learning approach eliminates the need for
extensive data collection or retraining. Our experimental validation includes
testing on RELLIS-3D, Freiburg Forest, and RUGD datasets and demonstrate
real-world deployment on multiple robot platforms. The results show that
AnyTraverse performs better than GA-NAV and Off-seg while offering a
vehicle-agnostic approach to off-road traversability that balances automation
with targeted human supervision.

</details>


### [87] [Camera Calibration via Circular Patterns: A Comprehensive Framework with Measurement Uncertainty and Unbiased Projection Model](https://arxiv.org/abs/2506.16842)
*Chaehyeon Song,Dongjae Lee,Jongwoo Lim,Ayoung Kim*

Main category: cs.CV

TL;DR: 论文提出了一种无偏的圆形图案投影模型，解决了现有圆形图案在镜头畸变下的偏差问题，并通过引入不确定性提高了校准的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有圆形图案的投影模型在镜头畸变下存在偏差，导致校准性能不佳。

Method: 提出无偏的圆形图案投影模型，引入不确定性，并基于马尔可夫随机场建模边界点。

Result: 新框架显著提高了校准的准确性和鲁棒性。

Conclusion: 该方法为相机校准提供了更精确和鲁棒的解决方案，并提供了相关代码和演示视频。

Abstract: Camera calibration using planar targets has been widely favored, and two
types of control points have been mainly considered as measurements: the
corners of the checkerboard and the centroid of circles. Since a centroid is
derived from numerous pixels, the circular pattern provides more precise
measurements than the checkerboard. However, the existing projection model of
circle centroids is biased under lens distortion, resulting in low performance.
To surmount this limitation, we propose an unbiased projection model of the
circular pattern and demonstrate its superior accuracy compared to the
checkerboard. Complementing this, we introduce uncertainty into circular
patterns to enhance calibration robustness and completeness. Defining centroid
uncertainty improves the performance of calibration components, including
pattern detection, optimization, and evaluation metrics. We also provide
guidelines for performing good camera calibration based on the evaluation
metric. The core concept of this approach is to model the boundary points of a
two-dimensional shape as a Markov random field, considering its connectivity.
The shape distribution is propagated to the centroid uncertainty through an
appropriate shape representation based on the Green theorem. Consequently, the
resulting framework achieves marked gains in calibration accuracy and
robustness. The complete source code and demonstration video are available at
https://github.com/chaehyeonsong/discocal.

</details>


### [88] [Controllable and Expressive One-Shot Video Head Swapping](https://arxiv.org/abs/2506.16852)
*Chaonan Ji,Jinwei Qi,Peng Zhang,Bang Zhang,Liefeng Bo*

Main category: cs.CV

TL;DR: 提出了一种基于扩散的多条件可控视频头部替换框架，支持头部表情和动作的动态调整，解决了现有方法在整体头部形态、发型多样性和复杂背景上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有面部替换方法主要关注局部面部替换，忽略整体头部形态；头部替换方法则难以处理发型多样性和复杂背景，且不支持替换后表情调整。

Method: 1) 身份保留上下文融合：通过形状无关掩码策略分离头部身份特征与背景/身体上下文，结合发型增强策略；2) 表情感知地标重定向与编辑：通过解耦的3DMM驱动模块分离身份、表情和头部姿态，支持表情编辑。

Result: 实验结果表明，该方法在无缝背景融合和源肖像身份保留方面表现优异，同时展示了适用于真实和虚拟角色的卓越表情转移能力。

Conclusion: 该方法通过创新策略解决了现有方法的局限性，实现了高质量的头部替换和动态表情调整。

Abstract: In this paper, we propose a novel diffusion-based multi-condition
controllable framework for video head swapping, which seamlessly transplant a
human head from a static image into a dynamic video, while preserving the
original body and background of target video, and further allowing to tweak
head expressions and movements during swapping as needed. Existing
face-swapping methods mainly focus on localized facial replacement neglecting
holistic head morphology, while head-swapping approaches struggling with
hairstyle diversity and complex backgrounds, and none of these methods allow
users to modify the transplanted head expressions after swapping. To tackle
these challenges, our method incorporates several innovative strategies through
a unified latent diffusion paradigm. 1) Identity-preserving context fusion: We
propose a shape-agnostic mask strategy to explicitly disentangle foreground
head identity features from background/body contexts, combining hair
enhancement strategy to achieve robust holistic head identity preservation
across diverse hair types and complex backgrounds. 2) Expression-aware landmark
retargeting and editing: We propose a disentangled 3DMM-driven retargeting
module that decouples identity, expression, and head poses, minimizing the
impact of original expressions in input images and supporting expression
editing. While a scale-aware retargeting strategy is further employed to
minimize cross-identity expression distortion for higher transfer precision.
Experimental results demonstrate that our method excels in seamless background
integration while preserving the identity of the source portrait, as well as
showcasing superior expression transfer capabilities applicable to both real
and virtual characters.

</details>


### [89] [ParkFormer: A Transformer-Based Parking Policy with Goal Embedding and Pedestrian-Aware Control](https://arxiv.org/abs/2506.16856)
*Jun Fu,Bin Tian,Haonan Chen,Shi Meng,Tingting Yao*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的端到端自动驾驶停车框架，通过专家演示学习，结合BEV特征与目标点，并利用GRU预测行人轨迹，在CARLA模拟器中验证了高成功率。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的停车系统难以应对环境不确定性和动态场景，而人类驾驶员能直观停车，因此提出一种学习专家演示的方法。

Method: 使用Transformer框架，输入包括环视摄像头图像、目标点表示、车辆运动和行人轨迹，输出离散控制序列。通过交叉注意力模块和GRU行人预测器增强性能。

Result: 在CARLA模拟器中，模型停车成功率达96.57%，平均位置误差0.21米，方向误差0.41度。消融实验验证了关键模块的有效性。

Conclusion: 提出的方法在自动驾驶停车任务中表现优异，关键模块如行人预测和目标点注意力融合显著提升了性能。

Abstract: Autonomous parking plays a vital role in intelligent vehicle systems,
particularly in constrained urban environments where high-precision control is
required. While traditional rule-based parking systems struggle with
environmental uncertainties and lack adaptability in crowded or dynamic scenes,
human drivers demonstrate the ability to park intuitively without explicit
modeling. Inspired by this observation, we propose a Transformer-based
end-to-end framework for autonomous parking that learns from expert
demonstrations. The network takes as input surround-view camera images,
goal-point representations, ego vehicle motion, and pedestrian trajectories. It
outputs discrete control sequences including throttle, braking, steering, and
gear selection. A novel cross-attention module integrates BEV features with
target points, and a GRU-based pedestrian predictor enhances safety by modeling
dynamic obstacles. We validate our method on the CARLA 0.9.14 simulator in both
vertical and parallel parking scenarios. Experiments show our model achieves a
high success rate of 96.57\%, with average positional and orientation errors of
0.21 meters and 0.41 degrees, respectively. The ablation studies further
demonstrate the effectiveness of key modules such as pedestrian prediction and
goal-point attention fusion. The code and dataset will be released at:
https://github.com/little-snail-f/ParkFormer.

</details>


### [90] [With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You](https://arxiv.org/abs/2506.16895)
*Fabian Gröger,Shuo Wen,Huyen Le,Maria Brbić*

Main category: cs.CV

TL;DR: 论文提出了一种在有限配对数据下构建多模态模型的方法，通过对齐预训练的单模态基础模型，仅需少量样本即可实现高质量对齐。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型依赖大量配对数据，成本高且难以获取，研究旨在探索在有限数据下实现多模态对齐的可行性。

Method: 引入STRUCTURE正则化技术，保持单模态编码器潜在空间的邻域几何结构，并优化对齐层选择。

Result: 在24个零样本分类和检索任务中，平均相对提升51.6%（分类）和91.8%（检索）。

Conclusion: 该方法为资源受限领域提供了高效的多模态学习框架。

Abstract: Multimodal models have demonstrated powerful capabilities in complex tasks
requiring multimodal alignment including zero-shot classification and
cross-modal retrieval. However, existing models typically rely on millions of
paired multimodal samples, which are prohibitively expensive or infeasible to
obtain in many domains. In this work, we explore the feasibility of building
multimodal models with limited amount of paired data by aligning pretrained
unimodal foundation models. We show that high-quality alignment is possible
with as few as tens of thousands of paired samples$\unicode{x2013}$less than
$1\%$ of the data typically used in the field. To achieve this, we introduce
STRUCTURE, an effective regularization technique that preserves the
neighborhood geometry of the latent space of unimodal encoders. Additionally,
we show that aligning last layers is often suboptimal and demonstrate the
benefits of aligning the layers with the highest representational similarity
across modalities. These two components can be readily incorporated into
existing alignment methods, yielding substantial gains across 24 zero-shot
image classification and retrieval benchmarks, with average relative
improvement of $51.6\%$ in classification and $91.8\%$ in retrieval tasks. Our
results highlight the effectiveness and broad applicability of our framework
for limited-sample multimodal learning and offer a promising path forward for
resource-constrained domains.

</details>


### [91] [LunarLoc: Segment-Based Global Localization on the Moon](https://arxiv.org/abs/2506.16940)
*Annika Thomas,Robaire Galliath,Aleksander Garbuz,Luke Anger,Cormac O'Neill,Trevor Johst,Dami Thomas,George Lordos,Jonathan P. How*

Main category: cs.CV

TL;DR: LunarLoc是一种利用实例分割提取月球表面地标的方法，通过图论数据关联实现高精度全局定位，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 在月球表面缺乏GPS等导航基础设施的情况下，实现自主操作需要精确的全局定位，以支持长期任务和复杂地形操作。

Method: 利用实例分割从立体图像中提取地标，构建地形图，并通过图论数据关联与参考地图对齐。

Result: LunarLoc在多会话全局定位实验中达到亚厘米级精度，显著优于现有技术。

Conclusion: LunarLoc为月球全局定位提供了高效解决方案，并公开数据集以促进进一步研究。

Abstract: Global localization is necessary for autonomous operations on the lunar
surface where traditional Earth-based navigation infrastructure, such as GPS,
is unavailable. As NASA advances toward sustained lunar presence under the
Artemis program, autonomous operations will be an essential component of tasks
such as robotic exploration and infrastructure deployment. Tasks such as
excavation and transport of regolith require precise pose estimation, but
proposed approaches such as visual-inertial odometry (VIO) accumulate odometry
drift over long traverses. Precise pose estimation is particularly important
for upcoming missions such as the ISRU Pilot Excavator (IPEx) that rely on
autonomous agents to operate over extended timescales and varied terrain. To
help overcome odometry drift over long traverses, we propose LunarLoc, an
approach to global localization that leverages instance segmentation for
zero-shot extraction of boulder landmarks from onboard stereo imagery. Segment
detections are used to construct a graph-based representation of the terrain,
which is then aligned with a reference map of the environment captured during a
previous session using graph-theoretic data association. This method enables
accurate and drift-free global localization in visually ambiguous settings.
LunarLoc achieves sub-cm level accuracy in multi-session global localization
experiments, significantly outperforming the state of the art in lunar global
localization. To encourage the development of further methods for global
localization on the Moon, we release our datasets publicly with a playback
module: https://github.com/mit-acl/lunarloc-data.

</details>


### [92] [LAION-C: An Out-of-Distribution Benchmark for Web-Scale Vision Models](https://arxiv.org/abs/2506.16950)
*Fanfei Li,Thomas Klein,Wieland Brendel,Robert Geirhos,Roland S. Zimmermann*

Main category: cs.CV

TL;DR: 论文提出LAION-C作为ImageNet-C的替代基准，旨在解决现有基准在评估OOD鲁棒性时的局限性，并展示了其对现代模型的挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有OOD基准（如ImageNet-C）在Web规模数据集时代已不再适用，因为其失真类型可能已被训练数据覆盖，无法真实反映模型的OOD泛化能力。

Method: 提出LAION-C，包含六种新型失真类型，专门设计为对Web规模数据集（如LAION）仍为OOD。通过评估现代模型（如Gemini和GPT-4o）及人类观察者的表现，验证其有效性。

Result: LAION-C对当代模型构成显著挑战，且最佳模型的OOD泛化能力已接近或超越人类观察者。

Conclusion: LAION-C为OOD鲁棒性评估提供了更合适的基准，标志着模型在OOD泛化能力上的范式转变。

Abstract: Out-of-distribution (OOD) robustness is a desired property of computer vision
models. Improving model robustness requires high-quality signals from
robustness benchmarks to quantify progress. While various benchmark datasets
such as ImageNet-C were proposed in the ImageNet era, most ImageNet-C
corruption types are no longer OOD relative to today's large, web-scraped
datasets, which already contain common corruptions such as blur or JPEG
compression artifacts. Consequently, these benchmarks are no longer well-suited
for evaluating OOD robustness in the era of web-scale datasets. Indeed, recent
models show saturating scores on ImageNet-era OOD benchmarks, indicating that
it is unclear whether models trained on web-scale datasets truly become better
at OOD generalization or whether they have simply been exposed to the test
distortions during training. To address this, we introduce LAION-C as a
benchmark alternative for ImageNet-C. LAION-C consists of six novel distortion
types specifically designed to be OOD, even for web-scale datasets such as
LAION. In a comprehensive evaluation of state-of-the-art models, we find that
the LAION-C dataset poses significant challenges to contemporary models,
including MLLMs such as Gemini and GPT-4o. We additionally conducted a
psychophysical experiment to evaluate the difficulty of our corruptions for
human observers, enabling a comparison of models to lab-quality human
robustness data. We observe a paradigm shift in OOD generalization: from humans
outperforming models, to the best models now matching or outperforming the best
human observers.

</details>


### [93] [Visual-Instructed Degradation Diffusion for All-in-One Image Restoration](https://arxiv.org/abs/2506.16960)
*Wenyang Luo,Haina Qin,Zewen Chen,Libin Wang,Dandan Zheng,Yuming Li,Yufan Liu,Bing Li,Weiming Hu*

Main category: cs.CV

TL;DR: Defusion提出了一种基于视觉指令引导退化扩散的全能图像修复框架，能够处理多种退化类型，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像修复方法通常需要针对每种退化类型设计独立模型，限制了其在混合或未知退化场景中的泛化能力。

Method: Defusion通过构建明确的视觉指令（基于标准化视觉元素的退化）来引导扩散模型，直接在退化空间中重建高质量图像。

Result: 实验表明，Defusion在多种图像修复任务中（包括复杂和真实世界的退化）优于现有方法。

Conclusion: Defusion通过视觉指令引导的扩散模型，实现了更稳定和通用的图像修复。

Abstract: Image restoration tasks like deblurring, denoising, and dehazing usually need
distinct models for each degradation type, restricting their generalization in
real-world scenarios with mixed or unknown degradations. In this work, we
propose \textbf{Defusion}, a novel all-in-one image restoration framework that
utilizes visual instruction-guided degradation diffusion. Unlike existing
methods that rely on task-specific models or ambiguous text-based priors,
Defusion constructs explicit \textbf{visual instructions} that align with the
visual degradation patterns. These instructions are grounded by applying
degradations to standardized visual elements, capturing intrinsic degradation
features while agnostic to image semantics. Defusion then uses these visual
instructions to guide a diffusion-based model that operates directly in the
degradation space, where it reconstructs high-quality images by denoising the
degradation effects with enhanced stability and generalizability. Comprehensive
experiments demonstrate that Defusion outperforms state-of-the-art methods
across diverse image restoration tasks, including complex and real-world
degradations.

</details>


### [94] [Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs](https://arxiv.org/abs/2506.16962)
*Haoran Sun,Yankai Jiang,Wenjie Lou,Yujie Zhang,Wenjie Li,Lilong Wang,Mianxin Liu,Lei Liu,Xiaosong Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为MICS的新方法，用于生成高质量的医学推理链数据，并构建了一个多任务医学推理数据集MMRP和新的医学MLLM模型Chiron-o1。实验表明，Chiron-o1在多个医学视觉问答和推理基准上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前医学领域的多模态大语言模型（MLLMs）在推理能力上仍处于早期阶段，缺乏全面的框架来搜索和评估有效的推理路径。

Method: 提出MICS方法，通过导师-实习生协作搜索生成严格的医学推理链数据，并构建MMRP数据集和Chiron-o1模型。

Result: Chiron-o1在多个医学视觉问答和推理基准上表现优异，达到了最先进的性能。

Conclusion: MICS方法有效提升了医学MLLMs的推理能力，Chiron-o1模型展示了强大的视觉问答和泛化推理能力。

Abstract: Multimodal large language models (MLLMs) have begun to demonstrate robust
reasoning capabilities on general tasks, yet their application in the medical
domain remains in its early stages. Constructing chain-of-thought (CoT)
training data is essential for bolstering the reasoning abilities of medical
MLLMs. However, existing approaches exhibit a deficiency in offering a
comprehensive framework for searching and evaluating effective reasoning paths
towards critical diagnosis. To address this challenge, we propose Mentor-Intern
Collaborative Search (MICS), a novel reasoning-path searching scheme to
generate rigorous and effective medical CoT data. MICS first leverages mentor
models to initialize the reasoning, one step at a time, then prompts each
intern model to continue the thinking along those initiated paths, and finally
selects the optimal reasoning path according to the overall reasoning
performance of multiple intern models. The reasoning performance is determined
by an MICS-Score, which assesses the quality of generated reasoning paths.
Eventually, we construct MMRP, a multi-task medical reasoning dataset with
ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum
learning strategy, with robust visual question-answering and generalizable
reasoning capabilities. Extensive experiments demonstrate that Chiron-o1,
trained on our CoT dataset constructed using MICS, achieves state-of-the-art
performance across a list of medical visual question answering and reasoning
benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing
Step-by-Step and Verifiable Medical Reasoning in MLLMs

</details>


### [95] [ForestFormer3D: A Unified Framework for End-to-End Segmentation of Forest LiDAR 3D Point Clouds](https://arxiv.org/abs/2506.16991)
*Binbin Xiang,Maciej Wielgosz,Stefano Puliti,Kamil Král,Martin Krůček,Azim Missarov,Rasmus Astrup*

Main category: cs.CV

TL;DR: ForestFormer3D是一种新的端到端框架，用于森林LiDAR点云的个体树和语义分割，结合了多种创新技术，在FOR-instanceV2数据集上表现优异，并展示了良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法在处理复杂多变的自然森林环境时存在困难，需要一种更精确的分割框架。

Method: ForestFormer3D采用ISA引导的查询点选择、基于分数的块合并策略和一对多关联机制。

Result: 在FOR-instanceV2数据集上达到最先进性能，并在未见过的测试集（Wytham woods和LAUTx）上表现稳健。

Conclusion: ForestFormer3D是一种高效且通用的森林点云分割框架，数据集和代码将公开。

Abstract: The segmentation of forest LiDAR 3D point clouds, including both individual
tree and semantic segmentation, is fundamental for advancing forest management
and ecological research. However, current approaches often struggle with the
complexity and variability of natural forest environments. We present
ForestFormer3D, a new unified and end-to-end framework designed for precise
individual tree and semantic segmentation. ForestFormer3D incorporates
ISA-guided query point selection, a score-based block merging strategy during
inference, and a one-to-many association mechanism for effective training. By
combining these new components, our model achieves state-of-the-art performance
for individual tree segmentation on the newly introduced FOR-instanceV2
dataset, which spans diverse forest types and regions. Additionally,
ForestFormer3D generalizes well to unseen test sets (Wytham woods and LAUTx),
showcasing its robustness across different forest conditions and sensor
modalities. The FOR-instanceV2 dataset and the ForestFormer3D code will be
released soon.

</details>


### [96] [Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for Resource-Constrained Environments](https://arxiv.org/abs/2506.16994)
*Yasir Ali Farrukh,Syed Wali,Irfan Khan,Nathaniel D. Bastian*

Main category: cs.CV

TL;DR: Prmpt2Adpt是一种轻量级、高效的零样本域适应框架，通过提示驱动的特征对齐实现快速适应，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 解决现有提示驱动UDA方法依赖大型模型和完整源域数据的问题，提升在资源受限环境中的适用性。

Method: 基于教师-学生范式，使用蒸馏和微调的CLIP模型作为教师骨干，通过提示驱动的实例归一化对齐特征，生成高质量伪标签指导学生模型适应。

Result: 在MDS-A数据集上表现优异，适应速度快7倍，推理速度快5倍，且仅需少量源图像。

Conclusion: Prmpt2Adpt是一种实用且可扩展的解决方案，适用于低资源领域的实时适应。

Abstract: Unsupervised Domain Adaptation (UDA) is a critical challenge in real-world
vision systems, especially in resource-constrained environments like drones,
where memory and computation are limited. Existing prompt-driven UDA methods
typically rely on large vision-language models and require full access to
source-domain data during adaptation, limiting their applicability. In this
work, we propose Prmpt2Adpt, a lightweight and efficient zero-shot domain
adaptation framework built around a teacher-student paradigm guided by
prompt-based feature alignment. At the core of our method is a distilled and
fine-tuned CLIP model, used as the frozen backbone of a Faster R-CNN teacher. A
small set of low-level source features is aligned to the target domain
semantics-specified only through a natural language prompt-via Prompt-driven
Instance Normalization (PIN). These semantically steered features are used to
briefly fine-tune the detection head of the teacher model. The adapted teacher
then generates high-quality pseudo-labels, which guide the on-the-fly
adaptation of a compact student model. Experiments on the MDS-A dataset
demonstrate that Prmpt2Adpt achieves competitive detection performance compared
to state-of-the-art methods, while delivering up to 7x faster adaptation and 5x
faster inference speed using few source images-making it a practical and
scalable solution for real-time adaptation in low-resource domains.

</details>


### [97] [A Synthetic Benchmark for Collaborative 3D Semantic Occupancy Prediction in V2X Autonomous Driving](https://arxiv.org/abs/2506.17004)
*Hanlin Wu,Pengfei Lin,Ehsan Javanmardi,Naren Bao,Bo Qian,Hao Si,Manabu Tsukada*

Main category: cs.CV

TL;DR: 论文提出了一种协作式3D语义占用预测方法，通过多车协作解决单车的感知限制，并在CARLA中生成密集标注数据集。


<details>
  <summary>Details</summary>
Motivation: 单车感知受限于遮挡、传感器范围和视角狭窄，协作感知可以交换互补信息，提升感知完整性和准确性。

Method: 在CARLA中重放协作感知数据集，生成高分辨率语义体素标注；提出基于空间对齐和注意力聚合的基线模型。

Result: 基线模型在扩展预测范围时表现优于单机模型，增益随范围扩大而增加。

Conclusion: 协作感知显著提升3D语义占用预测性能，尤其在长距离预测中效果更明显。

Abstract: 3D semantic occupancy prediction is an emerging perception paradigm in
autonomous driving, providing a voxel-level representation of both geometric
details and semantic categories. However, the perception capability of a single
vehicle is inherently constrained by occlusion, restricted sensor range, and
narrow viewpoints. To address these limitations, collaborative perception
enables the exchange of complementary information, thereby enhancing the
completeness and accuracy. In the absence of a dedicated dataset for
collaborative 3D semantic occupancy prediction, we augment an existing
collaborative perception dataset by replaying it in CARLA with a
high-resolution semantic voxel sensor to provide dense and comprehensive
occupancy annotations. In addition, we establish benchmarks with varying
prediction ranges designed to systematically assess the impact of spatial
extent on collaborative prediction. We further develop a baseline model that
performs inter-agent feature fusion via spatial alignment and attention
aggregation. Experimental results demonstrate that our baseline model
consistently outperforms single-agent models, with increasing gains observed as
the prediction range expands.

</details>


### [98] [Stretching Beyond the Obvious: A Gradient-Free Framework to Unveil the Hidden Landscape of Visual Invariance](https://arxiv.org/abs/2506.17040)
*Lorenzo Tausani,Paolo Muratore,Morgan B. Talbot,Giacomo Amerio,Gabriel Kreiman,Davide Zoccolan*

Main category: cs.CV

TL;DR: 论文提出了一种名为Stretch-and-Squeeze (SnS)的无偏、模型无关且无需梯度的框架，用于系统表征视觉单元的不变性及其对对抗扰动的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 理解高级视觉单元如何编码特征组合以及图像如何转化为支持识别的表示形式是关键问题。现有方法通常只能推断单元最兴奋的图像，但不足以揭示其不变性变换的多样性。

Method: SnS将变换建模为双目标优化问题：1）探索保持单元激活但最大化改变参考刺激表示的扰动；2）探索最小化改变刺激但抑制单元激活的扰动。

Result: 在卷积神经网络（CNN）中，SnS发现的图像变化比仿射变换更远离参考图像，同时更强烈地保留目标单元的响应。不同优化表示的选择导致不变性图像显著不同。

Conclusion: SnS揭示了视觉单元的不变性特性，并支持鲁棒CNN作为视觉系统模型的更高保真度。

Abstract: Uncovering which features' combinations high-level visual units encode is
critical to understand how images are transformed into representations that
support recognition. While existing feature visualization approaches typically
infer a unit's most exciting images, this is insufficient to reveal the
manifold of transformations under which responses remain invariant, which is
key to generalization in vision. Here we introduce Stretch-and-Squeeze (SnS),
an unbiased, model-agnostic, and gradient-free framework to systematically
characterize a unit's invariance landscape and its vulnerability to adversarial
perturbations in both biological and artificial visual systems. SnS frames
these transformations as bi-objective optimization problems. To probe
invariance, SnS seeks image perturbations that maximally alter the
representation of a reference stimulus in a given processing stage while
preserving unit activation. To probe adversarial sensitivity, SnS seeks
perturbations that minimally alter the stimulus while suppressing unit
activation. Applied to convolutional neural networks (CNNs), SnS revealed image
variations that were further from a reference image in pixel-space than those
produced by affine transformations, while more strongly preserving the target
unit's response. The discovered invariant images differed dramatically
depending on the choice of image representation used for optimization:
pixel-level changes primarily affected luminance and contrast, while stretching
mid- and late-layer CNN representations altered texture and pose respectively.
Notably, the invariant images from robust networks were more recognizable by
human subjects than those from standard networks, supporting the higher
fidelity of robust CNNs as models of the visual system.

</details>


### [99] [Relaxed syntax modeling in Transformers for future-proof license plate recognition](https://arxiv.org/abs/2506.17051)
*Florent Meyer,Laurent Guichard,Denis Coquenet,Guillaume Gravier,Yann Soullard,Bertrand Coüasnon*

Main category: cs.CV

TL;DR: 论文提出了一种名为SaLT的语法无关Transformer模型，用于解决现有Transformer在车牌识别中对过去语法过度依赖的问题，显著提升了未来车牌的识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的车牌识别系统在面对新语法车牌时性能显著下降，不适合实际生产环境。

Method: 通过分析Transformer编码器-解码器中位置和上下文信息的流动，识别问题原因，并设计架构改进方案，提出SaLT模型。

Result: 实验表明，SaLT在现有语法车牌上表现优异，且对未来车牌的性能几乎保持不变。

Conclusion: SaLT通过语法无关建模显著提升了车牌识别的鲁棒性，适用于动态变化的实际场景。

Abstract: Effective license plate recognition systems are required to be resilient to
constant change, as new license plates are released into traffic daily. While
Transformer-based networks excel in their recognition at first sight, we
observe significant performance drop over time which proves them unsuitable for
tense production environments. Indeed, such systems obtain state-of-the-art
results on plates whose syntax is seen during training. Yet, we show they
perform similarly to random guessing on future plates where legible characters
are wrongly recognized due to a shift in their syntax. After highlighting the
flows of positional and contextual information in Transformer encoder-decoders,
we identify several causes for their over-reliance on past syntax. Following,
we devise architectural cut-offs and replacements which we integrate into SaLT,
an attempt at a Syntax-Less Transformer for syntax-agnostic modeling of license
plate representations. Experiments on both real and synthetic datasets show
that our approach reaches top accuracy on past syntax and most importantly
nearly maintains performance on future license plates. We further demonstrate
the robustness of our architecture enhancements by way of various ablations.

</details>


### [100] [Assembler: Scalable 3D Part Assembly via Anchor Point Diffusion](https://arxiv.org/abs/2506.17074)
*Wang Zhao,Yan-Pei Cao,Jiale Xu,Yuejiang Dong,Ying Shan*

Main category: cs.CV

TL;DR: Assembler是一个可扩展且通用的3D部件组装框架，通过扩散模型和稀疏锚点云表示，实现了高质量的对象重建。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在多样化、真实世界对象组装中的局限性，如依赖确定性姿态预测和类别特定训练。

Method: 将部件组装视为生成问题，使用扩散模型采样配置；引入基于稀疏锚点云的形状表示；构建大规模数据集。

Result: 在PartNet上实现最先进性能，首次展示复杂真实世界对象的高质量组装。

Conclusion: Assembler为交互式和组合式设计提供了潜力，支持高分辨率、可编辑对象的生成。

Abstract: We present Assembler, a scalable and generalizable framework for 3D part
assembly that reconstructs complete objects from input part meshes and a
reference image. Unlike prior approaches that mostly rely on deterministic part
pose prediction and category-specific training, Assembler is designed to handle
diverse, in-the-wild objects with varying part counts, geometries, and
structures. It addresses the core challenges of scaling to general 3D part
assembly through innovations in task formulation, representation, and data.
First, Assembler casts part assembly as a generative problem and employs
diffusion models to sample plausible configurations, effectively capturing
ambiguities arising from symmetry, repeated parts, and multiple valid
assemblies. Second, we introduce a novel shape-centric representation based on
sparse anchor point clouds, enabling scalable generation in Euclidean space
rather than SE(3) pose prediction. Third, we construct a large-scale dataset of
over 320K diverse part-object assemblies using a synthesis and filtering
pipeline built on existing 3D shape repositories. Assembler achieves
state-of-the-art performance on PartNet and is the first to demonstrate
high-quality assembly for complex, real-world objects. Based on Assembler, we
further introduce an interesting part-aware 3D modeling system that generates
high-resolution, editable objects from images, demonstrating potential for
interactive and compositional design. Project page:
https://assembler3d.github.io

</details>


### [101] [Acquiring and Accumulating Knowledge from Diverse Datasets for Multi-label Driving Scene Classification](https://arxiv.org/abs/2506.17101)
*Ke Li,Chenyu Zhang,Yuxin Ding,Xianbiao Hu,Ruwen Qin*

Main category: cs.CV

TL;DR: 论文提出了一种结合知识获取与积累（KAA）和基于一致性的主动学习（CAL）的新系统，用于解决驾驶场景识别的多标签分类问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 增强自动驾驶车辆对复杂驾驶环境的理解能力，解决多标签分类中数据集不平衡和任务学习平衡的挑战。

Method: 通过KAA从单标签数据集中获取知识，再通过CAL解决知识分布差异问题。

Result: 在DSI数据集上性能提升56.1%，KAA贡献31.3%，CAL贡献24.8%；在BDD100K和HSD数据集上表现优于SOTA模型，且数据使用量减少85%。

Conclusion: KAA-CAL系统在多标签驾驶场景识别中表现出色，显著提升了性能和数据效率。

Abstract: Driving scene identification, which assigns multiple non-exclusive class
labels to a scene, provides the contextual awareness necessary for enhancing
autonomous vehicles' ability to understand, reason about, and interact with the
complex driving environment. As a multi-label classification problem, it is
better tackled via multitasking learning. However, directly training a
multi-label classification model for driving scene identification through
multitask learning presents two main challenges: acquiring a balanced,
comprehensively annotated multi-label dataset and balancing learning across
different tasks. This paper introduces a novel learning system that synergizes
knowledge acquisition and accumulation (KAA) with consistency-based active
learning (CAL) to address those challenges. KAA acquires and accumulates
knowledge about scene identification from various single-label datasets via
monotask learning. Subsequently, CAL effectively resolves the knowledge gap
caused by the discrepancy between the marginal distributions of individual
attributes and their joint distribution. An ablation study on our Driving Scene
Identification (DSI) dataset demonstrates a 56.1% performance increase over the
baseline model pretrained on ImageNet. Of this, KAA accounts for 31.3% of the
gain, and CAL contributes 24.8%. Moreover, KAA-CAL stands out as the best
performer when compared to state-of-the-art (SOTA) multi-label models on two
public datasets, BDD100K and HSD, achieving this while using 85% less data. The
DSI dataset and the implementation code for KAA-CAL are available at
https://github.com/KELISBU/KAA-CAL .

</details>


### [102] [MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation](https://arxiv.org/abs/2506.17113)
*Shoubin Yu,Yue Zhang,Ziyang Wang,Jaehong Yoon,Mohit Bansal*

Main category: cs.CV

TL;DR: MEXA是一个无需训练的框架，通过动态选择专家模型并聚合其输出，实现跨模态和跨任务的高效推理。


<details>
  <summary>Details</summary>
Motivation: 解决多模态推理中因输入模态多样性和任务复杂性带来的统一框架构建挑战。

Method: 动态选择专家模型生成文本推理输出，通过大型推理模型（LRM）聚合生成最终答案。

Result: 在多种多模态基准测试中表现优于基线，验证了其有效性和广泛适用性。

Conclusion: MEXA通过专家驱动的选择和聚合，实现了灵活透明的多模态推理。

Abstract: Combining pre-trained expert models offers substantial potential for scalable
multimodal reasoning, but building a unified framework remains challenging due
to the increasing diversity of input modalities and task complexity. For
instance, medical diagnosis requires precise reasoning over structured clinical
tables, while financial forecasting depends on interpreting plot-based data to
make informed predictions. To tackle this challenge, we introduce MEXA, a
training-free framework that performs modality- and task-aware aggregation of
multiple expert models to enable effective multimodal reasoning across diverse
and distinct domains. MEXA dynamically selects expert models based on the input
modality and the task-specific reasoning demands (i.e., skills). Each expert
model, specialized in a modality task pair, generates interpretable textual
reasoning outputs. MEXA then aggregates and reasons over these outputs using a
Large Reasoning Model (LRM) to produce the final answer. This modular design
allows flexible and transparent multimodal reasoning across diverse domains
without additional training overhead. We extensively evaluate our approach on
diverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D
Understanding, and Medical QA. MEXA consistently delivers performance
improvements over strong multimodal baselines, highlighting the effectiveness
and broad applicability of our expert-driven selection and aggregation in
diverse multimodal reasoning tasks.

</details>


### [103] [RGBTrack: Fast, Robust Depth-Free 6D Pose Estimation and Tracking](https://arxiv.org/abs/2506.17119)
*Teng Guo,Jingjin Yu*

Main category: cs.CV

TL;DR: RGBTrack是一个仅基于RGB数据的实时6D姿态估计与跟踪框架，无需深度输入，通过结合二进制搜索策略和渲染-比较机制实现高效深度推断和姿态假设生成。


<details>
  <summary>Details</summary>
Motivation: 解决动态场景中基于RGB数据的精确姿态跟踪问题，避免对深度输入的依赖。

Method: 结合FoundationPose架构，采用二进制搜索策略和渲染-比较机制，集成2D对象跟踪（XMem）、卡尔曼滤波和状态机以实现稳定跟踪。

Result: 在基准数据集上表现出竞争性精度和实时性能。

Conclusion: RGBTrack是一种实用的解决方案，适用于机器人、增强现实和计算机视觉等领域。

Abstract: We introduce a robust framework, RGBTrack, for real-time 6D pose estimation
and tracking that operates solely on RGB data, thereby eliminating the need for
depth input for such dynamic and precise object pose tracking tasks. Building
on the FoundationPose architecture, we devise a novel binary search strategy
combined with a render-and-compare mechanism to efficiently infer depth and
generate robust pose hypotheses from true-scale CAD models. To maintain stable
tracking in dynamic scenarios, including rapid movements and occlusions,
RGBTrack integrates state-of-the-art 2D object tracking (XMem) with a Kalman
filter and a state machine for proactive object pose recovery. In addition,
RGBTrack's scale recovery module dynamically adapts CAD models of unknown scale
using an initial depth estimate, enabling seamless integration with modern
generative reconstruction techniques. Extensive evaluations on benchmark
datasets demonstrate that RGBTrack's novel depth-free approach achieves
competitive accuracy and real-time performance, making it a promising practical
solution candidate for application areas including robotics, augmented reality,
and computer vision.
  The source code for our implementation will be made publicly available at
https://github.com/GreatenAnoymous/RGBTrack.git.

</details>


### [104] [Dynamic Watermark Generation for Digital Images using Perimeter Gated SPAD Imager PUFs](https://arxiv.org/abs/2506.17134)
*Md Sakibur Sajal,Marc Dandin*

Main category: cs.CV

TL;DR: 提出了一种基于pgSPAD成像器的数字水印技术，利用DSNU特性实现源识别和篡改检测。


<details>
  <summary>Details</summary>
Motivation: 探索SPAD成像器在数字水印中的应用，填补现有研究空白。

Method: 利用三个64x64 pgSPAD芯片的DSNU特性，模拟水印并测试标准图像。

Result: 实现了源识别和篡改检测，并具备可控的灵敏度-鲁棒性权衡。

Conclusion: pgSPAD成像器可用于动态水印生成，具有实际应用潜力。

Abstract: Digital image watermarks as a security feature can be derived from the
imager's physically unclonable functions (PUFs) by utilizing the manufacturing
variations, i.e., the dark signal non-uniformity (DSNU). While a few
demonstrations focused on the CMOS image sensors (CIS) and active pixel sensors
(APS), single photon avalanche diode (SPAD) imagers have never been
investigated for this purpose. In this work, we have proposed a novel
watermarking technique using perimeter gated SPAD (pgSPAD) imagers. We utilized
the DSNU of three 64 x 64 pgSPAD imager chips, fabricated in a 0.35 {\mu}m
standard CMOS process and analyzed the simulated watermarks for standard test
images from publicly available database. Our observation shows that both source
identification and tamper detection can be achieved using the proposed
source-scene-specific dynamic watermarks with a controllable
sensitivity-robustness trade-off.

</details>


### [105] [Semi-Supervised Multi-Modal Medical Image Segmentation for Complex Situations](https://arxiv.org/abs/2506.17136)
*Dongdong Meng,Sheng Li,Hao Wu,Guoping Wang,Xueqing Yan*

Main category: cs.CV

TL;DR: 提出了一种新的半监督多模态医学图像分割方法，通过多阶段融合和对比互学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决半监督条件下多模态融合方法难以有效利用未标记数据的问题。

Method: 采用多阶段多模态融合与增强策略，结合对比互学习约束模态间预测一致性。

Result: 在两个多模态数据集上验证了方法的优越性能和鲁棒性。

Conclusion: 该方法在复杂场景下的医学图像分割任务中具有潜在应用价值。

Abstract: Semi-supervised learning addresses the issue of limited annotations in
medical images effectively, but its performance is often inadequate for complex
backgrounds and challenging tasks. Multi-modal fusion methods can significantly
improve the accuracy of medical image segmentation by providing complementary
information. However, they face challenges in achieving significant
improvements under semi-supervised conditions due to the challenge of
effectively leveraging unlabeled data. There is a significant need to create an
effective and reliable multi-modal learning strategy for leveraging unlabeled
data in semi-supervised segmentation. To address these issues, we propose a
novel semi-supervised multi-modal medical image segmentation approach, which
leverages complementary multi-modal information to enhance performance with
limited labeled data. Our approach employs a multi-stage multi-modal fusion and
enhancement strategy to fully utilize complementary multi-modal information,
while reducing feature discrepancies and enhancing feature sharing and
alignment. Furthermore, we effectively introduce contrastive mutual learning to
constrain prediction consistency across modalities, thereby facilitating the
robustness of segmentation results in semi-supervised tasks. Experimental
results on two multi-modal datasets demonstrate the superior performance and
robustness of the proposed framework, establishing its valuable potential for
solving medical image segmentation tasks in complex scenarios.

</details>


### [106] [On the Theory of Conditional Feature Alignment for Unsupervised Domain-Adaptive Counting](https://arxiv.org/abs/2506.17137)
*Zhuonan Liang,Dongnan Liu,Jianan Fan,Yaxuan Song,Qiang Qu,Yu Yao,Peng Fu,Weidong Cai*

Main category: cs.CV

TL;DR: 论文提出了一种条件特征对齐的理论框架，通过分区测量条件差异，证明条件对齐能降低联合误差，并在实验中验证了其优于现有无监督域适应方法。


<details>
  <summary>Details</summary>
Motivation: 解决目标计数模型在跨域部署时因密度变化导致的性能下降问题，传统域适应方法无法处理任务相关的密度变化。

Method: 提出条件特征对齐框架，分区测量条件差异，推导联合误差边界，并设计实际适应策略。

Result: 实验表明，该方法在多个密度分布不同的计数数据集上优于现有无监督域适应方法。

Conclusion: 条件特征对齐能有效提升跨域计数任务的泛化性能，理论和实验均验证了其优势。

Abstract: Object counting models suffer when deployed across domains with differing
density variety, since density shifts are inherently task-relevant and violate
standard domain adaptation assumptions. To address this, we propose a
theoretical framework of conditional feature alignment. We first formalize the
notion of conditional divergence by partitioning each domain into subsets
(e.g., object vs. background) and measuring divergences per condition. We then
derive a joint error bound showing that, under discrete label spaces treated as
condition sets, aligning distributions conditionally leads to tighter bounds on
the combined source-target decision error than unconditional alignment. These
insights motivate a general conditional adaptation principle: by preserving
task-relevant variations while filtering out nuisance shifts, one can achieve
superior cross-domain generalization for counting. We provide both defining
conditional divergence then proving its benefit in lowering joint error and a
practical adaptation strategy that preserves task-relevant information in
unsupervised domain-adaptive counting. We demonstrate the effectiveness of our
approach through extensive experiments on multiple counting datasets with
varying density distributions. The results show that our method outperforms
existing unsupervised domain adaptation methods, empirically validating the
theoretical insights on conditional feature alignment.

</details>


### [107] [Do We Need Large VLMs for Spotting Soccer Actions?](https://arxiv.org/abs/2506.17144)
*Ritabrata Chakraborty,Rajatsubhra Chakraborty,Avijit Dasgupta,Sandeep Chaurasia*

Main category: cs.CV

TL;DR: 论文提出了一种基于文本的轻量级足球动作识别方法，利用大型语言模型（LLMs）替代传统的视觉语言模型（VLMs），通过专家评论识别关键动作。


<details>
  <summary>Details</summary>
Motivation: 传统视频处理方法计算量大且复杂，而专家评论提供了丰富的细粒度描述和上下文信息，足以可靠地识别关键动作。

Method: 使用SoccerNet Echoes数据集，通过三个LLMs分别评估评论窗口，识别进球、黄牌等动作并生成时间戳。

Result: 实验表明，这种基于语言的方法能有效检测关键比赛事件，且无需训练。

Conclusion: 该方法为动作识别提供了一种轻量级、无需训练的替代方案。

Abstract: Traditional video-based tasks like soccer action spotting rely heavily on
visual inputs, often requiring complex and computationally expensive models to
process dense video data. In this work, we propose a shift from this
video-centric approach to a text-based task, making it lightweight and scalable
by utilizing Large Language Models (LLMs) instead of Vision-Language Models
(VLMs). We posit that expert commentary, which provides rich, fine-grained
descriptions and contextual cues such as excitement and tactical insights,
contains enough information to reliably spot key actions in a match. To
demonstrate this, we use the SoccerNet Echoes dataset, which provides
timestamped commentary, and employ a system of three LLMs acting as judges
specializing in outcome, excitement, and tactics. Each LLM evaluates sliding
windows of commentary to identify actions like goals, cards, and substitutions,
generating accurate timestamps for these events. Our experiments show that this
language-centric approach performs effectively in detecting critical match
events, providing a lightweight and training-free alternative to traditional
video-based methods for action spotting.

</details>


### [108] [Co-Seg++: Mutual Prompt-Guided Collaborative Learning for Versatile Medical Segmentation](https://arxiv.org/abs/2506.17159)
*Qing Xu,Yuxiang Luo,Wenting Duan,Zhen Chen*

Main category: cs.CV

TL;DR: Co-Seg++框架通过联合语义和实例分割任务，提升医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常独立处理不同分割任务，忽略了任务间的相互依赖性，导致分割性能不佳。

Method: 提出Co-Seg++框架，包括STP-Encoder捕获空间和时间关系，以及MTC-Decoder通过跨任务指导增强一致性。

Result: 在多种CT和组织病理学数据集上，Co-Seg++在语义、实例和全景分割任务中表现优于现有方法。

Conclusion: Co-Seg++通过任务协同显著提升了医学图像分割的准确性和理解能力。

Abstract: Medical image analysis is critical yet challenged by the need of jointly
segmenting organs or tissues, and numerous instances for anatomical structures
and tumor microenvironment analysis. Existing studies typically formulated
different segmentation tasks in isolation, which overlooks the fundamental
interdependencies between these tasks, leading to suboptimal segmentation
performance and insufficient medical image understanding. To address this
issue, we propose a Co-Seg++ framework for versatile medical segmentation.
Specifically, we introduce a novel co-segmentation paradigm, allowing semantic
and instance segmentation tasks to mutually enhance each other. We first devise
a spatio-temporal prompt encoder (STP-Encoder) to capture long-range spatial
and temporal relationships between segmentation regions and image embeddings as
prior spatial constraints. Moreover, we devise a multi-task collaborative
decoder (MTC-Decoder) that leverages cross-guidance to strengthen the
contextual consistency of both tasks, jointly computing semantic and instance
segmentation masks. Extensive experiments on diverse CT and histopathology
datasets demonstrate that the proposed Co-Seg++ outperforms state-of-the-arts
in the semantic, instance, and panoptic segmentation of dental anatomical
structures, histopathology tissues, and nuclei instances. The source code is
available at https://github.com/xq141839/Co-Seg-Plus.

</details>


### [109] [YASMOT: Yet another stereo image multi-object tracker](https://arxiv.org/abs/2506.17186)
*Ketil Malde*

Main category: cs.CV

TL;DR: YASMOT是一个轻量级、灵活的对象跟踪器，用于处理来自流行对象检测器的输出，支持单目或立体相机配置，并能生成检测器集合的共识检测。


<details>
  <summary>Details</summary>
Motivation: 在图像时间序列中，跟踪对象并保持其身份可以提高检测性能，并为下游任务（如行为分类和丰度估计）提供支持。

Method: YASMOT处理对象检测器的输出，支持单目或立体相机配置，并能生成检测器集合的共识检测。

Result: YASMOT能够有效跟踪对象并生成共识检测。

Conclusion: YASMOT是一个轻量级且灵活的工具，适用于对象跟踪和检测器集合的共识生成。

Abstract: There now exists many popular object detectors based on deep learning that
can analyze images and extract locations and class labels for occurrences of
objects. For image time series (i.e., video or sequences of stills), tracking
objects over time and preserving object identity can help to improve object
detection performance, and is necessary for many downstream tasks, including
classifying and predicting behaviors, and estimating total abundances. Here we
present yasmot, a lightweight and flexible object tracker that can process the
output from popular object detectors and track objects over time from either
monoscopic or stereoscopic camera configurations. In addition, it includes
functionality to generate consensus detections from ensembles of object
detectors.

</details>


### [110] [Facial Landmark Visualization and Emotion Recognition Through Neural Networks](https://arxiv.org/abs/2506.17191)
*Israel Juárez-Jiménez,Tiffany Guadalupe Martínez Paredes,Jesús García-Ramírez,Eric Ramos Aguilar*

Main category: cs.CV

TL;DR: 论文提出了一种通过面部标志箱线图识别数据集中异常值的方法，并比较了两种面部标志特征，发现神经网络优于随机森林分类器。


<details>
  <summary>Details</summary>
Motivation: 面部图像情感识别在人机交互中很重要，但现有研究缺乏对数据集的深入分析，且面部标志可视化提取有意义信息具有挑战性。

Method: 提出面部标志箱线图可视化技术，比较两种面部标志特征：绝对位置和从中性表情到情绪峰值时的位移。

Result: 神经网络在性能上优于随机森林分类器。

Conclusion: 面部标志箱线图有助于识别数据集异常值，且神经网络更适合此类任务。

Abstract: Emotion recognition from facial images is a crucial task in human-computer
interaction, enabling machines to learn human emotions through facial
expressions. Previous studies have shown that facial images can be used to
train deep learning models; however, most of these studies do not include a
through dataset analysis. Visualizing facial landmarks can be challenging when
extracting meaningful dataset insights; to address this issue, we propose
facial landmark box plots, a visualization technique designed to identify
outliers in facial datasets. Additionally, we compare two sets of facial
landmark features: (i) the landmarks' absolute positions and (ii) their
displacements from a neutral expression to the peak of an emotional expression.
Our results indicate that a neural network achieves better performance than a
random forest classifier.

</details>


### [111] [Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition](https://arxiv.org/abs/2506.17201)
*Jiaqi Li,Junshu Tang,Zhiyong Xu,Longhuang Wu,Yuan Zhou,Shuai Shao,Tianbao Yu,Zhiguo Cao,Qinglin Lu*

Main category: cs.CV

TL;DR: Hunyuan-GameCraft 是一个用于游戏环境中高动态交互视频生成的新框架，通过统一输入、混合历史条件训练和模型蒸馏，显著提升了视频生成的动态性、一致性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散和可控视频生成的方法在动态性、通用性、长期一致性和效率方面存在局限，限制了多样化游戏视频的生成能力。

Method: 统一键盘和鼠标输入到共享相机表示空间，提出混合历史条件训练策略，并通过模型蒸馏提高推理效率。

Result: 在超过100款AAA游戏的百万级数据集上训练，实验表明Hunyuan-GameCraft在真实感和可玩性上显著优于现有模型。

Conclusion: Hunyuan-GameCraft 通过创新方法解决了现有技术的局限性，为交互式游戏视频生成提供了更高质量的解决方案。

Abstract: Recent advances in diffusion-based and controllable video generation have
enabled high-quality and temporally coherent video synthesis, laying the
groundwork for immersive interactive gaming experiences. However, current
methods face limitations in dynamics, generality, long-term consistency, and
efficiency, which limit the ability to create various gameplay videos. To
address these gaps, we introduce Hunyuan-GameCraft, a novel framework for
high-dynamic interactive video generation in game environments. To achieve
fine-grained action control, we unify standard keyboard and mouse inputs into a
shared camera representation space, facilitating smooth interpolation between
various camera and movement operations. Then we propose a hybrid
history-conditioned training strategy that extends video sequences
autoregressively while preserving game scene information. Additionally, to
enhance inference efficiency and playability, we achieve model distillation to
reduce computational overhead while maintaining consistency across long
temporal sequences, making it suitable for real-time deployment in complex
interactive environments. The model is trained on a large-scale dataset
comprising over one million gameplay recordings across over 100 AAA games,
ensuring broad coverage and diversity, then fine-tuned on a carefully annotated
synthetic dataset to enhance precision and control. The curated game scene data
significantly improves the visual fidelity, realism and action controllability.
Extensive experiments demonstrate that Hunyuan-GameCraft significantly
outperforms existing models, advancing the realism and playability of
interactive game video generation.

</details>


### [112] [UniFork: Exploring Modality Alignment for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2506.17202)
*Teng Li,Quanfeng Lu,Lirui Zhao,Hao Li,Xizhou Zhu,Yu Qiao,Jun Zhang,Wenqi Shao*

Main category: cs.CV

TL;DR: UniFork是一种新型Y形架构，通过共享浅层和任务特定分支的深层，解决了统一模型中理解和生成任务之间的模态对齐冲突，性能优于传统共享架构。


<details>
  <summary>Details</summary>
Motivation: 统一图像理解和生成模型的设计存在挑战，因为理解和生成任务对模态对齐的需求不同，传统共享架构会导致性能妥协。

Method: 分析了任务特定模型和统一模型的模态对齐行为，提出UniFork架构，共享浅层并采用任务特定分支。

Result: UniFork性能优于传统共享架构，与任务特定模型相当或更好。

Conclusion: UniFork通过平衡共享学习和任务专业化，有效解决了统一模型中的任务冲突问题。

Abstract: Unified image understanding and generation has emerged as a promising
paradigm in multimodal artificial intelligence. Despite recent progress, the
optimal architectural design for such unified models remains an open challenge.
In this work, we start by analyzing the modality alignment behaviors of
task-specific expert models for understanding and generation, as well as
current unified models. Our analysis reveals a crucial observation:
understanding tasks benefit from a progressively increasing modality alignment
across network depth, which helps build up semantic information for better
comprehension; In contrast, generation tasks follow a different trend: modality
alignment increases in the early layers but decreases in the deep layers to
recover spatial details. These divergent alignment patterns create a
fundamental conflict in fully shared Transformer backbones, where a uniform
representational flow often leads to performance compromises across two tasks.
Motivated by this finding, we introduce UniFork, a novel Y-shaped architecture
that shares the shallow layers for cross-task representation learning, while
employing task-specific branches in deeper layers to avoid task interference.
This design effectively balances shared learning and task specialization.
Through extensive ablation experiments, we demonstrate that Unifork
consistently outperforms conventional fully shared Transformer architectures,
and achieves performance on par with or better than task-specific models.

</details>


### [113] [Part$^{2}$GS: Part-aware Modeling of Articulated Objects using 3D Gaussian Splatting](https://arxiv.org/abs/2506.17212)
*Tianjiao Yu,Vedant Shah,Muntasir Wahed,Ying Shen,Kiet A. Nguyen,Ismini Lourentzou*

Main category: cs.CV

TL;DR: Part$^{2}$GS是一种新型框架，用于建模多部件物体的高保真几何和物理一致运动，通过部分感知的3D高斯表示和物理约束实现。


<details>
  <summary>Details</summary>
Motivation: 现实世界中关节物体常见，但其结构和运动的建模仍是3D重建方法的挑战。

Method: 采用部分感知的3D高斯表示，结合物理约束（如接触强制、速度一致性和矢量场对齐）和排斥点场以防止部件碰撞。

Result: 在合成和真实数据集上，Part$^{2}$GS在可移动部件的Chamfer距离上优于现有方法10倍。

Conclusion: Part$^{2}$GS为关节物体的高保真建模提供了有效解决方案，显著提升了运动一致性。

Abstract: Articulated objects are common in the real world, yet modeling their
structure and motion remains a challenging task for 3D reconstruction methods.
In this work, we introduce Part$^{2}$GS, a novel framework for modeling
articulated digital twins of multi-part objects with high-fidelity geometry and
physically consistent articulation. Part$^{2}$GS leverages a part-aware 3D
Gaussian representation that encodes articulated components with learnable
attributes, enabling structured, disentangled transformations that preserve
high-fidelity geometry. To ensure physically consistent motion, we propose a
motion-aware canonical representation guided by physics-based constraints,
including contact enforcement, velocity consistency, and vector-field
alignment. Furthermore, we introduce a field of repel points to prevent part
collisions and maintain stable articulation paths, significantly improving
motion coherence over baselines. Extensive evaluations on both synthetic and
real-world datasets show that Part$^{2}$GS consistently outperforms
state-of-the-art methods by up to 10$\times$ in Chamfer Distance for movable
parts.

</details>


### [114] [Long-term Traffic Simulation with Interleaved Autoregressive Motion and Scenario Generation](https://arxiv.org/abs/2506.17213)
*Xiuyu Yang,Shuhan Tan,Philipp Krähenbühl*

Main category: cs.CV

TL;DR: InfGen是一个统一的下一代预测模型，通过交替进行闭环运动模拟和场景生成，实现稳定的长期交通模拟。


<details>
  <summary>Details</summary>
Motivation: 现有的交通模拟器主要关注初始场景的闭环运动模拟，无法满足长期模拟的需求，因为场景中的智能体会随着时间进出。

Method: InfGen采用统一的下一代预测模型，交替进行闭环运动模拟和场景生成，自动切换模式以实现长期模拟。

Result: InfGen在短期（9秒）交通模拟中达到最先进水平，在长期（30秒）模拟中显著优于其他方法。

Conclusion: InfGen为长期交通模拟提供了一种有效的解决方案，并将在未来公开发布代码和模型。

Abstract: An ideal traffic simulator replicates the realistic long-term point-to-point
trip that a self-driving system experiences during deployment. Prior models and
benchmarks focus on closed-loop motion simulation for initial agents in a
scene. This is problematic for long-term simulation. Agents enter and exit the
scene as the ego vehicle enters new regions. We propose InfGen, a unified
next-token prediction model that performs interleaved closed-loop motion
simulation and scene generation. InfGen automatically switches between
closed-loop motion simulation and scene generation mode. It enables stable
long-term rollout simulation. InfGen performs at the state-of-the-art in
short-term (9s) traffic simulation, and significantly outperforms all other
methods in long-term (30s) simulation. The code and model of InfGen will be
released at https://orangesodahub.github.io/InfGen

</details>


### [115] [Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens](https://arxiv.org/abs/2506.17218)
*Zeyuan Yang,Xueyang Yu,Delin Chen,Maohao Shen,Chuang Gan*

Main category: cs.CV

TL;DR: 论文提出了一种名为Mirage的框架，通过隐式视觉令牌增强视觉语言模型（VLM）的解码能力，避免显式图像生成，从而提升多模态推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在需要视觉想象的任务中表现受限，因为它们必须将视觉推理转化为文本输出。显式图像生成训练又可能损害推理能力。

Method: Mirage框架引入隐式视觉令牌，通过蒸馏和强化学习训练模型在解码时交替使用文本和视觉令牌。

Result: 实验证明Mirage在不生成显式图像的情况下，显著提升了多模态推理能力。

Conclusion: Mirage展示了通过隐式视觉令牌实现高效多模态推理的潜力。

Abstract: Vision-language models (VLMs) excel at multimodal understanding, yet their
text-only decoding forces them to verbalize visual reasoning, limiting
performance on tasks that demand visual imagination. Recent attempts train VLMs
to render explicit images, but the heavy image-generation pre-training often
hinders the reasoning ability. Inspired by the way humans reason with mental
imagery-the internal construction and manipulation of visual cues-we
investigate whether VLMs can reason through interleaved multimodal trajectories
without producing explicit images. To this end, we present a Machine Mental
Imagery framework, dubbed as Mirage, which augments VLM decoding with latent
visual tokens alongside ordinary text. Concretely, whenever the model chooses
to ``think visually'', it recasts its hidden states as next tokens, thereby
continuing a multimodal trajectory without generating pixel-level images. Begin
by supervising the latent tokens through distillation from ground-truth image
embeddings, we then switch to text-only supervision to make the latent
trajectory align tightly with the task objective. A subsequent reinforcement
learning stage further enhances the multimodal reasoning capability.
Experiments on diverse benchmarks demonstrate that Mirage unlocks stronger
multimodal reasoning without explicit image generation.

</details>


### [116] [Emergent Temporal Correspondences from Video Diffusion Transformers](https://arxiv.org/abs/2506.17220)
*Jisu Nam,Soowon Son,Dahyun Chung,Jiyoung Kim,Siyoon Jin,Junhwa Hur,Seungryong Kim*

Main category: cs.CV

TL;DR: DiffTrack是一个定量分析框架，用于研究视频扩散模型（DiTs）如何建立和表示帧间时间对应关系。


<details>
  <summary>Details</summary>
Motivation: 探索视频扩散模型内部如何建立时间对应关系，填补研究空白。

Method: 构建带伪标注的视频数据集，提出新评估指标，分析3D注意力机制中各组件的作用。

Result: 发现特定层的查询-键相似性在时间匹配中起关键作用，且在去噪过程中愈发显著。

Conclusion: DiffTrack为理解视频DiTs提供了重要见解，并在零样本点跟踪和视频生成中展示了应用潜力。

Abstract: Recent advancements in video diffusion models based on Diffusion Transformers
(DiTs) have achieved remarkable success in generating temporally coherent
videos. Yet, a fundamental question persists: how do these models internally
establish and represent temporal correspondences across frames? We introduce
DiffTrack, the first quantitative analysis framework designed to answer this
question. DiffTrack constructs a dataset of prompt-generated video with pseudo
ground-truth tracking annotations and proposes novel evaluation metrics to
systematically analyze how each component within the full 3D attention
mechanism of DiTs (e.g., representations, layers, and timesteps) contributes to
establishing temporal correspondences. Our analysis reveals that query-key
similarities in specific, but not all, layers play a critical role in temporal
matching, and that this matching becomes increasingly prominent during the
denoising process. We demonstrate practical applications of DiffTrack in
zero-shot point tracking, where it achieves state-of-the-art performance
compared to existing vision foundation and self-supervised video models.
Further, we extend our findings to motion-enhanced video generation with a
novel guidance method that improves temporal consistency of generated videos
without additional training. We believe our work offers crucial insights into
the inner workings of video DiTs and establishes a foundation for further
research and applications leveraging their temporal understanding.

</details>


### [117] [VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.17221)
*Zhangyang Qi,Zhixiong Zhang,Yizhou Yu,Jiaqi Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: VLN-R1是一个端到端框架，利用大型视觉语言模型（LVLM）直接从第一视角视频流生成连续导航动作，通过GRPO训练和两阶段微调（SFT和RFT）提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于语言模型的导航系统依赖于离散拓扑图，限制了路径规划的灵活性。VLN-R1旨在通过LVLM实现更连续的导航动作。

Method: 使用Habitat构建VLN-Ego数据集，提出长短期记忆采样平衡历史与当前观察。采用两阶段训练：监督微调（SFT）对齐专家演示，强化微调（RFT）结合时间衰减奖励（TDR）。

Result: VLN-R1在VLN-CE基准测试中表现优异，验证了LVLM在导航任务中的潜力。

Conclusion: VLN-R1展示了LVLM在具身导航中的有效性，并通过数据高效、奖励驱动的后训练增强了任务推理能力。

Abstract: Vision-Language Navigation (VLN) is a core challenge in embodied AI,
requiring agents to navigate real-world environments using natural language
instructions. Current language model-based navigation systems operate on
discrete topological graphs, limiting path planning to predefined node
connections. We propose VLN-R1, an end-to-end framework that leverages Large
Vision-Language Models (LVLM) to directly translate egocentric video streams
into continuous navigation actions, adopting GRPO-based training inspired by
DeepSeek-R1. To enable effective training, we first construct the VLN-Ego
dataset using a 3D simulator, Habitat, and propose Long-Short Memory Sampling
to balance historical and current observations. While large language models can
supervise complete textual instructions, they lack fine-grained action-level
control. Our framework employs a two-stage training approach: a) Supervised
fine-tuning (SFT) to align the model's action sequence text predictions with
expert demonstrations, followed by b) Reinforcement fine-tuning (RFT) enhanced
with a Time-Decayed Reward (TDR) mechanism that strategically weights
multi-step future actions. Experimental results show VLN-R1 achieves strong
performance on VLN-CE benchmark. VLN-R1 proves LVLMs can drive embodied
navigation and enhance task-specific reasoning through data-efficient,
reward-driven post-training.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [118] [Pixel-wise Modulated Dice Loss for Medical Image Segmentation](https://arxiv.org/abs/2506.15744)
*Seyed Mohsen Hosseini*

Main category: eess.IV

TL;DR: 论文提出了一种改进的Dice损失函数（PM Dice损失），通过像素级调制项同时解决医学分割任务中的类别不平衡和难度不平衡问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学分割任务中存在类别不平衡和难度不平衡问题，传统方法（如交叉熵损失或改进的Dice损失）计算成本高且效果有限。

Method: 提出了一种简单的改进Dice损失函数（PM Dice损失），通过像素级调制项在保持类别不平衡处理能力的同时解决难度不平衡问题。

Result: 在三种常用医学分割任务上的实验表明，PM Dice损失优于其他针对难度不平衡问题的方法。

Conclusion: PM Dice损失是一种计算成本低且高效的方法，能够同时解决医学分割中的类别不平衡和难度不平衡问题。

Abstract: Class imbalance and the difficulty imbalance are the two types of data
imbalance that affect the performance of neural networks in medical
segmentation tasks. In class imbalance the loss is dominated by the majority
classes and in difficulty imbalance the loss is dominated by easy to classify
pixels. This leads to an ineffective training. Dice loss, which is based on a
geometrical metric, is very effective in addressing the class imbalance
compared to the cross entropy (CE) loss, which is adopted directly from
classification tasks. To address the difficulty imbalance, the common approach
is employing a re-weighted CE loss or a modified Dice loss to focus the
training on difficult to classify areas. The existing modification methods are
computationally costly and with limited success. In this study we propose a
simple modification to the Dice loss with minimal computational cost. With a
pixel level modulating term, we take advantage of the effectiveness of Dice
loss in handling the class imbalance to also handle the difficulty imbalance.
Results on three commonly used medical segmentation tasks show that the
proposed Pixel-wise Modulated Dice loss (PM Dice loss) outperforms other
methods, which are designed to tackle the difficulty imbalance problem.

</details>


### [119] [InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video Understanding](https://arxiv.org/abs/2506.15745)
*Minsoo Kim,Kyuhong Shim,Jungwook Choi,Simyung Chang*

Main category: eess.IV

TL;DR: InfiniPot-V是一种无需训练、与查询无关的框架，通过动态压缩KV缓存，解决MLLMs在流式视频理解中的内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型（MLLMs）在流式视频处理中KV缓存线性增长导致内存不足的问题。

Method: 采用轻量级压缩策略，包括基于时间冗余的令牌移除（TaR）和基于语义重要性的令牌保留（VaN）。

Result: 在多个基准测试中，峰值GPU内存减少高达94%，保持实时生成，且精度不降。

Conclusion: InfiniPot-V无需重新训练或查询知识，为设备端流式视频助手提供了可行解决方案。

Abstract: Modern multimodal large language models (MLLMs) can reason over hour-long
video, yet their key-value (KV) cache grows linearly with time--quickly
exceeding the fixed memory of phones, AR glasses, and edge robots. Prior
compression schemes either assume the whole video and user query are available
offline or must first build the full cache, so memory still scales with stream
length. InfiniPot-V is the first training-free, query-agnostic framework that
enforces a hard, length-independent memory cap for streaming video
understanding. During video encoding it monitors the cache and, once a user-set
threshold is reached, runs a lightweight compression pass that (i) removes
temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)
keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four
open-source MLLMs and four long-video and two streaming-video benchmarks,
InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,
and matches or surpasses full-cache accuracy--even in multi-turn dialogues. By
dissolving the KV cache bottleneck without retraining or query knowledge,
InfiniPot-V closes the gap for on-device streaming video assistants.

</details>


### [120] [Diffusion-based Counterfactual Augmentation: Towards Robust and Interpretable Knee Osteoarthritis Grading](https://arxiv.org/abs/2506.15748)
*Zhe Wang,Yuhua Ru,Aladine Chetouani,Tina Shiang,Fang Chen,Fabian Bauer,Liping Zhang,Didier Hans,Rachid Jennane,William Ewing Palmer,Mohamed Jarraya,Yung Hsin Chen*

Main category: eess.IV

TL;DR: 论文提出了一种基于扩散模型的反事实增强框架（DCA），通过生成针对性反事实样本提升模型鲁棒性和可解释性，显著提高了膝关节骨关节炎（KOA）自动分级的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决膝关节骨关节炎（KOA）自动分级中观察者间差异大及深度学习模型在关键决策边界附近鲁棒性不足的问题。

Method: 使用扩散模型和随机微分方程（SDE）生成反事实样本，结合自校正学习策略优化分类器。

Result: 在OAI和MOST数据集上显著提升了分类准确性，并验证了潜在空间拓扑与KOA临床进展的一致性。

Conclusion: DCA框架将模型不确定性转化为鲁棒训练信号，为开发更准确可靠的自动诊断系统提供了新途径。

Abstract: Automated grading of Knee Osteoarthritis (KOA) from radiographs is challenged
by significant inter-observer variability and the limited robustness of deep
learning models, particularly near critical decision boundaries. To address
these limitations, this paper proposes a novel framework, Diffusion-based
Counterfactual Augmentation (DCA), which enhances model robustness and
interpretability by generating targeted counterfactual examples. The method
navigates the latent space of a diffusion model using a Stochastic Differential
Equation (SDE), governed by balancing a classifier-informed boundary drive with
a manifold constraint. The resulting counterfactuals are then used within a
self-corrective learning strategy to improve the classifier by focusing on its
specific areas of uncertainty. Extensive experiments on the public
Osteoarthritis Initiative (OAI) and Multicenter Osteoarthritis Study (MOST)
datasets demonstrate that this approach significantly improves classification
accuracy across multiple model architectures. Furthermore, the method provides
interpretability by visualizing minimal pathological changes and revealing that
the learned latent space topology aligns with clinical knowledge of KOA
progression. The DCA framework effectively converts model uncertainty into a
robust training signal, offering a promising pathway to developing more
accurate and trustworthy automated diagnostic systems. Our code is available at
https://github.com/ZWang78/DCA.

</details>


### [121] [D2Diff : A Dual Domain Diffusion Model for Accurate Multi-Contrast MRI Synthesis](https://arxiv.org/abs/2506.15750)
*Sanuwani Dayarathna,Himashi Peiris,Kh Tohidul Islam,Tien-Tsin Wong,Zhaolin Chen*

Main category: eess.IV

TL;DR: 提出了一种双域学习框架，结合空间和频域信息，用于多对比MRI合成，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多对比MRI合成因复杂非线性关系和对比间差异而具有挑战性，现有方法难以同时捕捉全局和局部特征。

Method: 采用双域学习框架，结合空间和频域特征，通过两个去噪网络和共享批评网络实现，并使用不确定性驱动的掩码损失。

Result: 实验表明，该方法优于现有基准，且合成结果对下游分割任务具有诊断价值。

Conclusion: 双域学习框架有效提升了多对比MRI合成的性能，具有实际应用潜力。

Abstract: Multi contrast MRI synthesis is inherently challenging due to the complex and
nonlinear relationships among different contrasts. Each MRI contrast highlights
unique tissue properties, but their complementary information is difficult to
exploit due to variations in intensity distributions and contrast specific
textures. Existing methods for multi contrast MRI synthesis primarily utilize
spatial domain features, which capture localized anatomical structures but
struggle to model global intensity variations and distributed patterns.
Conversely, frequency domain features provide structured inter contrast
correlations but lack spatial precision, limiting their ability to retain finer
details. To address this, we propose a dual domain learning framework that
integrates spatial and frequency domain information across multiple MRI
contrasts for enhanced synthesis. Our method employs two mutually trained
denoising networks, one conditioned on spatial domain and the other on
frequency domain contrast features through a shared critic network.
Additionally, an uncertainty driven mask loss directs the models focus toward
more critical regions, further improving synthesis accuracy. Extensive
experiments show that our method outperforms SOTA baselines, and the downstream
segmentation performance highlights the diagnostic value of the synthetic
results.

</details>


### [122] [Implicit neural representations for accurate estimation of the standard model of white matter](https://arxiv.org/abs/2506.15762)
*Tom Hendriks,Gerrit Arends,Edwin Versteeg,Anna Vilanova,Maxime Chamberland,Chantal M. W. Tax*

Main category: eess.IV

TL;DR: 该论文提出了一种基于隐式神经表示（INR）的新框架，用于估计白质标准模型（SM）参数，在低信噪比条件下表现出更高的准确性，并支持空间上采样和无监督学习。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在高维白质标准模型参数估计中因噪声和参数退化导致的准确性挑战。

Method: 使用隐式神经表示（INR）结合空间正则化，通过正弦编码输入坐标，无需标记训练数据，支持快速推理和噪声鲁棒性。

Result: INR方法在合成和体内数据集中表现出更高的参数估计准确性，尤其在低信噪比条件下，并能连续表示解剖结构。

Conclusion: INR框架为分析和解释扩散MRI数据提供了潜在的重要工具，具有适应性和鲁棒性。

Abstract: Diffusion magnetic resonance imaging (dMRI) enables non-invasive
investigation of tissue microstructure. The Standard Model (SM) of white matter
aims to disentangle dMRI signal contributions from intra- and extra-axonal
water compartments. However, due to the model its high-dimensional nature,
extensive acquisition protocols with multiple b-values and diffusion tensor
shapes are typically required to mitigate parameter degeneracies. Even then,
accurate estimation remains challenging due to noise. This work introduces a
novel estimation framework based on implicit neural representations (INRs),
which incorporate spatial regularization through the sinusoidal encoding of the
input coordinates. The INR method is evaluated on both synthetic and in vivo
datasets and compared to parameter estimates using cubic polynomials,
supervised neural networks, and nonlinear least squares. Results demonstrate
superior accuracy of the INR method in estimating SM parameters, particularly
in low signal-to-noise conditions. Additionally, spatial upsampling of the INR
can represent the underlying dataset anatomically plausibly in a continuous
way, which is unattainable with linear or cubic interpolation. The INR is fully
unsupervised, eliminating the need for labeled training data. It achieves fast
inference ($\sim$6 minutes), is robust to both Gaussian and Rician noise,
supports joint estimation of SM kernel parameters and the fiber orientation
distribution function with spherical harmonics orders up to at least 8 and
non-negativity constraints, and accommodates spatially varying acquisition
protocols caused by magnetic gradient non-uniformities. The combination of
these properties along with the possibility to easily adapt the framework to
other dMRI models, positions INRs as a potentially important tool for analyzing
and interpreting diffusion MRI data.

</details>


### [123] [MoNetV2: Enhanced Motion Network for Freehand 3D Ultrasound Reconstruction](https://arxiv.org/abs/2506.15835)
*Mingyuan Luo,Xin Yang,Zhongnuo Yan,Yan Cao,Yuanji Zhang,Xindi Hu,Jin Wang,Haoxuan Ding,Wei Han,Litao Sun,Dong Ni*

Main category: eess.IV

TL;DR: MoNetV2通过融合图像与运动信息、多级一致性约束及多模态自监督策略，显著提升了自由手3D超声重建的精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 自由手3D超声重建在复杂运动轨迹下存在累积漂移和精度不足的问题，需要一种更鲁棒的方法。

Method: 提出MoNetV2，包括传感器辅助的多分支结构、在线多级一致性约束和多模态自监督策略。

Result: 在三个大型数据集上，MoNetV2在重建质量和泛化性能上均优于现有方法。

Conclusion: MoNetV2为自由手3D超声重建提供了一种高精度、高泛化性的解决方案。

Abstract: Three-dimensional (3D) ultrasound (US) aims to provide sonographers with the
spatial relationships of anatomical structures, playing a crucial role in
clinical diagnosis. Recently, deep-learning-based freehand 3D US has made
significant advancements. It reconstructs volumes by estimating transformations
between images without external tracking. However, image-only reconstruction
poses difficulties in reducing cumulative drift and further improving
reconstruction accuracy, particularly in scenarios involving complex motion
trajectories. In this context, we propose an enhanced motion network (MoNetV2)
to enhance the accuracy and generalizability of reconstruction under diverse
scanning velocities and tactics. First, we propose a sensor-based temporal and
multi-branch structure that fuses image and motion information from a velocity
perspective to improve image-only reconstruction accuracy. Second, we devise an
online multi-level consistency constraint that exploits the inherent
consistency of scans to handle various scanning velocities and tactics. This
constraint exploits both scan-level velocity consistency, path-level appearance
consistency, and patch-level motion consistency to supervise inter-frame
transformation estimation. Third, we distill an online multi-modal
self-supervised strategy that leverages the correlation between network
estimation and motion information to further reduce cumulative errors.
Extensive experiments clearly demonstrate that MoNetV2 surpasses existing
methods in both reconstruction quality and generalizability performance across
three large datasets.

</details>


### [124] [Cross-Modality Learning for Predicting IHC Biomarkers from H&E-Stained Whole-Slide Images](https://arxiv.org/abs/2506.15853)
*Amit Das,Naofumi Tomita,Kyle J. Syme,Weijie Ma,Paige O'Connor,Kristin N. Corbett,Bing Ren,Xiaoying Liu,Saeed Hassanpour*

Main category: eess.IV

TL;DR: HistoStainAlign是一种深度学习框架，直接从H&E全切片图像预测IHC染色模式，无需注释或组织配准，提高了诊断效率。


<details>
  <summary>Details</summary>
Motivation: IHC染色成本高、耗时长且资源密集，本研究旨在通过计算替代方案减少对IHC的依赖。

Method: 采用对比训练策略，整合H&E和IHC的嵌入表示，学习形态学和分子特征的联合表示。

Result: 在胃肠道和肺组织WSIs上，对P53、PD-L1和Ki-67三种IHC染色，加权F1分数分别为0.735、0.830和0.723。

Conclusion: HistoStainAlign可作为预筛查工具，优化IHC染色流程，提升工作效率。

Abstract: Hematoxylin and Eosin (H&E) staining is a cornerstone of pathological
analysis, offering reliable visualization of cellular morphology and tissue
architecture for cancer diagnosis, subtyping, and grading. Immunohistochemistry
(IHC) staining provides molecular insights by detecting specific proteins
within tissues, enhancing diagnostic accuracy, and improving treatment
planning. However, IHC staining is costly, time-consuming, and
resource-intensive, requiring specialized expertise. To address these
limitations, this study proposes HistoStainAlign, a novel deep learning
framework that predicts IHC staining patterns directly from H&E whole-slide
images (WSIs) by learning joint representations of morphological and molecular
features. The framework integrates paired H&E and IHC embeddings through a
contrastive training strategy, capturing complementary features across staining
modalities without patch-level annotations or tissue registration. The model
was evaluated on gastrointestinal and lung tissue WSIs with three commonly used
IHC stains: P53, PD-L1, and Ki-67. HistoStainAlign achieved weighted F1 scores
of 0.735 [95% Confidence Interval (CI): 0.670-0.799], 0.830 [95% CI:
0.772-0.886], and 0.723 [95% CI: 0.607-0.836], respectively for these three IHC
stains. Embedding analyses demonstrated the robustness of the contrastive
alignment in capturing meaningful cross-stain relationships. Comparisons with a
baseline model further highlight the advantage of incorporating contrastive
learning for improved stain pattern prediction. This study demonstrates the
potential of computational approaches to serve as a pre-screening tool, helping
prioritize cases for IHC staining and improving workflow efficiency.

</details>


### [125] [Fast Training-free Perceptual Image Compression](https://arxiv.org/abs/2506.16102)
*Ziran Zhu,Tongda Xu,Minye Huang,Dailan He,Xingtong Ge,Xinjie Zhang,Ling Li,Yan Wang*

Main category: eess.IV

TL;DR: 提出了一种无需训练的算法，显著提升现有图像编解码器的感知质量，并针对不同解码时间预算优化实现。


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的感知图像编解码器依赖耗时的方法（如扩散反转或样本通信），解码时间过长（1分钟或更长），限制了实际应用。

Method: 提出一种理论保证的算法，适用于不同解码时间预算（≈0.1秒、0.1-10秒、≥10秒），并支持非可微分编解码器（如VTM）。

Result: 将解码时间从1分钟缩短至0.1-10秒，感知质量相当；在FID指标上优于HiFiC和MS-ILLM等基于条件生成模型的编解码器。

Conclusion: 该方法高效、通用，适用于多种编解码器，显著提升感知质量和解码速度。

Abstract: Training-free perceptual image codec adopt pre-trained unconditional
generative model during decoding to avoid training new conditional generative
model. However, they heavily rely on diffusion inversion or sample
communication, which take 1 min to intractable amount of time to decode a
single image. In this paper, we propose a training-free algorithm that improves
the perceptual quality of any existing codec with theoretical guarantee. We
further propose different implementations for optimal perceptual quality when
decoding time budget is $\approx 0.1$s, $0.1-10$s and $\ge 10$s. Our approach:
1). improves the decoding time of training-free codec from 1 min to $0.1-10$s
with comparable perceptual quality. 2). can be applied to non-differentiable
codec such as VTM. 3). can be used to improve previous perceptual codecs, such
as MS-ILLM. 4). can easily achieve perception-distortion trade-off.
Empirically, we show that our approach successfully improves the perceptual
quality of ELIC, VTM and MS-ILLM with fast decoding. Our approach achieves
comparable FID to previous training-free codec with significantly less decoding
time. And our approach still outperforms previous conditional generative model
based codecs such as HiFiC and MS-ILLM in terms of FID. The source code is
provided in the supplementary material.

</details>


### [126] [Enhanced Dermatology Image Quality Assessment via Cross-Domain Training](https://arxiv.org/abs/2506.16116)
*Ignacio Hernández Montilla,Alfonso Medela,Paola Pasquali,Andy Aguilar,Taig Mac Carthy,Gerardo Fernández,Antonio Martorell,Enrique Onieva*

Main category: eess.IV

TL;DR: 论文提出了一种跨域训练的图像质量评估（IQA）方法，结合皮肤病学和非皮肤病学的IQA数据集，解决了皮肤病学IQA数据规模小的问题，提升了远程皮肤病诊疗中图像质量的管理。


<details>
  <summary>Details</summary>
Motivation: 远程皮肤病诊疗中图像质量差是一个未解决的问题，影响了远程咨询的效果。现有皮肤病学IQA研究较少，且未充分利用非皮肤病学IQA的最新进展。

Method: 通过跨域训练IQA模型，结合皮肤病学和非皮肤病学的数据集，并创建了一个新的皮肤病学IQA数据库Legit.Health-DIQA-Artificial。

Result: 跨域训练在多个领域表现出最优性能，克服了皮肤病学IQA数据规模小的限制，提升了图像质量管理的效果。

Conclusion: 跨域训练方法为远程皮肤病诊疗中的图像质量管理提供了有效解决方案，具有实际应用价值。

Abstract: Teledermatology has become a widely accepted communication method in daily
clinical practice, enabling remote care while showing strong agreement with
in-person visits. Poor image quality remains an unsolved problem in
teledermatology and is a major concern to practitioners, as bad-quality images
reduce the usefulness of the remote consultation process. However, research on
Image Quality Assessment (IQA) in dermatology is sparse, and does not leverage
the latest advances in non-dermatology IQA, such as using larger image
databases with ratings from large groups of human observers. In this work, we
propose cross-domain training of IQA models, combining dermatology and
non-dermatology IQA datasets. For this purpose, we created a novel dermatology
IQA database, Legit.Health-DIQA-Artificial, using dermatology images from
several sources and having them annotated by a group of human observers. We
demonstrate that cross-domain training yields optimal performance across
domains and overcomes one of the biggest limitations in dermatology IQA, which
is the small scale of data, and leads to models trained on a larger pool of
image distortions, resulting in a better management of image quality in the
teledermatology process.

</details>


### [127] [From Coarse to Continuous: Progressive Refinement Implicit Neural Representation for Motion-Robust Anisotropic MRI Reconstruction](https://arxiv.org/abs/2506.16210)
*Zhenxuan Zhang,Lipei Zhang,Yanqi Cheng,Zi Wang,Fanwen Wang,Haosen Zhang,Yue Yang,Yinzhe Wu,Jiahao Huang,Angelica I Aviles-Rivero,Zhifan Gao,Guang Yang,Peter J. Lally*

Main category: eess.IV

TL;DR: 提出了一种渐进细化隐式神经表示（PR-INR）框架，用于运动鲁棒MRI中的切片到体积重建，解决了局部细节丢失、全局结构混叠和体积各向异性问题。


<details>
  <summary>Details</summary>
Motivation: 在加速采集或患者运动情况下，切片到体积重建对恢复解剖一致的3D脑体积至关重要，但由于层次结构破坏（如k空间欠采样、运动引起的混叠和各向异性）而具有挑战性。

Method: PR-INR框架结合运动校正、结构细化和体积合成，包括运动感知扩散模块（生成粗略重建）、隐式细节恢复模块（校正局部结构）和体素连续感知表示模块（恢复高频细节）。

Result: 在五个公共MRI数据集上测试，PR-INR在定量重建指标和视觉质量上优于现有方法，并展示了跨未见领域的泛化性和鲁棒性。

Conclusion: PR-INR框架有效解决了MRI重建中的多尺度结构问题，具有广泛的应用潜力。

Abstract: In motion-robust magnetic resonance imaging (MRI), slice-to-volume
reconstruction is critical for recovering anatomically consistent 3D brain
volumes from 2D slices, especially under accelerated acquisitions or patient
motion. However, this task remains challenging due to hierarchical structural
disruptions. It includes local detail loss from k-space undersampling, global
structural aliasing caused by motion, and volumetric anisotropy. Therefore, we
propose a progressive refinement implicit neural representation (PR-INR)
framework. Our PR-INR unifies motion correction, structural refinement, and
volumetric synthesis within a geometry-aware coordinate space. Specifically, a
motion-aware diffusion module is first employed to generate coarse volumetric
reconstructions that suppress motion artifacts and preserve global anatomical
structures. Then, we introduce an implicit detail restoration module that
performs residual refinement by aligning spatial coordinates with visual
features. It corrects local structures and enhances boundary precision.
Further, a voxel continuous-aware representation module represents the image as
a continuous function over 3D coordinates. It enables accurate inter-slice
completion and high-frequency detail recovery. We evaluate PR-INR on five
public MRI datasets under various motion conditions (3% and 5% displacement),
undersampling rates (4x and 8x) and slice resolutions (scale = 5). Experimental
results demonstrate that PR-INR outperforms state-of-the-art methods in both
quantitative reconstruction metrics and visual quality. It further shows
generalization and robustness across diverse unseen domains.

</details>


### [128] [CF-Seg: Counterfactuals meet Segmentation](https://arxiv.org/abs/2506.16213)
*Raghav Mehta,Fabio De Sousa Ribeiro,Tian Xia,Melanie Roschewitz,Ainkaran Santhirasekaram,Dominic C. Marshall,Ben Glocker*

Main category: eess.IV

TL;DR: 通过生成反事实图像模拟无疾病状态下的解剖结构，提升医学图像分割准确性。


<details>
  <summary>Details</summary>
Motivation: 疾病会改变健康组织的表现，导致分割模型在真实数据上表现不佳，可能引发误诊。

Method: 生成反事实图像模拟无疾病状态，不改变底层结构，用于分割目标结构。

Result: 在两个真实临床胸部X光数据集上，反事实图像显著提升了分割效果。

Conclusion: 反事实图像有助于改善解剖结构分割，支持临床决策。

Abstract: Segmenting anatomical structures in medical images plays an important role in
the quantitative assessment of various diseases. However, accurate segmentation
becomes significantly more challenging in the presence of disease. Disease
patterns can alter the appearance of surrounding healthy tissues, introduce
ambiguous boundaries, or even obscure critical anatomical structures. As such,
segmentation models trained on real-world datasets may struggle to provide good
anatomical segmentation, leading to potential misdiagnosis. In this paper, we
generate counterfactual (CF) images to simulate how the same anatomy would
appear in the absence of disease without altering the underlying structure. We
then use these CF images to segment structures of interest, without requiring
any changes to the underlying segmentation model. Our experiments on two
real-world clinical chest X-ray datasets show that the use of counterfactual
images improves anatomical segmentation, thereby aiding downstream clinical
decision-making.

</details>


### [129] [AGE-US: automated gestational age estimation based on fetal ultrasound images](https://arxiv.org/abs/2506.16256)
*César Díaz-Parga,Marta Nuñez-Garcia,Maria J. Carreira,Gabriel Bernardino,Nicolás Vila-Blanco*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的自动计算孕龄方法，通过新颖的分割架构和距离图解决数据限制和标注稀缺问题，性能接近现有最佳模型，适用于资源有限环境。


<details>
  <summary>Details</summary>
Motivation: 准确估计孕龄对监测胎儿生长至关重要，但传统方法（如末次月经）在某些情况下难以获取，而超声方法依赖人工测量，存在变异性。

Method: 采用可解释的深度学习方法，结合新颖的分割架构和距离图，解决数据集限制和分割标注稀缺问题。

Result: 方法性能接近现有最佳模型，复杂度更低，尤其适用于资源有限环境，且距离图特别适合估计股骨端点。

Conclusion: 该方法为孕龄计算提供了高效、可靠的自动化解决方案，尤其适合资源有限和标注数据稀缺的场景。

Abstract: Being born small carries significant health risks, including increased
neonatal mortality and a higher likelihood of future cardiac diseases. Accurate
estimation of gestational age is critical for monitoring fetal growth, but
traditional methods, such as estimation based on the last menstrual period, are
in some situations difficult to obtain. While ultrasound-based approaches offer
greater reliability, they rely on manual measurements that introduce
variability. This study presents an interpretable deep learning-based method
for automated gestational age calculation, leveraging a novel segmentation
architecture and distance maps to overcome dataset limitations and the scarcity
of segmentation masks. Our approach achieves performance comparable to
state-of-the-art models while reducing complexity, making it particularly
suitable for resource-constrained settings and with limited annotated data.
Furthermore, our results demonstrate that the use of distance maps is
particularly suitable for estimating femur endpoints.

</details>


### [130] [VesselSDF: Distance Field Priors for Vascular Network Reconstruction](https://arxiv.org/abs/2506.16556)
*Salvatore Esposito,Daniel Rebain,Arno Onken,Changjian Li,Oisin Mac Aodha*

Main category: eess.IV

TL;DR: VesselSDF是一种基于符号距离场（SDF）的新方法，用于从稀疏CT扫描切片中准确分割血管网络，解决了现有深度学习方法在结构连续性和几何保真度上的不足。


<details>
  <summary>Details</summary>
Motivation: 稀疏CT扫描切片中血管网络的准确分割是一个挑战，尤其是由于血管的细长分支特性和切片间的稀疏性。现有基于二值体素分类的深度学习方法难以保持结构连续性和几何保真度。

Method: VesselSDF将血管分割问题重新定义为连续的SDF回归问题，通过自适应高斯正则化器消除SDF常见伪影，同时保持血管表面的精确几何形状。

Result: 实验结果表明，VesselSDF显著优于现有方法，能够更好地保持血管的几何形状和连通性。

Conclusion: VesselSDF为临床环境中的血管分析提供了更可靠的解决方案。

Abstract: Accurate segmentation of vascular networks from sparse CT scan slices remains
a significant challenge in medical imaging, particularly due to the thin,
branching nature of vessels and the inherent sparsity between imaging planes.
Existing deep learning approaches, based on binary voxel classification, often
struggle with structural continuity and geometric fidelity. To address this
challenge, we present VesselSDF, a novel framework that leverages signed
distance fields (SDFs) for robust vessel reconstruction. Our method
reformulates vessel segmentation as a continuous SDF regression problem, where
each point in the volume is represented by its signed distance to the nearest
vessel surface. This continuous representation inherently captures the smooth,
tubular geometry of blood vessels and their branching patterns. We obtain
accurate vessel reconstructions while eliminating common SDF artifacts such as
floating segments, thanks to our adaptive Gaussian regularizer which ensures
smoothness in regions far from vessel surfaces while producing precise geometry
near the surface boundaries. Our experimental results demonstrate that
VesselSDF significantly outperforms existing methods and preserves vessel
geometry and connectivity, enabling more reliable vascular analysis in clinical
settings.

</details>


### [131] [DiffO: Single-step Diffusion for Image Compression at Ultra-Low Bitrates](https://arxiv.org/abs/2506.16572)
*Chanung Park,Joo Chan Lee,Jong Hwan Ko*

Main category: eess.IV

TL;DR: DiffO是一种单步扩散模型，用于图像压缩，在极低比特率下提供高质量和快速解码。


<details>
  <summary>Details</summary>
Motivation: 现有图像压缩方法在极低比特率下质量下降严重，且扩散模型解码延迟高。

Method: 结合VQ残差训练和速率自适应噪声调制，分别捕捉全局几何与高频细节，并动态调整去噪强度。

Result: DiffO在压缩性能和解码速度上均优于现有方法，解码速度提升约50倍。

Conclusion: DiffO显著提升了生成编解码器的实用性，适用于极低比特率场景。

Abstract: Although image compression is fundamental to visual data processing and has
inspired numerous standard and learned codecs, these methods still suffer
severe quality degradation at extremely low bits per pixel. While recent
diffusion based models provided enhanced generative performance at low
bitrates, they still yields limited perceptual quality and prohibitive decoding
latency due to multiple denoising steps. In this paper, we propose the first
single step diffusion model for image compression (DiffO) that delivers high
perceptual quality and fast decoding at ultra low bitrates. DiffO achieves
these goals by coupling two key innovations: (i) VQ Residual training, which
factorizes a structural base code and a learned residual in latent space,
capturing both global geometry and high frequency details; and (ii) rate
adaptive noise modulation, which tunes denoising strength on the fly to match
the desired bitrate. Extensive experiments show that DiffO surpasses state of
the art compression performance while improving decoding speed by about 50x
compared to prior diffusion-based methods, greatly improving the practicality
of generative codecs. The code will be available at
https://github.com/Freemasti/DiffO.

</details>


### [132] [Hybrid Attention Network for Accurate Breast Tumor Segmentation in Ultrasound Images](https://arxiv.org/abs/2506.16592)
*Muhammad Azeem Aslam,Asim Naveed,Nisar Ahmed*

Main category: eess.IV

TL;DR: 提出了一种基于混合注意力的网络用于乳腺超声图像中的肿瘤分割，结合了DenseNet121编码器和多分支注意力增强解码器，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺超声图像中的肿瘤分割因噪声、病变尺度变化和模糊边界而具有挑战性，需要更精确的方法。

Method: 使用预训练的DenseNet121作为编码器，结合多分支注意力增强解码器，引入全局空间注意力、位置编码和缩放点积注意力，并通过空间特征增强块优化特征。采用混合损失函数（BCE和Jaccard Index）优化分割结果。

Result: 在公开数据集上的实验表明，该方法优于现有方法，能够更准确地分割肿瘤区域。

Conclusion: 该方法在乳腺超声图像分割中表现出色，有望辅助放射科医生实现早期和准确的乳腺癌诊断。

Abstract: Breast ultrasound imaging is a valuable tool for early breast cancer
detection, but automated tumor segmentation is challenging due to inherent
noise, variations in scale of lesions, and fuzzy boundaries. To address these
challenges, we propose a novel hybrid attention-based network for lesion
segmentation. Our proposed architecture integrates a pre-trained DenseNet121 in
the encoder part for robust feature extraction with a multi-branch
attention-enhanced decoder tailored for breast ultrasound images. The
bottleneck incorporates Global Spatial Attention (GSA), Position Encoding (PE),
and Scaled Dot-Product Attention (SDPA) to learn global context, spatial
relationships, and relative positional features. The Spatial Feature
Enhancement Block (SFEB) is embedded at skip connections to refine and enhance
spatial features, enabling the network to focus more effectively on tumor
regions. A hybrid loss function combining Binary Cross-Entropy (BCE) and
Jaccard Index loss optimizes both pixel-level accuracy and region-level overlap
metrics, enhancing robustness to class imbalance and irregular tumor shapes.
Experiments on public datasets demonstrate that our method outperforms existing
approaches, highlighting its potential to assist radiologists in early and
accurate breast cancer diagnosis.

</details>


### [133] [Overfitting in Histopathology Model Training: The Need for Customized Architectures](https://arxiv.org/abs/2506.16631)
*Saghir Alfasly,Ghazal Alabtah,H. R. Tizhoosh*

Main category: eess.IV

TL;DR: 研究表明，在组织病理学图像分析中直接采用自然图像分析的大规模模型会导致过拟合和性能不佳，需定制专用架构。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在组织病理学图像分析中的过拟合问题，避免直接采用自然图像分析模型的不足。

Method: 通过实验比较不同模型架构（如ResNet和ViT），验证模型容量增加不一定提升性能，并提出定制化架构。

Result: 在有限数据集上，简单的领域专用架构性能优于或等同于大型模型，且能减少过拟合。

Conclusion: 组织病理学图像分析需设计专用架构，而非依赖自然图像分析的通用模型。

Abstract: This study investigates the critical problem of overfitting in deep learning
models applied to histopathology image analysis. We show that simply adopting
and fine-tuning large-scale models designed for natural image analysis often
leads to suboptimal performance and significant overfitting when applied to
histopathology tasks. Through extensive experiments with various model
architectures, including ResNet variants and Vision Transformers (ViT), we show
that increasing model capacity does not necessarily improve performance on
histopathology datasets. Our findings emphasize the need for customized
architectures specifically designed for histopathology image analysis,
particularly when working with limited datasets. Using Oesophageal
Adenocarcinomas public dataset, we demonstrate that simpler, domain-specific
architectures can achieve comparable or better performance while minimizing
overfitting.

</details>


### [134] [A Prior-Guided Joint Diffusion Model in Projection Domain for PET Tracer Conversion](https://arxiv.org/abs/2506.16733)
*Fang Chen,Weifeng Zhang,Xingyu Ai,BingXuan Li,An Li,Qiegen Liu*

Main category: eess.IV

TL;DR: 提出了一种基于先验引导的联合扩散模型（PJDM），用于在投影域中将18F-FDG PET图像转换为18F-DOPA PET图像，以提高图像质量和合成效果。


<details>
  <summary>Details</summary>
Motivation: 18F-FDG PET在特定肿瘤中效果有限，而18F-DOPA具有更高特异性，但其合成复杂且临床应用受限。通过直接利用投影域数据，减少重建过程中的误差累积。

Method: 训练粗估计模型和先验细化模型，结合高阶混合采样器生成初始合成图像，并通过迭代细化过程优化结果。

Result: 实验表明PJDM显著提高了投影域图像质量和合成效果。

Conclusion: PJDM为18F-DOPA PET图像的生成提供了一种有效方法，代码已开源。

Abstract: Positron emission tomography (PET) is widely used to assess metabolic
activity, but its application is limited by the availability of radiotracers.
18F-labeled fluorodeoxyglucose (18F-FDG) is the most commonly used tracer but
shows limited effectiveness for certain tumors. In contrast,
6-18F-fluoro-3,4-dihydroxy-L-phenylalanine (18F-DOPA) offers higher specificity
for neuroendocrine tumors and neurological disorders. However, its complex
synthesis and limitations in transportation and clinical use hinder widespread
adoption. During PET imaging, the sinogram represents a form of raw data
acquired by the scanner. Therefore, modeling in projection domain enables more
direct utilization of the original information, potentially reducing the
accumulation of errors introduced during the image reconstruction process.
Inspired by these factors, this study proposes a prior-guided joint diffusion
model (PJDM) for transforming 18F-FDG PET images into 18F-DOPA PET images in
projection domain. Specifically, a coarse estimation model and a prior
refinement model are trained independently. During inference, an initial
synthetic 18F-DOPA PET sinogram is generated using a higher-order hybrid
sampler. This sinogram is then degraded and serves as an additional condition
to guide the iterative refinement process using learned prior. Experimental
results demonstrated that PJDM effectively improved both sinogram quality and
synthetic outcomes. The code is available at: https://github.com/yqx7150/PJDM.

</details>


### [135] [Temperature calibration of surface emissivities with an improved thermal image enhancement network](https://arxiv.org/abs/2506.16803)
*Ning Chu,Siya Zheng,Shanqing Zhang,Li Li,Caifang Cai,Ali Mohammad-Djafari,Feng Zhao,Yuanbo Song*

Main category: eess.IV

TL;DR: 本文提出了一种物理引导的神经网络框架，通过对称跳跃-CNN架构和发射率感知注意力模块，统一了温度校正和图像增强，解决了红外热成像中材料发射率变化导致的温度精度问题。


<details>
  <summary>Details</summary>
Motivation: 红外热成像中材料发射率的变化导致温度精度问题，现有方法常忽略辐射校准和图像退化的联合优化。

Method: 采用对称跳跃-CNN架构和发射率感知注意力模块，通过双约束损失函数（均值-方差对齐和基于KL散度的直方图匹配）优化温度校正和图像增强。

Result: 在工业鼓风机系统验证中，该方法实现了热辐射特征与空间背景的动态融合，并在多种工业条件下获得准确校准结果。

Conclusion: 该方法有效抑制了发射率伪影并恢复结构细节，为红外热成像的温度精度提供了可靠解决方案。

Abstract: Infrared thermography faces persistent challenges in temperature accuracy due
to material emissivity variations, where existing methods often neglect the
joint optimization of radiometric calibration and image degradation. This study
introduces a physically guided neural framework that unifies temperature
correction and image enhancement through a symmetric skip-CNN architecture and
an emissivity-aware attention module. The pre-processing stage segments the
ROIs of the image and and initially corrected the firing rate. A novel
dual-constrained loss function strengthens the statistical consistency between
the target and reference regions through mean-variance alignment and histogram
matching based on Kullback-Leibler dispersion. The method works by dynamically
fusing thermal radiation features and spatial context, and the model suppresses
emissivity artifacts while recovering structural details. After validating the
industrial blower system under different conditions, the improved network
realizes the dynamic fusion of thermal radiation characteristics and spatial
background, with accurate calibration results in various industrial conditions.

</details>


### [136] [PET Tracer Separation Using Conditional Diffusion Transformer with Multi-latent Space Learning](https://arxiv.org/abs/2506.16934)
*Bin Huang,Feihong Xu,Xinchong Shi,Shan Huang,Binxuan Li,Fei Li,Qiegen Liu*

Main category: eess.IV

TL;DR: 提出了一种多潜在空间引导的纹理条件扩散Transformer模型（MS-CDT），用于PET示踪剂分离，结合扩散和Transformer架构，通过纹理掩码和多潜在空间先验提升图像细节和分离效果。


<details>
  <summary>Details</summary>
Motivation: 多示踪剂PET成像能提供更全面的生理和病理信息，但由于不同示踪剂的光子对能量相同，信号难以区分。

Method: 提出MS-CDT模型，整合扩散和Transformer架构，利用纹理掩码作为条件输入和多潜在空间先验捕获多层次特征。

Result: 在脑部和胸部3D PET数据集上，MS-CDT在图像质量和临床信息保留方面表现优异。

Conclusion: MS-CDT首次结合纹理条件和多潜在空间，为PET示踪剂分离提供了高效且准确的解决方案。

Abstract: In clinical practice, single-radiotracer positron emission tomography (PET)
is commonly used for imaging. Although multi-tracer PET imaging can provide
supplementary information of radiotracers that are sensitive to physiological
function changes, enabling a more comprehensive characterization of
physiological and pathological states, the gamma-photon pairs generated by
positron annihilation reactions of different tracers in PET imaging have the
same energy, making it difficult to distinguish the tracer signals. In this
study, a multi-latent space guided texture conditional diffusion transformer
model (MS-CDT) is proposed for PET tracer separation. To the best of our
knowledge, this is the first attempt to use texture condition and multi-latent
space for tracer separation in PET imaging. The proposed model integrates
diffusion and transformer architectures into a unified optimization framework,
with the novel addition of texture masks as conditional inputs to enhance image
details. By leveraging multi-latent space prior derived from different tracers,
the model captures multi-level feature representations, aiming to balance
computational efficiency and detail preservation. The texture masks, serving as
conditional guidance, help the model focus on salient structural patterns,
thereby improving the extraction and utilization of fine-grained image
textures. When combined with the diffusion transformer backbone, this
conditioning mechanism contributes to more accurate and robust tracer
separation. To evaluate its effectiveness, the proposed MS-CDT is compared with
several advanced methods on two types of 3D PET datasets: brain and chest
scans. Experimental results indicate that MS-CDT achieved competitive
performance in terms of image quality and preservation of clinically relevant
information. Code is available at: https://github.com/yqx7150/MS-CDT.

</details>


### [137] [Robust Training with Data Augmentation for Medical Imaging Classification](https://arxiv.org/abs/2506.17133)
*Josué Martínez-Martínez,Olivia Brown,Mostafa Karami,Sheida Nabavi*

Main category: eess.IV

TL;DR: 提出了一种鲁棒训练算法（RTDA）用于增强医学图像分类模型对抗对抗性攻击和分布偏移的能力，并在多种成像技术中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在医学影像诊断中易受对抗性攻击和分布偏移影响，降低了诊断可靠性和信任度。

Method: 采用数据增强的鲁棒训练算法（RTDA），并与六种基线技术（包括对抗训练和数据增强）进行对比。

Result: RTDA在对抗性攻击和分布偏移下表现出更高的鲁棒性和泛化能力，同时保持高准确率。

Conclusion: RTDA是一种有效的解决方案，可提升医学图像分类模型的鲁棒性和可靠性。

Abstract: Deep neural networks are increasingly being used to detect and diagnose
medical conditions using medical imaging. Despite their utility, these models
are highly vulnerable to adversarial attacks and distribution shifts, which can
affect diagnostic reliability and undermine trust among healthcare
professionals. In this study, we propose a robust training algorithm with data
augmentation (RTDA) to mitigate these vulnerabilities in medical image
classification. We benchmark classifier robustness against adversarial
perturbations and natural variations of RTDA and six competing baseline
techniques, including adversarial training and data augmentation approaches in
isolation and combination, using experimental data sets with three different
imaging technologies (mammograms, X-rays, and ultrasound). We demonstrate that
RTDA achieves superior robustness against adversarial attacks and improved
generalization performance in the presence of distribution shift in each image
classification task while maintaining high clean accuracy.

</details>


### [138] [MeDi: Metadata-Guided Diffusion Models for Mitigating Biases in Tumor Classification](https://arxiv.org/abs/2506.17140)
*David Jacob Drexlin,Jonas Dippel,Julius Hense,Niklas Prenißl,Grégoire Montavon,Frederick Klauschen,Klaus-Robert Müller*

Main category: eess.IV

TL;DR: 提出了一种名为MeDi的元数据引导生成扩散模型框架，用于解决深度学习模型在组织学预测任务中因数据偏差导致的鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在组织学预测中表现优异，但缺乏对染色、扫描仪、医院和人口统计等变化的鲁棒性，导致对少数子群体的预测偏差。

Method: 通过MeDi框架，将元数据建模到生成扩散模型中，生成高质量的合成数据以平衡训练数据，减少下游模型的偏差。

Result: 实验表明，MeDi能生成高质量的组织病理学图像，提升生成图像的保真度，并改善下游分类器在子群体偏移数据集上的性能。

Conclusion: MeDi为利用生成模型减少数据偏差提供了概念验证。

Abstract: Deep learning models have made significant advances in histological
prediction tasks in recent years. However, for adaptation in clinical practice,
their lack of robustness to varying conditions such as staining, scanner,
hospital, and demographics is still a limiting factor: if trained on
overrepresented subpopulations, models regularly struggle with less frequent
patterns, leading to shortcut learning and biased predictions. Large-scale
foundation models have not fully eliminated this issue. Therefore, we propose a
novel approach explicitly modeling such metadata into a Metadata-guided
generative Diffusion model framework (MeDi). MeDi allows for a targeted
augmentation of underrepresented subpopulations with synthetic data, which
balances limited training data and mitigates biases in downstream models. We
experimentally show that MeDi generates high-quality histopathology images for
unseen subpopulations in TCGA, boosts the overall fidelity of the generated
images, and enables improvements in performance for downstream classifiers on
datasets with subpopulation shifts. Our work is a proof-of-concept towards
better mitigating data biases with generative models.

</details>


### [139] [Proportional Sensitivity in Generative Adversarial Network (GAN)-Augmented Brain Tumor Classification Using Convolutional Neural Network](https://arxiv.org/abs/2506.17165)
*Mahin Montasir Afif,Abdullah Al Noman,K. M. Tahsin Kabir,Md. Mortuza Ahmmed,Md. Mostafizur Rahman,Mufti Mahmud,Md. Ashraful Babu*

Main category: eess.IV

TL;DR: 研究探讨了GAN生成的脑肿瘤MRI图像与真实图像不同比例对CNN分类性能的影响，发现少量GAN数据能显著提升性能，但过多会降低模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像数据有限的问题，探索GAN生成数据对CNN分类性能的影响。

Method: 使用DCGAN生成合成图像，与真实图像按不同比例混合训练CNN，并在真实测试集上评估。

Result: 少量GAN数据（如100张）加入900张真实图像时，模型性能最佳（准确率95.2%），但GAN比例过高时性能下降。

Conclusion: GAN生成数据可有效扩充有限数据集，但需控制比例以避免引入过多人工痕迹影响泛化能力。

Abstract: Generative Adversarial Networks (GAN) have shown potential in expanding
limited medical imaging datasets. This study explores how different ratios of
GAN-generated and real brain tumor MRI images impact the performance of a CNN
in classifying healthy vs. tumorous scans. A DCGAN was used to create synthetic
images which were mixed with real ones at various ratios to train a custom CNN.
The CNN was then evaluated on a separate real-world test set. Our results
indicate that the model maintains high sensitivity and precision in tumor
classification, even when trained predominantly on synthetic data. When only a
small portion of GAN data was added, such as 900 real images and 100 GAN
images, the model achieved excellent performance, with test accuracy reaching
95.2%, and precision, recall, and F1-score all exceeding 95%. However, as the
proportion of GAN images increased further, performance gradually declined.
This study suggests that while GANs are useful for augmenting limited datasets
especially when real data is scarce, too much synthetic data can introduce
artifacts that affect the model's ability to generalize to real world cases.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [140] [FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space](https://arxiv.org/abs/2506.15742)
*Black Forest Labs,Stephen Batifol,Andreas Blattmann,Frederic Boesel,Saksham Consul,Cyril Diagne,Tim Dockhorn,Jack English,Zion English,Patrick Esser,Sumith Kulal,Kyle Lacey,Yam Levi,Cheng Li,Dominik Lorenz,Jonas Müller,Dustin Podell,Robin Rombach,Harry Saini,Axel Sauer,Luke Smith*

Main category: cs.GR

TL;DR: FLUX.1 Kontext是一个统一的生成流匹配模型，结合图像生成和编辑功能，通过文本和图像输入的语义上下文生成新视图。它在单次和多轮任务中表现优异，速度快且性能强。


<details>
  <summary>Details</summary>
Motivation: 当前编辑模型在多轮任务中存在字符一致性和稳定性下降的问题，FLUX.1 Kontext旨在解决这些问题，并提供更高效的统一架构。

Method: 采用简单的序列拼接方法，统一处理局部编辑和生成任务，结合语义上下文。

Result: 模型在单次和多轮任务中表现优异，保持对象和字符一致性，生成速度快，性能优于现有系统。

Conclusion: FLUX.1 Kontext在统一图像处理模型中设定了新标准，适用于交互式应用和快速原型设计。

Abstract: We present evaluation results for FLUX.1 Kontext, a generative flow matching
model that unifies image generation and editing. The model generates novel
output views by incorporating semantic context from text and image inputs.
Using a simple sequence concatenation approach, FLUX.1 Kontext handles both
local editing and generative in-context tasks within a single unified
architecture. Compared to current editing models that exhibit degradation in
character consistency and stability across multiple turns, we observe that
FLUX.1 Kontext improved preservation of objects and characters, leading to
greater robustness in iterative workflows.The model achieves competitive
performance with current state-of-the-art systems while delivering
significantly faster generation times, enabling interactive applications and
rapid prototyping workflows. To validate these improvements, we introduce
KontextBench, a comprehensive benchmark with 1026 image-prompt pairs covering
five task categories: local editing, global editing, character reference, style
reference and text editing. Detailed evaluations show the superior performance
of FLUX.1 Kontext in terms of both single-turn quality and multi-turn
consistency, setting new standards for unified image processing models.

</details>


### [141] [Graphics4Science: Computer Graphics for Scientific Impacts](https://arxiv.org/abs/2506.15786)
*Peter Yichen Chen,Minghao Guo,Hanspeter Pfister,Ming Lin,William Freeman,Qixing Huang,Han-Wei Shen,Wojciech Matusik*

Main category: cs.GR

TL;DR: 该课程探讨计算机图形学与科学的深层关系，展示图形学方法如何作为科学建模语言，并邀请图形学社区参与解决科学问题。


<details>
  <summary>Details</summary>
Motivation: 计算机图形学在科学领域有广泛应用潜力，但两领域间存在词汇鸿沟，需通过课程促进交流与合作。

Method: 课程通过几何推理和物理建模等核心方法，为科学问题提供归纳偏置，尤其在数据稀缺场景中。

Result: 课程旨在重新定义图形学为科学的建模语言，并推动图形学社区参与高影响力科学问题。

Conclusion: Graphics4Science课程为图形学与科学合作搭建桥梁，促进未来科学发现。

Abstract: Computer graphics, often associated with films, games, and visual effects,
has long been a powerful tool for addressing scientific challenges--from its
origins in 3D visualization for medical imaging to its role in modern
computational modeling and simulation. This course explores the deep and
evolving relationship between computer graphics and science, highlighting past
achievements, ongoing contributions, and open questions that remain. We show
how core methods, such as geometric reasoning and physical modeling, provide
inductive biases that help address challenges in both fields, especially in
data-scarce settings. To that end, we aim to reframe graphics as a modeling
language for science by bridging vocabulary gaps between the two communities.
Designed for both newcomers and experts, Graphics4Science invites the graphics
community to engage with science, tackle high-impact problems where graphics
expertise can make a difference, and contribute to the future of scientific
discovery. Additional details are available on the course website:
https://graphics4science.github.io

</details>


### [142] [GratNet: A Photorealistic Neural Shader for Diffractive Surfaces](https://arxiv.org/abs/2506.15815)
*Narayan Kandel,Daljit Singh J. S. Dhillon*

Main category: cs.GR

TL;DR: 提出了一种基于多层感知机（MLP）的数据驱动方法，用于高效准确地渲染衍射表面，显著减少数据存储需求。


<details>
  <summary>Details</summary>
Motivation: 当前结构着色模型依赖密集预处理数据，缺乏对隐式神经表示的全面研究，需要更高效的数据压缩和渲染方法。

Method: 采用MLP进行数据驱动的渲染，从数据压缩角度设计训练和建模方法，避免过拟合并具备稳健的重采样行为。

Result: 通过PSNR、SSIM和FLIP评估，高质量重建地面真实数据，内存占用减少两个数量级，性能显著提升。

Conclusion: 该方法在保持主观相似结果的同时，显著提高了渲染效率和数据压缩能力。

Abstract: Structural coloration is commonly modeled using wave optics for reliable and
photorealistic rendering of natural, quasi-periodic and complex nanostructures.
Such models often rely on dense, preliminary or preprocessed data to accurately
capture the nuanced variations in diffractive surface reflectances. This heavy
data dependency warrants implicit neural representation which has not been
addressed comprehensively in the current literature. In this paper, we present
a multi-layer perceptron (MLP) based method for data-driven rendering of
diffractive surfaces with high accuracy and efficiency. We primarily approach
this problem from a data compression perspective to devise a nuanced training
and modeling method which is attuned to the domain and range characteristics of
diffractive reflectance datasets. Importantly, our approach avoids over-fitting
and has robust resampling behavior. Using Peak-Signal-to-Noise (PSNR),
Structural Similarity Index Measure (SSIM) and a flipping difference evaluator
(FLIP) as evaluation metrics, we demonstrate the high-quality reconstruction of
the ground-truth. In comparison to a recent state-of-the-art offline,
wave-optical, forward modeling approach, our method reproduces subjectively
similar results with significant performance gains. We reduce the memory
footprint of the raw datasets by two orders of magnitude in general. Lastly, we
depict the working of our method with actual surface renderings.

</details>


### [143] [VEIGAR: View-consistent Explicit Inpainting and Geometry Alignment for 3D object Removal](https://arxiv.org/abs/2506.15821)
*Pham Khai Nguyen Do,Bao Nguyen Tran,Nam Nguyen,Duc Dung Nguyen*

Main category: cs.GR

TL;DR: VEIGAR是一种高效的新视角合成框架，无需初始3D重建阶段，通过轻量级模型和新型监督策略，显著提升重建质量和跨视角一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖初始3D重建阶段，计算成本高且效果有限，亟需一种更高效的解决方案。

Method: VEIGAR采用轻量级基础模型在像素空间对齐先验，并引入基于尺度不变深度损失的新型监督策略。

Result: VEIGAR在重建质量和跨视角一致性上达到新标杆，训练时间减少三倍。

Conclusion: VEIGAR在效率和效果上优于现有方法，为NVS任务提供了更优的解决方案。

Abstract: Recent advances in Novel View Synthesis (NVS) and 3D generation have
significantly improved editing tasks, with a primary emphasis on maintaining
cross-view consistency throughout the generative process. Contemporary methods
typically address this challenge using a dual-strategy framework: performing
consistent 2D inpainting across all views guided by embedded priors either
explicitly in pixel space or implicitly in latent space; and conducting 3D
reconstruction with additional consistency guidance. Previous strategies, in
particular, often require an initial 3D reconstruction phase to establish
geometric structure, introducing considerable computational overhead. Even with
the added cost, the resulting reconstruction quality often remains suboptimal.
In this paper, we present VEIGAR, a computationally efficient framework that
outperforms existing methods without relying on an initial reconstruction
phase. VEIGAR leverages a lightweight foundation model to reliably align priors
explicitly in the pixel space. In addition, we introduce a novel supervision
strategy based on scale-invariant depth loss, which removes the need for
traditional scale-and-shift operations in monocular depth regularization.
Through extensive experimentation, VEIGAR establishes a new state-of-the-art
benchmark in reconstruction quality and cross-view consistency, while achieving
a threefold reduction in training time compared to the fastest existing method,
highlighting its superior balance of efficiency and effectiveness.

</details>


### [144] [User-Guided Force-Directed Graph Layout](https://arxiv.org/abs/2506.15860)
*Hasan Balci,Augustin Luna*

Main category: cs.GR

TL;DR: 提出了一种基于用户手绘草图的力导向布局方法，通过直观控制生成符合用户预期的图形布局。


<details>
  <summary>Details</summary>
Motivation: 现有布局算法通常需要用户调整复杂参数，难以直观表达意图。

Method: 利用经典图像分析技术从手绘草图中提取结构信息，生成位置约束以指导布局过程。

Result: 在多种真实和合成图形上验证了该方法，能够生成符合用户期望的布局。

Conclusion: 该方法通过用户友好的草图控制，提升了布局的直观性和可解释性。

Abstract: Visual analysis of relational data is essential for many real-world analytics
tasks, with layout quality being key to interpretability. However, existing
layout algorithms often require users to navigate complex parameters to express
their intent. We present a user-guided force-directed layout approach that
enables intuitive control through freehand sketching. Our method uses classical
image analysis techniques to extract structural information from sketches,
which is then used to generate positional constraints that guide the layout
process. We evaluate the approach on various real and synthetic graphs ranging
from small to medium scale, demonstrating its ability to produce layouts
aligned with user expectations. An implementation of our method along with
documentation and a demo page is freely available on GitHub at
https://github.com/sciluna/uggly.

</details>


### [145] [FlatCAD: Fast Curvature Regularization of Neural SDFs for CAD Models](https://arxiv.org/abs/2506.16627)
*Haotian Yin,Aleksander Plocharski,Michal Jan Wlodarczyk,Mikolaj Kida,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 提出了一种新的曲率代理方法，用于神经符号距离场（SDF）的几何学习，避免了昂贵的全Hessian计算，提升了效率和内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有的高斯曲率惩罚方法需要全Hessian评估和二阶自动微分，计算成本高。

Method: 提出了两种曲率代理方法：有限差分代理和自动微分代理，仅正则化混合二阶项（Weingarten项）。

Result: 在ABC基准测试中，代理方法在重建保真度上与基于Hessian的基线相当或更好，同时减少了GPU内存使用和计算时间。

Conclusion: 该方法为工程级形状重建提供了一种高效、可扩展的曲率感知SDF学习路径。

Abstract: Neural signed-distance fields (SDFs) have become a versatile backbone for
geometric learning, yet enforcing developable, CAD-style behavior still hinges
on Gaussian curvature penalties that require full Hessian evaluation and
second-order automatic differentiation, both of which are costly in memory and
runtime. We present a curvature proxy that regularizes only the mixed
second-order term (Weingarten term), allowing the two principal curvatures to
adapt freely to data while suppressing unwanted warp. Two complementary
instantiations realize this idea: (i) a finite-difference proxy that replaces
each Hessian entry with four forward SDF evaluations and a single first-order
gradient, and (ii) an autodiff proxy that computes the same mixed derivative
via one Hessian-vector product, sidestepping explicit full Hessian assembly and
remaining faster in practice. Both variants converge to the exact mixed second
derivative, thus preserving the intended geometric bias without incurring full
second-order graphs. On the ABC benchmarks, the proxies match or exceed the
reconstruction fidelity of Hessian-based baselines while reducing GPU memory
use and wall-clock time by a factor of two. Because the method is drop-in and
framework-agnostic, it opens a practical path toward scalable, curvature-aware
SDF learning for engineering-grade shape reconstruction.

</details>


### [146] [Beyond Blur: A Fluid Perspective on Generative Diffusion Models](https://arxiv.org/abs/2506.16827)
*Grzegorz Gruszczynski,Michal Jan Wlodarczyk,Jakub J Meixner,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 提出了一种基于PDE的图像生成方法，结合了平流-扩散过程，通过物理驱动的PDE实现图像腐蚀，并利用神经网络逆向操作生成图像。


<details>
  <summary>Details</summary>
Motivation: 旨在通过物理驱动的PDE方法改进图像生成的质量和多样性，同时保持色彩一致性。

Method: 使用平流-扩散PDE和GPU加速的Lattice Boltzmann求解器，生成随机速度场以模拟湍流，神经网络学习逆向操作。

Result: 该方法能够生成更高质量的图像，且色彩不受影响，同时推广了现有的PDE方法。

Conclusion: 结合流体动力学和深度学习，为基于扩散的图像生成提供了新的物理视角。

Abstract: We propose a novel PDE-driven corruption process for generative image
synthesis based on advection-diffusion processes which generalizes existing
PDE-based approaches. Our forward pass formulates image corruption via a
physically motivated PDE that couples directional advection with isotropic
diffusion and Gaussian noise, controlled by dimensionless numbers (Peclet,
Fourier). We implement this PDE numerically through a GPU-accelerated custom
Lattice Boltzmann solver for fast evaluation. To induce realistic turbulence,
we generate stochastic velocity fields that introduce coherent motion and
capture multi-scale mixing. In the generative process, a neural network learns
to reverse the advection-diffusion operator thus constituting a novel
generative model. We discuss how previous methods emerge as specific cases of
our operator, demonstrating that our framework generalizes prior PDE-based
corruption techniques. We illustrate how advection improves the diversity and
quality of the generated images while keeping the overall color palette
unaffected. This work bridges fluid dynamics, dimensionless PDE theory, and
deep generative modeling, offering a fresh perspective on physically informed
image corruption processes for diffusion-based synthesis.

</details>


### [147] [DreamCube: 3D Panorama Generation via Multi-plane Synchronization](https://arxiv.org/abs/2506.17206)
*Yukun Huang,Yanning Zhou,Jianan Wang,Kaiyi Huang,Xihui Liu*

Main category: cs.GR

TL;DR: 论文提出了一种通过多平面同步技术将2D基础模型能力扩展到全景领域的方法，并开发了DreamCube模型，用于生成高质量3D全景内容。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因3D全景数据稀缺而依赖2D基础模型时，因3D全景与2D单视图不兼容导致效果受限的问题。

Method: 采用多平面同步技术扩展2D基础模型能力，并设计DreamCube（一种多平面RGB-D扩散模型）以生成3D全景内容。

Result: 实验表明，该方法在全景图像生成、全景深度估计和3D场景生成中表现优异。

Conclusion: 通过多平面同步和DreamCube模型，成功实现了高质量3D全景内容的生成，同时保持了多视图一致性。

Abstract: 3D panorama synthesis is a promising yet challenging task that demands
high-quality and diverse visual appearance and geometry of the generated
omnidirectional content. Existing methods leverage rich image priors from
pre-trained 2D foundation models to circumvent the scarcity of 3D panoramic
data, but the incompatibility between 3D panoramas and 2D single views limits
their effectiveness. In this work, we demonstrate that by applying multi-plane
synchronization to the operators from 2D foundation models, their capabilities
can be seamlessly extended to the omnidirectional domain. Based on this design,
we further introduce DreamCube, a multi-plane RGB-D diffusion model for 3D
panorama generation, which maximizes the reuse of 2D foundation model priors to
achieve diverse appearances and accurate geometry while maintaining multi-view
consistency. Extensive experiments demonstrate the effectiveness of our
approach in panoramic image generation, panoramic depth estimation, and 3D
scene generation.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [148] [Assessment of the Earth orientation parameter accuracy from concurrent VLBI observations](https://arxiv.org/abs/2506.15859)
*Leonid Petrov,Christian Ploetz,Matthias Schartner*

Main category: physics.geo-ph

TL;DR: 通过比较多个并发VLBI观测计划的地球定向参数（EOP）估计值，发现其均方根差异是可靠的精度指标。形式误差假设不相关噪声几乎无用，高级调度策略对EOP精度无显著影响。EOP误差随季节变化，冬季较小，夏季较大，且随观测时长增加呈分段幂律分布。


<details>
  <summary>Details</summary>
Motivation: 评估不同VLBI观测计划对EOP估计的准确性，并探讨影响精度的因素。

Method: 比较多个并发VLBI观测计划的EOP估计值，分析其均方根差异、形式误差、调度策略及季节变化的影响。

Result: EOP估计的均方根差异可靠；形式误差无用；高级调度策略无显著影响；误差随季节变化，冬季较小；误差随观测时长呈分段幂律分布。

Conclusion: EOP精度受大气噪声相关性和季节影响，调度策略非主要限制因素。

Abstract: We have assessed accuracy of estimates of Earth orientation parameters (EOP)
determined from several very long baseline interferometry (VLBI) observing
programs that ran concurrently at different networks. We consider that the root
mean square of differences in EOP estimates derived from concurrent
observations is a reliable measure of accuracy. We confirmed that formal errors
based on the assumption that the noise in observables is uncorrelated are close
to useless. We found no evidence that advanced scheduling strategies with
special considerations regarding the ability to better solve for atmospheric
path in zenith direction applied for 1-hr single-baseline sessions have any
measurable impact on the accuracy of EOP estimates. From this, we conclude that
there is a certain limit in our ability to solve for the atmospheric path delay
using microwave observations themselves and a scheduling strategy is not the
factor that impairs accuracy of EOP determination. We determined that EOP
errors vary with season, being smaller in winter and greater in summer. We
found that the EOP errors are scaled with an increase in duration of an
observing session as a broken power law with the power of -0.3 at durations
longer than 2-4 hours, which we explain as manifestation of the presence of
correlations in the atmospheric noise.

</details>


### [149] [Quantifying the influence of fault geometry via mesh morphing with applications to earthquake dynamic rupture and thermal models of subduction](https://arxiv.org/abs/2506.15892)
*Gabrielle M. Hobson,Dave A. May,Alice-Agnes Gabriel*

Main category: physics.geo-ph

TL;DR: 提出了一种网格变形方法，用于自动生成几何变化的网格，支持高效的不确定性量化与敏感性分析。


<details>
  <summary>Details</summary>
Motivation: 地下几何形状对地球物理过程有重要影响，但传统方法生成网格耗时且计算成本高。

Method: 采用网格变形技术，保持网格连接性，自动生成大量几何变化的网格。

Result: 在动态破裂和俯冲带热模型中，变形网格保持高质量且模拟结果准确；非侵入式降阶模型实现了高达10^9倍的加速。

Conclusion: 网格变形是一种强大且通用的工具，可用于计算地球物理学中的不确定性量化与敏感性分析。

Abstract: Subsurface geometries are often poorly constrained, yet they exert
first-order control on key geophysical processes, including subduction zone
thermal structure and earthquake rupture dynamics. Quantifying model
sensitivity to geometric variability remains challenging due to the manual
effort of mesh generation and the computational cost of exploring
high-dimensional parameter spaces in high-fidelity simulations. We present a
mesh morphing approach that deforms a reference mesh into geometrically varying
configurations while preserving mesh connectivity. This enables the automated
generation of large ensembles of geometrically variable meshes with minimal
user input. Importantly, the preserved connectivity allows for the application
of data-driven, non-intrusive reduced-order models (ROMs) to perform robust
sensitivity analysis and uncertainty quantification. We demonstrate mesh
morphing in two geophysical applications: (i) 3D dynamic rupture simulations
with fault dip angles varying across a 40{\deg} range, and (ii) 2D thermal
models of subduction zones incorporating realistic slab interface curvature and
depth uncertainties informed by the Slab2 geometry dataset. In both cases,
morphed meshes retain high quality and lead to accurate simulation results that
closely match those obtained using exactly generated meshes. For the dynamic
rupture case, we further construct ROMs that efficiently predict surface
displacement and velocity time series as functions of fault geometry, achieving
speedups of up to $10^9 \times$ relative to full simulations. Our results show
that mesh morphing can be a powerful and generalizable tool for incorporating
geometric uncertainty into physics-based modeling. The method supports
efficient ensemble modeling for rigorous sensitivity studies applicable across
a range of problems in computational geophysics.

</details>


### [150] [Impact of gas/liquid phase change of CO$_2$ during injection for sequestration](https://arxiv.org/abs/2506.15996)
*Mina Karimi,Elizabeth Cochran,Mehrdad Massoudi,Noel Walkington,Matteo Pozzi,Kaushik Dayal*

Main category: physics.geo-ph

TL;DR: 论文提出了一种基于多相热力学的孔隙力学模型，用于预测CO₂在注入过程中的相变行为及其对压力和应力分布的影响。


<details>
  <summary>Details</summary>
Motivation: CO₂封存是控制排放的重要手段，但相变行为对泄漏和封存预测至关重要，现有模型缺乏准确性。

Method: 采用多相热力学孔隙力学模型和有限元方法，分析CO₂在盐水相中的相变行为及其对密度和迁移能力的影响。

Result: 发现CO₂相变会显著影响密度空间分布并降低迁移能力，模型能预测相界面位置和饱和度变化。

Conclusion: 该模型无需假设相界面位置，为CO₂封存提供了更准确的预测工具。

Abstract: CO$_2$ sequestration in deep saline formations is an effective and important
process to control the rapid rise in CO$_2$ emissions. The process of injecting
CO$_2$ requires reliable predictions of the stress in the formation and the
fluid pressure distributions -- particularly since monitoring of the CO$_2$
migration is difficult -- to mitigate leakage, prevent induced seismicity, and
analyze wellbore stability. A key aspect of CO$_2$ is the gas-liquid phase
transition at the temperatures and pressures of relevance to leakage and
sequestration, which has been recognized as being critical for accurate
predictions but has been challenging to model without \textit{ad hoc}
empiricisms.
  This paper presents a robust multiphase thermodynamics-based poromechanics
model to capture the complex phase transition behavior of CO$_2$ and predict
the stress and pressure distribution under super- and sub- critical conditions
during the injection process. A finite element implementation of the model is
applied to analyze the behavior of a multiphase porous system with CO$_2$ as it
displaces the fluid brine phase. We find that if CO$_2$ undergoes a phase
transition in the geologic reservoir, the spatial variation of the density is
significantly affected, and the migration mobility of CO$_2$ decreases in the
reservoir. A key feature of our approach is that we do not \textit{a priori}
assume the location of the CO$_2$ gas/liquid interface -- or even if it occurs
at all -- but rather, this is a prediction of the model, along with the spatial
variation of the phase of CO$_2$ and the change of the saturation profile due
to the phase change.

</details>


### [151] [An equation of motion for unsteady frictional slip pulses](https://arxiv.org/abs/2506.16097)
*Eran Bouchbinder*

Main category: physics.geo-ph

TL;DR: 论文提出了一个描述摩擦滑动中脉冲状破裂的解析运动方程，揭示了其不稳定性的缓慢发展，并通过大规模模拟验证了其预测。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补对地壳地震中占主导地位的脉冲状破裂的基本理解空白。

Method: 基于近期进展，提出了一个单自由度的速率-状态摩擦脉冲解析运动方程，并通过大规模模拟验证。

Result: 方程预测得到支持，揭示了脉冲不稳定性的缓慢发展，解释了其在自然和人工摩擦系统中的动态相关性。

Conclusion: 研究为理解脉冲状破裂的动态行为提供了新视角，对地震和其他摩擦系统的研究具有重要意义。

Abstract: Frictional sliding, e.g., earthquakes along geological faults, are mediated
either by frictional crack-like ruptures, where interfacial (fault) slip is
accumulated during the entire sliding event, or by frictional pulse-like
ruptures, featuring a finite length over which slip is accumulated. Our basic
understanding of slip pulses, which are believed to dominate most crustal
earthquakes, is still incomplete. Here, building on recent progress, we present
an analytic equation of motion for rate-and-state frictional slip pulses, which
are intrinsically unstable spatiotemporal objects, in terms of a single degree
of freedom. The predictions of the equation are supported by large-scale
simulations of growing pulses and reveal the origin of the slow development of
their instability, which explains the dynamic relevance of pulses in a broad
range of natural and manmade frictional systems.

</details>


### [152] [A new 1D $V_p$ and $V_s$ velocity model of the western Rift of Corinth, Greece, using a fully non-linear tomography algorithm](https://arxiv.org/abs/2506.16222)
*Mark S. Noble,Alexandrine Gesret,Hélène Lyon-Caen,Anne Deschamps*

Main category: physics.geo-ph

TL;DR: 提出了一种新的1-D $V_p$和$V_s$速度模型，用于精确地震定位，显著降低了P波和S波的残差。


<details>
  <summary>Details</summary>
Motivation: 改进科林斯裂谷西部的地震定位精度。

Method: 结合两种非线性算法和网格搜索方法，探索速度模型空间并最小化P波和S波的到达时间残差。

Result: 新模型使P波和S波残差全局降低30%，$V_p$和$V_s$速度在浅层显著降低，$V_p/V_s$比值随深度变化。

Conclusion: 新模型显著改善了地震定位精度，尤其是震源深度的估计。

Abstract: The objective of this study is to propose a new updated accurate 1-D $V_p$
and $V_s$ velocity model of the western Rift of Corinth for precise absolute
earthquake locations. The methodology used to obtain this new model associates
two fully non-linear algorithms, a complete exploration of the $V_p$ and $V_s$
model space combined with a grid search method for earthquake locations to
minimize the P and S arrival time residuals. We calculated the misfit function
for approximately 2.10 6 velocity models. For the best model (minimum misfit)
we observed a global decrease of 30 % for both the P and S residuals with
respect to the commonly used velocity model. The main features of this new
model compared to other studies is a significant decrease of $V_p$ and $V_s$
velocities from surface down to a depth of approximately 3 km and a variable
$V_p/V_s$ ratio decreasing from 1.86 at the surface to 1.78 at a depth of 8 km.
The major influence on the locations of events is a global decrease of focal
depths that range from a few hundred metres for deep events to more than one
kilometre for shallow earthquakes.

</details>


### [153] [Frequency Differences between Clocks on the Earth and the Moon](https://arxiv.org/abs/2506.16377)
*Mingyue Zhang,Jürgen Müller,Sergei M. Kopeikin*

Main category: physics.geo-ph

TL;DR: 论文探讨了利用月球表面时钟作为全球时钟比较的参考，模拟了地球与月球时钟之间的频率差异，并分析了重力势能对观测的影响。


<details>
  <summary>Details</summary>
Motivation: 月球表面时钟因其低噪声、高轨道稳定性和广泛的地球可见性，有望成为全球时钟比较的参考，同时未来月球导航需求推动了独立月球时间系统的建立。

Method: 通过建模三种关键时间转换（地球时钟和月球时钟的本地时间到坐标时间，以及地球与月球之间的坐标时间关系），模拟了地球与月球时钟之间的频率差异。

Result: 重力势能差异对观测的影响在10^-10级别，坐标时间比率的影响在10^-11级别。静态、潮汐和非潮汐势能、天体自转及不同天体的贡献被评估。

Conclusion: 月球时钟可作为全球时钟比较的可靠参考，同时需考虑重力势能对时间转换的影响。

Abstract: Based on general relativity, clock comparisons enable the determination of
the gravity potential relative to a stable reference. Lunar surface clocks,
owing to the Moon's low-noise conditions, high orbital stability, and broad
Earth visibility, are promising reference clocks for global-scale comparisons
between terrestrial clocks. Meanwhile, the need for an independent lunar time
system-driven by future lunar navigation-requires maintaining links to
terrestrial standards. This Letter simulates fractional frequency differences
between Earth (E) and Moon (L) clocks by modeling three key time
transformations: proper-to-coordinate time for E-clocks and for L-clocks (both
linked to the local gravity potential), and the coordinate time relation
between Earth and Moon. Signal propagation effects are not addressed. Gravity
potential differences impact observations at the 10^-10 level, and the
coordinate time ratio at 10^-11. Contributions from static, tidal, and
non-tidal potentials, body self-rotation, and different celestial bodies are
evaluated.

</details>


### [154] [Unifying the Gutenberg-Richter Law with Probabilistic Catalog Completeness](https://arxiv.org/abs/2506.16849)
*Jiawei Li,Xinyi Wang,Didier Sornette*

Main category: physics.geo-ph

TL;DR: 论文提出了一种概率方法，通过四种增强的Gutenberg-Richter（GR）定律建模目录不完整性，并测试了其性能。GR-AEReLU模型表现最佳，提供了更稳健的地震参数估计。


<details>
  <summary>Details</summary>
Motivation: 地震目录的不完整性影响频率-震级分布（FMD）的准确性，需要一种统一且物理意义明确的建模方法。

Method: 提出了四种增强GR模型，引入mc和σc参数描述不完整性，并在合成和实际目录上测试性能。

Result: GR-AEReLU模型表现最优，能更准确地估计地震参数（如b值），并揭示了区域b值的系统性差异。

Conclusion: 该框架统一了不完整性建模，参数具有明确意义，挑战了地震学中b值普遍为1.0的假设，强调了区域特异性在风险评估中的重要性。

Abstract: We propose a probabilistic approach to modeling catalog incompleteness
through four candidate augmented Gutenberg-Richter (GR) laws, which
incorporates incompleteness into the frequency-magnitude distribution (FMD)
using two parameters, mc, the transition magnitude, and {\sigma}c, which
defines the transition range from incompleteness to completeness. The four GR
models are tested on synthetic and empirical catalogs, using multiple
performance evaluation metrics. The GR-AEReLU model, which allows for an
asymmetry in the convergence to the pure linear GR law for m > mc relative to
the censorship of earthquakes of sizes smaller than mc, is found to
consistently outperform, providing more robust estimates of seismological
parameters (e.g., b-value) that better reflect realistic physical conditions
and observational characteristics. This augmented framework offers three main
advantages: (1) unified modeling of incompleteness into the FMD, (2) parameters
with clear physical and statistical meaning, and (3) the ability to capture
nonlinear and asymmetric detection behaviors. Finally, our analysis reveals
systematic regional variations in earthquake b-values that deviate
significantly from the assumed universal value of 1.0, challenging a
fundamental paradigm in seismology and demonstrating the need for
region-specific values that reflect local tectonic conditions in seismic hazard
assessments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [155] [The Safety Reminder: A Soft Prompt to Reactivate Delayed Safety Awareness in Vision-Language Models](https://arxiv.org/abs/2506.15734)
*Peiyuan Tang,Haojie Xin,Xiaodong Zhang,Jun Sun,Qin Xia,Zijiang Yang*

Main category: cs.AI

TL;DR: 论文提出了一种名为“安全提醒”的软提示调优方法，通过优化可学习的提示标记来增强多模态视觉语言模型（VLM）的安全性，防止有害内容生成。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型（VLM）在代码生成和聊天机器人等实际应用中的能力增强，其安全性问题日益突出。由于多模态特性，VLM面临独特的漏洞，攻击者可能通过修改视觉或文本输入绕过安全防护，生成有害内容。

Method: 通过系统分析VLM在攻击下的行为，发现了一种称为“延迟安全感知”的现象。基于此，提出了“安全提醒”方法，通过优化可学习的提示标记，在文本生成过程中定期注入以增强安全感知。

Result: 在三个安全基准和一个对抗攻击测试中，该方法显著降低了攻击成功率，同时保持了模型的正常功能。

Conclusion: “安全提醒”方法为实际应用中部署更安全的VLM提供了一种实用解决方案。

Abstract: As Vision-Language Models (VLMs) demonstrate increasing capabilities across
real-world applications such as code generation and chatbot assistance,
ensuring their safety has become paramount. Unlike traditional Large Language
Models (LLMs), VLMs face unique vulnerabilities due to their multimodal nature,
allowing adversaries to modify visual or textual inputs to bypass safety
guardrails and trigger the generation of harmful content. Through systematic
analysis of VLM behavior under attack, we identify a novel phenomenon termed
``delayed safety awareness''. Specifically, we observe that safety-aligned VLMs
may initially be compromised to produce harmful content, but eventually
recognize the associated risks and attempt to self-correct. This pattern
suggests that VLMs retain their underlying safety awareness but experience a
temporal delay in their activation. Building on this insight, we hypothesize
that VLMs' safety awareness can be proactively reactivated through carefully
designed prompts. To this end, we introduce ``The Safety Reminder'', a soft
prompt tuning approach that optimizes learnable prompt tokens, which are
periodically injected during the text generation process to enhance safety
awareness, effectively preventing harmful content generation. Additionally, our
safety reminder only activates when harmful content is detected, leaving normal
conversations unaffected and preserving the model's performance on benign
tasks. Through comprehensive evaluation across three established safety
benchmarks and one adversarial attacks, we demonstrate that our approach
significantly reduces attack success rates while maintaining model utility,
offering a practical solution for deploying safer VLMs in real-world
applications.

</details>


### [156] [IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks](https://arxiv.org/abs/2506.16402)
*Xiaoya Lu,Zeren Chen,Xuhao Hu,Yijin Zhou,Weichen Zhang,Dongrui Liu,Lu Sheng,Jing Shao*

Main category: cs.AI

TL;DR: 论文提出IS-Bench，首个多模态交互安全基准，用于评估VLM驱动的具身代理在动态环境中的安全能力，发现当前代理缺乏交互安全意识。


<details>
  <summary>Details</summary>
Motivation: 现有静态评估范式无法模拟动态风险，导致具身代理在真实家庭任务中存在安全隐患。

Method: 提出IS-Bench基准，包含161个场景和388个安全风险，支持过程导向评估。

Result: 实验显示当前代理缺乏交互安全意识，安全感知思维链虽能提升性能，但常影响任务完成。

Conclusion: IS-Bench为开发更安全可靠的具身AI系统奠定了基础。

Abstract: Flawed planning from VLM-driven embodied agents poses significant safety
hazards, hindering their deployment in real-world household tasks. However,
existing static, non-interactive evaluation paradigms fail to adequately assess
risks within these interactive environments, since they cannot simulate dynamic
risks that emerge from an agent's actions and rely on unreliable post-hoc
evaluations that ignore unsafe intermediate steps. To bridge this critical gap,
we propose evaluating an agent's interactive safety: its ability to perceive
emergent risks and execute mitigation steps in the correct procedural order. We
thus present IS-Bench, the first multi-modal benchmark designed for interactive
safety, featuring 161 challenging scenarios with 388 unique safety risks
instantiated in a high-fidelity simulator. Crucially, it facilitates a novel
process-oriented evaluation that verifies whether risk mitigation actions are
performed before/after specific risk-prone steps. Extensive experiments on
leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current
agents lack interactive safety awareness, and that while safety-aware
Chain-of-Thought can improve performance, it often compromises task completion.
By highlighting these critical limitations, IS-Bench provides a foundation for
developing safer and more reliable embodied AI systems.

</details>


### [157] [AI's Blind Spots: Geographic Knowledge and Diversity Deficit in Generated Urban Scenario](https://arxiv.org/abs/2506.16898)
*Ciro Beneduce,Massimiliano Luca,Bruno Lepri*

Main category: cs.AI

TL;DR: 论文研究了图像生成模型在美国地理知识中的表现及其偏见，发现模型倾向于大城市而忽视农村地区和小城市。


<details>
  <summary>Details</summary>
Motivation: 探索图像生成模型在地理知识中的表现及其潜在的偏见，填补相关文献的空白。

Method: 使用FLUX 1和Stable Diffusion 3.5生成美国各州及首都的合成图像，通过DINO-v2 ViT-S/14和Fr\'echet Inception Distances测量图像相似性。

Result: 模型隐含学习了美国地理知识，但在生成“美国”图像时偏向大都市，且对欧洲风格名称存在歧义问题。

Conclusion: 图像生成模型存在地理偏见和实体歧义问题，需进一步优化以减少偏见。

Abstract: Image generation models are revolutionizing many domains, and urban analysis
and design is no exception. While such models are widely adopted, there is a
limited literature exploring their geographic knowledge, along with the biases
they embed. In this work, we generated 150 synthetic images for each state in
the USA and related capitals using FLUX 1 and Stable Diffusion 3.5, two
state-of-the-art models for image generation. We embed each image using DINO-v2
ViT-S/14 and the Fr\'echet Inception Distances to measure the similarity
between the generated images. We found that while these models have implicitly
learned aspects of USA geography, if we prompt the models to generate an image
for "United States" instead of specific cities or states, the models exhibit a
strong representative bias toward metropolis-like areas, excluding rural states
and smaller cities. {\color{black} In addition, we found that models
systematically exhibit some entity-disambiguation issues with European-sounding
names like Frankfort or Devon.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [158] [Smartphone-integrated RPA-CRISPR-Cas12a Detection System with Microneedle Sampling for Point-of-Care Diagnosis of Potato Late Blight in Early Stage](https://arxiv.org/abs/2506.15728)
*Jiangnan Zhao,Hanbo Xu,Cifu Xu,Wenlong Yin,Laixin Luo,Gang Liu,Yan Wang*

Main category: q-bio.QM

TL;DR: 便携式RPA-CRISPR诊断系统结合智能手机，用于马铃薯晚疫病的早期检测，具有高灵敏度和特异性。


<details>
  <summary>Details</summary>
Motivation: 传统植物病害检测方法（如PCR和LAMP）依赖昂贵且笨重的实验室设备，不适合田间即时诊断。

Method: 采用PVA微针贴片快速提取植物叶片样本，结合RPA-CRISPR-Cas12a等温扩增技术，通过智能手机分析荧光图像。

Result: 系统对P. infestans的检测限为2 pg/uL，接种后第3天和第4天的检测率分别达到80%和100%。

Conclusion: 该系统为田间植物病害的早期检测和控制提供了高效便携的解决方案。

Abstract: Potato late blight, caused by the oomycete pathogen Phytophthora infestans,
is one of the most devastating diseases affecting potato crops in the history.
Although conventional detection methods of plant diseases such as PCR and LAMP
are highly sensitive and specific, they rely on bulky and expensive laboratory
equipment and involve complex operations, making them impracticable for
point-of care diagnosis in the field. Here in this study, we report a portable
RPA-CRISPR based diagnosis system for plant disease, integrating smartphone for
acquisition and analysis of fluorescent images. A polyvinyl alcohol (PVA)
microneedle patch was employed for sample extraction on the plant leaves within
one minute, the DNA extraction efficiency achieved 56 ug/mg, which is
approximately 3 times to the traditional CTAB methods (18 ug/mg). The system of
RPA-CRISPR-Cas12a isothermal assay was established to specifically target P.
infestans with no cross-reactivity observed against closely-related species (P.
sojae, P. capsici). The system demonstrated a detection limit of 2 pg/uL for P.
infestans genomic DNA, offering sensitivity comparable to that of benchtop
laboratory equipment. The system demonstrates the early-stage diagnosis
capability by achieving a approximately 80% and 100% detection rate on the
third and fourth day post-inoculation respectively, before visible symptoms
observed on the leaves. The smartphone-based "sample-to-result" system
decouples the limitations of traditional methods that rely heavily on
specialized equipment, offering a promising way for early-stage plant disease
detection and control in the field.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [159] [TrajSceneLLM: A Multimodal Perspective on Semantic GPS Trajectory Analysis](https://arxiv.org/abs/2506.16401)
*Chunhou Ji,Qiumeng Li*

Main category: cs.CY

TL;DR: TrajSceneLLM是一种多模态框架，通过结合地图图像和LLM生成的文本描述，增强GPS轨迹的语义理解，显著提升了旅行模式识别的性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以提取GPS轨迹的深层语义表示和结合地图上下文信息，因此需要一种更有效的方法。

Method: 提出TrajSceneLLM框架，整合地图图像和LLM生成的文本描述，生成多模态嵌入，并通过简单MLP分类器进行任务验证。

Result: 实验表明，该方法在旅行模式识别任务中表现显著优于传统方法，减少了对手工特征的依赖。

Conclusion: TrajSceneLLM展示了在多模态语义增强和地理空间人工智能领域的潜力，代码和数据集已开源。

Abstract: GPS trajectory data reveals valuable patterns of human mobility and urban
dynamics, supporting a variety of spatial applications. However, traditional
methods often struggle to extract deep semantic representations and incorporate
contextual map information. We propose TrajSceneLLM, a multimodal perspective
for enhancing semantic understanding of GPS trajectories. The framework
integrates visualized map images (encoding spatial context) and textual
descriptions generated through LLM reasoning (capturing temporal sequences and
movement dynamics). Separate embeddings are generated for each modality and
then concatenated to produce trajectory scene embeddings with rich semantic
content which are further paired with a simple MLP classifier. We validate the
proposed framework on Travel Mode Identification (TMI), a critical task for
analyzing travel choices and understanding mobility behavior. Our experiments
show that these embeddings achieve significant performance improvement,
highlighting the advantage of our LLM-driven method in capturing deep
spatio-temporal dependencies and reducing reliance on handcrafted features.
This semantic enhancement promises significant potential for diverse downstream
applications and future research in geospatial artificial intelligence. The
source code and dataset are publicly available at:
https://github.com/februarysea/TrajSceneLLM.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [160] [DT-UFC: Universal Large Model Feature Coding via Peaky-to-Balanced Distribution Transformation](https://arxiv.org/abs/2506.16495)
*Changsheng Gao,Zijie Liu,Li Li,Dong Liu,Xiaoyan Sun,Weisi Lin*

Main category: cs.MM

TL;DR: 论文提出了一种通用特征编码方法，通过分布变换解决不同大模型特征分布不兼容的问题，显著提升了压缩效率和跨模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有特征编码方法多针对特定任务或模型，缺乏通用性，难以应对不同大模型特征分布的多样性问题。

Method: 提出了一种数据驱动的非均匀分布变换方法，将不同模型的特征分布对齐到一个平衡的目标空间，无需修改下游编解码器。

Result: 在LLaMA3、DINOv2和SD3等多个大模型和任务上验证，压缩效率和跨模型泛化能力显著优于任务特定基线。

Conclusion: 该方法为通用特征编码提供了有效解决方案，未来研究可进一步扩展其应用范围。

Abstract: Like image coding in visual data transmission, feature coding is essential
for the distributed deployment of large models by significantly reducing
transmission and storage overhead. However, prior studies have mostly targeted
task- or model-specific scenarios, leaving the challenge of universal feature
coding across diverse large models largely unaddressed. In this paper, we
present the first systematic study on universal feature coding for large
models. The key challenge lies in the inherently diverse and distributionally
incompatible nature of features extracted from different models. For example,
features from DINOv2 exhibit highly peaky, concentrated distributions, while
those from Stable Diffusion 3 (SD3) are more dispersed and uniform. This
distributional heterogeneity severely hampers both compression efficiency and
cross-model generalization. To address this, we propose a learned
peaky-to-balanced distribution transformation, which reshapes highly skewed
feature distributions into a common, balanced target space. This transformation
is non-uniform, data-driven, and plug-and-play, enabling effective alignment of
heterogeneous distributions without modifying downstream codecs. With this
alignment, a universal codec trained on the balanced target distribution can
effectively generalize to features from different models and tasks. We validate
our approach on three representative large models-LLaMA3, DINOv2, and
SD3-across multiple tasks and modalities. Extensive experiments show that our
method achieves notable improvements in both compression efficiency and
cross-model generalization over task-specific baselines. All source code will
be released for future research.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [161] [Information-computation trade-offs in non-linear transforms](https://arxiv.org/abs/2506.15948)
*Connor Ding,Abhiram Rao Gorle,Jiwon Jeong,Naomi Sagan,Tsachy Weissman*

Main category: cs.IT

TL;DR: 论文探讨了非线性变换在压缩中的信息与计算权衡，分析了INR和GS两种方法，提出了文本变换和LZ78变换，揭示了编码效率与计算成本之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究非线性变换在压缩任务中的表现，探索其在分类、去噪和生成AI等任务中的潜力。

Method: 分析了INR和GS的表示特性，提出了文本变换和LZ78变换，结合去噪任务验证效果。

Result: 揭示了INR和GS的优缺点，文本变换在超低比特率下表现优异，LZ78变换保留了算法的通用性。

Conclusion: 非线性变换在压缩和扩展任务中具有潜力，为平衡资源与性能提供了新思路。

Abstract: In this work, we explore the interplay between information and computation in
non-linear transform-based compression for broad classes of modern
information-processing tasks. We first investigate two emerging nonlinear data
transformation frameworks for image compression: Implicit Neural
Representations (INRs) and 2D Gaussian Splatting (GS). We analyze their
representational properties, behavior under lossy compression, and convergence
dynamics. Our results highlight key trade-offs between INR's compact,
resolution-flexible neural field representations and GS's highly
parallelizable, spatially interpretable fitting, providing insights for future
hybrid and compression-aware frameworks. Next, we introduce the textual
transform that enables efficient compression at ultra-low bitrate regimes and
simultaneously enhances human perceptual satisfaction. When combined with the
concept of denoising via lossy compression, the textual transform becomes a
powerful tool for denoising tasks. Finally, we present a Lempel-Ziv (LZ78)
"transform", a universal method that, when applied to any member of a broad
compressor family, produces new compressors that retain the asymptotic
universality guarantees of the LZ78 algorithm. Collectively, these three
transforms illuminate the fundamental trade-offs between coding efficiency and
computational cost. We discuss how these insights extend beyond compression to
tasks such as classification, denoising, and generative AI, suggesting new
pathways for using non-linear transformations to balance resource constraints
and performance.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [162] [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)
*Ho Yin 'Sam' Ng,Ting-Yao Hsu,Aashish Anantha Ramakrishnan,Branislav Kveton,Nedim Lipka,Franck Dernoncourt,Dongwon Lee,Tong Yu,Sungchul Kim,Ryan A. Rossi,Ting-Hao 'Kenneth' Huang*

Main category: cs.CL

TL;DR: LaMP-Cap是一个用于个性化图标题生成的多模态数据集，通过结合图像和文本信息，帮助生成更接近作者风格的标题。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成的图标题通常需要作者修改以匹配其风格和领域需求，凸显了个性化的重要性。然而，现有技术多关注纯文本场景，忽略了多模态输入和配置的需求。

Method: LaMP-Cap数据集提供了目标图的图像及同一文档中的其他图（包含图像、标题和提及段落）作为配置，以多模态方式表征上下文。实验使用了四种大型语言模型。

Result: 实验表明，使用配置信息能显著生成更接近作者原始标题的标题。消融研究发现，配置中的图像比提及段落更有帮助。

Conclusion: 多模态配置比纯文本配置更具优势，LaMP-Cap为个性化图标题生成提供了有效工具。

Abstract: Figure captions are crucial for helping readers understand and remember a
figure's key message. Many models have been developed to generate these
captions, helping authors compose better quality captions more easily. Yet,
authors almost always need to revise generic AI-generated captions to match
their writing style and the domain's style, highlighting the need for
personalization. Despite language models' personalization (LaMP) advances,
these technologies often focus on text-only settings and rarely address
scenarios where both inputs and profiles are multimodal. This paper introduces
LaMP-Cap, a dataset for personalized figure caption generation with multimodal
figure profiles. For each target figure, LaMP-Cap provides not only the needed
inputs, such as figure images, but also up to three other figures from the same
document--each with its image, caption, and figure-mentioning paragraphs--as a
profile to characterize the context. Experiments with four LLMs show that using
profile information consistently helps generate captions closer to the original
author-written ones. Ablation studies reveal that images in the profile are
more helpful than figure-mentioning paragraphs, highlighting the advantage of
using multimodal profiles over text-only ones.

</details>


### [163] [Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models](https://arxiv.org/abs/2506.16760)
*Lei Jiang,Zixun Zhang,Zizhou Wang,Xiaobing Sun,Zhen Li,Liangli Zhen,Xiaohua Xu*

Main category: cs.CL

TL;DR: CAMO是一种新型黑盒越狱攻击框架，通过将恶意提示分解为视觉和文本片段，利用LVLMs的跨模态推理能力绕过安全机制。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒越狱方法易被检测且效率低，CAMO旨在提供更隐蔽高效的攻击方式。

Method: CAMO将恶意提示分解为语义无害的视觉和文本片段，利用LVLMs的跨模态推理能力重构有害指令。

Result: CAMO在主流LVLMs上表现优异，具有强跨模型迁移性和高效性。

Conclusion: 当前安全机制存在显著漏洞，需开发更先进的视觉语言系统安全解决方案。

Abstract: Large Vision-Language Models (LVLMs) demonstrate exceptional performance
across multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass
built-in safety mechanisms to elicit restricted content generation. Existing
black-box jailbreak methods primarily rely on adversarial textual prompts or
image perturbations, yet these approaches are highly detectable by standard
content filtering systems and exhibit low query and computational efficiency.
In this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO),
a novel black-box jailbreak attack framework that decomposes malicious prompts
into semantically benign visual and textual fragments. By leveraging LVLMs'
cross-modal reasoning abilities, CAMO covertly reconstructs harmful
instructions through multi-step reasoning, evading conventional detection
mechanisms. Our approach supports adjustable reasoning complexity and requires
significantly fewer queries than prior attacks, enabling both stealth and
efficiency. Comprehensive evaluations conducted on leading LVLMs validate
CAMO's effectiveness, showcasing robust performance and strong cross-model
transferability. These results underscore significant vulnerabilities in
current built-in safety mechanisms, emphasizing an urgent need for advanced,
alignment-aware security and safety solutions in vision-language systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [164] [Global Context-aware Representation Learning for Spatially Resolved Transcriptomics](https://arxiv.org/abs/2506.15698)
*Yunhak Oh,Junseok Lee,Yeongmin Kim,Sangwoo Seo,Namkyeong Lee,Chanyoung Park*

Main category: cs.LG

TL;DR: Spotscape是一个新型框架，通过引入Similarity Telescope模块和相似性缩放策略，解决了现有图方法在空间转录组学中边界区域表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图方法在空间转录组学中难以有效处理边界区域的点，因为它们过于依赖相邻点而忽略了全局关系。

Method: 提出Spotscape框架，包含Similarity Telescope模块和相似性缩放策略，用于捕捉全局关系并整合多切片数据。

Result: 实验表明，Spotscape在单切片和多切片场景的下游任务中表现优越。

Conclusion: Spotscape通过改进全局关系捕捉和多切片整合，显著提升了空间转录组学分析的性能。

Abstract: Spatially Resolved Transcriptomics (SRT) is a cutting-edge technique that
captures the spatial context of cells within tissues, enabling the study of
complex biological networks. Recent graph-based methods leverage both gene
expression and spatial information to identify relevant spatial domains.
However, these approaches fall short in obtaining meaningful spot
representations, especially for spots near spatial domain boundaries, as they
heavily emphasize adjacent spots that have minimal feature differences from an
anchor node. To address this, we propose Spotscape, a novel framework that
introduces the Similarity Telescope module to capture global relationships
between multiple spots. Additionally, we propose a similarity scaling strategy
to regulate the distances between intra- and inter-slice spots, facilitating
effective multi-slice integration. Extensive experiments demonstrate the
superiority of Spotscape in various downstream tasks, including single-slice
and multi-slice scenarios. Our code is available at the following link: https:
//github.com/yunhak0/Spotscape.

</details>


### [165] [Shadow defense against gradient inversion attack in federated learning](https://arxiv.org/abs/2506.15711)
*Le Jiang,Liyan Ma,Guang Yang*

Main category: cs.LG

TL;DR: 该论文提出了一种针对联邦学习中梯度反转攻击的防御框架，通过影子模型识别敏感区域并针对性注入噪声，在保护隐私的同时最小化对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私保护分布式训练中具有潜力，但梯度反转攻击可能导致隐私泄露，现有防御方法缺乏针对性，影响模型准确性或保护不足。

Method: 利用具有可解释性的影子模型识别敏感区域，实现针对性噪声注入。

Result: 在ChestXRay和EyePACS数据集上，PSNR和SSIM指标显著优于无防御情况，模型性能损失小于1%。

Conclusion: 该框架在多种医学图像上验证了泛化能力，提供对多种梯度反转攻击的通用防御，尤其保护图像敏感区域。

Abstract: Federated learning (FL) has emerged as a transformative framework for
privacy-preserving distributed training, allowing clients to collaboratively
train a global model without sharing their local data. This is especially
crucial in sensitive fields like healthcare, where protecting patient data is
paramount. However, privacy leakage remains a critical challenge, as the
communication of model updates can be exploited by potential adversaries.
Gradient inversion attacks (GIAs), for instance, allow adversaries to
approximate the gradients used for training and reconstruct training images,
thus stealing patient privacy. Existing defense mechanisms obscure gradients,
yet lack a nuanced understanding of which gradients or types of image
information are most vulnerable to such attacks. These indiscriminate
calibrated perturbations result in either excessive privacy protection
degrading model accuracy, or insufficient one failing to safeguard sensitive
information. Therefore, we introduce a framework that addresses these
challenges by leveraging a shadow model with interpretability for identifying
sensitive areas. This enables a more targeted and sample-specific noise
injection. Specially, our defensive strategy achieves discrepancies of 3.73 in
PSNR and 0.2 in SSIM compared to the circumstance without defense on the
ChestXRay dataset, and 2.78 in PSNR and 0.166 in the EyePACS dataset. Moreover,
it minimizes adverse effects on model performance, with less than 1\% F1
reduction compared to SOTA methods. Our extensive experiments, conducted across
diverse types of medical images, validate the generalization of the proposed
framework. The stable defense improvements for FedAvg are consistently over
1.5\% times in LPIPS and SSIM. It also offers a universal defense against
various GIA types, especially for these sensitive areas in images.

</details>


### [166] [Tripartite Weight-Space Ensemble for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2506.15720)
*Juntae Lee,Munawar Hayat,Sungrack Yun*

Main category: cs.LG

TL;DR: 论文提出了一种新的少样本类增量学习（FSCIL）方法，通过权重空间三元集成（Tri-WE）和放大的数据知识蒸馏，解决了灾难性遗忘和过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 固定特征提取器限制了模型对新类的适应性，因此需要一种能够动态更新整个模型的方法。

Method: 提出Tri-WE方法，在权重空间中插值基础模型、前一个模型和当前模型，并通过放大的数据知识蒸馏正则化损失项。

Result: 在miniImageNet、CUB200和CIFAR100数据集上取得了最先进的结果。

Conclusion: Tri-WE和放大的数据知识蒸馏有效解决了FSCIL中的灾难性遗忘和过拟合问题。

Abstract: Few-shot class incremental learning (FSCIL) enables the continual learning of
new concepts with only a few training examples. In FSCIL, the model undergoes
substantial updates, making it prone to forgetting previous concepts and
overfitting to the limited new examples. Most recent trend is typically to
disentangle the learning of the representation from the classification head of
the model. A well-generalized feature extractor on the base classes (many
examples and many classes) is learned, and then fixed during incremental
learning. Arguing that the fixed feature extractor restricts the model's
adaptability to new classes, we introduce a novel FSCIL method to effectively
address catastrophic forgetting and overfitting issues. Our method enables to
seamlessly update the entire model with a few examples. We mainly propose a
tripartite weight-space ensemble (Tri-WE). Tri-WE interpolates the base,
immediately previous, and current models in weight-space, especially for the
classification heads of the models. Then, it collaboratively maintains
knowledge from the base and previous models. In addition, we recognize the
challenges of distilling generalized representations from the previous model
from scarce data. Hence, we suggest a regularization loss term using amplified
data knowledge distillation. Simply intermixing the few-shot data, we can
produce richer data enabling the distillation of critical knowledge from the
previous model. Consequently, we attain state-of-the-art results on the
miniImageNet, CUB200, and CIFAR100 datasets.

</details>


### [167] [Watermarking Autoregressive Image Generation](https://arxiv.org/abs/2506.16349)
*Nikola Jovanović,Ismail Labiad,Tomáš Souček,Martin Vechev,Pierre Fernandez*

Main category: cs.LG

TL;DR: 本文提出了一种在自回归图像生成模型的输出中添加水印的方法，解决了重新标记图像令牌时水印被擦除的问题，并通过实验证明了其可靠性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 生成模型的输出水印是追踪其来源的有效方法，但目前缺乏针对自回归图像生成模型的令牌级水印技术。

Method: 通过调整语言模型水印技术，引入自定义的标记器-解标记器微调程序以提高反向循环一致性（RCC），并添加水印同步层以增强鲁棒性。

Result: 实验表明，该方法能够可靠且鲁棒地检测水印，并提供理论支持的p值。

Conclusion: 本文首次实现了自回归图像生成模型的令牌级水印，解决了RCC问题，并通过技术改进提升了水印的鲁棒性。

Abstract: Watermarking the outputs of generative models has emerged as a promising
approach for tracking their provenance. Despite significant interest in
autoregressive image generation models and their potential for misuse, no prior
work has attempted to watermark their outputs at the token level. In this work,
we present the first such approach by adapting language model watermarking
techniques to this setting. We identify a key challenge: the lack of reverse
cycle-consistency (RCC), wherein re-tokenizing generated image tokens
significantly alters the token sequence, effectively erasing the watermark. To
address this and to make our method robust to common image transformations,
neural compression, and removal attacks, we introduce (i) a custom
tokenizer-detokenizer finetuning procedure that improves RCC, and (ii) a
complementary watermark synchronization layer. As our experiments demonstrate,
our approach enables reliable and robust watermark detection with theoretically
grounded p-values.

</details>


### [168] [Subspace-Boosted Model Merging](https://arxiv.org/abs/2506.16506)
*Ronald Skorobogat,Karsten Roth,Mariana-Iuliana Georgescu,Zeynep Akata*

Main category: cs.LG

TL;DR: 论文提出Subspace Boosting方法，通过维护任务向量空间的秩来解决模型合并中的秩崩溃问题，显著提升合并效果。


<details>
  <summary>Details</summary>
Motivation: 随着合并专家模型数量的增加，性能增益递减，现有方法存在任务向量空间秩崩溃的问题。

Method: 引入Subspace Boosting方法，基于奇异值分解维护任务向量秩，并利用高阶广义奇异值分解量化任务相似性。

Result: 在视觉基准测试中，Subspace Boosting将合并效果提升超过10%，支持多达20个专家模型。

Conclusion: Subspace Boosting有效解决了模型合并中的秩崩溃问题，为任务相似性提供了新的可解释视角。

Abstract: Model merging enables the combination of multiple specialized expert models
into a single model capable of performing multiple tasks. However, the benefits
of merging an increasing amount of specialized experts generally lead to
diminishing returns and reduced overall performance gains. In this work, we
offer an explanation and analysis from a task arithmetic perspective; revealing
that as the merging process (across numerous existing merging methods)
continues for more and more experts, the associated task vector space
experiences rank collapse. To mitigate this issue, we introduce Subspace
Boosting, which operates on the singular value decomposed task vector space and
maintains task vector ranks. Subspace Boosting raises merging efficacy for up
to 20 expert models by large margins of more than 10% when evaluated on vision
benchmarks. Moreover, we propose employing Higher-Order Generalized Singular
Value Decomposition to further quantify task similarity, offering a new
interpretable perspective on model merging.

</details>


### [169] [From Lab to Factory: Pitfalls and Guidelines for Self-/Unsupervised Defect Detection on Low-Quality Industrial Images](https://arxiv.org/abs/2506.16890)
*Sebastian Hönel,Jonas Nordqvist*

Main category: cs.LG

TL;DR: 论文探讨了工业产品表面缺陷的无监督检测方法，针对现有方法在低质量数据和实际场景中的不足，提出了改进框架。


<details>
  <summary>Details</summary>
Motivation: 传统手动检测成本高且易出错，机器学习可替代，但现有方法在低质量数据和实际场景中表现不佳，需改进。

Method: 评估两种先进模型，识别并改进生产数据质量问题，避免重新采集数据。

Result: 提供了实践指南，帮助识别模型或数据问题，并改进了基于似然的方法。

Conclusion: 提出了更适合实际场景的框架，解决了现有方法的不足。

Abstract: The detection and localization of quality-related problems in industrially
mass-produced products has historically relied on manual inspection, which is
costly and error-prone. Machine learning has the potential to replace manual
handling. As such, the desire is to facilitate an unsupervised (or
self-supervised) approach, as it is often impossible to specify all conceivable
defects ahead of time. A plethora of prior works have demonstrated the aptitude
of common reconstruction-, embedding-, and synthesis-based methods in
laboratory settings. However, in practice, we observe that most methods do not
handle low data quality well or exude low robustness in unfavorable, but
typical real-world settings. For practitioners it may be very difficult to
identify the actual underlying problem when such methods underperform. Worse,
often-reported metrics (e.g., AUROC) are rarely suitable in practice and may
give misleading results. In our setting, we attempt to identify subtle
anomalies on the surface of blasted forged metal parts, using rather
low-quality RGB imagery only, which is a common industrial setting. We
specifically evaluate two types of state-of-the-art models that allow us to
identify and improve quality issues in production data, without having to
obtain new data. Our contribution is to provide guardrails for practitioners
that allow them to identify problems related to, e.g., (lack of) robustness or
invariance, in either the chosen model or the data reliably in similar
scenarios. Furthermore, we exemplify common pitfalls in and shortcomings of
likelihood-based approaches and outline a framework for proper empirical risk
estimation that is more suitable for real-world scenarios.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [170] [PRISM-Loc: a Lightweight Long-range LiDAR Localization in Urban Environments with Topological Maps](https://arxiv.org/abs/2506.15849)
*Kirill Muravyev,Vasily Yuryev,Oleg Bulichev,Dmitry Yudin,Konstantin Yakovlev*

Main category: cs.RO

TL;DR: PRISM-Loc是一种基于拓扑地图的定位方法，用于大型环境中的实时定位，通过全局地点识别和局部姿态估计的两步流程，结合2D特征和点优化算法，在3公里路线上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在长距离导航中，使用密集全局激光雷达地图进行实时定位可能困难且内存消耗大，因此需要利用拓扑地图来解决这一问题。

Method: 提出PRISM-Loc方法，采用全局地点识别和局部姿态估计的两步流程，局部姿态估计使用基于2D特征和点优化的激光雷达扫描匹配算法。

Result: 在ITLP-Campus数据集的3公里路线上测试，PRISM-Loc在质量和计算效率上均优于现有方法。

Conclusion: PRISM-Loc是一种高效且高质量的定位方法，适用于大型环境中的实时导航。

Abstract: Localization in the environment is one of the crucial tasks of navigation of
a mobile robot or a self-driving vehicle. For long-range routes, performing
localization within a dense global lidar map in real time may be difficult, and
the creation of such a map may require much memory. To this end, leveraging
topological maps may be useful. In this work, we propose PRISM-Loc -- a
topological map-based approach for localization in large environments. The
proposed approach leverages a twofold localization pipeline, which consists of
global place recognition and estimation of the local pose inside the found
location. For local pose estimation, we introduce an original lidar scan
matching algorithm, which is based on 2D features and point-based optimization.
We evaluate the proposed method on the ITLP-Campus dataset on a 3 km route, and
compare it against the state-of-the-art metric map-based and place
recognition-based competitors. The results of the experiments show that the
proposed method outperforms its competitors both quality-wise and
computationally-wise.

</details>


### [171] [Semantic and Feature Guided Uncertainty Quantification of Visual Localization for Autonomous Vehicles](https://arxiv.org/abs/2506.15851)
*Qiyuan Wu,Mark Campbell*

Main category: cs.RO

TL;DR: 论文提出了一种用于自动驾驶视觉定位的轻量级传感器误差模型，通过学习图像特征和语义信息预测二维误差分布，从而量化不确定性。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶等安全关键应用中，传感器测量与深度学习网络的不确定性量化至关重要。

Method: 使用轻量级传感器误差模型，结合图像特征和语义信息预测误差分布，并通过高斯混合模型捕捉复杂场景下的不确定性。

Result: 在Ithaca365数据集上验证了方法的准确性，表明在恶劣天气和光照条件下，高斯混合模型比高斯分布更能预测测量误差。

Conclusion: 该方法能有效量化视觉定位中的不确定性，并适用于复杂场景。

Abstract: The uncertainty quantification of sensor measurements coupled with deep
learning networks is crucial for many robotics systems, especially for
safety-critical applications such as self-driving cars. This paper develops an
uncertainty quantification approach in the context of visual localization for
autonomous driving, where locations are selected based on images. Key to our
approach is to learn the measurement uncertainty using light-weight sensor
error model, which maps both image feature and semantic information to
2-dimensional error distribution. Our approach enables uncertainty estimation
conditioned on the specific context of the matched image pair, implicitly
capturing other critical, unannotated factors (e.g., city vs highway, dynamic
vs static scenes, winter vs summer) in a latent manner. We demonstrate the
accuracy of our uncertainty prediction framework using the Ithaca365 dataset,
which includes variations in lighting and weather (sunny, night, snowy). Both
the uncertainty quantification of the sensor+network is evaluated, along with
Bayesian localization filters using unique sensor gating method. Results show
that the measurement error does not follow a Gaussian distribution with poor
weather and lighting conditions, and is better predicted by our Gaussian
Mixture model.

</details>


### [172] [Noise Fusion-based Distillation Learning for Anomaly Detection in Complex Industrial Environments](https://arxiv.org/abs/2506.16050)
*Jiawen Yu,Jieji Ren,Yang Chang,Qiaojun Yu,Xuan Tong,Boyang Wang,Yan Song,You Li,Xinji Mai,Wenqiang Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于异构教师网络（HetNet）的新方法，用于复杂工业环境中的异常检测与定位，性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂工业环境中检测工件缺陷时表现不佳，需要更鲁棒的解决方案。

Method: 采用异构教师网络（HetNet）、自适应局部-全局特征融合模块和局部多元高斯噪声生成模块。

Result: 在主流基准测试中表现优异，MSC-AD指标提升约10%，并在其他数据集上达到SOTA。

Conclusion: HetNet能有效应对环境波动，提升工业异常检测系统的可靠性，适合实时应用。

Abstract: Anomaly detection and localization in automated industrial manufacturing can
significantly enhance production efficiency and product quality. Existing
methods are capable of detecting surface defects in pre-defined or controlled
imaging environments. However, accurately detecting workpiece defects in
complex and unstructured industrial environments with varying views, poses and
illumination remains challenging. We propose a novel anomaly detection and
localization method specifically designed to handle inputs with perturbative
patterns. Our approach introduces a new framework based on a collaborative
distillation heterogeneous teacher network (HetNet), an adaptive local-global
feature fusion module, and a local multivariate Gaussian noise generation
module. HetNet can learn to model the complex feature distribution of normal
patterns using limited information about local disruptive changes. We conducted
extensive experiments on mainstream benchmarks. HetNet demonstrates superior
performance with approximately 10% improvement across all evaluation metrics on
MSC-AD under industrial conditions, while achieving state-of-the-art results on
other datasets, validating its resilience to environmental fluctuations and its
capability to enhance the reliability of industrial anomaly detection systems
across diverse scenarios. Tests in real-world environments further confirm that
HetNet can be effectively integrated into production lines to achieve robust
and real-time anomaly detection. Codes, images and videos are published on the
project website at: https://zihuatanejoyu.github.io/HetNet/

</details>


### [173] [FlowRAM: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation](https://arxiv.org/abs/2506.16201)
*Sen Wang,Le Wang,Sanping Zhou,Jingyi Tian,Jiayi Li,Haowen Sun,Wei Tang*

Main category: cs.RO

TL;DR: FlowRAM是一种新型框架，利用生成模型实现区域感知，提升高精度机器人操作的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散的策略学习方法在推理时计算效率低，且未充分利用生成模型在3D环境中的信息探索潜力。

Method: 提出FlowRAM框架，采用动态半径调度实现自适应感知，结合状态空间模型和条件流匹配学习动作姿态。

Result: 在RLBench基准测试中表现优异，平均成功率提升12%，推理速度显著提高。

Conclusion: FlowRAM在高精度任务中表现出色，兼具高效性和性能优势。

Abstract: Robotic manipulation in high-precision tasks is essential for numerous
industrial and real-world applications where accuracy and speed are required.
Yet current diffusion-based policy learning methods generally suffer from low
computational efficiency due to the iterative denoising process during
inference. Moreover, these methods do not fully explore the potential of
generative models for enhancing information exploration in 3D environments. In
response, we propose FlowRAM, a novel framework that leverages generative
models to achieve region-aware perception, enabling efficient multimodal
information processing. Specifically, we devise a Dynamic Radius Schedule,
which allows adaptive perception, facilitating transitions from global scene
comprehension to fine-grained geometric details. Furthermore, we integrate
state space models to integrate multimodal information, while preserving linear
computational complexity. In addition, we employ conditional flow matching to
learn action poses by regressing deterministic vector fields, simplifying the
learning process while maintaining performance. We verify the effectiveness of
the FlowRAM in the RLBench, an established manipulation benchmark, and achieve
state-of-the-art performance. The results demonstrate that FlowRAM achieves a
remarkable improvement, particularly in high-precision tasks, where it
outperforms previous methods by 12.0% in average success rate. Additionally,
FlowRAM is able to generate physically plausible actions for a variety of
real-world tasks in less than 4 time steps, significantly increasing inference
speed.

</details>


### [174] [Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control](https://arxiv.org/abs/2506.16565)
*Yuxin Chen,Jianglan Wei,Chenfeng Xu,Boyi Li,Masayoshi Tomizuka,Andrea Bajcsy,Ran Tian*

Main category: cs.RO

TL;DR: 论文提出了一种名为ReOI的方法，通过检测并移除视觉干扰物，改进世界模型在开放世界中的预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型在面对训练中未见的视觉干扰物时表现脆弱，导致预测失效。

Method: ReOI通过检测干扰物、修改观测并重新预测未来结果，结合干扰物后处理。

Result: ReOI显著提升了任务成功率，在新型干扰物下表现优异。

Conclusion: ReOI是一种简单有效的测试时策略，适用于开放世界场景。

Abstract: World models enable robots to "imagine" future observations given current
observations and planned actions, and have been increasingly adopted as
generalized dynamics models to facilitate robot learning. Despite their
promise, these models remain brittle when encountering novel visual distractors
such as objects and background elements rarely seen during training.
Specifically, novel distractors can corrupt action outcome predictions, causing
downstream failures when robots rely on the world model imaginations for
planning or action verification. In this work, we propose Reimagination with
Observation Intervention (ReOI), a simple yet effective test-time strategy that
enables world models to predict more reliable action outcomes in open-world
scenarios where novel and unanticipated visual distractors are inevitable.
Given the current robot observation, ReOI first detects visual distractors by
identifying which elements of the scene degrade in physically implausible ways
during world model prediction. Then, it modifies the current observation to
remove these distractors and bring the observation closer to the training
distribution. Finally, ReOI "reimagines" future outcomes with the modified
observation and reintroduces the distractors post-hoc to preserve visual
consistency for downstream planning and verification. We validate our approach
on a suite of robotic manipulation tasks in the context of action verification,
where the verifier needs to select desired action plans based on predictions
from a world model. Our results show that ReOI is robust to both
in-distribution and out-of-distribution visual distractors. Notably, it
improves task success rates by up to 3x in the presence of novel distractors,
significantly outperforming action verification that relies on world model
predictions without imagination interventions.

</details>


### [175] [CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity](https://arxiv.org/abs/2506.16652)
*Guang Yin,Yitong Li,Yixuan Wang,Dale McConachie,Paarth Shah,Kunimatsu Hashimoto,Huan Zhang,Katherine Liu,Yunzhu Li*

Main category: cs.RO

TL;DR: 论文提出了一种新型机器人操作框架，通过视觉语言模型（VLM）解析自然语言指令中的模糊性，并生成可执行代码，结合感知模块生成3D注意力图，有效解决指令歧义问题。


<details>
  <summary>Details</summary>
Motivation: 自然语言指令在机器人操作任务中常存在模糊性，现有端到端模型因缺乏模块化和可解释性导致性能不佳。

Method: 采用视觉语言模型（VLM）解析指令并生成任务特定代码，结合感知模块生成3D注意力图以解决歧义。

Result: 实验表明该方法在语言模糊性、接触密集操作和多物体交互等任务中表现优异。

Conclusion: 该框架通过模块化和可解释性设计，显著提升了机器人处理模糊指令的能力。

Abstract: Natural language instructions for robotic manipulation tasks often exhibit
ambiguity and vagueness. For instance, the instruction "Hang a mug on the mug
tree" may involve multiple valid actions if there are several mugs and branches
to choose from. Existing language-conditioned policies typically rely on
end-to-end models that jointly handle high-level semantic understanding and
low-level action generation, which can result in suboptimal performance due to
their lack of modularity and interpretability. To address these challenges, we
introduce a novel robotic manipulation framework that can accomplish tasks
specified by potentially ambiguous natural language. This framework employs a
Vision-Language Model (VLM) to interpret abstract concepts in natural language
instructions and generates task-specific code - an interpretable and executable
intermediate representation. The generated code interfaces with the perception
module to produce 3D attention maps that highlight task-relevant regions by
integrating spatial and semantic information, effectively resolving ambiguities
in instructions. Through extensive experiments, we identify key limitations of
current imitation learning methods, such as poor adaptation to language and
environmental variations. We show that our approach excels across challenging
manipulation tasks involving language ambiguity, contact-rich manipulation, and
multi-object interactions.

</details>


### [176] [Monocular One-Shot Metric-Depth Alignment for RGB-Based Robot Grasping](https://arxiv.org/abs/2506.17110)
*Teng Guo,Baichuan Huang,Jingjin Yu*

Main category: cs.RO

TL;DR: 论文提出了一种名为MOMA的单目RGB图像度量深度恢复框架，通过一次性适配改进现有单目深度估计模型，解决了透明物体和低成本传感器的深度估计问题。


<details>
  <summary>Details</summary>
Motivation: 当前6D物体姿态估计依赖昂贵且噪声较大的深度传感器，而单目深度估计模型无法直接提供度量深度。MOMA旨在通过一次性适配实现准确的度量深度恢复。

Method: MOMA利用相机标定中的尺度-旋转-平移对齐，结合稀疏真实深度点指导，无需额外数据收集或模型重训练。

Result: 实验表明，MOMA在透明物体上表现良好，并在实际机器人抓取和分拣任务中取得高成功率。

Conclusion: MOMA通过一次性适配实现了单目RGB图像的准确度量深度估计，具有广泛的应用潜力。

Abstract: Accurate 6D object pose estimation is a prerequisite for successfully
completing robotic prehensile and non-prehensile manipulation tasks. At
present, 6D pose estimation for robotic manipulation generally relies on depth
sensors based on, e.g., structured light, time-of-flight, and stereo-vision,
which can be expensive, produce noisy output (as compared with RGB cameras),
and fail to handle transparent objects. On the other hand, state-of-the-art
monocular depth estimation models (MDEMs) provide only affine-invariant depths
up to an unknown scale and shift. Metric MDEMs achieve some successful
zero-shot results on public datasets, but fail to generalize. We propose a
novel framework, Monocular One-shot Metric-depth Alignment (MOMA), to recover
metric depth from a single RGB image, through a one-shot adaptation building on
MDEM techniques. MOMA performs scale-rotation-shift alignments during camera
calibration, guided by sparse ground-truth depth points, enabling accurate
depth estimation without additional data collection or model retraining on the
testing setup. MOMA supports fine-tuning the MDEM on transparent objects,
demonstrating strong generalization capabilities. Real-world experiments on
tabletop 2-finger grasping and suction-based bin-picking applications show MOMA
achieves high success rates in diverse tasks, confirming its effectiveness.

</details>


### [177] [Dex1B: Learning with 1B Demonstrations for Dexterous Manipulation](https://arxiv.org/abs/2506.17198)
*Jianglong Ye,Keyi Wang,Chengjing Yuan,Ruihan Yang,Yiquan Li,Jiyue Zhu,Yuzhe Qin,Xueyan Zou,Xiaolong Wang*

Main category: cs.RO

TL;DR: 论文介绍了Dex1B数据集，通过生成模型创建了大规模、多样化的灵巧手操作演示，并在仿真和真实实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 灵巧手操作的大规模演示生成具有挑战性，生成模型为解决这一问题提供了新思路。

Method: 提出了一种结合几何约束和多样性条件的生成模型，构建了包含10亿演示的Dex1B数据集。

Result: 在仿真和真实实验中，模型显著优于现有方法，表现出高效性和鲁棒性。

Conclusion: Dex1B数据集和生成模型为灵巧手操作研究提供了高质量资源，推动了该领域的发展。

Abstract: Generating large-scale demonstrations for dexterous hand manipulation remains
challenging, and several approaches have been proposed in recent years to
address this. Among them, generative models have emerged as a promising
paradigm, enabling the efficient creation of diverse and physically plausible
demonstrations. In this paper, we introduce Dex1B, a large-scale, diverse, and
high-quality demonstration dataset produced with generative models. The dataset
contains one billion demonstrations for two fundamental tasks: grasping and
articulation. To construct it, we propose a generative model that integrates
geometric constraints to improve feasibility and applies additional conditions
to enhance diversity. We validate the model on both established and newly
introduced simulation benchmarks, where it significantly outperforms prior
state-of-the-art methods. Furthermore, we demonstrate its effectiveness and
robustness through real-world robot experiments. Our project page is at
https://jianglongye.com/dex1b

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [178] [Volumetric Parameterization for 3-Dimensional Simply-Connected Manifolds](https://arxiv.org/abs/2506.17025)
*Zhiyuan Lyu,Qiguang Chen,Gary P. T. Choi,Lok Ming Lui*

Main category: cs.CG

TL;DR: 本文提出了一种新的体积参数化方法，用于3D单连通流形，能够平衡几何结构和密度失真。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以控制3D流形参数化的双射性和局部几何失真，且无法平衡不同属性。

Method: 提出了几种新方法，结合几何结构保持、密度均衡和失真平衡的模型。

Result: 方法在多种3D流形参数化和网格重划分应用中表现出高效性和准确性。

Conclusion: 新方法有效解决了传统方法的局限性，实现了多种理想属性的参数化。

Abstract: With advances in technology, there has been growing interest in developing
effective mapping methods for 3-dimensional objects in recent years. Volumetric
parameterization for 3D solid manifolds plays an important role in processing
3D data. However, the conventional approaches cannot control the bijectivity
and local geometric distortions of the result mappings due to the complex
structure of the solid manifolds. Moreover, prior methods mainly focus on one
property instead of balancing different properties during the mapping process.
In this paper, we propose several novel methods for computing volumetric
parameterizations for 3D simply-connected manifolds. Analogous to surface
parameterization, our framework incorporates several models designed to
preserve geometric structure, achieve density equalization, and optimally
balance geometric and density distortions. With these methods, various 3D
manifold parameterizations with different desired properties can be achieved.
These methods are tested on different examples and manifold remeshing
applications, demonstrating their effectiveness and accuracy.

</details>


### [179] [Wavelet-based Global Orientation and Surface Reconstruction for Point Clouds](https://arxiv.org/abs/2506.16299)
*Yueji Ma,Yanzun Meng,Dong Xiao,Zuoqiang Shi,Bin Wang*

Main category: cs.CG

TL;DR: 提出了一种基于小波的方法，用于处理无定向点云的重建问题，通过改进核函数和构建无散度函数场，显著提升了稀疏点云的重建效果和速度。


<details>
  <summary>Details</summary>
Motivation: 传统小波重建方法仅适用于定向点云，而现有改进方法对稀疏点云效果不佳，因此需要一种更高效且稳定的解决方案。

Method: 利用小波基函数的紧支撑性和正交性，通过改进核函数平滑表面不连续性，并构建无散度函数场作为额外约束。

Result: 实验表明，该方法在稀疏点云的定向和重建任务中达到最优性能，且在CPU上运行高效。

Conclusion: 该方法显著提升了无定向稀疏点云的重建效果和速度，具有实际应用价值。

Abstract: Unoriented surface reconstruction is an important task in computer graphics
and has extensive applications. Based on the compact support of wavelet and
orthogonality properties, classic wavelet surface reconstruction achieves good
and fast reconstruction. However, this method can only handle oriented points.
Despite some improved attempts for unoriented points, such as iWSR, these
methods perform poorly on sparse point clouds. To address these shortcomings,
we propose a wavelet-based method to represent the mollified indicator function
and complete both the orientation and surface reconstruction tasks. We use the
modifying kernel function to smoothen out discontinuities on the surface,
aligning with the continuity of the wavelet basis function. During the
calculation of coefficient, we fully utilize the properties of the
convolutional kernel function to shift the modifying computation onto wavelet
basis to accelerate. In addition, we propose a novel method for constructing
the divergence-free function field and using them to construct the additional
homogeneous constraints to improve the effectiveness and stability. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
in both orientation and reconstruction for sparse models. We align the matrix
construction with the compact support property of wavelet basis functions to
further accelerate our method, resulting in efficient performance on CPU. Our
source codes will be released on GitHub.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [180] [BLADE: An Automated Framework for Classifying Light Curves from the Center for Near-Earth Object Studies (CNEOS) Fireball Database](https://arxiv.org/abs/2506.16099)
*Elizabeth A. Silber,Vedant Sawal*

Main category: astro-ph.EP

TL;DR: BLADE是一个用于分析火球光变曲线的框架，通过滤波、峰值检测和梯度分析，系统识别和分类火球的分裂事件及其能量释放特征。


<details>
  <summary>Details</summary>
Motivation: 火球是陨石和小行星进入地球大气层时产生的高能发光现象，其光变曲线提供了关于陨石进入动力学、分裂行为和大气能量沉积的重要信息。

Method: BLADE框架结合Savitzky-Golay滤波、基于显著性的峰值检测和梯度分析，系统分析火球光变曲线。

Result: 初步结果表明，BLADE能可靠区分不同的火球行为，为大规模光变曲线数据集提供客观、可扩展的分析方法。

Conclusion: BLADE为火球研究开辟了新途径，在行星防御和全球大气监测中具有潜在应用价值。

Abstract: Fireballs (bolides) are high-energy luminous phenomena produced when
meteoroids and small asteroids enter Earth's atmosphere at hypersonic speeds,
often resulting in fragmentation or complete disintegration accompanied by
significant energy release. The resulting bolide light curves capture temporal
brightness variations as these objects traverse increasingly dense atmospheric
layers, providing essential information on meteoroid entry dynamics,
fragmentation behavior, and atmospheric energy deposition processes. The Center
for Near-Earth Object Studies' (CNEOS) continuously expanding fireball database
offers a globally comprehensive archive of bolide events, including light
curves and associated metadata. Events associated with infrasound detections
allow direct correlations between acoustic signatures and light-curve features,
therefore enabling detailed analyses of fragmentation dynamics and energy
deposition. Here, we introduce BLADE (Bolide Light-curve Analysis and
Discrimination Explorer), a robust and high-fidelity framework specifically
designed to analyze bolide light curves for objects detected from space. BLADE
incorporates a processing pipeline integrating Savitzky-Golay filtering,
prominence-based peak detection, and gradient analysis, enabling systematic
identification and classification of fragmentation events and their associated
energy release characteristics. Preliminary results demonstrate that BLADE
reliably distinguishes distinct bolide behaviors, providing an objective,
scalable methodology for characterization and analysis of large bolide light
curve datasets. This foundational work establishes a novel pathway for advanced
bolide research, with promising applications in planetary defense and global
atmospheric monitoring.

</details>


### [181] [Exoplanet Classification through Vision Transformers with Temporal Image Analysis](https://arxiv.org/abs/2506.16597)
*Anupma Choudhary,Sohith Bandari,B. S. Kushvah,C. Swastik*

Main category: astro-ph.EP

TL;DR: 该研究提出了一种利用Gramian Angular Fields和Recurrence Plots技术处理开普勒任务光变曲线数据，并结合Vision Transformer模型高效分类系外行星的方法。


<details>
  <summary>Details</summary>
Motivation: 传统系外行星分类方法耗费资源且效率低，需要机器学习技术提升分类效率。

Method: 将光变曲线数据转换为GAFs和RPs图像，输入Vision Transformer模型，并通过5折交叉验证评估性能。

Result: RPs表现优于GAFs，ViT模型召回率为89.46%，精确率为85.09%。

Conclusion: 研究展示了ViT模型在系外行星分类中的潜力，但数据集规模限制仍需优化模型架构。

Abstract: The classification of exoplanets has been a longstanding challenge in
astronomy, requiring significant computational and observational resources.
Traditional methods demand substantial effort, time, and cost, highlighting the
need for advanced machine learning techniques to enhance classification
efficiency. In this study, we propose a methodology that transforms raw light
curve data from NASA's Kepler mission into Gramian Angular Fields (GAFs) and
Recurrence Plots (RPs) using the Gramian Angular Difference Field and
recurrence plot techniques. These transformed images serve as inputs to the
Vision Transformer (ViT) model, leveraging its ability to capture intricate
temporal dependencies. We assess the performance of the model through recall,
precision, and F1 score metrics, using a 5-fold cross-validation approach to
obtain a robust estimate of the model's performance and reduce evaluation bias.
Our comparative analysis reveals that RPs outperform GAFs, with the ViT model
achieving an 89.46$\%$ recall and an 85.09$\%$ precision rate, demonstrating
its significant capability in accurately identifying exoplanetary transits.
Despite using under-sampling techniques to address class imbalance, dataset
size reduction remains a limitation. This study underscores the importance of
further research into optimizing model architectures to enhance automation,
performance, and generalization of the model.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [182] [Optimized cerebral blood flow measurement in speckle contrast optical spectroscopy via refinement of noise calibration](https://arxiv.org/abs/2506.15843)
*Ninghe Liu,Yu Xi Huang,Simon Mahler,Changhuei Yang*

Main category: eess.SP

TL;DR: 提出了一种基于优化的自适应噪声校准方法，显著提升了SCOS系统在低信号水平下的CBF测量准确性。


<details>
  <summary>Details</summary>
Motivation: SCOS在监测CBF时，噪声预校准误差会导致测量失真，尤其是在低信号水平下，这些误差会模仿CBV波形，影响CBF测量的可靠性。

Method: 采用优化框架对噪声校准进行自适应细化，通过降低CBF-CBV波形相关性来减少CBV模仿伪影。

Result: 在10名人类受试者中验证，该方法将可靠CBF信号的信号阈值从97降至26电子/像素，显著提升了测量精度。

Conclusion: 该方法提高了SCOS系统在大源-探测器距离下对深层组织的CBF测量准确性和鲁棒性。

Abstract: Speckle contrast optical spectroscopy (SCOS) offers a non-invasive and
cost-effective method for monitoring cerebral blood flow (CBF). However,
extracting accurate CBF from SCOS necessitates precise noise pre-calibration.
Errors from this can degrade CBF measurement fidelity, particularly when the
overall signal level is low. Such errors primarily stem from residual speckle
contrast associated with camera and shot noise, whose fluctuations exhibit a
temporal structure that mimics cerebral blood volume (CBV) waveforms. We
propose an optimization-based framework that performs an adaptive refinement of
noise calibration, mitigating the CBV-mimicking artifacts by reducing the
CBF-CBV waveform correlation. Validated on 10 human subjects, our approach
effectively lowered the signal threshold for reliable CBF signal from 97 to 26
electrons per pixel for a 1920x1200 pixels SCOS system. This improvement
enables more accurate and robust CBF measurements in SCOS, especially at large
source-detector (SD) distances for deeper tissue interrogation.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [183] [Toward Understanding Similarity of Visualization Techniques](https://arxiv.org/abs/2506.17032)
*Abdulhaq Adetunji Salako,Christian Tominski*

Main category: cs.HC

TL;DR: 该论文探讨了可视化技术的相似性问题，提出了模型驱动和专家驱动两种方法，并初步分析了13种基本和高级可视化技术的相似性。


<details>
  <summary>Details</summary>
Motivation: 理解可视化技术的相似性是一个开放的研究问题，现有分类方法难以全面捕捉技术间的相似性。

Method: 采用模型驱动方法（定义技术签名并比较其相似性）和专家驱动方法（通过在线研究收集专家对技术相似性的直观评估）。

Result: 初步分析了13种可视化技术的相似性，结果虽为初步但为未来研究提供了方向。

Conclusion: 研究为理解可视化技术相似性迈出了第一步，未来需进一步验证和扩展。

Abstract: The literature describes many visualization techniques for different types of
data, tasks, and application contexts, and new techniques are proposed on a
regular basis. Visualization surveys try to capture the immense space of
techniques and structure it with meaningful categorizations. Yet, it remains
difficult to understand the similarity of visualization techniques in general.
We approach this open research question from two angles. First, we follow a
model-driven approach that is based on defining the signature of visualization
techniques and interpreting the similarity of signatures as the similarity of
their associated techniques. Second, following an expert-driven approach, we
asked visualization experts in a small online study for their ad-hoc intuitive
assessment of the similarity of pairs visualization techniques. From both
approaches, we gain insight into the similarity of a set of 13 basic and
advanced visualizations for different types of data. While our results are so
far preliminary and academic, they are first steps toward better understanding
the similarity of visualization techniques.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [184] [Bias Variation Compensation in Perimeter-Gated SPAD TRNGs](https://arxiv.org/abs/2506.15888)
*Md Sakibur Sajal,Hunter Guthrie,Marc Dandin*

Main category: physics.ins-det

TL;DR: 论文提出了一种基于64x64阵列的pgSPADs的随机数生成器，通过BV补偿技术实现低偏差，并通过Von Neumann算法去偏，结果通过NIST测试。


<details>
  <summary>Details</summary>
Motivation: 解决随机数生成器中因熵源阵列导致的偏差变化问题，传统去偏算法无法适应宽范围BV。

Method: 使用64x64阵列的pgSPADs作为熵源，通过调节栅极电压补偿BV，生成低偏差的原始比特流，再采用Von Neumann算法去偏。

Result: 在室温下，原始比特生成率为2 kHz/像素，BV小于1%，去偏后比特通过NIST全部16项测试。

Conclusion: 提出的方法有效解决了BV问题，生成了高质量的随机比特流。

Abstract: Random number generators that utilize arrays of entropy source elements
suffer from bias variation (BV). Despite the availability of efficient
debiasing algorithms, optimized implementations of hardware friendly options
depend on the bit bias in the raw bit streams and cannot accommodate a wide BV.
In this work, we present a 64 x 64 array of perimeter gated single photon
avalanche diodes (pgSPADs), fabricated in a 0.35 {\mu}m standard CMOS
technology, as a source of entropy to generate random binary strings with a BV
compensation technique. By applying proper gate voltages based on the devices'
native dark count rates, we demonstrate less than 1% BV for a raw-bit
generation rate of 2 kHz/pixel at room temperature. The raw bits were debiased
using the classical iterative Von Neumann's algorithm and the debiased bits
were found to pass all of the 16 tests from NIST's Statistical Test Suite.

</details>
