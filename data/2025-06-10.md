<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 189]
- [eess.IV](#eess.IV) [Total: 11]
- [cs.GR](#cs.GR) [Total: 12]
- [physics.geo-ph](#physics.geo-ph) [Total: 5]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.CL](#cs.CL) [Total: 4]
- [cs.IR](#cs.IR) [Total: 1]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 11]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 17]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.CY](#cs.CY) [Total: 1]
- [eess.SP](#eess.SP) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Facial Foundational Model Advances Early Warning of Coronary Artery Disease from Live Videos with DigitalShadow](https://arxiv.org/abs/2506.06283)
*Juexiao Zhou,Zhongyi Han,Mankun Xin,Xingwei He,Guotao Wang,Jiaoyan Song,Gongning Luo,Wenjia He,Xintong Li,Yuetan Chu,Juanwen Chen,Bo Wang,Xia Wu,Wenwen Duan,Zhixia Guo,Liyan Bai,Yilin Pan,Xuefei Bi,Lu Liu,Long Feng,Xiaonan He,Xin Gao*

Main category: cs.CV

TL;DR: DigitalShadow是一种基于面部特征的无接触CAD早期预警系统，通过预训练和微调的面部模型实现被动风险评估。


<details>
  <summary>Details</summary>
Motivation: 全球老龄化加剧，CAD是主要死因之一，早期检测和管理至关重要。

Method: 系统预训练2100万张面部图像，微调为LiveCAD模型，使用7004张中国四家医院的面部图像，通过视频流被动提取特征。

Result: 生成自然语言风险报告和个性化健康建议，支持本地部署以保护隐私。

Conclusion: DigitalShadow为CAD的早期检测提供了一种高效、隐私安全的解决方案。

Abstract: Global population aging presents increasing challenges to healthcare systems,
with coronary artery disease (CAD) responsible for approximately 17.8 million
deaths annually, making it a leading cause of global mortality. As CAD is
largely preventable, early detection and proactive management are essential. In
this work, we introduce DigitalShadow, an advanced early warning system for
CAD, powered by a fine-tuned facial foundation model. The system is pre-trained
on 21 million facial images and subsequently fine-tuned into LiveCAD, a
specialized CAD risk assessment model trained on 7,004 facial images from 1,751
subjects across four hospitals in China. DigitalShadow functions passively and
contactlessly, extracting facial features from live video streams without
requiring active user engagement. Integrated with a personalized database, it
generates natural language risk reports and individualized health
recommendations. With privacy as a core design principle, DigitalShadow
supports local deployment to ensure secure handling of user data.

</details>


### [2] [Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images](https://arxiv.org/abs/2506.06389)
*Rifat Sadik,Tanvir Rahman,Arpan Bhattacharjee,Bikash Chandra Halder,Ismail Hossain*

Main category: cs.CV

TL;DR: 论文研究了基于Transformer的视觉模型（ViTs）在医学图像中对对抗性水印攻击的脆弱性，并通过对抗训练提高了其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着ViTs在计算机视觉任务中的成功应用，研究其在医学图像（如皮肤病图像）中对对抗性攻击的脆弱性具有重要意义。

Method: 使用投影梯度下降（PGD）生成对抗性水印，测试ViTs和CNNs的脆弱性，并评估对抗训练的效果。

Result: ViTs对对抗性攻击非常脆弱（准确率降至27.6%），但对抗训练可显著提升其鲁棒性（准确率达90.0%）。

Conclusion: ViTs在医学图像中易受对抗性攻击，但对抗训练是一种有效的防御手段。

Abstract: Deep learning models have shown remarkable success in dermatological image
analysis, offering potential for automated skin disease diagnosis. Previously,
convolutional neural network(CNN) based architectures have achieved immense
popularity and success in computer vision (CV) based task like skin image
recognition, generation and video analysis. But with the emergence of
transformer based models, CV tasks are now are nowadays carrying out using
these models. Vision Transformers (ViTs) is such a transformer-based models
that have shown success in computer vision. It uses self-attention mechanisms
to achieve state-of-the-art performance across various tasks. However, their
reliance on global attention mechanisms makes them susceptible to adversarial
perturbations. This paper aims to investigate the susceptibility of ViTs for
medical images to adversarial watermarking-a method that adds so-called
imperceptible perturbations in order to fool models. By generating adversarial
watermarks through Projected Gradient Descent (PGD), we examine the
transferability of such attacks to CNNs and analyze the performance defense
mechanism -- adversarial training. Results indicate that while performance is
not compromised for clean images, ViTs certainly become much more vulnerable to
adversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless,
adversarial training raises it up to 90.0%.

</details>


### [3] [(LiFT) Lightweight Fitness Transformer: A language-vision model for Remote Monitoring of Physical Training](https://arxiv.org/abs/2506.06480)
*A. Postlmayr,P. Cosman,S. Dey*

Main category: cs.CV

TL;DR: 提出了一种基于RGB智能手机摄像头的远程健身追踪系统，具有隐私性、可扩展性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 现有健身追踪模型要么运动种类有限，要么过于复杂难以部署，需要一种更通用且高效的方法。

Method: 开发了一个多任务运动分析模型，结合大规模数据集Olympia（覆盖1900多种运动），利用视觉-语言模型进行运动检测和重复计数。

Result: 模型在Olympia数据集上运动检测准确率为76.5%，重复计数准确率为85.3%。

Conclusion: 通过单一视觉-语言模型实现运动识别和计数，推动了AI健身追踪的普及。

Abstract: We introduce a fitness tracking system that enables remote monitoring for
exercises using only a RGB smartphone camera, making fitness tracking more
private, scalable, and cost effective. Although prior work explored automated
exercise supervision, existing models are either too limited in exercise
variety or too complex for real-world deployment. Prior approaches typically
focus on a small set of exercises and fail to generalize across diverse
movements. In contrast, we develop a robust, multitask motion analysis model
capable of performing exercise detection and repetition counting across
hundreds of exercises, a scale far beyond previous methods. We overcome
previous data limitations by assembling a large-scale fitness dataset, Olympia
covering more than 1,900 exercises. To our knowledge, our vision-language model
is the first that can perform multiple tasks on skeletal fitness data. On
Olympia, our model can detect exercises with 76.5% accuracy and count
repetitions with 85.3% off-by-one accuracy, using only RGB video. By presenting
a single vision-language transformer model for both exercise identification and
rep counting, we take a significant step toward democratizing AI-powered
fitness tracking.

</details>


### [4] [GS4: Generalizable Sparse Splatting Semantic SLAM](https://arxiv.org/abs/2506.06517)
*Mingqi Jiang,Chanho Kim,Chen Ziwen,Li Fuxin*

Main category: cs.CV

TL;DR: 提出了一种基于高斯泼溅（GS）的可泛化语义SLAM算法，通过学习网络从RGB-D视频流增量构建3D场景表示，解决了现有GS方法依赖场景优化且泛化性差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM算法生成的地图分辨率低且不完整，而现有GS方法依赖场景优化且泛化性差，需要一种高效且通用的解决方案。

Method: 使用RGB-D图像识别骨干网络预测高斯参数，集成3D语义分割，并通过全局定位后仅优化1次高斯泼溅来修正定位漂移。

Result: 在ScanNet基准测试中表现最优，高斯数量比其他GS方法少一个数量级，并在NYUv2和TUM RGB-D数据集上展示了零样本泛化能力。

Conclusion: 该方法在语义SLAM中实现了高效、高精度的3D地图构建，并具备良好的泛化能力。

Abstract: Traditional SLAM algorithms are excellent at camera tracking but might
generate lower resolution and incomplete 3D maps. Recently, Gaussian Splatting
(GS) approaches have emerged as an option for SLAM with accurate, dense 3D map
building. However, existing GS-based SLAM methods rely on per-scene
optimization which is time-consuming and does not generalize to diverse scenes
well. In this work, we introduce the first generalizable GS-based semantic SLAM
algorithm that incrementally builds and updates a 3D scene representation from
an RGB-D video stream using a learned generalizable network. Our approach
starts from an RGB-D image recognition backbone to predict the Gaussian
parameters from every downsampled and backprojected image location.
Additionally, we seamlessly integrate 3D semantic segmentation into our GS
framework, bridging 3D mapping and recognition through a shared backbone. To
correct localization drifting and floaters, we propose to optimize the GS for
only 1 iteration following global localization. We demonstrate state-of-the-art
semantic SLAM performance on the real-world benchmark ScanNet with an order of
magnitude fewer Gaussians compared to other recent GS-based methods, and
showcase our model's generalization capability through zero-shot transfer to
the NYUv2 and TUM RGB-D datasets.

</details>


### [5] [Bridging Audio and Vision: Zero-Shot Audiovisual Segmentation by Connecting Pretrained Models](https://arxiv.org/abs/2506.06537)
*Seung-jae Lee,Paul Hongsuck Seo*

Main category: cs.CV

TL;DR: 提出了一种零样本视听分割框架，通过整合预训练模型实现无需任务特定训练的高效分割。


<details>
  <summary>Details</summary>
Motivation: 传统视听分割方法依赖大规模像素级标注，成本高且耗时。

Method: 整合音频、视觉和文本表示，探索预训练模型连接策略。

Result: 在多个数据集上实现了最先进的零样本视听分割性能。

Conclusion: 多模态模型整合在细粒度视听分割中具有显著效果。

Abstract: Audiovisual segmentation (AVS) aims to identify visual regions corresponding
to sound sources, playing a vital role in video understanding, surveillance,
and human-computer interaction. Traditional AVS methods depend on large-scale
pixel-level annotations, which are costly and time-consuming to obtain. To
address this, we propose a novel zero-shot AVS framework that eliminates
task-specific training by leveraging multiple pretrained models. Our approach
integrates audio, vision, and text representations to bridge modality gaps,
enabling precise sound source segmentation without AVS-specific annotations. We
systematically explore different strategies for connecting pretrained models
and evaluate their efficacy across multiple datasets. Experimental results
demonstrate that our framework achieves state-of-the-art zero-shot AVS
performance, highlighting the effectiveness of multimodal model integration for
finegrained audiovisual segmentation.

</details>


### [6] [Securing Traffic Sign Recognition Systems in Autonomous Vehicles](https://arxiv.org/abs/2506.06563)
*Thushari Hapuarachchi,Long Dang,Kaiqi Xiong*

Main category: cs.CV

TL;DR: 论文研究了交通标志识别中深度神经网络（DNNs）的鲁棒性，提出了一种基于数据增强的训练方法以抵御误差最小化攻击，并展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于DNNs在交通标志识别中广泛使用且训练数据来源未知，确保模型安全免受攻击或污染至关重要。

Method: 通过添加不可察觉的扰动进行误差最小化攻击，并提出基于非线性变换的数据增强训练方法以提高模型鲁棒性。

Result: 攻击将DNNs预测准确率从99.90%降至10.6%，但提出的方法成功恢复至96.05%，并优于对抗训练。此外，检测模型对攻击的识别成功率超过99%。

Conclusion: 研究强调了在交通标志识别系统中采用先进训练方法以抵御数据污染攻击的必要性。

Abstract: Deep Neural Networks (DNNs) are widely used for traffic sign recognition
because they can automatically extract high-level features from images. These
DNNs are trained on large-scale datasets obtained from unknown sources.
Therefore, it is important to ensure that the models remain secure and are not
compromised or poisoned during training. In this paper, we investigate the
robustness of DNNs trained for traffic sign recognition. First, we perform the
error-minimizing attacks on DNNs used for traffic sign recognition by adding
imperceptible perturbations on training data. Then, we propose a data
augmentation-based training method to mitigate the error-minimizing attacks.
The proposed training method utilizes nonlinear transformations to disrupt the
perturbations and improve the model robustness. We experiment with two
well-known traffic sign datasets to demonstrate the severity of the attack and
the effectiveness of our mitigation scheme. The error-minimizing attacks reduce
the prediction accuracy of the DNNs from 99.90% to 10.6%. However, our
mitigation scheme successfully restores the prediction accuracy to 96.05%.
Moreover, our approach outperforms adversarial training in mitigating the
error-minimizing attacks. Furthermore, we propose a detection model capable of
identifying poisoned data even when the perturbations are imperceptible to
human inspection. Our detection model achieves a success rate of over 99% in
identifying the attack. This research highlights the need to employ advanced
training methods for DNNs in traffic sign recognition systems to mitigate the
effects of data poisoning attacks.

</details>


### [7] [Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models](https://arxiv.org/abs/2506.06569)
*Yannis Spyridis,Vasileios Argyriou*

Main category: cs.CV

TL;DR: 论文研究了利用RGB图像和深度学习技术（如迁移学习和基础模型）实现纺织品自动分类和污染物分割的可行性，为纺织品回收自动化提供了关键技术支持。


<details>
  <summary>Details</summary>
Motivation: 纺织品回收自动化中，准确识别材料成分和检测污染物是主要挑战，而RGB图像是一种成本低廉的传感方式。

Method: 使用预训练架构（如EfficientNetB0）进行纺织品分类，结合Grounding DINO和Segment Anything Model（SAM）进行零样本污染物分割。

Result: 分类任务中EfficientNetB0达到81.25%准确率；分割任务中mIoU为0.90，表现优异。

Conclusion: RGB图像结合现代深度学习技术可有效支持纺织品回收自动化的关键分析步骤。

Abstract: Automated sorting is crucial for improving the efficiency and scalability of
textile recycling, but accurately identifying material composition and
detecting contaminants from sensor data remains challenging. This paper
investigates the use of standard RGB imagery, a cost-effective sensing
modality, for key pre-processing tasks in an automated system. We present
computer vision components designed for a conveyor belt setup to perform (a)
classification of four common textile types and (b) segmentation of non-textile
features such as buttons and zippers. For classification, several pre-trained
architectures were evaluated using transfer learning and cross-validation, with
EfficientNetB0 achieving the best performance on a held-out test set with
81.25\% accuracy. For feature segmentation, a zero-shot approach combining the
Grounding DINO open-vocabulary detector with the Segment Anything Model (SAM)
was employed, demonstrating excellent performance with a mIoU of 0.90 for the
generated masks against ground truth. This study demonstrates the feasibility
of using RGB images coupled with modern deep learning techniques, including
transfer learning for classification and foundation models for zero-shot
segmentation, to enable essential analysis steps for automated textile
recycling pipelines.

</details>


### [8] [A Deep Learning Approach for Facial Attribute Manipulation and Reconstruction in Surveillance and Reconnaissance](https://arxiv.org/abs/2506.06578)
*Anees Nashath Shaik,Barbara Villarini,Vasileios Argyriou*

Main category: cs.CV

TL;DR: 论文提出了一种数据驱动平台，通过生成合成训练数据来弥补数据集偏差，提升监控系统的准确性和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有监控系统因图像质量低和AI模型在肤色变化及遮挡面部时的偏见问题，导致识别准确性下降。

Method: 利用深度学习的面部属性操作和重建技术（如自动编码器和GANs）生成多样化高质量面部数据，并集成图像增强模块。

Result: 在CelebA数据集上验证，平台提升了训练数据多样性和模型公平性。

Conclusion: 该工作减少了AI面部分析的偏见，提升了监控系统在复杂环境中的准确性和可靠性。

Abstract: Surveillance systems play a critical role in security and reconnaissance, but
their performance is often compromised by low-quality images and videos,
leading to reduced accuracy in face recognition. Additionally, existing
AI-based facial analysis models suffer from biases related to skin tone
variations and partially occluded faces, further limiting their effectiveness
in diverse real-world scenarios. These challenges are the results of data
limitations and imbalances, where available training datasets lack sufficient
diversity, resulting in unfair and unreliable facial recognition performance.
To address these issues, we propose a data-driven platform that enhances
surveillance capabilities by generating synthetic training data tailored to
compensate for dataset biases. Our approach leverages deep learning-based
facial attribute manipulation and reconstruction using autoencoders and
Generative Adversarial Networks (GANs) to create diverse and high-quality
facial datasets. Additionally, our system integrates an image enhancement
module, improving the clarity of low-resolution or occluded faces in
surveillance footage. We evaluate our approach using the CelebA dataset,
demonstrating that the proposed platform enhances both training data diversity
and model fairness. This work contributes to reducing bias in AI-based facial
analysis and improving surveillance accuracy in challenging environments,
leading to fairer and more reliable security applications.

</details>


### [9] [EV-LayerSegNet: Self-supervised Motion Segmentation using Event Cameras](https://arxiv.org/abs/2506.06596)
*Youssef Farah,Federico Paredes-Vallés,Guido De Croon,Muhammad Ahmed Humais,Hussain Sajwani,Yahya Zweiri*

Main category: cs.CV

TL;DR: 论文提出了一种自监督CNN模型EV-LayerSegNet，用于事件相机的运动分割，通过分层场景动态表示学习光流和分割掩码，并利用去模糊质量作为自监督损失。


<details>
  <summary>Details</summary>
Motivation: 事件相机在运动任务中表现优异，但训练事件网络因标注数据昂贵且易错而困难。

Method: 采用分层动态表示，分别学习仿射光流和分割掩码，通过去模糊质量作为自监督损失。

Result: 在仅含仿射运动的模拟数据集上，IoU和检测率分别达到71%和87%。

Conclusion: EV-LayerSegNet在自监督条件下有效解决了事件相机的运动分割问题。

Abstract: Event cameras are novel bio-inspired sensors that capture motion dynamics
with much higher temporal resolution than traditional cameras, since pixels
react asynchronously to brightness changes. They are therefore better suited
for tasks involving motion such as motion segmentation. However, training
event-based networks still represents a difficult challenge, as obtaining
ground truth is very expensive, error-prone and limited in frequency. In this
article, we introduce EV-LayerSegNet, a self-supervised CNN for event-based
motion segmentation. Inspired by a layered representation of the scene
dynamics, we show that it is possible to learn affine optical flow and
segmentation masks separately, and use them to deblur the input events. The
deblurring quality is then measured and used as self-supervised learning loss.
We train and test the network on a simulated dataset with only affine motion,
achieving IoU and detection rate up to 71% and 87% respectively.

</details>


### [10] [RARL: Improving Medical VLM Reasoning and Generalization with Reinforcement Learning and LoRA under Data and Hardware Constraints](https://arxiv.org/abs/2506.06600)
*Tan-Hanh Pham,Chris Ngo*

Main category: cs.CV

TL;DR: 论文提出了一种名为RARL的强化学习框架，旨在提升医学视觉语言模型（VLMs）的推理能力，同时保持计算效率，适用于资源受限的环境。


<details>
  <summary>Details</summary>
Motivation: 当前医学VLMs在泛化性、透明度和计算效率方面存在局限性，阻碍了其在现实世界中的应用。

Method: 采用低秩适应（Low-Rank Adaptation）和自定义奖励函数对轻量级基础模型Qwen2-VL-2B-Instruct进行微调，结合诊断准确性和推理质量。

Result: RARL在医学图像分析和临床推理任务中表现优异，比监督微调提升约7.78%，且在未见数据集上泛化能力更强。

Conclusion: 推理引导学习和推理提示能显著提升医学VLMs的透明性、准确性和资源效率。

Abstract: The growing integration of vision-language models (VLMs) in medical
applications offers promising support for diagnostic reasoning. However,
current medical VLMs often face limitations in generalization, transparency,
and computational efficiency-barriers that hinder deployment in real-world,
resource-constrained settings. To address these challenges, we propose a
Reasoning-Aware Reinforcement Learning framework, \textbf{RARL}, that enhances
the reasoning capabilities of medical VLMs while remaining efficient and
adaptable to low-resource environments. Our approach fine-tunes a lightweight
base model, Qwen2-VL-2B-Instruct, using Low-Rank Adaptation and custom reward
functions that jointly consider diagnostic accuracy and reasoning quality.
Training is performed on a single NVIDIA A100-PCIE-40GB GPU, demonstrating the
feasibility of deploying such models in constrained environments. We evaluate
the model using an LLM-as-judge framework that scores both correctness and
explanation quality. Experimental results show that RARL significantly improves
VLM performance in medical image analysis and clinical reasoning, outperforming
supervised fine-tuning on reasoning-focused tasks by approximately 7.78%, while
requiring fewer computational resources. Additionally, we demonstrate the
generalization capabilities of our approach on unseen datasets, achieving
around 27% improved performance compared to supervised fine-tuning and about 4%
over traditional RL fine-tuning. Our experiments also illustrate that diversity
prompting during training and reasoning prompting during inference are crucial
for enhancing VLM performance. Our findings highlight the potential of
reasoning-guided learning and reasoning prompting to steer medical VLMs toward
more transparent, accurate, and resource-efficient clinical decision-making.
Code and data are publicly available.

</details>


### [11] [Zero Shot Composed Image Retrieval](https://arxiv.org/abs/2506.06602)
*Santhosh Kakarla,Gautama Shastry Bulusu Venkata*

Main category: cs.CV

TL;DR: 论文提出了一种改进的零样本组合图像检索（CIR）方法，通过微调BLIP-2和轻量级Q-Former融合视觉与文本特征，显著提升了FashionIQ基准上的性能。同时，分析了Retrieval-DPO方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决零样本CIR在FashionIQ基准上Recall@10仅为20-25%的问题，提升组合图像检索的性能。

Method: 1. 微调BLIP-2，使用轻量级Q-Former融合视觉和文本特征；2. 尝试Retrieval-DPO方法，通过Direct Preference Optimization损失微调CLIP的文本编码器。

Result: BLIP-2方法将Recall@10提升至45.6%（衬衫）、40.1%（裙子）和50.4%（T恤），平均Recall@50达67.6%。Retrieval-DPO方法表现不佳，仅0.02% Recall@10。

Conclusion: 有效的基于偏好的CIR需要多模态融合、排名感知目标和高质量负样本。Retrieval-DPO因缺乏多模态融合等问题表现不佳。

Abstract: Composed image retrieval (CIR) allows a user to locate a target image by
applying a fine-grained textual edit (e.g., ``turn the dress blue'' or ``remove
stripes'') to a reference image. Zero-shot CIR, which embeds the image and the
text with separate pretrained vision-language encoders, reaches only 20-25\%
Recall@10 on the FashionIQ benchmark. We improve this by fine-tuning BLIP-2
with a lightweight Q-Former that fuses visual and textual features into a
single embedding, raising Recall@10 to 45.6\% (shirt), 40.1\% (dress), and
50.4\% (top-tee) and increasing the average Recall@50 to 67.6\%. We also
examine Retrieval-DPO, which fine-tunes CLIP's text encoder with a Direct
Preference Optimization loss applied to FAISS-mined hard negatives. Despite
extensive tuning of the scaling factor, index, and sampling strategy,
Retrieval-DPO attains only 0.02\% Recall@10 -- far below zero-shot and
prompt-tuned baselines -- because it (i) lacks joint image-text fusion, (ii)
uses a margin objective misaligned with top-$K$ metrics, (iii) relies on
low-quality negatives, and (iv) keeps the vision and Transformer layers frozen.
Our results show that effective preference-based CIR requires genuine
multimodal fusion, ranking-aware objectives, and carefully curated negatives.

</details>


### [12] [PhysLab: A Benchmark Dataset for Multi-Granularity Visual Parsing of Physics Experiments](https://arxiv.org/abs/2506.06631)
*Minghao Zou,Qingtian Zeng,Yongping Miao,Shangkun Liu,Zilong Wang,Hantao Liu,Wei Zhou*

Main category: cs.CV

TL;DR: PhysLab是一个针对教育场景的视频数据集，专注于学生进行复杂物理实验，提供多级注释以支持多种视觉任务。


<details>
  <summary>Details</summary>
Motivation: 现有数据集在注释粒度、领域覆盖和程序指导方面存在不足，限制了视觉解析的进展。

Method: 引入PhysLab数据集，包含620个长视频，涵盖四种代表性实验，提供多级注释。

Result: 建立了强基线并进行了广泛评估，突出了解析程序性教育视频的关键挑战。

Conclusion: PhysLab有望推动细粒度视觉解析和智能教室系统的发展，促进计算机视觉与教育技术的结合。

Abstract: Visual parsing of images and videos is critical for a wide range of
real-world applications. However, progress in this field is constrained by
limitations of existing datasets: (1) insufficient annotation granularity,
which impedes fine-grained scene understanding and high-level reasoning; (2)
limited coverage of domains, particularly a lack of datasets tailored for
educational scenarios; and (3) lack of explicit procedural guidance, with
minimal logical rules and insufficient representation of structured task
process. To address these gaps, we introduce PhysLab, the first video dataset
that captures students conducting complex physics experiments. The dataset
includes four representative experiments that feature diverse scientific
instruments and rich human-object interaction (HOI) patterns. PhysLab comprises
620 long-form videos and provides multilevel annotations that support a variety
of vision tasks, including action recognition, object detection, HOI analysis,
etc. We establish strong baselines and perform extensive evaluations to
highlight key challenges in the parsing of procedural educational videos. We
expect PhysLab to serve as a valuable resource for advancing fine-grained
visual parsing, facilitating intelligent classroom systems, and fostering
closer integration between computer vision and educational technologies. The
dataset and the evaluation toolkit are publicly available at
https://github.com/ZMH-SDUST/PhysLab.

</details>


### [13] [Dark Channel-Assisted Depth-from-Defocus from a Single Image](https://arxiv.org/abs/2506.06643)
*Moushumi Medhi,Rajiv Ranjan Sahay*

Main category: cs.CV

TL;DR: 本文提出了一种利用暗通道作为辅助线索，从单张空间变异散焦模糊图像中估计场景深度的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的深度从散焦（DFD）技术通常依赖多张不同光圈或对焦设置的图像来恢复深度信息，而单张散焦图像的DFD研究较少。本文旨在解决这一未充分探索的问题。

Method: 通过利用局部散焦模糊与对比度变化之间的关系作为关键深度线索，并结合暗通道先验，以端到端的方式训练整个流程。

Result: 在真实数据上的实验表明，将暗通道先验引入单图像DFD能够产生有意义的深度估计结果。

Conclusion: 该方法验证了暗通道先验在单图像DFD中的有效性。

Abstract: In this paper, we utilize the dark channel as a complementary cue to estimate
the depth of a scene from a single space-variant defocus blurred image due to
its effectiveness in implicitly capturing the local statistics of blurred
images and the scene structure. Existing depth-from-defocus (DFD) techniques
typically rely on multiple images with varying apertures or focus settings to
recover depth information. Very few attempts have focused on DFD from a single
defocused image due to the underconstrained nature of the problem. Our method
capitalizes on the relationship between local defocus blur and contrast
variations as key depth cues to enhance the overall performance in estimating
the scene's structure. The entire pipeline is trained adversarially in a fully
end-to-end fashion. Experiments conducted on real data with realistic
depth-induced defocus blur demonstrate that incorporating dark channel prior
into single image DFD yields meaningful depth estimation results, validating
the effectiveness of our approach.

</details>


### [14] [Parametric Gaussian Human Model: Generalizable Prior for Efficient and Realistic Human Avatar Modeling](https://arxiv.org/abs/2506.06645)
*Cheng Peng,Jingxiang Sun,Yushuo Chen,Zhaoqi Su,Zhuo Su,Yebin Liu*

Main category: cs.CV

TL;DR: PGHM是一种基于3D高斯泼溅的通用高效框架，通过引入UV对齐的潜在身份图和分离的多头U-Net，实现了从单目视频快速重建高保真人类化身。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单目输入下泛化能力差且优化耗时，PGHM旨在解决这些问题，提升虚拟/增强现实和数字娱乐中人类化身的真实感和动画能力。

Method: PGHM包含UV对齐的潜在身份图编码几何和外观，以及分离的多头U-Net预测高斯属性，分解静态、姿态依赖和视角依赖组件。

Result: PGHM仅需约20分钟即可生成视觉质量相当的化身，效率显著优于从头优化的方法。

Conclusion: PGHM展示了其在真实世界单目化身创建中的实用性，为快速高质量化身重建提供了可行方案。

Abstract: Photorealistic and animatable human avatars are a key enabler for
virtual/augmented reality, telepresence, and digital entertainment. While
recent advances in 3D Gaussian Splatting (3DGS) have greatly improved rendering
quality and efficiency, existing methods still face fundamental challenges,
including time-consuming per-subject optimization and poor generalization under
sparse monocular inputs. In this work, we present the Parametric Gaussian Human
Model (PGHM), a generalizable and efficient framework that integrates human
priors into 3DGS for fast and high-fidelity avatar reconstruction from
monocular videos. PGHM introduces two core components: (1) a UV-aligned latent
identity map that compactly encodes subject-specific geometry and appearance
into a learnable feature tensor; and (2) a disentangled Multi-Head U-Net that
predicts Gaussian attributes by decomposing static, pose-dependent, and
view-dependent components via conditioned decoders. This design enables robust
rendering quality under challenging poses and viewpoints, while allowing
efficient subject adaptation without requiring multi-view capture or long
optimization time. Experiments show that PGHM is significantly more efficient
than optimization-from-scratch methods, requiring only approximately 20 minutes
per subject to produce avatars with comparable visual quality, thereby
demonstrating its practical applicability for real-world monocular avatar
creation.

</details>


### [15] [Flood-DamageSense: Multimodal Mamba with Multitask Learning for Building Flood Damage Assessment using SAR Remote Sensing Imagery](https://arxiv.org/abs/2506.06667)
*Yu-Hsuan Ho,Ali Mostafavi*

Main category: cs.CV

TL;DR: Flood-DamageSense是一种深度学习框架，专为洪水灾害后的建筑损坏评估设计，融合多模态数据，显著提升了损坏分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有模型在洪水灾害后建筑损坏分类中表现不佳，主要因为洪水破坏的痕迹不明显。

Method: 结合SAR/InSAR数据、高分辨率光学底图和洪水风险层，采用多模态Mamba架构，预测建筑损坏等级、洪水范围和建筑轮廓。

Result: 在Hurricane Harvey数据上，F1分数比现有方法提升19个百分点，尤其在“轻微”和“中等”损坏类别中表现突出。

Conclusion: Flood-DamageSense通过风险感知建模和SAR全天候能力，为灾害后决策提供了更快、更精细的损坏评估。

Abstract: Most post-disaster damage classifiers succeed only when destructive forces
leave clear spectral or structural signatures -- conditions rarely present
after inundation. Consequently, existing models perform poorly at identifying
flood-related building damages. The model presented in this study,
Flood-DamageSense, addresses this gap as the first deep-learning framework
purpose-built for building-level flood-damage assessment. The architecture
fuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical
basemaps and an inherent flood-risk layer that encodes long-term exposure
probabilities, guiding the network toward plausibly affected structures even
when compositional change is minimal. A multimodal Mamba backbone with a
semi-Siamese encoder and task-specific decoders jointly predicts (1) graded
building-damage states, (2) floodwater extent, and (3) building footprints.
Training and evaluation on Hurricane Harvey (2017) imagery from Harris County,
Texas -- supported by insurance-derived property-damage extents -- show a mean
F1 improvement of up to 19 percentage points over state-of-the-art baselines,
with the largest gains in the frequently misclassified "minor" and "moderate"
damage categories. Ablation studies identify the inherent-risk feature as the
single most significant contributor to this performance boost. An end-to-end
post-processing pipeline converts pixel-level outputs to actionable,
building-scale damage maps within minutes of image acquisition. By combining
risk-aware modeling with SAR's all-weather capability, Flood-DamageSense
delivers faster, finer-grained, and more reliable flood-damage intelligence to
support post-disaster decision-making and resource allocation.

</details>


### [16] [Interpretation of Deep Learning Model in Embryo Selection for In Vitro Fertilization (IVF) Treatment](https://arxiv.org/abs/2506.06680)
*Radha Kodali,Venkata Rao Dhulipalla,Venkata Siva Kishor Tatavarty,Madhavi Nadakuditi,Bharadwaj Thiruveedhula,Suryanarayana Gunnam,Durga Prasad Bavirisetti*

Main category: cs.CV

TL;DR: 本文提出了一种基于CNN-LSTM的可解释人工智能框架，用于高效分类胚胎图像，提高体外受精（IVF）中的胚胎选择准确性。


<details>
  <summary>Details</summary>
Motivation: 不孕症对生活质量的影响日益严重，传统胚胎评级方法效率低下且耗时，需要更高效的解决方案。

Method: 采用CNN-LSTM混合架构，结合深度学习技术，实现对胚胎图像的高精度分类，并通过XAI保持模型的可解释性。

Result: 模型在胚胎分类中表现出高准确性，同时具备可解释性。

Conclusion: 该框架为IVF中的胚胎选择提供了高效且可解释的解决方案。

Abstract: Infertility has a considerable impact on individuals' quality of life,
affecting them socially and psychologically, with projections indicating a rise
in the upcoming years. In vitro fertilization (IVF) emerges as one of the
primary techniques within economically developed nations, employed to address
the rising problem of low fertility. Expert embryologists conventionally grade
embryos by reviewing blastocyst images to select the most optimal for transfer,
yet this process is time-consuming and lacks efficiency. Blastocyst images
provide a valuable resource for assessing embryo viability. In this study, we
introduce an explainable artificial intelligence (XAI) framework for
classifying embryos, employing a fusion of convolutional neural network (CNN)
and long short-term memory (LSTM) architecture, referred to as CNN-LSTM.
Utilizing deep learning, our model achieves high accuracy in embryo
classification while maintaining interpretability through XAI.

</details>


### [17] [A Systematic Investigation on Deep Learning-Based Omnidirectional Image and Video Super-Resolution](https://arxiv.org/abs/2506.06710)
*Qianqian Zhao,Chunle Guo,Tianyi Zhang,Junpei Zhang,Peiyang Jia,Tan Su,Wenjie Jiang,Chongyi Li*

Main category: cs.CV

TL;DR: 本文综述了基于深度学习的全方位图像和视频超分辨率方法，并提出了一个真实退化数据集360Insta，填补了现有数据集的不足。


<details>
  <summary>Details</summary>
Motivation: 全方位图像和视频超分辨率在虚拟现实和增强现实中至关重要，但现有数据集多为合成退化，无法反映真实场景的复杂性。

Method: 系统回顾了基于深度学习的超分辨率方法，并引入真实退化数据集360Insta，进行定性和定量评估。

Result: 360Insta数据集填补了现有数据集的空白，提升了超分辨率方法的泛化能力评估。

Conclusion: 本文为全方位超分辨率研究提供了系统综述和新数据集，并展望了未来研究方向。

Abstract: Omnidirectional image and video super-resolution is a crucial research topic
in low-level vision, playing an essential role in virtual reality and augmented
reality applications. Its goal is to reconstruct high-resolution images or
video frames from low-resolution inputs, thereby enhancing detail preservation
and enabling more accurate scene analysis and interpretation. In recent years,
numerous innovative and effective approaches have been proposed, predominantly
based on deep learning techniques, involving diverse network architectures,
loss functions, projection strategies, and training datasets. This paper
presents a systematic review of recent progress in omnidirectional image and
video super-resolution, focusing on deep learning-based methods. Given that
existing datasets predominantly rely on synthetic degradation and fall short in
capturing real-world distortions, we introduce a new dataset, 360Insta, that
comprises authentically degraded omnidirectional images and videos collected
under diverse conditions, including varying lighting, motion, and exposure
settings. This dataset addresses a critical gap in current omnidirectional
benchmarks and enables more robust evaluation of the generalization
capabilities of omnidirectional super-resolution methods. We conduct
comprehensive qualitative and quantitative evaluations of existing methods on
both public datasets and our proposed dataset. Furthermore, we provide a
systematic overview of the current status of research and discuss promising
directions for future exploration. All datasets, methods, and evaluation
metrics introduced in this work are publicly available and will be regularly
updated. Project page: https://github.com/nqian1/Survey-on-ODISR-and-ODVSR.

</details>


### [18] [NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods to mental imagery](https://arxiv.org/abs/2506.06898)
*Reese Kneeland,Paul S. Scotti,Ghislain St-Yves,Jesse Breedlove,Kendrick Kay,Thomas Naselaris*

Main category: cs.CV

TL;DR: NSD-Imagery是一个与人类fMRI活动配对的基准数据集，用于评估模型在心理图像重建上的表现，补充了现有NSD数据集。研究发现，模型在心理图像上的表现与视觉重建表现脱钩，且架构选择对跨解码性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 心理图像在医学和脑机接口等实际应用中至关重要，但现有模型仅在视觉图像重建上评估，缺乏对心理图像重建能力的验证。

Method: 使用NSD-Imagery数据集，评估了多种NSD训练的视觉解码模型（如MindEye1、Brain Diffuser等）在心理图像重建上的表现。

Result: 心理图像解码性能与视觉重建性能脱钩；简单线性解码架构和多模态特征解码模型在心理图像上表现更好，复杂架构容易过拟合。

Conclusion: 心理图像数据集对实际应用开发至关重要，NSD-Imagery为视觉解码方法的优化提供了重要资源。

Abstract: We release NSD-Imagery, a benchmark dataset of human fMRI activity paired
with mental images, to complement the existing Natural Scenes Dataset (NSD), a
large-scale dataset of fMRI activity paired with seen images that enabled
unprecedented improvements in fMRI-to-image reconstruction efforts. Recent
models trained on NSD have been evaluated only on seen image reconstruction.
Using NSD-Imagery, it is possible to assess how well these models perform on
mental image reconstruction. This is a challenging generalization requirement
because mental images are encoded in human brain activity with relatively lower
signal-to-noise and spatial resolution; however, generalization from seen to
mental imagery is critical for real-world applications in medical domains and
brain-computer interfaces, where the desired information is always internally
generated. We provide benchmarks for a suite of recent NSD-trained open-source
visual decoding models (MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et
al.) on NSD-Imagery, and show that the performance of decoding methods on
mental images is largely decoupled from performance on vision reconstruction.
We further demonstrate that architectural choices significantly impact
cross-decoding performance: models employing simple linear decoding
architectures and multimodal feature decoding generalize better to mental
imagery, while complex architectures tend to overfit visual training data. Our
findings indicate that mental imagery datasets are critical for the development
of practical applications, and establish NSD-Imagery as a useful resource for
better aligning visual decoding methods with this goal.

</details>


### [19] [Active Contour Models Driven by Hyperbolic Mean Curvature Flow for Image Segmentation](https://arxiv.org/abs/2506.06712)
*Saiyu Hu,Chunlei He,Jianfeng Zhang,Dexing Kong,Shoujun Huang*

Main category: cs.CV

TL;DR: 论文提出了基于双曲平均曲率流的主动轮廓模型（HMCF-ACMs）和双模式正则化流驱动的主动轮廓模型（HDRF-ACMs），通过可调初始速度场和边缘感知力调制，提升了图像分割的精度和抗噪性。


<details>
  <summary>Details</summary>
Motivation: 传统的抛物线平均曲率流驱动的主动轮廓模型（PMCF-ACMs）对初始曲线配置依赖性强，限制了其适应性。本文旨在通过引入双曲平均曲率流和正则化技术，解决这一问题。

Method: 提出HMCF-ACMs和HDRF-ACMs，利用可调初始速度场和边缘感知力调制；通过水平集方法和带符号距离函数建立数值等价性；优化加权四阶Runge-Kutta算法求解波动方程。

Result: 实验表明，HMCF-ACMs和HDRF-ACMs在噪声抑制和数值稳定性方面表现优异，能够实现更精确的分割。

Conclusion: 双曲平均曲率流驱动的主动轮廓模型通过自适应初始配置和正则化技术，显著提升了图像分割的性能。

Abstract: Parabolic mean curvature flow-driven active contour models (PMCF-ACMs) are
widely used in image segmentation, which however depend heavily on the
selection of initial curve configurations. In this paper, we firstly propose
several hyperbolic mean curvature flow-driven ACMs (HMCF-ACMs), which introduce
tunable initial velocity fields, enabling adaptive optimization for diverse
segmentation scenarios. We shall prove that HMCF-ACMs are indeed normal flows
and establish the numerical equivalence between dissipative HMCF formulations
and certain wave equations using the level set method with signed distance
function. Building on this framework, we furthermore develop hyperbolic
dual-mode regularized flow-driven ACMs (HDRF-ACMs), which utilize smooth
Heaviside functions for edge-aware force modulation to suppress over-diffusion
near weak boundaries. Then, we optimize a weighted fourth-order Runge-Kutta
algorithm with nine-point stencil spatial discretization when solving the
above-mentioned wave equations. Experiments show that both HMCF-ACMs and
HDRF-ACMs could achieve more precise segmentations with superior noise
resistance and numerical stability due to task-adaptive configurations of
initial velocities and initial contours.

</details>


### [20] [Improving Wildlife Out-of-Distribution Detection: Africas Big Five](https://arxiv.org/abs/2506.06719)
*Mufhumudzi Muthivhi,Jiahao Huo,Fredrik Gustafsson,Terence L. van Zyl*

Main category: cs.CV

TL;DR: 论文研究了野生动物（特别是非洲五大动物）的分布外检测（OOD），比较了参数化与非参数化方法，发现基于特征的方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决人类与野生动物冲突需要准确识别潜在威胁个体，但现有分类模型在未知类别上表现不佳，需改进OOD检测能力。

Method: 采用参数化的最近类均值（NCM）和非参数化的对比学习方法作为基线，并与文献中常见的OOD方法进行比较。

Result: 基于特征的方法（如NCM）在泛化能力上表现更优，AUPR-IN、AUPR-OUT和AUTC分别提升2%、4%和22%。

Conclusion: 特征方法在野生动物OOD检测中更具潜力，为实际应用提供了改进方向。

Abstract: Mitigating human-wildlife conflict seeks to resolve unwanted encounters
between these parties. Computer Vision provides a solution to identifying
individuals that might escalate into conflict, such as members of the Big Five
African animals. However, environments often contain several varied species.
The current state-of-the-art animal classification models are trained under a
closed-world assumption. They almost always remain overconfident in their
predictions even when presented with unknown classes. This study investigates
out-of-distribution (OOD) detection of wildlife, specifically the Big Five. To
this end, we select a parametric Nearest Class Mean (NCM) and a non-parametric
contrastive learning approach as baselines to take advantage of pretrained and
projected features from popular classification encoders. Moreover, we compare
our baselines to various common OOD methods in the literature. The results show
feature-based methods reflect stronger generalisation capability across varying
classification thresholds. Specifically, NCM with ImageNet pre-trained features
achieves a 2%, 4% and 22% improvement on AUPR-IN, AUPR-OUT and AUTC over the
best OOD methods, respectively. The code can be found here
https://github.com/pxpana/BIG5OOD

</details>


### [21] [Mitigating Object Hallucination via Robust Local Perception Search](https://arxiv.org/abs/2506.06729)
*Zixian Gao,Chao Yang,Zhanhui Zhou,Xing Xu,Chaochao Lu*

Main category: cs.CV

TL;DR: 提出了一种名为LPS的解码方法，通过利用局部视觉先验信息来减少多模态大语言模型中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在视觉和语言整合方面取得了显著成功，但仍存在幻觉问题，即输出看似合理但与图像内容不符。

Method: 引入LPS（Local Perception Search），一种简单且无需训练的解码方法，利用局部视觉先验信息修正解码过程。

Result: 在广泛使用的幻觉基准测试和噪声数据上，LPS显著减少了幻觉现象，尤其在噪声环境下表现优异。

Conclusion: LPS是一种即插即用的方法，适用于多种模型，能有效抑制幻觉现象。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled
them to effectively integrate vision and language, addressing a variety of
downstream tasks. However, despite their significant success, these models
still exhibit hallucination phenomena, where the outputs appear plausible but
do not align with the content of the images. To mitigate this issue, we
introduce Local Perception Search (LPS), a decoding method during inference
that is both simple and training-free, yet effectively suppresses
hallucinations. This method leverages local visual prior information as a value
function to correct the decoding process. Additionally, we observe that the
impact of the local visual prior on model performance is more pronounced in
scenarios with high levels of image noise. Notably, LPS is a plug-and-play
approach that is compatible with various models. Extensive experiments on
widely used hallucination benchmarks and noisy data demonstrate that LPS
significantly reduces the incidence of hallucinations compared to the baseline,
showing exceptional performance, particularly in noisy settings.

</details>


### [22] [Adaptive Blind Super-Resolution Network for Spatial-Specific and Spatial-Agnostic Degradations](https://arxiv.org/abs/2506.07705)
*Weilei Wen,Chunle Guo,Wenqi Ren,Hongpeng Wang,Xiuli Shao*

Main category: cs.CV

TL;DR: 论文提出了一种动态滤波网络，通过全局和局部分支处理两种主要退化类型，显著提升了图像重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了不同退化类型的多样性，采用单一网络模型处理多种退化，导致效果不佳。

Method: 提出动态滤波网络，包括全局动态滤波层（处理空间无关退化）和局部动态滤波层（处理空间相关退化）。

Result: 在合成和真实图像数据集上，该方法优于现有盲超分辨率算法。

Conclusion: 动态滤波网络能有效处理不同退化类型，显著提升图像重建质量。

Abstract: Prior methodologies have disregarded the diversities among distinct
degradation types during image reconstruction, employing a uniform network
model to handle multiple deteriorations. Nevertheless, we discover that
prevalent degradation modalities, including sampling, blurring, and noise, can
be roughly categorized into two classes. We classify the first class as
spatial-agnostic dominant degradations, less affected by regional changes in
image space, such as downsampling and noise degradation. The second class
degradation type is intimately associated with the spatial position of the
image, such as blurring, and we identify them as spatial-specific dominant
degradations. We introduce a dynamic filter network integrating global and
local branches to address these two degradation types. This network can greatly
alleviate the practical degradation problem. Specifically, the global dynamic
filtering layer can perceive the spatial-agnostic dominant degradation in
different images by applying weights generated by the attention mechanism to
multiple parallel standard convolution kernels, enhancing the network's
representation ability. Meanwhile, the local dynamic filtering layer converts
feature maps of the image into a spatially specific dynamic filtering operator,
which performs spatially specific convolution operations on the image features
to handle spatial-specific dominant degradations. By effectively integrating
both global and local dynamic filtering operators, our proposed method
outperforms state-of-the-art blind super-resolution algorithms in both
synthetic and real image datasets.

</details>


### [23] [RecipeGen: A Step-Aligned Multimodal Benchmark for Real-World Recipe Generation](https://arxiv.org/abs/2506.06733)
*Ruoxuan Zhang,Jidong Gao,Bin Wen,Hongxia Xie,Chenming Zhang,Honghan-shuai,Wen-Huang Cheng*

Main category: cs.CV

TL;DR: RecipeGen是首个大规模、真实世界的食谱生成基准，支持文本到图像（T2I）、图像到视频（I2V）和文本到视频（T2V）生成，并提供领域特定的评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏食谱目标、步骤说明与视觉内容的细粒度对齐，限制了食谱生成的应用。

Method: 提出RecipeGen数据集，包含26,453个食谱、196,724张图片和4,491个视频，覆盖多样化的食材和烹饪过程。

Result: 提出了领域特定的评估指标，并评估了代表性模型，为未来食谱生成模型提供见解。

Conclusion: RecipeGen为食谱生成领域提供了重要的基准和评估工具，推动了相关研究的发展。

Abstract: Creating recipe images is a key challenge in food computing, with
applications in culinary education and multimodal recipe assistants. However,
existing datasets lack fine-grained alignment between recipe goals, step-wise
instructions, and visual content. We present RecipeGen, the first large-scale,
real-world benchmark for recipe-based Text-to-Image (T2I), Image-to-Video
(I2V), and Text-to-Video (T2V) generation. RecipeGen contains 26,453 recipes,
196,724 images, and 4,491 videos, covering diverse ingredients, cooking
procedures, styles, and dish types. We further propose domain-specific
evaluation metrics to assess ingredient fidelity and interaction modeling,
benchmark representative T2I, I2V, and T2V models, and provide insights for
future recipe generation models. Project page is available now.

</details>


### [24] [THU-Warwick Submission for EPIC-KITCHEN Challenge 2025: Semi-Supervised Video Object Segmentation](https://arxiv.org/abs/2506.06748)
*Mingqi Gao,Haoran Duan,Tianlu Zhang,Jungong Han*

Main category: cs.CV

TL;DR: 提出了一种结合视觉预训练和深度几何线索的自中心视频对象分割方法，在VISOR测试集上J&F得分达90.1%。


<details>
  <summary>Details</summary>
Motivation: 解决复杂场景和长期跟踪的自中心视频对象分割问题。

Method: 结合大规模视觉预训练（SAM2）和深度几何线索，构建统一框架。

Result: 在VISOR测试集上J&F得分达到90.1%。

Conclusion: 该方法通过整合视觉和几何信号，实现了高性能的自中心视频对象分割。

Abstract: In this report, we describe our approach to egocentric video object
segmentation. Our method combines large-scale visual pretraining from SAM2 with
depth-based geometric cues to handle complex scenes and long-term tracking. By
integrating these signals in a unified framework, we achieve strong
segmentation performance. On the VISOR test set, our method reaches a J&F score
of 90.1%.

</details>


### [25] [A Comparative Study of U-Net Architectures for Change Detection in Satellite Images](https://arxiv.org/abs/2506.07925)
*Yaxita Amin,Naimisha S Trivedi,Rashmi Bhattad*

Main category: cs.CV

TL;DR: 本文通过分析34篇论文，比较了18种U-Net变体在遥感变化检测中的应用，特别关注了如Siamese Swin-U-Net等专为变化检测设计的变体，并评估了其优缺点。


<details>
  <summary>Details</summary>
Motivation: 遥感变化检测对监测地球不断变化的景观至关重要，但U-Net架构在该领域的应用尚未充分探索。本文旨在填补这一空白。

Method: 通过综合分析34篇论文，比较18种U-Net变体，评估其在遥感变化检测中的潜力，特别关注专为变化检测设计的变体。

Result: 研究强调了处理不同时间数据和管理长距离关系对提高变化检测精度的重要性，并提供了选择U-Net版本的实用建议。

Conclusion: 本文为研究人员和从业者在遥感变化检测任务中选择U-Net变体提供了有价值的见解。

Abstract: Remote sensing change detection is essential for monitoring the everchanging
landscapes of the Earth. The U-Net architecture has gained popularity for its
capability to capture spatial information and perform pixel-wise
classification. However, their application in the Remote sensing field remains
largely unexplored. Therefore, this paper fill the gap by conducting a
comprehensive analysis of 34 papers. This study conducts a comparison and
analysis of 18 different U-Net variations, assessing their potential for
detecting changes in remote sensing. We evaluate both benefits along with
drawbacks of each variation within the framework of this particular
application. We emphasize variations that are explicitly built for change
detection, such as Siamese Swin-U-Net, which utilizes a Siamese architecture.
The analysis highlights the significance of aspects such as managing data from
different time periods and collecting relationships over a long distance to
enhance the precision of change detection. This study provides valuable
insights for researchers and practitioners that choose U-Net versions for
remote sensing change detection tasks.

</details>


### [26] [SAR2Struct: Extracting 3D Semantic Structural Representation of Aircraft Targets from Single-View SAR Image](https://arxiv.org/abs/2506.06757)
*Ziyu Yue,Ruixi You,Feng Xu*

Main category: cs.CV

TL;DR: 论文提出了一种新任务：从单视角SAR图像中恢复目标的结构，包括组件及其关系（对称性和相邻性），并通过两步算法框架实现。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注3D表面重建或局部几何特征提取，忽略了结构建模在语义信息捕捉中的作用。

Method: 提出基于结构描述符的两步算法框架：训练阶段从真实SAR图像检测2D关键点，并学习其到3D层次结构的映射；测试阶段整合这两步从真实SAR图像推断3D结构。

Result: 实验验证了各步骤的有效性，首次证明可以从单视角SAR图像直接推导飞机目标的3D语义结构表示。

Conclusion: 该方法成功实现了从SAR图像中直接获取目标的结构语义信息，为SAR高级信息检索提供了新思路。

Abstract: To translate synthetic aperture radar (SAR) image into interpretable forms
for human understanding is the ultimate goal of SAR advanced information
retrieval. Existing methods mainly focus on 3D surface reconstruction or local
geometric feature extraction of targets, neglecting the role of structural
modeling in capturing semantic information. This paper proposes a novel task:
SAR target structure recovery, which aims to infer the components of a target
and the structural relationships between its components, specifically symmetry
and adjacency, from a single-view SAR image. Through learning the structural
consistency and geometric diversity across the same type of targets as observed
in different SAR images, it aims to derive the semantic representation of
target directly from its 2D SAR image. To solve this challenging task, a
two-step algorithmic framework based on structural descriptors is developed.
Specifically, in the training phase, it first detects 2D keypoints from real
SAR images, and then learns the mapping from these keypoints to 3D hierarchical
structures using simulated data. During the testing phase, these two steps are
integrated to infer the 3D structure from real SAR images. Experimental results
validated the effectiveness of each step and demonstrated, for the first time,
that 3D semantic structural representation of aircraft targets can be directly
derived from a single-view SAR image.

</details>


### [27] [LitMAS: A Lightweight and Generalized Multi-Modal Anti-Spoofing Framework for Biometric Security](https://arxiv.org/abs/2506.06759)
*Nidheesh Gorthi,Kartik Thakral,Rishabh Ranjan,Richa Singh,Mayank Vatsa*

Main category: cs.CV

TL;DR: LitMAS是一个轻量级、通用的多模态反欺骗框架，用于检测语音、人脸、虹膜和指纹生物识别系统中的欺骗攻击。


<details>
  <summary>Details</summary>
Motivation: 生物识别认证系统在关键应用中广泛部署，但仍易受欺骗攻击。现有研究多关注特定模态的反欺骗技术，缺乏统一的跨模态解决方案。

Method: LitMAS采用模态对齐集中损失（Modality-Aligned Concentration Loss），增强类间分离性并保持跨模态一致性。

Result: LitMAS仅需6M参数，在七个数据集上的平均EER比现有方法提高1.36%，表现出高效性和强泛化能力。

Conclusion: LitMAS为边缘部署提供了高效、通用的多模态反欺骗解决方案。

Abstract: Biometric authentication systems are increasingly being deployed in critical
applications, but they remain susceptible to spoofing. Since most of the
research efforts focus on modality-specific anti-spoofing techniques, building
a unified, resource-efficient solution across multiple biometric modalities
remains a challenge. To address this, we propose LitMAS, a
$\textbf{Li}$gh$\textbf{t}$ weight and generalizable $\textbf{M}$ulti-modal
$\textbf{A}$nti-$\textbf{S}$poofing framework designed to detect spoofing
attacks in speech, face, iris, and fingerprint-based biometric systems. At the
core of LitMAS is a Modality-Aligned Concentration Loss, which enhances
inter-class separability while preserving cross-modal consistency and enabling
robust spoof detection across diverse biometric traits. With just 6M
parameters, LitMAS surpasses state-of-the-art methods by $1.36\%$ in average
EER across seven datasets, demonstrating high efficiency, strong
generalizability, and suitability for edge deployment. Code and trained models
are available at https://github.com/IAB-IITJ/LitMAS.

</details>


### [28] [LoopDB: A Loop Closure Dataset for Large Scale Simultaneous Localization and Mapping](https://arxiv.org/abs/2506.06771)
*Mohammad-Maher Nakshbandi,Ziad Sharawy,Dorian Cojocaru,Sorin Grigorescu*

Main category: cs.CV

TL;DR: LoopDB是一个包含1000多张多样化环境图像的闭环数据集，用于评估和训练SLAM中的闭环算法。


<details>
  <summary>Details</summary>
Motivation: 提供高质量数据集以支持闭环算法的基准测试和深度学习训练。

Method: 使用高分辨率相机采集图像序列，并提供旋转和平移的真值数据。

Result: 数据集包含多样化场景，适合算法评估和模型训练。

Conclusion: LoopDB公开可用，支持闭环算法的研究和开发。

Abstract: In this study, we introduce LoopDB, which is a challenging loop closure
dataset comprising over 1000 images captured across diverse environments,
including parks, indoor scenes, parking spaces, as well as centered around
individual objects. Each scene is represented by a sequence of five consecutive
images. The dataset was collected using a high resolution camera, providing
suitable imagery for benchmarking the accuracy of loop closure algorithms,
typically used in simultaneous localization and mapping. As ground truth
information, we provide computed rotations and translations between each
consecutive images. Additional to its benchmarking goal, the dataset can be
used to train and fine-tune loop closure methods based on deep neural networks.
LoopDB is publicly available at https://github.com/RovisLab/LoopDB.

</details>


### [29] [Continuous-Time SO(3) Forecasting with Savitzky--Golay Neural Controlled Differential Equations](https://arxiv.org/abs/2506.06780)
*Lennart Bastian,Mohammad Rashed,Nassir Navab,Tolga Birdal*

Main category: cs.CV

TL;DR: 提出了一种基于神经控制微分方程和Savitzky-Golay路径的连续时间旋转物体动力学建模方法，解决了SO(3)外推中的噪声、稀疏观测和复杂动态问题。


<details>
  <summary>Details</summary>
Motivation: 解决SO(3)外推中的挑战，包括噪声和稀疏观测、复杂动态以及长期预测需求。

Method: 使用神经控制微分方程和Savitzky-Golay路径建模连续时间旋转物体动力学，保留旋转的几何结构。

Result: 在真实数据上展示了优于现有方法的预测能力。

Conclusion: 该方法能够有效建模旋转物体的复杂动态，并在长期预测中表现优异。

Abstract: Tracking and forecasting the rotation of objects is fundamental in computer
vision and robotics, yet SO(3) extrapolation remains challenging as (1) sensor
observations can be noisy and sparse, (2) motion patterns can be governed by
complex dynamics, and (3) application settings can demand long-term
forecasting. This work proposes modeling continuous-time rotational object
dynamics on $SO(3)$ using Neural Controlled Differential Equations guided by
Savitzky-Golay paths. Unlike existing methods that rely on simplified motion
assumptions, our method learns a general latent dynamical system of the
underlying object trajectory while respecting the geometric structure of
rotations. Experimental results on real-world data demonstrate compelling
forecasting capabilities compared to existing approaches.

</details>


### [30] [Training-Free Identity Preservation in Stylized Image Generation Using Diffusion Models](https://arxiv.org/abs/2506.06802)
*Mohammad Ali Rezaei,Helia Hajikazem,Saeed Khanehgir,Mahdi Javanmardi*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的无需训练的身份保持风格化图像合成框架，通过“马赛克恢复内容图像”技术和内容一致性损失，显著提升了身份保留能力。


<details>
  <summary>Details</summary>
Motivation: 现有风格迁移技术在保持身份的同时实现高质量风格化方面存在困难，尤其是在面部较小或相机距离较远的图像中。

Method: 采用“马赛克恢复内容图像”技术和无需训练的内容一致性损失，增强身份保留和内容细节。

Result: 实验表明，该方法在保持高风格保真度和身份完整性方面显著优于基线模型，尤其是在复杂场景中。

Conclusion: 该方法无需重新训练或微调，即可有效解决身份保留问题，适用于复杂场景。

Abstract: While diffusion models have demonstrated remarkable generative capabilities,
existing style transfer techniques often struggle to maintain identity while
achieving high-quality stylization. This limitation is particularly acute for
images where faces are small or exhibit significant camera-to-face distances,
frequently leading to inadequate identity preservation. To address this, we
introduce a novel, training-free framework for identity-preserved stylized
image synthesis using diffusion models. Key contributions include: (1) the
"Mosaic Restored Content Image" technique, significantly enhancing identity
retention, especially in complex scenes; and (2) a training-free content
consistency loss that enhances the preservation of fine-grained content details
by directing more attention to the original image during stylization. Our
experiments reveal that the proposed approach substantially surpasses the
baseline model in concurrently maintaining high stylistic fidelity and robust
identity integrity, particularly under conditions of small facial regions or
significant camera-to-face distances, all without necessitating model
retraining or fine-tuning.

</details>


### [31] [Stepwise Decomposition and Dual-stream Focus: A Novel Approach for Training-free Camouflaged Object Segmentation](https://arxiv.org/abs/2506.06818)
*Chao Yin,Hao Li,Kequan Yang,Jide Li,Pinpin Zhu,Xiaoqiang Li*

Main category: cs.CV

TL;DR: 论文提出RDVP-MSD框架，通过结合区域约束的双流视觉提示和多模态逐步分解思维链，解决伪装物体分割中的语义模糊和空间分离问题，无需训练即可实现高效分割。


<details>
  <summary>Details</summary>
Motivation: 当前任务通用提示分割方法在伪装物体分割中存在语义模糊和空间分离问题，导致分割不准确。

Method: 提出RDVP-MSD框架，结合区域约束的双流视觉提示（RDVP）和多模态逐步分解思维链（MSD-CoT），逐步消除语义模糊并优化视觉提示。

Result: 在多个COS基准测试中达到最先进的分割效果，且推理速度更快。

Conclusion: RDVP-MSD显著提高了伪装物体分割的准确性和效率，无需训练或监督。

Abstract: While promptable segmentation (\textit{e.g.}, SAM) has shown promise for
various segmentation tasks, it still requires manual visual prompts for each
object to be segmented. In contrast, task-generic promptable segmentation aims
to reduce the need for such detailed prompts by employing only a task-generic
prompt to guide segmentation across all test samples. However, when applied to
Camouflaged Object Segmentation (COS), current methods still face two critical
issues: 1) \textit{\textbf{semantic ambiguity in getting instance-specific text
prompts}}, which arises from insufficient discriminative cues in holistic
captions, leading to foreground-background confusion; 2)
\textit{\textbf{semantic discrepancy combined with spatial separation in
getting instance-specific visual prompts}}, which results from global
background sampling far from object boundaries with low feature correlation,
causing SAM to segment irrelevant regions. To address the issues above, we
propose \textbf{RDVP-MSD}, a novel training-free test-time adaptation framework
that synergizes \textbf{R}egion-constrained \textbf{D}ual-stream
\textbf{V}isual \textbf{P}rompting (RDVP) via \textbf{M}ultimodal
\textbf{S}tepwise \textbf{D}ecomposition Chain of Thought (MSD-CoT). MSD-CoT
progressively disentangles image captions to eliminate semantic ambiguity,
while RDVP injects spatial constraints into visual prompting and independently
samples visual prompts for foreground and background points, effectively
mitigating semantic discrepancy and spatial separation. Without requiring any
training or supervision, RDVP-MSD achieves a state-of-the-art segmentation
result on multiple COS benchmarks and delivers a faster inference speed than
previous methods, demonstrating significantly improved accuracy and efficiency.
The codes will be available at
\href{https://github.com/ycyinchao/RDVP-MSD}{https://github.com/ycyinchao/RDVP-MSD}

</details>


### [32] [Hi-LSplat: Hierarchical 3D Language Gaussian Splatting](https://arxiv.org/abs/2506.06822)
*Chenlu Zhan,Yufei Zhang,Gaoang Wang,Hongwei Wang*

Main category: cs.CV

TL;DR: Hi-LSplat提出了一种基于3D高斯泼溅的分层语言模型，解决了现有方法在视图一致性和开放词汇语义理解上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS模型依赖2D基础模型，导致视图不一致和开放词汇语义理解困难。

Method: 通过构建3D分层语义树和引入对比损失，实现视图一致的3D分层语义表示。

Result: 实验表明Hi-LSplat在3D开放词汇分割和定位中表现优越，并能捕捉复杂的分层语义。

Conclusion: Hi-LSplat通过分层语义树和对比损失，显著提升了3D场景中的语义理解能力。

Abstract: Modeling 3D language fields with Gaussian Splatting for open-ended language
queries has recently garnered increasing attention. However, recent 3DGS-based
models leverage view-dependent 2D foundation models to refine 3D semantics but
lack a unified 3D representation, leading to view inconsistencies.
Additionally, inherent open-vocabulary challenges cause inconsistencies in
object and relational descriptions, impeding hierarchical semantic
understanding. In this paper, we propose Hi-LSplat, a view-consistent
Hierarchical Language Gaussian Splatting work for 3D open-vocabulary querying.
To achieve view-consistent 3D hierarchical semantics, we first lift 2D features
to 3D features by constructing a 3D hierarchical semantic tree with layered
instance clustering, which addresses the view inconsistency issue caused by 2D
semantic features. Besides, we introduce instance-wise and part-wise
contrastive losses to capture all-sided hierarchical semantic representations.
Notably, we construct two hierarchical semantic datasets to better assess the
model's ability to distinguish different semantic levels. Extensive experiments
highlight our method's superiority in 3D open-vocabulary segmentation and
localization. Its strong performance on hierarchical semantic datasets
underscores its ability to capture complex hierarchical semantics within 3D
scenes.

</details>


### [33] [Exploring Visual Prompting: Robustness Inheritance and Beyond](https://arxiv.org/abs/2506.06823)
*Qi Li,Liangzhi Li,Zhouqiang Jiang,Bowen Wang,Keke Tang*

Main category: cs.CV

TL;DR: 本文探讨了视觉提示（VP）在稳健源模型下的表现，提出了一种名为Prompt Boundary Loosening（PBL）的策略，以解决VP在继承稳健性和泛化能力之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 研究VP在稳健源模型下的表现，探索其是否能继承稳健性及是否存在稳健性与泛化能力的权衡。

Method: 提出PBL策略，作为一种轻量级、即插即用的方法，与VP兼容，旨在解决稳健性与泛化能力的权衡。

Result: 实验表明PBL能有效继承稳健性并显著提升VP在下游任务中的泛化能力。

Conclusion: PBL策略在多种数据集上均表现出色，为VP在稳健源模型下的应用提供了有效解决方案。

Abstract: Visual Prompting (VP), an efficient method for transfer learning, has shown
its potential in vision tasks. However, previous works focus exclusively on VP
from standard source models, it is still unknown how it performs under the
scenario of a robust source model: Can the robustness of the source model be
successfully inherited? Does VP also encounter the same trade-off between
robustness and generalization ability as the source model during this process?
If such a trade-off exists, is there a strategy specifically tailored to VP to
mitigate this limitation? In this paper, we thoroughly explore these three
questions for the first time and provide affirmative answers to them. To
mitigate the trade-off faced by VP, we propose a strategy called Prompt
Boundary Loosening (PBL). As a lightweight, plug-and-play strategy naturally
compatible with VP, PBL effectively ensures the successful inheritance of
robustness when the source model is a robust model, while significantly
enhancing VP's generalization ability across various downstream datasets.
Extensive experiments across various datasets show that our findings are
universal and demonstrate the significant benefits of the proposed strategy.

</details>


### [34] [Controllable Coupled Image Generation via Diffusion Models](https://arxiv.org/abs/2506.06826)
*Chenfei Yuan,Nanshan Jia,Hangqi Li,Peter W. Glynn,Zeyu Zheng*

Main category: cs.CV

TL;DR: 提出了一种注意力级别控制方法，用于耦合图像生成任务，确保背景相似的同时保持中心对象的灵活性。


<details>
  <summary>Details</summary>
Motivation: 解决多图像生成中背景耦合但中心对象灵活的需求。

Method: 通过解耦背景和实体的交叉注意力模块，并引入时间变化的权重控制参数。

Result: 在背景耦合、文本对齐和视觉质量上优于现有方法。

Conclusion: 该方法有效实现了背景耦合与对象灵活性的平衡。

Abstract: We provide an attention-level control method for the task of coupled image
generation, where "coupled" means that multiple simultaneously generated images
are expected to have the same or very similar backgrounds. While backgrounds
coupled, the centered objects in the generated images are still expected to
enjoy the flexibility raised from different text prompts. The proposed method
disentangles the background and entity components in the model's
cross-attention modules, attached with a sequence of time-varying weight
control parameters depending on the time step of sampling. We optimize this
sequence of weight control parameters with a combined objective that assesses
how coupled the backgrounds are as well as text-to-image alignment and overall
visual quality. Empirical results demonstrate that our method outperforms
existing approaches across these criteria.

</details>


### [35] [EndoARSS: Adapting Spatially-Aware Foundation Model for Efficient Activity Recognition and Semantic Segmentation in Endoscopic Surgery](https://arxiv.org/abs/2506.06830)
*Guankun Wang,Rui Tang,Mengya Xu,Long Bai,Huxin Gao,Hongliang Ren*

Main category: cs.CV

TL;DR: EndoARSS是一个基于DINOv2的多任务学习框架，用于内窥镜手术活动识别和语义分割，通过低秩适配器和空间感知多尺度注意力提升性能。


<details>
  <summary>Details</summary>
Motivation: 内窥镜手术场景复杂，传统深度学习模型在多任务中表现不佳，需解决跨任务干扰问题。

Method: 结合低秩适配器（LoRA）和任务高效共享低秩适配器，引入空间感知多尺度注意力模块。

Result: 在三个新数据集上表现优异，显著提升准确性和鲁棒性。

Conclusion: EndoARSS有望推动AI驱动的内窥镜手术系统发展，提升手术安全性和效率。

Abstract: Endoscopic surgery is the gold standard for robotic-assisted minimally
invasive surgery, offering significant advantages in early disease detection
and precise interventions. However, the complexity of surgical scenes,
characterized by high variability in different surgical activity scenarios and
confused image features between targets and the background, presents challenges
for surgical environment understanding. Traditional deep learning models often
struggle with cross-activity interference, leading to suboptimal performance in
each downstream task. To address this limitation, we explore multi-task
learning, which utilizes the interrelated features between tasks to enhance
overall task performance. In this paper, we propose EndoARSS, a novel
multi-task learning framework specifically designed for endoscopy surgery
activity recognition and semantic segmentation. Built upon the DINOv2
foundation model, our approach integrates Low-Rank Adaptation to facilitate
efficient fine-tuning while incorporating Task Efficient Shared Low-Rank
Adapters to mitigate gradient conflicts across diverse tasks. Additionally, we
introduce the Spatially-Aware Multi-Scale Attention that enhances feature
representation discrimination by enabling cross-spatial learning of global
information. In order to evaluate the effectiveness of our framework, we
present three novel datasets, MTLESD, MTLEndovis and MTLEndovis-Gen, tailored
for endoscopic surgery scenarios with detailed annotations for both activity
recognition and semantic segmentation tasks. Extensive experiments demonstrate
that EndoARSS achieves remarkable performance across multiple benchmarks,
significantly improving both accuracy and robustness in comparison to existing
models. These results underscore the potential of EndoARSS to advance AI-driven
endoscopic surgical systems, offering valuable insights for enhancing surgical
safety and efficiency.

</details>


### [36] [Harnessing Vision-Language Models for Time Series Anomaly Detection](https://arxiv.org/abs/2506.06836)
*Zelin He,Sarah Alnegheimish,Matthew Reimherr*

Main category: cs.CV

TL;DR: 提出了一种基于视觉语言模型（VLM）的两阶段时间序列异常检测方法，结合视觉筛选和全局上下文推理，显著提升了检测性能与效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏视觉-时间推理能力，无法像人类专家一样识别上下文异常。

Method: 两阶段方法：1) ViT4TS（视觉筛选阶段）定位候选异常；2) VLM4TS（VLM推理阶段）结合全局上下文优化检测。

Result: VLM4TS在未进行时间序列训练的情况下，F1-max得分比最佳基线提升24.6%，且效率显著提高。

Conclusion: 该方法展示了VLM在时间序列异常检测中的潜力，性能与效率均优于现有方法。

Abstract: Time-series anomaly detection (TSAD) has played a vital role in a variety of
fields, including healthcare, finance, and industrial monitoring. Prior
methods, which mainly focus on training domain-specific models on numerical
data, lack the visual-temporal reasoning capacity that human experts have to
identify contextual anomalies. To fill this gap, we explore a solution based on
vision language models (VLMs). Recent studies have shown the ability of VLMs
for visual reasoning tasks, yet their direct application to time series has
fallen short on both accuracy and efficiency. To harness the power of VLMs for
TSAD, we propose a two-stage solution, with (1) ViT4TS, a vision-screening
stage built on a relatively lightweight pretrained vision encoder, which
leverages 2-D time-series representations to accurately localize candidate
anomalies; (2) VLM4TS, a VLM-based stage that integrates global temporal
context and VLM reasoning capacity to refine the detection upon the candidates
provided by ViT4TS. We show that without any time-series training, VLM4TS
outperforms time-series pretrained and from-scratch baselines in most cases,
yielding a 24.6 percent improvement in F1-max score over the best baseline.
Moreover, VLM4TS also consistently outperforms existing language-model-based
TSAD methods and is on average 36 times more efficient in token usage.

</details>


### [37] [Multi-StyleGS: Stylizing Gaussian Splatting with Multiple Styles](https://arxiv.org/abs/2506.06846)
*Yangkai Lin,Jiabao Lei,Kui jia*

Main category: cs.CV

TL;DR: 提出了一种名为Multi-StyleGS的新方法，用于在3D高斯泼溅（GS）场景中实现多风格匹配，同时保持内存高效训练。


<details>
  <summary>Details</summary>
Motivation: 当前3D GS场景风格化方法难以适应多风格匹配，且内存效率低，需要更高效的解决方案。

Method: 采用二分匹配机制自动匹配风格图像与局部区域，提出语义风格损失函数和局部-全局特征匹配，优化分割网络以增强语义标签。

Result: 实验表明，该方法在风格化效果、内存效率和编辑灵活性上优于现有方法。

Conclusion: Multi-StyleGS为3D场景风格化提供了高效、灵活且高质量的解决方案。

Abstract: In recent years, there has been a growing demand to stylize a given 3D scene
to align with the artistic style of reference images for creative purposes.
While 3D Gaussian Splatting(GS) has emerged as a promising and efficient method
for realistic 3D scene modeling, there remains a challenge in adapting it to
stylize 3D GS to match with multiple styles through automatic local style
transfer or manual designation, while maintaining memory efficiency for
stylization training. In this paper, we introduce a novel 3D GS stylization
solution termed Multi-StyleGS to tackle these challenges. In particular, we
employ a bipartite matching mechanism to au tomatically identify
correspondences between the style images and the local regions of the rendered
images. To facilitate local style transfer, we introduce a novel semantic style
loss function that employs a segmentation network to apply distinct styles to
various objects of the scene and propose a local-global feature matching to
enhance the multi-view consistency. Furthermore, this technique can achieve
memory efficient training, more texture details and better color match. To
better assign a robust semantic label to each Gaussian, we propose several
techniques to regularize the segmentation network. As demonstrated by our
comprehensive experiments, our approach outperforms existing ones in producing
plausible stylization results and offering flexible editing.

</details>


### [38] [Deep Inertial Pose: A deep learning approach for human pose estimation](https://arxiv.org/abs/2506.06850)
*Sara M. Cerqueira,Manuel Palermo,Cristina P. Santos*

Main category: cs.CV

TL;DR: 论文研究了使用神经网络替代复杂生物力学模型进行姿态估计，比较了不同架构和方法，发现Hybrid LSTM-Madgwick方法效果最佳。


<details>
  <summary>Details</summary>
Motivation: 惯性动作捕捉系统因其便携性和无约束性受到关注，但现有方法复杂且昂贵，需简化流程。

Method: 比较不同神经网络架构和方法，使用低成本和高端的MARG传感器进行姿态估计。

Result: Hybrid LSTM-Madgwick方法误差最低（7.96），且通过消融研究分析了各因素对误差的影响。

Conclusion: 神经网络可有效估计人体姿态，性能接近当前最优融合滤波器。

Abstract: Inertial-based Motion capture system has been attracting growing attention
due to its wearability and unsconstrained use. However, accurate human joint
estimation demands several complex and expertise demanding steps, which leads
to expensive software such as the state-of-the-art MVN Awinda from Xsens
Technologies. This work aims to study the use of Neural Networks to abstract
the complex biomechanical models and analytical mathematics required for pose
estimation. Thus, it presents a comparison of different Neural Network
architectures and methodologies to understand how accurately these methods can
estimate human pose, using both low cost(MPU9250) and high end (Mtw Awinda)
Magnetic, Angular Rate, and Gravity (MARG) sensors. The most efficient method
was the Hybrid LSTM-Madgwick detached, which achieved an Quaternion Angle
distance error of 7.96, using Mtw Awinda data. Also, an ablation study was
conducted to study the impact of data augmentation, output representation,
window size, loss function and magnetometer data on the pose estimation error.
This work indicates that Neural Networks can be trained to estimate human pose,
with results comparable to the state-of-the-art fusion filters.

</details>


### [39] [Position Prediction Self-Supervised Learning for Multimodal Satellite Imagery Semantic Segmentation](https://arxiv.org/abs/2506.06852)
*John Waithaka,Moise Busogi*

Main category: cs.CV

TL;DR: 论文提出了一种基于位置预测的自监督学习方法（LOCA），用于多模态卫星图像语义分割，显著优于基于重建的方法。


<details>
  <summary>Details</summary>
Motivation: 卫星图像语义分割受限于标注数据不足，现有自监督方法（如MAE）侧重于重建而非定位，而定位是分割任务的关键。

Method: 扩展SatMAE的通道分组至多模态数据，引入同组注意力掩码以促进跨模态交互，采用相对补丁位置预测任务以增强空间推理。

Result: 在Sen1Floods11洪水映射数据集上，该方法显著优于基于重建的自监督学习方法。

Conclusion: 针对多模态卫星图像的位置预测任务能学习到更有效的表示，优于重建方法。

Abstract: Semantic segmentation of satellite imagery is crucial for Earth observation
applications, but remains constrained by limited labelled training data. While
self-supervised pretraining methods like Masked Autoencoders (MAE) have shown
promise, they focus on reconstruction rather than localisation-a fundamental
aspect of segmentation tasks. We propose adapting LOCA (Location-aware), a
position prediction self-supervised learning method, for multimodal satellite
imagery semantic segmentation. Our approach addresses the unique challenges of
satellite data by extending SatMAE's channel grouping from multispectral to
multimodal data, enabling effective handling of multiple modalities, and
introducing same-group attention masking to encourage cross-modal interaction
during pretraining. The method uses relative patch position prediction,
encouraging spatial reasoning for localisation rather than reconstruction. We
evaluate our approach on the Sen1Floods11 flood mapping dataset, where it
significantly outperforms existing reconstruction-based self-supervised
learning methods for satellite imagery. Our results demonstrate that position
prediction tasks, when properly adapted for multimodal satellite imagery, learn
representations more effective for satellite image semantic segmentation than
reconstruction-based approaches.

</details>


### [40] [DONUT: A Decoder-Only Model for Trajectory Prediction](https://arxiv.org/abs/2506.06854)
*Markus Knoche,Daan de Geus,Bastian Leibe*

Main category: cs.CV

TL;DR: DONUT是一种仅解码器网络，用于预测轨迹，通过自回归模型和历史轨迹编码，实现了迭代预测和性能提升。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要预测其他代理的运动，现有编码器-解码器模型存在不足，因此提出仅解码器模型。

Method: 使用自回归模型编码历史轨迹并预测未来轨迹，引入‘过预测’策略以提升性能。

Result: 在Argoverse 2单代理运动预测基准上，DONUT优于基线模型并达到新SOTA。

Conclusion: 仅解码器模型在轨迹预测中表现优异，未来可进一步优化。

Abstract: Predicting the motion of other agents in a scene is highly relevant for
autonomous driving, as it allows a self-driving car to anticipate. Inspired by
the success of decoder-only models for language modeling, we propose DONUT, a
Decoder-Only Network for Unrolling Trajectories. Different from existing
encoder-decoder forecasting models, we encode historical trajectories and
predict future trajectories with a single autoregressive model. This allows the
model to make iterative predictions in a consistent manner, and ensures that
the model is always provided with up-to-date information, enhancing the
performance. Furthermore, inspired by multi-token prediction for language
modeling, we introduce an 'overprediction' strategy that gives the network the
auxiliary task of predicting trajectories at longer temporal horizons. This
allows the model to better anticipate the future, and further improves the
performance. With experiments, we demonstrate that our decoder-only approach
outperforms the encoder-decoder baseline, and achieves new state-of-the-art
results on the Argoverse 2 single-agent motion forecasting benchmark.

</details>


### [41] [Vision-EKIPL: External Knowledge-Infused Policy Learning for Visual Reasoning](https://arxiv.org/abs/2506.06856)
*Chaoyang Wang,Zeyu Zhang,Haiyun Jiang*

Main category: cs.CV

TL;DR: 论文提出了一种名为Vision-EKIPL的新型强化学习框架，通过引入外部辅助模型生成的高质量动作来优化策略模型，显著提升了多模态大语言模型的视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法仅从策略模型本身采样动作组，限制了模型的推理能力上限并导致训练效率低下。

Method: 提出Vision-EKIPL框架，在强化学习训练过程中引入外部辅助模型生成的高质量动作，以指导策略模型的优化。

Result: 在Reason-RFT-CoT Benchmark上实现了5%的性能提升，优于当前最优方法。

Conclusion: Vision-EKIPL克服了传统强化学习方法的局限性，显著提升了视觉推理性能，为该领域研究提供了新范式。

Abstract: Visual reasoning is crucial for understanding complex multimodal data and
advancing Artificial General Intelligence. Existing methods enhance the
reasoning capability of Multimodal Large Language Models (MLLMs) through
Reinforcement Learning (RL) fine-tuning (e.g., GRPO). However, current RL
approaches sample action groups solely from the policy model itself, which
limits the upper boundary of the model's reasoning capability and leads to
inefficient training. To address these limitations, this paper proposes a novel
RL framework called \textbf{Vision-EKIPL}. The core of this framework lies in
introducing high-quality actions generated by external auxiliary models during
the RL training process to guide the optimization of the policy model. The
policy learning with knowledge infusion from external models significantly
expands the model's exploration space, effectively improves the reasoning
boundary, and substantially accelerates training convergence speed and
efficiency. Experimental results demonstrate that our proposed Vision-EKIPL
achieved up to a 5\% performance improvement on the Reason-RFT-CoT Benchmark
compared to the state-of-the-art (SOTA). It reveals that Vision-EKIPL can
overcome the limitations of traditional RL methods, significantly enhance the
visual reasoning performance of MLLMs, and provide a new effective paradigm for
research in this field.

</details>


### [42] [Face recognition on point cloud with cgan-top for denoising](https://arxiv.org/abs/2506.06864)
*Junyu Liu,Jianfeng Ren,Sunhong Liang,Xudong Jiang*

Main category: cs.CV

TL;DR: 提出了一种端到端的3D人脸识别方法，结合去噪和识别模块，显著提高了噪声点云下的识别精度。


<details>
  <summary>Details</summary>
Motivation: 原始点云常因传感器不完美而包含大量噪声，影响识别效果。

Method: 设计了cGAN-TOP用于去噪，并采用LDGCNN进行多尺度特征提取和识别。

Result: 在Bosphorus数据集上验证，识别精度显著提升，最高增益达14.81%。

Conclusion: 该方法有效解决了噪声点云下的3D人脸识别问题。

Abstract: Face recognition using 3D point clouds is gaining growing interest, while raw
point clouds often contain a significant amount of noise due to imperfect
sensors. In this paper, an end-to-end 3D face recognition on a noisy point
cloud is proposed, which synergistically integrates the denoising and
recognition modules. Specifically, a Conditional Generative Adversarial Network
on Three Orthogonal Planes (cGAN-TOP) is designed to effectively remove the
noise in the point cloud, and recover the underlying features for subsequent
recognition. A Linked Dynamic Graph Convolutional Neural Network (LDGCNN) is
then adapted to recognize faces from the processed point cloud, which
hierarchically links both the local point features and neighboring features of
multiple scales. The proposed method is validated on the Bosphorus dataset. It
significantly improves the recognition accuracy under all noise settings, with
a maximum gain of 14.81%.

</details>


### [43] [Hybrid Vision Transformer-Mamba Framework for Autism Diagnosis via Eye-Tracking Analysis](https://arxiv.org/abs/2506.06886)
*Wafaa Kasri,Yassine Himeur,Abigail Copiaco,Wathiq Mansoor,Ammar Albanna,Valsamma Eapen*

Main category: cs.CV

TL;DR: 提出了一种结合Vision Transformers和Vision Mamba的混合深度学习框架，用于通过眼动追踪数据检测自闭症谱系障碍（ASD），显著提升了诊断准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 早期干预对ASD诊断至关重要，但传统方法依赖手工特征且缺乏透明度。

Method: 采用注意力融合机制整合视觉、语音和面部线索，结合ViT和Vision Mamba捕捉时空动态。

Result: 在Saliency4ASD数据集上表现优异，准确率0.96，F1分数0.95，灵敏度0.97，特异性0.94。

Conclusion: 该模型为资源有限或偏远地区的ASD筛查提供了可扩展且可解释的解决方案。

Abstract: Accurate Autism Spectrum Disorder (ASD) diagnosis is vital for early
intervention. This study presents a hybrid deep learning framework combining
Vision Transformers (ViT) and Vision Mamba to detect ASD using eye-tracking
data. The model uses attention-based fusion to integrate visual, speech, and
facial cues, capturing both spatial and temporal dynamics. Unlike traditional
handcrafted methods, it applies state-of-the-art deep learning and explainable
AI techniques to enhance diagnostic accuracy and transparency. Tested on the
Saliency4ASD dataset, the proposed ViT-Mamba model outperformed existing
methods, achieving 0.96 accuracy, 0.95 F1-score, 0.97 sensitivity, and 0.94
specificity. These findings show the model's promise for scalable,
interpretable ASD screening, especially in resource-constrained or remote
clinical settings where access to expert diagnosis is limited.

</details>


### [44] [KNN-Defense: Defense against 3D Adversarial Point Clouds using Nearest-Neighbor Search](https://arxiv.org/abs/2506.06906)
*Nima Jamali,Matina Mahdizadeh Sani,Hanieh Naderi,Shohreh Kasaei*

Main category: cs.CV

TL;DR: KNN-Defense是一种轻量级防御策略，通过利用训练集中邻近样本的语义相似性恢复受扰动的3D点云数据，显著提升对抗攻击下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在3D点云数据分析中表现优异，但对对抗攻击（如点丢弃、移动和添加）的脆弱性影响了3D视觉系统的可靠性。现有防御机制效果有限，需一种高效解决方案。

Method: 基于流形假设和特征空间中的最近邻搜索，KNN-Defense通过语义相似性恢复受扰动的输入，而非重建表面几何或强制均匀点分布。

Result: 在ModelNet40数据集上，KNN-Defense显著提升了多种攻击类型下的鲁棒性，特别是在点丢弃攻击下，对PointNet等模型的准确率提升显著。

Conclusion: KNN-Defense是一种可扩展且高效的解决方案，适用于增强3D点云分类器的对抗鲁棒性，适合实时应用。

Abstract: Deep neural networks (DNNs) have demonstrated remarkable performance in
analyzing 3D point cloud data. However, their vulnerability to adversarial
attacks-such as point dropping, shifting, and adding-poses a critical challenge
to the reliability of 3D vision systems. These attacks can compromise the
semantic and structural integrity of point clouds, rendering many existing
defense mechanisms ineffective. To address this issue, a defense strategy named
KNN-Defense is proposed, grounded in the manifold assumption and
nearest-neighbor search in feature space. Instead of reconstructing surface
geometry or enforcing uniform point distributions, the method restores
perturbed inputs by leveraging the semantic similarity of neighboring samples
from the training set. KNN-Defense is lightweight and computationally
efficient, enabling fast inference and making it suitable for real-time and
practical applications. Empirical results on the ModelNet40 dataset
demonstrated that KNN-Defense significantly improves robustness across various
attack types. In particular, under point-dropping attacks-where many existing
methods underperform due to the targeted removal of critical points-the
proposed method achieves accuracy gains of 20.1%, 3.6%, 3.44%, and 7.74% on
PointNet, PointNet++, DGCNN, and PCT, respectively. These findings suggest that
KNN-Defense offers a scalable and effective solution for enhancing the
adversarial resilience of 3D point cloud classifiers. (An open-source
implementation of the method, including code and data, is available at
https://github.com/nimajam41/3d-knn-defense).

</details>


### [45] [Gaussian Mapping for Evolving Scenes](https://arxiv.org/abs/2506.06909)
*Vladimir Yugay,Thies Kersten,Luca Carlone,Theo Gevers,Martin R. Oswald,Lukas Schmid*

Main category: cs.CV

TL;DR: 论文提出了一种动态场景适应机制和关键帧管理机制，用于解决3D高斯泼溅技术在长期动态场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯泼溅技术在静态场景中表现优异，但在长期动态场景（如场景在视野外变化）中表现不足。

Method: 引入动态场景适应机制持续更新3D表示，并提出关键帧管理机制以维护几何和语义一致性。

Result: 在合成和真实数据集上评估，GaME方法比现有技术更准确。

Conclusion: GaME方法有效解决了长期动态场景的挑战，提升了3D高斯泼溅技术的实用性。

Abstract: Mapping systems with novel view synthesis (NVS) capabilities are widely used
in computer vision, with augmented reality, robotics, and autonomous driving
applications. Most notably, 3D Gaussian Splatting-based systems show high NVS
performance; however, many current approaches are limited to static scenes.
While recent works have started addressing short-term dynamics (motion within
the view of the camera), long-term dynamics (the scene evolving through changes
out of view) remain less explored. To overcome this limitation, we introduce a
dynamic scene adaptation mechanism that continuously updates the 3D
representation to reflect the latest changes. In addition, since maintaining
geometric and semantic consistency remains challenging due to stale
observations disrupting the reconstruction process, we propose a novel keyframe
management mechanism that discards outdated observations while preserving as
much information as possible. We evaluate Gaussian Mapping for Evolving Scenes
(GaME) on both synthetic and real-world datasets and find it to be more
accurate than the state of the art.

</details>


### [46] [Sleep Stage Classification using Multimodal Embedding Fusion from EOG and PSM](https://arxiv.org/abs/2506.06912)
*Olivier Papillon,Rafik Goubran,James Green,Julien Larivière-Chartier,Caitlin Higginson,Frank Knoefel,Rébecca Robillard*

Main category: cs.CV

TL;DR: 该论文提出了一种基于ImageBind的多模态嵌入深度学习模型，结合EOG和PSM数据用于睡眠阶段分类，显著提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 传统PSG依赖EEG，复杂且难以在家中使用，因此研究使用EOG和PSM作为替代方案。

Method: 利用ImageBind模型整合PSM数据和双通道EOG信号进行分类。

Result: 该方法在85晚患者记录中表现优异，优于现有单模态和多模态模型。

Conclusion: 预训练多模态模型可有效用于睡眠阶段分类，接近依赖复杂EEG数据的系统。

Abstract: Accurate sleep stage classification is essential for diagnosing sleep
disorders, particularly in aging populations. While traditional polysomnography
(PSG) relies on electroencephalography (EEG) as the gold standard, its
complexity and need for specialized equipment make home-based sleep monitoring
challenging. To address this limitation, we investigate the use of
electrooculography (EOG) and pressure-sensitive mats (PSM) as less obtrusive
alternatives for five-stage sleep-wake classification. This study introduces a
novel approach that leverages ImageBind, a multimodal embedding deep learning
model, to integrate PSM data with dual-channel EOG signals for sleep stage
classification. Our method is the first reported approach that fuses PSM and
EOG data for sleep stage classification with ImageBind. Our results demonstrate
that fine-tuning ImageBind significantly improves classification accuracy,
outperforming existing models based on single-channel EOG (DeepSleepNet),
exclusively PSM data (ViViT), and other multimodal deep learning approaches
(MBT). Notably, the model also achieved strong performance without fine-tuning,
highlighting its adaptability to specific tasks with limited labeled data,
making it particularly advantageous for medical applications. We evaluated our
method using 85 nights of patient recordings from a sleep clinic. Our findings
suggest that pre-trained multimodal embedding models, even those originally
developed for non-medical domains, can be effectively adapted for sleep
staging, with accuracies approaching systems that require complex EEG data.

</details>


### [47] [Reading in the Dark with Foveated Event Vision](https://arxiv.org/abs/2506.06918)
*Carl Brander,Giovanni Cioffi,Nico Messikommer,Davide Scaramuzza*

Main category: cs.CV

TL;DR: 提出了一种基于事件的OCR方法，利用用户眼动注视减少带宽，并在低光和高动态场景中优于传统RGB相机。


<details>
  <summary>Details</summary>
Motivation: 解决智能眼镜在低光和高动态场景中因运动模糊和带宽限制导致的文本识别问题。

Method: 结合眼动注视聚焦事件流，通过深度二元重建和合成数据训练，利用多模态LLM进行OCR。

Result: 在低光环境下成功读取文本，带宽消耗比RGB相机减少2400倍。

Conclusion: 该方法显著提升了智能眼镜在复杂场景中的文本识别能力，同时大幅降低带宽需求。

Abstract: Current smart glasses equipped with RGB cameras struggle to perceive the
environment in low-light and high-speed motion scenarios due to motion blur and
the limited dynamic range of frame cameras. Additionally, capturing dense
images with a frame camera requires large bandwidth and power consumption,
consequently draining the battery faster. These challenges are especially
relevant for developing algorithms that can read text from images. In this
work, we propose a novel event-based Optical Character Recognition (OCR)
approach for smart glasses. By using the eye gaze of the user, we foveate the
event stream to significantly reduce bandwidth by around 98% while exploiting
the benefits of event cameras in high-dynamic and fast scenes. Our proposed
method performs deep binary reconstruction trained on synthetic data and
leverages multimodal LLMs for OCR, outperforming traditional OCR solutions. Our
results demonstrate the ability to read text in low light environments where
RGB cameras struggle while using up to 2400 times less bandwidth than a
wearable RGB camera.

</details>


### [48] [How Important are Videos for Training Video LLMs?](https://arxiv.org/abs/2506.06928)
*George Lydakis,Alexander Hermans,Ali Athar,Daan de Geus,Bastian Leibe*

Main category: cs.CV

TL;DR: 研究发现，仅通过图像训练的Video LLMs在时间推理能力上表现优于预期，而视频特定训练的改进效果较小。


<details>
  <summary>Details</summary>
Motivation: 探讨Video LLMs在时间推理上的表现，尤其是图像训练与视频训练的效果差异。

Method: 使用LongVU算法训练LLMs，并在TVBench上测试时间推理能力；提出一种基于标注图像序列的微调方案。

Result: 图像训练的LLMs在时间推理上表现显著高于随机水平，且接近或优于视频训练的LLMs。

Conclusion: 当前视频训练方案未充分利用时间特征，需进一步研究图像训练LLMs的时间推理机制及视频训练的瓶颈。

Abstract: Research into Video Large Language Models (LLMs) has progressed rapidly, with
numerous models and benchmarks emerging in just a few years. Typically, these
models are initialized with a pretrained text-only LLM and finetuned on both
image- and video-caption datasets. In this paper, we present findings
indicating that Video LLMs are more capable of temporal reasoning after
image-only training than one would assume, and that improvements from
video-specific training are surprisingly small. Specifically, we show that
image-trained versions of two LLMs trained with the recent LongVU algorithm
perform significantly above chance level on TVBench, a temporal reasoning
benchmark. Additionally, we introduce a simple finetuning scheme involving
sequences of annotated images and questions targeting temporal capabilities.
This baseline results in temporal reasoning performance close to, and
occasionally higher than, what is achieved by video-trained LLMs. This suggests
suboptimal utilization of rich temporal features found in real video by current
models. Our analysis motivates further research into the mechanisms that allow
image-trained LLMs to perform temporal reasoning, as well as into the
bottlenecks that render current video training schemes inefficient.

</details>


### [49] [Polar Hierarchical Mamba: Towards Streaming LiDAR Object Detection with Point Clouds as Egocentric Sequences](https://arxiv.org/abs/2506.06944)
*Mellon M. Zhang,Glen Chou,Saibal Mukhopadhyay*

Main category: cs.CV

TL;DR: PHiM是一种新型状态空间模型（SSM）架构，专为极坐标流式LiDAR设计，通过局部双向Mamba块和全局前向Mamba块实现高效检测，性能优于现有流式检测器。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要低延迟、高吞吐量的实时感知，传统LiDAR处理方法存在延迟问题，而流式方法因极坐标几何与卷积不匹配导致性能下降。

Method: PHiM采用局部双向Mamba块进行扇区内空间编码，全局前向Mamba块进行扇区间时序建模，替代卷积和位置编码，实现高效极坐标流式处理。

Result: 在Waymo Open Dataset上，PHiM性能优于现有流式检测器10%，且吞吐量翻倍，达到全扫描基线水平。

Conclusion: PHiM为极坐标流式LiDAR提供了一种高效解决方案，显著提升了检测性能和吞吐量。

Abstract: Accurate and efficient object detection is essential for autonomous vehicles,
where real-time perception requires low latency and high throughput. LiDAR
sensors provide robust depth information, but conventional methods process full
360{\deg} scans in a single pass, introducing significant delay. Streaming
approaches address this by sequentially processing partial scans in the native
polar coordinate system, yet they rely on translation-invariant convolutions
that are misaligned with polar geometry -- resulting in degraded performance or
requiring complex distortion mitigation. Recent Mamba-based state space models
(SSMs) have shown promise for LiDAR perception, but only in the full-scan
setting, relying on geometric serialization and positional embeddings that are
memory-intensive and ill-suited to streaming. We propose Polar Hierarchical
Mamba (PHiM), a novel SSM architecture designed for polar-coordinate streaming
LiDAR. PHiM uses local bidirectional Mamba blocks for intra-sector spatial
encoding and a global forward Mamba for inter-sector temporal modeling,
replacing convolutions and positional encodings with distortion-aware,
dimensionally-decomposed operations. PHiM sets a new state-of-the-art among
streaming detectors on the Waymo Open Dataset, outperforming the previous best
by 10\% and matching full-scan baselines at twice the throughput. Code will be
available at https://github.com/meilongzhang/Polar-Hierarchical-Mamba .

</details>


### [50] [LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer](https://arxiv.org/abs/2506.06952)
*Ying Shen,Zhiyang Xu,Jiuhai Chen,Shizhe Diao,Jiaxin Zhang,Yuguang Yao,Joy Rimchala,Ismini Lourentzou,Lifu Huang*

Main category: cs.CV

TL;DR: LaTtE-Flow是一种新型高效架构，统一了图像理解和生成，通过分层时间步专家流和残差注意力机制，显著提升了推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有统一模型需要大量预训练且性能不足，生成速度慢，限制了实际应用。

Method: 基于预训练视觉语言模型，采用分层时间步专家流架构和残差注意力机制，优化推理效率。

Result: 在理解任务上表现优异，生成质量竞争性，推理速度提升约6倍。

Conclusion: LaTtE-Flow高效统一了图像理解与生成，具有实际部署潜力。

Abstract: Recent advances in multimodal foundation models unifying image understanding
and generation have opened exciting avenues for tackling a wide range of
vision-language tasks within a single framework. Despite progress, existing
unified models typically require extensive pretraining and struggle to achieve
the same level of performance compared to models dedicated to each task.
Additionally, many of these models suffer from slow image generation speeds,
limiting their practical deployment in real-time or resource-constrained
settings. In this work, we propose Layerwise Timestep-Expert Flow-based
Transformer (LaTtE-Flow), a novel and efficient architecture that unifies image
understanding and generation within a single multimodal model. LaTtE-Flow
builds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong
multimodal understanding capabilities, and extends them with a novel Layerwise
Timestep Experts flow-based architecture for efficient image generation.
LaTtE-Flow distributes the flow-matching process across specialized groups of
Transformer layers, each responsible for a distinct subset of timesteps. This
design significantly improves sampling efficiency by activating only a small
subset of layers at each sampling timestep. To further enhance performance, we
propose a Timestep-Conditioned Residual Attention mechanism for efficient
information reuse across layers. Experiments demonstrate that LaTtE-Flow
achieves strong performance on multimodal understanding tasks, while achieving
competitive image generation quality with around 6x faster inference speed
compared to recent unified multimodal models.

</details>


### [51] [Task-driven real-world super-resolution of document scans](https://arxiv.org/abs/2506.06953)
*Maciej Zyrek,Tomasz Tarasiewicz,Jakub Sadel,Aleksandra Krzywon,Michal Kawulok*

Main category: cs.CV

TL;DR: 论文提出了一种多任务学习框架，通过结合高级视觉任务的辅助损失函数，优化超分辨率网络在光学字符识别任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在模拟数据集上表现良好，但在真实场景（如文档扫描）中泛化能力不足，需解决复杂退化和语义变化问题。

Method: 采用多任务学习框架，结合文本检测、识别、关键点定位和色调一致性等辅助损失函数，并使用动态权重平均机制平衡目标。

Result: 在模拟和真实文档数据集上验证，方法提升了文本检测性能（IoU指标），同时保持图像保真度。

Conclusion: 多目标优化有助于弥合模拟训练与真实部署之间的差距，提升超分辨率模型的实用性。

Abstract: Single-image super-resolution refers to the reconstruction of a
high-resolution image from a single low-resolution observation. Although recent
deep learning-based methods have demonstrated notable success on simulated
datasets -- with low-resolution images obtained by degrading and downsampling
high-resolution ones -- they frequently fail to generalize to real-world
settings, such as document scans, which are affected by complex degradations
and semantic variability. In this study, we introduce a task-driven, multi-task
learning framework for training a super-resolution network specifically
optimized for optical character recognition tasks. We propose to incorporate
auxiliary loss functions derived from high-level vision tasks, including text
detection using the connectionist text proposal network, text recognition via a
convolutional recurrent neural network, keypoints localization using Key.Net,
and hue consistency. To balance these diverse objectives, we employ dynamic
weight averaging mechanism, which adaptively adjusts the relative importance of
each loss term based on its convergence behavior. We validate our approach upon
the SRResNet architecture, which is a well-established technique for
single-image super-resolution. Experimental evaluations on both simulated and
real-world scanned document datasets demonstrate that the proposed approach
improves text detection, measured with intersection over union, while
preserving overall image fidelity. These findings underscore the value of
multi-objective optimization in super-resolution models for bridging the gap
between simulated training regimes and practical deployment in real-world
scenarios.

</details>


### [52] [AR-RAG: Autoregressive Retrieval Augmentation for Image Generation](https://arxiv.org/abs/2506.06962)
*Jingyuan Qi,Zhiyang Xu,Qifan Wang,Lifu Huang*

Main category: cs.CV

TL;DR: AR-RAG是一种新的图像生成方法，通过自回归方式在生成过程中动态检索并融合相关图像块，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图像生成中通常使用静态检索，容易导致过度复制或风格偏差等问题，AR-RAG旨在通过动态检索提升生成质量。

Method: 提出了两种并行框架：DAiD（无需训练的解码策略）和FAiD（参数高效的微调方法），分别通过分布增强和特征增强优化生成过程。

Result: 在Midjourney-30K、GenEval和DPG-Bench等基准测试中，AR-RAG显著优于现有图像生成模型。

Conclusion: AR-RAG通过动态检索和增强策略，有效提升了图像生成的灵活性和质量。

Abstract: We introduce Autoregressive Retrieval Augmentation (AR-RAG), a novel paradigm
that enhances image generation by autoregressively incorporating knearest
neighbor retrievals at the patch level. Unlike prior methods that perform a
single, static retrieval before generation and condition the entire generation
on fixed reference images, AR-RAG performs context-aware retrievals at each
generation step, using prior-generated patches as queries to retrieve and
incorporate the most relevant patch-level visual references, enabling the model
to respond to evolving generation needs while avoiding limitations (e.g.,
over-copying, stylistic bias, etc.) prevalent in existing methods. To realize
AR-RAG, we propose two parallel frameworks: (1) Distribution-Augmentation in
Decoding (DAiD), a training-free plug-and-use decoding strategy that directly
merges the distribution of model-predicted patches with the distribution of
retrieved patches, and (2) Feature-Augmentation in Decoding (FAiD), a
parameter-efficient fine-tuning method that progressively smooths the features
of retrieved patches via multi-scale convolution operations and leverages them
to augment the image generation process. We validate the effectiveness of
AR-RAG on widely adopted benchmarks, including Midjourney-30K, GenEval and
DPG-Bench, demonstrating significant performance gains over state-of-the-art
image generation models.

</details>


### [53] [Dual-view Spatio-Temporal Feature Fusion with CNN-Transformer Hybrid Network for Chinese Isolated Sign Language Recognition](https://arxiv.org/abs/2506.06966)
*Siyuan Jing,Guangxue Wang,Haoyang Zhai,Qin Tao,Jun Yang,Bing Wang,Peng Jin*

Main category: cs.CV

TL;DR: 本文提出了一个双视角中国手语数据集NationalCSL-DP，并设计了一个CNN-Transformer网络作为基线模型，通过简单有效的融合策略提升了孤立手语识别的性能。


<details>
  <summary>Details</summary>
Motivation: 现有手语数据集覆盖不全且多为单视角RGB视频，难以处理手部遮挡问题，因此需要构建更全面的双视角数据集。

Method: 构建了覆盖中国手语词汇的双视角数据集NationalCSL-DP，并提出CNN-Transformer网络及融合策略作为基线。

Result: 实验证明融合策略显著提升了性能，但序列到序列模型难以从双视角视频中学习互补特征。

Conclusion: 双视角数据集和融合策略有效提升了孤立手语识别性能，但互补特征学习仍需改进。

Abstract: Due to the emergence of many sign language datasets, isolated sign language
recognition (ISLR) has made significant progress in recent years. In addition,
the development of various advanced deep neural networks is another reason for
this breakthrough. However, challenges remain in applying the technique in the
real world. First, existing sign language datasets do not cover the whole sign
vocabulary. Second, most of the sign language datasets provide only single view
RGB videos, which makes it difficult to handle hand occlusions when performing
ISLR. To fill this gap, this paper presents a dual-view sign language dataset
for ISLR named NationalCSL-DP, which fully covers the Chinese national sign
language vocabulary. The dataset consists of 134140 sign videos recorded by ten
signers with respect to two vertical views, namely, the front side and the left
side. Furthermore, a CNN transformer network is also proposed as a strong
baseline and an extremely simple but effective fusion strategy for prediction.
Extensive experiments were conducted to prove the effectiveness of the datasets
as well as the baseline. The results show that the proposed fusion strategy can
significantly increase the performance of the ISLR, but it is not easy for the
sequence-to-sequence model, regardless of whether the early-fusion or
late-fusion strategy is applied, to learn the complementary features from the
sign videos of two vertical views.

</details>


### [54] [Guiding Cross-Modal Representations with MLLM Priors via Preference Alignment](https://arxiv.org/abs/2506.06970)
*Pengfei Zhao,Rongbo Luan,Wei Zhang,Peng Wu,Sifeng He*

Main category: cs.CV

TL;DR: 论文提出MAPLE框架，利用MLLM的细粒度对齐特性改进跨模态检索，通过强化学习和新损失函数显著提升性能。


<details>
  <summary>Details</summary>
Motivation: CLIP在多模态检索中存在模态鸿沟，而现有MLLM虽具备对齐能力但机制粗糙，限制了潜力。

Method: MAPLE结合MLLM的细粒度对齐先验，采用强化学习框架，包括自动偏好数据构建和新的RPA损失函数。

Result: 实验表明MAPLE在细粒度跨模态检索中显著提升性能。

Conclusion: MAPLE通过精细对齐机制有效解决了跨模态检索中的语义区分问题。

Abstract: Despite Contrastive Language-Image Pretraining (CLIP)'s remarkable capability
to retrieve content across modalities, a substantial modality gap persists in
its feature space. Intriguingly, we discover that off-the-shelf MLLMs
(Multimodal Large Language Models) demonstrate powerful inherent modality
alignment properties. While recent MLLM-based retrievers with unified
architectures partially mitigate this gap, their reliance on coarse modality
alignment mechanisms fundamentally limits their potential. In this work, We
introduce MAPLE (Modality-Aligned Preference Learning for Embeddings), a novel
framework that leverages the fine grained alignment priors inherent in MLLM to
guide cross modal representation learning. MAPLE formulates the learning
process as reinforcement learning with two key components: (1) Automatic
preference data construction using off-the-shelf MLLM, and (2) a new Relative
Preference Alignment (RPA) loss, which adapts Direct Preference Optimization
(DPO) to the embedding learning setting. Experimental results show that our
preference-guided alignment achieves substantial gains in fine-grained
cross-modal retrieval, underscoring its effectiveness in handling nuanced
semantic distinctions.

</details>


### [55] [Hybrid Mesh-Gaussian Representation for Efficient Indoor Scene Reconstruction](https://arxiv.org/abs/2506.06988)
*Binxiao Huang,Zhihao Li,Shiyong Liu,Xiao Tang,Jiajun Tang,Jiaqi Lin,Yuxin Cheng,Zhenyu Chen,Xiaofei Wu,Ngai Wong*

Main category: cs.CV

TL;DR: 3D高斯泼溅（3DGS）在图像3D重建和实时渲染中表现优异，但复杂纹理区域需要大量高斯元导致效率低。本文提出一种混合表示方法，结合3DGS和纹理网格，优化渲染速度。


<details>
  <summary>Details</summary>
Motivation: 解决3DGS在复杂纹理区域效率低下的问题，通过结合纹理网格提升渲染性能。

Method: 提出混合表示方法，用纹理网格处理纹理丰富的平坦区域，保留高斯元建模复杂几何。通过修剪和优化网格，结合联合优化策略。

Result: 实验表明，混合表示在保持渲染质量的同时，显著提升了FPS并减少了高斯元数量。

Conclusion: 混合表示方法在效率和渲染质量上取得了平衡，适用于室内场景的实时渲染。

Abstract: 3D Gaussian splatting (3DGS) has demonstrated exceptional performance in
image-based 3D reconstruction and real-time rendering. However, regions with
complex textures require numerous Gaussians to capture significant color
variations accurately, leading to inefficiencies in rendering speed. To address
this challenge, we introduce a hybrid representation for indoor scenes that
combines 3DGS with textured meshes. Our approach uses textured meshes to handle
texture-rich flat areas, while retaining Gaussians to model intricate
geometries. The proposed method begins by pruning and refining the extracted
mesh to eliminate geometrically complex regions. We then employ a joint
optimization for 3DGS and mesh, incorporating a warm-up strategy and
transmittance-aware supervision to balance their contributions
seamlessly.Extensive experiments demonstrate that the hybrid representation
maintains comparable rendering quality and achieves superior frames per second
FPS with fewer Gaussian primitives.

</details>


### [56] [Boosting Adversarial Transferability via Commonality-Oriented Gradient Optimization](https://arxiv.org/abs/2506.06992)
*Yanting Gao,Yepeng Liu,Junming Liu,Qi Zhang,Hongyun Zhang,Duoqian Miao,Cairong Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种共性导向的梯度优化策略（COGO），通过增强共性信息和抑制个体特征，显著提高了对抗样本在黑盒设置中的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 探索高效且可迁移的对抗样本对于理解Vision Transformers（ViTs）的特性和机制至关重要。现有方法在可迁移性上存在不足，未能充分利用代理模型的共享和独特特征。

Method: COGO策略包含共性增强（CE）和个体抑制（IS）两部分。CE扰动中低频区域，IS通过自适应阈值评估梯度相关性并加权。

Result: 实验表明，COGO显著提高了对抗攻击的可迁移成功率，优于现有方法。

Conclusion: COGO通过优化共性信息和抑制个体特征，为提升对抗样本的可迁移性提供了有效途径。

Abstract: Exploring effective and transferable adversarial examples is vital for
understanding the characteristics and mechanisms of Vision Transformers (ViTs).
However, adversarial examples generated from surrogate models often exhibit
weak transferability in black-box settings due to overfitting. Existing methods
improve transferability by diversifying perturbation inputs or applying uniform
gradient regularization within surrogate models, yet they have not fully
leveraged the shared and unique features of surrogate models trained on the
same task, leading to suboptimal transfer performance. Therefore, enhancing
perturbations of common information shared by surrogate models and suppressing
those tied to individual characteristics offers an effective way to improve
transferability. Accordingly, we propose a commonality-oriented gradient
optimization strategy (COGO) consisting of two components: Commonality
Enhancement (CE) and Individuality Suppression (IS). CE perturbs the mid-to-low
frequency regions, leveraging the fact that ViTs trained on the same dataset
tend to rely more on mid-to-low frequency information for classification. IS
employs adaptive thresholds to evaluate the correlation between backpropagated
gradients and model individuality, assigning weights to gradients accordingly.
Extensive experiments demonstrate that COGO significantly improves the transfer
success rates of adversarial attacks, outperforming current state-of-the-art
methods.

</details>


### [57] [DM$^3$Net: Dual-Camera Super-Resolution via Domain Modulation and Multi-scale Matching](https://arxiv.org/abs/2506.06993)
*Cong Guan,Jiacheng Ying,Yuya Ieiri,Osamu Yoshie*

Main category: cs.CV

TL;DR: DM$^3$Net是一种基于域调制和多尺度匹配的双摄像头超分辨率网络，旨在通过参考图像提升广角图像的分辨率。


<details>
  <summary>Details</summary>
Motivation: 智能手机摄影中，双摄像头超分辨率技术具有实际应用价值，但现有方法在域差异和高频细节传递上存在不足。

Method: 提出DM$^3$Net，通过域调制学习压缩的全局表示，并设计多尺度匹配模块以提升匹配精度和鲁棒性。同时引入Key Pruning减少内存和推理时间。

Result: 在三个真实数据集上，DM$^3$Net优于现有方法。

Conclusion: DM$^3$Net在双摄像头超分辨率任务中表现出色，同时兼顾效率和性能。

Abstract: Dual-camera super-resolution is highly practical for smartphone photography
that primarily super-resolve the wide-angle images using the telephoto image as
a reference. In this paper, we propose DM$^3$Net, a novel dual-camera
super-resolution network based on Domain Modulation and Multi-scale Matching.
To bridge the domain gap between the high-resolution domain and the degraded
domain, we learn two compressed global representations from image pairs
corresponding to the two domains. To enable reliable transfer of high-frequency
structural details from the reference image, we design a multi-scale matching
module that conducts patch-level feature matching and retrieval across multiple
receptive fields to improve matching accuracy and robustness. Moreover, we also
introduce Key Pruning to achieve a significant reduction in memory usage and
inference time with little model performance sacrificed. Experimental results
on three real-world datasets demonstrate that our DM$^3$Net outperforms the
state-of-the-art approaches.

</details>


### [58] [Technical Report for ICRA 2025 GOOSE 3D Semantic Segmentation Challenge: Adaptive Point Cloud Understanding for Heterogeneous Robotic Systems](https://arxiv.org/abs/2506.06995)
*Xiaoya Zhang*

Main category: cs.CV

TL;DR: 本文介绍了ICRA 2025 GOOSE 3D语义分割挑战赛冠军解决方案的实现细节，通过结合Point Prompt Tuning和Point Transformer v3，实现了对异构LiDAR数据的自适应处理。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人平台采集的多样化户外环境3D点云语义分割问题。

Method: 采用Point Prompt Tuning（PPT）与Point Transformer v3（PTv3）结合，通过平台特定条件化和跨数据集类别对齐策略处理异构数据。

Result: 模型在不依赖外部数据的情况下，性能显著提升，mIoU最高提升22.59%。

Conclusion: 证明了自适应点云理解在野外机器人应用中的有效性。

Abstract: This technical report presents the implementation details of the winning
solution for the ICRA 2025 GOOSE 3D Semantic Segmentation Challenge. This
challenge focuses on semantic segmentation of 3D point clouds from diverse
unstructured outdoor environments collected from multiple robotic platforms.
This problem was addressed by implementing Point Prompt Tuning (PPT) integrated
with Point Transformer v3 (PTv3) backbone, enabling adaptive processing of
heterogeneous LiDAR data through platform-specific conditioning and
cross-dataset class alignment strategies. The model is trained without
requiring additional external data. As a result, this approach achieved
substantial performance improvements with mIoU increases of up to 22.59% on
challenging platforms compared to the baseline PTv3 model, demonstrating the
effectiveness of adaptive point cloud understanding for field robotics
applications.

</details>


### [59] [BePo: Leveraging Birds Eye View and Sparse Points for Efficient and Accurate 3D Occupancy Prediction](https://arxiv.org/abs/2506.07002)
*Yunxiao Shi,Hong Cai,Jisoo Jeong,Yinhao Zhu,Shizhong Han,Amin Ansari,Fatih Porikli*

Main category: cs.CV

TL;DR: 提出了一种结合BEV和稀疏点表示的新型3D占用预测方法BePo，通过双分支设计解决BEV和稀疏点各自的不足，并在实验中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D占用预测中存在高计算成本或信息丢失问题，BEV对小物体信息损失严重，稀疏点对大物体或平坦表面效率低。

Method: 采用双分支设计：基于查询的稀疏点分支和BEV分支，通过交叉注意力共享信息，最终融合输出预测3D占用。

Result: 在Occ3D-nuScenes和Occ3D-Waymo基准测试中表现优越，推理速度与最新高效方法相当。

Conclusion: BePo有效结合BEV和稀疏点的优势，解决了各自局限性，提升了3D占用预测的性能和效率。

Abstract: 3D occupancy provides fine-grained 3D geometry and semantics for scene
understanding which is critical for autonomous driving. Most existing methods,
however, carry high compute costs, requiring dense 3D feature volume and
cross-attention to effectively aggregate information. More recent works have
adopted Bird's Eye View (BEV) or sparse points as scene representation with
much reduced cost, but still suffer from their respective shortcomings. More
concretely, BEV struggles with small objects that often experience significant
information loss after being projected to the ground plane. On the other hand,
points can flexibly model little objects in 3D, but is inefficient at capturing
flat surfaces or large objects. To address these challenges, in this paper, we
present a novel 3D occupancy prediction approach, BePo, which combines BEV and
sparse points based representations. We propose a dual-branch design: a
query-based sparse points branch and a BEV branch. The 3D information learned
in the sparse points branch is shared with the BEV stream via cross-attention,
which enriches the weakened signals of difficult objects on the BEV plane. The
outputs of both branches are finally fused to generate predicted 3D occupancy.
We conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo
benchmarks that demonstrate the superiority of our proposed BePo. Moreover,
BePo also delivers competitive inference speed when compared to the latest
efficient approaches.

</details>


### [60] [UNO: Unified Self-Supervised Monocular Odometry for Platform-Agnostic Deployment](https://arxiv.org/abs/2506.07013)
*Wentao Zhao,Yihe Niu,Yanbo Wang,Tianchen Deng,Shenghai Yuan,Zhenli Wang,Rui Guo,Jingchuan Wang*

Main category: cs.CV

TL;DR: UNO是一个统一的单目视觉里程计框架，能够在多样化环境中实现鲁棒且自适应的位姿估计。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖于特定部署的调优或预定义的运动先验，而UNO旨在泛化到多种实际场景，如自动驾驶、无人机、移动机器人和手持设备。

Method: 采用专家混合策略进行局部状态估计，结合多个专用解码器处理不同类别的自运动模式，并使用可微分的Gumbel-Softmax模块构建帧间相关图、选择最佳解码器并剔除错误估计。后端结合预训练的尺度无关深度先验和轻量级捆绑调整以确保几何一致性。

Result: 在KITTI、EuRoC-MAV和TUM-RGBD三个主要基准数据集上表现出最先进的性能。

Conclusion: UNO框架在多样化环境中实现了鲁棒且自适应的位姿估计，优于传统方法。

Abstract: This work presents UNO, a unified monocular visual odometry framework that
enables robust and adaptable pose estimation across diverse environments,
platforms, and motion patterns. Unlike traditional methods that rely on
deployment-specific tuning or predefined motion priors, our approach
generalizes effectively across a wide range of real-world scenarios, including
autonomous vehicles, aerial drones, mobile robots, and handheld devices. To
this end, we introduce a Mixture-of-Experts strategy for local state
estimation, with several specialized decoders that each handle a distinct class
of ego-motion patterns. Moreover, we introduce a fully differentiable
Gumbel-Softmax module that constructs a robust inter-frame correlation graph,
selects the optimal expert decoder, and prunes erroneous estimates. These cues
are then fed into a unified back-end that combines pre-trained,
scale-independent depth priors with a lightweight bundling adjustment to
enforce geometric consistency. We extensively evaluate our method on three
major benchmark datasets: KITTI (outdoor/autonomous driving), EuRoC-MAV
(indoor/aerial drones), and TUM-RGBD (indoor/handheld), demonstrating
state-of-the-art performance.

</details>


### [61] [TABLET: Table Structure Recognition using Encoder-only Transformers](https://arxiv.org/abs/2506.07015)
*Qiyu Hou,Jun Wang*

Main category: cs.CV

TL;DR: 提出了一种基于Split-Merge的表格结构识别方法，通过序列标注和Transformer编码器优化处理大型密集表格。


<details>
  <summary>Details</summary>
Motivation: 解决表格结构识别中的挑战，尤其是大型密集表格的处理问题。

Method: 将行和列的分割任务视为序列标注问题，使用双Transformer编码器捕捉特征交互；合并任务则通过网格分类和额外Transformer编码器实现。

Result: 在FinTabNet和PubTabNet上表现优异，准确率高且处理速度快。

Conclusion: 该方法为大规模表格识别提供了高效、可扩展的解决方案，适合工业应用。

Abstract: To address the challenges of table structure recognition, we propose a novel
Split-Merge-based top-down model optimized for large, densely populated tables.
Our approach formulates row and column splitting as sequence labeling tasks,
utilizing dual Transformer encoders to capture feature interactions. The
merging process is framed as a grid cell classification task, leveraging an
additional Transformer encoder to ensure accurate and coherent merging. By
eliminating unstable bounding box predictions, our method reduces resolution
loss and computational complexity, achieving high accuracy while maintaining
fast processing speed. Extensive experiments on FinTabNet and PubTabNet
demonstrate the superiority of our model over existing approaches, particularly
in real-world applications. Our method offers a robust, scalable, and efficient
solution for large-scale table recognition, making it well-suited for
industrial deployment.

</details>


### [62] [MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks](https://arxiv.org/abs/2506.07016)
*Sanjoy Chowdhury,Mohamed Elmoghany,Yohan Abeysinghe,Junjie Fei,Sayan Nag,Salman Khan,Mohamed Elhoseiny,Dinesh Manocha*

Main category: cs.CV

TL;DR: 论文提出了一种新任务AV-HaystacksQA，旨在评估大型多模态模型在多视频检索和时间定位中的能力，并提出了一个多智能体框架MAGNET和新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答基准局限于单视频查询，无法满足实际应用中大规模音频-视觉检索和推理的需求。

Method: 提出了AVHaystacks基准和MAGNET框架，用于多视频检索和时间定位任务。

Result: MAGNET在BLEU@4和GPT评估分数上分别比基线方法提高了89%和65%。

Conclusion: AVHaystacks和MAGNET为多视频检索和时间定位任务提供了有效的评估和解决方案。

Abstract: Large multimodal models (LMMs) have shown remarkable progress in audio-visual
understanding, yet they struggle with real-world scenarios that require complex
reasoning across extensive video collections. Existing benchmarks for video
question answering remain limited in scope, typically involving one clip per
query, which falls short of representing the challenges of large-scale,
audio-visual retrieval and reasoning encountered in practical applications. To
bridge this gap, we introduce a novel task named AV-HaystacksQA, where the goal
is to identify salient segments across different videos in response to a query
and link them together to generate the most informative answer. To this end, we
present AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA
pairs designed to assess the capabilities of LMMs in multi-video retrieval and
temporal grounding task. Additionally, we propose a model-agnostic, multi-agent
framework MAGNET to address this challenge, achieving up to 89% and 65%
relative improvements over baseline methods on BLEU@4 and GPT evaluation scores
in QA task on our proposed AVHaystacks. To enable robust evaluation of
multi-video retrieval and temporal grounding for optimal response generation,
we introduce two new metrics, STEM, which captures alignment errors between a
ground truth and a predicted step sequence and MTGS, to facilitate balanced and
interpretable evaluation of segment-level grounding performance. Project:
https://schowdhury671.github.io/magnet_project/

</details>


### [63] [Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs](https://arxiv.org/abs/2506.07045)
*Yikun Ji,Hong Yan,Jun Lan,Huijia Zhu,Weiqiang Wang,Qi Fan,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态大语言模型（MLLMs）的方法，用于检测AI生成图像并提供可解释的视觉定位和文本解释。通过构建标注数据集和多阶段优化策略，模型在检测和定位视觉缺陷方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法多为黑箱，缺乏可解释性；MLLMs虽具分析能力，但存在幻觉问题，需改进以对齐视觉与文本推理。

Method: 构建标注数据集，标注合成伪影；采用多阶段优化策略微调MLLMs，平衡检测、定位和解释目标。

Result: 模型在检测AI生成图像和定位视觉缺陷方面显著优于基线方法。

Conclusion: 通过数据标注和多阶段优化，实现了可解释且高性能的AI图像检测方法。

Abstract: The rapid advancement of image generation technologies intensifies the demand
for interpretable and robust detection methods. Although existing approaches
often attain high accuracy, they typically operate as black boxes without
providing human-understandable justifications. Multi-modal Large Language
Models (MLLMs), while not originally intended for forgery detection, exhibit
strong analytical and reasoning capabilities. When properly fine-tuned, they
can effectively identify AI-generated images and offer meaningful explanations.
However, existing MLLMs still struggle with hallucination and often fail to
align their visual interpretations with actual image content and human
reasoning. To bridge this gap, we construct a dataset of AI-generated images
annotated with bounding boxes and descriptive captions that highlight synthesis
artifacts, establishing a foundation for human-aligned visual-textual grounded
reasoning. We then finetune MLLMs through a multi-stage optimization strategy
that progressively balances the objectives of accurate detection, visual
localization, and coherent textual explanation. The resulting model achieves
superior performance in both detecting AI-generated images and localizing
visual flaws, significantly outperforming baseline methods.

</details>


### [64] [From Swath to Full-Disc: Advancing Precipitation Retrieval with Multimodal Knowledge Expansion](https://arxiv.org/abs/2506.07050)
*Zheng Wang,Kai Ying,Bin Xu,Chunjiao Wang,Cong Bai*

Main category: cs.CV

TL;DR: 论文提出PRE-Net模型，通过多模态知识扩展实现高精度红外降水反演，超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有红外降水反演精度低，而微波和雷达方法范围有限，因此提出PRE任务以扩展红外反演范围。

Method: 采用两阶段流程：扫描带蒸馏阶段（CoMWE）和全盘适应阶段（Self-MaskTune），结合多模态数据。

Result: PRE-Net在PRE基准测试中显著优于PERSIANN-CCS、PDIR和IMERG等领先产品。

Conclusion: PRE-Net为红外降水反演提供了高精度解决方案，代码将开源。

Abstract: Accurate near-real-time precipitation retrieval has been enhanced by
satellite-based technologies. However, infrared-based algorithms have low
accuracy due to weak relations with surface precipitation, whereas passive
microwave and radar-based methods are more accurate but limited in range. This
challenge motivates the Precipitation Retrieval Expansion (PRE) task, which
aims to enable accurate, infrared-based full-disc precipitation retrievals
beyond the scanning swath. We introduce Multimodal Knowledge Expansion, a
two-stage pipeline with the proposed PRE-Net model. In the Swath-Distilling
stage, PRE-Net transfers knowledge from a multimodal data integration model to
an infrared-based model within the scanning swath via Coordinated Masking and
Wavelet Enhancement (CoMWE). In the Full-Disc Adaptation stage, Self-MaskTune
refines predictions across the full disc by balancing multimodal and full-disc
infrared knowledge. Experiments on the introduced PRE benchmark demonstrate
that PRE-Net significantly advanced precipitation retrieval performance,
outperforming leading products like PERSIANN-CCS, PDIR, and IMERG. The code
will be available at https://github.com/Zjut-MultimediaPlus/PRE-Net.

</details>


### [65] [A Layered Self-Supervised Knowledge Distillation Framework for Efficient Multimodal Learning on the Edge](https://arxiv.org/abs/2506.07055)
*Tarique Dahri,Zulfiqar Ali Memon,Zhenyu Yu,Mohd. Yamani Idna Idris,Sheheryar Khan,Sadiq Ahmad,Maged Shoman,Saddam Aziz,Rizwan Qureshi*

Main category: cs.CV

TL;DR: LSSKD是一种自监督知识蒸馏框架，通过中间特征图的辅助分类器生成多样化知识，无需依赖预训练教师网络，提升了模型性能并减少了计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖预训练的教师网络，而LSSKD旨在通过自监督方式提升紧凑模型的性能，适用于低计算设备。

Method: 在中间特征图上添加辅助分类器，实现多样化的自监督知识传递。

Result: 在CIFAR-100和ImageNet等数据集上优于现有方法，推理时无额外计算成本。

Conclusion: LSSKD有效提升了模型泛化能力，适用于多模态传感和低计算环境。

Abstract: We introduce Layered Self-Supervised Knowledge Distillation (LSSKD) framework
for training compact deep learning models. Unlike traditional methods that rely
on pre-trained teacher networks, our approach appends auxiliary classifiers to
intermediate feature maps, generating diverse self-supervised knowledge and
enabling one-to-one transfer across different network stages. Our method
achieves an average improvement of 4.54\% over the state-of-the-art PS-KD
method and a 1.14% gain over SSKD on CIFAR-100, with a 0.32% improvement on
ImageNet compared to HASSKD. Experiments on Tiny ImageNet and CIFAR-100 under
few-shot learning scenarios also achieve state-of-the-art results. These
findings demonstrate the effectiveness of our approach in enhancing model
generalization and performance without the need for large over-parameterized
teacher networks. Importantly, at the inference stage, all auxiliary
classifiers can be removed, yielding no extra computational cost. This makes
our model suitable for deploying small language models on affordable
low-computing devices. Owing to its lightweight design and adaptability, our
framework is particularly suitable for multimodal sensing and cyber-physical
environments that require efficient and responsive inference. LSSKD facilitates
the development of intelligent agents capable of learning from limited sensory
data under weak supervision.

</details>


### [66] [D2R: dual regularization loss with collaborative adversarial generation for model robustness](https://arxiv.org/abs/2506.07056)
*Zhenyu Liu,Huizhi Liang,Rajiv Ranjan,Zhanxing Zhu,Vaclav Snasel,Varun Ojha*

Main category: cs.CV

TL;DR: 提出了一种双正则化损失（D2R Loss）和协作对抗生成（CAG）策略，用于增强深度神经网络对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在目标模型损失函数引导不足和对抗生成非协作性方面存在局限。

Method: D2R Loss通过对抗分布和干净分布优化增强模型鲁棒性；CAG通过梯度协作生成对抗样本。

Result: 在多个基准数据库和目标模型上验证了D2R Loss与CAG的高效性。

Conclusion: D2R Loss与CAG显著提升了模型的鲁棒性。

Abstract: The robustness of Deep Neural Network models is crucial for defending models
against adversarial attacks. Recent defense methods have employed collaborative
learning frameworks to enhance model robustness. Two key limitations of
existing methods are (i) insufficient guidance of the target model via loss
functions and (ii) non-collaborative adversarial generation. We, therefore,
propose a dual regularization loss (D2R Loss) method and a collaborative
adversarial generation (CAG) strategy for adversarial training. D2R loss
includes two optimization steps. The adversarial distribution and clean
distribution optimizations enhance the target model's robustness by leveraging
the strengths of different loss functions obtained via a suitable function
space exploration to focus more precisely on the target model's distribution.
CAG generates adversarial samples using a gradient-based collaboration between
guidance and target models. We conducted extensive experiments on three
benchmark databases, including CIFAR-10, CIFAR-100, Tiny ImageNet, and two
popular target models, WideResNet34-10 and PreActResNet18. Our results show
that D2R loss with CAG produces highly robust models.

</details>


### [67] [FLAIR-HUB: Large-scale Multimodal Dataset for Land Cover and Crop Mapping](https://arxiv.org/abs/2506.07080)
*Anatol Garioud,Sébastien Giordano,Nicolas David,Nicolas Gonthier*

Main category: cs.CV

TL;DR: FLAIR-HUB是一个多传感器土地覆盖数据集，结合六种对齐模态数据，支持监督和多模态预训练，用于土地覆盖和作物分类。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率地球观测数据的处理和标注挑战，支持全球土地覆盖和作物类型监测。

Method: 结合六种模态数据（航空影像、Sentinel-1/2时间序列、SPOT影像、地形数据和历史航空影像），使用深度学习模型（CNN、transformer）进行多模态融合和多任务学习。

Result: 最佳土地覆盖分类性能为78.2%准确率和65.8% mIoU，几乎使用了所有模态数据。

Conclusion: FLAIR-HUB为土地覆盖和作物分类提供了高质量的多模态数据集，展示了多模态融合的复杂性。

Abstract: The growing availability of high-quality Earth Observation (EO) data enables
accurate global land cover and crop type monitoring. However, the volume and
heterogeneity of these datasets pose major processing and annotation
challenges. To address this, the French National Institute of Geographical and
Forest Information (IGN) is actively exploring innovative strategies to exploit
diverse EO data, which require large annotated datasets. IGN introduces
FLAIR-HUB, the largest multi-sensor land cover dataset with
very-high-resolution (20 cm) annotations, covering 2528 km2 of France. It
combines six aligned modalities: aerial imagery, Sentinel-1/2 time series, SPOT
imagery, topographic data, and historical aerial images. Extensive benchmarks
evaluate multimodal fusion and deep learning models (CNNs, transformers) for
land cover or crop mapping and also explore multi-task learning. Results
underscore the complexity of multimodal fusion and fine-grained classification,
with best land cover performance (78.2% accuracy, 65.8% mIoU) achieved using
nearly all modalities. FLAIR-HUB supports supervised and multimodal
pretraining, with data and code available at
https://ignf.github.io/FLAIR/flairhub.

</details>


### [68] [UCOD-DPL: Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning](https://arxiv.org/abs/2506.07087)
*Weiqi Yan,Lvhai Chen,Huaijia Kou,Shengchuan Zhang,Yan Zhang,Liujuan Cao*

Main category: cs.CV

TL;DR: 提出了一种基于动态伪标签学习的无监督伪装目标检测方法（UCOD-DPL），通过自适应伪标签模块、双分支对抗解码器和二次观察机制，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督伪装目标检测方法因伪标签噪声和简单解码器导致性能较低，需解决伪标签噪声和语义特征学习问题。

Method: 采用教师-学生框架，结合自适应伪标签模块（APM）、双分支对抗解码器（DBA）和二次观察机制，动态优化伪标签并提升特征学习能力。

Result: 实验表明，该方法性能优异，甚至超过部分全监督方法。

Conclusion: UCOD-DPL通过动态伪标签学习和多模块协同，有效解决了现有方法的不足，提升了无监督伪装目标检测的性能。

Abstract: Unsupervised Camoflaged Object Detection (UCOD) has gained attention since it
doesn't need to rely on extensive pixel-level labels. Existing UCOD methods
typically generate pseudo-labels using fixed strategies and train 1 x1
convolutional layers as a simple decoder, leading to low performance compared
to fully-supervised methods. We emphasize two drawbacks in these approaches:
1). The model is prone to fitting incorrect knowledge due to the pseudo-label
containing substantial noise. 2). The simple decoder fails to capture and learn
the semantic features of camouflaged objects, especially for small-sized
objects, due to the low-resolution pseudo-labels and severe confusion between
foreground and background pixels. To this end, we propose a UCOD method with a
teacher-student framework via Dynamic Pseudo-label Learning called UCOD-DPL,
which contains an Adaptive Pseudo-label Module (APM), a Dual-Branch Adversarial
(DBA) decoder, and a Look-Twice mechanism. The APM module adaptively combines
pseudo-labels generated by fixed strategies and the teacher model to prevent
the model from overfitting incorrect knowledge while preserving the ability for
self-correction; the DBA decoder takes adversarial learning of different
segmentation objectives, guides the model to overcome the foreground-background
confusion of camouflaged objects, and the Look-Twice mechanism mimics the human
tendency to zoom in on camouflaged objects and performs secondary refinement on
small-sized objects. Extensive experiments show that our method demonstrates
outstanding performance, even surpassing some existing fully supervised
methods. The code is available now.

</details>


### [69] [SceneLCM: End-to-End Layout-Guided Interactive Indoor Scene Generation with Latent Consistency Model](https://arxiv.org/abs/2506.07091)
*Yangkai Lin,Jiabao Lei,Kui Jia*

Main category: cs.CV

TL;DR: SceneLCM是一个端到端框架，结合LLM进行布局设计和LCM进行场景优化，解决了现有室内场景生成方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在室内场景生成中存在编辑限制、物理不一致、人力需求高、单房间限制和材质质量差等问题。

Method: SceneLCM将场景生成分解为四个模块化流程：布局生成（LLM指导）、家具生成（CTS损失）、环境优化（多分辨率纹理场）和物理编辑（物理模拟）。

Result: 实验验证了SceneLCM优于现有技术，展示了广泛的应用潜力。

Conclusion: SceneLCM通过模块化设计和优化技术，实现了高效、高质量的室内场景生成。

Abstract: Our project page: https://scutyklin.github.io/SceneLCM/. Automated generation
of complex, interactive indoor scenes tailored to user prompt remains a
formidable challenge. While existing methods achieve indoor scene synthesis,
they struggle with rigid editing constraints, physical incoherence, excessive
human effort, single-room limitations, and suboptimal material quality. To
address these limitations, we propose SceneLCM, an end-to-end framework that
synergizes Large Language Model (LLM) for layout design with Latent Consistency
Model(LCM) for scene optimization. Our approach decomposes scene generation
into four modular pipelines: (1) Layout Generation. We employ LLM-guided 3D
spatial reasoning to convert textual descriptions into parametric blueprints(3D
layout). And an iterative programmatic validation mechanism iteratively refines
layout parameters through LLM-mediated dialogue loops; (2) Furniture
Generation. SceneLCM employs Consistency Trajectory Sampling(CTS), a
consistency distillation sampling loss guided by LCM, to form fast,
semantically rich, and high-quality representations. We also offer two
theoretical justification to demonstrate that our CTS loss is equivalent to
consistency loss and its distillation error is bounded by the truncation error
of the Euler solver; (3) Environment Optimization. We use a multiresolution
texture field to encode the appearance of the scene, and optimize via CTS loss.
To maintain cross-geometric texture coherence, we introduce a normal-aware
cross-attention decoder to predict RGB by cross-attending to the anchors
locations in geometrically heterogeneous instance. (4)Physically Editing.
SceneLCM supports physically editing by integrating physical simulation,
achieved persistent physical realism. Extensive experiments validate SceneLCM's
superiority over state-of-the-art techniques, showing its wide-ranging
potential for diverse applications.

</details>


### [70] [EdgeSpotter: Multi-Scale Dense Text Spotting for Industrial Panel Monitoring](https://arxiv.org/abs/2506.07112)
*Changhong Fu,Hua Lin,Haobo Zuo,Liangliang Yao,Liguo Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为EdgeSpotter的多尺度密集文本检测方法，用于工业面板的智能监测，解决了跨尺度定位和密集文本区域模糊边界的问题。


<details>
  <summary>Details</summary>
Motivation: 工业面板文本检测面临跨尺度定位和密集文本区域模糊边界的挑战，现有方法多关注单一文本形状，缺乏多尺度特征的综合探索。

Method: 开发了一种新型Transformer结构，结合高效混合器学习多级特征间的依赖关系，并设计了基于Catmull-Rom样条的特征采样方法，编码文本的形状、位置和语义信息。

Result: 在自建的工业面板监测数据集（IPM）上验证了方法的优越性能，并通过边缘AI视觉系统的实际测试证明了其实用性。

Conclusion: EdgeSpotter方法在工业面板监测任务中表现出色，具有较高的准确性和鲁棒性，代码和演示已开源。

Abstract: Text spotting for industrial panels is a key task for intelligent monitoring.
However, achieving efficient and accurate text spotting for complex industrial
panels remains challenging due to issues such as cross-scale localization and
ambiguous boundaries in dense text regions. Moreover, most existing methods
primarily focus on representing a single text shape, neglecting a comprehensive
exploration of multi-scale feature information across different texts. To
address these issues, this work proposes a novel multi-scale dense text spotter
for edge AI-based vision system (EdgeSpotter) to achieve accurate and robust
industrial panel monitoring. Specifically, a novel Transformer with efficient
mixer is developed to learn the interdependencies among multi-level features,
integrating multi-layer spatial and semantic cues. In addition, a new feature
sampling with catmull-rom splines is designed, which explicitly encodes the
shape, position, and semantic information of text, thereby alleviating missed
detections and reducing recognition errors caused by multi-scale or dense text
regions. Furthermore, a new benchmark dataset for industrial panel monitoring
(IPM) is constructed. Extensive qualitative and quantitative evaluations on
this challenging benchmark dataset validate the superior performance of the
proposed method in different challenging panel monitoring tasks. Finally,
practical tests based on the self-designed edge AI-based vision system
demonstrate the practicality of the method. The code and demo will be available
at https://github.com/vision4robotics/EdgeSpotter.

</details>


### [71] [Image segmentation and classification of E-waste for waste segregation](https://arxiv.org/abs/2506.07122)
*Prakriti Tripathi,Theertha Biju,Maniram Thota,Rakesh Lingam*

Main category: cs.CV

TL;DR: 使用YOLOv11和Mask-RCNN模型对电子废物进行分类，分别达到70和41 mAP，未来将整合到分拣机器人中。


<details>
  <summary>Details</summary>
Motivation: 解决电子废物分类问题，为分拣机器人提供支持。

Method: 创建自定义数据集，训练YOLOv11和Mask-RCNN模型。

Result: YOLOv11达到70 mAP，Mask-RCNN达到41 mAP。

Conclusion: 模型将整合到分拣机器人中，实现电子废物自动分类。

Abstract: Industry partners provided a problem statement that involves classifying
electronic waste using machine learning models that will be used by
pick-and-place robots for waste segregation. We started by taking common
electronic waste items, such as a mouse and charger, unsoldering them, and
taking pictures to create a custom dataset. Then state-of-the art YOLOv11 model
was trained and run to achieve 70 mAP in real-time. Mask-RCNN model was also
trained and achieved 41 mAP. The model will be further integrated with
pick-and-place robots to perform segregation of e-waste.

</details>


### [72] [Hi-VAE: Efficient Video Autoencoding with Global and Detailed Motion](https://arxiv.org/abs/2506.07136)
*Huaize Liu,Wenzhang Sun,Qiyuan Zhang,Donglin Di,Biao Gong,Hao Li,Chen Wei,Changqing Zou*

Main category: cs.CV

TL;DR: Hi-VAE是一种高效的视频自动编码框架，通过分层编码视频动态的粗到细运动表示，显著减少时空冗余，实现高压缩比和高保真重建。


<details>
  <summary>Details</summary>
Motivation: 现有视频自动编码方法未能高效建模动态中的时空冗余，导致压缩效果不佳和下游任务训练成本过高。

Method: Hi-VAE将视频动态分解为全局运动和详细运动两个潜在空间，使用自监督运动编码器压缩视频潜在表示，并通过条件扩散解码器重建视频。

Result: 实验显示Hi-VAE压缩比达1428倍，远超基线方法（如Cosmos-VAE的48倍），同时保持高重建质量和下游生成任务性能。

Conclusion: Hi-VAE在高效压缩和高质量重建方面表现优异，具有可解释性和可扩展性，为视频潜在表示和生成提供了新视角。

Abstract: Recent breakthroughs in video autoencoders (Video AEs) have advanced video
generation, but existing methods fail to efficiently model spatio-temporal
redundancies in dynamics, resulting in suboptimal compression factors. This
shortfall leads to excessive training costs for downstream tasks. To address
this, we introduce Hi-VAE, an efficient video autoencoding framework that
hierarchically encode coarse-to-fine motion representations of video dynamics
and formulate the decoding process as a conditional generation task.
Specifically, Hi-VAE decomposes video dynamics into two latent spaces: Global
Motion, capturing overarching motion patterns, and Detailed Motion, encoding
high-frequency spatial details. Using separate self-supervised motion encoders,
we compress video latents into compact motion representations to reduce
redundancy significantly. A conditional diffusion decoder then reconstructs
videos by combining hierarchical global and detailed motions, enabling
high-fidelity video reconstructions. Extensive experiments demonstrate that
Hi-VAE achieves a high compression factor of 1428$\times$, almost 30$\times$
higher than baseline methods (e.g., Cosmos-VAE at 48$\times$), validating the
efficiency of our approach. Meanwhile, Hi-VAE maintains high reconstruction
quality at such high compression rates and performs effectively in downstream
generative tasks. Moreover, Hi-VAE exhibits interpretability and scalability,
providing new perspectives for future exploration in video latent
representation and generation.

</details>


### [73] [Learning Compact Vision Tokens for Efficient Large Multimodal Models](https://arxiv.org/abs/2506.07138)
*Hao Tang,Chengchao Shen*

Main category: cs.CV

TL;DR: 论文提出了一种空间令牌融合（STF）方法和多块令牌融合（MBTF）模块，以减少视觉令牌序列长度并提升推理效率，同时保持多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）因大型语言模型（LLMs）的高成本和长视觉令牌序列的二次复杂度而面临计算挑战。

Method: 提出STF方法融合空间相邻令牌以减少序列长度，并引入MBTF模块补充多粒度特征。

Result: 在8个流行视觉语言基准测试中，仅使用基线25%的视觉令牌，性能与基线相当或更优。

Conclusion: STF和MBTF模块能平衡令牌减少和信息保留，提升推理效率而不牺牲多模态推理能力。

Abstract: Large multimodal models (LMMs) suffer significant computational challenges
due to the high cost of Large Language Models (LLMs) and the quadratic
complexity of processing long vision token sequences. In this paper, we explore
the spatial redundancy among vision tokens and shorten the length of vision
token sequences for inference acceleration. Specifically, we propose a Spatial
Token Fusion (STF) method to learn compact vision tokens for short vision token
sequence, where spatial-adjacent tokens are fused into one. Meanwhile,
weight-frozen vision encoder can not well adapt to the demand of extensive
downstream vision-language tasks. To this end, we further introduce a
Multi-Block Token Fusion (MBTF) module to supplement multi-granularity features
for the reduced token sequence. Overall, we combine STF and MBTF module to
balance token reduction and information preservation, thereby improving
inference efficiency without sacrificing multimodal reasoning capabilities.
Experimental results demonstrate that our method based on LLaVA-1.5 achieves
comparable or even superior performance to the baseline on 8 popular
vision-language benchmarks with only $25\%$ vision tokens of baseline. The
source code and trained weights are available at
https://github.com/visresearch/LLaVA-STF.

</details>


### [74] [GoTrack: Generic 6DoF Object Pose Refinement and Tracking](https://arxiv.org/abs/2506.07155)
*Van Nguyen Nguyen,Christian Forster,Sindi Shkodrani,Vincent Lepetit,Bugra Tekin,Cem Keskin,Tomas Hodan*

Main category: cs.CV

TL;DR: GoTrack是一种基于CAD的高效6DoF物体姿态优化与跟踪方法，无需对象特定训练。通过结合模型到帧和帧到帧的注册，利用光流估计实现稳定跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有跟踪方法仅依赖模型到帧注册，计算成本高且不稳定。GoTrack通过整合帧到帧注册，优化计算效率并提升跟踪稳定性。

Method: 使用标准神经网络块（基于DINOv2的Transformer）实现模型到帧注册，并采用轻量级光流模型处理帧到帧注册。

Result: GoTrack与现有粗姿态估计方法结合，在标准6DoF姿态估计和跟踪基准测试中达到RGB-only的先进水平。

Conclusion: GoTrack提供了一种高效、稳定的6DoF物体姿态跟踪解决方案，适用于多样化对象，且无需特定训练。

Abstract: We introduce GoTrack, an efficient and accurate CAD-based method for 6DoF
object pose refinement and tracking, which can handle diverse objects without
any object-specific training. Unlike existing tracking methods that rely solely
on an analysis-by-synthesis approach for model-to-frame registration, GoTrack
additionally integrates frame-to-frame registration, which saves compute and
stabilizes tracking. Both types of registration are realized by optical flow
estimation. The model-to-frame registration is noticeably simpler than in
existing methods, relying only on standard neural network blocks (a transformer
is trained on top of DINOv2) and producing reliable pose confidence scores
without a scoring network. For the frame-to-frame registration, which is an
easier problem as consecutive video frames are typically nearly identical, we
employ a light off-the-shelf optical flow model. We demonstrate that GoTrack
can be seamlessly combined with existing coarse pose estimation methods to
create a minimal pipeline that reaches state-of-the-art RGB-only results on
standard benchmarks for 6DoF object pose estimation and tracking. Our source
code and trained models are publicly available at
https://github.com/facebookresearch/gotrack

</details>


### [75] [Faster than Fast: Accelerating Oriented FAST Feature Detection on Low-end Embedded GPUs](https://arxiv.org/abs/2506.07164)
*Qiong Chang,Xinyuan Chen,Xiang Li,Weimin Wang,Jun Miyazaki*

Main category: cs.CV

TL;DR: 论文提出两种方法加速低端嵌入式GPU上的Oriented FAST特征检测，通过二进制编码和可分离Harris检测策略，实现了7.3倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 当前基于ORB的SLAM系统在移动平台上难以满足实时处理需求，主要因Oriented FAST计算耗时。

Method: 优化FAST特征点检测和Harris角点检测，采用二进制编码策略和可分离Harris检测策略。

Result: 在Jetson TX2上实现平均7.3倍速度提升。

Conclusion: 方法显著提升处理速度，适用于移动和资源受限环境。

Abstract: The visual-based SLAM (Simultaneous Localization and Mapping) is a technology
widely used in applications such as robotic navigation and virtual reality,
which primarily focuses on detecting feature points from visual images to
construct an unknown environmental map and simultaneously determines its own
location. It usually imposes stringent requirements on hardware power
consumption, processing speed and accuracy. Currently, the ORB (Oriented FAST
and Rotated BRIEF)-based SLAM systems have exhibited superior performance in
terms of processing speed and robustness. However, they still fall short of
meeting the demands for real-time processing on mobile platforms. This
limitation is primarily due to the time-consuming Oriented FAST calculations
accounting for approximately half of the entire SLAM system. This paper
presents two methods to accelerate the Oriented FAST feature detection on
low-end embedded GPUs. These methods optimize the most time-consuming steps in
Oriented FAST feature detection: FAST feature point detection and Harris corner
detection, which is achieved by implementing a binary-level encoding strategy
to determine candidate points quickly and a separable Harris detection strategy
with efficient low-level GPU hardware-specific instructions. Extensive
experiments on a Jetson TX2 embedded GPU demonstrate an average speedup of over
7.3 times compared to widely used OpenCV with GPU support. This significant
improvement highlights its effectiveness and potential for real-time
applications in mobile and resource-constrained environments.

</details>


### [76] [Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Models](https://arxiv.org/abs/2506.07177)
*Sangwon Jang,Taekyung Ki,Jaehyeong Jo,Jaehong Yoon,Soo Ye Kim,Zhe Lin,Sung Ju Hwang*

Main category: cs.CV

TL;DR: 提出了一种无需训练的帧级信号引导方法Frame Guidance，用于可控视频生成，显著减少内存使用并支持多样化任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖对大模型进行微调，随着模型规模增长变得不切实际，因此需要一种无需训练的可控视频生成方法。

Method: 提出Frame Guidance，基于帧级信号（如关键帧、风格参考图等），采用简单的潜在处理方法和全局一致的潜在优化策略。

Result: 实验表明，Frame Guidance能生成高质量可控视频，适用于多种任务和输入信号。

Conclusion: Frame Guidance是一种高效、无需训练的可控视频生成方法，兼容性强且实用。

Abstract: Advancements in diffusion models have significantly improved video quality,
directing attention to fine-grained controllability. However, many existing
methods depend on fine-tuning large-scale video models for specific tasks,
which becomes increasingly impractical as model sizes continue to grow. In this
work, we present Frame Guidance, a training-free guidance for controllable
video generation based on frame-level signals, such as keyframes, style
reference images, sketches, or depth maps. For practical training-free
guidance, we propose a simple latent processing method that dramatically
reduces memory usage, and apply a novel latent optimization strategy designed
for globally coherent video generation. Frame Guidance enables effective
control across diverse tasks, including keyframe guidance, stylization, and
looping, without any training, compatible with any video models. Experimental
results show that Frame Guidance can produce high-quality controlled videos for
a wide range of tasks and input signals.

</details>


### [77] [Hierarchical Feature-level Reverse Propagation for Post-Training Neural Networks](https://arxiv.org/abs/2506.07188)
*Ni Ding,Lei He,Shengbo Eben Li,Keqiang Li*

Main category: cs.CV

TL;DR: 本文提出了一种分层解耦的后训练框架，通过重构中间特征图引入代理监督信号，提升模型透明度和训练灵活性。


<details>
  <summary>Details</summary>
Motivation: 端到端自动驾驶的黑盒模型在可解释性和安全性方面存在挑战，需要改进透明度和训练灵活性。

Method: 利用真实标签重构中间特征图，在过渡层引入代理监督信号，独立训练组件，避免传统端到端反向传播的复杂性。

Result: 在多个标准图像分类基准上，该方法表现出优异的泛化性能和计算效率。

Conclusion: 该方法为特征级反向计算提供了新颖且高效的训练范式，验证了其有效性和潜力。

Abstract: End-to-end autonomous driving has emerged as a dominant paradigm, yet its
highly entangled black-box models pose significant challenges in terms of
interpretability and safety assurance. To improve model transparency and
training flexibility, this paper proposes a hierarchical and decoupled
post-training framework tailored for pretrained neural networks. By
reconstructing intermediate feature maps from ground-truth labels, surrogate
supervisory signals are introduced at transitional layers to enable independent
training of specific components, thereby avoiding the complexity and coupling
of conventional end-to-end backpropagation and providing interpretable insights
into networks' internal mechanisms. To the best of our knowledge, this is the
first method to formalize feature-level reverse computation as well-posed
optimization problems, which we rigorously reformulate as systems of linear
equations or least squares problems. This establishes a novel and efficient
training paradigm that extends gradient backpropagation to feature
backpropagation. Extensive experiments on multiple standard image
classification benchmarks demonstrate that the proposed method achieves
superior generalization performance and computational efficiency compared to
traditional training approaches, validating its effectiveness and potential.

</details>


### [78] [SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning](https://arxiv.org/abs/2506.07196)
*Mengya Xu,Zhongzhen Huang,Dillan Imans,Yiru Ye,Xiaofan Zhang,Qi Dou*

Main category: cs.CV

TL;DR: SAP-Bench是一个用于评估多模态大语言模型（MLLM）在手术动作规划任务中性能的大规模高质量数据集，填补了现有基准的不足。


<details>
  <summary>Details</summary>
Motivation: 当前基准无法充分评估手术决策中的复杂性和可靠性，尤其是在生命关键领域。

Method: 提出SAP-Bench数据集，包含1226个临床验证的手术动作片段，并开发MLLM-SAP框架，利用MLLM生成下一步动作建议。

Result: 评估了7种先进MLLM模型，揭示了它们在下一步动作预测性能上的关键差距。

Conclusion: SAP-Bench为手术动作规划任务提供了有效的评估工具，并展示了当前模型的局限性。

Abstract: Effective evaluation is critical for driving advancements in MLLM research.
The surgical action planning (SAP) task, which aims to generate future action
sequences from visual inputs, demands precise and sophisticated analytical
capabilities. Unlike mathematical reasoning, surgical decision-making operates
in life-critical domains and requires meticulous, verifiable processes to
ensure reliability and patient safety. This task demands the ability to
distinguish between atomic visual actions and coordinate complex, long-horizon
procedures, capabilities that are inadequately evaluated by current benchmarks.
To address this gap, we introduce SAP-Bench, a large-scale, high-quality
dataset designed to enable multimodal large language models (MLLMs) to perform
interpretable surgical action planning. Our SAP-Bench benchmark, derived from
the cholecystectomy procedures context with the mean duration of 1137.5s, and
introduces temporally-grounded surgical action annotations, comprising the
1,226 clinically validated action clips (mean duration: 68.7s) capturing five
fundamental surgical actions across 74 procedures. The dataset provides 1,152
strategically sampled current frames, each paired with the corresponding next
action as multimodal analysis anchors. We propose the MLLM-SAP framework that
leverages MLLMs to generate next action recommendations from the current
surgical scene and natural language instructions, enhanced with injected
surgical domain knowledge. To assess our dataset's effectiveness and the
broader capabilities of current models, we evaluate seven state-of-the-art
MLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5,
Step-1o, and GLM-4v) and reveal critical gaps in next action prediction
performance.

</details>


### [79] [TV-LiVE: Training-Free, Text-Guided Video Editing via Layer Informed Vitality Exploitation](https://arxiv.org/abs/2506.07205)
*Min-Jung Kim,Dongjin Kim,Seokju Yun,Jaegul Choo*

Main category: cs.CV

TL;DR: TV-LiVE是一种无需训练、基于文本引导的视频编辑框架，通过利用关键层信息实现复杂编辑任务，如添加新对象和非刚性变换。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型在视频生成中的快速发展，对更易用和可控的视频编辑方法的需求增加，尤其是复杂任务如添加新对象和非刚性变换。

Method: 通过识别视频生成模型中的关键层（与RoPE相关），选择性注入源模型的关键特征到目标模型，并提取掩码区域以实现对象添加。

Result: 实验表明，TV-LiVE在对象添加和非刚性视频编辑任务上优于现有方法。

Conclusion: TV-LiVE提供了一种高效且无需训练的视频编辑解决方案，适用于复杂编辑任务。

Abstract: Video editing has garnered increasing attention alongside the rapid progress
of diffusion-based video generation models. As part of these advancements,
there is a growing demand for more accessible and controllable forms of video
editing, such as prompt-based editing. Previous studies have primarily focused
on tasks such as style transfer, background replacement, object substitution,
and attribute modification, while maintaining the content structure of the
source video. However, more complex tasks, including the addition of novel
objects and nonrigid transformations, remain relatively unexplored. In this
paper, we present TV-LiVE, a Training-free and text-guided Video editing
framework via Layerinformed Vitality Exploitation. We empirically identify
vital layers within the video generation model that significantly influence the
quality of generated outputs. Notably, these layers are closely associated with
Rotary Position Embeddings (RoPE). Based on this observation, our method
enables both object addition and non-rigid video editing by selectively
injecting key and value features from the source model into the corresponding
layers of the target model guided by the layer vitality. For object addition,
we further identify prominent layers to extract the mask regions corresponding
to the newly added target prompt. We found that the extracted masks from the
prominent layers faithfully indicate the region to be edited. Experimental
results demonstrate that TV-LiVE outperforms existing approaches for both
object addition and non-rigid video editing. Project Page:
https://emjay73.github.io/TV_LiVE/

</details>


### [80] [Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation](https://arxiv.org/abs/2506.07214)
*Zhiyuan Zhong,Zhen Sun,Yepang Liu,Xinlei He,Guanhong Tao*

Main category: cs.CV

TL;DR: 论文提出了一种针对视觉语言模型（VLMs）的新型后门攻击方法BadSem，利用跨模态语义不匹配作为隐式触发器，攻击成功率高且难以防御。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击主要依赖单模态触发器，未充分探索VLMs的跨模态融合特性，因此研究跨模态语义漏洞具有重要意义。

Method: 通过数据投毒，故意在训练时错配图像-文本对，构建SIMBad数据集进行语义操纵（如颜色和物体属性）。

Result: 在四种广泛使用的VLMs上，BadSem平均攻击成功率（ASR）超过98%，且能泛化到分布外数据集和跨模态传递。

Conclusion: 语义后门攻击难以通过现有防御策略（如系统提示和监督微调）缓解，凸显了VLMs语义安全问题的紧迫性。

Abstract: Vision Language Models (VLMs) have shown remarkable performance, but are also
vulnerable to backdoor attacks whereby the adversary can manipulate the model's
outputs through hidden triggers. Prior attacks primarily rely on
single-modality triggers, leaving the crucial cross-modal fusion nature of VLMs
largely unexplored. Unlike prior work, we identify a novel attack surface that
leverages cross-modal semantic mismatches as implicit triggers. Based on this
insight, we propose BadSem (Backdoor Attack with Semantic Manipulation), a data
poisoning attack that injects stealthy backdoors by deliberately misaligning
image-text pairs during training. To perform the attack, we construct SIMBad, a
dataset tailored for semantic manipulation involving color and object
attributes. Extensive experiments across four widely used VLMs show that BadSem
achieves over 98% average ASR, generalizes well to out-of-distribution
datasets, and can transfer across poisoning modalities. Our detailed analysis
using attention visualization shows that backdoored models focus on
semantically sensitive regions under mismatched conditions while maintaining
normal behavior on clean inputs. To mitigate the attack, we try two defense
strategies based on system prompt and supervised fine-tuning but find that both
of them fail to mitigate the semantic backdoor. Our findings highlight the
urgent need to address semantic vulnerabilities in VLMs for their safer
deployment.

</details>


### [81] [AugmentGest: Can Random Data Cropping Augmentation Boost Gesture Recognition Performance?](https://arxiv.org/abs/2506.07216)
*Nada Aboudeshish,Dmitry Ignatov,Radu Timofte*

Main category: cs.CV

TL;DR: 本文提出了一种综合数据增强框架，通过几何变换、随机裁剪、旋转、缩放和强度变换等方法，显著提升了骨架数据集的多样性和模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决骨架数据集中数据多样性不足的问题，提升模型在真实场景中的泛化能力和鲁棒性。

Method: 集成多种数据增强技术（几何变换、随机裁剪、旋转、缩放、亮度对比调整等），为每个样本生成三个增强版本，扩充数据集并丰富手势表示。

Result: 在多个基准数据集（DHG14/28、SHREC'17、JHMDB）和模型（e2eET、FPPR-PCD、DD-Net）上验证了框架的有效性，显著提升了模型性能。

Conclusion: 该框架不仅实现了当前最佳性能，还为手势识别和动作识别应用提供了可扩展的解决方案。

Abstract: Data augmentation is a crucial technique in deep learning, particularly for
tasks with limited dataset diversity, such as skeleton-based datasets. This
paper proposes a comprehensive data augmentation framework that integrates
geometric transformations, random cropping, rotation, zooming and
intensity-based transformations, brightness and contrast adjustments to
simulate real-world variations. Random cropping ensures the preservation of
spatio-temporal integrity while addressing challenges such as viewpoint bias
and occlusions. The augmentation pipeline generates three augmented versions
for each sample in addition to the data set sample, thus quadrupling the data
set size and enriching the diversity of gesture representations. The proposed
augmentation strategy is evaluated on three models: multi-stream e2eET, FPPR
point cloud-based hand gesture recognition (HGR), and DD-Network. Experiments
are conducted on benchmark datasets including DHG14/28, SHREC'17, and JHMDB.
The e2eET model, recognized as the state-of-the-art for hand gesture
recognition on DHG14/28 and SHREC'17. The FPPR-PCD model, the second-best
performing model on SHREC'17, excels in point cloud-based gesture recognition.
DD-Net, a lightweight and efficient architecture for skeleton-based action
recognition, is evaluated on SHREC'17 and the Human Motion Data Base (JHMDB).
The results underline the effectiveness and versatility of the proposed
augmentation strategy, significantly improving model generalization and
robustness across diverse datasets and architectures. This framework not only
establishes state-of-the-art results on all three evaluated models but also
offers a scalable solution to advance HGR and action recognition applications
in real-world scenarios. The framework is available at
https://github.com/NadaAbodeshish/Random-Cropping-augmentation-HGR

</details>


### [82] [Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning](https://arxiv.org/abs/2506.07227)
*Tianyi Bai,Yuxuan Fan,Jiantao Qiu,Fupeng Sun,Jiayi Song,Junlin Han,Zichen Liu,Conghui He,Wentao Zhang,Binhang Yuan*

Main category: cs.CV

TL;DR: 论文提出了一种针对多模态大语言模型（MLLMs）在细粒度视觉差异任务中表现不佳的解决方案，通过生成微编辑数据集（MED）和引入特征级一致性损失的监督微调框架，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: MLLMs在视觉语言任务中表现优秀，但在细粒度视觉差异上存在幻觉或语义遗漏问题，主要源于训练数据和学习目标的局限性。

Method: 提出可控数据生成管道构建MED数据集（50K图像-文本对，涵盖11类细粒度编辑），并设计监督微调框架，加入特征级一致性损失。

Result: 在微编辑检测基准上，方法显著提升了差异检测准确率并减少幻觉，同时在标准视觉语言任务（如图像描述和视觉问答）中表现一致提升。

Conclusion: 结合针对性数据和目标对齐能有效增强MLLMs的细粒度视觉推理能力。

Abstract: Multimodal large language models (MLLMs) have achieved strong performance on
vision-language tasks but still struggle with fine-grained visual differences,
leading to hallucinations or missed semantic shifts. We attribute this to
limitations in both training data and learning objectives. To address these
issues, we propose a controlled data generation pipeline that produces
minimally edited image pairs with semantically aligned captions. Using this
pipeline, we construct the Micro Edit Dataset (MED), containing over 50K
image-text pairs spanning 11 fine-grained edit categories, including attribute,
count, position, and object presence changes. Building on MED, we introduce a
supervised fine-tuning (SFT) framework with a feature-level consistency loss
that promotes stable visual embeddings under small edits. We evaluate our
approach on the Micro Edit Detection benchmark, which includes carefully
balanced evaluation pairs designed to test sensitivity to subtle visual
variations across the same edit categories. Our method improves difference
detection accuracy and reduces hallucinations compared to strong baselines,
including GPT-4o. Moreover, it yields consistent gains on standard
vision-language tasks such as image captioning and visual question answering.
These results demonstrate the effectiveness of combining targeted data and
alignment objectives for enhancing fine-grained visual reasoning in MLLMs.

</details>


### [83] [Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification](https://arxiv.org/abs/2506.07235)
*Tianyi Bai,Zengjie Hu,Fupeng Sun,Jiantao Qiu,Yizhen Jiang,Guangxin He,Bohan Zeng,Conghui He,Binhang Yuan,Wentao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种动态推理框架，通过迭代验证机制改进多模态大语言模型（MLLMs）的视觉推理能力，显著提升了准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLMs采用静态推理范式，无法动态调整视觉理解，限制了其适应性和迭代优化能力。本文旨在模拟人类动态感知机制，提出一种更灵活的推理方法。

Method: 将问题建模为马尔可夫决策过程，结合提议视觉动作的推理器和基于多步直接偏好优化（DPO）训练的验证器，支持迭代推理。同时发布了VTS数据集。

Result: 该方法在多种视觉推理基准测试中显著优于现有方法，不仅提高了准确性，还增强了推理过程的可解释性和基础性。

Conclusion: 动态推理机制为下一代MLLMs实现细粒度、上下文感知的视觉推理提供了潜力。

Abstract: Multi-modal large language models (MLLMs) have achieved remarkable
capabilities by integrating visual perception with language understanding,
enabling applications such as image-grounded dialogue, visual question
answering, and scientific analysis. However, most MLLMs adopt a static
inference paradigm, encoding the entire image into fixed visual tokens upfront,
which limits their ability to iteratively refine understanding or adapt to
context during inference. This contrasts sharply with human perception, which
is dynamic, selective, and feedback-driven. In this work, we introduce a novel
framework for inference-time visual token scaling that enables MLLMs to perform
iterative, verifier-guided reasoning over visual content. We formulate the
problem as a Markov Decision Process, involving a reasoner that proposes visual
actions and a verifier, which is trained via multi-step Direct Preference
Optimization (DPO), that evaluates these actions and determines when reasoning
should terminate. To support this, we present a new dataset, VTS, comprising
supervised reasoning trajectories (VTS-SFT) and preference-labeled reasoning
comparisons (VTS-DPO). Our method significantly outperforms existing approaches
across diverse visual reasoning benchmarks, offering not only improved accuracy
but also more interpretable and grounded reasoning processes. These results
demonstrate the promise of dynamic inference mechanisms for enabling
fine-grained, context-aware visual reasoning in next-generation MLLMs.

</details>


### [84] [From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models](https://arxiv.org/abs/2506.07280)
*Pablo Acuaviva,Aram Davtyan,Mariam Hassan,Sebastian Stapf,Ahmad Rahimi,Alexandre Alahi,Paolo Favaro*

Main category: cs.CV

TL;DR: VDMs不仅是视频生成工具，还能通过少量样本微调适应新任务，展现为通用视觉学习器。


<details>
  <summary>Details</summary>
Motivation: 探讨VDMs在训练过程中是否内化了视觉世界的结构化表示，并验证其作为通用视觉学习器的潜力。

Method: 提出一个少样本微调框架，将任务转化为视觉转换，通过LoRA权重训练冻结的VDM。

Result: 模型在低层和高层视觉任务中表现出强泛化能力。

Conclusion: VDMs可视为未来视觉基础模型的核心组件。

Abstract: Video Diffusion Models (VDMs) have emerged as powerful generative tools,
capable of synthesizing high-quality spatiotemporal content. Yet, their
potential goes far beyond mere video generation. We argue that the training
dynamics of VDMs, driven by the need to model coherent sequences, naturally
pushes them to internalize structured representations and an implicit
understanding of the visual world. To probe the extent of this internal
knowledge, we introduce a few-shot fine-tuning framework that repurposes VDMs
for new tasks using only a handful of examples. Our method transforms each task
into a visual transition, enabling the training of LoRA weights on short
input-output sequences without altering the generative interface of a frozen
VDM. Despite minimal supervision, the model exhibits strong generalization
across diverse tasks, from low-level vision (for example, segmentation and pose
estimation) to high-level reasoning (for example, on ARC-AGI). These results
reframe VDMs as more than generative engines. They are adaptable visual
learners with the potential to serve as the backbone for future foundation
models in vision.

</details>


### [85] [Multi-Step Guided Diffusion for Image Restoration on Edge Devices: Toward Lightweight Perception in Embodied AI](https://arxiv.org/abs/2506.07286)
*Aditya Chakravarty*

Main category: cs.CV

TL;DR: 论文提出了一种在去噪步骤中采用多步优化的策略，显著提升了图像质量、感知准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如MPGD）在去噪步骤中仅应用单次梯度更新，限制了恢复的保真度和鲁棒性，尤其是在嵌入式或分布外场景中。

Method: 在每次去噪时间步中引入多步优化策略，增加梯度更新次数。

Result: 实验表明，该方法在超分辨率和高斯去模糊任务中提升了LPIPS和PSNR指标，且在Jetson Orin Nano上验证了其泛化能力。

Conclusion: MPGD具有作为轻量级、即插即用恢复模块的潜力，适用于无人机和移动机器人等实时视觉感知任务。

Abstract: Diffusion models have shown remarkable flexibility for solving inverse
problems without task-specific retraining. However, existing approaches such as
Manifold Preserving Guided Diffusion (MPGD) apply only a single gradient update
per denoising step, limiting restoration fidelity and robustness, especially in
embedded or out-of-distribution settings. In this work, we introduce a
multistep optimization strategy within each denoising timestep, significantly
enhancing image quality, perceptual accuracy, and generalization. Our
experiments on super-resolution and Gaussian deblurring demonstrate that
increasing the number of gradient updates per step improves LPIPS and PSNR with
minimal latency overhead. Notably, we validate this approach on a Jetson Orin
Nano using degraded ImageNet and a UAV dataset, showing that MPGD, originally
trained on face datasets, generalizes effectively to natural and aerial scenes.
Our findings highlight MPGD's potential as a lightweight, plug-and-play
restoration module for real-time visual perception in embodied AI agents such
as drones and mobile robots.

</details>


### [86] [FANVID: A Benchmark for Face and License Plate Recognition in Low-Resolution Videos](https://arxiv.org/abs/2506.07304)
*Kavitha Viswanathan,Vrinda Goel,Shlesh Gholap,Devayan Ghosh,Madhav Gupta,Dhruvi Ganatra,Sanket Potdar,Amit Sethi*

Main category: cs.CV

TL;DR: FANVID是一个新的视频基准数据集，包含低分辨率（LR）视频片段，用于提升时间识别模型，支持人脸匹配和车牌识别任务。


<details>
  <summary>Details</summary>
Motivation: 现实监控中低分辨率帧难以识别，需利用时间信息提升识别能力。

Method: 数据集包含1,463个LR视频片段，定义人脸匹配和车牌识别任务，并引入评估指标。

Result: 基线方法在两项任务中分别得分为0.58和0.42，显示任务的可行性与挑战性。

Conclusion: FANVID旨在推动低分辨率时间建模的创新，适用于监控、法医和自动驾驶等领域。

Abstract: Real-world surveillance often renders faces and license plates unrecognizable
in individual low-resolution (LR) frames, hindering reliable identification. To
advance temporal recognition models, we present FANVID, a novel video-based
benchmark comprising nearly 1,463 LR clips (180 x 320, 20--60 FPS) featuring 63
identities and 49 license plates from three English-speaking countries. Each
video includes distractor faces and plates, increasing task difficulty and
realism. The dataset contains 31,096 manually verified bounding boxes and
labels.
  FANVID defines two tasks: (1) face matching -- detecting LR faces and
matching them to high-resolution mugshots, and (2) license plate recognition --
extracting text from LR plates without a predefined database. Videos are
downsampled from high-resolution sources to ensure that faces and text are
indecipherable in single frames, requiring models to exploit temporal
information. We introduce evaluation metrics adapted from mean Average
Precision at IoU > 0.5, prioritizing identity correctness for faces and
character-level accuracy for text.
  A baseline method with pre-trained video super-resolution, detection, and
recognition achieved performance scores of 0.58 (face matching) and 0.42 (plate
recognition), highlighting both the feasibility and challenge of the tasks.
FANVID's selection of faces and plates balances diversity with recognition
challenge. We release the software for data access, evaluation, baseline, and
annotation to support reproducibility and extension. FANVID aims to catalyze
innovation in temporal modeling for LR recognition, with applications in
surveillance, forensics, and autonomous vehicles.

</details>


### [87] [AllTracker: Efficient Dense Point Tracking at High Resolution](https://arxiv.org/abs/2506.07310)
*Adam W. Harley,Yang You,Xinglong Sun,Yang Zheng,Nikhil Raghuraman,Yunqi Gu,Sheldon Liang,Wen-Hsuan Chu,Achal Dave,Pavel Tokmakov,Suya You,Rares Ambrus,Katerina Fragkiadaki,Leonidas J. Guibas*

Main category: cs.CV

TL;DR: AllTracker是一种通过估计查询帧与视频中每一帧之间的流场来估计长距离点轨迹的模型，提供高分辨率和密集（全像素）对应字段。


<details>
  <summary>Details</summary>
Motivation: 现有点跟踪方法无法提供高分辨率和密集对应字段，而现有光流方法仅能对应下一帧，无法扩展到数百帧。

Method: 结合光流和点跟踪技术，设计新架构，通过低分辨率网格迭代推理，利用2D卷积层空间传播信息，像素对齐注意力层时间传播信息。

Result: 模型高效（1600万参数），在高分辨率（768x1024像素）下实现最先进的点跟踪精度，支持多数据集训练。

Conclusion: 通过广泛消融研究验证架构和训练方法的关键细节，代码和模型权重已开源。

Abstract: We introduce AllTracker: a model that estimates long-range point tracks by
way of estimating the flow field between a query frame and every other frame of
a video. Unlike existing point tracking methods, our approach delivers
high-resolution and dense (all-pixel) correspondence fields, which can be
visualized as flow maps. Unlike existing optical flow methods, our approach
corresponds one frame to hundreds of subsequent frames, rather than just the
next frame. We develop a new architecture for this task, blending techniques
from existing work in optical flow and point tracking: the model performs
iterative inference on low-resolution grids of correspondence estimates,
propagating information spatially via 2D convolution layers, and propagating
information temporally via pixel-aligned attention layers. The model is fast
and parameter-efficient (16 million parameters), and delivers state-of-the-art
point tracking accuracy at high resolution (i.e., tracking 768x1024 pixels, on
a 40G GPU). A benefit of our design is that we can train on a wider set of
datasets, and we find that doing so is crucial for top performance. We provide
an extensive ablation study on our architecture details and training recipe,
making it clear which details matter most. Our code and model weights are
available at https://alltracker.github.io .

</details>


### [88] ["CASE: Contrastive Activation for Saliency Estimation](https://arxiv.org/abs/2506.07327)
*Dane Williamson,Yangfeng Ji,Matthew Dwyer*

Main category: cs.CV

TL;DR: 本文提出了一种诊断测试（class sensitivity）来评估显著性方法区分不同类别标签的能力，发现许多方法对类别不敏感，并提出了对比性解释方法CASE以提高解释的忠实性和类别特异性。


<details>
  <summary>Details</summary>
Motivation: 显著性方法在可视化模型预测时可能存在局限性，尤其是对类别标签的区分能力不足。本文旨在揭示这一问题并提出改进方法。

Method: 通过诊断测试评估显著性方法的类别敏感性，发现其局限性后，提出对比性解释方法CASE，并通过实验验证其有效性。

Result: 实验表明，许多显著性方法对类别不敏感，而CASE能够生成更忠实且类别特异性的解释。

Conclusion: CASE方法在解释忠实性和类别特异性上优于现有方法，揭示了显著性方法的结构性缺陷。

Abstract: Saliency methods are widely used to visualize which input features are deemed
relevant to a model's prediction. However, their visual plausibility can
obscure critical limitations. In this work, we propose a diagnostic test for
class sensitivity: a method's ability to distinguish between competing class
labels on the same input. Through extensive experiments, we show that many
widely used saliency methods produce nearly identical explanations regardless
of the class label, calling into question their reliability. We find that
class-insensitive behavior persists across architectures and datasets,
suggesting the failure mode is structural rather than model-specific. Motivated
by these findings, we introduce CASE, a contrastive explanation method that
isolates features uniquely discriminative for the predicted class. We evaluate
CASE using the proposed diagnostic and a perturbation-based fidelity test, and
show that it produces faithful and more class-specific explanations than
existing methods.

</details>


### [89] [Hierarchical Scoring with 3D Gaussian Splatting for Instance Image-Goal Navigation](https://arxiv.org/abs/2506.07338)
*Yijie Deng,Shuaihang Yuan,Geeta Chandra Raju Bethala,Anthony Tzes,Yu-Shen Liu,Yi Fang*

Main category: cs.CV

TL;DR: 本文提出了一种基于分层评分范式的实例图像目标导航（IIN）框架，通过优化视点选择减少冗余，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖随机采样视点或轨迹，导致冗余和效率低下，缺乏优化的视点选择机制。

Method: 结合跨层级语义评分（基于CLIP的相似性）和局部几何评分（精确位姿估计），选择最优视点进行目标匹配。

Result: 在模拟IIN基准测试中达到最优性能，并展示实际应用潜力。

Conclusion: 提出的分层评分范式显著提升了IIN任务的效率和准确性。

Abstract: Instance Image-Goal Navigation (IIN) requires autonomous agents to identify
and navigate to a target object or location depicted in a reference image
captured from any viewpoint. While recent methods leverage powerful novel view
synthesis (NVS) techniques, such as three-dimensional Gaussian splatting
(3DGS), they typically rely on randomly sampling multiple viewpoints or
trajectories to ensure comprehensive coverage of discriminative visual cues.
This approach, however, creates significant redundancy through overlapping
image samples and lacks principled view selection, substantially increasing
both rendering and comparison overhead. In this paper, we introduce a novel IIN
framework with a hierarchical scoring paradigm that estimates optimal
viewpoints for target matching. Our approach integrates cross-level semantic
scoring, utilizing CLIP-derived relevancy fields to identify regions with high
semantic similarity to the target object class, with fine-grained local
geometric scoring that performs precise pose estimation within promising
regions. Extensive evaluations demonstrate that our method achieves
state-of-the-art performance on simulated IIN benchmarks and real-world
applicability.

</details>


### [90] [CBAM-STN-TPS-YOLO: Enhancing Agricultural Object Detection through Spatially Adaptive Attention Mechanisms](https://arxiv.org/abs/2506.07357)
*Satvik Praveen,Yoonsung Jung*

Main category: cs.CV

TL;DR: CBAM-STN-TPS-YOLO模型结合了TPS和CBAM，提升了植物检测中对遮挡和非刚性变形的处理能力，显著提高了检测精度。


<details>
  <summary>Details</summary>
Motivation: 在精准农业中，目标检测对植物监测至关重要，但现有模型如YOLO在遮挡和非刚性变形下表现不佳。

Method: 提出CBAM-STN-TPS-YOLO模型，通过TPS实现非刚性空间变换，结合CBAM抑制背景噪声并突出关键特征。

Result: 在PGP数据集上，模型在精度、召回率和mAP上优于STN-YOLO，假阳性减少12%。

Conclusion: 该模型轻量且适合实时边缘部署，是智能农业中高效监测的理想选择。

Abstract: Object detection is vital in precision agriculture for plant monitoring,
disease detection, and yield estimation. However, models like YOLO struggle
with occlusions, irregular structures, and background noise, reducing detection
accuracy. While Spatial Transformer Networks (STNs) improve spatial invariance
through learned transformations, affine mappings are insufficient for non-rigid
deformations such as bent leaves and overlaps.
  We propose CBAM-STN-TPS-YOLO, a model integrating Thin-Plate Splines (TPS)
into STNs for flexible, non-rigid spatial transformations that better align
features. Performance is further enhanced by the Convolutional Block Attention
Module (CBAM), which suppresses background noise and emphasizes relevant
spatial and channel-wise features.
  On the occlusion-heavy Plant Growth and Phenotyping (PGP) dataset, our model
outperforms STN-YOLO in precision, recall, and mAP. It achieves a 12% reduction
in false positives, highlighting the benefits of improved spatial flexibility
and attention-guided refinement. We also examine the impact of the TPS
regularization parameter in balancing transformation smoothness and detection
performance.
  This lightweight model improves spatial awareness and supports real-time edge
deployment, making it ideal for smart farming applications requiring accurate
and efficient monitoring.

</details>


### [91] [Multiple Object Stitching for Unsupervised Representation Learning](https://arxiv.org/abs/2506.07364)
*Chengchao Shen,Dawei Liu,Jianxin Wang*

Main category: cs.CV

TL;DR: 论文提出了一种名为MOS的方法，通过拼接单目标图像生成多目标图像，优化无监督表示学习，提升多目标图像的表现。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法在多目标图像上表现不佳，需要改进无监督表示学习。

Method: 通过拼接单目标图像生成多目标图像，利用预定义的对象对应关系优化表示学习。

Result: 在ImageNet、CIFAR和COCO数据集上表现领先，适用于单目标和多目标图像。

Conclusion: MOS方法有效提升了无监督表示学习的性能，适用于复杂下游任务。

Abstract: Contrastive learning for single object centric images has achieved remarkable
progress on unsupervised representation, but suffering inferior performance on
the widespread images with multiple objects. In this paper, we propose a simple
but effective method, Multiple Object Stitching (MOS), to refine the
unsupervised representation for multi-object images. Specifically, we construct
the multi-object images by stitching the single object centric ones, where the
objects in the synthesized multi-object images are predetermined. Hence,
compared to the existing contrastive methods, our method provides additional
object correspondences between multi-object images without human annotations.
In this manner, our method pays more attention to the representations of each
object in multi-object image, thus providing more detailed representations for
complicated downstream tasks, such as object detection and semantic
segmentation. Experimental results on ImageNet, CIFAR and COCO datasets
demonstrate that our proposed method achieves the leading unsupervised
representation performance on both single object centric images and
multi-object ones. The source code is available at
https://github.com/visresearch/MultipleObjectStitching.

</details>


### [92] [C3S3: Complementary Competition and Contrastive Selection for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.07368)
*Jiaying He,Yitong Lin,Jiahe Chen,Honghui Xu,Jianwei Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种名为C3S3的半监督医学图像分割模型，通过结合互补竞争和对比选择，显著提升了边界细节的捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 医学领域标注样本不足，现有方法在边界细节捕捉上表现不佳，导致诊断误差。

Method: C3S3包含结果驱动的对比学习模块和动态互补竞争模块，利用两个高性能子网络生成伪标签。

Result: 在两个公开数据集（MRI和CT）上验证，性能优于现有方法，95HD和ASD指标提升至少6%。

Conclusion: C3S3在医学图像分割中表现出色，尤其在边界细节处理上有显著改进。

Abstract: For the immanent challenge of insufficiently annotated samples in the medical
field, semi-supervised medical image segmentation (SSMIS) offers a promising
solution. Despite achieving impressive results in delineating primary target
areas, most current methodologies struggle to precisely capture the subtle
details of boundaries. This deficiency often leads to significant diagnostic
inaccuracies. To tackle this issue, we introduce C3S3, a novel semi-supervised
segmentation model that synergistically integrates complementary competition
and contrastive selection. This design significantly sharpens boundary
delineation and enhances overall precision. Specifically, we develop an
$\textit{Outcome-Driven Contrastive Learning}$ module dedicated to refining
boundary localization. Additionally, we incorporate a $\textit{Dynamic
Complementary Competition}$ module that leverages two high-performing
sub-networks to generate pseudo-labels, thereby further improving segmentation
quality. The proposed C3S3 undergoes rigorous validation on two publicly
accessible datasets, encompassing the practices of both MRI and CT scans. The
results demonstrate that our method achieves superior performance compared to
previous cutting-edge competitors. Especially, on the 95HD and ASD metrics, our
approach achieves a notable improvement of at least $6\%$, highlighting the
significant advancements. The code is available at
https://github.com/Y-TARL/C3S3.

</details>


### [93] [Generative Models at the Frontier of Compression: A Survey on Generative Face Video Coding](https://arxiv.org/abs/2506.07369)
*Bolin Chen,Shanzhi Yin,Goluck Konuko,Giuseppe Valenzise,Zihan Zhang,Shiqi Wang,Yan Ye*

Main category: cs.CV

TL;DR: 该论文综述了生成式人脸视频编码（GFVC）技术，分析了其在高保真低码率视频通信中的优势，并提出了标准化和未来应用的展望。


<details>
  <summary>Details</summary>
Motivation: 随着深度生成模型的兴起，GFVC在视频压缩领域展现出巨大潜力，但其理论和工业标准化之间仍存在差距，本文旨在填补这一空白。

Method: 通过综述现有GFVC方法、构建大规模主观评分数据库、总结标准化潜力并开发低复杂度系统。

Result: GFVC在超低码率下表现优于最新视频编码标准VVC，并提出了适合GFVC的质量评估指标。

Conclusion: GFVC在工业应用中具有广阔前景，但仍需解决当前挑战以推动未来发展。

Abstract: The rise of deep generative models has greatly advanced video compression,
reshaping the paradigm of face video coding through their powerful capability
for semantic-aware representation and lifelike synthesis. Generative Face Video
Coding (GFVC) stands at the forefront of this revolution, which could
characterize complex facial dynamics into compact latent codes for bitstream
compactness at the encoder side and leverages powerful deep generative models
to reconstruct high-fidelity face signal from the compressed latent codes at
the decoder side. As such, this well-designed GFVC paradigm could enable
high-fidelity face video communication at ultra-low bitrate ranges, far
surpassing the capabilities of the latest Versatile Video Coding (VVC)
standard. To pioneer foundational research and accelerate the evolution of
GFVC, this paper presents the first comprehensive survey of GFVC technologies,
systematically bridging critical gaps between theoretical innovation and
industrial standardization. In particular, we first review a broad range of
existing GFVC methods with different feature representations and optimization
strategies, and conduct a thorough benchmarking analysis. In addition, we
construct a large-scale GFVC-compressed face video database with subjective
Mean Opinion Scores (MOSs) based on human perception, aiming to identify the
most appropriate quality metrics tailored to GFVC. Moreover, we summarize the
GFVC standardization potentials with a unified high-level syntax and develop a
low-complexity GFVC system which are both expected to push forward future
practical deployments and applications. Finally, we envision the potential of
GFVC in industrial applications and deliberate on the current challenges and
future opportunities.

</details>


### [94] [ARGUS: Hallucination and Omission Evaluation in Video-LLMs](https://arxiv.org/abs/2506.07371)
*Ruchit Rawal,Reza Shirkavand,Heng Huang,Gowthami Somepalli,Tom Goldstein*

Main category: cs.CV

TL;DR: ARGUS是一个新的VideoLLM基准测试，专注于自由文本生成任务（如视频字幕），通过量化幻觉率和遗漏率来全面评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的VideoLLM基准测试主要依赖选择题，无法有效评估自由文本生成任务中的幻觉问题，因此需要更全面的评估方法。

Method: 提出ARGUS基准测试，通过比较VideoLLM生成的自由文本字幕与人工标注的真实字幕，量化幻觉率（错误内容）和遗漏率（重要细节缺失）。

Result: ARGUS能够全面评估VideoLLM在视频字幕任务中的表现，尤其是幻觉和遗漏问题。

Conclusion: ARGUS为VideoLLM的自由文本生成任务提供了更全面的评估标准，有助于改进模型性能。

Abstract: Video large language models have not yet been widely deployed, largely due to
their tendency to hallucinate. Typical benchmarks for Video-LLMs rely simply on
multiple-choice questions. Unfortunately, VideoLLMs hallucinate far more
aggressively on freeform text generation tasks like video captioning than they
do on multiple choice verification tasks. To address this weakness, we propose
ARGUS, a VideoLLM benchmark that measures freeform video captioning
performance. By comparing VideoLLM outputs to human ground truth captions,
ARGUS quantifies dual metrics. First, we measure the rate of hallucinations in
the form of incorrect statements about video content or temporal relationships.
Second, we measure the rate at which the model omits important descriptive
details. Together, these dual metrics form a comprehensive view of video
captioning performance.

</details>


### [95] [DINO-CoDT: Multi-class Collaborative Detection and Tracking with Vision Foundation Models](https://arxiv.org/abs/2506.07375)
*Xunjie He,Christina Dao Wen Lee,Meiling Wang,Chengran Yuan,Zefan Huang,Yufeng Yue,Marcelo H. Ang Jr*

Main category: cs.CV

TL;DR: 提出了一种多类别协作检测与跟踪框架，通过全局空间注意力融合模块和视觉语义增强的REID模块，显著提升了检测与跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 现有协作感知研究主要针对车辆类别，缺乏对多类别对象的有效解决方案，限制了实际应用。

Method: 设计了全局空间注意力融合模块（GSAF）增强多尺度特征学习，引入视觉语义的REID模块减少ID切换错误，并开发了基于速度的自适应跟踪管理模块（VATM）。

Result: 在V2X-Real和OPV2V数据集上，方法显著优于现有最优方法。

Conclusion: 该框架为多类别协作感知提供了有效解决方案，提升了检测与跟踪的鲁棒性和准确性。

Abstract: Collaborative perception plays a crucial role in enhancing environmental
understanding by expanding the perceptual range and improving robustness
against sensor failures, which primarily involves collaborative 3D detection
and tracking tasks. The former focuses on object recognition in individual
frames, while the latter captures continuous instance tracklets over time.
However, existing works in both areas predominantly focus on the vehicle
superclass, lacking effective solutions for both multi-class collaborative
detection and tracking. This limitation hinders their applicability in
real-world scenarios, which involve diverse object classes with varying
appearances and motion patterns. To overcome these limitations, we propose a
multi-class collaborative detection and tracking framework tailored for diverse
road users. We first present a detector with a global spatial attention fusion
(GSAF) module, enhancing multi-scale feature learning for objects of varying
sizes. Next, we introduce a tracklet RE-IDentification (REID) module that
leverages visual semantics with a vision foundation model to effectively reduce
ID SWitch (IDSW) errors, in cases of erroneous mismatches involving small
objects like pedestrians. We further design a velocity-based adaptive tracklet
management (VATM) module that adjusts the tracking interval dynamically based
on object motion. Extensive experiments on the V2X-Real and OPV2V datasets show
that our approach significantly outperforms existing state-of-the-art methods
in both detection and tracking accuracy.

</details>


### [96] [Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation](https://arxiv.org/abs/2506.07376)
*Jintao Tong,Ran Ma,Yixiong Zou,Guangyao Chen,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: 论文提出了一种跨域少样本分割方法（CD-FSS），通过适配器解耦域信息，并设计了Domain Feature Navigator（DFN）和SAM-SVN方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决跨域少样本分割中的域差距和少样本微调问题。

Method: 提出DFN作为结构化解耦器，结合SAM-SVN防止过拟合，冻结模型并微调DFN。

Result: 在1-shot和5-shot场景下分别超越现有方法2.69%和4.68% MIoU。

Conclusion: DFN和SAM-SVN有效解耦域信息，提升跨域少样本分割性能。

Abstract: Cross-domain few-shot segmentation (CD-FSS) is proposed to pre-train the
model on a source-domain dataset with sufficient samples, and then transfer the
model to target-domain datasets where only a few samples are available for
efficient fine-tuning. There are majorly two challenges in this task: (1) the
domain gap and (2) fine-tuning with scarce data. To solve these challenges, we
revisit the adapter-based methods, and discover an intriguing insight not
explored in previous works: the adapter not only helps the fine-tuning of
downstream tasks but also naturally serves as a domain information decoupler.
Then, we delve into this finding for an interpretation, and find the model's
inherent structure could lead to a natural decoupling of domain information.
Building upon this insight, we propose the Domain Feature Navigator (DFN),
which is a structure-based decoupler instead of loss-based ones like current
works, to capture domain-specific information, thereby directing the model's
attention towards domain-agnostic knowledge. Moreover, to prevent the potential
excessive overfitting of DFN during the source-domain training, we further
design the SAM-SVN method to constrain DFN from learning sample-specific
knowledge. On target domains, we freeze the model and fine-tune the DFN to
learn target-specific knowledge specific. Extensive experiments demonstrate
that our method surpasses the state-of-the-art method in CD-FSS significantly
by 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios, respectively.

</details>


### [97] [MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems](https://arxiv.org/abs/2506.07399)
*Peiru Yang,Jinhua Yin,Haoran Zheng,Xueying Bai,Huili Wang,Yufei Sun,Xintian Li,Shangguang Wang,Yongfeng Huang,Tao Qi*

Main category: cs.CV

TL;DR: 论文提出了一种针对多模态RAG系统的黑盒成员推理攻击框架MrM，通过多目标数据扰动和反事实攻击，有效泄露敏感信息。


<details>
  <summary>Details</summary>
Motivation: 多模态RAG系统在增强视觉语言模型的同时，可能泄露敏感信息，而现有攻击方法主要关注文本模态，视觉模态研究不足。

Method: MrM采用多目标数据扰动框架，结合反事实攻击，通过对象感知扰动和掩码选择策略，提取成员信息。

Result: 在两个视觉数据集和八个主流视觉语言模型上，MrM在样本级和集合级评估中表现优异，且对自适应防御具有鲁棒性。

Conclusion: MrM填补了多模态RAG系统隐私攻击的空白，为隐私保护提供了新的研究方向。

Abstract: Multimodal retrieval-augmented generation (RAG) systems enhance large
vision-language models by integrating cross-modal knowledge, enabling their
increasing adoption across real-world multimodal tasks. These knowledge
databases may contain sensitive information that requires privacy protection.
However, multimodal RAG systems inherently grant external users indirect access
to such data, making them potentially vulnerable to privacy attacks,
particularly membership inference attacks (MIAs). % Existing MIA methods
targeting RAG systems predominantly focus on the textual modality, while the
visual modality remains relatively underexplored. To bridge this gap, we
propose MrM, the first black-box MIA framework targeted at multimodal RAG
systems. It utilizes a multi-object data perturbation framework constrained by
counterfactual attacks, which can concurrently induce the RAG systems to
retrieve the target data and generate information that leaks the membership
information. Our method first employs an object-aware data perturbation method
to constrain the perturbation to key semantics and ensure successful retrieval.
Building on this, we design a counterfact-informed mask selection strategy to
prioritize the most informative masked regions, aiming to eliminate the
interference of model self-knowledge and amplify attack efficacy. Finally, we
perform statistical membership inference by modeling query trials to extract
features that reflect the reconstruction of masked semantics from response
patterns. Experiments on two visual datasets and eight mainstream commercial
visual-language models (e.g., GPT-4o, Gemini-2) demonstrate that MrM achieves
consistently strong performance across both sample-level and set-level
evaluations, and remains robust under adaptive defenses.

</details>


### [98] [Compressed Feature Quality Assessment: Dataset and Baselines](https://arxiv.org/abs/2506.07412)
*Changsheng Gao,Wei Zhou,Guosheng Lin,Weisi Lin*

Main category: cs.CV

TL;DR: 论文提出了压缩特征质量评估（CFQA）问题，并创建了一个包含原始和压缩特征的基准数据集，评估了三种常用指标的性能，发现需要更精细的指标来量化语义失真。


<details>
  <summary>Details</summary>
Motivation: 在资源受限环境中，特征编码的语义退化难以量化，需要新的评估方法。

Method: 提出了CFQA问题，创建了包含300个原始特征和12000个压缩特征的数据集，评估了MSE、余弦相似度和中心核对齐三种指标。

Result: 数据集具有代表性，现有指标无法完全捕捉语义失真，需要更精细的指标。

Conclusion: 论文为CFQA研究提供了基准数据集和开源代码，推动该领域发展。

Abstract: The widespread deployment of large models in resource-constrained
environments has underscored the need for efficient transmission of
intermediate feature representations. In this context, feature coding, which
compresses features into compact bitstreams, becomes a critical component for
scenarios involving feature transmission, storage, and reuse. However, this
compression process introduces inherent semantic degradation that is
notoriously difficult to quantify with traditional metrics. To address this,
this paper introduces the research problem of Compressed Feature Quality
Assessment (CFQA), which seeks to evaluate the semantic fidelity of compressed
features. To advance CFQA research, we propose the first benchmark dataset,
comprising 300 original features and 12000 compressed features derived from
three vision tasks and four feature codecs. Task-specific performance drops are
provided as true semantic distortion for the evaluation of CFQA metrics. We
assess the performance of three widely used metrics (MSE, cosine similarity,
and Centered Kernel Alignment) in capturing semantic degradation. The results
underscore the representativeness of the dataset and highlight the need for
more refined metrics capable of addressing the nuances of semantic distortion
in compressed features. To facilitate the ongoing development of CFQA research,
we release the dataset and all accompanying source code at
\href{https://github.com/chansongoal/Compressed-Feature-Quality-Assessment}{https://github.com/chansongoal/Compressed-Feature-Quality-Assessment}.
This contribution aims to advance the field and provide a foundational resource
for the community to explore CFQA.

</details>


### [99] [DPFormer: Dynamic Prompt Transformer for Continual Learning](https://arxiv.org/abs/2506.07414)
*Sheng-Kai Huang,Jiun-Feng Chang,Chun-Rong Huang*

Main category: cs.CV

TL;DR: 提出了一种动态提示变换器（DPFormer）及其提示方案，以解决持续学习中的灾难性遗忘和任务间混淆问题，并在多个数据集上取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中的灾难性遗忘和稳定性-可塑性困境，以及任务间缺乏知识交换导致的混淆问题。

Method: 采用动态提示变换器（DPFormer）和提示方案，结合统一的分类模块（包含二元交叉熵损失、知识蒸馏损失和辅助损失）进行端到端训练。

Result: 在CIFAR-100、ImageNet100和ImageNet1K数据集的不同类增量设置下，性能优于现有方法。

Conclusion: DPFormer及其提示方案有效解决了持续学习中的关键问题，并在多个数据集上表现优异。

Abstract: In continual learning, solving the catastrophic forgetting problem may make
the models fall into the stability-plasticity dilemma. Moreover, inter-task
confusion will also occur due to the lack of knowledge exchanges between
different tasks. In order to solve the aforementioned problems, we propose a
novel dynamic prompt transformer (DPFormer) with prompt schemes. The prompt
schemes help the DPFormer memorize learned knowledge of previous classes and
tasks, and keep on learning new knowledge from new classes and tasks under a
single network structure with a nearly fixed number of model parameters.
Moreover, they also provide discrepant information to represent different tasks
to solve the inter-task confusion problem. Based on prompt schemes, a unified
classification module with the binary cross entropy loss, the knowledge
distillation loss and the auxiliary loss is proposed to train the whole model
in an end-to-end trainable manner. Compared with state-of-the-art methods, our
method achieves the best performance in the CIFAR-100, ImageNet100 and
ImageNet1K datasets under different class-incremental settings in continual
learning. The source code will be available at our GitHub after acceptance.

</details>


### [100] [FAMSeg: Fetal Femur and Cranial Ultrasound Segmentation Using Feature-Aware Attention and Mamba Enhancement](https://arxiv.org/abs/2506.07431)
*Jie He,Minglang Chen,Minying Lu,Bocheng Liang,Junming Wei,Guiyan Peng,Jiaxi Chen,Ying Tan*

Main category: cs.CV

TL;DR: 提出了一种基于特征感知和Mamba增强的胎儿股骨和颅骨超声图像分割模型，解决了现有模型在高噪声和高相似性超声图像中的适应性问题。


<details>
  <summary>Details</summary>
Motivation: 超声图像分割的准确性对生物计量和评估至关重要，但现有模型难以适应高噪声和高相似性的超声对象，尤其是小物体分割时锯齿效应明显。

Method: 设计了纵向和横向独立视角扫描卷积块及特征感知模块，结合Mamba优化的残差结构，增强局部细节捕捉和上下文信息融合，抑制原始噪声干扰。

Result: FAMSeg网络在实验中实现了最快的损失减少和最佳分割性能，适用于不同大小和方向的图像。

Conclusion: 提出的模型显著提升了超声图像分割的准确性和效率，尤其适用于复杂场景下的胎儿股骨和颅骨分割。

Abstract: Accurate ultrasound image segmentation is a prerequisite for precise
biometrics and accurate assessment. Relying on manual delineation introduces
significant errors and is time-consuming. However, existing segmentation models
are designed based on objects in natural scenes, making them difficult to adapt
to ultrasound objects with high noise and high similarity. This is particularly
evident in small object segmentation, where a pronounced jagged effect occurs.
Therefore, this paper proposes a fetal femur and cranial ultrasound image
segmentation model based on feature perception and Mamba enhancement to address
these challenges. Specifically, a longitudinal and transverse independent
viewpoint scanning convolution block and a feature perception module were
designed to enhance the ability to capture local detail information and improve
the fusion of contextual information. Combined with the Mamba-optimized
residual structure, this design suppresses the interference of raw noise and
enhances local multi-dimensional scanning. The system builds global information
and local feature dependencies, and is trained with a combination of different
optimizers to achieve the optimal solution. After extensive experimental
validation, the FAMSeg network achieved the fastest loss reduction and the best
segmentation performance across images of varying sizes and orientations.

</details>


### [101] [Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition](https://arxiv.org/abs/2506.07436)
*Nishi Chaudhary,S M Jamil Uddin,Sathvik Sharath Chandra,Anto Ovid,Alex Albert*

Main category: cs.CV

TL;DR: 该研究比较了五种多模态大语言模型（LLMs）在建筑工地视觉危险识别任务中的表现，发现提示策略（如零样本、少样本和思维链）对性能有显著影响，其中思维链提示效果最佳。GPT-4.5和GPT-o3表现最优，提示设计对提升准确性至关重要。


<details>
  <summary>Details</summary>
Motivation: 探索多模态LLMs在建筑安全领域的应用潜力，填补其在视觉危险识别任务中性能评估的研究空白。

Method: 对五种LLMs（Claude-3 Opus、GPT-4.5、GPT-4o、GPT-o3和Gemini 2.0 Pro）进行对比评估，采用零样本、少样本和思维链三种提示策略，使用精确率、召回率和F1分数作为量化指标。

Result: 思维链提示策略在所有模型中表现最佳，GPT-4.5和GPT-o3在多数情况下优于其他模型。提示设计对模型性能有显著影响。

Conclusion: 研究为多模态LLMs在建筑安全领域的应用提供了实用见解，强调了提示工程的重要性，有助于开发更可靠的AI辅助安全系统。

Abstract: The recent emergence of multimodal large language models (LLMs) has
introduced new opportunities for improving visual hazard recognition on
construction sites. Unlike traditional computer vision models that rely on
domain-specific training and extensive datasets, modern LLMs can interpret and
describe complex visual scenes using simple natural language prompts. However,
despite growing interest in their applications, there has been limited
investigation into how different LLMs perform in safety-critical visual tasks
within the construction domain. To address this gap, this study conducts a
comparative evaluation of five state-of-the-art LLMs: Claude-3 Opus, GPT-4.5,
GPT-4o, GPT-o3, and Gemini 2.0 Pro, to assess their ability to identify
potential hazards from real-world construction images. Each model was tested
under three prompting strategies: zero-shot, few-shot, and chain-of-thought
(CoT). Zero-shot prompting involved minimal instruction, few-shot incorporated
basic safety context and a hazard source mnemonic, and CoT provided
step-by-step reasoning examples to scaffold model thinking. Quantitative
analysis was performed using precision, recall, and F1-score metrics across all
conditions. Results reveal that prompting strategy significantly influenced
performance, with CoT prompting consistently producing higher accuracy across
models. Additionally, LLM performance varied under different conditions, with
GPT-4.5 and GPT-o3 outperforming others in most settings. The findings also
demonstrate the critical role of prompt design in enhancing the accuracy and
consistency of multimodal LLMs for construction safety applications. This study
offers actionable insights into the integration of prompt engineering and LLMs
for practical hazard recognition, contributing to the development of more
reliable AI-assisted safety systems.

</details>


### [102] [PhysiInter: Integrating Physical Mapping for High-Fidelity Human Interaction Generation](https://arxiv.org/abs/2506.07456)
*Wei Yao,Yunlian Sun,Chang Liu,Hongwen Zhang,Jinhui Tang*

Main category: cs.CV

TL;DR: 论文提出了一种结合物理映射的方法，用于生成符合物理约束的多人体运动，解决了现有技术中的穿透、滑动和漂浮等问题。


<details>
  <summary>Details</summary>
Motivation: 现有运动捕捉技术和生成模型常忽略物理约束，导致多人体运动生成中出现不真实的现象。

Method: 通过物理仿真环境中的运动模仿，将目标运动映射到物理有效空间，并引入运动一致性和标记交互损失函数。

Result: 实验显示，该方法在生成运动质量上有显著提升，物理保真度提高了3%-89%。

Conclusion: 提出的物理映射和定制化运动表示框架有效提升了多人体运动生成的物理真实性和语义保留。

Abstract: Driven by advancements in motion capture and generative artificial
intelligence, leveraging large-scale MoCap datasets to train generative models
for synthesizing diverse, realistic human motions has become a promising
research direction. However, existing motion-capture techniques and generative
models often neglect physical constraints, leading to artifacts such as
interpenetration, sliding, and floating. These issues are exacerbated in
multi-person motion generation, where complex interactions are involved. To
address these limitations, we introduce physical mapping, integrated throughout
the human interaction generation pipeline. Specifically, motion imitation
within a physics-based simulation environment is used to project target motions
into a physically valid space. The resulting motions are adjusted to adhere to
real-world physics constraints while retaining their original semantic meaning.
This mapping not only improves MoCap data quality but also directly informs
post-processing of generated motions. Given the unique interactivity of
multi-person scenarios, we propose a tailored motion representation framework.
Motion Consistency (MC) and Marker-based Interaction (MI) loss functions are
introduced to improve model performance. Experiments show our method achieves
impressive results in generated human motion quality, with a 3%-89% improvement
in physical fidelity. Project page http://yw0208.github.io/physiinter

</details>


### [103] [GLOS: Sign Language Generation with Temporally Aligned Gloss-Level Conditioning](https://arxiv.org/abs/2506.07460)
*Taeryung Lee,Hyeongjin Nam,Gyeongsik Moon,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: 论文提出GLOS框架，通过时间对齐的gloss级条件改进手语生成，解决了现有方法中词汇顺序错误和语义准确性低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有手语生成方法因句子级条件编码导致词汇顺序混乱和语义模糊，无法捕捉手语的时间结构和单词级语义。

Method: 采用gloss级条件（时间对齐的gloss嵌入序列）和条件融合模块TAC，实现细粒度控制和词汇顺序保持。

Result: 在CSL-Daily和Phoenix-2014T数据集上表现优于现有方法，生成的手语词汇顺序正确且语义准确。

Conclusion: GLOS框架通过gloss级条件和TAC模块显著提升了手语生成的质量。

Abstract: Sign language generation (SLG), or text-to-sign generation, bridges the gap
between signers and non-signers. Despite recent progress in SLG, existing
methods still often suffer from incorrect lexical ordering and low semantic
accuracy. This is primarily due to sentence-level condition, which encodes the
entire sentence of the input text into a single feature vector as a condition
for SLG. This approach fails to capture the temporal structure of sign language
and lacks the granularity of word-level semantics, often leading to disordered
sign sequences and ambiguous motions. To overcome these limitations, we propose
GLOS, a sign language generation framework with temporally aligned gloss-level
conditioning. First, we employ gloss-level conditions, which we define as
sequences of gloss embeddings temporally aligned with the motion sequence. This
enables the model to access both the temporal structure of sign language and
word-level semantics at each timestep. As a result, this allows for
fine-grained control of signs and better preservation of lexical order. Second,
we introduce a condition fusion module, temporal alignment conditioning (TAC),
to efficiently deliver the word-level semantic and temporal structure provided
by the gloss-level condition to the corresponding motion timesteps. Our method,
which is composed of gloss-level conditions and TAC, generates signs with
correct lexical order and high semantic accuracy, outperforming prior methods
on CSL-Daily and Phoenix-2014T.

</details>


### [104] [DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO](https://arxiv.org/abs/2506.07464)
*Jinyoung Park,Jeehye Na,Jinyoung Kim,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: 论文探讨了GRPO在视频大语言模型中的应用，提出了Reg-GRPO和难度感知数据增强策略以解决其学习效率问题，实验证明DeepVideo-R1显著提升了视频推理性能。


<details>
  <summary>Details</summary>
Motivation: 研究GRPO在视频大语言模型中的应用，并解决其依赖安全措施和优势消失问题。

Method: 提出Reg-GRPO（将GRPO目标重构为回归任务）和难度感知数据增强策略。

Result: DeepVideo-R1在多个视频推理基准测试中表现显著提升。

Conclusion: Reg-GRPO和难度感知数据增强策略有效解决了GRPO在视频大语言模型中的学习问题。

Abstract: Recent works have demonstrated the effectiveness of reinforcement learning
(RL)-based post-training in enhancing the reasoning capabilities of large
language models (LLMs). In particular, Group Relative Policy Optimization
(GRPO) has shown impressive success by employing a PPO-style reinforcement
algorithm with group-based normalized rewards. However, the application of GRPO
to Video Large Language Models (Video LLMs) has been less studied. In this
paper, we explore GRPO for video LLMs and identify two primary issues that
impede its effective learning: (1) reliance on safeguards, and (2) the
vanishing advantage problem. To mitigate these challenges, we propose
DeepVideo-R1, a video large language model trained with our proposed Reg-GRPO
(Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO
reformulates the GRPO objective as a regression task, directly predicting the
advantage in GRPO. This design eliminates the need for safeguards like clipping
and min functions, thereby facilitating more direct policy guidance by aligning
the model with the advantage values. We also design the difficulty-aware data
augmentation strategy that dynamically augments training samples at solvable
difficulty levels, fostering diverse and informative reward signals. Our
comprehensive experiments show that DeepVideo-R1 significantly improves video
reasoning performance across multiple video reasoning benchmarks.

</details>


### [105] [Ambiguity-Restrained Text-Video Representation Learning for Partially Relevant Video Retrieval](https://arxiv.org/abs/2506.07471)
*CH Cho,WJ Moon,W Jun,MS Jung,JP Heo*

Main category: cs.CV

TL;DR: PRVR任务旨在检索与文本查询相关的视频片段。论文提出ARL框架，通过检测模糊文本-视频对并利用多正对比学习和双三重边际损失优化语义关系，提升检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有PRVR训练假设文本与视频一一对应，忽略了文本与视频内容之间的模糊性。论文旨在解决这一问题。

Method: 提出ARL框架，基于不确定性和相似性检测模糊对，通过多正对比学习和双三重边际损失优化语义关系，并引入跨模型模糊检测。

Result: ARL框架有效提升了PRVR任务的性能。

Conclusion: 通过解决文本-视频模糊性问题，ARL框架为PRVR任务提供了更鲁棒的解决方案。

Abstract: Partially Relevant Video Retrieval~(PRVR) aims to retrieve a video where a
specific segment is relevant to a given text query. Typical training processes
of PRVR assume a one-to-one relationship where each text query is relevant to
only one video. However, we point out the inherent ambiguity between text and
video content based on their conceptual scope and propose a framework that
incorporates this ambiguity into the model learning process. Specifically, we
propose Ambiguity-Restrained representation Learning~(ARL) to address ambiguous
text-video pairs. Initially, ARL detects ambiguous pairs based on two criteria:
uncertainty and similarity. Uncertainty represents whether instances include
commonly shared context across the dataset, while similarity indicates
pair-wise semantic overlap. Then, with the detected ambiguous pairs, our ARL
hierarchically learns the semantic relationship via multi-positive contrastive
learning and dual triplet margin loss. Additionally, we delve into fine-grained
relationships within the video instances. Unlike typical training at the
text-video level, where pairwise information is provided, we address the
inherent ambiguity within frames of the same untrimmed video, which often
contains multiple contexts. This allows us to further enhance learning at the
text-frame level. Lastly, we propose cross-model ambiguity detection to
mitigate the error propagation that occurs when a single model is employed to
detect ambiguous pairs for its training. With all components combined, our
proposed method demonstrates its effectiveness in PRVR.

</details>


### [106] [CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization](https://arxiv.org/abs/2506.07484)
*Dasol Hong,Wooju Lee,Hyun Myung*

Main category: cs.CV

TL;DR: 论文提出了一种名为CoCoA-Mix的方法，通过混淆感知损失（CoA-loss）和置信感知权重（CoA-weights）提升视觉语言模型的专用性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 冻结编码器常导致特征不对齐，引发类别混淆，限制了模型的专用性。

Method: 提出CoA-loss优化决策边界以减少混淆，并结合CoA-weights的混合模型增强泛化能力。

Result: 实验表明CoCoA-Mix在专用性和泛化性上优于现有方法。

Conclusion: CoCoA-Mix通过混淆感知机制和混合模型设计，有效提升了模型性能。

Abstract: Prompt tuning, which adapts vision-language models by freezing model
parameters and optimizing only the prompt, has proven effective for
task-specific adaptations. The core challenge in prompt tuning is improving
specialization for a specific task and generalization for unseen domains.
However, frozen encoders often produce misaligned features, leading to
confusion between classes and limiting specialization. To overcome this issue,
we propose a confusion-aware loss (CoA-loss) that improves specialization by
refining the decision boundaries between confusing classes. Additionally, we
mathematically demonstrate that a mixture model can enhance generalization
without compromising specialization. This is achieved using confidence-aware
weights (CoA-weights), which adjust the weights of each prediction in the
mixture model based on its confidence within the class domains. Extensive
experiments show that CoCoA-Mix, a mixture model with CoA-loss and CoA-weights,
outperforms state-of-the-art methods by enhancing specialization and
generalization. Our code is publicly available at
https://github.com/url-kaist/CoCoA-Mix.

</details>


### [107] [Drive Any Mesh: 4D Latent Diffusion for Mesh Deformation from Video](https://arxiv.org/abs/2506.07489)
*Yahao Shi,Yang Liu,Yanmin Wu,Xing Liu,Chen Zhao,Jie Luo,Bin Zhou*

Main category: cs.CV

TL;DR: DriveAnyMesh是一种基于单目视频驱动网格的方法，通过4D扩散模型生成高质量动画，适用于现代渲染引擎。


<details>
  <summary>Details</summary>
Motivation: 解决现有4D生成技术在渲染效率、手动需求和跨类别泛化方面的不足，以及动画化现有3D资产的需求。

Method: 使用4D扩散模型对潜在集序列去噪，并通过基于变压器的变分自编码器解码生成网格动画。

Result: 实验表明，DriveAnyMesh能快速生成复杂运动的高质量动画，且与现代渲染引擎兼容。

Conclusion: 该方法在游戏和影视行业具有应用潜力。

Abstract: We propose DriveAnyMesh, a method for driving mesh guided by monocular video.
Current 4D generation techniques encounter challenges with modern rendering
engines. Implicit methods have low rendering efficiency and are unfriendly to
rasterization-based engines, while skeletal methods demand significant manual
effort and lack cross-category generalization. Animating existing 3D assets,
instead of creating 4D assets from scratch, demands a deep understanding of the
input's 3D structure. To tackle these challenges, we present a 4D diffusion
model that denoises sequences of latent sets, which are then decoded to produce
mesh animations from point cloud trajectory sequences. These latent sets
leverage a transformer-based variational autoencoder, simultaneously capturing
3D shape and motion information. By employing a spatiotemporal,
transformer-based diffusion model, information is exchanged across multiple
latent frames, enhancing the efficiency and generalization of the generated
results. Our experimental results demonstrate that DriveAnyMesh can rapidly
produce high-quality animations for complex motions and is compatible with
modern rendering engines. This method holds potential for applications in both
the gaming and filming industries.

</details>


### [108] [SpatialLM: Training Large Language Models for Structured Indoor Modeling](https://arxiv.org/abs/2506.07491)
*Yongsen Mao,Junhao Zhong,Chuan Fang,Jia Zheng,Rui Tang,Hao Zhu,Ping Tan,Zihan Zhou*

Main category: cs.CV

TL;DR: SpatialLM是一个处理3D点云数据并生成结构化3D场景理解输出的大型语言模型，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 增强现代LLM的空间理解能力，应用于增强现实和机器人等领域。

Method: 基于开源LLM架构，通过大规模合成数据集进行微调。

Result: 在布局估计和3D物体检测中表现优异。

Conclusion: 展示了提升LLM空间理解能力的可行路径。

Abstract: SpatialLM is a large language model designed to process 3D point cloud data
and generate structured 3D scene understanding outputs. These outputs include
architectural elements like walls, doors, windows, and oriented object boxes
with their semantic categories. Unlike previous methods which exploit
task-specific network designs, our model adheres to the standard multimodal LLM
architecture and is fine-tuned directly from open-source LLMs.
  To train SpatialLM, we collect a large-scale, high-quality synthetic dataset
consisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with
ground-truth 3D annotations, and conduct a careful study on various modeling
and training decisions. On public benchmarks, our model gives state-of-the-art
performance in layout estimation and competitive results in 3D object
detection. With that, we show a feasible path for enhancing the spatial
understanding capabilities of modern LLMs for applications in augmented
reality, embodied robotics, and more.

</details>


### [109] [Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency](https://arxiv.org/abs/2506.07497)
*Xiangyu Guo,Zhanqian Wu,Kaixin Xiong,Ziyang Xu,Lijun Zhou,Gangwei Xu,Shaoqing Xu,Haiyang Sun,Bing Wang,Guang Chen,Hangjun Ye,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: Genesis是一个统一框架，用于联合生成多视角驾驶视频和LiDAR序列，确保时空和跨模态一致性。它采用两阶段架构，结合DiT视频扩散模型和3D-VAE编码，以及BEV感知的LiDAR生成器。通过共享潜在空间实现视觉和几何域的一致性，并通过DataCrafter模块提供语义指导。实验表明Genesis在nuScenes基准测试中表现优异，并提升了下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态数据（视频和LiDAR）生成中的时空和跨模态一致性问题，为自动驾驶等应用提供高质量合成数据。

Method: 采用两阶段架构：1）DiT视频扩散模型与3D-VAE编码；2）BEV感知的LiDAR生成器，结合NeRF渲染和自适应采样。通过共享潜在空间耦合两种模态，并引入DataCrafter模块提供语义监督。

Result: 在nuScenes基准测试中，Genesis在视频和LiDAR指标上达到最优（FVD 16.95, FID 4.24, Chamfer 0.611），并提升了下游任务（如分割和3D检测）的性能。

Conclusion: Genesis通过统一框架实现了高质量的多模态数据生成，验证了其语义保真度和实际应用价值。

Abstract: We present Genesis, a unified framework for joint generation of multi-view
driving videos and LiDAR sequences with spatio-temporal and cross-modal
consistency. Genesis employs a two-stage architecture that integrates a
DiT-based video diffusion model with 3D-VAE encoding, and a BEV-aware LiDAR
generator with NeRF-based rendering and adaptive sampling. Both modalities are
directly coupled through a shared latent space, enabling coherent evolution
across visual and geometric domains. To guide the generation with structured
semantics, we introduce DataCrafter, a captioning module built on
vision-language models that provides scene-level and instance-level
supervision. Extensive experiments on the nuScenes benchmark demonstrate that
Genesis achieves state-of-the-art performance across video and LiDAR metrics
(FVD 16.95, FID 4.24, Chamfer 0.611), and benefits downstream tasks including
segmentation and 3D detection, validating the semantic fidelity and practical
utility of the generated data.

</details>


### [110] [MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts](https://arxiv.org/abs/2506.07533)
*Wei Tao,Haocheng Lu,Xiaoyang Qu,Bin Zhang,Kai Lu,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: MoQAE是一种通过混合量化感知专家的新型混合精度量化方法，旨在优化大语言模型的长上下文推理中的KV缓存内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有量化方法无法同时兼顾效果和效率，而KV缓存的高内存消耗是优化大语言模型的主要挑战。

Method: 提出MoQAE方法，将不同量化位宽配置视为专家，采用分块输入路由器和轻量级路由器微调，结合路由冻结和共享机制。

Result: 在多个基准数据集上的实验表明，MoQAE在效率和效果上均优于现有KV缓存量化方法。

Conclusion: MoQAE通过混合精度量化和优化路由机制，有效平衡了模型精度和内存使用，为长上下文推理提供了高效解决方案。

Abstract: One of the primary challenges in optimizing large language models (LLMs) for
long-context inference lies in the high memory consumption of the Key-Value
(KV) cache. Existing approaches, such as quantization, have demonstrated
promising results in reducing memory usage. However, current quantization
methods cannot take both effectiveness and efficiency into account. In this
paper, we propose MoQAE, a novel mixed-precision quantization method via
mixture of quantization-aware experts. First, we view different quantization
bit-width configurations as experts and use the traditional mixture of experts
(MoE) method to select the optimal configuration. To avoid the inefficiency
caused by inputting tokens one by one into the router in the traditional MoE
method, we input the tokens into the router chunk by chunk. Second, we design a
lightweight router-only fine-tuning process to train MoQAE with a comprehensive
loss to learn the trade-off between model accuracy and memory usage. Finally,
we introduce a routing freezing (RF) and a routing sharing (RS) mechanism to
further reduce the inference overhead. Extensive experiments on multiple
benchmark datasets demonstrate that our method outperforms state-of-the-art KV
cache quantization approaches in both efficiency and effectiveness.

</details>


### [111] [Domain Randomization for Object Detection in Manufacturing Applications using Synthetic Data: A Comprehensive Study](https://arxiv.org/abs/2506.07539)
*Xiaomeng Zhu,Jacob Henningsson,Duruo Li,Pär Mårtensson,Lars Hanson,Mårten Björkman,Atsuto Maki*

Main category: cs.CV

TL;DR: 论文提出了一种用于制造业目标检测的合成数据生成方法，通过域随机化技术生成接近真实数据分布的数据集，并在公开数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决制造业目标检测中合成数据生成的问题，提高模拟到真实场景的适应性。

Method: 提出一个综合数据生成管道，考虑对象特性、背景、光照等因素，并引入SIP15-OD数据集。

Result: 在Yolov8模型上，合成数据训练的模型在公开数据集上表现优异，mAP@50得分高达96.4%。

Conclusion: 域随机化方法有效，能够生成接近真实数据分布的数据，适用于制造业目标检测。

Abstract: This paper addresses key aspects of domain randomization in generating
synthetic data for manufacturing object detection applications. To this end, we
present a comprehensive data generation pipeline that reflects different
factors: object characteristics, background, illumination, camera settings, and
post-processing. We also introduce the Synthetic Industrial Parts Object
Detection dataset (SIP15-OD) consisting of 15 objects from three industrial use
cases under varying environments as a test bed for the study, while also
employing an industrial dataset publicly available for robotic applications. In
our experiments, we present more abundant results and insights into the
feasibility as well as challenges of sim-to-real object detection. In
particular, we identified material properties, rendering methods,
post-processing, and distractors as important factors. Our method, leveraging
these, achieves top performance on the public dataset with Yolov8 models
trained exclusively on synthetic data; mAP@50 scores of 96.4% for the robotics
dataset, and 94.1%, 99.5%, and 95.3% across three of the SIP15-OD use cases,
respectively. The results showcase the effectiveness of the proposed domain
randomization, potentially covering the distribution close to real data for the
applications.

</details>


### [112] [APTOS-2024 challenge report: Generation of synthetic 3D OCT images from fundus photographs](https://arxiv.org/abs/2506.07542)
*Bowen Liu,Weiyi Zhang,Peranut Chotcomwongse,Xiaolan Chen,Ruoyu Chen,Pawin Pakaymaskul,Niracha Arjkongharn,Nattaporn Vongsa,Xuelian Cheng,Zongyuan Ge,Kun Huang,Xiaohui Li,Yiru Duan,Zhenbang Wang,BaoYe Xie,Qiang Chen,Huazhu Fu,Michael A. Mahr,Jiaqi Qu,Wangyiyang Chen,Shiye Wang,Yubo Tan,Yongjie Li,Mingguang He,Danli Shi,Paisan Ruamviboonsuk*

Main category: cs.CV

TL;DR: APTOS-2024挑战赛探索了从2D眼底图像生成3D OCT图像的可行性，旨在解决OCT设备成本高和操作复杂的问题。


<details>
  <summary>Details</summary>
Motivation: OCT设备成本高且依赖专业操作人员，限制了其普及；2D眼底图像更易获取，但缺乏3D信息。生成AI可能弥合这一差距。

Method: 挑战赛提供了基准数据集和评估方法（基于图像和视频的保真度指标），吸引了342个团队参与。领先方法包括数据预处理、预训练和模型架构改进。

Result: 42个初步提交和9个决赛团队展示了从眼底图像生成3D OCT图像的可行性。

Conclusion: APTOS-2024挑战赛首次证明了眼底到3D OCT合成的潜力，有助于提升资源匮乏地区的眼科护理可及性。

Abstract: Optical Coherence Tomography (OCT) provides high-resolution, 3D, and
non-invasive visualization of retinal layers in vivo, serving as a critical
tool for lesion localization and disease diagnosis. However, its widespread
adoption is limited by equipment costs and the need for specialized operators.
In comparison, 2D color fundus photography offers faster acquisition and
greater accessibility with less dependence on expensive devices. Although
generative artificial intelligence has demonstrated promising results in
medical image synthesis, translating 2D fundus images into 3D OCT images
presents unique challenges due to inherent differences in data dimensionality
and biological information between modalities. To advance generative models in
the fundus-to-3D-OCT setting, the Asia Pacific Tele-Ophthalmology Society
(APTOS-2024) organized a challenge titled Artificial Intelligence-based OCT
Generation from Fundus Images. This paper details the challenge framework
(referred to as APTOS-2024 Challenge), including: the benchmark dataset,
evaluation methodology featuring two fidelity metrics-image-based distance
(pixel-level OCT B-scan similarity) and video-based distance (semantic-level
volumetric consistency), and analysis of top-performing solutions. The
challenge attracted 342 participating teams, with 42 preliminary submissions
and 9 finalists. Leading methodologies incorporated innovations in hybrid data
preprocessing or augmentation (cross-modality collaborative paradigms),
pre-training on external ophthalmic imaging datasets, integration of vision
foundation models, and model architecture improvement. The APTOS-2024 Challenge
is the first benchmark demonstrating the feasibility of fundus-to-3D-OCT
synthesis as a potential solution for improving ophthalmic care accessibility
in under-resourced healthcare settings, while helping to expedite medical
research and clinical applications.

</details>


### [113] [Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries](https://arxiv.org/abs/2506.07555)
*Haoxiang Wang,Zinan Lin,Da Yu,Huishuai Zhang*

Main category: cs.CV

TL;DR: SPTI是一种通过文本中介生成高分辨率差分隐私图像的新方法，无需训练模型，显著提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私图像合成方法难以生成高分辨率且忠实于原始数据的图像，SPTI旨在解决这一问题。

Method: SPTI将图像转换为文本描述，使用改进的Private Evolution算法生成差分隐私文本，再通过文本到图像模型重建图像。

Result: 在LSUN Bedroom和MM CelebA HQ数据集上，SPTI的FID显著优于现有方法（如26.71 vs 40.36）。

Conclusion: SPTI提供了一种资源高效且兼容专有模型的框架，显著扩展了私有视觉数据集的访问。

Abstract: Generating high fidelity, differentially private (DP) synthetic images offers
a promising route to share and analyze sensitive visual data without
compromising individual privacy. However, existing DP image synthesis methods
struggle to produce high resolution outputs that faithfully capture the
structure of the original data. In this paper, we introduce a novel method,
referred to as Synthesis via Private Textual Intermediaries (SPTI), that can
generate high resolution DP images with easy adoption. The key idea is to shift
the challenge of DP image synthesis from the image domain to the text domain by
leveraging state of the art DP text generation methods. SPTI first summarizes
each private image into a concise textual description using image to text
models, then applies a modified Private Evolution algorithm to generate DP
text, and finally reconstructs images using text to image models. Notably, SPTI
requires no model training, only inference with off the shelf models. Given a
private dataset, SPTI produces synthetic images of substantially higher quality
than prior DP approaches. On the LSUN Bedroom dataset, SPTI attains an FID less
than or equal to 26.71 under epsilon equal to 1.0, improving over Private
Evolution FID of 40.36. Similarly, on MM CelebA HQ, SPTI achieves an FID less
than or equal to 33.27 at epsilon equal to 1.0, compared to 57.01 from DP fine
tuning baselines. Overall, our results demonstrate that Synthesis via Private
Textual Intermediaries provides a resource efficient and proprietary model
compatible framework for generating high resolution DP synthetic images,
greatly expanding access to private visual datasets.

</details>


### [114] [Cross-channel Perception Learning for H&E-to-IHC Virtual Staining](https://arxiv.org/abs/2506.07559)
*Hao Yang,JianYu Wu,Run Fang,Xuelian Zhao,Yuan Ji,Zhiyu Chen,Guibin He,Junceng Guo,Yang Liu,Xinhua Zeng*

Main category: cs.CV

TL;DR: 提出了一种新的跨通道感知学习（CCPL）策略，用于解决H&E到IHC染色研究中忽略的细胞核与细胞膜跨通道相关性，通过双通道特征提取和特征蒸馏损失提升虚拟染色质量。


<details>
  <summary>Details</summary>
Motivation: 现有H&E-to-IHC研究常忽略细胞核与细胞膜的跨通道相关性，影响病理图像分析的准确性。

Method: CCPL将HER2免疫组化染色分解为Hematoxylin和DAB染色通道，利用Gigapath的Tile Encoder提取双通道特征，计算跨通道相关性，并通过特征蒸馏损失和光学密度统计分析提升模型性能。

Result: 实验表明CCPL在PSNR、SSIM、PCC和FID等指标上表现优异，生成的虚拟染色图像质量高，得到病理学家的专业认可。

Conclusion: CCPL能有效保留病理特征，为多媒体医疗数据支持的自动化病理诊断提供有力工具。

Abstract: With the rapid development of digital pathology, virtual staining has become
a key technology in multimedia medical information systems, offering new
possibilities for the analysis and diagnosis of pathological images. However,
existing H&E-to-IHC studies often overlook the cross-channel correlations
between cell nuclei and cell membranes. To address this issue, we propose a
novel Cross-Channel Perception Learning (CCPL) strategy. Specifically, CCPL
first decomposes HER2 immunohistochemical staining into Hematoxylin and DAB
staining channels, corresponding to cell nuclei and cell membranes,
respectively. Using the pathology foundation model Gigapath's Tile Encoder,
CCPL extracts dual-channel features from both the generated and real images and
measures cross-channel correlations between nuclei and membranes. The features
of the generated and real stained images, obtained through the Tile Encoder,
are also used to calculate feature distillation loss, enhancing the model's
feature extraction capabilities without increasing the inference burden.
Additionally, CCPL performs statistical analysis on the focal optical density
maps of both single channels to ensure consistency in staining distribution and
intensity. Experimental results, based on quantitative metrics such as PSNR,
SSIM, PCC, and FID, along with professional evaluations from pathologists,
demonstrate that CCPL effectively preserves pathological features, generates
high-quality virtual stained images, and provides robust support for automated
pathological diagnosis using multimedia medical data.

</details>


### [115] [OpenDance: Multimodal Controllable 3D Dance Generation Using Large-scale Internet Data](https://arxiv.org/abs/2506.07565)
*Jinlu Zhang,Zixi Kang,Yizhou Wang*

Main category: cs.CV

TL;DR: 论文提出了OpenDance5D数据集和OpenDanceNet框架，用于解决音乐驱动舞蹈生成中的多模态数据不足和可控性问题。


<details>
  <summary>Details</summary>
Motivation: 音乐驱动舞蹈生成缺乏细粒度多模态数据，且多条件生成灵活性不足，限制了生成的多样性和可控性。

Method: 构建OpenDance5D数据集（14种舞蹈类型，101小时数据，5种模态），并提出OpenDanceNet框架，基于掩码建模实现多条件可控舞蹈生成。

Result: OpenDanceNet实现了高保真和灵活可控的舞蹈生成。

Conclusion: OpenDance5D和OpenDanceNet为音乐驱动舞蹈生成提供了数据支持和高效方法。

Abstract: Music-driven dance generation offers significant creative potential yet faces
considerable challenges. The absence of fine-grained multimodal data and the
difficulty of flexible multi-conditional generation limit previous works on
generation controllability and diversity in practice. In this paper, we build
OpenDance5D, an extensive human dance dataset comprising over 101 hours across
14 distinct genres. Each sample has five modalities to facilitate robust
cross-modal learning: RGB video, audio, 2D keypoints, 3D motion, and
fine-grained textual descriptions from human arts. Furthermore, we propose
OpenDanceNet, a unified masked modeling framework for controllable dance
generation conditioned on music and arbitrary combinations of text prompts,
keypoints, or character positioning. Comprehensive experiments demonstrate that
OpenDanceNet achieves high-fidelity and flexible controllability.

</details>


### [116] [Towards the Influence of Text Quantity on Writer Retrieval](https://arxiv.org/abs/2506.07566)
*Marco Peer,Robert Sablatnig,Florian Kleber*

Main category: cs.CV

TL;DR: 本文研究了基于笔迹相似性的作者检索任务，探讨了文本量对检索性能的影响，发现四行文本即可保持90%以上的全页检索性能，并指出深度学习方法的优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注页面级检索，本文旨在探索文本量（行级和词级）对作者检索性能的影响。

Method: 评估了三种先进作者检索系统（包括手工特征和深度学习方法），在CVL和IAM数据集上测试不同文本量的性能。

Result: 实验表明，仅用一行文本时性能下降20-30%，但四行文本即可保持90%以上的全页性能；深度学习方法在低文本量场景中表现更优。

Conclusion: 文本量对作者检索性能有显著影响，深度学习方法在低文本量场景中优于传统方法。

Abstract: This paper investigates the task of writer retrieval, which identifies
documents authored by the same individual within a dataset based on handwriting
similarities. While existing datasets and methodologies primarily focus on page
level retrieval, we explore the impact of text quantity on writer retrieval
performance by evaluating line- and word level retrieval. We examine three
state-of-the-art writer retrieval systems, including both handcrafted and deep
learning-based approaches, and analyze their performance using varying amounts
of text. Our experiments on the CVL and IAM dataset demonstrate that while
performance decreases by 20-30% when only one line of text is used as query and
gallery, retrieval accuracy remains above 90% of full-page performance when at
least four lines are included. We further show that text-dependent retrieval
can maintain strong performance in low-text scenarios. Our findings also
highlight the limitations of handcrafted features in low-text scenarios, with
deep learning-based methods like NetVLAD outperforming traditional VLAD
encoding.

</details>


### [117] [LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization](https://arxiv.org/abs/2506.07570)
*Yixuan Yang,Zhen Luo,Tongsheng Ding,Junru Lu,Mingqi Gao,Jinyu Yang,Victor Sanchez,Feng Zheng*

Main category: cs.CV

TL;DR: 论文提出3D-SynthPlace数据集和OptiScene模型，通过两阶段训练优化室内布局生成，显著提升生成质量和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有室内布局生成方法存在空间不一致、计算成本高或泛化能力差的问题，需要改进。

Method: 结合合成数据与人类检查构建3D-SynthPlace数据集，并通过监督微调（SFT）和多轮直接偏好优化（DPO）两阶段训练优化LLM模型OptiScene。

Result: OptiScene在生成质量和成功率上优于传统方法，并在交互任务中表现出潜力。

Conclusion: 3D-SynthPlace和OptiScene为室内布局生成提供了高效且泛化能力强的解决方案。

Abstract: Automatic indoor layout generation has attracted increasing attention due to
its potential in interior design, virtual environment construction, and
embodied AI. Existing methods fall into two categories: prompt-driven
approaches that leverage proprietary LLM services (e.g., GPT APIs) and
learning-based methods trained on layout data upon diffusion-based models.
Prompt-driven methods often suffer from spatial inconsistency and high
computational costs, while learning-based methods are typically constrained by
coarse relational graphs and limited datasets, restricting their generalization
to diverse room categories. In this paper, we revisit LLM-based indoor layout
generation and present 3D-SynthPlace, a large-scale dataset that combines
synthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline,
upgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000
scenes, covering four common room types -- bedroom, living room, kitchen, and
bathroom -- enriched with diverse objects and high-level spatial annotations.
We further introduce OptiScene, a strong open-source LLM optimized for indoor
layout generation, fine-tuned based on our 3D-SynthPlace dataset through our
two-stage training. For the warum-up stage I, we adopt supervised fine-tuning
(SFT), which is taught to first generate high-level spatial descriptions then
conditionally predict concrete object placements. For the reinforcing stage II,
to better align the generated layouts with human design preferences, we apply
multi-turn direct preference optimization (DPO), which significantly improving
layout quality and generation success rates. Extensive experiments demonstrate
that OptiScene outperforms traditional prompt-driven and learning-based
baselines. Moreover, OptiScene shows promising potential in interactive tasks
such as scene editing and robot navigation.

</details>


### [118] [Learning Speaker-Invariant Visual Features for Lipreading](https://arxiv.org/abs/2506.07572)
*Yu Li,Feng Xue,Shujie Li,Jinrui Zhang,Shuang Yang,Dan Guo,Richang Hong*

Main category: cs.CV

TL;DR: SIFLip是一种新的唇读框架，通过解耦说话者特定特征，提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有唇读方法提取的视觉特征包含说话者特定属性，导致虚假相关性，影响准确性和泛化能力。

Method: SIFLip采用隐式解耦和显式解耦模块，利用文本嵌入和说话者识别子任务解耦说话者特定特征。

Result: 实验表明，SIFLip在多个公开数据集上显著提升泛化性能，优于现有方法。

Conclusion: SIFLip通过解耦说话者特定特征，有效提升了唇读模型的泛化能力和准确性。

Abstract: Lipreading is a challenging cross-modal task that aims to convert visual lip
movements into spoken text. Existing lipreading methods often extract visual
features that include speaker-specific lip attributes (e.g., shape, color,
texture), which introduce spurious correlations between vision and text. These
correlations lead to suboptimal lipreading accuracy and restrict model
generalization. To address this challenge, we introduce SIFLip, a
speaker-invariant visual feature learning framework that disentangles
speaker-specific attributes using two complementary disentanglement modules
(Implicit Disentanglement and Explicit Disentanglement) to improve
generalization. Specifically, since different speakers exhibit semantic
consistency between lip movements and phonetic text when pronouncing the same
words, our implicit disentanglement module leverages stable text embeddings as
supervisory signals to learn common visual representations across speakers,
implicitly decoupling speaker-specific features. Additionally, we design a
speaker recognition sub-task within the main lipreading pipeline to filter
speaker-specific features, then further explicitly disentangle these
personalized visual features from the backbone network via gradient reversal.
Experimental results demonstrate that SIFLip significantly enhances
generalization performance across multiple public datasets. Experimental
results demonstrate that SIFLip significantly improves generalization
performance across multiple public datasets, outperforming state-of-the-art
methods.

</details>


### [119] [Uncertainty-o: One Model-agnostic Framework for Unveiling Uncertainty in Large Multimodal Models](https://arxiv.org/abs/2506.07575)
*Ruiyang Zhang,Hu Zhang,Hao Fei,Zhedong Zheng*

Main category: cs.CV

TL;DR: 论文提出Uncertainty-o框架，用于评估和量化多模态模型（LMMs）的不确定性，并通过实验验证其在多种任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态模型（LMMs）被认为比纯语言模型（LLMs）更鲁棒，但其不确定性评估仍存在三个关键问题：统一评估方法、如何提示LMMs显示不确定性，以及如何量化不确定性以支持下游任务。

Method: 提出Uncertainty-o框架，包括模型无关的评估方法、多模态提示扰动的实证研究，以及多模态语义不确定性的量化公式。

Result: 在18个基准测试和10种LMMs上的实验表明，Uncertainty-o能可靠估计LMMs的不确定性，并提升幻觉检测、缓解及不确定性感知推理等下游任务。

Conclusion: Uncertainty-o为多模态模型的不确定性评估提供了统一且有效的解决方案，具有广泛的应用潜力。

Abstract: Large Multimodal Models (LMMs), harnessing the complementarity among diverse
modalities, are often considered more robust than pure Language Large Models
(LLMs); yet do LMMs know what they do not know? There are three key open
questions remaining: (1) how to evaluate the uncertainty of diverse LMMs in a
unified manner, (2) how to prompt LMMs to show its uncertainty, and (3) how to
quantify uncertainty for downstream tasks. In an attempt to address these
challenges, we introduce Uncertainty-o: (1) a model-agnostic framework designed
to reveal uncertainty in LMMs regardless of their modalities, architectures, or
capabilities, (2) an empirical exploration of multimodal prompt perturbations
to uncover LMM uncertainty, offering insights and findings, and (3) derive the
formulation of multimodal semantic uncertainty, which enables quantifying
uncertainty from multimodal responses. Experiments across 18 benchmarks
spanning various modalities and 10 LMMs (both open- and closed-source)
demonstrate the effectiveness of Uncertainty-o in reliably estimating LMM
uncertainty, thereby enhancing downstream tasks such as hallucination
detection, hallucination mitigation, and uncertainty-aware Chain-of-Thought
reasoning.

</details>


### [120] [Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding](https://arxiv.org/abs/2506.07576)
*Boyu Chen,Siran Chen,Kunchang Li,Qinglin Xu,Yu Qiao,Yali Wang*

Main category: cs.CV

TL;DR: 提出了一种统一的超级编码网络（SEN），通过递归关联多模态编码器，提升视频理解任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态基础模型缺乏深层次的多模态交互，难以理解复杂视频场景中的目标运动。

Method: 将预训练编码器视为“超级神经元”，设计递归关联（RA）块，逐步融合多模态信息。

Result: 显著提升了跟踪、识别、聊天和编辑等视频任务性能，例如跟踪任务的Jaccard指数提高2.7%。

Conclusion: SEN通过深层次多模态交互，有效提升了视频理解能力。

Abstract: Video understanding has been considered as one critical step towards world
modeling, which is an important long-term problem in AI research. Recently,
multi-modal foundation models have shown such potential via large-scale
pretraining. However, these models simply align encoders of different
modalities via contrastive learning, while lacking deeper multi-modal
interactions, which is critical for understanding complex target movements with
diversified video scenes. To fill this gap, we propose a unified Super Encoding
Network (SEN) for video understanding, which builds up such distinct
interactions through recursive association of multi-modal encoders in the
foundation models. Specifically, we creatively treat those well-trained
encoders as "super neurons" in our SEN. Via designing a Recursive Association
(RA) block, we progressively fuse multi-modalities with the input video, based
on knowledge integrating, distributing, and prompting of super neurons in a
recursive manner. In this way, our SEN can effectively encode deeper
multi-modal interactions, for prompting various video understanding tasks in
downstream. Extensive experiments show that, our SEN can remarkably boost the
four most representative video tasks, including tracking, recognition,
chatting, and editing, e.g., for pixel-level tracking, the average jaccard
index improves 2.7%, temporal coherence(TC) drops 8.8% compared to the popular
CaDeX++ approach. For one-shot video editing, textual alignment improves 6.4%,
and frame consistency increases 4.1% compared to the popular TuneA-Video
approach.

</details>


### [121] [Explore the vulnerability of black-box models via diffusion models](https://arxiv.org/abs/2506.07590)
*Jiacheng Shi,Yanfu Zhang,Huajie Shao,Ashley Gao*

Main category: cs.CV

TL;DR: 论文揭示了一种利用扩散模型API生成合成图像的新安全威胁，攻击者可通过少量查询训练高性能替代模型，实现模型提取和对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的高保真图像生成能力可能被恶意利用，导致隐私泄露和版权问题，因此研究其安全威胁至关重要。

Method: 攻击者利用扩散模型API生成高分辨率、多样化的合成图像，训练替代模型，以最小查询量实现模型提取和对抗攻击。

Result: 在七个基准测试中，该方法平均性能提升27.37%，仅用0.01倍查询预算，对抗攻击成功率高达98.68%。

Conclusion: 扩散模型API可能被滥用为安全威胁，需加强防范措施。

Abstract: Recent advancements in diffusion models have enabled high-fidelity and
photorealistic image generation across diverse applications. However, these
models also present security and privacy risks, including copyright violations,
sensitive information leakage, and the creation of harmful or offensive content
that could be exploited maliciously. In this study, we uncover a novel security
threat where an attacker leverages diffusion model APIs to generate synthetic
images, which are then used to train a high-performing substitute model. This
enables the attacker to execute model extraction and transfer-based adversarial
attacks on black-box classification models with minimal queries, without
needing access to the original training data. The generated images are
sufficiently high-resolution and diverse to train a substitute model whose
outputs closely match those of the target model. Across the seven benchmarks,
including CIFAR and ImageNet subsets, our method shows an average improvement
of 27.37% over state-of-the-art methods while using just 0.01 times of the
query budget, achieving a 98.68% success rate in adversarial attacks on the
target model.

</details>


### [122] [SceneRAG: Scene-level Retrieval-Augmented Generation for Video Understanding](https://arxiv.org/abs/2506.07600)
*Nianbo Zeng,Haowen Hou,Fei Richard Yu,Si Shi,Ying Tiffany He*

Main category: cs.CV

TL;DR: SceneRAG是一个基于大语言模型的框架，通过分析视频的ASR转录本和时间元数据，将视频分割为叙事一致的场景，并结合视觉和文本信息构建知识图谱，显著提升了长视频理解任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于固定长度分块的RAG方法破坏了视频的上下文连续性，无法捕捉真实的场景边界，而人类能够自然地将连续经验组织为连贯场景，这启发了SceneRAG的开发。

Method: SceneRAG利用大语言模型处理ASR转录本和时间元数据，分割视频为叙事一致的场景，并通过轻量级启发式和迭代修正优化边界。结合视觉和文本信息构建动态知识图谱，支持多跳检索和生成。

Result: 在LongerVideos基准测试中，SceneRAG显著优于现有基线，生成任务的胜率高达72.5%。

Conclusion: SceneRAG通过场景分割和多模态信息融合，有效解决了长视频理解的挑战，为未来的视频分析任务提供了新思路。

Abstract: Despite recent advances in retrieval-augmented generation (RAG) for video
understanding, effectively understanding long-form video content remains
underexplored due to the vast scale and high complexity of video data. Current
RAG approaches typically segment videos into fixed-length chunks, which often
disrupts the continuity of contextual information and fails to capture
authentic scene boundaries. Inspired by the human ability to naturally organize
continuous experiences into coherent scenes, we present SceneRAG, a unified
framework that leverages large language models to segment videos into
narrative-consistent scenes by processing ASR transcripts alongside temporal
metadata. SceneRAG further sharpens these initial boundaries through
lightweight heuristics and iterative correction. For each scene, the framework
fuses information from both visual and textual modalities to extract entity
relations and dynamically builds a knowledge graph, enabling robust multi-hop
retrieval and generation that account for long-range dependencies. Experiments
on the LongerVideos benchmark, featuring over 134 hours of diverse content,
confirm that SceneRAG substantially outperforms prior baselines, achieving a
win rate of up to 72.5 percent on generation tasks.

</details>


### [123] [SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis](https://arxiv.org/abs/2506.07603)
*Jianhui Wei,Zikai Xiao,Danyu Sun,Luqi Gong,Zongxin Yang,Zuozhu Liu,Jian Wu*

Main category: cs.CV

TL;DR: SurgBench是一个统一的手术视频基准框架，包含预训练数据集SurgBench-P和评估基准SurgBench-E，旨在解决手术视频基础模型发展中数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 手术视频理解对自动化术中决策、技能评估和术后质量改进至关重要，但缺乏大规模多样化数据集阻碍了进展。

Method: 提出SurgBench框架，包含预训练数据集SurgBench-P（53百万帧，22种手术）和评估基准SurgBench-E（72个细粒度任务）。

Result: 现有视频基础模型在多样化任务上泛化能力不足，而基于SurgBench-P的预训练显著提升了性能及跨领域泛化能力。

Conclusion: SurgBench为手术视频分析提供了全面的数据集和评估标准，推动了基础模型的发展。

Abstract: Surgical video understanding is pivotal for enabling automated intraoperative
decision-making, skill assessment, and postoperative quality improvement.
However, progress in developing surgical video foundation models (FMs) remains
hindered by the scarcity of large-scale, diverse datasets for pretraining and
systematic evaluation. In this paper, we introduce \textbf{SurgBench}, a
unified surgical video benchmarking framework comprising a pretraining dataset,
\textbf{SurgBench-P}, and an evaluation benchmark, \textbf{SurgBench-E}.
SurgBench offers extensive coverage of diverse surgical scenarios, with
SurgBench-P encompassing 53 million frames across 22 surgical procedures and 11
specialties, and SurgBench-E providing robust evaluation across six categories
(phase classification, camera motion, tool recognition, disease diagnosis,
action classification, and organ detection) spanning 72 fine-grained tasks.
Extensive experiments reveal that existing video FMs struggle to generalize
across varied surgical video analysis tasks, whereas pretraining on SurgBench-P
yields substantial performance improvements and superior cross-domain
generalization to unseen procedures and modalities. Our dataset and code are
available upon request.

</details>


### [124] [DragNeXt: Rethinking Drag-Based Image Editing](https://arxiv.org/abs/2506.07611)
*Yuan Zhou,Junbao Zhou,Qingshan Xu,Kesen Zhao,Yuxuan Wang,Hao Fei,Richang Hong,Hanwang Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种新的基于拖拽的图像编辑方法DragNeXt，通过明确指定拖拽区域和类型解决模糊性问题，并简化了编辑流程。


<details>
  <summary>Details</summary>
Motivation: 现有基于拖拽的图像编辑方法存在模糊性和复杂性问题，难以满足用户需求。

Method: 将拖拽编辑重新定义为用户指定区域的变形、旋转和平移，提出Latent Region Optimization (LRO)框架，并通过Progressive Backward Self-Intervention (PBSI)实现优化。

Result: DragNeXt在NextBench上表现优异，显著优于现有方法。

Conclusion: DragNeXt通过区域级结构信息和渐进式指导，简化了编辑流程并提升了结果质量。

Abstract: Drag-Based Image Editing (DBIE), which allows users to manipulate images by
directly dragging objects within them, has recently attracted much attention
from the community. However, it faces two key challenges:
(\emph{\textcolor{magenta}{i}}) point-based drag is often highly ambiguous and
difficult to align with users' intentions; (\emph{\textcolor{magenta}{ii}})
current DBIE methods primarily rely on alternating between motion supervision
and point tracking, which is not only cumbersome but also fails to produce
high-quality results. These limitations motivate us to explore DBIE from a new
perspective -- redefining it as deformation, rotation, and translation of
user-specified handle regions. Thereby, by requiring users to explicitly
specify both drag areas and types, we can effectively address the ambiguity
issue. Furthermore, we propose a simple-yet-effective editing framework, dubbed
\textcolor{SkyBlue}{\textbf{DragNeXt}}. It unifies DBIE as a Latent Region
Optimization (LRO) problem and solves it through Progressive Backward
Self-Intervention (PBSI), simplifying the overall procedure of DBIE while
further enhancing quality by fully leveraging region-level structure
information and progressive guidance from intermediate drag states. We validate
\textcolor{SkyBlue}{\textbf{DragNeXt}} on our NextBench, and extensive
experiments demonstrate that our proposed method can significantly outperform
existing approaches. Code will be released on github.

</details>


### [125] [Scaling Human Activity Recognition: A Comparative Evaluation of Synthetic Data Generation and Augmentation Techniques](https://arxiv.org/abs/2506.07612)
*Zikang Leng,Archith Iyer,Thomas Plötz*

Main category: cs.CV

TL;DR: 论文比较了两种虚拟IMU数据生成方法（视频和语言）与传统数据增强技术，发现虚拟数据在有限数据条件下显著提升HAR性能。


<details>
  <summary>Details</summary>
Motivation: 解决HAR中标记数据稀缺的问题，探索虚拟IMU数据的生成方法。

Method: 构建大规模虚拟IMU数据集，比较视频、语言和传统数据增强方法在HAR任务中的效果。

Result: 虚拟IMU数据显著优于真实或增强数据，尤其在数据有限时。

Conclusion: 提供了选择数据生成策略的实用建议，并分析了每种方法的优缺点。

Abstract: Human activity recognition (HAR) is often limited by the scarcity of labeled
datasets due to the high cost and complexity of real-world data collection. To
mitigate this, recent work has explored generating virtual inertial measurement
unit (IMU) data via cross-modality transfer. While video-based and
language-based pipelines have each shown promise, they differ in assumptions
and computational cost. Moreover, their effectiveness relative to traditional
sensor-level data augmentation remains unclear. In this paper, we present a
direct comparison between these two virtual IMU generation approaches against
classical data augmentation techniques. We construct a large-scale virtual IMU
dataset spanning 100 diverse activities from Kinetics-400 and simulate sensor
signals at 22 body locations. The three data generation strategies are
evaluated on benchmark HAR datasets (UTD-MHAD, PAMAP2, HAD-AW) using four
popular models. Results show that virtual IMU data significantly improves
performance over real or augmented data alone, particularly under limited-data
conditions. We offer practical guidance on choosing data generation strategies
and highlight the distinct advantages and disadvantages of each approach.

</details>


### [126] [Event-Priori-Based Vision-Language Model for Efficient Visual Understanding](https://arxiv.org/abs/2506.07627)
*Haotong Qin,Cheng Hu,Michele Magno*

Main category: cs.CV

TL;DR: EP-VLM利用动态事件视觉的运动先验，通过稀疏化RGB视觉输入并保留位置信息，显著提升了视觉语言模型的效率，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLMs）计算需求高，难以部署在资源受限的边缘设备上，且视觉输入中存在大量冗余信息。

Method: EP-VLM通过事件数据引导RGB输入的稀疏化，并采用位置保留的标记化策略处理稀疏输入。

Result: EP-VLM在Qwen2-VL-2B上实现了50%的计算量节省，同时保持98%的原始精度。

Conclusion: 事件视觉先验可显著提升VLM效率，为边缘设备上的可持续视觉理解提供了新方向。

Abstract: Large Language Model (LLM)-based Vision-Language Models (VLMs) have
substantially extended the boundaries of visual understanding capabilities.
However, their high computational demands hinder deployment on
resource-constrained edge devices. A key source of inefficiency stems from the
VLM's need to process dense and redundant visual information. Visual inputs
contain significant regions irrelevant to text semantics, rendering the
associated computations ineffective for inference. This paper introduces a
novel Event-Priori-Based Vision-Language Model, termed EP-VLM. Its core
contribution is a novel mechanism leveraging motion priors derived from dynamic
event vision to enhance VLM efficiency. Inspired by human visual cognition,
EP-VLM first employs event data to guide the patch-wise sparsification of RGB
visual inputs, progressively concentrating VLM computation on salient regions
of the visual input. Subsequently, we construct a position-preserving
tokenization strategy for the visual encoder within the VLM architecture. This
strategy processes the event-guided, unstructured, sparse visual input while
accurately preserving positional understanding within the visual input.
Experimental results demonstrate that EP-VLM achieves significant efficiency
improvements while maintaining nearly lossless accuracy compared to baseline
models from the Qwen2-VL series. For instance, against the original
Qwen2-VL-2B, EP-VLM achieves 50% FLOPs savings while retaining 98% of the
original accuracy on the RealWorldQA dataset. This work demonstrates the
potential of event-based vision priors for improving VLM inference efficiency,
paving the way for creating more efficient and deployable VLMs for sustainable
visual understanding at the edge.

</details>


### [127] [HuSc3D: Human Sculpture dataset for 3D object reconstruction](https://arxiv.org/abs/2506.07628)
*Weronika Smolak-Dyżewska,Dawid Malarz,Grzegorz Wilczyński,Rafał Tobiasz,Joanna Waczyńska,Piotr Borycki,Przemysław Spurek*

Main category: cs.CV

TL;DR: HuSc3D是一个专为评估3D重建模型在真实采集挑战下的性能而设计的新数据集，包含六个高度细节化的白色雕塑场景，具有复杂穿孔和低纹理变化，图像数量差异显著。


<details>
  <summary>Details</summary>
Motivation: 现有数据集集中于理想化合成或精心捕获的真实数据，未能反映真实场景中的动态背景和采集差异（如白平衡问题）。

Method: 提出HuSc3D数据集，包含六个白色雕塑场景，具有复杂几何细节和低纹理变化，图像数量差异显著。

Result: 评估流行3D重建方法，显示HuSc3D能有效区分模型性能，揭示方法对几何细节、颜色模糊和数据变化的敏感性。

Conclusion: HuSc3D填补了现有数据集的不足，为3D重建模型在真实挑战下的评估提供了更全面的基准。

Abstract: 3D scene reconstruction from 2D images is one of the most important tasks in
computer graphics. Unfortunately, existing datasets and benchmarks concentrate
on idealized synthetic or meticulously captured realistic data. Such benchmarks
fail to convey the inherent complexities encountered in newly acquired
real-world scenes. In such scenes especially those acquired outside, the
background is often dynamic, and by popular usage of cell phone cameras, there
might be discrepancies in, e.g., white balance. To address this gap, we present
HuSc3D, a novel dataset specifically designed for rigorous benchmarking of 3D
reconstruction models under realistic acquisition challenges. Our dataset
uniquely features six highly detailed, fully white sculptures characterized by
intricate perforations and minimal textural and color variation. Furthermore,
the number of images per scene varies significantly, introducing the additional
challenge of limited training data for some instances alongside scenes with a
standard number of views. By evaluating popular 3D reconstruction methods on
this diverse dataset, we demonstrate the distinctiveness of HuSc3D in
effectively differentiating model performance, particularly highlighting the
sensitivity of methods to fine geometric details, color ambiguity, and varying
data availability--limitations often masked by more conventional datasets.

</details>


### [128] [HieraEdgeNet: A Multi-Scale Edge-Enhanced Framework for Automated Pollen Recognition](https://arxiv.org/abs/2506.07637)
*Yuchong Long,Wen Sun,Ningxiao Sun,Wenxiao Wang,Chao Li,Shan Yin*

Main category: cs.CV

TL;DR: HieraEdgeNet是一种多尺度边缘增强框架，通过三个协同模块显著提升了自动化花粉识别的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 传统花粉识别方法效率低且主观性强，现有深度学习模型在微小目标（如花粉）的定位精度上表现不佳。

Method: 提出HieraEdgeNet框架，包含三个模块：HEM（多尺度边缘特征提取）、SEF（边缘与语义信息融合）和CSPOKM（细节优化）。

Result: 在120类花粉数据集上，HieraEdgeNet的mAP@.5达到0.9501，优于YOLOv12n和RT-DETR等基线模型。

Conclusion: HieraEdgeNet通过系统整合边缘信息，为高精度、高效率的微观目标检测提供了强大解决方案。

Abstract: Automated pollen recognition is vital to paleoclimatology, biodiversity
monitoring, and public health, yet conventional methods are hampered by
inefficiency and subjectivity. Existing deep learning models often struggle to
achieve the requisite localization accuracy for microscopic targets like
pollen, which are characterized by their minute size, indistinct edges, and
complex backgrounds. To overcome this limitation, we introduce HieraEdgeNet, a
multi-scale edge-enhancement framework. The framework's core innovation is the
introduction of three synergistic modules: the Hierarchical Edge Module (HEM),
which explicitly extracts a multi-scale pyramid of edge features that
corresponds to the semantic hierarchy at early network stages; the Synergistic
Edge Fusion (SEF) module, for deeply fusing these edge priors with semantic
information at each respective scale; and the Cross Stage Partial Omni-Kernel
Module (CSPOKM), which maximally refines the most detail-rich feature layers
using an Omni-Kernel operator - comprising anisotropic large-kernel
convolutions and mixed-domain attention - all within a computationally
efficient Cross-Stage Partial (CSP) framework. On a large-scale dataset
comprising 120 pollen classes, HieraEdgeNet achieves a mean Average Precision
(mAP@.5) of 0.9501, significantly outperforming state-of-the-art baseline
models such as YOLOv12n and RT-DETR. Furthermore, qualitative analysis confirms
that our approach generates feature representations that are more precisely
focused on object boundaries. By systematically integrating edge information,
HieraEdgeNet provides a robust and powerful solution for high-precision,
high-efficiency automated detection of microscopic objects.

</details>


### [129] [Synthetic Visual Genome](https://arxiv.org/abs/2506.07643)
*Jae Sung Park,Zixian Ma,Linjie Li,Chenhao Zheng,Cheng-Yu Hsieh,Ximing Lu,Khyathi Chandu,Quan Kong,Norimasa Kobori,Ali Farhadi,Yejin Choi,Ranjay Krishna*

Main category: cs.CV

TL;DR: ROBIN是一个通过密集标注关系训练的多模态语言模型，能够生成高质量的场景图，并在关系理解任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态语言模型在视觉理解方面取得进展，但在关系和生成方面的精确推理仍具挑战性。

Method: 通过合成数据集SVG训练ROBIN，并引入SG-EDIT框架利用GPT-4o优化场景图。

Result: ROBIN-3B模型在关系理解任务中超越同类模型，并在指代表达理解中达到88.9分。

Conclusion: 精炼的场景图数据对提升视觉推理任务性能至关重要。

Abstract: Reasoning over visual relationships-spatial, functional, interactional,
social, etc.-is considered to be a fundamental component of human cognition.
Yet, despite the major advances in visual comprehension in multimodal language
models (MLMs), precise reasoning over relationships and their generations
remains a challenge. We introduce ROBIN: an MLM instruction-tuned with densely
annotated relationships capable of constructing high-quality dense scene graphs
at scale. To train ROBIN, we curate SVG, a synthetic scene graph dataset by
completing the missing relations of selected objects in existing scene graphs
using a teacher MLM and a carefully designed filtering process to ensure
high-quality. To generate more accurate and rich scene graphs at scale for any
image, we introduce SG-EDIT: a self-distillation framework where GPT-4o further
refines ROBIN's predicted scene graphs by removing unlikely relations and/or
suggesting relevant ones. In total, our dataset contains 146K images and 5.6M
relationships for 2.6M objects. Results show that our ROBIN-3B model, despite
being trained on less than 3 million instances, outperforms similar-size models
trained on over 300 million instances on relationship understanding benchmarks,
and even surpasses larger models up to 13B parameters. Notably, it achieves
state-of-the-art performance in referring expression comprehension with a score
of 88.9, surpassing the previous best of 87.4. Our results suggest that
training on the refined scene graph data is crucial to maintaining high
performance across diverse visual reasoning task.

</details>


### [130] [FMaMIL: Frequency-Driven Mamba Multi-Instance Learning for Weakly Supervised Lesion Segmentation in Medical Images](https://arxiv.org/abs/2506.07652)
*Hangbei Cheng,Xiaorong Dong,Xueyu Liu,Jianan Zhang,Xuetao Ma,Mingqiang Wei,Liansheng Wang,Junxin Chen,Yongfei Wu*

Main category: cs.CV

TL;DR: FMaMIL是一种基于图像级标签的弱监督病变分割框架，通过两阶段方法实现高效分割。


<details>
  <summary>Details</summary>
Motivation: 解决组织病理学图像中病变分割的挑战，尤其是像素级标注成本高的问题。

Method: 两阶段框架：第一阶段使用Mamba编码器和频域编码模块生成CAMs；第二阶段通过软标签监督和自校正机制优化伪标签。

Result: 在公开和私有数据集上表现优于现有弱监督方法，无需像素级标注。

Conclusion: FMaMIL在数字病理学中具有高效性和应用潜力。

Abstract: Accurate lesion segmentation in histopathology images is essential for
diagnostic interpretation and quantitative analysis, yet it remains challenging
due to the limited availability of costly pixel-level annotations. To address
this, we propose FMaMIL, a novel two-stage framework for weakly supervised
lesion segmentation based solely on image-level labels. In the first stage, a
lightweight Mamba-based encoder is introduced to capture long-range
dependencies across image patches under the MIL paradigm. To enhance spatial
sensitivity and structural awareness, we design a learnable frequency-domain
encoding module that supplements spatial-domain features with spectrum-based
information. CAMs generated in this stage are used to guide segmentation
training. In the second stage, we refine the initial pseudo labels via a
CAM-guided soft-label supervision and a self-correction mechanism, enabling
robust training even under label noise. Extensive experiments on both public
and private histopathology datasets demonstrate that FMaMIL outperforms
state-of-the-art weakly supervised methods without relying on pixel-level
annotations, validating its effectiveness and potential for digital pathology
applications.

</details>


### [131] [ProSplat: Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline Sparse Views](https://arxiv.org/abs/2506.07670)
*Xiaohan Lu,Jiaye Fu,Jiaqi Zhang,Zetian Song,Chuanmin Jia,Siwei Ma*

Main category: cs.CV

TL;DR: ProSplat是一种两阶段前馈框架，用于在宽基线条件下实现高保真渲染，通过3D高斯生成器和改进模型提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯溅射在宽基线场景下因纹理细节不足和几何不一致导致的性能下降问题。

Method: 两阶段框架：首先生成3D高斯基元，然后通过改进模型（基于扩散模型）增强渲染视图，结合MORI和DWEA技术。

Result: 在RealEstate10K和DL3DV-10K数据集上，PSNR平均提升1 dB。

Conclusion: ProSplat在宽基线条件下显著提升了渲染质量，优于现有方法。

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) has recently demonstrated promising
results for novel view synthesis (NVS) from sparse input views, particularly
under narrow-baseline conditions. However, its performance significantly
degrades in wide-baseline scenarios due to limited texture details and
geometric inconsistencies across views. To address these challenges, in this
paper, we propose ProSplat, a two-stage feed-forward framework designed for
high-fidelity rendering under wide-baseline conditions. The first stage
involves generating 3D Gaussian primitives via a 3DGS generator. In the second
stage, rendered views from these primitives are enhanced through an improvement
model. Specifically, this improvement model is based on a one-step diffusion
model, further optimized by our proposed Maximum Overlap Reference view
Injection (MORI) and Distance-Weighted Epipolar Attention (DWEA). MORI
supplements missing texture and color by strategically selecting a reference
view with maximum viewpoint overlap, while DWEA enforces geometric consistency
using epipolar constraints. Additionally, we introduce a divide-and-conquer
training strategy that aligns data distributions between the two stages through
joint optimization. We evaluate ProSplat on the RealEstate10K and DL3DV-10K
datasets under wide-baseline settings. Experimental results demonstrate that
ProSplat achieves an average improvement of 1 dB in PSNR compared to recent
SOTA methods.

</details>


### [132] [OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian Splatting](https://arxiv.org/abs/2506.07697)
*Jens Piekenbrinck,Christian Schmidt,Alexander Hermans,Narunas Vaskevicius,Timm Linder,Bastian Leibe*

Main category: cs.CV

TL;DR: OpenSplat3D扩展了3D高斯泼溅（3DGS）的能力，实现了无需手动标注的开放词汇3D实例分割。


<details>
  <summary>Details</summary>
Motivation: 提升3D场景理解的细粒度，通过结合语义信息与高斯泼溅技术，实现基于自然语言的灵活对象识别与分割。

Method: 利用特征泼溅技术将语义信息与高斯关联，结合Segment Anything Model的实例掩码和对比损失，以及视觉语言模型的语言嵌入。

Result: 在LERF-mask、LERF-OVS和ScanNet++验证集上展示了方法的有效性。

Conclusion: OpenSplat3D为3D场景的开放词汇实例分割提供了一种高效且灵活的方法。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a powerful representation for
neural scene reconstruction, offering high-quality novel view synthesis while
maintaining computational efficiency. In this paper, we extend the capabilities
of 3DGS beyond pure scene representation by introducing an approach for
open-vocabulary 3D instance segmentation without requiring manual labeling,
termed OpenSplat3D. Our method leverages feature-splatting techniques to
associate semantic information with individual Gaussians, enabling fine-grained
scene understanding. We incorporate Segment Anything Model instance masks with
a contrastive loss formulation as guidance for the instance features to achieve
accurate instance-level segmentation. Furthermore, we utilize language
embeddings of a vision-language model, allowing for flexible, text-driven
instance identification. This combination enables our system to identify and
segment arbitrary objects in 3D scenes based on natural language descriptions.
We show results on LERF-mask and LERF-OVS as well as the full ScanNet++
validation set, demonstrating the effectiveness of our approach.

</details>


### [133] [NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D Generation](https://arxiv.org/abs/2506.07698)
*Yuxiao Yang,Peihao Li,Yuhong Zhang,Junzhe Lu,Xianglong He,Minghan Qin,Weitao Wang,Haoqian Wang*

Main category: cs.CV

TL;DR: NOVA3D是一个创新的单图像到3D生成框架，通过利用预训练视频扩散模型的3D先验和几何信息，提升多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖图像扩散模型生成3D内容，但缺乏足够的3D先验，导致多视角一致性不足。

Method: 提出Geometry-Temporal Alignment (GTA)注意力机制和去冲突几何融合算法，整合颜色与几何信息。

Result: 实验验证NOVA3D在多视角一致性和纹理保真度上优于现有基线。

Conclusion: NOVA3D通过强3D先验和几何优化，显著提升了单图像到3D生成的质量。

Abstract: 3D AI-generated content (AIGC) has made it increasingly accessible for anyone
to become a 3D content creator. While recent methods leverage Score
Distillation Sampling to distill 3D objects from pretrained image diffusion
models, they often suffer from inadequate 3D priors, leading to insufficient
multi-view consistency. In this work, we introduce NOVA3D, an innovative
single-image-to-3D generation framework. Our key insight lies in leveraging
strong 3D priors from a pretrained video diffusion model and integrating
geometric information during multi-view video fine-tuning. To facilitate
information exchange between color and geometric domains, we propose the
Geometry-Temporal Alignment (GTA) attention mechanism, thereby improving
generalization and multi-view consistency. Moreover, we introduce the
de-conflict geometry fusion algorithm, which improves texture fidelity by
addressing multi-view inaccuracies and resolving discrepancies in pose
alignment. Extensive experiments validate the superiority of NOVA3D over
existing baselines.

</details>


### [134] [Consistent Video Editing as Flow-Driven Image-to-Video Generation](https://arxiv.org/abs/2506.07713)
*Ge Wang,Songlin Fan,Hangxu Liu,Quanjian Song,Hewei Wang,Jinfeng Xu*

Main category: cs.CV

TL;DR: FlowV2V提出了一种基于光流的视频编辑方法，通过分解任务为第一帧编辑和条件图像到视频生成，解决了复杂运动建模和时序一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法难以处理复杂运动模式（如非刚性物体运动），且局限于物体替换任务。光流为复杂运动建模提供了新思路。

Method: FlowV2V将任务分解为第一帧编辑和条件I2V生成，通过模拟伪光流序列确保编辑过程中的时序一致性。

Result: 在DAVIS-EDIT数据集上，FlowV2V在DOVER和warping error指标上分别提升了13.67%和50.66%，表现出优越的时序一致性和样本质量。

Conclusion: FlowV2V通过光流驱动的I2V生成，显著提升了视频编辑的复杂运动建模能力和时序一致性，为视频编辑任务提供了新范式。

Abstract: With the prosper of video diffusion models, down-stream applications like
video editing have been significantly promoted without consuming much
computational cost. One particular challenge in this task lies at the motion
transfer process from the source video to the edited one, where it requires the
consideration of the shape deformation in between, meanwhile maintaining the
temporal consistency in the generated video sequence. However, existing methods
fail to model complicated motion patterns for video editing, and are
fundamentally limited to object replacement, where tasks with non-rigid object
motions like multi-object and portrait editing are largely neglected. In this
paper, we observe that optical flows offer a promising alternative in complex
motion modeling, and present FlowV2V to re-investigate video editing as a task
of flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V
decomposes the entire pipeline into first-frame editing and conditional I2V
generation, and simulates pseudo flow sequence that aligns with the deformed
shape, thus ensuring the consistency during editing. Experimental results on
DAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error
illustrate the superior temporal consistency and sample quality of FlowV2V
compared to existing state-of-the-art ones. Furthermore, we conduct
comprehensive ablation studies to analyze the internal functionalities of the
first-frame paradigm and flow alignment in the proposed method.

</details>


### [135] [ReverB-SNN: Reversing Bit of the Weight and Activation for Spiking Neural Networks](https://arxiv.org/abs/2506.07720)
*Yufei Guo,Yuhan Zhang,Zhou Jie,Xiaode Liu,Xin Tong,Yuanpei Chen,Weihang Peng,Zhe Ma*

Main category: cs.CV

TL;DR: 提出了一种名为ReverB-SNN的方法，通过反转权重和激活的比特位，结合实值激活和二进制权重，提升SNN的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 解决SNN中二进制激活映射信息不足导致的精度下降问题。

Method: 采用实值激活和二进制权重，并引入可训练因子自适应学习权重幅度，推理时通过重参数化恢复标准形式。

Result: 在多种网络架构和数据集上表现优于现有方法。

Conclusion: ReverB-SNN在保持SNN高效性的同时显著提升了精度。

Abstract: The Spiking Neural Network (SNN), a biologically inspired neural network
infrastructure, has garnered significant attention recently. SNNs utilize
binary spike activations for efficient information transmission, replacing
multiplications with additions, thereby enhancing energy efficiency. However,
binary spike activation maps often fail to capture sufficient data information,
resulting in reduced accuracy. To address this challenge, we advocate reversing
the bit of the weight and activation for SNNs, called \textbf{ReverB-SNN},
inspired by recent findings that highlight greater accuracy degradation from
quantizing activations compared to weights. Specifically, our method employs
real-valued spike activations alongside binary weights in SNNs. This preserves
the event-driven and multiplication-free advantages of standard SNNs while
enhancing the information capacity of activations. Additionally, we introduce a
trainable factor within binary weights to adaptively learn suitable weight
amplitudes during training, thereby increasing network capacity. To maintain
efficiency akin to vanilla \textbf{ReverB-SNN}, our trainable binary weight
SNNs are converted back to standard form using a re-parameterization technique
during inference. Extensive experiments across various network architectures
and datasets, both static and dynamic, demonstrate that our approach
consistently outperforms state-of-the-art methods.

</details>


### [136] [ETA: Efficiency through Thinking Ahead, A Dual Approach to Self-Driving with Large Models](https://arxiv.org/abs/2506.07725)
*Shadi Hamdan,Chonghao Sima,Zetong Yang,Hongyang Li,Fatma Güney*

Main category: cs.CV

TL;DR: 论文提出了一种异步系统ETA，通过将大模型的计算提前到过去的时间步，实现快速响应，同时结合小模型实时特征，提升自动驾驶系统的性能。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统中大模型推理速度慢的问题，同时不牺牲性能。

Method: 引入ETA异步系统，通过提前计算大模型的特征、小模型实时提取特征，并结合动作掩码机制。

Result: 在Bench2Drive CARLA Leaderboard-v2基准测试中，性能提升8%，驾驶得分69.53，推理速度50ms。

Conclusion: ETA系统成功平衡了大模型的高性能和小模型的快速响应，为自动驾驶系统提供了高效解决方案。

Abstract: How can we benefit from large models without sacrificing inference speed, a
common dilemma in self-driving systems? A prevalent solution is a dual-system
architecture, employing a small model for rapid, reactive decisions and a
larger model for slower but more informative analyses. Existing dual-system
designs often implement parallel architectures where inference is either
directly conducted using the large model at each current frame or retrieved
from previously stored inference results. However, these works still struggle
to enable large models for a timely response to every online frame. Our key
insight is to shift intensive computations of the current frame to previous
time steps and perform a batch inference of multiple time steps to make large
models respond promptly to each time step. To achieve the shifting, we
introduce Efficiency through Thinking Ahead (ETA), an asynchronous system
designed to: (1) propagate informative features from the past to the current
frame using future predictions from the large model, (2) extract current frame
features using a small model for real-time responsiveness, and (3) integrate
these dual features via an action mask mechanism that emphasizes
action-critical image regions. Evaluated on the Bench2Drive CARLA
Leaderboard-v2 benchmark, ETA advances state-of-the-art performance by 8% with
a driving score of 69.53 while maintaining a near-real-time inference speed at
50 ms.

</details>


### [137] [SpikeSMOKE: Spiking Neural Networks for Monocular 3D Object Detection with Cross-Scale Gated Coding](https://arxiv.org/abs/2506.07737)
*Xuemei Chen,Huamin Wang,Hangchi Shen,Shukai Duan,Shiping Wen,Tingwen Huang*

Main category: cs.CV

TL;DR: 论文提出了一种基于脉冲神经网络（SNN）的低功耗单目3D目标检测方法SpikeSMOKE，通过跨尺度门控编码机制（CSGC）增强特征表示，并设计了轻量级残差块以减少计算量。实验表明，该方法在KITTI数据集上性能提升显著，同时大幅降低能耗。


<details>
  <summary>Details</summary>
Motivation: 随着3D目标检测在自动驾驶等领域的广泛应用，能耗问题日益突出。SNN因其低功耗特性成为潜在解决方案，但其离散信号会导致信息损失，限制了特征表达能力。

Method: 提出SpikeSMOKE架构，结合跨尺度门控编码机制（CSGC）增强特征表示，并设计轻量级残差块以减少计算量和加速训练。

Result: 在KITTI数据集上，SpikeSMOKE性能显著提升（AP|R11指标），同时能耗降低72.2%，轻量版进一步减少参数和计算量。

Conclusion: SpikeSMOKE为低功耗单目3D目标检测提供了有效解决方案，平衡了性能与能耗，具有实际应用潜力。

Abstract: Low energy consumption for 3D object detection is an important research area
because of the increasing energy consumption with their wide application in
fields such as autonomous driving. The spiking neural networks (SNNs) with
low-power consumption characteristics can provide a novel solution for this
research. Therefore, we apply SNNs to monocular 3D object detection and propose
the SpikeSMOKE architecture in this paper, which is a new attempt for low-power
monocular 3D object detection. As we all know, discrete signals of SNNs will
generate information loss and limit their feature expression ability compared
with the artificial neural networks (ANNs).In order to address this issue,
inspired by the filtering mechanism of biological neuronal synapses, we propose
a cross-scale gated coding mechanism(CSGC), which can enhance feature
representation by combining cross-scale fusion of attentional methods and gated
filtering mechanisms.In addition, to reduce the computation and increase the
speed of training, we present a novel light-weight residual block that can
maintain spiking computing paradigm and the highest possible detection
performance. Compared to the baseline SpikeSMOKE under the 3D Object Detection,
the proposed SpikeSMOKE with CSGC can achieve 11.78 (+2.82, Easy), 10.69 (+3.2,
Moderate), and 10.48 (+3.17, Hard) on the KITTI autonomous driving dataset by
AP|R11 at 0.7 IoU threshold, respectively. It is important to note that the
results of SpikeSMOKE can significantly reduce energy consumption compared to
the results on SMOKE. For example,the energy consumption can be reduced by
72.2% on the hard category, while the detection performance is reduced by only
4%. SpikeSMOKE-L (lightweight) can further reduce the amount of parameters by 3
times and computation by 10 times compared to SMOKE.

</details>


### [138] [AssetDropper: Asset Extraction via Diffusion Models with Reward-Driven Optimization](https://arxiv.org/abs/2506.07738)
*Lanjiong Li,Guanhua Zhao,Lingting Zhu,Zeyu Cai,Lequan Yu,Jian Zhang,Zeyu Wang*

Main category: cs.CV

TL;DR: AssetDropper是一个从参考图像中提取标准化资产的框架，解决了设计师在开放世界场景中高效提取高质量资产的挑战。


<details>
  <summary>Details</summary>
Motivation: 设计师需要标准化资产库，但现有生成模型未充分满足这一需求。

Method: 提出AssetDropper框架，通过预训练奖励模型实现闭环反馈，优化资产提取。

Result: 在合成和真实数据集上取得最优性能。

Conclusion: AssetDropper为设计师提供了高效的资产提取工具，推动了相关下游任务的研究。

Abstract: Recent research on generative models has primarily focused on creating
product-ready visual outputs; however, designers often favor access to
standardized asset libraries, a domain that has yet to be significantly
enhanced by generative capabilities. Although open-world scenes provide ample
raw materials for designers, efficiently extracting high-quality, standardized
assets remains a challenge. To address this, we introduce AssetDropper, the
first framework designed to extract assets from reference images, providing
artists with an open-world asset palette. Our model adeptly extracts a front
view of selected subjects from input images, effectively handling complex
scenarios such as perspective distortion and subject occlusion. We establish a
synthetic dataset of more than 200,000 image-subject pairs and a real-world
benchmark with thousands more for evaluation, facilitating the exploration of
future research in downstream tasks. Furthermore, to ensure precise asset
extraction that aligns well with the image prompts, we employ a pre-trained
reward model to fulfill a closed-loop with feedback. We design the reward model
to perform an inverse task that pastes the extracted assets back into the
reference sources, which assists training with additional consistency and
mitigates hallucination. Extensive experiments show that, with the aid of
reward-driven optimization, AssetDropper achieves the state-of-the-art results
in asset extraction. Project page: AssetDropper.github.io.

</details>


### [139] [ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models](https://arxiv.org/abs/2506.07739)
*Jing Zhong,Jun Yin,Peilin Li,Pengyu Zeng,Miao Zhang,Shuai Lu,Ran Luo*

Main category: cs.CV

TL;DR: 该研究提出了一个基于视觉语言模型的框架ArchiLense，用于自动识别和分类建筑风格，并通过数据集ArchDiffBench实现了高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统建筑文化研究依赖主观专家解读和历史文献，存在区域偏见和解释范围有限的问题。

Method: 构建了ArchDiffBench数据集，并提出ArchiLense框架，结合计算机视觉和深度学习技术，实现建筑风格的自动识别与分类。

Result: ArchiLense在风格识别上表现优异，与专家标注的一致性达92.4%，分类准确率为84.5%。

Conclusion: 该方法超越了传统分析的主观性，为建筑文化比较研究提供了更客观准确的视角。

Abstract: Architectural cultures across regions are characterized by stylistic
diversity, shaped by historical, social, and technological contexts in addition
to geograph-ical conditions. Understanding architectural styles requires the
ability to describe and analyze the stylistic features of different architects
from various regions through visual observations of architectural imagery.
However, traditional studies of architectural culture have largely relied on
subjective expert interpretations and historical literature reviews, often
suffering from regional biases and limited ex-planatory scope. To address these
challenges, this study proposes three core contributions: (1) We construct a
professional architectural style dataset named ArchDiffBench, which comprises
1,765 high-quality architectural images and their corresponding style
annotations, collected from different regions and historical periods. (2) We
propose ArchiLense, an analytical framework grounded in Vision-Language Models
and constructed using the ArchDiffBench dataset. By integrating ad-vanced
computer vision techniques, deep learning, and machine learning algo-rithms,
ArchiLense enables automatic recognition, comparison, and precise
classi-fication of architectural imagery, producing descriptive language
outputs that ar-ticulate stylistic differences. (3) Extensive evaluations show
that ArchiLense achieves strong performance in architectural style recognition,
with a 92.4% con-sistency rate with expert annotations and 84.5% classification
accuracy, effec-tively capturing stylistic distinctions across images. The
proposed approach transcends the subjectivity inherent in traditional analyses
and offers a more objective and accurate perspective for comparative studies of
architectural culture.

</details>


### [140] [Flow-Anything: Learning Real-World Optical Flow Estimation from Large-Scale Single-view Images](https://arxiv.org/abs/2506.07740)
*Yingping Liang,Ying Fu,Yutao Hu,Wenqi Shao,Jiaming Liu,Debing Zhang*

Main category: cs.CV

TL;DR: Flow-Anything 是一个大规模数据生成框架，用于从单视角图像学习光流估计，解决了合成数据与实际应用之间的领域差距。


<details>
  <summary>Details</summary>
Motivation: 现有光流估计方法依赖合成数据训练，导致实际应用中的领域差距，限制了数据扩展的效益。

Method: 通过单视角图像生成3D表示，结合虚拟相机渲染光流和新视角图像，并开发了对象无关的体积渲染和深度感知修复模块。

Result: 生成的FA-Flow数据集显著提升了光流估计性能，优于现有无监督和监督方法。

Conclusion: Flow-Anything 展示了从真实世界图像生成训练数据的优势，可作为基础模型提升下游视频任务性能。

Abstract: Optical flow estimation is a crucial subfield of computer vision, serving as
a foundation for video tasks. However, the real-world robustness is limited by
animated synthetic datasets for training. This introduces domain gaps when
applied to real-world applications and limits the benefits of scaling up
datasets. To address these challenges, we propose \textbf{Flow-Anything}, a
large-scale data generation framework designed to learn optical flow estimation
from any single-view images in the real world. We employ two effective steps to
make data scaling-up promising. First, we convert a single-view image into a 3D
representation using advanced monocular depth estimation networks. This allows
us to render optical flow and novel view images under a virtual camera. Second,
we develop an Object-Independent Volume Rendering module and a Depth-Aware
Inpainting module to model the dynamic objects in the 3D representation. These
two steps allow us to generate realistic datasets for training from large-scale
single-view images, namely \textbf{FA-Flow Dataset}. For the first time, we
demonstrate the benefits of generating optical flow training data from
large-scale real-world images, outperforming the most advanced unsupervised
methods and supervised methods on synthetic datasets. Moreover, our models
serve as a foundation model and enhance the performance of various downstream
video tasks.

</details>


### [141] [Difference Inversion: Interpolate and Isolate the Difference with Token Consistency for Image Analogy Generation](https://arxiv.org/abs/2506.07750)
*Hyunsoo Kim,Donghyun Kim,Suhyun Kim*

Main category: cs.CV

TL;DR: 提出了一种名为“Difference Inversion”的方法，通过提取A和A'之间的差异并应用于B，生成B'，解决了现有方法对特定模型的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如视觉上下文学习或视觉指令）通常局限于特定模型（如InstructPix2Pix），可能导致偏见或编辑能力受限。

Method: 通过Delta插值提取差异，结合Token一致性损失和零初始化Token嵌入，构建适合稳定扩散模型的“完整提示”。

Result: 实验表明，Difference Inversion在定量和定性上均优于现有基线，能够以模型无关的方式生成更可行的B'。

Conclusion: Difference Inversion是一种通用且高效的方法，适用于多种扩散模型，解决了模型依赖性问题。

Abstract: How can we generate an image B' that satisfies A:A'::B:B', given the input
images A,A' and B? Recent works have tackled this challenge through approaches
like visual in-context learning or visual instruction. However, these methods
are typically limited to specific models (e.g. InstructPix2Pix. Inpainting
models) rather than general diffusion models (e.g. Stable Diffusion, SDXL).
This dependency may lead to inherited biases or lower editing capabilities. In
this paper, we propose Difference Inversion, a method that isolates only the
difference from A and A' and applies it to B to generate a plausible B'. To
address model dependency, it is crucial to structure prompts in the form of a
"Full Prompt" suitable for input to stable diffusion models, rather than using
an "Instruction Prompt". To this end, we accurately extract the Difference
between A and A' and combine it with the prompt of B, enabling a plug-and-play
application of the difference. To extract a precise difference, we first
identify it through 1) Delta Interpolation. Additionally, to ensure accurate
training, we propose the 2) Token Consistency Loss and 3) Zero Initialization
of Token Embeddings. Our extensive experiments demonstrate that Difference
Inversion outperforms existing baselines both quantitatively and qualitatively,
indicating its ability to generate more feasible B' in a model-agnostic manner.

</details>


### [142] [Trend-Aware Fashion Recommendation with Visual Segmentation and Semantic Similarity](https://arxiv.org/abs/2506.07773)
*Mohamed Djilani,Nassim Ali Ousalah,Nidhal Eddine Chenni*

Main category: cs.CV

TL;DR: 提出了一种结合视觉表示、语义分割和用户行为模拟的时尚推荐系统，通过加权评分函数实现个性化推荐，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决时尚推荐中如何平衡个人风格与流行趋势的问题，同时模拟真实用户行为以提高推荐质量。

Method: 使用语义分割提取服装区域特征，结合预训练CNN提取视觉嵌入，通过用户行为模拟生成购买历史，加权评分函数融合视觉相似性、语义一致性和流行度。

Result: 在DeepFashion数据集上，ResNet-50表现最佳，类别相似性达64.95%，流行度MAE最低。消融实验验证了视觉与流行度线索的互补作用。

Conclusion: 该方法为个性化时尚推荐提供了可扩展框架，有效平衡个人风格与流行趋势。

Abstract: We introduce a trend-aware and visually-grounded fashion recommendation
system that integrates deep visual representations, garment-aware segmentation,
semantic category similarity and user behavior simulation. Our pipeline
extracts focused visual embeddings by masking non-garment regions via semantic
segmentation followed by feature extraction using pretrained CNN backbones
(ResNet-50, DenseNet-121, VGG16). To simulate realistic shopping behavior, we
generate synthetic purchase histories influenced by user-specific trendiness
and item popularity. Recommendations are computed using a weighted scoring
function that fuses visual similarity, semantic coherence and popularity
alignment. Experiments on the DeepFashion dataset demonstrate consistent gender
alignment and improved category relevance, with ResNet-50 achieving 64.95%
category similarity and lowest popularity MAE. An ablation study confirms the
complementary roles of visual and popularity cues. Our method provides a
scalable framework for personalized fashion recommendations that balances
individual style with emerging trends. Our implementation is available at
https://github.com/meddjilani/FashionRecommender

</details>


### [143] [Language-Vision Planner and Executor for Text-to-Visual Reasoning](https://arxiv.org/abs/2506.07778)
*Yichang Xu,Gaowen Liu,Ramana Rao Kompella,Sihao Hu,Tiansheng Huang,Fatih Ilhan,Selim Furkan Tekin,Zachary Yahn,Ling Liu*

Main category: cs.CV

TL;DR: VLAgent是一个结合规划与执行的多模态视觉-文本推理系统，通过上下文学习优化逻辑推理，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）在泛化性能上表现不足，VLAgent旨在通过规划与执行结合解决这一问题。

Method: VLAgent通过上下文学习微调LLM生成分步计划，设计语法-语义解析器修正逻辑错误，并采用集成方法提升执行泛化性能。

Result: 在四个视觉推理基准测试中，VLAgent性能显著优于现有VLMs和LLM视觉组合方法。

Conclusion: VLAgent通过优化模块（如SS-Parser和Plan Repairer）显著提升了多模态推理能力。

Abstract: The advancement in large language models (LLMs) and large vision models has
fueled the rapid progress in multi-modal visual-text reasoning capabilities.
However, existing vision-language models (VLMs) to date suffer from
generalization performance. Inspired by recent development in LLMs for visual
reasoning, this paper presents VLAgent, an AI system that can create a
step-by-step visual reasoning plan with an easy-to-understand script and
execute each step of the plan in real time by integrating planning script with
execution verifications via an automated process supported by VLAgent. In the
task planning phase, VLAgent fine-tunes an LLM through in-context learning to
generate a step-by-step planner for each user-submitted text-visual reasoning
task. During the plan execution phase, VLAgent progressively refines the
composition of neuro-symbolic executable modules to generate high-confidence
reasoning results. VLAgent has three unique design characteristics: First, we
improve the quality of plan generation through in-context learning, improving
logic reasoning by reducing erroneous logic steps, incorrect programs, and LLM
hallucinations. Second, we design a syntax-semantics parser to identify and
correct additional logic errors of the LLM-generated planning script prior to
launching the plan executor. Finally, we employ the ensemble method to improve
the generalization performance of our step-executor. Extensive experiments with
four visual reasoning benchmarks (GQA, MME, NLVR2, VQAv2) show that VLAgent
achieves significant performance enhancement for multimodal text-visual
reasoning applications, compared to the exiting representative VLMs and LLM
based visual composition approaches like ViperGPT and VisProg, thanks to the
novel optimization modules of VLAgent back-engine (SS-Parser, Plan Repairer,
Output Verifiers). Code and data will be made available upon paper acceptance.

</details>


### [144] [Design and Evaluation of Deep Learning-Based Dual-Spectrum Image Fusion Methods](https://arxiv.org/abs/2506.07779)
*Beining Xu,Junxian Li*

Main category: cs.CV

TL;DR: 论文提出了一种高质量的双光谱数据集和全面的评估框架，用于可见光和红外图像融合，并验证了融合模型在下游任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前可见光和红外图像融合方法缺乏标准化评估和高质量数据集，限制了研究进展。

Method: 构建了一个包含1,369对对齐图像的数据集，并提出基于融合速度、通用指标和下游任务性能的评估框架。

Result: 实验表明，针对下游任务优化的融合模型在目标检测中表现更优，尤其是在低光和遮挡场景中。

Conclusion: 论文贡献包括高质量数据集、任务感知的评估框架以及对现有融合方法的全面分析，为未来研究提供了方向。

Abstract: Visible images offer rich texture details, while infrared images emphasize
salient targets. Fusing these complementary modalities enhances scene
understanding, particularly for advanced vision tasks under challenging
conditions. Recently, deep learning-based fusion methods have gained attention,
but current evaluations primarily rely on general-purpose metrics without
standardized benchmarks or downstream task performance. Additionally, the lack
of well-developed dual-spectrum datasets and fair algorithm comparisons hinders
progress.
  To address these gaps, we construct a high-quality dual-spectrum dataset
captured in campus environments, comprising 1,369 well-aligned visible-infrared
image pairs across four representative scenarios: daytime, nighttime, smoke
occlusion, and underpasses. We also propose a comprehensive and fair evaluation
framework that integrates fusion speed, general metrics, and object detection
performance using the lang-segment-anything model to ensure fairness in
downstream evaluation.
  Extensive experiments benchmark several state-of-the-art fusion algorithms
under this framework. Results demonstrate that fusion models optimized for
downstream tasks achieve superior performance in target detection, especially
in low-light and occluded scenes. Notably, some algorithms that perform well on
general metrics do not translate to strong downstream performance, highlighting
limitations of current evaluation practices and validating the necessity of our
proposed framework.
  The main contributions of this work are: (1)a campus-oriented dual-spectrum
dataset with diverse and challenging scenes; (2) a task-aware, comprehensive
evaluation framework; and (3) thorough comparative analysis of leading fusion
methods across multiple datasets, offering insights for future development.

</details>


### [145] [Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger](https://arxiv.org/abs/2506.07785)
*Qi Yang,Chenghao Zhang,Lubin Fan,Kun Ding,Jieping Ye,Shiming Xiang*

Main category: cs.CV

TL;DR: 提出了一种多模态RAG框架RCTS，通过构建推理上下文丰富的知识库和树搜索重排序方法，提升LVLMs在VQA任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在知识推理示例稀缺和检索知识响应不稳定方面存在挑战。

Method: 引入自洽评估机制丰富知识库，并提出带启发式奖励的蒙特卡洛树搜索（MCTS-HR）重排序方法。

Result: 在多个VQA数据集上实现最先进性能，显著优于ICL和Vanilla-RAG方法。

Conclusion: RCTS框架通过高质量推理上下文和重排序方法有效提升了LVLMs的性能。

Abstract: Recent advancements in Large Vision Language Models (LVLMs) have
significantly improved performance in Visual Question Answering (VQA) tasks
through multimodal Retrieval-Augmented Generation (RAG). However, existing
methods still face challenges, such as the scarcity of knowledge with reasoning
examples and erratic responses from retrieved knowledge. To address these
issues, in this study, we propose a multimodal RAG framework, termed RCTS,
which enhances LVLMs by constructing a Reasoning Context-enriched knowledge
base and a Tree Search re-ranking method. Specifically, we introduce a
self-consistent evaluation mechanism to enrich the knowledge base with
intrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with
Heuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This
ensures that LVLMs can leverage high-quality contextual reasoning for better
and more consistent responses. Extensive experiments demonstrate that our
framework achieves state-of-the-art performance on multiple VQA datasets,
significantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods.
It highlights the effectiveness of our knowledge base and re-ranking method in
improving LVLMs. Our code is available at https://github.com/yannqi/RCTS-RAG.

</details>


### [146] [Image Reconstruction as a Tool for Feature Analysis](https://arxiv.org/abs/2506.07803)
*Eduard Allakhverdov,Dmitrii Tarasov,Elizaveta Goncharova,Andrey Kuznetsov*

Main category: cs.CV

TL;DR: 论文提出了一种通过图像重建解释视觉特征的新方法，比较了SigLIP和SigLIP2模型，发现基于图像任务预训练的编码器保留更多图像信息，并揭示了特征空间的操作对重建图像的影响。


<details>
  <summary>Details</summary>
Motivation: 理解视觉编码器内部如何表示特征是现代应用中的重要问题，但目前尚不明确。

Method: 通过图像重建方法比较不同视觉编码器（如SigLIP和SigLIP2），并分析其特征空间的信息量和可操作性。

Result: 基于图像任务预训练的编码器保留更多图像信息；特征空间的正交旋转控制颜色编码。

Conclusion: 该方法可应用于任何视觉编码器，揭示了其内部特征空间的结构。

Abstract: Vision encoders are increasingly used in modern applications, from
vision-only models to multimodal systems such as vision-language models.
Despite their remarkable success, it remains unclear how these architectures
represent features internally. Here, we propose a novel approach for
interpreting vision features via image reconstruction. We compare two related
model families, SigLIP and SigLIP2, which differ only in their training
objective, and show that encoders pre-trained on image-based tasks retain
significantly more image information than those trained on non-image tasks such
as contrastive learning. We further apply our method to a range of vision
encoders, ranking them by the informativeness of their feature representations.
Finally, we demonstrate that manipulating the feature space yields predictable
changes in reconstructed images, revealing that orthogonal rotations (rather
than spatial transformations) control color encoding. Our approach can be
applied to any vision encoder, shedding light on the inner structure of its
feature space. The code and model weights to reproduce the experiments are
available in GitHub.

</details>


### [147] [Incorporating Uncertainty-Guided and Top-k Codebook Matching for Real-World Blind Image Super-Resolution](https://arxiv.org/abs/2506.07809)
*Weilei Wen,Tianyi Zhang,Qianqian Zhao,Zhaohui Zheng,Chunle Guo,Xiuli Shao,Chongyi Li*

Main category: cs.CV

TL;DR: 提出了一种基于不确定性引导和Top-k代码书匹配的超分辨率框架（UGTSR），解决了现有方法中特征匹配不准确和纹理细节重建差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有代码书超分辨率方法存在特征匹配不准确和纹理细节重建差的问题。

Method: UGTSR框架包含不确定性学习机制、Top-k特征匹配策略和Align-Attention模块。

Result: 实验结果表明，UGTSR在纹理真实性和重建保真度上显著优于现有方法。

Conclusion: UGTSR框架有效提升了超分辨率任务的性能，代码将公开发布。

Abstract: Recent advancements in codebook-based real image super-resolution (SR) have
shown promising results in real-world applications. The core idea involves
matching high-quality image features from a codebook based on low-resolution
(LR) image features. However, existing methods face two major challenges:
inaccurate feature matching with the codebook and poor texture detail
reconstruction. To address these issues, we propose a novel Uncertainty-Guided
and Top-k Codebook Matching SR (UGTSR) framework, which incorporates three key
components: (1) an uncertainty learning mechanism that guides the model to
focus on texture-rich regions, (2) a Top-k feature matching strategy that
enhances feature matching accuracy by fusing multiple candidate features, and
(3) an Align-Attention module that enhances the alignment of information
between LR and HR features. Experimental results demonstrate significant
improvements in texture realism and reconstruction fidelity compared to
existing methods. We will release the code upon formal publication.

</details>


### [148] [Looking Beyond Visible Cues: Implicit Video Question Answering via Dual-Clue Reasoning](https://arxiv.org/abs/2506.07811)
*Tieyuan Chen,Huabin Liu,Yi Wang,Chaofan Gan,Mingxi Lyu,Gui Zou,Weiyao Lin*

Main category: cs.CV

TL;DR: 论文提出了一种新任务I-VQA，专注于回答无法直接获取显式视觉证据的问题，并提出了IRM推理框架，通过双流建模和视觉增强模块显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有VideoQA方法依赖显式视觉证据，但在涉及符号意义或深层意图的问题上表现不佳，因此需要解决隐式视觉证据的问题。

Method: 提出IRM框架，包含Action-Intent Module（AIM）和Visual Enhancement Module（VEM），通过双流建模和关系推理处理隐式问题。

Result: IRM在I-VQA任务中表现优异，分别超越GPT-4o、OpenAI-o3和VideoChat2 0.76%、1.37%和4.87%，并在类似任务中达到SOTA。

Conclusion: IRM框架有效解决了隐式视觉证据问题，为VideoQA领域提供了新方向。

Abstract: Video Question Answering (VideoQA) aims to answer natural language questions
based on the given video, with prior work primarily focusing on identifying the
duration of relevant segments, referred to as explicit visual evidence.
However, explicit visual evidence is not always directly available,
particularly when questions target symbolic meanings or deeper intentions,
leading to significant performance degradation. To fill this gap, we introduce
a novel task and dataset, $\textbf{I}$mplicit $\textbf{V}$ideo
$\textbf{Q}$uestion $\textbf{A}$nswering (I-VQA), which focuses on answering
questions in scenarios where explicit visual evidence is inaccessible. Given an
implicit question and its corresponding video, I-VQA requires answering based
on the contextual visual cues present within the video. To tackle I-VQA, we
propose a novel reasoning framework, IRM (Implicit Reasoning Model),
incorporating dual-stream modeling of contextual actions and intent clues as
implicit reasoning chains. IRM comprises the Action-Intent Module (AIM) and the
Visual Enhancement Module (VEM). AIM deduces and preserves question-related
dual clues by generating clue candidates and performing relation deduction. VEM
enhances contextual visual representation by leveraging key contextual clues.
Extensive experiments validate the effectiveness of our IRM in I-VQA tasks,
outperforming GPT-4o, OpenAI-o3, and fine-tuned VideoChat2 by $0.76\%$,
$1.37\%$, and $4.87\%$, respectively. Additionally, IRM performs SOTA on
similar implicit advertisement understanding and future prediction in
traffic-VQA. Datasets and codes are available for double-blind review in
anonymous repo: https://github.com/tychen-SJTU/Implicit-VideoQA.

</details>


### [149] [Self-Cascaded Diffusion Models for Arbitrary-Scale Image Super-Resolution](https://arxiv.org/abs/2506.07813)
*Junseo Bang,Joonhee Lee,Kyeonghyun Lee,Haechang Lee,Dong Un Kang,Se Young Chun*

Main category: cs.CV

TL;DR: CasArbi是一种新型的自级联扩散框架，用于任意尺度图像超分辨率，通过逐步增强分辨率实现灵活上采样。


<details>
  <summary>Details</summary>
Motivation: 传统固定尺度超分辨率灵活性不足，而现有方法在连续尺度分布上学习困难，CasArbi旨在解决这一问题。

Method: 采用自级联扩散框架，将大尺度分解为小尺度逐步上采样，结合坐标引导的残差扩散模型学习连续图像表示。

Result: CasArbi在多种任意尺度超分辨率基准测试中，感知和失真性能均优于现有方法。

Conclusion: CasArbi通过逐步扩散策略和连续表示学习，实现了高效且灵活的任意尺度超分辨率。

Abstract: Arbitrary-scale image super-resolution aims to upsample images to any desired
resolution, offering greater flexibility than traditional fixed-scale
super-resolution. Recent approaches in this domain utilize regression-based or
generative models, but many of them are a single-stage upsampling process,
which may be challenging to learn across a wide, continuous distribution of
scaling factors. Progressive upsampling strategies have shown promise in
mitigating this issue, yet their integration with diffusion models for flexible
upscaling remains underexplored. Here, we present CasArbi, a novel
self-cascaded diffusion framework for arbitrary-scale image super-resolution.
CasArbi meets the varying scaling demands by breaking them down into smaller
sequential factors and progressively enhancing the image resolution at each
step with seamless transitions for arbitrary scales. Our novel
coordinate-guided residual diffusion model allows for the learning of
continuous image representations while enabling efficient diffusion sampling.
Extensive experiments demonstrate that our CasArbi outperforms prior arts in
both perceptual and distortion performance metrics across diverse
arbitrary-scale super-resolution benchmarks.

</details>


### [150] [M2Restore: Mixture-of-Experts-based Mamba-CNN Fusion Framework for All-in-One Image Restoration](https://arxiv.org/abs/2506.07814)
*Yongzhen Wang,Yongjun Li,Zhuoran Zheng,Xiao-Ping Zhang,Mingqiang Wei*

Main category: cs.CV

TL;DR: M2Restore提出了一种基于Mixture-of-Experts的Mamba-CNN融合框架，用于高效且鲁棒的一体化图像修复，解决了现有方法在动态退化场景下泛化能力不足和局部细节与全局依赖平衡不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 自然图像常受复合退化（如雨、雪、雾）影响，现有修复方法在动态退化场景下的泛化能力和局部与全局平衡方面表现不足。

Method: 1. 使用CLIP引导的MoE门控机制融合任务条件提示和CLIP语义先验；2. 设计双流架构结合CNN的局部表征能力和Mamba的长程建模效率；3. 引入边缘感知动态门控机制自适应平衡全局与局部修复。

Result: 在多个图像修复基准测试中，M2Restore在视觉质量和定量性能上均表现出优越性。

Conclusion: M2Restore通过创新的架构设计和机制优化，显著提升了图像修复的泛化能力和细节恢复效果。

Abstract: Natural images are often degraded by complex, composite degradations such as
rain, snow, and haze, which adversely impact downstream vision applications.
While existing image restoration efforts have achieved notable success, they
are still hindered by two critical challenges: limited generalization across
dynamically varying degradation scenarios and a suboptimal balance between
preserving local details and modeling global dependencies. To overcome these
challenges, we propose M2Restore, a novel Mixture-of-Experts (MoE)-based
Mamba-CNN fusion framework for efficient and robust all-in-one image
restoration. M2Restore introduces three key contributions: First, to boost the
model's generalization across diverse degradation conditions, we exploit a
CLIP-guided MoE gating mechanism that fuses task-conditioned prompts with
CLIP-derived semantic priors. This mechanism is further refined via cross-modal
feature calibration, which enables precise expert selection for various
degradation types. Second, to jointly capture global contextual dependencies
and fine-grained local details, we design a dual-stream architecture that
integrates the localized representational strength of CNNs with the long-range
modeling efficiency of Mamba. This integration enables collaborative
optimization of global semantic relationships and local structural fidelity,
preserving global coherence while enhancing detail restoration. Third, we
introduce an edge-aware dynamic gating mechanism that adaptively balances
global modeling and local enhancement by reallocating computational attention
to degradation-sensitive regions. This targeted focus leads to more efficient
and precise restoration. Extensive experiments across multiple image
restoration benchmarks validate the superiority of M2Restore in both visual
quality and quantitative performance.

</details>


### [151] [R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving Simulation](https://arxiv.org/abs/2506.07826)
*William Ljungbergh,Bernardo Taveira,Wenzhao Zheng,Adam Tonderski,Chensheng Peng,Fredrik Kahl,Christoffer Petersson,Michael Felsberg,Kurt Keutzer,Masayoshi Tomizuka,Wei Zhan*

Main category: cs.CV

TL;DR: 论文提出R3D2，一种轻量级扩散模型，用于在自动驾驶验证中实现真实3D资产插入，提升场景真实感。


<details>
  <summary>Details</summary>
Motivation: 传统仿真平台资源密集且与现实数据存在领域差距，神经重建方法（如3DGS）虽可扩展但动态对象操作和复用性不足。

Method: R3D2通过一步扩散模型生成逼真的渲染效果（如阴影和光照），训练基于3DGS对象资产的新数据集。

Result: R3D2显著提升插入资产的真实感，支持文本到3D资产插入和跨场景对象转移，实现自动驾驶验证的可扩展性。

Conclusion: R3D2解决了现有方法的局限性，为自动驾驶验证提供了更高效和真实的仿真工具，数据集和代码将开源。

Abstract: Validating autonomous driving (AD) systems requires diverse and
safety-critical testing, making photorealistic virtual environments essential.
Traditional simulation platforms, while controllable, are resource-intensive to
scale and often suffer from a domain gap with real-world data. In contrast,
neural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a
scalable solution for creating photorealistic digital twins of real-world
driving scenes. However, they struggle with dynamic object manipulation and
reusability as their per-scene optimization-based methodology tends to result
in incomplete object models with integrated illumination effects. This paper
introduces R3D2, a lightweight, one-step diffusion model designed to overcome
these limitations and enable realistic insertion of complete 3D assets into
existing scenes by generating plausible rendering effects-such as shadows and
consistent lighting-in real time. This is achieved by training R3D2 on a novel
dataset: 3DGS object assets are generated from in-the-wild AD data using an
image-conditioned 3D generative model, and then synthetically placed into
neural rendering-based virtual environments, allowing R3D2 to learn realistic
integration. Quantitative and qualitative evaluations demonstrate that R3D2
significantly enhances the realism of inserted assets, enabling use-cases like
text-to-3D asset insertion and cross-scene/dataset object transfer, allowing
for true scalability in AD validation. To promote further research in scalable
and realistic AD simulation, we will release our dataset and code, see
https://research.zenseact.com/publications/R3D2/.

</details>


### [152] [Diffusion models under low-noise regime](https://arxiv.org/abs/2506.07841)
*Elizabeth Pavlova,Xue-Xin Wei*

Main category: cs.CV

TL;DR: 扩散模型在低噪声条件下的行为研究，揭示其在数据流形附近的训练集大小、数据几何和模型目标对去噪轨迹的影响。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型在小噪声条件下的行为，填补其在低噪声扩散动力学中的研究空白，提升模型鲁棒性和可解释性。

Method: 使用CelebA子集和解析高斯混合基准，研究模型在低噪声条件下的表现。

Result: 模型在数据流形附近的行为随训练集大小、数据几何和模型目标变化，揭示了其学习数据分布的方式。

Conclusion: 研究为理解生成模型在小扰动常见场景中的可靠性提供了初步见解。

Abstract: Recent work on diffusion models proposed that they operate in two regimes:
memorization, in which models reproduce their training data, and
generalization, in which they generate novel samples. While this has been
tested in high-noise settings, the behavior of diffusion models as effective
denoisers when the corruption level is small remains unclear. To address this
gap, we systematically investigated the behavior of diffusion models under
low-noise diffusion dynamics, with implications for model robustness and
interpretability. Using (i) CelebA subsets of varying sample sizes and (ii)
analytic Gaussian mixture benchmarks, we reveal that models trained on disjoint
data diverge near the data manifold even when their high-noise outputs
converge. We quantify how training set size, data geometry, and model objective
choice shape denoising trajectories and affect score accuracy, providing
insights into how these models actually learn representations of data
distributions. This work starts to address gaps in our understanding of
generative model reliability in practical applications where small
perturbations are common.

</details>


### [153] [F2Net: A Frequency-Fused Network for Ultra-High Resolution Remote Sensing Segmentation](https://arxiv.org/abs/2506.07847)
*Hengzhi Chen,Liqian Feng,Wenhua Wu,Xiaogang Zhu,Shawn Leo,Kun Hu*

Main category: cs.CV

TL;DR: F2Net 是一种频率感知框架，通过分解超高清遥感图像为高频和低频分量进行专门处理，解决了传统方法在细节丢失和全局上下文碎片化之间的权衡问题，并在训练中通过新颖的损失函数稳定梯度动态。


<details>
  <summary>Details</summary>
Motivation: 超高清遥感图像的语义分割在环境监测和城市规划中至关重要，但传统方法因下采样或分块处理导致细节丢失或上下文碎片化，多分支网络则存在计算效率低和梯度冲突问题。

Method: F2Net 将图像分解为高频和低频分量，高频分支保留全分辨率细节，低频分支通过双子分支捕获短程和远程依赖，并通过混合频率融合模块整合结果，同时引入交叉频率对齐损失和平衡损失以优化训练。

Result: 在 DeepGlobe 和 Inria Aerial 基准测试中，F2Net 分别达到 80.22 和 83.39 的 mIoU，性能领先。

Conclusion: F2Net 通过频率分解和融合模块，结合新颖损失函数，实现了超高清遥感图像语义分割的高效和稳定训练，性能优异。

Abstract: Semantic segmentation of ultra-high-resolution (UHR) remote sensing imagery
is critical for applications like environmental monitoring and urban planning
but faces computational and optimization challenges. Conventional methods
either lose fine details through downsampling or fragment global context via
patch processing. While multi-branch networks address this trade-off, they
suffer from computational inefficiency and conflicting gradient dynamics during
training. We propose F2Net, a frequency-aware framework that decomposes UHR
images into high- and low-frequency components for specialized processing. The
high-frequency branch preserves full-resolution structural details, while the
low-frequency branch processes downsampled inputs through dual sub-branches
capturing short- and long-range dependencies. A Hybrid-Frequency Fusion module
integrates these observations, guided by two novel objectives: Cross-Frequency
Alignment Loss ensures semantic consistency between frequency components, and
Cross-Frequency Balance Loss regulates gradient magnitudes across branches to
stabilize training. Evaluated on DeepGlobe and Inria Aerial benchmarks, F2Net
achieves state-of-the-art performance with mIoU of 80.22 and 83.39,
respectively. Our code will be publicly available.

</details>


### [154] [PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement](https://arxiv.org/abs/2506.07848)
*Teng Hu,Zhentao Yu,Zhengguang Zhou,Jiangning Zhang,Yuan Zhou,Qinglin Lu,Ran Yi*

Main category: cs.CV

TL;DR: PolyVivid是一个多主体视频定制框架，通过VLLM和3D-RoPE模块实现身份一致性和交互增强，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在多主体定制中缺乏细粒度控制和身份一致性。

Method: 设计了VLLM文本-图像融合模块、3D-RoPE增强模块和注意力继承身份注入模块，并构建了MLLM数据管道。

Result: 实验表明PolyVivid在身份保真度、视频真实性和主体对齐方面表现优异。

Conclusion: PolyVivid在多主体视频定制中实现了灵活且身份一致的生成，性能超越现有基线。

Abstract: Despite recent advances in video generation, existing models still lack
fine-grained controllability, especially for multi-subject customization with
consistent identity and interaction. In this paper, we propose PolyVivid, a
multi-subject video customization framework that enables flexible and
identity-consistent generation. To establish accurate correspondences between
subject images and textual entities, we design a VLLM-based text-image fusion
module that embeds visual identities into the textual space for precise
grounding. To further enhance identity preservation and subject interaction, we
propose a 3D-RoPE-based enhancement module that enables structured
bidirectional fusion between text and image embeddings. Moreover, we develop an
attention-inherited identity injection module to effectively inject fused
identity features into the video generation process, mitigating identity drift.
Finally, we construct an MLLM-based data pipeline that combines MLLM-based
grounding, segmentation, and a clique-based subject consolidation strategy to
produce high-quality multi-subject data, effectively enhancing subject
distinction and reducing ambiguity in downstream video generation. Extensive
experiments demonstrate that PolyVivid achieves superior performance in
identity fidelity, video realism, and subject alignment, outperforming existing
open-source and commercial baselines.

</details>


### [155] [SAM2Auto: Auto Annotation Using FLASH](https://arxiv.org/abs/2506.07850)
*Arash Rocky,Q. M. Jonathan Wu*

Main category: cs.CV

TL;DR: SAM2Auto是一个全自动视频数据集标注工具，无需人工干预或数据集特定训练，通过结合对象检测和视频实例分割技术，显著减少标注时间和成本。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型（VLMs）因标注数据稀缺而发展受限的问题，传统标注方法耗时且昂贵。

Method: 结合SMART-OD（自动掩码生成与开放世界对象检测）和FLASH（实时视频实例分割），通过统计方法减少检测错误并保持对象跟踪一致性。

Result: 实验表明，SAM2Auto的标注精度与人工标注相当，同时大幅减少时间和成本，适用于多样化数据集。

Conclusion: SAM2Auto为自动化视频标注设定了新基准，有望加速视觉语言模型的发展。

Abstract: Vision-Language Models (VLMs) lag behind Large Language Models due to the
scarcity of annotated datasets, as creating paired visual-textual annotations
is labor-intensive and expensive. To address this bottleneck, we introduce
SAM2Auto, the first fully automated annotation pipeline for video datasets
requiring no human intervention or dataset-specific training. Our approach
consists of two key components: SMART-OD, a robust object detection system that
combines automatic mask generation with open-world object detection
capabilities, and FLASH (Frame-Level Annotation and Segmentation Handler), a
multi-object real-time video instance segmentation (VIS) that maintains
consistent object identification across video frames even with intermittent
detection gaps. Unlike existing open-world detection methods that require
frame-specific hyperparameter tuning and suffer from numerous false positives,
our system employs statistical approaches to minimize detection errors while
ensuring consistent object tracking throughout entire video sequences.
Extensive experimental validation demonstrates that SAM2Auto achieves
comparable accuracy to manual annotation while dramatically reducing annotation
time and eliminating labor costs. The system successfully handles diverse
datasets without requiring retraining or extensive parameter adjustments,
making it a practical solution for large-scale dataset creation. Our work
establishes a new baseline for automated video annotation and provides a
pathway for accelerating VLM development by addressing the fundamental dataset
bottleneck that has constrained progress in vision-language understanding.

</details>


### [156] [LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic Segmentation of 3D Point Clouds](https://arxiv.org/abs/2506.07857)
*Zihui Zhang,Weisheng Dai,Hongtao Wen,Bo Yang*

Main category: cs.CV

TL;DR: LogoSP是一种无监督3D语义分割方法，通过结合局部和全局点特征学习语义信息，利用频域中的全局模式生成高精度伪标签，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法仅依赖局部特征，缺乏对更丰富语义先验的探索。

Method: 提出LogoSP，通过频域中的全局模式分组超点，生成语义伪标签训练分割网络。

Result: 在两个室内和一个室外数据集上表现优异，显著超越现有方法。

Conclusion: LogoSP在无监督3D语义分割中达到最优性能，且全局模式能有效表示语义信息。

Abstract: We study the problem of unsupervised 3D semantic segmentation on raw point
clouds without needing human labels in training. Existing methods usually
formulate this problem into learning per-point local features followed by a
simple grouping strategy, lacking the ability to discover additional and
possibly richer semantic priors beyond local features. In this paper, we
introduce LogoSP to learn 3D semantics from both local and global point
features. The key to our approach is to discover 3D semantic information by
grouping superpoints according to their global patterns in the frequency
domain, thus generating highly accurate semantic pseudo-labels for training a
segmentation network. Extensive experiments on two indoor and an outdoor
datasets show that our LogoSP surpasses all existing unsupervised methods by
large margins, achieving the state-of-the-art performance for unsupervised 3D
semantic segmentation. Notably, our investigation into the learned global
patterns reveals that they truly represent meaningful 3D semantics in the
absence of human labels during training.

</details>


### [157] [Egocentric Event-Based Vision for Ping Pong Ball Trajectory Prediction](https://arxiv.org/abs/2506.07860)
*Ivan Alberico,Marco Cannici,Giovanni Cioffi,Davide Scaramuzza*

Main category: cs.CV

TL;DR: 提出了一种基于事件相机的实时乒乓球轨迹预测系统，利用事件相机的高时间分辨率解决传统相机的高延迟和运动模糊问题。


<details>
  <summary>Details</summary>
Motivation: 传统相机在高速运动场景下存在高延迟和运动模糊问题，而事件相机能提供更高的时间分辨率，适合实时轨迹预测。

Method: 系统结合事件相机数据和Meta Project Aria眼镜的传感器数据，采用生物启发的注视点视觉处理技术，仅处理注视点区域的事件数据。

Result: 系统在检测延迟上显著优于传统帧率系统（4.5 ms vs. 66 ms），并实现了10.81倍的资源优化。

Conclusion: 这是首个基于事件相机和第一人称视角的乒乓球轨迹预测方法，展示了事件相机在实时运动分析中的潜力。

Abstract: In this paper, we present a real-time egocentric trajectory prediction system
for table tennis using event cameras. Unlike standard cameras, which suffer
from high latency and motion blur at fast ball speeds, event cameras provide
higher temporal resolution, allowing more frequent state updates, greater
robustness to outliers, and accurate trajectory predictions using just a short
time window after the opponent's impact. We collect a dataset of ping-pong game
sequences, including 3D ground-truth trajectories of the ball, synchronized
with sensor data from the Meta Project Aria glasses and event streams. Our
system leverages foveated vision, using eye-gaze data from the glasses to
process only events in the viewer's fovea. This biologically inspired approach
improves ball detection performance and significantly reduces computational
latency, as it efficiently allocates resources to the most perceptually
relevant regions, achieving a reduction factor of 10.81 on the collected
trajectories. Our detection pipeline has a worst-case total latency of 4.5 ms,
including computation and perception - significantly lower than a frame-based
30 FPS system, which, in the worst case, takes 66 ms solely for perception.
Finally, we fit a trajectory prediction model to the estimated states of the
ball, enabling 3D trajectory forecasting in the future. To the best of our
knowledge, this is the first approach to predict table tennis trajectories from
an egocentric perspective using event cameras.

</details>


### [158] [VIVAT: Virtuous Improving VAE Training through Artifact Mitigation](https://arxiv.org/abs/2506.07863)
*Lev Novitskiy,Viacheslav Vasilev,Maria Kovaleva,Vladimir Arkhipkin,Denis Dimitrov*

Main category: cs.CV

TL;DR: VIVAT通过简单调整（如损失权重、填充策略和空间条件归一化）显著改善了KL-VAE训练中的常见伪影问题，提升了重建和生成质量。


<details>
  <summary>Details</summary>
Motivation: KL-VAE训练中常见的伪影（如颜色偏移、网格图案等）降低了生成和重建质量，需要一种无需大幅架构改动的方法来解决。

Method: 提出VIVAT，通过调整损失权重、优化填充策略和引入空间条件归一化等方法，系统性地解决五种常见伪影问题。

Result: 在多个基准测试中，VIVAT在图像重建（PSNR和SSIM）和文本到图像生成（CLIP分数）方面达到最优性能。

Conclusion: VIVAT在保持KL-VAE框架简单性的同时，有效解决了其实际挑战，为优化VAE训练提供了实用方法。

Abstract: Variational Autoencoders (VAEs) remain a cornerstone of generative computer
vision, yet their training is often plagued by artifacts that degrade
reconstruction and generation quality. This paper introduces VIVAT, a
systematic approach to mitigating common artifacts in KL-VAE training without
requiring radical architectural changes. We present a detailed taxonomy of five
prevalent artifacts - color shift, grid patterns, blur, corner and droplet
artifacts - and analyze their root causes. Through straightforward
modifications, including adjustments to loss weights, padding strategies, and
the integration of Spatially Conditional Normalization, we demonstrate
significant improvements in VAE performance. Our method achieves
state-of-the-art results in image reconstruction metrics (PSNR and SSIM) across
multiple benchmarks and enhances text-to-image generation quality, as evidenced
by superior CLIP scores. By preserving the simplicity of the KL-VAE framework
while addressing its practical challenges, VIVAT offers actionable insights for
researchers and practitioners aiming to optimize VAE training.

</details>


### [159] [FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity](https://arxiv.org/abs/2506.07865)
*Jinxi Li,Ziyang Song,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: FreeGave方法通过引入物理编码和无散度模块，无需对象先验即可学习复杂动态3D场景的物理特性，避免了低效的PINN损失。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理复杂物理运动或边界时表现不佳，且需要对象先验（如掩码或类型）。FreeGave旨在无需这些先验即可学习场景的物理特性。

Method: 提出FreeGave方法，引入物理编码和无散度模块，估计每个高斯的速度场，避免使用PINN损失。

Result: 在三个公共数据集和一个新收集的真实数据集上表现出色，尤其在未来帧外推和运动分割任务中。

Conclusion: FreeGave成功学习了无标签的3D物理运动模式，验证了其有效性。

Abstract: In this paper, we aim to model 3D scene geometry, appearance, and the
underlying physics purely from multi-view videos. By applying various governing
PDEs as PINN losses or incorporating physics simulation into neural networks,
existing works often fail to learn complex physical motions at boundaries or
require object priors such as masks or types. In this paper, we propose
FreeGave to learn the physics of complex dynamic 3D scenes without needing any
object priors. The key to our approach is to introduce a physics code followed
by a carefully designed divergence-free module for estimating a per-Gaussian
velocity field, without relying on the inefficient PINN losses. Extensive
experiments on three public datasets and a newly collected challenging
real-world dataset demonstrate the superior performance of our method for
future frame extrapolation and motion segmentation. Most notably, our
investigation into the learned physics codes reveals that they truly learn
meaningful 3D physical motion patterns in the absence of any human labels in
training.

</details>


### [160] [Spatio-Temporal State Space Model For Efficient Event-Based Optical Flow](https://arxiv.org/abs/2506.07878)
*Muhammad Ahmed Humais,Xiaoqian Huang,Hussain Sajwani,Sajid Javed,Yahya Zweiri*

Main category: cs.CV

TL;DR: 提出了一种基于时空状态空间模型（STSSM）的高效事件相机运动估计算法，显著提升了计算效率和性能。


<details>
  <summary>Details</summary>
Motivation: 事件相机在低延迟运动估计中潜力巨大，但现有深度学习方法（如CNN、RNN、ViT）计算效率不足，而异步事件方法（如SNN、GNN）又无法充分捕捉时空信息。

Method: 引入STSSM模块和新型网络架构，利用状态空间模型高效捕捉事件数据的时空相关性。

Result: 在DSEC基准测试中，模型推理速度提升4.5倍，计算量减少8倍（相比TMA）或2倍（相比EV-FlowNet），性能具有竞争力。

Conclusion: STSSM为事件相机的运动估计提供了一种高效且高性能的解决方案。

Abstract: Event cameras unlock new frontiers that were previously unthinkable with
standard frame-based cameras. One notable example is low-latency motion
estimation (optical flow), which is critical for many real-time applications.
In such applications, the computational efficiency of algorithms is paramount.
Although recent deep learning paradigms such as CNN, RNN, or ViT have shown
remarkable performance, they often lack the desired computational efficiency.
Conversely, asynchronous event-based methods including SNNs and GNNs are
computationally efficient; however, these approaches fail to capture sufficient
spatio-temporal information, a powerful feature required to achieve better
performance for optical flow estimation. In this work, we introduce
Spatio-Temporal State Space Model (STSSM) module along with a novel network
architecture to develop an extremely efficient solution with competitive
performance. Our STSSM module leverages state-space models to effectively
capture spatio-temporal correlations in event data, offering higher performance
with lower complexity compared to ViT, CNN-based architectures in similar
settings. Our model achieves 4.5x faster inference and 8x lower computations
compared to TMA and 2x lower computations compared to EV-FlowNet with
competitive performance on the DSEC benchmark. Our code will be available at
https://github.com/AhmedHumais/E-STMFlow

</details>


### [161] [CrosswalkNet: An Optimized Deep Learning Framework for Pedestrian Crosswalk Detection in Aerial Images with High-Performance Computing](https://arxiv.org/abs/2506.07885)
*Zubin Bhuyan,Yuanchang Xie,AngkeaReach Rith,Xintong Yan,Nasko Apostolov,Jimi Oke,Chengbo Ai*

Main category: cs.CV

TL;DR: CrosswalkNet是一种高效的深度学习框架，用于从高分辨率航拍图像中检测行人横道，通过优化技术和OBB提升精度，并在多地区数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着航拍和卫星图像的普及，深度学习在交通资产管理、安全分析和城市规划中具有巨大潜力。

Method: 采用定向边界框（OBB）和多种优化技术（如注意力机制和余弦退火）提升检测精度。

Result: 在麻省数据集上达到96.5%的精确率和93.3%的召回率，并在其他地区无需微调即表现优异。

Conclusion: CrosswalkNet为决策者和规划者提供了提升行人安全和城市交通效率的有效工具。

Abstract: With the increasing availability of aerial and satellite imagery, deep
learning presents significant potential for transportation asset management,
safety analysis, and urban planning. This study introduces CrosswalkNet, a
robust and efficient deep learning framework designed to detect various types
of pedestrian crosswalks from 15-cm resolution aerial images. CrosswalkNet
incorporates a novel detection approach that improves upon traditional object
detection strategies by utilizing oriented bounding boxes (OBB), enhancing
detection precision by accurately capturing crosswalks regardless of their
orientation. Several optimization techniques, including Convolutional Block
Attention, a dual-branch Spatial Pyramid Pooling-Fast module, and cosine
annealing, are implemented to maximize performance and efficiency. A
comprehensive dataset comprising over 23,000 annotated crosswalk instances is
utilized to train and validate the proposed framework. The best-performing
model achieves an impressive precision of 96.5% and a recall of 93.3% on aerial
imagery from Massachusetts, demonstrating its accuracy and effectiveness.
CrosswalkNet has also been successfully applied to datasets from New Hampshire,
Virginia, and Maine without transfer learning or fine-tuning, showcasing its
robustness and strong generalization capability. Additionally, the crosswalk
detection results, processed using High-Performance Computing (HPC) platforms
and provided in polygon shapefile format, have been shown to accelerate data
processing and detection, supporting real-time analysis for safety and mobility
applications. This integration offers policymakers, transportation engineers,
and urban planners an effective instrument to enhance pedestrian safety and
improve urban mobility.

</details>


### [162] [EgoM2P: Egocentric Multimodal Multitask Pretraining](https://arxiv.org/abs/2506.07886)
*Gen Li,Yutong Chen,Yiqian Wu,Kaifeng Zhao,Marc Pollefeys,Siyu Tang*

Main category: cs.CV

TL;DR: 论文提出EgoM2P框架，通过高效的时间标记器和掩码建模解决多模态自我中心视觉任务中的挑战，支持多任务处理并优于专业模型。


<details>
  <summary>Details</summary>
Motivation: 自我中心视觉中的多模态信号理解对AR、机器人和人机交互至关重要，但数据异构性和缺失模态标注问题限制了现有方法的扩展性。

Method: 引入高效时间标记器和EgoM2P框架，利用时间感知多模态标记进行掩码建模，支持多任务处理。

Result: EgoM2P在多任务中表现优于专业模型，速度更快，并可作为条件视频生成模型。

Conclusion: EgoM2P为自我中心4D理解提供通用解决方案，开源以推动研究。

Abstract: Understanding multimodal signals in egocentric vision, such as RGB video,
depth, camera poses, and gaze, is essential for applications in augmented
reality, robotics, and human-computer interaction. These capabilities enable
systems to better interpret the camera wearer's actions, intentions, and
surrounding environment. However, building large-scale egocentric multimodal
and multitask models presents unique challenges. Egocentric data are inherently
heterogeneous, with large variations in modality coverage across devices and
settings. Generating pseudo-labels for missing modalities, such as gaze or
head-mounted camera trajectories, is often infeasible, making standard
supervised learning approaches difficult to scale. Furthermore, dynamic camera
motion and the complex temporal and spatial structure of first-person video
pose additional challenges for the direct application of existing multimodal
foundation models.
  To address these challenges, we introduce a set of efficient temporal
tokenizers and propose EgoM2P, a masked modeling framework that learns from
temporally aware multimodal tokens to train a large, general-purpose model for
egocentric 4D understanding. This unified design supports multitasking across
diverse egocentric perception and synthesis tasks, including gaze prediction,
egocentric camera tracking, and monocular depth estimation from egocentric
video. EgoM2P also serves as a generative model for conditional egocentric
video synthesis. Across these tasks, EgoM2P matches or outperforms specialist
models while being an order of magnitude faster. We will fully open-source
EgoM2P to support the community and advance egocentric vision research. Project
page: https://egom2p.github.io/

</details>


### [163] [Video Unlearning via Low-Rank Refusal Vector](https://arxiv.org/abs/2506.07891)
*Simone Facchiano,Stefano Saravalle,Matteo Migliarini,Edoardo De Matteis,Alessio Sampieri,Andrea Pilzer,Emanuele Rodolà,Indro Spinelli,Luca Franco,Fabio Galasso*

Main category: cs.CV

TL;DR: 论文提出了一种针对视频扩散模型的无学习技术，通过少量多模态提示对生成“拒绝向量”，以消除模型中的有害概念，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型可能继承训练数据中的偏见和有害内容，导致用户生成不良或非法内容。

Method: 使用5对多模态提示（安全与不安全示例）生成拒绝向量，并通过低秩分解方法优化，直接嵌入模型权重。

Result: 方法有效中和多种有害内容（如裸露、暴力、版权等），且无需重新训练或原始数据。

Conclusion: 该方法在保持生成质量的同时，显著提升了模型对有害内容的抑制能力，且对抗性绕过更困难。

Abstract: Video generative models democratize the creation of visual content through
intuitive instruction following, but they also inherit the biases and harmful
concepts embedded within their web-scale training data. This inheritance
creates a significant risk, as users can readily generate undesirable and even
illegal content. This work introduces the first unlearning technique tailored
explicitly for video diffusion models to address this critical issue. Our
method requires 5 multi-modal prompt pairs only. Each pair contains a "safe"
and an "unsafe" example that differ only by the target concept. Averaging their
per-layer latent differences produces a "refusal vector", which, once
subtracted from the model parameters, neutralizes the unsafe concept. We
introduce a novel low-rank factorization approach on the covariance difference
of embeddings that yields robust refusal vectors. This isolates the target
concept while minimizing collateral unlearning of other semantics, thus
preserving the visual quality of the generated video. Our method preserves the
model's generation quality while operating without retraining or access to the
original training data. By embedding the refusal direction directly into the
model's weights, the suppression mechanism becomes inherently more robust
against adversarial bypass attempts compared to surface-level input-output
filters. In a thorough qualitative and quantitative evaluation, we show that we
can neutralize a variety of harmful contents, including explicit nudity,
graphic violence, copyrights, and trademarks. Project page:
https://www.pinlab.org/video-unlearning.

</details>


### [164] [WeThink: Toward General-purpose Vision-Language Reasoning via Reinforcement Learning](https://arxiv.org/abs/2506.07905)
*Jie Yang,Feipeng Ma,Zitian Wang,Dacheng Yin,Kang Rong,Fengyun Rao,Ruimao Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种基于强化学习的多模态推理方法，通过生成上下文感知的QA对和开源数据集WeThink，显著提升了多模态大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 扩展文本推理模型到多模态领域，解决通用视觉-语言推理的挑战。

Method: 1. 提出可扩展的多模态QA合成管道；2. 开源WeThink数据集；3. 探索混合奖励机制的强化学习。

Result: 在14个多模态基准测试中表现显著提升，数据管道持续增加多样性。

Conclusion: WeThink数据集和自动化管道有效提升多模态推理能力，具有广泛适用性。

Abstract: Building on the success of text-based reasoning models like DeepSeek-R1,
extending these capabilities to multimodal reasoning holds great promise. While
recent works have attempted to adapt DeepSeek-R1-style reinforcement learning
(RL) training paradigms to multimodal large language models (MLLM), focusing on
domain-specific tasks like math and visual perception, a critical question
remains: How can we achieve the general-purpose visual-language reasoning
through RL? To address this challenge, we make three key efforts: (1) A novel
Scalable Multimodal QA Synthesis pipeline that autonomously generates
context-aware, reasoning-centric question-answer (QA) pairs directly from the
given images. (2) The open-source WeThink dataset containing over 120K
multimodal QA pairs with annotated reasoning paths, curated from 18 diverse
dataset sources and covering various question domains. (3) A comprehensive
exploration of RL on our dataset, incorporating a hybrid reward mechanism that
combines rule-based verification with model-based assessment to optimize RL
training efficiency across various task domains. Across 14 diverse MLLM
benchmarks, we demonstrate that our WeThink dataset significantly enhances
performance, from mathematical reasoning to diverse general multimodal tasks.
Moreover, we show that our automated data pipeline can continuously increase
data diversity to further improve model performance.

</details>


### [165] [Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models](https://arxiv.org/abs/2506.07936)
*Chengyue Huang,Yuchen Zhu,Sichen Zhu,Jingyun Xiao,Moises Andrade,Shivang Chopra,Zsolt Kira*

Main category: cs.CV

TL;DR: 研究发现，视觉语言模型（VLM）在多模态上下文学习（MM-ICL）中依赖浅层启发式方法，而非真正任务理解。通过分布偏移评估，发现性能随演示增加而下降，模型倾向于复制答案。提出新方法MM-ICL with Reasoning，但实验表明当前VLM未能有效利用演示信息。


<details>
  <summary>Details</summary>
Motivation: 探讨VLM在多模态上下文学习中的表现，验证其是否真正具备任务理解能力。

Method: 提出MM-ICL with Reasoning方法，为每个演示生成答案和理由，并在不同数据集和模型上进行实验。

Result: 性能对演示数量、检索方法、理由质量和分布变化不敏感，表明VLM未能有效利用演示信息。

Conclusion: 当前VLM在多模态上下文学习中表现有限，需进一步改进以提升任务理解能力。

Abstract: Vision-language models (VLMs) are widely assumed to exhibit in-context
learning (ICL), a property similar to that of their language-only counterparts.
While recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies
show they often rely on shallow heuristics -- such as copying or majority
voting -- rather than true task understanding. We revisit this assumption by
evaluating VLMs under distribution shifts, where support examples come from a
dataset different from the query. Surprisingly, performance often degrades with
more demonstrations, and models tend to copy answers rather than learn from
them. To investigate further, we propose a new MM-ICL with Reasoning pipeline
that augments each demonstration with a generated rationale alongside the
answer. We conduct extensive and comprehensive experiments on both perception-
and reasoning-required datasets with open-source VLMs ranging from 3B to 72B
and proprietary models such as Gemini 2.0. We conduct controlled studies
varying shot count, retrieval method, rationale quality, and distribution. Our
results show limited performance sensitivity across these factors, suggesting
that current VLMs do not effectively utilize demonstration-level information as
intended in MM-ICL.

</details>


### [166] [Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations](https://arxiv.org/abs/2506.07943)
*Yizhen Li,Dell Zhang,Xuelong Li,Yiqing Shen*

Main category: cs.CV

TL;DR: DTwinSeger提出了一种新的Reasoning Segmentation方法，通过Digital Twin表示将感知与推理解耦，利用LLM进行显式推理，实现了在多模态任务中的先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在图像标记化过程中破坏了对象的连续空间关系，需要一种能保留空间和语义信息的新方法。

Method: DTwinSeger将任务分为两阶段：首先生成结构化DT表示，再用LLM进行推理。提出了一种针对LLM的监督微调方法和数据集Seg-DT。

Result: 在多个基准测试中达到最先进性能，证明了DT表示作为视觉与文本桥梁的有效性。

Conclusion: DTwinSeger通过DT表示和LLM的结合，成功解决了复杂多模态推理任务，展示了其潜力。

Abstract: Reasoning Segmentation (RS) is a multimodal vision-text task that requires
segmenting objects based on implicit text queries, demanding both precise
visual perception and vision-text reasoning capabilities. Current RS approaches
rely on fine-tuning vision-language models (VLMs) for both perception and
reasoning, but their tokenization of images fundamentally disrupts continuous
spatial relationships between objects. We introduce DTwinSeger, a novel RS
approach that leverages Digital Twin (DT) representation as an intermediate
layer to decouple perception from reasoning. Innovatively, DTwinSeger
reformulates RS as a two-stage process, where the first transforms the image
into a structured DT representation that preserves spatial relationships and
semantic properties and then employs a Large Language Model (LLM) to perform
explicit reasoning over this representation to identify target objects. We
propose a supervised fine-tuning method specifically for LLM with DT
representation, together with a corresponding fine-tuning dataset Seg-DT, to
enhance the LLM's reasoning capabilities with DT representations. Experiments
show that our method can achieve state-of-the-art performance on two image RS
benchmarks and three image referring segmentation benchmarks. It yields that DT
representation functions as an effective bridge between vision and text,
enabling complex multimodal reasoning tasks to be accomplished solely with an
LLM.

</details>


### [167] [Creating a Historical Migration Dataset from Finnish Church Records, 1800-1920](https://arxiv.org/abs/2506.07960)
*Ari Vesalainen,Jenna Kanerva,Aida Nitsch,Kiia Korsu,Ilari Larkiola,Laura Ruotsalainen,Filip Ginter*

Main category: cs.CV

TL;DR: 本文介绍了利用深度学习技术从芬兰1800-1920年的教会迁移记录中提取结构化数据的大规模项目，数据集包含600多万条记录，可用于研究历史人口模式。


<details>
  <summary>Details</summary>
Motivation: 教会迁移记录是研究历史人口模式的重要资源，但手工处理大量手写档案效率低下，因此需要自动化方法。

Method: 采用深度学习流水线，包括布局分析、表格检测、单元格分类和手写识别，自动化提取数据。

Result: 成功构建了包含600多万条记录的结构化数据集，并通过案例研究展示了其在研究中的应用。

Conclusion: 该工作展示了如何将大量手写档案转化为结构化数据，支持历史和人口研究。

Abstract: This article presents a large-scale effort to create a structured dataset of
internal migration in Finland between 1800 and 1920 using digitized church
moving records. These records, maintained by Evangelical-Lutheran parishes,
document the migration of individuals and families and offer a valuable source
for studying historical demographic patterns. The dataset includes over six
million entries extracted from approximately 200,000 images of handwritten
migration records.
  The data extraction process was automated using a deep learning pipeline that
included layout analysis, table detection, cell classification, and handwriting
recognition. The complete pipeline was applied to all images, resulting in a
structured dataset suitable for research.
  The dataset can be used to study internal migration, urbanization, and family
migration, and the spread of disease in preindustrial Finland. A case study
from the Elim\"aki parish shows how local migration histories can be
reconstructed. The work demonstrates how large volumes of handwritten archival
material can be transformed into structured data to support historical and
demographic research.

</details>


### [168] [SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from Design](https://arxiv.org/abs/2506.07964)
*Wenxin Tang,Jingyu Xiao,Wenxuan Jiang,Xi Xiao,Yuhang Wang,Xuxin Tang,Qing Li,Yuehe Ma,Junliang Liu,Shisong Tang,Michael R. Lyu*

Main category: cs.CV

TL;DR: 论文提出Slide2Code基准和SlideCoder框架，用于从参考图像生成可编辑幻灯片，解决了现有方法在视觉和结构设计上的不足。


<details>
  <summary>Details</summary>
Motivation: 手动制作幻灯片耗时且需专业知识，现有基于自然语言的LLM方法难以捕捉幻灯片设计的视觉和结构细节。

Method: 提出SlideCoder框架，结合颜色梯度分割算法和分层检索增强生成方法，分解复杂任务并优化代码生成。

Result: SlideCoder在布局保真度、执行准确性和视觉一致性上优于现有基线方法，最高提升40.5分。

Conclusion: SlideCoder在幻灯片生成任务中表现出色，为自动化幻灯片设计提供了有效解决方案。

Abstract: Manual slide creation is labor-intensive and requires expert prior knowledge.
Existing natural language-based LLM generation methods struggle to capture the
visual and structural nuances of slide designs. To address this, we formalize
the Reference Image to Slide Generation task and propose Slide2Code, the first
benchmark with difficulty-tiered samples based on a novel Slide Complexity
Metric. We introduce SlideCoder, a layout-aware, retrieval-augmented framework
for generating editable slides from reference images. SlideCoder integrates a
Color Gradient-based Segmentation algorithm and a Hierarchical
Retrieval-Augmented Generation method to decompose complex tasks and enhance
code generation. We also release SlideMaster, a 7B open-source model fine-tuned
with improved reverse-engineered data. Experiments show that SlideCoder
outperforms state-of-the-art baselines by up to 40.5 points, demonstrating
strong performance across layout fidelity, execution accuracy, and visual
consistency. Our code is available at
https://github.com/vinsontang1/SlideCoder.

</details>


### [169] [SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence](https://arxiv.org/abs/2506.07966)
*Ziyang Gong,Wenhao Li,Oliver Ma,Songyuan Li,Jiayi Ji,Xue Yang,Gen Luo,Junchi Yan,Rongrong Ji*

Main category: cs.CV

TL;DR: SpaCE-10是一个用于评估多模态大语言模型（MLLMs）空间智能的综合基准，包含10种原子空间能力和8种组合能力，通过5k+ QA对和811个室内场景进行测试。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以全面评估MLLMs的空间智能，尤其是从原子能力到组合能力的层次。

Method: 提出SpaCE-10基准，定义10种原子空间能力和8种组合能力，采用分层标注流程生成高质量QA对。

Result: 测试发现，即使最先进的MLLM与人类表现仍有较大差距，计数能力不足显著限制了组合空间能力。

Conclusion: SpaCE-10填补了评估空白，揭示了MLLMs的局限性，为社区提供了重要参考。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in
various multimodal tasks. To pursue higher intelligence in space, MLLMs require
integrating multiple atomic spatial capabilities to handle complex and dynamic
tasks. However, existing benchmarks struggle to comprehensively evaluate the
spatial intelligence of common MLLMs from the atomic level to the compositional
level. To fill this gap, we present SpaCE-10, a comprehensive benchmark for
compositional spatial evaluations. In SpaCE-10, we define 10 atomic spatial
capabilities, which are combined to form 8 compositional capabilities. Based on
these definitions, we propose a novel hierarchical annotation pipeline to
generate high-quality and diverse question-answer (QA) pairs. With over 150+
hours of human expert effort, we obtain over 5k QA pairs for 811 real indoor
scenes in SpaCE-10, which covers various evaluation settings like point cloud
input and multi-choice QA. We conduct an extensive evaluation of common MLLMs
on SpaCE-10 and find that even the most advanced MLLM still lags behind humans
by large margins. Through our careful study, we also draw several significant
findings that benefit the MLLM community. For example, we reveal that the
shortcoming of counting capability greatly limits the compositional spatial
capabilities of existing MLLMs. The evaluation code and benchmark datasets are
available at https://github.com/Cuzyoung/SpaCE-10.

</details>


### [170] [CyberV: Cybernetics for Test-time Scaling in Video Understanding](https://arxiv.org/abs/2506.07971)
*Jiahao Meng,Shuyang Sun,Yue Tan,Lu Qi,Yunhai Tong,Xiangtai Li,Longyin Wen*

Main category: cs.CV

TL;DR: CyberV是一个基于控制论原理的框架，通过动态资源分配和自我修正提升多模态大语言模型（MLLMs）在长视频理解中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs因前馈处理方式在长视频理解中存在计算需求高、鲁棒性差和准确性不足的问题，尤其是参数较少的模型。

Method: CyberV引入了一个控制论循环，包括MLLM推理系统、传感器和控制器，动态监控和修正推理过程。

Result: 实验显示CyberV显著提升模型性能，如Qwen2.5-VL-7B提升8.3%，甚至接近人类专家水平。

Conclusion: CyberV通过动态适应机制有效提升MLLMs的鲁棒性和准确性，适用于动态视频理解任务。

Abstract: Current Multimodal Large Language Models (MLLMs) may struggle with
understanding long or complex videos due to computational demands at test time,
lack of robustness, and limited accuracy, primarily stemming from their
feed-forward processing nature. These limitations could be more severe for
models with fewer parameters. To address these limitations, we propose a novel
framework inspired by cybernetic principles, redesigning video MLLMs as
adaptive systems capable of self-monitoring, self-correction, and dynamic
resource allocation during inference. Our approach, CyberV, introduces a
cybernetic loop consisting of an MLLM Inference System, a Sensor, and a
Controller. Specifically, the sensor monitors forward processes of the MLLM and
collects intermediate interpretations, such as attention drift, then the
controller determines when and how to trigger self-correction and generate
feedback to guide the next round. This test-time adaptive scaling framework
enhances frozen MLLMs without requiring retraining or additional components.
Experiments demonstrate significant improvements: CyberV boosts Qwen2.5-VL-7B
by 8.3% and InternVL3-8B by 5.5% on VideoMMMU, surpassing the competitive
proprietary model GPT-4o. When applied to Qwen2.5-VL-72B, it yields a 10.0%
improvement, achieving performance even comparable to human experts.
Furthermore, our method demonstrates consistent gains on general-purpose
benchmarks, such as VideoMME and WorldSense, highlighting its effectiveness and
generalization capabilities in making MLLMs more robust and accurate for
dynamic video understanding. The code is released at
https://github.com/marinero4972/CyberV.

</details>


### [171] [OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation](https://arxiv.org/abs/2506.07977)
*Jingjing Chang,Yixiao Fang,Peng Xing,Shuhan Wu,Wei Cheng,Rui Wang,Xianfang Zeng,Gang Yu,Hai-Bao Chen*

Main category: cs.CV

TL;DR: OneIG-Bench是一个全面的文本到图像（T2I）模型评估框架，专注于多维度评估，包括推理、文本渲染和风格化等。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型评估系统未能全面覆盖推理能力等前沿问题，需要更细致的评估工具。

Method: 设计OneIG-Bench框架，支持多维度（如对齐、推理、风格化）的灵活评估。

Result: OneIG-Bench提供了公开的代码和数据集，支持可重复的评估和跨模型比较。

Conclusion: OneIG-Bench填补了T2I模型评估的空白，为研究者和实践者提供了深入分析工具。

Abstract: Text-to-image (T2I) models have garnered significant attention for generating
high-quality images aligned with text prompts. However, rapid T2I model
advancements reveal limitations in early benchmarks, lacking comprehensive
evaluations, for example, the evaluation on reasoning, text rendering and
style. Notably, recent state-of-the-art models, with their rich knowledge
modeling capabilities, show promising results on the image generation problems
requiring strong reasoning ability, yet existing evaluation systems have not
adequately addressed this frontier. To systematically address these gaps, we
introduce OneIG-Bench, a meticulously designed comprehensive benchmark
framework for fine-grained evaluation of T2I models across multiple dimensions,
including prompt-image alignment, text rendering precision, reasoning-generated
content, stylization, and diversity. By structuring the evaluation, this
benchmark enables in-depth analysis of model performance, helping researchers
and practitioners pinpoint strengths and bottlenecks in the full pipeline of
image generation. Specifically, OneIG-Bench enables flexible evaluation by
allowing users to focus on a particular evaluation subset. Instead of
generating images for the entire set of prompts, users can generate images only
for the prompts associated with the selected dimension and complete the
corresponding evaluation accordingly. Our codebase and dataset are now publicly
available to facilitate reproducible evaluation studies and cross-model
comparisons within the T2I research community.

</details>


### [172] [Real-time Localization of a Soccer Ball from a Single Camera](https://arxiv.org/abs/2506.07981)
*Dmitrii Vorobev,Artem Prosvetov,Karim Elhadji Daou*

Main category: cs.CV

TL;DR: 提出一种高效的单摄像头实时三维足球轨迹重建方法，通过多模式状态模型加速优化，保持厘米级精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在遮挡、运动模糊和复杂背景下的性能问题，同时降低对多摄像头和昂贵设备的需求。

Method: 采用多模式状态模型（$W$离散模式）加速优化，适用于标准CPU，实现低延迟。

Result: 在6K分辨率俄罗斯超级联赛数据集上表现优异，性能接近多摄像头系统。

Conclusion: 提供了一种实用、低成本且高精度的三维足球轨迹跟踪方法。

Abstract: We propose a computationally efficient method for real-time three-dimensional
football trajectory reconstruction from a single broadcast camera. In contrast
to previous work, our approach introduces a multi-mode state model with $W$
discrete modes to significantly accelerate optimization while preserving
centimeter-level accuracy -- even in cases of severe occlusion, motion blur,
and complex backgrounds. The system operates on standard CPUs and achieves low
latency suitable for live broadcast settings. Extensive evaluation on a
proprietary dataset of 6K-resolution Russian Premier League matches
demonstrates performance comparable to multi-camera systems, without the need
for specialized or costly infrastructure. This work provides a practical method
for accessible and accurate 3D ball tracking in professional football
environments.

</details>


### [173] [CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and zero-shot disease classification from chest X-ray](https://arxiv.org/abs/2506.07984)
*Mingquan Lin,Gregory Holste,Song Wang,Yiliang Zhou,Yishu Wei,Imon Banerjee,Pengyi Chen,Tianjie Dai,Yuexi Du,Nicha C. Dvornek,Yuyan Ge,Zuowei Guo,Shouhei Hanaoka,Dongkyun Kim,Pablo Messina,Yang Lu,Denis Parra,Donghyun Son,Álvaro Soto,Aisha Urooj,René Vidal,Yosuke Yamagishi,Zefan Yang,Ruichi Zhang,Yang Zhou,Leo Anthony Celi,Ronald M. Summers,Zhiyong Lu,Hao Chen,Adam Flanders,George Shih,Zhangyang Wang,Yifan Peng*

Main category: cs.CV

TL;DR: CXR-LT 2024是一个社区驱动的项目，旨在通过扩展数据集（377,110张胸部X光片和45种疾病标签）和改进任务设计（包括零样本学习）来提升肺部疾病分类的性能。


<details>
  <summary>Details</summary>
Motivation: 解决开放长尾肺部疾病分类中的挑战，并提升现有技术的可测量性。

Method: 扩展数据集，引入零样本学习任务，并采用多模态模型、生成方法和零样本学习策略。

Result: 提供了更全面的疾病覆盖和更真实的临床场景表示，为未来研究提供了宝贵资源。

Conclusion: 通过整合参与团队的见解和创新，该项目推动了临床现实和通用性诊断模型的发展。

Abstract: The CXR-LT series is a community-driven initiative designed to enhance lung
disease classification using chest X-rays (CXR). It tackles challenges in open
long-tailed lung disease classification and enhances the measurability of
state-of-the-art techniques. The first event, CXR-LT 2023, aimed to achieve
these goals by providing high-quality benchmark CXR data for model development
and conducting comprehensive evaluations to identify ongoing issues impacting
lung disease classification performance. Building on the success of CXR-LT
2023, the CXR-LT 2024 expands the dataset to 377,110 chest X-rays (CXRs) and 45
disease labels, including 19 new rare disease findings. It also introduces a
new focus on zero-shot learning to address limitations identified in the
previous event. Specifically, CXR-LT 2024 features three tasks: (i) long-tailed
classification on a large, noisy test set, (ii) long-tailed classification on a
manually annotated "gold standard" subset, and (iii) zero-shot generalization
to five previously unseen disease findings. This paper provides an overview of
CXR-LT 2024, detailing the data curation process and consolidating
state-of-the-art solutions, including the use of multimodal models for rare
disease detection, advanced generative approaches to handle noisy labels, and
zero-shot learning strategies for unseen diseases. Additionally, the expanded
dataset enhances disease coverage to better represent real-world clinical
settings, offering a valuable resource for future research. By synthesizing the
insights and innovations of participating teams, we aim to advance the
development of clinically realistic and generalizable diagnostic models for
chest radiography.

</details>


### [174] [Rethinking Crowd-Sourced Evaluation of Neuron Explanations](https://arxiv.org/abs/2506.07985)
*Tuomas Oikarinen,Ge Yan,Akshay Kulkarni,Tsui-Wei Weng*

Main category: cs.CV

TL;DR: 本文提出了一种高效且精确的众包评估策略，用于衡量神经元解释的可靠性，并通过重要性采样和贝叶斯方法显著降低了成本。


<details>
  <summary>Details</summary>
Motivation: 现有神经元解释方法的可靠性难以评估，且传统众包评估成本高、噪声大。

Method: 引入重要性采样选择最有价值的输入样本，并采用贝叶斯方法聚合多个评分以减少噪声。

Result: 实现了约30倍的成本降低和约5倍的评分效率提升，并比较了两种视觉模型的神经元解释质量。

Conclusion: 提出的方法显著提高了神经元解释评估的效率和准确性，为未来研究提供了实用工具。

Abstract: Interpreting individual neurons or directions in activations space is an
important component of mechanistic interpretability. As such, many algorithms
have been proposed to automatically produce neuron explanations, but it is
often not clear how reliable these explanations are, or which methods produce
the best explanations. This can be measured via crowd-sourced evaluations, but
they can often be noisy and expensive, leading to unreliable results. In this
paper, we carefully analyze the evaluation pipeline and develop a
cost-effective and highly accurate crowdsourced evaluation strategy. In
contrast to previous human studies that only rate whether the explanation
matches the most highly activating inputs, we estimate whether the explanation
describes neuron activations across all inputs. To estimate this effectively,
we introduce a novel application of importance sampling to determine which
inputs are the most valuable to show to raters, leading to around 30x cost
reduction compared to uniform sampling. We also analyze the label noise present
in crowd-sourced evaluations and propose a Bayesian method to aggregate
multiple ratings leading to a further ~5x reduction in number of ratings
required for the same accuracy. Finally, we use these methods to conduct a
large-scale study comparing the quality of neuron explanations produced by the
most popular methods for two different vision models.

</details>


### [175] [Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers](https://arxiv.org/abs/2506.07986)
*Zhengyao Lv,Tianlin Pan,Chenyang Si,Zhaoxi Chen,Wangmeng Zuo,Ziwei Liu,Kwan-Yee K. Wong*

Main category: cs.CV

TL;DR: 论文提出了一种名为TACA的方法，通过动态调整跨模态注意力，解决了MM-DiT模型中文本与图像对齐的问题，显著提升了生成内容与文本提示的匹配度。


<details>
  <summary>Details</summary>
Motivation: 当前MM-DiT模型（如FLUX）在文本驱动的视觉生成中表现优异，但仍存在文本与生成内容对齐不精确的问题，主要源于跨模态注意力机制的不足。

Method: 提出TACA方法，通过温度缩放和时间步依赖的调整动态平衡跨模态交互，并结合LoRA微调。

Result: 在T2I-CompBench基准测试中，TACA显著提升了文本-图像对齐效果，且计算开销极小。

Conclusion: TACA方法有效改善了跨模态注意力的平衡，提升了文本到图像扩散模型的语义保真度。

Abstract: Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress
in text-driven visual generation. However, even state-of-the-art MM-DiT models
like FLUX struggle with achieving precise alignment between text prompts and
generated content. We identify two key issues in the attention mechanism of
MM-DiT, namely 1) the suppression of cross-modal attention due to token
imbalance between visual and textual modalities and 2) the lack of
timestep-aware attention weighting, which hinder the alignment. To address
these issues, we propose \textbf{Temperature-Adjusted Cross-modal Attention
(TACA)}, a parameter-efficient method that dynamically rebalances multimodal
interactions through temperature scaling and timestep-dependent adjustment.
When combined with LoRA fine-tuning, TACA significantly enhances text-image
alignment on the T2I-CompBench benchmark with minimal computational overhead.
We tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating
its ability to improve image-text alignment in terms of object appearance,
attribute binding, and spatial relationships. Our findings highlight the
importance of balancing cross-modal attention in improving semantic fidelity in
text-to-image diffusion models. Our codes are publicly available at
\href{https://github.com/Vchitect/TACA}

</details>


### [176] [PairEdit: Learning Semantic Variations for Exemplar-based Image Editing](https://arxiv.org/abs/2506.07992)
*Haoguang Lu,Jiacheng Chen,Zhenguo Yang,Aurele Tohokantche Gnanha,Fu Lee Wang,Li Qing,Xudong Mao*

Main category: cs.CV

TL;DR: PairEdit是一种无需文本指导、仅通过少量图像对学习复杂编辑语义的视觉编辑方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于示例的编辑方法仍需依赖文本描述或隐式文本指令，而某些编辑语义难以通过文本精确描述。

Method: 提出目标噪声预测和内容保持噪声调度，通过优化LoRAs分离语义变化与内容学习。

Result: PairEdit成功学习复杂语义，显著提升内容一致性，优于基线方法。

Conclusion: PairEdit为无需文本指导的图像编辑提供了有效解决方案。

Abstract: Recent advancements in text-guided image editing have achieved notable
success by leveraging natural language prompts for fine-grained semantic
control. However, certain editing semantics are challenging to specify
precisely using textual descriptions alone. A practical alternative involves
learning editing semantics from paired source-target examples. Existing
exemplar-based editing methods still rely on text prompts describing the change
within paired examples or learning implicit text-based editing instructions. In
this paper, we introduce PairEdit, a novel visual editing method designed to
effectively learn complex editing semantics from a limited number of image
pairs or even a single image pair, without using any textual guidance. We
propose a target noise prediction that explicitly models semantic variations
within paired images through a guidance direction term. Moreover, we introduce
a content-preserving noise schedule to facilitate more effective semantic
learning. We also propose optimizing distinct LoRAs to disentangle the learning
of semantic variations from content. Extensive qualitative and quantitative
evaluations demonstrate that PairEdit successfully learns intricate semantics
while significantly improving content consistency compared to baseline methods.
Code will be available at https://github.com/xudonmao/PairEdit.

</details>


### [177] [UA-Pose: Uncertainty-Aware 6D Object Pose Estimation and Online Object Completion with Partial References](https://arxiv.org/abs/2506.07996)
*Ming-Feng Li,Xin Yang,Fu-En Wang,Hritam Basak,Yuyin Sun,Shreekant Gayaka,Min Sun,Cheng-Hao Kuo*

Main category: cs.CV

TL;DR: UA-Pose提出了一种不确定性感知的6D物体姿态估计方法，适用于部分参考数据，显著提升了在不完整观测下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要完整3D模型或大量参考图像，而部分参考数据下的6D姿态估计仍具挑战性。

Method: 基于RGBD图像或单张2D图像初始化部分3D模型，引入不确定性区分可见与不可见区域，并采用不确定性感知采样策略。

Result: 在YCB-Video等数据集上表现优于现有方法，尤其在不完整观测下。

Conclusion: UA-Pose通过不确定性建模和在线补全，显著提升了部分参考数据下的6D姿态估计精度和物体完整性。

Abstract: 6D object pose estimation has shown strong generalizability to novel objects.
However, existing methods often require either a complete, well-reconstructed
3D model or numerous reference images that fully cover the object. Estimating
6D poses from partial references, which capture only fragments of an object's
appearance and geometry, remains challenging. To address this, we propose
UA-Pose, an uncertainty-aware approach for 6D object pose estimation and online
object completion specifically designed for partial references. We assume
access to either (1) a limited set of RGBD images with known poses or (2) a
single 2D image. For the first case, we initialize a partial object 3D model
based on the provided images and poses, while for the second, we use
image-to-3D techniques to generate an initial object 3D model. Our method
integrates uncertainty into the incomplete 3D model, distinguishing between
seen and unseen regions. This uncertainty enables confidence assessment in pose
estimation and guides an uncertainty-aware sampling strategy for online object
completion, enhancing robustness in pose estimation accuracy and improving
object completeness. We evaluate our method on the YCB-Video, YCBInEOAT, and
HO3D datasets, including RGBD sequences of YCB objects manipulated by robots
and human hands. Experimental results demonstrate significant performance
improvements over existing methods, particularly when object observations are
incomplete or partially captured. Project page:
https://minfenli.github.io/UA-Pose/

</details>


### [178] [MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation](https://arxiv.org/abs/2506.07999)
*Junhao Chen,Yulia Tsvetkov,Xiaochuang Han*

Main category: cs.CV

TL;DR: MADFormer是一种结合自回归（AR）和扩散模型的混合生成模型，通过分区和混合层设计优化高分辨率图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有混合模型缺乏系统指导，无法有效分配AR和扩散模型的能力。

Method: MADFormer将图像生成分为空间块，AR层用于全局条件，扩散层用于局部细化。

Result: 实验表明，块分区显著提升高分辨率图像性能，混合层设计在质量与效率间取得更好平衡。

Conclusion: MADFormer为未来混合生成模型提供了实用设计原则。

Abstract: Recent progress in multimodal generation has increasingly combined
autoregressive (AR) and diffusion-based approaches, leveraging their
complementary strengths: AR models capture long-range dependencies and produce
fluent, context-aware outputs, while diffusion models operate in continuous
latent spaces to refine high-fidelity visual details. However, existing hybrids
often lack systematic guidance on how and why to allocate model capacity
between these paradigms. In this work, we introduce MADFormer, a Mixed
Autoregressive and Diffusion Transformer that serves as a testbed for analyzing
AR-diffusion trade-offs. MADFormer partitions image generation into spatial
blocks, using AR layers for one-pass global conditioning across blocks and
diffusion layers for iterative local refinement within each block. Through
controlled experiments on FFHQ-1024 and ImageNet, we identify two key insights:
(1) block-wise partitioning significantly improves performance on
high-resolution images, and (2) vertically mixing AR and diffusion layers
yields better quality-efficiency balances--improving FID by up to 75% under
constrained inference compute. Our findings offer practical design principles
for future hybrid generative models.

</details>


### [179] [Aligning Text, Images, and 3D Structure Token-by-Token](https://arxiv.org/abs/2506.08002)
*Aadarsh Sahoo,Vansh Tibrewal,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 论文提出了一种统一的LLM框架，用于对齐语言、图像和3D场景，并提供了关键设计选择的详细指南。


<details>
  <summary>Details</summary>
Motivation: 为设计师和机器人提供3D场景理解能力，推动3D建模和交互技术的发展。

Method: 采用自回归模型，结合量化形状编码，优化数据表示和模态特定目标。

Result: 在渲染、识别、指令跟随和问答等任务中表现优异，并在合成和真实数据集上验证。

Conclusion: 该框架在3D场景理解和任务处理中具有潜力，为未来研究提供了实用指南。

Abstract: Creating machines capable of understanding the world in 3D is essential in
assisting designers that build and edit 3D environments and robots navigating
and interacting within a three-dimensional space. Inspired by advances in
language and image modeling, we investigate the potential of autoregressive
models for a new modality: structured 3D scenes. To this end, we propose a
unified LLM framework that aligns language, images, and 3D scenes and provide a
detailed ''cookbook'' outlining critical design choices for achieving optimal
training and performance addressing key questions related to data
representation, modality-specific objectives, and more. We evaluate performance
across four core 3D tasks -- rendering, recognition, instruction-following, and
question-answering -- and four 3D datasets, synthetic and real-world. We extend
our approach to reconstruct complex 3D object shapes by enriching our 3D
modality with quantized shape encodings, and show our model's effectiveness on
real-world 3D object recognition tasks. Project webpage:
https://glab-caltech.github.io/kyvo/

</details>


### [180] [Audio-Sync Video Generation with Multi-Stream Temporal Control](https://arxiv.org/abs/2506.08003)
*Shuchen Weng,Haojie Zheng,Zheng Chang,Si Li,Boxin Shi,Xinlong Wang*

Main category: cs.CV

TL;DR: MTV是一个用于音频同步视频生成的框架，通过分离音频轨道实现精细控制，并在DEMIX数据集上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 音频与视频的精确同步在多样化和复杂的音频类型中仍具挑战性，需要高质量的视频生成方法。

Method: MTV框架将音频分为语音、音效和音乐轨道，分别控制唇部动作、事件时间和视觉氛围。

Result: MTV在视频质量、文本-视频一致性和音频-视频对齐等六个标准指标上达到最先进性能。

Conclusion: MTV通过分离音频轨道实现了高质量和语义对齐的视频生成，为音频驱动的视频生成提供了新思路。

Abstract: Audio is inherently temporal and closely synchronized with the visual world,
making it a naturally aligned and expressive control signal for controllable
video generation (e.g., movies). Beyond control, directly translating audio
into video is essential for understanding and visualizing rich audio narratives
(e.g., Podcasts or historical recordings). However, existing approaches fall
short in generating high-quality videos with precise audio-visual
synchronization, especially across diverse and complex audio types. In this
work, we introduce MTV, a versatile framework for audio-sync video generation.
MTV explicitly separates audios into speech, effects, and music tracks,
enabling disentangled control over lip motion, event timing, and visual mood,
respectively -- resulting in fine-grained and semantically aligned video
generation. To support the framework, we additionally present DEMIX, a dataset
comprising high-quality cinematic videos and demixed audio tracks. DEMIX is
structured into five overlapped subsets, enabling scalable multi-stage training
for diverse generation scenarios. Extensive experiments demonstrate that MTV
achieves state-of-the-art performance across six standard metrics spanning
video quality, text-video consistency, and audio-video alignment. Project page:
https://hjzheng.net/projects/MTV/.

</details>


### [181] [Dynamic View Synthesis as an Inverse Problem](https://arxiv.org/abs/2506.08004)
*Hidir Yesiltepe,Pinar Yanardag*

Main category: cs.CV

TL;DR: 通过重新设计预训练视频扩散模型的噪声初始化阶段，实现无需权重更新或辅助模块的高保真动态视图合成。


<details>
  <summary>Details</summary>
Motivation: 解决单目视频动态视图合成的逆问题，特别是在零终端信噪比（SNR）计划下确定性反转的基本障碍。

Method: 引入K阶递归噪声表示和随机潜在调制，实现潜在空间的精确对齐和遮挡区域合成。

Result: 实验表明，通过噪声初始化阶段的结构化潜在操作，可以有效实现动态视图合成。

Conclusion: 动态视图合成可以通过噪声初始化阶段的潜在操作高效完成，无需额外训练或模块。

Abstract: In this work, we address dynamic view synthesis from monocular videos as an
inverse problem in a training-free setting. By redesigning the noise
initialization phase of a pre-trained video diffusion model, we enable
high-fidelity dynamic view synthesis without any weight updates or auxiliary
modules. We begin by identifying a fundamental obstacle to deterministic
inversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and
resolve it by introducing a novel noise representation, termed K-order
Recursive Noise Representation. We derive a closed form expression for this
representation, enabling precise and efficient alignment between the
VAE-encoded and the DDIM inverted latents. To synthesize newly visible regions
resulting from camera motion, we introduce Stochastic Latent Modulation, which
performs visibility aware sampling over the latent space to complete occluded
regions. Comprehensive experiments demonstrate that dynamic view synthesis can
be effectively performed through structured latent manipulation in the noise
initialization phase.

</details>


### [182] [ZeroVO: Visual Odometry with Minimal Assumptions](https://arxiv.org/abs/2506.08005)
*Lei Lai,Zekai Yin,Eshed Ohn-Bar*

Main category: cs.CV

TL;DR: ZeroVO是一种新型视觉里程计算法，无需预定义或静态相机校准，实现跨多样相机和环境的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义或静态相机校准，限制了其适用性。ZeroVO旨在克服这一限制，提供更通用的解决方案。

Method: 1. 设计无校准、几何感知的网络结构；2. 引入基于语言的先验增强语义信息；3. 开发半监督训练范式，适应新场景。

Result: 在KITTI、nuScenes和Argoverse 2等基准测试中，性能提升超过30%，并在GTA合成数据集上验证。

Conclusion: ZeroVO无需微调或相机校准，扩展了视觉里程计的适用性，适合大规模实际部署。

Abstract: We introduce ZeroVO, a novel visual odometry (VO) algorithm that achieves
zero-shot generalization across diverse cameras and environments, overcoming
limitations in existing methods that depend on predefined or static camera
calibration setups. Our approach incorporates three main innovations. First, we
design a calibration-free, geometry-aware network structure capable of handling
noise in estimated depth and camera parameters. Second, we introduce a
language-based prior that infuses semantic information to enhance robust
feature extraction and generalization to previously unseen domains. Third, we
develop a flexible, semi-supervised training paradigm that iteratively adapts
to new scenes using unlabeled data, further boosting the models' ability to
generalize across diverse real-world scenarios. We analyze complex autonomous
driving contexts, demonstrating over 30% improvement against prior methods on
three standard benchmarks, KITTI, nuScenes, and Argoverse 2, as well as a newly
introduced, high-fidelity synthetic dataset derived from Grand Theft Auto
(GTA). By not requiring fine-tuning or camera calibration, our work broadens
the applicability of VO, providing a versatile solution for real-world
deployment at scale.

</details>


### [183] [Dreamland: Controllable World Creation with Simulator and Generative Models](https://arxiv.org/abs/2506.08006)
*Sicheng Mo,Ziyang Leng,Leon Liu,Weizhen Wang,Honglin He,Bolei Zhou*

Main category: cs.CV

TL;DR: Dreamland是一个结合物理模拟器和生成模型的混合世界生成框架，通过分层抽象增强可控性，支持现有生成模型的即插即用。


<details>
  <summary>Details</summary>
Motivation: 现有大规模视频生成模型缺乏元素级可控性，限制了其在场景编辑和AI代理训练中的应用。

Method: 设计分层世界抽象，编码像素级和对象级语义与几何信息，作为模拟器与生成模型之间的中间表示。

Result: 实验显示Dreamland在图像质量上提升50.8%，可控性增强17.9%，并显著提升AI代理训练效果。

Conclusion: Dreamland通过分层抽象和早期对齐，显著提升了生成模型的可控性和实用性，支持未来生成模型的扩展。

Abstract: Large-scale video generative models can synthesize diverse and realistic
visual content for dynamic world creation, but they often lack element-wise
controllability, hindering their use in editing scenes and training embodied AI
agents. We propose Dreamland, a hybrid world generation framework combining the
granular control of a physics-based simulator and the photorealistic content
output of large-scale pretrained generative models. In particular, we design a
layered world abstraction that encodes both pixel-level and object-level
semantics and geometry as an intermediate representation to bridge the
simulator and the generative model. This approach enhances controllability,
minimizes adaptation cost through early alignment with real-world
distributions, and supports off-the-shelf use of existing and future pretrained
generative models. We further construct a D3Sim dataset to facilitate the
training and evaluation of hybrid generation pipelines. Experiments demonstrate
that Dreamland outperforms existing baselines with 50.8% improved image
quality, 17.9% stronger controllability, and has great potential to enhance
embodied agent training. Code and data will be made available.

</details>


### [184] [Hidden in plain sight: VLMs overlook their visual representations](https://arxiv.org/abs/2506.08008)
*Stephanie Fu,Tyler Bonnen,Devin Guillory,Trevor Darrell*

Main category: cs.CV

TL;DR: 研究发现，视觉语言模型（VLMs）在视觉任务中表现不如其视觉编码器，主要原因是未能有效整合视觉和语言信息。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在整合视觉和语言信息方面的能力，并诊断其失败模式。

Method: 通过一系列视觉中心基准测试（如深度估计、对应关系）比较VLMs与其视觉编码器的表现，并分析模型各部分的表现。

Result: VLMs在视觉任务中表现显著下降，接近随机水平，主要瓶颈在于未能有效利用视觉信息。

Conclusion: VLMs在视觉任务中的表现受限于语言模型的先验知识，未来研究需改进视觉信息的整合方式。

Abstract: Language provides a natural interface to specify and evaluate performance on
visual tasks. To realize this possibility, vision language models (VLMs) must
successfully integrate visual and linguistic information. Our work compares
VLMs to a direct readout of their visual encoders to understand their ability
to integrate across these modalities. Across a series of vision-centric
benchmarks (e.g., depth estimation, correspondence), we find that VLMs perform
substantially worse than their visual encoders, dropping to near-chance
performance. We investigate these results through a series of analyses across
the entire VLM: namely 1) the degradation of vision representations, 2)
brittleness to task prompt, and 3) the language model's role in solving the
task. We find that the bottleneck in performing these vision-centric tasks lies
in this third category; VLMs are not effectively using visual information
easily accessible throughout the entire model, and they inherit the language
priors present in the LLM. Our work helps diagnose the failure modes of
open-source VLMs, and presents a series of evaluations useful for future
investigations into visual understanding within VLMs.

</details>


### [185] [Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion](https://arxiv.org/abs/2506.08009)
*Xun Huang,Zhengqi Li,Guande He,Mingyuan Zhou,Eli Shechtman*

Main category: cs.CV

TL;DR: Self Forcing是一种新的自回归视频扩散模型训练范式，通过KV缓存和自生成输出解决曝光偏差问题，实现高效实时视频生成。


<details>
  <summary>Details</summary>
Motivation: 解决自回归视频扩散模型中曝光偏差问题，即模型在推理时需基于自身不完美输出生成序列。

Method: 采用KV缓存进行自回归展开，结合全视频级监督损失和随机梯度截断策略，提升训练效率和生成质量。

Result: 实验表明，该方法在单GPU上实现亚秒级延迟的实时视频生成，性能优于非因果扩散模型。

Conclusion: Self Forcing通过创新训练策略，显著提升视频生成效率和质量，具有实际应用潜力。

Abstract: We introduce Self Forcing, a novel training paradigm for autoregressive video
diffusion models. It addresses the longstanding issue of exposure bias, where
models trained on ground-truth context must generate sequences conditioned on
their own imperfect outputs during inference. Unlike prior methods that denoise
future frames based on ground-truth context frames, Self Forcing conditions
each frame's generation on previously self-generated outputs by performing
autoregressive rollout with key-value (KV) caching during training. This
strategy enables supervision through a holistic loss at the video level that
directly evaluates the quality of the entire generated sequence, rather than
relying solely on traditional frame-wise objectives. To ensure training
efficiency, we employ a few-step diffusion model along with a stochastic
gradient truncation strategy, effectively balancing computational cost and
performance. We further introduce a rolling KV cache mechanism that enables
efficient autoregressive video extrapolation. Extensive experiments demonstrate
that our approach achieves real-time streaming video generation with sub-second
latency on a single GPU, while matching or even surpassing the generation
quality of significantly slower and non-causal diffusion models. Project
website: http://self-forcing.github.io/

</details>


### [186] [Vision Transformers Don't Need Trained Registers](https://arxiv.org/abs/2506.08010)
*Nick Jiang,Amil Dravid,Alexei Efros,Yossi Gandelsman*

Main category: cs.CV

TL;DR: 研究发现Vision Transformers中存在高范数令牌导致注意力图噪声的问题，提出一种无需重新训练的解决方案，通过转移高范数激活到未训练的令牌中，改善模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索Vision Transformers中高范数令牌导致注意力图噪声的机制，并提出无需重新训练的解决方案。

Method: 通过将高范数激活从特定神经元转移到未训练的令牌中，模拟注册令牌的效果。

Result: 方法能生成更清晰的注意力和特征图，提升下游视觉任务性能，效果接近显式训练注册令牌的模型。

Conclusion: 测试时注册令牌可替代训练时的注册令牌，为预训练模型提供无需重新训练的解决方案。

Abstract: We investigate the mechanism underlying a previously identified phenomenon in
Vision Transformers -- the emergence of high-norm tokens that lead to noisy
attention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a
sparse set of neurons is responsible for concentrating high-norm activations on
outlier tokens, leading to irregular attention patterns and degrading
downstream visual processing. While the existing solution for removing these
outliers involves retraining models from scratch with additional learned
register tokens, we use our findings to create a training-free approach to
mitigate these artifacts. By shifting the high-norm activations from our
discovered register neurons into an additional untrained token, we can mimic
the effect of register tokens on a model already trained without registers. We
demonstrate that our method produces cleaner attention and feature maps,
enhances performance over base models across multiple downstream visual tasks,
and achieves results comparable to models explicitly trained with register
tokens. We then extend test-time registers to off-the-shelf vision-language
models to improve their interpretability. Our results suggest that test-time
registers effectively take on the role of register tokens at test-time,
offering a training-free solution for any pre-trained model released without
them.

</details>


### [187] [Play to Generalize: Learning to Reason Through Game Play](https://arxiv.org/abs/2506.08011)
*Yunfei Xie,Yinsong Ma,Shiyi Lan,Alan Yuille,Junfei Xiao,Chen Wei*

Main category: cs.CV

TL;DR: ViGaL是一种通过玩街机游戏（如Snake）增强多模态大语言模型（MLLMs）泛化推理能力的新方法。


<details>
  <summary>Details</summary>
Motivation: 受认知科学启发，游戏能促进可迁移的认知技能，因此提出通过游戏提升MLLMs的跨领域推理能力。

Method: 采用强化学习（RL）对7B参数的MLLM进行后训练，训练任务为简单的街机游戏。

Result: 模型在MathVista和MMMU等任务上表现显著提升，甚至超越专用模型，同时保持基础模型在通用视觉任务上的性能。

Conclusion: 规则化游戏可作为可控、可扩展的预训练任务，释放MLLMs的泛化多模态推理能力。

Abstract: Developing generalizable reasoning capabilities in multimodal large language
models (MLLMs) remains challenging. Motivated by cognitive science literature
suggesting that gameplay promotes transferable cognitive skills, we propose a
novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs
develop out-of-domain generalization of multimodal reasoning through playing
arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM
via reinforcement learning (RL) on simple arcade-like games, e.g. Snake,
significantly enhances its downstream performance on multimodal math benchmarks
like MathVista, and on multi-discipline questions like MMMU, without seeing any
worked solutions, equations, or diagrams during RL, suggesting the capture of
transferable reasoning skills. Remarkably, our model outperforms specialist
models tuned on multimodal reasoning data in multimodal reasoning benchmarks,
while preserving the base model's performance on general visual benchmarks, a
challenge where specialist models often fall short. Our findings suggest a new
post-training paradigm: synthetic, rule-based games can serve as controllable
and scalable pre-text tasks that unlock generalizable multimodal reasoning
abilities in MLLMs.

</details>


### [188] [StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets](https://arxiv.org/abs/2506.08013)
*Anh-Quan Cao,Ivan Lopes,Raoul de Charette*

Main category: cs.CV

TL;DR: StableMTL利用扩散模型在零样本设置下进行多任务学习，通过潜在回归和任务注意力机制提升任务间协同，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 多任务学习需要大量标注数据，而部分任务标注的零样本设置可以降低标注需求，扩散模型的泛化能力为此提供了可能。

Method: 采用扩散模型的潜在回归框架，结合任务编码和任务注意力机制，统一潜在损失替代多任务损失，提升任务间协同。

Result: 在8个基准测试的7个任务上，StableMTL表现优于基线方法。

Conclusion: StableMTL通过扩散模型和任务注意力机制，实现了零样本多任务学习的有效协同和性能提升。

Abstract: Multi-task learning for dense prediction is limited by the need for extensive
annotation for every task, though recent works have explored training with
partial task labels. Leveraging the generalization power of diffusion models,
we extend the partial learning setup to a zero-shot setting, training a
multi-task model on multiple synthetic datasets, each labeled for only a subset
of tasks. Our method, StableMTL, repurposes image generators for latent
regression. Adapting a denoising framework with task encoding, per-task
conditioning and a tailored training scheme. Instead of per-task losses
requiring careful balancing, a unified latent loss is adopted, enabling
seamless scaling to more tasks. To encourage inter-task synergy, we introduce a
multi-stream model with a task-attention mechanism that converts N-to-N task
interactions into efficient 1-to-N attention, promoting effective cross-task
sharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.

</details>


### [189] [4DGT: Learning a 4D Gaussian Transformer Using Real-World Monocular Videos](https://arxiv.org/abs/2506.08015)
*Zhen Xu,Zhengqin Li,Zhao Dong,Xiaowei Zhou,Richard Newcombe,Zhaoyang Lv*

Main category: cs.CV

TL;DR: 4DGT是一种基于4D高斯和Transformer的动态场景重建模型，通过单目视频训练，统一静态和动态组件，实现高效渲染。


<details>
  <summary>Details</summary>
Motivation: 动态场景重建需要处理复杂的时间变化环境，传统方法效率低且难以扩展。

Method: 使用4D高斯作为归纳偏置，提出密度控制策略，处理长时空输入，采用滚动窗口预测一致的4D高斯。

Result: 4DGT在真实视频中显著优于其他高斯网络，在跨域视频中与优化方法精度相当，且推理时间从小时级降至秒级。

Conclusion: 4DGT通过高效的前馈推理和密度控制策略，实现了动态场景的高效重建，适用于长视频序列。

Abstract: We propose 4DGT, a 4D Gaussian-based Transformer model for dynamic scene
reconstruction, trained entirely on real-world monocular posed videos. Using 4D
Gaussian as an inductive bias, 4DGT unifies static and dynamic components,
enabling the modeling of complex, time-varying environments with varying object
lifespans. We proposed a novel density control strategy in training, which
enables our 4DGT to handle longer space-time input and remain efficient
rendering at runtime. Our model processes 64 consecutive posed frames in a
rolling-window fashion, predicting consistent 4D Gaussians in the scene. Unlike
optimization-based methods, 4DGT performs purely feed-forward inference,
reducing reconstruction time from hours to seconds and scaling effectively to
long video sequences. Trained only on large-scale monocular posed video
datasets, 4DGT can outperform prior Gaussian-based networks significantly in
real-world videos and achieve on-par accuracy with optimization-based methods
on cross-domain videos. Project page: https://4dgt.github.io

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [190] [ResPF: Residual Poisson Flow for Efficient and Physically Consistent Sparse-View CT Reconstruction](https://arxiv.org/abs/2506.06400)
*Changsheng Fang,Yongtong Liu,Bahareh Morovati,Shuo Han,Yu Shi,Li Zhou,Shuyi Fan,Hengyong Yu*

Main category: eess.IV

TL;DR: 提出了一种基于Poisson Flow生成模型（ResPF）的高效稀疏视图CT重建方法，通过条件引导和残差融合模块，显著降低了计算成本并提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图CT在减少辐射剂量的同时，重建精度面临挑战。现有深度学习和扩散模型缺乏物理可解释性或计算成本高。

Method: 基于PFGM++，ResPF引入条件引导和残差融合模块，跳过冗余初始步骤并嵌入数据一致性约束。

Result: 在合成和临床数据集上，ResPF表现出优越的重建质量、更快的推理速度和更强的鲁棒性。

Conclusion: ResPF是首个将Poisson Flow模型应用于稀疏视图CT的方法，显著优于现有技术。

Abstract: Sparse-view computed tomography (CT) is a practical solution to reduce
radiation dose, but the resulting ill-posed inverse problem poses significant
challenges for accurate image reconstruction. Although deep learning and
diffusion-based methods have shown promising results, they often lack physical
interpretability or suffer from high computational costs due to iterative
sampling starting from random noise. Recent advances in generative modeling,
particularly Poisson Flow Generative Models (PFGM), enable high-fidelity image
synthesis by modeling the full data distribution. In this work, we propose
Residual Poisson Flow (ResPF) Generative Models for efficient and accurate
sparse-view CT reconstruction. Based on PFGM++, ResPF integrates conditional
guidance from sparse measurements and employs a hijacking strategy to
significantly reduce sampling cost by skipping redundant initial steps.
However, skipping early stages can degrade reconstruction quality and introduce
unrealistic structures. To address this, we embed a data-consistency into each
iteration, ensuring fidelity to sparse-view measurements. Yet, PFGM sampling
relies on a fixed ordinary differential equation (ODE) trajectory induced by
electrostatic fields, which can be disrupted by step-wise data consistency,
resulting in unstable or degraded reconstructions. Inspired by ResNet, we
introduce a residual fusion module to linearly combine generative outputs with
data-consistent reconstructions, effectively preserving trajectory continuity.
To the best of our knowledge, this is the first application of Poisson flow
models to sparse-view CT. Extensive experiments on synthetic and clinical
datasets demonstrate that ResPF achieves superior reconstruction quality,
faster inference, and stronger robustness compared to state-of-the-art
iterative, learning-based, and diffusion models.

</details>


### [191] [SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation](https://arxiv.org/abs/2506.06890)
*Sumit Sharma,Gopi Raju Matta,Kaushik Mitra*

Main category: eess.IV

TL;DR: 提出了一种两阶段框架，将二进制SPC图像转换为高质量彩色新视图，通过生成模型和3D重建技术实现。


<details>
  <summary>Details</summary>
Motivation: SPC图像因二进制特性导致信息丢失，传统3D合成技术无效，需新方法解决。

Method: 第一阶段用Pix2PixHD进行图像转换，第二阶段用NeRF或3DGS进行3D重建。

Result: 实验验证了框架在感知质量和几何一致性上的显著提升。

Conclusion: 两阶段框架有效解决了SPC图像的信息丢失问题，生成高质量彩色视图。

Abstract: Single Photon Avalanche Diodes (SPADs) represent a cutting-edge imaging
technology, capable of detecting individual photons with remarkable timing
precision. Building on this sensitivity, Single Photon Cameras (SPCs) enable
image capture at exceptionally high speeds under both low and high
illumination. Enabling 3D reconstruction and radiance field recovery from such
SPC data holds significant promise. However, the binary nature of SPC images
leads to severe information loss, particularly in texture and color, making
traditional 3D synthesis techniques ineffective. To address this challenge, we
propose a modular two-stage framework that converts binary SPC images into
high-quality colorized novel views. The first stage performs image-to-image
(I2I) translation using generative models such as Pix2PixHD, converting binary
SPC inputs into plausible RGB representations. The second stage employs 3D
scene reconstruction techniques like Neural Radiance Fields (NeRF) or Gaussian
Splatting (3DGS) to generate novel views. We validate our two-stage pipeline
(Pix2PixHD + Nerf/3DGS) through extensive qualitative and quantitative
experiments, demonstrating significant improvements in perceptual quality and
geometric consistency over the alternative baseline.

</details>


### [192] [Quanta Diffusion](https://arxiv.org/abs/2506.06945)
*Prateek Chennuri,Dongdong Fu,Stanley H. Chan*

Main category: eess.IV

TL;DR: QuDi是一种基于扩散算法的生成视频重建方法，专为单光子成像设计，显著提升了低光条件下的成像质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在极低光条件下同时处理运动和强散粒噪声的困难。

Method: 将基于物理的前向模型注入扩散算法，并在循环中保持运动估计。

Result: 相比现有最佳方法，平均PSNR提升2.4 dB。

Conclusion: QuDi在单光子成像中表现出色，为低光视频重建提供了有效解决方案。

Abstract: We present Quanta Diffusion (QuDi), a powerful generative video
reconstruction method for single-photon imaging. QuDi is an algorithm
supporting the latest Quanta Image Sensors (QIS) and Single Photon Avalanche
Diodes (SPADs) for extremely low-light imaging conditions. Compared to existing
methods, QuDi overcomes the difficulties of simultaneously managing the motion
and the strong shot noise. The core innovation of QuDi is to inject a
physics-based forward model into the diffusion algorithm, while keeping the
motion estimation in the loop. QuDi demonstrates an average of 2.4 dB PSNR
improvement over the best existing methods.

</details>


### [193] [Optimal Transport Driven Asymmetric Image-to-Image Translation for Nuclei Segmentation of Histological Images](https://arxiv.org/abs/2506.07023)
*Suman Mahapatra,Pradipta Maji*

Main category: eess.IV

TL;DR: 论文提出了一种新的深度生成模型，用于从组织学图像中分割细胞核结构，解决了图像到图像转换模型在信息不对称时的失败问题。


<details>
  <summary>Details</summary>
Motivation: 组织学图像中细胞核区域的分割有助于疾病的检测和诊断，但现有图像到图像转换模型在信息不对称时表现不佳。

Method: 模型引入嵌入空间处理信息不对称问题，结合最优传输和测度理论，开发了可逆生成器，并引入空间约束压缩操作。

Result: 模型在公开数据集上表现优于现有方法，提供了网络复杂度和性能的更好平衡。

Conclusion: 提出的模型通过可逆生成器和空间约束操作，显著提升了细胞核分割的性能和效率。

Abstract: Segmentation of nuclei regions from histological images enables morphometric
analysis of nuclei structures, which in turn helps in the detection and
diagnosis of diseases under consideration. To develop a nuclei segmentation
algorithm, applicable to different types of target domain representations,
image-to-image translation networks can be considered as they are invariant to
target domain image representations. One of the important issues with
image-to-image translation models is that they fail miserably when the
information content between two image domains are asymmetric in nature. In this
regard, the paper introduces a new deep generative model for segmenting nuclei
structures from histological images. The proposed model considers an embedding
space for handling information-disparity between information-rich histological
image space and information-poor segmentation map domain. Integrating
judiciously the concepts of optimal transport and measure theory, the model
develops an invertible generator, which provides an efficient optimization
framework with lower network complexity. The concept of invertible generator
automatically eliminates the need of any explicit cycle-consistency loss. The
proposed model also introduces a spatially-constrained squeeze operation within
the framework of invertible generator to maintain spatial continuity within the
image patches. The model provides a better trade-off between network complexity
and model performance compared to other existing models having complex network
architectures. The performance of the proposed deep generative model, along
with a comparison with state-of-the-art nuclei segmentation methods, is
demonstrated on publicly available histological image data sets.

</details>


### [194] [SiliCoN: Simultaneous Nuclei Segmentation and Color Normalization of Histological Images](https://arxiv.org/abs/2506.07028)
*Suman Mahapatra,Pradipta Maji*

Main category: eess.IV

TL;DR: 论文提出了一种新颖的深度生成模型，用于同时分割细胞核结构和标准化染色组织图像的颜色外观。


<details>
  <summary>Details</summary>
Motivation: 解决染色组织图像中颜色变化对细胞核分割的影响，并提升分割和颜色标准化的效果。

Method: 结合截断正态分布和空间注意力机制，假设颜色外观信息与细胞核分割图和嵌入信息独立。

Result: 模型在公开数据集上表现优异，并与现有先进算法进行了比较。

Conclusion: 该模型具有通用性和适应性，颜色外观信息的修改或丢失不会影响细胞核分割结果。

Abstract: Segmentation of nuclei regions from histological images is an important task
for automated computer-aided analysis of histological images, particularly in
the presence of impermissible color variation in the color appearance of
stained tissue images. While color normalization enables better nuclei
segmentation, accurate segmentation of nuclei structures makes color
normalization rather trivial. In this respect, the paper proposes a novel deep
generative model for simultaneously segmenting nuclei structures and
normalizing color appearance of stained histological images.This model
judiciously integrates the merits of truncated normal distribution and spatial
attention. The model assumes that the latent color appearance information,
corresponding to a particular histological image, is independent of respective
nuclei segmentation map as well as embedding map information. The disentangled
representation makes the model generalizable and adaptable as the modification
or loss in color appearance information cannot be able to affect the nuclei
segmentation map as well as embedding information. Also, for dealing with the
stain overlap of associated histochemical reagents, the prior for latent color
appearance code is assumed to be a mixture of truncated normal distributions.
The proposed model incorporates the concept of spatial attention for
segmentation of nuclei regions from histological images. The performance of the
proposed approach, along with a comparative analysis with related
state-of-the-art algorithms, has been demonstrated on publicly available
standard histological image data sets.

</details>


### [195] [Simultaneous Segmentation of Ventricles and Normal/Abnormal White Matter Hyperintensities in Clinical MRI using Deep Learning](https://arxiv.org/abs/2506.07123)
*Mahdi Bashiri Bawil,Mousa Shamsi,Abolhassan Shakeri Bavil*

Main category: eess.IV

TL;DR: 提出了一种基于2D pix2pix的深度学习框架，用于同时分割脑室和白质高信号，并能区分正常和病理高信号，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前MRI分割方法存在独立分割结构、难以区分正常与病理高信号、对临床各向异性数据优化不足等问题。

Method: 采用2D pix2pix深度学习框架，开发并验证于300名MS患者的FLAIR MRI数据。

Result: 在脑室分割（Dice: 0.801）和WMH分割（Dice: 0.624）上表现优异，并能区分正常与异常高信号（Dice: 0.647），计算效率高（4秒/例）。

Conclusion: 该方法在准确性、临床区分能力和计算效率上显著改进，有望整合到常规临床工作流中，提升MS诊断和监测。

Abstract: Multiple sclerosis (MS) diagnosis and monitoring rely heavily on accurate
assessment of brain MRI biomarkers, particularly white matter hyperintensities
(WMHs) and ventricular changes. Current segmentation approaches suffer from
several limitations: they typically segment these structures independently
despite their pathophysiological relationship, struggle to differentiate
between normal and pathological hyperintensities, and are poorly optimized for
anisotropic clinical MRI data. We propose a novel 2D pix2pix-based deep
learning framework for simultaneous segmentation of ventricles and WMHs with
the unique capability to distinguish between normal periventricular
hyperintensities and pathological MS lesions. Our method was developed and
validated on FLAIR MRI scans from 300 MS patients. Compared to established
methods (SynthSeg, Atlas Matching, BIANCA, LST-LPA, LST-LGA, and WMH-SynthSeg),
our approach achieved superior performance for both ventricle segmentation
(Dice: 0.801+/-0.025, HD95: 18.46+/-7.1mm) and WMH segmentation (Dice:
0.624+/-0.061, precision: 0.755+/-0.161). Furthermore, our method successfully
differentiated between normal and abnormal hyperintensities with a Dice
coefficient of 0.647. Notably, our approach demonstrated exceptional
computational efficiency, completing end-to-end processing in approximately 4
seconds per case, up to 36 times faster than baseline methods, while
maintaining minimal resource requirements. This combination of improved
accuracy, clinically relevant differentiation capability, and computational
efficiency addresses critical limitations in current neuroimaging analysis,
potentially enabling integration into routine clinical workflows and enhancing
MS diagnosis and monitoring.

</details>


### [196] [Transfer Learning and Explainable AI for Brain Tumor Classification: A Study Using MRI Data from Bangladesh](https://arxiv.org/abs/2506.07228)
*Shuvashis Sarker*

Main category: eess.IV

TL;DR: 研究提出了一种基于深度学习的自动化脑肿瘤分类系统，结合可解释AI（XAI）方法，提高了MRI数据的分类准确性和可解释性，适用于医疗资源有限的地区。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤的及时诊断对改善患者预后至关重要，尤其在医疗基础设施有限的国家（如孟加拉国）。手动MRI分析效率低且易出错，需要自动化解决方案。

Method: 使用VGG16、VGG19和ResNet50等深度学习模型对MRI数据进行分类，并采用Grad-CAM和Grad-CAM++等XAI方法增强模型可解释性。

Result: VGG16模型表现最佳，准确率达到99.17%。XAI方法提升了系统的透明度和稳定性。

Conclusion: 深度学习与XAI结合可有效提升脑肿瘤检测能力，适用于资源有限地区的临床应用。

Abstract: Brain tumors, regardless of being benign or malignant, pose considerable
health risks, with malignant tumors being more perilous due to their swift and
uncontrolled proliferation, resulting in malignancy. Timely identification is
crucial for enhancing patient outcomes, particularly in nations such as
Bangladesh, where healthcare infrastructure is constrained. Manual MRI analysis
is arduous and susceptible to inaccuracies, rendering it inefficient for prompt
diagnosis. This research sought to tackle these problems by creating an
automated brain tumor classification system utilizing MRI data obtained from
many hospitals in Bangladesh. Advanced deep learning models, including VGG16,
VGG19, and ResNet50, were utilized to classify glioma, meningioma, and various
brain cancers. Explainable AI (XAI) methodologies, such as Grad-CAM and
Grad-CAM++, were employed to improve model interpretability by emphasizing the
critical areas in MRI scans that influenced the categorization. VGG16 achieved
the most accuracy, attaining 99.17%. The integration of XAI enhanced the
system's transparency and stability, rendering it more appropriate for clinical
application in resource-limited environments such as Bangladesh. This study
highlights the capability of deep learning models, in conjunction with
explainable artificial intelligence (XAI), to enhance brain tumor detection and
identification in areas with restricted access to advanced medical
technologies.

</details>


### [197] [A Comprehensive Analysis of COVID-19 Detection Using Bangladeshi Data and Explainable AI](https://arxiv.org/abs/2506.07234)
*Shuvashis Sarker*

Main category: eess.IV

TL;DR: 研究利用VGG19模型在CXR图像中检测COVID-19，准确率达98%，并采用LIME和SMOTE提升模型透明度和平衡数据。


<details>
  <summary>Details</summary>
Motivation: COVID-19对全球和孟加拉国造成严重影响，亟需高效检测方法。

Method: 使用ML、DL和TL模型（如VGG19），结合LIME解释预测，SMOTE处理数据不平衡。

Result: VGG19模型达到98%准确率，LIME揭示了关键分类特征。

Conclusion: 研究强调XAI在提升模型透明度和可靠性中的重要性，优化了CXR图像的COVID-19检测。

Abstract: COVID-19 is a rapidly spreading and highly infectious virus which has
triggered a global pandemic, profoundly affecting millions across the world.
The pandemic has introduced unprecedented challenges in public health, economic
stability, and societal structures, necessitating the implementation of
extensive and multifaceted health interventions globally. It had a tremendous
impact on Bangladesh by April 2024, with around 29,495 fatalities and more than
2 million confirmed cases. This study focuses on improving COVID-19 detection
in CXR images by utilizing a dataset of 4,350 images from Bangladesh
categorized into four classes: Normal, Lung-Opacity, COVID-19 and
Viral-Pneumonia. ML, DL and TL models are employed with the VGG19 model
achieving an impressive 98% accuracy. LIME is used to explain model
predictions, highlighting the regions and features influencing classification
decisions. SMOTE is applied to address class imbalances. By providing insight
into both correct and incorrect classifications, the study emphasizes the
importance of XAI in enhancing the transparency and reliability of models,
ultimately improving the effectiveness of detection from CXR images.

</details>


### [198] [A Narrative Review on Large AI Models in Lung Cancer Screening, Diagnosis, and Treatment Planning](https://arxiv.org/abs/2506.07236)
*Jiachen Zhong,Yiting Wang,Di Zhu,Ziwei Wang*

Main category: eess.IV

TL;DR: 本文综述了大型AI模型在肺癌筛查、诊断、预后和治疗中的应用，分类并分析了现有模型及其性能，同时讨论了局限性和未来方向。


<details>
  <summary>Details</summary>
Motivation: 肺癌是全球高发且致命的疾病，需要更准确的诊断和治疗方法。大型AI模型在医学图像理解和临床决策方面展现出潜力。

Method: 系统调查了现有的大型AI模型，将其分为模态特定编码器、编码器-解码器框架和联合编码器架构，并评估了它们在多模态学习任务中的性能。

Result: 这些模型在肺结节检测、基因突变预测、多组学整合和个性化治疗规划中表现出色，部分已进入临床验证阶段。

Conclusion: 大型AI模型具有改变肺癌护理的潜力，但仍需解决泛化性、可解释性和法规合规性等挑战。

Abstract: Lung cancer remains one of the most prevalent and fatal diseases worldwide,
demanding accurate and timely diagnosis and treatment. Recent advancements in
large AI models have significantly enhanced medical image understanding and
clinical decision-making. This review systematically surveys the
state-of-the-art in applying large AI models to lung cancer screening,
diagnosis, prognosis, and treatment. We categorize existing models into
modality-specific encoders, encoder-decoder frameworks, and joint encoder
architectures, highlighting key examples such as CLIP, BLIP, Flamingo,
BioViL-T, and GLoRIA. We further examine their performance in multimodal
learning tasks using benchmark datasets like LIDC-IDRI, NLST, and MIMIC-CXR.
Applications span pulmonary nodule detection, gene mutation prediction,
multi-omics integration, and personalized treatment planning, with emerging
evidence of clinical deployment and validation. Finally, we discuss current
limitations in generalizability, interpretability, and regulatory compliance,
proposing future directions for building scalable, explainable, and clinically
integrated AI systems. Our review underscores the transformative potential of
large AI models to personalize and optimize lung cancer care.

</details>


### [199] [Text-guided multi-stage cross-perception network for medical image segmentation](https://arxiv.org/abs/2506.07475)
*Gaoyu Chen*

Main category: eess.IV

TL;DR: 论文提出了一种文本引导的多阶段交叉感知网络（TMC），通过多阶段交叉注意力模块和多阶段对齐损失，提升了医学图像分割的性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法因目标与非目标区域对比度低，导致语义表达弱。文本提示信息有潜力解决这一问题，但现有文本引导方法存在跨模态交互不足和特征表达不充分的问题。

Method: 提出TMC网络，引入多阶段交叉注意力模块增强语义细节理解，并使用多阶段对齐损失提升跨模态语义一致性。

Result: 在三个公开数据集（QaTa-COV19、MosMedData和Breast）上，TMC的Dice分数分别为84.77%、78.50%和88.73%，优于UNet和现有文本引导方法。

Conclusion: TMC通过改进跨模态交互和特征表达，显著提升了医学图像分割的性能。

Abstract: Medical image segmentation plays a crucial role in clinical medicine, serving
as a tool for auxiliary diagnosis, treatment planning, and disease monitoring,
thus facilitating physicians in the study and treatment of diseases. However,
existing medical image segmentation methods are limited by the weak semantic
expression of the target segmentation regions, which is caused by the low
contrast between the target and non-target segmentation regions. To address
this limitation, text prompt information has greast potential to capture the
lesion location. However, existing text-guided methods suffer from insufficient
cross-modal interaction and inadequate cross-modal feature expression. To
resolve these issues, we propose the Text-guided Multi-stage Cross-perception
network (TMC). In TMC, we introduce a multistage cross-attention module to
enhance the model's understanding of semantic details and a multi-stage
alignment loss to improve the consistency of cross-modal semantics. The results
of the experiments demonstrate that our TMC achieves a superior performance
with Dice of 84.77%, 78.50%, 88.73% in three public datasets (QaTa-COV19,
MosMedData and Breast), outperforming UNet based networks and text-guided
methods.

</details>


### [200] [Fine-Grained Motion Compression and Selective Temporal Fusion for Neural B-Frame Video Coding](https://arxiv.org/abs/2506.07709)
*Xihua Sheng,Peilin Chen,Meng Wang,Li Zhang,Shiqi Wang,Dapeng Oliver Wu*

Main category: eess.IV

TL;DR: 论文提出了一种改进的神经B帧视频编码方法，通过细粒度运动压缩和选择性时间融合提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有神经B帧编码器直接采用P帧工具，未能解决B帧压缩的独特挑战，导致性能不佳。

Method: 设计了细粒度运动压缩方法（交互式双分支运动自编码器）和选择性时间融合方法（预测双向融合权重）。

Result: 实验表明，该方法优于现有神经B帧编码器，性能接近或超过H.266/VVC参考软件。

Conclusion: 提出的方法有效解决了B帧压缩的挑战，提升了编码性能。

Abstract: With the remarkable progress in neural P-frame video coding, neural B-frame
coding has recently emerged as a critical research direction. However, most
existing neural B-frame codecs directly adopt P-frame coding tools without
adequately addressing the unique challenges of B-frame compression, leading to
suboptimal performance. To bridge this gap, we propose novel enhancements for
motion compression and temporal fusion for neural B-frame coding. First, we
design a fine-grained motion compression method. This method incorporates an
interactive dual-branch motion auto-encoder with per-branch adaptive
quantization steps, which enables fine-grained compression of bi-directional
motion vectors while accommodating their asymmetric bitrate allocation and
reconstruction quality requirements. Furthermore, this method involves an
interactive motion entropy model that exploits correlations between
bi-directional motion latent representations by interactively leveraging
partitioned latent segments as directional priors. Second, we propose a
selective temporal fusion method that predicts bi-directional fusion weights to
achieve discriminative utilization of bi-directional multi-scale temporal
contexts with varying qualities. Additionally, this method introduces a
hyperprior-based implicit alignment mechanism for contextual entropy modeling.
By treating the hyperprior as a surrogate for the contextual latent
representation, this mechanism implicitly mitigates the misalignment in the
fused bi-directional temporal priors. Extensive experiments demonstrate that
our proposed codec outperforms state-of-the-art neural B-frame codecs and
achieves comparable or even superior compression performance to the H.266/VVC
reference software under random-access configurations.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [201] [Vid2Sim: Generalizable, Video-based Reconstruction of Appearance, Geometry and Physics for Mesh-free Simulation](https://arxiv.org/abs/2506.06440)
*Chuhao Chen,Zhiyang Dou,Chen Wang,Yiming Huang,Anjun Chen,Qiao Feng,Jiatao Gu,Lingjie Liu*

Main category: cs.GR

TL;DR: Vid2Sim是一种基于视频的通用框架，通过无网格简化模拟（基于线性混合蒙皮）高效恢复几何和物理属性。


<details>
  <summary>Details</summary>
Motivation: 从视频中忠实重建纹理形状和物理属性是一个具有挑战性的问题，现有方法依赖复杂的优化流程，计算成本高且泛化性差。

Method: Vid2Sim使用前馈神经网络从视频中重建物理系统配置，并通过轻量级优化流程细化几何和物理属性。

Result: 实验表明，Vid2Sim在几何和物理属性重建中具有高精度和高效性。

Conclusion: Vid2Sim提供了一种高效、通用的视频重建方法，显著优于现有技术。

Abstract: Faithfully reconstructing textured shapes and physical properties from videos
presents an intriguing yet challenging problem. Significant efforts have been
dedicated to advancing such a system identification problem in this area.
Previous methods often rely on heavy optimization pipelines with a
differentiable simulator and renderer to estimate physical parameters. However,
these approaches frequently necessitate extensive hyperparameter tuning for
each scene and involve a costly optimization process, which limits both their
practicality and generalizability. In this work, we propose a novel framework,
Vid2Sim, a generalizable video-based approach for recovering geometry and
physical properties through a mesh-free reduced simulation based on Linear
Blend Skinning (LBS), offering high computational efficiency and versatile
representation capability. Specifically, Vid2Sim first reconstructs the
observed configuration of the physical system from video using a feed-forward
neural network trained to capture physical world knowledge. A lightweight
optimization pipeline then refines the estimated appearance, geometry, and
physical properties to closely align with video observations within just a few
minutes. Additionally, after the reconstruction, Vid2Sim enables high-quality,
mesh-free simulation with high efficiency. Extensive experiments demonstrate
that our method achieves superior accuracy and efficiency in reconstructing
geometry and physical properties from video data.

</details>


### [202] [Splat and Replace: 3D Reconstruction with Repetitive Elements](https://arxiv.org/abs/2506.06462)
*Nicolás Violante,Andreas Meuleman,Alban Gauthier,Frédo Durand,Thibault Groueix,George Drettakis*

Main category: cs.GR

TL;DR: 利用3D场景中的重复元素提升新视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如NeRF和3DGS）在新视角合成中表现优异，但在训练视角不足或遮挡严重时，渲染质量下降。环境中的重复元素为解决这一问题提供了可能。

Method: 提出一种方法，通过分割3DGS重建中的重复实例、对齐实例并共享信息，同时考虑实例间的外观差异，提升几何和渲染质量。

Result: 在合成和真实场景中验证了方法的有效性，显著提升了新视角合成的质量。

Conclusion: 利用重复元素可有效改善因视角不足或遮挡导致的渲染问题。

Abstract: We leverage repetitive elements in 3D scenes to improve novel view synthesis.
Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly
improved novel view synthesis but renderings of unseen and occluded parts
remain low-quality if the training views are not exhaustive enough. Our key
observation is that our environment is often full of repetitive elements. We
propose to leverage those repetitions to improve the reconstruction of
low-quality parts of the scene due to poor coverage and occlusions. We propose
a method that segments each repeated instance in a 3DGS reconstruction,
registers them together, and allows information to be shared among instances.
Our method improves the geometry while also accounting for appearance
variations across instances. We demonstrate our method on a variety of
synthetic and real scenes with typical repetitive elements, leading to a
substantial improvement in the quality of novel view synthesis.

</details>


### [203] [Noise Consistency Regularization for Improved Subject-Driven Image Synthesis](https://arxiv.org/abs/2506.06483)
*Yao Ni,Song Wen,Piotr Koniusz,Anoop Cherian*

Main category: cs.GR

TL;DR: 论文提出两种辅助一致性损失（先验一致性正则化损失和主题一致性正则化损失），以解决微调Stable Diffusion模型时的欠拟合和过拟合问题，提升生成图像的多样性和主题保真度。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法在主题驱动图像合成中存在欠拟合（无法可靠捕捉主题）和过拟合（记忆主题图像并减少背景多样性）的问题。

Method: 提出两种辅助一致性损失：先验一致性正则化损失（保持非主题图像的扩散噪声预测与预训练模型一致）和主题一致性正则化损失（增强模型对噪声调制潜在代码的鲁棒性）。

Result: 实验表明，加入这些损失后，模型在主题保真度和图像多样性上优于DreamBooth，CLIP分数、背景变化和视觉质量均有提升。

Conclusion: 提出的方法有效解决了微调中的欠拟合和过拟合问题，同时提升了生成图像的质量和多样性。

Abstract: Fine-tuning Stable Diffusion enables subject-driven image synthesis by
adapting the model to generate images containing specific subjects. However,
existing fine-tuning methods suffer from two key issues: underfitting, where
the model fails to reliably capture subject identity, and overfitting, where it
memorizes the subject image and reduces background diversity. To address these
challenges, we propose two auxiliary consistency losses for diffusion
fine-tuning. First, a prior consistency regularization loss ensures that the
predicted diffusion noise for prior (non-subject) images remains consistent
with that of the pretrained model, improving fidelity. Second, a subject
consistency regularization loss enhances the fine-tuned model's robustness to
multiplicative noise modulated latent code, helping to preserve subject
identity while improving diversity. Our experimental results demonstrate that
incorporating these losses into fine-tuning not only preserves subject identity
but also enhances image diversity, outperforming DreamBooth in terms of CLIP
scores, background variation, and overall visual quality.

</details>


### [204] [JGS2: Near Second-order Converging Jacobi/Gauss-Seidel for GPU Elastodynamics](https://arxiv.org/abs/2506.06494)
*Lei Lan,Zixuan Lu,Chun Yuan,Weiwei Xu,Hao Su,Huamin Wang,Chenfanfu Jiang,Yin Yang*

Main category: cs.GR

TL;DR: 本文提出了一种新颖的GPU算法，在保持类似Jacobi方法良好并行性的同时，实现了接近全空间牛顿法的收敛速度。通过解决过冲现象并引入理论上的二阶最优解，算法在计算成本仅略高于Jacobi方法的情况下，实现了接近二次收敛。


<details>
  <summary>Details</summary>
Motivation: 并行模拟中，收敛性和并行性常被视为冲突目标。本文旨在解决这一问题，提出一种既能保持高并行性又能实现快速收敛的方法。

Method: 通过分析过冲现象，推导出理论上的二阶最优解，并将其转化为可预计算的形式。利用Cubature采样降低运行时成本，同时引入全坐标公式以提高预计算效率。

Result: 实验结果表明，该方法在刚性和软材料模拟中均实现了二阶收敛，性能优于现有GPU方法50至100倍。

Conclusion: 本文提出的算法成功解决了并行模拟中收敛性与并行性的矛盾，为高质量模拟提供了高效解决方案。

Abstract: In parallel simulation, convergence and parallelism are often seen as
inherently conflicting objectives. Improved parallelism typically entails
lighter local computation and weaker coupling, which unavoidably slow the
global convergence. This paper presents a novel GPU algorithm that achieves
convergence rates comparable to fullspace Newton's method while maintaining
good parallelizability just like the Jacobi method. Our approach is built on a
key insight into the phenomenon of overshoot. Overshoot occurs when a local
solver aggressively minimizes its local energy without accounting for the
global context, resulting in a local update that undermines global convergence.
To address this, we derive a theoretically second-order optimal solution to
mitigate overshoot. Furthermore, we adapt this solution into a pre-computable
form. Leveraging Cubature sampling, our runtime cost is only marginally higher
than the Jacobi method, yet our algorithm converges nearly quadratically as
Newton's method. We also introduce a novel full-coordinate formulation for more
efficient pre-computation. Our method integrates seamlessly with the
incremental potential contact method and achieves second-order convergence for
both stiff and soft materials. Experimental results demonstrate that our
approach delivers high-quality simulations and outperforms state-of-the-art GPU
methods with 50 to 100 times better convergence.

</details>


### [205] [CrossGen: Learning and Generating Cross Fields for Quad Meshing](https://arxiv.org/abs/2506.07020)
*Qiujie Dong,Jiepeng Wang,Rui Xu,Cheng Lin,Yuan Liu,Shiqing Xin,Zichun Zhong,Xin Li,Changhe Tu,Taku Komura,Leif Kobbelt,Scott Schaefer,Wenping Wang*

Main category: cs.GR

TL;DR: CrossGen是一个快速生成高质量交叉场的新框架，通过联合几何和交叉场表示在潜在空间中，支持前馈预测和潜在生成建模。


<details>
  <summary>Details</summary>
Motivation: 现有方法在计算效率和生成质量之间难以平衡，通常需要缓慢的逐形状优化。

Method: 使用自动编码器网络架构，将点云表面编码为稀疏体素网格，解码为SDF几何和交叉场。结合扩散模型支持部分输入生成。

Result: 能够在不到一秒内生成高质量交叉场，具有高几何保真度、噪声鲁棒性和快速推理能力。

Conclusion: CrossGen在四边形网格生成任务中表现出色，适用于多种表面形状。

Abstract: Cross fields play a critical role in various geometry processing tasks,
especially for quad mesh generation. Existing methods for cross field
generation often struggle to balance computational efficiency with generation
quality, using slow per-shape optimization. We introduce CrossGen, a novel
framework that supports both feed-forward prediction and latent generative
modeling of cross fields for quad meshing by unifying geometry and cross field
representations within a joint latent space. Our method enables extremely fast
computation of high-quality cross fields of general input shapes, typically
within one second without per-shape optimization. Our method assumes a
point-sampled surface, or called a point-cloud surface, as input, so we can
accommodate various different surface representations by a straightforward
point sampling process. Using an auto-encoder network architecture, we encode
input point-cloud surfaces into a sparse voxel grid with fine-grained latent
spaces, which are decoded into both SDF-based surface geometry and cross
fields. We also contribute a dataset of models with both high-quality signed
distance fields (SDFs) representations and their corresponding cross fields,
and use it to train our network. Once trained, the network is capable of
computing a cross field of an input surface in a feed-forward manner, ensuring
high geometric fidelity, noise resilience, and rapid inference. Furthermore,
leveraging the same unified latent representation, we incorporate a diffusion
model for computing cross fields of new shapes generated from partial input,
such as sketches. To demonstrate its practical applications, we validate
CrossGen on the quad mesh generation task for a large variety of surface
shapes. Experimental results...

</details>


### [206] [Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization](https://arxiv.org/abs/2506.07069)
*Zhican Wang,Guanghui He,Dantong Liu,Lingjun Gao,Shell Xu Hu,Chen Zhang,Zhuoran Song,Nicholas Lane,Wayne Luk,Hongxiang Fan*

Main category: cs.GR

TL;DR: 3D高斯泼溅（3DGS）在高效视图合成中表现优异，但在资源受限设备上实时渲染仍具挑战。本文提出架构-算法协同设计，通过轴定向光栅化、神经排序和可重构处理阵列，显著提升性能和能效。


<details>
  <summary>Details</summary>
Motivation: 解决3DGS在资源受限设备上实时渲染的挑战，减少计算冗余和硬件资源消耗。

Method: 1. 轴定向光栅化减少重复计算；2. 神经排序替代硬件排序；3. 可重构处理阵列支持光栅化和神经网络推理；4. π轨迹瓦片调度优化内存访问。

Result: 相比边缘GPU，速度提升23.4~27.8倍，能耗降低28.8~51.4倍，同时保持渲染质量。

Conclusion: 提出的协同设计显著提升了3DGS的实时渲染性能和能效，计划开源以推动领域发展。

Abstract: 3D Gaussian Splatting (3DGS) has recently gained significant attention for
high-quality and efficient view synthesis, making it widely adopted in fields
such as AR/VR, robotics, and autonomous driving. Despite its impressive
algorithmic performance, real-time rendering on resource-constrained devices
remains a major challenge due to tight power and area budgets. This paper
presents an architecture-algorithm co-design to address these inefficiencies.
First, we reveal substantial redundancy caused by repeated computation of
common terms/expressions during the conventional rasterization. To resolve
this, we propose axis-oriented rasterization, which pre-computes and reuses
shared terms along both the X and Y axes through a dedicated hardware design,
effectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by
identifying the resource and performance inefficiency of the sorting process,
we introduce a novel neural sorting approach that predicts order-independent
blending weights using an efficient neural network, eliminating the need for
costly hardware sorters. A dedicated training framework is also proposed to
improve its algorithmic stability. Third, to uniformly support rasterization
and neural network inference, we design an efficient reconfigurable processing
array that maximizes hardware utilization and throughput. Furthermore, we
introduce a $\pi$-trajectory tile schedule, inspired by Morton encoding and
Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead.
Comprehensive experiments demonstrate that the proposed design preserves
rendering quality while achieving a speedup of $23.4\sim27.8\times$ and energy
savings of $28.8\sim51.4\times$ compared to edge GPUs for real-world scenes. We
plan to open-source our design to foster further development in this field.

</details>


### [207] [HOI-PAGE: Zero-Shot Human-Object Interaction Generation with Part Affordance Guidance](https://arxiv.org/abs/2506.07209)
*Lei Li,Angela Dai*

Main category: cs.GR

TL;DR: HOI-PAGE是一种从文本提示中零样本合成4D人-物交互（HOI）的新方法，通过部分级可动性推理实现。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注全局的全身-物体运动，而生成真实多样的HOI需要更细粒度的理解，即人体部分如何与物体部分交互。

Method: 引入部分可动性图（PAGs），从大语言模型中提取细粒度部分信息，并分三阶段合成：分解3D物体、生成参考视频并提取运动约束、优化4D HOI序列。

Result: 实验表明，该方法能灵活生成复杂多对象或多人的交互序列，显著提升了零样本4D HOI生成的真实性和文本对齐性。

Conclusion: HOI-PAGE通过部分级推理实现了更真实和多样化的4D HOI合成。

Abstract: We present HOI-PAGE, a new approach to synthesizing 4D human-object
interactions (HOIs) from text prompts in a zero-shot fashion, driven by
part-level affordance reasoning. In contrast to prior works that focus on
global, whole body-object motion for 4D HOI synthesis, we observe that
generating realistic and diverse HOIs requires a finer-grained understanding --
at the level of how human body parts engage with object parts. We thus
introduce Part Affordance Graphs (PAGs), a structured HOI representation
distilled from large language models (LLMs) that encodes fine-grained part
information along with contact relations. We then use these PAGs to guide a
three-stage synthesis: first, decomposing input 3D objects into geometric
parts; then, generating reference HOI videos from text prompts, from which we
extract part-based motion constraints; finally, optimizing for 4D HOI motion
sequences that not only mimic the reference dynamics but also satisfy
part-level contact constraints. Extensive experiments show that our approach is
flexible and capable of generating complex multi-object or multi-person
interaction sequences, with significantly improved realism and text alignment
for zero-shot 4D HOI generation.

</details>


### [208] [Immersive Visualization of Flat Surfaces Using Ray Marching](https://arxiv.org/abs/2506.07558)
*Fabian Lander,Diaaeldin Taha*

Main category: cs.GR

TL;DR: 提出了一种基于光线步进的平面可视化方法，适用于探索平移曲面、镜面房间、展开多面体和平移棱柱，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 为复杂几何结构提供直观的可视化方法，便于理解和探索。

Method: 使用光线步进技术，结合计算优化，实现高效的平面可视化。

Result: 通过多种示例展示了方法的实用性，并提供了代码实现。

Conclusion: 该方法不仅适用于学术研究，还可用于教育和科普推广。

Abstract: We present an effective method for visualizing flat surfaces using ray
marching. Our approach provides an intuitive way to explore translation
surfaces, mirror rooms, unfolded polyhedra, and translation prisms while
maintaining computational efficiency. We demonstrate the utility of the method
through various examples and provide implementation insights for programmers.
Finally, we discuss the use of our visualizations in outreach. We make our
simulations and code available online.

</details>


### [209] [PIG: Physically-based Multi-Material Interaction with 3D Gaussians](https://arxiv.org/abs/2506.07657)
*Zeyu Xiao,Zhenyi Wu,Mingyang Sun,Qipeng Yan,Yufan Guo,Zhuoer Liang,Lihua Zhang*

Main category: cs.GR

TL;DR: PIG方法结合3D高斯分割与物理模拟，解决了3D高斯场景中物体交互的精度问题，提升了视觉质量和几何一致性。


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯场景中物体交互的3D分割不准确、变形不精确和渲染伪影问题。

Method: 1. 快速准确地将2D像素映射到3D高斯；2. 为分割对象分配物理属性以支持多材料交互；3. 嵌入约束尺度到变形梯度中，消除伪影。

Result: 实验表明，PIG在视觉质量上超越SOTA，并为物理真实场景生成开辟新方向。

Conclusion: PIG方法显著提升了3D高斯场景的交互精度和视觉一致性，具有广泛的应用潜力。

Abstract: 3D Gaussian Splatting has achieved remarkable success in reconstructing both
static and dynamic 3D scenes. However, in a scene represented by 3D Gaussian
primitives, interactions between objects suffer from inaccurate 3D
segmentation, imprecise deformation among different materials, and severe
rendering artifacts. To address these challenges, we introduce PIG:
Physically-Based Multi-Material Interaction with 3D Gaussians, a novel approach
that combines 3D object segmentation with the simulation of interacting objects
in high precision. Firstly, our method facilitates fast and accurate mapping
from 2D pixels to 3D Gaussians, enabling precise 3D object-level segmentation.
Secondly, we assign unique physical properties to correspondingly segmented
objects within the scene for multi-material coupled interactions. Finally, we
have successfully embedded constraint scales into deformation gradients,
specifically clamping the scaling and rotation properties of the Gaussian
primitives to eliminate artifacts and achieve geometric fidelity and visual
consistency. Experimental results demonstrate that our method not only
outperforms the state-of-the-art (SOTA) in terms of visual quality, but also
opens up new directions and pipelines for the field of physically realistic
scene generation.

</details>


### [210] [GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution](https://arxiv.org/abs/2506.07897)
*Shuja Khalid,Mohamed Ibrahim,Yang Liu*

Main category: cs.GR

TL;DR: 提出一种轻量级生成模型，通过Hessian辅助采样策略提升3D高斯泼溅的分辨率和几何保真度，突破输入分辨率的限制。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法受限于输入分辨率，无法重建比训练视图更精细的细节。

Method: 采用轻量级生成模型预测并细化额外的3D高斯分布，结合Hessian辅助采样策略智能识别需要密集化的区域。

Result: 在几何精度和渲染质量上显著优于现有方法，且实时性高（单GPU每次推理0.015秒）。

Conclusion: 提出了一种分辨率无关的3D场景增强新范式，适用于交互式应用。

Abstract: We present a novel approach for enhancing the resolution and geometric
fidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution.
Current 3DGS methods are fundamentally limited by their input resolution,
producing reconstructions that cannot extrapolate finer details than are
present in the training views. Our work breaks this limitation through a
lightweight generative model that predicts and refines additional 3D Gaussians
where needed most. The key innovation is our Hessian-assisted sampling
strategy, which intelligently identifies regions that are likely to benefit
from densification, ensuring computational efficiency. Unlike computationally
intensive GANs or diffusion approaches, our method operates in real-time
(0.015s per inference on a single consumer-grade GPU), making it practical for
interactive applications. Comprehensive experiments demonstrate significant
improvements in both geometric accuracy and rendering quality compared to
state-of-the-art methods, establishing a new paradigm for resolution-free 3D
scene enhancement.

</details>


### [211] [Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes](https://arxiv.org/abs/2506.07917)
*Allen Tu,Haiyang Ying,Alex Hanson,Yonghan Lee,Tom Goldstein,Matthias Zwicker*

Main category: cs.GR

TL;DR: SpeeDe3DGS通过时间敏感剪枝和GroupFlow技术，显著加速动态3DGS渲染，减少计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 动态3DGS中逐高斯神经推理效率低，限制了渲染速度和计算资源。

Method: 提出时间敏感剪枝和GroupFlow技术，分别优化高斯选择和运动预测。

Result: 渲染速度提升10.37倍，模型大小减少7.71倍，训练时间缩短2.71倍。

Conclusion: SpeeDe3DGS是模块化方法，可集成到任何动态3DGS框架中，显著提升效率。

Abstract: Recent extensions of 3D Gaussian Splatting (3DGS) to dynamic scenes achieve
high-quality novel view synthesis by using neural networks to predict the
time-varying deformation of each Gaussian. However, performing per-Gaussian
neural inference at every frame poses a significant bottleneck, limiting
rendering speed and increasing memory and compute requirements. In this paper,
we present Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), a general
pipeline for accelerating the rendering speed of dynamic 3DGS and 4DGS
representations by reducing neural inference through two complementary
techniques. First, we propose a temporal sensitivity pruning score that
identifies and removes Gaussians with low contribution to the dynamic scene
reconstruction. We also introduce an annealing smooth pruning mechanism that
improves pruning robustness in real-world scenes with imprecise camera poses.
Second, we propose GroupFlow, a motion analysis technique that clusters
Gaussians by trajectory similarity and predicts a single rigid transformation
per group instead of separate deformations for each Gaussian. Together, our
techniques accelerate rendering by $10.37\times$, reduce model size by
$7.71\times$, and shorten training time by $2.71\times$ on the NeRF-DS dataset.
SpeeDe3DGS also improves rendering speed by $4.20\times$ and $58.23\times$ on
the D-NeRF and HyperNeRF vrig datasets. Our methods are modular and can be
integrated into any deformable 3DGS or 4DGS framework.

</details>


### [212] [Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor](https://arxiv.org/abs/2506.07932)
*Rishit Dagli,Yushi Guan,Sankeerth Durvasula,Mohammadreza Mofayezi,Nandita Vijaykumar*

Main category: cs.GR

TL;DR: Squeeze3D是一种利用预训练3D生成模型隐式先验知识的高压缩比3D数据压缩框架。


<details>
  <summary>Details</summary>
Motivation: 现有3D数据压缩方法需要大量真实数据训练，且压缩比有限。Squeeze3D旨在通过预训练模型实现高压缩比，同时减少对真实数据的依赖。

Method: 通过可训练的映射网络连接预训练编码器和生成模型的潜在空间，将3D数据（如网格、点云或辐射场）编码为紧凑潜在代码，再通过生成模型解压缩。

Result: Squeeze3D实现了纹理网格2187倍、点云55倍、辐射场619倍的高压缩比，同时保持视觉质量，且压缩/解压延迟低。

Conclusion: Squeeze3D为3D数据压缩提供了高效、灵活的解决方案，适用于多种格式且无需真实数据训练。

Abstract: We propose Squeeze3D, a novel framework that leverages implicit prior
knowledge learnt by existing pre-trained 3D generative models to compress 3D
data at extremely high compression ratios. Our approach bridges the latent
spaces between a pre-trained encoder and a pre-trained generation model through
trainable mapping networks. Any 3D model represented as a mesh, point cloud, or
a radiance field is first encoded by the pre-trained encoder and then
transformed (i.e. compressed) into a highly compact latent code. This latent
code can effectively be used as an extremely compressed representation of the
mesh or point cloud. A mapping network transforms the compressed latent code
into the latent space of a powerful generative model, which is then conditioned
to recreate the original 3D model (i.e. decompression). Squeeze3D is trained
entirely on generated synthetic data and does not require any 3D datasets. The
Squeeze3D architecture can be flexibly used with existing pre-trained 3D
encoders and existing generative models. It can flexibly support different
formats, including meshes, point clouds, and radiance fields. Our experiments
demonstrate that Squeeze3D achieves compression ratios of up to 2187x for
textured meshes, 55x for point clouds, and 619x for radiance fields while
maintaining visual quality comparable to many existing methods. Squeeze3D only
incurs a small compression and decompression latency since it does not involve
training object-specific networks to compress an object.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [213] [Algorithmic Analysis of GTFS-RT vehicle position accuracy](https://arxiv.org/abs/2506.06479)
*Joshua Wong*

Main category: physics.geo-ph

TL;DR: 论文提出了三种新的算法用于计算椭球面上的测地线交点，并通过加州实时交通数据的案例研究评估车辆位置漂移。


<details>
  <summary>Details</summary>
Motivation: 解决椭球面上测地线交点计算的问题，并应用于实时交通数据中的位置漂移分析。

Method: 提出了三种新算法，并在加州实时交通数据中进行案例研究。

Result: 研究发现部分数据异常可被修正，但大规模差异仍然存在。

Conclusion: 论文提出了一套实用解决方案，可由数据生产者或消费者实施，以显著提高位置准确性。

Abstract: This paper presents three novel algorithms for calculating geodesic
intersections on an ellipsoid. These algorithms are applied in a case study
analyzing real-time transit data in California to assess vehicle position
drift. The analysis reveals that while certain data anomalies can be corrected,
large-scale discrepancies persist. The paper concludes by proposing a set of
practical solutions that can be implemented by either data producers or
consumers to significantly improve positional accuracy.

</details>


### [214] [On the three laws of earthquake physics](https://arxiv.org/abs/2506.06504)
*A. V. Guglielmi,A. D. Zavyalov,O. D. Zotov,B. I. Klain*

Main category: physics.geo-ph

TL;DR: 论文概述了地震物理学研究的新方法，基于Omori、Gutenberg-Richter和Bath定律，提出了构造地震三合体的概念，并分类了六种三合体类型。


<details>
  <summary>Details</summary>
Motivation: 探索地震物理学问题的新方法，验证经典定律的深度和适用性。

Method: 利用构造地震三合体（前震、主震和余震）分类，分析三定律在不同类型三合体中的参数。

Result: 经典定律在地震研究中是可靠工具，并能扩展出新的重要结论。

Conclusion: Omori、Gutenberg-Richter和Bath定律在地震实验和理论研究中具有重要价值。

Abstract: The paper provides a synoptic overview of a series of works carried out by a
group of researchers at the Institute of Physics of the Earth RAS with the aim
of finding new approaches to the problems of earthquake physics. The
fundamental laws of Omori, Gutenberg-Richter and Bath have served as a constant
support and reference point in the course of many years of research. The
concept of the tectonic earthquake triad as a natural trinity of foreshocks,
main shocks and aftershocks is used in the article to organise the thematic
material. A classification of main shocks into six types of triads found in
experience is given. The parameters appearing in the three laws for different
types of triads are given. The axiomatic theory of the evolution of aftershocks
is outlined. The concepts of source deactivation, Omori epoch and source
bifurcation are introduced, and the notion of the proper time of unsteady
lithospheric processes is introduced. Convergence of foreshocks and divergence
of aftershocks are mentioned. The general conclusion is that the Omori,
Gutenberg-Richter and Bath laws are reliable tools in the experimental and
theoretical study of earthquakes. The laws have a depth of content that has
been demonstrated by the ability to enrich the original formulations of the
discoverers with interesting and important additional statements.

</details>


### [215] [Towards End-to-End Earthquake Monitoring Using a Multitask Deep Learning Model](https://arxiv.org/abs/2506.06939)
*Weiqiang Zhu,Junhao Song,Haoyu Wang,Jannes Münchmeyer*

Main category: physics.geo-ph

TL;DR: 论文提出了一种多任务框架PhaseNet+，扩展了现有的PhaseNet模型，用于同时完成地震波相位到达时间拾取、初动极性判定和相位关联任务，以提高地震监测效率。


<details>
  <summary>Details</summary>
Motivation: 当前地震监测任务中，深度学习模型多为单一任务设计，缺乏端到端的多任务框架，限制了地震监测的效率和统一性。

Method: 扩展PhaseNet模型，构建多任务框架PhaseNet+，同时处理相位拾取、初动极性判定和相位关联。

Result: PhaseNet+能够高效完成多项地震监测任务，其输出可进一步用于地震定位和震源机制分析。

Conclusion: 多任务框架PhaseNet+为地震监测提供了一种更统一和高效的解决方案，并可推广至其他先进模型。

Abstract: Seismic waveforms contain rich information about earthquake processes, making
effective data analysis crucial for earthquake monitoring, source
characterization, and seismic hazard assessment. With rapid developments in
deep learning, the state-of-the-art approach in artificial intelligence, many
neural network models have been developed to enhance earthquake monitoring
tasks, such as earthquake detection, phase picking, and phase association.
However, most current efforts focus on developing separate models for each
specific task, leaving the potential of an end-to-end framework relatively
unexplored. To address this gap, we extend an existing phase picking model,
PhaseNet, to create a multitask framework. This extended model, PhaseNet+,
simultaneously performs phase arrival-time picking, first-motion polarity
determination, and phase association. The outputs from these perception-based
models can then be processed by specialized physics-based algorithms to
accurately determine earthquake location and focal mechanism. The multitask
approach is not limited to the PhaseNet model and can be applied to other
state-of-the-art phase picking models, ultimately improving seismic monitoring
through a more unified and efficient approach.

</details>


### [216] [Electrical Conductivity of Superionic Hydrous SiO2 and the Origin of Lower-mantle High Conductivity Anomalies Beneath Subduction Zones](https://arxiv.org/abs/2506.07352)
*Mako Inada,Yoshiyuki Okuda,Kenta Oka,Hideharu Kuwahara,Steeve Gréaux,Kei Hirose*

Main category: physics.geo-ph

TL;DR: 论文研究了含水Al-SiO2相在高压高温下的超离子态转变及其对电导率的影响，实验验证了电导率显著增加的现象，并探讨了其对地幔深部水循环的启示。


<details>
  <summary>Details</summary>
Motivation: 验证含水Al-SiO2相在高压高温条件下转变为超离子态时电导率增强的假设，并解释地幔深部高电导率异常现象。

Method: 采用新开发的透明材料测量技术，在高达82 GPa和2610 K的条件下测量含1750 ppm H2O的Al-SiO2的电导率。

Result: 实验显示电导率在1100-2200 K时突然增加至约10 S/m，比周围地幔高出数倍至十倍，证实了超离子态转变。

Conclusion: 超离子态SiO2可能显著增强俯冲MORB地壳的电导率，解释了东北中国下方俯冲带的高电导率异常，为地幔深部水循环提供了新见解。

Abstract: Electrical conductivity (EC) is one of the important physical properties of
minerals and rocks that can be used to characterize the composition and
structure of the deep interior of the Earth.Theoretical studies have predicted
that the CaCl2-type hydrous Al-bearing SiO2 phase, present in subducted crustal
materials, becomes superionic-meaning that protons are no longer bonded to a
specific oxygen atom but instead become mobile within the SiO2 lattice-under
high-pressure and high-temperature conditions corresponding to the lower
mantle. The enhancement of the EC upon such superionic transition has not been
experimentally verified yet. Here, we measured the EC of Al-bearing SiO2
containing 1750 ppm H2O at pressures up to 82 GPa and temperatures up to 2610 K
by employing a recently developed technique designed for measuring transparent
materials. Results demonstrate a sudden increase in EC to approximately 10 S/m
at temperatures of 1100-2200 K, depending on pressure, which is several to ten
times higher than that of the surrounding shallow to middle part of the lower
mantle, which is attributed to a transition to the superionic state. If hydrous
SiO2 is substantially weaker than other coexisting phases and thus forms an
interconnected film in subducted MORB crust, the EC of the bulk MORB materials
is significantly enhanced by superionic SiO2 in the lower mantle up to ~1800 km
depth, which may explain the high EC anomalies observed at subduction zones
underneath northeastern China. The observed EC anomalies can be matched by the
EC of subducted MORB materials containing Al-bearing SiO2 with a water content
of approximately 0.2 wt%, providing insights into the deep H2O circulation and
distribution in the Earth's mantle.

</details>


### [217] [Efficient Seismic Data Interpolation via Sparse Attention Transformer and Diffusion Model](https://arxiv.org/abs/2506.07923)
*Xiaoli Wei,Chunxia Zhang,Baisong Jiang,Anxiang Di,Deng Xiong,Jiangshe Zhang,Mingming Gong*

Main category: physics.geo-ph

TL;DR: 提出Diff-spaformer，一种结合Transformer和扩散模型的新框架，用于高效地震数据插值。


<details>
  <summary>Details</summary>
Motivation: 解决现有插值方法因迭代重采样导致的计算效率低下问题。

Method: 通过Seismic Prior Extraction Network（SPEN）结合Transformer和扩散模型，使用稀疏多头注意力和前馈传播捕获全局信息，扩散模型提供先验指导。

Result: 实验表明，该方法在随机和连续缺失数据下均提高了插值保真度和计算效率。

Conclusion: Diff-spaformer为复杂地质条件下的高效地震数据重建提供了新范式。

Abstract: Seismic data interpolation is a critical pre-processing step for improving
seismic imaging quality and remains a focus of academic innovation. To address
the computational inefficiencies caused by extensive iterative resampling in
current plug-and-play diffusion interpolation methods, we propose the
diffusion-enhanced sparse attention transformer (Diff-spaformer), a novel deep
learning framework. Our model integrates transformer architectures and
diffusion models via a Seismic Prior Extraction Network (SPEN), which serves as
a bridge module. Full-layer sparse multi-head attention and feed-forward
propagation capture global information distributions, while the diffusion model
provides robust prior guidance. To mitigate the computational burden of
high-dimensional representations, self-attention is computed along the channel
rather than the spatial dimension. We show that using negative squared
Euclidean distance to compute sparse affinity matrices better suits seismic
data modeling, enabling broader contribution from amplitude feature nodes. An
adaptive ReLU function further discards low or irrelevant self-attention
values. We conduct training within a single-stage optimization framework,
requiring only a few reverse diffusion sampling steps during inference.
Extensive experiments demonstrate improved interpolation fidelity and
computational efficiency for both random and continuous missing data, offering
a new paradigm for high-efficiency seismic data reconstruction under complex
geological conditions.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [218] [MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models](https://arxiv.org/abs/2506.07400)
*Philip Liu,Sparsh Bansal,Jimmy Dinh,Aditya Pawar,Ramani Satishkumar,Shail Desai,Neeraj Gupta,Xin Wang,Shu Hu*

Main category: cs.MA

TL;DR: MedChat是一个多代理诊断框架，结合专用视觉模型和角色特定的LLM代理，以提升青光眼检测的可靠性和临床报告效率。


<details>
  <summary>Details</summary>
Motivation: 解决通用LLM在医学影像中存在的幻觉、解释性不足和领域知识缺乏问题，同时模拟多学科医疗团队的复杂推理。

Method: 结合专用视觉模型与多个角色特定的LLM代理，通过一个导演代理协调，设计交互式诊断报告界面。

Result: 提高了可靠性，减少了幻觉风险，并支持临床审查和教育用途的交互式报告。

Conclusion: MedChat通过多代理框架有效解决了通用LLM在医学应用中的局限性，提升了青光眼检测的临床实用性。

Abstract: The integration of deep learning-based glaucoma detection with large language
models (LLMs) presents an automated strategy to mitigate ophthalmologist
shortages and improve clinical reporting efficiency. However, applying general
LLMs to medical imaging remains challenging due to hallucinations, limited
interpretability, and insufficient domain-specific medical knowledge, which can
potentially reduce clinical accuracy. Although recent approaches combining
imaging models with LLM reasoning have improved reporting, they typically rely
on a single generalist agent, restricting their capacity to emulate the diverse
and complex reasoning found in multidisciplinary medical teams. To address
these limitations, we propose MedChat, a multi-agent diagnostic framework and
platform that combines specialized vision models with multiple role-specific
LLM agents, all coordinated by a director agent. This design enhances
reliability, reduces hallucination risk, and enables interactive diagnostic
reporting through an interface tailored for clinical review and educational
use. Code available at https://github.com/Purdue-M2/MedChat.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [219] [Experimental Evaluation of Static Image Sub-Region-Based Search Models Using CLIP](https://arxiv.org/abs/2506.06938)
*Bastian Jäckl,Vojtěch Kloda,Daniel A. Keim,Jakub Lokoč*

Main category: cs.MM

TL;DR: 研究探讨了在高度同质的专业领域中，通过添加基于位置的提示来增强模糊文本查询的检索性能。


<details>
  <summary>Details</summary>
Motivation: 在专业领域中，用户通常只能提供模糊的文本描述，缺乏区分同质实体的专业知识，因此需要辅助方法提升检索效果。

Method: 收集了741个人工标注的数据集，包含短/长文本描述和边界框，评估CLIP模型在不同图像子区域上的检索性能。

Result: 结果显示，简单的3x3分区和5网格重叠方法显著提升了检索效果，并对标注框的扰动具有鲁棒性。

Conclusion: 基于位置的提示可以有效增强模糊文本查询在专业领域的检索性能。

Abstract: Advances in multimodal text-image models have enabled effective text-based
querying in extensive image collections. While these models show convincing
performance for everyday life scenes, querying in highly homogeneous,
specialized domains remains challenging. The primary problem is that users can
often provide only vague textual descriptions as they lack expert knowledge to
discriminate between homogenous entities. This work investigates whether adding
location-based prompts to complement these vague text queries can enhance
retrieval performance. Specifically, we collected a dataset of 741 human
annotations, each containing short and long textual descriptions and bounding
boxes indicating regions of interest in challenging underwater scenes. Using
these annotations, we evaluate the performance of CLIP when queried on various
static sub-regions of images compared to the full image. Our results show that
both a simple 3-by-3 partitioning and a 5-grid overlap significantly improve
retrieval effectiveness and remain robust to perturbations of the annotation
box.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [220] [A Culturally-diverse Multilingual Multimodal Video Benchmark & Model](https://arxiv.org/abs/2506.07032)
*Bhuiyan Sanjid Shafique,Ashmal Vayani,Muhammad Maaz,Hanoona Abdul Rasheed,Dinura Dissanayake,Mohammed Irfan Kurpath,Yahya Hmaiti,Go Inoue,Jean Lahoud,Md. Safirur Rashid,Shadid Intisar Quasem,Maheen Fatima,Franco Vidal,Mykola Maslych,Ketan Pravin More,Sanoojan Baliah,Hasindri Watawana,Yuhao Li,Fabian Farestam,Leon Schaller,Roman Tymtsiv,Simon Weber,Hisham Cholakkal,Ivan Laptev,Shin'ichi Satoh,Michael Felsberg,Mubarak Shah,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CL

TL;DR: 论文提出了一种多语言视频LMM基准ViMUL-Bench，涵盖14种语言，旨在促进文化和语言包容性研究。


<details>
  <summary>Details</summary>
Motivation: 现有LMM多为英语，缺乏对多语言和文化包容性的探索，尤其是在视频LMM领域。

Method: 开发ViMUL-Bench基准，包含8k手动验证样本，涵盖15个类别；提出多语言视频训练集（1.2M样本）和简单多语言视频LMM ViMUL。

Result: ViMUL在多语言视频理解中表现出更好的高低资源语言平衡。

Conclusion: ViMUL-Bench和ViMUL有望推动多语言视频LMM的发展，促进文化和语言包容性。

Abstract: Large multimodal models (LMMs) have recently gained attention due to their
effectiveness to understand and generate descriptions of visual content. Most
existing LMMs are in English language. While few recent works explore
multilingual image LMMs, to the best of our knowledge, moving beyond the
English language for cultural and linguistic inclusivity is yet to be
investigated in the context of video LMMs. In pursuit of more inclusive video
LMMs, we introduce a multilingual Video LMM benchmark, named ViMUL-Bench, to
evaluate Video LMMs across 14 languages, including both low- and high-resource
languages: English, Chinese, Spanish, French, German, Hindi, Arabic, Russian,
Bengali, Urdu, Sinhala, Tamil, Swedish, and Japanese. Our ViMUL-Bench is
designed to rigorously test video LMMs across 15 categories including eight
culturally diverse categories, ranging from lifestyles and festivals to foods
and rituals and from local landmarks to prominent cultural personalities.
ViMUL-Bench comprises both open-ended (short and long-form) and multiple-choice
questions spanning various video durations (short, medium, and long) with 8k
samples that are manually verified by native language speakers. In addition, we
also introduce a machine translated multilingual video training set comprising
1.2 million samples and develop a simple multilingual video LMM, named ViMUL,
that is shown to provide a better tradeoff between high-and low-resource
languages for video understanding. We hope our ViMUL-Bench and multilingual
video LMM along with a large-scale multilingual video training set will help
ease future research in developing cultural and linguistic inclusive
multilingual video LMMs. Our proposed benchmark, video LMM and training data
will be publicly released at https://mbzuai-oryx.github.io/ViMUL/.

</details>


### [221] [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/abs/2506.07044)
*LASA Team,Weiwen Xu,Hou Pong Chan,Long Li,Mahani Aljunied,Ruifeng Yuan,Jianyu Wang,Chenghao Xiao,Guizhen Chen,Chaoqun Liu,Zhaodonghui Li,Yu Sun,Junao Shen,Chaojun Wang,Jie Tan,Deli Zhao,Tingyang Xu,Hao Zhang,Yu Rong*

Main category: cs.CL

TL;DR: 论文提出了一种针对医学领域的多模态大语言模型Lingshu，通过改进数据收集和训练策略，解决了现有医学MLLMs的局限性，并在多项任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有医学MLLMs在知识覆盖、幻觉问题和推理能力方面存在不足，限制了其在医学应用中的效果。

Method: 提出综合数据收集方法，构建多模态数据集，并通过多阶段训练和强化学习增强模型能力。

Result: Lingshu在多项医学任务中表现优于现有开源多模态模型。

Conclusion: Lingshu通过改进数据和训练策略，显著提升了医学MLLMs的性能和应用潜力。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities in understanding common visual elements, largely due to their
large-scale datasets and advanced training strategies. However, their
effectiveness in medical applications remains limited due to the inherent
discrepancies between data and tasks in medical scenarios and those in the
general domain. Concretely, existing medical MLLMs face the following critical
limitations: (1) limited coverage of medical knowledge beyond imaging, (2)
heightened susceptibility to hallucinations due to suboptimal data curation
processes, (3) lack of reasoning capabilities tailored for complex medical
scenarios. To address these challenges, we first propose a comprehensive data
curation procedure that (1) efficiently acquires rich medical knowledge data
not only from medical imaging but also from extensive medical texts and
general-domain data; and (2) synthesizes accurate medical captions, visual
question answering (VQA), and reasoning samples. As a result, we build a
multimodal dataset enriched with extensive medical knowledge. Building on the
curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu
undergoes multi-stage training to embed medical expertise and enhance its
task-solving capabilities progressively. Besides, we preliminarily explore the
potential of applying reinforcement learning with verifiable rewards paradigm
to enhance Lingshu's medical reasoning ability. Additionally, we develop
MedEvalKit, a unified evaluation framework that consolidates leading multimodal
and textual medical benchmarks for standardized, fair, and efficient model
assessment. We evaluate the performance of Lingshu on three fundamental medical
tasks, multimodal QA, text-based QA, and medical report generation. The results
show that Lingshu consistently outperforms the existing open-source multimodal
models on most tasks ...

</details>


### [222] [Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs](https://arxiv.org/abs/2506.07180)
*Wenrui Zhou,Shu Yang,Qingsong Yang,Zikun Guo,Lijie Hu,Di Wang*

Main category: cs.CL

TL;DR: 论文提出了VISE基准，用于评估视频大语言模型（Video-LLMs）在误导性用户输入下的迎合行为，填补了该领域缺乏系统性评估工具的空白。


<details>
  <summary>Details</summary>
Motivation: 随着Video-LLMs在需要多模态推理的实际应用中的普及，确保其事实一致性和可靠性至关重要。然而，模型的迎合行为（即使与视觉证据矛盾也倾向于与用户输入一致）会削弱其可信度。目前的研究忽视了视频语言领域中的迎合行为，缺乏系统性评估工具。

Method: 提出VISE基准，首次针对Video-LLMs设计，涵盖多种问题格式、提示偏见和视觉推理任务，并将语言领域的迎合行为分析引入视觉领域。此外，探索了关键帧选择作为一种无需训练的缓解策略。

Result: VISE基准能够对Video-LLMs的迎合行为进行细粒度分析，并揭示了通过增强视觉基础减少迎合偏见的潜在路径。

Conclusion: VISE填补了视频语言领域迎合行为研究的空白，为提升Video-LLMs的可靠性提供了新的评估工具和缓解策略。

Abstract: As video large language models (Video-LLMs) become increasingly integrated
into real-world applications that demand grounded multimodal reasoning,
ensuring their factual consistency and reliability is of critical importance.
However, sycophancy, the tendency of these models to align with user input even
when it contradicts the visual evidence, undermines their trustworthiness in
such contexts. Current sycophancy research has largely overlooked its specific
manifestations in the video-language domain, resulting in a notable absence of
systematic benchmarks and targeted evaluations to understand how Video-LLMs
respond under misleading user input. To fill this gap, we propose VISE
(Video-LLM Sycophancy Benchmarking and Evaluation), the first dedicated
benchmark designed to evaluate sycophantic behavior in state-of-the-art
Video-LLMs across diverse question formats, prompt biases, and visual reasoning
tasks. Specifically, VISE pioneeringly brings linguistic perspectives on
sycophancy into the visual domain, enabling fine-grained analysis across
multiple sycophancy types and interaction patterns. In addition, we explore
key-frame selection as an interpretable, training-free mitigation strategy,
which reveals potential paths for reducing sycophantic bias by strengthening
visual grounding.

</details>


### [223] [Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline](https://arxiv.org/abs/2506.07631)
*Brian Gordon,Yonatan Bitton,Andreea Marzoca,Yasumasa Onoe,Xiao Wang,Daniel Cohen-Or,Idan Szpektor*

Main category: cs.CL

TL;DR: DOCCI-Critique是一个用于评估大视觉语言模型（VLM）生成段落长度图像描述的事实准确性的基准，包含1,400个标注的段落描述和10,216个句子级人工注释。VNLI-Critique是基于此开发的自动化事实分类和错误解释模型，展示了强大的泛化能力和实际应用价值。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法难以捕捉VLM生成的段落长度描述的细粒度错误，缺乏验证数据集。

Method: 引入DOCCI-Critique基准和VNLI-Critique模型，用于自动化事实分类和错误解释，并开发了Critic-and-Revise流程。

Result: VNLI-Critique在多个基准测试中表现优异，AutoRater与人类判断高度一致，Critic-and-Revise流程显著提升描述的事实准确性。

Conclusion: 该研究提供了重要的基准和实用工具，显著提升了VLM图像理解的细粒度评估标准。

Abstract: Large Vision-Language Models (VLMs) now generate highly detailed,
paragraphlength image captions, yet evaluating their factual accuracy remains
challenging. Current methods often miss fine-grained errors, being designed for
shorter texts or lacking datasets with verified inaccuracies. We introduce
DOCCI-Critique, a benchmark with 1,400 VLM-generated paragraph captions (100
images, 14 VLMs) featuring over 10,216 sentence-level human annotations of
factual correctness and explanatory rationales for errors, all within paragraph
context. Building on this, we develop VNLI-Critique, a model for automated
sentence-level factuality classification and critique generation. We highlight
three key applications: (1) VNLI-Critique demonstrates robust generalization,
validated by state-of-the-art performance on the M-HalDetect benchmark and
strong results in CHOCOLATE claim verification. (2) The VNLI-Critique driven
AutoRater for DOCCI-Critique provides reliable VLM rankings, showing excellent
alignment with human factuality judgments (e.g., 0.98 Spearman). (3) An
innovative Critic-and-Revise pipeline, where critiques from VNLI-Critique guide
LLM-based corrections, achieves substantial improvements in caption factuality
(e.g., a 46% gain on DetailCaps-4870). Our work offers a crucial benchmark
alongside practical tools, designed to significantly elevate the standards for
fine-grained evaluation and foster the improvement of VLM image understanding.
Project page: https://google.github.io/unblocking-detail-caption

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [224] [HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language Models for Efficient Multimodal Hotel Retrieval](https://arxiv.org/abs/2506.07296)
*Arian Askari,Emmanouil Stergiadis,Ilya Gusev,Moran Beladev*

Main category: cs.IR

TL;DR: HotelMatch-LLM是一种多模态密集检索模型，用于旅游领域的自然语言属性搜索，解决了传统旅游搜索引擎的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统旅游搜索引擎需要用户从目的地开始并编辑搜索参数，限制了搜索的灵活性和效率。

Method: 结合了领域特定的多任务优化、不对称密集检索架构和广泛的图像处理技术。

Result: 在四个测试集上显著优于现有模型（如VISTA和MARVEL），主查询类型的得分为0.681，而最佳基线MARVEL为0.603。

Conclusion: HotelMatch-LLM在多任务优化、泛化性和可扩展性方面表现出色，适用于大规模图像库处理。

Abstract: We present HotelMatch-LLM, a multimodal dense retrieval model for the travel
domain that enables natural language property search, addressing the
limitations of traditional travel search engines which require users to start
with a destination and editing search parameters. HotelMatch-LLM features three
key innovations: (1) Domain-specific multi-task optimization with three novel
retrieval, visual, and language modeling objectives; (2) Asymmetrical dense
retrieval architecture combining a small language model (SLM) for efficient
online query processing and a large language model (LLM) for embedding hotel
data; and (3) Extensive image processing to handle all property image
galleries. Experiments on four diverse test sets show HotelMatch-LLM
significantly outperforms state-of-the-art models, including VISTA and MARVEL.
Specifically, on the test set -- main query type -- we achieve 0.681 for
HotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our
analysis highlights the impact of our multi-task optimization, the
generalizability of HotelMatch-LLM across LLM architectures, and its
scalability for processing large image galleries.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [225] [Pendulum Tracker -- SimuFísica: A Web-based Tool for Real-time Measurement of Oscillatory Motion](https://arxiv.org/abs/2506.07301)
*Marco P. M. de Souza,Juciane G. Maia,Lilian N. de Andrade*

Main category: physics.ed-ph

TL;DR: Pendulum Tracker是一个基于计算机视觉的应用程序，用于实时测量物理摆的振荡运动，适用于教育平台SimuFísica。


<details>
  <summary>Details</summary>
Motivation: 为物理教学提供一个实时测量和分析摆运动的工具，提升实验教学的便捷性和准确性。

Method: 使用OpenCV.js库，通过设备摄像头自动检测摆的位置，实时显示角度-时间图和振荡周期估计。

Result: 实验结果显示与理论预测高度一致，验证了系统的准确性和教育适用性。

Conclusion: Pendulum Tracker是一个多功能工具，适用于实验物理教学，具有用户友好界面和数据导出功能。

Abstract: We present Pendulum Tracker, a computer vision-based application that enables
real-time measurement of the oscillatory motion of a physical pendulum.
Integrated into the educational platform SimuF\'isica, the system uses the
OpenCV.js library and runs directly in the browser, working on computers,
tablets, and smartphones. The application automatically detects the pendulum's
position via the device's camera, displaying in real time the angle-versus-time
graph and estimates of the oscillation period. Experimental case studies
demonstrate its effectiveness in measuring the period, determining
gravitational acceleration, and analyzing damped oscillations. The results show
excellent agreement with theoretical predictions, confirming the system's
accuracy and its applicability in educational contexts. The accessible
interface and the ability to export raw data make Pendulum Tracker a versatile
tool for experimental physics teaching.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [226] [SMaRCSim: Maritime Robotics Simulation Modules](https://arxiv.org/abs/2506.07781)
*Mart Kartašev,David Dörner,Özer Özkahraman,Petter Ögren,Ivan Stenius,John Folkesson*

Main category: cs.RO

TL;DR: SMaRCSim是一套仿真工具包，旨在解决水下机器人开发中的快速测试、多智能体协同及任务规划集成问题。


<details>
  <summary>Details</summary>
Motivation: 现有仿真工具无法满足水下机器人学习算法开发、多智能体协同及任务规划集成的需求。

Method: 开发了SMaRCSim仿真工具包，支持水下、水面和空中多智能体的协同仿真与任务规划。

Result: SMaRCSim为水下机器人领域提供了快速测试和功能开发的解决方案。

Conclusion: SMaRCSim填补了现有仿真工具的不足，为水下机器人研究提供了高效工具。

Abstract: Developing new functionality for underwater robots and testing them in the
real world is time-consuming and resource-intensive. Simulation environments
allow for rapid testing before field deployment. However, existing tools lack
certain functionality for use cases in our project: i) developing
learning-based methods for underwater vehicles; ii) creating teams of
autonomous underwater, surface, and aerial vehicles; iii) integrating the
simulation with mission planning for field experiments. A holistic solution to
these problems presents great potential for bringing novel functionality into
the underwater domain. In this paper we present SMaRCSim, a set of simulation
packages that we have developed to help us address these issues.

</details>


### [227] [UruBots Autonomous Cars Challenge Pro Team Description Paper for FIRA 2025](https://arxiv.org/abs/2506.07348)
*Pablo Moraes,Mónica Rodríguez,Sebastian Barcelona,Angel Da Silva,Santiago Fernandez,Hiago Sodre,Igor Nunes,Bruna Guterres,Ricardo Grando*

Main category: cs.RO

TL;DR: UruBots团队开发了一款用于2025 FIRA自动驾驶汽车挑战赛的自主小车，采用深度学习模型实现实时导航。


<details>
  <summary>Details</summary>
Motivation: 参与2025 FIRA自动驾驶汽车挑战赛，展示自主导航技术。

Method: 构建小型电动车，结合机械电子组件和CNN模型处理摄像头输入，控制转向和油门。

Result: 车辆在30秒内完成赛道，速度约0.4米/秒，成功避障。

Conclusion: 项目验证了深度学习在小型自动驾驶车辆中的实用性。

Abstract: This paper describes the development of an autonomous car by the UruBots team
for the 2025 FIRA Autonomous Cars Challenge (Pro). The project involves
constructing a compact electric vehicle, approximately the size of an RC car,
capable of autonomous navigation through different tracks. The design
incorporates mechanical and electronic components and machine learning
algorithms that enable the vehicle to make real-time navigation decisions based
on visual input from a camera. We use deep learning models to process camera
images and control vehicle movements. Using a dataset of over ten thousand
images, we trained a Convolutional Neural Network (CNN) to drive the vehicle
effectively, through two outputs, steering and throttle. The car completed the
track in under 30 seconds, achieving a pace of approximately 0.4 meters per
second while avoiding obstacles.

</details>


### [228] [Active Illumination Control in Low-Light Environments using NightHawk](https://arxiv.org/abs/2506.06394)
*Yash Turkar,Youngjin Kim,Karthik Dantu*

Main category: cs.RO

TL;DR: NightHawk框架通过结合主动照明和曝光控制，优化地下环境中的图像质量，显著提升特征检测和匹配性能。


<details>
  <summary>Details</summary>
Motivation: 地下环境（如涵洞）光线昏暗且缺乏特征，现有照明方法存在反射、过曝和功耗问题。

Method: 提出基于贝叶斯优化的在线框架，动态调整光照强度和曝光时间，并使用特征检测器作为优化目标。

Result: 实验表明，特征检测和匹配性能提升47-197%，视觉估计更可靠。

Conclusion: NightHawk有效解决了地下环境中的视觉挑战，提升了机器人导航能力。

Abstract: Subterranean environments such as culverts present significant challenges to
robot vision due to dim lighting and lack of distinctive features. Although
onboard illumination can help, it introduces issues such as specular
reflections, overexposure, and increased power consumption. We propose
NightHawk, a framework that combines active illumination with exposure control
to optimize image quality in these settings. NightHawk formulates an online
Bayesian optimization problem to determine the best light intensity and
exposure-time for a given scene. We propose a novel feature detector-based
metric to quantify image utility and use it as the cost function for the
optimizer. We built NightHawk as an event-triggered recursive optimization
pipeline and deployed it on a legged robot navigating a culvert beneath the
Erie Canal. Results from field experiments demonstrate improvements in feature
detection and matching by 47-197% enabling more reliable visual estimation in
challenging lighting conditions.

</details>


### [229] [Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception](https://arxiv.org/abs/2506.06474)
*Everett Richards,Bipul Thapa,Lena Mashayekhy*

Main category: cs.RO

TL;DR: 论文提出了一种基于边缘计算和多车协作的实时物体检测框架ECOD，通过PACE和VOTE算法提升CAV的感知能力，实验显示其分类准确率比传统方法高75%。


<details>
  <summary>Details</summary>
Motivation: 传统车载感知系统因遮挡和盲区精度有限，云端解决方案延迟高，无法满足实时需求，因此需要一种低延迟、高精度的协作感知方案。

Method: ECOD框架结合了PACE（感知数据聚合）和VOTE（共识投票分类）算法，利用边缘计算实现多车协作实时检测。

Result: 实验表明，ECOD在物体分类准确率上比传统方法提升75%，同时实现低延迟实时处理。

Conclusion: 边缘计算在提升协作感知能力方面具有潜力，尤其适用于延迟敏感的自动驾驶系统。

Abstract: Accurate and reliable object detection is critical for ensuring the safety
and efficiency of Connected Autonomous Vehicles (CAVs). Traditional on-board
perception systems have limited accuracy due to occlusions and blind spots,
while cloud-based solutions introduce significant latency, making them
unsuitable for real-time processing demands required for autonomous driving in
dynamic environments. To address these challenges, we introduce an innovative
framework, Edge-Enabled Collaborative Object Detection (ECOD) for CAVs, that
leverages edge computing and multi-CAV collaboration for real-time,
multi-perspective object detection. Our ECOD framework integrates two key
algorithms: Perceptive Aggregation and Collaborative Estimation (PACE) and
Variable Object Tally and Evaluation (VOTE). PACE aggregates detection data
from multiple CAVs on an edge server to enhance perception in scenarios where
individual CAVs have limited visibility. VOTE utilizes a consensus-based voting
mechanism to improve the accuracy of object classification by integrating data
from multiple CAVs. Both algorithms are designed at the edge to operate in
real-time, ensuring low-latency and reliable decision-making for CAVs. We
develop a hardware-based controlled testbed consisting of camera-equipped
robotic CAVs and an edge server to evaluate the efficacy of our framework. Our
experimental results demonstrate the significant benefits of ECOD in terms of
improved object classification accuracy, outperforming traditional
single-perspective onboard approaches by up to 75%, while ensuring low-latency,
edge-driven real-time processing. This research highlights the potential of
edge computing to enhance collaborative perception for latency-sensitive
autonomous systems.

</details>


### [230] [DriveSuprim: Towards Precise Trajectory Selection for End-to-End Planning](https://arxiv.org/abs/2506.06659)
*Wenhao Yao,Zhenxin Li,Shiyi Lan,Zi Wang,Xinglong Sun,Jose M. Alvarez,Zuxuan Wu*

Main category: cs.RO

TL;DR: DriveSuprim通过渐进候选过滤、旋转增强和自蒸馏框架提升自动驾驶轨迹选择的安全性和性能，在NAVSIM数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在复杂环境中需确保安全导航，但现有方法难以精确选择最佳轨迹或处理罕见场景。

Method: 采用渐进候选过滤、旋转增强和自蒸馏框架优化轨迹选择。

Result: 在NAVSIM v1和v2上分别达到93.5% PDMS和87.1% EPDMS，展现卓越的安全性和轨迹质量。

Conclusion: DriveSuprim显著提升了自动驾驶的安全性和性能，适用于多样化驾驶场景。

Abstract: In complex driving environments, autonomous vehicles must navigate safely.
Relying on a single predicted path, as in regression-based approaches, usually
does not explicitly assess the safety of the predicted trajectory.
Selection-based methods address this by generating and scoring multiple
trajectory candidates and predicting the safety score for each, but face
optimization challenges in precisely selecting the best option from thousands
of possibilities and distinguishing subtle but safety-critical differences,
especially in rare or underrepresented scenarios. We propose DriveSuprim to
overcome these challenges and advance the selection-based paradigm through a
coarse-to-fine paradigm for progressive candidate filtering, a rotation-based
augmentation method to improve robustness in out-of-distribution scenarios, and
a self-distillation framework to stabilize training. DriveSuprim achieves
state-of-the-art performance, reaching 93.5% PDMS in NAVSIM v1 and 87.1% EPDMS
in NAVSIM v2 without extra data, demonstrating superior safetycritical
capabilities, including collision avoidance and compliance with rules, while
maintaining high trajectory quality in various driving scenarios.

</details>


### [231] [Generalized Trajectory Scoring for End-to-end Multimodal Planning](https://arxiv.org/abs/2506.06664)
*Zhenxin Li,Wenhao Yao,Zi Wang,Xinglong Sun,Joshua Chen,Nadine Chang,Maying Shen,Zuxuan Wu,Shiyi Lan,Jose M. Alvarez*

Main category: cs.RO

TL;DR: GTRS提出了一种统一的端到端多模态规划框架，结合粗粒度和细粒度轨迹评估，解决了静态和动态轨迹评分方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有轨迹评分方法在泛化能力上存在不足，静态方法缺乏细粒度适应性，动态方法难以捕捉广泛轨迹分布。

Method: GTRS包含三个创新：扩散基轨迹生成器、词汇泛化技术和传感器增强策略。

Result: GTRS在Navsim v2挑战赛中表现优异，接近依赖真实感知的特权方法。

Conclusion: GTRS通过结合粗粒度和细粒度评估，显著提升了轨迹评分的泛化能力。

Abstract: End-to-end multi-modal planning is a promising paradigm in autonomous
driving, enabling decision-making with diverse trajectory candidates. A key
component is a robust trajectory scorer capable of selecting the optimal
trajectory from these candidates. While recent trajectory scorers focus on
scoring either large sets of static trajectories or small sets of dynamically
generated ones, both approaches face significant limitations in generalization.
Static vocabularies provide effective coarse discretization but struggle to
make fine-grained adaptation, while dynamic proposals offer detailed precision
but fail to capture broader trajectory distributions. To overcome these
challenges, we propose GTRS (Generalized Trajectory Scoring), a unified
framework for end-to-end multi-modal planning that combines coarse and
fine-grained trajectory evaluation. GTRS consists of three complementary
innovations: (1) a diffusion-based trajectory generator that produces diverse
fine-grained proposals; (2) a vocabulary generalization technique that trains a
scorer on super-dense trajectory sets with dropout regularization, enabling its
robust inference on smaller subsets; and (3) a sensor augmentation strategy
that enhances out-of-domain generalization while incorporating refinement
training for critical trajectory discrimination. As the winning solution of the
Navsim v2 Challenge, GTRS demonstrates superior performance even with
sub-optimal sensor inputs, approaching privileged methods that rely on
ground-truth perception. Code will be available at
https://github.com/NVlabs/GTRS.

</details>


### [232] [RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation](https://arxiv.org/abs/2506.06677)
*Songhao Han,Boxiang Qiu,Yue Liao,Siyuan Huang,Chen Gao,Shuicheng Yan,Si Liu*

Main category: cs.RO

TL;DR: 论文介绍了RoboCerebra，一个用于评估机器人长期操作中高层推理能力的基准，包括数据集、分层框架和评估协议。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注反应性策略，未充分利用视觉语言模型在语义推理和长期规划中的潜力。

Method: 提出RoboCerebra基准，包含大规模模拟数据集、分层框架（高层VLM规划器和低层VLA控制器）及评估协议。

Result: 相比现有基准，RoboCerebra具有更长的动作序列和更密集的标注，并评估了先进VLM作为System 2模块的性能。

Conclusion: RoboCerebra推动了更具能力和泛化性的机器人规划器的发展。

Abstract: Recent advances in vision-language models (VLMs) have enabled
instruction-conditioned robotic systems with improved generalization. However,
most existing work focuses on reactive System 1 policies, underutilizing VLMs'
strengths in semantic reasoning and long-horizon planning. These System 2
capabilities-characterized by deliberative, goal-directed thinking-remain under
explored due to the limited temporal scale and structural complexity of current
benchmarks. To address this gap, we introduce RoboCerebra, a benchmark for
evaluating high-level reasoning in long-horizon robotic manipulation.
RoboCerebra includes: (1) a large-scale simulation dataset with extended task
horizons and diverse subtask sequences in household environments; (2) a
hierarchical framework combining a high-level VLM planner with a low-level
vision-language-action (VLA) controller; and (3) an evaluation protocol
targeting planning, reflection, and memory through structured System 1-System 2
interaction. The dataset is constructed via a top-down pipeline, where GPT
generates task instructions and decomposes them into subtask sequences. Human
operators execute the subtasks in simulation, yielding high-quality
trajectories with dynamic object variations. Compared to prior benchmarks,
RoboCerebra features significantly longer action sequences and denser
annotations. We further benchmark state-of-the-art VLMs as System 2 modules and
analyze their performance across key cognitive dimensions, advancing the
development of more capable and generalizable robotic planners.

</details>


### [233] [SpikePingpong: High-Frequency Spike Vision-based Robot Learning for Precise Striking in Table Tennis Game](https://arxiv.org/abs/2506.06690)
*Hao Wang,Chengkai Hou,Xianglong Li,Yankai Fu,Chenxuan Li,Ning Chen,Gaole Dai,Jiaming Liu,Tiejun Huang,Shanghang Zhang*

Main category: cs.RO

TL;DR: SpikePingpong系统结合脉冲视觉与模仿学习，通过SONIC和IMPACT模块解决高速乒乓球机器人控制中的视觉预测和策略规划问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 高速物体控制在机器人领域具有挑战性，乒乓球作为测试平台，需要快速拦截和精确轨迹调整，适合推动机器人控制技术的发展。

Method: 系统采用20 kHz脉冲相机进行高精度球跟踪，结合SONIC模块补偿空气阻力和摩擦，IMPACT模块实现精确落点规划。

Result: 实验显示，SpikePingpong在30 cm和20 cm精度任务中分别达到91%和71%的成功率，超越现有方法38%和37%。

Conclusion: SpikePingpong为高速动态任务中的机器人控制提供了新视角，实现了战术性游戏策略的稳健执行。

Abstract: Learning to control high-speed objects in the real world remains a
challenging frontier in robotics. Table tennis serves as an ideal testbed for
this problem, demanding both rapid interception of fast-moving balls and
precise adjustment of their trajectories. This task presents two fundamental
challenges: it requires a high-precision vision system capable of accurately
predicting ball trajectories, and it necessitates intelligent strategic
planning to ensure precise ball placement to target regions. The dynamic nature
of table tennis, coupled with its real-time response requirements, makes it
particularly well-suited for advancing robotic control capabilities in
fast-paced, precision-critical domains. In this paper, we present
SpikePingpong, a novel system that integrates spike-based vision with imitation
learning for high-precision robotic table tennis. Our approach introduces two
key attempts that directly address the aforementioned challenges: SONIC, a
spike camera-based module that achieves millimeter-level precision in
ball-racket contact prediction by compensating for real-world uncertainties
such as air resistance and friction; and IMPACT, a strategic planning module
that enables accurate ball placement to targeted table regions. The system
harnesses a 20 kHz spike camera for high-temporal resolution ball tracking,
combined with efficient neural network models for real-time trajectory
correction and stroke planning. Experimental results demonstrate that
SpikePingpong achieves a remarkable 91% success rate for 30 cm accuracy target
area and 71% in the more challenging 20 cm accuracy task, surpassing previous
state-of-the-art approaches by 38% and 37% respectively. These significant
performance improvements enable the robust implementation of sophisticated
tactical gameplay strategies, providing a new research perspective for robotic
control in high-speed dynamic tasks.

</details>


### [234] [Multimodal Spatial Language Maps for Robot Navigation and Manipulation](https://arxiv.org/abs/2506.06862)
*Chenguang Huang,Oier Mees,Andy Zeng,Wolfram Burgard*

Main category: cs.RO

TL;DR: 提出了一种多模态空间语言地图（VLMaps和AVLMaps），结合预训练多模态特征与3D环境重建，支持自然语言命令转换为空间目标，并增强多模态导航能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在环境映射中缺乏空间精度或多模态信息整合不足，需改进以支持更复杂的导航任务。

Method: 通过标准探索自主构建多模态空间语言地图，融合视觉、语言和音频信息，结合大型语言模型（LLMs）实现目标定位。

Result: 实验表明，该方法在模拟和真实环境中实现零样本空间和多模态目标导航，模糊场景中召回率提升50%。

Conclusion: 多模态空间语言地图显著提升了机器人在复杂环境中的导航和交互能力。

Abstract: Grounding language to a navigating agent's observations can leverage
pretrained multimodal foundation models to match perceptions to object or event
descriptions. However, previous approaches remain disconnected from environment
mapping, lack the spatial precision of geometric maps, or neglect additional
modality information beyond vision. To address this, we propose multimodal
spatial language maps as a spatial map representation that fuses pretrained
multimodal features with a 3D reconstruction of the environment. We build these
maps autonomously using standard exploration. We present two instances of our
maps, which are visual-language maps (VLMaps) and their extension to
audio-visual-language maps (AVLMaps) obtained by adding audio information. When
combined with large language models (LLMs), VLMaps can (i) translate natural
language commands into open-vocabulary spatial goals (e.g., "in between the
sofa and TV") directly localized in the map, and (ii) be shared across
different robot embodiments to generate tailored obstacle maps on demand.
Building upon the capabilities above, AVLMaps extend VLMaps by introducing a
unified 3D spatial representation integrating audio, visual, and language cues
through the fusion of features from pretrained multimodal foundation models.
This enables robots to ground multimodal goal queries (e.g., text, images, or
audio snippets) to spatial locations for navigation. Additionally, the
incorporation of diverse sensory inputs significantly enhances goal
disambiguation in ambiguous environments. Experiments in simulation and
real-world settings demonstrate that our multimodal spatial language maps
enable zero-shot spatial and multimodal goal navigation and improve recall by
50% in ambiguous scenarios. These capabilities extend to mobile robots and
tabletop manipulators, supporting navigation and interaction guided by visual,
audio, and spatial cues.

</details>


### [235] [MapBERT: Bitwise Masked Modeling for Real-Time Semantic Mapping Generation](https://arxiv.org/abs/2506.07350)
*Yijie Deng,Shuaihang Yuan,Congcong Wen,Hao Huang,Anthony Tzes,Geeta Chandra Raju Bethala,Yi Fang*

Main category: cs.RO

TL;DR: MapBERT是一个新颖的框架，通过BitVAE和掩码变换器有效建模未观察区域的语义分布，提升室内语义地图生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实时生成未观察区域时表现不佳，且难以泛化到新环境。MapBERT旨在解决稀疏、不平衡对象类别和多样空间尺度带来的挑战。

Method: 利用BitVAE将语义地图编码为紧凑的比特令牌，结合掩码变换器推断缺失区域，并提出对象感知掩码策略增强对象中心推理。

Result: 在Gibson基准测试中，MapBERT实现了最先进的语义地图生成，平衡了计算效率和未观察区域的准确重建。

Conclusion: MapBERT通过比特编码和对象感知策略，显著提升了室内语义分布建模的效率和准确性。

Abstract: Spatial awareness is a critical capability for embodied agents, as it enables
them to anticipate and reason about unobserved regions. The primary challenge
arises from learning the distribution of indoor semantics, complicated by
sparse, imbalanced object categories and diverse spatial scales. Existing
methods struggle to robustly generate unobserved areas in real time and do not
generalize well to new environments. To this end, we propose \textbf{MapBERT},
a novel framework designed to effectively model the distribution of unseen
spaces. Motivated by the observation that the one-hot encoding of semantic maps
aligns naturally with the binary structure of bit encoding, we, for the first
time, leverage a lookup-free BitVAE to encode semantic maps into compact
bitwise tokens. Building on this, a masked transformer is employed to infer
missing regions and generate complete semantic maps from limited observations.
To enhance object-centric reasoning, we propose an object-aware masking
strategy that masks entire object categories concurrently and pairs them with
learnable embeddings, capturing implicit relationships between object
embeddings and spatial tokens. By learning these relationships, the model more
effectively captures indoor semantic distributions crucial for practical
robotic tasks. Experiments on Gibson benchmarks show that MapBERT achieves
state-of-the-art semantic map generation, balancing computational efficiency
with accurate reconstruction of unobserved regions.

</details>


### [236] [BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation](https://arxiv.org/abs/2506.07530)
*Hongyu Wang,Chuyan Xiong,Ruiping Wang,Xilin Chen*

Main category: cs.RO

TL;DR: BitVLA是首个用于机器人操作的1位VLA模型，通过三元参数和1.58位权重压缩技术，显著减少内存占用，性能接近4位量化模型。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型在资源受限机器人系统上部署的挑战，探索1位预训练在VLA模型中的应用。

Method: 提出BitVLA模型，采用三元参数和蒸馏感知训练策略压缩视觉编码器权重。

Result: 在LIBERO基准测试中，BitVLA性能接近OpenVLA-OFT（4位量化），内存占用仅为29.8%。

Conclusion: BitVLA展示了在内存受限边缘设备上部署的潜力，代码和模型已开源。

Abstract: Vision-Language-Action (VLA) models have shown impressive capabilities across
a wide range of robotics manipulation tasks. However, their growing model size
poses significant challenges for deployment on resource-constrained robotic
systems. While 1-bit pretraining has proven effective for enhancing the
inference efficiency of large language models with minimal performance loss,
its application to VLA models remains underexplored. In this work, we present
BitVLA, the first 1-bit VLA model for robotics manipulation, in which every
parameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint
of the vision encoder, we propose the distillation-aware training strategy that
compresses the full-precision encoder to 1.58-bit weights. During this process,
a full-precision encoder serves as a teacher model to better align latent
representations. Despite the lack of large-scale robotics pretraining, BitVLA
achieves performance comparable to the state-of-the-art model OpenVLA-OFT with
4-bit post-training quantization on the LIBERO benchmark, while consuming only
29.8% of the memory. These results highlight BitVLA's promise for deployment on
memory-constrained edge devices. We release the code and model weights in
https://github.com/ustcwhy/BitVLA.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [237] [QForce-RL: Quantized FPGA-Optimized Reinforcement Learning Compute Engine](https://arxiv.org/abs/2506.07046)
*Anushka Jha,Tanushree Dewangan,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: QForce-RL通过量化和轻量级架构提升FPGA上的RL性能，减少资源消耗，性能提升2.3倍，FPS提升2.6倍。


<details>
  <summary>Details</summary>
Motivation: FPGA部署RL资源消耗大，传统方法在高性能图像训练中计算量大，QForce-RL旨在解决这些问题。

Method: 结合E2HRL减少RL动作学习策略，利用QuaRL量化SIMD硬件加速，优化模型大小和计算操作。

Result: 性能提升2.3倍，FPS提升2.6倍，适用于资源受限设备，灵活优化延迟、吞吐量和能效。

Conclusion: QForce-RL是一种高效、可扩展的RL部署方案，显著提升性能并降低资源消耗。

Abstract: Reinforcement Learning (RL) has outperformed other counterparts in sequential
decision-making and dynamic environment control. However, FPGA deployment is
significantly resource-expensive, as associated with large number of
computations in training agents with high-quality images and possess new
challenges. In this work, we propose QForce-RL takes benefits of quantization
to enhance throughput and reduce energy footprint with light-weight RL
architecture, without significant performance degradation. QForce-RL takes
advantages from E2HRL to reduce overall RL actions to learn desired policy and
QuaRL for quantization based SIMD for hardware acceleration. We have also
provided detailed analysis for different RL environments, with emphasis on
model size, parameters, and accelerated compute ops. The architecture is
scalable for resource-constrained devices and provide parametrized efficient
deployment with flexibility in latency, throughput, power, and energy
efficiency. The proposed QForce-RL provides performance enhancement up to 2.3x
and better FPS - 2.6x compared to SoTA works.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [238] [W4S4: WaLRUS Meets S4 for Long-Range Sequence Modeling](https://arxiv.org/abs/2506.07920)
*Hossein Babaei,Mel White,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: 论文提出了一种新的状态空间模型（SSM）变体W4S4，基于冗余小波框架构建，改进了长程依赖处理能力，并在多个任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的SSM在处理长程依赖时效果依赖于状态矩阵的选择和初始化，需要更高效且理论支持的方法。

Method: 基于SaFARi框架和WaLRUS SSM，引入W4S4，利用冗余小波框架构建，支持快速核计算且无需低秩近似。

Result: W4S4在延迟重构、分类任务和长序列建模中表现优于基于HiPPO的SSM。

Conclusion: W4S4为下一代深度SSM模型提供了可扩展且高效的基础。

Abstract: State Space Models (SSMs) have emerged as powerful components for sequence
modeling, enabling efficient handling of long-range dependencies via linear
recurrence and convolutional computation. However, their effectiveness depends
heavily on the choice and initialization of the state matrix. In this work, we
build on the SaFARi framework and existing WaLRUS SSMs to introduce a new
variant, W4S4 (WaLRUS for S4), a new class of SSMs constructed from redundant
wavelet frames. WaLRUS admits a stable diagonalization and supports fast kernel
computation without requiring low-rank approximations, making it both
theoretically grounded and computationally efficient. We show that WaLRUS
retains information over long horizons significantly better than HiPPO-based
SSMs, both in isolation and when integrated into deep architectures such as S4.
Our experiments demonstrate consistent improvements across delay reconstruction
tasks, classification benchmarks, and long-range sequence modeling, confirming
that high-quality, structured initialization enabled by wavelet-based state
dynamic offers substantial advantages over existing alternatives. WaLRUS
provides a scalable and versatile foundation for the next generation of deep
SSM-based models.

</details>


### [239] [CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning](https://arxiv.org/abs/2506.06290)
*Mingyu Lu,Ethan Weinberger,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: CellCLIP是一种用于高内涵筛选数据的跨模态对比学习框架，通过预训练图像编码器和新型通道编码方案，显著提升了性能并减少了计算时间。


<details>
  <summary>Details</summary>
Motivation: 高内涵筛选数据（如Cell Painting）的语义与自然图像差异大，且不同扰动类型（如小分子与CRISPR基因敲除）难以在同一潜在空间表示，因此需要开发新的方法。

Method: CellCLIP结合预训练图像编码器、新型通道编码方案和自然语言编码器，学习统一的潜在空间，对齐扰动与其形态效应。

Result: CellCLIP在跨模态检索和生物相关下游任务中表现最佳，同时显著减少计算时间。

Conclusion: CellCLIP为高内涵筛选数据提供了一种高效且性能优越的跨模态对比学习解决方案。

Abstract: High-content screening (HCS) assays based on high-throughput microscopy
techniques such as Cell Painting have enabled the interrogation of cells'
morphological responses to perturbations at an unprecedented scale. The
collection of such data promises to facilitate a better understanding of the
relationships between different perturbations and their effects on cellular
state. Towards achieving this goal, recent advances in cross-modal contrastive
learning could, in theory, be leveraged to learn a unified latent space that
aligns perturbations with their corresponding morphological effects. However,
the application of such methods to HCS data is not straightforward due to
substantial differences in the semantics of Cell Painting images compared to
natural images, and the difficulty of representing different classes of
perturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent
space. In response to these challenges, here we introduce CellCLIP, a
cross-modal contrastive learning framework for HCS data. CellCLIP leverages
pre-trained image encoders coupled with a novel channel encoding scheme to
better capture relationships between different microscopy channels in image
embeddings, along with natural language encoders for representing
perturbations. Our framework outperforms current open-source models,
demonstrating the best performance in both cross-modal retrieval and
biologically meaningful downstream tasks while also achieving significant
reductions in computation time.

</details>


### [240] [NeurNCD: Novel Class Discovery via Implicit Neural Representation](https://arxiv.org/abs/2506.06412)
*Junming Wang,Yi Shi*

Main category: cs.LG

TL;DR: NeurNCD是一个用于开放世界新类发现的框架，通过结合Embedding-NeRF模型和KL散度，替代传统显式3D分割图，提升语义嵌入和视觉嵌入空间的信息聚合能力。


<details>
  <summary>Details</summary>
Motivation: 传统显式表示（如对象描述符或3D分割图）存在离散、易产生空洞和噪声的问题，限制了新类发现的准确性。

Method: 采用Embedding-NeRF模型和KL散度，结合特征查询、调制和聚类等关键组件，实现语义分割网络与隐式神经表示之间的高效信息交换。

Result: 在NYUv2和Replica数据集上显著优于现有方法，无需密集标注数据或人工干预。

Conclusion: NeurNCD为开放世界新类发现提供了一种高效且数据友好的解决方案。

Abstract: Discovering novel classes in open-world settings is crucial for real-world
applications. Traditional explicit representations, such as object descriptors
or 3D segmentation maps, are constrained by their discrete, hole-prone, and
noisy nature, which hinders accurate novel class discovery. To address these
challenges, we introduce NeurNCD, the first versatile and data-efficient
framework for novel class discovery that employs the meticulously designed
Embedding-NeRF model combined with KL divergence as a substitute for
traditional explicit 3D segmentation maps to aggregate semantic embedding and
entropy in visual embedding space. NeurNCD also integrates several key
components, including feature query, feature modulation and clustering,
facilitating efficient feature augmentation and information exchange between
the pre-trained semantic segmentation network and implicit neural
representations. As a result, our framework achieves superior segmentation
performance in both open and closed-world settings without relying on densely
labelled datasets for supervised training or human interaction to generate
sparse label supervision. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches on the NYUv2 and Replica
datasets.

</details>


### [241] [Vision-QRWKV: Exploring Quantum-Enhanced RWKV Models for Image Classification](https://arxiv.org/abs/2506.06633)
*Chi-Sheng Chen*

Main category: cs.LG

TL;DR: 论文提出Vision-QRWKV，一种量子-经典混合的RWKV架构，首次应用于图像分类任务，通过集成变分量子电路提升非线性特征变换能力。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习在复杂高维数据领域显示出潜力，本文旨在通过量子增强提升RWKV架构在视觉任务中的表现。

Method: 将变分量子电路（VQC）集成到RWKV的通道混合组件中，构建量子-经典混合模型。

Result: 在14个医学和标准图像分类基准测试中，量子增强模型在多数数据集上优于经典模型，尤其在噪声或细微类别区分任务中表现突出。

Conclusion: 研究首次系统地将量子增强RWKV应用于视觉领域，为轻量高效视觉任务的量子模型提供了架构权衡和未来潜力的见解。

Abstract: Recent advancements in quantum machine learning have shown promise in
enhancing classical neural network architectures, particularly in domains
involving complex, high-dimensional data. Building upon prior work in temporal
sequence modeling, this paper introduces Vision-QRWKV, a hybrid
quantum-classical extension of the Receptance Weighted Key Value (RWKV)
architecture, applied for the first time to image classification tasks. By
integrating a variational quantum circuit (VQC) into the channel mixing
component of RWKV, our model aims to improve nonlinear feature transformation
and enhance the expressive capacity of visual representations.
  We evaluate both classical and quantum RWKV models on a diverse collection of
14 medical and standard image classification benchmarks, including MedMNIST
datasets, MNIST, and FashionMNIST. Our results demonstrate that the
quantum-enhanced model outperforms its classical counterpart on a majority of
datasets, particularly those with subtle or noisy class distinctions (e.g.,
ChestMNIST, RetinaMNIST, BloodMNIST). This study represents the first
systematic application of quantum-enhanced RWKV in the visual domain, offering
insights into the architectural trade-offs and future potential of quantum
models for lightweight and efficient vision tasks.

</details>


### [242] [Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning](https://arxiv.org/abs/2506.06637)
*Olimjon Toirov,Wei Yu*

Main category: cs.LG

TL;DR: 提出了一种结合图像负载特征和持续学习的非侵入式负载监测方法，显著提高了识别精度。


<details>
  <summary>Details</summary>
Motivation: 传统NILM方法在复杂负载组合和应用环境下特征鲁棒性差、模型泛化能力不足。

Method: 将多维电力信号转换为图像负载特征，结合深度卷积神经网络进行设备识别，并引入自监督预训练和持续在线学习策略。

Result: 在高采样率负载数据集上实验表明，该方法在识别精度上有显著提升。

Conclusion: 该方法有效解决了传统NILM的局限性，适应新负载的出现，具有实际应用潜力。

Abstract: Non-Intrusive Load Monitoring (NILM) identifies the operating status and
energy consumption of each electrical device in the circuit by analyzing the
electrical signals at the bus, which is of great significance for smart power
management. However, the complex and changeable load combinations and
application environments lead to the challenges of poor feature robustness and
insufficient model generalization of traditional NILM methods. To this end,
this paper proposes a new non-intrusive load monitoring method that integrates
"image load signature" and continual learning. This method converts
multi-dimensional power signals such as current, voltage, and power factor into
visual image load feature signatures, and combines deep convolutional neural
networks to realize the identification and classification of multiple devices;
at the same time, self-supervised pre-training is introduced to improve feature
generalization, and continual online learning strategies are used to overcome
model forgetting to adapt to the emergence of new loads. This paper conducts a
large number of experiments on high-sampling rate load datasets, and compares a
variety of existing methods and model variants. The results show that the
proposed method has achieved significant improvements in recognition accuracy.

</details>


### [243] [The OCR Quest for Generalization: Learning to recognize low-resource alphabets with model editing](https://arxiv.org/abs/2506.06761)
*Adrià Molina Rodríguez,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.LG

TL;DR: 该论文提出了一种通过模型编辑和领域合并的方法，提升低资源语言（如古代手稿和非西方语言）在识别系统中的泛化能力，显著优于传统微调策略。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言在识别系统中因数据不足而被忽视的问题，提升模型对新数据分布（如新字母表）的适应能力。

Method: 利用模型编辑技术，结合领域合并方法，在稀疏数据分布中实现高效学习，无需依赖原型或其他传统需求。

Result: 实验表明，即使在相同训练数据下，该方法在迁移学习和跨域评估中表现显著优于现有方法，特别是在历史加密文本和非拉丁字母任务中。

Conclusion: 该研究为构建能够快速适应低资源字母表的模型提供了新思路，扩展了文档识别的应用范围和文化包容性。

Abstract: Achieving robustness in recognition systems across diverse domains is crucial
for their practical utility. While ample data availability is usually assumed,
low-resource languages, such as ancient manuscripts and non-western languages,
tend to be kept out of the equations of massive pretraining and foundational
techniques due to an under representation. In this work, we aim for building
models which can generalize to new distributions of data, such as alphabets,
faster than centralized fine-tune strategies. For doing so, we take advantage
of the recent advancements in model editing to enhance the incorporation of
unseen scripts (low-resource learning). In contrast to state-of-the-art
meta-learning, we showcase the effectiveness of domain merging in sparse
distributions of data, with agnosticity of its relation to the overall
distribution or any other prototyping necessity. Even when using the same exact
training data, our experiments showcase significant performance boosts in
\textbf{transfer learning} to new alphabets and \textbf{out-of-domain
evaluation} in challenging domain shifts, including historical ciphered texts
and non-Latin scripts. This research contributes a novel approach into building
models that can easily adopt under-represented alphabets and, therefore, enable
document recognition to a wider set of contexts and cultures.

</details>


### [244] [Feature-Based Instance Neighbor Discovery: Advanced Stable Test-Time Adaptation in Dynamic World](https://arxiv.org/abs/2506.06782)
*Qinting Jiang,Chuyang Ye,Dongyan Wei,Bingli Wang,Yuan Xue,Jingyan Jiang,Zhi Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为FIND的新方法，通过特征实例邻居发现解决深度神经网络在分布偏移下的性能下降问题，显著提升了动态测试场景中的准确性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在训练和测试域之间的分布偏移下性能下降，影响应用体验。现有测试时适应方法难以应对动态多测试分布的挑战。

Method: FIND方法包括三个关键组件：层间特征解耦（LFD）、特征感知批量归一化（FABN）和选择性FABN（S-FABN）。LFD通过构建图结构稳定捕获相似分布特征，FABN结合源统计和测试分布统计，S-FABN优化特征分区的层选择。

Result: 实验表明，FIND在动态场景中显著优于现有方法，准确率提升30%，同时保持计算效率。

Conclusion: FIND通过特征解耦和动态归一化策略，有效解决了分布偏移问题，提升了模型在动态测试环境中的性能。

Abstract: Despite progress, deep neural networks still suffer performance declines
under distribution shifts between training and test domains, leading to a
substantial decrease in Quality of Experience (QoE) for applications. Existing
test-time adaptation (TTA) methods are challenged by dynamic, multiple test
distributions within batches. We observe that feature distributions across
different domains inherently cluster into distinct groups with varying means
and variances. This divergence reveals a critical limitation of previous global
normalization strategies in TTA, which inevitably distort the original data
characteristics. Based on this insight, we propose Feature-based Instance
Neighbor Discovery (FIND), which comprises three key components: Layer-wise
Feature Disentanglement (LFD), Feature Aware Batch Normalization (FABN) and
Selective FABN (S-FABN). LFD stably captures features with similar
distributions at each layer by constructing graph structures. While FABN
optimally combines source statistics with test-time distribution specific
statistics for robust feature representation. Finally, S-FABN determines which
layers require feature partitioning and which can remain unified, thereby
enhancing inference efficiency. Extensive experiments demonstrate that FIND
significantly outperforms existing methods, achieving a 30\% accuracy
improvement in dynamic scenarios while maintaining computational efficiency.

</details>


### [245] [FREE: Fast and Robust Vision Language Models with Early Exits](https://arxiv.org/abs/2506.06884)
*Divya Jyoti Bajpai,Manjesh Kumar Hanawal*

Main category: cs.LG

TL;DR: 论文提出了一种名为FREE的对抗训练方法，用于在视觉语言模型中实现早期退出策略，以提升推理速度并保持性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）虽然性能显著，但模型体积大导致推理延迟问题，限制了实际应用。

Method: 采用基于GAN的对抗训练框架，每个退出点包含一个Transformer层和一个分类器，通过对抗训练使中间层特征接近最终层。

Result: 实验表明，该方法在保持性能的同时，推理速度提升1.51倍以上，并缓解了过思考和中间危机现象。

Conclusion: FREE方法有效提升了视觉语言模型的推理效率，同时保持了模型的准确性和鲁棒性。

Abstract: In recent years, Vision-Language Models (VLMs) have shown remarkable
performance improvements in Vision-Language tasks. However, their large size
poses challenges for real-world applications where inference latency is a
concern. To tackle this issue, we propose employing Early Exit (EE) strategies
in VLMs. However, training exit classifiers in VLMs is challenging,
particularly with limited labeled training data. To address this, we introduce
FREE, an adversarial training approach within a GAN-based framework. Here, each
exit consists of a transformer layer and a classifier. The transformer layer is
adversarially trained to produce feature representations similar to the final
layer, while a feature classifier serves as the discriminator. Our method
focuses on performing input-adaptive inference that increases inference speed
with minimal drop in performance. Experimental results demonstrate the
effectiveness of our approach in enhancing accuracy and model robustness by
mitigating overthinking and the phenomenon of mid-crisis that we highlight. We
experimentally validate that our method speeds up the inference process by more
than 1.51x while retaining comparable performance. The source code is available
at https://github.com/Div290/FREE.

</details>


### [246] [Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry](https://arxiv.org/abs/2506.06933)
*Mahdi Salmani,Alireza Abdollahpoorrostam,Seyed-Mohsen Moosavi-Dezfooli*

Main category: cs.LG

TL;DR: 本文提出了一种针对非对称查询成本的决策型黑盒对抗攻击框架，通过改进搜索策略和梯度估计过程，显著降低了总查询成本和扰动大小。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设所有查询成本相同，而实际应用中某些查询成本更高（如触发额外审查的类别）。针对这一非对称成本场景，现有算法效果有限。

Method: 提出了非对称搜索（AS）和非对称梯度估计（AGREST），通过减少对高成本查询的依赖和调整采样分布，平衡不同类型查询。

Result: 在多种成本设置下，该方法总查询成本和扰动大小均优于现有方法，某些场景下提升达40%。

Conclusion: 该框架可轻松集成到现有黑盒攻击中，为非对称成本场景提供了高效解决方案。

Abstract: Traditional decision-based black-box adversarial attacks on image classifiers
aim to generate adversarial examples by slightly modifying input images while
keeping the number of queries low, where each query involves sending an input
to the model and observing its output. Most existing methods assume that all
queries have equal cost. However, in practice, queries may incur asymmetric
costs; for example, in content moderation systems, certain output classes may
trigger additional review, enforcement, or penalties, making them more costly
than others. While prior work has considered such asymmetric cost settings,
effective algorithms for this scenario remain underdeveloped. In this paper, we
propose a general framework for decision-based attacks under asymmetric query
costs, which we refer to as asymmetric black-box attacks. We modify two core
components of existing attacks: the search strategy and the gradient estimation
process. Specifically, we propose Asymmetric Search (AS), a more conservative
variant of binary search that reduces reliance on high-cost queries, and
Asymmetric Gradient Estimation (AGREST), which shifts the sampling distribution
to favor low-cost queries. We design efficient algorithms that minimize total
attack cost by balancing different query types, in contrast to earlier methods
such as stealthy attacks that focus only on limiting expensive (high-cost)
queries. Our method can be integrated into a range of existing black-box
attacks with minimal changes. We perform both theoretical analysis and
empirical evaluation on standard image classification benchmarks. Across
various cost regimes, our method consistently achieves lower total query cost
and smaller perturbations than existing approaches, with improvements of up to
40% in some settings.

</details>


### [247] [Towards Physics-informed Diffusion for Anomaly Detection in Trajectories](https://arxiv.org/abs/2506.06999)
*Arun Sharma,Mingzhou Yang,Majid Farhadloo,Subhankar Ghosh,Bharat Jayaprakash,Shashi Shekhar*

Main category: cs.LG

TL;DR: 提出了一种基于物理约束的扩散模型，用于检测异常轨迹（如GPS欺骗），在真实数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决国际水域非法活动（如非法捕鱼和石油走私）中的GPS欺骗问题，现有方法因忽略时空依赖性和物理知识导致高误报率。

Method: 结合物理约束的扩散模型，利用运动学规律识别不符合物理定律的轨迹。

Result: 在海上和城市数据集上，模型在异常检测和轨迹生成方面表现出更高的准确性和更低的误差率。

Conclusion: 提出的物理信息扩散模型能有效检测异常轨迹，为遏制非法活动提供了技术支持。

Abstract: Given trajectory data, a domain-specific study area, and a user-defined
threshold, we aim to find anomalous trajectories indicative of possible GPS
spoofing (e.g., fake trajectory). The problem is societally important to curb
illegal activities in international waters, such as unauthorized fishing and
illicit oil transfers. The problem is challenging due to advances in AI
generated in deep fakes generation (e.g., additive noise, fake trajectories)
and lack of adequate amount of labeled samples for ground-truth verification.
Recent literature shows promising results for anomalous trajectory detection
using generative models despite data sparsity. However, they do not consider
fine-scale spatiotemporal dependencies and prior physical knowledge, resulting
in higher false-positive rates. To address these limitations, we propose a
physics-informed diffusion model that integrates kinematic constraints to
identify trajectories that do not adhere to physical laws. Experimental results
on real-world datasets in the maritime and urban domains show that the proposed
framework results in higher prediction accuracy and lower estimation error rate
for anomaly detection and trajectory generation methods, respectively. Our
implementation is available at
https://github.com/arunshar/Physics-Informed-Diffusion-Probabilistic-Model.

</details>


### [248] [Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward](https://arxiv.org/abs/2506.07218)
*Tong Xiao,Xin Xu,Zhenya Huang,Hongyu Gao,Quan Liu,Qi Liu,Enhong Chen*

Main category: cs.LG

TL;DR: 论文提出Perception-R1方法，通过引入视觉感知奖励增强多模态大语言模型（MLLMs）的感知和推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习与可验证奖励（RLVR）方法未能有效提升MLLMs的多模态感知能力，限制了其推理能力的进一步提升。

Method: 提出Perception-R1，通过视觉感知奖励激励模型准确感知视觉内容，并利用文本视觉注释和评判LLM进行奖励分配。

Result: 在多个多模态推理基准测试中，Perception-R1仅用1,442训练数据即达到最先进性能。

Conclusion: Perception-R1有效提升了MLLMs的感知和推理能力，为多模态任务提供了新思路。

Abstract: Enhancing the multimodal reasoning capabilities of Multimodal Large Language
Models (MLLMs) is a challenging task that has attracted increasing attention in
the community. Recently, several studies have applied Reinforcement Learning
with Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the
reasoning abilities of MLLMs. However, these works largely overlook the
enhancement of multimodal perception capabilities in MLLMs, which serve as a
core prerequisite and foundational component of complex multimodal reasoning.
Through McNemar's test, we find that existing RLVR method fails to effectively
enhance the multimodal perception capabilities of MLLMs, thereby limiting their
further improvement in multimodal reasoning. To address this limitation, we
propose Perception-R1, which introduces a novel visual perception reward that
explicitly encourages MLLMs to perceive the visual content accurately, thereby
can effectively incentivizing both their multimodal perception and reasoning
capabilities. Specifically, we first collect textual visual annotations from
the CoT trajectories of multimodal problems, which will serve as visual
references for reward assignment. During RLVR training, we employ a judging LLM
to assess the consistency between the visual annotations and the responses
generated by MLLM, and assign the visual perception reward based on these
consistency judgments. Extensive experiments on several multimodal reasoning
benchmarks demonstrate the effectiveness of our Perception-R1, which achieves
state-of-the-art performance on most benchmarks using only 1,442 training data.

</details>


### [249] [Variational Supervised Contrastive Learning](https://arxiv.org/abs/2506.07413)
*Ziwen Wang,Jiajun Fan,Thao Nguyen,Heng Ji,Ge Liu*

Main category: cs.LG

TL;DR: VarCon通过变分推理改进对比学习，解决了嵌入分布无明确调控和过度依赖大批量负样本的问题，在多个数据集上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前对比学习存在嵌入分布无明确调控和过度依赖大批量负样本的问题，导致语义相关实例可能被错误分离且泛化能力受限。

Method: 提出VarCon，将监督对比学习重新表述为对潜在类别变量的变分推理，最大化后验加权的证据下界（ELBO），实现高效的类感知匹配和对嵌入空间内类内分散的精细控制。

Result: 在CIFAR-10、CIFAR-100、ImageNet-100和ImageNet-1K上，VarCon达到SOTA性能（如ImageNet-1K上79.36% Top-1准确率），并表现出更清晰的决策边界和语义组织。

Conclusion: VarCon在对比学习框架中显著提升了性能、泛化能力和鲁棒性，适用于少样本学习和多种增强策略。

Abstract: Contrastive learning has proven to be highly efficient and adaptable in
shaping representation spaces across diverse modalities by pulling similar
samples together and pushing dissimilar ones apart. However, two key
limitations persist: (1) Without explicit regulation of the embedding
distribution, semantically related instances can inadvertently be pushed apart
unless complementary signals guide pair selection, and (2) excessive reliance
on large in-batch negatives and tailored augmentations hinders generalization.
To address these limitations, we propose Variational Supervised Contrastive
Learning (VarCon), which reformulates supervised contrastive learning as
variational inference over latent class variables and maximizes a
posterior-weighted evidence lower bound (ELBO) that replaces exhaustive
pair-wise comparisons for efficient class-aware matching and grants
fine-grained control over intra-class dispersion in the embedding space.
Trained exclusively on image data, our experiments on CIFAR-10, CIFAR-100,
ImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art
performance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy
on ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while
converging in just 200 epochs; (2) yields substantially clearer decision
boundaries and semantic organization in the embedding space, as evidenced by
KNN classification, hierarchical clustering results, and transfer-learning
assessments; and (3) demonstrates superior performance in few-shot learning
than supervised baseline and superior robustness across various augmentation
strategies.

</details>


### [250] [Language Embedding Meets Dynamic Graph: A New Exploration for Neural Architecture Representation Learning](https://arxiv.org/abs/2506.07735)
*Haizhao Jing,Haokui Zhang,Zhenhao Shang,Rong Xiao,Peng Wang,Yanning Zhang*

Main category: cs.LG

TL;DR: LeDG-Former 是一种结合语言嵌入和动态图表示学习的框架，解决了现有方法忽略硬件属性和静态邻接矩阵的局限性，实现了跨硬件平台的零样本预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视硬件属性信息且依赖静态邻接矩阵，限制了模型的实际应用和编码效果。

Method: 提出语言嵌入框架和动态图变换器，将神经架构和硬件规格投影到统一语义空间，并改进神经架构建模。

Result: 在 NNLQP 基准测试中超越现有方法，实现跨硬件延迟预测，并在 NAS-Bench-101 和 NAS-Bench-201 数据集上表现优异。

Conclusion: LeDG-Former 通过语言和动态图的结合，显著提升了神经架构表示学习的性能和应用范围。

Abstract: Neural Architecture Representation Learning aims to transform network models
into feature representations for predicting network attributes, playing a
crucial role in deploying and designing networks for real-world applications.
Recently, inspired by the success of transformers, transformer-based models
integrated with Graph Neural Networks (GNNs) have achieved significant progress
in representation learning. However, current methods still have some
limitations. First, existing methods overlook hardware attribute information,
which conflicts with the current trend of diversified deep learning hardware
and limits the practical applicability of models. Second, current encoding
approaches rely on static adjacency matrices to represent topological
structures, failing to capture the structural differences between computational
nodes, which ultimately compromises encoding effectiveness. In this paper, we
introduce LeDG-Former, an innovative framework that addresses these limitations
through the synergistic integration of language-based semantic embedding and
dynamic graph representation learning. Specifically, inspired by large language
models (LLMs), we propose a language embedding framework where both neural
architectures and hardware platform specifications are projected into a unified
semantic space through tokenization and LLM processing, enabling zero-shot
prediction across different hardware platforms for the first time. Then, we
propose a dynamic graph-based transformer for modeling neural architectures,
resulting in improved neural architecture modeling performance. On the NNLQP
benchmark, LeDG-Former surpasses previous methods, establishing a new SOTA
while demonstrating the first successful cross-hardware latency prediction
capability. Furthermore, our framework achieves superior performance on the
cell-structured NAS-Bench-101 and NAS-Bench-201 datasets.

</details>


### [251] [Identifiable Object Representations under Spatial Ambiguities](https://arxiv.org/abs/2506.07806)
*Avinash Kori,Francesca Toni,Ben Glocker*

Main category: cs.LG

TL;DR: 提出了一种多视角概率方法，解决空间模糊性问题，无需视角标注，并在实验中验证了其鲁棒性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 模块化物体中心表示对人类推理至关重要，但在空间模糊性（如遮挡和视角模糊）下难以实现。

Method: 引入多视角概率方法，聚合视角特定槽以捕捉不变内容信息，同时学习解耦的全局视角级信息。

Result: 解决了空间模糊性问题，提供了可识别性的理论保证，并在实验中表现出鲁棒性和可扩展性。

Conclusion: 该方法在无需视角标注的情况下，有效解决了空间模糊性问题，并在复杂数据集中表现优异。

Abstract: Modular object-centric representations are essential for *human-like
reasoning* but are challenging to obtain under spatial ambiguities, *e.g. due
to occlusions and view ambiguities*. However, addressing challenges presents
both theoretical and practical difficulties. We introduce a novel multi-view
probabilistic approach that aggregates view-specific slots to capture
*invariant content* information while simultaneously learning disentangled
global *viewpoint-level* information. Unlike prior single-view methods, our
approach resolves spatial ambiguities, provides theoretical guarantees for
identifiability, and requires *no viewpoint annotations*. Extensive experiments
on standard benchmarks and novel complex datasets validate our method's
robustness and scalability.

</details>


### [252] [Diffusion Counterfactual Generation with Semantic Abduction](https://arxiv.org/abs/2506.07883)
*Rajat Rasal,Avinash Kori,Fabio De Sousa Ribeiro,Tian Xia,Ben Glocker*

Main category: cs.LG

TL;DR: 该论文提出了一种基于扩散模型的因果机制框架，用于生成反事实图像，解决了现有方法在可扩展性和保真度上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有自动编码框架在反事实图像生成中存在可扩展性和保真度问题，而扩散模型在视觉质量和感知对齐方面表现优异，因此探索如何利用扩散模型改进反事实图像编辑。

Method: 提出了一套基于扩散模型的因果机制，包括空间、语义和动态反演概念，并将语义表示通过Pearl因果理论整合到扩散模型中，实现反事实推理的图像编辑。

Result: 首次实现了扩散模型在反事实图像生成中的高级语义身份保留，并展示了语义控制在因果控制与身份保留之间的权衡。

Conclusion: 该框架为反事实图像生成提供了新的解决方案，展示了扩散模型在因果控制与身份保留方面的潜力。

Abstract: Counterfactual image generation presents significant challenges, including
preserving identity, maintaining perceptual quality, and ensuring faithfulness
to an underlying causal model. While existing auto-encoding frameworks admit
semantic latent spaces which can be manipulated for causal control, they
struggle with scalability and fidelity. Advancements in diffusion models
present opportunities for improving counterfactual image editing, having
demonstrated state-of-the-art visual quality, human-aligned perception and
representation learning capabilities. Here, we present a suite of
diffusion-based causal mechanisms, introducing the notions of spatial, semantic
and dynamic abduction. We propose a general framework that integrates semantic
representations into diffusion models through the lens of Pearlian causality to
edit images via a counterfactual reasoning process. To our knowledge, this is
the first work to consider high-level semantic identity preservation for
diffusion counterfactuals and to demonstrate how semantic control enables
principled trade-offs between faithful causal control and identity
preservation.

</details>


### [253] [Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces](https://arxiv.org/abs/2506.07903)
*Kevin Rojas,Yuchen Zhu,Sichen Zhu,Felix X. -F. Ye,Molei Tao*

Main category: cs.LG

TL;DR: 提出了一种新的多模态扩散模型框架，支持跨模态数据的原生生成，无需依赖外部预处理协议。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外部预处理协议（如分词器和变分自编码器）统一多模态数据，对编码器和解码器精度要求高，限制了在数据有限场景的应用。

Method: 引入解耦的噪声调度策略，支持无条件生成和模态条件生成，适用于任意状态空间的多模态数据。

Result: 在文本-图像生成和混合类型表格数据合成任务中表现优异。

Conclusion: 新框架突破了多模态数据生成的限制，具有广泛的应用潜力。

Abstract: Diffusion models have demonstrated remarkable performance in generating
unimodal data across various tasks, including image, video, and text
generation. On the contrary, the joint generation of multimodal data through
diffusion models is still in the early stages of exploration. Existing
approaches heavily rely on external preprocessing protocols, such as tokenizers
and variational autoencoders, to harmonize varied data representations into a
unified, unimodal format. This process heavily demands the high accuracy of
encoders and decoders, which can be problematic for applications with limited
data. To lift this restriction, we propose a novel framework for building
multimodal diffusion models on arbitrary state spaces, enabling native
generation of coupled data across different modalities. By introducing an
innovative decoupled noise schedule for each modality, we enable both
unconditional and modality-conditioned generation within a single model
simultaneously. We empirically validate our approach for text-image generation
and mixed-type tabular data synthesis, demonstrating that it achieves
competitive performance.

</details>


### [254] [Generative Modeling of Weights: Generalization or Memorization?](https://arxiv.org/abs/2506.07998)
*Boya Zeng,Yida Yin,Zhiqiu Xu,Zhuang Liu*

Main category: cs.LG

TL;DR: 研究发现当前生成模型在合成神经网络权重时主要依赖记忆训练数据，而非生成新颖权重，且性能不及简单基线方法。


<details>
  <summary>Details</summary>
Motivation: 探索生成模型在合成高性能神经网络权重方面的能力，并评估其是否能够生成新颖权重。

Method: 对四种代表性方法进行测试，分析其生成权重的特性，并与简单基线方法（如添加噪声或权重集成）进行比较。

Result: 现有方法主要通过记忆训练数据生成权重，无法超越简单基线方法，且通过常见缓解记忆的策略无效。

Conclusion: 研究揭示了生成模型在新领域的局限性，强调需要更谨慎的评估方法。

Abstract: Generative models, with their success in image and video generation, have
recently been explored for synthesizing effective neural network weights. These
approaches take trained neural network checkpoints as training data, and aim to
generate high-performing neural network weights during inference. In this work,
we examine four representative methods on their ability to generate novel model
weights, i.e., weights that are different from the checkpoints seen during
training. Surprisingly, we find that these methods synthesize weights largely
by memorization: they produce either replicas, or at best simple
interpolations, of the training checkpoints. Current methods fail to outperform
simple baselines, such as adding noise to the weights or taking a simple weight
ensemble, in obtaining different and simultaneously high-performing models. We
further show that this memorization cannot be effectively mitigated by
modifying modeling factors commonly associated with memorization in image
diffusion models, or applying data augmentations. Our findings provide a
realistic assessment of what types of data current generative models can model,
and highlight the need for more careful evaluation of generative models in new
domains. Our code is available at
https://github.com/boyazeng/weight_memorization.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [255] [Contextual Experience Replay for Self-Improvement of Language Agents](https://arxiv.org/abs/2506.06698)
*Yitao Liu,Chenglei Si,Karthik Narasimhan,Shunyu Yao*

Main category: cs.AI

TL;DR: 论文提出了一种无需训练的框架CER，通过动态记忆缓冲积累和综合过去经验，提升语言模型代理在复杂任务中的适应性和表现。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型代理在复杂任务中缺乏环境特定经验，且无法在推理时持续学习，限制了其表现。

Method: 提出Contextual Experience Replay (CER)框架，通过动态记忆缓冲积累和综合环境动态及决策模式的经验。

Result: 在WebArena和VisualWebArena基准测试中，CER分别实现了36.7%和31.9%的成功率，相对GPT-4o基线提升了51.0%。

Conclusion: CER框架有效提升了语言模型代理的适应性和任务表现，证明了其高效性和有效性。

Abstract: Large language model (LLM) agents have been applied to sequential
decision-making tasks such as web navigation, but without any
environment-specific experiences, they often fail in these complex tasks.
Moreover, current LLM agents are not designed to continually learn from past
experiences during inference time, which could be crucial for them to gain
these environment-specific experiences. To address this, we propose Contextual
Experience Replay (CER), a training-free framework to enable efficient
self-improvement for language agents in their context window. Specifically, CER
accumulates and synthesizes past experiences into a dynamic memory buffer.
These experiences encompass environment dynamics and common decision-making
patterns, allowing the agents to retrieve and augment themselves with relevant
knowledge in new tasks, enhancing their adaptability in complex environments.
We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On
VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena,
CER also gets a competitive average success rate of 36.7%, relatively improving
the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a
comprehensive analysis on it to prove its efficiency, validity and understand
it better.

</details>


### [256] [VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs](https://arxiv.org/abs/2506.06727)
*Can Li,Ting Zhang,Mei Wang,Hua Huang*

Main category: cs.AI

TL;DR: VisioMath是一个评估多模态数学推理能力的数据集，针对图像选项的数学问题设计，现有大型多模态模型在此任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型（LMMs）在图像选项的数学推理能力上研究不足，VisioMath填补了这一空白。

Method: 提出VisioMath数据集，包含8,070张图像和1,800道多选题，答案选项均为图像，用于评估LMMs在图像选项数学推理中的表现。

Result: 现有先进模型（如GPT-4o）在VisioMath上准确率仅为45.9%，显示其在视觉相似选项推理上的局限性。

Conclusion: VisioMath为多模态推理研究提供了严格测试基准，推动未来技术进步。

Abstract: Large Multimodal Models (LMMs) have demonstrated remarkable problem-solving
capabilities across various domains. However, their ability to perform
mathematical reasoning when answer options are represented as images--an
essential aspect of multi-image comprehension--remains underexplored. To bridge
this gap, we introduce VisioMath, a benchmark designed to evaluate mathematical
reasoning in multimodal contexts involving image-based answer choices.
VisioMath comprises 8,070 images and 1,800 multiple-choice questions, where
each answer option is an image, presenting unique challenges to existing LMMs.
To the best of our knowledge, VisioMath is the first dataset specifically
tailored for mathematical reasoning in image-based-option scenarios, where
fine-grained distinctions between answer choices are critical for accurate
problem-solving. We systematically evaluate state-of-the-art LMMs on VisioMath
and find that even the most advanced models struggle with this task. Notably,
GPT-4o achieves only 45.9% accuracy, underscoring the limitations of current
models in reasoning over visually similar answer choices. By addressing a
crucial gap in existing benchmarks, VisioMath establishes a rigorous testbed
for future research, driving advancements in multimodal reasoning.

</details>


### [257] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
*Akash Gupta,Amos Storkey,Mirella Lapata*

Main category: cs.AI

TL;DR: 论文提出了一种元学习方法，通过从任务相关图像特征中提取软提示，提升小型LMM在少样本任务中的表现，优于传统ICL方法。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMM）在少样本任务中依赖上下文学习（ICL），但性能不稳定且不随示例增加单调提升，推测原因是图像嵌入中的冗余信息干扰。

Method: 提出元学习方法，通过注意力映射模块和软提示蒸馏任务相关特征，结合LLaVA v1.5架构，实现少样本任务适应。

Result: 在VL-ICL Bench上评估，该方法优于ICL和其他提示调优方法，即使在图像扰动下也能提升视觉问答任务的表现。

Conclusion: 该方法有效解决了LMM在少样本任务中的性能不稳定问题，为任务适应提供了新思路。

Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to
perform new tasks with minimal supervision. However, ICL performance,
especially in smaller LMMs, is inconsistent and does not always improve
monotonically with increasing examples. We hypothesize that this occurs due to
the LMM being overwhelmed by additional information present in the image
embeddings, which is not required for the downstream task. To address this, we
propose a meta-learning approach that provides an alternative for inducing
few-shot capabilities in LMMs, using a fixed set of soft prompts that are
distilled from task-relevant image features and can be adapted at test time
using a few examples. To facilitate this distillation, we introduce an
attention-mapper module that can be easily integrated with the popular LLaVA
v1.5 architecture and is jointly learned with soft prompts, enabling task
adaptation in LMMs under low-data regimes with just a few gradient steps.
Evaluation on the VL-ICL Bench shows that our method consistently outperforms
ICL and related prompt-tuning approaches, even under image perturbations,
improving task induction and reasoning across visual question answering tasks.

</details>


### [258] [Long-Tailed Learning for Generalized Category Discovery](https://arxiv.org/abs/2506.06965)
*Cuong Manh Hoang*

Main category: cs.AI

TL;DR: 论文提出了一种新框架，用于在长尾分布中进行广义类别发现（GCD），通过自引导标记技术和表示平衡过程提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在平衡分布的人工数据集上表现良好，但在现实世界的不平衡数据集中效果不佳，因此需要解决这一问题。

Method: 采用自引导标记技术生成伪标签以减少偏差，并通过表示平衡过程挖掘样本邻域以提升尾部类别的关注度。

Result: 实验表明，该框架在公共数据集上优于现有最先进方法。

Conclusion: 提出的方法有效解决了长尾分布中的广义类别发现问题，性能显著提升。

Abstract: Generalized Category Discovery (GCD) utilizes labeled samples of known
classes to discover novel classes in unlabeled samples. Existing methods show
effective performance on artificial datasets with balanced distributions.
However, real-world datasets are always imbalanced, significantly affecting the
effectiveness of these methods. To solve this problem, we propose a novel
framework that performs generalized category discovery in long-tailed
distributions. We first present a self-guided labeling technique that uses a
learnable distribution to generate pseudo-labels, resulting in less biased
classifiers. We then introduce a representation balancing process to derive
discriminative representations. By mining sample neighborhoods, this process
encourages the model to focus more on tail classes. We conduct experiments on
public datasets to demonstrate the effectiveness of the proposed framework. The
results show that our model exceeds previous state-of-the-art methods.

</details>


### [259] [Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images](https://arxiv.org/abs/2506.07184)
*Liangliang You,Junchi Yao,Shu Yang,Guimin Hu,Lijie Hu,Di Wang*

Main category: cs.AI

TL;DR: 本文提出了一种名为SHE的轻量级框架，用于检测和缓解多模态大语言模型中的行为幻觉问题，并提出了新的评估指标BEACH。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在广泛领域应用中存在幻觉问题，尤其是行为幻觉尚未被充分研究。本文旨在填补这一空白。

Method: SHE框架通过两阶段方法（视觉-文本对齐检查和正交投影）检测和缓解行为幻觉，并提出自适应时间窗口和BEACH指标。

Result: 实验表明，SHE在BEACH指标上减少了超过10%的行为幻觉，同时保持了描述准确性。

Conclusion: SHE框架有效解决了行为幻觉问题，为多模态模型的可靠性提供了新思路。

Abstract: While multimodal large language models excel at various tasks, they still
suffer from hallucinations, which limit their reliability and scalability for
broader domain applications. To address this issue, recent research mainly
focuses on objective hallucination. However, for sequential images, besides
objective hallucination, there is also behavioral hallucination, which is less
studied. This work aims to fill in the gap. We first reveal that behavioral
hallucinations mainly arise from two key factors: prior-driven bias and the
snowball effect. Based on these observations, we introduce SHE (Sequence
Hallucination Eradication), a lightweight, two-stage framework that (1) detects
hallucinations via visual-textual alignment check using our proposed adaptive
temporal window and (2) mitigates them via orthogonal projection onto the joint
embedding space. We also propose a new metric (BEACH) to quantify behavioral
hallucination severity. Empirical results on standard benchmarks demonstrate
that SHE reduces behavioral hallucination by over 10% on BEACH while
maintaining descriptive accuracy.

</details>


### [260] [Reinforcing Multimodal Understanding and Generation with Dual Self-rewards](https://arxiv.org/abs/2506.07963)
*Jixiang Hong,Yiran Zhang,Guanzhong Wang,Yi Liu,Ji-Rong Wen,Rui Yan*

Main category: cs.AI

TL;DR: 本文提出了一种自监督的双奖励机制，通过理解与生成的逆对偶任务，提升大型多模态模型的性能，无需外部监督。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）在图像-文本对齐方面表现不佳，现有方法依赖外部监督且仅解决单向任务。

Method: 引入自监督双奖励机制，通过采样多输出并反转输入-输出对计算对偶似然作为自奖励进行优化。

Result: 实验表明，该方法显著提升了模型在视觉理解和生成任务中的性能，尤其在文本到图像任务中表现突出。

Conclusion: 自监督双奖励机制有效提升了LMMs的理解与生成能力，无需外部监督。

Abstract: Building upon large language models (LLMs), recent large multimodal models
(LMMs) unify cross-model understanding and generation into a single framework.
However, LMMs still struggle to achieve accurate image-text alignment, prone to
generating text responses contradicting the visual input or failing to follow
the text-to-image prompts. Current solutions require external supervision
(e.g., human feedback or reward models) and only address unidirectional
tasks-either understanding or generation. In this work, based on the
observation that understanding and generation are inverse dual tasks, we
introduce a self-supervised dual reward mechanism to reinforce the
understanding and generation capabilities of LMMs. Specifically, we sample
multiple outputs for a given input in one task domain, then reverse the
input-output pairs to compute the dual likelihood of the model as self-rewards
for optimization. Extensive experimental results on visual understanding and
generation benchmarks demonstrate that our method can effectively enhance the
performance of the model without any external supervision, especially achieving
remarkable improvements in text-to-image tasks.

</details>


### [261] [GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior](https://arxiv.org/abs/2506.08012)
*Penghao Wu,Shengnan Ma,Bo Wang,Jiaheng Yu,Lewei Lu,Ziwei Liu*

Main category: cs.AI

TL;DR: GUI-Reflection框架通过自动生成数据和多阶段训练，为多模态GUI模型引入自我反思和错误纠正能力。


<details>
  <summary>Details</summary>
Motivation: 现有GUI模型依赖无错误的离线轨迹，缺乏反思和错误恢复能力。

Method: 提出GUI-Reflection框架，包括预训练、离线微调和在线反思调优三个阶段，自动生成反思数据并设计任务套件。

Result: 模型具备自我反思和错误纠正能力，提升了GUI自动化的鲁棒性和适应性。

Conclusion: 框架为更智能的GUI自动化铺平道路，相关数据和工具将公开。

Abstract: Multimodal Large Language Models (MLLMs) have shown great potential in
revolutionizing Graphical User Interface (GUI) automation. However, existing
GUI models mostly rely on learning from nearly error-free offline trajectories,
thus lacking reflection and error recovery capabilities. To bridge this gap, we
propose GUI-Reflection, a novel framework that explicitly integrates
self-reflection and error correction capabilities into end-to-end multimodal
GUI models throughout dedicated training stages: GUI-specific pre-training,
offline supervised fine-tuning (SFT), and online reflection tuning.
GUI-reflection enables self-reflection behavior emergence with fully automated
data generation and learning processes without requiring any human annotation.
Specifically, 1) we first propose scalable data pipelines to automatically
construct reflection and error correction data from existing successful
trajectories. While existing GUI models mainly focus on grounding and UI
understanding ability, we propose the GUI-Reflection Task Suite to learn and
evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a
diverse and efficient environment for online training and data collection of
GUI models on mobile devices. 3) We also present an iterative online reflection
tuning algorithm leveraging the proposed environment, enabling the model to
continuously enhance its reflection and error correction abilities. Our
framework equips GUI agents with self-reflection and correction capabilities,
paving the way for more robust, adaptable, and intelligent GUI automation, with
all data, models, environments, and tools to be released publicly.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [262] [LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment](https://arxiv.org/abs/2506.06355)
*Lingyao Li,Dawei Li,Zhenhui Ou,Xiaoran Xu,Jingxiao Liu,Zihui Ma,Runlong Yu,Min Deng*

Main category: cs.CY

TL;DR: 该研究利用大型语言模型（LLMs）模拟地震影响，结合多模态数据预测地震烈度，结果显示与真实报告高度一致。


<details>
  <summary>Details</summary>
Motivation: 提高对突发性灾害（如地震）的主动准备能力，利用LLMs模拟复杂场景。

Method: 结合地理空间、社会经济、建筑和街景等多模态数据，生成地震烈度预测，并通过RAG和ICL技术优化性能。

Result: 在2014年纳帕和2019年里奇克雷斯特地震中，预测与实际报告高度一致（相关系数0.88，RMSE 0.77）。

Conclusion: LLMs在模拟灾害影响方面具有潜力，有助于加强事前规划。

Abstract: Efficient simulation is essential for enhancing proactive preparedness for
sudden-onset disasters such as earthquakes. Recent advancements in large
language models (LLMs) as world models show promise in simulating complex
scenarios. This study examines multiple LLMs to proactively estimate perceived
earthquake impacts. Leveraging multimodal datasets including geospatial,
socioeconomic, building, and street-level imagery data, our framework generates
Modified Mercalli Intensity (MMI) predictions at zip code and county scales.
Evaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS ''Did
You Feel It? (DYFI)'' reports demonstrate significant alignment, as evidenced
by a high correlation of 0.88 and a low RMSE of 0.77 as compared to real
reports at the zip code level. Techniques such as RAG and ICL can improve
simulation performance, while visual inputs notably enhance accuracy compared
to structured numerical data alone. These findings show the promise of LLMs in
simulating disaster impacts that can help strengthen pre-event planning.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [263] [Enhancing Contrastive Learning-based Electrocardiogram Pretrained Model with Patient Memory Queue](https://arxiv.org/abs/2506.06310)
*Xiaoyu Sun,Yang Yang,Xunde Dong*

Main category: eess.SP

TL;DR: 论文提出了一种基于患者记忆队列（PMQ）的对比学习ECG预训练模型，通过增加患者样本数量和数据增强方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 在ECG自动诊断领域，标记数据有限，如何利用未标记数据构建鲁棒的预训练模型是关键。现有方法未能充分利用患者一致性。

Method: 提出PMQ模型，通过患者记忆队列缓解样本不足问题，并引入两种数据增强方法优化对比学习。

Result: 在三个公开数据集上的实验表明，该方法性能优于现有对比学习方法，且在标记数据有限时更具鲁棒性。

Conclusion: PMQ模型有效提升了ECG预训练的性能和鲁棒性，代码已开源。

Abstract: In the field of automatic Electrocardiogram (ECG) diagnosis, due to the
relatively limited amount of labeled data, how to build a robust ECG pretrained
model based on unlabeled data is a key area of focus for researchers. Recent
advancements in contrastive learning-based ECG pretrained models highlight the
potential of exploiting the additional patient-level self-supervisory signals
inherent in ECG. They are referred to as patient contrastive learning. Its
rationale is that multiple physical recordings from the same patient may share
commonalities, termed patient consistency, so redefining positive and negative
pairs in contrastive learning as intrapatient and inter-patient samples
provides more shared context to learn an effective representation. However,
these methods still fail to efficiently exploit patient consistency due to the
insufficient amount of intra-inter patient samples existing in a batch. Hence,
we propose a contrastive learning-based ECG pretrained model enhanced by the
Patient Memory Queue (PMQ), which incorporates a large patient memory queue to
mitigate model degeneration that can arise from insufficient intra-inter
patient samples. In order to further enhance the performance of the pretrained
model, we introduce two extra data augmentation methods to provide more
perspectives of positive and negative pairs for pretraining. Extensive
experiments were conducted on three public datasets with three different data
ratios. The experimental results show that the comprehensive performance of our
method outperforms previous contrastive learning methods and exhibits greater
robustness in scenarios with limited labeled data. The code is available at
https://github.com/3hiuwoo/PMQ.

</details>


### [264] [Benchmarking Early Agitation Prediction in Community-Dwelling People with Dementia Using Multimodal Sensors and Machine Learning](https://arxiv.org/abs/2506.06306)
*Ali Abedi,Charlene H. Chu,Shehroz S. Khan*

Main category: eess.SP

TL;DR: 该研究开发并比较了多种机器学习方法，用于通过多模态传感器数据早期预测社区居住的老年痴呆患者的躁动行为，并引入了一组新的与躁动相关的上下文特征。


<details>
  <summary>Details</summary>
Motivation: 老年痴呆患者的躁动行为常见且影响生活质量，早期预测可减轻护理负担并改善患者和护理者的生活质量。

Method: 研究使用了多模态传感器数据，评估了多种机器学习和深度学习模型，包括二进制分类和异常检测，并引入了新的上下文特征。

Result: 最佳模型（轻梯度提升机）在预测躁动时表现出色，AUC-ROC为0.9720，AUC-PR为0.4320。

Conclusion: 该研究为基于隐私保护传感器数据的躁动预测提供了首个全面基准，支持主动护理和居家养老。

Abstract: Agitation is one of the most common responsive behaviors in people living
with dementia, particularly among those residing in community settings without
continuous clinical supervision. Timely prediction of agitation can enable
early intervention, reduce caregiver burden, and improve the quality of life
for both patients and caregivers. This study aimed to develop and benchmark
machine learning approaches for the early prediction of agitation in
community-dwelling older adults with dementia using multimodal sensor data. A
new set of agitation-related contextual features derived from activity data was
introduced and employed for agitation prediction. A wide range of machine
learning and deep learning models was evaluated across multiple problem
formulations, including binary classification for single-timestamp tabular
sensor data and multi-timestamp sequential sensor data, as well as anomaly
detection for single-timestamp tabular sensor data. The study utilized the
Technology Integrated Health Management (TIHM) dataset, the largest publicly
available dataset for remote monitoring of people living with dementia,
comprising 2,803 days of in-home activity, physiology, and sleep data. The most
effective setting involved binary classification of sensor data using the
current 6-hour timestamp to predict agitation at the subsequent timestamp.
Incorporating additional information, such as time of day and agitation
history, further improved model performance, with the highest AUC-ROC of 0.9720
and AUC-PR of 0.4320 achieved by the light gradient boosting machine. This work
presents the first comprehensive benchmarking of state-of-the-art techniques
for agitation prediction in community-based dementia care using
privacy-preserving sensor data. The approach enables accurate, explainable, and
efficient agitation prediction, supporting proactive dementia care and aging in
place.

</details>


### [265] [An Open-Source Python Framework and Synthetic ECG Image Datasets for Digitization, Lead and Lead Name Detection, and Overlapping Signal Segmentation](https://arxiv.org/abs/2506.06315)
*Masoud Rahimi,Reza Karbasi,Abdol-Hossein Vahabie*

Main category: eess.SP

TL;DR: 开源Python框架生成合成ECG图像数据集，支持ECG数字化、导联区域与名称检测及波形分割等深度学习任务。


<details>
  <summary>Details</summary>
Motivation: 推动ECG分析中的深度学习任务，如ECG数字化、导联检测和波形分割。

Method: 基于PTB-XL信号数据集，生成四种开放数据集，包括配对的ECG图像与时间序列信号、YOLO格式标注的导联区域与名称、单导联分割掩码（含重叠版本）。

Result: 提供四种公开数据集，支持多种ECG分析任务，框架与数据集已开源。

Conclusion: 该框架与数据集为ECG分析的深度学习研究提供了实用资源。

Abstract: We introduce an open-source Python framework for generating synthetic ECG
image datasets to advance critical deep learning-based tasks in ECG analysis,
including ECG digitization, lead region and lead name detection, and
pixel-level waveform segmentation. Using the PTB-XL signal dataset, our
proposed framework produces four open-access datasets: (1) ECG images in
various lead configurations paired with time-series signals for ECG
digitization, (2) ECG images annotated with YOLO-format bounding boxes for
detection of lead region and lead name, (3)-(4) cropped single-lead images with
segmentation masks compatible with U-Net-based models in normal and overlapping
versions. In the overlapping case, waveforms from neighboring leads are
superimposed onto the target lead image, while the segmentation masks remain
clean. The open-source Python framework and datasets are publicly available at
https://github.com/rezakarbasi/ecg-image-and-signal-dataset and
https://doi.org/10.5281/zenodo.15484519, respectively.

</details>


### [266] [Heart Rate Classification in ECG Signals Using Machine Learning and Deep Learning](https://arxiv.org/abs/2506.06349)
*Thien Nhan Vo,Thanh Xuan Truong*

Main category: eess.SP

TL;DR: 研究比较了传统机器学习（手工特征）和深度学习（ECG图像）在心电图分类中的表现，发现LightGBM模型表现最佳，准确率达99%。


<details>
  <summary>Details</summary>
Motivation: 解决心电图信号中心跳分类的问题，比较手工特征和图像转换两种方法的优劣。

Method: 1. 传统机器学习：提取HRV、均值、方差等特征，训练SVM、Random Forest等模型。2. 深度学习：将ECG信号转换为GAF、MTF等图像，用CNN分类。

Result: LightGBM表现最优（准确率99%，F1分数0.94），优于图像方法（F1分数0.85）。SVM和AdaBoost表现较差。

Conclusion: 手工特征能更好地捕捉ECG信号的时空变化，未来可结合多导联信号和时序依赖提升分类效果。

Abstract: This study addresses the classification of heartbeats from ECG signals
through two distinct approaches: traditional machine learning utilizing
hand-crafted features and deep learning via transformed images of ECG beats.
The dataset underwent preprocessing steps, including downsampling, filtering,
and normalization, to ensure consistency and relevance for subsequent analysis.
In the first approach, features such as heart rate variability (HRV), mean,
variance, and RR intervals were extracted to train various classifiers,
including SVM, Random Forest, AdaBoost, LSTM, Bi-directional LSTM, and
LightGBM. The second approach involved transforming ECG signals into images
using Gramian Angular Field (GAF), Markov Transition Field (MTF), and
Recurrence Plots (RP), with these images subsequently classified using CNN
architectures like VGG and Inception.
  Experimental results demonstrate that the LightGBM model achieved the highest
performance, with an accuracy of 99% and an F1 score of 0.94, outperforming the
image-based CNN approach (F1 score of 0.85). Models such as SVM and AdaBoost
yielded significantly lower scores, indicating limited suitability for this
task. The findings underscore the superior ability of hand-crafted features to
capture temporal and morphological variations in ECG signals compared to
image-based representations of individual beats. Future investigations may
benefit from incorporating multi-lead ECG signals and temporal dependencies
across successive beats to enhance classification accuracy further.

</details>
