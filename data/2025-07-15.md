<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 160]
- [eess.IV](#eess.IV) [Total: 17]
- [cs.GR](#cs.GR) [Total: 6]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 4]
- [eess.AS](#eess.AS) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [physics.bio-ph](#physics.bio-ph) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.LG](#cs.LG) [Total: 12]
- [cs.MM](#cs.MM) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [From images to properties: a NeRF-driven framework for granular material parameter inversion](https://arxiv.org/abs/2507.09005)
*Cheng-Hsi Hsiao,Krishna Kumar*

Main category: cs.CV

TL;DR: 提出了一种结合NeRF和MPM的新框架，通过视觉观测推断颗粒材料特性，摩擦角估计误差在2度以内。


<details>
  <summary>Details</summary>
Motivation: 解决在直接测量不可行的情况下，如何通过视觉观测表征颗粒材料特性的问题。

Method: 生成合成实验数据，利用NeRF重建3D几何，初始化MPM模拟，通过贝叶斯优化最小化图像损失估计摩擦角。

Result: 摩擦角估计误差在2度以内，验证了方法的有效性。

Conclusion: 该方法为颗粒材料特性表征提供了一种可行的视觉解决方案。

Abstract: We introduce a novel framework that integrates Neural Radiance Fields (NeRF)
with Material Point Method (MPM) simulation to infer granular material
properties from visual observations. Our approach begins by generating
synthetic experimental data, simulating an plow interacting with sand. The
experiment is rendered into realistic images as the photographic observations.
These observations include multi-view images of the experiment's initial state
and time-sequenced images from two fixed cameras. Using NeRF, we reconstruct
the 3D geometry from the initial multi-view images, leveraging its capability
to synthesize novel viewpoints and capture intricate surface details. The
reconstructed geometry is then used to initialize material point positions for
the MPM simulation, where the friction angle remains unknown. We render images
of the simulation under the same camera setup and compare them to the observed
images. By employing Bayesian optimization, we minimize the image loss to
estimate the best-fitting friction angle. Our results demonstrate that friction
angle can be estimated with an error within 2 degrees, highlighting the
effectiveness of inverse analysis through purely visual observations. This
approach offers a promising solution for characterizing granular materials in
real-world scenarios where direct measurement is impractical or impossible.

</details>


### [2] [View Invariant Learning for Vision-Language Navigation in Continuous Environments](https://arxiv.org/abs/2507.08831)
*Josh Qixuan Sun,Xiaoying Xing,Huaiyuan Weng,Chul Min Yeum,Mark Crowley*

Main category: cs.CV

TL;DR: 论文提出V2-VLNCE（多视角VLNCE）和VIL（视角不变学习），通过对比学习和师生框架增强导航策略对视角变化的鲁棒性，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有导航策略对视角变化敏感的问题，提升在连续环境中的视觉语言导航性能。

Method: 提出VIL策略，采用对比学习框架学习稀疏且视角不变的特征，并引入师生框架优化Waypoint Predictor Module。

Result: 在V2-VLNCE上性能提升8-15%，在标准VLNCE设置下也有改进，且在RxR-CE数据集上达到最优。

Conclusion: VIL是一种即插即用的后训练方法，能显著提升导航策略的鲁棒性和性能。

Abstract: Vision-Language Navigation in Continuous Environments (VLNCE), where an agent
follows instructions and moves freely to reach a destination, is a key research
problem in embodied AI. However, most navigation policies are sensitive to
viewpoint changes, i.e., variations in camera height and viewing angle that
alter the agent's observation. In this paper, we introduce a generalized
scenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View
Invariant Learning), a view-invariant post-training strategy that enhances the
robustness of existing navigation policies to changes in camera viewpoint. VIL
employs a contrastive learning framework to learn sparse and view-invariant
features. Additionally, we introduce a teacher-student framework for the
Waypoint Predictor Module, a core component of most VLNCE baselines, where a
view-dependent teacher model distills knowledge into a view-invariant student
model. We employ an end-to-end training paradigm to jointly optimize these
components, thus eliminating the cost for individual module training. Empirical
results show that our method outperforms state-of-the-art approaches on
V2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets
R2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE
setting and find that, despite being trained for varied viewpoints, it often
still improves performance. On the more challenging RxR-CE dataset, our method
also achieved state-of-the-art performance across all metrics when compared to
other map-free methods. This suggests that adding VIL does not diminish the
standard viewpoint performance and can serve as a plug-and-play post-training
method.

</details>


### [3] [Detecting Deepfake Talking Heads from Facial Biometric Anomalies](https://arxiv.org/abs/2507.08917)
*Justin D. Norman,Hany Farid*

Main category: cs.CV

TL;DR: 提出了一种基于面部生物特征异常模式的深度学习检测技术，用于识别深度伪造视频。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术被广泛用于欺诈、诈骗和政治虚假信息，亟需有效的检测方法。

Method: 利用面部生物特征中的异常模式，开发了一种新型机器学习技术。

Result: 在大规模深度伪造数据集上评估了该技术，并测试了其对视频篡改的鲁棒性和泛化能力。

Conclusion: 该技术能有效检测深度伪造视频，并具备一定的泛化性和鲁棒性。

Abstract: The combination of highly realistic voice cloning, along with visually
compelling avatar, face-swap, or lip-sync deepfake video generation, makes it
relatively easy to create a video of anyone saying anything. Today, such
deepfake impersonations are often used to power frauds, scams, and political
disinformation. We propose a novel forensic machine learning technique for the
detection of deepfake video impersonations that leverages unnatural patterns in
facial biometrics. We evaluate this technique across a large dataset of
deepfake techniques and impersonations, as well as assess its reliability to
video laundering and its generalization to previously unseen video deepfake
generators.

</details>


### [4] [PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection](https://arxiv.org/abs/2507.08979)
*Mahdiyar Molahasani,Azadeh Motamedi,Michael Greenspan,Il-Min Kim,Ali Etemad*

Main category: cs.CV

TL;DR: PRISM是一种无需数据和任务无关的方法，用于减少视觉语言模型（如CLIP）中的隐含偏差。它通过生成场景描述和对比式去偏损失来最小化虚假相关性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）常因训练数据中的偏差而产生偏斜预测，PRISM旨在无需预定义偏差类别或额外数据的情况下解决这一问题。

Method: PRISM分两阶段：1）用LLM生成包含虚假相关性的场景描述；2）使用对比式去偏损失学习投影，最小化虚假相关性并保持图像与文本嵌入的对齐。

Result: PRISM在Waterbirds和CelebA数据集上优于现有去偏方法。

Conclusion: PRISM提供了一种有效的数据无关和任务无关的去偏解决方案，代码已公开。

Abstract: We introduce Projection-based Reduction of Implicit Spurious bias in
vision-language Models (PRISM), a new data-free and task-agnostic solution for
bias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in
their training data, leading to skewed predictions. PRISM is designed to debias
VLMs without relying on predefined bias categories or additional external data.
It operates in two stages: first, an LLM is prompted with simple class prompts
to generate scene descriptions that contain spurious correlations. Next, PRISM
uses our novel contrastive-style debiasing loss to learn a projection that maps
the embeddings onto a latent space that minimizes spurious correlations while
preserving the alignment between image and text embeddings.Extensive
experiments demonstrate that PRISM outperforms current debiasing methods on the
commonly used Waterbirds and CelebA datasets We make our code public at:
https://github.com/MahdiyarMM/PRISM.

</details>


### [5] [Video Inference for Human Mesh Recovery with Vision Transformer](https://arxiv.org/abs/2507.08981)
*Hanbyel Cho,Jaesung Ahn,Yooshin Cho,Junmo Kim*

Main category: cs.CV

TL;DR: 提出了一种结合时空和运动学信息的人体网格恢复方法HMR-ViT，通过构建时空-运动学特征图像并使用Vision Transformer进行编码，实现了竞争性的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的人体网格恢复方法仅利用时空信息或运动学关系，缺乏同时利用两者的方法，因此提出HMR-ViT以填补这一空白。

Method: 构建时空-运动学特征图像，使用通道重排矩阵（CRM）使相似运动学特征空间相邻，再通过Vision Transformer和回归网络推断SMPL参数。

Result: 在3DPW和Human3.6M数据集上表现出竞争性性能。

Conclusion: HMR-ViT通过同时利用时空和运动学信息，提升了人体网格恢复的准确性。

Abstract: Human Mesh Recovery (HMR) from an image is a challenging problem because of
the inherent ambiguity of the task. Existing HMR methods utilized either
temporal information or kinematic relationships to achieve higher accuracy, but
there is no method using both. Hence, we propose "Video Inference for Human
Mesh Recovery with Vision Transformer (HMR-ViT)" that can take into account
both temporal and kinematic information. In HMR-ViT, a Temporal-kinematic
Feature Image is constructed using feature vectors obtained from video frames
by an image encoder. When generating the feature image, we use a Channel
Rearranging Matrix (CRM) so that similar kinematic features could be located
spatially close together. The feature image is then further encoded using
Vision Transformer, and the SMPL pose and shape parameters are finally inferred
using a regression network. Extensive evaluation on the 3DPW and Human3.6M
datasets indicates that our method achieves a competitive performance in HMR.

</details>


### [6] [VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels](https://arxiv.org/abs/2507.09008)
*Xiwei Xuan,Xiaoqi Wang,Wenbin He,Jorge Piazentin Ono,Liang Gou,Kwan-Liu Ma,Liu Ren*

Main category: cs.CV

TL;DR: VISTA是一个视觉分析框架，旨在提升多模态基础模型生成标签的质量，通过结合多阶段数据验证策略和人类专业知识，解决现有方法在数据质量评估上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注数据量而非质量，缺乏对多模态基础模型生成标签质量的全面评估，导致在无真实标签的大规模数据验证中面临挑战。

Method: VISTA框架整合多阶段数据验证策略与人类专业知识，帮助识别和修正标签中的隐藏问题。

Result: 在开放词汇图像分割领域的两个基准数据集上，VISTA通过定量和定性分析证明了其有效性。

Conclusion: VISTA通过提升数据质量，显著增强了多模态模型的性能，为类似任务提供了可行的解决方案。

Abstract: The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)
have facilitated the auto-labeling of large-scale datasets, enhancing model
performance in challenging downstream tasks such as open-vocabulary object
detection and segmentation. However, the quality of FM-generated labels is less
studied as existing approaches focus more on data quantity over quality. This
is because validating large volumes of data without ground truth presents a
considerable challenge in practice. Existing methods typically rely on limited
metrics to identify problematic data, lacking a comprehensive perspective, or
apply human validation to only a small data fraction, failing to address the
full spectrum of potential issues. To overcome these challenges, we introduce
VISTA, a visual analytics framework that improves data quality to enhance the
performance of multi-modal models. Targeting the complex and demanding domain
of open-vocabulary image segmentation, VISTA integrates multi-phased data
validation strategies with human expertise, enabling humans to identify,
understand, and correct hidden issues within FM-generated labels. Through
detailed use cases on two benchmark datasets and expert reviews, we demonstrate
VISTA's effectiveness from both quantitative and qualitative perspectives.

</details>


### [7] [BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis](https://arxiv.org/abs/2507.09036)
*Florian Kofler,Marcel Rosier,Mehdi Astaraki,Hendrik Möller,Ilhem Isra Mekki,Josef A. Buchner,Anton Schmick,Arianna Pfiffer,Eva Oswald,Lucas Zimmer,Ezequiel de la Rosa,Sarthak Pati,Julian Canisius,Arianna Piffer,Ujjwal Baid,Mahyar Valizadeh,Akis Linardos,Jan C. Peeken,Surprosanna Shit,Felix Steinbauer,Daniel Rueckert,Rolf Heckemann,Spyridon Bakas,Jan Kirschke,Constantin von See,Ivan Ezhov,Marie Piraud,Benedikt Wiestler,Bjoern Menze*

Main category: cs.CV

TL;DR: BrainLesion Suite是一个Python工具包，用于构建模块化的脑部病变图像分析流程，简化开发过程并提供灵活的预处理功能。


<details>
  <summary>Details</summary>
Motivation: 旨在为临床和科研实践提供一种高效、低认知负担的工具，支持多模态图像处理和病变分析。

Method: 基于Pythonic原则设计，包含预处理模块（如配准、去颅骨等），并利用BraTS挑战赛算法进行模态合成、病变修复和分割。

Result: 支持脑部病变（如胶质瘤、转移瘤和多发性硬化）的分析，并可扩展到其他生物医学图像应用。

Conclusion: BrainLesion Suite是一个功能强大且灵活的工具，适用于脑部病变图像分析，未来可扩展至更广泛的应用领域。

Abstract: BrainLesion Suite is a versatile toolkit for building modular brain lesion
image analysis pipelines in Python. Following Pythonic principles, BrainLesion
Suite is designed to provide a 'brainless' development experience, minimizing
cognitive effort and streamlining the creation of complex workflows for
clinical and scientific practice. At its core is an adaptable preprocessing
module that performs co-registration, atlas registration, and optional
skull-stripping and defacing on arbitrary multi-modal input images. BrainLesion
Suite leverages algorithms from the BraTS challenge to synthesize missing
modalities, inpaint lesions, and generate pathology-specific tumor
segmentations. BrainLesion Suite also enables quantifying segmentation model
performance, with tools such as panoptica to compute lesion-wise metrics.
Although BrainLesion Suite was originally developed for image analysis
pipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,
it can be adapted for other biomedical image analysis applications. The
individual BrainLesion Suite packages and tutorials are accessible on GitHub.

</details>


### [8] [Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?](https://arxiv.org/abs/2507.09052)
*Fang Chen,Alex Villa,Gongbo Liang,Xiaoyi Lu,Meng Tang*

Main category: cs.CV

TL;DR: 论文提出两种对比损失函数，用于解决类别不平衡数据中尾部类别图像合成多样性不足的问题，同时保持头部类别图像的质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 类别不平衡数据导致尾部类别图像合成多样性降低，影响整体性能。

Method: 引入无监督InfoNCE损失和MSE损失，通过对比学习增强尾部类别多样性。

Result: 方法在多个数据集上优于标准DDPM和其他替代方法。

Conclusion: 对比学习框架简单有效，显著提升了类别不平衡扩散模型的性能。

Abstract: Training data for class-conditional image synthesis often exhibit a
long-tailed distribution with limited images for tail classes. Such an
imbalance causes mode collapse and reduces the diversity of synthesized images
for tail classes. For class-conditional diffusion models trained on imbalanced
data, we aim to improve the diversity of tail class images without compromising
the fidelity and diversity of head class images. We achieve this by introducing
two deceptively simple but highly effective contrastive loss functions.
Firstly, we employ an unsupervised InfoNCE loss utilizing negative samples to
increase the distance/dissimilarity among synthetic images, particularly for
tail classes. To further enhance the diversity of tail classes, our second loss
is an MSE loss that contrasts class-conditional generation with unconditional
generation at large timesteps. This second loss makes the denoising process
insensitive to class conditions for the initial steps, which enriches tail
classes through knowledge sharing from head classes. Conditional-unconditional
alignment has been shown to enhance the performance of long-tailed GAN. We are
the first to adapt such alignment to diffusion models. We successfully
leveraged contrastive learning for class-imbalanced diffusion models. Our
contrastive learning framework is easy to implement and outperforms standard
DDPM and alternative methods for class-imbalanced diffusion models across
various datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and
ImageNetLT.

</details>


### [9] [Infinite Video Understanding](https://arxiv.org/abs/2507.09068)
*Dell Zhang,Xiangyu Chen,Jixiang Luo,Mengxi Jia,Changzhi Sun,Ruilong Ren,Jingren Liu,Hao Sun,Xuelong Li*

Main category: cs.CV

TL;DR: 论文探讨了当前大语言模型（LLM）和多模态扩展（MLLM）在视频理解中的局限性，尤其是处理长时间视频时的计算和记忆挑战，并提出了‘无限视频理解’作为未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管视频理解技术取得了显著进展，但处理长时间视频时仍面临计算、记忆和时序一致性的挑战，需要新的研究方向。

Method: 通过分析现有技术（如Video-XL-2、HoPE、VideoRoPE++等）的局限性，提出‘无限视频理解’作为未来目标，并探讨了相关研究方向。

Result: 当前模型在长时间视频处理中存在显著限制，需创新架构和机制以实现‘无限视频理解’。

Conclusion: ‘无限视频理解’是一个具有挑战性但重要的研究方向，将为多媒体和AI领域带来突破性进展。

Abstract: The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.

</details>


### [10] [BlindSight: Harnessing Sparsity for Efficient VLMs](https://arxiv.org/abs/2507.09071)
*Tharun Adithya Srikrishnan,Deval Shah,Steven K. Reinhardt*

Main category: cs.CV

TL;DR: BlindSight是一种无需训练的方法，通过利用注意力稀疏性优化视觉语言模型的推理，减少FLOPs 32%-41%，精度变化在-2%到+2%之间。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）因处理图像数据导致提示长度增加和注意力计算复杂度高，推理速度慢。

Method: 分析VLMs中的注意力模式，发现稀疏性，提出基于输入模板的注意力稀疏掩码BlindSight。

Result: 在Qwen2-VL等模型上测试，FLOPs减少32%-41%，精度变化小。

Conclusion: BlindSight有效优化VLM推理，显著减少计算开销，精度影响可接受。

Abstract: Large vision-language models (VLMs) enable the joint processing of text and
images. However, the inclusion of vision data significantly expands the prompt
length. Along with the quadratic complexity of the attention computation, this
results in a longer prefill duration. An approach to mitigate this bottleneck
is to leverage the inherent sparsity in the attention computation. In our
analysis of attention patterns in VLMs, we observe that a substantial portion
of layers exhibit minimal cross-image attention, except through attention-sink
tokens per image. These sparse attention patterns fall into distinct
categories: sink-only, document mask and a hybrid document-sink mask. Based on
this, we propose BlindSight: a training-free approach to optimize VLM inference
using a input template-aware attention sparsity mask. We utilize samples from a
dataset to derive a prompt-agnostic sparsity categorization for every attention
head. We evaluate the proposed technique using VLMs such as Qwen2-VL,
Qwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on
average with -2%-+2% accuracy compared to the original model in most evaluated
multi-image understanding benchmarks.

</details>


### [11] [From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion](https://arxiv.org/abs/2507.09081)
*Zhenyu Yu,Mohd Yamani Idna Idris,Hua Wang,Pei Wang,Junyi Chen,Kun Wang*

Main category: cs.CV

TL;DR: 论文综述了定量遥感反演技术的演变，从物理模型到数据驱动和基础模型方法，并展望了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着遥感系统和人工智能的发展，传统物理模型逐渐被数据驱动和基础模型取代，需要系统梳理技术演变和挑战。

Method: 系统回顾了物理模型（如PROSPECT）、机器学习（如深度学习）和基础模型（如SatMAE）的方法，比较了其假设、应用和局限。

Result: 总结了基础模型在自监督预训练、多模态融合和跨任务适应方面的进展，并指出物理可解释性、领域泛化等挑战。

Conclusion: 展望了下一代基础模型的发展方向，强调统一建模能力、跨领域泛化和物理可解释性。

Abstract: Quantitative remote sensing inversion aims to estimate continuous surface
variables-such as biomass, vegetation indices, and evapotranspiration-from
satellite observations, supporting applications in ecosystem monitoring, carbon
accounting, and land management. With the evolution of remote sensing systems
and artificial intelligence, traditional physics-based paradigms are giving way
to data-driven and foundation model (FM)-based approaches. This paper
systematically reviews the methodological evolution of inversion techniques,
from physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods
(e.g., deep learning, multimodal fusion), and further to foundation models
(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application
scenarios, and limitations of each paradigm, with emphasis on recent FM
advances in self-supervised pretraining, multi-modal integration, and
cross-task adaptation. We also highlight persistent challenges in physical
interpretability, domain generalization, limited supervision, and uncertainty
quantification. Finally, we envision the development of next-generation
foundation models for remote sensing inversion, emphasizing unified modeling
capacity, cross-domain generalization, and physical interpretability.

</details>


### [12] [Taming generative video models for zero-shot optical flow extraction](https://arxiv.org/abs/2507.09082)
*Seungwoo Kim,Khai Loong Aw,Klemen Kotar,Cristobal Eyzaguirre,Wanhee Lee,Yunong Liu,Jared Watrous,Stefan Stojanov,Juan Carlos Niebles,Jiajun Wu,Daniel L. K. Yamins*

Main category: cs.CV

TL;DR: 论文提出了一种无需微调的方法，通过扰动和KL散度从自监督视频模型中提取光流，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以利用冻结的自监督视频模型（仅用于未来帧预测）直接提取光流，避免因标签稀缺和合成数据与真实数据差距带来的微调困难。

Method: 基于CWM范式，提出KL-tracing方法，通过注入局部扰动并计算KL散度来提取光流，利用LRAS架构的特性。

Result: 在TAP-Vid DAVIS和Kubric数据集上分别实现了16.6%和4.7%的相对改进，性能优于现有方法。

Conclusion: 研究表明，通过可控生成视频模型的因果提示是一种高效且可扩展的光流提取方法，优于监督或光度损失方法。

Abstract: Extracting optical flow from videos remains a core computer vision problem.
Motivated by the success of large general-purpose models, we ask whether frozen
self-supervised video models trained only for future frame prediction can be
prompted, without fine-tuning, to output flow. Prior work reading out depth or
illumination from video generators required fine-tuning, which is impractical
for flow where labels are scarce and synthetic datasets suffer from a
sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,
which can obtain point-wise correspondences by injecting a small tracer
perturbation into a next-frame predictor and tracking its propagation, we
extend this idea to generative video models. We explore several popular
architectures and find that successful zero-shot flow extraction in this manner
is aided by three model properties: (1) distributional prediction of future
frames (avoiding blurry or noisy outputs); (2) factorized latents that treat
each spatio-temporal patch independently; and (3) random-access decoding that
can condition on any subset of future pixels. These properties are uniquely
present in the recent Local Random Access Sequence (LRAS) architecture.
Building on LRAS, we propose KL-tracing: a novel test-time procedure that
injects a localized perturbation into the first frame, rolls out the model one
step, and computes the Kullback-Leibler divergence between perturbed and
unperturbed predictive distributions. Without any flow-specific fine-tuning,
our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS
dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid
Kubric (4.7% relative improvement). Our results indicate that counterfactual
prompting of controllable generative video models is a scalable and effective
alternative to supervised or photometric-loss approaches for high-quality flow.

</details>


### [13] [MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks](https://arxiv.org/abs/2507.09092)
*Ram S Iyer,Narayan S Iyer,Rugmini Ammal P*

Main category: cs.CV

TL;DR: 本文提出了一种名为MI CAM的新型后验视觉解释方法，通过激活映射和互信息加权生成显著性可视化，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着机器视觉在医疗和自动化电厂等关键领域的应用，理解卷积神经网络的内部机制及其推理原因变得至关重要。

Method: MI CAM基于激活映射，通过互信息加权特征图，并通过线性组合生成最终结果，同时通过反事实分析验证因果解释。

Result: MI CAM在视觉性能和模型推理的公正性方面表现出色，与现有方法相当甚至在某些方面更优。

Conclusion: MI CAM为卷积神经网络提供了一种有效的视觉解释方法，具有实际应用潜力。

Abstract: With the intervention of machine vision in our crucial day to day necessities
including healthcare and automated power plants, attention has been drawn to
the internal mechanisms of convolutional neural networks, and the reason why
the network provides specific inferences. This paper proposes a novel post-hoc
visual explanation method called MI CAM based on activation mapping. Differing
from previous class activation mapping based approaches, MI CAM produces
saliency visualizations by weighing each feature map through its mutual
information with the input image and the final result is generated by a linear
combination of weights and activation maps. It also adheres to producing causal
interpretations as validated with the help of counterfactual analysis. We aim
to exhibit the visual performance and unbiased justifications for the model
inferencing procedure achieved by MI CAM. Our approach works at par with all
state-of-the-art methods but particularly outperforms some in terms of
qualitative and quantitative measures. The implementation of proposed method
can be found on https://anonymous.4open.science/r/MI-CAM-4D27

</details>


### [14] [RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze](https://arxiv.org/abs/2507.09097)
*Yunsoo Kim,Jinge Wu,Honghan Wu*

Main category: cs.CV

TL;DR: RadEyeVideo提出了一种新方法，通过整合放射科医生的眼动视频序列来提升LVLMs在胸部X光分析中的表现，显著提高了报告生成和疾病诊断的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了眼动的时序信息，而RadEyeVideo旨在捕捉眼动的时空动态，以更有效地利用专家知识。

Method: 将放射科医生的眼动数据作为视频序列输入到支持视频的LVLMs中，评估其在胸部X光报告生成和疾病诊断中的表现。

Result: 使用眼动视频后，模型性能在报告生成任务中提升24.6%，在两项任务中平均提升15.2%，甚至超越专业医学LVLMs。

Conclusion: RadEyeVideo展示了专家知识与通用LVLMs结合的巨大潜力，为医疗图像分析提供了一种可扩展的人本方法。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated promising performance
in chest X-ray (CXR) analysis. To enhance human-computer interaction, several
studies have incorporated radiologists' eye gaze, typically through heatmaps or
textual prompts. However, these methods often overlook the sequential order of
eye movements, which could provide valuable insights by highlighting both the
areas of interest and the order in which they are examined. In this work, we
propose a novel approach called RadEyeVideo that integrates radiologists'
eye-fixation data as a video sequence, capturing both the temporal and spatial
dynamics of their gaze. We evaluate this method in CXR report generation and
disease diagnosis using three general-domain, open-source LVLMs with video
input capabilities. When prompted with eye-gaze videos, model performance
improves by up to 24.6% in the report generation task and on average 15.2% for
both tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an
open-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs
such as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work
highlights that domain expert's knowledge (eye-gaze information in this case),
when effectively integrated with LVLMs, can significantly enhance
general-domain models' capabilities in clinical tasks. RadEyeVideo is a step
toward a scalable human-centered approach of utilizing LVLMs in medical image
analytics.

</details>


### [15] [Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning](https://arxiv.org/abs/2507.09102)
*Yiyang Chen,Shanshan Zhao,Lunhao Duan,Changxing Ding,Dacheng Tao*

Main category: cs.CV

TL;DR: PointSD利用Stable Diffusion模型增强3D点云自监督学习，通过点云引导去噪图像并提取特征，提升3D表示。


<details>
  <summary>Details</summary>
Motivation: 现有3D扩散模型受限于数据集规模，而文本到图像扩散模型（如Stable Diffusion）在大规模数据上表现优异，可用于改进3D学习。

Method: 提出PointSD框架，将Stable Diffusion的文本编码器替换为3D编码器，训练点云到图像的扩散模型，并提取特征对齐3D主干网络。

Result: 实验表明，PointSD能有效提升点云自监督学习性能。

Conclusion: Stable Diffusion模型可成功应用于3D点云表示学习，代码已开源。

Abstract: Diffusion-based models, widely used in text-to-image generation, have proven
effective in 2D representation learning. Recently, this framework has been
extended to 3D self-supervised learning by constructing a conditional point
generator for enhancing 3D representations. However, its performance remains
constrained by the 3D diffusion model, which is trained on the available 3D
datasets with limited size. We hypothesize that the robust capabilities of
text-to-image diffusion models, particularly Stable Diffusion (SD), which is
trained on large-scale datasets, can help overcome these limitations. To
investigate this hypothesis, we propose PointSD, a framework that leverages the
SD model for 3D self-supervised learning. By replacing the SD model's text
encoder with a 3D encoder, we train a point-to-image diffusion model that
allows point clouds to guide the denoising of rendered noisy images. With the
trained point-to-image diffusion model, we use noise-free images as the input
and point clouds as the condition to extract SD features. Next, we train a 3D
backbone by aligning its features with these SD features, thereby facilitating
direct semantic learning. Comprehensive experiments on downstream point cloud
tasks and ablation studies demonstrate that the SD model can enhance point
cloud self-supervised learning. Code is publicly available at
https://github.com/wdttt/PointSD.

</details>


### [16] [Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production](https://arxiv.org/abs/2507.09105)
*Maoxiao Ye,Xinfeng Ye,Mano Manoharan*

Main category: cs.CV

TL;DR: 论文提出了一种结合自回归和扩散模型的混合方法，用于手语生成（SLP），通过多尺度姿态表示和置信感知因果注意力机制提升生成质量和实时性。


<details>
  <summary>Details</summary>
Motivation: 传统自回归方法在推理阶段存在错误累积问题，而扩散模型因迭代性质难以满足实时需求。

Method: 采用混合自回归和扩散模型，设计多尺度姿态表示模块和置信感知因果注意力机制。

Result: 在PHOENIX14T和How2Sign数据集上验证了方法的生成质量和实时效率。

Conclusion: 混合方法有效结合了两种模型的优势，提升了手语生成的准确性和实时性。

Abstract: Earlier Sign Language Production (SLP) models typically relied on
autoregressive methods that generate output tokens one by one, which inherently
provide temporal alignment. Although techniques like Teacher Forcing can
prevent model collapse during training, they still cannot solve the problem of
error accumulation during inference, since ground truth is unavailable at that
stage. In contrast, more recent approaches based on diffusion models leverage
step-by-step denoising to enable high-quality generation. However, the
iterative nature of these models and the requirement to denoise entire
sequences limit their applicability in real-time tasks like SLP. To address it,
we apply a hybrid approach combining autoregressive and diffusion models to SLP
for the first time, leveraging the strengths of both models in sequential
dependency modeling and output refinement. To capture fine-grained body
movements, we design a Multi-Scale Pose Representation module that separately
extracts detailed features from distinct articulators and integrates them via a
Multi-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal
Attention mechanism that utilizes joint-level confidence scores to dynamically
guide the pose generation process, improving accuracy and robustness. Extensive
experiments on the PHOENIX14T and How2Sign datasets demonstrate the
effectiveness of our method in both generation quality and real-time streaming
efficiency.

</details>


### [17] [RoHOI: Robustness Benchmark for Human-Object Interaction Detection](https://arxiv.org/abs/2507.09111)
*Di Wen,Kunyu Peng,Kailun Yang,Yufan Chen,Ruiping Liu,Junwei Zheng,Alina Roitberg,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 论文提出了首个针对人-物交互（HOI）检测的鲁棒性基准RoHOI，并提出了语义感知掩码渐进学习（SAMPL）策略，以提升模型在复杂环境下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有HOI检测模型在真实世界中的不可预见干扰下性能下降，亟需提升鲁棒性。

Method: 提出了RoHOI基准，包含20种干扰类型，并设计了SAMPL策略，通过动态调整优化过程增强鲁棒性。

Result: 实验表明SAMPL优于现有方法，显著提升了模型在干扰环境下的表现。

Conclusion: RoHOI为HOI检测的鲁棒性研究提供了新标准，SAMPL策略有效提升了模型性能。

Abstract: Human-Object Interaction (HOI) detection is crucial for robot-human
assistance, enabling context-aware support. However, models trained on clean
datasets degrade in real-world conditions due to unforeseen corruptions,
leading to inaccurate prediction. To address this, we introduce the first
robustness benchmark for HOI detection, evaluating model resilience under
diverse challenges. Despite advances, current models struggle with
environmental variability, occlusion, and noise. Our benchmark, RoHOI, includes
20 corruption types based on HICO-DET and V-COCO datasets and a new
robustness-focused metric. We systematically analyze existing models in the
related field, revealing significant performance drops under corruptions. To
improve robustness, we propose a Semantic-Aware Masking-based Progressive
Learning (SAMPL) strategy to guide the model to be optimized based on holistic
and partial cues, dynamically adjusting the model's optimization to enhance
robust feature learning. Extensive experiments show our approach outperforms
state-of-the-art methods, setting a new standard for robust HOI detection.
Benchmarks, datasets, and code will be made publicly available at
https://github.com/Kratos-Wen/RoHOI.

</details>


### [18] [Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning](https://arxiv.org/abs/2507.09118)
*Linlan Huang,Xusheng Cao,Haori Lu,Yifan Meng,Fei Yang,Xialei Liu*

Main category: cs.CV

TL;DR: 论文提出了一种名为MG-CLIP的方法，通过分析模态间隙的变化来改进CLIP在持续学习中的性能，无需额外回放数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了CLIP中的模态间隙，而该间隙对其泛化和适应性至关重要。本文旨在利用模态间隙改进持续学习。

Method: 提出MG-CLIP方法，通过保持模态间隙以减少遗忘，并通过补偿模态间隙增强新数据学习能力。

Result: 在多个基准测试中，MG-CLIP表现优于现有方法，且无需额外回放数据。

Conclusion: 模态间隙是持续学习中的关键因素，MG-CLIP为持续学习提供了新的视角。

Abstract: Continual learning aims to enable models to learn sequentially from
continuously incoming data while retaining performance on previously learned
tasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting
strong capabilities across various downstream tasks, there has been growing
interest in leveraging CLIP for continual learning in such scenarios. Most
existing works overlook the inherent modality gap in CLIP, a key factor in its
generalization and adaptability. In this paper, we analyze the variations in
the modality gap during the fine-tuning of vision-language pre-trained models.
Our observations reveal that the modality gap effectively reflects the extent
to which pre-trained knowledge is preserved. Based on these insights, we
propose a simple yet effective method, MG-CLIP, that improves CLIP's
performance in class-incremental learning. Our approach leverages modality gap
preservation to mitigate forgetting and modality gap compensation to enhance
the capacity for new data, introducing a novel modality-gap-based perspective
for continual learning. Extensive experiments on multiple benchmarks
demonstrate that our method outperforms existing approaches without requiring
additional replay data. Our code is available at
https://github.com/linlany/MindtheGap.

</details>


### [19] [SnapMoGen: Human Motion Generation from Expressive Texts](https://arxiv.org/abs/2507.09122)
*Chuan Guo,Inwoo Hwang,Jian Wang,Bing Zhou*

Main category: cs.CV

TL;DR: SnapMoGen是一个新的文本-动作数据集，包含高质量动作捕捉数据和详细的文本标注，支持长期动作生成研究。MoMask++模型通过多尺度标记序列和生成掩码变换器，在HumanML3D和SnapMoGen基准上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前文本到动作生成方法受限于短文本提示和数据集约束，缺乏细粒度控制和泛化能力。

Method: 引入SnapMoGen数据集，包含20K动作片段和122K详细文本描述；提出MoMask++模型，将动作转换为多尺度标记序列，使用单一生成掩码变换器生成所有标记。

Result: MoMask++在HumanML3D和SnapMoGen基准上实现最佳性能，并能通过LLM处理用户提示。

Conclusion: SnapMoGen和MoMask++显著提升了文本到动作生成的细粒度控制和长期动作生成能力。

Abstract: Text-to-motion generation has experienced remarkable progress in recent
years. However, current approaches remain limited to synthesizing motion from
short or general text prompts, primarily due to dataset constraints. This
limitation undermines fine-grained controllability and generalization to unseen
prompts. In this paper, we introduce SnapMoGen, a new text-motion dataset
featuring high-quality motion capture data paired with accurate, expressive
textual annotations. The dataset comprises 20K motion clips totaling 44 hours,
accompanied by 122K detailed textual descriptions averaging 48 words per
description (vs. 12 words of HumanML3D). Importantly, these motion clips
preserve original temporal continuity as they were in long sequences,
facilitating research in long-term motion generation and blending. We also
improve upon previous generative masked modeling approaches. Our model,
MoMask++, transforms motion into multi-scale token sequences that better
exploit the token capacity, and learns to generate all tokens using a single
generative masked transformer. MoMask++ achieves state-of-the-art performance
on both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the
ability to process casual user prompts by employing an LLM to reformat inputs
to align with the expressivity and narration style of SnapMoGen. Project
webpage: https://snap-research.github.io/SnapMoGen/

</details>


### [20] [PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment](https://arxiv.org/abs/2507.09139)
*Dewen Zhang,Tahir Hussain,Wangpeng An,Hayaru Shouno*

Main category: cs.CV

TL;DR: PoseLLM提出了一种基于LLM的姿态估计框架，通过非线性MLP视觉语言连接器提升定位精度，优于LocLLM，并在零样本泛化上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统姿态估计方法依赖关键点先验，泛化能力有限；语言引导方法（如LocLLM）虽能零样本泛化，但其线性投影器无法捕捉复杂空间-文本交互。

Method: PoseLLM用非线性MLP替换线性投影器，实现分层跨模态特征转换，增强视觉块与文本关键点描述的融合。

Result: 在COCO验证集上达到77.8 AP，优于LocLLM（+0.4 AP），并在Human-Art和MPII上保持强零样本泛化能力。

Conclusion: 非线性连接器显著提升定位精度且不牺牲泛化能力，推动了语言引导姿态估计的先进技术。

Abstract: Human pose estimation traditionally relies on architectures that encode
keypoint priors, limiting their generalization to novel poses or unseen
keypoints. Recent language-guided approaches like LocLLM reformulate keypoint
localization as a vision-language task, enabling zero-shot generalization
through textual descriptions. However, LocLLM's linear projector fails to
capture complex spatial-textual interactions critical for high-precision
localization. To address this, we propose PoseLLM, the first Large Language
Model (LLM)-based pose estimation framework that replaces the linear projector
with a nonlinear MLP vision-language connector. This lightweight two-layer MLP
with GELU activation enables hierarchical cross-modal feature transformation,
enhancing the fusion of visual patches and textual keypoint descriptions.
Trained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO
validation set, outperforming LocLLM by +0.4 AP, while maintaining strong
zero-shot generalization on Human-Art and MPII. Our work demonstrates that a
simple yet powerful nonlinear connector significantly boosts localization
accuracy without sacrificing generalization, advancing the state-of-the-art in
language-guided pose estimation. Code is available at
https://github.com/Ody-trek/PoseLLM.

</details>


### [21] [$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting](https://arxiv.org/abs/2507.09144)
*Zhimin Liao,Ping Wei,Ruijie Zhang,Shuaijia Chen,Haoxuan Wang,Ziyang Ren*

Main category: cs.CV

TL;DR: 提出了$I^{2}$-World框架，用于高效4D占用预测，通过解耦场景标记化实现动态表达和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决3D场景标记化的复杂性，提升自动驾驶系统中场景演化的预测能力。

Method: 采用双标记器设计（场景内和场景间），结合编码器-解码器架构，实现高效4D占用预测。

Result: 在mIoU和IoU上分别提升25.1%和36.9%，训练内存仅需2.9GB，实时推理达37.0FPS。

Conclusion: $I^{2}$-World在性能和效率上均优于现有方法，适用于自动驾驶场景预测。

Abstract: Forecasting the evolution of 3D scenes and generating unseen scenarios via
occupancy-based world models offers substantial potential for addressing corner
cases in autonomous driving systems. While tokenization has revolutionized
image and video generation, efficiently tokenizing complex 3D scenes remains a
critical challenge for 3D world models. To address this, we propose
$I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method
decouples scene tokenization into intra-scene and inter-scene tokenizers. The
intra-scene tokenizer employs a multi-scale residual quantization strategy to
hierarchically compress 3D scenes while preserving spatial details. The
inter-scene tokenizer residually aggregates temporal dependencies across
timesteps. This dual design preserves the compactness of 3D tokenizers while
retaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only
GPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder
architecture. The encoder aggregates spatial context from the current scene and
predicts a transformation matrix to enable high-level control over scene
generation. The decoder, conditioned on this matrix and historical tokens,
ensures temporal consistency during generation. Experiments demonstrate that
$I^{2}$-World achieves state-of-the-art performance, outperforming existing
methods by 25.1\% in mIoU and 36.9\% in IoU for 4D occupancy forecasting while
exhibiting exceptional computational efficiency: it requires merely 2.9 GB of
training memory and achieves real-time inference at 37.0 FPS. Our code is
available on https://github.com/lzzzzzm/II-World.

</details>


### [22] [DAA*: Deep Angular A Star for Image-based Path Planning](https://arxiv.org/abs/2507.09305)
*Zhiwei Xu*

Main category: cs.CV

TL;DR: 论文提出了一种名为DAA*的新方法，通过引入路径角度自由度（PAF）改进A*算法，提升路径平滑性和相似性。实验表明DAA*在路径相似性和长度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 路径平滑性在模仿学习中被忽视，作者希望通过改进A*算法提升路径相似性和平滑性。

Method: 提出DAA*方法，结合PAF优化路径缩短和平滑，通过联合优化启发式距离和PAF实现。

Result: 在7个数据集上，DAA*在路径相似性和长度上显著优于神经A*和TransPath。

Conclusion: DAA*在路径最优性和搜索效率间取得平衡，显著提升了模仿学习的性能。

Abstract: Path smoothness is often overlooked in path imitation learning from expert
demonstrations. In this paper, we introduce a novel learning method, termed
deep angular A* (DAA*), by incorporating the proposed path angular freedom
(PAF) into A* to improve path similarity through adaptive path smoothness. The
PAF aims to explore the effect of move angles on path node expansion by finding
the trade-off between their minimum and maximum values, allowing for high
adaptiveness for imitation learning. DAA* improves path optimality by closely
aligning with the reference path through joint optimization of path shortening
and smoothing, which correspond to heuristic distance and PAF, respectively.
Throughout comprehensive evaluations on 7 datasets, including 4 maze datasets,
2 video-game datasets, and a real-world drone-view dataset containing 2
scenarios, we demonstrate remarkable improvements of our DAA* over neural A* in
path similarity between the predicted and reference paths with a shorter path
length when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,
and 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path
loss and path probability map loss, DAA* significantly outperforms the
state-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also
discuss the minor trade-off between path optimality and search efficiency where
applicable.

</details>


### [23] [Stable Score Distillation](https://arxiv.org/abs/2507.09168)
*Haiming Zhu,Yangyang Xu,Chenshu Xu,Tingrui Shen,Wenxi Liu,Yong Du,Jun Yu,Shengfeng He*

Main category: cs.CV

TL;DR: 提出了一种名为Stable Score Distillation (SSD)的框架，通过简化优化过程提升文本引导图像和3D编辑的稳定性和对齐性。


<details>
  <summary>Details</summary>
Motivation: 现有方法如Delta Denoising Score在稳定性、空间控制和编辑强度方面存在不足，主要依赖复杂的辅助结构，导致优化信号冲突和局部编辑受限。

Method: SSD利用Classifier-Free Guidance (CFG)方程实现跨提示对齐，并引入常数项null-text分支以稳定优化过程，同时通过提示增强分支提升编辑强度。

Result: SSD在2D和3D编辑任务（如NeRF和文本驱动风格编辑）中取得最优结果，收敛更快且复杂度更低。

Conclusion: SSD为文本引导编辑提供了高效且稳健的解决方案，保持了原始内容结构并确保编辑轨迹与源提示紧密对齐。

Abstract: Text-guided image and 3D editing have advanced with diffusion-based models,
yet methods like Delta Denoising Score often struggle with stability, spatial
control, and editing strength. These limitations stem from reliance on complex
auxiliary structures, which introduce conflicting optimization signals and
restrict precise, localized edits. We introduce Stable Score Distillation
(SSD), a streamlined framework that enhances stability and alignment in the
editing process by anchoring a single classifier to the source prompt.
Specifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves
cross-prompt alignment, and introduces a constant term null-text branch to
stabilize the optimization process. This approach preserves the original
content's structure and ensures that editing trajectories are closely aligned
with the source prompt, enabling smooth, prompt-specific modifications while
maintaining coherence in surrounding regions. Additionally, SSD incorporates a
prompt enhancement branch to boost editing strength, particularly for style
transformations. Our method achieves state-of-the-art results in 2D and 3D
editing tasks, including NeRF and text-driven style edits, with faster
convergence and reduced complexity, providing a robust and efficient solution
for text-guided editing.

</details>


### [24] [Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model](https://arxiv.org/abs/2507.09681)
*Osher Rafaeli,Tal Svoray,Ariel Nahlieli*

Main category: cs.CV

TL;DR: 提出了一种基于提示的单目深度估计框架，用于高分辨率数字高程模型（DEM）的绝对全局高程映射，实现了从30米到30厘米的100倍分辨率提升。


<details>
  <summary>Details</summary>
Motivation: 高分辨率高程估计对水文、城市形态和生态系统监测至关重要，但现有方法（如超分辨率技术和单目深度估计）存在局限性。

Method: 利用低分辨率SRTM高程数据作为提示，结合高分辨率NAIP RGB图像，通过视觉变换器编码器和LiDAR DEM微调，实现DEM估计、填补和更新。

Result: 在三个美国景观中验证，分辨率提升100倍，MAE低于5米，优于SRTM高达18%，适用于水文和环境研究。

Conclusion: 该框架具有强泛化能力和可扩展性，代码和预训练模型已公开。

Abstract: High-resolution elevation estimations are essential to understand catchment
and hillslope hydrology, study urban morphology and dynamics, and monitor the
growth, decline, and mortality of terrestrial ecosystems. Various deep learning
approaches (e.g., super-resolution techniques, monocular depth estimation) have
been developed to create high-resolution Digital Elevation Models (DEMs).
However, super-resolution techniques are limited by the upscaling factor, and
monocular depth estimation lacks global elevation context, making its
conversion to a seamless DEM restricted. The recently introduced technique of
prompt-based monocular depth estimation has opened new opportunities to extract
estimates of absolute elevation in a global context. We present here a
framework for the estimation of high-resolution DEMs as a new paradigm for
absolute global elevation mapping. It is exemplified using low-resolution
Shuttle Radar Topography Mission (SRTM) elevation data as prompts and
high-resolution RGB imagery from the National Agriculture Imagery Program
(NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived
DEMs and employs a versatile prompting strategy, enabling tasks such as DEM
estimation, void filling, and updating. Our framework achieves a 100x
resolution gain (from 30-m to 30-cm), surpassing prior methods by an order of
magnitude. Evaluations across three diverse U.S. landscapes show robust
generalization, capturing urban structures and fine-scale terrain features with
< 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological
analysis confirms suitability for hazard and environmental studies. We
demonstrate scalability by applying the framework to large regions in the U.S.
and Israel. All code and pretrained models are publicly available at:
https://osherr1996.github.io/prompt2dem_propage/.

</details>


### [25] [Learning and Transferring Better with Depth Information in Visual Reinforcement Learning](https://arxiv.org/abs/2507.09180)
*Zichun Xu,Yuntao Li,Zhaomin Wang,Lei Zhuang,Guocai Yang,Jingdong Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于视觉Transformer的视觉主干网络，融合RGB和深度模态以增强泛化能力，结合对比学习和课程学习优化训练。


<details>
  <summary>Details</summary>
Motivation: 深度信息对场景外观变化具有鲁棒性且携带3D空间细节，因此希望通过融合RGB和深度模态提升模型的泛化能力。

Method: 分别通过CNN处理不同模态，将卷积特征输入可扩展的视觉Transformer；设计对比学习方案和课程学习计划以优化训练。

Result: 模型能够有效融合多模态信息，提升泛化能力和样本效率。

Conclusion: 提出的方法在多模态融合和泛化能力方面表现优异，适用于sim2real迁移任务。

Abstract: Depth information is robust to scene appearance variations and inherently
carries 3D spatial details. In this paper, a visual backbone based on the
vision transformer is proposed to fuse RGB and depth modalities for enhancing
generalization. Different modalities are first processed by separate CNN stems,
and the combined convolutional features are delivered to the scalable vision
transformer to obtain visual representations. Moreover, a contrastive
unsupervised learning scheme is designed with masked and unmasked tokens to
accelerate the sample efficiency during the reinforcement learning progress.
For sim2real transfer, a flexible curriculum learning schedule is developed to
deploy domain randomization over training processes.

</details>


### [26] [Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning](https://arxiv.org/abs/2507.09183)
*Yongwei Jiang,Yixiong Zou,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: 本文研究了Few-Shot Class-Incremental Learning (FSCIL)中的性能下降问题，提出了LGSP-Prompt方法，通过空间维度提示解决数据稀缺和增量学习的挑战。


<details>
  <summary>Details</summary>
Motivation: FSCIL在现实场景中面临数据稀缺和增量学习的双重挑战，现有提示池方法在FSCIL中的性能下降尚未被研究。

Method: 提出LGSP-Prompt方法，将提示学习从token维度转移到空间维度，结合局部空间特征和全局频域表示生成空间提示。

Result: 实验表明，LGSP-Prompt在多个FSCIL基准测试中达到最优性能，显著优于现有方法。

Conclusion: LGSP-Prompt通过空间提示有效解决了FSCIL中的性能下降问题，实现了知识保留和增量学习的平衡。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data
scarcity and incremental learning in real-world scenarios. While pool-based
prompting methods have demonstrated success in traditional incremental
learning, their effectiveness in FSCIL settings remains unexplored. This paper
presents the first study of current prompt pool methods in FSCIL tasks,
revealing an unanticipated performance degradation in incremental sessions.
Through comprehensive analysis, we identify that this phenomenon stems from
token-dimension saturation: with limited data, excessive prompts compete for
task-relevant information, leading to model overfitting. Based on this finding,
we propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively
shifts pool-based prompt learning from the token dimension to the spatial
dimension. LGSP-Prompt generates spatial prompts by synergistically combining
local spatial features and global frequency-domain representations to highlight
key patterns in input images. We construct two spatial prompt pools enabling
dynamic prompt selection to maintain acquired knowledge while effectively
learning novel sessions. Extensive experiments demonstrate that our approach
achieves state-of-the-art performance across multiple FSCIL benchmarks, showing
significant advantages in both base knowledge preservation and incremental
learning. Our implementation is available at
https://github.com/Jywsuperman/LGSP.

</details>


### [27] [Spatial Lifting for Dense Prediction](https://arxiv.org/abs/2507.10222)
*Mingzhi Xu,Yizhe Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为Spatial Lifting（SL）的新方法，通过将输入提升到高维空间进行处理，显著减少了模型参数和推理成本，同时在密集预测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统密集预测方法通常计算成本高且参数多，SL旨在通过维度提升解决这些问题。

Method: 将2D输入（如图像）提升到更高维度（如3D），并使用高维网络（如3D U-Net）处理，生成结构化输出。

Result: 在19个基准数据集上验证，模型参数减少98%以上，推理成本降低，性能与传统方法相当。

Conclusion: SL为密集预测任务提供了一种更高效、准确和可靠的建模范式。

Abstract: We present Spatial Lifting (SL), a novel methodology for dense prediction
tasks. SL operates by lifting standard inputs, such as 2D images, into a
higher-dimensional space and subsequently processing them using networks
designed for that higher dimension, such as a 3D U-Net. Counterintuitively,
this dimensionality lifting allows us to achieve good performance on benchmark
tasks compared to conventional approaches, while reducing inference costs and
significantly lowering the number of model parameters. The SL framework
produces intrinsically structured outputs along the lifted dimension. This
emergent structure facilitates dense supervision during training and enables
robust, near-zero-additional-cost prediction quality assessment at test time.
We validate our approach across 19 benchmark datasets (13 for semantic
segmentation and 6 for depth estimation), demonstrating competitive dense
prediction performance while reducing the model parameter count by over 98% (in
the U-Net case) and lowering inference costs. Spatial Lifting introduces a new
vision modeling paradigm that offers a promising path toward more efficient,
accurate, and reliable deep networks for dense prediction tasks in vision.

</details>


### [28] [MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2507.09184)
*Qiyan Zhao,Xiaofeng Zhang,Yiheng Li,Yun Xing,Xiaosong Yuan,Feilong Tang,Sinan Fan,Xuhang Chen,Xuyao Zhang,Dahan Wang*

Main category: cs.CV

TL;DR: 论文提出MCA-LLaVA方法，通过改进RoPE的位置编码衰减问题，缓解LVLMs中的图像对齐偏差，从而减少幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）中多模态特征的对齐问题导致幻觉现象严重，研究发现RoPE的长期衰减是主要原因之一。

Method: 提出基于曼哈顿距离的MCA-LLaVA方法，将一维序列顺序和二维空间位置结合，改进位置编码的衰减方式。

Result: 实验结果表明，MCA-LLaVA在多种幻觉和通用基准测试中表现有效且通用。

Conclusion: MCA-LLaVA通过缓解图像对齐偏差，有效减少了LVLMs中的幻觉现象。

Abstract: Hallucinations pose a significant challenge in Large Vision Language Models
(LVLMs), with misalignment between multimodal features identified as a key
contributing factor. This paper reveals the negative impact of the long-term
decay in Rotary Position Encoding (RoPE), used for positional modeling in
LVLMs, on multimodal alignment. Concretely, under long-term decay, instruction
tokens exhibit uneven perception of image tokens located at different positions
within the two-dimensional space: prioritizing image tokens from the
bottom-right region since in the one-dimensional sequence, these tokens are
positionally closer to the instruction tokens. This biased perception leads to
insufficient image-instruction interaction and suboptimal multimodal alignment.
We refer to this phenomenon as image alignment bias. To enhance instruction's
perception of image tokens at different spatial locations, we propose
MCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a
two-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the
one-dimensional sequence order and two-dimensional spatial position of image
tokens for positional modeling, mitigating hallucinations by alleviating image
alignment bias. Experimental results of MCA-LLaVA across various hallucination
and general benchmarks demonstrate its effectiveness and generality. The code
can be accessed in https://github.com/ErikZ719/MCA-LLaVA.

</details>


### [29] [THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage](https://arxiv.org/abs/2507.09200)
*Trong-Thuan Nguyen,Pha Nguyen,Jackson Cothren,Alper Yilmaz,Minh-Triet Tran,Khoa Luu*

Main category: cs.CV

TL;DR: 论文提出了一种名为THYME的方法，通过层次特征聚合和循环时间细化，解决了动态场景图中细粒度空间细节和长程时间依赖的捕捉问题，并在新数据集AeroEye-v1.0上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 动态场景理解在自动驾驶、监控等领域需求迫切，但现有方法在空间细节和时间一致性上表现不足。

Method: 提出THYME方法，结合层次特征聚合和循环时间细化，同时引入AeroEye-v1.0数据集。

Result: 在ASPIRe和AeroEye-v1.0数据集上，THYME优于现有方法。

Conclusion: THYME方法在动态场景图生成中表现出色，适用于地面和空中场景。

Abstract: The rapid proliferation of video in applications such as autonomous driving,
surveillance, and sports analytics necessitates robust methods for dynamic
scene understanding. Despite advances in static scene graph generation and
early attempts at video scene graph generation, previous methods often suffer
from fragmented representations, failing to capture fine-grained spatial
details and long-range temporal dependencies simultaneously. To address these
limitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME)
approach, which synergistically integrates hierarchical feature aggregation
with cyclic temporal refinement to address these limitations. In particular,
THYME effectively models multi-scale spatial context and enforces temporal
consistency across frames, yielding more accurate and coherent scene graphs. In
addition, we present AeroEye-v1.0, a novel aerial video dataset enriched with
five types of interactivity that overcome the constraints of existing datasets
and provide a comprehensive benchmark for dynamic scene graph generation.
Empirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that
the proposed THYME approach outperforms state-of-the-art methods, offering
improved scene understanding in ground-view and aerial scenarios.

</details>


### [30] [RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening](https://arxiv.org/abs/2507.10461)
*Tao Tang,Chengxu Yang*

Main category: cs.CV

TL;DR: RAPNet提出了一种基于内容自适应卷积的遥感图像融合方法，通过RAPConv和PAN-DFF模块提升空间细节和光谱保真度。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在遥感图像融合中因卷积核的均匀应用而忽略局部内容变化，限制了性能。

Method: RAPNet采用RAPConv生成空间自适应卷积核，并结合PAN-DFF模块通过注意力机制优化空间细节和光谱保真度。

Result: 实验表明RAPNet在公开数据集上优于现有方法，定量和定性评估均验证其优越性。

Conclusion: RAPNet通过自适应组件显著提升了遥感图像融合的性能。

Abstract: Pansharpening refers to the process of integrating a high resolution
panchromatic (PAN) image with a lower resolution multispectral (MS) image to
generate a fused product, which is pivotal in remote sensing. Despite the
effectiveness of CNNs in addressing this challenge, they are inherently
constrained by the uniform application of convolutional kernels across all
spatial positions, overlooking local content variations. To overcome this
issue, we introduce RAPNet, a new architecture that leverages content-adaptive
convolution. At its core, RAPNet employs the Receptive-field Adaptive
Pansharpening Convolution (RAPConv), designed to produce spatially adaptive
kernels responsive to local feature context, thereby enhancing the precision of
spatial detail extraction. Additionally, the network integrates the
Pansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an
attention mechanism to achieve an optimal balance between spatial detail
enhancement and spectral fidelity. Comprehensive evaluations on publicly
available datasets confirm that RAPNet delivers superior performance compared
to existing approaches, as demonstrated by both quantitative metrics and
qualitative assessments. Ablation analyses further substantiate the
effectiveness of the proposed adaptive components.

</details>


### [31] [Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves](https://arxiv.org/abs/2507.09207)
*Alexander C. Ogren,Berthy T. Feng,Jihoon Ahn,Katherine L. Bouman,Chiara Daraio*

Main category: cs.CV

TL;DR: 通过视频分析表面波的传播，推断材料厚度和刚度的方法。


<details>
  <summary>Details</summary>
Motivation: 利用表面波传播信息，开发非侵入式测量技术，适用于健康监测和人机交互等领域。

Method: 从视频中提取波的色散关系，通过物理优化问题求解厚度和刚度参数。

Result: 在模拟和真实数据中验证，与实测结果高度一致。

Conclusion: 该方法为非侵入式健康监测提供了概念验证，并适用于其他领域。

Abstract: Wave propagation on the surface of a material contains information about
physical properties beneath its surface. We propose a method for inferring the
thickness and stiffness of a structure from just a video of waves on its
surface. Our method works by extracting a dispersion relation from the video
and then solving a physics-based optimization problem to find the best-fitting
thickness and stiffness parameters. We validate our method on both simulated
and real data, in both cases showing strong agreement with ground-truth
measurements. Our technique provides a proof-of-concept for at-home health
monitoring of medically-informative tissue properties, and it is further
applicable to fields such as human-computer interaction.

</details>


### [32] [Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models](https://arxiv.org/abs/2507.09209)
*Xiao Liang,Di Wang,Zhicheng Jiao,Ronghan Li,Pengfei Yang,Quan Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出了一种名为Expert-CFG的专家参与框架，无需额外训练即可将医学视觉语言模型与临床专业知识对齐，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉语言模型存在概率不确定性，易产生错误或未验证的响应，且现有方法成本高且与临床专业知识对齐不足。

Method: 引入不确定性估计策略识别不可靠输出，检索相关参考文献辅助专家标注关键术语，并应用无分类器引导优化模型输出。

Result: 在三个医学视觉问答基准测试中，Expert-CFG以4.2B参数和有限专家标注优于13B参数的现有最佳模型。

Conclusion: Expert-CFG展示了在资源有限环境中部署临床应用的可行性。

Abstract: The rapid advancements in Vision Language Models (VLMs) have prompted the
development of multi-modal medical assistant systems. Despite this progress,
current models still have inherent probabilistic uncertainties, often producing
erroneous or unverified responses-an issue with serious implications in medical
applications. Existing methods aim to enhance the performance of Medical Vision
Language Model (MedVLM) by adjusting model structure, fine-tuning with
high-quality data, or through preference fine-tuning. However, these
training-dependent strategies are costly and still lack sufficient alignment
with clinical expertise. To address these issues, we propose an
expert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance
(Expert-CFG) to align MedVLM with clinical expertise without additional
training. This framework introduces an uncertainty estimation strategy to
identify unreliable outputs. It then retrieves relevant references to assist
experts in highlighting key terms and applies classifier-free guidance to
refine the token embeddings of MedVLM, ensuring that the adjusted outputs are
correct and align with expert highlights. Evaluations across three medical
visual question answering benchmarks demonstrate that the proposed Expert-CFG,
with 4.2B parameters and limited expert annotations, outperforms
state-of-the-art models with 13B parameters. The results demonstrate the
feasibility of deploying such a system in resource-limited settings for
clinical use.

</details>


### [33] [Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline](https://arxiv.org/abs/2507.09214)
*Shiyi Mu,Zichong Gu,Hanqi Lyu,Yilin Gao,Shugong Xu*

Main category: cs.CV

TL;DR: 论文提出了一种基于立体视觉的3D异常物体检测算法（S3AD），通过解耦2D和3D训练策略提升模型对任意形状目标的泛化能力，并设计了异常评分算法。同时，通过合成数据集KITTI-AR验证了算法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D检测模型在开放道路场景中对罕见异常物体的误检或漏检问题，提升模型的泛化能力。

Method: 提出S3AD算法，解耦2D和3D训练策略，设计基于前景置信度预测的异常评分算法，并合成KITTI-AR数据集用于验证。

Result: 实验验证了算法和数据集的有效性，提升了3D异常检测的泛化能力。

Conclusion: S3AD算法和KITTI-AR数据集为3D异常检测提供了有效解决方案，未来可进一步优化模型和数据集。

Abstract: 3D detection technology is widely used in the field of autonomous driving,
with its application scenarios gradually expanding from enclosed highways to
open conventional roads. For rare anomaly categories that appear on the road,
3D detection models trained on closed sets often misdetect or fail to detect
anomaly objects. To address this risk, it is necessary to enhance the
generalization ability of 3D detection models for targets of arbitrary shapes
and to possess the capability to filter out anomalies. The generalization of 3D
detection is limited by two factors: the coupled training of 2D and 3D, and the
insufficient diversity in the scale distribution of training samples. This
paper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm,
which decouples the training strategy of 3D and 2D to release the
generalization ability for arbitrary 3D foreground detection, and proposes an
anomaly scoring algorithm based on foreground confidence prediction, achieving
target-level anomaly scoring. In order to further verify and enhance the
generalization of anomaly detection, we use a 3D rendering method to synthesize
two augmented reality binocular stereo 3D detection datasets which named
KITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k
pairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories
as extra training data to address the sparse sample distribution issue.
Additionally, 58 rare categories form the KITTI-AR-OoD subset, which are not
used in training to simulate zero-shot scenarios in real-world settings, solely
for evaluating 3D anomaly detection. Finally, the performance of the algorithm
and the dataset is verified in the experiments. (Code and dataset can be
obtained at https://github.com/xxxx/xxx).

</details>


### [34] [360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models](https://arxiv.org/abs/2507.09216)
*Jingguo Liu,Han Yu,Shigang Li,Jianfeng Li*

Main category: cs.CV

TL;DR: 提出了一种新颖的球形采样方法，用于全景图像，直接利用现有的二维预训练模型，减少失真并提升性能。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏大规模全景图像数据集，现有任务依赖二维预训练模型，但这些模型无法处理全景图像的失真和不连续性，影响性能。

Method: 采用基于预训练模型权重的球形离散采样方法，减少失真并获得良好的初始训练值；将该方法应用于全景图像分割，利用球形模型特征作为特定通道注意力的掩码。

Result: 在常用室内数据集Stanford2D3D上取得了良好效果。

Conclusion: 球形采样方法有效解决了全景图像任务中预训练模型的失真问题，提升了性能。

Abstract: Due to the current lack of large-scale datasets at the million-scale level,
tasks involving panoramic images predominantly rely on existing two-dimensional
pre-trained image benchmark models as backbone networks. However, these
networks are not equipped to recognize the distortions and discontinuities
inherent in panoramic images, which adversely affects their performance in such
tasks. In this paper, we introduce a novel spherical sampling method for
panoramic images that enables the direct utilization of existing pre-trained
models developed for two-dimensional images. Our method employs spherical
discrete sampling based on the weights of the pre-trained models, effectively
mitigating distortions while achieving favorable initial training values.
Additionally, we apply the proposed sampling method to panoramic image
segmentation, utilizing features obtained from the spherical model as masks for
specific channel attentions, which yields commendable results on commonly used
indoor datasets, Stanford2D3D.

</details>


### [35] [Online Long-term Point Tracking in the Foundation Model Era](https://arxiv.org/abs/2507.09217)
*Görkay Aydemir*

Main category: cs.CV

TL;DR: 论文提出了一种在线点跟踪方法Track-On，利用视觉基础模型和Transformer架构，实现了无需未来帧信息的长时跟踪，并在多个基准测试中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 现实场景需要在线点跟踪，但现有方法多为离线处理，无法满足实时需求。视觉基础模型虽能提供几何表示，但缺乏时序推理能力。

Method: 提出Track-On模型，基于Transformer架构，将每个跟踪点视为查询，逐帧处理视频，并通过记忆机制保持时序一致性。

Result: Track-On在七个公开基准测试中达到最优性能，验证了无需未来帧信息的长时跟踪可行性。

Conclusion: Track-On通过结合视觉基础模型和Transformer，成功实现了在线长时点跟踪，为实时应用提供了有效解决方案。

Abstract: Point tracking aims to identify the same physical point across video frames
and serves as a geometry-aware representation of motion. This representation
supports a wide range of applications, from robotics to augmented reality, by
enabling accurate modeling of dynamic environments. Most existing long-term
tracking approaches operate in an offline setting, where future frames are
available to refine predictions and recover from occlusions. However,
real-world scenarios often demand online predictions: the model must operate
causally, using only current and past frames. This constraint is critical in
streaming video and embodied AI, where decisions must be made immediately based
on past observations. Under such constraints, viewpoint invariance becomes
essential. Visual foundation models, trained on diverse large-scale datasets,
offer the potential for robust geometric representations. While they lack
temporal reasoning on their own, they can be integrated into tracking pipelines
to enrich spatial features. In this thesis, we address the problem of long-term
point tracking in an online setting, where frames are processed sequentially
without access to future information or sliding windows. We begin by evaluating
the suitability of visual foundation models for this task and find that they
can serve as useful initializations and be integrated into tracking pipelines.
However, to enable long-term tracking in an online setting, a dedicated design
is still required. In particular, maintaining coherence over time in this
causal regime requires memory to propagate appearance and context across
frames. To address this, we introduce Track-On, a transformer-based model that
treats each tracked point as a query and processes video frames one at a time.
Track-On sets a new state of the art across seven public benchmarks,
demonstrating the feasibility of long-term tracking without future access.

</details>


### [36] [Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift](https://arxiv.org/abs/2507.09222)
*Behraj Khan,Tahir Syed*

Main category: cs.CV

TL;DR: StaRFM是一个统一框架，通过Fisher信息惩罚（FIP）和置信度对齐惩罚（CMP）解决分布偏移和置信度不对齐问题，提升CLIP和SAM在视觉分类和医学分割任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决基础模型（如CLIP和SAM）在部署时面临的分布偏移和置信度不对齐问题，这些问题在不同任务中表现不同，但现有解决方案多为领域特定。

Method: 提出StaRFM框架，包括FIP（通过Fisher信息惩罚减少分布偏移）和CMP（通过Brier分数优化校准置信度），并扩展到3D医学数据。

Result: 在19个视觉数据集上提升3.5%准确率，降低28% ECE；在医学分割任务中达到84.7% DSC和4.8mm HD95，跨域性能差距降低40%。

Conclusion: StaRFM是一个即插即用的统一框架，能有效解决分布偏移和置信度不对齐问题，适用于多种任务和领域。

Abstract: Foundation models like CLIP and SAM have transformed computer vision and
medical imaging via low-shot transfer learning. However, deployment of these
models hindered by two key challenges: \textit{distribution shift} between
training and test data, and \textit{confidence misalignment} that leads to
overconfident incorrect predictions. These issues manifest differently in
vision-language classification and medical segmentation tasks, yet existing
solutions remain domain-specific. We propose \textit{StaRFM}, a unified
framework addressing both challenges. It introduces a Fisher information
penalty (FIP), extended to 3D medical data via patch-wise regularization, to
reduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence
misalignment penalty (CMP), reformulated for voxel-level predictions,
calibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes
bounds showing FIP controls generalization via the Fisher-Rao norm, while CMP
minimizes calibration error through Brier score optimization. StaRFM shows
consistent performance like \texttt{+}3.5\% accuracy and 28\% lower ECE on 19
vision datasets (e.g., ImageNet, Office-Home), 84.7\% DSC and 4.8mm HD95 in
medical segmentation (e.g., BraTS, ATLAS), and 40\% lower cross-domain
performance gap compared to prior benchmarking methods. The framework is
plug-and-play, requiring minimal architectural changes for seamless integration
with foundation models. Code and models will be released at
https://anonymous.4open.science/r/StaRFM-C0CD/README.md

</details>


### [37] [EgoAnimate: Generating Human Animations from Egocentric top-down Views](https://arxiv.org/abs/2507.09230)
*G. Kutay Türkoglu,Julian Tanke,Iheb Belgacem,Lev Markhasin*

Main category: cs.CV

TL;DR: 本文提出了一种基于生成先验的方法，从第一人称视角重建可动画化虚拟形象，利用Stable Diffusion减少训练负担并提高泛化能力。


<details>
  <summary>Details</summary>
Motivation: 理想数字远程呈现需要准确复制人体、服装和动作，但第一人称视角带来遮挡和身体比例失真等挑战。现有方法多依赖多视角数据集训练，本文首次使用生成式骨干网络解决这一问题。

Method: 基于Stable Diffusion，结合ControlNet，提出从遮挡的俯视图像生成真实正面视图的流程，并将其输入图像到动作模型生成虚拟形象动作。

Result: 方法成功从单张俯视图像生成真实正面视图，并进一步生成虚拟形象动作，为更易用和泛化的远程呈现系统奠定基础。

Conclusion: 本文首次利用生成式骨干网络从第一人称视角重建可动画化虚拟形象，显著减少训练需求并提高泛化能力，为远程呈现技术提供了新思路。

Abstract: An ideal digital telepresence experience requires accurate replication of a
person's body, clothing, and movements. To capture and transfer these movements
into virtual reality, the egocentric (first-person) perspective can be adopted,
which enables the use of a portable and cost-effective device without
front-view cameras. However, this viewpoint introduces challenges such as
occlusions and distorted body proportions.
  There are few works reconstructing human appearance from egocentric views,
and none use a generative prior-based approach. Some methods create avatars
from a single egocentric image during inference, but still rely on multi-view
datasets during training. To our knowledge, this is the first study using a
generative backbone to reconstruct animatable avatars from egocentric inputs.
Based on Stable Diffusion, our method reduces training burden and improves
generalizability.
  Inspired by methods such as SiTH and MagicMan, which perform 360-degree
reconstruction from a frontal image, we introduce a pipeline that generates
realistic frontal views from occluded top-down images using ControlNet and a
Stable Diffusion backbone.
  Our goal is to convert a single top-down egocentric image into a realistic
frontal representation and feed it into an image-to-motion model. This enables
generation of avatar motions from minimal input, paving the way for more
accessible and generalizable telepresence systems.

</details>


### [38] [PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process](https://arxiv.org/abs/2507.09242)
*Shiqi Jiang,Xinpeng Li,Xi Mao,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: 提出了一种新的框架PPJudge，用于评估绘画过程，填补了现有方法仅关注静态图像的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了绘画过程的动态性和多阶段性，需要一种更全面的评估方法。

Method: 引入了PPAD数据集和基于Transformer的PPJudge模型，结合时间感知位置编码和混合专家架构。

Result: 实验表明，PPJudge在准确性、鲁棒性和与人类判断的一致性上优于现有基线。

Conclusion: 该方法为计算创意和艺术教育提供了新见解。

Abstract: Artistic image assessment has become a prominent research area in computer
vision. In recent years, the field has witnessed a proliferation of datasets
and methods designed to evaluate the aesthetic quality of paintings. However,
most existing approaches focus solely on static final images, overlooking the
dynamic and multi-stage nature of the artistic painting process. To address
this gap, we propose a novel framework for human-aligned assessment of painting
processes. Specifically, we introduce the Painting Process Assessment Dataset
(PPAD), the first large-scale dataset comprising real and synthetic painting
process images, annotated by domain experts across eight detailed attributes.
Furthermore, we present PPJudge (Painting Process Judge), a Transformer-based
model enhanced with temporally-aware positional encoding and a heterogeneous
mixture-of-experts architecture, enabling effective assessment of the painting
process. Experimental results demonstrate that our method outperforms existing
baselines in accuracy, robustness, and alignment with human judgment, offering
new insights into computational creativity and art education.

</details>


### [39] [AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition](https://arxiv.org/abs/2507.09248)
*Varsha Devi,Amine Bohi,Pardeep Kumar*

Main category: cs.CV

TL;DR: AGCD-Net通过注意力引导和因果干预模块解决情感识别中的上下文偏差问题，提出混合ConvNeXt编码器，实验显示其在CAER-S数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统情感识别方法存在上下文偏差问题，导致背景与情感标签的虚假关联，影响识别准确性。

Method: 提出AGCD-Net模型，结合混合ConvNeXt编码器和注意力引导的因果干预模块（AG-CIM），通过特征重校准和因果干预减少偏差。

Result: 在CAER-S数据集上实现最优性能，验证了因果去偏对复杂场景下情感识别的有效性。

Conclusion: AGCD-Net通过注意力引导和因果干预显著提升了情感识别的鲁棒性，为复杂场景下的应用提供了新思路。

Abstract: Context-aware emotion recognition (CAER) enhances affective computing in
real-world scenarios, but traditional methods often suffer from context
bias-spurious correlation between background context and emotion labels (e.g.
associating ``garden'' with ``happy''). In this paper, we propose
\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces
\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the
ConvNeXt backbone by integrating Spatial Transformer Network and
Squeeze-and-Excitation layers for enhanced feature recalibration. At the core
of AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),
which applies causal theory, perturbs context features, isolates spurious
correlations, and performs an attention-driven correction guided by face
features to mitigate context bias. Experimental results on the CAER-S dataset
demonstrate the effectiveness of AGCD-Net, achieving state-of-the-art
performance and highlighting the importance of causal debiasing for robust
emotion recognition in complex settings.

</details>


### [40] [Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching](https://arxiv.org/abs/2507.09256)
*Junyu Chen,Yihua Gao,Mingyuan Ge,Mingyong Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为AAHR的框架，通过动态聚类原型对比学习和全局/局部特征提取机制，解决了图像-文本匹配中的高阶关联和语义模糊问题，显著提升了匹配性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理高阶关联和语义模糊（如软正样本和软负样本的区分）时表现不佳，且未能充分利用训练批次中样本的邻域关系。

Method: AAHR框架结合了动态聚类原型对比学习、全局/局部特征提取、自适应聚合网络、GNN增强语义交互以及动量对比学习扩展负样本集。

Result: 在Flickr30K、MSCOCO和ECCV Caption数据集上，AAHR显著优于现有方法，提高了匹配的准确性和效率。

Conclusion: AAHR通过多策略联合优化，有效解决了图像-文本匹配中的关键问题，为相关领域提供了新的解决方案。

Abstract: Image-text matching is crucial for bridging the semantic gap between computer
vision and natural language processing. However, existing methods still face
challenges in handling high-order associations and semantic ambiguities among
similar instances. These ambiguities arise from subtle differences between soft
positive samples (semantically similar but incorrectly labeled) and soft
negative samples (locally matched but globally inconsistent), creating matching
uncertainties. Furthermore, current methods fail to fully utilize the
neighborhood relationships among semantically similar instances within training
batches, limiting the model's ability to learn high-order shared knowledge.
This paper proposes the Ambiguity-Aware and High-order Relation learning
framework (AAHR) to address these issues. AAHR constructs a unified
representation space through dynamic clustering prototype contrastive learning,
effectively mitigating the soft positive sample problem. The framework
introduces global and local feature extraction mechanisms and an adaptive
aggregation network, significantly enhancing full-grained semantic
understanding capabilities. Additionally, AAHR employs intra-modal and
inter-modal correlation matrices to investigate neighborhood relationships
among sample instances thoroughly. It incorporates GNN to enhance semantic
interactions between instances. Furthermore, AAHR integrates momentum
contrastive learning to expand the negative sample set. These combined
strategies significantly improve the model's ability to discriminate between
features. Experimental results demonstrate that AAHR outperforms existing
state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,
considerably improving the accuracy and efficiency of image-text matching. The
code and model checkpoints for this research are available at
https://github.com/Image-Text-Matching/AAHR .

</details>


### [41] [SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation](https://arxiv.org/abs/2507.09266)
*JianHe Low,Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 提出了一种无需依赖注释的分段感知视觉标记化框架，显著降低计算需求并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有无注释手语翻译方法模型复杂、计算需求高的问题，提升可扩展性。

Method: 采用分段感知视觉标记化框架，结合标记间对比对齐目标和双重监督，优化跨模态对齐。

Result: 在PHOENIX14T基准测试中性能超越现有方法，序列长度减少50%，内存使用降低2.67倍。

Conclusion: 该方法在性能和可扩展性上均有显著提升，验证了标记化和对齐策略的有效性。

Abstract: Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving
strong performances without relying on gloss annotations. However, these gains
have often come with increased model complexity and high computational demands,
raising concerns about scalability, especially as large-scale sign language
datasets become more common. We propose a segment-aware visual tokenization
framework that leverages sign segmentation to convert continuous video into
discrete, sign-informed visual tokens. This reduces input sequence length by up
to 50% compared to prior methods, resulting in up to 2.67x lower memory usage
and better scalability on larger datasets. To bridge the visual and linguistic
modalities, we introduce a token-to-token contrastive alignment objective,
along with a dual-level supervision that aligns both language embeddings and
intermediate hidden states. This improves fine-grained cross-modal alignment
without relying on gloss-level supervision. Our approach notably exceeds the
performance of state-of-the-art methods on the PHOENIX14T benchmark, while
significantly reducing sequence length. Further experiments also demonstrate
our improved performance over prior work under comparable sequence-lengths,
validating the potential of our tokenization and alignment strategies.

</details>


### [42] [Cross Knowledge Distillation between Artificial and Spiking Neural Networks](https://arxiv.org/abs/2507.09269)
*Shuhan Ye,Yuanbin Qian,Chong Wang,Sunqi Lin,Jiazhen Xu,Jiangbo Qian,Yuqi Li*

Main category: cs.CV

TL;DR: 该论文提出了一种跨知识蒸馏（CKD）方法，通过利用语义相似性和滑动替换解决跨模态挑战，并采用间接分阶段知识蒸馏解决跨架构挑战，从而提升脉冲神经网络（SNNs）在DVS数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络（SNNs）在计算机视觉领域具有潜力，但由于标注事件数据集的稀缺和SNN架构的不成熟，其性能仍不如人工神经网络（ANNs）。论文旨在通过知识蒸馏提升SNNs在DVS数据上的表现。

Method: 提出跨知识蒸馏（CKD），结合语义相似性和滑动替换解决跨模态问题，并通过间接分阶段知识蒸馏解决跨架构问题。

Result: 在主流神经形态数据集（如N-Caltech101和CEP-DVS）上验证，CKD方法优于当前最先进方法。

Conclusion: CKD方法有效提升了SNNs在DVS数据上的性能，解决了跨模态和跨架构的挑战。

Abstract: Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in
computer vision domain due to their high biological plausibility, event-driven
characteristic and energy-saving efficiency. Still, limited annotated
event-based datasets and immature SNN architectures result in their performance
inferior to that of Artificial Neural Networks (ANNs). To enhance the
performance of SNNs on their optimal data format, DVS data, we explore using
RGB data and well-performing ANNs to implement knowledge distillation. In this
case, solving cross-modality and cross-architecture challenges is necessary. In
this paper, we propose cross knowledge distillation (CKD), which not only
leverages semantic similarity and sliding replacement to mitigate the
cross-modality challenge, but also uses an indirect phased knowledge
distillation to mitigate the cross-architecture challenge. We validated our
method on main-stream neuromorphic datasets, including N-Caltech101 and
CEP-DVS. The experimental results show that our method outperforms current
State-of-the-Art methods. The code will be available at
https://github.com/ShawnYE618/CKD

</details>


### [43] [Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models](https://arxiv.org/abs/2507.09279)
*Anita Kriz,Elizabeth Laura Janes,Xing Shen,Tal Arbel*

Main category: cs.CV

TL;DR: Prompt4Trust是一个强化学习框架，用于提升多模态大语言模型（MLLMs）在医疗领域的置信度校准，同时提高任务准确性。


<details>
  <summary>Details</summary>
Motivation: MLLMs在医疗等安全关键领域的应用受到提示设计敏感性和高置信度错误响应的限制，需要改进其置信度校准以提高可靠性。

Method: 通过训练一个轻量级LLM生成上下文感知的辅助提示，指导下游任务MLLM生成置信度更准确的响应。

Result: 在PMC-VQA基准测试中实现了最先进的医学视觉问答性能，并展示了零样本泛化能力。

Conclusion: Prompt4Trust展示了自动化提示工程在提升MLLMs可信度方面的潜力，尤其适用于安全关键场景。

Abstract: Multimodal large language models (MLLMs) hold considerable promise for
applications in healthcare. However, their deployment in safety-critical
settings is hindered by two key limitations: (i) sensitivity to prompt design,
and (ii) a tendency to generate incorrect responses with high confidence. As
clinicians may rely on a model's stated confidence to gauge the reliability of
its predictions, it is especially important that when a model expresses high
confidence, it is also highly accurate. We introduce Prompt4Trust, the first
reinforcement learning (RL) framework for prompt augmentation targeting
confidence calibration in MLLMs. A lightweight LLM is trained to produce
context-aware auxiliary prompts that guide a downstream task MLLM to generate
responses in which the expressed confidence more accurately reflects predictive
accuracy. Unlike conventional calibration techniques, Prompt4Trust specifically
prioritizes aspects of calibration most critical for safe and trustworthy
clinical decision-making. Beyond improvements driven by this clinically
motivated calibration objective, our proposed method also improves task
accuracy, achieving state-of-the-art medical visual question answering (VQA)
performance on the PMC-VQA benchmark, which is composed of multiple-choice
questions spanning diverse medical imaging modalities. Moreover, our framework
trained with a small downstream task MLLM showed promising zero-shot
generalization to larger MLLMs in our experiments, suggesting the potential for
scalable calibration without the associated computational costs. This work
demonstrates the potential of automated yet human-aligned prompt engineering
for improving the the trustworthiness of MLLMs in safety critical settings. Our
codebase can be found at https://github.com/xingbpshen/vccrl-llm.

</details>


### [44] [Generative Latent Kernel Modeling for Blind Motion Deblurring](https://arxiv.org/abs/2507.09285)
*Chenhao Ding,Jiangtao Zhang,Zongsheng Yue,Hui Wang,Qian Zhao,Deyu Meng*

Main category: cs.CV

TL;DR: 提出了一种基于深度生成模型的盲运动去模糊（BMD）新框架，通过预训练的GAN生成器和初始化器优化模糊核的初始估计，解决了现有方法对初始核敏感的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有深度先验方法在盲运动去模糊中因优化过程的高度非凸性对初始模糊核极度敏感，限制了性能。

Method: 预训练基于GAN的模糊核生成器和初始化器，提供高质量初始核估计，并将其与现有BMD方法结合。

Result: 在挑战性基准数据集上实现了最先进的性能，且无需额外先验即可处理非均匀运动去模糊。

Conclusion: 提出的框架通过优化模糊核初始化，显著提升了BMD的性能和鲁棒性，且易于与现有方法集成。

Abstract: Deep prior-based approaches have demonstrated remarkable success in blind
motion deblurring (BMD) recently. These methods, however, are often limited by
the high non-convexity of the underlying optimization process in BMD, which
leads to extreme sensitivity to the initial blur kernel. To address this issue,
we propose a novel framework for BMD that leverages a deep generative model to
encode the kernel prior and induce a better initialization for the blur kernel.
Specifically, we pre-train a kernel generator based on a generative adversarial
network (GAN) to aptly characterize the kernel's prior distribution, as well as
a kernel initializer to provide a well-informed and high-quality starting point
for kernel estimation. By combining these two components, we constrain the BMD
solution within a compact latent kernel manifold, thus alleviating the
aforementioned sensitivity for kernel initialization. Notably, the kernel
generator and initializer are designed to be easily integrated with existing
BMD methods in a plug-and-play manner, enhancing their overall performance.
Furthermore, we extend our approach to tackle blind non-uniform motion
deblurring without the need for additional priors, achieving state-of-the-art
performance on challenging benchmark datasets. The source code is available at
https://github.com/dch0319/GLKM-Deblur.

</details>


### [45] [Supercharging Floorplan Localization with Semantic Rays](https://arxiv.org/abs/2507.09291)
*Yuval Grader,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 本文提出了一种语义感知的定位框架，结合深度和语义信息，显著提升了楼层平面图的定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有楼层平面图定位技术主要依赖深度结构信息，忽略了丰富的语义信息，如窗户和门的位置。

Method: 通过联合估计深度和语义射线，构建结构-语义概率体积，并采用由粗到细的采样策略进行优化。

Result: 在两个标准基准测试中，该方法显著优于现有技术，召回率显著提升，并能轻松整合额外元数据（如房间标签）以进一步提高性能。

Conclusion: 该框架通过结合语义信息，显著提升了楼层平面图定位的准确性和效率。

Abstract: Floorplans provide a compact representation of the building's structure,
revealing not only layout information but also detailed semantics such as the
locations of windows and doors. However, contemporary floorplan localization
techniques mostly focus on matching depth-based structural cues, ignoring the
rich semantics communicated within floorplans. In this work, we introduce a
semantic-aware localization framework that jointly estimates depth and semantic
rays, consolidating over both for predicting a structural-semantic probability
volume. Our probability volume is constructed in a coarse-to-fine manner: We
first sample a small set of rays to obtain an initial low-resolution
probability volume. We then refine these probabilities by performing a denser
sampling only in high-probability regions and process the refined values for
predicting a 2D location and orientation angle. We conduct an evaluation on two
standard floorplan localization benchmarks. Our experiments demonstrate that
our approach substantially outperforms state-of-the-art methods, achieving
significant improvements in recall metrics compared to prior works. Moreover,
we show that our framework can easily incorporate additional metadata such as
room labels, enabling additional gains in both accuracy and efficiency.

</details>


### [46] [Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection](https://arxiv.org/abs/2507.09294)
*Rui Tang,Haochen Yin,Guankun Wang,Long Bai,An Wang,Huxin Gao,Jiazheng Wang,Hongliang Ren*

Main category: cs.CV

TL;DR: Geo-RepNet是一种结合RGB图像和深度信息的几何感知卷积框架，用于提升内镜黏膜下剥离术（ESD）中手术阶段识别的性能。


<details>
  <summary>Details</summary>
Motivation: 手术阶段识别对开发智能辅助系统至关重要，但RGB图像中视觉相似性高且缺乏结构线索，深度信息能提供有价值的几何线索。

Method: 提出Geo-RepNet框架，基于RepVGG骨干网络，包含深度引导几何先验生成模块（DGPG）和几何增强多尺度注意力（GEMA）。

Result: 在真实ESD视频数据集上，Geo-RepNet实现了最先进的性能，同时保持鲁棒性和高效计算。

Conclusion: 深度信息的引入显著提升了手术阶段识别的性能，Geo-RepNet在复杂手术场景中表现出色。

Abstract: Surgical phase recognition plays a critical role in developing intelligent
assistance systems for minimally invasive procedures such as Endoscopic
Submucosal Dissection (ESD). However, the high visual similarity across
different phases and the lack of structural cues in RGB images pose significant
challenges. Depth information offers valuable geometric cues that can
complement appearance features by providing insights into spatial relationships
and anatomical structures. In this paper, we pioneer the use of depth
information for surgical phase recognition and propose Geo-RepNet, a
geometry-aware convolutional framework that integrates RGB image and depth
information to enhance recognition performance in complex surgical scenes.
Built upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the
Depth-Guided Geometric Prior Generation (DGPG) module that extracts geometry
priors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention
(GEMA) to inject spatial guidance through geometry-aware cross-attention and
efficient multi-scale aggregation. To evaluate the effectiveness of our
approach, we construct a nine-phase ESD dataset with dense frame-level
annotations from real-world ESD videos. Extensive experiments on the proposed
dataset demonstrate that Geo-RepNet achieves state-of-the-art performance while
maintaining robustness and high computational efficiency under complex and
low-texture surgical environments.

</details>


### [47] [ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation](https://arxiv.org/abs/2507.09299)
*Abdulvahap Mutlu,Şengül Doğan,Türker Tuncer*

Main category: cs.CV

TL;DR: ViT-ProtoNet结合ViT-Small和原型网络，显著提升少样本图像分类性能，优于CNN原型网络和部分Transformer方法。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers（ViTs）在少样本图像分类中的潜力未充分挖掘，本文旨在利用ViT提升原型网络的性能。

Method: 通过平均少量支持样本的类别条件token嵌入构建原型，ViT-ProtoNet在5-shot设置下实现泛化。

Result: 在多个标准基准测试中，ViT-ProtoNet表现优于CNN原型网络，5-shot准确率提升3.2%，且特征可分性更强。

Conclusion: ViT-ProtoNet为少样本分类提供了一种强大灵活的方法，并为基于Transformer的元学习设定了新基准。

Abstract: The remarkable representational power of Vision Transformers (ViTs) remains
underutilized in few-shot image classification. In this work, we introduce
ViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical
Network framework. By averaging class conditional token embeddings from a
handful of support examples, ViT-ProtoNet constructs robust prototypes that
generalize to novel categories under 5-shot settings. We conduct an extensive
empirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,
CUB-200, and CIFAR-FS, including overlapped support variants to assess
robustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based
prototypical counterparts, achieving up to a 3.2\% improvement in 5-shot
accuracy and demonstrating superior feature separability in latent space.
Furthermore, it outperforms or is competitive with transformer-based
competitors using a more lightweight backbone. Comprehensive ablations examine
the impact of transformer depth, patch size, and fine-tuning strategy. To
foster reproducibility, we release code and pretrained weights. Our results
establish ViT-ProtoNet as a powerful, flexible approach for few-shot
classification and set a new baseline for transformer-based meta-learners.

</details>


### [48] [AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning](https://arxiv.org/abs/2507.09308)
*Zile Wang,Hao Yu,Jiabo Zhan,Chun Yuan*

Main category: cs.CV

TL;DR: ALPHA benchmark和ALPHAVAE模型首次解决了RGBA图像生成缺乏大规模基准的问题，通过改进的VAE和复合目标训练，显著提升了透明图像的重建和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有潜在扩散模型在RGB图像合成上表现优异，但透明或分层内容（RGBA图像）的生成因缺乏大规模基准而未被充分探索。

Method: 提出ALPHA基准，将标准RGB指标适配到四通道图像；开发ALPHAVAE模型，扩展预训练RGB VAE以包含专用alpha通道，采用复合目标训练。

Result: ALPHAVAE在仅8K图像训练下，PSNR提升4.9 dB，SSIM提升3.2%，优于LayerDiffuse；在潜在扩散框架中微调后生成效果更优。

Conclusion: ALPHA和ALPHAVAE填补了RGBA图像生成的空白，为透明内容合成提供了高效解决方案。

Abstract: Recent advances in latent diffusion models have achieved remarkable results
in high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress
and reconstruct pixel data at low computational cost. However, the generation
of transparent or layered content (RGBA image) remains largely unexplored, due
to the lack of large-scale benchmarks. In this work, we propose ALPHA, the
first comprehensive RGBA benchmark that adapts standard RGB metrics to
four-channel images via alpha blending over canonical backgrounds. We further
introduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB
VAE by incorporating a dedicated alpha channel. The model is trained with a
composite objective that combines alpha-blended pixel reconstruction,
patch-level fidelity, perceptual consistency, and dual KL divergence
constraints to ensure latent fidelity across both RGB and alpha
representations. Our RGBA VAE, trained on only 8K images in contrast to 1M used
by prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase
in SSIM over LayerDiffuse in reconstruction. It also enables superior
transparent image generation when fine-tuned within a latent diffusion
framework. Our code, data, and models are released on
https://github.com/o0o0o00o0/AlphaVAE for reproducibility.

</details>


### [49] [ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models](https://arxiv.org/abs/2507.09313)
*Yueqian Wang,Xiaojun Meng,Yifan Wang,Huishuai Zhang,Dongyan Zhao*

Main category: cs.CV

TL;DR: 论文介绍了ProactiveBench，首个评估多模态对话系统主动交互能力的基准，并提出PAUC指标以考虑响应时间动态性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态对话系统研究的深入，用户期望系统更具主动性，如实时决定多轮响应时机。

Method: 提出ProactiveBench基准和PAUC指标，评估系统在主动交互中的表现。

Result: PAUC比传统指标更符合人类偏好，能更准确评估用户体验。

Conclusion: PAUC为主动交互场景提供了更可靠的评估方法。

Abstract: With the growing research focus on multimodal dialogue systems, the
capability for proactive interaction is gradually gaining recognition. As an
alternative to conventional turn-by-turn dialogue, users increasingly expect
multimodal systems to be more initiative, for example, by autonomously
determining the timing of multi-turn responses in real time during video
playback. To facilitate progress in this emerging area, we introduce
ProactiveBench, the first comprehensive benchmark to evaluate a system's
ability to engage in proactive interaction. Since model responses are generated
at varying timestamps, we further propose PAUC, the first metric that accounts
for the temporal dynamics of model responses. This enables a more accurate
evaluation of systems operating in proactive settings. Through extensive
benchmarking of various baseline systems on ProactiveBench and a user study of
human preferences, we show that PAUC is in better agreement with human
preferences than traditional evaluation metrics, which typically only consider
the textual content of responses. These findings demonstrate that PAUC provides
a more faithful assessment of user experience in proactive interaction
scenarios. Project homepage:
https://github.com/yellow-binary-tree/ProactiveBench

</details>


### [50] [Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition](https://arxiv.org/abs/2507.09323)
*Kaixuan Cong,Yifan Wang,Rongkun Xue,Yuyang Jiang,Yiming Feng,Jing Yang*

Main category: cs.CV

TL;DR: 论文提出了一种动态调整类别间混淆损失的音频-视频预训练编码器DICCAE，通过细粒度对齐和认知对比增强模型对相似活动的区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视频预训练方法仅关注整体模态对齐，忽略了通过认知归纳和对比强化易混淆类别的区分能力。

Method: 提出DICCAE编码器，动态调整类别间混淆损失，并结合音频、视频及其融合的新训练框架；采用聚类引导的自监督预训练策略缓解数据稀缺问题。

Result: 在VGGSound数据集上达到65.5%的top-1准确率，接近最先进水平。

Conclusion: DICCAE通过细粒度对齐和动态混淆损失显著提升了模型性能，验证了各模块的必要性。

Abstract: Humans do not understand individual events in isolation; rather, they
generalize concepts within classes and compare them to others. Existing
audio-video pre-training paradigms only focus on the alignment of the overall
audio-video modalities, without considering the reinforcement of distinguishing
easily confused classes through cognitive induction and contrast during
training. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder
(DICCAE), an encoder that aligns audio-video representations at a fine-grained,
category-level. DICCAE addresses category confusion by dynamically adjusting
the confusion loss based on inter-class confusion degrees, thereby enhancing
the model's ability to distinguish between similar activities. To further
extend the application of DICCAE, we also introduce a novel training framework
that incorporates both audio and video modalities, as well as their fusion. To
mitigate the scarcity of audio-video data in the human activity recognition
task, we propose a cluster-guided audio-video self-supervised pre-training
strategy for DICCAE. DICCAE achieves near state-of-the-art performance on the
VGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its
feature representation quality through extensive ablation studies, validating
the necessity of each module.

</details>


### [51] [Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding](https://arxiv.org/abs/2507.09334)
*Wencan Huang,Daizong Liu,Wei Hu*

Main category: cs.CV

TL;DR: Fast3D是一个用于3D多模态大语言模型（MLLMs）的视觉令牌剪枝框架，通过全局注意力预测和样本自适应剪枝技术，显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 3D MLLMs在场景理解方面表现优异，但计算效率低下限制了其实际部署。现有视觉令牌剪枝方法在3D领域尚未充分探索。

Method: 提出Fast3D框架，包含全局注意力预测（GAP）和样本自适应剪枝（SAP）技术，无需修改目标模型参数。

Result: 在五个基准测试中验证了Fast3D的有效性，尤其在高视觉令牌剪枝比例下表现突出。

Conclusion: Fast3D为3D MLLMs提供了一种高效且灵活的视觉令牌剪枝解决方案，显著提升了计算效率。

Abstract: While 3D Multi-modal Large Language Models (MLLMs) demonstrate remarkable
scene understanding capabilities, their practical deployment faces critical
challenges due to computational inefficiency. The key bottleneck stems from
processing excessive object-centric visual tokens required for comprehensive 3D
scene representation. Although visual token pruning has shown promise in
accelerating 2D MLLMs, its applicability to 3D domains remains largely
unexplored due to fundamental disparities in token structures. In this paper,
we reveal two critical insights: (1) Significant redundancy exists in
object-level 3D token representations, analogous to patch-level redundancy in
2D systems; (2) Global attention patterns exhibit strong predictive power for
identifying non-essential tokens in 3D contexts. Building on these
observations, we propose Fast3D, a plug-and-play visual token pruning framework
for 3D MLLMs featuring two technical innovations: (1) Global Attention
Prediction (GAP), where a lightweight neural network learns to predict the
global attention distributions of the target model, enabling efficient token
importance estimation for precise pruning guidance; (2) Sample-Adaptive visual
token Pruning (SAP), which introduces dynamic token budgets through
attention-based complexity assessment, automatically adjusting layer-wise
pruning ratios based on input characteristics. Both of these two techniques
operate without modifying the parameters of the target model. Extensive
evaluations across five benchmarks validate the effectiveness of Fast3D,
particularly under high visual token pruning ratios. Code is available at
https://github.com/wencan25/Fast3D

</details>


### [52] [Simplifying Traffic Anomaly Detection with Video Foundation Models](https://arxiv.org/abs/2507.09338)
*Svetlana Orlova,Tommie Kerssies,Brunó B. Englert,Gijs Dubbelman*

Main category: cs.CV

TL;DR: 研究发现，简单的基于预训练的视频视觉变换器（Video ViTs）在交通异常检测（TAD）中表现优异，甚至超越复杂方法。


<details>
  <summary>Details</summary>
Motivation: 探讨预训练是否能让简单架构在TAD中超越复杂方法。

Method: 使用简单的编码器架构（Video ViTs），研究不同预训练方式（自监督、弱监督、全监督）的影响。

Result: 强预训练使简单模型性能媲美或超越复杂方法；自监督MVM效果最佳；域自适应预训练（DAPT）进一步提升性能。

Conclusion: 预训练是关键，简单高效的TAD模型可行，无需复杂架构。

Abstract: Recent methods for ego-centric Traffic Anomaly Detection (TAD) often rely on
complex multi-stage or multi-representation fusion architectures, yet it
remains unclear whether such complexity is necessary. Recent findings in visual
perception suggest that foundation models, enabled by advanced pre-training,
allow simple yet flexible architectures to outperform specialized designs.
Therefore, in this work, we investigate an architecturally simple encoder-only
approach using plain Video Vision Transformers (Video ViTs) and study how
pre-training enables strong TAD performance. We find that: (i) strong
pre-training enables simple encoder-only models to match or even surpass the
performance of specialized state-of-the-art TAD methods, while also being
significantly more efficient; (ii) although weakly- and fully-supervised
pre-training are advantageous on standard benchmarks, we find them less
effective for TAD. Instead, self-supervised Masked Video Modeling (MVM)
provides the strongest signal; and (iii) Domain-Adaptive Pre-Training (DAPT) on
unlabeled driving videos further improves downstream performance, without
requiring anomalous examples. Our findings highlight the importance of
pre-training and show that effective, efficient, and scalable TAD models can be
built with minimal architectural complexity. We release our code,
domain-adapted encoders, and fine-tuned models to support future work:
https://github.com/tue-mps/simple-tad.

</details>


### [53] [Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture](https://arxiv.org/abs/2507.09375)
*Sourish Suri,Yifei Shao*

Main category: cs.CV

TL;DR: 该研究提出了一种基于卷积神经网络（CNN）的图像分类系统，用于自动检测和分类八种常见作物病害，并通过移动平台提供实时诊断和治疗建议。


<details>
  <summary>Details</summary>
Motivation: 作物病害对农业生产和全球粮食安全构成重大威胁，尤其是在大规模农业中，早期识别往往延迟或不准确。

Method: 采用完整的深度学习流程：图像采集、预处理（调整大小、归一化和增强）、模型训练（使用TensorFlow和Keras的Sequential API），CNN架构包括三个卷积层、ReLU激活、最大池化、展平和全连接层，最后使用softmax输出进行多类分类。

Result: 系统在训练数据上达到约90%的准确率，验证准确率为约60%，表明存在轻微过拟合。模型还集成了治疗建议模块，并部署在开源移动平台上。

Conclusion: 该研究为精准农业提供了一个可扩展且易用的工具，结合深度学习和实际农艺支持，展示了CNN在作物健康监测和全球粮食生产中的潜力。

Abstract: Crop diseases present a significant barrier to agricultural productivity and
global food security, especially in large-scale farming where early
identification is often delayed or inaccurate. This research introduces a
Convolutional Neural Network (CNN)-based image classification system designed
to automate the detection and classification of eight common crop diseases
using leaf imagery. The methodology involves a complete deep learning pipeline:
image acquisition from a large, labeled dataset, preprocessing via resizing,
normalization, and augmentation, and model training using TensorFlow with
Keras' Sequential API. The CNN architecture comprises three convolutional
layers with increasing filter sizes and ReLU activations, followed by max
pooling, flattening, and fully connected layers, concluding with a softmax
output for multi-class classification. The system achieves high training
accuracy (~90%) and demonstrates reliable performance on unseen data, although
a validation accuracy of ~60% suggests minor overfitting. Notably, the model
integrates a treatment recommendation module, providing actionable guidance by
mapping each detected disease to suitable pesticide or fungicide interventions.
Furthermore, the solution is deployed on an open-source, mobile-compatible
platform, enabling real-time image-based diagnostics for farmers in remote
areas. This research contributes a scalable and accessible tool to the field of
precision agriculture, reducing reliance on manual inspection and promoting
sustainable disease management practices. By merging deep learning with
practical agronomic support, this work underscores the potential of CNNs to
transform crop health monitoring and enhance food production resilience on a
global scale.

</details>


### [54] [GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups](https://arxiv.org/abs/2507.09410)
*Bernie Boscoe,Shawn Johnson,Andrea Osborn,Chandler Campbell,Karen Mager*

Main category: cs.CV

TL;DR: 本文提出了一种低资源处理相机陷阱数据的流程，结合ML/AI功能，适合资源有限的小型研究团队。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱数据量大、标注复杂、环境多变，且现有ML/AI工具难以集成到资源有限的工作流程中。

Method: 开发了一种低资源流程，支持数据本地处理，结合ML/AI功能，包括数据传输、推理和评估。

Result: 该流程为小型研究团队提供了实用的解决方案，能够从大量相机陷阱数据中提取有价值的信息。

Conclusion: 通过本地化处理和定制化ML/AI工具，该流程为资源有限的研究团队提供了高效的数据处理方案。

Abstract: Camera traps have long been used by wildlife researchers to monitor and study
animal behavior, population dynamics, habitat use, and species diversity in a
non-invasive and efficient manner. While data collection from the field has
increased with new tools and capabilities, methods to develop, process, and
manage the data, especially the adoption of ML/AI tools, remain challenging.
These challenges include the sheer volume of data generated, the need for
accurate labeling and annotation, variability in environmental conditions
affecting data quality, and the integration of ML/AI tools into existing
workflows that often require domain-specific customization and computational
resources. This paper provides a guide to a low-resource pipeline to process
camera trap data on-premise, incorporating ML/AI capabilities tailored for
small research groups with limited resources and computational expertise. By
focusing on practical solutions, the pipeline offers accessible approaches for
data transmission, inference, and evaluation, enabling researchers to discover
meaningful insights from their ever-increasing camera trap datasets.

</details>


### [55] [Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data](https://arxiv.org/abs/2507.09420)
*Timothy Chase Jr,Karthik Dantu*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级神经网络方法，用于实时天体表面地形特征检测与跟踪，解决了传统方法计算量大和数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 传统的光度测量方法计算成本高、处理速度慢且泛化能力有限，而新兴的基于学习的方法又面临计算资源不足和训练数据稀缺的挑战。

Method: 采用轻量级神经网络架构，改进领域适应方法用于地标检测，并提出注意力对齐机制用于地标描述。

Result: 提出的系统在性能上优于现有技术，适用于实时操作。

Conclusion: 该方法为航天器自主导航提供了高效、实时的解决方案。

Abstract: The detection and tracking of celestial surface terrain features are crucial
for autonomous spaceflight applications, including Terrain Relative Navigation
(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data
collection. Traditional photoclinometry-based pipelines often rely on extensive
a priori imaging and offline processing, constrained by the computational
limitations of radiation-hardened systems. While historically effective, these
approaches typically increase mission costs and duration, operate at low
processing rates, and have limited generalization. Recently, learning-based
computer vision has gained popularity to enhance spacecraft autonomy and
overcome these limitations. While promising, emerging techniques frequently
impose computational demands exceeding the capabilities of typical spacecraft
hardware for real-time operation and are further challenged by the scarcity of
labeled training data for diverse extraterrestrial environments. In this work,
we present novel formulations for in-situ landmark tracking via detection and
description. We utilize lightweight, computationally efficient neural network
architectures designed for real-time execution on current-generation spacecraft
flight processors. For landmark detection, we propose improved domain
adaptation methods that enable the identification of celestial terrain features
with distinct, cheaply acquired training data. Concurrently, for landmark
description, we introduce a novel attention alignment formulation that learns
robust feature representations that maintain correspondence despite significant
landmark viewpoint variations. Together, these contributions form a unified
system for landmark tracking that demonstrates superior performance compared to
existing state-of-the-art techniques.

</details>


### [56] [Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions](https://arxiv.org/abs/2507.09446)
*Yuanhong Zheng,Ruixuan Yu,Jian Sun*

Main category: cs.CV

TL;DR: 提出了一种计算高效的多人3D运动预测模型，通过简化时空交互，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 多人3D运动预测复杂且计算成本高，需要同时考虑个体运动和交互。

Method: 设计轻量级双分支分别学习局部和全局表示，并引入跨级交互块和空间距离嵌入。

Result: 在CMU-Mocap、MuPoTS-3D和3DPW数据集上取得最优性能，同时显著降低计算成本。

Conclusion: 该方法在高效建模时空交互方面表现出色，为多人运动预测提供了实用解决方案。

Abstract: 3D multi-person motion prediction is a highly complex task, primarily due to
the dependencies on both individual past movements and the interactions between
agents. Moreover, effectively modeling these interactions often incurs
substantial computational costs. In this work, we propose a computationally
efficient model for multi-person motion prediction by simplifying spatial and
temporal interactions. Our approach begins with the design of lightweight dual
branches that learn local and global representations for individual and
multiple persons separately. Additionally, we introduce a novel cross-level
interaction block to integrate the spatial and temporal representations from
both branches. To further enhance interaction modeling, we explicitly
incorporate the spatial inter-person distance embedding. With above efficient
temporal and spatial design, we achieve state-of-the-art performance for
multiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while
significantly reducing the computational cost. Code is available at
https://github.com/Yuanhong-Zheng/EMPMP.

</details>


### [57] [SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation](https://arxiv.org/abs/2507.09459)
*Zhihan Kang,Boyu Wang*

Main category: cs.CV

TL;DR: SegVec3D是一个新颖的3D点云实例分割框架，结合注意力机制、嵌入学习和跨模态对齐，支持无监督实例分割和零样本检索。


<details>
  <summary>Details</summary>
Motivation: 解决3D点云实例分割中几何结构建模和多模态理解的统一问题，同时减少监督需求并提高实用性。

Method: 构建分层特征提取器增强几何建模，通过对比聚类实现无监督实例分割，并在共享语义空间中对齐3D数据与自然语言查询。

Result: 在实例分割和多模态理解方面优于Mask3D和ULIP等方法，且具有更少的监督需求和更高的实用性。

Conclusion: SegVec3D成功统一了实例分割和多模态理解，为3D点云处理提供了高效且实用的解决方案。

Abstract: We propose SegVec3D, a novel framework for 3D point cloud instance
segmentation that integrates attention mechanisms, embedding learning, and
cross-modal alignment. The approach builds a hierarchical feature extractor to
enhance geometric structure modeling and enables unsupervised instance
segmentation via contrastive clustering. It further aligns 3D data with natural
language queries in a shared semantic space, supporting zero-shot retrieval.
Compared to recent methods like Mask3D and ULIP, our method uniquely unifies
instance segmentation and multimodal understanding with minimal supervision and
practical deployability.

</details>


### [58] [CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning](https://arxiv.org/abs/2507.09471)
*Lingfeng He,De Cheng,Zhiheng Ma,Huaijie Wang,Dingwen Zhang,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: CKAA框架通过双级知识对齐和任务置信度引导的适配器混合，提升了持续学习模型对误导任务ID的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决参数高效微调（PEFT）方法中因独立训练子模块导致特征子空间不对齐，从而在误导任务ID下产生模糊决策的问题。

Method: 提出双级知识对齐（DKA）和任务置信度引导的适配器混合（TC-MoA），分别用于训练和推理阶段。

Result: 实验表明CKAA优于现有PEFT-based CL方法。

Conclusion: CKAA通过知识对齐和自适应聚合，显著提升了模型对误导任务ID的鲁棒性。

Abstract: Continual Learning (CL) empowers AI models to continuously learn from
sequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based
CL methods have garnered increasing attention due to their superior
performance. They typically allocate a unique sub-module for learning each
task, with a task recognizer to select the appropriate sub-modules for testing
images. However, due to the feature subspace misalignment from independently
trained sub-modules, these methods tend to produce ambiguous decisions under
misleading task-ids. To address this, we propose Cross-subspace Knowledge
Alignment and Aggregation (CKAA), a novel framework that enhances model
robustness against misleading task-ids through two key innovations: (1)
Dual-level Knowledge Alignment (DKA): By aligning intra-class feature
distributions across different subspaces and learning a robust global
classifier through a feature simulation process, DKA enables the model to
distinguish features from both correct and incorrect subspaces during training.
(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference
scheme that adaptively aggregates task-specific knowledge from relevant
sub-modules based on task-confidence scores, avoiding overconfidence in
misleading task-id predictions. Extensive experiments demonstrate that CKAA
outperforms existing PEFT-based CL methods.

</details>


### [59] [HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space](https://arxiv.org/abs/2507.09487)
*Changli Wang,Fang Yin,Jiafeng Liu,Rui Wu*

Main category: cs.CV

TL;DR: HMID-Net提出了一种在双曲空间中结合掩码图像建模（MIM）和知识蒸馏的高效方法，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 视觉和语义概念通常以层次结构组织，但如何高效训练模型以捕捉和利用这种层次结构仍是一个关键问题。

Method: HMID-Net在双曲空间中整合MIM和知识蒸馏技术，并设计了专门的双曲空间蒸馏损失函数。

Result: 实验表明，该方法在图像分类和检索任务中显著优于MERU和CLIP等模型。

Conclusion: 双曲空间中的MIM和知识蒸馏技术能取得与欧几里得空间相同的显著成功。

Abstract: Visual and semantic concepts are often structured in a hierarchical manner.
For instance, textual concept `cat' entails all images of cats. A recent study,
MERU, successfully adapts multimodal learning techniques from Euclidean space
to hyperbolic space, effectively capturing the visual-semantic hierarchy.
However, a critical question remains: how can we more efficiently train a model
to capture and leverage this hierarchy? In this paper, we propose the
\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel
and efficient method that integrates Masked Image Modeling (MIM) and knowledge
distillation techniques within hyperbolic space. To the best of our knowledge,
this is the first approach to leverage MIM and knowledge distillation in
hyperbolic space to train highly efficient models. In addition, we introduce a
distillation loss function specifically designed to facilitate effective
knowledge transfer in hyperbolic space. Our experiments demonstrate that MIM
and knowledge distillation techniques in hyperbolic space can achieve the same
remarkable success as in Euclidean space. Extensive evaluations show that our
method excels across a wide range of downstream tasks, significantly
outperforming existing models like MERU and CLIP in both image classification
and retrieval.

</details>


### [60] [GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?](https://arxiv.org/abs/2507.09491)
*Yiyang Zhou,Linjie Li,Shi Qiu,Zhengyuan Yang,Yuyang Zhao,Siwei Han,Yangfan He,Kangqi Li,Haonian Ji,Zihao Zhao,Haibo Tong,Lijuan Wang,Huaxiu Yao*

Main category: cs.CV

TL;DR: GLIMPSE是一个新的视频基准测试，旨在评估大型视觉语言模型（LVLMs）是否能真正理解视频内容，而不仅仅是静态帧分析。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准测试往往依赖静态帧分析，无法评估模型是否具备深度时间推理能力。

Method: GLIMPSE包含3,269个视频和4,342个视觉中心问题，覆盖11个类别，要求模型观看完整视频并进行推理。

Result: 人类评估准确率为94.82%，而最佳模型GPT-o3仅达到66.43%，显示LVLMs在深度视频理解上仍有挑战。

Conclusion: GLIMPSE揭示了LVLMs在真正理解视频内容方面的不足，为未来研究提供了方向。

Abstract: Existing video benchmarks often resemble image-based benchmarks, with
question types like "What actions does the person perform throughout the
video?" or "What color is the woman's dress in the video?" For these, models
can often answer by scanning just a few key frames, without deep temporal
reasoning. This limits our ability to assess whether large vision-language
models (LVLMs) can truly think with videos rather than perform superficial
frame-level analysis. To address this, we introduce GLIMPSE, a benchmark
specifically designed to evaluate whether LVLMs can genuinely think with
videos. Unlike prior benchmarks, GLIMPSE emphasizes comprehensive video
understanding beyond static image cues. It consists of 3,269 videos and over
4,342 highly visual-centric questions across 11 categories, including
Trajectory Analysis, Temporal Reasoning, and Forensics Detection. All questions
are carefully crafted by human annotators and require watching the entire video
and reasoning over full video context-this is what we mean by thinking with
video. These questions cannot be answered by scanning selected frames or
relying on text alone. In human evaluations, GLIMPSE achieves 94.82% accuracy,
but current LVLMs face significant challenges. Even the best-performing model,
GPT-o3, reaches only 66.43%, highlighting that LVLMs still struggle to move
beyond surface-level reasoning to truly think with videos.

</details>


### [61] [SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification](https://arxiv.org/abs/2507.09492)
*Fuyin Ye,Erwen Yao,Jianyong Chen,Fengmei He,Junxiang Zhang,Lihao Ni*

Main category: cs.CV

TL;DR: 提出了一种结合张量分解和正则化的自适应性网络（SDTN）和轻量级网络（TRN），用于高光谱图像分类，显著提升了精度并降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理高维数据、光谱-空间冗余和标记样本稀缺时表现不佳，需要更高效的解决方案。

Method: SDTN通过动态调整张量秩优化特征表示，TRN则整合SDTN提取的特征，构建轻量级多尺度网络。

Result: 在PaviaU数据集上，该方法在精度和模型参数减少方面优于现有技术。

Conclusion: SDTN和TRN的组合为高光谱图像分类提供了一种高效、实时的解决方案。

Abstract: Hyperspectral image classification plays a pivotal role in precision
agriculture, providing accurate insights into crop health monitoring, disease
detection, and soil analysis. However, traditional methods struggle with
high-dimensional data, spectral-spatial redundancy, and the scarcity of labeled
samples, often leading to suboptimal performance. To address these challenges,
we propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines
tensor decomposition with regularization mechanisms to dynamically adjust
tensor ranks, ensuring optimal feature representation tailored to the
complexity of the data. Building upon SDTN, we propose the Tensor-Regularized
Network (TRN), which integrates the features extracted by SDTN into a
lightweight network capable of capturing spectral-spatial features at multiple
scales. This approach not only maintains high classification accuracy but also
significantly reduces computational complexity, making the framework highly
suitable for real-time deployment in resource-constrained environments.
Experiments on PaviaU datasets demonstrate significant improvements in accuracy
and reduced model parameters compared to state-of-the-art methods.

</details>


### [62] [Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations](https://arxiv.org/abs/2507.09500)
*Yiwen Liang,Hui Chen,Yizhe Xiong,Zihan Zhou,Mengyao Lyu,Zijia Lin,Shuaicheng Niu,Sicheng Zhao,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: 提出了一种可靠的测试时适应方法（ReTA），通过一致性感知熵重加权和多样性驱动的分布校准，解决了缓存方法在分布偏移下的不可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）在零样本任务中表现优异，但在无标注数据的分布偏移任务中性能下降，测试时适应（TTA）方法旨在提升推理性能。缓存方法虽有效，但面临熵不可靠和决策边界不灵活的问题。

Method: ReTA结合两种策略：一致性感知熵重加权（CER）用于优化缓存更新，多样性驱动的分布校准（DDC）用于自适应决策边界。

Result: 实验表明，ReTA在真实世界分布偏移任务中优于现有方法。

Conclusion: ReTA通过提升缓存质量和决策边界灵活性，显著提高了VLMs在分布偏移任务中的可靠性。

Abstract: Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but
struggle with distribution shifts in downstream tasks when labeled data is
unavailable, which has motivated the development of Test-Time Adaptation (TTA)
to improve VLMs' performance during inference without annotations. Among
various TTA approaches, cache-based methods show promise by preserving
historical knowledge from low-entropy samples in a dynamic cache and fostering
efficient adaptation. However, these methods face two critical reliability
challenges: (1) entropy often becomes unreliable under distribution shifts,
causing error accumulation in the cache and degradation in adaptation
performance; (2) the final predictions may be unreliable due to inflexible
decision boundaries that fail to accommodate large downstream shifts. To
address these challenges, we propose a Reliable Test-time Adaptation (ReTA)
method that integrates two complementary strategies to enhance reliability from
two perspectives. First, to mitigate the unreliability of entropy as a sample
selection criterion for cache construction, we introduce Consistency-aware
Entropy Reweighting (CER), which incorporates consistency constraints to weight
entropy during cache updating. While conventional approaches rely solely on low
entropy for cache prioritization and risk introducing noise, our method
leverages predictive consistency to maintain a high-quality cache and
facilitate more robust adaptation. Second, we present Diversity-driven
Distribution Calibration (DDC), which models class-wise text embeddings as
multivariate Gaussian distributions, enabling adaptive decision boundaries for
more accurate predictions across visually diverse content. Extensive
experiments demonstrate that ReTA consistently outperforms state-of-the-art
methods, particularly under challenging real-world distribution shifts.

</details>


### [63] [Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention](https://arxiv.org/abs/2507.09512)
*Pengyu Liu,Kun Li,Fei Wang,Yanyan Wei,Junhui She,Dan Guo*

Main category: cs.CV

TL;DR: HFUT-VUT团队提出了一种新方法，用于IJCAI 2025 MiGA挑战赛中的微手势在线识别任务，通过数据增强和时空注意力机制显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 微手势在线识别任务具有挑战性，需要精确定位和分类视频中的微手势实例，且微手势与其他人类动作差异较大。

Method: 采用手工数据增强和时空注意力机制，提升模型的分类和定位能力。

Result: F1分数达到38.03，比之前的最佳方法提升了37.9%，在比赛中排名第一。

Conclusion: 该方法在微手势识别任务中表现出色，为未来研究提供了新思路。

Abstract: In this paper, we introduce the latest solution developed by our team,
HFUT-VUT, for the Micro-gesture Online Recognition track of the IJCAI 2025 MiGA
Challenge. The Micro-gesture Online Recognition task is a highly challenging
problem that aims to locate the temporal positions and recognize the categories
of multiple micro-gesture instances in untrimmed videos. Compared to
traditional temporal action detection, this task places greater emphasis on
distinguishing between micro-gesture categories and precisely identifying the
start and end times of each instance. Moreover, micro-gestures are typically
spontaneous human actions, with greater differences than those found in other
human actions. To address these challenges, we propose hand-crafted data
augmentation and spatial-temporal attention to enhance the model's ability to
classify and localize micro-gestures more accurately. Our solution achieved an
F1 score of 38.03, outperforming the previous state-of-the-art by 37.9%. As a
result, our method ranked first in the Micro-gesture Online Recognition track.

</details>


### [64] [QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models](https://arxiv.org/abs/2507.09514)
*Tien-Yu Chi,Hung-Yueh Chiang,Diana Marculescu,Kai-Chiang Wu*

Main category: cs.CV

TL;DR: QuarterMap是一种后训练激活剪枝方法，通过去除冗余空间激活并恢复维度，提升VMamba等SSM模型的吞吐量，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 解决VMamba等基于状态空间模型的视觉骨干在四向扫描中的空间冗余问题，提升效率。

Method: 提出QuarterMap方法，通过后训练剪枝去除冗余空间激活，并通过最近邻上采样恢复维度。

Result: 在ImageNet-1K上实现11%的速度提升，精度下降小于0.9%；在ADE20K和MedMamba上也表现良好。

Conclusion: QuarterMap是一种即插即用的工具，适用于部署时效率提升，且不影响模型的可迁移性。

Abstract: State space models (SSMs) reduce the quadratic complexity of transformers by
leveraging linear recurrence. Recently, VMamba has emerged as a strong
SSM-based vision backbone, yet remains bottlenecked by spatial redundancy in
its four-directional scan. We propose QuarterMap, a post-training activation
pruning method that removes redundant spatial activations before scanning and
restores dimensions via nearest-neighbor upsampling. Our method improves
throughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11%
speedup on VMamba with less than 0.9% accuracy drop, and yields similar gains
on ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a
domain-specific model that shares the same four-directional scanning structure,
where it consistently improves throughput while preserving accuracy across
multiple medical imaging tasks. Compared to token merging methods like ToMe,
QuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our
method offers a plug-and-play tool for deployment-time efficiency without
compromising transferability.

</details>


### [65] [When Schrödinger Bridge Meets Real-World Image Dehazing with Unpaired Training](https://arxiv.org/abs/2507.09524)
*Yunwei Lan,Zhigao Cui,Xin Luo,Chang Liu,Nian Wang,Menglin Zhang,Yanzhao Su,Dong Liu*

Main category: cs.CV

TL;DR: 提出DehazeSB框架，基于Schrödinger Bridge和最优传输理论，解决无配对去雾中生成器映射能力不足的问题，结合细节保留正则化和CLIP模型提示学习，提升去雾效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于GAN的无配对去雾方法因生成器映射能力有限，效果受限。

Method: 利用Schrödinger Bridge和最优传输理论直接桥接雾图和清晰图的分布，引入细节保留正则化和CLIP模型提示学习。

Result: 在多个真实数据集上表现优越。

Conclusion: DehazeSB通过优化传输映射和细节保留，显著提升无配对去雾性能。

Abstract: Recent advancements in unpaired dehazing, particularly those using GANs, show
promising performance in processing real-world hazy images. However, these
methods tend to face limitations due to the generator's limited transport
mapping capability, which hinders the full exploitation of their effectiveness
in unpaired training paradigms. To address these challenges, we propose
DehazeSB, a novel unpaired dehazing framework based on the Schr\"odinger
Bridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges
the distributions between hazy and clear images. This enables optimal transport
mappings from hazy to clear images in fewer steps, thereby generating
high-quality results. To ensure the consistency of structural information and
details in the restored images, we introduce detail-preserving regularization,
which enforces pixel-level alignment between hazy inputs and dehazed outputs.
Furthermore, we propose a novel prompt learning to leverage pre-trained CLIP
models in distinguishing hazy images and clear ones, by learning a haze-aware
vision-language alignment. Extensive experiments on multiple real-world
datasets demonstrate our method's superiority. Code:
https://github.com/ywxjm/DehazeSB.

</details>


### [66] [VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization](https://arxiv.org/abs/2507.09531)
*Son Nguyen,Giang Nguyen,Hung Dao,Thao Do,Daeyoung Kim*

Main category: cs.CV

TL;DR: VDInstruct是一种多模态大语言模型，通过内容感知的标记化策略和显式布局建模，显著提升了密集文档的理解效率，并在KIE基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在密集文档上表现不佳，且视觉标记化方法存在计算冗余和内存效率低的问题。

Method: 采用内容感知的标记化策略，根据文档复杂度生成标记，并结合三阶段训练范式。

Result: 在KIE基准测试中达到SOTA，零样本评估中F1分数超过基线5.5分，图像标记数量减少3.6倍。

Conclusion: 内容感知标记化与显式布局建模为文档理解提供了有前景的方向。

Abstract: Key Information Extraction (KIE) underpins the understanding of visual
documents (e.g., receipts and contracts) by extracting precise semantic content
and accurately capturing spatial structure. Yet existing multimodal large
language models (MLLMs) often perform poorly on dense documents and rely on
vision tokenization approaches that scale with image size, leading to redundant
computation and memory inefficiency. To address these challenges, we introduce
VDInstruct, an MLLM that separates spatial region detection from semantic
feature extraction. Central to our model is a content-aware tokenization
strategy: rather than fragmenting the entire image uniformly, it generates
tokens in proportion to document complexity, preserving critical structure
while eliminating wasted tokens. Leveraging a three-stage training paradigm,
our model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching
or exceeding the accuracy of leading approaches while reducing the number of
image tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses
strong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its
robustness to unseen documents. These findings show that content-aware
tokenization combined with explicit layout modeling offers a promising
direction forward for document understanding. Data, source code, and model
weights will be made publicly available.

</details>


### [67] [DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection](https://arxiv.org/abs/2507.09541)
*Zihao Xiong,Fei Zhou,Fengyi Wu,Shuai Yuan,Maixia Fu,Zhenming Peng,Jian Yang,Yimian Dai*

Main category: cs.CV

TL;DR: 提出了一种动态RPCA网络（DRPCA-Net），通过深度展开网络结合稀疏先验，提升红外小目标检测的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在红外小目标检测中忽视了目标的稀疏性先验，导致模型复杂且泛化能力不足。

Method: 基于RPCA的模型范式，设计了动态展开机制和动态残差组模块，通过超网络生成输入相关的迭代参数。

Result: 在多个公开红外数据集上，DRPCA-Net显著优于现有方法。

Conclusion: DRPCA-Net通过动态机制和稀疏先验的结合，实现了高效且鲁棒的红外小目标检测。

Abstract: Infrared small target detection plays a vital role in remote sensing,
industrial monitoring, and various civilian applications. Despite recent
progress powered by deep learning, many end-to-end convolutional models tend to
pursue performance by stacking increasingly complex architectures, often at the
expense of interpretability, parameter efficiency, and generalization. These
models typically overlook the intrinsic sparsity prior of infrared small
targets--an essential cue that can be explicitly modeled for both performance
and efficiency gains. To address this, we revisit the model-based paradigm of
Robust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network
(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware
prior into a learnable architecture. Unlike conventional deep unfolding methods
that rely on static, globally learned parameters, DRPCA-Net introduces a
dynamic unfolding mechanism via a lightweight hypernetwork. This design enables
the model to adaptively generate iteration-wise parameters conditioned on the
input scene, thereby enhancing its robustness and generalization across diverse
backgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to
better capture contextual variations within the background, leading to more
accurate low-rank estimation and improved separation of small targets.
Extensive experiments on multiple public infrared datasets demonstrate that
DRPCA-Net significantly outperforms existing state-of-the-art methods in
detection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.

</details>


### [68] [SeqCSIST: Sequential Closely-Spaced Infrared Small Target Unmixing](https://arxiv.org/abs/2507.09556)
*Ximeng Zhai,Bohan Xu,Yaohong Chen,Hao Wang,Kehua Guo,Yimian Dai*

Main category: cs.CV

TL;DR: 论文提出了一种新任务——Sequential CSIST Unmixing，旨在从高密度红外小目标群中检测所有目标，并贡献了一个开源生态系统和DeRefNet模型。


<details>
  <summary>Details</summary>
Motivation: 由于光学镜头焦距和红外探测器分辨率的限制，远距离红外小目标群在图像中常表现为混合斑点，缺乏高质量数据集也限制了研究进展。

Method: 提出了Deformable Refinement Network（DeRefNet），包含Temporal Deformable Feature Alignment（TDFA）模块，用于自适应帧间信息聚合。

Result: 在SeqCSIST数据集上，DeRefNet的平均精度（mAP）比现有方法提高了5.3%。

Conclusion: 该研究首次在多帧范式中解决了CSIST Unmixing任务，并提供了开源数据集和工具包。

Abstract: Due to the limitation of the optical lens focal length and the resolution of
the infrared detector, distant Closely-Spaced Infrared Small Target (CSIST)
groups typically appear as mixing spots in the infrared image. In this paper,
we propose a novel task, Sequential CSIST Unmixing, namely detecting all
targets in the form of sub-pixel localization from a highly dense CSIST group.
However, achieving such precise detection is an extremely difficult challenge.
In addition, the lack of high-quality public datasets has also restricted the
research progress. To this end, firstly, we contribute an open-source
ecosystem, including SeqCSIST, a sequential benchmark dataset, and a toolkit
that provides objective evaluation metrics for this special task, along with
the implementation of 23 relevant methods. Furthermore, we propose the
Deformable Refinement Network (DeRefNet), a model-driven deep learning
framework that introduces a Temporal Deformable Feature Alignment (TDFA) module
enabling adaptive inter-frame information aggregation. To the best of our
knowledge, this work is the first endeavor to address the CSIST Unmixing task
within a multi-frame paradigm. Experiments on the SeqCSIST dataset demonstrate
that our method outperforms the state-of-the-art approaches with mean Average
Precision (mAP) metric improved by 5.3\%. Our dataset and toolkit are available
from https://github.com/GrokCV/SeqCSIST.

</details>


### [69] [EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation](https://arxiv.org/abs/2507.09560)
*Bolun Zheng,Xinjie Liu,Qianyu Zhang,Canjin Wang,Fangni Chen,Mingen Xu*

Main category: cs.CV

TL;DR: 提出了一种分段架构EHPE，专注于TIP和手腕的局部提取，以减少误差累积并提升整体手部姿态估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视TIP和手腕的重要性，导致远端关节误差累积，影响整体估计质量。

Method: EHPE分为TW阶段（提取TIP和手腕）和PG阶段（双分支交互网络细化其余关节）。

Result: 在两个广泛使用的基准测试中达到最先进性能。

Conclusion: EHPE通过分段处理显著提升了手部姿态估计的精度。

Abstract: 3D hand pose estimation has garnered great attention in recent years due to
its critical applications in human-computer interaction, virtual reality, and
related fields. The accurate estimation of hand joints is essential for
high-quality hand pose estimation. However, existing methods neglect the
importance of Distal Phalanx Tip (TIP) and Wrist in predicting hand joints
overall and often fail to account for the phenomenon of error accumulation for
distal joints in gesture estimation, which can cause certain joints to incur
larger errors, resulting in misalignments and artifacts in the pose estimation
and degrading the overall reconstruction quality. To address this challenge, we
propose a novel segmented architecture for enhanced hand pose estimation
(EHPE). We perform local extraction of TIP and wrist, thus alleviating the
effect of error accumulation on TIP prediction and further reduce the
predictive errors for all joints on this basis. EHPE consists of two key
stages: In the TIP and Wrist Joints Extraction stage (TW-stage), the positions
of the TIP and wrist joints are estimated to provide an initial accurate joint
configuration; In the Prior Guided Joints Estimation stage (PG-stage), a
dual-branch interaction network is employed to refine the positions of the
remaining joints. Extensive experiments on two widely used benchmarks
demonstrate that EHPE achieves state-of-the-arts performance. Code is available
at https://github.com/SereinNout/EHPE.

</details>


### [70] [Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges](https://arxiv.org/abs/2507.09562)
*Yidong Jiang*

Main category: cs.CV

TL;DR: 本文首次全面综述了Segment Anything Model（SAM）及其变体中的提示工程技术，揭示了其从简单几何输入到多模态方法的发展，并讨论了关键挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 探索提示工程在SAM成功中的关键作用，填补文献中对这一新兴领域系统性研究的空白。

Method: 系统整理和分析SAM及其变体中提示工程技术的研究，涵盖基础方法、实际应用和挑战。

Result: 揭示了提示工程从简单到复杂的演变，展示了SAM在医学影像和遥感等领域的适应性。

Conclusion: 本文为理解和推进分割基础模型中的提示工程提供了结构化框架，并指出了未来研究方向。

Abstract: The Segment Anything Model (SAM) has revolutionized image segmentation
through its innovative prompt-based approach, yet the critical role of prompt
engineering in its success remains underexplored. This paper presents the first
comprehensive survey focusing specifically on prompt engineering techniques for
SAM and its variants. We systematically organize and analyze the rapidly
growing body of work in this emerging field, covering fundamental
methodologies, practical applications, and key challenges. Our review reveals
how prompt engineering has evolved from simple geometric inputs to
sophisticated multimodal approaches, enabling SAM's adaptation across diverse
domains including medical imaging and remote sensing. We identify unique
challenges in prompt optimization and discuss promising research directions.
This survey fills an important gap in the literature by providing a structured
framework for understanding and advancing prompt engineering in foundation
models for segmentation.

</details>


### [71] [WordCraft: Interactive Artistic Typography with Attention Awareness and Noise Blending](https://arxiv.org/abs/2507.09573)
*Zhe Wang,Jingbo Zhang,Tianyi Wei,Wanchao Su,Can Wang*

Main category: cs.CV

TL;DR: WordCraft是一个基于扩散模型的交互式艺术字体系统，支持局部编辑、迭代优化和多语言输入，通过区域注意力机制和噪声混合技术提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统艺术字体设计依赖手工，现有生成模型缺乏交互性和灵活性，无法满足局部编辑和多字符组合的需求。

Method: 结合扩散模型，引入无训练的区域注意力机制和噪声混合技术，利用大语言模型解析用户提示。

Result: 系统能够高质量生成单字符和多字符的艺术字体，支持多语言和多样化用户需求。

Conclusion: WordCraft显著提升了艺术字体生成的交互性，为设计师提供了更多创意可能性。

Abstract: Artistic typography aims to stylize input characters with visual effects that
are both creative and legible. Traditional approaches rely heavily on manual
design, while recent generative models, particularly diffusion-based methods,
have enabled automated character stylization. However, existing solutions
remain limited in interactivity, lacking support for localized edits, iterative
refinement, multi-character composition, and open-ended prompt interpretation.
We introduce WordCraft, an interactive artistic typography system that
integrates diffusion models to address these limitations. WordCraft features a
training-free regional attention mechanism for precise, multi-region generation
and a noise blending that supports continuous refinement without compromising
visual quality. To support flexible, intent-driven generation, we incorporate a
large language model to parse and structure both concrete and abstract user
prompts. These components allow our framework to synthesize high-quality,
stylized typography across single- and multi-character inputs across multiple
languages, supporting diverse user-centered workflows. Our system significantly
enhances interactivity in artistic typography synthesis, opening up creative
possibilities for artists and designers.

</details>


### [72] [MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models](https://arxiv.org/abs/2507.09574)
*Haozhe Zhao,Zefan Cai,Shuzheng Si,Liang Chen,Jiuxiang Gu,Wen Xiao,Junjie Hu*

Main category: cs.CV

TL;DR: MENTOR是一种新型自回归框架，通过两阶段训练实现多模态输入与图像输出的细粒度对齐，无需额外适配器或交叉注意力模块，显著提升了图像生成的精确控制和效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到图像模型在精确视觉控制、多模态输入平衡和复杂多模态图像生成训练需求方面的不足。

Method: 结合自回归图像生成器和两阶段训练范式：多模态对齐阶段和指令调优阶段，实现细粒度的输入输出对齐。

Result: 在DreamBench++基准测试中表现优异，优于竞争基线，同时提供更高的图像重建保真度、广泛任务适应性和训练效率。

Conclusion: MENTOR框架在多模态图像生成中实现了高效、可控和高质量的生成效果。

Abstract: Recent text-to-image models produce high-quality results but still struggle
with precise visual control, balancing multimodal inputs, and requiring
extensive training for complex multimodal image generation. To address these
limitations, we propose MENTOR, a novel autoregressive (AR) framework for
efficient Multimodal-conditioned Tuning for Autoregressive multimodal image
generation. MENTOR combines an AR image generator with a two-stage training
paradigm, enabling fine-grained, token-level alignment between multimodal
inputs and image outputs without relying on auxiliary adapters or
cross-attention modules. The two-stage training consists of: (1) a multimodal
alignment stage that establishes robust pixel- and semantic-level alignment,
followed by (2) a multimodal instruction tuning stage that balances the
integration of multimodal inputs and enhances generation controllability.
Despite modest model size, suboptimal base components, and limited training
resources, MENTOR achieves strong performance on the DreamBench++ benchmark,
outperforming competitive baselines in concept preservation and prompt
following. Additionally, our method delivers superior image reconstruction
fidelity, broad task adaptability, and improved training efficiency compared to
diffusion-based methods. Dataset, code, and models are available at:
https://github.com/HaozheZhao/MENTOR

</details>


### [73] [Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation](https://arxiv.org/abs/2507.09577)
*Ming Yin,Fu Wang,Xujiong Ye,Yanda Meng,Zeyu Fu*

Main category: cs.CV

TL;DR: MA-SAM2改进SAM2，通过上下文感知和遮挡恢复内存模型提升手术视频分割性能，无需训练，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 解决SAM2在手术视频中因快速器械移动、频繁遮挡和复杂交互导致的性能下降问题。

Method: 提出MA-SAM2，采用上下文感知和遮挡恢复内存模型，支持多目标单循环单提示推理。

Result: 在EndoVis2017和EndoVis2018数据集上性能分别提升4.36%和6.1%。

Conclusion: MA-SAM2在手术视频分割中表现出高效性和鲁棒性，具有实际应用潜力。

Abstract: Surgical video segmentation is a critical task in computer-assisted surgery,
essential for enhancing surgical quality and patient outcomes. Recently, the
Segment Anything Model 2 (SAM2) framework has demonstrated remarkable
advancements in both image and video segmentation. However, the inherent
limitations of SAM2's greedy selection memory design are amplified by the
unique properties of surgical videos-rapid instrument movement, frequent
occlusion, and complex instrument-tissue interaction-resulting in diminished
performance in the segmentation of complex, long videos. To address these
challenges, we introduce Memory Augmented (MA)-SAM2, a training-free video
object segmentation strategy, featuring novel context-aware and
occlusion-resilient memory models. MA-SAM2 exhibits strong robustness against
occlusions and interactions arising from complex instrument movements while
maintaining accuracy in segmenting objects throughout videos. Employing a
multi-target, single-loop, one-prompt inference further enhances the efficiency
of the tracking process in multi-instrument videos. Without introducing any
additional parameters or requiring further training, MA-SAM2 achieved
performance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and
EndoVis2018 datasets, respectively, demonstrating its potential for practical
surgical applications.

</details>


### [74] [Demystifying Flux Architecture](https://arxiv.org/abs/2507.09595)
*Or Greenberg*

Main category: cs.CV

TL;DR: FLUX.1是一种基于扩散的文本到图像生成模型，性能优于主流模型，但缺乏官方技术文档。本报告通过逆向工程解析其架构。


<details>
  <summary>Details</summary>
Motivation: 由于FLUX.1缺乏官方技术文档，研究者希望通过逆向工程解析其架构，以支持未来研究和开发。

Method: 通过分析FLUX.1的源代码进行逆向工程，解析其模型架构和训练设置。

Result: 成功解析了FLUX.1的架构，为后续研究提供了基础。

Conclusion: 本报告为FLUX.1的进一步研究和应用提供了技术支持，尽管未经官方认可。

Abstract: FLUX.1 is a diffusion-based text-to-image generation model developed by Black
Forest Labs, designed to achieve faithful text-image alignment while
maintaining high image quality and diversity. FLUX is considered
state-of-the-art in text-to-image generation, outperforming popular models such
as Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly
available as open source, the authors have not released official technical
documentation detailing the model's architecture or training setup. This report
summarizes an extensive reverse-engineering effort aimed at demystifying FLUX's
architecture directly from its source code, to support its adoption as a
backbone for future research and development. This document is an unofficial
technical report and is not published or endorsed by the original developers or
their affiliated institutions.

</details>


### [75] [Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive](https://arxiv.org/abs/2507.09612)
*You Huang,Lichao Chen,Jiayi Ji,Liujuan Cao,Shengchuan Zhang,Rongrong Ji*

Main category: cs.CV

TL;DR: Inter2Former提出了一种优化密集令牌处理的计算分配方法，通过四种关键改进解决了交互式分割中速度与精度的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前密集令牌方法虽精度高但速度慢，而SAM虽快但牺牲了分割质量，因此需要一种兼顾速度与精度的解决方案。

Method: 提出Dynamic Prompt Embedding（DPE）、Dynamic Hybrid Attention（DHA）、Hybrid Mixture of Experts（HMoE）和Dynamic Local Upsampling（DLU）四种改进方法。

Result: 在高精度交互式分割基准测试中，Inter2Former在CPU设备上实现了最先进的性能和高效率。

Conclusion: Inter2Former通过优化计算分配，成功平衡了交互式分割的速度与精度，适用于实际应用场景。

Abstract: Interactive segmentation (IS) improves annotation efficiency by segmenting
target regions from user prompts, with widespread applications in real-world
scenarios. Current approaches face a critical trade-off: dense-token methods
achieve superior accuracy and detail preservation but suffer from prohibitively
slow processing on CPU devices, while the Segment Anything Model (SAM) advances
the field with sparse prompt tokens for fast inference but compromises
segmentation quality. In this paper, we propose Inter2Former to address this
challenge by optimizing computation allocation in dense-token processing, which
introduces four key enhancements. First, we propose Dynamic Prompt Embedding
(DPE) that adaptively processes only regions of interest while avoiding
additional overhead from background tokens. Second, we introduce Dynamic Hybrid
Attention (DHA), which leverages previous segmentation masks to route tokens
through either full attention (O(N2)) for boundary regions or our proposed
efficient BSQ attention (O(N)) for non-boundary regions. Third, we develop
Hybrid Mixture of Experts (HMoE), which applies similar adaptive computation
strategies in FFN modules with CPU-optimized parallel processing. Finally, we
present Dynamic Local Upsampling (DLU), a reverse operation of DPE, which
localizes objects with a lightweight MLP and performs fine-grained upsampling
only in detected regions. Experimental results on high-precision IS benchmarks
demonstrate that Inter2Former achieves SOTA performance with high efficiency on
CPU devices.

</details>


### [76] [Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score](https://arxiv.org/abs/2507.09615)
*Eman Ali,Sathira Silva,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: FAIR方法通过动态对齐图像和文本特征，改进了无监督适应中的伪标签生成，显著提升了细粒度分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在细粒度分类中无法捕捉动态的类别差异或计算成本高，FAIR旨在解决这些问题。

Method: FAIR通过类描述锚点（CDA）和学习对齐分数（LAS）动态对齐特征，并引入自训练加权机制优化伪标签。

Result: FAIR在13个细粒度数据集上平均性能提升2.78%，优于现有方法。

Conclusion: FAIR通过动态跨模态交互和伪标签优化，显著提升了无监督适应的性能。

Abstract: Vision-language models (VLMs) like CLIP excel in zero-shot learning by
aligning image and text representations through contrastive pretraining.
Existing approaches to unsupervised adaptation (UA) for fine-grained
classification with VLMs either rely on fixed alignment scores that cannot
capture evolving, subtle class distinctions or use computationally expensive
pseudo-labeling strategies that limit scalability. In contrast, we show that
modeling fine-grained cross-modal interactions during adaptation produces more
accurate, class-discriminative pseudo-labels and substantially improves
performance over state-of-the-art (SOTA) methods. We introduce Fine-grained
Alignment and Interaction Refinement (FAIR), an innovative approach that
dynamically aligns localized image features with descriptive language
embeddings through a set of Class Description Anchors (CDA). This enables the
definition of a Learned Alignment Score (LAS), which incorporates CDA as an
adaptive classifier, facilitating cross-modal interactions to improve
self-training in unsupervised adaptation. Furthermore, we propose a
self-training weighting mechanism designed to refine pseudo-labels in the
presence of inter-class ambiguities. Our approach, FAIR, delivers a substantial
performance boost in fine-grained unsupervised adaptation, achieving a notable
overall gain of 2.78% across 13 fine-grained datasets compared to SOTA methods.

</details>


### [77] [Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection](https://arxiv.org/abs/2507.09619)
*Yilin Lu,Jianghang Lin,Linhuang Xie,Kai Zhao,Yansong Qu,Shengchuan Zhang,Liujuan Cao,Rongrong Ji*

Main category: cs.CV

TL;DR: GAA提出了一种基于区域引导的少样本异常图像-掩码对生成框架，利用预训练的潜在扩散模型生成真实、多样且语义对齐的异常，解决了现有方法在真实性和泛化性上的不足。


<details>
  <summary>Details</summary>
Motivation: 工业制造中异常样本稀缺，现有数据增强方法在真实性、掩码对齐和泛化性上表现不佳。

Method: GAA通过局部概念分解建模异常语义和空间信息，结合自适应多轮异常聚类和区域引导掩码生成策略，提升异常生成质量。

Result: 在MVTec AD和LOCO数据集上，GAA在异常合成质量和下游任务（如定位和分类）中表现优异。

Conclusion: GAA框架显著提升了异常生成的真实性和实用性，为工业异常检测提供了有效的数据增强解决方案。

Abstract: Anomaly inspection plays a vital role in industrial manufacturing, but the
scarcity of anomaly samples significantly limits the effectiveness of existing
methods in tasks such as localization and classification. While several anomaly
synthesis approaches have been introduced for data augmentation, they often
struggle with low realism, inaccurate mask alignment, and poor generalization.
To overcome these limitations, we propose Generate Aligned Anomaly (GAA), a
region-guided, few-shot anomaly image-mask pair generation framework. GAA
leverages the strong priors of a pretrained latent diffusion model to generate
realistic, diverse, and semantically aligned anomalies using only a small
number of samples. The framework first employs Localized Concept Decomposition
to jointly model the semantic features and spatial information of anomalies,
enabling flexible control over the type and location of anomalies. It then
utilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained
semantic clustering of anomaly concepts, thereby enhancing the consistency of
anomaly representations. Subsequently, a region-guided mask generation strategy
ensures precise alignment between anomalies and their corresponding masks,
while a low-quality sample filtering module is introduced to further improve
the overall quality of the generated samples. Extensive experiments on the
MVTec AD and LOCO datasets demonstrate that GAA achieves superior performance
in both anomaly synthesis quality and downstream tasks such as localization and
classification.

</details>


### [78] [Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI](https://arxiv.org/abs/2507.09630)
*Shomukh Qari,Maha A. Thafar*

Main category: cs.CV

TL;DR: 该研究提出了一种基于MaxViT的AI框架，用于CT扫描图像的多类中风分类，结合数据增强和XAI技术，实现了高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 中风是全球主要死因之一，早期准确诊断对改善患者预后至关重要，尤其是在紧急情况下。

Method: 采用MaxViT作为主要深度学习模型，结合其他Transformer变体和数据增强技术，并集成Grad-CAM++提供可解释性。

Result: MaxViT模型在增强数据上表现最佳，准确率和F1分数达98.00%，优于其他模型和基线方法。

Conclusion: 该研究为中风诊断提供了可信赖的AI辅助工具，有助于临床实践中的早期检测和及时干预。

Abstract: Stroke is one of the leading causes of death globally, making early and
accurate diagnosis essential for improving patient outcomes, particularly in
emergency settings where timely intervention is critical. CT scans are the key
imaging modality because of their speed, accessibility, and cost-effectiveness.
This study proposed an artificial intelligence framework for multiclass stroke
classification (ischemic, hemorrhagic, and no stroke) using CT scan images from
a dataset provided by the Republic of Turkey's Ministry of Health. The proposed
method adopted MaxViT, a state-of-the-art Vision Transformer, as the primary
deep learning model for image-based stroke classification, with additional
transformer variants (vision transformer, transformer-in-transformer, and
ConvNext). To enhance model generalization and address class imbalance, we
applied data augmentation techniques, including synthetic image generation. The
MaxViT model trained with augmentation achieved the best performance, reaching
an accuracy and F1-score of 98.00%, outperforming all other evaluated models
and the baseline methods. The primary goal of this study was to distinguish
between stroke types with high accuracy while addressing crucial issues of
transparency and trust in artificial intelligence models. To achieve this,
Explainable Artificial Intelligence (XAI) was integrated into the framework,
particularly Grad-CAM++. It provides visual explanations of the model's
decisions by highlighting relevant stroke regions in the CT scans and
establishing an accurate, interpretable, and clinically applicable solution for
early stroke detection. This research contributed to the development of a
trustworthy AI-assisted diagnostic tool for stroke, facilitating its
integration into clinical practice and enhancing access to timely and optimal
stroke diagnosis in emergency departments, thereby saving more lives.

</details>


### [79] [Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams](https://arxiv.org/abs/2507.09640)
*Leonor Fernandes,Tiago Gonçalves,João Matos,Luis Filipe Nakayama,Jaime S. Cardoso*

Main category: cs.CV

TL;DR: 该研究评估了三种AI模型在糖尿病视网膜病变（DR）预测中的公平性和性能，并探讨了解缠技术对减少偏差的效果。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是导致工作年龄成年人视力丧失的主要原因，传统筛查方法成本高且难以普及，AI算法提供了可扩展的解决方案，但公平性和泛化能力仍需验证。

Method: 使用mBRSET眼底数据集，训练ConvNeXt V2、DINOv2和Swin V2模型预测DR及敏感属性（如年龄和性别），评估公平性并应用解缠技术减少偏差。

Result: 所有模型在DR预测中表现优异（最高94% AUROC），但公平性评估显示存在差异（如DINOv2中年龄组间10% AUROC差距）。解缠技术对模型效果影响不一。

Conclusion: 研究强调了医学影像AI中公平性的重要性，解缠技术的效果因模型而异，需进一步优化以实现公平可靠的医疗解决方案。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss in working-age
adults. While screening reduces the risk of blindness, traditional imaging is
often costly and inaccessible. Artificial intelligence (AI) algorithms present
a scalable diagnostic solution, but concerns regarding fairness and
generalization persist. This work evaluates the fairness and performance of
image-trained models in DR prediction, as well as the impact of disentanglement
as a bias mitigation technique, using the diverse mBRSET fundus dataset. Three
models, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to
predict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness
was assessed between subgroups of SAs, and disentanglement was applied to
reduce bias. All models achieved high DR prediction performance in diagnosing
(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%
AUROC, respectively). Fairness assessment suggests disparities, such as a 10%
AUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction
had varying results, depending on the model selected. Disentanglement improved
DINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2
and Swin V2 (7% and 3%, respectively). These findings highlight the complexity
of disentangling fine-grained features in fundus imaging and emphasize the
importance of fairness in medical imaging AI to ensure equitable and reliable
healthcare solutions.

</details>


### [80] [EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR](https://arxiv.org/abs/2507.09649)
*Zhengyuan Peng,Jianqing Xu,Shen Li,Jiazhen Ji,Yuge Huang,Jingyun Zhang,Jinmin Li,Shouhong Ding,Rizen Guo,Xin Tan,Lizhuang Ma*

Main category: cs.CV

TL;DR: EyeSeg是一种新颖的眼部分割框架，通过贝叶斯不确定性学习解决运动模糊、眼睑遮挡和域差距问题，提升AR/VR中的眼部分割和注视估计性能。


<details>
  <summary>Details</summary>
Motivation: 现有眼部分割方法在运动模糊、眼睑遮挡和域差距情况下性能不佳，需要一种鲁棒性更强的解决方案。

Method: 设计了一个基于贝叶斯不确定性学习的眼部分割框架，显式建模不确定性，并通过后验学习量化分割不确定性。

Result: EyeSeg在MIoU、E1、F1和ACC等指标上超越现有方法，尤其在运动模糊和跨域挑战下表现优异。

Conclusion: EyeSeg通过不确定性建模显著提升了眼部分割的鲁棒性和准确性，为AR/VR中的注视估计提供了更可靠的基础。

Abstract: Human-machine interaction through augmented reality (AR) and virtual reality
(VR) is increasingly prevalent, requiring accurate and efficient gaze
estimation which hinges on the accuracy of eye segmentation to enable smooth
user experiences. We introduce EyeSeg, a novel eye segmentation framework
designed to overcome key challenges that existing approaches struggle with:
motion blur, eyelid occlusion, and train-test domain gaps. In these situations,
existing models struggle to extract robust features, leading to suboptimal
performance. Noting that these challenges can be generally quantified by
uncertainty, we design EyeSeg as an uncertainty-aware eye segmentation
framework for AR/VR wherein we explicitly model the uncertainties by performing
Bayesian uncertainty learning of a posterior under the closed set prior.
Theoretically, we prove that a statistic of the learned posterior indicates
segmentation uncertainty levels and empirically outperforms existing methods in
downstream tasks, such as gaze estimation. EyeSeg outputs an uncertainty score
and the segmentation result, weighting and fusing multiple gaze estimates for
robustness, which proves to be effective especially under motion blur, eyelid
occlusion and cross-domain challenges. Moreover, empirical results suggest that
EyeSeg achieves segmentation improvements of MIoU, E1, F1, and ACC surpassing
previous approaches. The code is publicly available at
https://github.com/JethroPeng/EyeSeg.

</details>


### [81] [VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation](https://arxiv.org/abs/2507.09672)
*Xinyu Zhang,Zhonghao Ye,Jingwei Zhang,Xiang Tian,Zhisheng Liang,Shipeng Yu*

Main category: cs.CV

TL;DR: VST-Pose是一种基于WiFi的深度学习框架，通过双流时空注意力架构和速度建模分支，实现了高精度的人体姿态估计，在智能家居场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: WiFi姿态估计因其穿透性和隐私优势成为非视觉替代方案，但现有方法在精度和连续性上仍有不足。

Method: 提出ViSTA-Former双流时空注意力架构，分别捕捉时间依赖和关节结构关系，并集成速度建模分支以增强细微动作敏感性。

Result: 在自建数据集上PCK@50准确率达92.2%，优于现有方法8.3%，在公共MMFi数据集上也验证了其鲁棒性。

Conclusion: VST-Pose为室内连续运动分析提供了可靠且隐私保护的解决方案。

Abstract: WiFi-based human pose estimation has emerged as a promising non-visual
alternative approaches due to its pene-trability and privacy advantages. This
paper presents VST-Pose, a novel deep learning framework for accurate and
continuous pose estimation using WiFi channel state information. The proposed
method introduces ViSTA-Former, a spatiotemporal attention backbone with
dual-stream architecture that adopts a dual-stream architecture to separately
capture temporal dependencies and structural relationships among body joints.
To enhance sensitivity to subtle human motions, a velocity modeling branch is
integrated into the framework, which learns short-term keypoint dis-placement
patterns and improves fine-grained motion representation. We construct a 2D
pose dataset specifically designed for smart home care scenarios and
demonstrate that our method achieves 92.2% accuracy on the PCK@50 metric,
outperforming existing methods by 8.3% in PCK@50 on the self-collected dataset.
Further evaluation on the public MMFi dataset confirms the model's robustness
and effectiveness in 3D pose estimation tasks. The proposed system provides a
reliable and privacy-aware solution for continuous human motion analysis in
indoor environments. Our codes are available in
https://github.com/CarmenQing/VST-Pose.

</details>


### [82] [ExpStar: Towards Automatic Commentary Generation for Multi-discipline Scientific Experiments](https://arxiv.org/abs/2507.09693)
*Jiali Chen,Yujie Jia,Zihan Wu,Jinyu Yang,Jianpeng Chen,Xusen Hei,Jiayuan Xie,Yi Cai,Qing Li*

Main category: cs.CV

TL;DR: 论文提出自动生成多学科科学实验评论的任务，构建了首个实验评论数据集ExpInstruct，并提出检索增强模型ExpStar，显著优于现有大型多模态模型。


<details>
  <summary>Details</summary>
Motivation: 解决人工教师准备实验评论耗时且依赖专业知识的问题，探索大型多模态模型在生成细粒度实验评论中的潜力。

Method: 构建ExpInstruct数据集（7K+评论，21学科），提出检索增强模型ExpStar，自适应访问和利用外部知识。

Result: ExpStar显著优于14种领先的大型多模态模型，验证了数据集和模型的有效性。

Conclusion: ExpStar在AI辅助科学实验教学中具有重要潜力，推动了自动实验评论生成的发展。

Abstract: Experiment commentary is crucial in describing the experimental procedures,
delving into underlying scientific principles, and incorporating
content-related safety guidelines. In practice, human teachers rely heavily on
subject-specific expertise and invest significant time preparing such
commentary. To address this challenge, we introduce the task of automatic
commentary generation across multi-discipline scientific experiments. While
recent progress in large multimodal models (LMMs) has demonstrated promising
capabilities in video understanding and reasoning, their ability to generate
fine-grained and insightful experiment commentary remains largely
underexplored. In this paper, we make the following contributions: (i) We
construct \textit{ExpInstruct}, the first dataset tailored for experiment
commentary generation, featuring over 7\textit{K} step-level commentaries
across 21 scientific subjects from 3 core disciplines (\ie, science, healthcare
and engineering). Each sample includes procedural descriptions along with
potential scientific principles (\eg, chemical equations and physical laws) and
safety guidelines. (ii) We propose ExpStar, an automatic experiment commentary
generation model that leverages a retrieval-augmented mechanism to adaptively
access, evaluate, and utilize external knowledge. (iii) Extensive experiments
show that our ExpStar substantially outperforms 14 leading LMMs, which
highlights the superiority of our dataset and model. We believe that ExpStar
holds great potential for advancing AI-assisted scientific experiment
instruction.

</details>


### [83] [Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI](https://arxiv.org/abs/2507.09702)
*Phat Nguyen,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: 论文系统分类和比较了令牌压缩技术，并评估了其在标准和紧凑ViT架构上的表现，发现这些方法在紧凑设计上效果较差。


<details>
  <summary>Details</summary>
Motivation: 填补令牌压缩技术在系统分类和紧凑ViT架构应用上的研究空白。

Method: 提出首个系统分类法，并在标准和紧凑ViT架构上评估代表性令牌压缩技术。

Result: 令牌压缩方法在通用ViT上有效，但在紧凑设计上表现不佳。

Conclusion: 研究为未来适应紧凑Transformer的令牌优化技术提供了方向。

Abstract: Token compression techniques have recently emerged as powerful tools for
accelerating Vision Transformer (ViT) inference in computer vision. Due to the
quadratic computational complexity with respect to the token sequence length,
these methods aim to remove less informative tokens before the attention layers
to improve inference throughput. While numerous studies have explored various
accuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain.
First, there is a lack of unified survey that systematically categorizes and
compares token compression approaches based on their core strategies (e.g.,
pruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs.
plug-in). Second, most benchmarks are limited to standard ViT models (e.g.,
ViT-B, ViT-L), leaving open the question of whether such methods remain
effective when applied to structurally compressed transformers, which are
increasingly deployed on resource-constrained edge devices. To address these
gaps, we present the first systematic taxonomy and comparative study of token
compression methods, and we evaluate representative techniques on both standard
and compact ViT architectures. Our experiments reveal that while token
compression methods are effective for general-purpose ViTs, they often
underperform when directly applied to compact designs. These findings not only
provide practical insights but also pave the way for future research on
adapting token optimization techniques to compact transformer-based networks
for edge AI and AI agent applications.

</details>


### [84] [Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation](https://arxiv.org/abs/2507.09748)
*Yu Lei,Bingde Liu,Qingsong Xie,Haonan Lu,Zhijie Deng*

Main category: cs.CV

TL;DR: 本文提出了一种改进的变分分数蒸馏方法（$L^2$-VSD），通过线性化模型和前瞻优化顺序，解决了传统VSD方法收敛慢和不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 传统变分分数蒸馏（VSD）在实际应用中存在收敛慢和不稳定的问题，本文旨在通过优化模型交互和引入线性化方法提升生成质量。

Method: 提出线性化前瞻变分分数蒸馏（$L^2$-VSD），通过调整优化顺序和使用线性化模型改进梯度校正，提升训练稳定性。

Result: 实验表明$L^2$-VSD在生成质量和稳定性上优于现有方法，并能无缝集成到其他VSD框架中。

Conclusion: $L^2$-VSD通过线性化和前瞻优化显著提升了VSD的性能，为文本到3D生成提供了更高效的解决方案。

Abstract: Text-to-3D generation based on score distillation of pre-trained 2D diffusion
models has gained increasing interest, with variational score distillation
(VSD) as a remarkable example. VSD proves that vanilla score distillation can
be improved by introducing an extra score-based model, which characterizes the
distribution of images rendered from 3D models, to correct the distillation
gradient. Despite the theoretical foundations, VSD, in practice, is likely to
suffer from slow and sometimes ill-posed convergence. In this paper, we perform
an in-depth investigation of the interplay between the introduced score model
and the 3D model, and find that there exists a mismatching problem between LoRA
and 3D distributions in practical implementation. We can simply adjust their
optimization order to improve the generation quality. By doing so, the score
model looks ahead to the current 3D state and hence yields more reasonable
corrections. Nevertheless, naive lookahead VSD may suffer from unstable
training in practice due to the potential over-fitting. To address this, we
propose to use a linearized variant of the model for score distillation, giving
rise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD).
$L^2$-VSD can be realized efficiently with forward-mode autodiff
functionalities of existing deep learning libraries. Extensive experiments
validate the efficacy of $L^2$-VSD, revealing its clear superiority over prior
score distillation-based methods. We also show that our method can be
seamlessly incorporated into any other VSD-based text-to-3D framework.

</details>


### [85] [Pairwise Alignment & Compatibility for Arbitrarily Irregular Image Fragments](https://arxiv.org/abs/2507.09767)
*Ofir Itzhak Shahar,Gur Elkin,Ohad Ben-Shahar*

Main category: cs.CV

TL;DR: 提出了一种高效的混合（几何和图像）方法，用于计算碎片对的最佳对齐，无需假设其形状、尺寸或图像内容。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理真实拼图中碎片的几何特性，且依赖碎片形状的限制。

Method: 结合几何和图像信息计算碎片对齐，引入新的碎片数据集和侵蚀模型。

Result: 在RePAIR 2D数据集上实现了最先进的邻域级精度和召回率。

Conclusion: 该方法显著提升了兼容性性能，适用于考古拼图等实际应用。

Abstract: Pairwise compatibility calculation is at the core of most
fragments-reconstruction algorithms, in particular those designed to solve
different types of the jigsaw puzzle problem. However, most existing approaches
fail, or aren't designed to deal with fragments of realistic geometric
properties one encounters in real-life puzzles. And in all other cases,
compatibility methods rely strongly on the restricted shapes of the fragments.
In this paper, we propose an efficient hybrid (geometric and pictorial)
approach for computing the optimal alignment for pairs of fragments, without
any assumptions about their shapes, dimensions, or pictorial content. We
introduce a new image fragments dataset generated via a novel method for image
fragmentation and a formal erosion model that mimics real-world archaeological
erosion, along with evaluation metrics for the compatibility task. We then
embed our proposed compatibility into an archaeological puzzle-solving
framework and demonstrate state-of-the-art neighborhood-level precision and
recall on the RePAIR 2D dataset, directly reflecting compatibility performance
improvements.

</details>


### [86] [NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection](https://arxiv.org/abs/2507.09795)
*Amirhossein Ansari,Ke Wang,Pulei Xiong*

Main category: cs.CV

TL;DR: NegRefine提出了一种改进的负标签细化框架，通过过滤子类别标签和专有名词，并引入多匹配感知评分函数，提升了零样本OOD检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于负标签的方法（如NegLabel和CSP）在区分OOD样本时存在误判问题，尤其是当负标签为子类别或专有名词时。

Method: NegRefine通过过滤机制排除子类别标签和专有名词，并采用动态调整的多匹配感知评分函数。

Result: 在ImageNet-1K等大规模基准测试中表现优异。

Conclusion: NegRefine显著提升了OOD检测的准确性，解决了现有方法的局限性。

Abstract: Recent advancements in Vision-Language Models like CLIP have enabled
zero-shot OOD detection by leveraging both image and textual label information.
Among these, negative label-based methods such as NegLabel and CSP have shown
promising results by utilizing a lexicon of words to define negative labels for
distinguishing OOD samples. However, these methods suffer from detecting
in-distribution samples as OOD due to negative labels that are subcategories of
in-distribution labels or proper nouns. They also face limitations in handling
images that match multiple in-distribution and negative labels. We propose
NegRefine, a novel negative label refinement framework for zero-shot OOD
detection. By introducing a filtering mechanism to exclude subcategory labels
and proper nouns from the negative label set and incorporating a
multi-matching-aware scoring function that dynamically adjusts the
contributions of multiple labels matching an image, NegRefine ensures a more
robust separation between in-distribution and OOD samples. We evaluate
NegRefine on large-scale benchmarks, including ImageNet-1K. Source code is
available at https://github.com/ah-ansari/NegRefine.

</details>


### [87] [VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding](https://arxiv.org/abs/2507.09815)
*Younggun Kim,Ahmed S. Abdelrahman,Mohamed Abdel-Aty*

Main category: cs.CV

TL;DR: VRU-Accident是一个用于评估多模态大语言模型（MLLMs）在涉及弱势道路使用者（VRUs）的高风险交通场景中推理能力的大规模视觉语言基准。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统中弱势道路使用者（如行人和骑行者）安全问题，现有MLLMs缺乏标准化评估其在复杂、安全关键场景中的推理能力。

Method: 提出VRU-Accident基准，包含1K真实事故视频、6K多选题对、1K密集场景描述，聚焦VRU-车辆事故的时空动态和因果语义。

Result: 评估17个先进MLLMs发现，模型在视觉属性上表现良好，但在推理事故原因、类型和可预防性方面存在显著挑战。

Conclusion: VRU-Accident填补了MLLMs在安全关键场景评估的空白，揭示了模型在复杂推理任务中的不足。

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, is a critical challenge for autonomous driving systems, as crashes
involving VRUs often result in severe or fatal consequences. While multimodal
large language models (MLLMs) have shown promise in enhancing scene
understanding and decision making in autonomous vehicles, there is currently no
standardized benchmark to quantitatively evaluate their reasoning abilities in
complex, safety-critical scenarios involving VRUs. To address this gap, we
present VRU-Accident, a large-scale vision-language benchmark designed to
evaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident
comprises 1K real-world dashcam accident videos, annotated with 6K
multiple-choice question-answer pairs across six safety-critical categories
(with 24K candidate options and 3.4K unique answer choices), as well as 1K
dense scene descriptions. Unlike prior works, our benchmark focuses explicitly
on VRU-vehicle accidents, providing rich, fine-grained annotations that capture
both spatial-temporal dynamics and causal semantics of accidents. To assess the
current landscape of MLLMs, we conduct a comprehensive evaluation of 17
state-of-the-art models on the multiple-choice VQA task and on the dense
captioning task. Our findings reveal that while MLLMs perform reasonably well
on visually grounded attributes, they face significant challenges in reasoning
and describing accident causes, types, and preventability.

</details>


### [88] [Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models](https://arxiv.org/abs/2507.09830)
*Shuhao Fu,Philip J. Kellman,Hongjing Lu*

Main category: cs.CV

TL;DR: 论文探讨了人类与深度学习模型在识别3D物体时的表现差异，发现视觉变换器模型更接近人类表现。


<details>
  <summary>Details</summary>
Motivation: 研究深度学习模型是否形成与人类相似的3D形状表征。

Method: 通过实验操纵点密度、物体方向和局部几何结构，比较人类与两种深度学习模型（DGCNN和点变换器）的表现。

Result: 点变换器模型在解释人类表现上优于卷积模型，因其支持3D形状的层次抽象。

Conclusion: 点变换器模型在3D物体识别中更接近人类视觉表征。

Abstract: Both humans and deep learning models can recognize objects from 3D shapes
depicted with sparse visual information, such as a set of points randomly
sampled from the surfaces of 3D objects (termed a point cloud). Although deep
learning models achieve human-like performance in recognizing objects from 3D
shapes, it remains unclear whether these models develop 3D shape
representations similar to those used by human vision for object recognition.
We hypothesize that training with 3D shapes enables models to form
representations of local geometric structures in 3D shapes. However, their
representations of global 3D object shapes may be limited. We conducted two
human experiments systematically manipulating point density and object
orientation (Experiment 1), and local geometric structure (Experiment 2).
Humans consistently performed well across all experimental conditions. We
compared two types of deep learning models, one based on a convolutional neural
network (DGCNN) and the other on visual transformers (point transformer), with
human performance. We found that the point transformer model provided a better
account of human performance than the convolution-based model. The advantage
mainly results from the mechanism in the point transformer model that supports
hierarchical abstraction of 3D shapes.

</details>


### [89] [A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends](https://arxiv.org/abs/2507.09861)
*Yihao Ding,Siwen Luo,Yue Dai,Yanbei Jiang,Zechuan Li,Geoffrey Martin,Yifan Peng*

Main category: cs.CV

TL;DR: 综述了基于多模态大语言模型（MLLM）的视觉丰富文档理解（VRDU）的最新进展，包括特征编码与融合、训练范式及数据集，并探讨了未来方向。


<details>
  <summary>Details</summary>
Motivation: VRDU因需要自动处理包含复杂视觉、文本和布局信息的文档而成为关键领域，MLLM在此领域展现出潜力。

Method: 分析了MLLM在VRDU中的三个核心组件：特征编码与融合方法、训练范式（预训练、指令响应调优等）及数据集。

Result: 总结了MLLM在VRDU中的应用进展，强调了其在信息提取与解释方面的潜力。

Conclusion: 讨论了VRDU领域的挑战与机遇，提出了提升系统效率、泛化性和鲁棒性的未来方向。

Abstract: Visually-Rich Document Understanding (VRDU) has emerged as a critical field,
driven by the need to automatically process documents containing complex
visual, textual, and layout information. Recently, Multimodal Large Language
Models (MLLMs) have shown remarkable potential in this domain, leveraging both
Optical Character Recognition (OCR)-dependent and OCR-free frameworks to
extract and interpret information in document images. This survey reviews
recent advancements in MLLM-based VRDU, highlighting three core components: (1)
methods for encoding and fusing textual, visual, and layout features; (2)
training paradigms, including pretraining strategies, instruction-response
tuning, and the trainability of different model modules; and (3) datasets
utilized for pretraining, instruction-tuning, and supervised fine-tuning.
Finally, we discuss the challenges and opportunities in this evolving field and
propose future directions to advance the efficiency, generalizability, and
robustness of VRDU systems.

</details>


### [90] [SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation](https://arxiv.org/abs/2507.09862)
*Youliang Zhang,Zhaoyang Li,Duomin Wang,Jiahe Zhang,Deyu Zhou,Zixin Yin,Xili Dai,Gang Yu,Xiu Li*

Main category: cs.CV

TL;DR: 论文介绍了SpeakerVid-5M数据集，首个大规模、高质量的音频-视觉交互虚拟人生成数据集，包含520万视频片段，并提供基准模型和测试数据。


<details>
  <summary>Details</summary>
Motivation: 推动音频-视觉交互虚拟人研究，填补该领域缺乏高质量数据集的空白。

Method: 构建SpeakerVid-5M数据集，按交互类型和质量分类，并提供基于自回归的基准模型。

Result: 数据集包含520万视频片段，覆盖多种交互类型，并提供了基准模型VidChatBench。

Conclusion: SpeakerVid-5M为音频-视觉交互虚拟人研究提供了重要资源，未来工作可基于此展开。

Abstract: The rapid development of large-scale models has catalyzed significant
breakthroughs in the digital human domain. These advanced methodologies offer
high-fidelity solutions for avatar driving and rendering, leading academia to
focus on the next major challenge: audio-visual dyadic interactive virtual
human. To facilitate research in this emerging area, we present SpeakerVid-5M
dataset, the first large-scale, high-quality dataset designed for audio-visual
dyadic interactive virtual human generation. Totaling over 8,743 hours,
SpeakerVid-5M contains more than 5.2 million video clips of human portraits. It
covers diverse scales and interaction types, including monadic talking,
listening, and dyadic conversations. Crucially, the dataset is structured along
two key dimensions: interaction type and data quality. First, it is categorized
into four types (dialogue branch, single branch, listening branch and
multi-turn branch) based on the interaction scenario. Second, it is stratified
into a large-scale pre-training subset and a curated, high-quality subset for
Supervised Fine-Tuning (SFT). This dual structure accommodates a wide array of
2D virtual human tasks. In addition, we provide an autoregressive (AR)-based
video chat baseline trained on this data, accompanied by a dedicated set of
metrics and test data to serve as a benchmark VidChatBench for future work.
Both the dataset and the corresponding data processing code will be publicly
released. Project page: https://dorniwang.github.io/SpeakerVid-5M/

</details>


### [91] [ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models](https://arxiv.org/abs/2507.09876)
*Yongheng Zhang,Xu Liu,Ruihan Tao,Qiguang Chen,Hao Fei,Wanxiang Che,Libo Qin*

Main category: cs.CV

TL;DR: 论文提出了一种新的视频推理范式ViTCoT，结合视觉和文本信息，显著提升了视频理解性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖文本信息进行推理，忽视了视觉模态，而人类在推理时会自然重新审视视觉内容。

Method: 构建了Video-Text Interleaved Benchmark (ViTIB)，并探索了ViTCoT范式在视频理解领域的潜力。

Result: ViTCoT显著优于传统仅文本的CoT范式，并有效激活了MLLMs中更多神经元值。

Conclusion: ViTCoT为视频推理提供了一种更直观且认知对齐的方法，具有广泛应用前景。

Abstract: Video understanding plays a vital role in bridging low-level visual signals
with high-level cognitive reasoning, and is fundamental to applications such as
autonomous driving, embodied AI, and the broader pursuit of AGI. The rapid
development of large language models (LLMs), particularly those utilizing
Chain-of-Thought (CoT) technology, has significantly advanced video reasoning
capabilities. However, current approaches primarily depend on textual
information for reasoning, overlooking the visual modality in the actual video
reasoning process. In contrast, humans naturally re-examine visual content
while reasoning. Motivated by this, we introduce a novel video reasoning
paradigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive
and cognitively aligned reasoning. To the end, first, we construct the
Video-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for
key-video selection and manually verified. Furthermore, we extensively explore
the potential of the ViTCoT paradigm in the video understanding field.
Extensive experiments demonstrate that ViTCoT significantly enhances
performance compared to the traditional text-only CoT paradigm and effectively
activates more neuron values in MLLMs.

</details>


### [92] [OpenHuman4D: Open-Vocabulary 4D Human Parsing](https://arxiv.org/abs/2507.09880)
*Keito Suzuki,Bang Du,Runfa Blark Li,Kunyao Chen,Lei Wang,Peng Liu,Ning Bi,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出首个4D人体解析框架，通过减少推理时间和引入开放词汇能力，解决了现有方法依赖封闭数据集和推理时间长的问题。


<details>
  <summary>Details</summary>
Motivation: 动态3D人体表示在虚拟和扩展现实应用中日益重要，但现有方法受限于封闭数据集和长推理时间。

Method: 基于开放词汇3D人体解析技术，扩展支持4D视频，采用掩码视频对象跟踪、掩码验证模块和4D掩码融合模块。

Result: 实验证明方法有效且灵活，推理速度提升93.3%，优于现有方法。

Conclusion: 该框架显著提升了4D人体解析的效率和适用性。

Abstract: Understanding dynamic 3D human representation has become increasingly
critical in virtual and extended reality applications. However, existing human
part segmentation methods are constrained by reliance on closed-set datasets
and prolonged inference times, which significantly restrict their
applicability. In this paper, we introduce the first 4D human parsing framework
that simultaneously addresses these challenges by reducing the inference time
and introducing open-vocabulary capabilities. Building upon state-of-the-art
open-vocabulary 3D human parsing techniques, our approach extends the support
to 4D human-centric video with three key innovations: 1) We adopt mask-based
video object tracking to efficiently establish spatial and temporal
correspondences, avoiding the necessity of segmenting all frames. 2) A novel
Mask Validation module is designed to manage new target identification and
mitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating
memory-conditioned attention and logits equalization for robust embedding
fusion. Extensive experiments demonstrate the effectiveness and flexibility of
the proposed method on 4D human-centric parsing tasks, achieving up to 93.3%
acceleration compared to the previous state-of-the-art method, which was
limited to parsing fixed classes.

</details>


### [93] [Counterfactual Visual Explanation via Causally-Guided Adversarial Steering](https://arxiv.org/abs/2507.09881)
*Yiran Qiao,Disheng Liu,Yiren Lu,Yu Yin,Mengnan Du,Jing Ma*

Main category: cs.CV

TL;DR: CECAS框架通过因果引导的对抗方法生成反事实解释，避免虚假因素的干扰，提升解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有反事实视觉解释方法忽视因果关系和虚假相关性，导致解释质量受限。

Method: 提出CECAS框架，结合因果视角生成反事实解释，避免虚假因素的扰动。

Result: 在多个基准数据集上优于现有方法，实现有效性、稀疏性、接近性和真实性的平衡。

Conclusion: CECAS通过因果引导显著提升反事实解释的质量和实用性。

Abstract: Recent work on counterfactual visual explanations has contributed to making
artificial intelligence models more explainable by providing visual
perturbation to flip the prediction. However, these approaches neglect the
causal relationships and the spurious correlations behind the image generation
process, which often leads to unintended alterations in the counterfactual
images and renders the explanations with limited quality. To address this
challenge, we introduce a novel framework CECAS, which first leverages a
causally-guided adversarial method to generate counterfactual explanations. It
innovatively integrates a causal perspective to avoid unwanted perturbations on
spurious factors in the counterfactuals. Extensive experiments demonstrate that
our method outperforms existing state-of-the-art approaches across multiple
benchmark datasets and ultimately achieves a balanced trade-off among various
aspects of validity, sparsity, proximity, and realism.

</details>


### [94] [MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention](https://arxiv.org/abs/2507.09885)
*Zhanjiang Yang,Lijun Sun,Jiawei Dong,Xiaoxin An,Yang Liu,Meng Li*

Main category: cs.CV

TL;DR: MCGA提出了一种两阶段方法，通过先学习光谱模式再估计映射，解决了RGB到HSI重建的维度挑战，并结合注意力机制和测试时适应策略，实现了高效的高光谱图像重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接学习RGB到HSI的映射，忽略了从低维到高维转换的固有挑战，导致效果不佳。

Method: MCGA采用两阶段方法：1) 多尺度VQ-VAE学习光谱模式并提取混合码本；2) 通过查询码本特征优化RGB到HSI映射，并结合灰度感知注意力和量化自注意力。

Result: 实验表明MCGA在性能上达到最优，代码和模型已公开。

Conclusion: MCGA通过分阶段学习和物理驱动的注意力机制，实现了高效且鲁棒的高光谱图像重建。

Abstract: Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective
solution for various vision-based applications. However, most existing
learning-based hyperspectral reconstruction methods directly learn the
RGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent
challenge of transitioning from low-dimensional to high-dimensional
information. To address this limitation, we propose a two-stage approach, MCGA,
which first learns spectral patterns before estimating the mapping. In the
first stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI
datasets, extracting a Mixture of Codebooks (MoC). In the second stage, the
RGB-to-HSI mapping is refined by querying features from the MoC to replace
latent HSI representations, incorporating prior knowledge rather than forcing a
direct high-dimensional transformation. To further enhance reconstruction
quality, we introduce Grayscale-Aware Attention and Quantized Self-Attention,
which adaptively adjust feature map intensities to meet hyperspectral
reconstruction requirements. This physically motivated attention mechanism
ensures lightweight and efficient HSI recovery. Moreover, we propose an
entropy-based Test-Time Adaptation strategy to improve robustness in real-world
scenarios. Extensive experiments demonstrate that our method, MCGA, achieves
state-of-the-art performance. The code and models will be released at
https://github.com/Fibonaccirabbit/MCGA

</details>


### [95] [Measuring the Impact of Rotation Equivariance on Aerial Object Detection](https://arxiv.org/abs/2507.09896)
*Xiuyu Wu,Xinhao Wang,Xiubin Zhu,Lan Yang,Jiyuan Liu,Xingchen Hu*

Main category: cs.CV

TL;DR: 本文提出了一种严格旋转等变的单阶段检测器（MessDet），通过改进的网络结构和多分支头设计，在低参数量的情况下实现了在多个航空图像数据集上的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 航空图像中物体的任意方向性使得旋转等变性成为关键特性，但现有方法多为近似旋转等变，严格旋转等变的必要性尚不明确。

Method: 实现严格旋转等变的骨干和颈部网络，并提出多分支头设计以减少参数并提升精度。

Result: 在DOTA-v1.0、DOTA-v1.5和DIOR-R数据集上达到最优性能，且参数量极低。

Conclusion: 严格旋转等变对航空图像检测性能有显著提升，多分支头设计进一步优化了检测精度和参数效率。

Abstract: Due to the arbitrary orientation of objects in aerial images, rotation
equivariance is a critical property for aerial object detectors. However,
recent studies on rotation-equivariant aerial object detection remain scarce.
Most detectors rely on data augmentation to enable models to learn
approximately rotation-equivariant features. A few detectors have constructed
rotation-equivariant networks, but due to the breaking of strict rotation
equivariance by typical downsampling processes, these networks only achieve
approximately rotation-equivariant backbones. Whether strict rotation
equivariance is necessary for aerial image object detection remains an open
question. In this paper, we implement a strictly rotation-equivariant backbone
and neck network with a more advanced network structure and compare it with
approximately rotation-equivariant networks to quantitatively measure the
impact of rotation equivariance on the performance of aerial image detectors.
Additionally, leveraging the inherently grouped nature of rotation-equivariant
features, we propose a multi-branch head network that reduces the parameter
count while improving detection accuracy. Based on the aforementioned
improvements, this study proposes the Multi-branch head rotation-equivariant
single-stage Detector (MessDet), which achieves state-of-the-art performance on
the challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and DIOR-R with an
exceptionally low parameter count.

</details>


### [96] [IGD: Instructional Graphic Design with Multimodal Layer Generation](https://arxiv.org/abs/2507.09910)
*Yadong Qu,Shancheng Fang,Yuxin Wang,Xiaorui Wang,Zhineng Chen,Hongtao Xie,Yongdong Zhang*

Main category: cs.CV

TL;DR: IGD是一种基于自然语言指令的图形设计工具，通过参数化渲染和图像资产生成，快速生成可编辑的多模态层。


<details>
  <summary>Details</summary>
Motivation: 传统图形设计方法缺乏创造力和智能性，现有扩散模型生成的文件不可编辑且文本可读性差，无法满足自动化设计需求。

Method: IGD结合多模态理解和扩散模型，通过设计平台标准化格式，实现属性预测、层排序和布局，并生成图像资产。

Result: 实验证明IGD在复杂图形设计任务中具有可扩展性和优越性能。

Conclusion: IGD为图形设计提供了新的解决方案，支持可编辑性和自动化。

Abstract: Graphic design visually conveys information and data by creating and
combining text, images and graphics. Two-stage methods that rely primarily on
layout generation lack creativity and intelligence, making graphic design still
labor-intensive. Existing diffusion-based methods generate non-editable graphic
design files at image level with poor legibility in visual text rendering,
which prevents them from achieving satisfactory and practical automated graphic
design. In this paper, we propose Instructional Graphic Designer (IGD) to
swiftly generate multimodal layers with editable flexibility with only natural
language instructions. IGD adopts a new paradigm that leverages parametric
rendering and image asset generation. First, we develop a design platform and
establish a standardized format for multi-scenario design files, thus laying
the foundation for scaling up data. Second, IGD utilizes the multimodal
understanding and reasoning capabilities of MLLM to accomplish attribute
prediction, sequencing and layout of layers. It also employs a diffusion model
to generate image content for assets. By enabling end-to-end training, IGD
architecturally supports scalability and extensibility in complex graphic
design tasks. The superior experimental results demonstrate that IGD offers a
new solution for graphic design.

</details>


### [97] [Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios](https://arxiv.org/abs/2507.09915)
*Siyue Yao,Mingjie Sun,Eng Gee Lim,Ran Yi,Baojiang Zhong,Moncef Gabbouj*

Main category: cs.CV

TL;DR: Crucial-Diff是一种领域无关框架，通过生成关键样本解决数据稀缺问题，提升检测和分割性能。


<details>
  <summary>Details</summary>
Motivation: 数据稀缺导致模型过拟合和数据集不平衡，现有生成方法合成的样本重复且简单，无法针对下游模型弱点提供关键信息。

Method: 提出Crucial-Diff框架，包含场景无关特征提取器（SAFE）和弱点感知样本挖掘器（WASM），通过反馈生成多样化高质量样本。

Result: 在MVTec上达到83.63%的像素级AP和78.12%的F1-MAX；在息肉数据集上达到81.64%的mIoU和87.69%的mDice。

Conclusion: Crucial-Diff有效生成关键样本，显著提升下游任务性能。

Abstract: The scarcity of data in various scenarios, such as medical, industry and
autonomous driving, leads to model overfitting and dataset imbalance, thus
hindering effective detection and segmentation performance. Existing studies
employ the generative models to synthesize more training samples to mitigate
data scarcity. However, these synthetic samples are repetitive or simplistic
and fail to provide "crucial information" that targets the downstream model's
weaknesses. Additionally, these methods typically require separate training for
different objects, leading to computational inefficiencies. To address these
issues, we propose Crucial-Diff, a domain-agnostic framework designed to
synthesize crucial samples. Our method integrates two key modules. The Scene
Agnostic Feature Extractor (SAFE) utilizes a unified feature extractor to
capture target information. The Weakness Aware Sample Miner (WASM) generates
hard-to-detect samples using feedback from the detection results of downstream
model, which is then fused with the output of SAFE module. Together, our
Crucial-Diff framework generates diverse, high-quality training data, achieving
a pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset,
Crucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be
released after acceptance.

</details>


### [98] [Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis](https://arxiv.org/abs/2507.09950)
*Shubham Shukla,Kunal Sonalkar*

Main category: cs.CV

TL;DR: 论文评估了GPT-4o-mini和Gemini 2.0 Flash在细粒度时尚属性识别中的零样本性能，发现Gemini 2.0 Flash表现更优，并提出了领域微调的需求。


<details>
  <summary>Details</summary>
Motivation: 时尚零售业务依赖产品属性理解以提升客户体验和产品目录组织，但现有大语言模型在细粒度时尚属性识别上的表现尚未充分探索。

Method: 使用DeepFashion-MultiModal数据集，以图像为唯一输入，评估两种模型在18类时尚属性上的零样本表现。

Result: Gemini 2.0 Flash的宏F1得分为56.79%，优于GPT-4o-mini的43.28%。

Conclusion: 研究为电商产品属性任务提供了实用见解，并强调了领域微调的必要性，为时尚AI和多模态属性提取的未来研究奠定了基础。

Abstract: The fashion retail business is centered around the capacity to comprehend
products. Product attribution helps in comprehending products depending on the
business process. Quality attribution improves the customer experience as they
navigate through millions of products offered by a retail website. It leads to
well-organized product catalogs. In the end, product attribution directly
impacts the 'discovery experience' of the customer. Although large language
models (LLMs) have shown remarkable capabilities in understanding multimodal
data, their performance on fine-grained fashion attribute recognition remains
under-explored. This paper presents a zero-shot evaluation of state-of-the-art
LLMs that balance performance with speed and cost efficiency, mainly
GPT-4o-mini and Gemini 2.0 Flash. We have used the dataset
DeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to
evaluate these models in the attribution tasks of fashion products. Our study
evaluates these models across 18 categories of fashion attributes, offering
insight into where these models excel. We only use images as the sole input for
product information to create a constrained environment. Our analysis shows
that Gemini 2.0 Flash demonstrates the strongest overall performance with a
macro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a
macro F1 score of 43.28%. Through detailed error analysis, our findings provide
practical insights for deploying these LLMs in production e-commerce product
attribution-related tasks and highlight the need for domain-specific
fine-tuning approaches. This work also lays the groundwork for future research
in fashion AI and multimodal attribute extraction.

</details>


### [99] [4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion](https://arxiv.org/abs/2507.09953)
*Zifei Wang,Zian Mao,Xiaoya He,Xi Huang,Haoran Zhang,Chun Cheng,Shufen Chu,Tingzheng Hou,Xiaoqin Zeng,Yujun Xie*

Main category: cs.CV

TL;DR: 提出一种基于多图像超分辨率和卷积神经网络的4D-STEM方法，用于在极低电子剂量下实现原子级分辨率成像。


<details>
  <summary>Details</summary>
Motivation: 电子显微镜在原子分辨率成像中受到辐射损伤的限制，尤其是对光束敏感材料（如蛋白质和2D材料）。

Method: 结合多图像超分辨率（MISR）和卷积神经网络（CNN），通过融合多个低分辨率、亚像素位移的图像，实现原子级超分辨率成像。

Result: 在极低电子剂量条件下，实现了与传统ptychography相当的空间分辨率，适用于非晶、半晶和晶态光束敏感材料。

Conclusion: 该方法扩展了4D-STEM的能力，为辐射敏感材料的结构分析提供了一种通用且高效的新方法。

Abstract: While electron microscopy offers crucial atomic-resolution insights into
structure-property relationships, radiation damage severely limits its use on
beam-sensitive materials like proteins and 2D materials. To overcome this
challenge, we push beyond the electron dose limits of conventional electron
microscopy by adapting principles from multi-image super-resolution (MISR) that
have been widely used in remote sensing. Our method fuses multiple
low-resolution, sub-pixel-shifted views and enhances the reconstruction with a
convolutional neural network (CNN) that integrates features from synthetic,
multi-angle observations. We developed a dual-path, attention-guided network
for 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose
data. This provides robust atomic-scale visualization across amorphous,
semi-crystalline, and crystalline beam-sensitive specimens. Systematic
evaluations on representative materials demonstrate comparable spatial
resolution to conventional ptychography under ultra-low-dose conditions. Our
work expands the capabilities of 4D-STEM, offering a new and generalizable
method for the structural analysis of radiation-vulnerable materials.

</details>


### [100] [Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures](https://arxiv.org/abs/2507.09980)
*Zhipeng Xue,Yan Zhang,Ming Li,Chun Li,Yue Liu,Fei Yu*

Main category: cs.CV

TL;DR: KPHD-Net提出了一种基于Hölder散度的多视图分类和聚类方法，结合Dempster-Shafer证据理论和Kalman滤波器，提高了不确定性和融合结果的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖KL散度估计不确定性，忽略了模态间的领域差异，KPHD-Net旨在解决这一问题。

Method: 使用变分Dirichlet分布表示类别概率分布，结合Hölder散度和Dempster-Shafer证据理论，集成Kalman滤波器进行状态估计。

Result: 实验表明KPHD-Net在分类和聚类任务中优于现有方法，具有更高的准确性、鲁棒性和可靠性。

Conclusion: KPHD-Net通过改进的不确定性估计和多视图融合方法，显著提升了多视图学习的性能。

Abstract: Existing multi-view classification and clustering methods typically improve
task accuracy by leveraging and fusing information from different views.
However, ensuring the reliability of multi-view integration and final decisions
is crucial, particularly when dealing with noisy or corrupted data. Current
methods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty
of network predictions, ignoring domain gaps between different modalities. To
address this issue, KPHD-Net, based on H\"older divergence, is proposed for
multi-view classification and clustering tasks. Generally, our KPHD-Net employs
a variational Dirichlet distribution to represent class probability
distributions, models evidences from different views, and then integrates it
with Dempster-Shafer evidence theory (DST) to improve uncertainty estimation
effects. Our theoretical analysis demonstrates that Proper H\"older divergence
offers a more effective measure of distribution discrepancies, ensuring
enhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence
theory, recognized for its superior performance in multi-view fusion tasks, is
introduced and combined with the Kalman filter to provide future state
estimations. This integration further enhances the reliability of the final
fusion results. Extensive experiments show that the proposed KPHD-Net
outperforms the current state-of-the-art methods in both classification and
clustering tasks regarding accuracy, robustness, and reliability, with
theoretical guarantees.

</details>


### [101] [Latent Diffusion Models with Masked AutoEncoders](https://arxiv.org/abs/2507.09984)
*Junho Lee,Jeongwoo Shin,Hyungwook Choi,Joonseok Lee*

Main category: cs.CV

TL;DR: 本文分析了潜在扩散模型（LDMs）中自编码器的关键属性，提出了一种新的变分掩码自编码器（VMAEs），并将其整合为LDMAEs，显著提升了图像生成质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管潜在扩散模型在图像生成方面具有巨大潜力，但自编码器的理想属性和最优设计尚未充分探索。

Method: 作者分析了自编码器在LDMs中的角色，提出VMAEs，利用掩码自编码器的分层特征，并将其整合为LDMAEs。

Result: 实验表明，LDMAEs在图像生成质量和计算效率上均有显著提升。

Conclusion: VMAEs通过同时满足潜在平滑性、感知压缩质量和重建质量，为LDMs提供了更优的自编码器设计。

Abstract: In spite of remarkable potential of the Latent Diffusion Models (LDMs) in
image generation, the desired properties and optimal design of the autoencoders
have been underexplored. In this work, we analyze the role of autoencoders in
LDMs and identify three key properties: latent smoothness, perceptual
compression quality, and reconstruction quality. We demonstrate that existing
autoencoders fail to simultaneously satisfy all three properties, and propose
Variational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical
features maintained by Masked AutoEncoder. We integrate VMAEs into the LDM
framework, introducing Latent Diffusion Models with Masked AutoEncoders
(LDMAEs). Through comprehensive experiments, we demonstrate significantly
enhanced image generation quality and computational efficiency.

</details>


### [102] [3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving](https://arxiv.org/abs/2507.09993)
*Yixun Zhang,Lizhi Wang,Junjun Zhao,Wending Zhao,Feng Zhou,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯分布的对抗攻击方法（3DGAA），通过联合优化几何和外观属性，生成物理上可实现的对抗物体，显著降低目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有2D和3D物理攻击方法在平衡物理真实性和攻击鲁棒性方面存在不足，需要一种更有效的方法来评估自动驾驶感知系统的安全性。

Method: 利用3D高斯泼溅（3DGS）的14维参数化，联合优化几何（形状、尺度、旋转）和外观（颜色、透明度）属性，并引入物理过滤和增强模块以提高攻击泛化能力。

Result: 在虚拟和物理实验中，3DGAA将检测mAP从87.21%降至7.38%，显著优于现有方法，并展示了高迁移性。

Conclusion: 3DGAA是一种实用的对抗攻击框架，可用于评估自动驾驶感知系统的安全性，为物理可实现对抗攻击设定了新标准。

Abstract: Camera-based object detection systems play a vital role in autonomous
driving, yet they remain vulnerable to adversarial threats in real-world
environments. While existing 2D and 3D physical attacks typically optimize
texture, they often struggle to balance physical realism and attack robustness.
In this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel
adversarial object generation framework that leverages the full 14-dimensional
parameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry
and appearance in physically realizable ways. Unlike prior works that rely on
patches or texture, 3DGAA jointly perturbs both geometric attributes (shape,
scale, rotation) and appearance attributes (color, opacity) to produce
physically realistic and transferable adversarial objects. We further introduce
a physical filtering module to preserve geometric fidelity, and a physical
augmentation module to simulate complex physical scenarios, thus enhancing
attack generalization under real-world conditions. We evaluate 3DGAA on both
virtual benchmarks and physical-world setups using miniature vehicle models.
Experimental results show that 3DGAA achieves to reduce the detection mAP from
87.21% to 7.38%, significantly outperforming existing 3D physical attacks.
Moreover, our method maintains high transferability across different physical
conditions, demonstrating a new state-of-the-art in physically realizable
adversarial attacks. These results validate 3DGAA as a practical attack
framework for evaluating the safety of perception systems in autonomous
driving.

</details>


### [103] [Leveraging Swin Transformer for enhanced diagnosis of Alzheimer's disease using multi-shell diffusion MRI](https://arxiv.org/abs/2507.09996)
*Quentin Dessain,Nicolas Delinte,Bernard Hanseeuw,Laurence Dricot,Benoît Macq*

Main category: cs.CV

TL;DR: 该研究利用多壳层扩散MRI数据和视觉变换器深度学习框架，支持阿尔茨海默病的早期诊断和淀粉样蛋白积累检测。


<details>
  <summary>Details</summary>
Motivation: 通过利用多壳层扩散MRI数据的微观结构信息，结合深度学习技术，实现阿尔茨海默病的早期诊断和淀粉样蛋白检测。

Method: 采用Swin Transformer模型对多壳层dMRI数据进行分类，提取DTI和NODDI关键指标并投影到2D平面，结合低秩适应技术优化模型。

Result: 模型在阿尔茨海默病诊断中达到95.2%的平衡准确率，淀粉样蛋白检测中最高达到77.2%的平衡准确率。

Conclusion: 研究表明扩散MRI和变换器架构在阿尔茨海默病早期检测中具有潜力，支持数据有限的生物医学环境中的生物标志物驱动诊断。

Abstract: Objective: This study aims to support early diagnosis of Alzheimer's disease
and detection of amyloid accumulation by leveraging the microstructural
information available in multi-shell diffusion MRI (dMRI) data, using a vision
transformer-based deep learning framework.
  Methods: We present a classification pipeline that employs the Swin
Transformer, a hierarchical vision transformer model, on multi-shell dMRI data
for the classification of Alzheimer's disease and amyloid presence. Key metrics
from DTI and NODDI were extracted and projected onto 2D planes to enable
transfer learning with ImageNet-pretrained models. To efficiently adapt the
transformer to limited labeled neuroimaging data, we integrated Low-Rank
Adaptation. We assessed the framework on diagnostic group prediction
(cognitively normal, mild cognitive impairment, Alzheimer's disease dementia)
and amyloid status classification.
  Results: The framework achieved competitive classification results within the
scope of multi-shell dMRI-based features, with the best balanced accuracy of
95.2% for distinguishing cognitively normal individuals from those with
Alzheimer's disease dementia using NODDI metrics. For amyloid detection, it
reached 77.2% balanced accuracy in distinguishing amyloid-positive mild
cognitive impairment/Alzheimer's disease dementia subjects from
amyloid-negative cognitively normal subjects, and 67.9% for identifying
amyloid-positive individuals among cognitively normal subjects. Grad-CAM-based
explainability analysis identified clinically relevant brain regions, including
the parahippocampal gyrus and hippocampus, as key contributors to model
predictions.
  Conclusion: This study demonstrates the promise of diffusion MRI and
transformer-based architectures for early detection of Alzheimer's disease and
amyloid pathology, supporting biomarker-driven diagnostics in data-limited
biomedical settings.

</details>


### [104] [Vision-Based Anti Unmanned Aerial Technology: Opportunities and Challenges](https://arxiv.org/abs/2507.10006)
*Guanghai Ding,Yihua Ren,Yuting Liu,Qijun Zhao,Shuiwang Li*

Main category: cs.CV

TL;DR: 论文综述了反无人机跟踪技术的现状、挑战及未来方向，整理了公开数据集并分析了近年来的视觉和视觉融合算法。


<details>
  <summary>Details</summary>
Motivation: 随着无人机技术的快速发展和广泛应用，反无人机跟踪在公共安全等复杂场景中的重要性日益突出。

Method: 回顾反无人机检测与跟踪技术的特点和挑战，整理公开数据集，分析近年来的视觉和视觉融合算法。

Result: 提供了数据集链接和算法分析，为研究者提供了解决相关挑战的支持。

Conclusion: 论文总结了未来研究方向，旨在推动反无人机跟踪领域的进步。

Abstract: With the rapid advancement of UAV technology and its extensive application in
various fields such as military reconnaissance, environmental monitoring, and
logistics, achieving efficient and accurate Anti-UAV tracking has become
essential. The importance of Anti-UAV tracking is increasingly prominent,
especially in scenarios such as public safety, border patrol, search and
rescue, and agricultural monitoring, where operations in complex environments
can provide enhanced security. Current mainstream Anti-UAV tracking
technologies are primarily centered around computer vision techniques,
particularly those that integrate multi-sensor data fusion with advanced
detection and tracking algorithms. This paper first reviews the characteristics
and current challenges of Anti-UAV detection and tracking technologies. Next,
it investigates and compiles several publicly available datasets, providing
accessible links to support researchers in efficiently addressing related
challenges. Furthermore, the paper analyzes the major vision-based and
vision-fusion-based Anti-UAV detection and tracking algorithms proposed in
recent years. Finally, based on the above research, this paper outlines future
research directions, aiming to provide valuable insights for advancing the
field.

</details>


### [105] [Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry](https://arxiv.org/abs/2507.10009)
*Geyou Zhang,Kai Liu,Ce Zhu*

Main category: cs.CV

TL;DR: 提出了一种名为I-BSC的方法，通过加权求和同质条纹图像而非相位帧，显著降低了运动误差和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决相位移动轮廓测量（PSP）在动态测量中因物体运动导致的误差问题。

Method: 提出I-BSC方法，加权求和同质条纹图像，仅计算一次反正切函数。

Result: I-BSC在减少运动误差和计算复杂度上优于现有方法，实现了准单帧速率。

Conclusion: I-BSC显著提升了动态3D扫描的精度和效率。

Abstract: Phase shifting profilometry (PSP) is widely used in high-precision 3D
scanning due to its high accuracy, robustness, and pixel-wise handling.
However, a fundamental assumption of PSP that the object should remain static
does not hold in dynamic measurement, making PSP susceptible to object motion.
To address this challenge, our proposed solution, phase-sequential binomial
self-compensation (P-BSC), sums successive motion-affected phase frames
weighted by binomial coefficients. This approach exponentially reduces the
motion error in a pixel-wise and frame-wise loopable manner. Despite its
efficacy, P-BSC suffers from high computational overhead and error accumulation
due to its reliance on multi-frame phase calculations and weighted summations.
Inspired by P-BSC, we propose an image-sequential binomial self-compensation
(I-BSC) to weight sum the homogeneous fringe images instead of successive phase
frames, which generalizes the BSC concept from phase sequences to image
sequences. I-BSC computes the arctangent function only once, resolving both
limitations in P-BSC. Extensive analysis, simulations, and experiments show
that 1) the proposed BSC outperforms existing methods in reducing motion error
while achieving a quasi-single-shot frame rate, i.e., depth map frame rate
equals to the camera's acquisition rate, enabling 3D reconstruction with high
pixel-depth-temporal resolution; 2) compared to P-BSC, our I-BSC reduces the
computational complexity by one polynomial order, thereby accelerating the
computational frame rate by several to dozen times, while also reaching faster
motion error convergence.

</details>


### [106] [Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect](https://arxiv.org/abs/2507.10013)
*Tom Kouwenhoven,Kiana Shahrasbi,Tessa Verhoef*

Main category: cs.CV

TL;DR: 论文重新评估了两种CLIP变体（ResNet和ViT）在bouba-kiki效应中的表现，发现模型未能像人类一样一致地关联形状与伪词。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉-语言模型（VLMs）是否以反映人类认知的方式整合跨模态信息。

Method: 使用基于提示的概率评估和Grad-CAM分析视觉注意力，比较模型与人类在bouba-kiki任务中的表现。

Result: 模型未表现出一致的bouba-kiki效应，且与人类数据相比表现显著不足。

Conclusion: VLMs在跨模态概念理解和与人类认知对齐方面存在局限性。

Abstract: Recent advances in multimodal models have raised questions about whether
vision-and-language models (VLMs) integrate cross-modal information in ways
that reflect human cognition. One well-studied test case in this domain is the
bouba-kiki effect, where humans reliably associate pseudowords like "bouba"
with round shapes and "kiki" with jagged ones. Given the mixed evidence found
in prior studies for this effect in VLMs, we present a comprehensive
re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer
(ViT), given their centrality in many state-of-the-art VLMs. We apply two
complementary methods closely modelled after human experiments: a prompt-based
evaluation that uses probabilities as model preference, and we use Grad-CAM as
a novel way to interpret visual attention in shape-word matching tasks. Our
findings show that these models do not consistently exhibit the bouba-kiki
effect. While ResNet shows a preference for round shapes, overall performance
across both models lacks the expected associations. Moreover, direct comparison
with prior human data on the same task shows that the models' responses fall
markedly short of the robust, modality-integrated behaviour characteristic of
human cognition. These results contribute to the ongoing debate about the
extent to which VLMs truly understand cross-modal concepts, highlighting
limitations in their internal representations and alignment with human
intuitions.

</details>


### [107] [(Almost) Free Modality Stitching of Foundation Models](https://arxiv.org/abs/2507.10015)
*Jaisidh Singh,Diganta Misra,Boris Knyazev,Antonio Orvieto*

Main category: cs.CV

TL;DR: 论文提出了一种名为Hyma的新方法，通过超网络技术优化多模态模型中单模态模型的选择和连接器训练，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型通过连接多个预训练单模态模型实现，但选择和训练连接器模块的计算成本高，亟需高效解决方案。

Method: 利用超网络的参数预测能力，为多种单模态模型组合联合训练连接器模块，减少搜索成本。

Result: 实验表明，Hyma将最优单模态模型对的搜索成本降低10倍，同时性能与网格搜索相当。

Conclusion: Hyma为多模态模型的高效构建提供了一种创新且实用的解决方案。

Abstract: Foundation multi-modal models are often designed by stitching of multiple
existing pretrained uni-modal models: for example, an image classifier with an
autoregressive text model. This stitching process is performed by training a
connector module that aims to align the representation-representation or
representation-input spaces of these uni-modal models. However, given the
complexity of training such connectors on large scale web-based datasets
coupled with the ever-increasing number of available pretrained uni-modal
models, the task of uni-modal models selection and subsequent connector module
training becomes computationally demanding. To address this under-studied
critical problem, we propose Hypernetwork Model Alignment (Hyma), a novel
all-in-one solution for optimal uni-modal model selection and connector
training by leveraging hypernetworks. Specifically, our framework utilizes the
parameter prediction capability of a hypernetwork to obtain jointly trained
connector modules for $N \times M$ combinations of uni-modal models. In our
experiments, Hyma reduces the optimal uni-modal model pair search cost by
$10\times$ (averaged across all experiments), while matching the ranking and
trained connector performance obtained via grid search across a suite of
diverse multi-modal benchmarks.

</details>


### [108] [Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies](https://arxiv.org/abs/2507.10029)
*Seokeon Choi,Sunghyun Park,Hyoungwoo Park,Jeongho Kim,Sungrack Yun*

Main category: cs.CV

TL;DR: 提出了一种选择性优化框架，结合低分辨率反向传播（BP-low）和高分辨率零阶优化（ZO-high），以实现内存高效且高质量的文本到图像扩散模型个性化。


<details>
  <summary>Details</summary>
Motivation: 解决在边缘设备上适应文本到图像扩散模型时内存效率低和隐私保护的问题。

Method: 通过时间步感知的概率函数动态选择BP-low或ZO-high优化策略，结合两者的优势。

Result: 实验表明，该方法在显著降低内存消耗的同时保持了竞争性能。

Conclusion: 该框架实现了高效、高质量的设备端个性化，且不影响推理延迟。

Abstract: Memory-efficient personalization is critical for adapting text-to-image
diffusion models while preserving user privacy and operating within the limited
computational resources of edge devices. To this end, we propose a selective
optimization framework that adaptively chooses between backpropagation on
low-resolution images (BP-low) and zeroth-order optimization on high-resolution
images (ZO-high), guided by the characteristics of the diffusion process. As
observed in our experiments, BP-low efficiently adapts the model to
target-specific features, but suffers from structural distortions due to
resolution mismatch. Conversely, ZO-high refines high-resolution details with
minimal memory overhead but faces slow convergence when applied without prior
adaptation. By complementing both methods, our framework leverages BP-low for
effective personalization while using ZO-high to maintain structural
consistency, achieving memory-efficient and high-quality fine-tuning. To
maximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware
probabilistic function that dynamically selects the appropriate optimization
strategy based on diffusion timesteps. This function mitigates the overfitting
from BP-low at high timesteps, where structural information is critical, while
ensuring ZO-high is applied more effectively as training progresses.
Experimental results demonstrate that our method achieves competitive
performance while significantly reducing memory consumption, enabling scalable,
high-quality on-device personalization without increasing inference latency.

</details>


### [109] [LifelongPR: Lifelong knowledge fusion for point cloud place recognition based on replay and prompt learning](https://arxiv.org/abs/2507.10034)
*Xianghong Zou,Jianping Li,Zhe Chen,Zhen Cao,Zhen Dong,Qiegen Liu,Bisheng Yang*

Main category: cs.CV

TL;DR: 论文提出了一种名为LifelongPR的持续学习框架，用于解决点云地点识别中的灾难性遗忘问题，通过动态样本选择和提示学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 点云地点识别在自动驾驶等领域至关重要，但现有模型在适应新环境时易遗忘旧知识，导致性能下降。

Method: 提出动态样本选择方法和基于提示学习的CL框架，结合两阶段训练策略。

Result: 在公开和自采数据集上验证，性能显著优于现有方法，mIR@1提升6.50%，mR@1提升7.96%，F值降低8.95%。

Conclusion: LifelongPR有效解决了灾难性遗忘问题，提升了点云地点识别的实用性和可扩展性。

Abstract: Point cloud place recognition (PCPR) plays a crucial role in photogrammetry
and robotics applications such as autonomous driving, intelligent
transportation, and augmented reality. In real-world large-scale deployments of
a positioning system, PCPR models must continuously acquire, update, and
accumulate knowledge to adapt to diverse and dynamic environments, i.e., the
ability known as continual learning (CL). However, existing PCPR models often
suffer from catastrophic forgetting, leading to significant performance
degradation in previously learned scenes when adapting to new environments or
sensor types. This results in poor model scalability, increased maintenance
costs, and system deployment difficulties, undermining the practicality of
PCPR. To address these issues, we propose LifelongPR, a novel continual
learning framework for PCPR, which effectively extracts and fuses knowledge
from sequential point cloud data. First, to alleviate the knowledge loss, we
propose a replay sample selection method that dynamically allocates sample
sizes according to each dataset's information quantity and selects spatially
diverse samples for maximal representativeness. Second, to handle domain
shifts, we design a prompt learning-based CL framework with a lightweight
prompt module and a two-stage training strategy, enabling domain-specific
feature adaptation while minimizing forgetting. Comprehensive experiments on
large-scale public and self-collected datasets are conducted to validate the
effectiveness of the proposed method. Compared with state-of-the-art (SOTA)
methods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in
mR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly
available at https://github.com/zouxianghong/LifelongPR.

</details>


### [110] [CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books](https://arxiv.org/abs/2507.10053)
*Marc Serra Ortega,Emanuele Vivoli,Artemis Llabrés,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: CoSMo是一种新型多模态Transformer，用于漫画书页面流分割（PSS），显著优于传统方法和通用视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 漫画书页面流分割是自动化内容理解的关键任务，为角色分析、故事索引等下游任务提供基础。

Method: 开发了视觉和多模态版本的CoSMo，并在20,800页标注数据集上验证。

Result: CoSMo在F1-Macro、Panoptic Quality等指标上表现优异，视觉特征主导宏观结构，多模态有助于解决模糊性。

Conclusion: CoSMo为漫画书分析设立了新标准，支持可扩展的自动化分析。

Abstract: This paper introduces CoSMo, a novel multimodal Transformer for Page Stream
Segmentation (PSS) in comic books, a critical task for automated content
understanding, as it is a necessary first stage for many downstream tasks like
character analysis, story indexing, or metadata enrichment. We formalize PSS
for this unique medium and curate a new 20,800-page annotated dataset. CoSMo,
developed in vision-only and multimodal variants, consistently outperforms
traditional baselines and significantly larger general-purpose vision-language
models across F1-Macro, Panoptic Quality, and stream-level metrics. Our
findings highlight the dominance of visual features for comic PSS
macro-structure, yet demonstrate multimodal benefits in resolving challenging
ambiguities. CoSMo establishes a new state-of-the-art, paving the way for
scalable comic book analysis.

</details>


### [111] [Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning](https://arxiv.org/abs/2507.10056)
*A. K. M. Shoriful Islam,Md. Rakib Hassan,Macbah Uddin,Md. Shahidur Rahman*

Main category: cs.CV

TL;DR: 提出一种轻量级机器学习方法，通过分析家禽粪便图像检测疾病，结合多颜色空间特征提取和多种描述符，实现高效且低资源消耗的分类。


<details>
  <summary>Details</summary>
Motivation: 家禽养殖易受传染病影响，传统方法资源消耗高，需一种低成本、高效且可扩展的解决方案。

Method: 使用多颜色空间（RGB、HSV、LAB）特征提取，结合颜色、纹理和形状描述符，通过PCA和XGBoost降维，训练ANN分类器。

Result: 模型准确率达95.85%，无需GPU，执行时间仅638秒，资源消耗远低于深度学习模型。

Conclusion: 该方法为低资源农业环境提供了一种高效、可解释且可扩展的疾病检测替代方案。

Abstract: Poultry farming is a vital component of the global food supply chain, yet it
remains highly vulnerable to infectious diseases such as coccidiosis,
salmonellosis, and Newcastle disease. This study proposes a lightweight machine
learning-based approach to detect these diseases by analyzing poultry fecal
images. We utilize multi-color space feature extraction (RGB, HSV, LAB) and
explore a wide range of color, texture, and shape-based descriptors, including
color histograms, local binary patterns (LBP), wavelet transforms, and edge
detectors. Through a systematic ablation study and dimensionality reduction
using PCA and XGBoost feature selection, we identify a compact global feature
set that balances accuracy and computational efficiency. An artificial neural
network (ANN) classifier trained on these features achieved 95.85% accuracy
while requiring no GPU and only 638 seconds of execution time in Google Colab.
Compared to deep learning models such as Xception and MobileNetV3, our proposed
model offers comparable accuracy with drastically lower resource usage. This
work demonstrates a cost-effective, interpretable, and scalable alternative to
deep learning for real-time poultry disease detection in low-resource
agricultural settings.

</details>


### [112] [MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second](https://arxiv.org/abs/2507.10065)
*Chenguo Lin,Yuchen Lin,Panwang Pan,Yifan Yu,Honglei Yan,Katerina Fragkiadaki,Yadong Mu*

Main category: cs.CV

TL;DR: MoVieS是一种新型前馈模型，能够在一秒内从单目视频合成4D动态新视角。它通过高斯基元的像素对齐网格表示动态3D场景，并监督其时间变化运动，首次实现了外观、几何和运动的统一建模。


<details>
  <summary>Details</summary>
Motivation: 当前方法在动态场景的新视角合成和几何重建方面存在局限性，MoVieS旨在通过统一建模解决这些问题，同时减少对任务特定监督的依赖。

Method: MoVieS使用像素对齐的高斯基元网格表示动态3D场景，并显式监督其时间变化运动，实现了外观、几何和运动的统一建模。

Result: 实验表明，MoVieS在多个任务中表现出色，性能竞争力强且速度显著提升。

Conclusion: MoVieS通过统一建模动态场景的外观、几何和运动，实现了高效的新视角合成和几何重建，支持零样本应用，具有广泛潜力。

Abstract: We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic
novel views from monocular videos in one second. MoVieS represents dynamic 3D
scenes using pixel-aligned grids of Gaussian primitives, explicitly supervising
their time-varying motion. This allows, for the first time, the unified
modeling of appearance, geometry and motion, and enables view synthesis,
reconstruction and 3D point tracking within a single learning-based framework.
By bridging novel view synthesis with dynamic geometry reconstruction, MoVieS
enables large-scale training on diverse datasets with minimal dependence on
task-specific supervision. As a result, it also naturally supports a wide range
of zero-shot applications, such as scene flow estimation and moving object
segmentation. Extensive experiments validate the effectiveness and efficiency
of MoVieS across multiple tasks, achieving competitive performance while
offering several orders of magnitude speedups.

</details>


### [113] [Frequency Regulation for Exposure Bias Mitigation in Diffusion Models](https://arxiv.org/abs/2507.10072)
*Meng Yu,Kun Zhan*

Main category: cs.CV

TL;DR: 本文通过分析扩散模型中预测噪声图像能量的变化，提出了一种基于小波变换的频率域调节机制，显著改善了生成质量并解决了曝光偏差问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成能力上表现出色，但受曝光偏差影响较大。本文旨在通过分析能量变化规律，提出改进方法。

Method: 利用小波变换分别调节低频和高频子带，并基于能量变化分析曝光偏差。

Result: 提出的训练无关、即插即用方法显著提升了多种扩散模型的生成质量。

Conclusion: 频率域调节机制为扩散模型中的曝光偏差问题提供了有效解决方案。

Abstract: Diffusion models exhibit impressive generative capabilities but are
significantly impacted by exposure bias. In this paper, we make a key
observation: the energy of the predicted noisy images decreases during the
diffusion process. Building on this, we identify two important findings: 1) The
reduction in energy follows distinct patterns in the low-frequency and
high-frequency subbands; 2) This energy reduction results in amplitude
variations between the network-reconstructed clean data and the real clean
data. Based on the first finding, we introduce a frequency-domain regulation
mechanism utilizing wavelet transforms, which separately adjusts the low- and
high-frequency subbands. Leveraging the second insight, we provide a more
accurate analysis of exposure bias in the two subbands. Our method is
training-free and plug-and-play, significantly improving the generative quality
of various diffusion models and providing a robust solution to exposure bias
across different model architectures. The source code is available at
https://github.com/kunzhan/wpp.

</details>


### [114] [A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area](https://arxiv.org/abs/2507.10084)
*Haonan Chen,Xin Tong*

Main category: cs.CV

TL;DR: 提出了一种基于SegFormer模型的两阶段迁移学习策略，显著提升了遥感图像水体分割任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决遥感图像水体分割中领域偏移和小样本量的挑战。

Method: 采用两阶段迁移学习策略，先在源域训练基础模型，再在目标域微调。

Result: IoU从25.50%提升至64.84%，显著优于直接迁移。

Conclusion: 该策略有效解决了领域差异导致的性能下降，为数据稀缺且环境独特的遥感场景提供了高精度信息提取的技术范例。

Abstract: To address the prevalent challenges of domain shift and small sample sizes in
remote sensing image water body segmentation, this study proposes and validates
a two-stage transfer learning strategy based on the SegFormer model. The
approach begins by training a foundational segmentation model on a diverse
source domain, where it achieves an Intersection over Union (IoU) of 68.80% on
its validation set, followed by fine-tuning on data from the distinct target
domain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by
highly complex topography and spectral features -- the experimental results
demonstrate that this strategy significantly boosts the IoU for the water body
segmentation task from 25.50% (for direct transfer) to 64.84%. This not only
effectively resolves the model performance degradation caused by domain
discrepancy but also provides an effective technical paradigm for
high-precision thematic information extraction in data-scarce and
environmentally unique remote sensing scenarios.

</details>


### [115] [FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text](https://arxiv.org/abs/2507.10095)
*Bingchao Wang,Zhiwei Ning,Jianyu Ding,Xuanang Gao,Yin Li,Dongsheng Jiang,Jie Yang,Wei Liu*

Main category: cs.CV

TL;DR: FIX-CLIP改进CLIP的长文本处理能力，通过双分支训练、区域提示和分层特征对齐模块，在长短文本检索任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: CLIP在长文本输入（>77词）任务中表现受限，需改进以适应更广泛的应用。

Method: 提出FIX-CLIP，包含双分支训练、多区域提示和分层特征对齐模块，并利用合成数据训练。

Result: FIX-CLIP在长短文本检索任务中达到最优性能，并适用于扩散模型的长文本输入。

Conclusion: FIX-CLIP有效解决了CLIP的长文本处理问题，具有广泛的应用潜力。

Abstract: CLIP has shown promising performance across many short-text tasks in a
zero-shot manner. However, limited by the input length of the text encoder,
CLIP struggles on under-stream tasks with long-text inputs (>77 tokens). To
remedy this issue, we propose FIX-CLIP which includes three novel modules: (1)
A dual-branch training pipeline that aligns short and long texts with masked
and raw images respectively, which boosts the long-text representation while
preserving the short-text ability. (2) Multiple learnable regional prompts with
unidirectional masks in Transformer layers for regional information extraction.
(3) A hierarchical feature alignment module in the intermediate encoder layers
to promote the consistency of multi-scale features. Furthermore, we collect 30M
images and utilize existing MLLMs to synthesize long-text captions for
training. Extensive experiments show that FIX-CLIP achieves state-of-the-art
performance on both long-text and short-text retrieval benchmarks. For
downstream applications, we reveal that FIX-CLIP's text encoder delivers
promising performance in a plug-and-play manner for diffusion models with
long-text input.

</details>


### [116] [Glance-MCMT: A General MCMT Framework with Glance Initialization and Progressive Association](https://arxiv.org/abs/2507.10115)
*Hamidreza Hashempoor*

Main category: cs.CV

TL;DR: 提出了一种多摄像头多目标跟踪框架，通过轨迹和外观特征确保跨视角的全局身份一致性。


<details>
  <summary>Details</summary>
Motivation: 解决多摄像头环境下目标跟踪中全局身份分配的一致性问题。

Method: 采用BoT-SORT单摄像头跟踪，通过轨迹特征匹配初始化全局ID，后续帧使用优先全局匹配策略，结合深度图和校准进行空间验证。

Result: 实现了跨视角的全局身份一致性跟踪。

Conclusion: 该框架在多摄像头多目标跟踪中表现高效且一致。

Abstract: We propose a multi-camera multi-target (MCMT) tracking framework that ensures
consistent global identity assignment across views using trajectory and
appearance cues. The pipeline starts with BoT-SORT-based single-camera
tracking, followed by an initial glance phase to initialize global IDs via
trajectory-feature matching. In later frames, new tracklets are matched to
existing global identities through a prioritized global matching strategy. New
global IDs are only introduced when no sufficiently similar trajectory or
feature match is found. 3D positions are estimated using depth maps and
calibration for spatial validation.

</details>


### [117] [DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation](https://arxiv.org/abs/2507.10118)
*Ivan Martinović,Josip Šarić,Marin Oršić,Matej Kristan,Siniša Šegvić*

Main category: cs.CV

TL;DR: 提出了一种名为DEARLi的半监督全景分割方法，通过结合两个专用基础模型，显著提升了识别和定位能力，在标签稀缺的情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决像素级标注成本高的问题，利用少量标注图像和大量未标注图像进行半监督学习。

Method: 结合无监督掩码-变换器一致性和CLIP特征的零样本分类增强识别，通过类无关解码器预热和SAM伪标签增强定位。

Result: 在ADE20K数据集上仅用158张标注图像，取得了29.9 PQ和38.9 mIoU的优异表现，性能超越现有方法且GPU内存需求更低。

Conclusion: DEARLi在标签稀缺和大分类任务中表现突出，为半监督分割提供了高效解决方案。

Abstract: Pixel-level annotation is expensive and time-consuming. Semi-supervised
segmentation methods address this challenge by learning models on few labeled
images alongside a large corpus of unlabeled images. Although foundation models
could further account for label scarcity, effective mechanisms for their
exploitation remain underexplored. We address this by devising a novel
semi-supervised panoptic approach fueled by two dedicated foundation models. We
enhance recognition by complementing unsupervised mask-transformer consistency
with zero-shot classification of CLIP features. We enhance localization by
class-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting
decoupled enhancement of recognition and localization (DEARLi) particularly
excels in the most challenging semi-supervised scenarios with large taxonomies
and limited labeled data. Moreover, DEARLi outperforms the state of the art in
semi-supervised semantic segmentation by a large margin while requiring 8x less
GPU memory, in spite of being trained only for the panoptic objective. We
observe 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The
source code is available at https://github.com/helen1c/DEARLi.

</details>


### [118] [Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion](https://arxiv.org/abs/2507.10127)
*Md Abulkalam Azad,John Nyberg,Håvard Dalen,Bjørnar Grenne,Lasse Lovstakken,Andreas Østvik*

Main category: cs.CV

TL;DR: 该论文探讨了在心超图像中应用先进点跟踪方法的潜力，通过改进训练策略和提出轻量级网络，显著提升了运动估计的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如块匹配或光流）在复杂心脏运动估计中表现不佳，而现代点跟踪方法在心超领域尚未充分探索。

Method: 通过分析心超视频中的心脏运动，发现方向性运动偏差影响了现有训练策略，因此改进了训练过程并引入定制化增强方法，同时提出了一种轻量级网络。

Result: 实验表明，改进后的方法显著提升了性能（如EchoTracker位置精度提升60.7%），且在某些情况下优于复杂模型。

Conclusion: 这些方法在临床评估中表现出更好的可重复性，与专家验证的半自动化工具更接近。

Abstract: Accurate motion estimation for tracking deformable tissues in
echocardiography is essential for precise cardiac function measurements. While
traditional methods like block matching or optical flow struggle with intricate
cardiac motion, modern point tracking approaches remain largely underexplored
in this domain. This work investigates the potential of state-of-the-art (SOTA)
point tracking methods for ultrasound, with a focus on echocardiography.
Although these novel approaches demonstrate strong performance in general
videos, their effectiveness and generalizability in echocardiography remain
limited. By analyzing cardiac motion throughout the heart cycle in real B-mode
ultrasound videos, we identify that a directional motion bias across different
views is affecting the existing training strategies. To mitigate this, we
refine the training procedure and incorporate a set of tailored augmentations
to reduce the bias and enhance tracking robustness and generalization through
impartial cardiac motion. We also propose a lightweight network leveraging
multi-scale cost volumes from spatial context alone to challenge the advanced
spatiotemporal point tracking models. Experiments demonstrate that fine-tuning
with our strategies significantly improves models' performances over their
baselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker
boosts overall position accuracy by 60.7% and reduces median trajectory error
by 61.5% across heart cycle phases. Interestingly, several point tracking
models fail to outperform our proposed simple model in terms of tracking
accuracy and generalization, reflecting their limitations when applied to
echocardiography. Nevertheless, clinical evaluation reveals that these methods
improve GLS measurements, aligning more closely with expert-validated,
semi-automated tools and thus demonstrating better reproducibility in
real-world applications.

</details>


### [119] [Deep Recurrence for Dynamical Segmentation Models](https://arxiv.org/abs/2507.10143)
*David Calhas,Arlindo L. Oliveira*

Main category: cs.CV

TL;DR: 论文提出了一种受预测编码启发的反馈机制，通过循环优化内部状态提升模型性能，在噪声条件下和有限监督下表现优于前馈模型。


<details>
  <summary>Details</summary>
Motivation: 生物视觉系统依赖反馈连接优化感知，而人工神经网络多为前馈结构，缺乏迭代优化能力。

Method: 在U-Net架构中引入反馈循环，结合软最大投影和指数衰减确保稳定性。

Result: 反馈模型在噪声条件下表现更优，仅需两个训练样本即可超越随机性能，而前馈模型需至少四个样本。

Conclusion: 反馈机制提升了模型的鲁棒性和数据效率，为更自适应和生物启发的神经网络架构提供了方向。

Abstract: While biological vision systems rely heavily on feedback connections to
iteratively refine perception, most artificial neural networks remain purely
feedforward, processing input in a single static pass. In this work, we propose
a predictive coding inspired feedback mechanism that introduces a recurrent
loop from output to input, allowing the model to refine its internal state over
time. We implement this mechanism within a standard U-Net architecture and
introduce two biologically motivated operations, softmax projection and
exponential decay, to ensure stability of the feedback loop. Through controlled
experiments on a synthetic segmentation task, we show that the feedback model
significantly outperforms its feedforward counterpart in noisy conditions and
generalizes more effectively with limited supervision. Notably, feedback
achieves above random performance with just two training examples, while the
feedforward model requires at least four. Our findings demonstrate that
feedback enhances robustness and data efficiency, and offer a path toward more
adaptive and biologically inspired neural architectures. Code is available at:
github.com/DCalhas/feedback_segmentation.

</details>


### [120] [SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis](https://arxiv.org/abs/2507.10171)
*Youngmin Kim,Giyeong Oh,Kwangsoo Youm,Youngjae Yu*

Main category: cs.CV

TL;DR: SlumpGuard是一种基于AI的视频系统，用于实时自动评估混凝土的流动性，解决了传统坍落度测试的不足。


<details>
  <summary>Details</summary>
Motivation: 传统坍落度测试手动、耗时且不一致，无法满足实时监测需求。

Method: 开发AI驱动的视频系统，自动分析混凝土流动，无需人工干预。

Result: 系统提高了质量控制的准确性和效率，并通过实际部署验证了有效性。

Conclusion: SlumpGuard是混凝土质量保证的实用解决方案。

Abstract: Concrete workability is essential for construction quality, with the slump
test being the most common on-site method for its assessment. However,
traditional slump testing is manual, time-consuming, and prone to
inconsistency, limiting its applicability for real-time monitoring. To address
these challenges, we propose SlumpGuard, an AI-powered, video-based system that
automatically analyzes concrete flow from the truck chute to assess workability
in real time. Our system enables full-batch inspection without manual
intervention, improving both the accuracy and efficiency of quality control. We
present the system design, a the construction of a dedicated dataset, and
empirical results from real-world deployment, demonstrating the effectiveness
of SlumpGuard as a practical solution for modern concrete quality assurance.

</details>


### [121] [Minimizing the Pretraining Gap: Domain-aligned Text-Based Person Retrieval](https://arxiv.org/abs/2507.10195)
*Shuyu Yang,Yaxiong Wang,Yongrui Li,Li Zhu,Zhedong Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种基于文本的人物检索方法，通过图像和区域级别的域适应技术（DaD和MRA）解决合成数据与真实数据之间的域差距问题，并在多个数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题和手动标注成本高，合成数据常用于预训练模型，但其与真实数据之间的域差距（如光照、颜色和视角差异）限制了模型效果。

Method: 提出统一的人物检索流程，包含图像级域适应（Domain-aware Diffusion, DaD）和区域级多粒度关系对齐（Multi-granularity Relation Alignment, MRA）。

Result: 在CUHK-PEDES、ICFG-PEDES和RSTPReid数据集上实现了最优性能。

Conclusion: 双级别域适应方法有效解决了合成与真实数据间的域差距问题，提升了检索效果。

Abstract: In this work, we focus on text-based person retrieval, which aims to identify
individuals based on textual descriptions. Given the significant privacy issues
and the high cost associated with manual annotation, synthetic data has become
a popular choice for pretraining models, leading to notable advancements.
However, the considerable domain gap between synthetic pretraining datasets and
real-world target datasets, characterized by differences in lighting, color,
and viewpoint, remains a critical obstacle that hinders the effectiveness of
the pretrain-finetune paradigm. To bridge this gap, we introduce a unified
text-based person retrieval pipeline considering domain adaptation at both
image and region levels. In particular, it contains two primary components,
i.e., Domain-aware Diffusion (DaD) for image-level adaptation and
Multi-granularity Relation Alignment (MRA) for region-level adaptation. As the
name implies, Domain-aware Diffusion is to migrate the distribution of images
from the pretraining dataset domain to the target real-world dataset domain,
e.g., CUHK-PEDES. Subsequently, MRA performs a meticulous region-level
alignment by establishing correspondences between visual regions and their
descriptive sentences, thereby addressing disparities at a finer granularity.
Extensive experiments show that our dual-level adaptation method has achieved
state-of-the-art results on the CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets,
outperforming existing methodologies. The dataset, model, and code are
available at https://github.com/Shuyu-XJTU/MRA.

</details>


### [122] [A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images](https://arxiv.org/abs/2507.10202)
*Jaeseong Lee,Yeeun Choi,Heechan Choi,Hanjung Kim,Seonjoo Kim*

Main category: cs.CV

TL;DR: ECP框架通过两阶段方法提升多模态大语言模型在高分辨率图像上的性能，避免直接处理高分辨率图像的问题。


<details>
  <summary>Details</summary>
Motivation: MLLMs在高分辨率图像上表现不佳，因训练与测试分辨率不一致导致泛化能力差。

Method: 提出ECP框架，先提取候选区域再预测，保留细节同时避免高分辨率挑战。

Result: 在4K GUI和4K/8K MLLM感知任务上分别提升21.3%、5.8%、5.2%。

Conclusion: ECP有效解决了MLLMs在高分辨率图像上的局限性，显著提升性能。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in vision-language understanding, reasoning, and generation.
However, they struggle with tasks requiring fine-grained localization and
reasoning in high-resolution images. This constraint stems from the fact that
MLLMs are fine-tuned with fixed image resolution to align with the pre-trained
image encoder used in MLLM. Consequently, feeding high-resolution images
directly into MLLMs leads to poor generalization due to a train-test resolution
discrepancy, while downsampling these images-although ensuring
consistency-compromises fine-grained visual details and ultimately degrades
performance. To address this challenge, we propose Extract Candidate then
Predict (ECP), a novel training-free, task-agnostic two-stage framework
designed to enhance MLLM performance on high-resolution images. The key
intuition behind ECP is that while MLLMs struggle with high-resolution images,
their predictions on downsampled images still contain implicit localization
cues. By first identifying candidate region using the coarse prediction and
then predicting the final output based on candidate region, ECP effectively
preserves fine-grained details while mitigating the challenges posed by
high-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K
MLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared
to baseline respectively, demonstrating its effectiveness. Code is available at
https://github.com/yenncye/ECP.

</details>


### [123] [Improving Multimodal Learning via Imbalanced Learning](https://arxiv.org/abs/2507.10203)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: 论文提出了一种非对称表示学习（ARL）策略，通过不平衡优化提升多模态学习性能，证明了模态依赖比与方差比成反比时性能最优。


<details>
  <summary>Details</summary>
Motivation: 多模态学习常因不平衡优化导致性能不如单模态学习，现有方法通过梯度平衡解决，但作者认为平衡学习并非最优。

Method: 提出ARL策略，通过辅助正则化器计算模态预测方差，并基于方差比重新加权优化，同时引入预测偏差联合优化。

Result: 在多个数据集上的实验验证了ARL的有效性和通用性。

Conclusion: ARL无需额外参数，独立于多模态模型结构，显著提升了多模态学习性能。

Abstract: Multimodal learning often encounters the under-optimized problem and may
perform worse than unimodal learning. Existing approaches attribute this issue
to imbalanced learning across modalities and tend to address it through
gradient balancing. However, this paper argues that balanced learning is not
the optimal setting for multimodal learning. With bias-variance analysis, we
prove that imbalanced dependency on each modality obeying the inverse ratio of
their variances contributes to optimal performance. To this end, we propose the
Asymmetric Representation Learning(ARL) strategy to assist multimodal learning
via imbalanced optimization. ARL introduces auxiliary regularizers for each
modality encoder to calculate their prediction variance. ARL then calculates
coefficients via the unimodal variance to re-weight the optimization of each
modality, forcing the modality dependence ratio to be inversely proportional to
the modality variance ratio. Moreover, to minimize the generalization error,
ARL further introduces the prediction bias of each modality and jointly
optimizes them with multimodal loss. Notably, all auxiliary regularizers share
parameters with the multimodal model and rely only on the modality
representation. Thus the proposed ARL strategy introduces no extra parameters
and is independent of the structures and fusion methods of the multimodal
model. Finally, extensive experiments on various datasets validate the
effectiveness and versatility of ARL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-ARL}{https://github.com/shicaiwei123/ICCV2025-ARL}

</details>


### [124] [Is Micro-expression Ethnic Leaning?](https://arxiv.org/abs/2507.10209)
*Huai-Qian Khor,Yante Li,Xingxun Jiang,Guoying Zhao*

Main category: cs.CV

TL;DR: 该研究探讨了种族背景对情绪表达的影响，挑战了情绪普遍性假设，并提出了一种考虑种族差异的微表情识别框架。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证情绪表达是否真的如Ekman假设的那样具有普遍性，还是受到种族背景的影响。

Method: 构建了一个跨文化微表情数据库，并通过算法标注种族标签，进行了单一种族与混合种族的对比研究。

Result: 研究发现种族背景对情绪表达有显著影响，并提出了一个考虑种族差异的微表情识别框架。

Conclusion: 结论指出情绪普遍性假设是过度概括的，种族背景在情绪表达分析中应被纳入考虑。

Abstract: How much does ethnicity play its part in emotional expression? Emotional
expression and micro-expression research probe into understanding human
psychological responses to emotional stimuli, thereby revealing substantial
hidden yet authentic emotions that can be useful in the event of diagnosis and
interviews. While increased attention had been provided to micro-expression
analysis, the studies were done under Ekman's assumption of emotion
universality, where emotional expressions are identical across cultures and
social contexts. Our computational study uncovers some of the influences of
ethnic background in expression analysis, leading to an argument that the
emotional universality hypothesis is an overgeneralization from the perspective
of manual psychological analysis. In this research, we propose to investigate
the level of influence of ethnicity in a simulated micro-expression scenario.
We construct a cross-cultural micro-expression database and algorithmically
annotate the ethnic labels to facilitate the investigation. With the ethnically
annotated dataset, we perform a prima facie study to compare mono-ethnicity and
stereo-ethnicity in a controlled environment, which uncovers a certain
influence of ethnic bias via an experimental way. Building on this finding, we
propose a framework that integrates ethnic context into the emotional feature
learning process, yielding an ethnically aware framework that recognises
ethnicity differences in micro-expression recognition. For improved
understanding, qualitative analyses have been done to solidify the preliminary
investigation into this new realm of research. Code is publicly available at
https://github.com/IcedDoggie/ICMEW2025_EthnicMER

</details>


### [125] [Boosting Multimodal Learning via Disentangled Gradient Learning](https://arxiv.org/abs/2507.10213)
*Shicai Wei,Chunbo Luo,Yang Luo*

Main category: cs.CV

TL;DR: 论文揭示了多模态学习中模态编码器与模态融合模块之间的优化冲突，并提出解耦梯度学习（DGL）框架来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 多模态学习常因优化不足而表现不如单模态学习，现有方法未能解释主导模态在多模态模型中表现不佳的原因。

Method: 提出DGL框架，通过截断多模态损失的反向传播梯度并替换为单模态损失梯度，消除模态编码器与融合模块间的梯度干扰。

Result: 在多种模态、任务和框架上的实验验证了DGL的有效性和通用性。

Conclusion: DGL通过解耦优化过程，显著提升了多模态模型的性能。

Abstract: Multimodal learning often encounters the under-optimized problem and may have
worse performance than unimodal learning. Existing methods attribute this
problem to the imbalanced learning between modalities and rebalance them
through gradient modulation. However, they fail to explain why the dominant
modality in multimodal models also underperforms that in unimodal learning. In
this work, we reveal the optimization conflict between the modality encoder and
modality fusion module in multimodal models. Specifically, we prove that the
cross-modal fusion in multimodal models decreases the gradient passed back to
each modality encoder compared with unimodal models. Consequently, the
performance of each modality in the multimodal model is inferior to that in the
unimodal model. To this end, we propose a disentangled gradient learning (DGL)
framework to decouple the optimization of the modality encoder and modality
fusion module in the multimodal model. DGL truncates the gradient
back-propagated from the multimodal loss to the modality encoder and replaces
it with the gradient from unimodal loss. Besides, DGL removes the gradient
back-propagated from the unimodal loss to the modality fusion module. This
helps eliminate the gradient interference between the modality encoder and
modality fusion module while ensuring their respective optimization processes.
Finally, extensive experiments on multiple types of modalities, tasks, and
frameworks with dense cross-modal interaction demonstrate the effectiveness and
versatility of the proposed DGL. Code is available at
\href{https://github.com/shicaiwei123/ICCV2025-GDL}{https://github.com/shicaiwei123/ICCV2025-GDL}

</details>


### [126] [From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation](https://arxiv.org/abs/2507.10217)
*Jeongho Kim,Sunghyun Park,Hyoungwoo Park,Sungrack Yun,Jaegul Choo,Seokeon Cho*

Main category: cs.CV

TL;DR: 提出了一种名为Wardrobe Polyptych LoRA的新方法，通过部分级可控模型实现个性化人类图像生成，解决了现有方法计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 个性化人类图像生成需要精确且一致的属性保留，现有方法要么需要推理时微调，要么需要大规模数据集训练，计算成本高且不实用。

Method: 训练仅LoRA层，利用空间参考减少信息丢失，并引入选择性主题区域损失以提高生成图像与文本提示的对齐性。

Result: 实验表明，该方法在保真度和一致性上显著优于现有技术，实现了真实且保留身份的全身体合成。

Conclusion: Wardrobe Polyptych LoRA无需推理阶段额外参数，仅需少量训练样本即可生成高质量图像，为实时应用提供了实用解决方案。

Abstract: Recent diffusion models achieve personalization by learning specific
subjects, allowing learned attributes to be integrated into generated images.
However, personalized human image generation remains challenging due to the
need for precise and consistent attribute preservation (e.g., identity,
clothing details). Existing subject-driven image generation methods often
require either (1) inference-time fine-tuning with few images for each new
subject or (2) large-scale dataset training for generalization. Both approaches
are computationally expensive and impractical for real-time applications. To
address these limitations, we present Wardrobe Polyptych LoRA, a novel
part-level controllable model for personalized human image generation. By
training only LoRA layers, our method removes the computational burden at
inference while ensuring high-fidelity synthesis of unseen subjects. Our key
idea is to condition the generation on the subject's wardrobe and leverage
spatial references to reduce information loss, thereby improving fidelity and
consistency. Additionally, we introduce a selective subject region loss, which
encourages the model to disregard some of reference images during training. Our
loss ensures that generated images better align with text prompts while
maintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no
additional parameters at the inference stage and performs generation using a
single model trained on a few training samples. We construct a new dataset and
benchmark tailored for personalized human image generation. Extensive
experiments show that our approach significantly outperforms existing
techniques in fidelity and consistency, enabling realistic and
identity-preserving full-body synthesis.

</details>


### [127] [Straighten Viscous Rectified Flow via Noise Optimization](https://arxiv.org/abs/2507.10218)
*Jimin Dai,Jiexi Yan,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: VRFNO提出了一种改进Reflow的方法，通过噪声优化和速度场增强，显著提升单步和少步生成图像的质量。


<details>
  <summary>Details</summary>
Motivation: Reflow在训练中存在分布差距问题，无法快速生成高质量图像。

Method: VRFNO结合编码器和神经速度场，引入历史速度项和噪声优化，优化耦合训练。

Result: 实验表明VRFNO在单步和少步生成任务中表现优异。

Conclusion: VRFNO有效解决了Reflow的局限性，实现了先进的生成性能。

Abstract: The Reflow operation aims to straighten the inference trajectories of the
rectified flow during training by constructing deterministic couplings between
noises and images, thereby improving the quality of generated images in
single-step or few-step generation. However, we identify critical limitations
in Reflow, particularly its inability to rapidly generate high-quality images
due to a distribution gap between images in its constructed deterministic
couplings and real images. To address these shortcomings, we propose a novel
alternative called Straighten Viscous Rectified Flow via Noise Optimization
(VRFNO), which is a joint training framework integrating an encoder and a
neural velocity field. VRFNO introduces two key innovations: (1) a historical
velocity term that enhances trajectory distinction, enabling the model to more
accurately predict the velocity of the current trajectory, and (2) the noise
optimization through reparameterization to form optimized couplings with real
images which are then utilized for training, effectively mitigating errors
caused by Reflow's limitations. Comprehensive experiments on synthetic data and
real datasets with varying resolutions show that VRFNO significantly mitigates
the limitations of Reflow, achieving state-of-the-art performance in both
one-step and few-step generation tasks.

</details>


### [128] [ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users](https://arxiv.org/abs/2507.10223)
*Xiangyu Yin,Boyuan Yang,Weichen Liu,Qiyao Xue,Abrar Alamri,Goeran Fiedler,Wei Gao*

Main category: cs.CV

TL;DR: 论文介绍了一个名为ProGait的多用途数据集，用于支持视频对象分割、2D人体姿态估计和步态分析等任务，旨在解决视觉机器学习方法在假肢检测和分析中的挑战。


<details>
  <summary>Details</summary>
Motivation: 假肢在临床康复中至关重要，但现有的视觉机器学习方法在检测和分析假肢时面临困难，因其独特外观和新运动模式。

Method: 通过创建ProGait数据集，包含412个视频片段，记录四名大腿截肢者测试新假肢的行走试验，并提供基准任务和微调基线模型。

Result: 基线模型在假肢特定任务中表现出比预训练视觉模型更好的泛化能力。

Conclusion: ProGait数据集为假肢相关的视觉任务提供了实用工具，并展示了其在实际应用中的潜力。

Abstract: Prosthetic legs play a pivotal role in clinical rehabilitation, allowing
individuals with lower-limb amputations the ability to regain mobility and
improve their quality of life. Gait analysis is fundamental for optimizing
prosthesis design and alignment, directly impacting the mobility and life
quality of individuals with lower-limb amputations. Vision-based machine
learning (ML) methods offer a scalable and non-invasive solution to gait
analysis, but face challenges in correctly detecting and analyzing prosthesis,
due to their unique appearances and new movement patterns. In this paper, we
aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait,
to support multiple vision tasks including Video Object Segmentation, 2D Human
Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from
four above-knee amputees when testing multiple newly-fitted prosthetic legs
through walking trials, and depicts the presence, contours, poses, and gait
patterns of human subjects with transfemoral prosthetic legs. Alongside the
dataset itself, we also present benchmark tasks and fine-tuned baseline models
to illustrate the practical application and performance of the ProGait dataset.
We compared our baseline models against pre-trained vision models,
demonstrating improved generalizability when applying the ProGait dataset for
prosthesis-specific tasks. Our code is available at
https://github.com/pittisl/ProGait and dataset at
https://huggingface.co/datasets/ericyxy98/ProGait.

</details>


### [129] [Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection](https://arxiv.org/abs/2507.10225)
*Jinglun Li,Kaixun Jiang,Zhaoyu Chen,Bo Lin,Yao Tang,Weifeng Ge,Wenqiang Zhang*

Main category: cs.CV

TL;DR: SynOOD利用基础模型生成合成OOD数据，通过迭代修复和噪声调整增强CLIP模型的边界区分能力，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有预训练视觉语言模型在处理接近InD数据的OOD样本时易误分类的问题。

Method: 利用扩散模型和多模态大语言模型生成合成OOD数据，通过迭代修复和噪声调整优化样本，并微调CLIP模型。

Result: 在ImageNet基准测试中，AUROC提升2.80%，FPR95降低11.13%，性能显著优于现有方法。

Conclusion: SynOOD通过合成边界对齐的OOD数据，有效增强了模型的OOD检测能力，且计算开销小。

Abstract: Pre-trained vision-language models have exhibited remarkable abilities in
detecting out-of-distribution (OOD) samples. However, some challenging OOD
samples, which lie close to in-distribution (InD) data in image feature space,
can still lead to misclassification. The emergence of foundation models like
diffusion models and multimodal large language models (MLLMs) offers a
potential solution to this issue. In this work, we propose SynOOD, a novel
approach that harnesses foundation models to generate synthetic, challenging
OOD data for fine-tuning CLIP models, thereby enhancing boundary-level
discrimination between InD and OOD samples. Our method uses an iterative
in-painting process guided by contextual prompts from MLLMs to produce nuanced,
boundary-aligned OOD samples. These samples are refined through noise
adjustments based on gradients from OOD scores like the energy score,
effectively sampling from the InD/OOD boundary. With these carefully
synthesized images, we fine-tune the CLIP image encoder and negative label
features derived from the text encoder to strengthen connections between
near-boundary OOD samples and a set of negative labels. Finally, SynOOD
achieves state-of-the-art performance on the large-scale ImageNet benchmark,
with minimal increases in parameters and runtime. Our approach significantly
surpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by
11.13%. Codes are available in https://github.com/Jarvisgivemeasuit/SynOOD.

</details>


### [130] [Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?](https://arxiv.org/abs/2507.10236)
*Despina Konstantinidou,Dimitrios Karageorgiou,Christos Koutlis,Olga Papadopoulou,Emmanouil Schinas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 论文探讨了AI生成图像检测（AID）在现实世界中的挑战，提出了ITW-SM数据集，并分析了影响AID性能的四个关键因素，最终将AUC平均提升了26.87%。


<details>
  <summary>Details</summary>
Motivation: 随着生成技术的快速发展，AI生成图像的逼真度已足以欺骗观察者，亟需提升AID模型在现实场景中的检测能力。

Method: 引入ITW-SM数据集，系统评估了骨干架构、训练数据组成、预处理策略和数据增强组合对AID性能的影响。

Result: 通过优化上述因素，AID模型在现实条件下的AUC平均提升了26.87%。

Conclusion: 研究揭示了AID模型在现实世界中的局限性，并提出了改进方向，为未来研究提供了重要参考。

Abstract: The rapid advancement of generative technologies presents both unprecedented
creative opportunities and significant challenges, particularly in maintaining
social trust and ensuring the integrity of digital information. Following these
concerns, the challenge of AI-Generated Image Detection (AID) becomes
increasingly critical. As these technologies become more sophisticated, the
quality of AI-generated images has reached a level that can easily deceive even
the most discerning observers. Our systematic evaluation highlights a critical
weakness in current AI-Generated Image Detection models: while they perform
exceptionally well on controlled benchmark datasets, they struggle
significantly with real-world variations. To assess this, we introduce ITW-SM,
a new dataset of real and AI-generated images collected from major social media
platforms. In this paper, we identify four key factors that influence AID
performance in real-world scenarios: backbone architecture, training data
composition, pre-processing strategies and data augmentation combinations. By
systematically analyzing these components, we shed light on their impact on
detection efficacy. Our modifications result in an average AUC improvement of
26.87% across various AID models under real-world conditions.

</details>


### [131] [Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks](https://arxiv.org/abs/2507.10239)
*Ben Hamscher,Edgar Heinert,Annika Mütze,Kira Maag,Matthias Rottmann*

Main category: cs.CV

TL;DR: 通过风格迁移增强语义分割任务，减少纹理偏差并提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究风格迁移是否能在语义分割中减少纹理偏差并增强鲁棒性。

Method: 使用Voronoi细胞生成随机区域进行风格迁移，训练语义分割DNN。

Result: 风格迁移减少了纹理偏差，显著提高了对图像损坏和对抗攻击的鲁棒性。

Conclusion: 风格迁移在语义分割中具有普适性，能有效减少纹理依赖并增强鲁棒性。

Abstract: Recent research has investigated the shape and texture biases of deep neural
networks (DNNs) in image classification which influence their generalization
capabilities and robustness. It has been shown that, in comparison to regular
DNN training, training with stylized images reduces texture biases in image
classification and improves robustness with respect to image corruptions. In an
effort to advance this line of research, we examine whether style transfer can
likewise deliver these two effects in semantic segmentation. To this end, we
perform style transfer with style varying across artificial image areas. Those
random areas are formed by a chosen number of Voronoi cells. The resulting
style-transferred data is then used to train semantic segmentation DNNs with
the objective of reducing their dependence on texture cues while enhancing
their reliance on shape-based features. In our experiments, it turns out that
in semantic segmentation, style transfer augmentation reduces texture bias and
strongly increases robustness with respect to common image corruptions as well
as adversarial attacks. These observations hold for convolutional neural
networks and transformer architectures on the Cityscapes dataset as well as on
PASCAL Context, showing the generality of the proposed method.

</details>


### [132] [Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures](https://arxiv.org/abs/2507.10265)
*Xinlong Ding,Hongwei Yu,Jiawei Li,Feifan Li,Yu Shang,Bochao Zou,Huimin Ma,Jiansheng Chen*

Main category: cs.CV

TL;DR: 论文提出了一种基于多折径向对称的卡莱多背景攻击（KBA），通过优化卡莱多背景片段显著提升了攻击相机姿态估计模型的效果。


<details>
  <summary>Details</summary>
Motivation: 在稀疏输入的物体中心场景中，背景纹理对相机姿态估计的准确性有显著影响，因此需要一种有效的攻击方法。

Method: 使用相同片段形成多折径向对称的圆盘，并提出投影方向一致性损失优化卡莱多片段。

Result: 优化的对抗性卡莱多背景能有效攻击多种相机姿态估计模型。

Conclusion: KBA方法通过优化背景片段显著提升了攻击效果，证明了其在对抗相机姿态估计模型中的有效性。

Abstract: Camera pose estimation is a fundamental computer vision task that is
essential for applications like visual localization and multi-view stereo
reconstruction. In the object-centric scenarios with sparse inputs, the
accuracy of pose estimation can be significantly influenced by background
textures that occupy major portions of the images across different viewpoints.
In light of this, we introduce the Kaleidoscopic Background Attack (KBA), which
uses identical segments to form discs with multi-fold radial symmetry. These
discs maintain high similarity across different viewpoints, enabling effective
attacks on pose estimation models even with natural texture segments.
Additionally, a projected orientation consistency loss is proposed to optimize
the kaleidoscopic segments, leading to significant enhancement in the attack
effectiveness. Experimental results show that optimized adversarial
kaleidoscopic backgrounds can effectively attack various camera pose estimation
models.

</details>


### [133] [FTCFormer: Fuzzy Token Clustering Transformer for Image Classification](https://arxiv.org/abs/2507.10283)
*Muyi Bao,Changyu Zeng,Yifan Wang,Zhengni Yang,Zimu Wang,Guangliang Cheng,Jun Qi,Wei Wang*

Main category: cs.CV

TL;DR: FTCFormer是一种基于聚类的Transformer模型，通过动态生成语义相关的视觉标记来优化特征表示，显著提升了图像分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型将图像嵌入为均匀的网格标记，忽略了语义信息，导致特征表示不理想。

Method: 提出Fuzzy Token Clustering Transformer (FTCFormer)，结合聚类下采样模块、DPC-FKNN机制、SCS评分和Cmerge策略。

Result: 在32个数据集上验证，FTCFormer在图像分类任务中表现优于基线模型，提升幅度为1.43%（细粒度数据集）、1.09%（自然图像数据集）、0.97%（医学数据集）和0.55%（遥感数据集）。

Conclusion: FTCFormer通过语义驱动的标记生成策略，显著提升了Transformer模型的性能。

Abstract: Transformer-based deep neural networks have achieved remarkable success
across various computer vision tasks, largely attributed to their long-range
self-attention mechanism and scalability. However, most transformer
architectures embed images into uniform, grid-based vision tokens, neglecting
the underlying semantic meanings of image regions, resulting in suboptimal
feature representations. To address this issue, we propose Fuzzy Token
Clustering Transformer (FTCFormer), which incorporates a novel clustering-based
downsampling module to dynamically generate vision tokens based on the semantic
meanings instead of spatial positions. It allocates fewer tokens to less
informative regions and more to represent semantically important regions,
regardless of their spatial adjacency or shape irregularity. To further enhance
feature extraction and representation, we propose a Density Peak
Clustering-Fuzzy K-Nearest Neighbor (DPC-FKNN) mechanism for clustering center
determination, a Spatial Connectivity Score (SCS) for token assignment, and a
channel-wise merging (Cmerge) strategy for token merging. Extensive experiments
on 32 datasets across diverse domains validate the effectiveness of FTCFormer
on image classification, showing consistent improvements over the TCFormer
baseline, achieving gains of improving 1.43% on five fine-grained datasets,
1.09% on six natural image datasets, 0.97% on three medical datasets and 0.55%
on four remote sensing datasets. The code is available at:
https://github.com/BaoBao0926/FTCFormer/tree/main.

</details>


### [134] [Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration](https://arxiv.org/abs/2507.10293)
*Wenkang Han,Wang Lin,Yiyun Zhou,Qi Liu,Shulei Wang,Chang Yao,Jingyuan Chen*

Main category: cs.CV

TL;DR: IP-FVR是一种新方法，通过参考图像提供身份条件，结合交叉注意力和反馈学习，解决视频中身份漂移问题，显著提升人脸视频恢复质量。


<details>
  <summary>Details</summary>
Motivation: 传统方法在严重退化情况下难以保留细粒度和身份特征，导致结果缺乏个体特性。

Method: IP-FVR利用参考图像作为视觉提示，采用解耦交叉注意力机制和反馈学习方法，结合指数混合策略和多流负提示。

Result: 在合成和真实数据集上，IP-FVR在质量和身份保留方面优于现有方法。

Conclusion: IP-FVR在实用人脸视频恢复中具有显著潜力。

Abstract: Face Video Restoration (FVR) aims to recover high-quality face videos from
degraded versions. Traditional methods struggle to preserve fine-grained,
identity-specific features when degradation is severe, often producing
average-looking faces that lack individual characteristics. To address these
challenges, we introduce IP-FVR, a novel method that leverages a high-quality
reference face image as a visual prompt to provide identity conditioning during
the denoising process. IP-FVR incorporates semantically rich identity
information from the reference image using decoupled cross-attention
mechanisms, ensuring detailed and identity consistent results. For intra-clip
identity drift (within 24 frames), we introduce an identity-preserving feedback
learning method that combines cosine similarity-based reward signals with
suffix-weighted temporal aggregation. This approach effectively minimizes drift
within sequences of frames. For inter-clip identity drift, we develop an
exponential blending strategy that aligns identities across clips by
iteratively blending frames from previous clips during the denoising process.
This method ensures consistent identity representation across different clips.
Additionally, we enhance the restoration process with a multi-stream negative
prompt, guiding the model's attention to relevant facial attributes and
minimizing the generation of low-quality or incorrect features. Extensive
experiments on both synthetic and real-world datasets demonstrate that IP-FVR
outperforms existing methods in both quality and identity preservation,
showcasing its substantial potential for practical applications in face video
restoration.

</details>


### [135] [FaceLLM: A Multimodal Large Language Model for Face Understanding](https://arxiv.org/abs/2507.10300)
*Hatef Otroshi Shahreza,Sébastien Marcel*

Main category: cs.CV

TL;DR: FaceLLM是一个专为面部图像理解设计的MLLM，通过ChatGPT生成的弱监督数据训练，提升了面部任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在通用数据集上训练，缺乏对领域特定视觉线索（如面部图像）的推理能力。

Method: 提出弱监督流程，利用ChatGPT生成基于FairFace数据集的问答对，构建FairFaceGPT数据集。

Result: FaceLLM在多种面部任务中表现优异，达到SOTA性能。

Conclusion: FaceLLM展示了语言模型合成监督在领域专用MLLM中的潜力，为可信赖的多模态AI系统奠定了基础。

Abstract: Multimodal large language models (MLLMs) have shown remarkable performance in
vision-language tasks. However, existing MLLMs are primarily trained on generic
datasets, limiting their ability to reason on domain-specific visual cues such
as those in facial images. In particular, tasks that require detailed
understanding of facial structure, expression, emotion, and demographic
features remain underexplored by MLLMs due to the lack of large-scale annotated
face image-text datasets. In this work, we introduce FaceLLM, a multimodal
large language model trained specifically for facial image understanding. To
construct the training data, we propose a novel weakly supervised pipeline that
uses ChatGPT with attribute-aware prompts to generate high-quality
question-answer pairs based on images from the FairFace dataset. The resulting
corpus, called FairFaceGPT, covers a diverse set of attributes including
expression, pose, skin texture, and forensic information. Our experiments
demonstrate that FaceLLM improves the performance of MLLMs on various
face-centric tasks and achieves state-of-the-art performance. This work
highlights the potential of synthetic supervision via language models for
building domain-specialized MLLMs, and sets a precedent for trustworthy,
human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM
models are publicly available in the project page.

</details>


### [136] [DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs](https://arxiv.org/abs/2507.10302)
*Jiahe Zhao,Rongkun Zheng,Yi Wang,Helin Wang,Hengshuang Zhao*

Main category: cs.CV

TL;DR: DisCo是一种新的视觉封装方法，通过视觉概念判别器和时间焦点校准器，解决了视频MLLM中的语义模糊和时间不连贯问题，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有线性投影器在视频MLLM中导致语义模糊和时间不连贯，而重采样器结构虽有效但未充分探索。

Method: DisCo包含视觉概念判别器（VCD）和时间焦点校准器（TFC），分别解决语义和时间问题。

Result: DisCo在多个视频理解基准测试中显著优于现有方法，同时提高了令牌效率。

Conclusion: DisCo为视频MLLM提供了一种高效且语义清晰的视觉封装解决方案。

Abstract: In video Multimodal Large Language Models (video MLLMs), the visual
encapsulation process plays a pivotal role in converting video contents into
representative tokens for LLM input. While linear projectors are widely
employed for encapsulation, they introduce semantic indistinctness and temporal
incoherence when applied to videos. Conversely, the structure of resamplers
shows promise in tackling these challenges, but an effective solution remains
unexplored. Drawing inspiration from resampler structures, we introduce DisCo,
a novel visual encapsulation method designed to yield semantically distinct and
temporally coherent visual tokens for video MLLMs. DisCo integrates two key
components: (1) A Visual Concept Discriminator (VCD) module, assigning unique
semantics for visual tokens by associating them in pair with discriminative
concepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring
consistent temporal focus of visual tokens to video elements across every video
frame. Through extensive experiments on multiple video MLLM frameworks, we
demonstrate that DisCo remarkably outperforms previous state-of-the-art methods
across a variety of video understanding benchmarks, while also achieving higher
token efficiency thanks to the reduction of semantic indistinctness. The code:
https://github.com/ZJHTerry18/DisCo.

</details>


### [137] [Contrastive Pretraining with Dual Visual Encoders for Gloss-Free Sign Language Translation](https://arxiv.org/abs/2507.10306)
*Ozge Mercanoglu Sincan,Richard Bowden*

Main category: cs.CV

TL;DR: 提出了一种基于双视觉编码器的无注释手语翻译框架，通过对比视觉-语言预训练提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统手语翻译依赖昂贵且不完整的注释，无法完全捕捉连续手语的复杂性。

Method: 采用双视觉编码器框架，通过对比目标对齐视觉和文本嵌入，下游任务中融合视觉特征输入编码器-解码器模型。

Result: 在Phoenix-2014T基准测试中，双编码器架构优于单流变体，并在无注释方法中取得最高BLEU-4分数。

Conclusion: 双视觉编码器框架有效提升了无注释手语翻译的性能。

Abstract: Sign Language Translation (SLT) aims to convert sign language videos into
spoken or written text. While early systems relied on gloss annotations as an
intermediate supervision, such annotations are costly to obtain and often fail
to capture the full complexity of continuous signing. In this work, we propose
a two-phase, dual visual encoder framework for gloss-free SLT, leveraging
contrastive visual-language pretraining. During pretraining, our approach
employs two complementary visual backbones whose outputs are jointly aligned
with each other and with sentence-level text embeddings via a contrastive
objective. During the downstream SLT task, we fuse the visual features and
input them into an encoder-decoder model. On the Phoenix-2014T benchmark, our
dual encoder architecture consistently outperforms its single stream variants
and achieves the highest BLEU-4 score among existing gloss-free SLT approaches.

</details>


### [138] [Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching](https://arxiv.org/abs/2507.10318)
*Yuhan Liu,Jingwen Fu,Yang Wu,Kangyi Wu,Pengna Li,Jiayi Wu,Sanping Zhou,Jingmin Xin*

Main category: cs.CV

TL;DR: 论文提出了一种名为IMD的框架，通过结合生成式扩散模型和跨图像交互提示模块，解决了视觉基础模型在特征匹配中的不对齐问题，并在多实例场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有工作忽视了视觉基础模型在特征匹配中的不对齐问题，导致在多实例特征匹配中表现不佳。

Method: IMD框架结合了生成式扩散模型以捕捉实例级细节，并设计了跨图像交互提示模块促进双向信息交互。

Result: IMD在常用基准测试中达到新SOTA，并在多实例基准IMIM上提升12%。

Conclusion: IMD有效解决了视觉基础模型在特征匹配中的不对齐问题，显著提升了多实例场景的性能。

Abstract: Leveraging the vision foundation models has emerged as a mainstream paradigm
that improves the performance of image feature matching. However, previous
works have ignored the misalignment when introducing the foundation models into
feature matching. The misalignment arises from the discrepancy between the
foundation models focusing on single-image understanding and the cross-image
understanding requirement of feature matching. Specifically, 1) the embeddings
derived from commonly used foundation models exhibit discrepancies with the
optimal embeddings required for feature matching; 2) lacking an effective
mechanism to leverage the single-image understanding ability into cross-image
understanding. A significant consequence of the misalignment is they struggle
when addressing multi-instance feature matching problems. To address this, we
introduce a simple but effective framework, called IMD (Image feature Matching
with a pre-trained Diffusion model) with two parts: 1) Unlike the dominant
solutions employing contrastive-learning based foundation models that emphasize
global semantics, we integrate the generative-based diffusion models to
effectively capture instance-level details. 2) We leverage the prompt mechanism
in generative model as a natural tunnel, propose a novel cross-image
interaction prompting module to facilitate bidirectional information
interaction between image pairs. To more accurately measure the misalignment,
we propose a new benchmark called IMIM, which focuses on multi-instance
scenarios. Our proposed IMD establishes a new state-of-the-art in commonly
evaluated benchmarks, and the superior improvement 12% in IMIM indicates our
method efficiently mitigates the misalignment.

</details>


### [139] [Text Embedding Knows How to Quantize Text-Guided Diffusion Models](https://arxiv.org/abs/2507.10340)
*Hongjae Lee,Myungjun Son,Dongjea Kang,Seung-Won Jung*

Main category: cs.CV

TL;DR: QLIP是一种新的量化方法，利用文本提示指导扩散模型的量化，降低计算复杂度并提升生成图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成任务中表现出色，但计算复杂度高，限制了其在资源受限环境中的应用。现有量化方法未充分利用输入条件（如文本提示）的信息。

Method: 提出QLIP方法，通过文本提示动态选择每层和每个时间步的量化精度，并可与其他量化方法无缝集成。

Result: 实验表明，QLIP能有效降低计算复杂度，并在多个数据集上提升生成图像质量。

Conclusion: QLIP为扩散模型量化提供了一种高效且灵活的方法，显著提升了资源受限环境下的实用性。

Abstract: Despite the success of diffusion models in image generation tasks such as
text-to-image, the enormous computational complexity of diffusion models limits
their use in resource-constrained environments. To address this, network
quantization has emerged as a promising solution for designing efficient
diffusion models. However, existing diffusion model quantization methods do not
consider input conditions, such as text prompts, as an essential source of
information for quantization. In this paper, we propose a novel quantization
method dubbed Quantization of Language-to-Image diffusion models using text
Prompts (QLIP). QLIP leverages text prompts to guide the selection of bit
precision for every layer at each time step. In addition, QLIP can be
seamlessly integrated into existing quantization methods to enhance
quantization efficiency. Our extensive experiments demonstrate the
effectiveness of QLIP in reducing computational complexity and improving the
quality of the generated images across various datasets.

</details>


### [140] [FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans](https://arxiv.org/abs/2507.10343)
*Hugo Norrby,Gabriel Färm,Kevin Hernandez-Diaz,Fernando Alonso-Fernandez*

Main category: cs.CV

TL;DR: FGSSNet是一种多头部特征引导的语义分割架构，旨在提升平面图中墙体分割的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 改进平面图中墙体分割的泛化能力。

Method: 采用U-Net分割主干，结合多头部专用特征提取器，提取领域特定特征图并注入U-Net潜在空间以引导分割过程。

Result: 实验表明，注入特征后性能优于传统U-Net。

Conclusion: FGSSNet通过特征注入有效提升了墙体分割性能。

Abstract: We introduce FGSSNet, a novel multi-headed feature-guided semantic
segmentation (FGSS) architecture designed to improve the generalization ability
of wall segmentation on floorplans. FGSSNet features a U-Net segmentation
backbone with a multi-headed dedicated feature extractor used to extract
domain-specific feature maps which are injected into the latent space of U-Net
to guide the segmentation process. This dedicated feature extractor is trained
as an encoder-decoder with selected wall patches, representative of the walls
present in the input floorplan, to produce a compressed latent representation
of wall patches while jointly trained to predict the wall width. In doing so,
we expect that the feature extractor encodes texture and width features of wall
patches that are useful to guide the wall segmentation process. Our experiments
show increased performance by the use of such injected features in comparison
to the vanilla U-Net, highlighting the validity of the proposed approach.

</details>


### [141] [Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter](https://arxiv.org/abs/2507.10355)
*Bo Jiang,Xueyang Ze,Beibei Wang,Xixi Wang,Xixi Wan,Bin Luo*

Main category: cs.CV

TL;DR: 提出了一种基于随机图模型的文本适配器（VRGAdapter），用于捕捉类别描述的多样性和类间关系，并通过不确定性引导的多分支融合（UMF）提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有确定性文本适配器无法充分捕捉类别描述的多样性和类间关系，限制了视觉语言模型（VLM）在下游任务中的潜力。

Method: 提出VRGAdapter，利用顶点随机知识图（VRKG）建模类别描述的多样性和类间关系，通过概率消息传播学习上下文感知分布表示，并使用重参数化采样函数进行适配器学习。同时引入UMF动态集成多个预训练模型。

Result: 在多个基准数据集上的实验验证了VRGAdapter和UMF的有效性。

Conclusion: VRGAdapter提供了一种更通用的适配器解决方案，能够更好地利用类别描述的多样性和类间关系，提升下游任务的性能。

Abstract: Textual adapter-based tuning methods have shown significant potential in
transferring knowledge from pre-trained Vision-Language Models (VLMs) to
downstream tasks. Existing works generally employ the deterministic textual
feature adapter to refine each category textual representation. However, due to
inherent factors such as different attributes and contexts, there exists
significant diversity in textual descriptions for each category. Such
description diversity offers rich discriminative semantic knowledge that can
benefit downstream visual learning tasks. Obviously, traditional deterministic
adapter model cannot adequately capture this varied semantic information. Also,
it is desirable to exploit the inter-class relationships in VLM adapter. To
address these issues, we propose to exploit random graph model into VLM adapter
and develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first
models the inherent diverse descriptions of each category and inter-class
relationships of different categories simultaneously by leveraging a Vertex
Random Knowledge Graph (VRKG) model. Then, it employs probabilistic message
propagation on VRKG to learn context-aware distribution representation for each
class node. Finally, it adopts a reparameterized sampling function to achieve
textual adapter learning. Note that, VRGAdapter provides a more general adapter
solution that encompasses traditional graph-based adapter as a special case. In
addition, to enable more robust performance for downstream tasks, we also
introduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that
dynamically integrates multiple pre-trained models for ensemble prediction.
Extensive experiments on multiple benchmark datasets demonstrate the
effectiveness of our approach.

</details>


### [142] [Fine-Grained Zero-Shot Object Detection](https://arxiv.org/abs/2507.10358)
*Hongxu Ma,Chenbo Zhang,Lu Zhang,Jiaogen Zhou,Jihong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: 本文提出并解决了细粒度零样本目标检测（FG-ZSD）问题，开发了基于改进两阶段检测器的MSHC方法，并构建了首个FG-ZSD基准数据集FGZSD-Birds。实验表明该方法优于现有ZSD模型。


<details>
  <summary>Details</summary>
Motivation: 现有零样本目标检测（ZSD）主要针对粗粒度对象，而现实中的细粒度对象（如不同鸟类、鱼类和花卉）因视觉差异微小而难以区分，因此需要解决FG-ZSD问题。

Method: 提出MSHC方法，基于改进的两阶段检测器，采用多级语义感知嵌入对齐损失，确保视觉与语义空间的紧密耦合。

Result: 在构建的FGZSD-Birds数据集（包含1432个物种的148,820张图像）上，MSHC方法表现优于现有ZSD模型。

Conclusion: MSHC方法有效解决了FG-ZSD问题，为细粒度零样本目标检测提供了新思路和基准数据集。

Abstract: Zero-shot object detection (ZSD) aims to leverage semantic descriptions to
localize and recognize objects of both seen and unseen classes. Existing ZSD
works are mainly coarse-grained object detection, where the classes are
visually quite different, thus are relatively easy to distinguish. However, in
real life we often have to face fine-grained object detection scenarios, where
the classes are too similar to be easily distinguished. For example, detecting
different kinds of birds, fishes, and flowers.
  In this paper, we propose and solve a new problem called Fine-Grained
Zero-Shot Object Detection (FG-ZSD for short), which aims to detect objects of
different classes with minute differences in details under the ZSD paradigm. We
develop an effective method called MSHC for the FG-ZSD task, which is based on
an improved two-stage detector and employs a multi-level semantics-aware
embedding alignment loss, ensuring tight coupling between the visual and
semantic spaces. Considering that existing ZSD datasets are not suitable for
the new FG-ZSD task, we build the first FG-ZSD benchmark dataset FGZSD-Birds,
which contains 148,820 images falling into 36 orders, 140 families, 579 genera
and 1432 species. Extensive experiments on FGZSD-Birds show that our method
outperforms existing ZSD models.

</details>


### [143] [Test-Time Canonicalization by Foundation Models for Robust Perception](https://arxiv.org/abs/2507.10375)
*Utkarsh Singhal,Ryan Feng,Stella X. Yu,Atul Prakash*

Main category: cs.CV

TL;DR: FOCAL是一种测试时、数据驱动的框架，利用基础模型的互联网规模视觉先验，通过生成和优化候选变换来实现鲁棒感知，无需重新训练或架构更改。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖专用架构或预定义增强训练，限制了泛化能力，需要一种更通用的鲁棒感知方法。

Method: FOCAL通过生成和优化候选变换，使其朝向视觉上典型的“规范”视图，从而增强鲁棒性。

Result: 实验显示FOCAL提升了CLIP和SAM在2D/3D旋转、光照变化和昼夜变化等挑战性变换中的鲁棒性。

Conclusion: FOCAL挑战了变换特定训练的必要性，提供了一种可扩展的鲁棒感知路径。

Abstract: Real-world visual perception requires invariance to diverse transformations,
yet current methods rely heavily on specialized architectures or training on
predefined augmentations, limiting generalization. We propose FOCAL, a
test-time, data-driven framework that achieves robust perception by leveraging
internet-scale visual priors from foundation models. By generating and
optimizing candidate transformations toward visually typical, "canonical"
views, FOCAL enhances robustness without re-training or architectural changes.
Our experiments demonstrate improved robustness of CLIP and SAM across
challenging transformations, including 2D/3D rotations, illumination shifts
(contrast and color), and day-night variations. We also highlight potential
applications in active vision. Our approach challenges the assumption that
transform-specific training is necessary, instead offering a scalable path to
invariance. Our code is available at: https://github.com/sutkarsh/focal.

</details>


### [144] [Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks](https://arxiv.org/abs/2507.10381)
*Aaryam Sharma*

Main category: cs.CV

TL;DR: 论文提出了一种将拓扑数据分析（TDA）特征与深度学习模型结合的方法，显著提升了遥感图像分类的性能。


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络（CNN）偏向于纹理特征，而TDA能有效描述复杂数据的几何信息，因此结合两者以提升分类效果。

Method: 设计了一个TDA特征工程流程，并将其与ResNet18模型结合，应用于EuroSAT和RESISC45数据集。

Result: 在EuroSAT数据集上，模型准确率达到99.33%，超过ResNet50和XL Vision Transformers；在RESISC45数据集上，准确率比基线高1.82%。

Conclusion: TDA特征可以与深度学习模型结合，即使在没有显式拓扑结构的数据集上也能提升性能，拓展了TDA的应用范围。

Abstract: Topological data analysis (TDA) is a relatively new field that is gaining
rapid adoption due to its robustness and ability to effectively describe
complex datasets by quantifying geometric information. In imaging contexts, TDA
typically models data as filtered cubical complexes from which we can extract
discriminative features using persistence homology. Meanwhile, convolutional
neural networks (CNNs) have been shown to be biased towards texture based local
features. To address this limitation, we propose a TDA feature engineering
pipeline and a simple method to integrate topological features with deep
learning models on remote sensing classification. Our method improves the
performance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving
99.33% accuracy, which surpasses all previously reported single-model
accuracies, including those with larger architectures, such as ResNet50 (2x
larger) and XL Vision Transformers (197x larger). We additionally show that our
method's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45
dataset. To our knowledge, this is the first application of TDA features in
satellite scene classification with deep learning. This demonstrates that TDA
features can be integrated with deep learning models, even on datasets without
explicit topological structures, thereby increasing the applicability of TDA. A
clean implementation of our method will be made publicly available upon
publication.

</details>


### [145] [Devanagari Handwritten Character Recognition using Convolutional Neural Network](https://arxiv.org/abs/2507.10398)
*Diksha Mehta,Prateek Mehta*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度卷积神经网络的手写梵文字符识别方法，实现了高准确率。


<details>
  <summary>Details</summary>
Motivation: 梵文是印度最古老的语言之一，缺乏数字化工具，研究旨在通过自动化方法提升手写梵文字符识别的效率和准确性。

Method: 采用两层深度卷积神经网络，使用公开数据集DHCD（包含36类字符，每类1700张图像）进行训练和测试。

Result: 测试准确率为96.36%，训练准确率为99.55%。

Conclusion: 该方法在手写梵文字符识别中表现出色，为相关应用提供了有效解决方案。

Abstract: Handwritten character recognition is getting popular among researchers
because of its possible applications in facilitating technological search
engines, social media, recommender systems, etc. The Devanagari script is one
of the oldest language scripts in India that does not have proper digitization
tools. With the advancement of computing and technology, the task of this
research is to extract handwritten Hindi characters from an image of Devanagari
script with an automated approach to save time and obsolete data. In this
paper, we present a technique to recognize handwritten Devanagari characters
using two deep convolutional neural network layers. This work employs a
methodology that is useful to enhance the recognition rate and configures a
convolutional neural network for effective Devanagari handwritten text
recognition (DHTR). This approach uses the Devanagari handwritten character
dataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each
of these classes has 1700 images for training and testing purposes. This
approach obtains promising results in terms of accuracy by achieving 96.36%
accuracy in testing and 99.55% in training time.

</details>


### [146] [Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources](https://arxiv.org/abs/2507.10403)
*Daniele Rege Cambrin,Lorenzo Vaiani,Giuseppe Gallipoli,Luca Cagliero,Paolo Garza*

Main category: cs.CV

TL;DR: 论文提出CrisisLandMark数据集和CLOSP框架，通过多模态对齐提升卫星图像检索性能，并引入地理坐标增强特定任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像检索系统多限于RGB数据，未能充分利用其他传感器（如SAR和多光谱）的独特信息。

Method: 构建包含64.7万张Sentinel-1 SAR和Sentinel-2多光谱图像的数据集，提出CLOSP框架，通过对比学习对齐光学与SAR图像到统一嵌入空间。

Result: CLOSP将检索性能提升54%，GeoCLOSP进一步优化了地理相关任务的检索效果。

Conclusion: 多传感器数据与地理背景的整合对遥感档案的充分利用至关重要。

Abstract: Retrieving relevant imagery from vast satellite archives is crucial for
applications like disaster response and long-term climate monitoring. However,
most text-to-image retrieval systems are limited to RGB data, failing to
exploit the unique physical information captured by other sensors, such as the
all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the
spectral signatures in optical multispectral data. To bridge this gap, we
introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1
SAR and Sentinel-2 multispectral images paired with structured textual
annotations for land cover, land use, and crisis events harmonized from
authoritative land cover systems (CORINE and Dynamic World) and crisis-specific
sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),
a novel framework that uses text as a bridge to align unpaired optical and SAR
images into a unified embedding space. Our experiments show that CLOSP achieves
a new state-of-the-art, improving retrieval nDGC by 54% over existing models.
Additionally, we find that the unified training strategy overcomes the inherent
difficulty of interpreting SAR imagery by transferring rich semantic knowledge
from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which
integrates geographic coordinates into our framework, creates a powerful
trade-off between generality and specificity: while the CLOSP excels at general
semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving
location-dependent crisis events and rare geographic features. This work
highlights that the integration of diverse sensor data and geographic context
is essential for unlocking the full potential of remote sensing archives.

</details>


### [147] [Numerically Computing Galois Groups of Minimal Problems](https://arxiv.org/abs/2507.10407)
*Timothy Duff*

Main category: cs.CV

TL;DR: 论文探讨了代数、数值计算和计算机视觉中参数化代数方程组的求解问题，特别是在计算机视觉中用于鲁棒模型拟合的RanSaC方法。


<details>
  <summary>Details</summary>
Motivation: 研究参数化代数方程组求解的内在困难，并为实际应用提供解决方案。

Method: 结合代数、数值计算和计算机视觉的方法，分析参数化系统的求解难度。

Result: 提出了衡量参数化系统求解难度的框架，并取得实际解决方案的进展。

Conclusion: 论文为参数化代数方程组的求解提供了理论支持和实践方向。

Abstract: I discuss a seemingly unlikely confluence of topics in algebra, numerical
computation, and computer vision. The motivating problem is that of solving
multiples instances of a parametric family of systems of algebraic (polynomial
or rational function) equations. No doubt already of interest to ISSAC
attendees, this problem arises in the context of robust model-fitting paradigms
currently utilized by the computer vision community (namely "Random Sampling
and Consensus", aka "RanSaC".) This talk will give an overview of work in the
last 5+ years that aspires to measure the intrinsic difficulty of solving such
parametric systems, and makes strides towards practical solutions.

</details>


### [148] [Text-Visual Semantic Constrained AI-Generated Image Quality Assessment](https://arxiv.org/abs/2507.10432)
*Qiang Li,Qingsen Yan,Haojian Huang,Peng Wu,Haokui Zhang,Yanning Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为SC-AGIQA的统一框架，通过文本-视觉语义约束改进AI生成图像的质量评估，解决了现有方法的语义不对齐和细节感知缺失问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如CLIP或BLIP）在评估AI生成图像时存在语义不对齐和细节感知不足的问题，需要更全面的评估方法。

Method: SC-AGIQA框架包含两个核心模块：TSAM（利用MLLM生成图像描述并与原始提示对比）和FFDPM（基于HVS的频率域分析）。

Result: 在多个基准数据集上的实验表明，SC-AGIQA优于现有方法。

Conclusion: SC-AGIQA通过文本-视觉语义约束显著提升了AI生成图像的质量评估效果，代码已开源。

Abstract: With the rapid advancements in Artificial Intelligence Generated Image (AGI)
technology, the accurate assessment of their quality has become an increasingly
vital requirement. Prevailing methods typically rely on cross-modal models like
CLIP or BLIP to evaluate text-image alignment and visual quality. However, when
applied to AGIs, these methods encounter two primary challenges: semantic
misalignment and details perception missing. To address these limitations, we
propose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment
(SC-AGIQA), a unified framework that leverages text-visual semantic constraints
to significantly enhance the comprehensive evaluation of both text-image
consistency and perceptual distortion in AI-generated images. Our approach
integrates key capabilities from multiple models and tackles the aforementioned
challenges by introducing two core modules: the Text-assisted Semantic
Alignment Module (TSAM), which leverages Multimodal Large Language Models
(MLLMs) to bridge the semantic gap by generating an image description and
comparing it against the original prompt for a refined consistency check, and
the Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which
draws inspiration from Human Visual System (HVS) properties by employing
frequency domain analysis combined with perceptual sensitivity weighting to
better quantify subtle visual distortions and enhance the capture of
fine-grained visual quality details in images. Extensive experiments conducted
on multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing
state-of-the-art methods. The code is publicly available at
https://github.com/mozhu1/SC-AGIQA.

</details>


### [149] [4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos](https://arxiv.org/abs/2507.10437)
*Shanshan Zhong,Jiawei Peng,Zehan Zheng,Zhongzhan Huang,Wufei Ma,Guofeng Zhang,Qihao Liu,Alan Yuille,Jieneng Chen*

Main category: cs.CV

TL;DR: 4D-Animal提出了一种无需稀疏关键点标注的动画3D动物重建框架，通过密集特征网络和分层对齐策略提升重建效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖稀疏语义关键点拟合参数模型，但关键点标注耗时且检测器不可靠。

Method: 引入密集特征网络将2D表示映射到SMAL参数，结合分层对齐策略整合多级视觉线索。

Result: 实验表明4D-Animal优于基于模型和无模型基线，生成的高质量3D资产可应用于其他任务。

Conclusion: 4D-Animal为大规模应用提供了高效稳定的动画3D动物重建方案。

Abstract: Existing methods for reconstructing animatable 3D animals from videos
typically rely on sparse semantic keypoints to fit parametric models. However,
obtaining such keypoints is labor-intensive, and keypoint detectors trained on
limited animal data are often unreliable. To address this, we propose
4D-Animal, a novel framework that reconstructs animatable 3D animals from
videos without requiring sparse keypoint annotations. Our approach introduces a
dense feature network that maps 2D representations to SMAL parameters,
enhancing both the efficiency and stability of the fitting process.
Furthermore, we develop a hierarchical alignment strategy that integrates
silhouette, part-level, pixel-level, and temporal cues from pre-trained 2D
visual models to produce accurate and temporally coherent reconstructions
across frames. Extensive experiments demonstrate that 4D-Animal outperforms
both model-based and model-free baselines. Moreover, the high-quality 3D assets
generated by our method can benefit other 3D tasks, underscoring its potential
for large-scale applications. The code is released at
https://github.com/zhongshsh/4D-Animal.

</details>


### [150] [CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding](https://arxiv.org/abs/2507.10449)
*Hongyong Han,Wei Wang,Gaowei Zhang,Mingjie Li,Yi Wang*

Main category: cs.CV

TL;DR: CoralVQA是首个大规模珊瑚礁视觉问答数据集，包含12,805张真实珊瑚图像和277,653个问答对，用于评估珊瑚生态和健康状况，为珊瑚保护提供支持。


<details>
  <summary>Details</summary>
Motivation: 珊瑚礁监测需要专业领域知识，现有视觉问答技术在珊瑚图像分析中缺乏专用数据集。

Method: 开发半自动数据构建流程，结合海洋生物学家专业知识，创建CoralVQA数据集。

Result: 评估多个先进大视觉语言模型，揭示其在珊瑚图像分析中的局限性和潜力。

Conclusion: CoralVQA为珊瑚保护提供了新工具，并为未来视觉语言模型的发展奠定了基础。

Abstract: Coral reefs are vital yet vulnerable ecosystems that require continuous
monitoring to support conservation. While coral reef images provide essential
information in coral monitoring, interpreting such images remains challenging
due to the need for domain expertise. Visual Question Answering (VQA), powered
by Large Vision-Language Models (LVLMs), has great potential in user-friendly
interaction with coral reef images. However, applying VQA to coral imagery
demands a dedicated dataset that addresses two key challenges: domain-specific
annotations and multidimensional questions. In this work, we introduce
CoralVQA, the first large-scale VQA dataset for coral reef analysis. It
contains 12,805 real-world coral images from 67 coral genera collected from 3
oceans, along with 277,653 question-answer pairs that comprehensively assess
ecological and health-related conditions. To construct this dataset, we develop
a semi-automatic data construction pipeline in collaboration with marine
biologists to ensure both scalability and professional-grade data quality.
CoralVQA presents novel challenges and provides a comprehensive benchmark for
studying vision-language reasoning in the context of coral reef images. By
evaluating several state-of-the-art LVLMs, we reveal key limitations and
opportunities. These insights form a foundation for future LVLM development,
with a particular emphasis on supporting coral conservation efforts.

</details>


### [151] [RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction](https://arxiv.org/abs/2507.10470)
*Zhicun Yin,Junjie Chen,Ming Liu,Zhixin Wang,Fan Li,Renjing Pei,Xiaoming Li,Rynson W. H. Lau,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 提出了一种名为RefSTAR的盲人脸图像修复方法，通过参考图像的选择、转移和重建，解决了身份保留问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在身份保留方面表现不佳，主要由于对细节纹理的特征引入不当。

Method: 构建了参考选择模块（RefSel），设计了特征融合范式，并提出了参考图像重建机制。

Result: 在各种骨干模型上表现出优越性能，身份保留能力和参考特征转移质量更好。

Conclusion: RefSTAR方法在盲人脸图像修复中有效解决了身份保留问题。

Abstract: Blind facial image restoration is highly challenging due to unknown complex
degradations and the sensitivity of humans to faces. Although existing methods
introduce auxiliary information from generative priors or high-quality
reference images, they still struggle with identity preservation problems,
mainly due to improper feature introduction on detailed textures. In this
paper, we focus on effectively incorporating appropriate features from
high-quality reference images, presenting a novel blind facial image
restoration method that considers reference selection, transfer, and
reconstruction (RefSTAR). In terms of selection, we construct a reference
selection (RefSel) module. For training the RefSel module, we construct a
RefSel-HQ dataset through a mask generation pipeline, which contains annotating
masks for 10,000 ground truth-reference pairs. As for the transfer, due to the
trivial solution in vanilla cross-attention operations, a feature fusion
paradigm is designed to force the features from the reference to be integrated.
Finally, we propose a reference image reconstruction mechanism that further
ensures the presence of reference image features in the output image. The cycle
consistency loss is also redesigned in conjunction with the mask. Extensive
experiments on various backbone models demonstrate superior performance,
showing better identity preservation ability and reference feature transfer
quality. Source code, dataset, and pre-trained models are available at
https://github.com/yinzhicun/RefSTAR.

</details>


### [152] [GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space](https://arxiv.org/abs/2507.10473)
*David G. Shatwell,Ishan Rajendrakumar Dave,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: GT-Loc是一种基于检索的新方法，联合预测图像的捕获时间（小时和月份）和地理位置（GPS坐标），通过共享高维特征空间对齐嵌入，优于现有时间预测方法。


<details>
  <summary>Details</summary>
Motivation: 解决时间戳预测与地理定位的相互依赖问题，支持元数据校正、检索和数字取证等应用。

Method: 使用独立的图像、时间和位置编码器，在共享特征空间中对齐嵌入，并提出基于循环环形表面的时间度量学习目标。

Result: GT-Loc在时间预测上优于现有方法，即使后者使用真实地理位置作为输入，同时在地理定位任务中表现优异。

Conclusion: GT-Loc通过联合优化时间与地理定位，提供了一种高效且统一的解决方案，支持组合和基于文本的图像检索。

Abstract: Timestamp prediction aims to determine when an image was captured using only
visual information, supporting applications such as metadata correction,
retrieval, and digital forensics. In outdoor scenarios, hourly estimates rely
on cues like brightness, hue, and shadow positioning, while seasonal changes
and weather inform date estimation. However, these visual cues significantly
depend on geographic context, closely linking timestamp prediction to
geo-localization. To address this interdependence, we introduce GT-Loc, a novel
retrieval-based method that jointly predicts the capture time (hour and month)
and geo-location (GPS coordinates) of an image. Our approach employs separate
encoders for images, time, and location, aligning their embeddings within a
shared high-dimensional feature space. Recognizing the cyclical nature of time,
instead of conventional contrastive learning with hard positives and negatives,
we propose a temporal metric-learning objective providing soft targets by
modeling pairwise time differences over a cyclical toroidal surface. We present
new benchmarks demonstrating that our joint optimization surpasses previous
time prediction methods, even those using the ground-truth geo-location as an
input during inference. Additionally, our approach achieves competitive results
on standard geo-localization tasks, and the unified embedding space facilitates
compositional and text-based image retrieval.

</details>


### [153] [Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation](https://arxiv.org/abs/2507.10474)
*Seyed Alireza Rahimi Azghadi,Truong-Thanh-Hung Nguyen,Helene Fournier,Monica Wachowicz,Rene Richard,Francis Palma,Hung Cao*

Main category: cs.CV

TL;DR: 提出了一种结合半监督联邦学习、室内定位导航和视觉识别的跌倒检测框架，具有高准确性和隐私保护特性。


<details>
  <summary>Details</summary>
Motivation: 老龄化人口增长导致跌倒风险增加，及时检测可减少医疗费用和恢复时间，同时需解决隐私问题。

Method: 结合SF2D（半监督联邦学习）、室内定位导航系统和视觉识别系统，通过可穿戴设备和边缘设备实现多系统协作检测。

Result: SF2D准确率99.19%，视觉识别准确率96.3%，导航系统成功率95%，综合准确率达99.99%。

Conclusion: 该框架高效可靠，兼顾隐私保护，适合老年人跌倒检测。

Abstract: The aging population is growing rapidly, and so is the danger of falls in
older adults. A major cause of injury is falling, and detection in time can
greatly save medical expenses and recovery time. However, to provide timely
intervention and avoid unnecessary alarms, detection systems must be effective
and reliable while addressing privacy concerns regarding the user. In this
work, we propose a framework for detecting falls using several complementary
systems: a semi-supervised federated learning-based fall detection system
(SF2D), an indoor localization and navigation system, and a vision-based human
fall recognition system. A wearable device and an edge device identify a fall
scenario in the first system. On top of that, the second system uses an indoor
localization technique first to localize the fall location and then navigate a
robot to inspect the scenario. A vision-based detection system running on an
edge device with a mounted camera on a robot is used to recognize fallen
people. Each of the systems of this proposed framework achieves different
accuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to
99.19% accuracy, while the vision-based fallen people detection achieves 96.3%
accuracy. However, when we combine the accuracy of these two systems with the
accuracy of the navigation system (95% success rate), our proposed framework
creates a highly reliable performance for fall detection, with an overall
accuracy of 99.99%. Not only is the proposed framework safe for older adults,
but it is also a privacy-preserving solution for detecting falls.

</details>


### [154] [The Power of Certainty: How Confident Models Lead to Better Segmentation](https://arxiv.org/abs/2507.10490)
*Tugberk Erol,Tuba Caglikantar,Duygu Sarikaya*

Main category: cs.CV

TL;DR: 提出了一种基于置信度的自蒸馏方法，用于优化结肠镜息肉分割任务，减少资源消耗并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型参数多、易过拟合且泛化能力差，知识蒸馏方法资源消耗大。

Method: 利用动态置信系数计算批次内前后迭代的损失，仅需存储前次迭代数据。

Result: 方法在息肉分割任务中表现优于现有技术，且在多中心数据上泛化良好。

Conclusion: 提出的方法高效且性能优越，代码将公开。

Abstract: Deep learning models have been proposed for automatic polyp detection and
precise segmentation of polyps during colonoscopy procedures. Although these
state-of-the-art models achieve high performance, they often require a large
number of parameters. Their complexity can make them prone to overfitting,
particularly when trained on biased datasets, and can result in poor
generalization across diverse datasets. Knowledge distillation and
self-distillation are proposed as promising strategies to mitigate the
limitations of large, over-parameterized models. These approaches, however, are
resource-intensive, often requiring multiple models and significant memory
during training. We propose a confidence-based self-distillation approach that
outperforms state-of-the-art models by utilizing only previous iteration data
storage during training, without requiring extra computation or memory usage
during testing. Our approach calculates the loss between the previous and
current iterations within a batch using a dynamic confidence coefficient. To
evaluate the effectiveness of our approach, we conduct comprehensive
experiments on the task of polyp segmentation. Our approach outperforms
state-of-the-art models and generalizes well across datasets collected from
multiple clinical centers. The code will be released to the public once the
paper is accepted.

</details>


### [155] [BenchReAD: A systematic benchmark for retinal anomaly detection](https://arxiv.org/abs/2507.10492)
*Chenyu Lian,Hong-Yu Zhou,Zhanli Hu,Jing Qin*

Main category: cs.CV

TL;DR: 该论文提出了一个全面的视网膜异常检测基准，解决了现有方法在数据、算法和评估上的局限性，并提出了NFM-DRA方法，结合解耦异常表示和正常特征记忆，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 视网膜异常检测在疾病筛查中至关重要，但缺乏公开全面的基准，导致现有方法在异常类型、测试集饱和度和泛化性评估上受限。

Method: 论文引入了一个系统性基准，并提出NFM-DRA方法，结合解耦异常表示（DRA）和正常特征记忆，以提升性能。

Result: NFM-DRA方法在性能上优于现有方法，但某些未见异常仍会导致性能下降。

Conclusion: 该基准和方法为视网膜异常检测提供了更全面的评估框架，推动了领域的发展。

Abstract: Retinal anomaly detection plays a pivotal role in screening ocular and
systemic diseases. Despite its significance, progress in the field has been
hindered by the absence of a comprehensive and publicly available benchmark,
which is essential for the fair evaluation and advancement of methodologies.
Due to this limitation, previous anomaly detection work related to retinal
images has been constrained by (1) a limited and overly simplistic set of
anomaly types, (2) test sets that are nearly saturated, and (3) a lack of
generalization evaluation, resulting in less convincing experimental setups.
Furthermore, existing benchmarks in medical anomaly detection predominantly
focus on one-class supervised approaches (training only with negative samples),
overlooking the vast amounts of labeled abnormal data and unlabeled data that
are commonly available in clinical practice. To bridge these gaps, we introduce
a benchmark for retinal anomaly detection, which is comprehensive and
systematic in terms of data and algorithm. Through categorizing and
benchmarking previous methods, we find that a fully supervised approach
leveraging disentangled representations of abnormalities (DRA) achieves the
best performance but suffers from significant drops in performance when
encountering certain unseen anomalies. Inspired by the memory bank mechanisms
in one-class supervised learning, we propose NFM-DRA, which integrates DRA with
a Normal Feature Memory to mitigate the performance degradation, establishing a
new SOTA. The benchmark is publicly available at
https://github.com/DopamineLcy/BenchReAD.

</details>


### [156] [Cameras as Relative Positional Encoding](https://arxiv.org/abs/2507.10496)
*Ruilong Li,Brent Yi,Junchen Liu,Hang Gao,Yi Ma,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: 论文比较了多视角Transformer中相机几何条件化的方法，提出了一种新的相对位置编码（PRoPE），并在多个任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多视角计算机视觉任务中，相机几何关系对3D感知至关重要，但现有方法未能充分利用这些关系。

Method: 比较了三种相机条件化方法：token级光线图编码、attention级相对位姿编码和提出的PRoPE（投影位置编码）。

Result: 实验表明，PRoPE在多种任务（如新视角合成、立体深度估计）中表现优于其他方法，且能泛化到不同相机参数。

Conclusion: PRoPE是一种有效的相机几何条件化方法，适用于多视角Transformer任务。

Abstract: Transformers are increasingly prevalent for multi-view computer vision tasks,
where geometric relationships between viewpoints are critical for 3D
perception. To leverage these relationships, multi-view transformers must use
camera geometry to ground visual tokens in 3D space. In this work, we compare
techniques for conditioning transformers on cameras: token-level raymap
encodings, attention-level relative pose encodings, and a new relative encoding
we propose -- Projective Positional Encoding (PRoPE) -- that captures complete
camera frustums, both intrinsics and extrinsics, as a relative positional
encoding. Our experiments begin by showing how relative camera conditioning
improves performance in feedforward novel view synthesis, with further gains
from PRoPE. This holds across settings: scenes with both shared and varying
intrinsics, when combining token- and attention-level conditioning, and for
generalization to inputs with out-of-distribution sequence lengths and camera
intrinsics. We then verify that these benefits persist for different tasks,
stereo depth estimation and discriminative spatial cognition, as well as larger
model sizes.

</details>


### [157] [National level satellite-based crop field inventories in smallholder landscapes](https://arxiv.org/abs/2507.10499)
*Philippe Rufin,Pauline Lucie Hammer,Leon-Friedrich Thomas,Sá Nogueira Lisboa,Natasha Ribeiro,Almeida Sitoe,Patrick Hostert,Patrick Meyfroidt*

Main category: cs.CV

TL;DR: 利用高分辨率地球观测数据和深度学习技术，首次绘制了莫桑比克全国2100万块农田的地图，准确率达93%，揭示了小农户农业系统的复杂性和农田规模的社会经济环境影响。


<details>
  <summary>Details</summary>
Motivation: 设计科学政策以提升小农户农业可持续性，但缺乏对农田分布和规模等基础系统属性的理解。

Method: 结合高分辨率（1.5米）地球观测数据和深度迁移学习，最小化参考数据需求并提升可迁移性。

Result: 绘制了莫桑比克全国2100万块农田地图，准确率93%，农田规模中位数为0.16公顷，83%小于0.5公顷。

Conclusion: 农田规模是农业社会经济和环境影响的关键指标，研究为政策制定提供了重要数据支持。

Abstract: The design of science-based policies to improve the sustainability of
smallholder agriculture is challenged by a limited understanding of fundamental
system properties, such as the spatial distribution of active cropland and
field size. We integrate very high spatial resolution (1.5 m) Earth observation
data and deep transfer learning to derive crop field delineations in complex
agricultural systems at the national scale, while maintaining minimum reference
data requirements and enhancing transferability. We provide the first
national-level dataset of 21 million individual fields for Mozambique (covering
~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural
land use with an overall accuracy of 93% and balanced omission and commission
errors. Field-level spatial agreement reached median intersection over union
(IoU) scores of 0.81, advancing the state-of-the-art in large-area field
delineation in complex smallholder systems. The active cropland maps capture
fragmented rural regions with low cropland shares not yet identified in global
land cover or cropland maps. These regions are mostly located in agricultural
frontier regions which host 7-9% of the Mozambican population. Field size in
Mozambique is very low overall, with half of the fields being smaller than 0.16
ha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial
resolution (0.05{\deg}) is 0.32 ha, but it varies strongly across gradients of
accessibility, population density, and net forest cover change. This variation
reflects a diverse set of actors, ranging from semi-subsistence smallholder
farms to medium-scale commercial farming, and large-scale farming operations.
Our results highlight that field size is a key indicator relating to
socio-economic and environmental outcomes of agriculture (e.g., food
production, livelihoods, deforestation, biodiversity), as well as their
trade-offs.

</details>


### [158] [Quantize-then-Rectify: Efficient VQ-VAE Training](https://arxiv.org/abs/2507.10547)
*Borui Zhang,Qihang Rao,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: ReVQ框架通过预训练的VAE快速训练VQ-VAE，显著降低计算成本，同时保持高质量图像重建。


<details>
  <summary>Details</summary>
Motivation: 高压缩率VQ-VAE训练计算成本高，需探索更高效的方法。

Method: 利用预训练VAE，结合通道多组量化和后矫正器，减少量化误差。

Result: ReVQ在单GPU上22小时完成训练，图像重建质量高（rFID=1.06）。

Conclusion: ReVQ在效率和重建质量间取得优越平衡，大幅降低训练成本。

Abstract: Visual tokenizers are pivotal in multimodal large models, acting as bridges
between continuous inputs and discrete tokens. Nevertheless, training
high-compression-rate VQ-VAEs remains computationally demanding, often
necessitating thousands of GPU hours. This work demonstrates that a pre-trained
VAE can be efficiently transformed into a VQ-VAE by controlling quantization
noise within the VAE's tolerance threshold. We present
\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs
to enable rapid VQ-VAE training with minimal computational overhead. By
integrating \textbf{channel multi-group quantization} to enlarge codebook
capacity and a \textbf{post rectifier} to mitigate quantization errors, ReVQ
compresses ImageNet images into at most 512 tokens while sustaining competitive
reconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training
costs by over two orders of magnitude relative to state-of-the-art approaches:
ReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,
whereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental
results show that ReVQ achieves superior efficiency-reconstruction trade-offs.

</details>


### [159] [EmbRACE-3K: Embodied Reasoning and Action in Complex Environments](https://arxiv.org/abs/2507.10548)
*Mingxian Lin,Wei Huang,Yitang Li,Chengjie Jiang,Kui Wu,Fangwei Zhong,Shengju Qian,Xin Wang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 论文介绍了EmRACE-3K数据集，用于评估和提升视觉语言模型在具身环境中的推理能力，现有模型表现不佳，但通过微调可显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在被动任务中表现优秀，但在需要在线交互和主动场景理解的具身环境中表现有限，需要新的数据集和评估方法。

Method: 提出EmRACE-3K数据集，包含3000多个语言指导任务，涵盖导航、物体操作和多阶段目标执行等挑战，并使用监督学习和强化学习微调模型。

Result: 在零样本设置下，所有模型成功率低于20%，但微调后的Qwen2.5-VL-7B模型在所有挑战类别中均有显著提升。

Conclusion: EmRACE-3K数据集有效推动了具身推理能力的发展，现有模型在该领域仍有提升空间。

Abstract: Recent advanced vision-language models(VLMs) have demonstrated strong
performance on passive, offline image and video understanding tasks. However,
their effectiveness in embodied settings, which require online interaction and
active scene understanding remains limited. In such scenarios, an agent
perceives the environment from a first-person perspective, with each action
dynamically shaping subsequent observations. Even state-of-the-art models such
as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment
interactions, exhibiting clear limitations in spatial reasoning and
long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset
of over 3,000 language-guided tasks situated in diverse, photorealistic
environments constructed using Unreal Engine and the UnrealCV-Zoo framework.
The tasks encompass a wide range of embodied challenges, including navigation,
object manipulation, and multi-stage goal execution. Each task unfolds as a
multi-step trajectory, pairing first-person visual observations with high-level
instructions, grounded actions, and natural language rationales that express
the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to
evaluate the embodied reasoning capabilities of VLMs across three key
dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage
Goal Execution. In zero-shot settings, all models achieve success rates below
20%, underscoring the challenge posed by our benchmark and the current
limitations of VLMs in interactive environments. To demonstrate the utility of
EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning
followed by reinforcement learning. This approach yields substantial
improvements across all three challenge categories, highlighting the dataset's
effectiveness in enabling the development of embodied reasoning capabilities.

</details>


### [160] [Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder](https://arxiv.org/abs/2507.10552)
*Vladimir Iashin,Horace Lee,Dan Schofield,Andrew Zisserman*

Main category: cs.CV

TL;DR: 提出一种完全自监督的方法，利用DINOv2框架从无标签的相机陷阱视频中学习黑猩猩面部嵌入，无需身份标签即可实现高性能的开放集重识别。


<details>
  <summary>Details</summary>
Motivation: 解决野生动物监测中手动识别个体的瓶颈问题，探索自监督学习在生物多样性监测中的潜力。

Method: 使用DINOv2框架训练Vision Transformers，基于自动提取的面部图像块进行训练，无需身份标签。

Result: 在Bossou等挑战性基准测试中，开放集重识别性能优于监督基线方法。

Conclusion: 自监督学习在生物多样性监测中具有巨大潜力，为可扩展、非侵入性种群研究提供了新途径。

Abstract: Camera traps are revolutionising wildlife monitoring by capturing vast
amounts of visual data; however, the manual identification of individual
animals remains a significant bottleneck. This study introduces a fully
self-supervised approach to learning robust chimpanzee face embeddings from
unlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision
Transformers on automatically mined face crops, eliminating the need for
identity labels. Our method demonstrates strong open-set re-identification
performance, surpassing supervised baselines on challenging benchmarks such as
Bossou, despite utilising no labelled data during training. This work
underscores the potential of self-supervised learning in biodiversity
monitoring and paves the way for scalable, non-invasive population studies.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [161] [Multi-omic Prognosis of Alzheimer's Disease with Asymmetric Cross-Modal Cross-Attention Network](https://arxiv.org/abs/2507.08855)
*Yang Ming,Jiang Shi Zhong,Zhou Su Juan*

Main category: eess.IV

TL;DR: 本文提出了一种新型深度学习算法框架，通过融合多模态医学数据，利用非对称跨模态交叉注意力机制，显著提升了阿尔茨海默病（AD）诊断的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络和简单特征拼接方法在多模态数据融合中效果不佳，无法充分利用互补信息，导致关键信息丢失。

Method: 采用非对称跨模态交叉注意力机制，融合脑部PET、MRI、基因数据和临床数据等多模态信息。

Result: 算法模型在测试集上达到94.88%的准确率。

Conclusion: 非对称跨模态交叉注意力机制在多模态数据融合中表现优异，为AD诊断提供了新思路。

Abstract: Alzheimer's Disease (AD) is an irreversible neurodegenerative disease
characterized by progressive cognitive decline as its main symptom. In the
research field of deep learning-assisted diagnosis of AD, traditional
convolutional neural networks and simple feature concatenation methods fail to
effectively utilize the complementary information between multimodal data, and
the simple feature concatenation approach is prone to cause the loss of key
information during the process of modal fusion. In recent years, the
development of deep learning technology has brought new possibilities for
solving the problem of how to effectively fuse multimodal features. This paper
proposes a novel deep learning algorithm framework to assist medical
professionals in AD diagnosis. By fusing medical multi-view information such as
brain fluorodeoxyglucose positron emission tomography (PET), magnetic resonance
imaging (MRI), genetic data, and clinical data, it can accurately detect the
presence of AD, Mild Cognitive Impairment (MCI), and Cognitively Normal (CN).
The innovation of the algorithm lies in the use of an asymmetric cross-modal
cross-attention mechanism, which can effectively capture the key information
features of the interactions between different data modal features. This paper
compares the asymmetric cross-modal cross-attention mechanism with the
traditional algorithm frameworks of unimodal and multimodal deep learning
models for AD diagnosis, and evaluates the importance of the asymmetric
cross-modal cross-attention mechanism. The algorithm model achieves an accuracy
of 94.88% on the test set.

</details>


### [162] [Interpretable Artificial Intelligence for Detecting Acute Heart Failure on Acute Chest CT Scans](https://arxiv.org/abs/2507.08952)
*Silas Nyboe Ørting,Kristina Miger,Anne Sophie Overgaard Olesen,Mikael Ploug Boesen,Michael Brun Andersen,Jens Petersen,Olav W. Nielsen,Marleen de Bruijne*

Main category: eess.IV

TL;DR: 开发了一种可解释的AI模型，用于在胸部CT中检测急性心力衰竭（AHF）的放射学标志，性能与胸科放射科医生相当。


<details>
  <summary>Details</summary>
Motivation: 胸部CT在呼吸困难患者中的应用增加，但AHF的诊断仍具挑战性且报告延迟，AI可作为辅助工具提高诊断精度。

Method: 采用Boosted Trees模型，基于急性胸部CT扫描中分割的心肺结构测量值预测AHF，使用Shapley Additive explanations解释预测。

Result: 模型在独立测试集上ROC曲线下面积为0.87，部分误分类源于初始放射学报告的不准确性。

Conclusion: 开发的AI模型性能强且透明，可支持决策，与放射科医生相当。

Abstract: Introduction: Chest CT scans are increasingly used in dyspneic patients where
acute heart failure (AHF) is a key differential diagnosis. Interpretation
remains challenging and radiology reports are frequently delayed due to a
radiologist shortage, although flagging such information for emergency
physicians would have therapeutic implication. Artificial intelligence (AI) can
be a complementary tool to enhance the diagnostic precision. We aim to develop
an explainable AI model to detect radiological signs of AHF in chest CT with an
accuracy comparable to thoracic radiologists.
  Methods: A single-center, retrospective study during 2016-2021 at Copenhagen
University Hospital - Bispebjerg and Frederiksberg, Denmark. A Boosted Trees
model was trained to predict AHF based on measurements of segmented cardiac and
pulmonary structures from acute thoracic CT scans. Diagnostic labels for
training and testing were extracted from radiology reports. Structures were
segmented with TotalSegmentator. Shapley Additive explanations values were used
to explain the impact of each measurement on the final prediction.
  Results: Of the 4,672 subjects, 49% were female. The final model incorporated
twelve key features of AHF and achieved an area under the ROC of 0.87 on the
independent test set. Expert radiologist review of model misclassifications
found that 24 out of 64 (38%) false positives and 24 out of 61 (39%) false
negatives were actually correct model predictions, with the errors originating
from inaccuracies in the initial radiology reports.
  Conclusion: We developed an explainable AI model with strong discriminatory
performance, comparable to thoracic radiologists. The AI model's stepwise,
transparent predictions may support decision-making.

</details>


### [163] [VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models](https://arxiv.org/abs/2507.08982)
*Hanene F. Z. Brachemi Meftah,Wassim Hamidouche,Sid Ahmed Fezza,Olivier Déforges*

Main category: eess.IV

TL;DR: 该论文提出了一种针对视觉语言模型（VLMs）的新型隐私保护方法，通过选择性隐藏图像中的敏感区域（ROIs），防止模型访问私人信息，同时保持图像其余部分的语义完整性。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型的广泛应用，用户隐私问题日益突出，尤其是模型可能无意中处理或暴露私人视觉信息。

Method: 将隐私保护问题建模为对抗攻击问题，提出一种选择性隐藏ROIs的攻击策略，避免破坏图像全局语义。

Result: 实验表明，该方法在三种先进VLMs（LLaVA、Instruct-BLIP和BLIP2-T5）上实现了高达98%的目标ROIs检测率降低，同时保持图像语义完整性。

Conclusion: 该研究为多模态模型的隐私保护提供了实用工具，并推动了隐私意识的应用发展。

Abstract: Recent years have witnessed remarkable progress in developing Vision-Language
Models (VLMs) capable of processing both textual and visual inputs. These
models have demonstrated impressive performance, leading to their widespread
adoption in various applications. However, this widespread raises serious
concerns regarding user privacy, particularly when models inadvertently process
or expose private visual information. In this work, we frame the preservation
of privacy in VLMs as an adversarial attack problem. We propose a novel attack
strategy that selectively conceals information within designated Region Of
Interests (ROIs) in an image, effectively preventing VLMs from accessing
sensitive content while preserving the semantic integrity of the remaining
image. Unlike conventional adversarial attacks that often disrupt the entire
image, our method maintains high coherence in unmasked areas. Experimental
results across three state-of-the-art VLMs namely LLaVA, Instruct-BLIP, and
BLIP2-T5 demonstrate up to 98% reduction in detecting targeted ROIs, while
maintaining global image semantics intact, as confirmed by high similarity
scores between clean and adversarial outputs. We believe that this work
contributes to a more privacy conscious use of multimodal models and offers a
practical tool for further research, with the source code publicly available
at: https://github.com/hbrachemi/Vlm_defense-attack.

</details>


### [164] [Automatic Contouring of Spinal Vertebrae on X-Ray using a Novel Sandwich U-Net Architecture](https://arxiv.org/abs/2507.09158)
*Sunil Munthumoduku Krishna Murthy,Kumar Rajamani,Srividya Tirunellai Rajamani,Yupei Li,Qiyang Sun,Bjoern W. Schuller*

Main category: eess.IV

TL;DR: 提出了一种改进的U-Net模型，用于从X射线图像中准确分割胸椎，提高了分割精度和效率。


<details>
  <summary>Details</summary>
Motivation: 脊柱活动性疾病需要精确的椎骨分割和轮廓提取，传统手动方法耗时且易出错，自动化方法更高效。

Method: 采用一种新型的“三明治”U-Net结构，结合双激活函数，优化了椎骨分割性能。

Result: 与基线U-Net模型相比，Dice分数提高了4.1%，分割精度显著提升。

Conclusion: 改进的U-Net模型在椎骨分割任务中表现出更高的准确性和可靠性，适用于临床实践。

Abstract: In spinal vertebral mobility disease, accurately extracting and contouring
vertebrae is essential for assessing mobility impairments and monitoring
variations during flexion-extension movements. Precise vertebral contouring
plays a crucial role in surgical planning; however, this process is
traditionally performed manually by radiologists or surgeons, making it
labour-intensive, time-consuming, and prone to human error. In particular,
mobility disease analysis requires the individual contouring of each vertebra,
which is both tedious and susceptible to inconsistencies. Automated methods
provide a more efficient alternative, enabling vertebra identification,
segmentation, and contouring with greater accuracy and reduced time
consumption. In this study, we propose a novel U-Net variation designed to
accurately segment thoracic vertebrae from anteroposterior view on X-Ray
images. Our proposed approach, incorporating a ``sandwich" U-Net structure with
dual activation functions, achieves a 4.1\% improvement in Dice score compared
to the baseline U-Net model, enhancing segmentation accuracy while ensuring
reliable vertebral contour extraction.

</details>


### [165] [PanoDiff-SR: Synthesizing Dental Panoramic Radiographs using Diffusion and Super-resolution](https://arxiv.org/abs/2507.09227)
*Sanyam Jain,Bruna Neves de Freitas,Andreas Basse-OConnor,Alexandros Iosifidis,Ruben Pauwels*

Main category: eess.IV

TL;DR: 本文提出了一种结合扩散生成（PanoDiff）和超分辨率（SR）的方法，用于生成高质量合成牙科全景X光片（PRs）。实验结果表明，合成图像在视觉和统计指标上接近真实图像。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像数据稀缺问题，为人工智能研究和教育提供合成数据集。

Method: 结合扩散生成模型生成低分辨率种子图像，再通过超分辨率变换器提升分辨率，学习局部-全局关系以增强细节。

Result: 合成高分辨率图像的Frechet inception distance为40.69，临床专家区分真实与合成图像的平均准确率为68.5%。

Conclusion: 该方法能生成高质量的合成医学图像，为数据稀缺问题提供了有效解决方案。

Abstract: There has been increasing interest in the generation of high-quality,
realistic synthetic medical images in recent years. Such synthetic datasets can
mitigate the scarcity of public datasets for artificial intelligence research,
and can also be used for educational purposes. In this paper, we propose a
combination of diffusion-based generation (PanoDiff) and Super-Resolution (SR)
for generating synthetic dental panoramic radiographs (PRs). The former
generates a low-resolution (LR) seed of a PR (256 X 128) which is then
processed by the SR model to yield a high-resolution (HR) PR of size 1024 X
512. For SR, we propose a state-of-the-art transformer that learns local-global
relationships, resulting in sharper edges and textures. Experimental results
demonstrate a Frechet inception distance score of 40.69 between 7243 real and
synthetic images (in HR). Inception scores were 2.55, 2.30, 2.90 and 2.98 for
real HR, synthetic HR, real LR and synthetic LR images, respectively. Among a
diverse group of six clinical experts, all evaluating a mixture of 100
synthetic and 100 real PRs in a time-limited observation, the average accuracy
in distinguishing real from synthetic images was 68.5% (with 50% corresponding
to random guessing).

</details>


### [166] [Encryption and Authentication with a Lensless Camera Based on a Programmable Mask](https://arxiv.org/abs/2507.09236)
*Eric Bezzam,Martin Vetterli*

Main category: eess.IV

TL;DR: 论文提出了一种使用可编程掩模增强无透镜相机安全性的方法，通过动态变化的掩模模式抵御攻击，同时实现高质量图像恢复。


<details>
  <summary>Details</summary>
Motivation: 传统无透镜相机的静态掩模易受攻击，需提升安全性。

Method: 采用基于液晶显示器的低成本系统（约100美元），动态变化掩模模式。

Result: 实验表明，可变掩模能有效抵御多种攻击，加密强度超过AES-256，密钥长度超2500位。

Conclusion: 可编程掩模不仅增强安全性，还可用于图像认证和防伪，为对抗深度伪造提供新方案。

Abstract: Lensless cameras replace traditional optics with thin masks, leading to
highly multiplexed measurements akin to encryption. However, static masks in
conventional designs leave systems vulnerable to simple attacks. This work
explores the use of programmable masks to enhance security by dynamically
varying the mask patterns. We perform our experiments with a low-cost system
(around 100 USD) based on a liquid crystal display. Experimental results
demonstrate that variable masks successfully block a variety of attacks while
enabling high-quality recovery for legitimate users. The system's encryption
strength exceeds AES-256, achieving effective key lengths over 2'500 bits.
Additionally, we demonstrate how a programmable mask enables robust
authentication and verification, as each mask pattern leaves a unique
fingerprint on the image. When combined with a lensed system, lensless
measurements can serve as analog certificates, providing a novel solution for
verifying image authenticity and combating deepfakes.

</details>


### [167] [Deep Image Prior Assisted ISAR Imaging for Missing Data Case](https://arxiv.org/abs/2507.09393)
*Necmettin Bayar,Isin Erer,Deniz Kumlu*

Main category: eess.IV

TL;DR: 该论文提出了一种基于深度图像先验（DIP）的方法，用于完成ISAR中缺失的雷达回波数据，并通过传统傅里叶成像提高成像质量。


<details>
  <summary>Details</summary>
Motivation: 在ISAR中，雷达回波矩阵的随机缺失会降低成像质量，影响目标与背景的区分。现有的压缩感知或矩阵补全方法在高缺失率或稀疏性约束下效果不佳。

Method: 使用DIP分别补全雷达数据的实部和虚部，然后通过傅里叶成像生成雷达图像。

Result: 与IALM、2D-SL0和NNM方法相比，DIP方法在极端情况下RMSE提高了100%，相关性提高了50%，IC指标提高了30%。

Conclusion: DIP方法在ISAR数据补全和成像中表现出显著优势，尤其是在高缺失率情况下。

Abstract: In Inverse Synthetic Aperture Radar (ISAR), random missing entries of the
received radar echo matrix deteriorate the imaging quality, compromising target
distinction from the background. Compressive sensing techniques or matrix
completion prior to conventional imaging have been used in recent years to
solve this issue. However, while the former techniques fail to preserve target
continuity due to the sparsity constraint, the latter fails for high missing
ratios. This paper proposes to use deep image prior (DIP) to complete the
complex radar data and then obtain the radar image by conventional Fourier
imaging. Real and imaginary parts are separately completed by independent deep
structures and then put together for the imaging part. The proposed DIP based
imaging method has been compared with IALM, 2D-SL0 and NNM methods visually and
quantitatively for both simulated and real data. The results demonstrate an
increase of 100% for some extreme cases in terms of RMSE, 50% increase on
Correlation and 30% increase on IC metrics quantitatively.

</details>


### [168] [prNet: Data-Driven Phase Retrieval via Stochastic Refinement](https://arxiv.org/abs/2507.09608)
*Mehmet Onurcan Kaya,Figen S. Oktem*

Main category: eess.IV

TL;DR: 提出了一种基于Langevin动力学的新型相位检索框架，通过后验采样平衡失真与感知质量。


<details>
  <summary>Details</summary>
Motivation: 传统方法过于关注像素级精度，而忽略了感知质量与失真之间的权衡。

Method: 结合随机采样、学习去噪和模型更新，设计了三种复杂度递增的变体，包括Langevin推断、自适应噪声调度学习和并行采样。

Result: 在多个基准测试中实现了最先进的性能，兼顾保真度和感知质量。

Conclusion: 该框架为相位检索提供了一种高效且平衡的方法，优于传统方法。

Abstract: We propose a novel framework for phase retrieval that leverages Langevin
dynamics to enable efficient posterior sampling, yielding reconstructions that
explicitly balance distortion and perceptual quality. Unlike conventional
approaches that prioritize pixel-wise accuracy, our method navigates the
perception-distortion tradeoff through a principled combination of stochastic
sampling, learned denoising, and model-based updates. The framework comprises
three variants of increasing complexity, integrating theoretically grounded
Langevin inference, adaptive noise schedule learning, parallel reconstruction
sampling, and warm-start initialization from classical solvers. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
across multiple benchmarks, both in terms of fidelity and perceptual quality.

</details>


### [169] [I2I-PR: Deep Iterative Refinement for Phase Retrieval using Image-to-Image Diffusion Models](https://arxiv.org/abs/2507.09609)
*Mehmet Onurcan Kaya,Figen S. Oktem*

Main category: eess.IV

TL;DR: 提出了一种基于图像到图像扩散框架的新相位检索方法，结合混合迭代技术和加速机制，显著提高了训练效率和重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统相位检索算法对初始化和测量噪声敏感，扩散模型在图像重建任务中表现出色，因此探索其在相位检索中的应用。

Method: 采用混合输入-输出和误差减少方法的混合迭代技术，结合加速机制生成初始估计，再通过图像到图像扩散框架迭代优化。

Result: 方法在训练效率和重建质量上均有显著提升，优于传统和现代技术。

Conclusion: 该方法在相位检索中具有高效性和有效性，适用于多种应用场景。

Abstract: Phase retrieval involves recovering a signal from intensity-only
measurements, crucial in many fields such as imaging, holography, optical
computing, crystallography, and microscopy. Although there are several
well-known phase retrieval algorithms, including classical iterative solvers,
the reconstruction performance often remains sensitive to initialization and
measurement noise. Recently, image-to-image diffusion models have gained
traction in various image reconstruction tasks, yielding significant
theoretical insights and practical breakthroughs. In this work, we introduce a
novel phase retrieval approach based on an image-to-image diffusion framework
called Inversion by Direct Iteration. Our method begins with an enhanced
initialization stage that leverages a hybrid iterative technique, combining the
Hybrid Input-Output and Error Reduction methods and incorporating a novel
acceleration mechanism to obtain a robust crude estimate. Then, it iteratively
refines this initial crude estimate using the learned image-to-image pipeline.
Our method achieves substantial improvements in both training efficiency and
reconstruction quality. Furthermore, our approach utilizes aggregation
techniques to refine quality metrics and demonstrates superior results compared
to both classical and contemporary techniques. This highlights its potential
for effective and efficient phase retrieval across various applications.

</details>


### [170] [Pre-trained Under Noise: A Framework for Robust Bone Fracture Detection in Medical Imaging](https://arxiv.org/abs/2507.09731)
*Robby Hoover,Nelly Elsayed,Zag ElSayed,Chengcheng Li*

Main category: eess.IV

TL;DR: 论文研究了预训练深度学习模型在X射线图像中分类骨折的鲁棒性，通过模拟不同设备质量条件测试了ResNet50、VGG16和EfficientNetv2的性能，并分析了噪声对模型表现的影响。


<details>
  <summary>Details</summary>
Motivation: 通过技术手段解决全球医疗资源不均问题，研究预训练模型在不同质量X射线图像中的表现。

Method: 使用ResNet50、VGG16和EfficientNetv2三种预训练模型，通过逐步添加噪声模拟图像质量下降，评估模型性能变化。

Result: 研究发现噪声显著影响骨折检测的准确性，不同模型对噪声的鲁棒性表现各异。

Conclusion: 论文提出了评估AI模型性能退化的方法框架，为实际医疗环境中AI模型的适用性提供了实用见解。

Abstract: Medical Imagings are considered one of the crucial diagnostic tools for
different bones-related diseases, especially bones fractures. This paper
investigates the robustness of pre-trained deep learning models for classifying
bone fractures in X-ray images and seeks to address global healthcare disparity
through the lens of technology. Three deep learning models have been tested
under varying simulated equipment quality conditions. ResNet50, VGG16 and
EfficientNetv2 are the three pre-trained architectures which are compared.
These models were used to perform bone fracture classification as images were
progressively degraded using noise. This paper specifically empirically studies
how the noise can affect the bone fractures detection and how the pre-trained
models performance can be changes due to the noise that affect the quality of
the X-ray images. This paper aims to help replicate real world challenges
experienced by medical imaging technicians across the world. Thus, this paper
establishes a methodological framework for assessing AI model degradation using
transfer learning and controlled noise augmentation. The findings provide
practical insight into how robust and generalizable different pre-trained deep
learning powered computer vision models can be when used in different contexts.

</details>


### [171] [AI-Enhanced Pediatric Pneumonia Detection: A CNN-Based Approach Using Data Augmentation and Generative Adversarial Networks (GANs)](https://arxiv.org/abs/2507.09759)
*Abdul Manaf,Nimra Mughal*

Main category: eess.IV

TL;DR: 该研究提出了一种基于机器学习的儿科胸部肺炎分类系统，通过CNN模型和GAN生成的数据增强技术，提高了肺炎诊断的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 肺炎是五岁以下儿童死亡的主要原因，需要准确的胸部X光诊断。本研究旨在通过机器学习辅助医疗专业人员诊断肺炎。

Method: 使用5,863张标记的儿童胸部X光图像训练CNN模型，采用数据增强技术和GAN生成合成图像以解决数据不足和类别不平衡问题。

Result: 系统通过结合原始、增强和GAN生成的数据，在准确率和F1分数上表现优异，并通过Flask应用实现实时分类。

Conclusion: 深度学习和GAN技术可显著提升儿科肺炎分类的诊断准确性和效率，尤其适用于资源有限的临床环境。

Abstract: Pneumonia is a leading cause of mortality in children under five, requiring
accurate chest X-ray diagnosis. This study presents a machine learning-based
Pediatric Chest Pneumonia Classification System to assist healthcare
professionals in diagnosing pneumonia from chest X-ray images. The CNN-based
model was trained on 5,863 labeled chest X-ray images from children aged 0-5
years from the Guangzhou Women and Children's Medical Center. To address
limited data, we applied augmentation techniques (rotation, zooming, shear,
horizontal flipping) and employed GANs to generate synthetic images, addressing
class imbalance. The system achieved optimal performance using combined
original, augmented, and GAN-generated data, evaluated through accuracy and F1
score metrics. The final model was deployed via a Flask web application,
enabling real-time classification with probability estimates. Results
demonstrate the potential of deep learning and GANs in improving diagnostic
accuracy and efficiency for pediatric pneumonia classification, particularly
valuable in resource-limited clinical settings
https://github.com/AbdulManaf12/Pediatric-Chest-Pneumonia-Classification

</details>


### [172] [Resolution Revolution: A Physics-Guided Deep Learning Framework for Spatiotemporal Temperature Reconstruction](https://arxiv.org/abs/2507.09872)
*Shengjie Liu,Lu Zhang,Siqin Wang*

Main category: eess.IV

TL;DR: 提出了一种物理引导的深度学习框架，用于整合不同分辨率的温度数据，实现高时空分辨率的重建。


<details>
  <summary>Details</summary>
Motivation: 解决地球观测中温度数据的时空分辨率矛盾，满足实际应用对高分辨率数据的需求。

Method: 结合卷积神经网络和物理模型，利用卫星数据和地球系统模型，通过线性项放大粗分辨率数据。

Result: 在四个数据集上验证了框架的有效性，成功重建了高分辨率温度数据。

Conclusion: 该框架为全球范围内的高分辨率温度数据生成提供了新可能。

Abstract: Central to Earth observation is the trade-off between spatial and temporal
resolution. For temperature, this is especially critical because real-world
applications require high spatiotemporal resolution data. Current technology
allows for hourly temperature observations at 2 km, but only every 16 days at
100 m, a gap further exacerbated by cloud cover. Earth system models offer
continuous hourly temperature data, but at a much coarser spatial resolution
(9-31 km). Here, we present a physics-guided deep learning framework for
temperature data reconstruction that integrates these two data sources. The
proposed framework uses a convolutional neural network that incorporates the
annual temperature cycle and includes a linear term to amplify the coarse Earth
system model output into fine-scale temperature values observed from
satellites. We evaluated this framework using data from two satellites, GOES-16
(2 km, hourly) and Landsat (100 m, every 16 days), and demonstrated effective
temperature reconstruction with hold-out and in situ data across four datasets.
This physics-guided deep learning framework opens new possibilities for
generating high-resolution temperature data across spatial and temporal scales,
under all weather conditions and globally.

</details>


### [173] [Advanced U-Net Architectures with CNN Backbones for Automated Lung Cancer Detection and Segmentation in Chest CT Images](https://arxiv.org/abs/2507.09898)
*Alireza Golkarieha,Kiana Kiashemshakib,Sajjad Rezvani Boroujenic,Nasibeh Asadi Isakand*

Main category: eess.IV

TL;DR: 研究探讨了结合不同CNN骨干网络的U-Net架构在胸部CT图像中自动检测和分割肺癌的效果，结果显示U-Net与ResNet50结合在癌症分割中表现最佳，而U-Net与Xception结合的分类模型准确率高达99.1%。


<details>
  <summary>Details</summary>
Motivation: 解决临床环境中对准确诊断工具的需求，提升肺癌早期检测的效率和准确性。

Method: 使用CLAHE预处理832张胸部CT图像，构建U-Net模型（结合ResNet50、VGG16和Xception），并评估CNN分类器及混合模型的性能。

Result: U-Net与ResNet50在癌症分割中表现最佳（Dice: 0.9495），U-Net与Xception的分类模型准确率达99.1%。混合模型CNN-SVM-Xception表现优异。

Conclusion: U-Net结合先进CNN骨干网络为肺癌CT扫描的分割和分类提供了高效方法，支持早期诊断和临床决策。

Abstract: This study investigates the effectiveness of U-Net architectures integrated
with various convolutional neural network (CNN) backbones for automated lung
cancer detection and segmentation in chest CT images, addressing the critical
need for accurate diagnostic tools in clinical settings. A balanced dataset of
832 chest CT images (416 cancerous and 416 non-cancerous) was preprocessed
using Contrast Limited Adaptive Histogram Equalization (CLAHE) and resized to
128x128 pixels. U-Net models were developed with three CNN backbones: ResNet50,
VGG16, and Xception, to segment lung regions. After segmentation, CNN-based
classifiers and hybrid models combining CNN feature extraction with traditional
machine learning classifiers (Support Vector Machine, Random Forest, and
Gradient Boosting) were evaluated using 5-fold cross-validation. Metrics
included accuracy, precision, recall, F1-score, Dice coefficient, and ROC-AUC.
U-Net with ResNet50 achieved the best performance for cancerous lungs (Dice:
0.9495, Accuracy: 0.9735), while U-Net with VGG16 performed best for
non-cancerous segmentation (Dice: 0.9532, Accuracy: 0.9513). For
classification, the CNN model using U-Net with Xception achieved 99.1 percent
accuracy, 99.74 percent recall, and 99.42 percent F1-score. The hybrid
CNN-SVM-Xception model achieved 96.7 percent accuracy and 97.88 percent
F1-score. Compared to prior methods, our framework consistently outperformed
existing models. In conclusion, combining U-Net with advanced CNN backbones
provides a powerful method for both segmentation and classification of lung
cancer in CT scans, supporting early diagnosis and clinical decision-making.

</details>


### [174] [IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution](https://arxiv.org/abs/2507.09923)
*Sejin Park,Sangmin Lee,Kyong Hwan Jin,Seung-Won Jung*

Main category: eess.IV

TL;DR: 提出了一种名为IM-LUT的新框架，用于任意尺度图像超分辨率（ASISR），通过混合插值函数提高效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于查找表（LUT）的方法仅适用于固定尺度，而隐式神经表示方法计算成本高。

Method: 提出IM-Net网络预测插值函数的混合权重，并转化为IM-LUT以提升效率。

Result: 在多个基准数据集上，IM-LUT在图像质量和效率之间取得了优越的平衡。

Conclusion: IM-LUT是一种适用于资源受限应用的高效解决方案。

Abstract: Super-resolution (SR) has been a pivotal task in image processing, aimed at
enhancing image resolution across various applications. Recently, look-up table
(LUT)-based approaches have attracted interest due to their efficiency and
performance. However, these methods are typically designed for fixed scale
factors, making them unsuitable for arbitrary-scale image SR (ASISR). Existing
ASISR techniques often employ implicit neural representations, which come with
considerable computational cost and memory demands. To address these
limitations, we propose Interpolation Mixing LUT (IM-LUT), a novel framework
that operates ASISR by learning to blend multiple interpolation functions to
maximize their representational capacity. Specifically, we introduce IM-Net, a
network trained to predict mixing weights for interpolation functions based on
local image patterns and the target scale factor. To enhance efficiency of
interpolation-based methods, IM-Net is transformed into IM-LUT, where LUTs are
employed to replace computationally expensive operations, enabling lightweight
and fast inference on CPUs while preserving reconstruction quality.
Experimental results on several benchmark datasets demonstrate that IM-LUT
consistently achieves a superior balance between image quality and efficiency
compared to existing methods, highlighting its potential as a promising
solution for resource-constrained applications.

</details>


### [175] [A Brain Tumor Segmentation Method Based on CLIP and 3D U-Net with Cross-Modal Semantic Guidance and Multi-Level Feature Fusion](https://arxiv.org/abs/2507.09966)
*Mingda Zhang*

Main category: eess.IV

TL;DR: 提出了一种融合像素级、特征级和语义级信息的多层次架构，用于脑肿瘤MRI分割，显著提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤形态异质性和复杂的三维空间关系使自动分割具有挑战性，现有方法未充分利用医学报告中的语义知识。

Method: 结合CLIP模型的语义理解能力和3D U-Net的空间特征提取优势，通过3D-2D语义桥接、跨模态语义引导和基于语义的注意力机制实现多级融合。

Result: 在BraTS 2020数据集上，Dice系数达到0.8567，比传统3D U-Net提升4.8%，在增强肿瘤区域提升7.3%。

Conclusion: 多级融合架构有效整合了低层数据和高层语义，显著提升了脑肿瘤分割性能。

Abstract: Precise segmentation of brain tumors from magnetic resonance imaging (MRI) is
essential for neuro-oncology diagnosis and treatment planning. Despite advances
in deep learning methods, automatic segmentation remains challenging due to
tumor morphological heterogeneity and complex three-dimensional spatial
relationships. Current techniques primarily rely on visual features extracted
from MRI sequences while underutilizing semantic knowledge embedded in medical
reports. This research presents a multi-level fusion architecture that
integrates pixel-level, feature-level, and semantic-level information,
facilitating comprehensive processing from low-level data to high-level
concepts. The semantic-level fusion pathway combines the semantic understanding
capabilities of Contrastive Language-Image Pre-training (CLIP) models with the
spatial feature extraction advantages of 3D U-Net through three mechanisms:
3D-2D semantic bridging, cross-modal semantic guidance, and semantic-based
attention mechanisms. Experimental validation on the BraTS 2020 dataset
demonstrates that the proposed model achieves an overall Dice coefficient of
0.8567, representing a 4.8% improvement compared to traditional 3D U-Net, with
a 7.3% Dice coefficient increase in the clinically important enhancing tumor
(ET) region.

</details>


### [176] [Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor Segmentation (GMLN-BTS) in Edge Iterative MRI Lesion Localization System (EdgeIMLocSys)](https://arxiv.org/abs/2507.09995)
*Guohao Huo,Ruiting Dai,Hao Tang*

Main category: eess.IV

TL;DR: 提出了一种名为EdgeIMLocSys的系统，结合持续学习和人类反馈，通过GMLN-BTS网络实现高效脑肿瘤分割，显著提升了模型对MRI扫描仪差异的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤分割在临床诊断和治疗规划中至关重要，但不同MRI扫描仪的成像质量差异对模型泛化能力提出了挑战。

Method: 提出GMLN-BTS网络，包括M2AE编码器提取多尺度特征，G2MCIM模块建模跨模态关系，以及VRUM模块优化分割边界。

Result: 在BraTS2017数据集上达到85.1%的Dice分数，参数仅458万，比主流3D Transformer模型减少98%。

Conclusion: GMLN-BTS在资源受限的临床环境中实现了高精度、高效率的脑肿瘤分割。

Abstract: Brain tumor segmentation plays a critical role in clinical diagnosis and
treatment planning, yet the variability in imaging quality across different MRI
scanners presents significant challenges to model generalization. To address
this, we propose the Edge Iterative MRI Lesion Localization System
(EdgeIMLocSys), which integrates Continuous Learning from Human Feedback to
adaptively fine-tune segmentation models based on clinician feedback, thereby
enhancing robustness to scanner-specific imaging characteristics. Central to
this system is the Graph-based Multi-Modal Interaction Lightweight Network for
Brain Tumor Segmentation (GMLN-BTS), which employs a Modality-Aware Adaptive
Encoder (M2AE) to extract multi-scale semantic features efficiently, and a
Graph-based Multi-Modal Collaborative Interaction Module (G2MCIM) to model
complementary cross-modal relationships via graph structures. Additionally, we
introduce a novel Voxel Refinement UpSampling Module (VRUM) that
synergistically combines linear interpolation and multi-scale transposed
convolutions to suppress artifacts while preserving high-frequency details,
improving segmentation boundary accuracy. Our proposed GMLN-BTS model achieves
a Dice score of 85.1% on the BraTS2017 dataset with only 4.58 million
parameters, representing a 98% reduction compared to mainstream 3D Transformer
models, and significantly outperforms existing lightweight approaches. This
work demonstrates a synergistic breakthrough in achieving high-accuracy,
resource-efficient brain tumor segmentation suitable for deployment in
resource-constrained clinical environments.

</details>


### [177] [DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in Histopathology](https://arxiv.org/abs/2507.10250)
*Ashkan Shakarami,Lorenzo Nicole,Rocco Cappellesso,Angelo Paolo Dei Tos,Stefano Ghidoni*

Main category: eess.IV

TL;DR: DepViT-CAD是一种基于MAViT（多注意力视觉Transformer）的可部署AI系统，用于多类癌症诊断，在真实世界验证中表现出高敏感性。


<details>
  <summary>Details</summary>
Motivation: 通过AI辅助提高癌症诊断的准确性和及时性，以支持临床决策。

Method: 使用MAViT（多注意力视觉Transformer）从1008张全切片图像中提取细粒度形态学特征，覆盖11种诊断类别。

Result: 在两个独立队列中验证，诊断敏感性分别为94.11%和92%。

Conclusion: DepViT-CAD为AI辅助癌症诊断提供了稳健且可扩展的解决方案，代码将公开以支持透明性。

Abstract: Accurate and timely cancer diagnosis from histopathological slides is vital
for effective clinical decision-making. This paper introduces DepViT-CAD, a
deployable AI system for multi-class cancer diagnosis in histopathology. At its
core is MAViT, a novel Multi-Attention Vision Transformer designed to capture
fine-grained morphological patterns across diverse tumor types. MAViT was
trained on expert-annotated patches from 1008 whole-slide images, covering 11
diagnostic categories, including 10 major cancers and non-tumor tissue.
DepViT-CAD was validated on two independent cohorts: 275 WSIs from The Cancer
Genome Atlas and 50 routine clinical cases from pathology labs, achieving
diagnostic sensitivities of 94.11% and 92%, respectively. By combining
state-of-the-art transformer architecture with large-scale real-world
validation, DepViT-CAD offers a robust and scalable approach for AI-assisted
cancer diagnostics. To support transparency and reproducibility, software and
code will be made publicly available at GitHub.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [178] [Interactive Drawing Guidance for Anime Illustrations with Diffusion Model](https://arxiv.org/abs/2507.09140)
*Chuang Chen,Xiaoxuan Xie,Yongming Zhang,Tianyu Zhang,Haoran Xie*

Main category: cs.GR

TL;DR: 论文提出了一种交互式动漫绘图指导系统，通过实时指导和优化技术帮助用户提升绘图效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 动漫绘图对初学者具有挑战性，因其风格复杂且细节精细。

Method: 系统基于StreamDiffusion管道，结合LoRA微调的Stable Diffusion生成动漫风格图像，并通过Informative Drawings模型和自定义优化器生成结构化指导草图。

Result: 用户研究表明，系统显著提升了绘图效率和准确性。

Conclusion: 该系统为动漫绘图提供了有效的实时指导工具。

Abstract: Creating high-quality anime illustrations presents notable challenges,
particularly for beginners, due to the intricate styles and fine details
inherent in anime art. We present an interactive drawing guidance system
specifically designed for anime illustrations to address this issue. It offers
real-time guidance to help users refine their work and streamline the creative
process. Our system is built upon the StreamDiffusion pipeline to deliver
real-time drawing assistance. We fine-tune Stable Diffusion with LoRA to
synthesize anime style RGB images from user-provided hand-drawn sketches and
prompts. Leveraging the Informative Drawings model, we transform these RGB
images into rough sketches, which are further refined into structured guidance
sketches using a custom-designed optimizer. The proposed system offers precise,
real-time guidance aligned with the creative intent of the user, significantly
enhancing both the efficiency and accuracy of the drawing process. To assess
the effectiveness of our approach, we conducted a user study, gathering
empirical feedback on both system performance and interface usability.

</details>


### [179] [Physics-Aware Fluid Field Generation from User Sketches Using Helmholtz-Hodge Decomposition](https://arxiv.org/abs/2507.09146)
*Ryuichi Miyauchi,Hengyuan Chang,Tsukasa Fukusato,Kazunori Miyata,Haoran Xie*

Main category: cs.GR

TL;DR: 提出一种结合生成模型和物理编辑的方法，用于交互式设计2D矢量场，解决现有方法难以保持物理特性的问题。


<details>
  <summary>Details</summary>
Motivation: 流体模拟技术应用广泛，但控制复杂流体行为仍具挑战性。现有生成模型虽能直观生成矢量场，但难以保持物理特性（如不可压缩性）。

Method: 分两阶段：1）使用潜在扩散模型（LDM）从用户草图生成初始矢量场；2）通过Helmholtz-Hodge分解局部提取物理特性并重组。

Result: 实验验证了方法的有效性，能够根据用户意图生成符合物理特性的矢量场。

Conclusion: 该方法结合生成与编辑，为交互式流体设计提供了实用解决方案。

Abstract: Fluid simulation techniques are widely used in various fields such as film
production, but controlling complex fluid behaviors remains challenging. While
recent generative models enable intuitive generation of vector fields from user
sketches, they struggle to maintain physical properties such as
incompressibility. To address these issues, this paper proposes a method for
interactively designing 2D vector fields. Conventional generative models can
intuitively generate vector fields from user sketches, but remain difficult to
consider physical properties. Therefore, we add a simple editing process after
generating the vector field. In the first stage, we use a latent diffusion
model~(LDM) to automatically generate initial 2D vector fields from user
sketches. In the second stage, we apply the Helmholtz-Hodge decomposition to
locally extract physical properties such as incompressibility from the results
generated by LDM and recompose them according to user intentions. Through
multiple experiments, we demonstrate the effectiveness of our proposed method.

</details>


### [180] [RectifiedHR: High-Resolution Diffusion via Energy Profiling and Adaptive Guidance Scheduling](https://arxiv.org/abs/2507.09441)
*Ankit Sanjyal*

Main category: cs.GR

TL;DR: 提出自适应分类器自由引导（CFG）调度策略，解决扩散模型在高分辨率图像合成中的能量不稳定性和引导伪影问题。


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像合成中扩散模型存在能量不稳定和引导伪影问题，影响视觉质量。

Method: 分析采样过程中的潜在能量景观，提出能量感知的自适应CFG调度策略，动态调整引导强度。

Result: 采用线性递减CFG调度的DPM++ 2M模型表现最优，稳定性得分（0.9998）和一致性指标（0.9873）优于固定引导方法。

Conclusion: 能量分析框架为理解和改进扩散模型行为提供了有效工具，自适应CFG调度显著提升了图像质量和稳定性。

Abstract: High-resolution image synthesis with diffusion models often suffers from
energy instabilities and guidance artifacts that degrade visual quality. We
analyze the latent energy landscape during sampling and propose adaptive
classifier-free guidance (CFG) schedules that maintain stable energy
trajectories. Our approach introduces energy-aware scheduling strategies that
modulate guidance strength over time, achieving superior stability scores
(0.9998) and consistency metrics (0.9873) compared to fixed-guidance
approaches. We demonstrate that DPM++ 2M with linear-decreasing CFG scheduling
yields optimal performance, providing sharper, more faithful images while
reducing artifacts. Our energy profiling framework serves as a powerful
diagnostic tool for understanding and improving diffusion model behavior.

</details>


### [181] [Real-time and Controllable Reactive Motion Synthesis via Intention Guidance](https://arxiv.org/abs/2507.09704)
*Xiaotang Zhang,Ziyi Chang,Qianhui Men,Hubert Shum*

Main category: cs.GR

TL;DR: 提出了一种基于输入角色轨迹的实时反应性运动合成方法，通过意图预测器和对抗训练实现稳定、个性化的交互。


<details>
  <summary>Details</summary>
Motivation: 解决现有离线方法无法实时生成长期交互运动的问题，同时提高运动合成的稳定性和泛化能力。

Method: 引入意图预测器预测关键关节意图，将其编码到潜在空间并与代码库匹配，通过对抗训练增强模型鲁棒性。

Result: 实验表明，该方法在稳定性和泛化性上优于其他基于匹配的运动合成方法，支持用户主动控制运动方向。

Conclusion: 该方法实现了实时、长期的交互运动合成，用户可通过控制方向个性化交互路径。

Abstract: We propose a real-time method for reactive motion synthesis based on the
known trajectory of input character, predicting instant reactions using only
historical, user-controlled motions. Our method handles the uncertainty of
future movements by introducing an intention predictor, which forecasts key
joint intentions to make pose prediction more deterministic from the historical
interaction. The intention is later encoded into the latent space of its
reactive motion, matched with a codebook which represents mappings between
input and output. It samples a categorical distribution for pose generation and
strengthens model robustness through adversarial training. Unlike previous
offline approaches, the system can recursively generate intentions and reactive
motions using feedback from earlier steps, enabling real-time, long-term
realistic interactive synthesis. Both quantitative and qualitative experiments
show our approach outperforms other matching-based motion synthesis approaches,
delivering superior stability and generalizability. In our method, user can
also actively influence the outcome by controlling the moving directions,
creating a personalized interaction path that deviates from predefined
trajectories.

</details>


### [182] [CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD Design](https://arxiv.org/abs/2507.09792)
*Prashant Govindarajan,Davide Baldelli,Jay Pathak,Quentin Fournier,Sarath Chandar*

Main category: cs.GR

TL;DR: 论文提出了一种利用大型语言模型（LLMs）自动化CAD设计的方法，通过微调代码-LLMs生成基于自然语言描述的CAD序列，并引入新的几何和拓扑指标评估生成质量。


<details>
  <summary>Details</summary>
Motivation: CAD建模目前仍以耗时的手动操作为主，现有方法未能充分利用大型语言模型的潜力。

Method: 构建了一个包含17万多个CAD模型的数据集，使用GPT-4.1生成高质量描述，并微调代码-LLMs以从自然语言生成JSON格式的CAD序列。

Result: 实验表明，该方法能有效自动化CAD设计，显著加速新对象的设计过程。

Conclusion: 通过结合LLMs和新指标，CAD设计自动化成为可能，为工程和制造领域提供了高效工具。

Abstract: Computer-aided design (CAD) is the digital construction of 2D and 3D objects,
and is central to a wide range of engineering and manufacturing applications
like automobile and aviation. Despite its importance, CAD modeling remains
largely a time-intensive, manual task. Recent works have attempted to automate
this process with small transformer-based models and handcrafted CAD sequence
representations. However, there has been little effort to leverage the
potential of large language models (LLMs) for sequential CAD design. In this
work, we introduce a new large-scale dataset of more than 170k CAD models
annotated with high-quality, human-like descriptions generated with our
pipeline based on GPT-4.1. Using this dataset, we fine-tune powerful code-LLMs
to generate CAD sequences represented in a JSON-based format from natural
language descriptions, demonstrating the viability and effectiveness of this
approach for text-conditioned CAD generation. Because simple metrics often fail
to reflect the quality of generated objects, we introduce geometric and
topological metrics based on sphericity, mean curvature, and Euler
characteristic to provide richer structural insights. Our experiments and
ablation studies on both synthetic and human-annotated data demonstrate that
CADmium is able to automate CAD design, drastically speeding up the design of
new objects. The dataset, code, and fine-tuned models are available online.

</details>


### [183] [ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions](https://arxiv.org/abs/2507.10542)
*Shivangi Aneja,Sebastian Weiss,Irene Baeza,Prashanth Chandran,Gaspard Zoss,Matthias Nießner,Derek Bradley*

Main category: cs.GR

TL;DR: 提出了一种基于局部表情特征和3D高斯泼溅的高保真3D头部头像生成方法，实现了实时、高分辨率的动画序列。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在全局表情空间中无法捕捉细微面部特征和动态的问题，提升头像的真实感和表现力。

Method: 结合局部表情特征与3D高斯泼溅，利用基于块的几何3D人脸模型提取表情，并通过Scaffold-GS的锚点动态合成3D高斯。

Result: ScaffoldAvatar在实时生成中实现了最先进的性能，支持多样化的面部表情和风格。

Conclusion: 局部表情特征与3D高斯泼溅的结合为高保真3D头像生成提供了有效解决方案。

Abstract: Generating high-fidelity real-time animated sequences of photorealistic 3D
head avatars is important for many graphics applications, including immersive
telepresence and movies. This is a challenging problem particularly when
rendering digital avatar close-ups for showing character's facial microfeatures
and expressions. To capture the expressive, detailed nature of human heads,
including skin furrowing and finer-scale facial movements, we propose to couple
locally-defined facial expressions with 3D Gaussian splatting to enable
creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In
contrast to previous works that operate on a global expression space, we
condition our avatar's dynamics on patch-based local expression features and
synthesize 3D Gaussians at a patch level. In particular, we leverage a
patch-based geometric 3D face model to extract patch expressions and learn how
to translate these into local dynamic skin appearance and motion by coupling
the patches with anchor points of Scaffold-GS, a recent hierarchical scene
representation. These anchors are then used to synthesize 3D Gaussians
on-the-fly, conditioned by patch-expressions and viewing direction. We employ
color-based densification and progressive training to obtain high-quality
results and faster convergence for high resolution 3K training images. By
leveraging patch-level expressions, ScaffoldAvatar consistently achieves
state-of-the-art performance with visually natural motion, while encompassing
diverse facial expressions and styles in real time.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [184] [Resonant Seismic Motion Within a Sedimentary Basin Revisited](https://arxiv.org/abs/2507.09364)
*Armand Wirgin*

Main category: physics.geo-ph

TL;DR: 研究探讨了SH极化的平面体地震波在半圆形软质圆柱盆地中的传播问题，分析了位移响应的积分和部分波表示形式，揭示了能量守恒关系和共振现象。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解地震波在非均匀地质结构中的传播特性，特别是能量损失和共振现象。

Method: 采用空间-频率框架，通过积分和部分波表示形式分析位移响应，计算共振和非共振频率下的位移场。

Result: 发现位移场在共振频率下显著放大并集中在盆地内，能量损失分为辐射阻尼和体积材料阻尼。

Conclusion: 研究结果适用于一般形状的盆地，为地震波传播和能量损失提供了理论支持。

Abstract: The problem, of 2D canonical nature, examined herein in the space-frequency
framework, concerns a SH-polarized plane body seismic wave propagating in a
hard, non lossy half space (bedrock) containing a soft, lossy cylindrical basin
of semi-circular shape. The displacement response, at points both within and
outside the basin is represented alternatively in integral (for basins of
general shape) and partial wave (for basins of semi-circular shape) forms, the
integral representation leading to an expression of a sort of conservation of
energy relation, and an explanation of how the energetic gain provided by the
incident wave, is divided essentially into two sources of loss: radiation
damping (taking place in the bedrock) and volumic material damping (taking
place within the basin), whereas the partial wave representation enables a
detailed theoretical demonstration of the resonant nature of the response,
marked by amplification and concentration of the displacement field essentially
within the basin at certain (eigen-)frequencies and very little, or no
amplification and concentration, at other frequencies. These phenomena carry
over to the volumic material damping and radiation damping loss functions (of
frequency), with the equivalent of resonant displacement response residing in
peaks of the volumic material damping loss function and troughs of the
radiation damping loss function. Maps, for the semi-circular basin, are
computed herein of the displacement field both at resonant and non-resonant
frequencies which strongly evoke those (of numerical nature resulting from the
boundary element scheme) appearing in two publications by other authors for
basins of more realistic shape, so as to suggest that the results in the
present study are applicable, for a large part, to basins of general shape.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [185] [Non-linear, Team-based VR Training for Cardiac Arrest Care with enhanced CRM Toolkit](https://arxiv.org/abs/2507.08805)
*Mike Kentros,Manos Kamarianakis,Michael Cole,Vitaliy Popov,Antonis Protopsaltis,George Papagiannakis*

Main category: cs.HC

TL;DR: iREACT是一种新型VR模拟系统，用于改进传统心脏骤停培训的局限性，通过动态、协作环境提升团队资源管理技能。


<details>
  <summary>Details</summary>
Motivation: 传统心脏骤停培训方法难以模拟真实事件的动态性，限制了团队资源管理（CRM）技能的发展。

Method: iREACT提供了一个非线性、协作的VR环境，团队需应对变化的患者状态，同时捕获多模态数据（用户行为、认知负荷、视觉注视）并提供实时和事后反馈。

Result: 初步评估显示iREACT具有较高的可用性和教育价值，能够提升CRM评估效果。

Conclusion: iREACT不仅适用于心脏骤停培训，还可推广到其他高风险场景，以改善团队协作、沟通和决策能力。

Abstract: This paper introduces iREACT, a novel VR simulation addressing key
limitations in traditional cardiac arrest (CA) training. Conventional methods
struggle to replicate the dynamic nature of real CA events, hindering Crew
Resource Management (CRM) skill development. iREACT provides a non-linear,
collaborative environment where teams respond to changing patient states,
mirroring real CA complexities. By capturing multi-modal data (user actions,
cognitive load, visual gaze) and offering real-time and post-session feedback,
iREACT enhances CRM assessment beyond traditional methods. A formative
evaluation with medical experts underscores its usability and educational
value, with potential applications in other high-stakes training scenarios to
improve teamwork, communication, and decision-making.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [186] [Multimodal HD Mapping for Intersections by Intelligent Roadside Units](https://arxiv.org/abs/2507.08903)
*Zhongzhang Chen,Miao Fan,Shengtong Xu,Mengmeng Yang,Kun Jiang,Xiangzeng Liu,Haoyi Xiong*

Main category: cs.RO

TL;DR: 本文提出了一种基于摄像头-LiDAR融合的高清语义地图构建框架，并发布了RS-seq数据集，用于研究交叉路口的多模态互补性。


<details>
  <summary>Details</summary>
Motivation: 传统车辆方法在复杂交叉路口的高清语义地图构建中因遮挡和视角限制面临挑战，需利用路边智能单元（IRU）提升性能。

Method: 采用两阶段融合框架，结合摄像头高分辨率纹理和LiDAR精确几何数据，通过RS-seq数据集进行验证。

Result: 多模态方法在语义分割上的mIoU比单模态方法分别提高4%（图像）和18%（点云）。

Conclusion: 研究为基于IRU的高清语义地图构建提供了基准方法和数据集，推动基础设施辅助自动驾驶系统的发展。

Abstract: High-definition (HD) semantic mapping of complex intersections poses
significant challenges for traditional vehicle-based approaches due to
occlusions and limited perspectives. This paper introduces a novel camera-LiDAR
fusion framework that leverages elevated intelligent roadside units (IRUs).
Additionally, we present RS-seq, a comprehensive dataset developed through the
systematic enhancement and annotation of the V2X-Seq dataset. RS-seq includes
precisely labelled camera imagery and LiDAR point clouds collected from
roadside installations, along with vectorized maps for seven intersections
annotated with detailed features such as lane dividers, pedestrian crossings,
and stop lines. This dataset facilitates the systematic investigation of
cross-modal complementarity for HD map generation using IRU data. The proposed
fusion framework employs a two-stage process that integrates modality-specific
feature extraction and cross-modal semantic integration, capitalizing on camera
high-resolution texture and precise geometric data from LiDAR. Quantitative
evaluations using the RS-seq dataset demonstrate that our multimodal approach
consistently surpasses unimodal methods. Specifically, compared to unimodal
baselines evaluated on the RS-seq dataset, the multimodal approach improves the
mean Intersection-over-Union (mIoU) for semantic segmentation by 4\% over the
image-only results and 18\% over the point cloud-only results. This study
establishes a baseline methodology for IRU-based HD semantic mapping and
provides a valuable dataset for future research in infrastructure-assisted
autonomous driving systems.

</details>


### [187] [Visual Homing in Outdoor Robots Using Mushroom Body Circuits and Learning Walks](https://arxiv.org/abs/2507.09725)
*Gabriel G. Gattaux,Julien R. Serres,Franck Ruffier,Antoine Wystrach*

Main category: cs.RO

TL;DR: 本文提出了一种基于蚂蚁视觉归巢行为的生物启发方法，首次在真实世界中实现了一种侧向化蘑菇体（MB）架构，用于紧凑型自动驾驶车辆的视觉归巢。


<details>
  <summary>Details</summary>
Motivation: 蚂蚁仅需极少感官输入和少量学习行走即可实现稳健的视觉归巢，这启发了自主导航的生物模拟解决方案。蘑菇体模型虽用于机器人路径跟随，但尚未应用于视觉归巢。

Method: 通过四个渐进实验验证方法：1) 模拟展示巢穴吸引子动态；2) 真实世界中的解耦学习行走后的归巢行为；3) 使用模拟GPS-RTK噪声路径积分的随机行走归巢；4) 通过第五个MB输出神经元实现精准目标停止行为。

Result: 系统在自然户外环境中实现了稳健的视觉归巢，仅依赖视觉输入，运行频率为8 Hz，内存占用低于9 kB。

Conclusion: 该方法为自主视觉归巢提供了一种生物学基础且资源高效的解决方案，功能上类似于机器人中的基于路点的位置控制。

Abstract: Ants achieve robust visual homing with minimal sensory input and only a few
learning walks, inspiring biomimetic solutions for autonomous navigation. While
Mushroom Body (MB) models have been used in robotic route following, they have
not yet been applied to visual homing. We present the first real-world
implementation of a lateralized MB architecture for visual homing onboard a
compact autonomous car-like robot. We test whether the sign of the angular path
integration (PI) signal can categorize panoramic views, acquired during
learning walks and encoded in the MB, into "goal on the left" and "goal on the
right" memory banks, enabling robust homing in natural outdoor settings. We
validate this approach through four incremental experiments: (1) simulation
showing attractor-like nest dynamics; (2) real-world homing after decoupled
learning walks, producing nest search behavior; (3) homing after random walks
using noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal
behavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to
control velocity. This mimics the accurate homing behavior of ants and
functionally resembles waypoint-based position control in robotics, despite
relying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with
32x32 pixel views and a memory footprint under 9 kB, our system offers a
biologically grounded, resource-efficient solution for autonomous visual
homing.

</details>


### [188] [Probabilistic Human Intent Prediction for Mobile Manipulation: An Evaluation with Human-Inspired Constraints](https://arxiv.org/abs/2507.10131)
*Cesar Alan Contreras,Manolis Chiou,Alireza Rastegarpanah,Michal Szulik,Rustam Stolkin*

Main category: cs.RO

TL;DR: GUIDER是一个概率框架，用于机器人估计人类操作者的意图，通过双阶段（导航和操作）实现意图推断，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 提高人机协作的效率，避免因意图推断不准确导致的人类控制受限或冲突。

Method: GUIDER采用双阶段框架：导航阶段结合控制器速度和占用网格生成协同地图；操作阶段结合显著性检测和几何抓取可行性测试。

Result: 在25次试验中，GUIDER在导航和操作阶段的稳定性显著优于基线方法，分别提升39.5%和31.4%。

Conclusion: GUIDER的双阶段框架在移动操作任务中有效提升了意图推断的准确性和稳定性。

Abstract: Accurate inference of human intent enables human-robot collaboration without
constraining human control or causing conflicts between humans and robots. We
present GUIDER (Global User Intent Dual-phase Estimation for Robots), a
probabilistic framework that enables a robot to estimate the intent of human
operators. GUIDER maintains two coupled belief layers, one tracking navigation
goals and the other manipulation goals. In the Navigation phase, a Synergy Map
blends controller velocity with an occupancy grid to rank interaction areas.
Upon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.
The Manipulation phase combines U2Net saliency, FastSAM instance saliency, and
three geometric grasp-feasibility tests, with an end-effector kinematics-aware
update rule that evolves object probabilities in real-time. GUIDER can
recognize areas and objects of intent without predefined goals. We evaluated
GUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and
compared it with two baselines, one for navigation and one for manipulation.
Across the 25 trials, GUIDER achieved a median stability of 93-100% during
navigation, compared with 60-100% for the BOIR baseline, with an improvement of
39.5% in a redirection scenario (T5). During manipulation, stability reached
94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a
redirection task (T3). In geometry-constrained trials (manipulation), GUIDER
recognized the object intent three times earlier than Trajectron (median
remaining time to confident prediction 23.6 s vs 7.8 s). These results validate
our dual-phase framework and show improvements in intent inference in both
phases of mobile manipulation tasks.

</details>


### [189] [Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance](https://arxiv.org/abs/2507.10500)
*Kyungtae Han,Yitao Chen,Rohit Gupta,Onur Altintas*

Main category: cs.RO

TL;DR: SC-ADAS是一个结合生成式AI的模块化框架，通过多轮对话和场景感知提供实时、可解释的自适应驾驶辅助。


<details>
  <summary>Details</summary>
Motivation: 现有ADAS系统缺乏场景理解和自然语言交互能力，无法灵活适应动态环境或驾驶员意图。

Method: 集成大语言模型、视觉到文本解释和结构化函数调用，支持基于视觉和传感器上下文的多轮对话。

Result: 在CARLA模拟器中实现，无需模型微调即可执行用户意图，但存在延迟和令牌增长等权衡。

Conclusion: SC-ADAS展示了结合对话推理、场景感知和模块化ADAS控制的可行性，为下一代智能驾驶辅助提供了方向。

Abstract: While autonomous driving technologies continue to advance, current Advanced
Driver Assistance Systems (ADAS) remain limited in their ability to interpret
scene context or engage with drivers through natural language. These systems
typically rely on predefined logic and lack support for dialogue-based
interaction, making them inflexible in dynamic environments or when adapting to
driver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a
modular framework that integrates Generative AI components including large
language models, vision-to-text interpretation, and structured function calling
to enable real-time, interpretable, and adaptive driver assistance. SC-ADAS
supports multi-turn dialogue grounded in visual and sensor context, allowing
natural language recommendations and driver-confirmed ADAS control. Implemented
in the CARLA simulator with cloud-based Generative AI, the system executes
confirmed user intents as structured ADAS commands without requiring model
fine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and
revisited multi-turn interactions, highlighting trade-offs such as increased
latency from vision-based context retrieval and token growth from accumulated
dialogue history. These results demonstrate the feasibility of combining
conversational reasoning, scene perception, and modular ADAS control to support
the next generation of intelligent driver assistance.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [190] [Generative Audio Language Modeling with Continuous-valued Tokens and Masked Next-Token Prediction](https://arxiv.org/abs/2507.09834)
*Shu-wen Yang,Byeonggeun Kim,Kuan-Po Huang,Qingming Tang,Huy Phan,Bo-Ru Lu,Harsha Sundar,Shalini Ghosh,Hung-yi Lee,Chieh-Chi Kao,Chao Wang*

Main category: eess.AS

TL;DR: 论文提出了一种基于因果语言模型的音频生成方法，利用连续值令牌的扩散模型，显著优于离散方法，并在参数更少的情况下达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何将自回归Transformer解码器的成功应用于音频生成，解决音频连续性的挑战。

Method: 采用令牌级扩散模型建模连续值令牌分布，并引入掩码下一令牌预测任务。

Result: 在AudioCaps上，FAD和KL散度分别提升20%和40%，且参数更少。

Conclusion: 该方法在音频生成中表现出色，参数效率高，性能接近SOTA扩散模型。

Abstract: Autoregressive next-token prediction with the Transformer decoder has become
a de facto standard in large language models (LLMs), achieving remarkable
success in Natural Language Processing (NLP) at scale. Extending this paradigm
to audio poses unique challenges due to its inherently continuous nature. We
research audio generation with a causal language model (LM) without discrete
tokens. We leverage token-wise diffusion to model the continuous distribution
of the next continuous-valued token. Our approach delivers significant
improvements over previous discrete solution, AudioGen, achieving 20% and 40%
relative gains on AudioCaps in Frechet Audio Distance (FAD) and
Kullback-Leibler (KL) divergence, respectively. Additionally, we propose a
novel masked next-token prediction task that incorporates masked prediction
into the causal LM framework. On AudioCaps, the innovation yields 41% and 33%
relative FAD improvements over AudioGen Base (285M) and AudioGen Large (1B)
models, respectively, and is on par with the state-of-the-art (SOTA) diffusion
models. Furthermore, we achieve these results with significantly fewer
parameters -- 193M for our Base and 462M for our Large models.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [191] [Updated Earth Tomography Using Atmospheric Neutrinos at IceCube](https://arxiv.org/abs/2507.09763)
*Alex Wen*

Main category: astro-ph.HE

TL;DR: IceCube利用11年的中微子数据，通过地球层析成像技术测量地球密度和质量的分布。


<details>
  <summary>Details</summary>
Motivation: 研究中微子穿过地球时的衰减情况，以推断地球内部密度分布。

Method: 将地球建模为多层恒定密度壳，通过测量不同方向的上升中微子通量，推断各层密度。

Result: 展示了最新的分析灵敏度，并与之前测量结果进行了比较。

Conclusion: 这是最新的基于弱力的非引力地球密度和质量测量方法。

Abstract: The IceCube Neutrino Observatory has observed a sample of high purity,
primarily atmospheric, muon neutrino events over 11 years from all directions
below the horizon, spanning the energy range 500 GeV to 100 TeV. While this
sample was initially used for an eV-scale sterile neutrino search, its purity
and spanned parameter space can also be used to perform an earth tomography.
This flux of neutrinos traverses the earth and is attenuated in varying amounts
depending on the energy and traversed column density of the event. By
parameterizing the earth as multiple constant--density shells, IceCube can
measure the upgoing neutrino flux as a function of the declination, yielding an
inference of the density of each shell. In this talk, the latest sensitivities
of this analysis and comparisons with the previous measurement are presented.
In addition, the analysis procedure, details about the data sample, and
systematic effects are also explained. This analysis is one of the latest,
weak-force driven, non-gravitational, measurements of the earth's density and
mass.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [192] [CNeuroMod-THINGS, a densely-sampled fMRI dataset for visual neuroscience](https://arxiv.org/abs/2507.09024)
*Marie St-Laurent,Basile Pinsard,Oliver Contier,Elizabeth DuPre,Katja Seeliger,Valentina Borghesani,Julie A. Boyle,Lune Bellec,Martin N. Hebart*

Main category: q-bio.NC

TL;DR: CNeuroMod-THINGS整合了两个现有项目（THINGS和CNeuroMod），创建了一个大规模、密集采样的fMRI数据集，用于捕捉广泛语义概念的神经表征。


<details>
  <summary>Details</summary>
Motivation: 满足神经AI建模对大规模神经影像数据的需求，通过结合现有资源扩展对人类视觉体验的建模能力。

Method: 利用THINGS的注释图像集和CNeuroMod的fMRI数据采集方法，四名参与者完成了33-36次连续识别任务，使用约4000张图像。

Result: 报告了行为学和神经影像学指标，展示了数据的高质量。

Conclusion: CNeuroMod-THINGS通过整合资源，显著提升了人类视觉体验建模的能力。

Abstract: Data-hungry neuro-AI modelling requires ever larger neuroimaging datasets.
CNeuroMod-THINGS meets this need by capturing neural representations for a wide
set of semantic concepts using well-characterized stimuli in a new
densely-sampled, large-scale fMRI dataset. Importantly, CNeuroMod-THINGS
exploits synergies between two existing projects: the THINGS initiative
(THINGS) and the Courtois Project on Neural Modelling (CNeuroMod). THINGS has
developed a common set of thoroughly annotated images broadly sampling natural
and man-made objects which is used to acquire a growing collection of
large-scale multimodal neural responses. Meanwhile, CNeuroMod is acquiring
hundreds of hours of fMRI data from a core set of participants during
controlled and naturalistic tasks, including visual tasks like movie watching
and videogame playing. For CNeuroMod-THINGS, four CNeuroMod participants each
completed 33-36 sessions of a continuous recognition paradigm using
approximately 4000 images from the THINGS stimulus set spanning 720 categories.
We report behavioural and neuroimaging metrics that showcase the quality of the
data. By bridging together large existing resources, CNeuroMod-THINGS expands
our capacity to model broad slices of the human visual experience.

</details>


### [193] [Self-supervised pretraining of vision transformers for animal behavioral analysis and neural encoding](https://arxiv.org/abs/2507.09513)
*Yanchen Wang,Han Yu,Ari Blau,Yizi Zhang,The International Brain Laboratory,Liam Paninski,Cole Hurwitz,Matt Whiteway*

Main category: q-bio.NC

TL;DR: BEAST是一种新型自监督预训练框架，用于解决行为分析中标记数据不足的问题，通过结合掩码自编码和时间对比学习，提升了行为特征提取、姿态估计和动作分割的性能。


<details>
  <summary>Details</summary>
Motivation: 现代神经科学研究强调通过行为理解大脑，但现有视频分析方法依赖大量标记数据，限制了其应用。

Method: BEAST结合掩码自编码和时间对比学习，预训练实验专用的视觉变换器，利用未标记视频数据进行行为分析。

Result: 在多种物种中验证，BEAST在行为特征提取、姿态估计和动作分割任务中表现优异，尤其在标记数据稀缺的场景下。

Conclusion: BEAST为行为分析提供了高效且通用的模型，显著减少了标记数据的依赖。

Abstract: The brain can only be fully understood through the lens of the behavior it
generates -- a guiding principle in modern neuroscience research that
nevertheless presents significant technical challenges. Many studies capture
behavior with cameras, but video analysis approaches typically rely on
specialized models requiring extensive labeled data. We address this limitation
with BEAST (BEhavioral Analysis via Self-supervised pretraining of
Transformers), a novel and scalable framework that pretrains
experiment-specific vision transformers for diverse neuro-behavior analyses.
BEAST combines masked autoencoding with temporal contrastive learning to
effectively leverage unlabeled video data. Through comprehensive evaluation
across multiple species, we demonstrate improved performance in three critical
neuro-behavioral tasks: extracting behavioral features that correlate with
neural activity, and pose estimation and action segmentation in both the
single- and multi-animal settings. Our method establishes a powerful and
versatile backbone model that accelerates behavioral analysis in scenarios
where labeled data remains scarce.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [194] [On the relative humidity of the atmosphere](https://arxiv.org/abs/2507.08824)
*Raymond T. Pierrehumbert,Helene Brogniez,Remy Roca*

Main category: physics.ao-ph

TL;DR: 本文讨论了地球大气中水蒸气不饱和的原因及其影响，涉及观测、模型和理论分析。


<details>
  <summary>Details</summary>
Motivation: 研究水蒸气不饱和现象对水蒸气反馈和温室效应等过程的重要性。

Method: 结合观测数据、大气环流模型和理想化理论模型分析水蒸气不饱和的形成机制。

Result: 揭示了水蒸气不饱和的普遍性及其对地球和其他行星大气的影响。

Conclusion: 水蒸气不饱和是地球和其他行星大气中的普遍现象，其机制具有广泛适用性。

Abstract: Water vapour is highly subsaturated in much of the Earth's atmosphere. This
has important consequences for water vapour feedback, and also for general
phenomena such as the runaway greenhouse. In this chapter, we discuss the
processes that create subsaturation, with reference to both observations,
general circulation models, and a range of idealized theoretical models which
produce subsaturation. While this chapter focuses on subsaturation of water
vapour in Earth's atmosphere, the processes discussed are generic to
condensible substances in all planetary atmospheres.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [195] [Agent-based visualization of streaming text](https://arxiv.org/abs/2507.08884)
*Jordan Riley Benson,David Crist,Phil Lafleur,Benjamin Watson*

Main category: cs.MA

TL;DR: 该论文提出了一种基于代理的可视化基础设施，将数据元素映射到代理，代理的行为由这些元素参数化。动态可视化通过代理的位置变化、外观调整和相互响应实现。代理移动以最小化显示的代理间距离与理想距离矩阵之间的差异。当前应用是流文本的可视化，每个代理代表一个关键词，通过显示单词及其出现频率的圆圈大小来可视化。理想距离矩阵由词共现生成，高共现对应低距离。后端网络处理从新闻、博客等来源收集文章，生成的主题聚类可视化动态稳定。


<details>
  <summary>Details</summary>
Motivation: 为了动态可视化流文本数据中的主题和关键词关系，提供一种直观且稳定的可视化方法。

Method: 将数据元素映射到代理，代理行为由数据参数化，通过最小化代理间距离与理想距离矩阵的差异实现动态布局。理想距离矩阵由词共现生成，后端处理收集并分析文本数据。

Result: 生成的视觉化结果能够动态稳定地展示流文本中的主题聚类和关键词关系。

Conclusion: 该方法有效实现了流文本数据的动态可视化，能够直观展示主题和关键词关系，且布局稳定。

Abstract: We present a visualization infrastructure that maps data elements to agents,
which have behaviors parameterized by those elements. Dynamic visualizations
emerge as the agents change position, alter appearance and respond to one
other. Agents move to minimize the difference between displayed agent-to-agent
distances, and an input matrix of ideal distances. Our current application is
visualization of streaming text. Each agent represents a significant word,
visualizing it by displaying the word itself, centered in a circle sized by the
frequency of word occurrence. We derive the ideal distance matrix from word
cooccurrence, mapping higher co-occurrence to lower distance. To depict
co-occurrence in its textual context, the ratio of intersection to circle area
approximates the ratio of word co-occurrence to frequency. A networked backend
process gathers articles from news feeds, blogs, Digg or Twitter, exploiting
online search APIs to focus on user-chosen topics. Resulting visuals reveal the
primary topics in text streams as clusters, with agent-based layout moving
without instability as data streams change dynamically.

</details>


<div id='physics.bio-ph'></div>

# physics.bio-ph [[Back]](#toc)

### [196] [Exogeneous PpIX model for brain tumour assessment](https://arxiv.org/abs/2507.10230)
*John Raschke,Jean Pierre Ndabakuranye,Bobbi Fleiss,Arman Ahnood*

Main category: physics.bio-ph

TL;DR: 研究提出了一种外源性脑肿瘤模型，用于评估PpIX荧光检测，替代传统的动物模型，提高实验的可控性和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统动物模型具有不确定性，限制了荧光检测设备的临床前开发。

Method: 通过将PpIX溶液注射到切除的成年大鼠大脑皮层区域，模拟肿瘤区域的PpIX浓度梯度。

Result: 模型荧光分布与体内条件高度相关（R2>0.93），且溶剂DMSO不影响自发荧光。

Conclusion: 该模型能准确复制手术荧光条件，为荧光检测设备开发提供了更优的替代方案。

Abstract: Reliable in-vitro models are used for optoelectronic device development such
as fluorescence detection devices for fluorescence-guided surgery of gliomas. A
common approach is based on inducing gliomas in animal models. This is followed
by a dosage of 5-ALA to induce Protoporphyrin IX (PpIX) in the glioma and which
fluoresces. Although these approaches excel in capturing key biomolecular and
physiological features of the tumour, they are inherently indeterministic. This
limits the scope of their use for preclinical device development, where
consistent and controllable tumour reproduction across multiple animals is
needed. Approaches using fluorescence markers in gelatine provide a simple
replication but fail to capture the complexities of in-vivo models. In this
study, we introduce an exogenous brain tumour model for assessing PpIX
fluorescence detection. The model was developed by injecting a PpIX solution
into the cortical region of a resected adult rat brain, the injection site
simulated a tumoral region with elevated PpIX concentration. The tumoral region
had a gradient of concentrations, with a peak at the centre and a decrease
towards the margins, akin to in-vivo gliomas. The fluorescence profile was
compared to in-vivo conditions using 5-ALA and correlated well with other
reported works, achieving a correlation of R2>0.93. The model's validity was
tested by examining the effect of the solvent, DMSO, on the Autofluorescence
(AF) of the brain sample and the short-term effect of storage on AF was
analysed. Examinations confirmed the solvent did not alter AF, and the brain
sample should be stored in Hanks Balanced Salt Solution and refrigerated to
maintain moisture and preserve AF. The model accurately replicated surgical
fluorescence conditions and offers a suitable alternative to glioma induction,
benefiting the development of fluorescence detection devices across design
iterations.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [197] [Curvature-adaptive gigapixel microscopy at submicron resolution and centimeter scale](https://arxiv.org/abs/2507.09697)
*Xi Yang,Haitao Chen,Lucas Kreiss,Clare B. Cook,Genevieve Kuczewski,Mark Harfouche,Martin O. Bohlen,Roarke Horstmeyer*

Main category: physics.optics

TL;DR: PANORAMA是一种单次成像显微镜，能够在16.3×18.8 mm²的视场下实现0.84微米分辨率，无需机械扫描。


<details>
  <summary>Details</summary>
Motivation: 解决大视场显微镜在分辨率、成像速度和样本不平整性之间的权衡问题。

Method: 采用远心光刻镜头、大孔径管镜和带自适应对焦控制的微型相机阵列。

Result: 实现了无缝、千兆像素成像，适用于平坦或非平坦样本。

Conclusion: PANORAMA提高了成像通量和适应性，拓展了生物医学和材料成像的应用。

Abstract: Large-area microscopy with submicron resolution is limited by tradeoffs
between field of view (FOV), resolution, and imaging speed. Samples are rarely
flat across centimeter-scale FOV, which often requires existing solutions to
use mechanical scanning to ensure focused capture at reduced throughput. Here,
we present PANORAMA, a single-shot, re-imaging microscope that achieves
seamless, gigapixel imaging over a 16.3$\times$18.8 $\text{mm}^2$ FOV at 0.84
um resolution without mechanical scanning. By using a telecentric
photolithography lens, a large-aperture tube lens, and a flat micro-camera
array with adaptive per-camera focus control, PANORAMA maintains submicron
focus across flat, curved or uneven samples that span centimeters. This
approach improves imaging throughput and adaptability, enabling gigapixel
multi-modal microscopy of large flat and non-flat samples in one shot, thus
broadening its applications in biomedical and materials imaging.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [198] [Lightweight Deep Learning-Based Channel Estimation for RIS-Aided Extremely Large-Scale MIMO Systems on Resource-Limited Edge Devices](https://arxiv.org/abs/2507.09627)
*Muhammad Kamran Saeed,Ashfaq Khokhar,Shakil Ahmed*

Main category: cs.IT

TL;DR: 论文提出了一种轻量级深度学习框架，用于XL-MIMO系统中的高效级联信道估计，旨在降低计算复杂度并适用于资源受限的边缘设备。


<details>
  <summary>Details</summary>
Motivation: 6G等下一代无线技术需要满足超高数据速率、低延迟和增强连接性等要求，但XL-MIMO和RIS的潜力依赖于准确的CSI。现有估计模型在可扩展性和实际部署方面存在限制。

Method: 提出了一种基于空间相关性的轻量级深度学习框架，通过分块训练机制降低输入维度，同时保留关键信息。

Result: 仿真结果表明，该框架显著提高了估计精度并降低了计算复杂度，适用于大规模XL-MIMO系统。

Conclusion: 该框架为XL-MIMO系统中的高效信道估计提供了可行的解决方案，适用于资源受限的边缘设备。

Abstract: Next-generation wireless technologies such as 6G aim to meet demanding
requirements such as ultra-high data rates, low latency, and enhanced
connectivity. Extremely Large-Scale MIMO (XL-MIMO) and Reconfigurable
Intelligent Surface (RIS) are key enablers, with XL-MIMO boosting spectral and
energy efficiency through numerous antennas, and RIS offering dynamic control
over the wireless environment via passive reflective elements. However,
realizing their full potential depends on accurate Channel State Information
(CSI). Recent advances in deep learning have facilitated efficient cascaded
channel estimation. However, the scalability and practical deployment of
existing estimation models in XL-MIMO systems remain limited. The growing
number of antennas and RIS elements introduces a significant barrier to
real-time and efficient channel estimation, drastically increasing data volume,
escalating computational complexity, requiring advanced hardware, and resulting
in substantial energy consumption. To address these challenges, we propose a
lightweight deep learning framework for efficient cascaded channel estimation
in XL-MIMO systems, designed to minimize computational complexity and make it
suitable for deployment on resource-constrained edge devices. Using spatial
correlations in the channel, we introduce a patch-based training mechanism that
reduces the dimensionality of input to patch-level representations while
preserving essential information, allowing scalable training for large-scale
systems. Simulation results under diverse conditions demonstrate that our
framework significantly improves estimation accuracy and reduces computational
complexity, regardless of the increasing number of antennas and RIS elements in
XL-MIMO systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [199] [Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling](https://arxiv.org/abs/2507.09211)
*Xinyue Liu,Xiao Peng,Shuyue Yan,Yuntian Chen,Dongxiao Zhang,Zhixiao Niu,Hui-Min Wang,Xiaogang He*

Main category: cs.LG

TL;DR: DeepX-GAN模型用于生成超出历史记录的极端气候事件，揭示潜在风险。


<details>
  <summary>Details</summary>
Motivation: 现有气候极端事件记录不完整，且忽视空间依赖性，低估了同步灾害的风险。

Method: 开发DeepX-GAN模型，结合知识引导的深度生成方法，模拟罕见极端事件的空间结构。

Result: 模型成功生成统计上合理的“未见”极端事件，揭示中东和北非等脆弱地区的潜在风险。

Conclusion: 未来变暖可能重新分布极端事件，需制定空间适应性政策以应对新兴风险热点。

Abstract: Observed records of climate extremes provide an incomplete picture of risk,
missing "unseen" extremes that exceed historical bounds. In parallel,
neglecting spatial dependence undervalues the risk of synchronized hazards that
amplify impacts. To address these challenges, we develop DeepX-GAN
(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial
Network), a knowledge-informed deep generative model designed to better capture
the spatial structure of rare extremes. The zero-shot generalizability of
DeepX-GAN enables simulation of unseen extremes that fall outside historical
experience yet remain statistically plausible. We define two types of unseen
extremes: "checkmate" extremes that directly hit targets, and "stalemate"
extremes that narrowly miss. These unrealized scenarios expose latent risks in
fragile systems and may reinforce a false sense of resilience if overlooked.
Near misses, in particular, can prompt either proactive adaptation or dangerous
complacency, depending on how they are interpreted. Applying DeepX-GAN to the
Middle East and North Africa (MENA), we find that these unseen extremes
disproportionately affect regions with high vulnerability and low socioeconomic
readiness, but differ in urgency and interpretation. Future warming could
expand and redistribute these unseen extremes, with emerging exposure hotspots
in Indo-Pakistan and Central Africa. This distributional shift highlights
critical blind spots in conventional hazard planning and underscores the need
to develop spatially adaptive policies that anticipate emergent risk hotspots
rather than simply extrapolating from historical patterns.

</details>


### [200] [Enhanced DeepONet for 1-D consolidation operator learning: an architectural investigation](https://arxiv.org/abs/2507.10368)
*Yongjin Choi,Chenying Liu,Jorge Macedo*

Main category: cs.LG

TL;DR: DeepONets在岩土工程中的应用有限，本文评估了几种架构，提出了一种改进的Trunknet Fourier feature-enhanced DeepONet（Model 4），显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 探索DeepONets在岩土工程中的潜力，特别是针对一维固结问题，以推动科学机器学习在岩土领域的应用。

Method: 比较了三种DeepONet架构（标准型和物理启发型），并提出了改进的Model 4，用于捕捉快速变化的函数。

Result: Model 4表现最佳，计算速度提升1.5至100倍，适用于复杂系统。

Conclusion: DeepONets在岩土工程中具有高效、通用的潜力，推动了科学机器学习在该领域的早期发展。

Abstract: Deep Operator Networks (DeepONets) have emerged as a powerful surrogate
modeling framework for learning solution operators in PDE-governed systems.
While their use is expanding across engineering disciplines, applications in
geotechnical engineering remain limited. This study systematically evaluates
several DeepONet architectures for the one-dimensional consolidation problem.
We initially consider three architectures: a standard DeepONet with the
coefficient of consolidation embedded in the branch net (Models 1 and 2), and a
physics-inspired architecture with the coefficient embedded in the trunk net
(Model 3). Results show that Model 3 outperforms the standard configurations
(Models 1 and 2) but still has limitations when the target solution (excess
pore pressures) exhibits significant variation. To overcome this limitation, we
propose a Trunknet Fourier feature-enhanced DeepONet (Model 4) that addresses
the identified limitations by capturing rapidly varying functions. All proposed
architectures achieve speedups ranging from 1.5 to 100 times over traditional
explicit and implicit solvers, with Model 4 being the most efficient. Larger
computational savings are expected for more complex systems than the explored
1D case, which is promising. Overall, the study highlights the potential of
DeepONets to enable efficient, generalizable surrogate modeling in geotechnical
applications, advancing the integration of scientific machine learning in
geotechnics, which is at an early stage.

</details>


### [201] [Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer](https://arxiv.org/abs/2507.08839)
*Xiaowei Yu,Jing Zhang,Tong Chen,Yan Zhuang,Minheng Chen,Chao Cao,Yanjun Lyu,Lu Zhang,Li Su,Tianming Liu,Dajiang Zhu*

Main category: cs.LG

TL;DR: 提出了一种基于注意力机制的迁移学习模型（TAT），利用阿尔茨海默病（AD）数据增强路易体痴呆（LBD）的诊断，解决了数据稀缺和领域偏移问题。


<details>
  <summary>Details</summary>
Motivation: LBD是一种常见但研究不足的痴呆症，诊断面临数据稀缺和领域偏移的挑战，而AD数据丰富，可用于知识迁移。

Method: 采用结构MRI提取的结构连接性（SC）作为训练数据，提出TAT模型，通过注意力机制自适应分配权重，减少领域偏移。

Result: 实验证明TAT能有效提升LBD诊断准确性，尤其在数据稀缺情况下。

Conclusion: TAT是首个探索从AD到LBD领域适应的研究，为罕见病的诊断提供了新框架。

Abstract: Lewy Body Disease (LBD) is a common yet understudied form of dementia that
imposes a significant burden on public health. It shares clinical similarities
with Alzheimer's disease (AD), as both progress through stages of normal
cognition, mild cognitive impairment, and dementia. A major obstacle in LBD
diagnosis is data scarcity, which limits the effectiveness of deep learning. In
contrast, AD datasets are more abundant, offering potential for knowledge
transfer. However, LBD and AD data are typically collected from different sites
using different machines and protocols, resulting in a distinct domain shift.
To effectively leverage AD data while mitigating domain shift, we propose a
Transferability Aware Transformer (TAT) that adapts knowledge from AD to
enhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived
from structural MRI as training data. Built on the attention mechanism, TAT
adaptively assigns greater weights to disease-transferable features while
suppressing domain-specific ones, thereby reducing domain shift and improving
diagnostic accuracy with limited LBD data. The experimental results demonstrate
the effectiveness of TAT. To the best of our knowledge, this is the first study
to explore domain adaptation from AD to LBD under conditions of data scarcity
and domain shift, providing a promising framework for domain-adaptive diagnosis
of rare diseases.

</details>


### [202] [Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations](https://arxiv.org/abs/2507.09264)
*Payel Mukhopadhyay,Michael McCabe,Ruben Ohana,Miles Cranmer*

Main category: cs.LG

TL;DR: 论文提出两种轻量级模块（CKM和CSM），实现动态调整补丁大小，无需重新训练或损失精度，提升长期稳定性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 固定补丁大小限制了生产部署的灵活性，需要一种动态调整补丁大小的方法。

Method: 引入CKM和CSM模块，结合循环补丁大小策略，动态控制补丁大小。

Result: 在2D和3D PDE基准测试中提升了预测精度和运行效率。

Conclusion: 首次实现补丁大小可调的PDE代理框架，具有广泛适用性。

Abstract: Patch-based transformer surrogates have become increasingly effective for
modeling spatiotemporal dynamics, but the fixed patch size is a major
limitation for budget-conscience deployment in production. We introduce two
lightweight, architecture-agnostic modules-the Convolutional Kernel Modulator
(CKM) and Convolutional Stride Modulator (CSM)-that enable dynamic patch size
control at inference in patch based models, without retraining or accuracy
loss. Combined with a cyclic patch-size rollout, our method mitigates patch
artifacts and improves long-term stability for video-like prediction tasks.
Applied to a range of challenging 2D and 3D PDE benchmarks, our approach
improves rollout fidelity and runtime efficiency. To our knowledge, this is the
first framework to enable inference-time patch-size tunability in patch-based
PDE surrogates. Its plug-and-play design makes it broadly applicable across
architectures-establishing a general foundation for compute-adaptive modeling
in PDE surrogate tasks.

</details>


### [203] [Zero-Shot Neural Architecture Search with Weighted Response Correlation](https://arxiv.org/abs/2507.08841)
*Kun Jing,Luoyu Chen,Jungang Xu,Jianwei Tai,Yiyu Wang,Shuaimin Li*

Main category: cs.LG

TL;DR: 提出了一种名为WRCor的训练免费代理方法，用于加速神经架构搜索（NAS），通过响应相关性矩阵评估架构的表达性和泛化性，实验证明其高效性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本NAS方法的有效性、稳定性和通用性不足，需要更高效的训练免费代理来加速架构评估。

Method: 使用加权响应相关性（WRCor）作为代理，通过计算不同输入样本响应之间的相关系数矩阵来评估架构。

Result: WRCor及其投票代理在代理评估中表现优于现有方法，NAS算法在多个搜索空间中优于现有算法，4小时内发现ImageNet-1k上22.1%测试错误的架构。

Conclusion: WRCor是一种高效且通用的训练免费代理，显著提升了零样本NAS的性能和效率。

Abstract: Neural architecture search (NAS) is a promising approach for automatically
designing neural network architectures. However, the architecture estimation of
NAS is computationally expensive and time-consuming because of training
multiple architectures from scratch. Although existing zero-shot NAS methods
use training-free proxies to accelerate the architecture estimation, their
effectiveness, stability, and generality are still lacking. We present a novel
training-free estimation proxy called weighted response correlation (WRCor).
WRCor utilizes correlation coefficient matrices of responses across different
input samples to calculate the proxy scores of estimated architectures, which
can measure their expressivity and generalizability. Experimental results on
proxy evaluation demonstrate that WRCor and its voting proxies are more
efficient estimation strategies than existing proxies. We also apply them with
different search strategies in architecture search. Experimental results on
architecture search show that our zero-shot NAS algorithm outperforms most
existing NAS algorithms in different search spaces. Our NAS algorithm can
discover an architecture with a 22.1% test error on the ImageNet-1k dataset
within 4 GPU hours. All codes are publicly available at
https://github.com/kunjing96/ZSNAS-WRCor.git.

</details>


### [204] [Learning Diffusion Models with Flexible Representation Guidance](https://arxiv.org/abs/2507.08980)
*Chenyu Wang,Cai Zhou,Sharut Gupta,Zongyu Lin,Stefanie Jegelka,Stephen Bates,Tommi Jaakkola*

Main category: cs.LG

TL;DR: 论文提出了一种系统框架，通过表示引导改进扩散模型，引入两种新策略并展示了在图像、蛋白质序列和分子生成任务中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 通过将扩散模型的内部表示与预训练模型对齐，提高生成质量。

Method: 提出了两种策略：1）将目标表示与示例配对学习联合模型；2）设计平衡表示学习和数据生成的最优训练课程。

Result: 在ImageNet 256×256基准测试中，训练速度比原始SiT-XL快23.3倍，比REPA快4倍。

Conclusion: 表示引导显著提升了扩散模型的性能和训练效率。

Abstract: Diffusion models can be improved with additional guidance towards more
effective representations of input. Indeed, prior empirical work has already
shown that aligning internal representations of the diffusion model with those
of pre-trained models improves generation quality. In this paper, we present a
systematic framework for incorporating representation guidance into diffusion
models. We provide alternative decompositions of denoising models along with
their associated training criteria, where the decompositions determine when and
how the auxiliary representations are incorporated. Guided by our theoretical
insights, we introduce two new strategies for enhancing representation
alignment in diffusion models. First, we pair examples with target
representations either derived from themselves or arisen from different
synthetic modalities, and subsequently learn a joint model over the multimodal
pairs. Second, we design an optimal training curriculum that balances
representation learning and data generation. Our experiments across image,
protein sequence, and molecule generation tasks demonstrate superior
performance as well as accelerated training. In particular, on the
class-conditional ImageNet $256\times 256$ benchmark, our guidance results in
$23.3$ times faster training than the original SiT-XL as well as four times
speedup over the state-of-the-art method REPA. The code is available at
https://github.com/ChenyuWang-Monica/REED.

</details>


### [205] [Confounder-Free Continual Learning via Recursive Feature Normalization](https://arxiv.org/abs/2507.09031)
*Yash Shah,Camila Gonzalez,Mohammad H. Abbasi,Qingyu Zhao,Kilian M. Pohl,Ehsan Adeli*

Main category: cs.LG

TL;DR: 论文提出了一种递归元数据归一化层（R-MDN），用于在持续学习中消除混杂变量的影响，提升模型预测的公平性。


<details>
  <summary>Details</summary>
Motivation: 混杂变量会导致虚假相关性和预测偏差，现有方法（如MDN）在持续学习中难以有效处理。

Method: R-MDN通过递归最小二乘算法动态调整特征表示，适应数据和混杂变量的变化分布。

Result: 实验表明，R-MDN在静态学习和持续学习中均能减少混杂变量的影响，提升预测公平性。

Conclusion: R-MDN是一种通用且有效的解决方案，适用于任何深度学习架构和持续学习场景。

Abstract: Confounders are extraneous variables that affect both the input and the
target, resulting in spurious correlations and biased predictions. There are
recent advances in dealing with or removing confounders in traditional models,
such as metadata normalization (MDN), where the distribution of the learned
features is adjusted based on the study confounders. However, in the context of
continual learning, where a model learns continuously from new data over time
without forgetting, learning feature representations that are invariant to
confounders remains a significant challenge. To remove their influence from
intermediate feature representations, we introduce the Recursive MDN (R-MDN)
layer, which can be integrated into any deep learning architecture, including
vision transformers, and at any model stage. R-MDN performs statistical
regression via the recursive least squares algorithm to maintain and
continually update an internal model state with respect to changing
distributions of data and confounding variables. Our experiments demonstrate
that R-MDN promotes equitable predictions across population groups, both within
static learning and across different stages of continual learning, by reducing
catastrophic forgetting caused by confounder effects changing over time.

</details>


### [206] [Warm Starts Accelerate Generative Modelling](https://arxiv.org/abs/2507.09212)
*Jonas Scholz,Richard E. Turner*

Main category: cs.LG

TL;DR: 提出了一种名为“warm-start model”的确定性模型，通过提供更好的起始点加速条件生成，显著减少生成过程所需时间。


<details>
  <summary>Details</summary>
Motivation: 传统迭代生成模型（如扩散模型和流匹配）生成高保真样本时速度较慢，通常需要数百次函数评估。

Method: 引入warm-start模型，预测一个条件化的先验分布N(mu, sigma)作为起始点，而非传统的N(0, I)先验。结合条件归一化技巧，使其兼容任何标准生成模型和采样器。

Result: 在图像修复等任务中，仅需11次函数评估（1次用于warm start，10次用于生成）即可达到与1000步DDPM基准相当的效果。

Conclusion: warm-start模型显著提升了生成效率，且无需修改现有模型即可与其他高效采样技术结合。

Abstract: Iterative generative models, like diffusion and flow-matching, create
high-fidelity samples by progressively refining a noise vector into data.
However, this process is notoriously slow, often requiring hundreds of function
evaluations. We introduce the warm-start model, a simple, deterministic model
that dramatically accelerates conditional generation by providing a better
starting point. Instead of starting generation from an uninformed N(0, I)
prior, our warm-start model predicts an informed prior N(mu, sigma), whose
moments are conditioned on the input context. This "warm start" substantially
reduces the distance the generative process must traverse, particularly when
the conditioning information is strongly informative. On tasks like image
inpainting, our method achieves results competitive with a 1000-step DDPM
baseline using only 11 total function evaluations (1 for the warm start, 10 for
generation). A simple conditional normalization trick makes our method
compatible with any standard generative model and sampler without modification,
allowing it to be combined with other efficient sampling techniques for further
acceleration. Our implementation is available at
https://github.com/jonas-scholz123/warm-start-model.

</details>


### [207] [MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression](https://arxiv.org/abs/2507.09616)
*Ofir Gordon,Ariel Lapid,Elad Cohen,Yarden Yagil,Arnon Netzer,Hai Victor Habi*

Main category: cs.LG

TL;DR: MLoRQ是一种结合低秩近似和混合精度量化的新方法，用于在资源受限的边缘设备上部署Transformer网络，通过两阶段优化过程实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限的边缘设备上部署Transformer网络的挑战。

Method: 采用两阶段优化过程：层内优化选择最佳压缩方案，层间优化分配位宽和秩以满足内存限制，最后可选步骤通过自适应舍入技术减少误差。

Result: 在视觉Transformer任务中，性能提升高达15%。

Conclusion: MLoRQ是一种高效且兼容性强的压缩方法，适用于边缘设备。

Abstract: Deploying transformer-based neural networks on resource-constrained edge
devices presents a significant challenge. This challenge is often addressed
through various techniques, such as low-rank approximation and mixed-precision
quantization. In this work, we introduce Mixed Low-Rank and Quantization
(MLoRQ), a novel method that integrates both techniques. MLoRQ employs a
two-stage optimization process to determine optimal bit-width and rank
assignments for each layer, adhering to predefined memory constraints. This
process includes: (i) an intra-layer optimization that identifies potentially
optimal compression solutions out of all low-rank and quantization
combinations; (ii) an inter-layer optimization that assigns bit-width precision
and rank to each layer while ensuring the memory constraint is met. An optional
final step applies a sequential optimization process using a modified adaptive
rounding technique to mitigate compression-induced errors in joint low-rank
approximation and quantization. The method is compatible and can be seamlessly
integrated with most existing quantization algorithms. MLoRQ shows
state-of-the-art results with up to 15\% performance improvement, evaluated on
Vision Transformers for image classification, object detection, and instance
segmentation tasks.

</details>


### [208] [Universal Physics Simulation: A Foundational Diffusion Approach](https://arxiv.org/abs/2507.09733)
*Bradley Camburn*

Main category: cs.LG

TL;DR: 提出了一种基于边界条件数据直接学习物理定律的通用物理模拟基础AI模型，无需先验方程编码。


<details>
  <summary>Details</summary>
Motivation: 传统物理模拟方法（如PINNs和有限差分法）需要显式数学方程，限制了其泛化性和发现潜力。本文旨在通过AI直接生成物理准确的稳态解，实现通用物理模拟。

Method: 采用草图引导的扩散变换器方法，将模拟视为条件生成问题，利用增强的扩散变换器架构和空间关系编码，实现边界到稳态的直接映射。

Result: 模型能直接生成稳态解（SSIM > 0.8），边界精度达到亚像素级，且无需时间积分，避免了误差累积。

Conclusion: 该工作标志着从AI加速物理到AI发现物理的范式转变，建立了首个真正通用的物理模拟框架。

Abstract: We present the first foundational AI model for universal physics simulation
that learns physical laws directly from boundary-condition data without
requiring a priori equation encoding. Traditional physics-informed neural
networks (PINNs) and finite-difference methods necessitate explicit
mathematical formulation of governing equations, fundamentally limiting their
generalizability and discovery potential. Our sketch-guided diffusion
transformer approach reimagines computational physics by treating simulation as
a conditional generation problem, where spatial boundary conditions guide the
synthesis of physically accurate steady-state solutions.
  By leveraging enhanced diffusion transformer architectures with novel spatial
relationship encoding, our model achieves direct boundary-to-equilibrium
mapping and is generalizable to diverse physics domains. Unlike sequential
time-stepping methods that accumulate errors over iterations, our approach
bypasses temporal integration entirely, directly generating steady-state
solutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our
data-informed approach enables physics discovery through learned
representations analyzable via Layer-wise Relevance Propagation (LRP),
revealing emergent physical relationships without predetermined mathematical
constraints. This work represents a paradigm shift from AI-accelerated physics
to AI-discovered physics, establishing the first truly universal physics
simulation framework.

</details>


### [209] [Learning Private Representations through Entropy-based Adversarial Training](https://arxiv.org/abs/2507.10194)
*Tassilo Klein,Moin Nabi*

Main category: cs.LG

TL;DR: 提出了一种对抗性表示学习方法，通过引入焦点熵来减少信息泄露，同时保持高预测能力。


<details>
  <summary>Details</summary>
Motivation: 在保持用户隐私的同时，学习具有高预测能力的表示。

Method: 使用对抗性表示学习和焦点熵（一种熵的变体）来净化敏感内容。

Result: 在多个基准测试中验证了方法的可行性，结果显示高目标效用和中等隐私泄露。

Conclusion: 该方法在隐私保护和预测能力之间取得了平衡。

Abstract: How can we learn a representation with high predictive power while preserving
user privacy? We present an adversarial representation learning method for
sanitizing sensitive content from the learned representation. Specifically, we
introduce a variant of entropy - focal entropy, which mitigates the potential
information leakage of the existing entropy-based approaches. We showcase
feasibility on multiple benchmarks. The results suggest high target utility at
moderate privacy leakage.

</details>


### [210] [CLA: Latent Alignment for Online Continual Self-Supervised Learning](https://arxiv.org/abs/2507.10434)
*Giacomo Cignoni,Andrea Cossu,Alexandra Gomez-Villa,Joost van de Weijer,Antonio Carta*

Main category: cs.LG

TL;DR: CLA是一种自监督学习方法，用于在线持续学习，通过对齐当前与过去的表征来减少遗忘，加速训练收敛，并在有限计算预算下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线持续学习中，数据以小批量到达且无任务边界，现有自监督学习方法较少，需要一种高效策略来减少遗忘并提升性能。

Method: 提出Continual Latent Alignment (CLA)，通过对齐当前模型与过去学习的表征来减少遗忘。

Result: CLA加速了在线场景下的训练收敛，优于现有方法；作为预训练协议时，其早期表现优于完全独立同分布预训练。

Conclusion: CLA是一种有效的在线持续学习自监督策略，兼具高效性和性能优势。

Abstract: Self-supervised learning (SSL) is able to build latent representations that
generalize well to unseen data. However, only a few SSL techniques exist for
the online CL setting, where data arrives in small minibatches, the model must
comply with a fixed computational budget, and task boundaries are absent. We
introduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL
that aligns the representations learned by the current model with past
representations to mitigate forgetting. We found that our CLA is able to speed
up the convergence of the training process in the online scenario,
outperforming state-of-the-art approaches under the same computational budget.
Surprisingly, we also discovered that using CLA as a pretraining protocol in
the early stages of pretraining leads to a better final performance when
compared to a full i.i.d. pretraining.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [211] [ESG-Net: Event-Aware Semantic Guided Network for Dense Audio-Visual Event Localization](https://arxiv.org/abs/2507.09945)
*Huilai Li,Yonghao Dang,Ying Xing,Yiming Wang,Jianqin Yin*

Main category: cs.MM

TL;DR: 论文提出了一种名为ESG-Net的方法，通过多阶段语义引导和多事件关系建模，解决了密集视听事件定位中的模态语义鸿沟和事件相关性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究在中间层缺乏跨模态语义桥接，且未考虑事件间的相关性，导致难以区分事件相关内容和背景内容，并限制了复杂场景中并发事件的推断能力。

Method: 提出ESG-Net，包含早期语义交互（ESI）模块和依赖专家混合（MoDE）模块，分别实现多阶段语义引导和多事件依赖关系建模。

Result: 实验表明，该方法显著优于现有技术，同时大幅减少参数和计算负担。

Conclusion: ESG-Net通过分层语义理解和自适应事件依赖提取，有效提升了密集视听事件定位的性能。

Abstract: Dense audio-visual event localization (DAVE) aims to identify event
categories and locate the temporal boundaries in untrimmed videos. Most studies
only employ event-related semantic constraints on the final outputs, lacking
cross-modal semantic bridging in intermediate layers. This causes modality
semantic gap for further fusion, making it difficult to distinguish between
event-related content and irrelevant background content. Moreover, they rarely
consider the correlations between events, which limits the model to infer
concurrent events among complex scenarios. In this paper, we incorporate
multi-stage semantic guidance and multi-event relationship modeling, which
respectively enable hierarchical semantic understanding of audio-visual events
and adaptive extraction of event dependencies, thereby better focusing on
event-related information. Specifically, our eventaware semantic guided network
(ESG-Net) includes a early semantics interaction (ESI) module and a mixture of
dependency experts (MoDE) module. ESI applys multi-stage semantic guidance to
explicitly constrain the model in learning semantic information through
multi-modal early fusion and several classification loss functions, ensuring
hierarchical understanding of event-related content. MoDE promotes the
extraction of multi-event dependencies through multiple serial mixture of
experts with adaptive weight allocation. Extensive experiments demonstrate that
our method significantly surpasses the state-of-the-art methods, while greatly
reducing parameters and computational load. Our code will be released on
https://github.com/uchiha99999/ESG-Net.

</details>


### [212] [LayLens: Improving Deepfake Understanding through Simplified Explanations](https://arxiv.org/abs/2507.10066)
*Abhijeet Narang,Parul Gupta,Liuyijia Su,Abhinav Dhall*

Main category: cs.MM

TL;DR: LayLens是一个工具，通过三阶段流程（检测、简化解释、视觉重建）帮助用户理解深度伪造内容，提升透明度和信任度。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度伪造检测工具技术术语过多、难以理解的问题，使不同教育背景的用户都能轻松理解。

Method: 采用三阶段流程：1) 可解释的深度伪造检测；2) 技术解释的自然语言简化；3) 视觉重建原始图像。

Result: 用户研究表明，简化解释显著提高了清晰度，降低了认知负担，增强了用户识别深度伪造的信心。

Conclusion: LayLens为透明、可信且用户友好的深度伪造取证提供了新方向。

Abstract: This demonstration paper presents $\mathbf{LayLens}$, a tool aimed to make
deepfake understanding easier for users of all educational backgrounds. While
prior works often rely on outputs containing technical jargon, LayLens bridges
the gap between model reasoning and human understanding through a three-stage
pipeline: (1) explainable deepfake detection using a state-of-the-art forgery
localization model, (2) natural language simplification of technical
explanations using a vision-language model, and (3) visual reconstruction of a
plausible original image via guided image editing. The interface presents both
technical and layperson-friendly explanations in addition to a side-by-side
comparison of the uploaded and reconstructed images. A user study with 15
participants shows that simplified explanations significantly improve clarity
and reduce cognitive load, with most users expressing increased confidence in
identifying deepfakes. LayLens offers a step toward transparent, trustworthy,
and user-centric deepfake forensics.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [213] [TRACER: Efficient Object Re-Identification in Networked Cameras through Adaptive Query Processing](https://arxiv.org/abs/2507.09448)
*Pramod Chunduri,Yao Lu,Joy Arulraj*

Main category: cs.DB

TL;DR: Tracer是一种新型VDBMS，通过自适应查询处理框架高效处理Re-ID查询，解决了现有系统Spactula的局限性，并在性能上显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有系统Spactula在大型摄像头网络中时空过滤精度有限，且不支持自适应查询处理，无法满足高召回率的关键视频分析需求。

Method: Tracer采用递归网络建模长期历史相关性以选择最优摄像头，并通过概率自适应搜索模型加速查询。

Result: Tracer在多样化数据集上的性能平均优于现有技术3.9倍。

Conclusion: Tracer通过自适应框架和合成基准解决了Re-ID任务中的关键问题，显著提升了性能。

Abstract: Efficiently re-identifying and tracking objects across a network of cameras
is crucial for applications like traffic surveillance. Spatula is the
state-of-the-art video database management system (VDBMS) for processing Re-ID
queries. However, it suffers from two limitations. Its spatio-temporal
filtering scheme has limited accuracy on large camera networks due to localized
camera history. It is not suitable for critical video analytics applications
that require high recall due to a lack of support for adaptive query
processing.
  In this paper, we present Tracer, a novel VDBMS for efficiently processing
Re-ID queries using an adaptive query processing framework. Tracer selects the
optimal camera to process at each time step by training a recurrent network to
model long-term historical correlations. To accelerate queries under a high
recall constraint, Tracer incorporates a probabilistic adaptive search model
that processes camera feeds in incremental search windows and dynamically
updates the sampling probabilities using an exploration-exploitation strategy.
To address the paucity of benchmarks for the Re-ID task due to privacy
concerns, we present a novel synthetic benchmark for generating multi-camera
Re-ID datasets based on real-world traffic distribution. Our evaluation shows
that Tracer outperforms the state-of-the-art cross-camera analytics system by
3.9x on average across diverse datasets.

</details>
