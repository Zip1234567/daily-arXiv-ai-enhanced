<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 88]
- [eess.IV](#eess.IV) [Total: 13]
- [cs.GR](#cs.GR) [Total: 3]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting](https://arxiv.org/abs/2507.01305)
*Worameth Chinchuthakun,Pakkapon Phongthawee,Amit Raj,Varun Jampani,Pramook Khungurn,Supasorn Suwajanakorn*

Main category: cs.CV

TL;DR: 提出了一种通过将任务重新定义为铬球修复问题，从单张低动态范围（LDR）图像估计光照的简单有效技术，利用预训练扩散模型Stable Diffusion XL，并引入DiffusionLight和DiffusionLight-Turbo优化结果生成速度。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖有限的HDR全景数据集，泛化能力不足，而扩散模型在生成HDR格式铬球时存在内容不一致或错误的问题。

Method: 使用迭代修复生成中值铬球作为低频光照先验，结合Exposure LoRA生成多曝光LDR图像并合并为HDR光探针；进一步通过Turbo LoRA和LoRA交换技术加速生成过程。

Result: 实验结果表明，该方法在多样化场景中生成逼真的光照估计，并在实际场景中表现出优越的泛化能力。

Conclusion: DiffusionLight和DiffusionLight-Turbo通过创新技术解决了光照估计的挑战，显著提升了生成速度和结果质量。

Abstract: We introduce a simple yet effective technique for estimating lighting from a
single low-dynamic-range (LDR) image by reframing the task as a chrome ball
inpainting problem. This approach leverages a pre-trained diffusion model,
Stable Diffusion XL, to overcome the generalization failures of existing
methods that rely on limited HDR panorama datasets. While conceptually simple,
the task remains challenging because diffusion models often insert incorrect or
inconsistent content and cannot readily generate chrome balls in HDR format.
Our analysis reveals that the inpainting process is highly sensitive to the
initial noise in the diffusion process, occasionally resulting in unrealistic
outputs. To address this, we first introduce DiffusionLight, which uses
iterative inpainting to compute a median chrome ball from multiple outputs to
serve as a stable, low-frequency lighting prior that guides the generation of a
high-quality final result. To generate high-dynamic-range (HDR) light probes,
an Exposure LoRA is fine-tuned to create LDR images at multiple exposure
values, which are then merged. While effective, DiffusionLight is
time-intensive, requiring approximately 30 minutes per estimation. To reduce
this overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to
about 30 seconds with minimal quality loss. This 60x speedup is achieved by
training a Turbo LoRA to directly predict the averaged chrome balls from the
iterative process. Inference is further streamlined into a single denoising
pass using a LoRA swapping technique. Experimental results that show our method
produces convincing light estimates across diverse settings and demonstrates
superior generalization to in-the-wild scenarios. Our code is available at
https://diffusionlight.github.io/turbo

</details>


### [2] [Geometry-aware 4D Video Generation for Robot Manipulation](https://arxiv.org/abs/2507.01099)
*Zeyi Liu,Shuang Li,Eric Cousineau,Siyuan Feng,Benjamin Burchfiel,Shuran Song*

Main category: cs.CV

TL;DR: 提出了一种4D视频生成模型，通过跨视角点图对齐监督训练，实现多视角3D一致性，提升机器人动态场景预测能力。


<details>
  <summary>Details</summary>
Motivation: 增强机器人在复杂环境中的规划和交互能力，解决现有视频生成模型在时间连贯性和几何一致性上的不足。

Method: 利用RGB-D观测数据，通过跨视角点图对齐监督训练，学习共享3D场景表示，无需相机姿态输入即可预测未来视频序列。

Result: 在模拟和真实机器人数据集上生成更稳定、空间对齐的预测视频，并支持机器人末端执行器轨迹恢复。

Conclusion: 该方法显著提升了视频生成的几何一致性，支持机器人操作和视角泛化。

Abstract: Understanding and predicting the dynamics of the physical world can enhance a
robot's ability to plan and interact effectively in complex environments. While
recent video generation models have shown strong potential in modeling dynamic
scenes, generating videos that are both temporally coherent and geometrically
consistent across camera views remains a significant challenge. To address
this, we propose a 4D video generation model that enforces multi-view 3D
consistency of videos by supervising the model with cross-view pointmap
alignment during training. This geometric supervision enables the model to
learn a shared 3D representation of the scene, allowing it to predict future
video sequences from novel viewpoints based solely on the given RGB-D
observations, without requiring camera poses as inputs. Compared to existing
baselines, our method produces more visually stable and spatially aligned
predictions across multiple simulated and real-world robotic datasets. We
further show that the predicted 4D videos can be used to recover robot
end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting
robust robot manipulation and generalization to novel camera viewpoints.

</details>


### [3] [Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation](https://arxiv.org/abs/2507.01631)
*Camille Billouard,Dawa Derksen,Alexandre Constantin,Bruno Vallet*

Main category: cs.CV

TL;DR: Snake-NeRF是一种扩展到大场景的NeRF框架，通过分块处理和优化策略，解决了内存限制问题，实现了单设备上的高效3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有NeRF方法因内存限制仅适用于小场景，无法处理大尺度卫星图像。

Method: 将场景划分为无重叠的3D块，采用2×2块渐进策略和分段采样器，避免边缘重建错误。

Result: 实验表明，该方法能在单GPU上线性时间处理大卫星图像，且不损失质量。

Conclusion: Snake-NeRF为大规模3D重建提供了高效解决方案。

Abstract: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images and networks simultaneously, and
operates on a single device. We achieve this by dividing the region of interest
into NeRFs that 3D tile without overlap. Importantly, we crop the images with
overlap to ensure each NeRFs is trained with all the necessary pixels. We
introduce a novel $2\times 2$ 3D tile progression strategy and segmented
sampler, which together prevent 3D reconstruction errors along the tile edges.
Our experiments conclude that large satellite images can effectively be
processed with linear time complexity, on a single GPU, and without compromise
in quality.

</details>


### [4] [Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions](https://arxiv.org/abs/2507.01123)
*Rahul A. Burange,Harsh K. Shinde,Omkar Mutyalwar*

Main category: cs.CV

TL;DR: 该研究提出了一种结合多源卫星影像和深度学习模型的方法，以提高滑坡识别和预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 滑坡对基础设施、经济和人类生命构成严重威胁，需要准确检测和预测。

Method: 利用Sentinel-2多光谱数据和ALOS PALSAR衍生的坡度及DEM层，结合多种地理空间分析技术和深度学习模型（如U-Net、DeepLabV3+和Res-Net）进行滑坡检测。

Result: 研究结果为开发可靠的早期预警系统、改进灾害风险管理和可持续土地利用规划提供了有价值的见解。

Conclusion: 深度学习与多源遥感技术结合在构建稳健、可扩展和可迁移的滑坡预测模型中具有巨大潜力。

Abstract: Landslides pose severe threats to infrastructure, economies, and human lives,
necessitating accurate detection and predictive mapping across diverse
geographic regions. With advancements in deep learning and remote sensing,
automated landslide detection has become increasingly effective. This study
presents a comprehensive approach integrating multi-source satellite imagery
and deep learning models to enhance landslide identification and prediction. We
leverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and
Digital Elevation Model (DEM) layers to capture critical environmental features
influencing landslide occurrences. Various geospatial analysis techniques are
employed to assess the impact of terra in characteristics, vegetation cover,
and rainfall on detection accuracy. Additionally, we evaluate the performance
of multiple stateof-the-art deep learning segmentation models, including U-Net,
DeepLabV3+, and Res-Net, to determine their effectiveness in landslide
detection. The proposed framework contributes to the development of reliable
early warning systems, improved disaster risk management, and sustainable
land-use planning. Our findings provide valuable insights into the potential of
deep learning and multi-source remote sensing in creating robust, scalable, and
transferable landslide prediction models.

</details>


### [5] [cp_measure: API-first feature extraction for image-based profiling workflows](https://arxiv.org/abs/2507.01163)
*Alán F. Muñoz,Tim Treis,Alexandr A. Kalinin,Shatavisha Dasgupta,Fabian Theis,Anne E. Carpenter,Shantanu Singh*

Main category: cs.CV

TL;DR: cp_measure是一个Python库，将CellProfiler的核心测量功能模块化，便于程序化特征提取，支持机器学习工作流。


<details>
  <summary>Details</summary>
Motivation: 当前工具（如CellProfiler）在自动化和可重复性分析方面存在障碍，阻碍了机器学习工作流。

Method: 提取CellProfiler的核心测量功能，设计为模块化、API优先的工具。

Result: cp_measure特征与CellProfiler特征高度一致，并能无缝集成到Python生态系统中。

Conclusion: cp_measure支持可重复、自动化的图像分析流程，适用于计算生物学中的机器学习应用。

Abstract: Biological image analysis has traditionally focused on measuring specific
visual properties of interest for cells or other entities. A complementary
paradigm gaining increasing traction is image-based profiling - quantifying
many distinct visual features to form comprehensive profiles which may reveal
hidden patterns in cellular states, drug responses, and disease mechanisms.
While current tools like CellProfiler can generate these feature sets, they
pose significant barriers to automated and reproducible analyses, hindering
machine learning workflows. Here we introduce cp_measure, a Python library that
extracts CellProfiler's core measurement capabilities into a modular, API-first
tool designed for programmatic feature extraction. We demonstrate that
cp_measure features retain high fidelity with CellProfiler features while
enabling seamless integration with the scientific Python ecosystem. Through
applications to 3D astrocyte imaging and spatial transcriptomics, we showcase
how cp_measure enables reproducible, automated image-based profiling pipelines
that scale effectively for machine learning applications in computational
biology.

</details>


### [6] [Rapid Salient Object Detection with Difference Convolutional Neural Networks](https://arxiv.org/abs/2507.01182)
*Zhuo Su,Li Liu,Matthias Müller,Jiehua Zhang,Diana Wofk,Ming-Ming Cheng,Matti Pietikäinen*

Main category: cs.CV

TL;DR: 提出了一种高效的显著目标检测（SOD）网络设计，结合传统方法与现代CNN，通过像素差异卷积（PDC）和差异卷积重参数化（DCR）提升效率，并在资源受限设备上实现实时性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有显著目标检测模型在资源受限设备上计算成本高的问题，实现实时性能。

Method: 结合传统SOD方法和现代CNN，使用PDC编码特征对比，引入DCR策略减少推理计算量，并扩展至视频SOD的STDC。

Result: 模型SDNet和STDNet在Jetson Orin设备上分别以46 FPS和150 FPS运行，参数少于1M，速度和精度均优于其他轻量模型。

Conclusion: 提出的方法在效率和准确性之间取得了显著平衡，适用于资源受限设备的实时SOD任务。

Abstract: This paper addresses the challenge of deploying salient object detection
(SOD) on resource-constrained devices with real-time performance. While recent
advances in deep neural networks have improved SOD, existing top-leading models
are computationally expensive. We propose an efficient network design that
combines traditional wisdom on SOD and the representation power of modern CNNs.
Like biologically-inspired classical SOD methods relying on computing contrast
cues to determine saliency of image regions, our model leverages Pixel
Difference Convolutions (PDCs) to encode the feature contrasts. Differently,
PDCs are incorporated in a CNN architecture so that the valuable contrast cues
are extracted from rich feature maps. For efficiency, we introduce a difference
convolution reparameterization (DCR) strategy that embeds PDCs into standard
convolutions, eliminating computation and parameters at inference.
Additionally, we introduce SpatioTemporal Difference Convolution (STDC) for
video SOD, enhancing the standard 3D convolution with spatiotemporal contrast
capture. Our models, SDNet for image SOD and STDNet for video SOD, achieve
significant improvements in efficiency-accuracy trade-offs. On a Jetson Orin
device, our models with $<$ 1M parameters operate at 46 FPS and 150 FPS on
streamed images and videos, surpassing the second-best lightweight models in
our experiments by more than $2\times$ and $3\times$ in speed with superior
accuracy. Code will be available at https://github.com/hellozhuo/stdnet.git.

</details>


### [7] [Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using Hölder Divergence and Mutual Information-Enhanced Knowledge Transfer](https://arxiv.org/abs/2507.01254)
*Runze Cheng,Xihang Qiu,Ming Li,Ye Zhang,Chun Li,Fei Yu*

Main category: cs.CV

TL;DR: 提出一种鲁棒的单模态并行处理框架，用于处理多模态MRI中缺失模态的脑肿瘤分割问题。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在多模态MRI中因缺失某些模态（如图像质量、协议不一致、患者过敏或财务限制）而导致的性能下降问题。

Method: 利用Holder散度和互信息，设计了一个动态调整网络参数的模型，保持模态特异性特征。

Result: 在BraTS 2018和BraTS 2020数据集上表现优于现有方法，尤其在处理缺失模态时。

Conclusion: 该框架通过散度和信息损失函数有效量化预测与真实标签的差异，实现了高精度的分割。

Abstract: Multimodal MRI provides critical complementary information for accurate brain
tumor segmentation. However, conventional methods struggle when certain
modalities are missing due to issues such as image quality, protocol
inconsistencies, patient allergies, or financial constraints. To address this,
we propose a robust single-modality parallel processing framework that achieves
high segmentation accuracy even with incomplete modalities. Leveraging Holder
divergence and mutual information, our model maintains modality-specific
features while dynamically adjusting network parameters based on the available
inputs. By using these divergence- and information-based loss functions, the
framework effectively quantifies discrepancies between predictions and
ground-truth labels, resulting in consistently accurate segmentation. Extensive
evaluations on the BraTS 2018 and BraTS 2020 datasets demonstrate superior
performance over existing methods in handling missing modalities.

</details>


### [8] [AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation](https://arxiv.org/abs/2507.01255)
*Xiao Liu,Jiawei Zhang*

Main category: cs.CV

TL;DR: AIGVE-MACS是一个统一的AI生成视频评估模型，提供数值评分和多方面语言评论反馈，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标缺乏解释性，与人类评价对齐性低，需要更全面的评估框架。

Method: 结合Vision-Language Models、新颖的token-wise加权损失和动态帧采样策略，利用AIGVE-BENCH 2数据集。

Result: 在评分相关性和评论质量上达到SOTA，视频生成质量提升53.5%。

Conclusion: AIGVE-MACS为AI生成视频评估建立了全面、人类对齐的新范式。

Abstract: The rapid advancement of AI-generated video models has created a pressing
need for robust and interpretable evaluation frameworks. Existing metrics are
limited to producing numerical scores without explanatory comments, resulting
in low interpretability and human evaluation alignment. To address those
challenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video
Evaluation(AIGVE), which can provide not only numerical scores but also
multi-aspect language comment feedback in evaluating these generated videos.
Central to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising
2,500 AI-generated videos and 22,500 human-annotated detailed comments and
numerical scores across nine critical evaluation aspects. Leveraging
AIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a
novel token-wise weighted loss and a dynamic frame sampling strategy to better
align with human evaluators. Comprehensive experiments across supervised and
zero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art
performance in both scoring correlation and comment quality, significantly
outperforming prior baselines including GPT-4o and VideoScore. In addition, we
further showcase a multi-agent refinement framework where feedback from
AIGVE-MACS drives iterative improvements in video generation, leading to 53.5%
quality enhancement. This work establishes a new paradigm for comprehensive,
human-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2
and AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.

</details>


### [9] [Advancements in Weed Mapping: A Systematic Review](https://arxiv.org/abs/2507.01269)
*Mohammad Jahanbakht,Alex Olsen,Ross Marchant,Emilie Fillols,Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: 本文综述了杂草测绘的最新方法，填补了从数据采集到处理技术的全面文献空白，为可持续杂草管理提供指导。


<details>
  <summary>Details</summary>
Motivation: 杂草测绘对精准管理和减少环境影响至关重要，但缺乏全面的文献综述和分析。

Method: 系统评估数据采集（传感器与平台技术）、数据处理（标注与建模）和测绘技术（时空分析与决策工具）。

Result: 总结了杂草测绘领域的关键发现，提升了空间和时间分辨率。

Conclusion: 为未来研究和可持续杂草管理系统的发展提供了基础参考。

Abstract: Weed mapping plays a critical role in precision management by providing
accurate and timely data on weed distribution, enabling targeted control and
reduced herbicide use. This minimizes environmental impacts, supports
sustainable land management, and improves outcomes across agricultural and
natural environments. Recent advances in weed mapping leverage ground-vehicle
Red Green Blue (RGB) cameras, satellite and drone-based remote sensing combined
with sensors such as spectral, Near Infra-Red (NIR), and thermal cameras. The
resulting data are processed using advanced techniques including big data
analytics and machine learning, significantly improving the spatial and
temporal resolution of weed maps and enabling site-specific management
decisions. Despite a growing body of research in this domain, there is a lack
of comprehensive literature reviews specifically focused on weed mapping. In
particular, the absence of a structured analysis spanning the entire mapping
pipeline, from data acquisition to processing techniques and mapping tools,
limits progress in the field. This review addresses these gaps by
systematically examining state-of-the-art methods in data acquisition (sensor
and platform technologies), data processing (including annotation and
modelling), and mapping techniques (such as spatiotemporal analysis and
decision support tools). Following PRISMA guidelines, we critically evaluate
and synthesize key findings from the literature to provide a holistic
understanding of the weed mapping landscape. This review serves as a
foundational reference to guide future research and support the development of
efficient, scalable, and sustainable weed management systems.

</details>


### [10] [Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing](https://arxiv.org/abs/2507.01275)
*Chengxu Liu,Lu Qi,Jinshan Pan,Xueming Qian,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 提出了一种基于频域的扩散模型（\ours），用于无配对图像去雾，通过振幅残差编码器和相位校正模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于对比学习的方法引入无关内容且忽略频域特性，导致去雾效果不佳。

Method: 利用扩散模型从频域重建角度处理去雾任务，提出振幅残差编码器（ARE）和相位校正模块（PCM）。

Result: 在合成和真实数据集上优于现有方法。

Conclusion: 频域扩散模型能有效利用无配对清晰数据，提升去雾性能。

Abstract: Unpaired image dehazing has attracted increasing attention due to its
flexible data requirements during model training. Dominant methods based on
contrastive learning not only introduce haze-unrelated content information, but
also ignore haze-specific properties in the frequency domain (\ie,~haze-related
degradation is mainly manifested in the amplitude spectrum). To address these
issues, we propose a novel frequency domain-based diffusion model, named \ours,
for fully exploiting the beneficial knowledge in unpaired clear data. In
particular, inspired by the strong generative ability shown by Diffusion Models
(DMs), we tackle the dehazing task from the perspective of frequency domain
reconstruction and perform the DMs to yield the amplitude spectrum consistent
with the distribution of clear images. To implement it, we propose an Amplitude
Residual Encoder (ARE) to extract the amplitude residuals, which effectively
compensates for the amplitude gap from the hazy to clear domains, as well as
provide supervision for the DMs training. In addition, we propose a Phase
Correction Module (PCM) to eliminate artifacts by further refining the phase
spectrum during dehazing with a simple attention mechanism. Experimental
results demonstrate that our \ours outperforms other state-of-the-art methods
on both synthetic and real-world datasets.

</details>


### [11] [Learning an Ensemble Token from Task-driven Priors in Facial Analysis](https://arxiv.org/abs/2507.01290)
*Sunyong Seo,Semin Kim,Jongha Lee*

Main category: cs.CV

TL;DR: ET-Fuser是一种新方法，通过利用基于预训练模型任务先验的注意力机制，学习集成令牌，以统一特征表示。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单任务学习中缺乏统一的特征表示，ET-Fuser旨在解决这一问题。

Method: 提出了一种基于自注意力机制的集成令牌学习方法，共享预训练编码器的互信息。

Result: 在各种面部分析任务中表现提升，特征表示显著增强。

Conclusion: ET-Fuser高效且计算成本低，为面部分析提供了统一的特征表示方法。

Abstract: Facial analysis exhibits task-specific feature variations. While
Convolutional Neural Networks (CNNs) have enabled the fine-grained
representation of spatial information, Vision Transformers (ViTs) have
facilitated the representation of semantic information at the patch level.
Although the generalization of conventional methodologies has advanced visual
interpretability, there remains paucity of research that preserves the unified
feature representation on single task learning during the training process. In
this work, we introduce ET-Fuser, a novel methodology for learning ensemble
token by leveraging attention mechanisms based on task priors derived from
pre-trained models for facial analysis. Specifically, we propose a robust prior
unification learning method that generates a ensemble token within a
self-attention mechanism, which shares the mutual information along the
pre-trained encoders. This ensemble token approach offers high efficiency with
negligible computational cost. Our results show improvements across a variety
of facial analysis, with statistically significant enhancements observed in the
feature representations.

</details>


### [12] [Physics-informed Ground Reaction Dynamics from Human Motion Capture](https://arxiv.org/abs/2507.01340)
*Cuong Le,Huy-Phuong Le,Duc Le,Minh-Thien Duong,Van-Binh Nguyen,My-Ha Le*

Main category: cs.CV

TL;DR: 提出了一种基于物理约束的新方法，直接从运动捕捉数据估计地面反作用力，结合欧拉积分和PD算法，提高了估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖实验室专用的力板设备，限制了人体动力学学习的广泛应用。

Method: 结合物理定律和计算模拟，使用欧拉积分和PD算法从运动捕捉数据计算地面反作用力。

Result: 在GroundLink数据集上测试，优于基线模型，提高了地面反作用力估计和根轨迹精度。

Conclusion: 该方法通过物理约束提高了估计精度，为人体动力学分析提供了更灵活的工具。

Abstract: Body dynamics are crucial information for the analysis of human motions in
important research fields, ranging from biomechanics, sports science to
computer vision and graphics. Modern approaches collect the body dynamics,
external reactive force specifically, via force plates, synchronizing with
human motion capture data, and learn to estimate the dynamics from a black-box
deep learning model. Being specialized devices, force plates can only be
installed in laboratory setups, imposing a significant limitation on the
learning of human dynamics. To this end, we propose a novel method for
estimating human ground reaction dynamics directly from the more reliable
motion capture data with physics laws and computational simulation as
constrains. We introduce a highly accurate and robust method for computing
ground reaction forces from motion capture data using Euler's integration
scheme and PD algorithm. The physics-based reactive forces are used to inform
the learning model about the physics-informed motion dynamics thus improving
the estimation accuracy. The proposed approach was tested on the GroundLink
dataset, outperforming the baseline model on: 1) the ground reaction force
estimation accuracy compared to the force plates measurement; and 2) our
simulated root trajectory precision. The implementation code is available at
https://github.com/cuongle1206/Phys-GRD

</details>


### [13] [Learning Camera-Agnostic White-Balance Preferences](https://arxiv.org/abs/2507.01342)
*Luxi Zhao,Mahmoud Afifi,Michael S. Brown*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级方法，通过学习后照明估计映射，将中性白平衡校正转换为美学偏好校正，实现跨相机一致的颜色渲染。


<details>
  <summary>Details</summary>
Motivation: 商业自动白平衡（AWB）系统通常追求美学偏好而非中性颜色校正，且现有学习方法的跨相机泛化能力不足。

Method: 提出一种后照明估计映射方法，将中性白平衡转换为美学偏好校正，适用于不同相机传感器。

Result: 模型仅含约500参数，运行时间为0.024毫秒，在771张来自三款相机的智能手机图像上表现优异。

Conclusion: 该方法在保持轻量级的同时，实现了跨相机美学一致性，兼容现有技术且计算开销低。

Abstract: The image signal processor (ISP) pipeline in modern cameras consists of
several modules that transform raw sensor data into visually pleasing images in
a display color space. Among these, the auto white balance (AWB) module is
essential for compensating for scene illumination. However, commercial AWB
systems often strive to compute aesthetic white-balance preferences rather than
accurate neutral color correction. While learning-based methods have improved
AWB accuracy, they typically struggle to generalize across different camera
sensors -- an issue for smartphones with multiple cameras. Recent work has
explored cross-camera AWB, but most methods remain focused on achieving neutral
white balance. In contrast, this paper is the first to address aesthetic
consistency by learning a post-illuminant-estimation mapping that transforms
neutral illuminant corrections into aesthetically preferred corrections in a
camera-agnostic space. Once trained, our mapping can be applied after any
neutral AWB module to enable consistent and stylized color rendering across
unseen cameras. Our proposed model is lightweight -- containing only $\sim$500
parameters -- and runs in just 0.024 milliseconds on a typical flagship mobile
CPU. Evaluated on a dataset of 771 smartphone images from three different
cameras, our method achieves state-of-the-art performance while remaining fully
compatible with existing cross-camera AWB techniques, introducing minimal
computational and memory overhead.

</details>


### [14] [Learning from Random Subspace Exploration: Generalized Test-Time Augmentation with Self-supervised Distillation](https://arxiv.org/abs/2507.01347)
*Andrei Jelea,Ahmed Nabil Belbachir,Marius Leordeanu*

Main category: cs.CV

TL;DR: GTTA是一种通用的测试时间增强方法，通过扰动PCA子空间投影形成鲁棒集成，并结合自监督学习优化模型性能，适用于多种视觉和非视觉任务。


<details>
  <summary>Details</summary>
Motivation: 现有测试时间增强方法通用性不足，GTTA旨在提供一种通用且高效的方法，提升模型性能并减少计算成本。

Method: GTTA通过随机扰动PCA子空间投影形成集成，并引入自监督学习阶段，利用集成输出优化初始模型。

Result: GTTA在多种任务（如图像分类、分割、语音识别）中表现优异，并在特定任务（如低能见度水下视频中的三文鱼检测）中验证了其有效性。

Conclusion: GTTA是一种通用且高效的测试时间增强方法，显著提升了模型性能并降低了计算成本。

Abstract: We introduce Generalized Test-Time Augmentation (GTTA), a highly effective
method for improving the performance of a trained model, which unlike other
existing Test-Time Augmentation approaches from the literature is general
enough to be used off-the-shelf for many vision and non-vision tasks, such as
classification, regression, image segmentation and object detection. By
applying a new general data transformation, that randomly perturbs multiple
times the PCA subspace projection of a test input, GTTA forms robust ensembles
at test time in which, due to sound statistical properties, the structural and
systematic noises in the initial input data is filtered out and final estimator
errors are reduced. Different from other existing methods, we also propose a
final self-supervised learning stage in which the ensemble output, acting as an
unsupervised teacher, is used to train the initial single student model, thus
reducing significantly the test time computational cost, at no loss in
accuracy. Our tests and comparisons to strong TTA approaches and SoTA models on
various vision and non-vision well-known datasets and tasks, such as image
classification and segmentation, speech recognition and house price prediction,
validate the generality of the proposed GTTA. Furthermore, we also prove its
effectiveness on the more specific real-world task of salmon segmentation and
detection in low-visibility underwater videos, for which we introduce
DeepSalmon, the largest dataset of its kind in the literature.

</details>


### [15] [Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model](https://arxiv.org/abs/2507.01351)
*Chaoxiang Cai,Longrong Yang,Kaibing Chen,Fan Yang,Xi Li*

Main category: cs.CV

TL;DR: 提出了一种长尾分布感知路由器（LTDR），用于视觉语言模型的专家混合（MoE）框架，解决了模态间分布差异和视觉尾部令牌激活问题。


<details>
  <summary>Details</summary>
Motivation: 现有MoE框架专注于令牌到专家的路由（TER），但忽略了视觉和语言之间的分布差异，导致性能受限。

Method: 提出LTDR，包括模态特定路由策略（针对语言和视觉的不同分布）和视觉尾部令牌的专家激活增强策略。

Result: 在多个基准测试中验证了LTDR的有效性。

Conclusion: LTDR通过考虑模态间分布差异和增强尾部令牌处理，提升了视觉语言模型的性能。

Abstract: The mixture-of-experts (MoE), which replaces dense models with sparse
architectures, has gained attention in large vision-language models (LVLMs) for
achieving comparable performance with fewer activated parameters. Existing MoE
frameworks for LVLMs focus on token-to-expert routing (TER), encouraging
different experts to specialize in processing distinct tokens. However, these
frameworks often rely on the load balancing mechanism, overlooking the inherent
distributional differences between vision and language. To this end, we propose
a Long-Tailed Distribution-aware Router (LTDR) for vision-language TER,
tackling two challenges: (1) Distribution-aware router for modality-specific
routing. We observe that language TER follows a uniform distribution, whereas
vision TER exhibits a long-tailed distribution. This discrepancy necessitates
distinct routing strategies tailored to each modality. (2) Enhancing expert
activation for vision tail tokens. Recognizing the importance of vision tail
tokens, we introduce an oversampling-like strategy by increasing the number of
activated experts for these tokens. Experiments on extensive benchmarks
validate the effectiveness of our approach.

</details>


### [16] [3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation](https://arxiv.org/abs/2507.01367)
*Tianrui Lou,Xiaojun Jia,Siyuan Liang,Jiawei Liang,Ming Zhang,Yanjun Xiao,Xiaochun Cao*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯散射（3DGS）的物理攻击框架PGA，用于生成多视角鲁棒的对抗性伪装，解决了现有方法依赖模拟器和网格先验的问题。


<details>
  <summary>Details</summary>
Motivation: 物理对抗攻击在安全关键场景（如自动驾驶）中威胁巨大，但现有方法依赖模拟器和网格先验，效率低且与现实差异大，且多视角鲁棒性不足。

Method: 利用3DGS实现快速精确的3D重建和逼真渲染，通过防止高斯间的相互遮挡和自遮挡，结合min-max优化调整背景，提升对抗效果和多视角鲁棒性。

Result: 实验验证了PGA的有效性和优越性，显著提升了对抗攻击的鲁棒性和跨视角一致性。

Conclusion: PGA框架为物理对抗攻击提供了一种高效且鲁棒的解决方案，适用于复杂现实环境。

Abstract: Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to obtain and inevitably differ from the
real world. Moreover, due to the limitations of the backgrounds in training
images, previous methods often fail to produce multi-view robust adversarial
camouflage and tend to fall into sub-optimal solutions. Due to these reasons,
prior work lacks adversarial effectiveness and robustness across diverse
viewpoints and physical environments. We propose a physical attack framework
based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and
precise reconstruction with few images, along with photo-realistic rendering
capabilities. Our framework further enhances cross-view robustness and
adversarial effectiveness by preventing mutual and self-occlusion among
Gaussians and employing a min-max optimization approach that adjusts the
imaging background of each viewpoint, helping the algorithm filter out
non-robust adversarial features. Extensive experiments validate the
effectiveness and superiority of PGA. Our code is available
at:https://github.com/TRLou/PGA.

</details>


### [17] [Activation Reward Models for Few-Shot Model Alignment](https://arxiv.org/abs/2507.01368)
*Tianning Chai,Chancharik Mitra,Brandon Huang,Gautam Rajendrakumar Gare,Zhiqiu Lin,Assaf Arbelle,Leonid Karlinsky,Rogerio Feris,Trevor Darrell,Deva Ramanan,Roei Herzig*

Main category: cs.CV

TL;DR: 提出了一种名为Activation RMs的少样本奖励建模方法，通过激活导向构建对齐奖励信号，无需额外微调，性能优于现有方法，并在安全关键应用中有效减少奖励攻击行为。


<details>
  <summary>Details</summary>
Motivation: 对齐大型语言模型（LLMs）和大型多模态模型（LMMs）的生成输出与人类偏好是提升模型实际应用质量的核心挑战。传统奖励建模难以适应新偏好，需独立奖励模型和大规模偏好数据集。

Method: 引入Activation RMs，利用激活导向构建奖励信号，仅需少量监督且无需额外模型微调。

Result: Activation RMs在标准奖励建模基准上优于现有少样本方法（如LLM-as-a-judge、投票评分等），并在新提出的PreferenceHack基准上表现最佳，甚至超越GPT-4o。

Conclusion: Activation RMs为少样本奖励建模提供了高效解决方案，适用于安全关键应用，并在对抗奖励攻击行为中表现出色。

Abstract: Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to
human preferences is a central challenge in improving the quality of the
models' generative outputs for real-world applications. A common approach is to
use reward modeling to encode preferences, enabling alignment via post-training
using reinforcement learning. However, traditional reward modeling is not
easily adaptable to new preferences because it requires a separate reward
model, commonly trained on large preference datasets. To address this, we
introduce Activation Reward Models (Activation RMs) -- a novel few-shot reward
modeling method that leverages activation steering to construct well-aligned
reward signals using minimal supervision and no additional model finetuning.
Activation RMs outperform existing few-shot reward modeling approaches such as
LLM-as-a-judge with in-context learning, voting-based scoring, and token
probability scoring on standard reward modeling benchmarks. Furthermore, we
demonstrate the effectiveness of Activation RMs in mitigating reward hacking
behaviors, highlighting their utility for safety-critical applications. Toward
this end, we propose PreferenceHack, a novel few-shot setting benchmark, the
first to test reward models on reward hacking in a paired preference format.
Finally, we show that Activation RM achieves state-of-the-art performance on
this benchmark, surpassing even GPT-4o.

</details>


### [18] [DiffMark: Diffusion-based Robust Watermark Against Deepfakes](https://arxiv.org/abs/2507.01428)
*Chen Sun,Haiyang Sun,Zhiqing Guo,Yunfeng Diao,Liejun Wang,Dan Ma,Gaobo Yang,Keqin Li*

Main category: cs.CV

TL;DR: DiffMark是一种基于扩散模型的鲁棒水印框架，用于对抗Deepfake的面部操纵，通过改进训练和采样方案，结合交叉信息融合模块和对抗性指导，生成更鲁棒的水印图像。


<details>
  <summary>Details</summary>
Motivation: 现有的水印方法在对抗Deepfake操纵时鲁棒性不足，扩散模型在图像生成中的优异性能为水印融合提供了新思路。

Method: 通过修改扩散模型的训练和采样方案，结合面部图像和水印条件，使用交叉信息融合模块和对抗性指导增强水印鲁棒性。

Result: 实验证明DiffMark在典型Deepfake攻击下具有有效性。

Conclusion: DiffMark为对抗Deepfake操纵提供了一种鲁棒的水印解决方案。

Abstract: Deepfakes pose significant security and privacy threats through malicious
facial manipulations. While robust watermarking can aid in authenticity
verification and source tracking, existing methods often lack the sufficient
robustness against Deepfake manipulations. Diffusion models have demonstrated
remarkable performance in image generation, enabling the seamless fusion of
watermark with image during generation. In this study, we propose a novel
robust watermarking framework based on diffusion model, called DiffMark. By
modifying the training and sampling scheme, we take the facial image and
watermark as conditions to guide the diffusion model to progressively denoise
and generate corresponding watermarked image. In the construction of facial
condition, we weight the facial image by a timestep-dependent factor that
gradually reduces the guidance intensity with the decrease of noise, thus
better adapting to the sampling process of diffusion model. To achieve the
fusion of watermark condition, we introduce a cross information fusion (CIF)
module that leverages a learnable embedding table to adaptively extract
watermark features and integrates them with image features via cross-attention.
To enhance the robustness of the watermark against Deepfake manipulations, we
integrate a frozen autoencoder during training phase to simulate Deepfake
manipulations. Additionally, we introduce Deepfake-resistant guidance that
employs specific Deepfake model to adversarially guide the diffusion sampling
process to generate more robust watermarked images. Experimental results
demonstrate the effectiveness of the proposed DiffMark on typical Deepfakes.
Our code will be available at https://github.com/vpsg-research/DiffMark.

</details>


### [19] [Active Measurement: Efficient Estimation at Scale](https://arxiv.org/abs/2507.01372)
*Max Hamilton,Jinlin Lai,Wenlong Zhao,Subhransu Maji,Daniel Sheldon*

Main category: cs.CV

TL;DR: 提出了一种名为“主动测量”的人机协作AI框架，用于科学测量，通过重要性采样优化模型和估计精度。


<details>
  <summary>Details</summary>
Motivation: 当前AI在科学发现中虽能高效分析大数据，但缺乏准确性和统计保证，需要改进。

Method: 结合AI模型预测和人类标注，采用重要性采样优化模型，并通过蒙特卡洛估计提供无偏测量。

Result: 主动测量能在AI模型不完美时提供精确估计，并在模型准确时减少人工工作量。

Conclusion: 该方法在多个测量任务中优于替代方案，降低了估计误差。

Abstract: AI has the potential to transform scientific discovery by analyzing vast
datasets with little human effort. However, current workflows often do not
provide the accuracy or statistical guarantees that are needed. We introduce
active measurement, a human-in-the-loop AI framework for scientific
measurement. An AI model is used to predict measurements for individual units,
which are then sampled for human labeling using importance sampling. With each
new set of human labels, the AI model is improved and an unbiased Monte Carlo
estimate of the total measurement is refined. Active measurement can provide
precise estimates even with an imperfect AI model, and requires little human
effort when the AI model is very accurate. We derive novel estimators,
weighting schemes, and confidence intervals, and show that active measurement
reduces estimation error compared to alternatives in several measurement tasks.

</details>


### [20] [Towards Controllable Real Image Denoising with Camera Parameters](https://arxiv.org/abs/2507.01587)
*Youngjin Oh,Junhyeong Kwon,Keuntek Lee,Nam Ik Cho*

Main category: cs.CV

TL;DR: 提出了一种基于相机参数的可控去噪框架，通过ISO、快门速度和光圈值调整去噪强度，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法缺乏根据噪声水平、相机设置和用户偏好调整去噪强度的灵活性。

Method: 将ISO、快门速度和光圈值转换为向量，控制去噪网络的性能。

Result: 实验表明，该方法为标准去噪网络增加了可控性并提升了性能。

Conclusion: 该框架成功实现了基于相机参数的自适应去噪，代码已开源。

Abstract: Recent deep learning-based image denoising methods have shown impressive
performance; however, many lack the flexibility to adjust the denoising
strength based on the noise levels, camera settings, and user preferences. In
this paper, we introduce a new controllable denoising framework that adaptively
removes noise from images by utilizing information from camera parameters.
Specifically, we focus on ISO, shutter speed, and F-number, which are closely
related to noise levels. We convert these selected parameters into a vector to
control and enhance the performance of the denoising network. Experimental
results show that our method seamlessly adds controllability to standard
denoising neural networks and improves their performance. Code is available at
https://github.com/OBAKSA/CPADNet.

</details>


### [21] [MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing](https://arxiv.org/abs/2507.01384)
*Langyu Wang,Bingke Zhu,Yingying Chen,Yiyuan Zhang,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: 提出了一种基于伪标签增强的音频-视觉Mamba网络（MUG），用于改进弱监督音频-视觉视频解析任务中的段级和事件级预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在弱监督和模型架构的限制下，难以同时提升段级和事件级预测性能。

Method: 通过伪标签增强生成新数据，并使用音频-视觉Mamba网络进行特征处理和交互。

Result: 在LLP数据集上，MUG在所有指标上均优于现有方法（如视觉段级和音频段级指标分别提升2.1%和1.2%）。

Conclusion: MUG通过伪标签增强和Mamba网络有效提升了音频-视觉视频解析的性能。

Abstract: The weakly-supervised audio-visual video parsing (AVVP) aims to predict all
modality-specific events and locate their temporal boundaries. Despite
significant progress, due to the limitations of the weakly-supervised and the
deficiencies of the model architecture, existing methods are lacking in
simultaneously improving both the segment-level prediction and the event-level
prediction. In this work, we propose a audio-visual Mamba network with pseudo
labeling aUGmentation (MUG) for emphasising the uniqueness of each segment and
excluding the noise interference from the alternate modalities. Specifically,
we annotate some of the pseudo-labels based on previous work. Using unimodal
pseudo-labels, we perform cross-modal random combinations to generate new data,
which can enhance the model's ability to parse various segment-level event
combinations. For feature processing and interaction, we employ a audio-visual
mamba network. The AV-Mamba enhances the ability to perceive different segments
and excludes additional modal noise while sharing similar modal information.
Our extensive experiments demonstrate that MUG improves state-of-the-art
results on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of
visual Segment-level and audio Segment-level metrics). Our code is available at
https://github.com/WangLY136/MUG.

</details>


### [22] [Perception-Oriented Latent Coding for High-Performance Compressed Domain Semantic Inference](https://arxiv.org/abs/2507.01608)
*Xu Zhang,Ming Lu,Yan Chen,Zhan Ma*

Main category: cs.CV

TL;DR: POLC提出了一种感知导向的潜在编码方法，解决了传统MSE优化方法在语义丰富性和计算效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统基于MSE优化的图像编码模型在潜在空间语义丰富性上表现不足，且需要大量计算资源进行微调。

Method: 提出了POLC方法，通过丰富潜在特征的语义内容，仅需一个即插即用的适配器进行微调。

Result: POLC在保持与生成式图像编码方法相当的率感知性能的同时，显著提升了视觉任务性能，且微调开销极小。

Conclusion: POLC为压缩域语义推理提供了一种高效且语义丰富的方法。

Abstract: In recent years, compressed domain semantic inference has primarily relied on
learned image coding models optimized for mean squared error (MSE). However,
MSE-oriented optimization tends to yield latent spaces with limited semantic
richness, which hinders effective semantic inference in downstream tasks.
Moreover, achieving high performance with these models often requires
fine-tuning the entire vision model, which is computationally intensive,
especially for large models. To address these problems, we introduce
Perception-Oriented Latent Coding (POLC), an approach that enriches the
semantic content of latent features for high-performance compressed domain
semantic inference. With the semantically rich latent space, POLC requires only
a plug-and-play adapter for fine-tuning, significantly reducing the parameter
count compared to previous MSE-oriented methods. Experimental results
demonstrate that POLC achieves rate-perception performance comparable to
state-of-the-art generative image coding methods while markedly enhancing
performance in vision tasks, with minimal fine-tuning overhead. Code is
available at https://github.com/NJUVISION/POLC.

</details>


### [23] [FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases](https://arxiv.org/abs/2507.01390)
*Shuai Tan,Bill Gong,Bin Ji,Ye Pan*

Main category: cs.CV

TL;DR: FixTalk框架通过解耦身份信息和运动特征，同时解决身份泄漏和渲染伪影问题，提升说话头部生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在极端情况下存在身份泄漏（IL）和渲染伪影（RA）问题，影响生成质量。

Method: 提出增强运动指示器（EMI）解耦身份信息，增强细节指示器（EDI）利用泄漏信息补充细节。

Result: 实验表明FixTalk有效减少IL和RA，性能优于现有方法。

Conclusion: FixTalk通过创新设计解决了说话头部生成中的关键问题，为高质量渲染提供了新思路。

Abstract: Talking head generation is gaining significant importance across various
domains, with a growing demand for high-quality rendering. However, existing
methods often suffer from identity leakage (IL) and rendering artifacts (RA),
particularly in extreme cases. Through an in-depth analysis of previous
approaches, we identify two key insights: (1) IL arises from identity
information embedded within motion features, and (2) this identity information
can be leveraged to address RA. Building on these findings, this paper
introduces FixTalk, a novel framework designed to simultaneously resolve both
issues for high-quality talking head generation. Firstly, we propose an
Enhanced Motion Indicator (EMI) to effectively decouple identity information
from motion features, mitigating the impact of IL on generated talking heads.
To address RA, we introduce an Enhanced Detail Indicator (EDI), which utilizes
the leaked identity information to supplement missing details, thus fixing the
artifacts. Extensive experiments demonstrate that FixTalk effectively mitigates
IL and RA, achieving superior performance compared to state-of-the-art methods.

</details>


### [24] [Using Wavelet Domain Fingerprints to Improve Source Camera Identification](https://arxiv.org/abs/2507.01712)
*Xinle Tian,Matthew Nunes,Emiko Dupont,Shaunagh Downing,Freddie Lichtenstein,Matt Burns*

Main category: cs.CV

TL;DR: 提出了一种基于小波域的相机指纹提取方法，避免了传统方法的逆变换步骤，提高了检测精度和处理速度。


<details>
  <summary>Details</summary>
Motivation: 传统的小波去噪方法在提取传感器模式噪声（SPN）时需要将指纹构建为图像，并涉及逆变换步骤，效率较低。

Method: 提出小波域指纹概念，直接在频域进行指纹提取和比较，避免了逆变换步骤。

Result: 在真实数据集上的实验表明，该方法不仅检测精度更高，还能显著提升处理速度。

Conclusion: 该方法简化了提取和比较流程，为相机指纹检测提供了更高效的解决方案。

Abstract: Camera fingerprint detection plays a crucial role in source identification
and image forensics, with wavelet denoising approaches proving to be
particularly effective in extracting sensor pattern noise (SPN). In this
article, we propose a modification to wavelet-based SPN extraction. Rather than
constructing the fingerprint as an image, we introduce the notion of a wavelet
domain fingerprint. This avoids the final inversion step of the denoising
algorithm and allows fingerprint comparisons to be made directly in the wavelet
domain. As such, our modification streamlines the extraction and comparison
process. Experimental results on real-world datasets demonstrate that our
method not only achieves higher detection accuracy but can also significantly
improve processing speed.

</details>


### [25] [Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps](https://arxiv.org/abs/2507.01397)
*Khanh Son Pham,Christian Witte,Jens Behley,Johannes Betz,Cyrill Stachniss*

Main category: cs.CV

TL;DR: 提出了一种利用标准地图（SD）信息预测车道段及其拓扑和道路边界的方法，通过混合编码和去噪技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决高复杂度道路拓扑在线构建的挑战，减少对高精地图（HD）的依赖。

Method: 结合先验地图信息（SD地图）和去噪技术，设计网络架构，利用历史帧保证时间一致性。

Result: 实验表明，该方法显著优于现有方法。

Conclusion: 提出的建模方案有效提升了车道和道路拓扑预测的准确性和一致性。

Abstract: Most autonomous cars rely on the availability of high-definition (HD) maps.
Current research aims to address this constraint by directly predicting HD map
elements from onboard sensors and reasoning about the relationships between the
predicted map and traffic elements. Despite recent advancements, the coherent
online construction of HD maps remains a challenging endeavor, as it
necessitates modeling the high complexity of road topologies in a unified and
consistent manner. To address this challenge, we propose a coherent approach to
predict lane segments and their corresponding topology, as well as road
boundaries, all by leveraging prior map information represented by commonly
available standard-definition (SD) maps. We propose a network architecture,
which leverages hybrid lane segment encodings comprising prior information and
denoising techniques to enhance training stability and performance.
Furthermore, we facilitate past frames for temporal consistency. Our
experimental evaluation demonstrates that our approach outperforms previous
methods by a large margin, highlighting the benefits of our modeling scheme.

</details>


### [26] [Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound](https://arxiv.org/abs/2507.01401)
*Huanwen Liang,Jingxian Xu,Yuanji Zhang,Yuhao Huang,Yuhan Zhang,Xin Yang,Ran Li,Xuedong Deng,Yanjun Liu,Guowei Tao,Yun Wu,Sheng Zhao,Xinru Gao,Dong Ni*

Main category: cs.CV

TL;DR: 提出了一种基于多实例学习（MIL）的无标准平面定位方法，用于胎儿腹部异常的产前超声分类，结合注意力机制和医学知识驱动特征选择，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 胎儿腹部畸形是严重的先天性异常，需准确诊断以指导妊娠管理和降低死亡率。现有AI方法多关注图像级分类，缺乏病例级诊断。

Method: 采用混合注意力专家模块（MoAE）加权不同平面注意力，提出医学知识驱动特征选择模块（MFS）进行自监督图像标记选择，并结合提示原型学习（PPL）增强MFS。

Result: 在大规模数据集（2,419病例，24,748图像，6类别）上验证，性能优于现有方法。

Conclusion: 该方法在病例级诊断中表现出色，为产前超声腹部异常分类提供了新思路。

Abstract: Fetal abdominal malformations are serious congenital anomalies that require
accurate diagnosis to guide pregnancy management and reduce mortality. Although
AI has demonstrated significant potential in medical diagnosis, its application
to prenatal abdominal anomalies remains limited. Most existing studies focus on
image-level classification and rely on standard plane localization, placing
less emphasis on case-level diagnosis. In this paper, we develop a case-level
multiple instance learning (MIL)-based method, free of standard plane
localization, for classifying fetal abdominal anomalies in prenatal ultrasound.
Our contribution is three-fold. First, we adopt a mixture-of-attention-experts
module (MoAE) to weight different attention heads for various planes. Secondly,
we propose a medical-knowledge-driven feature selection module (MFS) to align
image features with medical knowledge, performing self-supervised image token
selection at the case-level. Finally, we propose a prompt-based prototype
learning (PPL) to enhance the MFS. Extensively validated on a large prenatal
abdominal ultrasound dataset containing 2,419 cases, with a total of 24,748
images and 6 categories, our proposed method outperforms the state-of-the-art
competitors. Codes are available at:https://github.com/LL-AC/AAcls.

</details>


### [27] [CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning](https://arxiv.org/abs/2507.01409)
*Kuniaki Saito,Donghyun Kim,Kwanyong Park,Atsushi Hashimoto,Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: 提出了一种名为CaptionSmiths的方法，通过量化标题的长度、描述性和单词独特性，实现单一模型对多样化语言模式的灵活控制。


<details>
  <summary>Details</summary>
Motivation: 现有图像描述模型难以精细控制生成标题的属性，如长度和描述性，且缺乏平滑过渡能力。

Method: 量化标题属性为连续标量值，通过端点向量插值实现条件控制。

Result: 模型能平滑调整标题属性，词汇对齐效果优于基线，长度控制误差减少506%。

Conclusion: CaptionSmiths成功实现了对标题属性的灵活控制，提升了生成标题的多样性和质量。

Abstract: An image captioning model flexibly switching its language pattern, e.g.,
descriptiveness and length, should be useful since it can be applied to diverse
applications. However, despite the dramatic improvement in generative
vision-language models, fine-grained control over the properties of generated
captions is not easy due to two reasons: (i) existing models are not given the
properties as a condition during training and (ii) existing models cannot
smoothly transition its language pattern from one state to the other. Given
this challenge, we propose a new approach, CaptionSmiths, to acquire a single
captioning model that can handle diverse language patterns. First, our approach
quantifies three properties of each caption, length, descriptiveness, and
uniqueness of a word, as continuous scalar values, without human annotation.
Given the values, we represent the conditioning via interpolation between two
endpoint vectors corresponding to the extreme states, e.g., one for a very
short caption and one for a very long caption. Empirical results demonstrate
that the resulting model can smoothly change the properties of the output
captions and show higher lexical alignment than baselines. For instance,
CaptionSmiths reduces the error in controlling caption length by 506\% despite
better lexical alignment. Code will be available on
https://github.com/omron-sinicx/captionsmiths.

</details>


### [28] [Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention](https://arxiv.org/abs/2507.01417)
*Jiawei Gu,Ziyue Qiao,Zechao Li*

Main category: cs.CV

TL;DR: 论文提出了一种基于梯度方向一致性的OOD检测方法，通过短路异常梯度来提升检测性能，同时保持ID分类准确性。


<details>
  <summary>Details</summary>
Motivation: 在开放世界中，OOD检测对安全部署深度学习模型至关重要。研究发现ID样本的梯度方向一致，而OOD样本的梯度方向混乱，这启发了新方法的提出。

Method: 提出了一种推理阶段技术，短路异常梯度，并通过一阶近似避免重复计算，保持ID分类性能。

Result: 在标准OOD基准测试中表现显著提升，方法轻量且易于集成。

Conclusion: 该方法为实际应用中的OOD检测提供了一种高效且实用的解决方案。

Abstract: Out-of-Distribution (OOD) detection is critical for safely deploying deep
models in open-world environments, where inputs may lie outside the training
distribution. During inference on a model trained exclusively with
In-Distribution (ID) data, we observe a salient gradient phenomenon: around an
ID sample, the local gradient directions for "enhancing" that sample's
predicted class remain relatively consistent, whereas OOD samples--unseen in
training--exhibit disorganized or conflicting gradient directions in the same
neighborhood. Motivated by this observation, we propose an inference-stage
technique to short-circuit those feature coordinates that spurious gradients
exploit to inflate OOD confidence, while leaving ID classification largely
intact. To circumvent the expense of recomputing the logits after this gradient
short-circuit, we further introduce a local first-order approximation that
accurately captures the post-modification outputs without a second forward
pass. Experiments on standard OOD benchmarks show our approach yields
substantial improvements. Moreover, the method is lightweight and requires
minimal changes to the standard inference pipeline, offering a practical path
toward robust OOD detection in real-world applications.

</details>


### [29] [DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal](https://arxiv.org/abs/2507.01422)
*Wenjie Liu,Bingshu Wang,Ze Wang,C. L. Philip Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为DocShaDiffusion的潜在空间扩散模型，用于文档图像阴影去除，并设计了阴影软掩模生成模块（SSGM）和阴影掩模引导扩散模块（SMGDM）以解决彩色阴影问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅处理恒定颜色背景的阴影，而忽略了彩色阴影，因此需要一种更有效的解决方案。

Method: 通过潜在空间扩散模型（DocShaDiffusion）和阴影软掩模生成模块（SSGM）生成精确阴影掩模，并利用阴影掩模引导扩散模块（SMGDM）进行阴影去除。

Result: 在三个公共数据集上的实验验证了该方法的优越性。

Conclusion: 该方法在文档图像阴影去除任务中表现优异，代码和数据集将公开。

Abstract: Document shadow removal is a crucial task in the field of document image
enhancement. However, existing methods tend to remove shadows with constant
color background and ignore color shadows. In this paper, we first design a
diffusion model in latent space for document image shadow removal, called
DocShaDiffusion. It translates shadow images from pixel space to latent space,
enabling the model to more easily capture essential features. To address the
issue of color shadows, we design a shadow soft-mask generation module (SSGM).
It is able to produce accurate shadow mask and add noise into shadow regions
specially. Guided by the shadow mask, a shadow mask-aware guided diffusion
module (SMGDM) is proposed to remove shadows from document images by
supervising the diffusion and denoising process. We also propose a
shadow-robust perceptual feature loss to preserve details and structures in
document images. Moreover, we develop a large-scale synthetic document color
shadow removal dataset (SDCSRD). It simulates the distribution of realistic
color shadows and provides powerful supports for the training of models.
Experiments on three public datasets validate the proposed method's superiority
over state-of-the-art. Our code and dataset will be publicly available.

</details>


### [30] [TurboReg: TurboClique for Robust and Efficient Point Cloud Registration](https://arxiv.org/abs/2507.01439)
*Shaocheng Yan,Pengcheng Shi,Zhenjun Zhao,Kaixin Wang,Kuang Cao,Ji Wu,Jiayuan Li*

Main category: cs.CV

TL;DR: TurboReg提出了一种快速鲁棒的PCR估计方法，通过轻量级TurboClique和并行化PGS算法，显著提升了速度和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于最大团搜索的方法虽然召回率高，但时间复杂度指数级增长，无法满足实时需求。

Method: 定义TurboClique为高度约束兼容图中的3-团，结合PGS算法高效搜索高内点率团。

Result: 在多个数据集上达到SOTA性能，速度提升显著（如3DMatch+FCGF上快208.22倍）。

Conclusion: TurboReg在保持高召回率的同时，大幅提升了效率，适用于实时应用。

Abstract: Robust estimation is essential in correspondence-based Point Cloud
Registration (PCR). Existing methods using maximal clique search in
compatibility graphs achieve high recall but suffer from exponential time
complexity, limiting their use in time-sensitive applications. To address this
challenge, we propose a fast and robust estimator, TurboReg, built upon a novel
lightweight clique, TurboClique, and a highly parallelizable Pivot-Guided
Search (PGS) algorithm. First, we define the TurboClique as a 3-clique within a
highly-constrained compatibility graph. The lightweight nature of the 3-clique
allows for efficient parallel searching, and the highly-constrained
compatibility graph ensures robust spatial consistency for stable
transformation estimation. Next, PGS selects matching pairs with high SC$^2$
scores as pivots, effectively guiding the search toward TurboCliques with
higher inlier ratios. Moreover, the PGS algorithm has linear time complexity
and is significantly more efficient than the maximal clique search with
exponential time complexity. Extensive experiments show that TurboReg achieves
state-of-the-art performance across multiple real-world datasets, with
substantial speed improvements. For example, on the 3DMatch+FCGF dataset,
TurboReg (1K) operates $208.22\times$ faster than 3DMAC while also achieving
higher recall. Our code is accessible at
\href{https://github.com/Laka-3DV/TurboReg}{\texttt{TurboReg}}.

</details>


### [31] [OoDDINO:A Multi-level Framework for Anomaly Segmentation on Complex Road Scenes](https://arxiv.org/abs/2507.01455)
*Yuxing Liu,Ji Zhang,Zhou Xuchuan,Jingzhong Xiao,Huimin Yang,Jiaxin Zhong*

Main category: cs.CV

TL;DR: OoDDINO是一个新颖的多级异常分割框架，通过粗到细的检测策略解决现有方法的局限性，结合不确定性引导的检测模型和像素级分割模型。


<details>
  <summary>Details</summary>
Motivation: 现有像素级方法忽视空间相关性且全局阈值策略导致分割不准确，需改进。

Method: 采用两阶段级联架构：正交不确定性感知融合策略（OUAFS）和自适应双阈值网络（ADT-Net）。

Result: 在多个基准数据集上验证了框架的优越性和兼容性。

Conclusion: OoDDINO显著提升了异常分割的准确性和适应性。

Abstract: Anomaly segmentation aims to identify Out-of-Distribution (OoD) anomalous
objects within images. Existing pixel-wise methods typically assign anomaly
scores individually and employ a global thresholding strategy to segment
anomalies. Despite their effectiveness, these approaches encounter significant
challenges in real-world applications: (1) neglecting spatial correlations
among pixels within the same object, resulting in fragmented segmentation; (2)
variabil ity in anomaly score distributions across image regions, causing
global thresholds to either generate false positives in background areas or
miss segments of anomalous objects. In this work, we introduce OoDDINO, a novel
multi-level anomaly segmentation framework designed to address these
limitations through a coarse-to-fine anomaly detection strategy. OoDDINO
combines an uncertainty-guided anomaly detection model with a pixel-level
segmentation model within a two-stage cascade architecture. Initially, we
propose an Orthogonal Uncertainty-Aware Fusion Strategy (OUAFS) that
sequentially integrates multiple uncertainty metrics with visual
representations, employing orthogonal constraints to strengthen the detection
model's capacity for localizing anomalous regions accurately. Subsequently, we
develop an Adaptive Dual-Threshold Network (ADT-Net), which dynamically
generates region-specific thresholds based on object-level detection outputs
and pixel-wise anomaly scores. This approach allows for distinct thresholding
strategies within foreground and background areas, achieving fine-grained
anomaly segmentation. The proposed framework is compatible with other
pixel-wise anomaly detection models, which acts as a plug-in to boost the
performance. Extensive experiments on two benchmark datasets validate our
framework's superiority and compatibility over state-of-the-art methods.

</details>


### [32] [NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation](https://arxiv.org/abs/2507.01463)
*Max Gandyra,Alessandro Santonicola,Michael Beetz*

Main category: cs.CV

TL;DR: NOCTIS是一种无需重新训练即可分割新对象实例的框架，结合Grounded-SAM 2和DINOv2，在BOP 2023挑战中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决无需重新训练即可分割各种新对象实例的难题。

Method: 利用Grounded-SAM 2生成对象提议和分割掩码，DINOv2生成图像嵌入，通过相似性评分匹配对象。

Result: 在BOP 2023挑战的七大数据集上优于其他RGB和RGB-D方法。

Conclusion: NOCTIS是一种简单而强大的框架，适用于新对象实例分割任务。

Abstract: Instance segmentation of novel objects instances in RGB images, given some
example images for each object, is a well known problem in computer vision.
Designing a model general enough to be employed, for all kinds of novel
objects, without (re-) training, has proven to be a difficult task. To handle
this, we propose a simple, yet powerful, framework, called: Novel Object Cyclic
Threshold based Instance Segmentation (NOCTIS). This work stems from and
improves upon previous ones like CNOS, SAM-6D and NIDS-Net; thus, it also
leverages on recent vision foundation models, namely: Grounded-SAM 2 and
DINOv2. It utilises Grounded-SAM 2 to obtain object proposals with precise
bounding boxes and their corresponding segmentation masks; while DINOv2's
zero-shot capabilities are employed to generate the image embeddings. The
quality of those masks, together with their embeddings, is of vital importance
to our approach; as the proposal-object matching is realized by determining an
object matching score based on the similarity of the class embeddings and the
average maximum similarity of the patch embeddings. Differently to SAM-6D,
calculating the latter involves a prior patch filtering based on the distance
between each patch and its corresponding cyclic/roundtrip patch in the image
grid. Furthermore, the average confidence of the proposals' bounding box and
mask is used as an additional weighting factor for the object matching score.
We empirically show that NOCTIS, without further training/fine tuning,
outperforms the best RGB and RGB-D methods on the seven core datasets of the
BOP 2023 challenge for the "Model-based 2D segmentation of unseen objects"
task.

</details>


### [33] [Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think](https://arxiv.org/abs/2507.01467)
*Ge Wu,Shen Zhang,Ruijing Shi,Shanghua Gao,Zhenyuan Chen,Lei Wang,Zhaowei Chen,Hongcheng Gao,Yao Tang,Jian Yang,Ming-Ming Cheng,Xiang Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为REG的方法，通过将低层图像潜在变量与预训练模型的高层类别标记纠缠，显著提升了扩散模型的生成质量和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如REPA）在训练扩散模型时未能充分利用判别性表示，因为外部对齐在去噪推理过程中缺失。

Method: REG方法将低层图像潜在变量与预训练基础模型的单一高层类别标记纠缠，直接从纯噪声中生成一致的图像-类别对。

Result: 在ImageNet 256×256上，REG显著加速了收敛速度（比SiT-XL/2快63倍，比SiT-XL/2 + REPA快23倍），且训练400K次即可超越REPA训练4M次的效果。

Conclusion: REG通过主动引导图像生成过程的语义知识，以极小的额外计算开销显著提升了扩散模型的性能。

Abstract: REPA and its variants effectively mitigate training challenges in diffusion
models by incorporating external visual representations from pretrained models,
through alignment between the noisy hidden projections of denoising networks
and foundational clean image representations. We argue that the external
alignment, which is absent during the entire denoising inference process, falls
short of fully harnessing the potential of discriminative representations. In
this work, we propose a straightforward method called Representation
Entanglement for Generation (REG), which entangles low-level image latents with
a single high-level class token from pretrained foundation models for
denoising. REG acquires the capability to produce coherent image-class pairs
directly from pure noise, substantially improving both generation quality and
training efficiency. This is accomplished with negligible additional inference
overhead, requiring only one single additional token for denoising (<0.5\%
increase in FLOPs and latency). The inference process concurrently reconstructs
both image latents and their corresponding global semantics, where the acquired
semantic knowledge actively guides and enhances the image generation process.
On ImageNet 256$\times$256, SiT-XL/2 + REG demonstrates remarkable convergence
acceleration, achieving $\textbf{63}\times$ and $\textbf{23}\times$ faster
training than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively,
SiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA
trained for 4M iterations ($\textbf{10}\times$ longer). Code is available at:
https://github.com/Martinser/REG.

</details>


### [34] [Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and Low-Power Solutions for Resource-Constrained Hardware](https://arxiv.org/abs/2507.01472)
*Jonáš Herec,Vít Růžička,Rado Pitoňák*

Main category: cs.CV

TL;DR: 论文提出了一种高效、低功耗的甲烷检测算法（Mag1c-SAS和CEM），结合机器学习模型，显著提升了在资源有限的星载硬件上的检测速度，同时保持了准确性。


<details>
  <summary>Details</summary>
Motivation: 甲烷是一种强效温室气体，通过高光谱卫星图像早期检测其泄漏有助于减缓气候变化。现有任务多为手动操作，易遗漏潜在事件，且传统方法计算需求高，不适合星载硬件。

Method: 测试了快速目标检测方法（ACE、CEM），并提出Mag1c-SAS（Mag1c的快速变体）。结合机器学习模型（U-Net、LinkNet），并评估三种波段选择策略。

Result: Mag1c-SAS和CEM在检测强羽流时表现良好，计算效率分别比原始Mag1c快约100倍和230倍。一种波段选择策略在减少通道数的同时优于传统方法。

Conclusion: 研究为星载甲烷检测提供了高效解决方案，代码和数据已开源，为未来低硬件需求的检测技术奠定了基础。

Abstract: Methane is a potent greenhouse gas, and detecting its leaks early via
hyperspectral satellite imagery can help mitigate climate change. Meanwhile,
many existing missions operate in manual tasking regimes only, thus missing
potential events of interest. To overcome slow downlink rates cost-effectively,
onboard detection is a viable solution. However, traditional methane
enhancement methods are too computationally demanding for resource-limited
onboard hardware. This work accelerates methane detection by focusing on
efficient, low-power algorithms. We test fast target detection methods (ACE,
CEM) that have not been previously used for methane detection and propose a
Mag1c-SAS - a significantly faster variant of the current state-of-the-art
algorithm for methane detection: Mag1c. To explore their true detection
potential, we integrate them with a machine learning model (U-Net, LinkNet).
Our results identify two promising candidates (Mag1c-SAS and CEM), both
acceptably accurate for the detection of strong plumes and computationally
efficient enough for onboard deployment: one optimized more for accuracy, the
other more for speed, achieving up to ~100x and ~230x faster computation than
original Mag1c on resource-limited hardware. Additionally, we propose and
evaluate three band selection strategies. One of them can outperform the method
traditionally used in the field while using fewer channels, leading to even
faster processing without compromising accuracy. This research lays the
foundation for future advancements in onboard methane detection with minimal
hardware requirements, improving timely data delivery. The produced code, data,
and models are open-sourced and can be accessed from
https://github.com/zaitra/methane-filters-benchmark.

</details>


### [35] [Active Control Points-based 6DoF Pose Tracking for Industrial Metal Objects](https://arxiv.org/abs/2507.01478)
*Chentao Shen,Ding Pan,Mingyu Mei,Zaixing He,Xinyue Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于主动控制点的6DoF姿态跟踪方法，用于解决金属物体在真实环境中的反射问题。


<details>
  <summary>Details</summary>
Motivation: 工业金属物体的姿态跟踪因反射特性而具有挑战性，需要一种更有效的方法。

Method: 使用图像控制点主动生成边缘特征进行优化，并引入最优控制点回归方法以提高鲁棒性。

Result: 在数据集评估和实际任务中均表现有效，适用于工业金属物体的实时跟踪。

Conclusion: 该方法为工业金属物体的实时跟踪提供了可行的解决方案，代码已开源。

Abstract: Visual pose tracking is playing an increasingly vital role in industrial
contexts in recent years. However, the pose tracking for industrial metal
objects remains a challenging task especially in the real world-environments,
due to the reflection characteristic of metal objects. To address this issue,
we propose a novel 6DoF pose tracking method based on active control points.
The method uses image control points to generate edge feature for optimization
actively instead of 6DoF pose-based rendering, and serve them as optimization
variables. We also introduce an optimal control point regression method to
improve robustness. The proposed tracking method performs effectively in both
dataset evaluation and real world tasks, providing a viable solution for
real-time tracking of industrial metal objects. Our source code is made
publicly available at: https://github.com/tomatoma00/ACPTracking.

</details>


### [36] [What Really Matters for Robust Multi-Sensor HD Map Construction?](https://arxiv.org/abs/2507.01484)
*Xiaoshuai Hao,Yuting Zhao,Yuheng Ji,Luanyuan Dai,Peng Hao,Dingzhe Li,Shuai Cheng,Rong Yin*

Main category: cs.CV

TL;DR: 本文提出了一种增强多模态融合方法鲁棒性的策略，用于高精地图构建，包括数据增强、新型多模态融合模块和模态丢弃训练策略，并在NuScenes数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注模型精度而忽视鲁棒性，而鲁棒性对自动驾驶系统的实际应用至关重要。

Method: 提出数据增强、新型多模态融合模块和模态丢弃训练策略。

Result: 在NuScenes数据集上显著提升了基线方法的鲁棒性，并在干净验证集上达到最优性能。

Conclusion: 研究为开发更鲁棒可靠的高精地图构建模型提供了重要见解，推动了其在自动驾驶中的应用。

Abstract: High-definition (HD) map construction methods are crucial for providing
precise and comprehensive static environmental information, which is essential
for autonomous driving systems. While Camera-LiDAR fusion techniques have shown
promising results by integrating data from both modalities, existing approaches
primarily focus on improving model accuracy and often neglect the robustness of
perception models, which is a critical aspect for real-world applications. In
this paper, we explore strategies to enhance the robustness of multi-modal
fusion methods for HD map construction while maintaining high accuracy. We
propose three key components: data augmentation, a novel multi-modal fusion
module, and a modality dropout training strategy. These components are
evaluated on a challenging dataset containing 10 days of NuScenes data. Our
experimental results demonstrate that our proposed methods significantly
enhance the robustness of baseline methods. Furthermore, our approach achieves
state-of-the-art performance on the clean validation set of the NuScenes
dataset. Our findings provide valuable insights for developing more robust and
reliable HD map construction models, advancing their applicability in
real-world autonomous driving scenarios. Project website:
https://robomap-123.github.io.

</details>


### [37] [AVC-DPO: Aligned Video Captioning via Direct Preference Optimization](https://arxiv.org/abs/2507.01492)
*Jiyang Tang,Hengyi Li,Yifan Du,Wayne Xin Zhao*

Main category: cs.CV

TL;DR: AVC-DPO是一种后训练框架，通过偏好对齐提升视频MLLMs的标题生成能力，专注于时空动态和空间信息，并在VDC基准测试中取得第一名。


<details>
  <summary>Details</summary>
Motivation: 尽管视频MLLMs在视频标题生成任务中取得进展，但如何根据人类偏好调整标题焦点仍具挑战性。

Method: 设计增强提示，针对时空动态和空间信息，通过偏好感知训练和标题对齐优化模型。

Result: 在LOVE@CVPR'25 Workshop Track 1A中表现优异，VDC基准测试中排名第一。

Conclusion: AVC-DPO通过偏好对齐有效提升了视频MLLMs的标题生成能力，满足人类偏好。

Abstract: Although video multimodal large language models (video MLLMs) have achieved
substantial progress in video captioning tasks, it remains challenging to
adjust the focal emphasis of video captions according to human preferences. To
address this limitation, we propose Aligned Video Captioning via Direct
Preference Optimization (AVC-DPO), a post-training framework designed to
enhance captioning capabilities in video MLLMs through preference alignment.
Our approach designs enhanced prompts that specifically target temporal
dynamics and spatial information-two key factors that humans care about when
watching a video-thereby incorporating human-centric preferences. AVC-DPO
leverages the same foundation model's caption generation responses under varied
prompt conditions to conduct preference-aware training and caption alignment.
Using this framework, we have achieved exceptional performance in the
LOVE@CVPR'25 Workshop Track 1A: Video Detailed Captioning Challenge, achieving
first place on the Video Detailed Captioning (VDC) benchmark according to the
VDCSCORE evaluation metric.

</details>


### [38] [Crop Pest Classification Using Deep Learning Techniques: A Review](https://arxiv.org/abs/2507.01494)
*Muhammad Hassam Ejaz,Muhammad Bilal,Usman Habib*

Main category: cs.CV

TL;DR: 本文综述了2018至2025年间37项关于AI害虫分类的研究，探讨了CNN、ViT和混合模型的应用，总结了技术挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统害虫监测方法效率低且难以扩展，AI技术（如CNN、ViT）为自动化害虫检测提供了高效解决方案。

Method: 分析了37项研究，按作物类型、害虫种类、模型架构、数据集和技术挑战分类。

Result: 早期研究多依赖CNN，近期转向混合和Transformer模型，精度更高，但数据不平衡、小害虫检测等挑战仍存。

Conclusion: AI害虫监测系统前景广阔，但需解决数据集、模型泛化和边缘部署等关键问题。

Abstract: Insect pests continue to bring a serious threat to crop yields around the
world, and traditional methods for monitoring them are often slow, manual, and
difficult to scale. In recent years, deep learning has emerged as a powerful
solution, with techniques like convolutional neural networks (CNNs), vision
transformers (ViTs), and hybrid models gaining popularity for automating pest
detection. This review looks at 37 carefully selected studies published between
2018 and 2025, all focused on AI-based pest classification. The selected
research is organized by crop type, pest species, model architecture, dataset
usage, and key technical challenges. The early studies relied heavily on CNNs
but latest work is shifting toward hybrid and transformer-based models that
deliver higher accuracy and better contextual understanding. Still, challenges
like imbalanced datasets, difficulty in detecting small pests, limited
generalizability, and deployment on edge devices remain significant hurdles.
Overall, this review offers a structured overview of the field, highlights
useful datasets, and outlines the key challenges and future directions for
AI-based pest monitoring systems.

</details>


### [39] [ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation](https://arxiv.org/abs/2507.01496)
*Jimyeong Kim,Jungwon Park,Yeji Song,Nojun Kwak,Wonjong Rhee*

Main category: cs.CV

TL;DR: 提出了一种无需训练、无需用户提供掩码的ReFlow图像编辑方法，通过分析多模态Transformer块的中间表示，利用中步潜在特征提升编辑效果。


<details>
  <summary>Details</summary>
Motivation: 尽管ReFlow在文本到图像生成中表现优异，但在真实图像编辑中仍面临挑战，因此需要一种更有效的方法。

Method: 分析多模态Transformer块的中间表示，提取关键特征；利用中步潜在特征保持结构；调整注意力注入以提升编辑性和文本对齐。

Result: 在两个基准测试中优于九种基线方法，并通过人类评估验证了用户偏好。

Conclusion: 该方法在无需训练和掩码的情况下，显著提升了ReFlow在真实图像编辑中的性能。

Abstract: Rectified Flow text-to-image models surpass diffusion models in image quality
and text alignment, but adapting ReFlow for real-image editing remains
challenging. We propose a new real-image editing method for ReFlow by analyzing
the intermediate representations of multimodal transformer blocks and
identifying three key features. To extract these features from real images with
sufficient structural preservation, we leverage mid-step latent, which is
inverted only up to the mid-step. We then adapt attention during injection to
improve editability and enhance alignment to the target text. Our method is
training-free, requires no user-provided mask, and can be applied even without
a source prompt. Extensive experiments on two benchmarks with nine baselines
demonstrate its superior performance over prior methods, further validated by
human evaluations confirming a strong user preference for our approach.

</details>


### [40] [Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images](https://arxiv.org/abs/2507.01502)
*Ozan Durgut,Beril Kallfelz-Sirmacek,Cem Unsalan*

Main category: cs.CV

TL;DR: 论文提出了一种结合传统方法和深度学习的规则化树冠检测方法，以提高检测的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 全球变暖、生物多样性丧失和空气污染等问题亟需森林监测技术，但现有方法存在不足。

Method: 结合传统方法（特征提取与分割）和深度学习（树冠检测），并通过规则化后处理优化结果。

Result: 提出的方法在检测树冠数量上表现更优，并分析了其优缺点和改进空间。

Conclusion: 规则化方法有效提升了树冠检测的准确性和鲁棒性，为未来研究提供了改进方向。

Abstract: Global warming, loss of biodiversity, and air pollution are among the most
significant problems facing Earth. One of the primary challenges in addressing
these issues is the lack of monitoring forests to protect them. To tackle this
problem, it is important to leverage remote sensing and computer vision methods
to automate monitoring applications. Hence, automatic tree crown detection
algorithms emerged based on traditional and deep learning methods. In this
study, we first introduce two different tree crown detection methods based on
these approaches. Then, we form a novel rule-based approach that integrates
these two methods to enhance robustness and accuracy of tree crown detection
results. While traditional methods are employed for feature extraction and
segmentation of forested areas, deep learning methods are used to detect tree
crowns in our method. With the proposed rule-based approach, we post-process
these results, aiming to increase the number of detected tree crowns through
neighboring trees and localized operations. We compare the obtained results
with the proposed method in terms of the number of detected tree crowns and
report the advantages, disadvantages, and areas for improvement of the obtained
outcomes.

</details>


### [41] [Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence](https://arxiv.org/abs/2507.01504)
*Robert Aufschläger,Youssef Shoeb,Azarm Nowzad,Michael Heigl,Fabian Bally,Martin Schramm*

Main category: cs.CV

TL;DR: 论文提出了一种名为cRID的跨模态框架，结合视觉语言模型和图注意力网络，用于检测个人可识别信息（PII）并增强行人重识别（Re-ID）。


<details>
  <summary>Details</summary>
Motivation: 街景数据作为开放数据对自动驾驶和AI研究至关重要，但其中包含的个人可识别信息（PII）对隐私构成威胁。

Method: 结合大型视觉语言模型、图注意力网络和表示学习，检测文本可描述的PII线索，并提升行人重识别性能。

Result: 实验表明，该框架在跨数据集行人重识别场景中表现优异，特别是在Market-1501到CUHK03-np的数据集上。

Conclusion: cRID框架能有效检测语义上有意义的PII，并在实际应用中提升行人重识别性能。

Abstract: The collection and release of street-level recordings as Open Data play a
vital role in advancing autonomous driving systems and AI research. However,
these datasets pose significant privacy risks, particularly for pedestrians,
due to the presence of Personally Identifiable Information (PII) that extends
beyond biometric traits such as faces. In this paper, we present cRID, a novel
cross-modal framework combining Large Vision-Language Models, Graph Attention
Networks, and representation learning to detect textual describable clues of
PII and enhance person re-identification (Re-ID). Our approach focuses on
identifying and leveraging interpretable features, enabling the detection of
semantically meaningful PII beyond low-level appearance cues. We conduct a
systematic evaluation of PII presence in person image datasets. Our experiments
show improved performance in practical cross-dataset Re-ID scenarios, notably
from Market-1501 to CUHK03-np (detected), highlighting the framework's
practical utility. Code is available at https://github.com/RAufschlaeger/cRID.

</details>


### [42] [Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation](https://arxiv.org/abs/2507.01509)
*Tapas K. Dutta,Snehashis Majhi,Deepak Ranjan Nayak,Debesh Jha*

Main category: cs.CV

TL;DR: SAM-MaGuP是一种基于Segment Anything Model的创新方法，通过边界蒸馏模块和1D-2D Mamba适配器，显著提升了息肉分割的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 息肉分割在结肠镜图像中至关重要，但现有方法在弱边界和模糊边界情况下表现不佳，且泛化能力不足。

Method: 结合边界蒸馏模块和1D-2D Mamba适配器，增强全局上下文交互和边界特征学习。

Result: 在五个数据集上表现优于现有方法，实现了更高的分割准确性和鲁棒性。

Conclusion: SAM-MaGuP通过创新设计，为息肉分割领域设定了新的基准。

Abstract: Polyp segmentation in colonoscopy images is crucial for early detection and
diagnosis of colorectal cancer. However, this task remains a significant
challenge due to the substantial variations in polyp shape, size, and color, as
well as the high similarity between polyps and surrounding tissues, often
compounded by indistinct boundaries. While existing encoder-decoder CNN and
transformer-based approaches have shown promising results, they struggle with
stable segmentation performance on polyps with weak or blurry boundaries. These
methods exhibit limited abilities to distinguish between polyps and non-polyps
and capture essential boundary cues. Moreover, their generalizability still
falls short of meeting the demands of real-time clinical applications. To
address these limitations, we propose SAM-MaGuP, a groundbreaking approach for
robust polyp segmentation. By incorporating a boundary distillation module and
a 1D-2D Mamba adapter within the Segment Anything Model (SAM), SAM-MaGuP excels
at resolving weak boundary challenges and amplifies feature learning through
enriched global contextual interactions. Extensive evaluations across five
diverse datasets reveal that SAM-MaGuP outperforms state-of-the-art methods,
achieving unmatched segmentation accuracy and robustness. Our key innovations,
a Mamba-guided boundary prior and a 1D-2D Mamba block, set a new benchmark in
the field, pushing the boundaries of polyp segmentation to new heights.

</details>


### [43] [Exploring Pose-based Sign Language Translation: Ablation Studies and Attention Insights](https://arxiv.org/abs/2507.01532)
*Tomas Zelezny,Jakub Straka,Vaclav Javorek,Ondrej Valach,Marek Hruz,Ivan Gruber*

Main category: cs.CV

TL;DR: 本文探讨了基于姿态的数据预处理技术（归一化、插值和增强）对手语翻译性能的影响，使用改进的T5编码器-解码器模型，实验表明这些技术能显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过姿态数据预处理技术提升手语翻译系统的性能和泛化能力。

Method: 采用改进的T5编码器-解码器模型处理姿态表示，并在YouTubeASL和How2Sign数据集上进行消融实验。

Result: 适当的归一化、插值和增强技术能显著提高模型的鲁棒性和泛化能力，且添加专用寄存器标记可进一步提升性能。

Conclusion: 姿态数据预处理技术对手语翻译性能有重要影响，未来可进一步优化模型架构。

Abstract: Sign Language Translation (SLT) has evolved significantly, moving from
isolated recognition approaches to complex, continuous gloss-free translation
systems. This paper explores the impact of pose-based data preprocessing
techniques - normalization, interpolation, and augmentation - on SLT
performance. We employ a transformer-based architecture, adapting a modified T5
encoder-decoder model to process pose representations. Through extensive
ablation studies on YouTubeASL and How2Sign datasets, we analyze how different
preprocessing strategies affect translation accuracy. Our results demonstrate
that appropriate normalization, interpolation, and augmentation techniques can
significantly improve model robustness and generalization abilities.
Additionally, we provide a deep analysis of the model's attentions and reveal
interesting behavior suggesting that adding a dedicated register token can
improve overall model performance. We publish our code on our GitHub
repository, including the preprocessed YouTubeASL data.

</details>


### [44] [TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV Object Tracking](https://arxiv.org/abs/2507.01535)
*Bingxi Liu,Calvin Chen,Junhao Li,Guyang Yu,Haoqian Song,Xuchen Liu,Jinqiang Cui,Hong Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于Mamba模型的TrackingMiM架构，解决了Vision Transformer在无人机跟踪任务中的二次复杂度问题，实现了高效的长序列建模和实时处理。


<details>
  <summary>Details</summary>
Motivation: Vision Transformer在无人机跟踪系统中因二次复杂度问题难以实时处理数据，而现有的Mamba方法存在时间连续性不足的缺陷。

Method: 提出TrackingMiM架构，采用嵌套的Mamba扫描机制，独立处理时空一致的图像块标记，并将模板帧编码为查询标记用于跟踪。

Result: 在五个无人机跟踪基准测试中，TrackingMiM实现了最优的精度和显著更高的速度。

Conclusion: TrackingMiM通过改进Mamba的时间连续性处理，为无人机跟踪任务提供了一种高效且精确的解决方案。

Abstract: The Vision Transformer (ViT) model has long struggled with the challenge of
quadratic complexity, a limitation that becomes especially critical in unmanned
aerial vehicle (UAV) tracking systems, where data must be processed in real
time. In this study, we explore the recently proposed State-Space Model, Mamba,
leveraging its computational efficiency and capability for long-sequence
modeling to effectively process dense image sequences in tracking tasks. First,
we highlight the issue of temporal inconsistency in existing Mamba-based
methods, specifically the failure to account for temporal continuity in the
Mamba scanning mechanism. Secondly, building upon this insight,we propose
TrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model
for handling image sequence of tracking problem. In our framework, the mamba
scan is performed in a nested way while independently process temporal and
spatial coherent patch tokens. While the template frame is encoded as query
token and utilized for tracking in every scan. Extensive experiments conducted
on five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves
state-of-the-art precision while offering noticeable higher speed in UAV
tracking.

</details>


### [45] [A Multi-Centric Anthropomorphic 3D CT Phantom-Based Benchmark Dataset for Harmonization](https://arxiv.org/abs/2507.01539)
*Mohammadreza Amirian,Michael Bach,Oscar Jimenez-del-Toro,Christoph Aberle,Roger Schaer,Vincent Andrearczyk,Jean-Félix Maestrati,Maria Martin Asiain,Kyriakos Flouris,Markus Obmann,Clarisse Dromain,Benoît Dufour,Pierre-Alexandre Alois Poletti,Hendrik von Tengg-Kobligk,Rolf Hügli,Martin Kretzschmar,Hatem Alkadhi,Ender Konukoglu,Henning Müller,Bram Stieltjes,Adrien Depeursinge*

Main category: cs.CV

TL;DR: 本文提出一个开源基准数据集，用于促进AI在CT分析中的泛化能力，通过减少数据分布偏移来提升AI性能。


<details>
  <summary>Details</summary>
Motivation: AI在医学影像分析中因数据分布偏移（如扫描设备或剂量变化）导致泛化能力差，需要开发AI协调技术。

Method: 使用仿人体模型CT扫描数据集（1378个图像系列，13台扫描仪），提供基线方法和开源代码评估图像和特征稳定性。

Result: 数据集和方法为开发AI协调策略提供了基准，支持图像和特征层面的稳定性分析及肝脏组织分类。

Conclusion: 该工作为AI在医学影像中的泛化问题提供了实用工具和基准，推动AI协调技术的发展。

Abstract: Artificial intelligence (AI) has introduced numerous opportunities for human
assistance and task automation in medicine. However, it suffers from poor
generalization in the presence of shifts in the data distribution. In the
context of AI-based computed tomography (CT) analysis, significant data
distribution shifts can be caused by changes in scanner manufacturer,
reconstruction technique or dose. AI harmonization techniques can address this
problem by reducing distribution shifts caused by various acquisition settings.
This paper presents an open-source benchmark dataset containing CT scans of an
anthropomorphic phantom acquired with various scanners and settings, which
purpose is to foster the development of AI harmonization techniques. Using a
phantom allows fixing variations attributed to inter- and intra-patient
variations. The dataset includes 1378 image series acquired with 13 scanners
from 4 manufacturers across 8 institutions using a harmonized protocol as well
as several acquisition doses. Additionally, we present a methodology, baseline
results and open-source code to assess image- and feature-level stability and
liver tissue classification, promoting the development of AI harmonization
strategies.

</details>


### [46] [Interpolation-Based Event Visual Data Filtering Algorithms](https://arxiv.org/abs/2507.01557)
*Marcin Kowlaczyk,Tomasz Kryjak*

Main category: cs.CV

TL;DR: 提出了一种基于无限脉冲响应（IIR）滤波器矩阵的方法，能去除事件相机数据中约99%的噪声，同时保留有效信号。


<details>
  <summary>Details</summary>
Motivation: 事件相机数据流中存在显著噪声，影响应用效果。

Method: 提出四种基于IIR滤波器矩阵的算法，并在多个事件数据集上进行测试，包括添加人工噪声和动态视觉传感器记录的噪声。

Result: 方法在1280x720分辨率传感器上仅需约30KB内存，适合嵌入式设备实现。

Conclusion: 该方法高效去噪且资源占用低，适用于嵌入式系统。

Abstract: The field of neuromorphic vision is developing rapidly, and event cameras are
finding their way into more and more applications. However, the data stream
from these sensors is characterised by significant noise. In this paper, we
propose a method for event data that is capable of removing approximately 99\%
of noise while preserving the majority of the valid signal. We have proposed
four algorithms based on the matrix of infinite impulse response (IIR) filters
method. We compared them on several event datasets that were further modified
by adding artificially generated noise and noise recorded with dynamic vision
sensor. The proposed methods use about 30KB of memory for a sensor with a
resolution of 1280 x 720 and is therefore well suited for implementation in
embedded devices.

</details>


### [47] [A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2507.01573)
*Hao Wang,Keyan Hu,Xin Guo,Haifeng Li,Chao Tao*

Main category: cs.CV

TL;DR: 论文提出了一种结合判别式学习和扩散生成模型的方法（IDGBR），用于改进遥感语义分割中的边界精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖判别式学习，擅长捕捉低频特征但忽略高频边界细节，而扩散生成模型擅长生成高频细节但低频语义推理不足。

Method: 结合判别式模型生成粗分割图，再通过扩散去噪过程细化边界。

Result: 在五个遥感数据集上验证了IDGBR能一致改进边界精度。

Conclusion: IDGBR框架有效结合了判别式和生成式学习的优势，提升了分割的边界精度。

Abstract: Remote sensing semantic segmentation must address both what the ground
objects are within an image and where they are located. Consequently,
segmentation models must ensure not only the semantic correctness of
large-scale patches (low-frequency information) but also the precise
localization of boundaries between patches (high-frequency information).
However, most existing approaches rely heavily on discriminative learning,
which excels at capturing low-frequency features, while overlooking its
inherent limitations in learning high-frequency features for semantic
segmentation. Recent studies have revealed that diffusion generative models
excel at generating high-frequency details. Our theoretical analysis confirms
that the diffusion denoising process significantly enhances the model's ability
to learn high-frequency features; however, we also observe that these models
exhibit insufficient semantic inference for low-frequency features when guided
solely by the original image. Therefore, we integrate the strengths of both
discriminative and generative learning, proposing the Integration of
Discriminative and diffusion-based Generative learning for Boundary Refinement
(IDGBR) framework. The framework first generates a coarse segmentation map
using a discriminative backbone model. This map and the original image are fed
into a conditioning guidance network to jointly learn a guidance representation
subsequently leveraged by an iterative denoising diffusion process refining the
coarse segmentation. Extensive experiments across five remote sensing semantic
segmentation datasets (binary and multi-class segmentation) confirm our
framework's capability of consistent boundary refinement for coarse results
from diverse discriminative architectures. The source code will be available at
https://github.com/KeyanHu-git/IDGBR.

</details>


### [48] [SketchColour: Channel Concat Guided DiT-based Sketch-to-Colour Pipeline for 2D Animation](https://arxiv.org/abs/2507.01586)
*Bryan Constantine Sadihin,Michael Hua Wang,Shei Pern Chua,Hang Su*

Main category: cs.CV

TL;DR: SketchColour提出了一种基于扩散变压器（DiT）的草图到色彩转换方法，显著降低了参数和GPU内存使用，并在SAKUGA数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统2D动画制作需要手工绘制大量帧，耗时耗力。

Method: 采用DiT架构替代U-Net去噪器，通过轻量级通道连接适配器和LoRA微调注入草图信息。

Result: 在SAKUGA数据集上表现优于现有方法，训练数据仅为竞争对手的一半，且生成动画时间一致、伪影少。

Conclusion: SketchColour为2D动画制作提供了一种高效、高质量的自动化解决方案。

Abstract: The production of high-quality 2D animation is highly labor-intensive
process, as animators are currently required to draw and color a large number
of frames by hand. We present SketchColour, the first sketch-to-colour pipeline
for 2D animation built on a diffusion transformer (DiT) backbone. By replacing
the conventional U-Net denoiser with a DiT-style architecture and injecting
sketch information via lightweight channel-concatenation adapters accompanied
with LoRA finetuning, our method natively integrates conditioning without the
parameter and memory bloat of a duplicated ControlNet, greatly reducing
parameter count and GPU memory usage. Evaluated on the SAKUGA dataset,
SketchColour outperforms previous state-of-the-art video colourization methods
across all metrics, despite using only half the training data of competing
models. Our approach produces temporally coherent animations with minimal
artifacts such as colour bleeding or object deformation. Our code is available
at: https://bconstantine.github.io/SketchColour .

</details>


### [49] [Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring](https://arxiv.org/abs/2507.01590)
*Ameer Hamza,Zuhaib Hussain But,Umar Arif,Samiya,M. Abdullah Asad,Muhammad Naeem*

Main category: cs.CV

TL;DR: 该研究提出了一种新型课堂监控系统，整合了多种模态（如睡意检测、手机使用追踪和人脸识别）以高精度评估学生注意力。系统采用YOLOv8和LResNet Occ FC等技术，实现了实时监测，并在测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 旨在通过多模态技术提升课堂监控的精确性和全面性，同时实现自动考勤记录。

Method: 结合YOLOv8模型检测手机和睡眠行为，使用LResNet Occ FC进行人脸识别，并通过ESP32-CAM硬件和PHP网页应用实现系统集成。

Result: 睡眠检测mAP@50达97.42%，人脸识别验证准确率为86.45%，手机检测mAP@50为85.89%。

Conclusion: 该系统为教育环境提供了一种高效、可扩展的监控解决方案，兼具实时性和自动化功能。

Abstract: This study presents a novel classroom surveillance system that integrates
multiple modalities, including drowsiness, tracking of mobile phone usage, and
face recognition,to assess student attentiveness with enhanced precision.The
system leverages the YOLOv8 model to detect both mobile phone and sleep
usage,(Ghatge et al., 2024) while facial recognition is achieved through
LResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These
models work in synergy to provide comprehensive, real-time monitoring, offering
insights into student engagement and behavior.(S et al., 2023) The framework is
trained on specialized datasets, such as the RMFD dataset for face recognition
and a Roboflow dataset for mobile phone detection. The extensive evaluation of
the system shows promising results. Sleep detection achieves 97. 42% mAP@50,
face recognition achieves 86. 45% validation accuracy and mobile phone
detection reach 85. 89% mAP@50. The system is implemented within a core PHP web
application and utilizes ESP32-CAM hardware for seamless data capture.(Neto et
al., 2024) This integrated approach not only enhances classroom monitoring, but
also ensures automatic attendance recording via face recognition as students
remain seated in the classroom, offering scalability for diverse educational
environments.(Banada,2025)

</details>


### [50] [DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation](https://arxiv.org/abs/2507.01603)
*Yue-Jiang Dong,Wang Zhao,Jiale Xu,Ying Shan,Song-Hai Zhang*

Main category: cs.CV

TL;DR: DepthSync提出了一种无需训练的框架，通过扩散引导实现长视频的尺度和几何一致的深度预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长视频时存在尺度差异和几何不一致的问题，忽略了视频深度的3D几何结构。

Method: 引入尺度引导和几何引导，协同调整去噪过程，确保跨窗口的尺度同步和窗口内的几何对齐。

Result: 实验表明，DepthSync在多个数据集上显著提升了深度预测的尺度和几何一致性。

Conclusion: DepthSync为长视频深度估计提供了一种高效且一致的解决方案。

Abstract: Diffusion-based video depth estimation methods have achieved remarkable
success with strong generalization ability. However, predicting depth for long
videos remains challenging. Existing methods typically split videos into
overlapping sliding windows, leading to accumulated scale discrepancies across
different windows, particularly as the number of windows increases.
Additionally, these methods rely solely on 2D diffusion priors, overlooking the
inherent 3D geometric structure of video depths, which results in geometrically
inconsistent predictions. In this paper, we propose DepthSync, a novel,
training-free framework using diffusion guidance to achieve scale- and
geometry-consistent depth predictions for long videos. Specifically, we
introduce scale guidance to synchronize the depth scale across windows and
geometry guidance to enforce geometric alignment within windows based on the
inherent 3D constraints in video depths. These two terms work synergistically,
steering the denoising process toward consistent depth predictions. Experiments
on various datasets validate the effectiveness of our method in producing depth
estimates with improved scale and geometry consistency, particularly for long
videos.

</details>


### [51] [Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems](https://arxiv.org/abs/2507.01607)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi,Eric Bourbao*

Main category: cs.CV

TL;DR: 本文首次系统研究了深度学习人脸识别系统中的后门攻击，展示了两种针对人脸检测任务的攻击，并验证了特征提取器的脆弱性，提出了防范措施。


<details>
  <summary>Details</summary>
Motivation: 深度学习人脸识别系统的广泛应用引发了安全隐患，但现有研究未充分关注真实场景下的后门攻击。

Method: 通过探索两种后门攻击（人脸生成和关键点偏移），验证特征提取器的脆弱性，并测试20种系统配置和15种攻击案例。

Result: 研究表明，单一后门可绕过系统功能，攻击成功率高。

Conclusion: 研究揭示了人脸识别系统的后门风险，并提出了防范建议。

Abstract: The widespread use of deep learning face recognition raises several security
concerns. Although prior works point at existing vulnerabilities, DNN backdoor
attacks against real-life, unconstrained systems dealing with images captured
in the wild remain a blind spot of the literature. This paper conducts the
first system-level study of backdoors in deep learning-based face recognition
systems. This paper yields four contributions by exploring the feasibility of
DNN backdoors on these pipelines in a holistic fashion. We demonstrate for the
first time two backdoor attacks on the face detection task: face generation and
face landmark shift attacks. We then show that face feature extractors trained
with large margin losses also fall victim to backdoor attacks. Combining our
models, we then show using 20 possible pipeline configurations and 15 attack
cases that a single backdoor enables an attacker to bypass the entire function
of a system. Finally, we provide stakeholders with several best practices and
countermeasures.

</details>


### [52] [Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss](https://arxiv.org/abs/2507.01630)
*Yuxiao Wang,Yu Lei,Zhenao Wei,Weiying Xue,Xinyu Jiang,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: P3HOT框架通过结合提示引导和人类近端感知，解决了HOT检测中的过度分割和类别一致性问题，并在多个指标上实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前HOT检测模型局限于单一图像类型，导致过度分割和类别不一致问题，需要更高效的解决方案。

Method: P3HOT框架结合语义驱动的提示机制和人类近端感知机制，利用深度信息消除非交互区域，并引入RJLoss和AD-Acc.新指标。

Result: 在HOT-Annotated数据集上，SC-Acc.、mIoU、wIoU和AD-Acc.指标分别提升0.7、2.0、1.6和11.0。

Conclusion: P3HOT通过多机制融合显著提升了HOT检测性能，为相关领域提供了新思路。

Abstract: The task of Human-Object conTact (HOT) detection involves identifying the
specific areas of the human body that are touching objects. Nevertheless,
current models are restricted to just one type of image, often leading to too
much segmentation in areas with little interaction, and struggling to maintain
category consistency within specific regions. To tackle this issue, a HOT
framework, termed \textbf{P3HOT}, is proposed, which blends \textbf{P}rompt
guidance and human \textbf{P}roximal \textbf{P}erception. To begin with, we
utilize a semantic-driven prompt mechanism to direct the network's attention
towards the relevant regions based on the correlation between image and text.
Then a human proximal perception mechanism is employed to dynamically perceive
key depth range around the human, using learnable parameters to effectively
eliminate regions where interactions are not expected. Calculating depth
resolves the uncertainty of the overlap between humans and objects in a 2D
perspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss
(RJLoss) has been created as a new loss to inhibit abnormal categories in the
same area. A new evaluation metric called ``AD-Acc.'' is introduced to address
the shortcomings of existing methods in addressing negative samples.
Comprehensive experimental results demonstrate that our approach achieves
state-of-the-art performance in four metrics across two benchmark datasets.
Specifically, our model achieves an improvement of \textbf{0.7}$\uparrow$,
\textbf{2.0}$\uparrow$, \textbf{1.6}$\uparrow$, and \textbf{11.0}$\uparrow$ in
SC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated
dataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.

</details>


### [53] [Depth Anything at Any Condition](https://arxiv.org/abs/2507.01634)
*Boyuan Sun,Modi Jin,Bowen Yin,Qibin Hou*

Main category: cs.CV

TL;DR: DepthAnything-AC是一种基础单目深度估计模型，能在多样环境条件下工作，通过无监督一致性正则化微调和空间距离约束提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基础MDE模型在复杂开放世界环境中表现不佳，如光照变化、恶劣天气和传感器失真等挑战条件。

Method: 提出无监督一致性正则化微调范式，仅需少量未标记数据，并引入空间距离约束以学习块级相对关系。

Result: 实验显示DepthAnything-AC在多种基准测试中具有零样本能力，包括恶劣天气、合成失真和通用场景。

Conclusion: DepthAnything-AC在多样化环境条件下表现出色，解决了数据稀缺和伪标签质量低的问题。

Abstract: We present Depth Anything at Any Condition (DepthAnything-AC), a foundation
monocular depth estimation (MDE) model capable of handling diverse
environmental conditions. Previous foundation MDE models achieve impressive
performance across general scenes but not perform well in complex open-world
environments that involve challenging conditions, such as illumination
variations, adverse weather, and sensor-induced distortions. To overcome the
challenges of data scarcity and the inability of generating high-quality
pseudo-labels from corrupted images, we propose an unsupervised consistency
regularization finetuning paradigm that requires only a relatively small amount
of unlabeled data. Furthermore, we propose the Spatial Distance Constraint to
explicitly enforce the model to learn patch-level relative relationships,
resulting in clearer semantic boundaries and more accurate details.
Experimental results demonstrate the zero-shot capabilities of DepthAnything-AC
across diverse benchmarks, including real-world adverse weather benchmarks,
synthetic corruption benchmarks, and general benchmarks.
  Project Page: https://ghost233lism.github.io/depthanything-AC-page
  Code: https://github.com/HVision-NKU/DepthAnythingAC

</details>


### [54] [SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via Gradual Feature Refinement](https://arxiv.org/abs/2507.01643)
*Weijie Yin,Dingkang Yang,Hongyuan Dong,Zijian Kang,Jiacong Wang,Xiao Liang,Chao Feng,Jiao Ran*

Main category: cs.CV

TL;DR: SAILViT是一种渐进式特征学习的ViT，用于提升多模态大语言模型（MLLMs）在复杂多模态交互中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有ViTs在直接与LLMs协同训练时存在参数初始化和模态语义差距问题，需要一种更有效的特征对齐方法。

Method: 提出SAILViT，通过渐进式特征细化实现从粗到细的特征对齐和知识注入。

Result: SAILViT在不同参数规模、架构、训练策略和数据规模下表现出强大的鲁棒性和泛化能力，显著提升MLLMs性能。

Conclusion: SAILViT为MLLMs提供了更高效的特征学习方案，并在OpenCompass基准测试中取得显著改进。

Abstract: Vision Transformers (ViTs) are essential as foundation backbones in
establishing the visual comprehension capabilities of Multimodal Large Language
Models (MLLMs). Although most ViTs achieve impressive performance through
image-text pair-based contrastive learning or self-supervised mechanisms, they
struggle to engage in connector-based co-training directly with LLMs due to
potential parameter initialization conflicts and modality semantic gaps. To
address the above challenges, this paper proposes SAILViT, a gradual feature
learning-enhanced ViT for facilitating MLLMs to break through performance
bottlenecks in complex multimodal interactions. SAILViT achieves
coarse-to-fine-grained feature alignment and world knowledge infusion with
gradual feature refinement, which better serves target training demands. We
perform thorough empirical analyses to confirm the powerful robustness and
generalizability of SAILViT across different dimensions, including parameter
sizes, model architectures, training strategies, and data scales. Equipped with
SAILViT, existing MLLMs show significant and consistent performance
improvements on the OpenCompass benchmark across extensive downstream tasks.
SAILViT series models are released at
https://huggingface.co/BytedanceDouyinContent.

</details>


### [55] [Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective](https://arxiv.org/abs/2507.01652)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Hui Deng,Xuyang Shen,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: 论文提出了一种新型注意力机制LASAD，通过保留2D空间关系解决了线性注意力在图像生成中的性能下降问题，并基于此构建了高效的自回归图像生成模型LASADGen。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型依赖Transformer架构，存在计算复杂度和内存开销高的问题，而线性注意力机制在图像生成中因无法捕捉长程依赖导致性能下降。

Method: 提出Linear Attention with Spatial-Aware Decay (LASAD)机制，通过基于真实2D空间位置的衰减因子保留空间关系，并构建LASADGen模型。

Result: 在ImageNet上实验表明，LASADGen在图像生成质量和计算效率上达到最优。

Conclusion: LASADGen成功平衡了线性注意力的高效性和高质量图像生成所需的空间理解能力。

Abstract: Autoregressive (AR) models have garnered significant attention in image
generation for their ability to effectively capture both local and global
structures within visual data. However, prevalent AR models predominantly rely
on the transformer architectures, which are beset by quadratic computational
complexity concerning input sequence length and substantial memory overhead due
to the necessity of maintaining key-value caches. Although linear attention
mechanisms have successfully reduced this burden in language models, our
initial experiments reveal that they significantly degrade image generation
quality because of their inability to capture critical long-range dependencies
in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a
novel attention mechanism that explicitly preserves genuine 2D spatial
relationships within the flattened image sequences by computing
position-dependent decay factors based on true 2D spatial location rather than
1D sequence positions. Based on this mechanism, we present LASADGen, an
autoregressive image generator that enables selective attention to relevant
spatial contexts with linear complexity. Experiments on ImageNet show LASADGen
achieves state-of-the-art image generation performance and computational
efficiency, bridging the gap between linear attention's efficiency and spatial
understanding needed for high-quality generation.

</details>


### [56] [RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather](https://arxiv.org/abs/2507.01653)
*Yuran Wang,Yingping Liang,Yutao Hu,Ying Fu*

Main category: cs.CV

TL;DR: RobuSTereo框架通过扩散模拟和稳健特征编码，提升立体匹配模型在恶劣天气下的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决恶劣天气下立体匹配模型因数据稀缺和特征提取困难导致的泛化问题。

Method: 1. 扩散模拟生成高质量恶劣天气数据；2. 结合ConvNet和去噪Transformer的稳健特征编码。

Result: 显著提升模型在多种恶劣天气下的鲁棒性和泛化能力。

Conclusion: RobuSTereo有效解决了恶劣天气下立体匹配的挑战，具有广泛的应用潜力。

Abstract: Learning-based stereo matching models struggle in adverse weather conditions
due to the scarcity of corresponding training data and the challenges in
extracting discriminative features from degraded images. These limitations
significantly hinder zero-shot generalization to out-of-distribution weather
conditions. In this paper, we propose \textbf{RobuSTereo}, a novel framework
that enhances the zero-shot generalization of stereo matching models under
adverse weather by addressing both data scarcity and feature extraction
challenges. First, we introduce a diffusion-based simulation pipeline with a
stereo consistency module, which generates high-quality stereo data tailored
for adverse conditions. By training stereo matching models on our synthetic
datasets, we reduce the domain gap between clean and degraded images,
significantly improving the models' robustness to unseen weather conditions.
The stereo consistency module ensures structural alignment across synthesized
image pairs, preserving geometric integrity and enhancing depth estimation
accuracy. Second, we design a robust feature encoder that combines a
specialized ConvNet with a denoising transformer to extract stable and reliable
features from degraded images. The ConvNet captures fine-grained local
structures, while the denoising transformer refines global representations,
effectively mitigating the impact of noise, low visibility, and weather-induced
distortions. This enables more accurate disparity estimation even under
challenging visual conditions. Extensive experiments demonstrate that
\textbf{RobuSTereo} significantly improves the robustness and generalization of
stereo matching models across diverse adverse weather scenarios.

</details>


### [57] [SPoT: Subpixel Placement of Tokens in Vision Transformers](https://arxiv.org/abs/2507.01654)
*Martine Hjelkrem-Tan,Marius Aasan,Gabriel Y. Arteaga,Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: SPoT是一种新的tokenization策略，通过连续放置token来避免网格限制，显著提升性能并减少推理所需的token数量。


<details>
  <summary>Details</summary>
Motivation: 标准tokenization方法限制了稀疏性利用，SPoT旨在解决这一问题。

Method: 提出Subpixel Placement of Tokens (SPoT)策略，结合oracle-guided搜索优化token位置。

Result: SPoT显著提升性能，减少推理token数量。

Conclusion: SPoT为ViT架构提供了灵活、高效且可解释的新方向，将稀疏性转化为优势。

Abstract: Vision Transformers naturally accommodate sparsity, yet standard tokenization
methods confine features to discrete patch grids. This constraint prevents
models from fully exploiting sparse regimes, forcing awkward compromises. We
propose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that
positions tokens continuously within images, effectively sidestepping
grid-based limitations. With our proposed oracle-guided search, we uncover
substantial performance gains achievable with ideal subpixel token positioning,
drastically reducing the number of tokens necessary for accurate predictions
during inference. SPoT provides a new direction for flexible, efficient, and
interpretable ViT architectures, redefining sparsity as a strategic advantage
rather than an imposed limitation.

</details>


### [58] [What does really matter in image goal navigation?](https://arxiv.org/abs/2507.01667)
*Gianluca Monaci,Philippe Weinzaepfel,Christian Wolf*

Main category: cs.CV

TL;DR: 研究探讨了端到端强化学习在图像目标导航任务中的有效性，分析了架构选择的影响，并揭示了模拟器设置对方法成功的影响。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以通过端到端强化学习训练完整代理来解决图像目标导航任务，从而避免依赖专用图像匹配或预训练模块。

Method: 通过大规模研究分析不同架构选择（如晚期融合、通道堆叠、空间到深度投影和交叉注意力）对导航训练中相对姿态估计器的影响。

Result: 发现模拟器设置对方法成功有影响，但部分能力可迁移到更现实场景；导航性能与相对姿态估计性能相关。

Conclusion: 端到端强化学习在图像目标导航中有效，但需注意模拟器设置的潜在影响，且导航性能与关键子技能相关。

Abstract: Image goal navigation requires two different skills: firstly, core navigation
skills, including the detection of free space and obstacles, and taking
decisions based on an internal representation; and secondly, computing
directional information by comparing visual observations to the goal image.
Current state-of-the-art methods either rely on dedicated image-matching, or
pre-training of computer vision modules on relative pose estimation. In this
paper, we study whether this task can be efficiently solved with end-to-end
training of full agents with RL, as has been claimed by recent work. A positive
answer would have impact beyond Embodied AI and allow training of relative pose
estimation from reward for navigation alone. In a large study we investigate
the effect of architectural choices like late fusion, channel stacking,
space-to-depth projections and cross-attention, and their role in the emergence
of relative pose estimators from navigation training. We show that the success
of recent methods is influenced up to a certain extent by simulator settings,
leading to shortcuts in simulation. However, we also show that these
capabilities can be transferred to more realistic setting, up to some extend.
We also find evidence for correlations between navigation performance and
probed (emerging) relative pose estimation performance, an important sub skill.

</details>


### [59] [Facial Emotion Learning with Text-Guided Multiview Fusion via Vision-Language Model for 3D/4D Facial Expression Recognition](https://arxiv.org/abs/2507.01673)
*Muzammil Behzad*

Main category: cs.CV

TL;DR: FACET-VLM是一个用于3D/4D面部表情识别的视觉语言框架，通过多视角面部表示学习和自然语言提示的语义引导，实现了高精度识别。


<details>
  <summary>Details</summary>
Motivation: 3D/4D面部表情识别在情感计算中具有挑战性，但对人类行为理解、医疗监测和人机交互至关重要。

Method: 提出CVSA用于视角一致融合，MTGF用于语义对齐，以及多视角一致性损失确保结构连贯。

Result: 在多个基准测试中达到最先进水平，并成功扩展到4D微表情识别。

Conclusion: FACET-VLM为多模态面部表情识别提供了强大、可扩展的解决方案。

Abstract: Facial expression recognition (FER) in 3D and 4D domains presents a
significant challenge in affective computing due to the complexity of spatial
and temporal facial dynamics. Its success is crucial for advancing applications
in human behavior understanding, healthcare monitoring, and human-computer
interaction. In this work, we propose FACET-VLM, a vision-language framework
for 3D/4D FER that integrates multiview facial representation learning with
semantic guidance from natural language prompts. FACET-VLM introduces three key
components: Cross-View Semantic Aggregation (CVSA) for view-consistent fusion,
Multiview Text-Guided Fusion (MTGF) for semantically aligned facial emotions,
and a multiview consistency loss to enforce structural coherence across views.
Our model achieves state-of-the-art accuracy across multiple benchmarks,
including BU-3DFE, Bosphorus, BU-4DFE, and BP4D-Spontaneous. We further extend
FACET-VLM to 4D micro-expression recognition (MER) on the 4DME dataset,
demonstrating strong performance in capturing subtle, short-lived emotional
cues. The extensive experimental results confirm the effectiveness and
substantial contributions of each individual component within the framework.
Overall, FACET-VLM offers a robust, extensible, and high-performing solution
for multimodal FER in both posed and spontaneous settings.

</details>


### [60] [Component Adaptive Clustering for Generalized Category Discovery](https://arxiv.org/abs/2507.01711)
*Mingfu Yan,Jiancheng Huang,Yifan Liu,Shifeng Chen*

Main category: cs.CV

TL;DR: AdaGCD是一种基于自适应槽注意力的聚类对比学习框架，用于解决广义类别发现（GCD）问题，无需预定义类别数量。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖预定义类别数量的假设，限制了处理真实数据复杂性的能力。

Method: 提出AdaGCD框架，结合自适应槽注意力（AdaSlot）动态确定槽数量，实现灵活聚类。

Result: 在公开和细粒度数据集上的实验验证了方法的有效性，特别是利用空间局部信息的能力。

Conclusion: AdaGCD通过自适应表示和动态槽分配，显著提升了开放场景下的类别发现能力。

Abstract: Generalized Category Discovery (GCD) tackles the challenging problem of
categorizing unlabeled images into both known and novel classes within a
partially labeled dataset, without prior knowledge of the number of unknown
categories. Traditional methods often rely on rigid assumptions, such as
predefining the number of classes, which limits their ability to handle the
inherent variability and complexity of real-world data. To address these
shortcomings, we propose AdaGCD, a cluster-centric contrastive learning
framework that incorporates Adaptive Slot Attention (AdaSlot) into the GCD
framework. AdaSlot dynamically determines the optimal number of slots based on
data complexity, removing the need for predefined slot counts. This adaptive
mechanism facilitates the flexible clustering of unlabeled data into known and
novel categories by dynamically allocating representational capacity. By
integrating adaptive representation with dynamic slot allocation, our method
captures both instance-specific and spatially clustered features, improving
class discovery in open-world scenarios. Extensive experiments on public and
fine-grained datasets validate the effectiveness of our framework, emphasizing
the advantages of leveraging spatial local information for category discovery
in unlabeled image datasets.

</details>


### [61] [Soft Self-labeling and Potts Relaxations for Weakly-Supervised Segmentation](https://arxiv.org/abs/2507.01721)
*Zhongwen Zhang,Yuri Boykov*

Main category: cs.CV

TL;DR: 论文提出了一种基于软自标记的弱监督分割方法，通过优化CRF/Potts损失的松弛形式，显著提升了基于涂鸦标签的训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用硬伪标签无法表示类别不确定性或错误，因此提出软自标记以改进网络训练。

Method: 提出了一种基于软伪标签的辅助损失，并系统评估了标准和新CRF松弛形式、邻域系统以及网络预测与软伪标签的连接项。

Result: 软自标记方法在标准架构下显著优于复杂的专用WSSS系统，甚至优于全像素级监督。

Conclusion: 软自标记方法在弱监督分割中表现优异，其思想可推广到其他弱监督问题/系统。

Abstract: We consider weakly supervised segmentation where only a fraction of pixels
have ground truth labels (scribbles) and focus on a self-labeling approach
optimizing relaxations of the standard unsupervised CRF/Potts loss on unlabeled
pixels. While WSSS methods can directly optimize such losses via gradient
descent, prior work suggests that higher-order optimization can improve network
training by introducing hidden pseudo-labels and powerful CRF sub-problem
solvers, e.g. graph cut. However, previously used hard pseudo-labels can not
represent class uncertainty or errors, which motivates soft self-labeling. We
derive a principled auxiliary loss and systematically evaluate standard and new
CRF relaxations (convex and non-convex), neighborhood systems, and terms
connecting network predictions with soft pseudo-labels. We also propose a
general continuous sub-problem solver. Using only standard architectures, soft
self-labeling consistently improves scribble-based training and outperforms
significantly more complex specialized WSSS systems. It can outperform full
pixel-precise supervision. Our general ideas apply to other weakly-supervised
problems/systems.

</details>


### [62] [When Does Pruning Benefit Vision Representations?](https://arxiv.org/abs/2507.01722)
*Enrico Cassano,Riccardo Renzulli,Andrea Bragagnolo,Marco Grangetto*

Main category: cs.CV

TL;DR: 该论文研究了剪枝对视觉模型在可解释性、无监督对象发现和人类感知对齐三个方面的影响，发现稀疏模型存在性能提升的“甜点”，但其效果依赖于网络架构和规模。


<details>
  <summary>Details</summary>
Motivation: 剪枝广泛用于降低深度学习模型的复杂度，但其对可解释性和表示学习的影响尚不明确。

Method: 分析不同视觉网络架构在不同稀疏度下对特征归因可解释性方法的影响，并探索剪枝是否促进更简洁的结构化表示，以及是否增强模型与人类感知的对齐。

Result: 研究发现稀疏模型存在“甜点”，即在某些条件下表现出更高的可解释性、下游泛化能力和人类对齐性，但这些效果高度依赖于网络架构和规模。

Conclusion: 剪枝对视觉表示的影响复杂，需进一步研究其何时及如何发挥作用。

Abstract: Pruning is widely used to reduce the complexity of deep learning models, but
its effects on interpretability and representation learning remain poorly
understood. This paper investigates how pruning influences vision models across
three key dimensions: (i) interpretability, (ii) unsupervised object discovery,
and (iii) alignment with human perception. We first analyze different vision
network architectures to examine how varying sparsity levels affect feature
attribution interpretability methods. Additionally, we explore whether pruning
promotes more succinct and structured representations, potentially improving
unsupervised object discovery by discarding redundant information while
preserving essential features. Finally, we assess whether pruning enhances the
alignment between model representations and human perception, investigating
whether sparser models focus on more discriminative features similarly to
humans. Our findings also reveal the presence of sweet spots, where sparse
models exhibit higher interpretability, downstream generalization and human
alignment. However, these spots highly depend on the network architectures and
their size in terms of trainable parameters. Our results suggest a complex
interplay between these three dimensions, highlighting the importance of
investigating when and how pruning benefits vision representations.

</details>


### [63] [ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving](https://arxiv.org/abs/2507.01735)
*Kai Chen,Ruiyuan Gao,Lanqing Hong,Hang Xu,Xu Jia,Holger Caesar,Dengxin Dai,Bingbing Liu,Dzmitry Tsishkou,Songcen Xu,Chunjing Xu,Qiang Xu,Huchuan Lu,Dit-Yan Yeung*

Main category: cs.CV

TL;DR: 首届W-CODA研讨会概述，聚焦自动驾驶极端场景的多模态感知与理解技术。


<details>
  <summary>Details</summary>
Motivation: 探索下一代自动驾驶解决方案，填补前沿技术与可靠自动驾驶之间的差距。

Method: 邀请5位学术与工业界专家分享最新进展，收集研究论文并举办双轨挑战赛（极端场景理解与生成）。

Result: 研讨会作为先驱努力，推动自动驾驶技术在极端场景下的鲁棒性。

Conclusion: W-CODA将持续致力于前沿自动驾驶技术与可靠智能体之间的桥梁作用。

Abstract: In this paper, we present details of the 1st W-CODA workshop, held in
conjunction with the ECCV 2024. W-CODA aims to explore next-generation
solutions for autonomous driving corner cases, empowered by state-of-the-art
multimodal perception and comprehension techniques. 5 Speakers from both
academia and industry are invited to share their latest progress and opinions.
We collect research papers and hold a dual-track challenge, including both
corner case scene understanding and generation. As the pioneering effort, we
will continuously bridge the gap between frontier autonomous driving techniques
and fully intelligent, reliable self-driving agents robust towards corner
cases.

</details>


### [64] [HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion](https://arxiv.org/abs/2507.01737)
*Lin Wu,Zhixiang Chen,Jianglin Lan*

Main category: cs.CV

TL;DR: HOI-Dyn框架通过驱动-响应系统生成3D人-物交互，利用轻量级Transformer模型预测物体响应，并通过动态损失提升一致性和质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法独立处理人和物体运动，导致物理上不合理和因果不一致的行为，需要更精确的交互动态建模。

Method: 提出HOI-Dyn框架，将交互建模为驱动-响应系统，使用Transformer预测物体响应，并引入动态损失优化训练。

Result: 实验表明，HOI-Dyn提升了交互生成质量，并提供了可行的评估指标。

Conclusion: HOI-Dyn通过动态建模和损失优化，显著改善了3D人-物交互的生成质量。

Abstract: Generating realistic 3D human-object interactions (HOIs) remains a
challenging task due to the difficulty of modeling detailed interaction
dynamics. Existing methods treat human and object motions independently,
resulting in physically implausible and causally inconsistent behaviors. In
this work, we present HOI-Dyn, a novel framework that formulates HOI generation
as a driver-responder system, where human actions drive object responses. At
the core of our method is a lightweight transformer-based interaction dynamics
model that explicitly predicts how objects should react to human motion. To
further enforce consistency, we introduce a residual-based dynamics loss that
mitigates the impact of dynamics prediction errors and prevents misleading
optimization signals. The dynamics model is used only during training,
preserving inference efficiency. Through extensive qualitative and quantitative
experiments, we demonstrate that our approach not only enhances the quality of
HOI generation but also establishes a feasible metric for evaluating the
quality of generated interactions.

</details>


### [65] [DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image Segmentation through Loopback Synergy](https://arxiv.org/abs/2507.01738)
*Ming Dai,Wenxuan Cheng,Jiang-jiang Liu,Sen Yang,Wenxiao Cai,Yanpeng Sun,Wankou Yang*

Main category: cs.CV

TL;DR: DeRIS框架将Referring Image Segmentation（RIS）分解为感知和认知两部分，发现当前模型的瓶颈在于多模态认知能力不足，并提出Loopback Synergy机制和非参照样本数据增强方法以提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有RIS框架缺乏对性能瓶颈的系统分析，尤其是多模态认知能力的不足。

Method: 提出DeRIS框架，分解RIS为感知和认知模块，引入Loopback Synergy机制和非参照样本数据增强。

Result: DeRIS在多场景下表现出色，无需专门架构修改即可适应非参照和多参照场景。

Conclusion: DeRIS通过模块化分析和改进机制，显著提升了RIS任务的性能与泛化能力。

Abstract: Referring Image Segmentation (RIS) is a challenging task that aims to segment
objects in an image based on natural language expressions. While prior studies
have predominantly concentrated on improving vision-language interactions and
achieving fine-grained localization, a systematic analysis of the fundamental
bottlenecks in existing RIS frameworks remains underexplored. To bridge this
gap, we propose DeRIS, a novel framework that decomposes RIS into two key
components: perception and cognition. This modular decomposition facilitates a
systematic analysis of the primary bottlenecks impeding RIS performance. Our
findings reveal that the predominant limitation lies not in perceptual
deficiencies, but in the insufficient multi-modal cognitive capacity of current
models. To mitigate this, we propose a Loopback Synergy mechanism, which
enhances the synergy between the perception and cognition modules, thereby
enabling precise segmentation while simultaneously improving robust image-text
comprehension. Additionally, we analyze and introduce a simple non-referent
sample conversion data augmentation to address the long-tail distribution issue
related to target existence judgement in general scenarios. Notably, DeRIS
demonstrates inherent adaptability to both non- and multi-referents scenarios
without requiring specialized architectural modifications, enhancing its
general applicability. The codes and models are available at
https://github.com/Dmmm1997/DeRIS.

</details>


### [66] [Calibrated Self-supervised Vision Transformers Improve Intracranial Arterial Calcification Segmentation from Clinical CT Head Scans](https://arxiv.org/abs/2507.01744)
*Benjamin Jin,Grant Mair,Joanna M. Wardlaw,Maria del C. Valdés Hernández*

Main category: cs.CV

TL;DR: 该论文探讨了在3D医学图像分割中使用Vision Transformers（ViTs）的潜力，特别是针对颅内动脉钙化（IAC）的自动量化。通过自监督的MAE框架预训练ViTs，并在IST-3临床试验数据上微调，结果显示ViTs在分割性能和临床风险分类上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 3D ViTs在医学图像分割中表现不佳，但其自监督训练能力（如MAE框架）可以避免昂贵的人工标注需求。IAC作为神经血管疾病的生物标志物，其自动量化有助于大规模风险评估。

Method: 使用MAE框架预训练ViTs，并在IST-3临床试验的异构数据上微调，用于IAC分割。研究了ViTs的关键参数（如patch大小和上采样方法）对性能的影响。

Result: 1）自监督ViT在Dice分数上比监督nnU-Net基线高3.2分；2）小patch尺寸和插值上采样对ViTs性能至关重要；3）ViTs提高了对高切片厚度的鲁棒性，临床风险分类提升了46%。

Conclusion: ViTs在3D医学图像分割中具有潜力，尤其是在自监督训练和临床风险评估方面表现优异，为IAC量化提供了高效解决方案。

Abstract: Vision Transformers (ViTs) have gained significant popularity in the natural
image domain but have been less successful in 3D medical image segmentation.
Nevertheless, 3D ViTs are particularly interesting for large medical imaging
volumes due to their efficient self-supervised training within the masked
autoencoder (MAE) framework, which enables the use of imaging data without the
need for expensive manual annotations. intracranial arterial calcification
(IAC) is an imaging biomarker visible on routinely acquired CT scans linked to
neurovascular diseases such as stroke and dementia, and automated IAC
quantification could enable their large-scale risk assessment. We pre-train
ViTs with MAE and fine-tune them for IAC segmentation for the first time. To
develop our models, we use highly heterogeneous data from a large clinical
trial, the third International Stroke Trial (IST-3). We evaluate key aspects of
MAE pre-trained ViTs in IAC segmentation, and analyse the clinical
implications. We show: 1) our calibrated self-supervised ViT beats a strong
supervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial
for ViTs for IAC segmentation and interpolation upsampling with regular
convolutions is preferable to transposed convolutions for ViT-based models, and
3) our ViTs increase robustness to higher slice thicknesses and improve risk
group classification in a clinical scenario by 46%. Our code is available
online.

</details>


### [67] [SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery](https://arxiv.org/abs/2507.01747)
*Nora Gourmelon,Marcel Dreier,Martin Mayr,Thorsten Seehaus,Dakota Pyles,Matthias Braun,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: 论文提出两种自监督多模态预训练技术和一种混合模型架构，用于从SAR图像中提取冰川崩解前沿位置，显著提升了精度。


<details>
  <summary>Details</summary>
Motivation: 冰川冰量快速流失，需要更精确的全年监测方法，特别是针对崩解过程的驱动因素。现有基于ImageNet预训练的模型因领域差异表现不佳。

Method: 提出两种自监督多模态预训练技术，利用新数据集SSL4SAR，并结合Swin Transformer编码器和残差CNN解码器的混合架构。

Result: 模型在CaFFe基准数据集上平均距离误差为293米，优于之前最佳模型67米；集成模型误差降至75米，接近人类水平的38米。

Conclusion: 该技术显著提升了冰川崩解前沿的季节性变化监测精度。

Abstract: Glaciers are losing ice mass at unprecedented rates, increasing the need for
accurate, year-round monitoring to understand frontal ablation, particularly
the factors driving the calving process. Deep learning models can extract
calving front positions from Synthetic Aperture Radar imagery to track seasonal
ice losses at the calving fronts of marine- and lake-terminating glaciers. The
current state-of-the-art model relies on ImageNet-pretrained weights. However,
they are suboptimal due to the domain shift between the natural images in
ImageNet and the specialized characteristics of remote sensing imagery, in
particular for Synthetic Aperture Radar imagery. To address this challenge, we
propose two novel self-supervised multimodal pretraining techniques that
leverage SSL4SAR, a new unlabeled dataset comprising 9,563 Sentinel-1 and 14
Sentinel-2 images of Arctic glaciers, with one optical image per glacier in the
dataset. Additionally, we introduce a novel hybrid model architecture that
combines a Swin Transformer encoder with a residual Convolutional Neural
Network (CNN) decoder. When pretrained on SSL4SAR, this model achieves a mean
distance error of 293 m on the "CAlving Fronts and where to Find thEm" (CaFFe)
benchmark dataset, outperforming the prior best model by 67 m. Evaluating an
ensemble of the proposed model on a multi-annotator study of the benchmark
dataset reveals a mean distance error of 75 m, approaching the human
performance of 38 m. This advancement enables precise monitoring of seasonal
changes in glacier calving fronts.

</details>


### [68] [Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis](https://arxiv.org/abs/2507.01756)
*Peng Zheng,Junke Wang,Yi Chang,Yizhou Yu,Rui Ma,Zuxuan Wu*

Main category: cs.CV

TL;DR: DisCon框架通过将离散标记作为条件信号而非生成目标，解决了连续表示建模的优化挑战，同时避免了量化带来的信息损失，显著提升了图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决自回归视觉生成模型中量化过程导致的信息损失问题，同时避免连续表示建模的挑战。

Method: 提出DisCon框架，通过离散标记作为条件信号建模连续表示的条件概率。

Result: 在ImageNet 256×256生成任务上，DisCon的gFID得分为1.38，优于现有自回归方法。

Conclusion: DisCon在避免信息损失和优化挑战的同时，显著提升了图像生成性能。

Abstract: Recent advances in large language models (LLMs) have spurred interests in
encoding images as discrete tokens and leveraging autoregressive (AR)
frameworks for visual generation. However, the quantization process in AR-based
visual generation models inherently introduces information loss that degrades
image fidelity. To mitigate this limitation, recent studies have explored to
autoregressively predict continuous tokens. Unlike discrete tokens that reside
in a structured and bounded space, continuous representations exist in an
unbounded, high-dimensional space, making density estimation more challenging
and increasing the risk of generating out-of-distribution artifacts. Based on
the above findings, this work introduces DisCon (Discrete-Conditioned
Continuous Autoregressive Model), a novel framework that reinterprets discrete
tokens as conditional signals rather than generation targets. By modeling the
conditional probability of continuous representations conditioned on discrete
tokens, DisCon circumvents the optimization challenges of continuous token
modeling while avoiding the information loss caused by quantization. DisCon
achieves a gFID score of 1.38 on ImageNet 256$\times$256 generation,
outperforming state-of-the-art autoregressive approaches by a clear margin.

</details>


### [69] [Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging](https://arxiv.org/abs/2507.01788)
*Montasir Shams,Chashi Mahiul Islam,Shaeke Salman,Phat Tran,Xiuwen Liu*

Main category: cs.CV

TL;DR: Vision transformers (ViTs)在医学影像任务中表现优异，但其表示缺乏语义意义且对微小变化敏感，导致分类结果不可靠。


<details>
  <summary>Details</summary>
Motivation: 研究ViT在医学影像分类中的表示是否具有语义意义，以及其对微小变化的鲁棒性。

Method: 使用基于投影梯度的算法分析ViT的表示。

Result: ViT的表示缺乏语义意义，对微小变化敏感，分类准确率可下降60%以上。

Conclusion: ViT在医学影像分类中的表示存在根本性问题，可能影响其在安全关键系统中的部署。

Abstract: Vision transformers (ViTs) have rapidly gained prominence in medical imaging
tasks such as disease classification, segmentation, and detection due to their
superior accuracy compared to conventional deep learning models. However, due
to their size and complex interactions via the self-attention mechanism, they
are not well understood. In particular, it is unclear whether the
representations produced by such models are semantically meaningful. In this
paper, using a projected gradient-based algorithm, we show that their
representations are not semantically meaningful and they are inherently
vulnerable to small changes. Images with imperceptible differences can have
very different representations; on the other hand, images that should belong to
different semantic classes can have nearly identical representations. Such
vulnerability can lead to unreliable classification results; for example,
unnoticeable changes cause the classification accuracy to be reduced by over
60\%. %. To the best of our knowledge, this is the first work to systematically
demonstrate this fundamental lack of semantic meaningfulness in ViT
representations for medical image classification, revealing a critical
challenge for their deployment in safety-critical systems.

</details>


### [70] [Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation](https://arxiv.org/abs/2507.01791)
*Zihong Guo,Chen Wan,Yayin Zheng,Hailing Kuang,Xiaohai Lu*

Main category: cs.CV

TL;DR: 提出了一种新的分段高斯金字塔（SGP）攻击方法，通过多尺度图像增强对抗样本的迁移性，显著提高了对黑盒防御模型的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 对抗样本的迁移性对深度神经网络构成重大安全威胁，现有方法多关注单尺度图像，限制了攻击效果。

Method: 采用高斯滤波和三种下采样方法构建多尺度样本，计算各尺度损失函数的梯度并取平均以确定对抗扰动。

Result: 实验表明，SGP显著提高了对黑盒防御模型的攻击成功率，平均提升2.3%至32.6%。

Conclusion: SGP作为一种可扩展的输入变换方法，易于集成到现有对抗攻击中，有效提升迁移性。

Abstract: The transferability of adversarial examples poses a significant security
challenge for deep neural networks, which can be attacked without knowing
anything about them. In this paper, we propose a new Segmented Gaussian Pyramid
(SGP) attack method to enhance the transferability, particularly against
defense models. Unlike existing methods that generally focus on single-scale
images, our approach employs Gaussian filtering and three types of downsampling
to construct a series of multi-scale examples. Then, the gradients of the loss
function with respect to each scale are computed, and their average is used to
determine the adversarial perturbations. The proposed SGP can be considered an
input transformation with high extensibility that is easily integrated into
most existing adversarial attacks. Extensive experiments demonstrate that in
contrast to the state-of-the-art methods, SGP significantly enhances attack
success rates against black-box defense models, with average attack success
rates increasing by 2.3% to 32.6%, based only on transferability.

</details>


### [71] [FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization](https://arxiv.org/abs/2507.01792)
*Peng Zheng,Ye Wang,Rui Ma,Zuxuan Wu*

Main category: cs.CV

TL;DR: FreeLoRA是一种无需训练的框架，通过融合多个特定主题的LoRA模块实现多主题个性化图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多主题个性化生成中存在复杂调优或联合优化的问题，FreeLoRA旨在简化这一过程。

Method: 采用Full Token Tuning策略训练特定主题的LoRA模块，并通过Subject-Aware Inference在推理时激活对应模块。

Result: 实验表明FreeLoRA在主题保真度和提示一致性方面表现优异。

Conclusion: FreeLoRA为多主题个性化图像生成提供了一种简单且通用的解决方案。

Abstract: Subject-driven image generation plays a crucial role in applications such as
virtual try-on and poster design. Existing approaches typically fine-tune
pretrained generative models or apply LoRA-based adaptations for individual
subjects. However, these methods struggle with multi-subject personalization,
as combining independently adapted modules often requires complex re-tuning or
joint optimization. We present FreeLoRA, a simple and generalizable framework
that enables training-free fusion of subject-specific LoRA modules for
multi-subject personalization. Each LoRA module is adapted on a few images of a
specific subject using a Full Token Tuning strategy, where it is applied across
all tokens in the prompt to encourage weakly supervised token-content
alignment. At inference, we adopt Subject-Aware Inference, activating each
module only on its corresponding subject tokens. This enables training-free
fusion of multiple personalized subjects within a single image, while
mitigating overfitting and mutual interference between subjects. Extensive
experiments show that FreeLoRA achieves strong performance in both subject
fidelity and prompt consistency.

</details>


### [72] [HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing Supervision](https://arxiv.org/abs/2507.01800)
*Shengli Zhou,Jianuo Zhu,Qilin Huang,Fangjing Wang,Yanfu Zhang,Feng Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种名为HCNQA的3D视觉问答模型，通过分层监督方法解决现有答案中心监督方法在推理路径上的不足，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D VQA模型仅监督最终输出，可能导致推理路径的浅层捷径问题，且慢思考方法存在欠思考问题。

Method: 提出分层浓度窄化监督方法，模拟人类从广泛到具体的注意力聚焦过程，分三阶段监督推理路径。

Result: 实验表明，该方法能有效确保模型发展合理的推理路径，并取得更好性能。

Conclusion: HCNQA通过分层监督解决了现有方法的不足，提升了3D VQA任务的性能。

Abstract: 3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the
physical world and perform spatial reasoning. Answer-centric supervision is a
commonly used training method for 3D VQA models. Many models that utilize this
strategy have achieved promising results in 3D VQA tasks. However, the
answer-centric approach only supervises the final output of models and allows
models to develop reasoning pathways freely. The absence of supervision on the
reasoning pathway enables the potential for developing superficial shortcuts
through common patterns in question-answer pairs. Moreover, although
slow-thinking methods advance large language models, they suffer from
underthinking. To address these issues, we propose \textbf{HCNQA}, a 3D VQA
model leveraging a hierarchical concentration narrowing supervision method. By
mimicking the human process of gradually focusing from a broad area to specific
objects while searching for answers, our method guides the model to perform
three phases of concentration narrowing through hierarchical supervision. By
supervising key checkpoints on a general reasoning pathway, our method can
ensure the development of a rational and effective reasoning pathway. Extensive
experimental results demonstrate that our method can effectively ensure that
the model develops a rational reasoning pathway and performs better. The code
is available at https://github.com/JianuoZhu/HCNQA.

</details>


### [73] [AMD: Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction](https://arxiv.org/abs/2507.01801)
*Bin Rao,Haicheng Liao,Yanchen Guan,Chengyue Wang,Bonan Wang,Jiaxun Zhang,Zhenning Li*

Main category: cs.CV

TL;DR: 提出了一种自适应动量和解耦对比学习框架（AMD），用于提升自动驾驶中长尾轨迹预测的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有研究在预测长尾轨迹时未考虑多样性和不确定性的问题。

Method: 结合改进的动量对比学习（MoCo-DT）和解耦对比学习（DCL），设计轨迹随机增强方法和在线迭代聚类策略。

Result: 在nuScenes和ETH/UCY数据集上，AMD在长尾轨迹预测和整体准确性上表现最优。

Conclusion: AMD框架有效提升了长尾轨迹预测能力，并展示了出色的整体性能。

Abstract: Accurately predicting the future trajectories of traffic agents is essential
in autonomous driving. However, due to the inherent imbalance in trajectory
distributions, tail data in natural datasets often represents more complex and
hazardous scenarios. Existing studies typically rely solely on a base model's
prediction error, without considering the diversity and uncertainty of
long-tail trajectory patterns. We propose an adaptive momentum and decoupled
contrastive learning framework (AMD), which integrates unsupervised and
supervised contrastive learning strategies. By leveraging an improved momentum
contrast learning (MoCo-DT) and decoupled contrastive learning (DCL) module,
our framework enhances the model's ability to recognize rare and complex
trajectories. Additionally, we design four types of trajectory random
augmentation methods and introduce an online iterative clustering strategy,
allowing the model to dynamically update pseudo-labels and better adapt to the
distributional shifts in long-tail data. We propose three different criteria to
define long-tail trajectories and conduct extensive comparative experiments on
the nuScenes and ETH$/$UCY datasets. The results show that AMD not only
achieves optimal performance in long-tail trajectory prediction but also
demonstrates outstanding overall prediction accuracy.

</details>


### [74] [Modulate and Reconstruct: Learning Hyperspectral Imaging from Misaligned Smartphone Views](https://arxiv.org/abs/2507.01835)
*Daniil Reutsky,Daniil Vladimirov,Yasin Mamedov,Georgy Perevozchikov,Nancy Mehta,Egor Ershov,Radu Timofte*

Main category: cs.CV

TL;DR: 提出了一种基于多图像的超光谱重建（MI-HSR）框架，利用三摄像头智能手机系统提升重建精度。


<details>
  <summary>Details</summary>
Motivation: 传统单RGB图像方法因光谱信息丢失严重，重建精度受限。

Method: 采用配备特定光谱滤镜的双镜头三摄像头系统，结合理论和实证分析，提出MI-HSR框架，并引入首个数据集Doomer。

Result: 新方法在基准测试中表现优于现有方法，光谱估计精度提升30%。

Conclusion: 多视角光谱滤波结合商用硬件可实现更准确、实用的超光谱成像。

Abstract: Hyperspectral reconstruction (HSR) from RGB images is a fundamentally
ill-posed problem due to severe spectral information loss. Existing approaches
typically rely on a single RGB image, limiting reconstruction accuracy. In this
work, we propose a novel multi-image-to-hyperspectral reconstruction (MI-HSR)
framework that leverages a triple-camera smartphone system, where two lenses
are equipped with carefully selected spectral filters. Our configuration,
grounded in theoretical and empirical analysis, enables richer and more diverse
spectral observations than conventional single-camera setups. To support this
new paradigm, we introduce Doomer, the first dataset for MI-HSR, comprising
aligned images from three smartphone cameras and a hyperspectral reference
camera across diverse scenes. We show that the proposed HSR model achieves
consistent improvements over existing methods on the newly proposed benchmark.
In a nutshell, our setup allows 30% towards more accurately estimated spectra
compared to an ordinary RGB camera. Our findings suggest that multi-view
spectral filtering with commodity hardware can unlock more accurate and
practical hyperspectral imaging solutions.

</details>


### [75] [MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices](https://arxiv.org/abs/2507.01838)
*Hailong Yan,Ao Li,Xiangtao Zhang,Zhe Liu,Zenglin Shi,Ce Zhu,Le Zhang*

Main category: cs.CV

TL;DR: 提出了一种超轻量级CNN框架，用于移动设备上的实时图像增强，仅需4K参数，支持1,100 FPS。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在资源受限平台上部署的高计算和内存需求问题。

Method: 结合重参数化和增量权重优化策略，引入特征自变换模块和分层双路径注意力机制，使用局部方差加权损失优化。

Result: 首次实现1,100 FPS的实时图像增强，同时保持竞争力的图像质量。

Conclusion: 该框架在速度和性能之间取得了最佳平衡，适用于多种图像增强任务。

Abstract: Recent advancements in deep neural networks have driven significant progress
in image enhancement (IE). However, deploying deep learning models on
resource-constrained platforms, such as mobile devices, remains challenging due
to high computation and memory demands. To address these challenges and
facilitate real-time IE on mobile, we introduce an extremely lightweight
Convolutional Neural Network (CNN) framework with around 4K parameters. Our
approach integrates reparameterization with an Incremental Weight Optimization
strategy to ensure efficiency. Additionally, we enhance performance with a
Feature Self-Transform module and a Hierarchical Dual-Path Attention mechanism,
optimized with a Local Variance-Weighted loss. With this efficient framework,
we are the first to achieve real-time IE inference at up to 1,100 frames per
second (FPS) while delivering competitive image quality, achieving the best
trade-off between speed and performance across multiple IE tasks. The code will
be available at https://github.com/AVC2-UESTC/MobileIE.git.

</details>


### [76] [Future Slot Prediction for Unsupervised Object Discovery in Surgical Video](https://arxiv.org/abs/2507.01882)
*Guiqiu Liao,Matjaz Jogan,Marcel Hussing,Edward Zhang,Eric Eaton,Daniel A. Hashimoto*

Main category: cs.CV

TL;DR: 论文提出了一种动态时序槽变换器（DTST）模块，用于解决手术视频中对象中心表示学习的问题，并在多个手术数据库中取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界应用（如手术视频）中的异构场景难以解析为有意义的一组槽，现有方法在手术视频上表现不佳。

Method: 提出动态时序槽变换器（DTST）模块，结合时序推理和预测未来最优槽初始化。

Result: 在多个手术数据库中实现了最优性能。

Conclusion: 无监督对象中心方法可应用于现实世界数据，成为医疗应用中的常见工具。

Abstract: Object-centric slot attention is an emerging paradigm for unsupervised
learning of structured, interpretable object-centric representations (slots).
This enables effective reasoning about objects and events at a low
computational cost and is thus applicable to critical healthcare applications,
such as real-time interpretation of surgical video. The heterogeneous scenes in
real-world applications like surgery are, however, difficult to parse into a
meaningful set of slots. Current approaches with an adaptive slot count perform
well on images, but their performance on surgical videos is low. To address
this challenge, we propose a dynamic temporal slot transformer (DTST) module
that is trained both for temporal reasoning and for predicting the optimal
future slot initialization. The model achieves state-of-the-art performance on
multiple surgical databases, demonstrating that unsupervised object-centric
methods can be applied to real-world data and become part of the common arsenal
in healthcare applications.

</details>


### [77] [Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for Semi-Supervised Lifelong Person Re-Identification](https://arxiv.org/abs/2507.01884)
*Kunlun Xu,Fan Zhuo,Jiangmeng Li,Xu Zou,Jiahuan Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种自增强原型进化与双知识协作框架（SPRED），用于解决半监督终身行人重识别（Semi-LReID）问题，通过动态原型生成高质量伪标签并利用新旧知识协作净化噪声。


<details>
  <summary>Details</summary>
Motivation: 现实场景中标注资源有限，现有方法在利用未标注数据时性能下降严重，因此需要一种能有效利用未标注数据并长期适应的新方法。

Method: 引入可学习的身份原型动态捕捉身份分布并生成伪标签，通过双知识协作方案整合当前模型和历史模型知识以净化噪声。

Result: 在Semi-LReID基准测试中，SPRED取得了最先进的性能。

Conclusion: SPRED通过自增强循环设计，显著提升了半监督终身行人重识别的性能。

Abstract: Current lifelong person re-identification (LReID) methods predominantly rely
on fully labeled data streams. However, in real-world scenarios where
annotation resources are limited, a vast amount of unlabeled data coexists with
scarce labeled samples, leading to the Semi-Supervised LReID (Semi-LReID)
problem where LReID methods suffer severe performance degradation. Existing
LReID methods, even when combined with semi-supervised strategies, suffer from
limited long-term adaptation performance due to struggling with the noisy
knowledge occurring during unlabeled data utilization. In this paper, we
pioneer the investigation of Semi-LReID, introducing a novel Self-Reinforcing
Prototype Evolution with Dual-Knowledge Cooperation framework (SPRED). Our key
innovation lies in establishing a self-reinforcing cycle between dynamic
prototype-guided pseudo-label generation and new-old knowledge collaborative
purification to enhance the utilization of unlabeled data. Specifically,
learnable identity prototypes are introduced to dynamically capture the
identity distributions and generate high-quality pseudo-labels. Then, the
dual-knowledge cooperation scheme integrates current model specialization and
historical model generalization, refining noisy pseudo-labels. Through this
cyclic design, reliable pseudo-labels are progressively mined to improve
current-stage learning and ensure positive knowledge propagation over long-term
learning. Experiments on the established Semi-LReID benchmarks show that our
SPRED achieves state-of-the-art performance. Our source code is available at
https://github.com/zhoujiahuan1991/ICCV2025-SPRED

</details>


### [78] [Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning](https://arxiv.org/abs/2507.01908)
*Qingdong He,Xueqin Chen,Chaoyi Wang,Yanjie Pan,Xiaobin Hu,Zhenye Gan,Yabiao Wang,Chengjie Wang,Xiangtai Li,Jiangning Zhang*

Main category: cs.CV

TL;DR: 论文提出Reason50K数据集和ReasonBrain框架，用于解决复杂隐含假设指令的图像编辑问题，通过细粒度推理线索提取和多模态增强提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有指令图像编辑方法难以处理复杂隐含假设指令，缺乏支持推理的数据集和架构。

Method: 提出Reason50K数据集和ReasonBrain框架，结合MLLM和扩散模型，引入FRCE和CME模块。

Result: ReasonBrain在推理场景中表现优异，并在传统任务上展示零样本泛化能力。

Conclusion: ReasonBrain和Reason50K为复杂指令图像编辑提供了有效解决方案，数据集和代码将公开。

Abstract: Instruction-based image editing (IIE) has advanced rapidly with the success
of diffusion models. However, existing efforts primarily focus on simple and
explicit instructions to execute editing operations such as adding, deleting,
moving, or swapping objects. They struggle to handle more complex implicit
hypothetical instructions that require deeper reasoning to infer plausible
visual changes and user intent. Additionally, current datasets provide limited
support for training and evaluating reasoning-aware editing capabilities.
Architecturally, these methods also lack mechanisms for fine-grained detail
extraction that support such reasoning. To address these limitations, we
propose Reason50K, a large-scale dataset specifically curated for training and
evaluating hypothetical instruction reasoning image editing, along with
ReasonBrain, a novel framework designed to reason over and execute implicit
hypothetical instructions across diverse scenarios. Reason50K includes over 50K
samples spanning four key reasoning scenarios: Physical, Temporal, Causal, and
Story reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs)
for editing guidance generation and a diffusion model for image synthesis,
incorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture
detailed visual and textual semantics essential for supporting instruction
reasoning. To mitigate the semantic loss, we further introduce a Cross-Modal
Enhancer (CME) that enables rich interactions between the fine-grained cues and
MLLM-derived features. Extensive experiments demonstrate that ReasonBrain
consistently outperforms state-of-the-art baselines on reasoning scenarios
while exhibiting strong zero-shot generalization to conventional IIE tasks. Our
dataset and code will be released publicly.

</details>


### [79] [Modality Agnostic, patient-specific digital twins modeling temporally varying digestive motion](https://arxiv.org/abs/2507.01909)
*Jorge Tapias Gomez,Nishant Nadkarni,Lando S. Bosma,Jue Jiang,Ergys D. Subashi,William P. Segars,James M. Balter,Mert R Sabuncu,Neelam Tyagi,Harini Veeraraghavan*

Main category: cs.CV

TL;DR: 提出了一种基于数字孪生（DT）的管道，用于评估可变形图像配准（DIR）在胃肠道（GI）器官运动中的准确性，并通过多种指标验证了DIR方法的性能。


<details>
  <summary>Details</summary>
Motivation: 临床实现DIR需要基于体素的空间精度指标，但胃肠道器官的高移动性使得手动标记具有挑战性。数字孪生模型可以模拟真实运动，从而解决这一问题。

Method: 通过半自动化管道，从静态3D患者扫描生成21个运动阶段的4D序列，模拟胃肠道运动。使用11个数据集（包括MRI和CT扫描）评估DIR方法的性能，包括目标配准误差、Dice相似系数和Hausdorff距离等指标。

Result: 数字孪生模型成功模拟了真实的胃肠道运动，运动幅度与真实患者数据相似。DIR方法的性能通过多种指标得到验证，并评估了剂量映射的准确性。

Conclusion: 该管道能够严格测试DIR工具在动态复杂区域的性能，为临床提供了空间和剂量精度的详细评估方法。

Abstract: Objective: Clinical implementation of deformable image registration (DIR)
requires voxel-based spatial accuracy metrics such as manually identified
landmarks, which are challenging to implement for highly mobile
gastrointestinal (GI) organs. To address this, patient-specific digital twins
(DT) modeling temporally varying motion were created to assess the accuracy of
DIR methods. Approach: 21 motion phases simulating digestive GI motion as 4D
sequences were generated from static 3D patient scans using published
analytical GI motion models through a semi-automated pipeline. Eleven datasets,
including six T2w FSE MRI (T2w MRI), two T1w 4D golden-angle stack-of-stars,
and three contrast-enhanced CT scans. The motion amplitudes of the DTs were
assessed against real patient stomach motion amplitudes extracted from
independent 4D MRI datasets. The generated DTs were then used to assess six
different DIR methods using target registration error, Dice similarity
coefficient, and the 95th percentile Hausdorff distance using summary metrics
and voxel-level granular visualizations. Finally, for a subset of T2w MRI scans
from patients treated with MR-guided radiation therapy, dose distributions were
warped and accumulated to assess dose warping errors, including evaluations of
DIR performance in both low- and high-dose regions for patient-specific error
estimation. Main results: Our proposed pipeline synthesized DTs modeling
realistic GI motion, achieving mean and maximum motion amplitudes and a mean
log Jacobian determinant within 0.8 mm and 0.01, respectively, similar to
published real-patient gastric motion data. It also enables the extraction of
detailed quantitative DIR performance metrics and rigorous validation of dose
mapping accuracy. Significance: The pipeline enables rigorously testing DIR
tools for dynamic, anatomically complex regions enabling granular spatial and
dosimetric accuracies.

</details>


### [80] [3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP](https://arxiv.org/abs/2507.01912)
*Ranjan Sapkota,Zhichao Meng,Martin Churuvija,Xiaoqiang Du,Zenghong Ma,Manoj Karkee*

Main category: cs.CV

TL;DR: 提出了一种多季节信息融合框架，结合休眠期和生长期的RGB-D图像，通过实例分割、3D重建和模型对齐，实现果园自动化管理。


<details>
  <summary>Details</summary>
Motivation: 果园自动化中，生长期的密集树叶遮挡了树的结构，限制了机器视觉系统的能力，而休眠期树的结构更清晰可见。

Method: 使用YOLOv9-Seg进行实例分割，Kinect Fusion进行3D重建，Fast GICP进行模型对齐，融合多季节数据。

Result: YOLOv9-Seg的MSE为0.0047，mAP@50为0.78；Kinect Fusion的RMSE为5.23 mm（树干直径）、4.50 mm（树枝直径）、13.72 mm（树枝间距）；Fast GICP的配准精度为0.00197。

Conclusion: 多季节融合框架提高了机器人系统对树结构的建模能力，提升了修剪和疏果等自动化操作的精度。

Abstract: In orchard automation, dense foliage during the canopy season severely
occludes tree structures, minimizing visibility to various canopy parts such as
trunks and branches, which limits the ability of a machine vision system.
However, canopy structure is more open and visible during the dormant season
when trees are defoliated. In this work, we present an information fusion
framework that integrates multi-seasonal structural data to support robotic and
automated crop load management during the entire growing season. The framework
combines high-resolution RGB-D imagery from both dormant and canopy periods
using YOLOv9-Seg for instance segmentation, Kinect Fusion for 3D
reconstruction, and Fast Generalized Iterative Closest Point (Fast GICP) for
model alignment. Segmentation outputs from YOLOv9-Seg were used to extract
depth-informed masks, which enabled accurate 3D point cloud reconstruction via
Kinect Fusion; these reconstructed models from each season were subsequently
aligned using Fast GICP to achieve spatially coherent multi-season fusion. The
YOLOv9-Seg model, trained on manually annotated images, achieved a mean squared
error (MSE) of 0.0047 and segmentation mAP@50 scores up to 0.78 for trunks in
dormant season dataset. Kinect Fusion enabled accurate reconstruction of tree
geometry, validated with field measurements resulting in root mean square
errors (RMSE) of 5.23 mm for trunk diameter, 4.50 mm for branch diameter, and
13.72 mm for branch spacing. Fast GICP achieved precise cross-seasonal
registration with a minimum fitness score of 0.00197, allowing integrated,
comprehensive tree structure modeling despite heavy occlusions during the
growing season. This fused structural representation enables robotic systems to
access otherwise obscured architectural information, improving the precision of
pruning, thinning, and other automated orchard operations.

</details>


### [81] [IC-Custom: Diverse Image Customization via In-Context Learning](https://arxiv.org/abs/2507.01926)
*Yaowei Li,Xiaoyu Li,Zhaoyang Zhang,Yuxuan Bian,Gan Liu,Xinyuan Li,Jiale Xu,Wenbo Hu,Yating Liu,Lingen Li,Jing Cai,Yuexian Zou,Yancheng He,Ying Shan*

Main category: cs.CV

TL;DR: IC-Custom是一个统一的图像定制框架，通过上下文学习整合位置感知和无位置定制，支持多种工业应用，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前图像定制方法缺乏通用框架，限制了多样化场景的应用。

Method: 提出IC-Custom框架，利用多模态注意力机制和任务导向的注册令牌，结合高质量数据集。

Result: 在多个基准测试中显著优于现有方法，人类偏好提升73%，仅训练0.4%的原始参数。

Conclusion: IC-Custom为图像定制提供了高效、通用的解决方案，适用于多种工业场景。

Abstract: Image customization, a crucial technique for industrial media production,
aims to generate content that is consistent with reference images. However,
current approaches conventionally separate image customization into
position-aware and position-free customization paradigms and lack a universal
framework for diverse customization, limiting their applications across various
scenarios. To overcome these limitations, we propose IC-Custom, a unified
framework that seamlessly integrates position-aware and position-free image
customization through in-context learning. IC-Custom concatenates reference
images with target images to a polyptych, leveraging DiT's multi-modal
attention mechanism for fine-grained token-level interactions. We introduce the
In-context Multi-Modal Attention (ICMA) mechanism with learnable task-oriented
register tokens and boundary-aware positional embeddings to enable the model to
correctly handle different task types and distinguish various inputs in
polyptych configurations. To bridge the data gap, we carefully curated a
high-quality dataset of 12k identity-consistent samples with 8k from real-world
sources and 4k from high-quality synthetic data, avoiding the overly glossy and
over-saturated synthetic appearance. IC-Custom supports various industrial
applications, including try-on, accessory placement, furniture arrangement, and
creative IP customization. Extensive evaluations on our proposed ProductBench
and the publicly available DreamBench demonstrate that IC-Custom significantly
outperforms community workflows, closed-source models, and state-of-the-art
open-source approaches. IC-Custom achieves approximately 73% higher human
preference across identity consistency, harmonicity, and text alignment
metrics, while training only 0.4% of the original model parameters. Project
page: https://liyaowei-stu.github.io/project/IC_Custom

</details>


### [82] [evMLP: An Efficient Event-Driven MLP Architecture for Vision](https://arxiv.org/abs/2507.01927)
*Zhentan Zheng*

Main category: cs.CV

TL;DR: 该论文提出了一种名为evMLP的新型视觉模型架构，通过事件驱动的局部更新机制，选择性处理图像或特征图中的变化区域，显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 探索多层感知机（MLPs）在视觉模型架构中的应用，并解决视频处理中的冗余计算问题。

Method: 提出evMLP模型，结合事件驱动机制，仅处理帧间变化区域（事件），避免冗余计算。

Result: 在ImageNet分类任务中表现优异，视频数据集上显著降低计算成本，同时保持输出一致性。

Conclusion: evMLP通过事件驱动机制，为高效视觉模型设计提供了新思路，特别适合视频处理任务。

Abstract: Deep neural networks have achieved remarkable results in computer vision
tasks. In the early days, Convolutional Neural Networks (CNNs) were the
mainstream architecture. In recent years, Vision Transformers (ViTs) have
become increasingly popular. In addition, exploring applications of multi-layer
perceptrons (MLPs) has provided new perspectives for research into vision model
architectures. In this paper, we present evMLP accompanied by a simple
event-driven local update mechanism. The proposed evMLP can independently
process patches on images or feature maps via MLPs. We define changes between
consecutive frames as "events". Under the event-driven local update mechanism,
evMLP selectively processes patches where events occur. For sequential image
data (e.g., video processing), this approach improves computational performance
by avoiding redundant computations. Through ImageNet image classification
experiments, evMLP attains accuracy competitive with state-of-the-art models.
More significantly, experimental results on multiple video datasets demonstrate
that evMLP reduces computational cost via its event-driven local update
mechanism while maintaining output consistency with its non-event-driven
baseline. The code and trained models are available at
https://github.com/i-evi/evMLP.

</details>


### [83] [CI-VID: A Coherent Interleaved Text-Video Dataset](https://arxiv.org/abs/2507.01938)
*Yiming Ju,Jijin Hu,Zhengxiong Luo,Haoge Deng,hanyu Zhao,Li Du,Chengwei Wu,Donglin Hao,Xinlong Wang,Tengfei Pan*

Main category: cs.CV

TL;DR: 论文介绍了CI-VID数据集，用于支持连贯多场景视频序列的生成，超越了现有的孤立文本-视频对数据集。


<details>
  <summary>Details</summary>
Motivation: 现有公共数据集多为孤立文本-视频对，无法支持连贯多场景视频序列的建模。

Method: 提出CI-VID数据集，包含34万样本，每个样本为连贯视频片段序列，附带文本描述。设计了多维基准验证其有效性。

Result: 实验表明，使用CI-VID训练的模型在生成视频序列时，准确性和内容一致性显著提升。

Conclusion: CI-VID数据集支持生成故事驱动内容，具有平滑视觉过渡和时间连贯性，质量和实用性高。

Abstract: Text-to-video (T2V) generation has recently attracted considerable attention,
resulting in the development of numerous high-quality datasets that have
propelled progress in this area. However, existing public datasets are
primarily composed of isolated text-video (T-V) pairs and thus fail to support
the modeling of coherent multi-clip video sequences. To address this
limitation, we introduce CI-VID, a dataset that moves beyond isolated
text-to-video (T2V) generation toward text-and-video-to-video (TV2V)
generation, enabling models to produce coherent, multi-scene video sequences.
CI-VID contains over 340,000 samples, each featuring a coherent sequence of
video clips with text captions that capture both the individual content of each
clip and the transitions between them, enabling visually and textually grounded
generation. To further validate the effectiveness of CI-VID, we design a
comprehensive, multi-dimensional benchmark incorporating human evaluation,
VLM-based assessment, and similarity-based metrics. Experimental results
demonstrate that models trained on CI-VID exhibit significant improvements in
both accuracy and content consistency when generating video sequences. This
facilitates the creation of story-driven content with smooth visual transitions
and strong temporal coherence, underscoring the quality and practical utility
of the CI-VID dataset We release the CI-VID dataset and the accompanying code
for data construction and evaluation at: https://github.com/ymju-BAAI/CI-VID

</details>


### [84] [LongAnimation: Long Animation Generation with Dynamic Global-Local Memory](https://arxiv.org/abs/2507.01945)
*Nan Chen,Mengqi Huang,Yihao Meng,Zhendong Mao*

Main category: cs.CV

TL;DR: 论文提出了一种动态全局-局部范式（LongAnimation框架），用于解决长动画着色中的颜色一致性问题，包括SketchDiT、DGLM模块和颜色一致性奖励。


<details>
  <summary>Details</summary>
Motivation: 长动画着色在动画产业中成本高昂，现有研究局限于短时着色且忽视全局信息，导致长期颜色一致性不足。

Method: 提出LongAnimation框架，结合SketchDiT提取混合参考特征，DGLM模块动态压缩全局历史特征并与当前生成特征融合，引入颜色一致性奖励优化。

Result: 在短时（14帧）和长时（平均500帧）动画上的实验验证了LongAnimation在保持颜色一致性方面的有效性。

Conclusion: 动态全局-局部范式显著提升了长动画着色的颜色一致性，适用于开放域动画着色任务。

Abstract: Animation colorization is a crucial part of real animation industry
production. Long animation colorization has high labor costs. Therefore,
automated long animation colorization based on the video generation model has
significant research value. Existing studies are limited to short-term
colorization. These studies adopt a local paradigm, fusing overlapping features
to achieve smooth transitions between local segments. However, the local
paradigm neglects global information, failing to maintain long-term color
consistency. In this study, we argue that ideal long-term color consistency can
be achieved through a dynamic global-local paradigm, i.e., dynamically
extracting global color-consistent features relevant to the current generation.
Specifically, we propose LongAnimation, a novel framework, which mainly
includes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color
Consistency Reward. The SketchDiT captures hybrid reference features to support
the DGLM module. The DGLM module employs a long video understanding model to
dynamically compress global historical features and adaptively fuse them with
the current generation features. To refine the color consistency, we introduce
a Color Consistency Reward. During inference, we propose a color consistency
fusion to smooth the video segment transition. Extensive experiments on both
short-term (14 frames) and long-term (average 500 frames) animations show the
effectiveness of LongAnimation in maintaining short-term and long-term color
consistency for open-domain animation colorization task. The code can be found
at https://cn-makers.github.io/long_animation_web/.

</details>


### [85] [Kwai Keye-VL Technical Report](https://arxiv.org/abs/2507.01949)
*Kwai Keye Team,Biao Yang,Bin Wen,Changyi Liu,Chenglong Chu,Chengru Song,Chongling Rao,Chuan Yi,Da Li,Dunju Zang,Fan Yang,Guorui Zhou,Hao Peng,Haojie Ding,Jiaming Huang,Jiangxia Cao,Jiankang Chen,Jingyun Hua,Jin Ouyang,Kaibing Chen,Kaiyu Jiang,Kaiyu Tang,Kun Gai,Shengnan Zhang,Siyang Mao,Sui Huang,Tianke Zhang,Tingting Gao,Wei Chen,Wei Yuan,Xiangyu Wu,Xiao Hu,Xingyu Lu,Yang Zhou,Yi-Fan Zhang,Yiping Yang,Yulong Chen,Zhenhua Wu,Zhenyu Li,Zhixin Ling,Ziming Li,Dehua Ma,Di Xu,Haixuan Gao,Hang Li,Jiawei Guo,Jing Wang,Lejian Ren,Muhao Wei,Qianqian Wang,Qigen Hu,Shiyao Wang,Tao Yu,Xinchen Luo,Yan Li,Yiming Liang,Yuhang Hu,Zeyi Lu,Zhuoran Yang,Zixing Zhang*

Main category: cs.CV

TL;DR: Kwai Keye-VL是一个80亿参数的多模态基础模型，专为短视频理解设计，通过创新的训练方法和高质量数据集实现了领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）在静态图像上表现优异，但在动态、信息密集的短视频理解上表现不足。

Method: 采用四阶段预训练和两阶段后训练方法，包括创新的五模式数据混合和强化学习优化。

Result: Keye-VL在公共视频基准测试中达到最优性能，并在通用图像任务中保持竞争力。

Conclusion: Kwai Keye-VL填补了短视频理解的空白，并通过新基准KC-MMBench验证了其优势。

Abstract: While Multimodal Large Language Models (MLLMs) demonstrate remarkable
capabilities on static images, they often fall short in comprehending dynamic,
information-dense short-form videos, a dominant medium in today's digital
landscape. To bridge this gap, we introduce \textbf{Kwai Keye-VL}, an
8-billion-parameter multimodal foundation model engineered for leading-edge
performance in short-video understanding while maintaining robust
general-purpose vision-language abilities. The development of Keye-VL rests on
two core pillars: a massive, high-quality dataset exceeding 600 billion tokens
with a strong emphasis on video, and an innovative training recipe. This recipe
features a four-stage pre-training process for solid vision-language alignment,
followed by a meticulous two-phase post-training process. The first
post-training stage enhances foundational capabilities like instruction
following, while the second phase focuses on stimulating advanced reasoning. In
this second phase, a key innovation is our five-mode ``cold-start'' data
mixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think
with image'', and high-quality video data. This mixture teaches the model to
decide when and how to reason. Subsequent reinforcement learning (RL) and
alignment steps further enhance these reasoning capabilities and correct
abnormal model behaviors, such as repetitive outputs. To validate our approach,
we conduct extensive evaluations, showing that Keye-VL achieves
state-of-the-art results on public video benchmarks and remains highly
competitive on general image-based tasks (Figure 1). Furthermore, we develop
and release the \textbf{KC-MMBench}, a new benchmark tailored for real-world
short-video scenarios, where Keye-VL shows a significant advantage.

</details>


### [86] [FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model](https://arxiv.org/abs/2507.01953)
*Yukang Cao,Chenyang Si,Jinghao Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: FreeMorph是一种无需调优的图像变形方法，适用于不同语义或布局的输入，通过创新设计实现高质量变形，速度提升10~50倍。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练扩散模型的微调，受限于时间和语义/布局差异，FreeMorph旨在解决这些问题。

Method: 1) 提出引导感知的球面插值设计，修改自注意力模块；2) 引入步进式变化趋势，混合自注意力模块以实现可控过渡。

Result: FreeMorph在图像变形任务中表现优异，速度提升10~50倍，达到新SOTA。

Conclusion: FreeMorph通过创新设计解决了现有方法的局限性，实现了高效高质量的图像变形。

Abstract: We present FreeMorph, the first tuning-free method for image morphing that
accommodates inputs with different semantics or layouts. Unlike existing
methods that rely on finetuning pre-trained diffusion models and are limited by
time constraints and semantic/layout discrepancies, FreeMorph delivers
high-fidelity image morphing without requiring per-instance training. Despite
their efficiency and potential, tuning-free methods face challenges in
maintaining high-quality results due to the non-linear nature of the multi-step
denoising process and biases inherited from the pre-trained diffusion model. In
this paper, we introduce FreeMorph to address these challenges by integrating
two key innovations. 1) We first propose a guidance-aware spherical
interpolation design that incorporates explicit guidance from the input images
by modifying the self-attention modules, thereby addressing identity loss and
ensuring directional transitions throughout the generated sequence. 2) We
further introduce a step-oriented variation trend that blends self-attention
modules derived from each input image to achieve controlled and consistent
transitions that respect both inputs. Our extensive evaluations demonstrate
that FreeMorph outperforms existing methods, being 10x ~ 50x faster and
establishing a new state-of-the-art for image morphing.

</details>


### [87] [How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks](https://arxiv.org/abs/2507.01955)
*Rahul Ramachandran,Ali Garjani,Roman Bachmann,Andrei Atanov,Oğuzhan Fatih Kar,Amir Zamir*

Main category: cs.CV

TL;DR: 论文评估了多模态基础模型在标准计算机视觉任务上的表现，发现它们虽不及专业模型，但作为通用模型表现尚可，且语义任务优于几何任务。GPT-4o在非推理模型中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究多模态基础模型（如GPT-4o）在视觉理解方面的实际能力，填补现有研究的空白。

Method: 通过提示链将标准视觉任务转化为文本可提示和API兼容的任务，建立标准化评估框架。

Result: 模型在语义任务上表现优于几何任务，GPT-4o在非推理模型中表现最佳，但整体不及专业模型。

Conclusion: 多模态基础模型在视觉任务中表现尚可，但仍有改进空间，尤其是几何任务和提示敏感性方面。

Abstract: Multimodal foundation models, such as GPT-4o, have recently made remarkable
progress, but it is not clear where exactly these models stand in terms of
understanding vision. In this paper, we benchmark the performance of popular
multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0
Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision
tasks (semantic segmentation, object detection, image classification, depth and
surface normal prediction) using established datasets (e.g., COCO, ImageNet and
its variants, etc).
  The main challenges to performing this are: 1) most models are trained to
output text and cannot natively express versatile domains, such as segments or
3D geometry, and 2) many leading models are proprietary and accessible only at
an API level, i.e., there is no weight access to adapt them. We address these
challenges by translating standard vision tasks into equivalent text-promptable
and API-compatible tasks via prompt chaining to create a standardized
benchmarking framework.
  We observe that 1) the models are not close to the state-of-the-art
specialist models at any task. However, 2) they are respectable generalists;
this is remarkable as they are presumably trained on primarily image-text-based
tasks. 3) They perform semantic tasks notably better than geometric ones. 4)
While the prompt-chaining techniques affect performance, better models exhibit
less sensitivity to prompt variations. 5) GPT-4o performs the best among
non-reasoning models, securing the top position in 4 out of 6 tasks, 6)
reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a
preliminary analysis of models with native image generation, like the latest
GPT-4o, shows they exhibit quirks like hallucinations and spatial
misalignments.

</details>


### [88] [Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation](https://arxiv.org/abs/2507.01957)
*Zhuoyang Zhang,Luke J. Huang,Chengyue Wu,Shang Yang,Kelly Peng,Yao Lu,Song Han*

Main category: cs.CV

TL;DR: Locality-aware Parallel Decoding (LPD) 通过灵活并行自回归建模和局部感知生成顺序，显著加速自回归图像生成，减少生成步骤且不损失质量。


<details>
  <summary>Details</summary>
Motivation: 传统自回归图像生成依赖顺序预测，导致高延迟，现有并行化方法效果有限。LPD旨在实现高并行化同时保持生成质量。

Method: 1. 灵活并行自回归建模：支持任意生成顺序和并行度，通过可学习位置查询令牌确保并行解码一致性。2. 局部感知生成顺序：最小化组内依赖，最大化上下文支持。

Result: 在ImageNet类条件生成中，生成步骤从256降至20（256×256分辨率），1024降至48（512×512分辨率），延迟降低至少3.4倍。

Conclusion: LPD通过创新架构和调度策略，高效加速自回归图像生成，为高质量并行解码提供了新思路。

Abstract: We present Locality-aware Parallel Decoding (LPD) to accelerate
autoregressive image generation. Traditional autoregressive image generation
relies on next-patch prediction, a memory-bound process that leads to high
latency. Existing works have tried to parallelize next-patch prediction by
shifting to multi-patch prediction to accelerate the process, but only achieved
limited parallelization. To achieve high parallelization while maintaining
generation quality, we introduce two key techniques: (1) Flexible Parallelized
Autoregressive Modeling, a novel architecture that enables arbitrary generation
ordering and degrees of parallelization. It uses learnable position query
tokens to guide generation at target positions while ensuring mutual visibility
among concurrently generated tokens for consistent parallel decoding. (2)
Locality-aware Generation Ordering, a novel schedule that forms groups to
minimize intra-group dependencies and maximize contextual support, enhancing
generation quality. With these designs, we reduce the generation steps from 256
to 20 (256$\times$256 res.) and 1024 to 48 (512$\times$512 res.) without
compromising quality on the ImageNet class-conditional generation, and
achieving at least 3.4$\times$ lower latency than previous parallelized
autoregressive models.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [89] [Prompt Mechanisms in Medical Imaging: A Comprehensive Survey](https://arxiv.org/abs/2507.01055)
*Hao Yang,Xinlong Liang,Zhang Li,Yue Sun,Zheyu Hu,Xinghe Xie,Behdad Dashtbozorg,Jincheng Huang,Shiwei Zhu,Luyi Han,Jiong Zhang,Shanshan Wang,Ritse Mann,Qifeng Yu,Tao Tan*

Main category: eess.IV

TL;DR: 本文系统回顾了基于提示的深度学习方法在医学影像中的应用，分析了其提升模型性能、适应性和可解释性的潜力，同时指出了优化设计和临床部署的挑战。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像中深度学习面临的数据稀缺、分布偏移和任务泛化问题，探索提示方法如何提升模型性能。

Method: 综述了文本指令、视觉提示和可学习嵌入等多种提示模态，及其在图像生成、分割和分类任务中的应用。

Result: 提示机制显著提高了任务准确性、鲁棒性和数据效率，同时减少了对人工特征工程的依赖并增强了模型可解释性。

Conclusion: 尽管取得了进展，提示设计优化和数据异质性仍是挑战，未来方向包括多模态提示和临床集成，以推动医学诊断和个性化治疗的发展。

Abstract: Deep learning offers transformative potential in medical imaging, yet its
clinical adoption is frequently hampered by challenges such as data scarcity,
distribution shifts, and the need for robust task generalization. Prompt-based
methodologies have emerged as a pivotal strategy to guide deep learning models,
providing flexible, domain-specific adaptations that significantly enhance
model performance and adaptability without extensive retraining. This
systematic review critically examines the burgeoning landscape of prompt
engineering in medical imaging. We dissect diverse prompt modalities, including
textual instructions, visual prompts, and learnable embeddings, and analyze
their integration for core tasks such as image generation, segmentation, and
classification. Our synthesis reveals how these mechanisms improve
task-specific outcomes by enhancing accuracy, robustness, and data efficiency
and reducing reliance on manual feature engineering while fostering greater
model interpretability by making the model's guidance explicit. Despite
substantial advancements, we identify persistent challenges, particularly in
prompt design optimization, data heterogeneity, and ensuring scalability for
clinical deployment. Finally, this review outlines promising future
trajectories, including advanced multimodal prompting and robust clinical
integration, underscoring the critical role of prompt-driven AI in accelerating
the revolution of diagnostics and personalized treatment planning in medicine.

</details>


### [90] [MID-INFRARED (MIR) OCT-based inspection in industry](https://arxiv.org/abs/2507.01074)
*N. P. García-de-la-Puente,Rocío del Amor,Fernando García-Torres,Niels Møller Israelsen,Coraline Lapre,Christian Rosenberg Petersen,Ole Bang,Dominik Brouczek,Martin Schwentenwein,Kevin Neumann,Niels Benson,Valery Naranjo*

Main category: eess.IV

TL;DR: 评估中红外光学相干断层扫描（MIR-OCT）系统穿透材料并检测次表面异常的能力，探索其在工业无损检测中的应用。


<details>
  <summary>Details</summary>
Motivation: 为工业生产过程提供无损检测工具，提升对复合材料及陶瓷等材料的次表面异常检测能力。

Method: 通过多次采集复合材料及陶瓷数据，评估系统性能，并研究预处理及AI增强视觉算法在异常检测中的应用。

Result: 探讨了系统参数选择的优化标准，并分析了其在异常检测中的优势与局限性。

Conclusion: MIR-OCT系统在工业无损检测中具有潜力，但需进一步优化参数和算法以提高检测精度。

Abstract: This paper aims to evaluate mid-infrared (MIR) Optical Coherence Tomography
(OCT) systems as a tool to penetrate different materials and detect sub-surface
irregularities. This is useful for monitoring production processes, allowing
Non-Destructive Inspection Techniques of great value to the industry. In this
exploratory study, several acquisitions are made on composite and ceramics to
know the capabilities of the system. In addition, it is assessed which
preprocessing and AI-enhanced vision algorithms can be anomaly-detection
methodologies capable of detecting abnormal zones in the analyzed objects.
Limitations and criteria for the selection of optimal parameters will be
discussed, as well as strengths and weaknesses will be highlighted.

</details>


### [91] [LotteryCodec: Searching the Implicit Representation in a Random Network for Low-Complexity Image Compression](https://arxiv.org/abs/2507.01204)
*Haotian Wu,Gongpu Chen,Pier Luigi Dragotti,Deniz Gündüz*

Main category: eess.IV

TL;DR: 彩票编码器假设提出，随机初始化网络中的未训练子网络可用于过拟合图像压缩，性能媲美训练网络。基于此，LotteryCodec通过编码图像统计到网络子结构，实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 探索随机初始化网络中的子网络是否可用于高效图像压缩，以简化传统训练网络的复杂过程。

Method: 提出LotteryCodec，利用过参数化随机网络和二进制掩码过拟合单张图像，并通过重绕调制机制优化子网络搜索。

Result: LotteryCodec在单图像压缩中超越VTM，达到新SOTA，并支持通过掩码比例调整解码复杂度。

Conclusion: 彩票编码器假设为图像压缩提供新范式，LotteryCodec展示了高效、灵活的压缩潜力。

Abstract: We introduce and validate the lottery codec hypothesis, which states that
untrained subnetworks within randomly initialized networks can serve as
synthesis networks for overfitted image compression, achieving rate-distortion
(RD) performance comparable to trained networks. This hypothesis leads to a new
paradigm for image compression by encoding image statistics into the network
substructure. Building on this hypothesis, we propose LotteryCodec, which
overfits a binary mask to an individual image, leveraging an over-parameterized
and randomly initialized network shared by the encoder and the decoder. To
address over-parameterization challenges and streamline subnetwork search, we
develop a rewind modulation mechanism that improves the RD performance.
LotteryCodec outperforms VTM and sets a new state-of-the-art in single-image
compression. LotteryCodec also enables adaptive decoding complexity through
adjustable mask ratios, offering flexible compression solutions for diverse
device constraints and application requirements.

</details>


### [92] [Classification based deep learning models for lung cancer and disease using medical images](https://arxiv.org/abs/2507.01279)
*Ahmad Chaddad,Jihao Peng,Yihang Wu*

Main category: eess.IV

TL;DR: 提出了一种名为ResNet+的新型深度卷积神经网络模型，用于改进肺癌和肺部疾病的预测，通过整合ResNet-D模块和卷积注意力模块，显著提高了准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统CNN在下采样过程中丢失特征信息的问题，并提升模型对输入图像相关区域的关注能力。

Method: 结合ResNet-D模块改进下采样层，并在瓶颈层加入卷积注意力模块，同时使用数据增强技术处理类别不平衡问题。

Result: 在多个公开数据集上表现优异，如LC2500数据集达到98.14%的准确率/F1值，计算成本也有所降低。

Conclusion: ResNet+模型在肺癌和肺部疾病预测中优于基线模型，具有更高的性能和效率。

Abstract: The use of deep learning (DL) in medical image analysis has significantly
improved the ability to predict lung cancer. In this study, we introduce a
novel deep convolutional neural network (CNN) model, named ResNet+, which is
based on the established ResNet framework. This model is specifically designed
to improve the prediction of lung cancer and diseases using the images. To
address the challenge of missing feature information that occurs during the
downsampling process in CNNs, we integrate the ResNet-D module, a variant
designed to enhance feature extraction capabilities by modifying the
downsampling layers, into the traditional ResNet model. Furthermore, a
convolutional attention module was incorporated into the bottleneck layers to
enhance model generalization by allowing the network to focus on relevant
regions of the input images. We evaluated the proposed model using five public
datasets, comprising lung cancer (LC2500 $n$=3183, IQ-OTH/NCCD $n$=1336, and
LCC $n$=25000 images) and lung disease (ChestXray $n$=5856, and COVIDx-CT
$n$=425024 images). To address class imbalance, we used data augmentation
techniques to artificially increase the representation of underrepresented
classes in the training dataset. The experimental results show that ResNet+
model demonstrated remarkable accuracy/F1, reaching 98.14/98.14\% on the
LC25000 dataset and 99.25/99.13\% on the IQ-OTH/NCCD dataset. Furthermore, the
ResNet+ model saved computational cost compared to the original ResNet series
in predicting lung cancer images. The proposed model outperformed the baseline
models on publicly available datasets, achieving better performance metrics.
Our codes are publicly available at
https://github.com/AIPMLab/Graduation-2024/tree/main/Peng.

</details>


### [93] [PanTS: The Pancreatic Tumor Segmentation Dataset](https://arxiv.org/abs/2507.01291)
*Wenxuan Li,Xinze Zhou,Qi Chen,Tianyu Lin,Pedro R. A. S. Bassi,Szymon Plotka,Jaroslaw B. Cwikla,Xiaoxi Chen,Chen Ye,Zheren Zhu,Kai Ding,Heng Li,Kang Wang,Yang Yang,Yucheng Tang,Daguang Xu,Alan L. Yuille,Zongwei Zhou*

Main category: eess.IV

TL;DR: PanTS是一个大规模、多机构的数据集，用于推进胰腺CT分析研究，包含36,390个CT扫描和993,000多个专家验证的体素级标注，显著提升了AI模型在胰腺肿瘤检测、定位和分割中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有公共数据集在胰腺CT分析中规模有限，PanTS旨在提供更大规模和更全面的资源，以推动AI模型的发展。

Method: 数据集包含36,390个CT扫描，覆盖胰腺肿瘤及周围24个解剖结构，每个扫描附带详细元数据。

Result: 在PanTS上训练的AI模型在胰腺肿瘤检测、定位和分割任务中表现显著优于现有数据集，性能提升归因于更大规模的标注和额外的解剖结构信息。

Conclusion: PanTS为胰腺CT分析提供了新的基准，是目前最大且最全面的资源。

Abstract: PanTS is a large-scale, multi-institutional dataset curated to advance
research in pancreatic CT analysis. It contains 36,390 CT scans from 145
medical centers, with expert-validated, voxel-wise annotations of over 993,000
anatomical structures, covering pancreatic tumors, pancreas head, body, and
tail, and 24 surrounding anatomical structures such as vascular/skeletal
structures and abdominal/thoracic organs. Each scan includes metadata such as
patient age, sex, diagnosis, contrast phase, in-plane spacing, slice thickness,
etc. AI models trained on PanTS achieve significantly better performance in
pancreatic tumor detection, localization, and segmentation compared to those
trained on existing public datasets. Our analysis indicates that these gains
are directly attributable to the 16x larger-scale tumor annotations and
indirectly supported by the 24 additional surrounding anatomical structures. As
the largest and most comprehensive resource of its kind, PanTS offers a new
benchmark for developing and evaluating AI models in pancreatic CT analysis.

</details>


### [94] [SWinMamba: Serpentine Window State Space Model for Vascular Segmentation](https://arxiv.org/abs/2507.01323)
*Rongchang Zhao,Huanchi Liu,Jian Zhang*

Main category: eess.IV

TL;DR: 提出了一种名为SWinMamba的新方法，通过结合蛇形窗口序列和双向状态空间模型，实现了精确的血管分割。


<details>
  <summary>Details</summary>
Motivation: 医学图像中的血管分割对疾病诊断和手术导航至关重要，但现有方法常因血管细长和先验建模不足导致分割结果不连续。

Method: SWinMamba利用蛇形窗口序列（SWToken）自适应分割输入图像，结合双向聚合模块（BAM）和空间-频率融合单元（SFFU）增强血管连续性建模。

Result: 在三个挑战性数据集上的实验表明，SWinMamba能够生成完整且连通的血管分割结果。

Conclusion: SWinMamba通过创新建模血管连续性，显著提升了血管分割的准确性。

Abstract: Vascular segmentation in medical images is crucial for disease diagnosis and
surgical navigation. However, the segmented vascular structure is often
discontinuous due to its slender nature and inadequate prior modeling. In this
paper, we propose a novel Serpentine Window Mamba (SWinMamba) to achieve
accurate vascular segmentation. The proposed SWinMamba innovatively models the
continuity of slender vascular structures by incorporating serpentine window
sequences into bidirectional state space models. The serpentine window
sequences enable efficient feature capturing by adaptively guiding global
visual context modeling to the vascular structure. Specifically, the Serpentine
Window Tokenizer (SWToken) adaptively splits the input image using overlapping
serpentine window sequences, enabling flexible receptive fields (RFs) for
vascular structure modeling. The Bidirectional Aggregation Module (BAM)
integrates coherent local features in the RFs for vascular continuity
representation. In addition, dual-domain learning with Spatial-Frequency Fusion
Unit (SFFU) is designed to enhance the feature representation of vascular
structure. Extensive experiments on three challenging datasets demonstrate that
the proposed SWinMamba achieves superior performance with complete and
connected vessels.

</details>


### [95] [Structure and Smoothness Constrained Dual Networks for MR Bias Field Correction](https://arxiv.org/abs/2507.01326)
*Dong Liang,Xingyu Qiu,Yuzhen Li,Wei Wang,Kuanquan Wang,Suyu Dong,Gongning Luo*

Main category: eess.IV

TL;DR: 提出了一种名为S2DNets的双网络模型，通过结构和平滑性约束自监督校正MR图像的偏置场，显著提升了图像质量和结构保留。


<details>
  <summary>Details</summary>
Motivation: MR图像存在强度不均匀问题，影响诊断和分析。现有深度学习模型忽略结构和平滑性约束，导致校正结果失真。

Method: 提出S2DNets，结合分段结构约束和偏置场平滑性，自监督校正MR图像。

Result: 在临床和模拟数据集上表现优于传统和深度学习模型，并提升了下游分割任务性能。

Conclusion: S2DNets有效解决了MR图像不均匀问题，保留了更多结构细节，具有实际应用价值。

Abstract: MR imaging techniques are of great benefit to disease diagnosis. However, due
to the limitation of MR devices, significant intensity inhomogeneity often
exists in imaging results, which impedes both qualitative and quantitative
medical analysis. Recently, several unsupervised deep learning-based models
have been proposed for MR image improvement. However, these models merely
concentrate on global appearance learning, and neglect constraints from image
structures and smoothness of bias field, leading to distorted corrected
results. In this paper, novel structure and smoothness constrained dual
networks, named S2DNets, are proposed aiming to self-supervised bias field
correction. S2DNets introduce piece-wise structural constraints and smoothness
of bias field for network training to effectively remove non-uniform intensity
and retain much more structural details. Extensive experiments executed on both
clinical and simulated MR datasets show that the proposed model outperforms
other conventional and deep learning-based models. In addition to comparison on
visual metrics, downstream MR image segmentation tasks are also used to
evaluate the impact of the proposed model. The source code is available at:
https://github.com/LeongDong/S2DNets}{https://github.com/LeongDong/S2DNets.

</details>


### [96] [BronchoGAN: Anatomically consistent and domain-agnostic image-to-image translation for video bronchoscopy](https://arxiv.org/abs/2507.01387)
*Ahmad Soliman,Ron Keuth,Marian Himstedt*

Main category: eess.IV

TL;DR: BronchoGAN利用解剖学约束和中间深度图像表示，实现了跨不同支气管镜输入域的鲁棒图像转换，生成逼真的人类气道图像。


<details>
  <summary>Details</summary>
Motivation: 支气管镜图像稀缺，限制了深度学习模型的训练。跨域图像转换对临床应用至关重要。

Method: 提出BronchoGAN，将解剖学约束（如支气管孔匹配）和基础模型生成的深度图像作为中间表示，集成到条件GAN中。

Result: 实验表明，不同输入域的图像可成功转换为逼真的人类气道图像，解剖学设置得到保留，FID、SSIM和Dice系数显著提升。

Conclusion: BronchoGAN通过解剖学约束和中间深度表示，填补了公共支气管镜图像的空白，为大规模数据集生成提供了解决方案。

Abstract: The limited availability of bronchoscopy images makes image synthesis
particularly interesting for training deep learning models. Robust image
translation across different domains -- virtual bronchoscopy, phantom as well
as in-vivo and ex-vivo image data -- is pivotal for clinical applications. This
paper proposes BronchoGAN introducing anatomical constraints for image-to-image
translation being integrated into a conditional GAN. In particular, we force
bronchial orifices to match across input and output images. We further propose
to use foundation model-generated depth images as intermediate representation
ensuring robustness across a variety of input domains establishing models with
substantially less reliance on individual training datasets. Moreover our
intermediate depth image representation allows to easily construct paired image
data for training. Our experiments showed that input images from different
domains (e.g. virtual bronchoscopy, phantoms) can be successfully translated to
images mimicking realistic human airway appearance. We demonstrated that
anatomical settings (i.e. bronchial orifices) can be robustly preserved with
our approach which is shown qualitatively and quantitatively by means of
improved FID, SSIM and dice coefficients scores. Our anatomical constraints
enabled an improvement in the Dice coefficient of up to 0.43 for synthetic
images. Through foundation models for intermediate depth representations,
bronchial orifice segmentation integrated as anatomical constraints into
conditional GANs we are able to robustly translate images from different
bronchoscopy input domains. BronchoGAN allows to incorporate public CT scan
data (virtual bronchoscopy) in order to generate large-scale bronchoscopy image
datasets with realistic appearance. BronchoGAN enables to bridge the gap of
missing public bronchoscopy images.

</details>


### [97] [Multi Source COVID-19 Detection via Kernel-Density-based Slice Sampling](https://arxiv.org/abs/2507.01564)
*Chia-Ming Lee,Bo-Cheng Qiu,Ting-Yao Chen,Ming-Han Sun,Fang-Ying Lin,Jung-Tse Tsai,I-An Tsai,Yu-Fan Lin,Chih-Chung Hsu*

Main category: eess.IV

TL;DR: 提出了一种基于SSFL和KDS的多源COVID-19检测方法，通过预处理和模型比较，EfficientNet表现优于Swin Transformer。


<details>
  <summary>Details</summary>
Motivation: 解决多源医疗数据（来自四个医疗中心）的变异性问题，提高COVID-19检测的准确性。

Method: 采用SSFL框架和KDS切片采样，结合肺部区域提取、质量控制和自适应切片选择，比较EfficientNet和Swin Transformer模型。

Result: EfficientNet的F1得分为94.68%，优于Swin Transformer的93.34%。

Conclusion: KDS预处理流程在多源数据中有效，数据集平衡对多机构医学影像评估至关重要。

Abstract: We present our solution for the Multi-Source COVID-19 Detection Challenge,
which classifies chest CT scans from four distinct medical centers. To address
multi-source variability, we employ the Spatial-Slice Feature Learning (SSFL)
framework with Kernel-Density-based Slice Sampling (KDS). Our preprocessing
pipeline combines lung region extraction, quality control, and adaptive slice
sampling to select eight representative slices per scan. We compare
EfficientNet and Swin Transformer architectures on the validation set. The
EfficientNet model achieves an F1-score of 94.68%, compared to the Swin
Transformer's 93.34%. The results demonstrate the effectiveness of our
KDS-based pipeline on multi-source data and highlight the importance of dataset
balance in multi-institutional medical imaging evaluation.

</details>


### [98] [Enhancing Multi-Exposure High Dynamic Range Imaging with Overlapped Codebook for Improved Representation Learning](https://arxiv.org/abs/2507.01588)
*Keuntek Lee,Jaehyun Park,Nam Ik Cho*

Main category: eess.IV

TL;DR: 提出了一种基于重叠码本（OLC）的方案，通过改进VQGAN框架学习隐式HDR表示，并结合新HDR网络提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决多曝光HDR成像中因帧间运动差异和曝光设置导致的饱和区域问题。

Method: 提出OLC方案改进VQGAN框架，并开发新HDR网络利用预训练VQ网络和OLC补偿饱和区域。

Result: 在多个数据集上测试，方法在质量和量化指标上优于先前方法。

Conclusion: OLC方案和新HDR网络有效提升了HDR图像重建性能。

Abstract: High dynamic range (HDR) imaging technique aims to create realistic HDR
images from low dynamic range (LDR) inputs. Specifically, Multi-exposure HDR
imaging uses multiple LDR frames taken from the same scene to improve
reconstruction performance. However, there are often discrepancies in motion
among the frames, and different exposure settings for each capture can lead to
saturated regions. In this work, we first propose an Overlapped codebook (OLC)
scheme, which can improve the capability of the VQGAN framework for learning
implicit HDR representations by modeling the common exposure bracket process in
the shared codebook structure. Further, we develop a new HDR network that
utilizes HDR representations obtained from a pre-trained VQ network and OLC.
This allows us to compensate for saturated regions and enhance overall visual
quality. We have tested our approach extensively on various datasets and have
demonstrated that it outperforms previous methods both qualitatively and
quantitatively

</details>


### [99] [Robust brain age estimation from structural MRI with contrastive learning](https://arxiv.org/abs/2507.01794)
*Carlo Alberto Barbano,Benoit Dufumier,Edouard Duchesnay,Marco Grangetto,Pietro Gori*

Main category: eess.IV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Estimating brain age from structural MRI has emerged as a powerful tool for
characterizing normative and pathological aging. In this work, we explore
contrastive learning as a scalable and robust alternative to supervised
approaches for brain age estimation. We introduce a novel contrastive loss
function, $\mathcal{L}^{exp}$, and evaluate it across multiple public
neuroimaging datasets comprising over 20,000 scans. Our experiments reveal four
key findings. First, scaling pre-training on diverse, multi-site data
consistently improves generalization performance, cutting external mean
absolute error (MAE) nearly in half. Second, $\mathcal{L}^{exp}$ is robust to
site-related confounds, maintaining low scanner-predictability as training size
increases. Third, contrastive models reliably capture accelerated aging in
patients with cognitive impairment and Alzheimer's disease, as shown through
brain age gap analysis, ROC curves, and longitudinal trends. Lastly, unlike
supervised baselines, $\mathcal{L}^{exp}$ maintains a strong correlation
between brain age accuracy and downstream diagnostic performance, supporting
its potential as a foundation model for neuroimaging. These results position
contrastive learning as a promising direction for building generalizable and
clinically meaningful brain representations.

</details>


### [100] [Autoadaptive Medical Segment Anything Model](https://arxiv.org/abs/2507.01828)
*Tyler Ward,Meredith K. Owen,O'Kira Coleman,Brian Noehren,Abdullah-Al-Zubaer Imran*

Main category: eess.IV

TL;DR: 提出ADA-SAM，一种基于SAM框架的多任务学习模型，用于医学图像分割，通过辅助分类器和梯度反馈机制提高分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统全监督分割模型依赖大量标注数据，成本高且易出错，需要高效自动化的方法。

Method: 结合辅助分类器的类激活图和梯度反馈机制，改进SAM框架的分支预测。

Result: 在有限标注数据下，ADA-SAM性能优于全监督和半监督基线模型，提升显著。

Conclusion: ADA-SAM为医学图像分割提供了一种高效、自动化的解决方案。

Abstract: Medical image segmentation is a key task in the imaging workflow, influencing
many image-based decisions. Traditional, fully-supervised segmentation models
rely on large amounts of labeled training data, typically obtained through
manual annotation, which can be an expensive, time-consuming, and error-prone
process. This signals a need for accurate, automatic, and annotation-efficient
methods of training these models. We propose ADA-SAM (automated,
domain-specific, and adaptive segment anything model), a novel multitask
learning framework for medical image segmentation that leverages class
activation maps from an auxiliary classifier to guide the predictions of the
semi-supervised segmentation branch, which is based on the Segment Anything
(SAM) framework. Additionally, our ADA-SAM model employs a novel gradient
feedback mechanism to create a learnable connection between the segmentation
and classification branches by using the segmentation gradients to guide and
improve the classification predictions. We validate ADA-SAM on real-world
clinical data collected during rehabilitation trials, and demonstrate that our
proposed method outperforms both fully-supervised and semi-supervised baselines
by double digits in limited label settings. Our code is available at:
https://github.com/tbwa233/ADA-SAM.

</details>


### [101] [A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs](https://arxiv.org/abs/2507.01881)
*Niccolò McConnell,Pardeep Vasudev,Daisuke Yamada,Daryl Cheng,Mehran Azimbagirad,John McCabe,Shahab Aslani,Ahmed H. Shahin,Yukun Zhou,The SUMMIT Consortium,Andre Altmann,Yipeng Hu,Paul Taylor,Sam M. Janes,Daniel C. Alexander,Joseph Jacob*

Main category: eess.IV

TL;DR: TANGERINE是一个开源、计算资源需求低的视觉基础模型，用于低剂量CT（LDCT）分析，可快速适应多种疾病检测任务，显著减少训练时间和数据需求。


<details>
  <summary>Details</summary>
Motivation: 解决LDCT扫描解读中放射科医生短缺的问题，同时扩展肺癌筛查（LCS）项目以涵盖更多呼吸系统疾病。

Method: 采用自监督学习预训练模型，基于超过98,000个胸部LDCT扫描数据，并通过掩码自编码器框架扩展到3D成像。

Result: 在14种疾病分类任务中达到领先性能，包括肺癌和多种呼吸系统疾病，且在不同临床中心表现稳健。

Conclusion: TANGERINE的开源、轻量设计为下一代医学影像工具提供了快速集成的基础，有望将LCS项目从单一肺癌检测扩展到全面的呼吸系统疾病管理。

Abstract: Low-dose computed tomography (LDCT) imaging employed in lung cancer screening
(LCS) programs is increasing in uptake worldwide. LCS programs herald a
generational opportunity to simultaneously detect cancer and non-cancer-related
early-stage lung disease. Yet these efforts are hampered by a shortage of
radiologists to interpret scans at scale. Here, we present TANGERINE, a
computationally frugal, open-source vision foundation model for volumetric LDCT
analysis. Designed for broad accessibility and rapid adaptation, TANGERINE can
be fine-tuned off the shelf for a wide range of disease-specific tasks with
limited computational resources and training data. Relative to models trained
from scratch, TANGERINE demonstrates fast convergence during fine-tuning,
thereby requiring significantly fewer GPU hours, and displays strong label
efficiency, achieving comparable or superior performance with a fraction of
fine-tuning data. Pretrained using self-supervised learning on over 98,000
thoracic LDCTs, including the UK's largest LCS initiative to date and 27 public
datasets, TANGERINE achieves state-of-the-art performance across 14 disease
classification tasks, including lung cancer and multiple respiratory diseases,
while generalising robustly across diverse clinical centres. By extending a
masked autoencoder framework to 3D imaging, TANGERINE offers a scalable
solution for LDCT analysis, departing from recent closed, resource-intensive
models by combining architectural simplicity, public availability, and modest
computational requirements. Its accessible, open-source lightweight design lays
the foundation for rapid integration into next-generation medical imaging tools
that could transform LCS initiatives, allowing them to pivot from a singular
focus on lung cancer detection to comprehensive respiratory disease management
in high-risk populations.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [102] [A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory](https://arxiv.org/abs/2507.01110)
*Felix Windisch,Lukas Radl,Thomas Köhler,Michael Steiner,Dieter Schmalstieg,Markus Steinberger*

Main category: cs.GR

TL;DR: 论文提出了一种名为'A LoD of Gaussians'的框架，用于在单个消费级GPU上训练和渲染超大规模高斯场景，无需分区。


<details>
  <summary>Details</summary>
Motivation: 解决高斯泼溅技术在大规模场景中因分区导致的边界伪影、训练复杂性和GPU内存限制问题。

Method: 采用层次化高斯与顺序点树结合的混合数据结构，动态流式传输相关高斯，并利用轻量级缓存和视图调度系统。

Result: 实现了无缝多尺度重建和复杂场景的交互式可视化，从空中俯瞰到地面细节。

Conclusion: 该方法突破了GPU内存限制，支持大规模场景的高效训练和实时渲染。

Abstract: Gaussian Splatting has emerged as a high-performance technique for novel view
synthesis, enabling real-time rendering and high-quality reconstruction of
small scenes. However, scaling to larger environments has so far relied on
partitioning the scene into chunks -- a strategy that introduces artifacts at
chunk boundaries, complicates training across varying scales, and is poorly
suited to unstructured scenarios such as city-scale flyovers combined with
street-level views. Moreover, rendering remains fundamentally limited by GPU
memory, as all visible chunks must reside in VRAM simultaneously. We introduce
A LoD of Gaussians, a framework for training and rendering ultra-large-scale
Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our
method stores the full scene out-of-core (e.g., in CPU memory) and trains a
Level-of-Detail (LoD) representation directly, dynamically streaming only the
relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with
Sequential Point Trees enables efficient, view-dependent LoD selection, while a
lightweight caching and view scheduling system exploits temporal coherence to
support real-time streaming and rendering. Together, these innovations enable
seamless multi-scale reconstruction and interactive visualization of complex
scenes -- from broad aerial views to fine-grained ground-level details.

</details>


### [103] [Semiautomatic Simplification](https://arxiv.org/abs/2507.01116)
*Gong Li,Benjamin Watson*

Main category: cs.GR

TL;DR: semisimp是一个半自动简化三维多边形模型的工具，允许用户干预简化过程以保留重要语义区域。


<details>
  <summary>Details</summary>
Motivation: 现有自动简化技术成熟但无法区分语义重要区域（如面部和肢体）或考虑模型用途（如动画）的简化约束。

Method: 用户可调整简化顺序、重新定位顶点并通过层次分区改进模型。

Result: 工具能够生成更符合用户需求的简化模型。

Conclusion: semisimp通过用户干预提升了简化模型的质量和适用性。

Abstract: We present semisimp, a tool for semiautomatic simplification of three
dimensional polygonal models. Existing automatic simplification technology is
quite mature, but is not sensitive to the heightened importance of distinct
semantic model regions such as faces and limbs, nor to simplification
constraints imposed by model usage such as animation. semisimp allows users to
preserve such regions by intervening in the simplification process. Users can
manipulate the order in which basic simplifications are applied to redistribute
model detail, improve the simplified models themselves by repositioning
vertices with propagation to neighboring levels of detail, and adjust the
hierarchical partitioning of the model surface to segment simplification and
improve control of reordering and position propagation.

</details>


### [104] [Multi-Focus Probes for Context-Preserving Network Exploration and Interaction in Immersive Analytics](https://arxiv.org/abs/2507.01140)
*Eric Zimmermann,Stefan Bruckner*

Main category: cs.GR

TL;DR: 提出了一种多焦点探针技术，用于沉浸式环境中管理网络数据的局部与全局视图切换。


<details>
  <summary>Details</summary>
Motivation: 解决用户在沉浸式环境中导航和交互复杂网络结构时，局部与全局视图切换的挑战。

Method: 采用多焦点探针技术，支持用户实例化多个局部子图视图，同时保持与全局网络的链接，并提供视觉和触觉引导机制。

Result: 技术能够有效支持多尺度交互，便于网络数据的细粒度检查和编辑。

Conclusion: 该技术在网络数据编辑中表现出良好的可用性。

Abstract: Immersive visualization of network data enables users to physically navigate
and interact with complex structures, but managing transitions between detailed
local (egocentric) views and global (exocentric) overviews remains a major
challenge. We present a multifocus probe technique for immersive environments
that allows users to instantiate multiple egocentric subgraph views while
maintaining persistent links to the global network context. Each probe acts as
a portable local focus, enabling fine-grained inspection and editing of distant
or occluded regions. Visual and haptic guidance mechanisms ensure context
preservation during multi-scale interaction. We demonstrate and discuss the
usability of our technique for the editing of network data.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [105] [Automated Classification of Volcanic Earthquakes Using Transformer Encoders: Insights into Data Quality and Model Interpretability](https://arxiv.org/abs/2507.01260)
*Y. Suzuki,Y. Yukutake,T. Ohminato,M. Yamasaki,Ahyi Kim*

Main category: physics.geo-ph

TL;DR: 该论文提出了一种基于Transformer编码器的深度学习模型，用于高效、客观地分类火山地震类型，优于传统CNN方法，并强调了数据质量和多样性对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 传统火山地震分类方法依赖主观人为判断，耗时耗力，需要更高效、客观的解决方案。

Method: 使用Transformer编码器构建深度学习模型，通过注意力权重可视化增强可解释性，并分析数据选择与增强的影响。

Result: 模型在Mount Asama的测试中表现优异（F1分数：0.930-0.980），且注意力权重与专家关注特征一致。数据不平衡和标签主观性影响分类准确性。

Conclusion: Transformer模型在火山地震分类中具有高效性和可解释性潜力，为其他火山区域的迁移学习提供了框架，有助于灾害评估与减灾策略。

Abstract: Precisely classifying earthquake types is crucial for elucidating the
relationship between volcanic earthquakes and volcanic activity. However,
traditional methods rely on subjective human judgment, which requires
considerable time and effort. To address this issue, we developed a deep
learning model using a transformer encoder for a more objective and efficient
classification. Tested on Mount Asama's diverse seismic activity, our model
achieved high F1 scores (0.930 for volcano tectonic, 0.931 for low-frequency
earthquakes, and 0.980 for noise), superior to a conventional CNN-based method.
To enhance interpretability, attention weight visualizations were analyzed,
revealing that the model focuses on key waveform features similarly to human
experts. However, inconsistencies in training data, such as ambiguously labeled
B-type events with S-waves, were found to influence classification accuracy and
attention weight distributions. Experiments addressing data selection and
augmentation demonstrated the importance of balancing data quality and
diversity. In addition, stations within 3 km of the crater played an important
role in improving model performance and interpretability. These findings
highlight the potential of Transformer-based models for automated volcanic
earthquake classification, particularly in improving efficiency and
interpretability. By addressing challenges such as data imbalance and
subjective labeling, our approach provides a robust framework for understanding
seismic activity at Mount Asama. Moreover, this framework offers opportunities
for transfer learning to other volcanic regions, paving the way for enhanced
volcanic hazard assessments and disaster mitigation strategies.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [106] [A Hybrid Ensemble Learning Framework for Image-Based Solar Panel Classification](https://arxiv.org/abs/2507.01778)
*Vivek Tetarwal,Sandeep Kumar*

Main category: cs.IT

TL;DR: 本文提出了一种新型的双集成神经网络（DENN），用于基于图像特征分类清洁和脏污的太阳能板，其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 太阳能系统安装量增加，需要有效的维护技术，尤其是自动区分清洁和脏污太阳能板的挑战。

Method: 采用双集成神经网络（DENN），结合多种集成模型的优势，提高分类准确性和鲁棒性。

Result: DENN在Deep Solar Eye数据集上表现优于其他方法，达到最先进的准确率。

Conclusion: 混合集成学习技术有望推动太阳能板自动检测的进一步发展，解决实际挑战。

Abstract: The installation of solar energy systems is on the rise, and therefore,
appropriate maintenance techniques are required to be used in order to maintain
maximum performance levels. One of the major challenges is the automated
discrimination between clean and dirty solar panels. This paper presents a
novel Dual Ensemble Neural Network (DENN) to classify solar panels using
image-based features. The suggested approach utilizes the advantages offered by
various ensemble models by integrating them into a dual framework, aimed at
improving both classification accuracy and robustness. The DENN model is
evaluated in comparison to current ensemble methods, showcasing its superior
performance across a range of assessment metrics. The proposed approach
performs the best compared to other methods and reaches state-of-the-art
accuracy on experimental results for the Deep Solar Eye dataset, effectively
serving predictive maintenance purposes in solar energy systems. It reveals the
potential of hybrid ensemble learning techniques to further advance the
prospects of automated solar panel inspections as a scalable solution to
real-world challenges.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [107] [How Do Vision-Language Models Process Conflicting Information Across Modalities?](https://arxiv.org/abs/2507.01790)
*Tianze Hua,Tian Yun,Ellie Pavlick*

Main category: cs.CL

TL;DR: 研究探讨多模态AI模型在输入信息冲突时的行为，发现模型倾向于优先处理某一模态，并识别出影响模态偏好的内部结构和注意力机制。


<details>
  <summary>Details</summary>
Motivation: 理解多模态模型如何处理冲突输入，以改进模型在复杂环境中的表现。

Method: 通过向视觉-语言模型提供不一致的输入（如图片与描述不符），观察模型对不同模态的偏好，并分析内部结构和注意力机制。

Result: 模型倾向于优先处理某一模态，且这种偏好与内部结构和特定注意力头相关；还发现模态无关的“路由头”可优化性能。

Conclusion: 研究为识别和控制多模态模型在冲突信号中的行为提供了关键步骤。

Abstract: AI models are increasingly required to be multimodal, integrating disparate
input streams into a coherent state representation on which subsequent
behaviors and actions can be based. This paper seeks to understand how such
models behave when input streams present conflicting information. Focusing
specifically on vision-language models, we provide inconsistent inputs (e.g.,
an image of a dog paired with the caption "A photo of a cat") and ask the model
to report the information present in one of the specific modalities (e.g.,
"What does the caption say / What is in the image?"). We find that models often
favor one modality over the other, e.g., reporting the image regardless of what
the caption says, but that different models differ in which modality they
favor. We find evidence that the behaviorally preferred modality is evident in
the internal representational structure of the model, and that specific
attention heads can restructure the representations to favor one modality over
the other. Moreover, we find modality-agnostic "router heads" which appear to
promote answers about the modality requested in the instruction, and which can
be manipulated or transferred in order to improve performance across datasets
and modalities. Together, the work provides essential steps towards identifying
and controlling if and how models detect and resolve conflicting signals within
complex multimodal environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [108] [Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models](https://arxiv.org/abs/2507.01201)
*Hyoseo,Yoon,Yisong Yue,Been Kim*

Main category: cs.LG

TL;DR: 论文提出了一种联合自动编码器调制器（JAM）框架，用于优化独立训练的视觉和语言模型之间的表示对齐，通过多目标优化实现模态间的一致性。


<details>
  <summary>Details</summary>
Motivation: 探索独立训练的视觉和语言模型是否可以通过优化实现表示对齐，验证柏拉图表示假设的可行性。

Method: 提出JAM框架，联合训练模态特定的自动编码器，结合重建和跨模态目标，实现表示对齐。

Result: 实验表明，JAM框架能有效诱导对齐，即使是在冻结的独立训练表示上，且在不同设计轴上表现稳定。

Conclusion: JAM框架为将通用单模态模型转化为专业多模态模型提供了理论和实践路径。

Abstract: Independently trained vision and language models inhabit disjoint
representational spaces, shaped by their respective modalities, objectives, and
architectures. Yet an emerging hypothesis - the Platonic Representation
Hypothesis - suggests that such models may nonetheless converge toward a shared
statistical model of reality. This compatibility, if it exists, raises a
fundamental question: can we move beyond post-hoc statistical detection of
alignment and explicitly optimize for it between such disjoint representations?
We cast this Platonic alignment problem as a multi-objective optimization task
- preserve each modality's native structure while aligning for mutual
coherence. We introduce the Joint Autoencoder Modulator (JAM) framework that
jointly trains modality-specific autoencoders on the latent representations of
pre-trained single modality models, encouraging alignment through both
reconstruction and cross-modal objectives. By analogy, this framework serves as
a method to escape Plato's Cave, enabling the emergence of shared structure
from disjoint inputs. We evaluate this framework across three critical design
axes: (i) the alignment objective - comparing contrastive loss (Con), its
hard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at
which alignment is most effective, and (iii) the impact of foundation model
scale on representational convergence. Our findings show that our lightweight
Pareto-efficient framework reliably induces alignment, even across frozen,
independently trained representations, offering both theoretical insight and
practical pathways for transforming generalist unimodal foundations into
specialist multimodal models.

</details>


### [109] [How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks](https://arxiv.org/abs/2507.01559)
*Lapo Frati,Neil Traft,Jeff Clune,Nick Cheney*

Main category: cs.LG

TL;DR: 研究探讨了神经网络最后一层权重重采样（“zapping”）在持续学习和少样本迁移学习中的作用，发现其能加速模型在新领域的适应，并揭示了优化器选择对学习动态的复杂影响。


<details>
  <summary>Details</summary>
Motivation: 尽管zapping在持续学习中表现出有效性，但其背后的机制尚不明确，本研究旨在揭示其作用及优化器选择对学习动态的影响。

Method: 在持续学习和少样本迁移学习场景下，通过卷积神经网络训练手写字符和自然图像，分析zapping和优化器选择对学习与遗忘模式的影响。

Result: 实验表明，zapping能帮助模型更快适应新领域，同时优化器选择会显著影响任务间的学习与遗忘动态，产生复杂的协同/干扰模式。

Conclusion: zapping和优化器选择在持续学习中起关键作用，未来研究可进一步探索其机制以优化模型性能。

Abstract: Recent work in continual learning has highlighted the beneficial effect of
resampling weights in the last layer of a neural network (``zapping"). Although
empirical results demonstrate the effectiveness of this approach, the
underlying mechanisms that drive these improvements remain unclear. In this
work, we investigate in detail the pattern of learning and forgetting that take
place inside a convolutional neural network when trained in challenging
settings such as continual learning and few-shot transfer learning, with
handwritten characters and natural images. Our experiments show that models
that have undergone zapping during training more quickly recover from the shock
of transferring to a new domain. Furthermore, to better observe the effect of
continual learning in a multi-task setting we measure how each individual task
is affected. This shows that, not only zapping, but the choice of optimizer can
also deeply affect the dynamics of learning and forgetting, causing complex
patterns of synergy/interference between tasks to emerge when the model learns
sequentially at transfer time.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [110] [Age Sensitive Hippocampal Functional Connectivity: New Insights from 3D CNNs and Saliency Mapping](https://arxiv.org/abs/2507.01411)
*Yifei Sun,Marshall A. Dalton,Robert D. Sanders,Yixuan Yuan,Xiang Li,Sharon L. Naismith,Fernando Calamante,Jinglei Lv*

Main category: q-bio.NC

TL;DR: 本文提出了一种可解释的深度学习框架，通过3D CNN和LayerCAM显著性映射，预测基于海马功能连接的大脑年龄，揭示了海马与特定皮层区域的功能重组。


<details>
  <summary>Details</summary>
Motivation: 海马灰质减少是神经生物学衰老的标志，但其功能连接的变化尚不明确。研究旨在揭示海马功能连接在衰老中的重组机制。

Method: 使用基于种子的功能连接分析，结合3D CNN和LayerCAM显著性映射，预测大脑年龄并识别关键海马-皮层连接。

Result: 发现海马与楔前叶、楔叶、后扣带回皮层等区域的功能连接对年龄高度敏感，并揭示了前后海马功能连接的差异。

Conclusion: 研究为海马衰老的功能机制提供了新见解，展示了可解释深度学习在神经影像数据中的潜力。

Abstract: Grey matter loss in the hippocampus is a hallmark of neurobiological aging,
yet understanding the corresponding changes in its functional connectivity
remains limited. Seed-based functional connectivity (FC) analysis enables
voxel-wise mapping of the hippocampus's synchronous activity with cortical
regions, offering a window into functional reorganization during aging. In this
study, we develop an interpretable deep learning framework to predict brain age
from hippocampal FC using a three-dimensional convolutional neural network (3D
CNN) combined with LayerCAM saliency mapping. This approach maps key
hippocampal-cortical connections, particularly with the precuneus, cuneus,
posterior cingulate cortex, parahippocampal cortex, left superior parietal
lobule, and right superior temporal sulcus, that are highly sensitive to age.
Critically, disaggregating anterior and posterior hippocampal FC reveals
distinct mapping aligned with their known functional specializations. These
findings provide new insights into the functional mechanisms of hippocampal
aging and demonstrate the power of explainable deep learning to uncover
biologically meaningful patterns in neuroimaging data.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [111] [VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process](https://arxiv.org/abs/2507.01284)
*Cristian Gariboldi,Hayato Tokida,Ken Kinjo,Yuki Asada,Alexander Carballo*

Main category: cs.RO

TL;DR: 提出了一种名为VLAD的视觉语言自动驾驶模型，通过微调VLM与VAD系统结合，提升空间推理能力，减少碰撞率31.82%。


<details>
  <summary>Details</summary>
Motivation: 利用开源视觉语言模型（VLMs）的通用知识，增强自动驾驶的感知、预测和规划能力。

Method: 采用定制问答数据集微调VLM，生成高级导航指令，结合VAD系统实现自动驾驶，并提供可解释的自然语言决策说明。

Result: 在nuScenes数据集上测试，碰撞率降低31.82%，优于基线方法。

Conclusion: VLAD为VLM增强的自动驾驶系统设定了新基准，提高了透明度和可信度。

Abstract: Recent advancements in open-source Visual Language Models (VLMs) such as
LLaVA, Qwen-VL, and Llama have catalyzed extensive research on their
integration with diverse systems. The internet-scale general knowledge
encapsulated within these models presents significant opportunities for
enhancing autonomous driving perception, prediction, and planning capabilities.
In this paper we propose VLAD, a vision-language autonomous driving model,
which integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end
system. We implement a specialized fine-tuning approach using custom
question-answer datasets designed specifically to improve the spatial reasoning
capabilities of the model. The enhanced VLM generates high-level navigational
commands that VAD subsequently processes to guide vehicle operation.
Additionally, our system produces interpretable natural language explanations
of driving decisions, thereby increasing transparency and trustworthiness of
the traditionally black-box end-to-end architecture. Comprehensive evaluation
on the real-world nuScenes dataset demonstrates that our integrated system
reduces average collision rates by 31.82% compared to baseline methodologies,
establishing a new benchmark for VLM-augmented autonomous driving systems.

</details>


### [112] [LANet: A Lane Boundaries-Aware Approach For Robust Trajectory Prediction](https://arxiv.org/abs/2507.01308)
*Muhammad Atta ur Rahman,Dooseop Choi,KyoungWook Min*

Main category: cs.RO

TL;DR: 提出了一种基于多向量地图元素的运动预测模型，通过融合车道边界和道路边缘等信息，提升自动驾驶车辆在复杂交通场景中的轨迹预测能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于车道中心线的运动预测模型无法充分捕捉道路环境和交通规则，限制了预测的准确性和全面性。

Method: 开发了一种特征融合策略和有效的连接剪枝机制，结合多向量地图元素，优化道路结构和与目标交互的表示。

Result: 在Argoverse 2数据集上验证了方法的竞争力，性能优于现有方法。

Conclusion: 该方法通过更丰富的地图表示和高效的计算机制，提升了自动驾驶运动预测的准确性和效率。

Abstract: Accurate motion forecasting is critical for safe and efficient autonomous
driving, enabling vehicles to predict future trajectories and make informed
decisions in complex traffic scenarios. Most of the current designs of motion
prediction models are based on the major representation of lane centerlines,
which limits their capability to capture critical road environments and traffic
rules and constraints. In this work, we propose an enhanced motion forecasting
model informed by multiple vector map elements, including lane boundaries and
road edges, that facilitates a richer and more complete representation of
driving environments. An effective feature fusion strategy is developed to
merge information in different vector map components, where the model learns
holistic information on road structures and their interactions with agents.
Since encoding more information about the road environment increases memory
usage and is computationally expensive, we developed an effective pruning
mechanism that filters the most relevant map connections to the target agent,
ensuring computational efficiency while maintaining essential spatial and
semantic relationships for accurate trajectory prediction. Overcoming the
limitations of lane centerline-based models, our method provides a more
informative and efficient representation of the driving environment and
advances the state of the art for autonomous vehicle motion forecasting. We
verify our approach with extensive experiments on the Argoverse 2 motion
forecasting dataset, where our method maintains competitiveness on AV2 while
achieving improved performance.
  Index Terms-Autonomous driving, trajectory prediction, vector map elements,
road topology, connection pruning, Argoverse 2.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [113] [Scaling particle-size segregation in wide-ranging sheared granular flows](https://arxiv.org/abs/2507.01248)
*Tianxiong Zhao,Daisuke Noto,Xia Li,Tomás Trewhela,Hugo N. Ulloa*

Main category: cond-mat.soft

TL;DR: 研究发现，剪切驱动的颗粒尺寸分离的标度关系仅在中等的惯性数范围内有效，而在准静态和碰撞状态下失效，导致连续模型预测偏差。


<details>
  <summary>Details</summary>
Motivation: 验证剪切率变化条件下颗粒尺寸分离标度关系的有效性。

Method: 采用离散元方法（DEM）模拟广泛剪切率条件下的颗粒尺寸分离。

Result: 标度关系仅在中等惯性数范围（0.01 < I < 0.1）内成立，在准静态和碰撞状态下失效。

Conclusion: 需要更通用的标度定律以覆盖更广泛的剪切率条件和状态。

Abstract: Scaling relationships have been proposed to describe shear-driven size
segregation based on intruder experiments and simulations. While these models
have shown agreement with experimental and numerical results under uniform
shear rate, their validity across varying shear-rate conditions remains
uncertain. Here, we employ Discrete Element Method (DEM) simulations to
investigate particle size segregation in sheared granular flows under
wide-ranging shear-rate conditions. We find that the scaling between
segregation velocity and local rheological conditions holds only within a
moderate inertial number range ($0.01 < I < 0.1$), and breaks down in both
quasi-static and collisional regimes. Furthermore, we show that this
discrepancy leads continuum models to mispredict segregation rates in
bidisperse mixtures. These findings emphasize the need for more generalized
scaling laws capable of capturing segregation dynamics across a broader
spectrum of shear-rate conditions and regimes.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [114] [Constraints on Earth's Core-Mantle boundary from nutation](https://arxiv.org/abs/2507.01671)
*J. Rekier,S. A. Trian,A. Barik,D. Abdulah,W. Kang*

Main category: astro-ph.EP

TL;DR: 地球自转轴的微小变化（章动）受太阳和月球引力影响，其共振放大与自由核章动（FCN）相关。传统认为电磁耦合是阻尼原因，但研究发现核心-地幔边界（CMB）地形激发的内波是主要机制。


<details>
  <summary>Details</summary>
Motivation: 探索地球自转响应与潮汐力相位滞后的原因，传统电磁耦合理论不足以解释观测结果。

Method: 借鉴海洋地形潮汐流理论，计算CMB地形引起的形状阻力和能量通量。

Result: 模型支持CMB地形振幅为几千米，且核心顶部弱分层。

Conclusion: CMB地形和内波机制为地球深部动力学提供了新约束，适用于其他行星研究。

Abstract: Periodic variations in the Sun and Moon's gravitational pull cause small
changes in Earth's rotational axis direction called nutation. Nutation
components in the retrograde quasi-diurnal frequency band measured in the
terrestrial reference frame are amplified by resonance with the Free Core
Nutation (FCN), a rotational mode of Earth's fluid core. Dissipative processes
at the core-mantle boundary (CMB) dampen this resonance, contributing to the
observed phase lag between tidal forcing and Earth's rotational response. This
phase lag is commonly attributed to electromagnetic (EM) coupling between the
core and the electrically conducting lower mantle. However, estimates of mantle
conductivity and radial magnetic field strength at the CMB suggest these
effects are insufficient. We show that the missing dissipation arises naturally
from the excitation of internal waves in the fluid core by topographic features
at the CMB. Adapting a theoretical framework originally developed for tidal
flow over oceanic topography, we compute the form drag and associated power
flux induced by CMB topography. Our results are consistent with a CMB
topography characterized by a root mean square amplitude of a few kilometers.
The model favors weak stratification at the top of the core, though stronger
stratification remains compatible with increased topographic amplitude. This
mechanism provides independent constraints on CMB topography and
stratification, complementing seismological and magnetic observations. Its
generality offers a new framework for probing deep-interior dynamics across
terrestrial planets.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [115] [Embedding-based Retrieval in Multimodal Content Moderation](https://arxiv.org/abs/2507.01066)
*Hanzhong Liang,Jinghao Shi,Xiang Shen,Zixuan Wang,Vera Wen,Ardalan Mehrani,Zhiqian Chen,Yifan Wu,Zhixin Zhang*

Main category: cs.IR

TL;DR: 论文提出了一种基于嵌入检索（EBR）的方法，用于补充传统分类方法在视频内容审核中的不足，显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统分类方法在快速响应和成本效益方面存在局限，尤其是在趋势适应和紧急升级场景中。

Method: 采用监督对比学习（SCL）框架训练基础嵌入模型，包括单模态和多模态架构，并设计了一个嵌入检索系统。

Result: 离线实验显示EBR将ROC-AUC从0.85提升至0.99，PR-AUC从0.35提升至0.95；在线实验显示行动率提高10.32%，运营成本降低80%。

Conclusion: EBR方法在视频内容审核中表现出更高的效率、灵活性和可解释性，优于传统分类方法。

Abstract: Video understanding plays a fundamental role for content moderation on short
video platforms, enabling the detection of inappropriate content. While
classification remains the dominant approach for content moderation, it often
struggles in scenarios requiring rapid and cost-efficient responses, such as
trend adaptation and urgent escalations. To address this issue, we introduce an
Embedding-Based Retrieval (EBR) method designed to complement traditional
classification approaches. We first leverage a Supervised Contrastive Learning
(SCL) framework to train a suite of foundation embedding models, including both
single-modal and multi-modal architectures. Our models demonstrate superior
performance over established contrastive learning methods such as CLIP and
MoCo. Building on these embedding models, we design and implement the
embedding-based retrieval system that integrates embedding generation and video
retrieval to enable efficient and effective trend handling. Comprehensive
offline experiments on 25 diverse emerging trends show that EBR improves
ROC-AUC from 0.85 to 0.99 and PR-AUC from 0.35 to 0.95. Further online
experiments reveal that EBR increases action rates by 10.32% and reduces
operational costs by over 80%, while also enhancing interpretability and
flexibility compared to classification-based solutions.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [116] [Automated Vehicles Should be Connected with Natural Language](https://arxiv.org/abs/2507.01059)
*Xiangbo Gao,Keshu Wu,Hao Zhang,Kexin Tian,Yang Zhou,Zhengzhong Tu*

Main category: cs.MA

TL;DR: 论文提出通过自然语言实现多智能体协作驾驶中的意图与推理通信，以解决现有通信媒介的局限性，提升交通安全性、效率和透明度。


<details>
  <summary>Details</summary>
Motivation: 现有通信媒介（如原始传感器数据、神经网络特征和感知结果）在带宽效率、信息完整性和智能体互操作性方面存在不足，且忽视了决策级融合。

Method: 提出从感知导向的数据交换转向使用自然语言进行明确的意图和推理通信。

Result: 自然语言平衡了语义密度与通信带宽，适应实时条件，并桥接异构智能体平台，实现主动协调。

Conclusion: 自然语言通信将协作驾驶从反应性感知数据共享转变为主动协调，提升了智能交通系统的安全性、效率和透明度。

Abstract: Multi-agent collaborative driving promises improvements in traffic safety and
efficiency through collective perception and decision making. However, existing
communication media -- including raw sensor data, neural network features, and
perception results -- suffer limitations in bandwidth efficiency, information
completeness, and agent interoperability. Moreover, traditional approaches have
largely ignored decision-level fusion, neglecting critical dimensions of
collaborative driving. In this paper we argue that addressing these challenges
requires a transition from purely perception-oriented data exchanges to
explicit intent and reasoning communication using natural language. Natural
language balances semantic density and communication bandwidth, adapts flexibly
to real-time conditions, and bridges heterogeneous agent platforms. By enabling
the direct communication of intentions, rationales, and decisions, it
transforms collaborative driving from reactive perception-data sharing into
proactive coordination, advancing safety, efficiency, and transparency in
intelligent transportation systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [117] [SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism](https://arxiv.org/abs/2507.01513)
*Beitao Chen,Xinyu Lyu,Lianli Gao,Jingkuan Song,Heng Tao Shen*

Main category: cs.CR

TL;DR: 论文分析了多模态大语言模型（MLLMs）的安全漏洞，提出了一种无需训练的防御框架SafePTR，通过选择性修剪有害令牌来提升安全性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在扩展视觉推理能力的同时，引入了新的安全漏洞，现有防御方法未能有效解决这些漏洞。

Method: 提出了SafePTR框架，通过选择性修剪早期中层的有害令牌并恢复良性特征，无需额外训练即可提升安全性。

Result: 实验表明，SafePTR在三种MLLMs和五个基准测试中显著降低了越狱风险，同时保持了模型效率。

Conclusion: SafePTR是一种高效且无需训练的防御方法，能够显著提升MLLMs的安全性。

Abstract: By incorporating visual inputs, Multimodal Large Language Models (MLLMs)
extend LLMs to support visual reasoning. However, this integration also
introduces new vulnerabilities, making MLLMs susceptible to multimodal
jailbreak attacks and hindering their safe deployment.Existing defense methods,
including Image-to-Text Translation, Safe Prompting, and Multimodal Safety
Tuning, attempt to address this by aligning multimodal inputs with LLMs'
built-in safeguards.Yet, they fall short in uncovering root causes of
multimodal vulnerabilities, particularly how harmful multimodal tokens trigger
jailbreak in MLLMs? Consequently, they remain vulnerable to text-driven
multimodal jailbreaks, often exhibiting overdefensive behaviors and imposing
heavy training overhead.To bridge this gap, we present an comprehensive
analysis of where, how and which harmful multimodal tokens bypass safeguards in
MLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers
are responsible for inducing unsafe behaviors, highlighting the potential of
precisely removing a small subset of harmful tokens, without requiring safety
tuning, can still effectively improve safety against jailbreaks. Motivated by
this, we propose Safe Prune-then-Restore (SafePTR), an training-free defense
framework that selectively prunes harmful tokens at vulnerable layers while
restoring benign features at subsequent layers.Without incurring additional
computational overhead, SafePTR significantly enhances the safety of MLLMs
while preserving efficiency. Extensive evaluations across three MLLMs and five
benchmarks demonstrate SafePTR's state-of-the-art performance in mitigating
jailbreak risks without compromising utility.

</details>


### [118] [Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems](https://arxiv.org/abs/2507.01808)
*Xiaoyu Ji,Jessica Shorland,Joshua Shank,Pascal Delpe-Brice,Latanya Sweeney,Jan Allebach,Ali Shakouri*

Main category: cs.CR

TL;DR: 论文提出了一种隐私保护平台，帮助中小型制造商安全共享数据，研究人员开发创新工具解决实际问题，并通过平台回传工具供他人使用。


<details>
  <summary>Details</summary>
Motivation: 中小型制造商因竞争和隐私问题不愿共享专有数据，但需要创新工具解决实际问题。

Method: 提出隐私保护平台，制造商通过安全方法共享数据，研究人员开发工具（如食品晶体图像分析工具），并通过平台回传解决方案。

Result: 开发了自动分析食品晶体大小分布和数量的工具，提高了效率和准确性，并通过隐私保护平台部署为Web应用。

Conclusion: 隐私保护平台成功解决了数据共享问题，未来可扩展更多应用。

Abstract: Small- and medium-sized manufacturers need innovative data tools but, because
of competition and privacy concerns, often do not want to share their
proprietary data with researchers who might be interested in helping. This
paper introduces a privacy-preserving platform by which manufacturers may
safely share their data with researchers through secure methods, so that those
researchers then create innovative tools to solve the manufacturers' real-world
problems, and then provide tools that execute solutions back onto the platform
for others to use with privacy and confidentiality guarantees. We illustrate
this problem through a particular use case which addresses an important problem
in the large-scale manufacturing of food crystals, which is that quality
control relies on image analysis tools. Previous to our research, food crystals
in the images were manually counted, which required substantial and
time-consuming human efforts, but we have developed and deployed a crystal
analysis tool which makes this process both more rapid and accurate. The tool
enables automatic characterization of the crystal size distribution and numbers
from microscope images while the natural imperfections from the sample
preparation are automatically removed; a machine learning model to count high
resolution translucent crystals and agglomeration of crystals was also
developed to aid in these efforts. The resulting algorithm was then packaged
for real-world use on the factory floor via a web-based app secured through the
originating privacy-preserving platform, allowing manufacturers to use it while
keeping their proprietary data secure. After demonstrating this full process,
future directions are also explored.

</details>
